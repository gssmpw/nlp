\section{Unveiling the Power and Pitfalls of Auxiliary Dataset}


\subsection{Problem Setup}
\label{sec::pre}
\textbf{Notations.} 
In this paper, we focus on the image classification task, where each sample $\mathbf{x} \in \mathcal{X}$ is associated with a label $y \in \mathcal{Y}$. A deep neural network (DNN) model $f_{\vtheta}$, parameterized by $\vtheta$, is employed to classify $\mathbf{x}$. The label set $\mathcal{Y} = \{1, \dots, K\}$ represents the space of possible classes ($K \geq 2$), and $\mathcal{X}$ denotes the input sample space. 



\textbf{Threat model.}
We consider a scenario in which an adversary manipulates a subset of the training data, embedding a hidden trigger to compromise the model. As a result, the victim model behaves normally on clean inputs but misclassifies any input containing the trigger $\Delta$ into a predefined target class $\hat{y}$. The proportion of manipulated samples in the training dataset is referred to as the \textbf{poisoning ratio}.

\textbf{Defender's goal and capabilities.}
The defender’s objective is to purify the victim model, mitigating the backdoor effect while preserving classification performance on clean, non-compromised inputs. This requires achieving an optimal balance between maintaining the model’s utility for its intended task and ensuring robustness against backdoor attacks. The defender has no prior knowledge of the backdoor trigger $\Delta$ or the target label $\hat{y}$, making the purification process more challenging. Additionally, the defender is provided with an auxiliary dataset, which is small and insufficient for training a new model from scratch.

\input{tables/overall}

\textbf{Classification of auxiliary datasets.}
As summarized in Table~\ref{overall}, we classify auxiliary datasets into two main categories based on their relationship to the training process of the victim models:
\begin{itemize}[leftmargin=*, topsep=2pt, itemsep=2pt, partopsep=2pt, parsep=2pt]
    \item \textbf{Seen data:} This category encompasses datasets that the victim models have been exposed to during the training phase. Such auxiliary datasets can be constructed by human inspection or applying some data filtering techniques \cite{chen2019detecting,zeng2022sift, zhu2023vdc} to the training datasets.
    \item \textbf{Unseen data:} Unseen dataset consist of data points that the model has not encountered during training. We further divided them into  two types:
    
    \ding{169} In-distribution data: These datasets contain samples that mirror the distribution of the training data but were excluded from the training process. Datasets in this type include but not limited to carefully selected subsets split from the original training or testing datasets.

    \ding{169} Out-of-distribution data: This type includes synthetic datasets\footnote{Advanced generative models may produce synthetic data that closely resembles in-distribution samples, but we still categorize these as OOD samples for simplicity and consistency.}, as well as external datasets sourced from external environments such as collecting from website. Out-of-distribution (OOD) data may significantly differ in characteristics from the training data, providing a challenge for backdoor purification.

\end{itemize}

\subsection{Role of Auxiliary Data in Backdoor Purification}
\label{sec::impact}

To analyze the role of auxiliary datasets in backdoor purification, we describe our experimental setup, including the dataset configurations, backdoor attack types, purification techniques, and evaluation metrics. All experiments are conducted on the standardized platform BackdoorBench \citep{wubackdoorbench, wu2024backdoorbench} to ensure fair comparisons.

\textbf{Datasets.} We conduct our experiments on three widely-used benchmark datasets: CIFAR-10 \citep{krizhevsky2009learning}, GTSRB \citep{stallkamp2011german}, and Tiny ImageNet \citep{le2015tiny}. 
For each dataset, we partition the original training set into two subsets:
\ding{182} \textbf{Training dataset} $\mathcal{D}_{\text{tr}}$, consisting of 95\% of the data, used for crafting poisoned dataset and training victim models. \ding{183} \textbf{Reserved dataset} $\mathcal{D}_{\text{unseen}}$, comprising the remaining 5\% in-distribution data and excluded from training.
To emulate practical defense scenarios, we construct clean \ding{184} \textbf{Seen dataset} $\Dseen$, derived from the poisoned dataset. For simplicity and clarity, $\Dseen$ is treated as an oracle dataset, assuming perfect knowledge of poisoning labels. This set allows us to assess the upper bound of backdoor purification performance when such dataset is available. Additionally, we generate \ding{185} \textbf{Synthetic dataset} $\Dsyn$, using generative models \citep{ho2020denoising,brock2018large}, enabling us to assess the utility of synthetic data in backdoor defenses.
%
To investigate the impact of more out-of-distribution data, we introduce \ding{186}\textbf{Brightness-transformed dataset} $\Dbri$, generated by applying brightness transformations which were intentionally excluded during training.This mimics auxiliary datasets collected under varying lighting conditions. For CIFAR-10, we include an \ding{187} \textbf{External dataset }$\Dext$, constructed by downsampling and selecting samples from ImageNet \citep{russakovsky2015imagenet} following the methodology in \citep{darlow2018cinic}, ensuring a diverse and realistic evaluation setting. More details of the datasets and visualization are provided in \app.


\textbf{Attack settings.} 
We evaluate seven widely studied backdoor attacks, including six dirty-label attacks (BadNets~\citep{gu2019badnets}, Blended~\citep{chen2017targeted}, WaNet~\citep{nguyen2021wanet}, Low Frequency  (LF)~\cite{zeng2021rethinking}, Input-aware~\citep{nguyen2020input} and Sample-Specific Backdoor Attack (SSBA)~\citep{li2021invisible}), and one clean-label attack (Sinusoidal Signal (SIG)~\citep{barni2019new}). By default, all attacks were executed with a 10\% poisoning rate, targeting the $0^{th}$ class unless otherwise specified. To provide a thorough analysis, we use two popular neural network architectures: PreAct-ResNet18 \citep{he2016identity} and VGG19-BN \citep{simonyan2014very}.

\textbf{Backdoor purification.}
We consider seven recent advanced backdoor defense methods, including two model modification techniques ANP \cite{wu2021adversarial} and NPD \cite{zhu2023neural},  five tuning-based methods (vanilla fine-tuning (FT), FT-SAM \cite{Zhu_2023_ICCV}, FST \cite{min2024towards}, NAD \citep{li2021neural}) and SAU \citep{wei2024shared}). To isolate the impact of auxiliary datasets, we standardize the size of the auxiliary dataset to 5\% of the original training dataset. This setting, in line with previous studies, is sufficient for most purification methods to demonstrate their effectiveness. \textbf{Remark}: Due to space constraints, we present only the most representative results here. Additional experimental findings, including results for VGG19-BN and other datasets such as Tiny ImageNet, are provided in the \app.

\textbf{Metrics.}
The effectiveness of each defense method is assessed using two key metrics: Accuracy on Clean Data (\(\textbf{ACC}\)) and Attack Success Rate (\(\textbf{ASR}\)). \(\textbf{ACC}\) evaluates the model's performance on unaltered samples, \(\textbf{ASR}\) quantifies the proportion of poisoned samples misclassified to the target label chosen by the attacker. An effective defense should maintain high \(\textbf{ACC}\), while minimizing \(\textbf{ASR}\) values, ensuring both model utility and robustness.

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/exp_1.pdf}
    \vspace{-0.3in}
    \caption{Performance of backdoor purification techniques equipped with different types of auxiliary dataset. Each experiment is run five times and the average value with error bar is reported. Results on more attacks and purification techniques are provided in \app.}
    \label{fig:curves}
    \vspace{-0.2in}
\end{figure*}

We conducted each experiment five times and report the average performance with error bars in Figure~\ref{fig:curves}. The results provide critical insights into the role of auxiliary datasets in backdoor purification:

\ding{182} \textbf{Semantic alignment drives clean accuracy preservation:} Auxiliary datasets derived from the training dataset (i.e., $\Dseen$) consistently achieve the highest clean accuracy across all defense techniques. This highlights the pivotal role of semantic alignment with the model’s training distribution in preserving performance on clean samples. In contrast, auxiliary datasets that introduce semantic shifts, such as brightness-transformed dataset ($\Dbri$), synthetic dataset ($\Dsyn$), or external dataset ($\Dext$), bring significant performance degradation in ACC. This suggests that models perform optimally when they receive auxiliary data that aligns with the  data they were trained on.

\ding{183} \textbf{Unseen data can facilitate purification:} This advantage is especially evident in scenarios where seen dataset struggles, such as fine-tuning (FT) defenses against Blended attacks. The likely explanation is that the semantic shift introduced by unseen data pushes models away from the backdoored initializations, effectively reducing the attack’s effectiveness. These findings align with recent studies \citep{Zhu_2023_ICCV, wei2024backdoor}, which suggest that significant deviations from the model’s initial weights, such as those introduced by unseen data, can aid in backdoor purification can aid backdoor purification.  
%
\emph{However, the effect of unseen data is not universally beneficial.} For attacks that are already mitigated well by seen data, the introduction of unseen dataset can sometimes increase the variability in ASR,  leading to either further reductions or unexpected increases. This variability underscores the complex relationship between the diversity of auxiliary data and the effectiveness of purification methods, highlighting that unseen data may not always yield consistent improvements.

\ding{184} \textbf{Purification techniques exhibit different sensitivity to auxiliary dataset variations}: The impact of auxiliary dataset characteristics varies significantly across backdoor defense techniques. Specifically, model modification techniques, such as ANP  \cite{wu2021adversarial} and NPD  \cite{zhu2023neural}, which incur limited changes to model parameters exhibit slightly less sensitivity to auxiliary dataset, showing less reduction in ACC when unseen auxiliary dataset is employed. In contrast, fine-tuning-based methods, which involve more substantial adjustments to the model's weights, are highly sensitive to the quality of the auxiliary data.  While they effectively reduce ASR,  they often experience notable ACC degradation when the auxiliary dataset diverges significantly from the training distribution. This sensitivity underscores the importance of carefully selecting auxiliary datasets when using fine-tuning-based methods for backdoor purification.



\textbf{Investigation in latent space.} To gain deeper insights into the role of auxiliary datasets in backdoor purification, We analyze the latent space representations of the victim model. Specifically, we visualize the feature distributions of both the seen dataset and various auxiliary datasets using t-SNE, which allows us to highlight the shifts introduced by the auxiliary data.
%
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/tsne_1.pdf}
    % \vspace{-0.2in}
    \caption{t-SNE visualization of features from different auxiliary datasets. In each plot, points representing different classes are depicted in distinct colors. Features from the seen dataset are marked with dots, while those from other datasets are represented with crosses.}
    \label{tsnep}
    % \vspace{-0.25in}
\end{figure}
%
We conduct experiments using PreAct-ResNet-18 on a victim model compromised by the BadNets attack. As summarized in Figure~\ref{tsnep}, we compare the feature distributions of the seen dataset with those of each unseen dataset. The t-SNE visualizations reveal that while the victim model can effectively extract features from diverse datasets, unseen auxiliary data introduce noticeable shifts in the feature space. This effect is particularly pronounced when the auxiliary dataset diverges from the original training distribution. For instance, brightness-transformed data exhibits a moderate deviation, indicating that even simple transformations impact the model’s  feature representations. Synthetic and external datasets show the most significant shifts, indicating that semantic transformations substantially alter the feature.

\textbf{In summary,} These findings emphasize the sensitivity of backdoor purification techniques to the choice of auxiliary datasets. Datasets with substantial semantic shifts can degrade model accuracy and destabilize attack success rates, underscoring the importance of carefully selecting high-quality auxiliary datasets.

