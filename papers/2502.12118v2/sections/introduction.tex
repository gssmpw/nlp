\begin{figure}[!h]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=0.73\linewidth]{figures/figure-abstract.pdf}
    \vspace{-0.2cm}
    \caption{\footnotesize{\textbf{Scaling test-time compute}}: \textbf{\emph{(Top)}} Given a set of problems, verifier-free (VF) methods query expert traces, whereas verifier-based (VB) methods collect reward annotations for rollouts from the base LLM. Crucially, one aims to mimic ``good'' traces and the other seeks to improve via access to verification. We prove a $\sqrt{H}$ gap between a simple VB method and \emph{any} VF method as we scale data $n$ and compute $H$. \textbf{\emph{(Bottom)}} Fixing $n$, and scaling $H$, we verify the gap between VF and VB in practice by comparing the performance of the recently released S1 model~\cite{muennighoff2025s1} trained with a VF approach: supervised distillation, and a simple VB method: best-of-N search (left). For the models we train, we also compare VF and VB when we scale \emph{both} test compute and verifier training data, where the gap between VF and VB grows, matching our theoretical result (right).}
    \label{fig:abstract-figure}
    \vspace{0.38cm}
\end{figure}

\abscontent

\vspace{-1.4cm}
\section{Introduction}
\label{sec:introduction}
\vspace{-0.10cm}


Pre-training and post-training  of large language models (LLMs) rely heavily on access to high-quality {``expert''} data, but it is projected that by 2028, the availability of such data on the Internet will diminish~\citep{villalobos2022will,liu2024best}, and improving model performance on several domains (\textit{e.g.}, reasoning, safety, \textit{etc.}) often requires more data~\cite{li2024common}. 
As a result, scaling test-time compute is emerging as an alternate paradigm for improving reasoning performance, where an LLM is made capable of executing search or refinement procedures, either implicitly by training for it, or by explicitly executing the search on top of the LLM outputs. The goal here is to try and find the best response for a test query, which naturally leads to responses that are often longer than a direct answer by spending more compute. Broadly, we can classify the set of prevalant approaches for scaling test compute into two categories (see Figure~\ref{fig:abstract-figure}).

The first class of approaches uses some form of \emph{verification}, \textit{e.g.}, a 0/1 outcome reward or a verifier, either for test-time search~\citep{cobbe2021training} or as rewards in reinforcement learning (RL), \textit{e.g.}, Deepseek R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, DeepScaleR~\cite{deepscaler2025}, and Kimi K1.5~\cite{MoonshotAI} that use the outcome reward. The second class of approaches circumvents verification altogether and runs supervised fine-tuning on ``expert'' traces, obtained either by piecing together tree search~\cite{gandhi2024stream,moon2024guided}, or by querying bigger models trained to generate longer traces, \textit{i.e.},  distillation approaches (\textit{e.g.}, S1~\cite{muennighoff2025s1}, OpenThinker~\cite{openthoughts},
\textit{etc.}). We refer to these as \emph{``verifier-free''}, as these methods do not query any verification signal for guiding learning. Despite the prevalence of both classes of methods, \textit{i.e.}, verifier-based (VB) and verifier-free (VF), it is not clear which class results in better use of test-time compute, nor which type of method will come out ahead as the amount of available test-time compute increases. In this paper, we theoretically and empirically show that when fine-tuning a pre-trained LLM, VB methods are expected to perform better on both of these fronts under realistic conditions on the properties of the pre-trained LLM. 


\emph{\textbf{Theoretically, we show in this paper that VB methods outperform VF methods as we scale data and compute.}} To do so, we operate in a setting where we are given a base LLM $\pibase$ and a dataset of problems. We represent the total available test-time compute in terms of the total number of tokens $H$ that can be used to produce a solution. Our goal is to finetune $\pibase$ to make efficient use of test-time compute, \textit{i.e.}, attain best performance within a given compute budget.
For learning, VF methods are allowed to obtain at most $n$ correct solution traces for these problems by querying an expert (\textit{e.g.}, humans, linearized search~\citep{gandhi2024stream}, \textit{etc.}). On the other hand, VB methods are allowed to query a reward annotator that measures correctness of a given response on $n$ samples generated from $\pibase$ but never observes expert traces. When operating in this setup (which reflects practical scenarios), we prove that as we scale training data $n$ and test-time compute budget $H$, the separation between a simple verifier-based approach based on RL, and \emph{any} verifier-free method grows (see Figure~\ref{fig:abstract-figure}). Since both class of approaches involve finetuning $\pibase$, the properties of the pre-trained  $\pibase$ plays a key role in our separation result, which we discuss next.

\emph{\textbf{What properties of the pre-trained LLM enable VB methods to scale better than VF methods?}} To evaluate the performance of different approaches across varying test-time compute budgets, we define rewards that are high when the LLM arrives at the correct solution, and does so without completely exhausting the provided token budget. 
We show that when the base LLM admits a sufficiently \emph{heterogeneous} distribution over rewards (\textit{i.e.}, it admits coverage over multiple correct sequences of varying lengths for a given problem), then scaling test-time compute by running \emph{any} VF approach is suboptimal. In contrast, under a specific form of heterogeneity (which is also generally satisfied in practice), the performance of the policy obtained by running RL with verifiers (either implicit 0/1 ``regex'' matching rewards or explicitly trained numerical, generative verifiers) is better at any given test-time compute budget, and, moreover, also scales better as we further increase test-time compute. We term this specific form of heterogeneity as  \emph{anti-concentration}: for a given problem, a base LLM is said to be anti-concentrated if it admits  non-trivial probability mass over solution traces that achieve rewards slightly better than the mean reward for that problem under the base policy. 
In Figure~\ref{fig:dist-sketch} we illustrate these two properties: heterogeneity and anti-concentration. Given a problem, if the base LLM samples correct responses of varying response length (heterogeneity), but also samples solution traces that are rewarded higher than the mean reward on that problem  (anti-concentration);  we derive the following result on the performance of VB and VF methods as we scale training data $n$ and test-time compute $H$.   
















\begin{AIbox}{Main theoretical result}
When the induced distribution of rewards under the pre-trained $\pibase$ is anti-concentrated, and $\pibase$ is heterogeneous: \textbf{(i)} for a fixed test-time compute budget $H$, verifier-based RL and search algorithms outperform \textbf{\emph{any}} verifier-free algorithm; \textbf{(ii)} the gap between the two grows as $\sqrt{H}$ as we scale both data $n = \Omega(H)$, and test compute $H$.
\end{AIbox}






\emph{\textbf{Overview of how we show the separation between VB and VF.}} To arrive at our main result,  we first prove an information-theoretic lower bound showing that the suboptimality of \emph{any} VF algorithm scales as the heterogeneity or diversity of the base policy being finetuned, which implies a suboptimality gap of $\Omega(H/\sqrt{n})$ in the worst case. For this we build on second-order suboptimality bounds for supervised finetuning in \citet{foster2024behavior}. 
That said, we then show that the suboptimality for a simple VB method that runs RL with a trained verifier or actual 0/1 outcome rewards scales as $\gO(H/n)$. 
In fact, we show that the heterogeneity of the base policy is often helpful for VB approaches, as long as the heterogeneity also implies anti-concentration, \textit{i.e.}, there is a reasonable chance to sample traces with rewards slightly better than the mean reward attained under the base policy. Consequently, the performance difference between VB and VF methods scales with the horizon or test compute $H$. This 
implies the need for training verifiers, running RL, or at the very least, using rewards when finetuning LLMs for test-time scaling. 

\begin{figure}
        \centering
    \includegraphics[width=0.75\linewidth]{figures/fig-intro-2.pdf}
    \caption{\footnotesize{\textbf{\emph{Properties of the base LLM that enable VB methods to outperform VF}:} We show toy illustrations of possible reward distributions under the base LLM, and which class of approaches to perform better for each. In particular, two key properties of the reward distribution  enables verifier-based methods to outperform verifier-free methods: heterogeneity (Property~\ref{def:exp-heterogeneity}), \textit{i.e.}, for any given problem, the variance of rewards under the base LLM is high;  and anti-concentration (Property~\ref{prp::anti-conc}), \textit{i.e.}, with high probability the base LLM samples solution traces with rewards that are better than the mean reward on that problem.}} 
    \label{fig:dist-sketch}
\end{figure}

\emph{\textbf{Empirical results corroborating theory.}} We corroborate our theoretical results on math reasoning with 3B/8B Llama models, and the S1~\cite{muennighoff2025s1} model. For the S1 model that is trained in a verifier-free manner, we show that a simple verifier-based approach performs better than S1 across  a set of test-time compute budgets (Figure~\ref{fig:abstract-figure}). For the LLama models, we  explicitly control the heterogeneity of the base LLM and show that VF methods perform poorly with more heterogeneous base LLMs, and that the gap between VB and VF performance scales with more test-time compute (Figure~\ref{fig:scaling-main-panel} in Section~\ref{sec:experiments-math-reasoning}). Our investigation also reveals that common pre-trained LLMs are indeed heterogeneous and satisfy anti-concentration, which are abstractions we introduce to prove our theoretical results (Figure~\ref{fig:sigma-dist-math},~\ref{fig:anti-conc-math}).
To the best our knowledge, this is the first theoretical result and systematic study showing a separation between VF and VB methods, under realistic assumptions on the base model.

\emph{\textbf{Implications and takeaways.}} To summarize, this paper presents several implications for practitioners. As long as the pre-trained LLM is sufficiently hetergeneous and anti-concentrated, which we verify are both satsified in practice for models we evaluate, our analysis implies:
\vspace{-0.1cm}
\begin{enumerate}[topsep=0pt,parsep=0pt, after=\vspace{-0.22\baselineskip}]
    \item While recent results imply that both verifier-free and verifier-based methods can work well at a large compute budget, we show that VB methods scale better as the test-time budget increases.
    \item This gap can be amplified further when the number of prompts for VB finetuning also scales linearly with the allowed test-time token budget.
\end{enumerate}
Another implication of our results is that since VF methods work well when heterogeneity is small, this means that in order for them to scale well, pre-trained LLMs must also exhibit low heterogeneity, but we believe that this is rarely the case with modern LLMs in practice.
