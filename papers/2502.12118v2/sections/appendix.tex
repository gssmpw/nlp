\part*{Appendices}













\textbf{A. Proofs from Section 5.}

\textbf{B. Additional Related Work.}

\textbf{C. Additional Experiments in the Didactic Setup.}

\textbf{D. Additional Experiments on MATH.}


\section{Proofs from Section~\ref{sec:theory-compare-vfree-vbased}}
\label{sec:proofs}



\input{sections/proofs/useful_lemmas}
\input{sections/proofs/expert-variance-lower-bound}
\input{sections/proofs/verifier-free-thm}
\input{sections/proofs/finding-mu}
\input{sections/proofs/vg-gap-lb}
\input{sections/proofs/verifier-accuracy-lzone}
\input{sections/proofs/single-prompt-verifier-free-thm}








\section{Additional Related Work}
\label{sec:additional-rel-work}


\textbf{Scaling test-time compute.} Recent works~\cite{sardana2023beyond,snell2024scaling} have shown that scaling test-time compute can improve performance at a rate faster than that afforded by traditional approaches of scaling data~\cite{li2024common} or model parameters~\cite{hoffmann2022training}, implying that training compute can often be traded off optimally for test-compute~\cite{villalobos2023trading,jones2021scaling}. There are two popular ways of spending test compute. First, is to autoregressively sample from the LLM long ``chains-of-thought''  that resemble linearized search traces~\cite{yao2023tree,gandhi2024stream} or an iterative refinement of answers~\cite{qu2024recursive,kumar2024training}. Second, is to explicitly implement search procedures~\cite{wu2024inference,beeching2024scalingtesttimecompute} with trained verifiers~\cite{cobbe2021gsm8k,setlur2024rewarding}. In our work, we empirically show that either of these approaches can scale well, and both theoretically and empirically examine a different and critical axis of separating these approaches: access to verification during training or inference. Additionally,  recent works~\cite{chen2024not,setlur2025opt} raise concerns about the unncessary wastage of test-time compute by sampling overly long responses for even simple questions~\cite{qwen2}. In our work, we use a ``bi-level'' reward formulation to capture what it means to efficiently use test-compute, and how to compare the asymptotic compute efficiency of verifier-free and verfier-based algorithms. 

\textbf{Access to verification.} We say that a finetuning algorithm has access to verification if it directly uses ground truth rewards to finetune LLMs, \textit{e.g.}, the 0/1 correctness labels on math solutions~\cite{uesato2022solving,bi2024deepseek}; or if it queries trained verifiers for collecting training data~\cite{hosseini2024v} and running search procedures at test-time~\cite{welleck2024decoding,chen2024more,chow2024inference}. The former approach of training LLMs to generate long ``chains of thought'' with final reward on-policy RL~\cite{MoonshotAI,deepseekai2025deepseekr1incentivizingreasoningcapability} has shown impressive gains on reasoning benchmarks. For off-policy RL algorithms~\cite{rafailov2023direct,zelikman2022star,singh2023beyond} that utilize verification, converting the same 0/1 rewards into value function based process verification has been shown to be critical~\cite{setlur2024rl}. 
Apart from these verification can also be generative~\cite{zhang2024generative} and implicit~\cite{yuan2024implicitprm} where the same LLM is trained to generate and self-verify responses iteratively.  In this work, we bucket all the above as verifier-based algorithms, and formally show that  the asymptotic performance of this class scales test-compute more efficiently than approaches that do not query any sort of rewards, highlighting the critical role played by access to verification. 




\textbf{Verifier-free algorithms.} Multiple works have proposed to scale test-time compute by finetuning pre-trained LLMs on manually stitched search traces~\cite{gandhi2024stream,nie2024evolve} that all lead to the correct solution. The goal here is to force the LLM to mimic known search procedures like Monte-Carlo tree search~\cite{yang2022chain,xie2024monte} or A$^\star$~\cite{lehnert2024beyond} on training questions, with the hope that the LLM learns to search for solutions on test problems too~\cite{sel2023algorithm}. Crucially these algorithms do not annotate search trajectories in the training data with any reward, and the LLM is forced to mimic multipe search traces that are ``heterogeneous'' in nature, \textit{i.e.}, different traces spending varying number of tokens (for search) to arrive at the same final solution. In our work, we analyze how this heterogeneous nature makes it hard for \emph{any} supervised finetuning algorithm to generalize, resulting in a poor test-time scaling law for these,  matching observations in practice ~\citep{kumar2024training,xiang2025towards}.







\section{Additional Experiments in the Didactic Setup}
\label{sec:additional-didactic}




\textbf{Details on the setup.} We generalize the planted subsequence problem from \cite{setlur2024rewarding}. The input prompt is a sequence of length $5$ with the tokens chosen randomly from the set $\{1,2, 3,\ldots,10\}$. We fix the unknown function to be $g(x)=2x+5$. We fix the vocabulary for the policy we are training to be the set $\gV \eqdef \{0, \ldots, 30\}$.  Here $0$ is treated as the padding token. 
Concretely, for an input problem  $\bx = (x_1,$$..$$, x_5)$, we say that a response $\by$ with $H$ tokens from the vocabulary $\gV$  
is a correct trace if there exists a \emph{gold} contiguous subsequence $(g(x_1),$$..,$$g(x_{5}))$ planted in $\by$. Here, the underlying mapping $g:$$[10]$$\mapsto$$[30]$ is fixed but unknown.  For a state $\bs \eqdef (\bx, a_1,$$ ..$$, a_h)$, the bi-level reward $r(\bs) = 1$ if and only if there exists some $h^\prime \leq h$ such that the last $5$ tokens before $h'$ \textit{i.e.},  $(a_{h^\prime - 4},$$..,$$a_{h^\prime})$
match the gold subsequence.  In order to use the same performance scale to compare  methods trained for different horizon $H$ values (test-time compute budget), we  $J_r(\pi)$ and divide it by the maximum reward of $H-4$. 


We wish to construct base policies $\pibase$ that: \textbf{(i)} differ in heterogeneity, and \textbf{(ii)} satisfy the anti-concentration condition. To do so, we finetune GPT2-xl~\cite{radford2019language} on samples obtained from a mixture of hand-designed ``procedural'' policies. Inspired from \citet{setlur2024rewarding}, a procedural policy $\mu_\gamma (\by^\star_{k+1} | \bs)$$\propto$ $\gamma$, when the last $k$ tokens in the state $\bs$, match the first $k$ tokens in the gold subsequence $\by^\star$.  Thus, the normalized return for $\mu_\gamma$$\rightarrow$$1$, as $\gamma$$\rightarrow$$\infty$. We vary the heterogeneity of $\pibase$ by finetuning GPT2-xl on data from a mixture of procedural policies with $\gamma $ $\in$ $\{5, 10, 20, 50, 100, 500\}$. 
Once the last $5$ tokens match the gold sequence, the procedural policy puts mass  $\propto \gamma$ on the padding token $0$. 
See Figure~\ref{fig:examples-didactic} for an illustration of data sampled from different procedural policies. 

For any compute budget $H$ (token length), we train separate SFT and RL policies, where SFT is run on traces that are $H$ tokens long. We also run RL on the same token budget, against a trained verifier. The verifier is trained on samples from the base policy. For this, we train a GPT2-xl transformer as a multiclass classifier, that takes in an $H$ length sequence and outputs a single value in $0$ to $H$ (i.e., it is an $H+1$-way classifier). 



\textbf{Experiment details.} For the RL runs, we use REINFORCE~\cite{ahmadian2024back} train for 20k iterations in both with a batch size of 64, and a constant learning rate of $1e-4$, with the Adam optimizer. The RL runs are initialized with the base policy, and to prevent reward hacking we also use a KL penalty (with weight $0.2$), in addition to the REINFORCE training objective. 
For every trace in a batch, we query the trained verifier, which outputs a value between $0$ and $H$, which directly tells us where the ``staircase'' appears in the bi-level reward. For example, a value of $2$ implies that the staircase appears on the second last token. We convert this outcome supervision into token-level 0/1 rewards and update the policy with the computed policy gradient. For SFT, we also use the Adam optimizer with a learning rate of $2e-4$, and a batch size of $64$. Similar to RL, we apply a KL regularization term in addition to the next token prediction loss (ignoring the padding token 0), where the strength of the KL term is the same as RL. SFT runs are also initialized with the base policy. Using the same hyperparameters, we obtain the base policy by running SFT on 200k data points sampled i.i.d. from  the uniform mixture over procedural policies outlined above. To collect training data for the verifier, we draw a random sample of $\nicefrac{n}{\log H}$ prompts in $\gD_\mathrm{tr}$, and then make $\log(H)$ calls on each of them to binary search for the token where the correct answer first appeared. This way, we only query reward annotator $n$ times. Finally, for our experiments, where we vary base and expert policy heterogeneity, we simply change $\gamma$ (reducing variance over it), in a way that the average performance of the base/expert policy remains roughtly the same. 


\begin{figure}
    \centering
    \includegraphics[width=0.55\linewidth]{figures/didactic-example.pdf}
    \caption{\textbf{\emph{Procedural policies for the generalized planted subsequence problem:}} For two values of $\gamma$: 10, and 1000, we show examples of two draws, over $H=10$ tokens from each. Here, the unknown mapping is $g(x)=2x+5$. When $\gamma$ is $1000$, the policy (over the first 5 tokens) is almost like a dirac delta distribution on the gold subsequence, followed by which it samples the padding tokens. On the other hand, when $\gamma=10$, it makes multiple attempts and completing the sequence. Once it fails, it makes a new attempt. In the second sample, we see that after a few tokens it gets the correct sequence, achieving a total bi-level reward of $3$, and normalizing it with $H-4$, we get a normalized reward of $0.5$.}
    \label{fig:examples-didactic}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/toy_verifier_acc.pdf}
    \hspace{2em}
    \includegraphics[width=0.35\linewidth]{figures/toy_vary_sigmab_acc.pdf}
    \caption{\textbf{\emph{Accuracy of trained verifier:}}
    (Left) we plot the accuracy of the verifier (black line), as we scale the horizon (black line). We also plot the performance of RL with ground-truth (GT) bi-level rewards, and compare it with RL using the trained verifier. (Right) As we vary base policy heterogeneity we plot the accuracy of the verifier on two distributions: (i) on base policy $\pibase$, (ii) on policy learned by running RL $\hat{\pi}^\mathrm{vb}_n$.    }
    \label{fig:didactic-verifier-acc-1}
\end{figure}


\textbf{Accuracy of trained verifier.} In Figure~\ref{fig:didactic-verifier-acc-1}(left), we plot the accuracy of the verifier (black line), as we scale the horizon. We fix the data budget to $n=2^14$ here. Since, here budget implies a multi-class classification over more classes, the problem hardness increases for the verifier, which explains the performance drop. Initially, we do see an improvement with $H$, since the coverage over high reward trajectories improves with $H$, as we sample the base policy for longer. 
We also plot the upper bound on RL performance, where we train the RL policy with ground-truth staircase rewards. Looking at its performance, it is clear that across all horizons, RL with trained verifier mainly suffers from the inaccuracy of the trained verifier (i.e., reward hacking issues). In Figure~\ref{fig:didactic-verifier-acc-1}(right), we plot the accuracy of the learned verifier on two distributions (base policy), and the policy learned by RL. As we reduce base policy heterogeneity, it is easier to generalize on the base policy, but the verifier is inaccurate outside the narrow distribution of the base policy, making it more susceptible to reward hacking. As a result, we observe poor accuracy on the learned policy's distribution. 

\vspace{-1em}

\section{Additional Experiments on MATH}
\label{sec:additional-math}




\textbf{Experiment details.} We run all our training on the questions in the training set of MATH~\cite{hendrycksmath2021}, and run our test on the MATH500 evaluation benchmark. Any problem instance is determined by the data budget $n$ and compute budget $H$. For a fixed compute budget $H$, we run SFT on trajectories of search traces. A single search trace consists of multiple turns of responses. Each response is at most 512 tokens long. A trace can thus comprise of anywhere between $1$ to $H/512$ turns (sometimes more for shorter responses).  Here, all turns except the last one are responses that result in incorrect final answers for the problem. This is exactly the recipe followed by \citet{qu2024recursive} and \citet{snell2024scaling}, for learning to self-correct, revise responses, or more generally use sequential compute at test-time. See below for examples of search traces of SFT. For training the verifier, we simply use samples from the base LLM, and use 0/1 rewards to train a model of the same size as the policy with cross-entropy loss. We use a batchsize of 32 and learning rate of $1e-6$ for all our experiments. We run SFT and verifier training for 10000 iterations on each instance. We use a weight decay of 0.01 for training both. At test-time, we measure the performance of SFT using majority voting over sequential revisions, following~\citet{snell2024scaling}. We sample rollouts with temperature 1.0.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.33\linewidth]{figures/ver_acc.pdf}
    \caption{\textbf{\emph{Accuracy of verifier:}} We plot the accuracy of verifier as we scale data, for Llama3-3b and 8b sized models. We find that the larger model generalizes better across  all data budgets.}
    \label{fig:verifier-acc}
\end{figure}

\textbf{Verifier accuracy.} In Figure~\ref{fig:verifier-acc} we plot the test accuracy of the trained verifier as we scale data. Interestingly, we find that the bigger model always generalizes better, and models of both sizes improve performance at the same rate, as we scale the data budget $n$.






\resizebox{0.99\linewidth}{!}{
\begin{analysisbox}[Search trace with sequential revisions for SFT (example with 2 episodes)]

\textbf{Query:} A farmer has three times as many chickens as cows on his farm. If he has 15 cows, how many chickens does he have?  

\textbf{Turn-1 Response:}  
Step 1: Let’s assume the number of cows as \( x \).  
Step 2: According to the given information, the farmer has three times as many chickens as cows. So, the number of chickens can be expressed as \( 3x \).  
Step 3: We are given that there are 15 cows, so let’s substitute \( x = 15 \) into the equation:  
\[
3x = 3(15) = 30
\]  
Answer: \(\boxed{30}\) INCORRECT

\textbf{Turn-3 Response:}  
Let’s solve the problem step by step:  
Step 1: Let’s denote the number of cows as \( x \).  
Step 2: We are given that the farmer has three times as many chickens as cows. So, the number of chickens can be expressed as \( 3x \).  
Step 3: We are also given that there are 15 cows, so we can substitute \( x = 15 \) into the expression for the number of chickens:  
\[
3 \times 15 = 45
\]  
Answer: \(\boxed{45}\)  CORRECT

\end{analysisbox}
}

\resizebox{0.99\linewidth}{!}{
\begin{analysisbox}[Search trace with sequential revisions for SFT (example with 3 episodes)]

\textbf{Query:} A rectangle’s length is five more than twice its width. If its perimeter is 58 cm, what is its width?  

\textbf{Turn-1 Response:}  
Step 1: Let’s assume the width of the rectangle as \( x \).  
Step 2: According to the given information, the length is five more than twice the width. So, the length can be expressed as \( 2x + 5 \).  
Step 3: The perimeter of a rectangle is given by the formula:  
\[
\text{Perimeter} = 2(\text{Length} + \text{Width})
\]  
Step 4: Substituting the given perimeter of 58 cm and our expressions for length and width:  
\[
2(x + 2x + 5) = 58
\]  
\[
x = \frac{53}{6}
\]  
Answer: \(\boxed{\frac{53}{6}}\) INCORRECT


\textbf{Turn-2 Response (Subtle Error Introduced):}  
Step 1: The width is \( x \) and the length is \( 2x + 5 \).  
Step 2: Using the perimeter formula:  
\[
2(x + 2x + 5) = 58
\]  
\[
6x + 5 = 58
\]  
\[
x = 7
\]  
Answer: \(\boxed{7}\) INCORRECT


\textbf{Turn-3 Response:}  
Let’s solve the problem step by step:  
Step 1: Let’s denote the width of the rectangle as \( x \).  
Step 2: The length is five more than twice the width, so it can be expressed as \( 2x + 5 \).  
Step 3: The perimeter formula for a rectangle is:  
\[
\text{Perimeter} = 2(\text{Length} + \text{Width})
\]  
Step 4: Substituting the given perimeter of 58 cm:  
\[
2(x + 2x + 5) = 58
\]  
\[
x = 8
\]  
Answer: \(\boxed{8}\) INCORRECT
\end{analysisbox}
}







