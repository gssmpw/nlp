\vspace{-0.20cm}
\section{Results: Large-Scale Math Reasoning}
\label{sec:experiments-math-reasoning}
\vspace{-0.1cm}

Next, we extend our empirical results to math reasoning problems where we compare VF supervised finetuning on manually stitched search traces,
and VB best-of-$N$ search (BoN)~\cite{cobbe2021training}. In BoN, we sample multiple responses from the base LLM, and choose the best one with an outcome verifier trained to predict 0/1 correctness labels. Here, the verifier is trained on 
$n$ samples generated from the base LLM for questions in the training data. Thus, BoN mimics the first few iterations of a VB online RL algorithm, initialized with the base LLM, and that maximizes rewards from a trained verifier.
We mainly evaluate performance on the MATH~\cite{hendrycksmath2021} reasoning benchmark, and use LLama-3.1/3.2 8B/3B instruct models~\cite{dubey2024llama} supervised finetuned on MATH as the base LLMs. We vary the test-time compute budget from $2^9$ to $2^{13}$ tokens, and also vary the training data budget $n$ from $2^{12}$ to $2^{16}$. Additional details are in Appendix~\ref{sec:additional-math}.

\textbf{Verifier-free approach: SFT on stitched search traces.} Motivated by the approach of scaling test-time compute via iterative revisions~\citep{qu2024recursive,snell2024scaling}, in this setting, we SFT $\pibase$ to spend the total test-time compute budget $H$ on running as many rounds of revision as possible within the budget~\cite{kumar2024training} . To construct SFT data, we follow the approach of \citet{snell2024scaling} and construct an expert policy that is ``close'' to $\pibase$ by first sampling a bunch of correct/incorrect \emph{solution} traces from $\pibase$, and then manually stitching a uniformly random number of incorrect solutions followed by the correct one together into one \emph{search} trace. 


\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/math_8b_and_3b_vary_compute.pdf}
        \caption*{\footnotesize(a)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/math_8b_and_3b_vary_samples.pdf}
        \caption*{\footnotesize(b)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]
        {figures/math_3b_vary_both.pdf}
        \caption*{\footnotesize(c)}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{\footnotesize{\textbf{\emph{Scaling test compute $H$ and training data $n$ on MATH:}} We compare two common algorithms for spending test compute: (i) verifier-free SFT on manually stitched sequential revisions~\cite{qu2024recursive,muennighoff2025s1} from an expert, and (ii) BoN~\cite{cobbe2021gsm8k} search using a verifier trained on base LLM. In (a), we scale $H$, with data size $n$$=$$2^{14}$, and find BoN scales test-compute by $8\times$ over SFT. In (b), we fix $H$$=$$2^{12}$, scale $n$, and note the $6\times$ gain in sample efficiency for BoN. In (c), we compare RL and SFT following Definition~\ref{def:h-alpha-scaling} where we scale both $n$ and $H$, and corroborating Theorem~\ref{thm:vg-gap-lower-bound} the gap between RL and SFT grows super linearly.}
    \vspace{-0.25cm}}
    \label{fig:scaling-main-panel}
\end{figure*}






\textbf{Verifier-based approach: Best-of-N sampling against a verifier.} For each training problem, we collect a given number of traces $\sim$ $\pibase$, and label them with a 0/1 correctness score based on final answer match. We then train a verifier with binary cross-entropy loss. On a test problem, we use the verifier to rank $N$ solutions from $\pibase(\cdot|\bx)$, at temperature $1.0$ and choose the best one ($N$ scales linearly in budget $H$). While we run online RL in Section~\ref{sec:didactice}, due to computational constraints at higher $H$, we only compare with BoN here, which runs 1-step of policy improvement.

\textbf{VB BoN scales compute by $8\times$, data by $6\times$ of VF SFT.} At a fixed data budget of $2^{14}$ samples, BoN scales test-time compute by $8\times$ over SFT, and at a fixed test compute of $2^{12}$ tokens, VB scales data efficiency by $6\times$ (Figure~\ref{fig:scaling-main-panel}(a)(b)). Revisiting Definition~\ref{def:h-alpha-scaling}, we scale $n$ with $H$ and analyze the gap between BoN and SFT. We find that the accuracy gap grows super linearly in $\log H$, \textit{i.e.}, the reward gap grows as $\Omega(\sqrt{H})$ (Figure~\ref{fig:scaling-main-panel}(c)), matching Theorem~\ref{thm:main-theorem}. 


\begin{figure}[!th]
    \centering
    \begin{subfigure}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/math_compare_with_s1.pdf}
    \end{subfigure}
    \begin{subfigure}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/aime_compare_with_s1.pdf}
    \end{subfigure}
    \vspace{-0.1cm}
    \caption{\footnotesize{\textbf{\emph{Results with s1:}} Fixing training data set, and only scaling test-time compute budget, we compare the performance of the  s1 model~\cite{muennighoff2025s1} trained with a verifier-free approach: supervised distillation, and our simple VB method: best-of-N search. In a compute matched evaluation, we find that sampling $N$ short responses and selecting the best one with the trained verifier outperforms the budget forcing approach used in \citet{muennighoff2025s1}.}
    \vspace{-0.2cm}
    }
    \label{fig:s1-results-panel}
\end{figure}

\textbf{s1 model trained with verifier-free distillation performs worse than simple BoN.} In Figure~\ref{fig:s1-results-panel}, across different test compute budgets, we plot the performance of the budget forcing method in \citet{muennighoff2025s1}, that scales test compute over the s1 model. The s1 model itself was trained by running supervised distillation over traces from the Gemini Thinking~\cite{geminithinking} model in a verifier-free manner. We compare this with BoN, where we sample $N$ responses of length $2^9$ (MATH500) or $2^{10}$ (AIME) and choose the best one with a trained outcome verifier. In a compute matched evaluation, we find that even when we fix the training data $n$, the verification based BoN approach improves over budget-constrained s1. 
\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/vary_sigma.pdf}
    \end{subfigure}
    \begin{subfigure}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sigma_dist_math_8b.pdf}
    \end{subfigure}
    \vspace{-0.1cm}
    \caption{\footnotesize{\textbf{\emph{Heterogeneity hurts SFT, but SFT outperforms BoN on homogeneous problems:}} Across  problems, we plot the distribution of $\sigma_{\bx}$ (Definition~\ref{def:exp-heterogeneity}), bucket problems by heterogeneity, and run SFT, BoN on each bucket. We find that verifier-free SFT can outperform BoN when the heterogeneity measured by  $\sigma_\bx$ is low, but the opposite is true when $\sigma_\bx$ is high.
    \vspace{-0.25cm}
    }}
    \label{fig:sigma-dist-math}
\end{figure}

\textbf{VF generalizes on less heterogeneous problems, but memorizes heterogeneous ones.} We analyze the performance of running SFT/BoN on different problem buckets, where each bucket consists of problems of low, medium or high value of heterogeneity, at token budget $2^{10}$ (Figure~\ref{fig:sigma-dist-math}). When $\sigma_\bx$ is small, VF SFT clones the trace well and improves over VB BoN, which can suffer from lack of coverage or inaccuracy of verifier (Appendix~\ref{sec:additional-math}). In contrast, when $\sigma_\bx$ is larger, VB BoN dominates since VF SFT fails to generalize under heterogeneity and mainly memorizes responses.
The distribution of $\sigma_\bx$ is also skewed towards higher values, resulting in VB methods performing better on average (Figure~\ref{fig:scaling-main-panel}).  
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.70\linewidth]{figures/reward_dist_math_8b.pdf}
    \vspace{-0.1cm}    
    \caption{\footnotesize{\textbf{\emph{Anti-concentration coefficient in practice:}} For easy and hard problem sets in MATH, we compute the distribution of bi-level rewards on the correct traces sampled from base LLM. We find that for $\kappa = 0.5$ (controls the $\chi^2$ ball of the expert policy), there is a non-trivial  $(\approx 1/4)$ probability of observing rewards better than the mean reward by atleast $\sigma \sqrt{\kappa}$ ($\sigma$ computed by averaging over prompts in the easy/hard bucket), implying that base policy is roughly $0.25$-anti-concentrated (Property~\ref{prp::anti-conc}).}}
    \vspace{-0.25cm}
    \label{fig:anti-conc-math}
\end{figure}

\textbf{Base LLM is anti-concentrated in practice.} In Figure~\ref{fig:anti-conc-math}, we plot the distribution over bi-level rewards (Property~\ref{prp:bi-level-reward}) that measure test-compute scaling, conditioned on correct answers. With $\kappa=0.5$, we mark in red the performance needed for  trained LLM to improve over any expert in $\kappa$-$\chi^2$ ball around $\pibase$. On both easy (acc. $>$$0.3$) and hard problems (acc. $<$$0.3$), the region beyond the red mark is $\approx$ $\nicefrac{1}{4}$, implying that $\pibase$ has an anti-concentration coefficient of $\approx$ acc. $\times$ 0.25 (Property~\ref{prp::anti-conc}). Thus, the VB BoN is able to cover correct answers, which only improves with more test compute. Theorem~\ref{thm:verifier-based-thm} suggests that with $\nicefrac{H}{\eta}$ samples BoN can outperform a policy that is $\eta$ close to the red mark.



\begin{AIbox}{Takeaways: Trends on MATH match our theory.}
\begin{itemize}[leftmargin=0em]
    \item Base LLMs (\textit{e.g.}, Llama-3.1-8B) exhibit heterogeneous and anticoncentrated reward distributions.
    \item VB methods outperform VF for a fixed test compute budget and the gap only grows as we increase training data and test budget. Although, when heterogeneity is indeed low, VF can outperform VB.
\end{itemize}
\end{AIbox}



\vspace{-0.2cm}
\section{Discussion, Limitations, and Future Work}
\vspace{-0.15cm}

Recent results show that capabilities of foundation models improve as we sample more tokens from them at test time. But, this paradigm of scaling test-time compute is only sustainable if there exist learning algorithms that can learn policies, which make efficient use of test-time compute and keep improving as we scale test-time budgets. To study this, we first formalize the problem of optimizing test-time compute efficiency under our bi-level rewards (Property~\ref{prp:bi-level-reward}). Then, we define what it means to scale test-time compute efficiency asymptotically, mainly when comparing a pair of algorithms (Definition~\ref{def:h-alpha-scaling}).  

Based on these definitions, we present a novel theoretical study that analyzes two classes of popular algorithms. These algorithms train LLMs to use higher compute budgets at test-time, much higher than the length of correct answers for typical problems. Crucially, we separate these classes along the axis of access to reward or verification signals, and find that without access to verification (which can be in the form of 0/1 rewards during training, or trained verifiers at test-time), the performance of learning algorithms may not scale efficiently to large budgets compares to a simple verification-based approach. We prove this separation under two conditions on the base pre-trained LLM we start with. In particular, we show that when the base policy is heterogeneous (i.e., conditioned on a problem, the distribution of bi-level rewards has a high variance), no verifier-free learning algorithm can accurately learn any expert in a $\chi^2$ ball around the base policy. While every verifier-free algorithm suffers from a heterogeneous base policy, we show that when the base policy satisfies a weak anti-concentration condition: for all problems, the pretrained LLM puts a constant mass on a region of rewards, slightly higher than mean performance on the problem, then a simple verifier-based algorithm we analyze is already good enough to closely approximate the expert policy, which is supposed to scale well as we scale test-time compute further. We verify that the above conditions of base policy heterogeneity and anti-concentration are satisfied in practice, which neatly ties our theoretical abstractions and results to practical settings and empirical observations. We also  compare our theoretical predictions on the gap between VF and VB methods on the MATH and AIME benchmarks (with the s1 model~\citep{muennighoff2025s1} and a sequential self-correction model~\cite{snell2024scaling}), and a didactic setting which allows us to control the heterogeneity explicitly.

\textbf{Limitations and future work.} In this work, we mainly group algorithms based on whether or not they utilize access to verification signals for learning. Future work should consider building on our analysis to compare verifier-based algorithms that query sparse vs. dense forms of verification, \textit{e.g.}, process-based rewards. Theoretically, it would also be interesting to extend our analysis of verifier-based algorithms with bi-level rewards to other classes of reward functions, including generative rewards~\citep{zhang2024generative}. Finally, since it is very expensive to train LLMs to use long contexts at test-time ($>32$k) an analysis of scaling behaviors for RL with outcome, or dense rewards, and other verifier-based approaches would be crucial for making progress in this area. We believe that our analysis could provide certain ``intuitions'' about how to set up the right problems for such a scaling study.


\vspace{-0.2cm}
\section*{Acknowledgements}
\vspace{-0.1cm}
All experiments in this work were run at Carnegie Mellon University.
We thank Max Simchowitz, Andrea Zanette, Yuxiao Qu, Max Sobol Mark, Kwanyoung Park, Matthew Yang, Bhavya Agrawalla, Christina Baek, Charlie Snell, Yifei Zhou, Yi Su, Paria Rashidinejad, Ahmad Beirami and members of the CMU AIRe lab for feedback on an earlier version of this paper and informative discussions. AK is thankful for the  support from Office of Naval Research under N00014-24-12206. AS is thankful for the generous support of JP Morgan AI PhD Fellowship. NR is thankful for support from NSF Grants IIS-1901252 and CCF-2211209. The authors thank the TRC program at Google Cloud and Lambda labs for providing compute resources that supported this work.












