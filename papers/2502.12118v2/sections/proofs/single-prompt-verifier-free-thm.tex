






\subsection{Proof of Theorem~\ref{thm:verifier-free-thm} for the single problem instance}
\label{subsec:proof-verifier-free-thm-single-problem}

This result follows using a similar approach as the instance-dependent lower bound against behavior cloning proved in \cite{foster2024behavior}.
For the case, where we have a single prompt $\bx$, we use the following lemma to argue that given an expert policy $\pi_e$, we can always construct another policy $\tilde{\pi}_e$, and a pair of rewards $\{r, \tilde{r}\}$ that satisfy certain properties, while ensuring that each policy observes a variance of $\sigma^2$ in the range $(0, H^2/4]$ for either of the rewards.

Next, we consider the following inequality, which holds for any \(\Delta > 0\):

\[
\min_{\texttt{Alg}} \max_{\pi \in \{\pi_e, \tilde{\pi}_e\}} \max_{r \in \{r, \tilde{r}\}} \mathbb{P}\left[J_r(\pi) - J_r(\hat{\pi}) \geq \Delta \right]
\geq
\min_{\texttt{Alg}} \max_{\pi \in \{\pi_e, \tilde{\pi}_e\}} \mathbb{P}\left[\left|J_{r}(\pi) - J_{r}(\hat{\pi})\right| \geq \Delta \right].
\]

Here, \(J_r(\pi)\) denotes the expected reward under the reward function \(r\), and for convenience, we abbreviate \(J(\pi) \equiv J_{r}(\pi)\) going forward. Let \(\mathbb{P}_n^\pi\) represent the probability distribution of the offline imitation learning dataset when the data is collected under policy \(\pi\). By choosing \(\Delta = \frac{\left|J(\pi_e) - J(\tilde{\pi}_e)\right|}{2}\), and applying the standard Le Cam two-point argument, we can conclude that:

\[
\max\left\{
\mathbb{P}_n^{\pi_e}\left[\left|J(\pi_e) - J(\hat{\pi})\right| \geq \Delta\right],
\mathbb{P}_n^{\tilde{\pi}_e}\left[\left|J(\tilde{\pi}_e) - J(\hat{\pi})\right| \geq \Delta\right]
\right\}
\]

is bounded below by:

\[
\frac{1}{2} \left(
1 - \mathbb{P}_n^{\pi_e}\left[\left|J(\pi_e) - J(\hat{\pi})\right| < \Delta\right] +
\mathbb{P}_n^{\tilde{\pi}_e}\left[\left|J(\tilde{\pi}_e) - J(\hat{\pi})\right| \geq \Delta\right]
\right).
\]

This, in turn, is further bounded below by:

\[
\frac{1}{2} \left(
1 - \mathbb{P}_n^{\pi_e}\left[\left|J(\tilde{\pi}_e) - J(\hat{\pi})\right| \geq \Delta\right] +
\mathbb{P}_n^{\tilde{\pi}_e}\left[\left|J(\tilde{\pi}_e) - J(\hat{\pi})\right| \geq \Delta\right]
\right),
\]

and by a standard application of the data processing inequality for the total variation distance, we have:

\[
\frac{1}{2} \left(1 - \mathrm{D}_{\text{TV}}\left(\mathbb{P}_n^{\pi_e}, \mathbb{P}_n^{\tilde{\pi}_e}\right)\right).
\]

Utilizing the tensorization property of the Hellinger distance~\cite{wainwright2019high}, we further lower bound this by:

\[
\frac{1}{2} \left(1 - \sqrt{n \cdot \mathrm{D}_{\text{H}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^{\tilde{\pi}_e}\right)}\right).
\]

Next, we proceed to show the following key inequality:

\[
\omega_{\pi_e}(\varepsilon) \coloneqq \sup_{\pi} \left\{\left|J(\pi) - J(\pi_e)\right| \, \middle| \, \mathrm{D}_{\text{H}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^\pi\right) \leq \varepsilon^2\right\}
\geq \Omega(1) \cdot \sqrt{\sigma_{\pi_e}^2 \cdot \varepsilon^2},
\]

for any \(\varepsilon > 0\) sufficiently small. The final result follows by setting \(\varepsilon^2 \propto \frac{1}{n}\), and defining:

\[
\tilde{\pi}_e = \arg\max_{\pi} \left\{\left|J(\pi) - J(\pi_e)\right| \, \middle| \, \mathrm{D}_{\text{H}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^\pi\right) \leq \varepsilon^2\right\}.
\]

To prove this, we invoke the following technical lemma:

\begin{lemma}[Lemma G.1 in ~\citet{foster2024behavior}]
\label{lem:kl_dual}
For any distribution \(\mathbb{Q}\) and any function \(h\) satisfying \(\left|h\right| \leq R\) almost surely, it holds that for all \(0 \leq \varepsilon^2 \leq \frac{\mathrm{Var}_{\mathbb{Q}}[h]}{4R^2}\), there exists a distribution \(\mathbb{P}\) such that:
\begin{enumerate}
    \item \(\mathbb{E}_{\mathbb{P}}[h] - \mathbb{E}_{\mathbb{Q}}[h] \geq 2^{-3} \sqrt{\mathrm{Var}_{\mathbb{Q}}[h] \cdot \varepsilon^2}\),
    \item \(\mathrm{D}_{\text{KL}}(\mathbb{Q} \| \mathbb{P}) \leq \varepsilon^2\).
\end{enumerate}
\end{lemma}

In the case of stochastic policies \(\pi\) in the autoregressive Markov Decision Process \(\mathcal{M}^*\), these policies are equivalent to defining arbitrary joint distributions over the sequence \((a_1, \ldots, a_H)\) using Bayes' rule. Consequently, since \(J(\pi) = \mathbb{E}^\pi\left[\sum_{h=1}^H r_h\right]\), Lemma~\ref{lem:kl_dual} ensures that for any \(\varepsilon^2 \leq \frac{\mathrm{Var}^{\pi_e}\left[\sum_{h=1}^H r_h\right]}{4R^2}\), there exists a policy \(\tilde{\pi}_e\) such that:

\[
\mathrm{D}_{\text{H}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^{\tilde{\pi}_e}\right) \leq \mathrm{D}_{\text{KL}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^{\tilde{\pi}_e}\right) \leq \varepsilon^2,
\]

and:

\[
J(\tilde{\pi}_e) - J(\pi_e) \geq 2^{-3} \sqrt{\mathrm{Var}^{\pi_e}\left[\sum_{h=1}^H r_h\right] \cdot \varepsilon^2}.
\]

This establishes the desired inequality. Setting \(\varepsilon^2 = \frac{c}{n}\) for some constant \(c > 0\), we achieve 
$$\sqrt{n \cdot \mathrm{D}_{\text{H}}\left(\mathbb{P}^{\pi_e}, \mathbb{P}^{\tilde{\pi}_e}\right)} \leq \frac{1}{2},$$ 
which is valid provided that \(n \geq c' \cdot \frac{R^2}{\sigma_{\pi_e}^2}\).
