
\subsection{Analyzing Verifier Accuracy Under 0/1 Loss}
\label{subsec:proof-verifier-accuracy}



Consider the following modified version of   Algorithm~\ref{alg:algorithm_simple_vb}.



\begin{algorithm}[ht]
\caption{Simple Verifier-Based Algorithm with $\ell_{0/1}$ loss}
\begin{algorithmic}[1]
\REQUIRE Base policy $\pibase$, dataset $\{(\bx_i, \tau_i)\}_{i=1}^n$ of prompts $\bx_i \sim \rho$ and traces $\tau_i \sim \pibase(\cdot \mid \bx)$.
\STATE For every $\tau_i$  annotate $(\bx_i, \tau_i)$ with bi-level reward $r(\tau_i)$. %
\STATE Learn set of classifiers $\hat{R}_\gamma \subset \gR$ that are $\gamma$-optimal, \textit{i.e.}, 
{
\begin{align*}
    \hat{R}_\gamma \eqdef \cbrck{r' \in \gR \middle| \frac{1}{n}\sum\nolimits_{i=1}^n\ell_{0/1}(r'(\tau_i), r(\tau_i)) \leq \gamma}
\end{align*}
}\STATE Return any optimal pessimistic verifier-based policy,
{
\begin{align*}
    \vspace{-0.1cm}
    \setlength{\abovedisplayskip}{0pt}
    \setlength{\belowdisplayskip}{0pt}
    \hat{\pi}^\mathrm{vb}_n \in \argmax_{\pi \in \Pi} \min_{r \in \hat{R}_\gamma} J_r(\hat{\pi}).
\end{align*}
}
\end{algorithmic}
\end{algorithm}


\begin{proposition}[Verifier accuracy]
    \label{prp:verifier-accuracy-lzone}
    For any bi-level reward $r$, base policy $\pibase$, 
    there exists an algorithm querying the at most reward annotator $n$ times to learn $\hat{r} \in \gR$, s.t. w.p. $1-\delta$,  
    {
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \begin{align*}
        \E_{\rho,\pibase} [\ellzone(r(\tau), \hat{r}(\tau))] =  \widetilde{\gO}_n \paren{\frac{\log(\nicefrac{|\mathcal{R}|}{\delta}) \log H }{n}} \eqdef \gamma_{\text{stat}}.
    \end{align*}
    In Algorithm~\ref{alg:algorithm_simple_vb}, setting $\gamma = \gamma_{\text{stat}} \implies r \in \hat{R}_\gamma$ w.p. $\ge 1-\delta$.
    }
\end{proposition}



\begin{definition}[Graph dimension] Let $\mathcal{H}$ be a hypothesis class on an input space $\mathcal{X}$ and label space $\mathcal{Y}$. Let $S \subseteq \mathcal{X}$. The class $\mathcal{H}$ is said to $G$-shatter $S$ if there exists an $f : S \to \mathcal{Y}$ such that for every $T \subseteq S$, there is a $g \in \mathcal{H}$ such that $\forall x \in T,\ g(x) = f (x)$, and $\forall x \in S \setminus T$, $g(x) \ne f (x)$. The graph dimension of $\mathcal{H}$, denoted $d_G (\mathcal{H})$, is the maximal cardinality of a set that is $G$-shattered by $\mathcal{H}$.
\end{definition}

\begin{theorem}[Sample complexity of multiclass classification \cite{daniely2011multiclass}] \label{theorem:multiclass}
There exists an absolute constant $C > 0$ such that for every hypothesis class $\mathcal{H}$, given a $\mathcal{H}$-realizable i.i.d. dataset $D$ of size $n \ge n (\varepsilon)$, where,
\begin{align} \label{eq:nstar}
    n (\varepsilon) = C \left( \frac{d_G (\mathcal{H}) \log ( 1/\varepsilon ) + \log ( 1/\delta)}{\varepsilon} \right),
\end{align}
empirical risk minimization on $D$ with the hypothesis class $\mathcal{H}$ incurs $0$-$1$ loss of at most $\varepsilon$ with probability at least $1-\delta$.
\end{theorem}

\begin{lemma}[Upper bound on the graph dimension]
For any hypothesis class $\mathcal{H}$, $d_G (\mathcal{H}) \le \log_2 (|\mathcal{H}|)$.
\end{lemma}
\begin{proof}
For a set $S \subseteq \mathcal{X}$ to be $G$-shattered by $\mathcal{H}$ if there exists a function $f$ such that for any subset $T \subseteq S$ there exists an discriminator $g_T \in \mathcal{H}$ that agrees with $f$ on $T$ and disagrees with it on $S \setminus T$. Across different choices of the subset $T \subseteq S$, the discriminating $g_T$ cannot be the same: indeed for $T_1 \ne T_2 \subseteq S$, $g_{T_1}$ and $g_{T_2}$ must disagree on points in $(T_1 \setminus T_2) \cup (T_2 \setminus T_1)$, the symmetric difference of the two subsets. This is simply because on points in $T_1 \setminus T_2$, $g_{T_1}$ agrees with $f$ and $g_{T_2}$ disagrees with $f$, while on points in $T_2 \setminus T_1$, $g_{T_2}$ agrees with $f$ and $g_{T_1}$ disagrees with $f$. Since the map $T \to g_T$ is injective, and there are $2^{|S|}$ choices of $T$, this means that $S$ can only be $G$-shattered if $|\mathcal{H}| \ge 2^{|S|}$.
\end{proof}

\begin{theorem} \label{thm:reward-estimation}
Given a dataset of $n(\varepsilon)$ trajectories from $\pibase$, there exists an algorithm which calls the verifier $n(\varepsilon) \lceil \log_2 (H) \rceil$ times and learns a reward model such that,
\begin{align}
    \mathbb{E}_{\rho, \pibase} \left[ \mathbb{I} ( r(\tau) \ne \hat{r} (\tau)) \right] \le \varepsilon.
\end{align}
\end{theorem}
\begin{proof}
Recall that $\mathcal{R}$ is assumed to be a bi-level reward class.
For each $r \in \mathcal{R}$, consider the multiclass classifier $f_r : (\mathcal{S} \times \mathcal{A})^H \to [H+1]$ which maps a trajectory $\tau = \{ (s_1,a_1),\cdots,(s_H,a_H) \}$ to the value of $h \in [H]$ such that $h$ is the first point in the trajectory where $r(s_h,a_h) = 1$, i.e., the location of the bi-level in the trajectory. If the reward stays $0$ entirely through the trajectory, then $f_r (\tau) = H+1$. First, we relate the $0$-$1$ error of a reward estimator $\hat{r}$ to the multiclass classification error of $f_r$, assuming the labels come from $f_r$. Observe that,
\begin{align}
    \mathbb{E}_{\rho,\pibase} \left[ \mathbb{I} (r(\tau) \ne \hat{r} (\tau)) \right] \le \mathbb{E}_{\rho,\pibase} \left[ \mathbb{I} (f_r (\tau) \ne f_{\hat{r}} (\tau)) \right].
\end{align}
This follows from the fact that, if $r(\tau) \ne \hat{r} (\tau)$, then the bi-level in this trajectory $\tau$ is identified incorrectly, implying that $f_r (\tau) \ne f_{\hat{r}} (\tau)$. Recall that the expert dataset is composed of $n=n(\varepsilon)$ trajectories $D = \{ (\bx_i,\tau_i) \}_{i=1}^n$ for some $\varepsilon > 0$ (see \Cref{eq:nstar} for the definition of $n(\varepsilon)$). Using the verifier to annotate rewards, by a binary searching, the location of the bi-level in any of these $n$ trajectories may be located: thus with $n \lceil \log_2 (H) \rceil$ calls to the verifier, a dataset of $n$ examples may be constructed of the form $\{ (\tau_i,f_r (\tau)) \}_{i=1}^n$ for the ground truth reward $r$. By carrying out empirical risk minimization over the hypothesis class $\mathcal{F} = \{ f_r : r \in \mathcal{R} \}$ to learn a hypothesis $\hat{f}$, and invoking \Cref{theorem:multiclass}, with probability $\ge 1-\delta$,
\begin{align}
    \mathbb{E}_{\rho, \pibase} \left[ \mathbb{I} (f_r (\tau) \ne \hat{f} (\tau)) \right] \le \varepsilon.
\end{align}
\end{proof}

