\subsection{Useful Lemmas}
\label{subsec:useful-lemmas}



For a pair of probability measures $P$ and $Q$, we define the total variation distance as $\tv{P}{Q}=\frac{1}{2}\int\abs{\mathrm{d}P-\mathrm{d}Q}$, and define the $\chi^2$-divergence by $\chisq{P}{Q}=\int\frac{(\mathrm{d}Q-\mathrm{d}Q)^2}{\mathrm{d}Q}$ if $P\ll Q$ and $\chi^2\paren{P \| Q}=+\infty$ otherwise. We define the KL divergence by $\kl{P}{Q}= \int{} \mathrm{d}P\log\paren{\frac{\mathrm{d}P}{\mathrm{d}Q}}$ if $P\ll Q$ and $\kl{P}{Q}=+\infty$ otherwise.


  \begin{lemma}[\cite{polyanskiy2014lecture}]
    \label{lem:pinsker}
    The following inequalities hold:
    \begin{itemize}
    \item $\tv{P}{Q}\leq \hell{P}{Q}\leq 2\tv{P}{Q}$.%
    \item $\frac{1}{6}\hell{P}{Q}
      \leq \chisq{P}{\frac{1}{2}(P+Q)}
      \leq  \hell{P}{Q} 
      $.
    \item $\tv{P}{Q} \leq \sqrt{\frac{1}{2} \kl{P}{Q}}$
    \end{itemize}
  \end{lemma}


\begin{lemma}[Change of measure \cite{polyanskiy2014lecture,foster2024behavior}] \label{lemma:CoM} Let $P$ and $Q$ be probability distributions over a measurable space $(\mathcal{Y}, \mathscr{F})$. Then for all functions $h: \mathcal{Y} \rightarrow \mathbb{R}$,
\label{lem:change-of-measure}
\begin{align}
    \left|\mathbb{E}_{P} [h(Y)]-\mathbb{E}_{Q} [h(Y)]\right| &\leq \sqrt{\Var_Q \left[h(Y)\right] \cdot \chisq{P}{Q}} \tag{$\chi^2$-CoM} \label{eq:chi2CoM}\\
    &\leq \sqrt{\frac{1}{2}\left(\mathbb{E}_P \left[h^2(Y )\right]+\mathbb{E}_Q \left[h^2(Y)\right]\right) \cdot D_{\mathrm{H}}^2(P,Q)} \label{eq:HellCoM}\tag{$\mathbb{H}$-CoM}
\end{align}
\end{lemma}

\begin{lemma}[Total expert heterogeneity]
  \label{lem:total-variance}
  For any policy $\pi$, recall the definition of heterogeneity in Definition~\ref{def:exp-heterogeneity}. For this definition of heterogeneity the following equivalance to the expected conditional variance of rewards is true:
  \begin{align*}
    \sigma^2_\pi = \E_{\bx \sim \rho} \Var_{\tau \sim \pi(\cdot \mid \bx)} \brck{r(\tau)}.  
  \end{align*}
\end{lemma}
\begin{proof}
Let us begin by recalling the definition of $\sigma^2_\pi$. 
 \begin{align*}    
    \sigma^2_{\pi} \eqdef \sum_{h=1}^{H} \E_{\bs_h \sim d^{\pi}_{h}}\brck{\Var_{\pi(\cdot \mid \bs_h)}\brck{Q_{\pi}(\bs_h, a_h)}}.
    \end{align*}
Now let us expand $\Var_{\pi(\cdot \mid \bs_h)}\brck{Q_{\pi}(\bs_h, a_h)}$ in the following way.
\begin{align*}
&\Var_{\pi}\brck{\sum_{h^\prime = h}^H r(\bs_{h^\prime}, a_{h^\prime}) \middle| \bs_h} \\
    &= \Var_{\pi}\brck{r(\bs_h, a_h) + \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) \middle| \bs_h} 
\end{align*}
\begin{align*}
     &= \E_{\pi}\brck{\paren{r(\bs_h, a_h) - V_\pi(\bs_h) + \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime})}^2 \middle| \bs_h} \\
    &=  \E_{\pi}\brck{\paren{r(\bs_h, a_h) + V_\pi(\bs_{h+1}) - V_\pi(\bs_h) + \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) - V_\pi(\bs_{h+1})}^2 \middle| \bs_h} \\
    &=  \E_{\pi}\brck{\paren{Q_\pi (\bs_h, a_h) - V_\pi(\bs_h) + \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) - V_\pi(\bs_{h+1})}^2 \middle| \bs_h}
\end{align*}
Breaking the above expectation into three terms by expanding the square, note that the third term is zero because, $\E_{\pi} \brck{Q_\pi(\bs_h, a_h) - V_\pi(\bs_{h+1}) \mid \bs_h} = 0$, for any state $\bs_h$ and in our autoregressive MDP with deterministic dynamics, $$Q_\pi(\bs_h, a_h) = r(\bs_h, a_h) + V_\pi(\bs_{h+1}),$$ also for every state $\bs_h$. Recall that, here the state $\bs_{h+1} = (\bs_h, a_h)$. Additionally, we also take the expecation over the state distribution of $\bs_h \sim d^\pi_h$, and since the equality is true individually for each value of $\bs_h$, it also holds under the expectation over $\bs_h$. This gives us the following.  
\begin{align*}
    & \E_{\bs_h \sim d^{\pi}_{h}}\brck{\E_{\pi}\brck{\paren{r(\bs_h, a_h) + V_\pi(\bs_{h+1}) - V_\pi(\bs_h) + \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) - V_\pi(\bs_{h+1})}^2 \middle| \bs_h}} \\
    &=   \E_{\bs_h \sim d^{\pi}_{h}}\brck{\E_{\pi}\brck{\paren{r(\bs_h, a_h) + V_\pi(\bs_{h+1}) - V_\pi(\bs_h)}^2 \middle| \bs_h} + \E_{\pi} \brck{\paren{ \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) - V_\pi(\bs_{h+1})}^2 \middle| \bs_h}} \\ 
    &\quad\quad + 2\cdot \E_{\bs_h \sim d^{\pi}_{h}}\brck{\E_{\pi}\brck{{r(\bs_h, a_h) + V_\pi(\bs_{h+1}) - V_\pi(\bs_h)} \middle| \bs_h} \cdot \E_{\pi} \brck{{ \sum_{h^\prime = h+1}^H r(\bs_{h^\prime}, a_{h^\prime}) - V_\pi(\bs_{h+1})} \middle| \bs_h}}  
\end{align*}
As we noted above, the third term in the summation above is zero. Thus,
\begin{align*}
    \E_{\bs_h \sim d^\pi_h} \brck{\Var_\pi \brck{ \sum_{h^\prime=h}^H r(\bs_{h^\prime}, a_{h^\prime}) \middle| \bs_h}} &= \E_{\bs_{h+1} \sim d^\pi_{h+1}}\brck{ \Var_\pi \brck{ \sum_{h^\prime=h+1}^H r(\bs_{h^\prime}, a_{h^\prime})} \middle| \bs_h} \\  
    & \quad\quad + \E_{\bs_{h} \sim d^\pi_h} \brck{ \Var_\pi \brck{Q_\pi (\bs_h, a_h)} \middle| \bs_h}   
\end{align*}
The above induction is true for all values of $h$.
Now, taking the sum over $h$, from $h=1$ to $h=H$ on both left and right sides of the equation and using the definition of $\sigma^2_\pi$, we get:
\begin{align*}
    \sigma_\pi^2 = \E_{\bs_1 \sim d^\pi_1} \left[ \Var_{\pi} \brck{\sum_{h=1}^H r(\bs_h, a_h) \middle| \bs_1} \right].  
\end{align*}
Recall from Section~\ref{sec:prelim} that the first state $\bs_1$ is simply the input prompt $\bx$. Thus $d_1^\pi$ is indpendent of $\pi$ and is simply the distribution over the input prompts $\bx$, which is defined as $\rho$. Plugging this into the above equation we get:
\begin{align*}
    \sigma^2 = \E_{\bx \sim \rho} \left[ \Var_{\pi} \brck{\sum_{h=1}^H r(\bs_h, a_h) \middle| \bx} \right] = \E_{\bx \sim \rho} \left[ \Var_{\tau \sim \pi(\cdot \mid \bx)} \brck{r(\tau)} \right].  
\end{align*}
\end{proof}


\begin{lemma} \label{lemma:Abound}
Consider a random variable $A$ which is almost surely non-negative and has mean $\mu$ and variance $\sigma^2$. For any $\theta \ge 0$,
\begin{align}
    \mathbb{E} \left[ \frac{\theta ( \mu - A )}{\sigma + \theta A} \right] \le 2 \theta^2
\end{align}
\end{lemma}
\begin{proof}
Let $f(\theta) = \mathbb{E} \left[ \frac{\theta ( \mu - A )}{\sigma + \theta A} \right]$. Observe that,
\begin{align*}
    f'(\theta) &= \mathbb{E} \left[ \frac{\mu - A}{\sigma + \theta A} \right] - \mathbb{E} \left[ \frac{\theta ( \mu - A ) A}{(\sigma + \theta A)^2} \right] \\
    f''(\theta) &= - 2 \mathbb{E} \left[ \frac{(\mu - A)A}{(\sigma + \theta A)^2} \right] + 2 \mathbb{E} \left[ \frac{\theta ( \mu - A ) A^2}{(\sigma + \theta A)^3} \right] \\
    &= 2\mathbb{E} \left[ \frac{\theta ( \mu - A ) A^2 - (\mu-A)A(\sigma + \theta A)}{(\sigma + \theta A)^3} \right] \\
    &= 2\sigma \mathbb{E} \left[ \frac{(A-\mu)A}{(\sigma + \theta A)^3} \right] \\
    &= 2\sigma \mathbb{E} \left[ \frac{\mu(A-\mu)}{(\sigma + \theta A)^3} \right] + 2\sigma \mathbb{E} \left[ \frac{(A-\mu)^2}{(\sigma + \theta A)^3} \right] \\
    &\le 2\sigma \mathbb{E} \left[ \frac{\mu(A-\mu)}{(\sigma + \theta A)^3} \right] + 2\sigma \mathbb{E} \left[ \frac{(A-\mu)^2}{\sigma^3} \right] \\
    &= 2\sigma \mathbb{E} \left[ \frac{\mu(A-\mu)}{(\sigma + \theta A)^3} \right] + 2
\end{align*}
Note that $\mu (A - \mu)$ and $(\sigma + \theta A)^3$ are both increasing functions in $A$, and therefore,
\begin{align*}
    \mathbb{E} \left[ \frac{\mu(A-\mu)}{(\sigma + \theta A)^3} \right] &\le 2\sigma \mathbb{E} \left[ \mu (A-\mu) \right] \mathbb{E} \left[ \frac{1}{(\sigma + \theta A)^3} \right] = 0.
\end{align*}
This results in the upper bound $\| f'' \|_\infty \le 2$. Since $f(0) = 0$ and $f'(0) = 0$, we have that,
\begin{align*}
    f(\theta) = \int_{0}^{\theta} f''(\alpha) \mathrm{d}\alpha \le 2 \theta^2.
\end{align*}
\end{proof}
