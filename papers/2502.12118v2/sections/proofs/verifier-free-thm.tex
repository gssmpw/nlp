\subsection{Proof of Theorem~\ref{thm:verifier-free-thm}}
\label{subsec:proof-verifier-free-thm}

We will state a slightly more formal version of \Cref{thm:verifier-free-thm} below in \Cref{subsubsec:5.4}. Prior to this, we introduce some relevant notation necessary to state the main result.

\subsubsection{Measure of complexity: \texorpdfstring{$L_k^\star$}{}}

Consider an arbitrary partitioning of the prompt space $\mathcal{X}$ into $k$ disjoint parts, denoted $\{ \mathcal{X}_i \}_{i=1}^k$. Let $\{ \mathcal{X}^\star_i \}_{i=1}^k$ denote the partitioning of the prompt space which maximizes,
\begin{align} \label{eq:Lstar}
    L(\{\mathcal{X}_i\}_{i=1}^k) \eqdef \min \left\{ \mathbb{E}_{\bx \sim \rho} [\sigma_{e,\bx} \mathbb{I} (\bx \in \cup_{i \in K} \mathcal{X}_i)] : K \subseteq [k] \text{ and } |K| \ge k/4 \right\}.
\end{align}
And let $L^\star_k = L(\{\mathcal{X}^\star_i\}_{i=1}^k)$. Our construction, and lower bounds derived therafter are stated in terms of $\{ \mathcal{X}_i^\star \}_{i=1}^k$ and $L^\star_k$. We devote the first part of this section toward interpretations of $L_k^\star$.

Recall that $\widetilde{\sigma}_e = \text{Median} ( \{ \sigma_{\pi,\bx} : \bx \in \mathcal{X} \} )$, $\overline{\sigma}_e = \mathbb{E}_{\bx \sim \rho}  [\sigma_{\piexp,\bx}]$ and $\sigma^2_e = \mathbb{E}_{\bx \sim \rho} [\sigma_{\piexp,\bx}^2]$. We will first show that $L_k^\star \gtrsim \widetilde{\sigma}_e$ always. Later, we will show that if $\sigma_e^2 \le c \overline{\sigma}_e^2$ for a sufficiently small constant $c > 1$, $L_k^\star \gtrsim \sigma_e$.

\subsubsection{Interpretations of, and bounds on \texorpdfstring{$L_k^\star$}{}}
\begin{lemma}
Consider any $8 \le k \le |\mathcal{X}|/4$. Then, $L_k^\star \ge \frac{1}{32} \widetilde{\sigma}_e$.
\end{lemma}
\begin{proof}
We will prove this statement in two assertions,
\begin{enumerate}
    \item When $k$ is a power of two, $L_{k/2}^\star \ge L_k^\star$.
    \item When $k$ is any power of two, and any $k/2 \le k' \le k$, $L_{k'}^\star \ge \frac{1}{2} L_{k}^\star$.
\end{enumerate}
For a subset $X \subseteq \mathcal{X}$, define its score $s(X) = \E_{\bx \sim \rho} [ \sigma_{e,\bx} \mathbb{I} (\bx \in X)]$. Assuming these two assertions, we will prove the main lemma first, and then come back to proving them.

\paragraph{Proof of the main lemma.} Consider $k$ as the largest power of $2$ between $|\mathcal{X}|/4$ and $|\mathcal{X}|/2$. For this choice, consider the partition of $\mathcal{X}$ into $k$ sets by choosing the first $k$ parts as singleton sets, consisting of the top $k$ prompts $\bx \in \mathcal{X}$ with the highest values of $\sigma_{e,\bx}$; the remaining prompts are distributed among sets in the partition arbitrarily. Notably, the score of each part $\mathcal{X}_i$ in this partition satisfies $s(\mathcal{X}_i) \ge \text{Median} ( \{ s( \{\bx\} ) : \bx \in \mathcal{X} \} )$; by implication, for any such value of $k$, 
\begin{align} \label{eq:median}
L_k^\star \ge \frac{k}{4} \text{Median} ( \{ s(\bx) : \bx \in \mathcal{X} \} ) \ge \frac{|\mathcal{X}|}{16} \text{Median} ( \{ s( \{ \bx \} ) : \bx \in \mathcal{X} \} ) = \frac{1}{16} \text{Median} ( \{ \sigma_{e,\bx} : \bx \in \mathcal{X} \} )
\end{align}
where the last equation uses the fact that $\rho$ is the uniform distribution over $\mathcal{X}$. Therefore, for any $k' \le k$, we have that $L_k^\star \ge \frac{1}{2} L_k^\star = \ge \frac{1}{32} \text{Median} ( \{ \sigma_{e,\bx} : \bx \in \mathcal{X} \} )$.

\paragraph{Proof of the first assertion.}  Consider the optimal partition which induces $L_k^\star$, $\{ \mathcal{X}^\star_i \}_{i=1}^k$, arranged in increasing order of scores. Note then, that $L^\star_k = \sum_{i=1}^{ k/4 } s(\mathcal{X}_i)$ Consider the partition of $\mathcal{X}$ into $k/2$ parts, as $\{ \mathcal{X}_1^\star \cup \mathcal{X}_2^\star, \mathcal{X}_3^\star \cup \mathcal{X}_4^\star, \cdots, \mathcal{X}_{k-1}^\star \cup \mathcal{X}_k^\star \}$. Since scores are additive, the $k/8$ parts with the lowest scores must be $\{ \mathcal{X}_i^\star \cup \mathcal{X}_{i+1}^\star \}_{i=1}^{k/8}$. This implies the first assertion.

\paragraph{Proof of the second assertion.} Consider the optimal partition which induces $L_k^\star$, $\{ \mathcal{X}^\star_i \}_{i=1}^k$. By dissolving the bottom $k-k'$ parts (in terms of score) of $\{ \mathcal{X}^\star_i \}_{i=1}^k$ and merging them with other parts, this results in a partitioning of $\mathcal{X}$ such that the sum of $k'/4$ worst scores of the parts must be at least $(k'/k) L_k^\star \ge L_k^\star/2$.


\end{proof}

\begin{lemma}
Suppose $\sigma_e^2 \le \frac{4}{3} \overline{\sigma}_e^2$, then $\widetilde{\sigma}_e \ge \frac{1}{10} \overline{\sigma}_e \ge \frac{1}{15} \sigma_e$.
\end{lemma}
\begin{proof}
By the Paley-Zygmund inequality,
\begin{align}
\Pr_{\bx \sim \rho} \left[ \sigma_{e,\bx} \ge \frac{1}{10} \overline{\sigma}_e \right] \ge \frac{4}{5} \times \frac{\overline{\sigma}_e^2}{\sigma_e^2}
\end{align}
When $\sigma_e^2 \le \frac{4}{3} \overline{\sigma}_e^2$, the LHS is at least $3/5$. This means that at least $3|\mathcal{X}|/5$ of the prompts satisfy $\sigma_{e,\bx} \ge \frac{1}{10} \overline{\sigma}_e$, and so $\text{Median} ( \{ \sigma_{e,\bx} : \bx \in \mathcal{X} \} ) \ge \frac{1}{10} \overline{\sigma}_e$.
\end{proof}

As a corollary of this lemma, we have that,

\begin{corollary}
Under the condition $\sigma_e^2 \le (4/3) \overline{\sigma}_e^2$, for every $k \le |\mathcal{X}|/4$, we have that $L_k^\star \ge c \sigma_e$ for some absolute constant $c > 0$.
\end{corollary}

Having introduced these interpretations of $L_k^\star$, we prove the following instance-dependent lower bound on the suboptimality of any verifier-free algorithm.


\subsubsection{Lower bounds on verifier-free approaches} \label{subsubsec:5.4}

Below we introduce the class of rewards for which we prove the instance-dependent lower bound in \Cref{thm:verifier-free-thm}.

\begin{definition}[Half-bi-level rewards] \label{def:R1/2}
Define the class of half-bi-level rewards, $\gR_{1/2}$, as those reward functions such that every trajectory contains a bi-level at or before time $t = \lfloor H/2 \rfloor$. Namely, for any trajectory $(s_1,a_1,\cdots,s_H,a_H)$, $r(s_t,a_t) = 1$ for every $t \ge \lfloor H/2 \rfloor$ for any reward $r \in \gR_{1/2}$.
\end{definition}

\begin{remark}
Although half-bi-level rewards are constrained to have all their bi-levels before time $H/2$, this does not preclude there from existing policies having high variance under rewards from this class. In particular, there exists a policy $\pi$ and a reward $r \in \mathcal{R}_{1/2}$ such that $\sigma_\pi^2 = H^2/16$.
\end{remark}

\begin{theorem}
Suppose $|\mathcal{X}| \ge 16$ and choose any $4 \le k \le |\mathcal{X}|/4$. Consider any autoregressive MDP and assume that $\rho = \text{Unif} (\mathcal{X})$. For any choice of reward $r \in \mathcal{R}_{1/2}$, base policy $\pibase$ and expert policy $\piexp \in \Pi_\varepsilon$, there exists an alternate family of expert policies $\Pi^\prime$ of size $\lceil 2^{k/4} \rceil$ and reward class $\mathcal{R}^\prime \subset \mathcal{R}$ (also of the same size), such that,
\begin{enumerate}
    \item $\piexp \in \Pi^\prime$ and $r \in \mathcal{R}^\prime$,
    \item $\Pi^\prime \subseteq \Pi_{\varepsilon^\prime}$ corresponds to a family of feasible expert policies with $\varepsilon^\prime = 3(1+\varepsilon) \cdot \max \left\{ \frac{H\sqrt{\varepsilon_{\text{stat}}}}{\sigma_{\min}}, \frac{H^2 \varepsilon_{\text{stat}}}{\sigma_{\min}^2} \right\}$.\\
    Here, $\sigma_{\min} = \min_{\bx \in \mathcal{X}} \sigma_{e,\bx}$.
    \item For every $r^\prime \in \mathcal{R}^\prime$ and policy $\pi^\prime \in \Pi^\prime$, $\sigma_{r^\prime}^2 (\pi^\prime) \le \sigma_e^2 + H \sigma_e \sqrt{\varepsilon_{\text{stat}}} + H^2 \varepsilon_{\text{stat}}$.
    \item For any realizable verifier-based learning algorithm, satisfying $\hat{\pi}^{\text{vf}}_n \in \Pi^\prime$,
    \begin{equation}
    \max_{\pi^\prime \in \Pi^\prime} \max_{r^\prime \in \mathcal{R}} \Pr \left( J_{r^\prime} (\pi^\prime) - J_{r^\prime} (\hat{\pi}^{\text{vf}}_n) \ge L_k^\star\sqrt{\varepsilon_{\text{stat}}} \right) \ge 1/8
\end{equation}
\end{enumerate}
Here, we define $\varepsilon_{\text{stat}} = \frac{\log(|\Pi^\prime|)}{16n}$ and assume that $n$ is sufficiently large so that $\varepsilon_{\text{stat}} \le \min_{\bx \in \mathcal{X}} \sigma_{e,\bx}^2 / (J_r(\piexp|\bx))^2$. 
\end{theorem}

\begin{proof}[Proof structure]
We define the alternate policy class $\Pi^\prime$ across \Cref{lem:packing} and \Cref{lemma:GV}, culminating in \Cref{subsubsec:PIR}. Property 2 (i.e., $\Pi^\prime \subseteq \Pi_{\varepsilon'}$) and Property 3 (i.e., the bound on the variance of policies in $\Pi^\prime$ on rewards in $\mathcal{R}^\prime$) are established in \Cref{lem:packing}.
\end{proof}

\begin{remark}
The results of \cite{foster2024behavior} establish a similar lower bound for autoregressive MDPs. However their construction specifically assumes, either $(i)$ there is a single prompt, or $(ii)$ the adversary constructing an alternate hard instance can change the initial state distribution $\rho$. This follows from the fact that their alternate policy is constructed in a way which does not preserve the initial state distribution of the MDP (cf. Lemma G.1 in their paper). 
\end{remark}


Our lower bound scales with $L^\star_k \gtrsim \widetilde{\sigma}_e$ where $\widetilde{\sigma}_e = \text{Median}( \{ \sigma_{e,\bx} : \bx \in \mathcal{X} \})$, rather than $\sigma_e$, as previous work \cite{foster2024behavior} hints in the case of a single prompt. In general, it turns out that it is not possible to have an instance-dependent lower bound that scales as $\Omega (\sigma_e \sqrt{\log(|\Pi|)/n})$. There exist a class of MDPs where verifier free approaches achieve an error of $\gO (\widetilde{\sigma}_e \sqrt{\log(|\Pi|)/n})$, even under the worst case choice of policy class, and improve over the suggested $\Theta (\sigma_e \sqrt{\log(|\Pi|)/n})$ instance-dependent error.  


\begin{theorem}
Consider an autoregressive MDP with $|\mathcal{A}| = 2$ and $H=1$. There exists an expert policy $\piexp$, such that \textbf{for any policy class} $\Pi \ni \piexp$ of size $|\Pi| \ge 2^{\Omega(|\mathcal{X}|)}$, there exists a verifier-free learner such that with probability at least $1-\delta$,
\begin{align*}
    \max_{r \in \mathcal{R}} J_r (\piexp) - J_r (\hat{\pi}_n^{\text{vf}}) &\le \widetilde{\gO}_{|\mathcal{X}|,\delta} \left( \widetilde{\sigma}_e \sqrt{\frac{\log (|\Pi|)}{n}} + \frac{\log(|\Pi|)}{n} \right) \\
    &= \widetilde{\Theta}_{|\mathcal{X}|,\delta} \left( \frac{\sigma_e}{\sqrt{|\mathcal{X}|} }\cdot \sqrt{\frac{\log (|\Pi|)}{n}} + \frac{\log(|\Pi|)}{n} \right)
\end{align*}
as long as $\delta \ge |\mathcal{X}|\exp ( - \frac{1}{2} \sqrt{n/|\mathcal{X}|})$.
\end{theorem}
\begin{proof}
WLOG, assume $\mathcal{A} = \{ 0,1 \}$. Consider the following expert: for the $i^{\text{th}}$ prompt, arranged in arbitrary order, let $\piexp (1|\bx_i) = \frac{1}{2i^2}$. Observe that,
\begin{align*}
    \widetilde{\sigma}_e &= \Theta \left( \frac{1}{|\mathcal{X}|} \right) \\
    \overline{\sigma}_e &= \mathbb{E}_\rho [\sigma_{e,\bx}] \le \mathbb{E}_\rho [\sqrt{\piexp (1|\bx)}] = \Theta \left( \frac{\log(|\mathcal{X}|)}{|\mathcal{X}|} \right) \\
    \sigma_e &= \sqrt{\mathbb{E}_\rho [\sigma_{e,\bx}^2]} \ge \sqrt{\frac{1}{2}\mathbb{E}_\rho [\piexp(1|\bx)]} = \Theta \left( \frac{1}{2 \sqrt{|\mathcal{X}|}} \right)
\end{align*}

For each action, construct the empirical distribution estimator, and return this policy as $\hat{\pi}_n^{\text{vf}} (0|\bx)$. Then, with probability at least $1-\delta$, conditioning on the number of samples $n_\bx$ observed with prompt $\bx$,
\begin{align*}
    | \hat{\pi}^{\text{vf}}_n (0|\bx) - \piexp (0|\bx) | \le \min \left\{ 1, \sqrt{\frac{\piexp (0|\bx) \log(2/\delta)}{n_\bx}} + \frac{ \log(2/\delta)}{n_\bx} \right\}
\end{align*}
Therefore, with probability at least $1 - \delta$,
\begin{align}
    \max_{r \in \mathcal{R}} J_r (\piexp) - J_r (\hat{\pi}^{\text{vf}}_n) &= \mathbb{E}_\rho \left[ \tv{\hat{\pi}^{\text{vf}}_n (\cdot|\bx)}{\piexp (\cdot|\bx)} \right] \nonumber\\
    &\le \mathbb{E}_\rho \left[ \left\{ 1, \sqrt{\frac{\piexp (0|\bx) \log(2|\mathcal{X}|/\delta)}{n_\bx}} + \frac{ \log(2/\delta)}{n_\bx} \right\} \right] \label{eq:090123}
\end{align}
With probability $1 - \delta$, we have that $n_\bx \ge \frac{n}{|\mathcal{X}|} - \sqrt{\frac{n}{|\mathcal{X}|} \log (1/\delta)}$ for every $\bx \in \mathcal{X}$. Assuming $\delta \ge |\mathcal{X}| \exp (-\frac{1}{2}\sqrt{n/|\mathcal{X}|})$, by union bounding, we have that with probability at least $1 - \delta$, for all $\bx \in \mathcal{X}$, $n_\bx \ge \frac{n}{2|\mathcal{X}|}$. Combining with \cref{eq:090123}, with probability at least $1-2\delta$,
\begin{align*}
    \max_{r \in \mathcal{R}} J_r (\piexp) - J_r (\hat{\pi}^{\text{vf}}_n) &\le 2 \sum_{\bx \in \mathcal{X}} \sqrt{\frac{\piexp (0|\bx) \log (|\mathcal{X}|/\delta)}{n |\mathcal{X}|}} + \frac{ \log(2/\delta)}{n}\\
    &\le 2\log(|\mathcal{X}|) \sqrt{\frac{\log (|\mathcal{X}|/\delta)}{n |\mathcal{X}|}} + 2\frac{|\mathcal{X}| \log(2/\delta)}{n} \\
    &\le 2\widetilde{\sigma}_e \cdot \sqrt{\log (|\Pi|) \frac{\log (|\mathcal{X}|/\delta)}{n}} + \frac{2\log(|\Pi|) \log(2/\delta)}{n}
\end{align*}
where the last inequality uses the fact that $|\Pi| \ge 2^{\Omega(|\mathcal{X}|)}$ and by construction, the value of $\widetilde{\sigma}_e$.

\end{proof}

\begin{lemma} \label{lemma:1-r}
For any reward $r \in \gR_{1/2}$, there exists another reward $\widetilde{r} \in \gR$ such that, for any policy $\pi \in \Pi$ and input distribution $\rho$,
\begin{align*}
    \E_{\rho, \pi} [r (\tau)] &= H -  \E_{\rho, \pi} [ \widetilde{r} (\tau)] \\
    \Var_{\rho,\pi} [ r (\tau) ] &= \Var_{\rho,\pi} [ \widetilde{r} (\tau) ]
\end{align*}
\end{lemma}
\begin{proof}
Consider the bi-level reward $r$, and consider the set of minimal states: $\cup_{\tau \in \mathcal{A}^H} \{ s_{t^\star} \text{ where } t^\star = \min \{ 1 \le t \le H : r(s_{t-1},a_t) > r(s_{t-2},a_{t-1}) \}$. These are the states where a bi-level may be first visited. For each such minimal state, the bi-level property implies that any trajectory which visits this state collects a reward of $1$ at every point in time regardless of the sequence of actions played. Based on this construction, we define the reward $\widetilde{r}$ as follows: for every minimal state $s$ which appears at time $t$, consider the subtree rooted at this node (i.e., the set of trajectories which visit this state). Delete this minimal state, and replace it by the set of all $2^{H-t}$ new minimal states corresponding to the set of all states in the subtree at depth $H-t$. Let $\widetilde{r}$ be induced by this new set of minimal states; moreover, it is feasible to construct this set because of the assumption that $r \in \mathcal{R}_{1/2}$: every minimal state appears at some value of $t \le H/2$.

Consider any trajectory $\tau$. Suppose this trajectory visits a bi-level at time $t \le H/2$. Now the same trajectory is guaranteed to visit a bi-level at time $H-t \ge H/2$. Thus, $\widetilde{r} (\tau) = H - r(\tau)$, and the assertions about $\E_{\rho,\pi} [\widetilde{r}(\tau)]$ and $\Var_{\rho,\pi} [ \widetilde{r} (\tau) ]$ follow suit.
\end{proof}






\begin{lemma}
\label{lem:packing}
For any policy $\pi$ and reward $r$, and $0 \leq \xi \leq \min_{\bx \in \mathcal{X}} \frac{\sigma_{e,\bx}^2}{4(J_r(\piexp|\bx))^2}$, there exists a class of $2^k$ policies, $\Pi_k = \{ \pi_{\bm{z}} : \bm{z} \in \{ 0,1 \}^k \}$ indexed by binary vectors, and a class of $2^k$ rewards indexed similarly as $\mathcal{R}_k = \{ r_{\bm{z}} : \bm{z} \in \{ 0,1 \}^k \}$, such that,
\begin{enumerate}
    \item For any $\bm{z}, \bm{z}' \in \{ 0,1 \}^k$, $\chisq{\pi_{\bm{z}}}{\pi_{\bm{z}'}} \le 8\xi$. Furthermore, $\chisq{\pi_{\bm{z}}}{\piexp} \le 8\xi$.
    \item $J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\pi_{\bm{z}'}) = \sqrt{\xi} \sum_{i=1}^k \mathbb{I} (\bm{z}_i \ne \bm{z}'_i (\bx)) \cdot \E_{\bx \sim \rho} [ \sigma_{e,\bx} \mathbb{I} (\bx \in \mathcal{X}^\star_i)]$,
    \item For every reward $r' \in \mathcal{R}_k$ and every $\pi' \in \Pi_k$: $\sigma_{e,\bx}^2 (\pi',r') \le \sigma_{e,\bx}^2 + H \sigma_{e,\bx} \sqrt{\xi} + H^2 \xi$.
    \item Recall that $\piexp \in \Pi_\varepsilon$, the $\varepsilon$-radius KL ball around $\pibase$. Then, every $\pi' \in \Pi_k$ belongs in the ball $\Pi_{\varepsilon'}$, where,
    \begin{align}
        \varepsilon' = 3 (1+\varepsilon) \cdot \max \left\{ \frac{\sqrt{\xi} H}{\sigma_{\min}} , \frac{\xi H^2}{\sigma_{\min}^2} \right\}.
    \end{align}
    and where $\sigma_{\min} = \min_{\bx \in \mathcal{X}} \sigma_{e,\bx}$.
\end{enumerate}
\end{lemma}
\begin{proof}
The policy $\pi_{\bm{z}}$ is defined as follows. For each $i \in [k]$ and $\bx \in \mathcal{X}_i$,
\begin{align}
    \pi_{\bm{z}} (\tau|\bx) \propto \begin{cases}
        (\sigma_{e,\bx} + \theta_\bx r(\tau))\piexp(\tau|\bx), \quad &\text{if } \bm{z}_i = 1 \\
        \piexp(\tau|\bx), &\text{otherwise.}
    \end{cases}
\end{align}
where $\theta_\bx \ge 0$ is a parameter to be determined later. Likewise, the reward $r_{\bm{z}}$ is defined as follows. For each $\bx \in \mathcal{X}_i$,
\begin{align}
    r_{\bm{z}} (\tau|\bx) \propto \begin{cases}
        r(\tau), \quad &\text{if } \bm{z}_i = 1 \\
        \widetilde{r} (\tau|\bx), &\text{otherwise.}
    \end{cases}
\end{align}
where $\widetilde{r}$ is the reward defined in \Cref{lemma:1-r}. Since we only care about values and variances, for all intents and purposes, $\widetilde{r}$ is the same as $1-r$ (which itself may not be a bi-level reward).

\paragraph{Assertion 1: Bounding the $\chi^2$-divergence between $\pi_{\bm{z}}$ and $\pi_{\bm{z}'}$.} Consider any pair of binary vectors $\bm{z},\bm{z}' \in \{ 0,1 \}^k$. If $\bm{z}_i = \bm{z}'_i$, then $\chisq{\pi_{\bm{z}} (\cdot|\bx)}{\pi_{\bm{z}'} (\cdot|\bx)} = 0$ for any $\bx \in \mathcal{X}_i$. Otherwise, if $\bm{z}_i = 1$ and $\bm{z}'_i = 0$, for any $\bx \in \mathcal{X}_i$,
\begin{align}
    \chisq{\pi_{\bm{z}} (\cdot|\bx)}{\pi_{\bm{z}'} (\cdot|\bx)} &= \chisq{\pi_{\bm{z}} (\cdot|\bx)}{\piexp(\cdot|\bx)} \nonumber\\
    &= \frac{\mathbb{E}_{\piexp} [(\sigma_{e,\bx} + \theta_\bx r(\tau))^2 | \bx ]}{\mathbb{E}_{\piexp} [\sigma_{e,\bx} + \theta_\bx r(\tau) | \bx ]^2} - 1 \nonumber\\
    &= \frac{\sigma_{e,\bx}^2 + 2 \theta_\bx \sigma_{e,\bx} J_r (\piexp | \bx) + \theta_\bx^2 ((J_r (\piexp | \bx))^2 + \sigma_{e,\bx}^2)}{(\sigma_e + \theta_\bx J_r(\piexp | \bx))^2} - 1 \nonumber\\
    &= \frac{\theta_\bx^2 \sigma_{e,\bx}^2}{(\sigma_{e,\bx} + \theta_\bx J_r(\piexp | \bx))^2} \nonumber\\
    &= \xi \label{eq:b1b'0}
\end{align}
where the last equation follows by choosing $\theta_\bx$ such that $\theta_\bx \sigma_{e,\bx} = \sqrt{\xi} (\sigma_{e,\bx} + \theta_\bx J_r(\piexp | \bx))$. There will always exist a feasible choice of $\theta_\bx \ge 0$ satisifying this equation as long as the condition $\sqrt{\xi} \le \sigma_{e,\bx} / J_r (\piexp | \bx)$ is satisfied, and under the stronger restriction $\sqrt{\xi} \le \sigma_{e,\bx} / 2 J_r (\piexp | \bx)$ we will have that $\theta_\bx \le 2\sqrt{\xi}$. On the other hand, if $\bm{z} (\bx) = 0$ and $\bm{z}' (\bx) = 1$, for any $\bx \in \mathcal{X}_i$,
\begin{align}
    \chisq{\pi_{\bm{z}} (\cdot|\bx)}{\pi_{\bm{z}'} (\cdot|\bx)} &= \chisq{\piexp(\cdot|\bx)}{\pi_{\bm{z}} (\cdot|\bx)} \nonumber\\
    &= \mathbb{E}_{\pi} [\sigma_{e,\bx} + \theta_\bx r(\tau) | \bx ] \cdot \mathbb{E}_{\pi} \left[ \frac{1}{\sigma_{e,\bx} + \theta_\bx r(\tau)} \middle| \bx \right] - 1 \nonumber\\
    &= \mathbb{E}_{\pi} \left[ \frac{\sigma_{e,\bx} + \theta_\bx J_r(\piexp | \bx)}{\sigma_{e,\bx} + \theta_\bx r(\tau)} \middle| \bx \right] - 1 \nonumber\\
    &= \mathbb{E}_{\pi} \left[ \frac{\theta_\bx ( J_r(\piexp | \bx) - r(\tau))}{\sigma_{e,\bx} + \theta_\bx r(\tau)} \middle| \bx \right] \nonumber\\
    &\overset{(i)}{\le} 2 \theta_\bx^2 \nonumber\\
    &\le 8 \xi \label{eq:b0b'1}
\end{align}
where $(i)$ follows from \Cref{lemma:Abound} and the last inequality relies on the choice of $\theta_\bx \le 2 \sqrt{\xi}$. Combining \cref{eq:b1b'0,eq:b0b'1} with an expectation over $\bx \sim \rho$ results in a proof of the first assertion.

\paragraph{Assertion 2: Bounding the value gap.} Observe that $J_r (\pi_{\bm{z}} | \bx) - J_r(\pi_{\bm{z}'} | \bx) = 0$ for any $\bx \in \mathcal{X}_i$ if $\bm{z}_i = \bm{z}'_i$. In case $\bm{z}_i = 1$ and $\bm{z}'_i = 0$ and any $\bx \in \mathcal{X}_i$, $r_{\bm{z}} (\tau) = r (\tau)$ for any $\tau$ which visits $\bx$ and,
\begin{align}
    J_{r_{\bm{z}}} (\pi_{\bm{z}} | \bx) - J_{r_{\bm{z}}} (\pi_{\bm{z}'} | \bx)
    &= \frac{\mathbb{E}_\pi [\sigma_{e} r (\tau) + \theta_\bx (r (\tau))^2 | \bx ]}{\mathbb{E}_\pi [\sigma_{e,\bx} + \theta_\bx r(\tau) | \bx ]} - J_{r_{\bm{z}}} (\piexp | \bx) \nonumber\\
    &= \frac{\sigma_{e,\bx} J_r (\piexp | \bx) + \theta_\bx (J_r(\piexp | \bx))^2 + \sigma_{e,\bx}^2)}{\sigma_{e,\bx} + \theta_\bx J_r(\piexp | \bx)} - J_r(\piexp | \bx) \nonumber\\
    &= \frac{\theta_\bx \sigma_{e,\bx}^2}{\sigma_{e,\bx} + \theta_\bx J_r(\piexp | \bx)} \nonumber\\
    &= \sigma_{e,\bx} \sqrt{\xi} \label{eq:883}
\end{align}
where the last equation follows by choice of $\theta_\bx$. When $\bm{z}_i = 0$ and $\bm{z}'_i = 1$, the same analysis results in the same bound $J_r (\pi_{\bm{z}} | \bx) - J_r(\pi_{\bm{z}'} | \bx) = \sigma_{e,\bx} \sqrt{\xi}$ for any $\bx \in \mathcal{X}_i$, and taking an expectation over $\bx \sim \rho$ proves the second assertion.

\paragraph{Assertion 3: Bound on variance of $\pi_{\bm{z}}$.} This follows from \Cref{eq:var-ub}, which bounds the variance of a policy which lies within a radius $\kappa$ $\chi^2$ ball of another: in particular, $\pi_{\bm{z}} (\cdot|\bx)$ lies in a $\xi$-sized KL ball around $\piexp (\cdot|\bx)$, which has variance $\sigma_{e,\bx}^2$, and taking an expectation over $\bx \sim \rho$. Note also that the reward $r_{\bm{z}}$ preserves variances across policies compared to $r$ (cf. \Cref{lemma:1-r} and the fact that $r_{\bm{z}}$ uses either $r$ or $\widetilde{r}$), so it suffices to carry out the variance computation under $r$.

\paragraph{Assertion 4: Bound on $\chisq{\pi}{\pibase}$ for $\pi \in \Pi_k$.} For any $\bm{z} \in \{ 0,1 \}^k$, note that $\pi_{\bm{z}}$ and $\piexp$ have density ratio upper bounded by,
\begin{align*}
    \left\| \frac{\pi_{\bm{z}} (\tau|\bx)}{\piexp(\tau|\bx)} \right\|_\infty &\le  \frac{\sigma_{e,\bx} + \theta_\bx H}{\sigma_{e,\bx} + \theta_\bx J_r (\piexp|\bx)} \\
    &\le 1+\frac{2\sqrt{\xi} H}{\sigma_{\min}}
\end{align*}
This upper bound on the density ratio implies that,
\begin{align*}
    \chisq{\pi_{\bm{z}}}{\pibase} &= \mathbb{E}_{\bx \sim \rho} \left[ \chisq{\pi_{\bm{z}}(\cdot|\bx)}{\pibase (\cdot|\bx)} \right] \\
    &\le \left( 1 + \frac{2\sqrt{\xi} H}{\sigma_{\min}} \right)^2  (1+\chisq{\piexp}{\pibase}) - 1 \\
    &\le 3 (1+\varepsilon) \cdot \max \left\{ \frac{\sqrt{\xi} H}{\sigma_{\min}} , \frac{\xi H^2}{\sigma_{\min}^2} \right\}
\end{align*}
\end{proof}


\begin{lemma} \label{lemma:GV}
There exists a subset $\mathcal{Z} \subseteq \{ 0,1 \}^k$ with $|\mathcal{Z}| = \lceil 2^{k/4} \rceil$ and such that every pair $\bm{z}, \bm{z}' \in \mathcal{Z}$ satisfies,
\begin{equation*}
    \sum_{i=1}^k \mathbb{I} (\bm{z}_i \ne \bm{z}'_i) \ge k/4
\end{equation*} 
\end{lemma}
\begin{proof}
This statement essentially follows from the Gilbert-Varshamov bound (cf. Theorem~5.2.6 in \cite{LingXing2004}).
\end{proof}


\subsubsection{Construction of policy class \texorpdfstring{$\Pi^\prime$}{} and reward class \texorpdfstring{$\mathcal{R}^\prime$}{}} \label{subsubsec:PIR}

Consider the set of policies $\Pi^\prime = \{ \pi_{\bm{z}} : \bm{z} \in \mathcal{Z} \} \subseteq \Pi_k$  and $\mathcal{R}^\prime = \{ r_{\bm{z}} : \bm{z} \in \mathcal{Z} \}$ (see the proof of \Cref{lem:packing} for a definition of $\pi_{\bm{z}}$, $\Pi_k$ and $\mathcal{R}_k$). By \Cref{lemma:GV}, $|\Pi^\prime| \approx 2^{k/4}$, and furthermore, for any $\bm{z}, \bm{z}' \in \mathcal{Z}$,
\begin{align} \label{eq:91222}
    J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\pi_{\bm{z}'}) \ge L_k^\star \sqrt{\xi} 
\end{align}
where $L_k^\star$ is defined in \Cref{eq:Lstar}. This bound follows from the first assertion in \Cref{lem:packing} and the fact that $\bm{z}$ and $\bm{z}'$ differ in at least $k/4$ coordinates; $L_k^\star$, by definition, captures the deviation for the worst-case choice of $k/4$ coordinates.

\begin{definition}[\cite{chen2016bayes,rajaraman2024statistical}]
The $\chi^2$-informativity is defined as,
\begin{align*}
I_{\chi^2}(X ; Y) \triangleq \inf _{Q_Y} \chi^2\left(P_{X Y} \| P_X \times Q_Y\right)    
\end{align*}
\end{definition}

\begin{theorem}
Consider the family of policies $\Pi^\prime$ defined above. Let $p_{\Pi^\prime}$ denote the uniform prior over them (alternately, the distribution over $\pi_{\bm{z}}$ for $\bm{z} \sim \text{Unif} (\mathcal{Z})$). Let the policy $\hat{\pi}$ be constructed via a dataset $D$ and assume that the verifier-free learner is realizable, satisfying $\hat{\pi}_n^{\text{vf}} \in \Pi^\prime$. Then,
\begin{equation*}
    \Pr ( J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\hat{\pi}_n^{\text{vf}}) \ge L_k^\star\sqrt{\xi} ) \ge 1 - \frac{1}{|\Pi^\prime|} \sqrt{I_{\chi^2} (\bm{z}; D) + 1}
\end{equation*}
\end{theorem}
\begin{proof}
Let $P$ be the joint distribution of $\bm{z}$ and $D$. Let $Q$ be the distribution $\text{Unif} (\mathcal{Z}) \times Q_{\text{data}}$ for a generic (arbitrary) data distribution $Q_{\text{data}}$. Let $T : (\bm{z}, D) \mapsto \mathbb{I} ( J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\hat{\pi}_n^{\text{vf}}) \ge L_k^\star \sqrt{\xi})$ be a generic map, and $P \circ T^{-1}$ and $Q \circ T^{-1}$ be the pushforward measures of $P$ and $Q$ by $T$. Letting $\mathcal{E} (\bm{z},D) = \{ J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\hat{\pi}_n^{\text{vf}}) \ge L_k^\star \sqrt{\xi}) \}$, the data-processing inequality gives,
\begin{align}
    \chisq{P}{Q} &\ge \chisq{P \circ T^{-1}}{Q \circ T^{-1}} \nonumber\\
    &= \frac{(P (\mathcal{E} (\bm{z},D)) - Q (\mathcal{E} (\bm{z},D)))^2}{Q(\mathcal{E} (\bm{z},D))(1 - Q(\mathcal{E} (\bm{z},D)))} \label{eq:443}
\end{align}
Let us assume that the learner's policy $\hat{\pi}$ is realizable, and satisfies $\hat{\pi} \in \Pi^\prime$.
By the product structure of $Q$, we have that,
\begin{align*}
    Q(\mathcal{E} (\bm{z},D)) \le \sup_{\pi \in \Pi^\prime} \Pr \left( J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\pi) \ge L_k^\star \sqrt{\xi} \right) = 1-\frac{1}{|\Pi^\prime|}.
\end{align*}
where the last inequality uses the fact that for any $\bm{z}' \ne \bm{z}$, $J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\pi_{\bm{z}'}) \ge L_k^\star \sqrt{\xi}$ (cf. \Cref{eq:91222}). Combining with \cref{eq:443}, rearranging, simplifying and taking the infimum over $Q_{\text{data}}$ completes the proof.
\end{proof}

\begin{lemma}
Consider any realizable verifier-free learner, satisfying $\hat{\pi}_n^{\text{vf}} \in \Pi^\prime$. Then,
\begin{align*}
    \Pr \left( J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\hat{\pi}_n^{\text{vf}}) \ge L_k^\star\sqrt{\frac{\log(|\Pi^\prime|)}{16n}} \right) \ge \frac{1}{4}
\end{align*}
\end{lemma}
\begin{proof}
Observe that,
\begin{align*}
    I_{\chi^2} (\bm{z}; D) + 1 &= \inf_{Q_{\text{data}}} \int \left[ \frac{(p_\Pi (\pi_{\bm{z}}))^2 \left( \prod_{\tau \in D} \pi_{\bm{z}} (\tau) \right)^2}{p_\Pi (\pi_{\bm{z}}) Q_{\text{data}} (D)} \right] dD d\pi \\
    &\overset{(i)}{\le} \int \left[ \frac{p_\Pi (\pi_{\bm{z}}) \left( \prod_{\tau \in D} \pi_{\bm{z}} (\tau) \right)^2}{\prod_{\tau \in D} \piexp (\tau)} \right] dD d\pi \\
    &= \int \left[ \frac{p_\Pi (\pi_{\bm{z}}) \left( \prod_{\tau \in D} \pi_{\bm{z}} (\tau) \right)^2}{\prod_{\tau \in D} \piexp (\tau)} \right] dD d\pi \\
    &= \mathbb{E}_{\pi \sim p_\Pi} [ (1+\chisq{\pi_{\bm{z}}}{\piexp})^n ] \\
    &\overset{(ii)}{\le} (1+8\xi)^n
\end{align*}
where in $(i)$ we choose $Q_{\text{data}}$ as the data distribution realized by $\piexp$ and in $(ii)$, we use the first assertion of \Cref{lem:packing}.
Choose $\xi = \varepsilon_{\text{stat}} = \frac{\log(
|\Pi^\prime|)}{16n}$, we get,
\begin{align*}
    \Pr \left( J_{r_{\bm{z}}} (\pi_{\bm{z}}) - J_{r_{\bm{z}}} (\hat{\pi}_n^{\text{vf}}) < L_k^\star\sqrt{\frac{\log(|\Pi^\prime|)}{n}} \right) \ge \frac{1}{4}
\end{align*}
\end{proof}





























