\subsection{Proof of Theorem~\ref{thm:vg-gap-lower-bound}}
\label{subsec:proof-vg-gap-lower-bound}

The proof of this result follows directly from the instance lower bound in Theorem~\ref{thm:verifier-free-thm} and suboptimality upper bound result in Theorem~\ref{thm:verifier-based-thm}. When, $\tilde{\sigma}_b =  \Omega(H)$, the lower bound on the suboptimality gap of any VF method scales as $\nicefrac{H\log(|\Pi|)}{n}$, with respect to any expert in a $O(1)$-$\chi^2$ ball around the base policy $\pibase$, where as if $\pibase$ is $c_0$ anti-concentrated, then there exists an algorithm that yields an upper bound on the suboptimality gap of $\nicefrac{H\log|\gR|}{n}$, with constant probability. Thus, in compliance with the definition of scaling test-time compute in Definition~\ref{def:h-alpha-scaling}, as we scale $n=\Omega(H)$, we get the result in Theorem~\ref{thm:main-theorem}.


As an example of such a $\pibase$, consider a single prompt, and a base policy that gets a reward of $1$ with probability $> \frac{3}{5}$ on any trajectory rolled out till horizon $H=H_0$, and that this mass remains constant as we scale $H \rightarrow \infty$, i.e., the fraction of in correct trajectories (in the set $\gS_{H_0}$) remain incorrect no matter how much we rollout $\pibase$. For this distribution, it is easy to see that $\widetilde{\sigma_b} = \Omega(H)$, but is $0.5$-anti-concentrated. 
