































\subsection{Bounding the performance of \Cref{alg:algorithm_simple_vb}}
\label{subsec:proof-verifier-based-thm}

\subsubsection{Understanding the anti-concentration assumption}

Recall that the anticoncentration assumption controls the probability of the reward $r(\tau)$ for $\tau \sim \pibase(\cdot|\bx)$ of exceeding its mean by a margin of $\sqrt{\varepsilon}$ times its standard deviation. Namely,
{
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\abovedisplayskip}{5pt}
    \begin{align*}
    \!\!\!c_\bx (\varepsilon) \eqdef \text{Pr}_{\pibase(\cdot|\bx)} \big( r(\tau) \ge \E_{\pibase(\cdot|\bx)}\brck{r(\tau)} + \sigma_{b, \bx} \sqrt{\varepsilon} \big).
    \end{align*}
}
The interpretation of $c_\bx (\varepsilon)$ is natural, as a prompt-conditional measure of anticoncentration of the rewards $r(\tau)$ collected by the base policy. However, as we discuss in the next lemma, the deviation term $\E_{\pibase(\cdot|\bx)}\brck{r(\tau)} + \sigma_{b, \bx} \sqrt{\varepsilon}$ serves a dual purpose: it precisely captures the maximum value achievable in a $\chi^2$ ball around $\pibase$ of radius $\varepsilon$.

\begin{lemma}[Characterizing the optimal value within the $\chi^2$ ball] \label{lemma:characterization} For a single prompt $\bx \in \mathcal{X}$, consider the set of policies $\Pi_{\varepsilon,\bx} = \{ \pi : \chisq{\pi(\cdot|\bs)}{\pibase(\cdot|\bx)} \le \varepsilon \}$. Then,
\begin{equation}
    \sup_{\pi \in \Pi_{\varepsilon,\bx}} \E_{\tau \sim \pi(\cdot|\bx)} [r(\tau)] \ge \E_{\pibase(\cdot|\bx)} [r(\tau)] + \sigma_{b,\bx} \sqrt{\varepsilon}.
\end{equation}
Furthermore, as long as $\varepsilon \le \frac{\sigma_{b,\bx}^2}{(J_r(\pibase|\bx))^2}$, this inequality is an equality.
\end{lemma}
\begin{proof}
Consider the candidate policy $\pi (\tau|\bx) \propto (\sigma_{b,\bx} + \theta r(\tau)) \pibase(\cdot|\bx)$ for $\theta$ to be chosen later. Mirroring the calculation in \Cref{eq:b1b'0} (with $\piexp$ replaced by $\pibase$), we see that,
\begin{align*}
    \chisq{\pi(\cdot|\bx)}{\pibase(\cdot|\bx)} = \frac{\theta^2 \sigma_{b,\bx}^2}{(\sigma_{b,\bx} + \theta J_r (\pibase|\bx))^2}
\end{align*}
The maximum achievable value of the $\chi^2$ divergence by this policy is $\frac{\sigma_{b,\bx}^2}{(J_r (\pibase|\bx))^2}$. Likewise, mirroring the calculation in \cref{eq:883},
\begin{align*}
    J_r (\pi|\bx) - J_r(\pibase|\bx) = \frac{\theta \sigma_{b,\bx}^2}{\sigma_\bx + \theta J_r(\pibase|\bx)} = \sigma_{b,\bx} \sqrt{\chisq{\pi(\cdot|\bx)}{\pibase(\cdot|\bx)}} = \sigma_{b,\bx} \sqrt{\varepsilon}
\end{align*}
Therefore, with the appropriate choice of $\theta$, this policy is a feasible policy achieving the supremum in the statement. What remains is to show that the supremum can be no larger. By \Cref{lemma:CoM}, with the choice of $Y = r(\tau)$, $P$ as the distribution over $\tau$ induced by $\pi(\cdot|\bx)$ and $Q$ the distribution over trajectories induced by $\pibase(\cdot|\bx)$. Then,
\begin{align*}
    \left| \mathbb{E}_{\tau \sim \pi(\cdot|\bx)} [r(\tau)] - \mathbb{E}_{\tau \sim \pibase(\cdot|\bx)} [r(\tau)] \right| &\le \sqrt{\Var_{\tau \sim \pibase(\cdot|\bx)} [r(\tau)] \cdot \chisq{\pi(\cdot|\bx)}{\pibase(\cdot|\bx)}} = \sigma_{b,\bx} \sqrt{\varepsilon}
\end{align*}
This shows that the supremizing value is exactly $\sigma_{b,\bx} \sqrt{\varepsilon}$.
\end{proof}


    \begin{property}[Regularity] \label{prop:reg}
Assume that for each $\bx \in \mathcal{X}$ that $J_r(\pibase|\bx) > 0$ and,
\begin{align*}
    \varepsilon_\bx \eqdef \chisq{\bar{\pi}_\kappa (\cdot|\bx)}{\pibase(\cdot|\bx)} \le \frac{\sigma_{b,\bx}^2}{(J_r(\pibase|\bx))^2}.
\end{align*}
where $\bar{\pi}_\kappa$ is any policy which collects the maximum value, while remaining within $\Pi_\kappa$.
\end{property}

\begin{lemma} \label{lemma:pi_lambda}
Suppose $\pibase$ is $c_0$-anticoncentrated for some problem horizon $h_0$ and assume that \Cref{prop:reg} holds true for the base policy at this value of $h_0$. Define a collection of parameters, $\lambda = \{ \lambda_\bx : \bx \in \mathcal{X} \}$ where $\mathbb{R} \ni \lambda_\bx \in (0,\sigma_b \sqrt{2/c_0}]$. Then, there exists a policy $\pi_c$ such that,
\begin{enumerate}
    \item Almost surely, $r(\tau) > 0$ for $\tau \sim \pi_c (\cdot|\bx)$ and any $\bx \in \mathcal{X}$.
    \item $\pi_c$ is no worse than $\piexp$. Namely, $J_r (\pi_c) \ge \sup_{\pi \in \Pi_\kappa} J_r(\pi) \ge J_r (\piexp) $.
    \item For every $\bx \in \mathcal{X}$, $\sup_{\tau : \Pr_{\pibase} (\tau|\bx) > 0} \frac{\Pr_{\pi_c} (\tau|\bx)}{\Pr_{\pibase} (\tau|\bx)} \le c_0^{-1}$ %
\end{enumerate}
\end{lemma}
\begin{proof}
Fix a prompt $\bx \in \mathcal{X}$. We will construct $\pi_c$ separately for each prompt and later argue about each of these three assertions. Since $\pibase$ is $c_0$-anticoncentrated for some problem horizon $h_0$, as long as $\varepsilon_\bx \eqdef \chisq{\bar{\pi}_\kappa (\cdot|\bx)}{\pibase(\cdot|\bx)} \le \frac{\sigma_{b,\bx}^2}{(J_r(\pibase|\bx))^2}$, by \Cref{lemma:characterization}, defining $\mathcal{T}$ as the set of trajectories $\{ r(\tau) \ge \sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \mathbb{E}_{\tau\sim\pi(\cdot|\bx)} [r(\tau)] \}$. Then,
\begin{align} \label{eq:plb}
    \Pr_{\tau \sim \pibase(\cdot|\bx)} \left( \tau \in \mathcal{T} \right) \ge c_0
\end{align}

Consider the policy $\pi_c (\cdot|\bx)$ which is the mixture over the trajectories $\mathcal{T} = \{ \tau : r(\tau) \geq  \E_{\tau \sim \pibase(\cdot \mid \bx)}[r(\tau)] + \sigma_{b,\bx} \sqrt{\varepsilon_\bx}\}$ with mixture weights $w_\tau \propto \Pr_{\pibase (\cdot|\bx)} (\tau)$. Since the MDP is autoregressive (i.e., tree-like), $\pi_c (\cdot|\bx)$ corresponds to a simple policy (as opposed to a mixture over policies), since two trajectories in $\mathcal{T}$ can not visit the same state again after a different action is played between them, i.e., a breakpoint. This implies that the mixture of these two trajectories is the same as the policies which agrees with them until the breakpoint and picks one of the trajectories to follow at the breakpoint, proportional to its weight. The same argument applies when considering a mixture over more than two trajectories. Next, we prove the three assertions of this lemma.

\paragraph{Assertion 1: Rewards are strictly positive.} $\pi_c(\cdot|\bx)$ is only supported on trajectories which collect rewards which exceed $\sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \mathbb{E}_{\tau\sim\pi(\cdot|\bx)} [r(\tau)] \ge \mathbb{E}_{\tau\sim\pibase(\cdot|\bx)} [r(\tau)]$. By \Cref{prop:reg}, we have that $\mathbb{E}_{\tau \sim\pibase(\cdot|\bx)} [r(\tau)] > 0$; this implies that the reward collected by every such trajectory is not only strictly positive, but must be at least $1$ (by the bi-level property of the rewards).


\paragraph{Assertion 2: Value bound.} $\pi_c (\cdot|\bx)$ is supported on trajectories which collect reward at least: $$\sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \E_{\tau \sim \pi(\cdot|\bx)} [r(\tau)].$$ Thus, with probability $1$, for any trajectory $\tau$ sampled from $\pi_c (\cdot|\bx)$, $r(\tau) \ge \sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \E_{\tau \sim \pi(\cdot|\bx)} [r(\tau)]$. Taking an expectation over $\tau \sim \pi_c (\cdot|\bx)$, we get, $\E_{\tau \sim \pi_c (\cdot|\bx)} [r(\tau)] \ge \sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \E_{\tau \sim \pi(\cdot|\bx)} [r(\tau)]$. Further, taking an expectation over $\bx \sim \rho$,
\begin{align*}
    \E_{\rho, \pi_c} [r(\tau)] &\ge \E_{\bx \sim \rho} \left[ \sup_{\pi \in \Pi_{\varepsilon_\bx,\bx}} \E_{\tau \sim \pi(\cdot|\bx)} [r(\tau)] \right] \\
    &\ge \sup_{\pi \in \bigcap_{\bx \in \mathcal{X}} \Pi_{\varepsilon_\bx,\bx}} \E_{\rho,\pi} [r(\tau)] \\
    &= \sup_{\pi \in \Pi_{\kappa}} \E_{\rho,\pi} [r(\tau)]
\end{align*}
where the last equation follows by definition of $\varepsilon_\bx$ (cf. \Cref{prop:reg}).

\paragraph{Assertion 3: Bounds on coverage.} Note that $\pi_c (\cdot|\bx)$ is the policy $\sum_{\tau \in \mathcal{T}} w_\tau \delta_\tau$. In particular, for any trajectory $\tau$ in the support of $\pi_c (\cdot|\bx)$,
\begin{align}
    \frac{\Pr_{\pi_c} (\tau|\bx)}{\Pr_{\pibase} (\tau|\bx)} = \frac{w_\tau}{\Pr_{\pibase(\cdot|\bx)} (\tau)} = \frac{1}{\sum_{\tau \in \mathcal{T}} \Pr_{\pibase (\cdot|\bx)} (\tau)}
\end{align}
where the last equation follows by definition of $w_\tau$. By \cref{eq:plb}, $\sum_{\tau \in \mathcal{T}} \Pr_{\pibase(\cdot|\bx)} (\tau) \ge c_0$. This completes the proof of the last assertion.
\end{proof}

\begin{lemma} \label{lemma:pi_lambda_tilde}
Suppose $\pi_b$ is $c_0$-anticoncentrated for some problem horizon $h_0$ and assume that \Cref{prop:reg} holds true for the base policy $\pibase$ at this value of $h_0$. Consider the policy $\pi_c$ introduced in \Cref{lemma:pi_lambda} at this value $h_0$. For any horizon $H > h_0$, there exists a policy $\widetilde{\pi}_c$ which satisfies essentially the same conditions,
\begin{enumerate}
    \item Almost surely, $r(\tau) > 0$ for $\tau \sim \widetilde{\pi}_c (\cdot|\bx)$ for any $\bx \in \mathcal{X}$,
    \item $\widetilde{\pi}_c$ is no worse than $\piexp$ when deployed on horizon $H$. Namely, $J_r^H (\widetilde{\pi}_c) \ge \sup_{\pi \in \Pi_\kappa^H} J_r^H (\pi) \ge J_r^H (\piexp) $.
    \item $\sup_{\tau : \Pr_{\pibase} (\tau|\bx) > 0} \frac{\Pr_{\pi_c} (\tau|\bx)}{\Pr_{\pibase} (\tau|\bx)} \le c_0^{-1}$. %
\end{enumerate}
Here, we point out that the in the third assertion (coverage), $(a)$ trajectories $\tau$ are of length $H$, and $(b)$ the variance term $\sigma_b (h_0)$ that appears is that of the base policy evaluated on the horizon $h_0$. Everywhere, we take care to superscript $J_r$ and $\Pi_\kappa$ to indicate the horizon over which the policies are considered.
\end{lemma}
\begin{proof}
Consider the ``extension'' of $\pi_c$, defined till time $h_0$, by $\pibase$ (which we assume is defined for every $t \in \mathbb{N}$). Namely, consider the policy $\widetilde{\pi}_c$ which follows $\pi_c$ till time $h_0$ and plays actions according to $\pibase$ thereon.

The first three assertions follow from the fact that $\pi_c$ is only supported on trajectories with strictly positive reward. By the bi-level property, each of these trajectories collect $1$ unit of reward at every $t > h_0$. Thus, $J^H_r (\widetilde{\pi}_c) = J^{h_0}_r (\widetilde{\pi}_c) + (H-h_0)$, while $\sup_{\pi \in \Pi_\kappa^H} J_r^H (\pi) \le \sup_{\pi \in \Pi_\kappa^{h_0}} J_r^H (\pi) + (H-h_0)$. This follows from the fact that the supremizing policy for the $H$ horizon problem can be truncated to the first $h_0$ steps to result in a candidate policy in $\Pi_\kappa^{h_0}$; in the process the value of the policy decreases by at most $H-h_0$. The last assertion follows from the fact that $\widetilde{\pi}_c$ and $\pibase$ agree after time $h_0$, so the worst-case density ratio cannot increase as $H$ increases beyond $h_0$.
\end{proof}










\subsubsection{Analysis of \Cref{alg:algorithm_simple_vb}: Proof of \Cref{thm:verifier-based-thm}}

Below, we provide implementation details of \Cref{alg:algorithm_simple_vb} and a slightly more formal version of \Cref{thm:verifier-based-thm}. We will define the confidence set $\hat{R}_\gamma$ below, and choose $\gamma$ appropriately as any upper bound to $\est(\delta)$ (see \cref{eq:estoff}). One such upper bound is provided in \Cref{lemma:estoff}. For the purpose of this section, we will assume that \Cref{alg:algorithm_simple_vb} carries out least square estimation with respect to some reward class $\gR_{\text{vb}}$ such that $r$ belongs to this class, and may be a subset or superset of the set of all bi-level rewards, $\mathcal{R}$.

\begin{theorem}[Formal version of \Cref{thm:verifier-based-thm}]
Consider a bi-level reward $r$,  base policy $\pibase$ that is $c_0$-anticoncentrated at some horizon $h_0 \leq H$ and assume that \Cref{prop:reg} is satisfied at $h_0$. Suppose the verifier is used to label the cumulative reward of every trajectory and results in a dataset of noisy reward annotations, $\{ (\bx_i,\tau_i,y_i) \}_{i=1}^n$: assume that the reward annotations are of the form $y_i = r(\tau_i) + Z_i$ where the $Z_i$'s are independent and standard normal with trajectory level variance $\Var[Z_i] \le \sigma_{\text{noise}}^2$. Then, the policy $\hat{\pi}_n^{\mathrm{vb}}$ returned by \Cref{alg:algorithm_simple_vb},  
the suboptimality gap w.r.t. the best expert $\bar{\pi}_\kappa \in \Pi_\kappa$ satisfies: with probability $\ge 1 - \delta$,
\begin{align*}   
    &J_r(\bar{\pi}_\kappa) - J_r(\hat{\pi}_{n}^\mathrm{vb}) \; \lsim \;  \frac{(H + \sigma_{\text{noise}}^2) \log(\nicefrac{|\gR_{\text{vb}}|}{\delta})}{n c_0},
\end{align*}  
With independent $O(1)$-variance noise at steps of a trajectory, note that $\sigma_{\text{noise}}^2 \le O(H)$.
\end{theorem}


Below we instantiate the confidence set $\hat{R}_\gamma$ in \Cref{alg:algorithm_simple_vb}. Recall that we assume that \Cref{alg:algorithm_simple_vb} carries out least square estimation with respect to some reward class $\gR_{\text{vb}}$: with $\hat{r}_{\text{ls}}$ as the least squares estimator,
\begin{align*}
    \hat{r}_{\text{ls}} &\gets \inf_{r' \in \gR_{\text{vb}}} \frac{1}{n} \sum_{i=1}^n (r'(\tau_i) - y_i)^2 \\
    \widetilde{R}_\gamma &= \left\{ r' \in \gR_{\text{vb}} \middle| \frac{1}{n} \sum_{i=1}^n (r'(\tau_i) - \hat{r}_{\text{ls}} (\tau_i))^2 \le \gamma \right\} \\
    \hat{R}_\gamma &= \left\{ \{ \textsf{round} (r'(\cdot)) \} : r' \in \widetilde{R}_\gamma \right\}
\end{align*}
Where $\textsf{round} (r(\cdot))$ is the ``rounding'' of the reward $r$, for every $\tau$, $r(\tau)$ is rounded to the nearest integer, breaking ties arbitrarily. We define the offline estimation error of the least-squares estimator below. Define $\mathcal{E}_\delta$ as the event,
\begin{align} \label{eq:estoff}
    \frac{1}{n} \sum_{i=1}^n (\hat{r}_{\text{ls}} (\tau_i) - r (\tau_i))^2 \le \est (\delta)
\end{align}
And suppose $\Pr (\mathcal{E}_\delta) \ge 1-\delta$ where the probability is computed over the randomness of the training dataset $\{ (\bx_i,\tau_i) \}_{i=1}^n$.

The analysis of the verifier-based learner in \Cref{alg:algorithm_simple_vb} follows the standard analysis of pessimism-based algorithms. For an arbitrary comparator policy $\pi_c$,
\begin{align}
    J_r (\pi_c) - J_r ( \hat{\pi}_n^{\text{vb}} ) &\le J_r (\pi_c) - \min_{\hat{r} \in \hat{R}_\gamma} J_{\hat{r}} ( \hat{\pi}_n^{\text{vb}} ) \nonumber\\
    &\le J_r (\pi_c) - \min_{\hat{r} \in \hat{R}_\gamma} J_{\hat{r}} ( \pi_c ) \nonumber\\
    &\le \sup_{\hat{r} \in \hat{R}_\gamma} \mathbb{E}_{\rho, \pi_c} \left[ | r (\tau) - \hat{r} (\tau) | \right]  \label{eq:003}
\end{align}
With the choice of the comparator policy $\pi_c = \widetilde{\pi}_c$, as defined in \Cref{lemma:pi_lambda_tilde},
\begin{align*}
    \sup_{\pi \in \Pi_\kappa} J_r (\pi) - J_r ( \hat{\pi}_n^{\text{vb}} ) &\le \sup_{\hat{r} \in \hat{R}_\gamma} c_0^{-1} \mathbb{E}_{\rho, \pibase} \left[ | r (\tau) - \hat{r} (\tau) | \right].
\end{align*}
where note that the base policy is assumed to be $c_0$-anticoncentrated for the horizon $h_0$. The performance of the algorithm thus relies on establishing a generalization bound for the reward estimation problem, which is proved below in \Cref{theorem:finalerror}. In conjunction, this results in the upper bound: with probability $1-\delta$,
\begin{align*}
\sup_{\pi \in \Pi_\kappa} J_r (\pi) - J_r ( \hat{\pi}_n^{\text{vb}} ) &\le \gO \left( \frac{(H + \sigma_{\text{noise}}^2) \cdot \log (\nicefrac{|\mathcal{R}_{\text{vb}}|}{\delta})}{c_0 n} \right)
\end{align*}

\begin{theorem} \label{theorem:finalerror}
Recall that the reward annotations are of the form $y_i = r(\tau_i) + Z_i$ where the noise $Z_i$ is assumed to be independent and standard normal with trajectory level variance $\sigma_{\text{noise}}^2$.
Consider any $\delta \in (0,1)$. Then, with probability $1-\delta$, simultaneously for all $r' \in \hat{R}_{\gamma}$,
\begin{align*}
    \E_{\rho,\pibase} [|r(\tau) - r'(\tau)|] \le \gO \left( \frac{(H + \sigma_{\text{noise}}^2) \cdot \log (\nicefrac{|\mathcal{R}_{\text{vb}}|}{\delta})}{n} \right)
\end{align*}
Note that with independent noise at each step, $\sigma_{\text{noise}}^2 \le O(H)$.
\end{theorem}
\begin{proof}
This result is a direct combination of \Cref{lemma:estoff,lemma:gb}.
\end{proof}


\begin{lemma}[Lemma C.1 in \cite{foster2024online}] \label{lemma:estoff}
It suffices to choose,
\begin{align} \label{eq:estoff-bd}
    \est (\delta) = \frac{8 \sigma_{\text{noise}}^2 \log(\nicefrac{|\gR_{\text{vb}}|}{\delta})}{n}
\end{align}
to guarantee that $\Pr (\mathcal{E}_\delta) \ge 1 - \delta$.
\end{lemma}

\begin{lemma} \label{lemma:empl1bound}
With the choice $\gamma = \est (\delta)$, under the event $\mathcal{E}_\delta$, $r \in \hat{R}_\gamma$. Under the same event, for every reward $r'' \in \hat{R}_\gamma$,
\begin{align*}
    \frac{1}{n} \sum_{i=1}^n |r''(\tau_i) - r (\tau_i)| \le 16 \cdot \est(\delta)
\end{align*}
\end{lemma}
\begin{proof}
The first assertion follows by definition of $\widetilde{R}_\gamma$ and \Cref{eq:estoff}, and the fact that $r$ is a bi-level reward, so it is unperturbed by the $\textsf{round} (\cdot)$ operation. For the second assertion: under $\mathcal{E}_\delta$, for any reward $r' \in \widetilde{R}_\gamma$,
\begin{align} \label{eq:332}
    \frac{1}{n} \sum_{i=1}^n (r'(\tau_i) - r(\tau_i))^2 \le \frac{2}{n} \sum_{i=1}^n (r'(\tau_i) - \hat{r}_{\text{ls}} (\tau_i))^2 + (r (\tau_i) - \hat{r}_{\text{ls}}(\tau_i)) )^2 \le 4\est (\delta)
\end{align}
Consider the $r'' = \textsf{round} (r') \in \hat{R}_\gamma$, for this choice of reward, observe that $r'' (\tau) - r(\tau) \in \mathbb{Z}$, since both rewards only take integer values. Furthermore, $(a)$ if $|r' (\tau) - r (\tau)| < 1/2$, then we know that $r''(\tau) - r (\tau) = 0$ surely, and $(b)$ if $|r' (\tau) - r (\tau)| \ge 1/2$, then $|r''(\tau) - r (\tau)| \le 2 |r' (\tau) - r (\tau)|$. This implies,
\begin{align*}
    \frac{1}{n} \sum_{i=1}^n |r''(\tau_i) - r (\tau_i)| &= \frac{1}{n} \sum_{i=1}^n |r''(\tau_i) - r (\tau_i)| \cdot \mathbb{I} (|r' (\tau) - r (\tau)| > 1/2)\\
    &\le \frac{2}{n} \sum_{i=1}^n |r'(\tau_i) - r (\tau_i)| \cdot \mathbb{I} (|r' (\tau) - r (\tau)| > 1/2)\\
    &\le \frac{4}{n} \sum_{i=1}^n |r'(\tau_i) - r (\tau_i)|^2 \cdot \mathbb{I} (|r' (\tau) - r (\tau)| > 1/2)\\
    &\le 16 \cdot \est(\delta)
\end{align*}
where the last inequality follows from \cref{eq:332}.
\end{proof}

\subsubsection{Proof of Proposition~\ref{prp:verifier-accuracy}}

\begin{lemma}[Generalization bound for learning in $L_1$-error] \label{lemma:gb}
With probability $1-2\delta$, simultaneously for all $r' \in \hat{R}_{\gamma}$,
\begin{align*}
    \E_{\rho,\pibase} [|r(\tau) - r'(\tau)|] \le \gO \left( \frac{H \cdot \log (\nicefrac{|\mathcal{R}_{\text{vb}}|}{\delta})}{n} + \est (\delta) \right)
\end{align*}
\end{lemma}
\begin{proof}
For any fixed reward $r' \in \mathcal{R}_{\text{vb}}$, by Bernstein concentration, with probability $\ge 1 - \delta$,
\begin{align*}
    \E_{\rho,\pibase} [|r(\tau) - r'(\tau)|] - \frac{1}{n} \sum_{i=1}^n [|r(\tau_i) - r' (\tau_i)|] &\le \sqrt{\frac{\Var_{\rho,\pibase} [ |r(\tau) - r'(\tau)|] \cdot \log (\nicefrac{1}{\delta})}{n}} \\
    &\le \sqrt{\frac{\E_{\rho,\pibase} [ (r(\tau) - r'(\tau))^2] \cdot \log (\nicefrac{1}{\delta})}{n}} \\
    &\le \sqrt{\frac{H \cdot \E_{\rho,\pibase} [ |r(\tau) - r'(\tau)|] \cdot \log (\nicefrac{1}{\delta})}{n}}
\end{align*}
Union bounding over rewards in $\mathcal{R}_{\text{vb}}$, and choosing an arbitrary $r' \in \hat{R}_\gamma$, by \Cref{lemma:empl1bound}, with probability $\ge 1-2\delta$,
\begin{align*}
    \E_{\rho,\pibase} [|r(\tau) - r'(\tau)|] \le 16 \cdot \est (\delta) + \sqrt{\frac{H \cdot \E_{\rho,\pibase} [ |r(\tau) - r'(\tau)|] \cdot \log (\nicefrac{|\mathcal{R}_{\text{vb}}|}{\delta})}{n}}
\end{align*}
Solving the quadratic equation results in the upper bound: with probability $\ge 1 - 2\delta$,
\begin{align*}
    \forall r' \in \hat{R}_\gamma,\quad \E_{\rho,\pibase} [|r(\tau) - r'(\tau)|] \le \gO \left( \frac{H \cdot \log (\nicefrac{|\mathcal{R}_{\text{vb}}|}{\delta})}{n} + \est (\delta) \right)
\end{align*}

\end{proof}

















