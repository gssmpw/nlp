\subsection{Lower bound on \texorpdfstring{$\sigma_e$}{}: Proof of Lemma~\ref{lem:expert-variance-lower-bound}}
\label{subsec:proof-expert-variance-lower-bound}

In this section, we show that for any base policy $\pibase$, and any expert policy $\piexp$ such that $\kl{\piexp}{\pibase} \le \kappa$,
\begin{align*}
    \sigma_e^2 \ge \sigma_b^2 - H \sigma_b \sqrt{\kappa/2}.
\end{align*}
Since $\kl{\cdot}{\cdot} \le \chi^2 (\cdot \| \cdot )$ pointwise, this implies the lower bound on $\sigma_e$ within the $\chi^2$ ball.

By definition, observe that,
\begin{align*}
    \sigma_\pi^2 &= \mathbb{E}_{\bx \sim \rho} [\Var_{\tau \sim \pi(\cdot \mid \bx)} \brck{r(\tau)}] \\
    &= \frac{1}{2} \mathbb{E}_{\bx \sim \rho} \left[ \mathbb{E}_{\tau, \tau' \sim \pi (\cdot \mid \bx)} [ ( r(\tau)-r(\tau'))^2 ] \right]
\end{align*}


Note that the squared Hellinger divergence $D_H^2$ satisfies $D_{\mathrm{H}}^2 (\cdot,\cdot) \le D_{\mathrm{KL}} (\cdot,\cdot)$ pointwise (cf. Lemma 2.4 in \citet{tsybakov2009nonparametric}). With the choice $Y = (\tau,\tau')$ in the change-of-measure argument in \cref{eq:HellCoM} of \Cref{lemma:CoM}, $h (Y) = ( r(\tau)-r(\tau'))^2$ and $P$ denote the distribution over trajectories $\pibase(\cdot\mid \bx)$ and $Q$ denote the distribution over trajectories induced by $\piexp (\cdot \mid \bx)$,
\begin{align}
| \Var_{\tau \sim \pibase (\cdot \mid \bx)} [ r (\tau)  ] - \Var_{\tau \sim \piexp (\cdot \mid \bx)} [ r (\tau)  ] | 
&\le \frac{1}{2} \sqrt{\frac{1}{2} \left(\mathbb{E}_P \left[h^2(Y )\right]+\mathbb{E}_Q \left[h^2(Y)\right]\right) \cdot \kl{(\tau_e,\tau_e')}{ (\tau_b, \tau_b')}} \nonumber\\
&\le \frac{1}{2} \sqrt{\left(\mathbb{E}_P \left[h^2(Y )\right]+\mathbb{E}_Q \left[h^2(Y)\right]\right) \cdot \kl{\tau_e}{\tau_b}} \label{eq:sq}
\end{align}
where in the last inequality, we use the fact that $\tau_e$ and $\tau_e'$ are i.i.d. $\sim \piexp(\cdot\mid\bx)$, and likewise $\tau_b$ and $\tau_b'$ are i.i.d. $\sim \pibase(\cdot\mid\bx)$, and the chain rule of KL divergence. What remains is to bound $\mathbb{E}_{\tau \sim \pi (\cdot \mid \bx)} [ ( r (\tau) - r (\tau'))^4 ]$ for $\pi = \piexp$ and $\pi = \pibase$. Since $|r(\tau) - r(\tau')| \le H$ almost surely,
\begin{align*}
    \mathbb{E}_{\tau \sim \pi (\cdot\mid\bx)} [ ( r (\tau) - r (\tau') )^4 ] &\le 2H^2 \Var_{\tau \sim \pi (\cdot\mid\bx)} [ r (\tau) ]
\end{align*}
Let's denote $A = \Var_{\tau \sim \piexp (\cdot\mid\bx)} [r(\tau)]$ and $B = \Var_{\tau \sim \pibase (\cdot\mid\bx)} [r(\tau)]$. Combining with \cref{eq:sq} and squaring, and denoting $\kl{\tau_e}{\tau_b} = \kappa_\bx$,
\begin{align}
    &\left( A - B \right)^2 \le \frac{H^2}{4} \left( A + B \right) \cdot \kappa_\bx \nonumber\\
    \implies &A^2 - \left( 2 B + \frac{\kappa_\bx H^2}{4} \right) A + \left( B^2 - \frac{\kappa_\bx H^2}{4} B \right)  \le 0 \label{eq:23}
\end{align}
This is a quadratic equation in $A$. Solving, we get,
\begin{align*}
    A &\ge \left( B + \frac{\kappa_\bx H^2}{8} \right) - \sqrt{\left( B + \frac{\kappa_\bx H^2}{8} \right)^2 - \left( B^2 - \frac{\kappa_\bx H^2}{4} B \right)} \\
    &= \left( B + \frac{\kappa_\bx H^2}{8} \right) - \sqrt{ \frac{\kappa_\bx H^2}{2} B + \frac{\kappa_\bx^2 H^4}{64}} \\
    &\ge B - H \sqrt{ \kappa_\bx B/2}
\end{align*}
where the last inequality uses the subadditivity of the $\sqrt{\cdot}$ function. This implies that,
\begin{align*}
    \Var_{\tau \sim \piexp (\cdot\mid\bx)} [r(\tau)] \ge \Var_{\tau \sim \pibase (\cdot\mid\bx)} [r(\tau)] - H \sqrt{ (\kappa_\bx/2) \Var_{\tau \sim \pibase (\cdot\mid\bx)} [r(\tau)] }
\end{align*}
Taking an expectation over $\bx \sim \rho$ on both sides,
and using Jensen's inequality,
\begin{align*}
    \sigma^2_e &\ge \sigma^2_b - H \mathbb{E}_{\bx \sim \rho} \left[ \sqrt{ (\kappa_\bx/2) \Var_{\tau \sim \pibase (\cdot\mid\bx)} [r(\tau)] } \right] \\
    &\ge \sigma^2_b - H \sqrt{ \E_{\bx \sim \rho} [\kappa_\bx/2] \mathbb{E}_{\bx \sim \rho} \left[ \Var_{\tau \sim \pibase (\cdot\mid\bx)} [r(\tau)] \right]} \\
    &= \sigma^2_b - H \sigma_b \sqrt{\kappa/2}
\end{align*}
Noting that $\mathbb{E}_{\bx \sim \rho} [\kappa_\bx] \le \kappa$. Solving for the larger root of the quadratic in \cref{eq:23}, we also arrive at the upper bound,
\begin{align}
    A &\le B + H \sqrt{\kappa_\bx B /2} + \frac{\kappa_\bx H^2}{4} \nonumber\\
    \implies \sigma_e^2 &\le \sigma_b^2 + H \sigma_b \sqrt{\kappa/2}+ \frac{\kappa H^2}{4}. \label{eq:var-ub}
\end{align}
which follows by taking an expectation over $\bx \sim \rho$.

\paragraph{Optimality of \Cref{lem:expert-variance-lower-bound}.} The above result is tight up to constants. Consider an autoregressive MDP with a single prompt, where picking action $a_0$ at time $1$ results in hitting a bi-level (so, regardless of future actions, a reward of $1$ is collected at each step) and picking action $a_1$ results in a reward of $0$ forever. $\pibase$ picks the first branch with probability $p$ and the second with probability $1-p$ at $t=1$. Then, $\sigma_b^2 = p (1-p) H^2$ and by scaling $p$ from $0$ to $1/2$, any $0 \le \sigma_b^2 \le H^2/4$ can be achieved. On the other hand, consider the policy $\piexp$ which plays $a_0$ with probability $p - \theta$ at $t=1$. Suppose $p$ is a constant. Then,
\begin{align*}
    \chi^2 (\piexp \| \pibase) &= \frac{(p - \theta)^2}{p} + \frac{1 - 2 (p - \theta) + (p-\theta)^2}{1-p} - 1 \\
    &= \frac{p^2 - 2p \theta + \theta^2}{p} + \frac{(1 - p)^2 + 2\theta(1-p) + \theta^2}{1-p} - 1 \\
    &= \frac{\theta^2}{p} + \frac{\theta^2}{1-p} \\
    &= \frac{\theta^2}{p(1-p)}
\end{align*}
Therefore, choosing $\theta = \min \{ p, \sqrt{\kappa p (1-p)} \}$, we get,
\begin{align*}
    \chi^2 (\piexp \| \pibase) \le \kappa
\end{align*}
And furthermore that, $\sigma_e^2 = (p-\theta) (1 - (p-\theta))H^2$ and therefore,
\begin{align*}
    \sigma_e^2 - \sigma_b^2 &= (p-\theta) (1 - (p-\theta))H^2 - p(1 - p)H^2 \\
    &= - (\theta + \theta^2 - 2p \theta) H^2,
\end{align*}
when $\theta = p$, we get $\sigma_e^2 = 0$. When $\theta = \sqrt{\kappa p (1-p)}$, this is assumed to be in the regime $\theta > p$ and so,
\begin{align*}
    \sigma_e^2 - \sigma_b^2 &\le - (\theta + p \theta - 2 p \theta) H^2 \\
    &\le - \frac{\theta}{2} H^2
\end{align*}
where in the last equation we recall the assumption that $p \le 1/2$. Plugging in $\theta$ and observing that $H^2 \theta = H \sigma_b \sqrt{\kappa}$ completes the proof.


