

\begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/toy_vary_compute.pdf}
        \caption*{\footnotesize(a)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]
        {figures/toy_vary_samples.pdf}
        \caption*{\footnotesize(b)}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \setlength{\abovecaptionskip}{0.5pt}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/toy_vary_compute_and_data.pdf}
        \caption*{\footnotesize(c)}
    \end{subfigure}
    \vspace{-0.1cm}
    \caption{\footnotesize{\textbf{\emph{Contextualized planted subsequence:}} We setup a heterogeneous base policy $\pibase$, and induce an expert by rejection sampling correct traces from $\pibase$. 
    (a) Fixing data size at $2^{10}$ we scale test compute, training separate SFT, RL policies for each compute budget. (b) For a fixed compute budget of $2^6$ we scale data, and train a set of SFT and RL policies for each $n$. In (a), (b) we find  RL scales both data and test-time compute efficiency over SFT. In (c) we scale both test compute and training data and note that the gap between the performance of RL and SFT grows super linearly, as predicted by our result in Theorem~\ref{thm:vg-gap-lower-bound}.}
    \vspace{-0.2cm}}
    \label{fig:toy-panel}
\end{figure*}

\vspace{-0.35cm}
\section{Illustrating Theory in Practice}
\label{sec:didactice}
\vspace{-0.1cm}
Our theoretical results in Section~\ref{sec:theory-compare-vfree-vbased} show that when the base policy is heterogeneous, VF approaches perform poorly. However, this can still be favorable for  VB Algorithm~\ref{alg:algorithm_simple_vb}, as long as the anti-concentration condition (Property~\ref{prp::anti-conc}) holds. We now use a didactic setting representative of typical LLM reasoning problems to validate our theoretical results, and study real math reasoning problems in the next section.

\textbf{Didactic setup.} We extend the planted subsequence problem from \citet{setlur2024rewarding}  to a contextual version for our analysis. Concretely, for an input problem  $\bx = (x_1,$$..$$, x_5)$, we say that a response $\by$ with $H$ tokens
is a correct trace if there exists a \emph{gold} contiguous subsequence $(g(x_1),$$..,$$g(x_{5}))$ planted in $\by$. Here, the underlying mapping $g:[10]$$\mapsto$$[30]$ is fixed but unknown.  For a state $\bs \eqdef (\bx, a_1,$$ ..$$, a_h)$, the bi-level reward $r(\bs) = 1$ if and only if there exists some $h^\prime \leq h$ such that the last $5$ tokens before $h'$  
match the gold subsequence.  In order to use the same performance scale to compare  methods trained for different horizon $H$ values (test-time compute budget), we  $J_r(\pi)$ and divide it by the maximum reward of $H-4$. Additional details regarding the setup are shown in Appendix~\ref{sec:additional-didactic}.


\textbf{Base policy.} We wish to construct base policies $\pibase$ that: \textbf{(i)} differ in their heterogeneity, and \textbf{(ii)} satisfy the anti-concentration condition. To do so, we finetune GPT2-xl~\cite{radford2019language} on samples obtained from a mixture of hand-designed ``procedural'' policies. Inspired from \citet{setlur2024rewarding}, a procedural policy $\mu_\gamma (\by^\star_{k+1} | \bs)$ $\propto$ $\gamma$, when the last $k$ tokens in the state $\bs$, match the first $k$ tokens in the gold subsequence $\by^\star$.  Thus, the normalized return for $\mu_\gamma$$\rightarrow$$1$, as $\gamma$$\rightarrow$$\infty$. We vary the heterogeneity of $\pibase$ by finetuning GPT2-xl on data from a mixture of procedural policies with $\gamma$ $\in$ $[1000]$.

\textbf{Verifier-free SFT \& verifier-based RL.} Given  $n$ prompts, we collect traces from an expert by running rejection sampling with $\pibase$, \textit{i.e.}, for each prompt, we  sample responses from  $\pibase$ until a correct trace is sampled. Next, we run SFT on this dataset in a verifier-free manner to obtain $\hat{\pi}^{\mathrm{vf}}_n$, similar to STaR~\citep{zelikman2022star}. 
For RL, we implement a practical version of Algorithm~\ref{alg:algorithm_simple_vb}. We train a verifier (GPT2-xl) as a multiclass classifier that predicts the bi-level reward over $H\!+\!1$ values: $0$ to $H$. To collect training data, we draw a response $\tau \sim \pibase(\cdot \mid \bx)$ for each of the $n$ prompts and annotate it ground-truth $r(\tau)$. 
Using this, we train a reward model $\hat{r}$, and learn policy  $\hat{\pi}^{\mathrm{vb}}_n$ by running REINFORCE 
 (with a KL constraint) against $\hat{r}$~\citep{ahmadian2024back}.







\textbf{Results: scaling test-time compute.} In Figure~\ref{fig:toy-panel}(a), we compare the test-time efficiency (normalized $J_r$) of SFT and RL as we scale test-time token budget $H$,  fixing $n$$=$$2^{10}$. The performance of any procedural policy $\mu_\gamma$ improves with $H$, since there is a greater chance of sampling the gold subsequence. A similar argument applies to base and expert policies that are mixtures over $\mu_\gamma$. But perhaps counterintuitively, the gap between SFT and expert policy worsens as $H$ increases, matching our result in Theorem~\ref{thm:verifier-free-thm} where the gap grows with $H$. This is because the heterogeneity of each procedural policy (and hence $\sigma_b$) scales with $H$. On the filp side, RL  nearly matches the expert (Theorem~\ref{thm:verifier-based-thm} shows suboptimality gap that is independent of $\sigma_b$), until a much higher $H$, after which it deviates slightly, likely because of decline in verifier accuracy at higher $H$ (Appendix~\ref{sec:additional-didactic}), resulting in reward hacking~\cite{gao2023scaling} during RL. One way of avoiding reward hacking is by scaling  data for training the reward model, along with scaling the token budget (compute). In this case, we find that the performance gap between the RL trained policy and the SFT trained one grows super linearly (see Figure~\ref{fig:toy-panel}(c)), as predicted by our result in Theorem~\ref{thm:vg-gap-lower-bound}.  




\begin{wrapfigure}{r}{0.35\textwidth}
\vspace{-0.2cm}
  \begin{center}
    \includegraphics[width=0.37\textwidth]{figures/toy_vary_sigmab.pdf}
  \end{center}
  \vspace{-0.4cm}
  \caption{\footnotesize \textbf{\emph{Varying $\sigma_b$:}} We vary the heterogeneity of $\pibase$ and find that when it is low, SFT can outperform RL. \vspace{-0.2cm}}
    \label{fig:toy-vary-sigma-b}
\end{wrapfigure}
We also conduct several ablation studies in this experiment:
\begin{itemize}[topsep=-2pt,itemsep=4pt]
    \item \textbf{Scaling data budget.} In Figure~\ref{fig:toy-panel}(b), we fix the test-time compute to $2^6$ tokens, and scale the data budget $n$. Expectedly, we see the performance of both SFT and RL improve, but the slope for the RL curve is much higher than that of SFT, which agrees with our theoretical result on VB being more sample efficient ($\nicefrac{1}{n}$) than VF ($\sqrt{\nicefrac{1}{n}}$ in Theorem~\ref{thm:verifier-free-thm}).
    \item \textbf{Effect of policy heterogeneity.} In Figure~\ref{fig:toy-vary-sigma-b}, we compare the performance of SFT and RL policies as we reduce the heterogeneity of the base policy. Consistent with our discussion in Section~\ref{subsec:verifier-free}, the suboptimality gap for SFT reduces with the base policy's heterogeneity. In this regime we find that VF methods outperform VB, primarily because of the decline in verifier accuracy (Appendix~\ref{sec:additional-didactic}), and perhaps the anti-concentration property is also not satisfied. 
\end{itemize}























