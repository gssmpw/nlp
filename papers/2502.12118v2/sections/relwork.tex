\vspace{-0.2cm}
\section{Related Work}
\label{sec:relwork}
\vspace{-0.15cm}
Recent works~\cite{sardana2023beyond,snell2024scaling} show that scaling test-time compute can improve performance at rates faster than scaling data~\cite{li2024common} or model size~\cite{hoffmann2022training}, either by \textbf{(i)} training an LLM to implement a search~\cite{yao2023tree,gandhi2024stream} or refinement~\citep{kumar2024training,qu2024recursive} procedure in its long chain-of-thought~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,deepscaler2025,MoonshotAI}; or \textbf{(ii)} wrapping LLMs with search procedures~\cite{wu2024inference,beeching2024scalingtesttimecompute} that use trained verifiers~\cite{cobbe2021gsm8k,setlur2024rewarding} as cost functions for search. We do not study this distinction in this paper, but rather focus on an orthogonal axis that can be used to separate test-time methods: whether or not there is access to a rewardsignal during training and/or inference.

Several works use 0/1 ``outcome'' reward annotations~\cite{uesato2022solving,bi2024deepseek} for training LLM reasoners. Some also use trained verifiers~\cite{hosseini2024v} or run search test-time search~\cite{welleck2024decoding,chen2024more}. More recently, using 0/1 verification signals for RL~\citep{MoonshotAI,deepseekai2025deepseekr1incentivizingreasoningcapability,deepscaler2025} has shown impressive results. Other  algorithms use verification by converting 0/1 rewards into a value function~\cite{rafailov2023direct,zelikman2022star,singh2023beyond,setlur2024rl}. Verification can be done generatively~\cite{zhang2024generative,mahan2024generative} and implicitly~\cite{yuan2024implicitprm,cui2025processreinforcementimplicitrewards}, all within one single LLM alternating between generation and verification. We bucket all of them as VB methods and show that querying verifiers or rewards is critical for test-time scaling.

With the goal of distilling search procedures~\citep{yang2022chain,xie2024monte,lehnert2024beyond,gandhi2024stream} or long chain-of-thought reasoning procedures~\citep{muennighoff2025s1,openthoughts}, some work also runs SFT on top of search traces~\cite{gandhi2024stream,nie2024evolve} or reasoning traces obtained from RL-trained models, that all succeed eventually. This is done so that the LLM learns to search for solutions on test problems~\cite{sel2023algorithm}. 
These methods are reward-free and are thus forced to mimic heterogeneous search traces with varying token counts, despite the fact that several of the most recent results employ excessive data subsampling~\citep{muennighoff2025s1,ye2025limo}.
This implies that generalization is likely difficult for \emph{any} SFT method, especially when the test-time budget is scaled. This general limitation of SFT-based methods aligns with prior findings~\citep{kumar2024training,xiang2025towards} and the more recent systematic studies~\citep{chu2025sft}. 

Some recent works also analyze self-improvement in LLMs under the assumption of a verification-generation gap~\cite{song2024mind}, or frame sharpening model confidence~\cite{huang2024self} as a form of self-improvement, where the most likely solution is assumed to be correct. In contrast, we do not assume that learning to verify is easier than learning to generate the correct trace, but instead show much weaker yet realistic conditions on the pre-trained LLM, where such a gap exists.









