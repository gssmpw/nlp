\vspace{-0.2cm}
\section{Notation and Preliminaries}
\label{sec:prelim}
\vspace{-0.1cm}

\textbf{Notation.} We use the usual ``big-oh'' notation, \textit{i.e.},  $\gO/\Omega$, where $a = \widetilde{\gO}(b)$ when $a = \gO(b \cdot \max(1, \mathrm{polylog}(b)))$, and $a \lsim b$ when $a = \gO(b)$. 
The set of integers $\{1,\ldots,n\}$ is denoted as $[n]$. For a set $\mathcal{S}$, the set of all measures over $\mathcal{S}$ is given by $\Delta(\mathcal{S})$. Next, we introduce some of the RL specific preliminaries we use.

\textbf{Preliminaries.} Following prior work~\cite{kazemnejad2024vineppo,setlur2024rl} we model language generation as a token-level Markov decision process (MDP): $\gM(\gS, \gA, r, H)$, with state space $\gS$, token space $\gA$, binary reward $r: \gS \times \gA \mapsto \{0, 1\}$ in class $\gR$, 
and horizon (token budget) $H$. Let $\gS_h$ denote the set of states at time $h$ (so,   $\gS  \eqdef \cup_{h=1}^H \gS_h$). The set of initial states $\gS_1$ is the set of input problems $\bx \in \mathcal{X}$, sampled from a distribution $\rho$. At time $h$, state $\bs_h$ is given exactly by the concatenation of the problem $\bx$ and the sequence of tokens sampled till step $h-1$, \textit{i.e.}, $\bs_h = (\bx, a_1, \ldots, a_{h-1})$. Upon producing token $a_h$, the environment deterministically transitions to state $\bs_{h+1} = (\bs_h, a_h)$ obtained by concatenation and collects reward $r_h \eqdef r(\bs_h, a_h)$. 
A policy $\pi \in \Pi$ is a function $\pi_h : \gS \mapsto \Delta(\gA)$ which produces a distribution over tokens at each state. We use $d^\pi_h$ to denote the distribution over $\gS_h$ induced by $\pi$. A \emph{solution trace} is a rollout  $\tau = (\bx, a_1, \dots a_H)$ in the MDP, and $r(\tau) = \sum_{h} r(\bs_h, a_h)$. We let the notation $\E_{\rho,\pi} [\cdot]$ denote the expectation $\mathbb{E}_{\bx \sim \rho} [\mathbb{E}_{\tau \sim \pi(\cdot\mid\bx)} [\cdot]]$.


\vspace{-0.2cm}
\section{What Does it Mean to Effectively Scale Test-Time Compute?}
\vspace{-0.1cm}
Our goal is to compare methods that finetune LLMs to most efficiently scale test-time compute. We say that an algorithm is effective at making \emph{consistent use} of test compute if it attains the best performance possible within a fixed compute budget. In practice, this means that an approach must strike a balance between directly ``guessing'' an answer, which uses the least number of tokens but is unlikely to succeed, and re-attempting sequentially (e.g., running linearized search or generative verification),
which is less token efficient and wastes compute, but is more likely to succeed at least once. This entails a procedure where models are deployed with an ever-growing upper bound on test-time token budgets in hopes to find more successes for a given prompt, or in other words, answer more questions within a given large test-time budget, underscoring the necessity of efficient asymptotic scaling as we formalize in this section.

Denoting a base LLM as an autoregressive policy $\pibase(a|\bs)$ and a given budget on test-time compute represented in terms of a maximum $H$ output token length, we evaluate a finetuning algorithm by measuring the performance of the policy produced by finetuning $\pibase$ under a specific reward function $r(\bs, a)$. This reward function should capture both the accuracy and the how quickly (in terms of the number of tokens), the LLM can find a solution. One such family of reward functions is a \textbf{\emph{bi-level reward}}. 





\begin{wrapfigure}{r}{0.35\textwidth}
\vspace{-0.2cm}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figures/bi_level_reward_example.pdf}
  \end{center}
  \vspace{-0.4cm}
  \caption{\footnotesize \textbf{\emph{Example of bi-level rewards:}} After step $1$ where reward is $1.0$, irrespective of future actions reward remains $1.0$.\vspace{-0.2cm}}
    \label{fig:bi-level-reward-example}
\end{wrapfigure}
\textbf{Bi-level reward.} 
As discussed in Property~\ref{prp:bi-level-reward}, we say that a reward function is a bi-level reward when on any given trajectory, the reward remains $0$ until it reaches a state corresponding to the correct solution, 
at which point it receives a reward of $1$ (for the first time), and then continues to collect $1$s in the future no matter what it samples (Figure~\ref{fig:bi-level-reward-example}).
That is, once the LLM generates a correct solution, it continues to attain high rewards.  
For a solution trace $\tau = (\bx, a_1, \dots a_H)$ we define the reward  $r(\tau) \eqdef  \sum_{h=1}^H r(\bs_h, a_h)$, and the performance (expected reward) of $\pi$ is $J_r(\pi) \eqdef \E_{\rho, \tau} \left[r(\tau)\right]$.
A \emph{correct trace} $\tau$ is one that gets the answer correct at some point within the budget of $H$ tokens, \textit{i.e.}, $r(\tau) > 0$. To maximize efficiency, we want $r(\tau)$ to be as high as possible in the distribution of the test problem, denoted $\rho$ (\cref{eq:optimization}). $Q_\pi(\bs_h, a_h)$ denotes the expected cumulative reward attained by a given LLM $\pi$, in expectation over test problems.
{
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
    \label{eq:optimization}
    \max_\pi~~~ J_r(\pi) := \mathbb{E}_{\rho, \pi} \left[ \sum_{t=0}^H r(\bs_t, a_t)  \right],\quad\quad Q_\pi(\bs_h, a_h) \eqdef \E_{\rho, \pi}\brck{\sum_{t=h}^H r(\bs_t, a_t) \mid \bs_h, a_h}.
\end{align}
}

\begin{tcolorbox}[colback=green!5!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{property}[Bi-level rewards]
\label{prp:bi-level-reward} For any trajectory $\tau$, rewards are binary and non-decreasing, \textit{i.e.} $\forall h \in [H]$, $r_{h+1} (\bs_{h+1}, a_{h+1}) \geq r_{h} (\bs_h, a_h)$, (example in Figure~\ref{fig:bi-level-reward-example}). 
\end{property}
\end{tcolorbox}
While practical approaches do not necessarily maximize bi-level reward directly, many of them utilize some form of length penalty~\citep{arora2025traininglanguagemodelsreason} or a curriculum~\citep{deepscaler2025}, to implicitly or explicitly force models to find solutions in the smallest number of tokens possible. Our notion of bi-level reward models this formally.

\textbf{Asymptotic test-time compute scaling.} Having defined how we can measure the efficacy of test-time scaling enabled by a finetuning algorithm, within a budget of $H$ tokens, we now turn to providing a formal definition that allows us to compare different algorithms. Concretely, Definition~\ref{def:h-alpha-scaling} defines what it means for an algorithm $\mathcal{A}_1$ to \emph{``asymptotically''} scale test-time compute by $H^\alpha$, compared to another algorithm $\mathcal{A}_2$. Under our bi-level reward formulation, a higher value of $\alpha$ implies that $\gA_1$ is able to succeed by spending $\approx H^\alpha$ less compute on average compared to $\gA_2$, as we scale $H$. Next, we show that verifier-based algorithms scales test compute by $\tilde{\Omega}(H^\alpha)$ compared to veriifer-free algorithms.   
\vspace{-0.1cm}
\begin{tcolorbox}[colback=green!5!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{definition}[Scaling test-time compute by $H^\alpha$]
\label{def:h-alpha-scaling}
Fix any bi-level reward $r$, base policy $\pibase$, horizon $H$ and data budget $n = \Omega(H)$, we say that algorithm  $\gA_1$ producing policy $\gA_1(H)$, \emph{asymptotically} scales test-time compute by $H^\alpha$ compared to $\gA_2$ producing  $\gA_2(H)$ if:
\begin{align*}
J_r(\gA_1(H)) -  J_r(\gA_2(H)) = \tilde{\Omega}(H^\alpha).
\end{align*}
\end{definition}
\end{tcolorbox}


















