\vspace{-0.2cm}
\section{Theory: When Does Verification Enable Asymptotic Scaling of Test Compute?}
\vspace{-0.1cm}
\label{sec:theory-compare-vfree-vbased}

In this section, we theoretically compare \emph{verifier-free} and \emph{verifier-based} algorithms when scaling test-time compute. We show that for any bi-level reward, there are base policies (pre-trained LLMs) that enable verification based algorithms to asymptotically scale test-time compute $H$, by a factor of $\Omega(\sqrt{H})$ relative to \emph{any} verifier-free approach, and quantify these properties of the pre-trained base LLM.

A \emph{\textbf{verifier-free (VF) algorithm}} finetunes the base LLM $\pibase$ to mimic data from an expert policy $\piexp$ without using any rewards or verification. The expert $\piexp$ can produce a solution trace that directly results in the final correct answer~\cite{zelikman2022star} or perform a number of search and/or backtracking operations to eventually end in the final correct answer~\cite{gandhi2024stream}. The expert policy samples correct  traces $\tau$, \textit{i.e.} $r(\tau) > 0$, however these traces are not guaranteed to be the most compute-efficient (i.e., $r(\tau) \neq H$) as each one may get to the answer spending varying number of tokens for search, backtracking, and CoT. 

The performance of any verifier-free algorithm is dependent on the \textbf{\emph{choice of the expert}}. So, how do we choose ``good'' experts for learning? Such experts must satisfy two conditions: \textbf{(a)} they should attain high rewards (end in a correct final answer),
and \textbf{(b)} the expert's distribution should be at least somewhat ``close'' to the base policy $\pibase$ to prevent issues such as memorization and optimization pathologies from finetuning~\citep{kang2024unfamiliar,tajwar2024preference}.
For e.g., one predominant way of constructing expert data is to first sample multiple traces from $\pibase$ and then retain all correct traces~\citep{zelikman2022star,gulcehre2023reinforced}. While existing theoretical abstractions do not prescribe an ideal condition to quantify \textbf{(b)}, we formalize this practical constraint by constraining the expert to be the highest reward policy in $\Pi_\kappa$: the set of all policies with   $\chi^2$ divergence $\leq\kappa$ w.r.t. the base $\pibase$. We choose $\chi^2$ over other f-divergences like KL for simplicity~\citep{huang2024correcting}. 
{
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
   \chisq{\piexp}{\pibase} \eqdef \E_{\rho,\pibase}  \left[\left( \frac{\piexp(\tau\mid\bx)}{\pibase(\tau\mid\bx)} -1 \right)^2 \right] \le \kappa.
\end{align}
}We refer to the $\kappa$-$\chi^2$ ball of expert policies as $\Pi_{\kappa}$, and the optimal expert, i.e., $\argmax_{\pi \in \Pi_{\kappa}} J_r(\pi)$, as $\bar{\pi}_{\kappa}$.

A \emph{\textbf{verifier-based (VB) algorithm}} is one that finetunes the base policy without accessing an expert policy, but instead queries an annotator to provide reward labels to solution traces sampled from the base policy $\pibase$. 
For \textit{e.g.}, RL with outcome rewards~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} or using generative verifiers~\citep{zhang2024generative} count as verifier-based methods. Note that this definition does \emph{not} necessarily require a learned verifier.
In all, these classes of methods \emph{differ in the learning signal being accessed}: access to an expert policy vs. access to a reward annotator that provides bi-level reward. 

We compare VF and VB methods, given access to sampling $n$ rollouts from expert policy for VF methods and $n$ base policy rollouts with reward annotations for VB. We are interested in evaluting whether VB methods scale test-time compute better than VF as per Definition~\ref{def:h-alpha-scaling}. Our main theoretical result, Theorem~\ref{thm:main-theorem}, states that for \emph{any} bi-level reward function, there exist base policies $\pibase$, representative of practical pre-trained LLMs, where a simple VB method scales test-time compute better than \emph{all} VF methods by at least $\Omega(\sqrt{H})$. Next, we formalize the class of base policies that induce this separation.  
{
\setlength{\abovedisplayskip}{-10pt}
\setlength{\belowdisplayskip}{-10pt}
\begin{tcolorbox}[colback=red!6!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{theorem}[Main result; informal]
    \label{thm:main-theorem}
    For any bi-level reward $r$ and sufficiently large data budget $n$, there exists a base policy $\pibase$, verifier-based algorithm $\gA$, such that finetuning $\pibase$ with $\gA$ scales test-time compute (Definition~\ref{def:h-alpha-scaling}) by $\tilde{\Omega}(\sqrt{H})$ relative to
    any verifier-free algorithm. 
\end{theorem}
\end{tcolorbox}
}

\textbf{Key insight.} To prove the result above, we establish an instance-dependent information-theoretic lower bound on the suboptimality gap of \emph{any} VF method, which is $H / \sqrt{n}$ when $\pibase$ is sufficiently \emph{heterogeneous}, \textit{i.e.},  solution traces for a given prompt vary a lot  in token efficiency.
Then, we show that a simple verifier-based method attains a suboptimality gap upper bound of only $H/n$, even when $\pibase$ is heterogeneous. 
For this, $\pibase$ need only cover some high-reward traces with a sufficient (constant) probability. 
Put formally, when the distribution over rewards  attained by traces sampled from $\pibase$ is heterogeneous  and not too ``sharply'' concentrated around its mean and $n = \Omega(H)$ (typically the case for best performance), VB methods scale test-time efficiency by $\sqrt{H}$ over VF methods. A pictorial illustration of these conditions is shown in Figure~\ref{fig:dist-sketch}, which we also show holds empirically (Section~\ref{sec:experiments-math-reasoning}). 
Then, we use techniques from second-order adaptive bounds to develop a novel analysis for proving the separation result. 



































\vspace{-0.3cm}
\subsection{Lower Bounds for Verifier-Free Expert Cloning}
\label{subsec:verifier-free}
\vspace{-0.1cm}
We first derive an information-theoretic lower bound for VF methods comparing them to the expert policy $\piexp$. To understand the implications of our theoretical result, we state our lower bound using a notion of ``\emph{base policy heterogeneity}'', which measures the variability in the token sequences that all attain the same final answer under $\pibase$. We define this notion of policy heterogeneity as follows:
{
\setlength{\abovedisplayskip}{-10pt}
\setlength{\belowdisplayskip}{-10pt}
\begin{tcolorbox}[colback=green!5!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{property}[Policy heterogeneity]
    \label{def:exp-heterogeneity}
    Given problem $\bx$, the heterogeneity of $\pi$ $\in$ $\Pi$ at $\bx$ is given by:
    {
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    \begin{align*}    
    \sigma^2_{\pi, \mathbf{x}} \eqdef \sum_{h=1}^{H} \E_{\bs_h \sim d^{\pi}_{h}}\brck{\Var_{a\sim\pi(\cdot \mid \bs_h)}\brck{Q^{\pi_e}(\bs_h, a_h)} \mid \bx}. 
\end{align*}}The total heterogeneity across problems is  $\sigma^2_\pi \eqdef \E_{\bx \sim \rho} \brck{\sigma_{\pi, \bx}^2}$, the median across problems  is $\widetilde{\sigma}_\pi := \text{Median}(\cbrck{\sigma_{\pi, \bx} : \bx \in \gX})$, and finally the mean heterogeneity across problems is $\bar{\sigma}_\pi = \E_{\bx \sim \rho} \brck{\sigma_{\pi, \bx}}$. 
\end{property}
\end{tcolorbox}
}
For the expert policy, heterogeneity is non-zero when different solution traces spend different tokens and token budgets to attain the final answer from \emph{any} state-action tuple attained in a trajectory. We expect most practical LLM finetuning datasets obtained by rejection sampling, concatenating search traces, collecting human thinking trace data, or distilling from larger models to induce quite a heterogeneous expert, since a high diversity of solution traces is often a desideratum employed by practitioners when generating training data in supervised finetuning~\citep{chen2024diversity}.
In order to obtain heterogeneous expert traces, we would also need the base policy $\pibase$ to be heterogeneous.
In fact, we show a useful intermediate result below relating heterogeneity of $\pi_e$ to that of $\pi_b$, which allows us to present our lower bound directly in terms of ${\sigma}_b$ of the base policy (instead of $\sigma_e$).
\begin{lemma}[Lower bound on expert heterogeneity]
\label{lem:expert-variance-lower-bound}
Let the  heterogeneity of base policy $\pibase$ be $\sigma^2_b$. 
For any expert $\piexp \in \Pi_\kappa$, its heterogeneity $\sigma^2_e$ satisfies $ |\sigma^2_{{e}} - \sigma^2_b| \le H\sigma_b \sqrt{\kappa/2}$.

\end{lemma}


{
\begin{tcolorbox}[colback=red!6!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{theorem}[\textbf{Information-theoretic lower bound on verifier-free algorithms}]
\label{thm:verifier-free-thm} Given any $\rho, r, \pibase$, expert policy $\pi_e$ and $k \le |\mathcal{X}|/4$ ($\mathcal{X}$ = space pf prompts), there exists a family of alternate expert policies $\Pi^\prime$ of size $2^k$ and reward class $\mathcal{R}^\prime \subseteq \mathcal{R}$, s.t., for any $\hat{\pi}_n^\mathrm{vf}$ returned by any verifier-free algorithm: 
{
\setlength{\belowdisplayskip}{4pt}
\setlength{\abovedisplayskip}{4pt}
    \begin{align*}
        \max_{\pi^\prime \in \Pi^\prime} \max_{r^\prime \in \mathcal{R}^\prime}  J_{r} (\pi^\prime) - J_{r^\prime} (\hat{\pi}_n^\mathrm{vf}) = \Omega\paren{\widetilde{\sigma}_e \sqrt{\frac{\log |\Pi^\prime|}{n}}}.
    \end{align*}
In addition, $\forall \pi^\prime \in \Pi^\prime$, $\sigma^2_{\pi^\prime} = O(\sigma_e^2)$ under any alternate reward function $r^\prime \in \gR^\prime$, and $\Pi^\prime \subseteq \Pi_{\Theta(\kappa)}$. Finally, when the total hetetogeneity $\sigma_e^2 \le (6/5) {\bar{\sigma}_e}^2$ (the mean heterogeneity), then the median heterogeneity across problems $\widetilde{\sigma}_e$ scales as $\Omega(\sigma_e)$, \textit{i.e.}, we can replace $\tilde{\sigma}_e$ in the bound above with $\sigma_e$.
}
\end{theorem}
\end{tcolorbox}
}


\textbf{Reading this statement.} This bound means that a dataset of $n$ datapoints from a fixed expert $\pi_e$ is fundamentally insufficient to resolve uncertainty as to which of the experts $\in \Pi'$ could have been the correct expert generating the data. Using a verifier-free algorithm here incurs a suboptimality that depends on $\nicefrac{\widetilde{\sigma}_e}{\sqrt{n}}$ for the worst choice of this expert in $\Pi'$, meaning that for any given reward function, base policy and expert, there is a problem instance $\Pi'$ and an alternate reward $r'$ where this guarantee is tight. Further, when the total heterogeneity $\sigma^2_e \approx \bar{\sigma}_e^2$, then we can replace the median with the expert's heterogeneity $\sigma_e$, which is in turn close to the base LLM's heterogeneity $\sigma_b$, as stated by our Lemma~\ref{lem:expert-variance-lower-bound}. 

To prove this result, {we extend the lower bound result from \citet{foster2024behavior}, which applies to only one prompt, to an instance-dependent lower bound that applies to a setting with more than one prompt and bi-level rewards. See Appendix~\ref{subsec:proof-verifier-free-thm} for a formal statement and a proof.} This result implies that it is challenging to clone highly heterogeneous experts: when $\widetilde{\sigma}_b$ scales as $\Omega(H)$, the bound grows as $\Omega(H/\sqrt{n})$. A linear dependence on horizon is unavoidable, even though the transition dynamics in this problem are trivial (i.e., just concatenation) and the transitions are known. The one scenario where this bound can be reasonable is when $\widetilde{\sigma}_b$ is small, but this is rarely the case in practice because pre-trained LLMs tend to be quite heterogeneous. At the very minimum, due to pathologies from training on narrow data, practitioners prefer using more heterogeneous base models and experts. 










\vspace{-0.3cm}
\subsection{A Simple Verifier-Based Algorithm}
\label{subsec:verifier-based}
\vspace{-0.1cm}
So far, we saw that heterogeneity can hurt the performance of any VF algorithm that uses expert data without reward annotations. Next, we show that this limitation does not exist for VB methods, by constructing a simple algorithm that trains a verifier using $n$ reward annotations on data sampled from the base policy $\pibase$ (which need not be an expert). Concretely, our algorithm first trains a verifier to predict sparse $0/1$ correctness of a given solution trace using the provided data, to the best possible accuracy. Then, it finetunes the LLM to maximize the verifier scores on the distribution of problems $\rho$. Note, that at test-time we sample problems \textit{i.i.d.} from $\rho$. Crucially, the algorithm does not assume access to ground-truth rewards over the entire distribution $\rho$, but only a small training dataset $\mathcal{D}_\mathrm{tr}$.
We present this approach formally in  Algorithm~\ref{alg:algorithm_simple_vb}. In particular,
Step 2 produces a class of verifiers $\hat{\gR}_\gamma$ that are $\gamma$-optimal as measured by squared loss. Step 3 produces a policy that performs optimally on the worst reward in $\hat{\gR}_\gamma$. This technique of optimizing a pessimistic reward is common in both theory and practice of offline RL~\citep{wang2024model}, and has also been useful for preventing reward overoptimization~\citep{coste2024reward}.


\textbf{Connecting Algorithm~\ref{alg:algorithm_simple_vb} to popular verification-based methods in practice.} In practice, post-training algorithms that use some sort of verification, either train a policy on ground-truth 0/1 rewards that are known on a fixed set of training problems, \textit{e.g.}, in R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, DeepScaleR~\cite{deepscaler2025}; or train outcome/process verifiers to predict ground-truth rewards on the same training problems, and use the verifiers at test time to run search over responses sampled from the base policy given test problems, \textit{e.g.}, best-of-N~\cite{cobbe2021gsm8k}, beam search~\cite{beeching2024scalingtesttimecompute,snell2024scaling,setlur2024rewarding}. The former trains a policy that generalizes onto test problems, and the latter trains a verifier with the expectation that the verifier's predictions are accurate on the test problems. Algorithm~\ref{alg:algorithm_simple_vb} that we describe falls in the latter category of verification-based approaches.

\begin{algorithm}[ht]
\caption{Simple Verifier-Based Algorithm}
\label{alg:algorithm_simple_vb}
\begin{algorithmic}[1]
\REQUIRE Base policy $\pibase$, dataset $\gD_\mathrm{tr} \eqdef\{(\bx_i, \tau_i)\}_{i=1}^n$ of problems $\bx_i \sim \rho$ and traces $\tau_i \sim \pibase(\cdot \mid \bx)$.
\STATE For every $\tau_i$  annotate $(\bx_i, \tau_i)$ with bi-level reward $y_i$. %
\STATE Learn set of classifiers $\hat{R}_\gamma \subset \gR$ that are $\gamma$-optimal, \textit{i.e.}, 
{
\begin{align*}
    \vspace{-0.1cm}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    \hat{R}_\gamma \eqdef \cbrck{r' \in \gR \middle| \frac{1}{n}\sum\nolimits_{i=1}^n (r'(\tau_i) - r(\tau_i))^2 \leq \gamma}
\end{align*}
}\STATE Return any optimal pessimistic verifier-based policy,
{
\begin{align*}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    \hat{\pi}^\mathrm{vb}_n \in \argmax_{\pi \in \Pi} \min_{r \in \hat{R}_\gamma} J_r(\hat{\pi}).
\end{align*}}
\vspace{-0.15cm}
\end{algorithmic}
\end{algorithm}
Next, we show that this VB algorithm attains a lower suboptimality gap than the lower bound for VF. To do so, we first prove an intermediate Lemma~\ref{prp:verifier-accuracy}, which upper bounds the accuracy of the verifier trained on $\gD_\mathrm{tr}$ in Algorithm~\ref{alg:algorithm_simple_vb}. 
\begin{proposition}[Verifier accuracy]
    \label{prp:verifier-accuracy}
    For any bi-level reward $r$, base policy $\pibase$, 
    and learned reward function $\hat r \in \hat{R}_\gamma$ from Algorithm~\ref{alg:algorithm_simple_vb}, with probability $1-\delta$, the following error bound is true:  
    {
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{0pt}
    \begin{align*}
        \E_{\rho,\pibase} \left[|r(\tau) - \hat{r}(\tau)|\right] \le  \widetilde{\gO}_H\paren{\nicefrac{H \cdot \log
        \paren{\frac{|\gR|}{\delta}}}{n}}. 
    \end{align*}
    }
\end{proposition}
Equipped with this result, we can now bound the suboptimality of the learned policy $\hat{\pi}_{n}^\mathrm{vb}$ in Algorithm~\ref{alg:algorithm_simple_vb}. We show that when used with a specific subset of heterogeneous $\pibase$--which are empirically show are representative of real pre-trained LLMs--this VB algorithm attains a stronger suboptimality guarantee of $H/n$, when compared to the best policy $\bar{\pi}_\kappa$ belonging to the $\chi^2$-ball, $\Pi_\kappa$, around the base policy. Intuitively, this subset of heterogeneous policies are characterized by a condition pertaining to how concentrated or ``sharp'' is the distribution of rewards induced by sampling traces from $\pibase$ on a given prompt. We call this the \textbf{anti-concentation} condition. As long as the reward distribution puts a constant probability mass on reward values that are $\approx \sigma_{\bx} \sqrt{\kappa}$ higher than the mean reward $\pibase$ gets on prompt $\bx$, we say that the policy is \emph{anti-concentrated} (Property~\ref{prp::anti-conc}; an illustration of this condition is shown in Figure~\ref{fig:dist-sketch}). 
\vspace{-0.1cm}
{
\begin{tcolorbox}[colback=green!5!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{property}[\textbf{\emph{Anti-concentrated $\pibase$}}]
\label{prp::anti-conc}
For problem $\bx$, horizon $H$, and base policy $\pibase$, let $c_\bx(\varepsilon)$ be the probability mass that reward $r(\tau)$ is larger than the mean $E_{\tau \sim \pibase(\cdot|\bx)} \left[r(\tau) \right]$ by a margin of  $\sigma_{b, \bx}\sqrt{\varepsilon}$. 
{
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\abovedisplayskip}{5pt}
    \begin{align*}
    \!\!\!c_\bx (\varepsilon) \eqdef \text{Pr}_{\tau \sim \pibase(\cdot|\bx)} \Big[ r(\tau) \ge \E_{\tau \sim \pibase(\cdot|\bx)}\brck{r(\tau)} + \sigma_{b, \bx} \sqrt{\varepsilon} \Big],
    \end{align*}
}Then base LLM $\pibase$ is said to be $c$-anticoncentrated if $\min_{\bx} c_\bx(\kappa_\bx) \geq c$, where $\kappa_\bx \eqdef \chisq{\bar{\pi}_\kappa (\cdot|\bx)}{\pibase(\cdot | \bx)}$ 
and $\bar{\pi}_\kappa$ denotes the best policy in $\Pi_\kappa$, i.e., the one with the highest performance $J_r(\cdot)$. The value of $\kappa_\bx$ depends on how much an expert is allowed to deviate from the base policy $\pibase$ on problem $\mathbf{x}$.  
\end{property}
\end{tcolorbox}
}

Even under high heterogeneity (Property~\ref{def:exp-heterogeneity}), an anti-concentrated $\pibase$ covers--with a constant mass--a policy that improves over its own mean. This means that an algorithm using the reward signal to fine-tune $\pibase$ should be able to discover this ``better'' policy. VF algorithms that do not utilize to the reward signal fail at finding this high-rewarding policy.
While a non-heterogeneous base policy (for e.g., one that always samples a single trace for a given $\bx$) will not satisfy Property~\ref{prp::anti-conc}, hetergeneous distributions can easily be anti-concentrated since heterogeneity is a property of a moment (i.e., variance) of the reward distribution whereas Property~\ref{prp::anti-conc} fundamentally relates to the shape or the CDF of the reward distribution. 
We demonstrate in our experiments that pre-trained LLMs often satisfy this property.

\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.6cm}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{figures/anticonc-coverage.pdf}
  \end{center}
  \vspace{-0.5cm}
  \caption{\footnotesize \textbf{\emph{Anti-concentration:}} Heterogeneous distributions admit coverage over high reward policies (dashed orange line) when anti-concentrated.\vspace{-0.3cm}}
    \label{fig:anti-conc-coverage}
\end{wrapfigure}
\textbf{\emph{How can VB algorithms benefit from anti-concentration of $\pibase?$}} As discussed above, Property~\ref{prp::anti-conc} ensures the existence of a good policy that is covered by the base policy, with high probability. Intuitively, running RL should be able to then sample traces that attain high rewards and learn to pick up on this reward with more training.
From a theoretical perspective, note that the suboptimality gap of any VB method depends on the distribution shift between the data-generating policy ($\pibase$ in our case) and the comparator policy that we wish to provide the guarantee against ($\bar{\pi}_\kappa$), since this shift dictates how generalization error during training gets amplified when the model is deployed.
This notion of distribution shift is typically formalized as a bounded \emph{coverage coefficient}~\citep{rashidinejad2021bridging} of an unknown comparator policy, which is restrictive. We strengthen the notion of this coverage coefficient in our analysis by leveraging anti-concentration, which allows us to optimally construct a high-reward comparator policy that is covered by the base policy (illustrated in Figure~\ref{fig:anti-conc-coverage}).  Formally, this results in Theorem~\ref{thm:verifier-based-thm} (full proof is provided in Appendix~\ref{subsec:proof-verifier-based-thm}).

{
\begin{tcolorbox}[colback=red!6!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{theorem}[Suboptimality upper bound for VB against any expert]
\label{thm:verifier-based-thm}
Consider a bi-level
reward $r$,  base policy $\pibase$ that is $c_0$-anticoncentrated  at some horizon $h_0 \leq H$. Then, w.p. $1-\delta$, for the policy $\hat{\pi}_n^{\mathrm{vb}}$ returned by Algorithm~\ref{alg:algorithm_simple_vb}, 
the suboptimality gap w.r.t. the best expert:  $\bar{\pi}_\kappa$:
{   \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \begin{align*}   
      &J_r(\bar{\pi}_\kappa) - J_r(\hat{\pi}_{n}^\mathrm{vb}) \; \lsim \;  \frac{1}{c_0} \cdot \frac{H\log(\nicefrac{|\gR|}{\delta})}{n}.
    \end{align*}
}  
\end{theorem}
\end{tcolorbox}
}
\vspace{-0.1cm}
Note that our simple VB method admits no direct dependency in $\sigma_b$ (base policy's heterogeneity), which scales as $\Omega(H)$ in the worst case. This implies that as long as $\pibase$ satisfies Property~\ref{prp::anti-conc} for some $h_0 \ll H$,  VB methods only incur suboptimality that scales as $O(1)$ when $n=\Omega(H)$ whereas for any VF method this is $\Omega(\sqrt{H})$. Mathematically, this is because once Property~\ref{prp::anti-conc} is satisfied for some $c_0$ at a given horizon $h_0$, then it continues to hold for $c_0$ and $\forall~ H > h_0$. This is a consequence of the structure of the bi-level reward as we show in Lemma~\ref{lemma:pi_lambda_tilde} in Appendix~\ref{subsec:proof-verifier-based-thm}. 

\textbf{Overall}, intuitively Theorem~\ref{thm:verifier-based-thm} implies that if $\pibase$ covers some correct solution traces for a given prompt, then VB methods can find these traces and minimize suboptimality, whereas VF methods may not be able to discover them and will spend unnecessary compute in trying to mimic multiple traces, which also naturally increases the chances of failing at the problem. Combining the upper and lower bounds (Theorem~\ref{thm:verifier-based-thm} and \ref{thm:verifier-free-thm}) allows us to bound the efficacy of test-time scaling with VB and VF methods.




\begin{tcolorbox}[colback=red!6!white,colframe=black,boxsep=0pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
\begin{theorem}[\textbf{\emph{Separation between test-time scaling of VB and VF methods}}]
\label{thm:vg-gap-lower-bound} For any heterogeneous $\pibase$ with $\tilde{\sigma}_b = \Omega(H)$, and is $c_0$-anticoncentrated for horizon $h_0 \ll H$, the policy $\hat{\pi}_n^{\mathrm{vb}}$ returned by the simple verifier-based Algorithm~\ref{alg:algorithm_simple_vb} and $\hat{\pi}_n^{\mathrm{vf}}$ returned by any verifier-free method satisfy:
{
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align*}
    J_r(\hat{\pi}_{n}^\mathrm{vb}) -  J_r(\hat{\pi}_{n}^\mathrm{vf})  = \Tilde{\Omega}\paren{\nicefrac{H}{\sqrt{n}}},
\end{align*}}
which implies our test-time scaling result in Theorem~\ref{thm:main-theorem}.
\end{theorem}
\end{tcolorbox}








\begin{AIbox}{Takeaways: Verification enables test-time scaling}
\begin{itemize}[leftmargin=0em]
    \setlength\itemsep{0em}
    \item VF algorithms suffer when the base policy (and consequently any expert \emph{realized} around the base policy) is highly heterogeneous.   
    \item VB algorithms outperform \emph{any} VF algorithm given that the base policy is heterogeneous and the induced reward distribution is anti-concentrated.
\end{itemize}
\end{AIbox}

\vspace{0.1cm}

\begin{remark}[\emph{\textbf{Comparison with old results in RL}}]
While we are not aware of any theoretical analysis that proves RL-style finetuning (with verification) to be better than SFT-style finetuning for LLMs, prior work~\citep{kumar2022should} outside of LLMs comparing offline RL (VB algorithm) and imitation learning (VF algorithm) does compare RL (verifier-based) and imitation learning (verifier-free) under certain structural conditions on the problem. This work considers an RL algorithm that trains value or $Q$ functions (which estimates expected reward-to-go) instead of a reward function or verifier and shows that when most states are ``non-critical'', meaning that a high volume of actions at such states attain large enough $Q$ values, then RL outperforms imitation. This condition somewhat resembles our anti-concentration condition that requires at least some constant coverage over good traces (actions).  That said, our work formalizes this condition in the context of LLM finetuning and shows a much stronger result: VB methods dominate \emph{all} VF methods, when our conditions are satisfied. This prior work only compares upper bounds.
\end{remark}

\vspace{0.075cm}
\begin{remark}[\emph{\textbf{VB methods improve over VF by solving more problems}}] 
One might wonder how our results using the bi-level reward translate to direct problem-solving accuracy since the bi-level reward increases not only when a problem is eventually solved but also when it is solved with fewer tokens. This raises a natural question: do VB methods perform better only because they can solve existing questions more efficiently than VF methods or by actually \emph{discovering} solutions to new questions? To answer this conceptually, we first note that the performance of the best expert policy $\bar{\pi}_\kappa$ which belongs the the $\kappa$ $\chi^2$-ball around $\pibase$ will only continue to improve as a function of $H$ by solving more questions (\textit{i.e.}, by finding new $\bx$ where $r$ flips from 0 to 1). To see why, note that $\mathrm{D}_{\chi^2}$ grows  in $H$ meaning that at large $H$, the expert $\bar{\pi}_\kappa$ cannot be simultaneously close to the base policy and maximize reward  unless it solves new questions too. Now note that if $n = \Omega(H)$ samples are used for training, then the VB algorithm attains a suboptimality of $\gO(1)$ compared to this best expert, but the VF algorithm still suffers from a horizon-dependent suboptimality (if $\pibase$ is heterogeneous). This means that if the suboptimality gap with respect to the best expert continues to be $\gO(1)$ as we increase the token budget, then we are solving harder problems, not just sampling more token efficient solutions to easier problems. 
\end{remark}



