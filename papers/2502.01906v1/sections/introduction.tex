\section{Introduction}\label{sec:intro}

Large Vision-and-Language Models (LVLMs) \citep{llava} have become pivotal in advancing artificial intelligence, enabling models to understand multimodal content by integrating visual and textual information.
These models have shown significant advancements in various applications, such as image captioning, visual question answering, and multi-modal assistant, marking a substantial leap forward in cross-modal reasoning.
By leveraging the strengths of pre-trained large language models (LLMs) like LLaMA \citep{touvron2023llama,zheng2023judging} and Mistral \citep{jiang2023mistral}, and powerful visual encoders such as CLIP \citep{clip}, LVLMs are pushing the boundaries of cross-modal understanding, making AI more capable of interpreting and reasoning about complex, real-world scenarios. 

In most state-of-the-art LVLMs \citep{llava,llava_onevision, tong2024cambrian}, visual inputs are processed within an LLM in the same manner as textual inputs.
Specifically, visual inputs are first encoded by a pre-trained visual encoder, such as CLIP, into a sequence of visual embeddings.
These embeddings are then passed through a lightweight adapter layer and concatenated with textual embeddings derived from the text prompts.
The concatenated visual and textual embeddings are treated as homogeneous input embeddings and subsequently fed into a pre-trained LLM.
In this approach, visual and textual embeddings are treated and processed uniformly.

In this paper, we rethink the homogeneity of visual and textual tokens in LVLMs and challenge this conventional paradigm:

\textit{``Visual and textual inputs are \textbf{created different}, and thus we propose to \textbf{process them differently} within a large vision-and-language model.''}


% In most current LVLMs, visual inputs are treated and processed in an LLM same way as text inputs.
% Concretely, visual inputs are first encoded by a pre-trained visual encoder, such as CLIP, into a sequence of visual embeddings.
% These visual embeddings are projected by a lightweight adapter layer and then concatenated with textual embeddings from text prompts as homogeneous input embeddings, which are then fed into a pre-trained LLM.
% In this paradigm, visual embeddings and textual embeddings are treated and processed homogeneously.
% However, we argue that:

% \textit{``Visual and textual inputs are \textbf{created different}, and thus should be \textbf{processed differently} within a large vision-and-language model.''}


% It is evident that the creation of visual and textual embeddings follows distinct processes.
It is evident that visual and textual embeddings are created different.
Visual embeddings are derived by passing one or more two-dimensional images through a visual encoder, while textual embeddings are generated through a lookup of learnable parameters from a one-dimensional sequence of text token IDs.
These distinctions introduce significant differences in the information encoded within each type of embedding, which necessitates different modeling and processing strategies within the LLM.
Key distinctions include:

\begin{itemize}[wide,labelindent=0pt]
\item Visual embeddings inherently encode contextual information from all other visual embeddings, whereas textual embeddings lack such intrinsic contextual awareness of other textual tokens.
\item Visual inputs are intrinsically multi-dimensional (\textit{e.g.} images are 2-D). Concatenating visual and textual embeddings into a 1-D sequence and processing them in a causal, language-centric manner can introduce undesirable modeling biases.
\end{itemize}


% It is self-evident that visual and textual embeddings are created different.
% Visual embeddings are created by passing one or several 2-D image(s) through a visual encoder such as CLIP, while textual embeddings are created by table lookup of learnable parameters from a 1-D sequence of text token IDs.
% This makes a significant difference on the information encoded in each type of embeddings and on how they should be modeled and processed within the LLM:

% \begin{itemize}[wide,labelindent=0pt]
% \item \label{enum:ctx} Each visual embedding has \emph{seen} all other visual embeddings (assume we use CLIP-style ViT visual encoder), while each textual embedding has \emph{no} such contextual information of other textual embeddings.
% \item \label{enum:pos} Visual inputs are intrinsically \emph{multi-dimensional}. Concatenating visual and textual emebddings into a 1-D sequence and process the sequence in a 1-D causal linguistic way may introduce undesirable modeling bias.
% \end{itemize}

To address these challenges, we introduce Decomposed Attention, or \method{}, a novel framework designed to handle visual inputs more efficiently and effectively in LVLMs.
In Section~\ref{sec:background}, we first demonstrate that the causal self-attention mechanism \citep{vaswani2017attention} in an LVLM can be decomposed into three components: (1) visual-to-visual self-attention (V2V Self-Attn), (2) textual-to-visual cross-attention (T2V Cross-Attn), and (3) textual-to-textual self-attention (T2T Self-Attn), as illustrated in Figure~\ref{fig:attn}.
By leveraging this decomposition, we concentrate on the vision-related components, specifically the V2V Self-Attn and T2V Cross-Attn, while addressing how to effectively merge the T2V and T2T attentions.

% To address these issues, we propose Decomposed Attention, \method{}, in this paper.
% We first show in Sec.~\ref{sec:background} that causal self-attention in an LVLM can be decomposed into three parts: (1) visual-to-visual self-attention (V2V Self-Attn), (2) textual-to-visual cross attention (T2V Cross-Attn), and (3) textual-to-textual self-attention (T2T Self-Attn), as shown in Fig.~\ref{fig:attn}.
% With this decomposition, we focus on the vision-related parts in an LVLM, including V2V Self-Attn, T2V Cross-Attn, and how to merge T2V and T2T attentions.

In Section~\ref{sec:v2v-attn}, we argue that since each visual embedding inherently encodes contextual information about other visual embeddings, it is redundant to relearn this information within the LVLM.
Therefore, we propose diagonalizing the V2V Self-Attn, significantly reducing the computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings without sacrificing performance.
This optimization is particularly advantageous when processing high-resolution images or long video sequences.

% In Sec.~\ref{sec:v2v-attn}, we point out that in V2V Self-Attn, since each visual embedding has already encoded the contextual information of other visual embeddings, there is no need to relearn this within an LVLM.
% Therefore, we propose to diagonalize the V2V Self-Attn to substantially reduce the computational complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$ for $N$ visual embeddgins without performance degradation.
% This is particularly valuable for high-resolution image or long video inputs.

In Section~\ref{sec:t2v-attn}, we identify an undesirable positional bias that arises from concatenating visual and textual embeddings into a 1-D sequence.
To address this issue, we propose to debias T2V Cross-Attn by removing rotary/relative positional encodings within T2V Cross-Attn.
Notably, this modification, though seemingly straightforward, is difficult to implement in conventional LVLMs without our proposed attention decomposition framework.

% Next, in Sec.~\ref{sec:t2v-attn}, we observe undesirable positional bias originated from the concatenation of visual and textual embeddings into an 1-D sequence.
% To eliminate this bias, we propose to remove the rotary/relative positional encodings within T2V Cross-Attn.
% It's worth noting that this seemingly simple change cannot be easily implemented in conventional LVLM without our proposed attention decomposition.

Finally, in Section~\ref{sec:alpha}, we derive an $\alpha$-weighting strategy to merge the visual information from T2V Cross-Attn with the textual information from T2T Self-Attn.
The $\alpha$-weighting approach is analytically equivalent to the inherent attention operations within LVLMs, introducing minimal architectural changes and requiring no additional learnable parameters.
This ensures that the pre-trained LLM retains its full capability for competitive downstream performance.

% Lastly, in Sec.~\ref{sec:alpha}, we analytically derive an $\alpha$ weighting strategy to merge the T2V visual and T2T textual information.
% $\alpha$ weighting is equivalent to the inherent attention operations within an LVLM.
% It introduces minimal architectural and operational changes, and no additional learnable parameter, thereby maximally retaining a pre-trained LLM's capability for competitive downstream performance.

In summary, our proposed \method{} not only reduces computational complexity but also outperforms its self-attention counterparts by a significant margin.
Under fair comparisons, \method{} is able to process 8x more visual embeddings or train 5x faster, consistently outperforming its self-attention counterpart across a range of image benchmarks.
We conduct rigorous ablation studies to validate the effectiveness of our V2V Diagonal-Attn, debiased positional encodings, and $\alpha$-weighting strategies.
Furthermore, we develop \method{} using open-source models and train it on publicly available datasets to ensure reproducibility.
Code, data, and models will be made publicly available.

% With the proposed enhancements, \method{} has lower computational complexity and performs substantially better than its self-attention counterparts.
% Under fair comparison, \method{} can consume 8x more visual embeddings or train 5x faster, and consistently outperform its self-attention counterpart on a series of image benchmarks.
% We also conduct rigorous ablations to validate the effectiveness of our proposed V2V Diagonal Attn, debiased positional encodings, and $\alpha$-weighting strategy.
% We construct \method{} with open-sourced models and train \method{} with public datasets to facilitate reproducibility.
% Code, data, and models will be released.
