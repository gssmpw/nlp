\section{Experiments}

\subsection{Implementation Details}

% In this section, we reveal the implementation details of our \method{} model.
% \method{} only uses publicly available data and pre-trained models for fair comparison and reproducibility.
% Code, data, and models will be released.

\textbf{Model:} Our proposed \method{} model is built based on the architecture of LLaVA~\citep{llava}.
It is constructed using three primary components: a pre-trained SigLip~\citep{zhai2023sigmoid} visual encoder, a randomly initialized two-layer MLP adapter with RMSNorm~\citep{zhang2019root}, and a pre-trained LLM.
We modify only the decoder layer and self-attention mechanisms within the LLM to implement our \method{}.
In this paper, we experiment with two different LLM families: Mistral v0.3 7B~\citep{jiang2023mistral}, and Gemma 2 9B~\citep{team2024gemma}.

% \textbf{Model}: We build our \method{} model similar to the open source model LLaVA \citep{llava}.
% \method{} is composed of a pre-trained SigLip \citep{zhai2023sigmoid}, a randomly initialized two-layers MLP projector with RMSNorm~\citep{zhang2019root}, and a pre-trained LLM.
% We only modify the decoder layer and the self-attention module within the LLM to implement our \method{}.
% Since our model does not introduce new parameters in an LLM, it can be safely initialized from a pre-trained LLM weights.
% In this paper, we experiment with different LLM families including Mistral v0.3 7B \citep{jiang2023mistral}, and Gemma 2 9B \citep{team2024gemma}.

\begin{wrapfigure}{r}{0.4\linewidth}
  \vspace{-24pt}
  \begin{center}
    \includegraphics[width=1\linewidth]{figures/radar.pdf}
  \end{center}
  \vspace{-5pt}
  \caption{Performance comparison between proposed \method{} models and their self-attention (S-Attn) counterparts on a range of popular image benchmarks. Detailed results are available in Table~\ref{tab:main}.}\label{fig:radar}
  \vspace{-18pt}
\end{wrapfigure}

\textbf{Training:} The training of \method{} follows a three-stage strategy outlined in ShareGPT4V~\citep{chen2023sharegpt4v}.
In the first stage, the MLP adapter is pre-trained on LLaVA's LAION/CC/SBU\citep{llava,schuhmann2022laion,sharma2018conceptual,ordonez2011im2text} 58k for modality alignment.
In the second stage, the entire model is fine-tuned using 1.25M dense captions from the ShareGPT4V-PT dataset~\citep{chen2023sharegpt4v}.
In the third and final stage, we perform instruction tuning using a combined dataset of 665k examples from LLaVA-1.5~\citep{llava} and 102k dense captions from ShareGPT4V~\citep{chen2023sharegpt4v}.
The entire training procedure completes in under 24 hours on 32 H100 GPUs.
Detailed hyperparameters are provided in the Appendix.

% \textbf{Training}:
% We follow the three-stage training strategy in ShareGPT4V \citep{chen2023sharegpt4v}.
% In the first stage, only the MLP projector is pre-trained on LLaVA's LAION/CC/SBU \citep{llava, schuhmann2022laion, sharma2018conceptual, ordonez2011im2text} 558k for modality alignment.
% In the second stage, the whole model is trained on the 1.25M ShareGPT4V-PT \citep{chen2023sharegpt4v} dense captions.
% In the third stage, the whole model is trained on the union of LLaVA-1.5 665K instruction tuning data and ShareGPT4V 102k dense captions.
% The training of these three stages can be done within one day on 32 H-100 GPUs. The model is implemented in PyTorch \citep{paszke2019pytorch} and optimized with AdamW \citep{loshchilov2017decoupled}.
% Please refer to Appendix for detailed list of hyperparameters.

\textbf{Evaluation:} Following LLaVA's evaluation protocol, we evaluate \method{} on ten image benchmarks, including VQA-v2~\citep{goyal2017making}, GQA~\citep{hudson2019gqa}, SQA-I~\citep{lu2022learn}, VQA-T~\citep{mao2016generation}, MME~\citep{fu2024mmecomprehensiveevaluationbenchmark}, MMB~\citep{liu2023mmbench}, SEED-I~\citep{li2023seed}, LLaVA-W~\citep{llava}, MMVP~\citep{tong2024eyes}, and MMStar~\citep{chen2024we}.

% \textbf{Evaluation}:
% Following LLaVA's evaluation protocol, we evaluate \method{} on ten image benchmarks, including VQA-v2 \citep{goyal2017making}, GQA \citep{hudson2019gqa}, SQA-I \citep{lu2022learn}, VQA-T \citep{mao2016generation}, MME \citep{fu2024mmecomprehensiveevaluationbenchmark}, MMB \citep{liu2023mmbench}, SEED-I \citep{li2023seed}, LLaVA-W \citep{llava}, MMVP \citep{tong2024eyes}, MMStar \citep{chen2024we}.

Our primary objective is not to achieve state-of-the-art performance but to rigorously validate the effectiveness of our proposed \method{} framework.
To ensure fair comparisons and facilitate reproducibility, we train \method{} using only publicly available datasets through supervised fine-tuning and construct the model with open-source pre-trained LLMs and visual encoders.
For stronger performance, researchers may scale up training data and models or apply more advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF)\citep{bai2022training} or Direct Preference Optimization (DPO)\citep{rafailov2024direct}, which we leave as future work.

% In this paper, we do not target SoTA performance.
% Instead, we conduct rigorous study to validate the effectiveness of our proposed \method{} framework.
% To ensure fair comparison and facilitate reproducibility, we train \method{} with publicly available data via supervised fine-tuning, and construct \method{} with open-sourced pre-trained LLMs and visual encoders.
% For stronger performance, researchers may scale up training data and models, or apply more advanced training techniques such as RLHF~\citep{bai2022training} or DPO~\citep{rafailov2024direct}, which are left as future directions of improvement.


\input{tables/image}
\subsection{Main Results}

As illustrated in Figure~\ref{fig:radar}, our \method{} models consistently outperform their self-attention (S-Attn) counterparts across a range of image benchmarks.
We conduct experiments using Gemma 2 9B and Mistral v0.3 7B LLMs.
To ensure a fair comparison, both \method{} and S-Attn models are trained on the same datasets using identical training strategies and are constructed with the same pre-trained visual encoders and LLMs.
This experiment validates the effectiveness of the proposed \method{} framework.
In Section~\ref{sec:ablation} and Table~\ref{tab:ablation-arch}, we further demonstrate that \method{} offers significant computational advantages over its S-Attn counterpart.
Specifically, by employing the V2V Diagonal-Attention mechanism, we reduce the computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings.

% We first show in Figure~\ref{fig:radar} that our \method{} models achieve consistently better performance than their self-attention (S-Attn) counterparts on a range of image benchmarks.
% We experiment with Gemma 2 9B and Mistral v0.3 7B LLMs.
% To ensure fair comparison, both \method{} and S-Attn models are trained on the same amount of data with the same training strategy, and constructed with the same pre-trained visual encoders and LLMs.
% This experiment validates the effectiveness of proposed \method{} framework.
% We will later show in Section~\ref{sec:ablation} and Table~\ref{tab:ablation-arch} that \method{} has significant computational advantage over its S-Attn counterpart thanks to the reduction of computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings via V2V Diagonal-Attn.

Table~\ref{tab:main} presents the results of our \method{} models and their S-Attn counterparts alongside other state-of-the-art LVLMs on ten popular image benchmarks.
For reference, we include models such as Instruction BLIP~\citep{dai2023instructblip}, BLIP3~\citep{blip3}, VILA~\citep{lin2024vila}, IDEFICS~\citep{laurenccon2024obelics}, Mini-Gemini~\citep{li2024mini}, Cambrian~\citep{tong2024cambrian}, Qwen-VL / Qwen2-VL~\citep{wang2024qwen2}, Intern-XC / Intern-XC 2.5~\citep{zhang2024internlm}, CuMo~\citep{li2024cumo}, and LLaVA-1.5 / LLaVA-1.6~\citep{llava, llava_next}.
When compared with other SoTA models, our \method{} models achieve competitive performance, despite being trained on much fewer and publicly available data only, and using a simple supervised fine-tuning training strategy.

% We then present the results of our \method{} models and their S-Attn counterparts alongside other SoTA LVLMs on ten popular image benchmarks in Table~\ref{tab:main}.
% For reference, we include Instruction BLIP \citep{dai2023instructblip}, BLIP3 \citep{blip3}, VILA \citep{lin2024vila}, IDEFICS \citep{laurenccon2024obelics}, Mini-Gemini \citep{li2024mini}, Cambrian \citep{tong2024cambrian}, Qwen-VL / Qwen2-VL \citep{wang2024qwen2}, Intern-XC / Intern-XC 2.5 \citep{zhang2024internlm}, CuMo \citep{li2024cumo}, and LLaVA-1.5 / LLaVA-1.6 \citep{llava, llava_next}.
% When compared with other SoTA models, our \method{} achieves competitive performance, despite trained on much fewer and publicly available data only and with the simple supervised fine-tuning training strategy.

Lastly, we present qualitative comparisons between our \method{} model and its S-Attn counterpart in Figure~\ref{fig:qualitative}.
We observe that the \method{} model provides answers that are more faithful to the input image and offers more visual details compared to the S-Attn model.
For example, in the first figure illustrating snowboarding and skiing, \method{} effectively distinguishes between the two activities, accurately identifying one person as skiing and the other as snowboarding.
While in the fourth Diamond Head figure, \method{} provides more details about the scene such as ``encircled by a road that winds its way around the base'', and ``Beyond the crater, the city of Honolulu sprawls out''.

% Lastly, we show some qualitative comparisons between our \method{} model and its S-Attn counterpart in Figure~\ref{fig:qualitative}.
% We can see in the figure that \method{} model answers the question more faithful to the input image, and is able to provide more visual details compared to the S-Attn model.
% For example, in the snowboarding and skiing figure, \method{} successfully differentiates that one person is skiing and the other is snowboarding.
% In the fruits figure, \method{} can more accurately identify whether a certain kind of fruit is present in the image or not.

\subsection{Ablations and Analyses}\label{sec:ablation}

% \begin{figure}[b]
% \centering
% \includegraphics[width=1\linewidth]{figures/umap.pdf}
% \caption{Feature distributions of visual and text embeddings at different layers in LLaVA-1.6.}\label{fig:umap}
% \end{figure}

% We first verify the major claim of this paper that visual and textual inputs are inherently different.
% Starting from LLaVA and followed by most subsequent LVLMs, a fundamental assumption is that visual embeddings can be aligned with textual embeddings into the same distribution by projecting the visual embeddings via an adapter layer.
% Here, we challenge this assumption and demonstrate that visual and textual embeddings are \emph{not} properly aligned in a trained LVLM, even on the training data.
% We verify this by plotting the feature distribution of visual and textual embeddings using UMAP~\citep{mcinnes2018umap}.
% We analyze the embeddings at the input, middle, and last LLM decoder layers in the LLaVA-1.6~\citep{llava_next} model, as shown in Figure~\ref{fig:umap}.
% The visualizations confirm that visual and textual embeddings occupy different regions in the feature space. Moreover, this discrepancy cannot be easily mitigated by a simple adapter layer applied to the visual embeddings.

% We first verify the major claim of this paper that visual and textual inputs are different.
% Started from LLaVA and followed by most subsequent LVLMs, a fundamental assumption is that visual embeddings can be aligned with textual embeddings into the same distribution by projecting the visual embeddings via an adapter layer.
% Here we challenge this assumption and show that visual and textual embeddings are not properly aligned for a trained LVLM even on the training data.
% We verify this by plotting the feature distribution of visual and textual embeddings via UMAP~\citep{mcinnes2018umap}.
% We use the LLaVA-1.6~\citep{llava_next} model, and examine the embeddings at the input, middle, and last LLM decoder layer in Figure~\ref{fig:umap}.
% These figures confirm our claim that visual and textual embeddings are different.
% Furthermore, this difference cannot be easily mitigated by a simple adapter layer applied on the visual embeddings.

\input{tables/ablation}

We first conduct ablation studies on the V2V Diagonal-Attn, as detailed in Table~\ref{tab:ablation-arch}.
To demonstrate the computational advantages, we measure the maximum number of visual embeddings ($|V|$) that an LVLM can process during training before encountering a GPU out-of-memory error.
We also record the training speed in seconds per iteration (sec/it) with the same $|V|$.
As shown in Table~\ref{tab:ablation-arch}, by diagonalizing the V2V Self-Attn, our model can process up to 8 times more visual embeddings or train up to 5 times faster.
While additional optimization techniques such as FlashAttention~\citep{dao2022flashattention}, DeepSpeed~\citep{rasley2020deepspeed}, or Megatron~\citep{shoeybi2019megatron} can further improve memory and speed, they are orthogonal to our V2V Diagonal-Attn and still fundamentally have a computational complexity of $\mathcal{O}(|V|^2)$ for the V2V attention.
In terms of performance, V2V Diagonal-Attn performs comparably to conventional LVLMs across various benchmarks, supporting our hypothesis that visual embeddings have already encoded contextual information, obviating the need for re-learning via the LLM's Self-Attn.

% We then conduct ablation studies on the V2V Diagonal-Attn in Table~\ref{tab:ablation-arch}.
% To demonstrate the computational advantage of V2V Diagonal-Attn, we measure the maximal number of visual embeddings $|V|$) an LVLM can consume during training before hitting a GPU out-of-memory error.
% We also measure the training speed in sec/it with the same $|V|$.
% As shown in Table~\ref{tab:ablation-arch}, by diagonalizing V2V Self-Attn, our model can consume 8x more visual embeddings or train 5x faster.
% One can further boost the memory and speed throughput by applying other optimization techniques orthogonal to our V2V Diagonal-Attn, such as flash-attention \citep{dao2022flashattention}, DeepSpeed \citep{rasley2020deepspeed}, Megatron \citep{Megatron}, etc.
% Nevertheless, the computational complexity of these efficient techniques is still fundamentally $\mathcal{O}(|V|^2)$ for the V2V attention.
% In terms of performance, V2V Diagonal-Attn performs comparably with a conventional LVLM on a series of benchmarks, validating our hypothesis that visual embeddings have already encoded contextual information and there is no need to relearn via LLM's Self-Attn.
% , ring-attention \citep{}

Next, we perform an ablation study on debiased positional encodings, also reported in Table~\ref{tab:ablation-arch}.
By debiasing the T2V Cross-Attn, our \method{} model achieves consistent performance improvements over models with biased positional encodings across multiple image benchmarks.
This modification cannot be easily implemented in conventional LVLMs but is rather straightforward with our proposed attention decomposition, and it brings no additional computational costs.

% Next, we conduct ablation study on debiased positional encodings in Table~\ref{tab:ablation-arch}.
% By debiasing T2V Cross-Attn, our \method{} achieves consistent performance improvements over the model with biased positional encodings across a range of image benchmarks.
% This removal cannot be easily implemented in conventional LVLMs, but is rather straightforward with our proposed attention decomposition, and brings no additional computational costs.

Furthermore, we experiment with different merging strategies in Table~\ref{tab:ablation-merge}, including (1) \textbf{Cascade}, where the T2V Cross-Attn module is decoupled and cascaded with T2T Self-Attn; (2) \textbf{Tanh}, where T2V Cross-Attn is weighted by a learnable tanh gate and then summed with T2T Self-Attn; (3) \textbf{Sigmoid}, where T2V Cross-Attn and T2T Self-Attn are weighted summed with learnable gates $\sigma$ and $1-\sigma$, respectively; and (4) $\bm{\alpha}$-\textbf{weighting} strategy proposed in this paper.
As shown in Table~\ref{tab:ablation-merge}, our $\alpha$-weighting strategy achieves superior performance compared to other strategies without introducing additional parameters like the cascade strategy.
Since $\alpha$-weighting introduces minimal architectural and operational changes to an LLM's self-attention module, it maximally retains the LLM's pre-trained capabilities, likely leading to superior fine-tuning performance on downstream tasks.

% Lastly, we experiment with different merging strategies in Table~\ref{tab:ablation-merge}, including (1) \textbf{Cascade}, where the T2V Cross-Attn module is decoupled and cascaded with T2T Self-Attn, (2) \textbf{Tanh}, where T2V Cross-Attn is weighted by a learnable tanh gate and then summed with T2T Self-Attn, (3) \textbf{Sigmoid}, where T2V Cross-Attn and T2T Self-Attn are weighted summed with learnable gates $\sigma$ and $1-\sigma$, respectively, and (4)
% $\bm{\alpha}$-\textbf{weighting} strategy proposed in this paper.
% We can see in Table~\ref{tab:ablation-merge} that our $\alpha$-weighting strategy achieves superior performance to other strategies without introducing substantially more parameters like the cascade strategy.
% Since $\alpha$-weighting introduces minimal architectural and operational changes to an LLM's self-attention module, it maximally retains an LLM's pre-trained capability, likely leading to superior fine-tuning performance at downstream tasks.

\input{tables/merge}

\input{tables/mme}
\input{tables/seed}
\input{tables/mmb}


% Lastly, to better understand which task benefits from our proposed D-Attn, we present the broken-down scores for SEED~\citep{li2023seed} in Table~\ref{tab:seed}, MMB~\citep{liu2023mmbench} in Table~\ref{tab:mmb}, and MME~\citep{fu2024mmecomprehensiveevaluationbenchmark} in Table~\ref{tab:mme}.
% Overall, we found that D-Attn model is particularly strong on tasks involving spatial and relational reasoning, such as (1) "position" in MME, (2) "Spatial Relation" in SEED, and (3) "object localization" and "spatial relationship" in MMB.
% D-Attn also performs well on tasks involving OCR and document understanding, such as (1) "OCR" in MME, (2) "Text Understanding" in SEED, and (3) "ocr" and "structuralized image-text understanding" in MMB.

Lastly, to gain deeper insights into the tasks that benefit most from our proposed \method{} model, we present the detailed scores for MME~\citep{fu2024mmecomprehensiveevaluationbenchmark} in Table~\ref{tab:mme}, SEED~\citep{li2023seed} in Table~\ref{tab:seed}, and MMB~\citep{liu2023mmbench} in Table~\ref{tab:mmb}.
Our analysis reveals that our \method{} model excels particularly in tasks requiring spatial and relational reasoning.
Notable examples include (1) "position" in MME, (2) "Spatial Relation" in SEED, and (3) "object localization" and "spatial relationship" in MMB.
In addition, our \method{} model demonstrates strong performance on tasks involving OCR and document understanding.
Specific examples include (1) "OCR" in MME, (2) "Text Understanding" in SEED, and (3) "ocr" and "structuralized image-text understanding" in MMB.


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/snow.pdf}
% \vspace{5pt}
\includegraphics[width=1\linewidth]{figures/fruit.pdf}
% \vspace{5pt}
\includegraphics[width=1\linewidth]{figures/ironing.pdf}
% \vspace{5pt}
\includegraphics[width=1\linewidth]{figures/diamond-head.pdf}
\caption{
Qualitative comparisons between \method{} and its Self-Attn (S-Attn) counterpart. Erroneous outputs from the S-Attn model are highlighted in red, while the accurate and preferred responses from \method{} are highlighted in blue. 
}\label{fig:qualitative}
\end{figure}
