\section{Related Works}
% List of all current LVLMs.
% LLaVA \citep{llava}, LLaVA NeXT \citep{llava_next}, LLaVA-OneVision \citep{llava_onevision}, Instruct BLIP \citep{dai2023instructblip}, BLIP3 \citep{blip3}, VILA \citep{lin2024vila}, IDEFICS \citep{idefics}, QWen2-VL \citep{wang2024qwen2}, CuMo \citep{li2024cumo}, Intern-XC-2.5 \citep{zhang2024internlm}, Flamingo \citep{flamingo}, Self-Attention \citep{vaswani2017attention}, miniGemini \citep{li2024mini}, Cambrian-1 \citep{tong2024cambrian}

% The integration of visual and textual modalities has been a central focus in artificial intelligence research, aiming to develop models that can comprehend and generate language grounded in visual content. This section reviews significant advancements in vision-language models, particularly those that have contributed to multimodal understanding and generation capabilities.

% Vision-Language Models and Multimodal Learning
% Self-Attention Mechanism. The self-attention mechanism introduced by \citet{vaswani2017attention} revolutionized natural language processing and has been fundamental in developing transformer-based architectures. This mechanism allows models to weigh the significance of different parts of the input data, facilitating the capture of long-range dependencies and contextual relationships. In multimodal tasks, self-attention is crucial for aligning visual and textual elements, enabling models to process and integrate information across modalities effectively.

% Flamingo. \citet{flamingo} introduced Flamingo, a family of visual language models capable of processing interleaved sequences of images and text. Flamingo leverages a frozen language model and integrates it with visual inputs through a perceiver resampler, enabling few-shot learning in multimodal settings. This approach allows the model to adapt to new tasks with minimal examples, demonstrating strong performance in image captioning and visual question answering without extensive task-specific fine-tuning.

% LLaVA and Its Extensions. The Large Language and Vision Assistant (LLaVA) proposed by \citet{llava} combines a vision encoder with a large language model to enable open-ended dialogues about images. LLaVA utilizes visual instruction tuning to align visual features with the language model, allowing for detailed and contextually relevant responses grounded in visual inputs. Building upon this, LLaVA NeXT \citep{llava_next} enhances the alignment between modalities through improved pretraining strategies and instruction tuning. LLaVA-OneVision \citep{llava_onevision} further extends this framework by integrating both 2D and 3D visual understanding, aiming to support applications that require spatial and depth perception, such as robotics and augmented reality.

% Instruction Tuning and Unified Frameworks
% InstructBLIP and BLIP3. \citet{dai2023instructblip} introduced InstructBLIP, enhancing the BLIP framework by incorporating instruction tuning for vision-language tasks. By fine-tuning on a diverse set of vision-language instructions, InstructBLIP improves the model's ability to follow complex instructions across various tasks without task-specific fine-tuning. This method enhances generalization and adaptability, making the model versatile in handling new and unseen tasks. BLIP3 \citep{blip3} continues this line of research by further unifying vision and language understanding through pretraining strategies that leverage large-scale image-text data, improving performance across multiple vision-language benchmarks.

% VILA and IDEFICS. VILA \citep{lin2024vila} proposes a modular approach to vision-language understanding, focusing on task decomposition and interaction with external knowledge sources. This framework allows for independent fine-tuning of vision and language components, leading to significant gains in complex multimodal tasks. IDEFICS \citep{idefics} introduces a fine-grained attention mechanism that dynamically adapts between image regions and text tokens, achieving better performance in tasks requiring detailed multimodal reasoning, such as visual question answering and image captioning.

% Advances in Efficiency and Scalability
% QWen2-VL and CuMo. QWen2-VL \citep{wang2024qwen2} presents a unified framework for cross-modal learning by integrating vision transformers with pre-trained language models. This approach addresses scalability in large-scale vision-language tasks. CuMo \citep{li2024cumo} introduces a dynamic mixture of experts architecture, allowing models to specialize in specific multimodal tasks while sharing knowledge across experts. This contributes to the efficiency of training large vision-language models, making it relevant in resource-constrained environments.

% Intern-XC-2.5 and miniGemini. Intern-XC-2.5 \citep{zhang2024internlm} emphasizes efficient training paradigms by utilizing optimized self-attention mechanisms to handle the computational costs associated with vision-language tasks. miniGemini \citep{li2024mini} builds upon transformer architectures by introducing a smaller yet efficient model tailored for vision-language tasks, reducing computational footprint while maintaining performance on benchmark datasets.

% Cambrian-1. \citet{tong2024cambrian} introduced Cambrian-1, a novel architecture designed for large-scale multimodal reasoning. It integrates advanced visual perception techniques with large language models, enhancing the traditional transformer architecture with modules for temporal and spatial reasoning. This addresses challenges in real-world visual tasks like video analysis and dynamic scene understanding.

% Notable developments include the LLaVA series—LLaVA \citep{llava}, LLaVA NeXT \citep{llava_next}, and LLaVA-OneVision \citep{llava_onevision}—which enhance visual instruction tuning and multimodal alignment for open-ended dialogues about images. InstructBLIP \citep{dai2023instructblip} and BLIP3 \citep{blip3} focus on instruction tuning to improve generalization across various vision-language tasks without task-specific fine-tuning. Models like VILA \citep{lin2024vila} and IDEFICS \citep{idefics} introduce modular approaches and fine-grained attention mechanisms for better multimodal reasoning. Advances in efficiency and scalability are addressed by QWen2-VL \citep{wang2024qwen2}, CuMo \citep{li2024cumo}, and Intern-XC-2.5 \citep{zhang2024internlm}, which optimize architectures for large-scale vision-language tasks. Smaller yet efficient models like miniGemini \citep{li2024mini} reduce computational footprints while maintaining performance. Cambrian-1 \citep{tong2024cambrian} enhances multimodal reasoning with modules for temporal and spatial understanding. The foundational self-attention mechanism \citep{vaswani2017attention} underpins these transformer-based models, enabling effective integration of visual and textual information.

% Emerging large vision language models \cite{GPT4o} have made significant progress on visual understanding in the form of VQA (Visual Question Answering). The predominant architectures can be summarized as different combinations/variants of vision encoder, connector, and LLM. The BLIP series \citep{dai2023instructblip,blip3} adopts QFormer as the connector between the vision encoder and LLM. Recent works mostly follow LLaVA \citep{llava} to use simple MLP as the connector and the concatenated visual and text tokens are fed into the LLM. LLaVA 1.6/LLaVA-NeXT \citep{llava_next} further scales up the data and model size with multi-resolution tricks. LLaVA-OneVision \citep{llava_onevision} proposes a new training paradigm that can train one model for single-image, multi-image, and video understanding. VILA \citep{lin2024vila} focuses on the pre-training of large vision language models with interleaved and text-only data. Mini-Gemini \citep{li2024mini} tends to bridge the gap between open-source models and GPT/Gemini with high resolution, high-quality data, and VLM-guided generation. CuMo \citep{li2024cumo} proposes to scale up the vision encoder and connector with the MoE (Mixture of Experts) \citep{shazeer2017outrageously} technique. Cambrian-1 \citep{tong2024cambrian} proposes an open-source model that excels on vision-centric tasks with large-scale training data. Intern-XC 2.5 \citep{zhang2024internlm} supports long-contextual input/output up to 96K via RoPE \citep{su2023roformerenhancedtransformerrotary} extrapolation. QWen2-VL \citep{wang2024qwen2} introduces the naive dynamic resolution mechanism which encodes images of different resolutions to different numbers of tokens. \textit{Despite the difference in data, vision encoder, or connectors, all these works follow the decoder-only LLM architecture which treats visual and text tokens equally with self-attention \cite{vaswani2017attention}.}

% Different from the pre-dominant architecture, Flamingo \citep{flamingo} integrates the visual information into LLM by adding gated cross-attention layers before the self-attention modules in LLM. It also has $O(|V|)$ computational complexity for $|V|$ visual tokens, but it introduces additional parameters for the cross-attention modules. Also, the parameters of LLM are frozen during training, leading to relatively low accuracy on VQA benchmarks. IDEFICS \citep{flamingo} follows the architecture of Flamingo with an improved training recipe. However, it does not fully take advantage of the knowledge of pre-trained LLM as the additional cross-attention modules might change the self-attention behavior of the original LLM. \textit{This issue is addressed by the proposed $\alpha$ weighting strategy in decomposed attention.}  

% Emerging large vision-language models (LVLMs) have made significant progress in visual understanding, particularly in Visual Question Answering (VQA).
% The predominant architectures can be summarized as different combinations of a vision encoder, an adapter, and a large language model (LLM).
% The BLIP series \citep{li2023blip,dai2023instructblip,blip3} adopts Q-Former as the adapter between the vision encoder and the LLM.
% Recent works largely follow LLaVA \citep{llava}, utilizing a simple multilayer perceptron (MLP) as the adapter and feeding concatenated visual and textual tokens into the LLM.
% LLaVA 1.6 and LLaVA-NeXT \citep{llava_next} further scale up data and model sizes using any-resolution techniques.
% LLaVA-OneVision \citep{llava_onevision} proposes a new training paradigm that allows a single model to handle single-image, multi-image, and video understanding tasks.
% VILA \citep{lin2024vila} focuses on pre-training large vision-language models using interleaved and text-only data.
% miniGemini \citep{li2024mini} aims to bridge the gap between open-source models and GPT/Gemini by leveraging high-resolution, high-quality data and VLM-guided generation.
% CuMo \citep{li2024cumo} proposes scaling up the vision encoder and adapter using the Mixture of Experts (MoE) technique \citep{shazeer2017outrageously}.
% Cambrian-1 \citep{tong2024cambrian} introduces an open-source model that excels in vision-centric tasks through large-scale training data. 
% Intern-XC 2.5 \citep{zhang2024internlm} supports long-context input/output sequences up to 96K tokens via Rotary Position Embedding (RoPE) extrapolation \citep{su2024roformer}.
% QWen2-VL \citep{wang2024qwen2} introduces a naive dynamic resolution mechanism that encodes images of different resolutions into varying numbers of tokens.
% \textit{Despite differences in data, vision encoders, or adapter, all these works adhere to a decoder-only LLM architecture that process visual and textual tokens homogeneously using the self-attention mechanism \citep{vaswani2017attention}.}
%\cite{li2023blip,dai2023instructblip,blip3,llava,llava_next,llava_onevision,lin2024vila,li2024mini,li2024cumo,tong2024cambrian,zhang2024internlm,wang2024qwen2}
Emerging large vision-language models (LVLMs) have made significant progress in visual understanding, particularly in Visual Question Answering (VQA).
The predominant architectures can be summarized as different combinations of a vision encoder, an adapter, and a large language model (LLM).
To name a few, LLaVA \citep{llava}, LLaVA NeXT \citep{llava_next}, LLaVA-OneVision \citep{llava_onevision}, Instruct BLIP \citep{dai2023instructblip}, BLIP3 \citep{blip3}, VILA \citep{lin2024vila}, QWen2-VL \citep{wang2024qwen2}, CuMo \citep{li2024cumo}, Intern-XC-2.5 \citep{zhang2024internlm}, miniGemini \citep{li2024mini}, Cambrian-1 \citep{tong2024cambrian}, Phi-3 VL \citep{abdin2024phi}, Chameleon~\citep{team2024chameleon}, Molmo \citep{deitke2024molmo}, Phi-3.5-Vision~\citep{abdin2024phi}.
Despite differences in data, vision encoders, or adapter, \textit{all these works adhere to a decoder-only LLM architecture that process visual and textual embeddings homogeneously} using the self-attention mechanism \citep{vaswani2017attention} within an LLM.

In contrast to predominant LVLM architectures, models like Flamingo~\citep{flamingo,awadalla2023openflamingo}, IDEFICS~\citep{laurenccon2024obelics}, and LLaMA 3~\citep{dubey2024llama} integrate visual information into LVLMs via cross-attention mechanisms between textual and visual embeddings.
These architectures share similarities with our proposed \method{}, such as employing T2V Cross-Attention to incorporate visual data and achieving a computational complexity of $\mathcal{O}(|V|)$ for $|V|$ visual embeddings.
However, this line of works differ notably in how they merge visual and textual modalities: by appending additional cross-attention modules or introducing tanh/sigmoiod gating to modulate visual information.
These substantial architectural changes can compromise the integrity of the pre-trained LLM, potentially degrading its inherent capabilities.
Indeed, \cite{laurenccon2024matters} show in IDEFICS-2 that cross-attention architectures underperform decoder-only architectures, leading them to discard the cross-attention design.
In this paper, we propose $\alpha$-weighting strategy equivalently derived from the native attention operations of LVLMs.
$\alpha$-weighting introduces minimal architectural changes and requires no additional learnable parameters, ensuring the pre-trained LLM retains its full capability for competitive downstream performance.


% In contrast to the predominant architecture, Flamingo \citep{flamingo} integrates visual information into the LLM by adding gated cross-attention layers before the self-attention modules.
% While it maintains $O(|V|)$ computational complexity for $|V|$ visual embeddings, it introduces additional parameters due to the added cross-attention modules.
% Moreover, because the parameters of the LLM are frozen during training, this leads to relatively lower accuracy on VQA benchmarks.
% IDEFICS \citep{laurenccon2024obelics} follows the architecture of Flamingo with an improved training recipe. However, it does not fully leverage the knowledge embedded in the pre-trained LLM, as the additional cross-attention modules might alter the self-attention behavior of the original LLM. \textit{This issue is addressed by the proposed $\alpha$-weighting strategy in decomposed attention.}

% stands out as a pioneering visual language model capable of processing interleaved sequences of images and text for open-ended multimodal few-shot learning. Flamingo integrates a frozen language model with a perceiver resampler that processes visual inputs, allowing the model to generate coherent and contextually relevant textual outputs based on images. This architecture supports in-context learning, enabling the model to adapt to new tasks with minimal examples, which is particularly effective for image captioning and visual question answering without extensive fine-tuning. Flamingo's ability to handle diverse and complex multimodal inputs marks a significant milestone in developing flexible and general-purpose vision-language models.