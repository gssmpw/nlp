\begin{abstract}
Predominant large vision-and-language models (LVLM) treat visual and text embeddings as homogeneous inputs to a large language models (LLM).
However, visual and textual embeddings are inherently different.
For example, visual inputs are intrinsically multi-dimensional and have already encoded contextual information between visual embeddings with a pre-trained visual encoder such as CLIP ViT.
% Simply concatenating visual and text embeddings equally could be suboptimal in term of performance and computation.
In this paper, we propose Decomposed Attention (\method{}) to process visual and textual embeddings differently by decomposing 1-D causal self-attention in an LVLM.
% In this paper, we propose a novel architecture for large multimodal models, i.e., \method{} (Decomposed Attention), which decomposes visual and text embeddings in the self-attention of LLM into visual-to-visual self-attention, text-to-text self-attention, and visual-to-text cross-attention.
In \method{}, we replace the visual-to-visual self-attention with diagonal attention, which significantly reduces the computation of visual embeddings from $\mathcal{O}(N^{2})$ to $\mathcal{O}(N)$ while maintaining comparable performance.
Furthermore, we eliminate undesirable positional bias in conventional LVLM attention between visual and textual embeddings, thereby enhancing the model's visual capability.
Lastly, we propose an $\alpha$ weighting strategy to merge visual and textual information with minimal changes, which maximally retains a pre-trained LLM's capability and thus leads to superior performance to other merging strategies.
% Key modifications are three-fold: 1) We replace the visual-to-visual self-attention with diagonal attention which reduces the computation of visual embeddings from $O(n^{2})$ to $O(n)$. 2) We rectified the positional encoding for visual embeddings so that text embeddings would not be biased on the attention toward different visual embeddings, leading to better visual capability. 3) We propose a weighting strategy between visual and text embeddings for merging so that the training is as stable as predominant self-attention architecture.
With all the above improvements, under fair comparison, our proposed \method{} can consume 8x more visual embeddings or train 5x faster compared to a conventional LVLM while achieving substantially better performance on a series of image benchmarks.
% outperforms its self-attention counterpart by a large margin with around $33\%$ computation reduction for image (LLaVA-1.5-7B) and video (LLaMA-vid-7B) VQA task.
% \cwk{talk about model performance here when available.}
We also conduct rigorous studies, and experiment with open-sourced models and data to ensure fair comparison and facilitate reproducibility.
Code, data, and models will be released.
\end{abstract}

%  and they are not perfectly aligned by the MLP connector