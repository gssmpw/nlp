\begin{abstract}
Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM).
However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure.
In this paper, we propose Decomposed Attention (\method{}), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs.
After the attention decomposition, \method{} diagonalizes visual-to-visual self-attention, reducing computation from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance.
Moreover, \method{} debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding.
Finally, we introduce an $\alpha$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM's capabilities with minimal modifications.
Extensive experiments and rigorous analyses validate the effectiveness of \method{}, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs.
Code, data, and models will be publicly available.
\end{abstract}
% and leading to superior performance across various benchmarks.
% Our \method{} allows LVLMs to process 8x more visual embeddings or train 5x faster while consistently outperforming traditional LVLMs.
% Our \method{} consistently outperforms conventional self-attention mechanism while significantly reducing computational costs.
