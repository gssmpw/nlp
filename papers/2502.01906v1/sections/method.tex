\section{Decomposed Attention}\label{sec:method}

\subsection{Background and Overview}\label{sec:background}

\begin{figure}[t]
\begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/attn.pdf}
    \caption{Decomposition of causal self-attention within an LVLM into visual-to-visual self-attention (\textcolor{blue}{V2V Self-Attn}), textual-to-visual cross-attention (\textcolor{orange}{T2V Cross-Attn}), and textual-to-textual self-attention (\textcolor{green}{T2T Self-Attn}).}\label{fig:attn}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/pos-bias.pdf}
    \caption{Positional bias in T2V Cross-Attn arising from the concatenation of visual and textual embeddings into a 1-D sequence and the resulting rotary/relative positional encodings. Embeddings further away have lower values (lighter color).}\label{fig:pos-bias}
\end{minipage}
\end{figure}

As discussed in Section~\ref{sec:intro}, visual and textual inputs are created different, and therefore we propose to process them differently within an LVLM.
We begin by decomposing the causal self-attention mechanism in an LVLM when both visual and textual embeddings are present.
As illustrated in Figure~\ref{fig:attn}, causal self-attention can be split into three distinct components: (1) visual-to-visual self-attention (V2V Self-Attn), (2) textual-to-visual cross-attention (T2V Cross-Attn), and (3) textual-to-textual self-attention (T2T Self-Attn).
Together, these attention components form the foundation for processing and integrating visual information in LVLMs:
\begin{itemize}[wide,labelindent=0pt]
\item \textbf{V2V Self-Attn} captures contextual relationships between visual embeddings by allowing each visual embedding to attend to other visual embeddings.
\item \textbf{T2V Cross-Attn} gathers visual information by allowing textual embeddings to attend to visual embeddings.
\item \textbf{Weighted combination} of T2V Cross-Attn and T2T Self-Attn merges visual and textual information into the textual embeddings.
\end{itemize}


% In this paper, we argue in Sec.~\ref{sec:intro} that visual and textual inputs are created different, and thus should be processed differently.
% To begin with, we analyze the causal self-attention within an LLM when both visual and textual embeddings are presented.
% As shown in Fig.~\ref{fig:attn}, causal self-attention can be decomposed into three parts: (1) visual-to-visual self-attention (V2V Self-Attn), (2) textual-to-visual cross attention (T2V Cross-Attn), and (3) textual-to-textual self-attention (T2T Self-Attn).
% For a visual embedding, it attends to other visual embeddings in V2V Self-Attn.
% For a textual embedding, it attends to \emph{both} visual embeddings via V2V Cross-Attn, and textual embeddings via T2T Self-Attn, which are then weighted summed together.
% The weights are adaptively determined according to the attention weights from T2V and T2T attentions.

% In summary, visual information is modeled and incorporated within an LVLM as follows:
% \begin{itemize}[wide,labelindent=0pt]
%     \item Contextual information between visual embeddings is modeled by V2V Self-Attn.
%     \item Visual information is gathered via T2V Cross-Attn between textual and visual embeddings.
%     \item Visual information from T2V Cross-Attn and textual information from T2T Self-Attn are weighted merged onto textual embeddings.
% \end{itemize}


% Our work focuses specifically on improving the vision-related aspects of LVLMs.
Since T2T Self-Attn operates similarly to standard attention in LLMs, we leave it unchanged and focus instead on the challenges unique to handling visual embeddings in LVLMs.
With the attention decomposition, we can easily manipulate and enhance these vision-related aspects of LVLMs.
In Section~\ref{sec:v2v-attn}, we propose diagonalizing the V2V Self-Attn, significantly reducing the computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance.
In Section~\ref{sec:v2v-attn}, we propose removing rotary/relative positional encodings within T2V Cross-Attn to mitigate undesirable positional bias between visual and textual embeddings.
Lastly, in Section~\ref{sec:alpha}, we derive an $\alpha$-weighting strategy for merging T2V Cross-Attn and T2T Self-Attn, introducing minimal changes and thus preserving the pre-trained LLM's capability for competitive downstream performance.

% In this paper, we are interested in improving LVLMs and thus focus on the vision-related parts, including V2V Self-Attn in Sec.~\ref{sec:v2v-attn}, T2V Cross-Attn in Sec.~\ref{sec:t2v-attn}, and merging T2V Cross-Attn and T2T Self-Attn in Sec.~\ref{sec:alpha}.
% The T2T Self-Attn works the same as in a conventional LLM between textual embeddings, and thus is not the focus of this paper.


\subsection{V2V Attention}\label{sec:v2v-attn}

In LVLMs, V2V Self-Attn is used to model the contextual relationships between visual embeddings.
Given that visual embeddings are created by passing visual inputs through a pre-trained encoder (such as CLIP ViT \citep{vit}), each visual embedding already encapsulates contextual information from other visual embeddings.
This insight suggests that relearning these contextual relationships through self-attention in the LVLM may be redundant.
To address this redundancy, we propose to diagonalize V2V Self-Attn, where each visual embedding attends only to itself, rather than to all other visual embeddings.
Specifically, for visual embeddings $V \in |V|\times d$:
\begin{align}
\bar{V} &= \texttt{SA}(V, V) = \texttt{fc}_o\left( \underbrace{\texttt{softmax}\left( \frac{\texttt{fc}_q(V)\ \texttt{fc}_k(V)^T}{\sqrt{d}},\ \texttt{dim}=1\right)}_{\text{diagonalize} }\texttt{fc}_v(V) \right)\\
&\Rightarrow \texttt{fc}_o\left(\mathds{1}\ \texttt{fc}_v(V) \right) =  \texttt{fc}_o\left(\texttt{fc}_v(V) \right)\label{eq:diag-attn}
\end{align}
, where $\mathds{1}$ is an identity matrix of size $|V|\times |V|$, and $\texttt{fc}_q$, $\texttt{fc}_k$, $\texttt{fc}_v$, and $\texttt{fc}_o$ are standard fully connected layers in an attention module for query, key, value, and output, respectively.

By turning the softmax attention matrix into an identity matrix, we essentially force each visual embedding to only attend to itself, bypassing the need for pairwise interactions between visual embeddings.
As shown in Equation~\ref{eq:diag-attn} and Figure~\ref{fig:arch-diag}, this diagonalization simplifies the self-attention operation to only two fully connected layers, thus significantly reducing the computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings.
V2V Diagonal-Attn is particularly valuable when dealing with high-resolution images or long video inputs, where the number of visual embeddings $|V|$ becomes large. 
Notably, in our experiments, we demonstrate that this method achieves similar performance to full attention while offering significant computational savings.

% In LVLM, V2V Self-Attn is utilized to model the contextual information between visual embeddings.
% Since visual embeddings are created by passing a visual input through a visual encoder such as CLIP ViT, each visual embedding vector has \emph{seen} all other visual embeddings.
% Therefore, we hypothesize that there is no need to \emph{re-learn} this contextual information between visual embeddings via LVLM's self-attention.
% We thus propose to diagonalize V2V Self-Attn such that each visual embedding only sees itself within the LVLM.
% Specifically, for visual embeddings $V \in N\times d$:
% \begin{align}
% \bar{V} &= \texttt{SA}(V, V) = \texttt{fc}_o\left( \underbrace{\texttt{softmax}\left( \frac{\texttt{fc}_q(V)\ \texttt{fc}_k(V)^T}{\sqrt{d}},\ \texttt{dim}=1\right)}_{\text{diagonalize} }\texttt{fc}_v(V) \right)\\
% &\Rightarrow \texttt{fc}_o\left(\mathds{1}\ \texttt{fc}_v(V) \right) =  \texttt{fc}_o\left(\texttt{fc}_v(V) \right)
% \end{align}
% , where $\mathds{1}$ is an identity matrix of size $N\times N$.


% As illustrated in Fig.~\ref{fig:arch-diag}, we skip the expensive $\mathcal{O}(N^2)$ computation of softmax attention weights, and reduce $\texttt{SA}(V, V)$ into two $\texttt{fc}$ layers, which has $\mathcal{O}(N)$ computational complexity with respect to the number of visual embeddings.
% This is particularly useful for high-resolution image or long video inputs.
% Furthermore, in the experiment section, we show that the model maintains comparable performance with the significant computational reduction.


\begin{figure*}[t]
\centering
\begin{subfigure}{.22\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/arch-attn.pdf}
  \caption{Conventional attention operations.}\label{fig:arch-attn}
\end{subfigure}
\hfill
\begin{subfigure}{.22\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/arch-diag.pdf}
  \caption{Diagonalized V2V attention.}\label{fig:arch-diag}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/arch-dcmp.pdf}
  \caption{Proposed $\alpha$-weighting strategy to merge T2V Cross-Attn and T2T Self-Attn.}\label{fig:arch-dcmp}
\end{subfigure}
\caption{
Module architecture and operations of
(a) conventional attention in LVLM with visual and texual embeddings concatenated as a homogeneous input sequence,
(b) V2V Diagonal-Attn, where the expensive computation of softmax attention weight is skipped, and
(c) $\alpha$-weighting strategy to merge T2V Cross-Attn and T2T Self-Attn equivalent to LVLM's inherent attention operations for retaining a pre-trained LLM's full capability.
}\label{fig:arch}
\end{figure*}


\subsection{T2V Attention}\label{sec:t2v-attn}

In T2V Cross-Attn, textual embeddings interact with visual embeddings to incorporate visual information.
To align with an LVLM's native self-attention operations and architecture, unlike previous methods that add separate cross-attention modules \citep{flamingo}, we reuse and share the existing weights from the LVLMâ€™s self-attention, modifying only the query, key, and value assignments, and revising the attention mask to be non-causal.
As shown in Figure~\ref{fig:arch-dcmp} left, textual embeddings are used as the query, while visual embeddings serve as the key and value.

% In T2V Cross-Attn, textual embeddings interact with the visual embeddings to gather visual information.
% To match the operations of LVLM's self-attention, instead of adding a new cross-attention module as in \textbf{XXX}, we reuse and share weights with LVLM's attention but swap the query, key, and value combination and revise the attention mask.
% Specifically, T2V Cross-Attn uses textual embeddings as query, and visual embeddings as key and value as shown in Fig.~\ref{fig:arch-dcmp} left.
% The attention mask is also revised from causal to non-causal.
% The internal operations and learnable parameters remain unchanged.

Additionally, we observe a significant issue with positional bias in T2V Cross-Attn if we follow the exact attention operation in LVLMs.
When visual and textual embeddings are concatenated into a single 1-D sequence, an example of the positional IDs for textual and visual embeddings is given in Figure~\ref{fig:pos-bias}.
We can see that the rotary/relative positional encodings skew attention weights based on the positional distance between visual and textual embeddings.
For example, distant pairs such as the textual embedding at P11 and visual embedding at P0 receive lower attention weight than pairs closer together, like the textual embedding at P7 and visual embedding at P6.
This bias can hinder effective vision-language interaction for tasks requiring a comprehensive understanding of visual context.

% Additionally, we observe undesirable positional bias introduced into T2V Cross-Attn in conventional LVLM.
% In conventional LVLM, visual and textual embeddings are concatenated into a 1-D sequence.
% We can see an example of the positional IDs for textual and visual embeddings in Fig.~\ref{fig:pos-bias}.
% Notably, in the figure the rotary/relative positional encodings introduce undesirable bias into the attention weights between visual and textual embeddings, where distant embeddings have lower attention weights.
% For instance, the attention weight between textual embedding at P11 and visual embedding at P0 is biased to be lower than other pairs such as textual embedding at P7 and visual embedding at P6.
% With this bias, textual embeddings further away from visual embeddings may fail to properly leverage visual information necessary to conduct a vision-centric task.

To address this issue, we propose to debiase T2V Cross-Attn by discarding the rotary/relative positional encodings within, effectively setting the relative positional differences to zero.
Notably, this modification is challenging to implement in conventional LVLMs but becomes straightforward with our decomposed T2V Cross-Attn and T2T Self-Attn framework.
To compensate for this removal, we introduce learnable positional encodings, similar to those used in CLIP, to the visual embeddings before they are passed into the LLM.

% To eliminate this bias, we propose to \emph{discard} the rotary/relative positional encodings within T2V Cross-Attn, equivalent to setting relative positional difference to zero.
% Additionally, to enhance the encoding of positional information, we add learnable position encodings similar to CLIP on visual embeddings before sent into an LLM.
% It is worth noting that the removal of positional encodings cannot be easily implemented in conventional LVLMs, but is quite straightforward with decomposed T2V Cross-Attn and T2T Self-Attn in our formulation.

\subsection{$\alpha$-Weighting}\label{sec:alpha}

Once visual information is gathered via T2V Cross-Attn and textual information via T2T Self-Attn, the next challenge is how to effectively merge these two streams of information.
Existing methods often cascade T2V Cross-Attn and T2T Self-Attn \citep{flamingo} or introduce learnable tanh/sigmoid gates \citep{flamingo}.
These approaches involve significant architectural changes or introduce additional parameters, which can break the integrity and degrade the performance of pre-trained LLMs.

% After gathering visual information from T2V Cross-Attn and textual information from T2T Self-Attn, a follow-up question is: \emph{how to properly merge these two sources of information?}
% Previous methods proposed to cascade T2V Cross-Attn and T2T Self-Attn, or use a learnable $\texttt{Tanh}$ gate or constant/learnable weights.
% These approaches involve significant architectural changes or introduce new learnable parameters, which break the integrity of a pre-trained LLM and lead to inferior performance

Instead, we propose an $\alpha$-weighting strategy for merging the T2V and T2T attentions, analytically derived from the original LVLM attention formulation.
This approach introduces no additional parameters and retains equivalence with conventional LVLM attention, thereby preserving the pre-trained LLM's capabilities.
For a textual query $t$, its attention to textual and visual embeddings can be expressed as:

% In this section, we analytically derive from the original attention formulation within an LVLM, and propose an $\alpha$ weighting strategy for merging T2V and T2T attentions.
% $\alpha$ weighting is equivalent to the original attention operations and introduces no additional parameters, thereby maximally retaining a pre-trained LLM's capability.
% We start with text to text plus vision attention in an LVLM.
% For a text query $t$:

\begin{equation}
\bar{t} = \texttt{Attn}(t, [V, T]) = \sum_{i}^{L} \frac{e^{\bm{q}_t\cdot \bm{k}_{i}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}}\bm{v}_{i} \label{eq:xattn}
\end{equation}

, where $\bm{q}, \bm{k}, \bm{v}$ are projected query, key, value within an attention module, respectively.
$\bm{k}$ and $\bm{v}$ are projected from the concatenated visual and textual embeddings $[V, T]$.
For $N$ visual embeddings and $M$ textual embeddings, $\bm{k}_i \in \{\bm{k}_{v_1},...,\bm{k}_{v_N},\bm{k}_{t_1},...,\bm{k}_{t_M},\}$, where $\bm{k}_{v_j}$ and $\bm{k}_{t_l}$ represent the key corresponding to the $j$-th visual embedding and $l$-th textual embeddings, respectively. 
Similarly $\bm{v}_i \in \{\bm{v}_{v_1},...,\bm{v}_{v_N},\bm{v}_{t_1},...,\bm{v}_{t_M}\}$.
We then rewrite Equation~\ref{eq:xattn} by splitting key value from $V$ and from $T$:

\begin{align}
\sum_{i}^{L} \frac{e^{\bm{q}_t\cdot \bm{k}_{i}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}}\bm{v}_{i}
&= \sum_{i}^{N} \frac{e^{\bm{q}_t\cdot \bm{k}_{v_i}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}}\bm{v}_{v_i} +
\sum_{i}^{M} \frac{e^{\bm{q}_t\cdot \bm{k}_{t_i}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}}\bm{v}_{t_i}\\
&= \frac{\sum_{n}^{N} e^{\bm{q}_{t}\cdot \bm{k}_{v_n}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}} \sum_{i}^{N} \frac{e^{\bm{q}_t\cdot \bm{k}_{v_i}}}{\sum_{n}^{N} e^{\bm{q}_{t}\cdot \bm{k}_{v_n}}}\bm{v}_{v_i} +
\frac{\sum_{m}^{M} e^{\bm{q}_{t}\cdot \bm{k}_{t_m}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}} \sum_{i}^{M} \frac{e^{\bm{q}_t\cdot \bm{k}_{t_i}}}{\sum_{m}^{M} e^{\bm{q}_{t}\cdot \bm{k}_{t_m}}}\bm{v}_{t_i}\\
&= \frac{\sum_{n}^{N} e^{\bm{q}_{t}\cdot \bm{k}_{v_n}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}} \texttt{XA}(t,V) +
\frac{\sum_{m}^{M} e^{\bm{q}_{t}\cdot \bm{k}_{t_m}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}} \texttt{SA}(t,T)\\
&\equiv \alpha_V\ \texttt{XA}(t, I) + \alpha_T\ \texttt{SA}(t,T) \label{eq:dattn}
\end{align}

For numerical stability, modern deep learning packages take log of the summed exponentials:

\begin{equation}
\text{Let}\ S_V = \log\left(\sum_{n}^{N} e^{\bm{q}_{t}\cdot \bm{k}_{v_n}}\right) \text{, and}\ S_T = \log\left(\sum_{m}^{M} e^{\bm{q}_{t}\cdot \bm{k}_{t_m}}\right)
\end{equation}
Then the weights $\alpha_V$ can be expressed as:
\begin{equation}
\alpha_V
= \frac{\sum_{n}^{N} e^{\bm{q}_{t}\cdot \bm{k}_{v_n}}}{\sum_{l}^{L} e^{\bm{q}_{t}\cdot \bm{k}_{l}}}
= \frac{e^{S_V}}{e^{S_V}+e^{S_T}}
= \frac{1}{1+e^{-(S_{V}-S_{T})}}
= \texttt{Sigmoid}(S_V - S_T) \label{eq:alpha}
\end{equation}

We can similarly derive that $\alpha_T=\texttt{Sigmoid}(S_T - S_V) = 1-\alpha_V$.

In summary, to merge visual information from T2V Cross-Attn and textual information from T2T Self-Attn while retaining equivalence to original attention in an LVLM, we propose $\alpha$ weighting, a weighted sum strategy with weights $\alpha_V$ and $\alpha_T$ analytically derived in Equation~\ref{eq:alpha}.
As shown in Figure~\ref{fig:arch-dcmp}, $\alpha$ weighting introduces no additional parameters and minimal architectural/operational changes and retains equivalence with the native LVLM attention, thereby retaining a pre-trained LLM's full capability and outperforming alternative merging strategies in our experiments.