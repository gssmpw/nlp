\section{Conclusion}

% \begin{figure}[!b]
% \vspace{-0.2cm}
% \centering
% \includegraphics[width=1\linewidth]{figures/snow.pdf}
% \includegraphics[width=1\linewidth]{figures/fruits.pdf}
% \includegraphics[width=1\linewidth]{figures/ironing.pdf}
% \includegraphics[width=1\linewidth]{figures/diamond-head.pdf}
% \caption{
% Qualitative comparisons between proposed \method{} and its S-Attn counterpart.
% Erroneous contents from S-Attn are highlighted in red.
% Correct or preferred contents from \method{} are highlighted in blue.
% }\label{fig:qualitative}
% \end{figure}

In this paper, we introduced Decomposed Attention (D-Attn), a novel and general framework designed to process visual and textual embeddings differently within LVLMs.
Through the decomposition of conventional causal self-attention in LVLMs, \method{} reduces the computational complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ by diagonalizing V2V Self-Attn, and improve model performance by debiasing T2V Cross-Attn.
To merge back visual and textual information, our proposed $\alpha$-weighting strategy preserves the capabilities of pre-trained LLMs with minimal modifications.
Extensive experiments and rigorous analyses demonstrate that \method{} consistently outperforms its S-Attn counterpart, offering both performance gains and substantial computational savings.
Our contributions highlight the importance of handling visual and textual inputs differently, paving the way for more efficient and effective LVLMs.
