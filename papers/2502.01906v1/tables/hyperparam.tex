\begin{table}[ht]
\caption{
Hyperparameters for three training stages and two types LLMs.
}\label{tab:hyperparams}
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{@{\extracolsep{4pt}}l|c|cc|cc@{}}
\toprule
& \textbf{Stage 1} & \multicolumn{2}{c|}{\textbf{Stage 2}} & \multicolumn{2}{c}{\textbf{Stage 3}} \\
& & Mistral 0.3 7B & Gemma 2 9B & Mistral 0.3 7B & Gemma 2 9B \\
\midrule
lr adapter & 1e-3 & 5e-6 & 2e-5 & 5e-6 & 2e-5\\
lr llm & 0.0 & 2e-6 & 1e-5 & 2e-6 & 1e-5 \\
lr vis-enc & 0.0 & 2e-7 & 1e-6 & 2e-7 & 1e-6 \\
weight decay & 0.0 & \multicolumn{2}{c|}{0.1} & \multicolumn{2}{c}{0.1} \\
optimizer & AdamW & \multicolumn{2}{c|}{AdamW} & \multicolumn{2}{c}{AdamW} \\
Adam $\beta_1$ & default (0.9) & \multicolumn{2}{c|}{0.9} & \multicolumn{2}{c}{0.9} \\
Adam $\beta_2$ & default (0.999) & \multicolumn{2}{c|}{0.95} & \multicolumn{2}{c}{0.95} \\
Adam $\epsilon$ & default (1e-8) & \multicolumn{2}{c|}{1e-5} & \multicolumn{2}{c}{1e-5} \\
warmup ratio & 0.03 & \multicolumn{2}{c|}{0.03} & \multicolumn{2}{c}{0.03} \\
lr scheduler & cosine & \multicolumn{2}{c|}{cosine} & \multicolumn{2}{c}{cosine} \\
epochs & 1 & \multicolumn{2}{c|}{1} & \multicolumn{2}{c}{1} \\
total batch size & 512 & \multicolumn{2}{c|}{256} & \multicolumn{2}{c}{128} \\
dtype & bfloat16 & \multicolumn{2}{c|}{bfloat16} & \multicolumn{2}{c}{bfloat16} \\
deepspeed & stage 2 & \multicolumn{2}{c|}{stage 3} & \multicolumn{2}{c}{stage 3} \\
\bottomrule
\end{tabular}
}
\end{table}


