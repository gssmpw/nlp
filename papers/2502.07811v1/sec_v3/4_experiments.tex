\section{Experiments}
\label{sec:experiments}


\subsection{Datasets} 



We evaluated our method on four action recognition video datasets: UCF101~\cite{soomro2012ucf101}, HMDB51~\cite{kuehne2011hmdb}, Kinetics-400 (K400)~\cite{kay2017kinetics}, and Something-Something V2 (SSv2)~\cite{goyal2017something} (\textit{refer to supplementary materials for details}).

For K400 and SSv2, where pre-sampled frame image datasets were unavailable, we created smaller subsets of the original datasets. This approach maintained class diversity while addressing the challenges of manual frame extraction from large datasets, which can be resource-intensive due to GPU constraints. We randomly sampled frames from each video in these smaller datasets to construct corresponding pre-sampled frame datasets. Ablation studies were performed on SSv2, and the optimized parameters were used for both K400 and SSv2 evaluations. For UCF101 and HMDB51, we fine-tuned a pre-trained K400 model directly without additional sampling.


\subsection{Preprocessing}

We follow the preprocessing protocols outlined in MAE~\cite{he2022masked} and SpatioTemporalMAE~\cite{feichtenhofer2022masked}. For the Kinetics-400 and SSv2 datasets, we sample 16 frames, from each raw video, setting each frame at a resolution of 224 × 224 pixels, and apply a temporal sliding window with a stride of 4, with the starting frame location selected randomly~\cite{feichtenhofer2022masked}. The default resolution for the videos and their corresponding sampled frames is 224 × 224. In addition to spatiotemporal augmentations such as random augmentation, random resizing, cropping,
horizontal flipping, random erasing mixup, and cutmix applied in the video branch (\S~\ref{sec:intra-modal}), we also apply spatial augmentations random resizing, cropping, and horizontal flipping to the sampled frames.



\subsection{Implementation Details}

We designed a three-tower architecture inspired by recent advancements in Masked Autoencoders (MAEs)~\cite{he2022masked,tong2022videomae,feichtenhofer2022masked}, as shown in Figure~\ref{fig:architecture}. The network comprises two main branches: the video branch and the image branch.

\noindent\textbf{Video Branch:} This branch comprises two shared-weight pre-trained SpatioTemporalMAE~\cite{feichtenhofer2022masked} configurations based on ViT-B/16, with masking ratios (\(\rho\)) ranging from 90\% to 95\%. These models take as input both the original and an augmented version of the video. The video encoder extracts video-level features and samples them to generate frame-level feature embeddings of the visible tokens. This encoder embedding facilitates improved information exchange across a sequence of frames, effectively capturing spatiotemporal dynamics.

\noindent \textbf{Image Branch:} The Image branch is built on a pre-trained MAE~\cite{he2022masked} ViT-B/16 configuration with a masking ratio (\(\rho\)) between 75\% and 90\%. This branch extracts frame-level feature embeddings of visible tokens, leveraging the spatial priors learned on sampled frames. These priors, which can be considered as a form of human prior, assist in learning spatial information for each frame in the video sequence. The encoder embedding further distils semantic knowledge from these sampled frames to the videos, enhancing the overall understanding of spatiotemporal content.

We utilize the test-time adaptation technique to mitigate the need for a large number of GPU resources to save GPU memory and reduce pre-training time. We use a patch size of \textcolor{blue}{2} $\times$ \textcolor{red}{3} $\times$ \textcolor{teal}{16} $\times$ \textcolor{teal}{16}, resulting in $\textcolor{blue}{\left(\frac{16}{2}\right)} \times \textcolor{red}{\left(\frac{3}{3}\right)} \times \textcolor{teal}{\left(\frac{224}{16}\right)} \times \textcolor{teal}{\left(\frac{224}{16}\right)} = \textcolor{purple}{1568}$ tokens for an input video of size \textcolor{blue}{16} $\times$ \textcolor{red}{3} $\times$ \textcolor{teal}{224} $\times$ \textcolor{teal}{224}. The differences in masking ratios and spatiotemporal-spatial feature correspondence strengthen our method. Both pre-trained SpatioTemporalMAE~\cite{feichtenhofer2022masked} and MAE~\cite{he2022masked} allow higher masking ratio ($\textcolor{orange}{\rho}$) and distill well-learned semantic attributes from sampled frames to videos through the difference between masking ratios, and spatiotemporal-spatial feature embedding correspondence of visible tokens. For all experiments, We use adamW~\cite{loshchilov2018decoupled} optimizer with a batch size of 32 and 8 GPUs with a decoder depth of 4. 

\subsection{Test-Time Adaptation (TTA)} 
Initial inference is conducted using pre-trained weights available from open-source repositories (MAE ~\cite{he2022masked} for the image branch and SpatioTemporalMAE ~\cite{feichtenhofer2022masked} for the video branch) to compute losses. We perform 20 gradient updates based on these losses during test time, refining the model weights. After the adaptation step, the final inference is performed to obtain the refined weights. This approach saves GPU memory and reduces pre-training time. TTA is a refinement step, not a replacement for pre-training, dynamically fine-tuning pre-trained weights (from MAE and SpatioTemporalMAE) to better align with test-time data during inference. Due to GPU constraints, we created smaller subsets of K400 and SSv2 from the subset randomly sampled frames to create corresponding pre-sampled frame datasets-our test-time data, as manual frame extraction from large datasets is resource-intensive. TTA complements pre-training by efficiently adapting weights with 20 lightweight gradient updates per batch, based on contrastive and reconstruction losses on test-time data, without requiring large-scale re-training. 


