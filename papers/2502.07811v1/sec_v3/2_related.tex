\section{Related Work}
\label{sec:related}

\noindent \textbf{Representation Learning on Videos:}
SSL video representation learning has been extensively studied, with early work leveraging pretext tasks like temporal order, space-time puzzles, and optical flow statistics for supervision~\cite{benaim2020speednet,misra2016shuffle,wang2015unsupervised,xu2019self}. Recently, contrastive learning has gained prominence by enforcing feature space invariance, bringing positive samples closer and separating negatives~\cite{chen2020simple,he2020momentum,li2021improve}. Several video-based methods have extended this by exploring spatial-temporal augmentations~\cite{diba2021vi2clr,feichtenhofer2021large,ge2021revitalizing,han2020memory,pan2021videomoco,qian2021spatiotemporal}. Additionally, Masked Image Modeling (MIM)~\cite{bao2022beit,he2022masked,feichtenhofer2022masked,wei2022masked} has been successfully adapted to videos~\cite{feichtenhofer2022masked,tong2022videomae,wei2022masked}, achieving strong results across various video tasks~\cite{simonyan2014two,tran2015learning,wang2018temporal,wang2021tdn,zhang2021co}.

\noindent \textbf{Masked Autoencoders (MAEs):}
MAEs~\cite{bandara2023adamae,feichtenhofer2022masked,huang2023mgmae,tong2022videomae,wang2023videomae} have made significant advances over contrastive learning in self-supervised vision tasks by utilizing high masking ratios during pre-training, resulting in simpler, more efficient models. Masking techniques are central to their success~\cite{feichtenhofer2022masked,tong2022videomae}, with common strategies including patch masking~\cite{feichtenhofer2022masked}, frame masking~\cite{qian2021spatiotemporal,wei2022masked}, and tube-based masking, which drops tokens across frames to avoid information leakage~\cite{wang2023videomae}. However, no single masking method generalizes well across datasets due to varying scene dynamics, data acquisition conditions, and spatiotemporal complexities~\cite{bandara2023adamae}. For instance, SpatioTemporalMAE~\cite{feichtenhofer2022masked} excelled on Kinetics-400 with random patch masking, while VideoMAE~\cite{tong2022videomae} performed best on Something-Something V2 using tube masking, highlighting the need for task-specific masking strategies.

\noindent \textbf{MAEs for Videos:}
Extending MAEs to videos, SpatioTemporalMAE~\cite{feichtenhofer2022masked} and VideoMAE~\cite{tong2022videomae} have made notable progress. BEVT~\cite{wang2022bevt} and OmniMAE~\cite{girdhar2023omnimae} further advanced the field by training unified image and video MAEs with shared weights across datasets. MAR~\cite{qing2023mar} reduced computational costs by using running cell masking, while VideoMAE v2~\cite{wang2023videomae} proposed masking decoder-reconstructed tokens. AdaMAE~\cite{bandara2023adamae} introduced adaptive masking to replace random techniques. Human priors, such as motion trajectories, were incorporated in MGMAE~\cite{huang2023mgmae}, MotionFormer~\cite{patrick2021keeping}, and MME~\cite{sun2023masked}, while SemMAE~\cite{li2022semmae} used semantic parts-guided masking. MaskViT~\cite{gupta2022maskvit} added spatial and spatiotemporal attention with variable token masking ratios.

\noindent \textbf{Cross-Modal Representation Learning:}Videos often include multiple modalities such as text, images, motion (e.g., optical flow), and audio, which provide rich supervision for understanding semantic context~\cite{desai2021virtex,radford2021learning,sariyildiz2020learning,castrejon2016learning,gong2014improving,karpathy2015deep,lu202012,miech2020end}. Cross-modal pre-training, combining text with images~\cite{desai2021virtex,radford2021learning} and audio with video~\cite{arandjelovic2017look,arandjelovic2018objects,morgado2021robust,morgado2021audio,korbar2018cooperative,owens2018audio}, has shown success in learning transferable representations for various downstream tasks. Approaches like BEVT~\cite{wang2022bevt} and OmniMAE~\cite{girdhar2023omnimae} integrate image and video pre-training, while CrossVideo~\cite{liu2024crossvideo} introduces point cloud video datasets paired with image datasets. Our method, however, addresses the lack of pre-sampled frame datasets for images by introducing a new sampling strategy for the image branch. We manually sample frames to enhance learning since video frames provide richer semantic context. In this context, CrossVideoMAE fuses SpatioTemporalMAE~\cite{feichtenhofer2022masked} (video branch) with a pre-trained MAE~\cite{he2022masked} (image branch) using ViT-B/16. This method aligns feature embeddings from sampled frames with corresponding videos at both frame and video levels, ensuring robustness to video augmentations while distilling semantic knowledge effectively.

