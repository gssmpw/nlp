\section{Proposed Method}
\label{sec:method}

\begin{figure*}[h!]
\vspace{-15pt}
\centering
\includegraphics[width=\textwidth]{Architecture.drawio.png}
\vspace{-18pt}
\captionsetup{font=footnotesize}
\caption{ \textbf{A).} The proposed CrossVideoMAE framework comprises two branches: the video branch and the image branch. The video branch employs intra-modal pre-training to ensure that the encoder develops invariance to augmentations within the video domain. The image branch leverages are cross-modal pre-training to distill semantic knowledge from pre-trained MAE~\cite{he2022masked}, transferring insights from sampled frames to corresponding videos. The model is pre-trained jointly across video and image domains using a combination of intra-modal and cross-modal contrastive learning objectives at both the video and frame levels. For downstream tasks, the image branch is discarded, and only the video branch encoder is utilized as the backbone. \textbf{B).} Zoom in version of the feature space. This approach demonstrates the spatiotemporal-spatial alignment of feature embedding correspondence for visible tokens, ensuring invariance at both the video level and frame level, enhancing the representation robustness.}
\label{fig:architecture}
\vspace{-15pt}
\end{figure*}

The overall architecture of our proposed method is illustrated in Fig.~\ref{fig:architecture}. In this section, we enhance self-supervised video representation learning by integrating both intra-modal and cross-modal contrastive learning at both video and frame levels. We provide a detailed explanation of our approach by adapting the design and methods described in ~\cite{liu2024crossvideo}. We begin by outlining the network architecture of the proposed method in \S~\ref{sec:perliminaries}. Subsequently, in \S~\ref{sec:intra-modal} and \S~\ref{sec:cross-modal}, we describe the contrastive learning loss functions developed for intra-modal and cross-modal settings at both video and frame levels. Finally, we detail our overall pre-training objective in \S~\ref{sec:objective}.

\subsection{Preliminaries}
\label{sec:perliminaries}
\textbf{Problem Setup:} Suppose that there is a dataset provided \(\mathcal{D} = \{(u_i, f_i)\}_{i=1}^{|\mathcal{D}|}\), with \(u_i \in \mathbb{R}^{\textcolor{blue}{T} \times \textcolor{teal}{H} \times \textcolor{teal}{W} \times \textcolor{red}{C}}\) and \(f_i \in \mathbb{R}^{\textcolor{teal}{H} \times \textcolor{teal}{W} \times \textcolor{red}{C}}\). Note that $f_i$ is obtained by randomly sampling frames from the video sequence $u_i$, where $u_i$ has a temporal sequence of frames of length $\textcolor{blue}{T}$. We define each $u_i$ as $u_i = \{f_1, f_2, \ldots, f_j, \ldots, f_{\textcolor{blue}{T}}\}$, where each $f$ denotes one frame. We tokenize the video and sampled frames into a sequence of tokens \(u_i = \{u_i^1, u_i^2, \ldots, u_i^N\}\), and \(f_i = \{f_i^1, f_i^2, \ldots, f_i^M\}\) for each sample $i$. For its masked version, we denote the visible tokens as $\{u_i^v\}, \{f_i^v\}$. The feature embedding of visible tokens $\{u_i^v\}$ obtained by $f_{u}(\{u_i^v\} + \{p_i^v\})$, where $\{p_i^v\}$ is the positional encoding. Our goal is to pre-train a video encoder \(f_{u}(\cdot)\) in a self-supervised fashion to be effectively transferable to downstream tasks. To this end, we use an image encoder \(f_{f}(\cdot)\), encoder embedding with multi-layer perceptron (MLP) \(g_{u}(\cdot)\) and \(g_{f}(\cdot)\) for the video and image, respectively. 
\textbf{Notations:} $u_i: \{u_i^v\} + \{p_i^v\}$,
$f_i: \{f_i^v\} + \{p_i^v\}$

\subsection{Intra-Modal Contrastive Learning}
\label{sec:intra-modal}
Building on the success of contrastive learning in image and video domains, we posit that intra-modal contrastive learning is essential for capturing view-invariant representations. At both video and frame levels, we enforce the feature embeddings of visible tokens to be invariant to a variety of data augmentations. For a given input video \(u_i\), we denote the visible tokens of the raw and augmented video as \(u_{i}^{1}\) and \(u_{i}^{2}\), respectively. The augmented video \(u_{i}^{2}\) is constructed by sequentially applying spatiotemporal augmentations to the original video and then randomly masking portions of the augmented video. These augmentations include transformations such as rotation, random cropping, scaling, and translation. Additionally, we apply spatial transformations like colour jittering, spatiotemporal transformations such as random augmentation, random resizing, cropping, horizontal flipping, random erasing, mixup, and cut mix, along with temporal transformations like frame extraction through down-sampling.

The video encoder \(f_{u}\) maps both \(u_{i}^{1}\) and \(u_{i}^{2}\) into a feature embedding space. These embedded vectors are then projected into a video-level invariant space \(\mathbb{R}^{|\mathcal{D}|}\) using the encoder embedding function \(g_{u}(\cdot)\). Subsequently, these projected vectors in the video-level invariant space $\mathbb{R}^{|\mathcal{D}|}$ are sampled to obtain frame-level invariant space within the same embedding space \(\mathbb{R}^{|\mathcal{D}|}\), where the contrastive loss is applied. This sampling process involves extracting frame-level embedding corresponding to each frame from video-level feature embedding of visible tokens to capture temporal variations effectively. Sampling from the video-level invariant space \(\mathbb{R}^{|\mathcal{D}|}\) to obtain frame-level invariant space involves extracting frame-specific embeddings from video-level feature embedding of the visible tokens. These frame-level embeddings correspond to each frame within the same embedding space \(\mathbb{R}^{|\mathcal{D}|}\), as in MAEs frame-level embedding distinguishable within the video-level embedding. We denote the video-level projected vectors of \(u_{i}^{1}\) and \(u_{i}^{2}\) as \(\textbf{z}_{u_i}^{1}\) and \(\textbf{z}_{u_i}^{2}\), respectively, and the frame-level projected vectors as \(\textbf{z}_{f_i}^{1}\) and \(\textbf{z}_{f_i}^{2}\). Here, each projected vector \(\textbf{z}_{i}^t\) is defined as \(\textbf{z}_{i}^t = g_{u}(f_{u}(u_{i}^t))\). The frame-level projected vectors $\textbf{z}_{f_i}^t$ is obtained manually by sampling the video-level projected vectors $\textbf{z}_{u_i}^t$. 

For both the video-level and frame-level objectives, our aim is to maximize the cosine similarity between \(\textbf{z}_{u_i}^{1}\) and \(\textbf{z}_{u_i}^{2}\) for video-level learning, and between \(\textbf{z}_{f_i}^{1}\) and \(\textbf{z}_{f_i}^{2}\) for frame-level learning, while minimizing the similarity with all other projected vectors within the mini-batch. We utilize the NT-Xent loss, as introduced in SimCLR~\cite{chen2020simple}, is used, to learn discriminative features. Notably, our approach does not rely on any memory bank, consistent with recent advancements in self-supervised contrastive learning. For both video and frame levels, we compute the loss functions \(L_{u}(i, 1, 2)\) and \(L_{f}(i, 1, 2)\) for the positive pairs \(\textbf{z}_{u_i}^{1}\) and \(\textbf{z}_{u_i}^{2}\), and \(\textbf{z}_{f_i}^{1}\) and \(\textbf{z}_{f_i}^{2}\), respectively, as follows:

\begin{equation}
\scriptsize
L_{u}(i, 1, 2) = -\log \frac{\exp(\text{s}(\textbf{z}_{u_i}^{1}, \textbf{z}_{u_i}^{2})/\tau)}{\sum\limits_{\substack{k=1 \\ k \neq i}}^{N} \exp(\text{s}(\textbf{z}_{u_i}^{1}, \textbf{z}_{u_k}^{1})/\tau) + \sum\limits_{k=1}^{N}\exp(\text{s}(\textbf{z}_{u_i}^{1}, \textbf{z}_{u_k}^{2})/\tau)}
\label{equ:equation1}
\end{equation}
\begin{equation}
\scriptsize
L_{f}(i, 1, 2) = -\log \frac{\exp(\text{s}(\textbf{z}_{f_i}^{1}, \textbf{z}_{f_i}^{2})/\tau)}{\sum\limits_{\substack{k=1 \\ k \neq i}}^{N} \exp(\text{s}(\textbf{z}_{f_i}^{1}, \textbf{z}_{f_k}^{1})/\tau) + \sum\limits_{k=1}^{N}\exp(\text{s}(\textbf{z}_{f_i}^{1}, \textbf{z}_{f_k}^{2})/\tau)}
\label{equ:equation2}
\end{equation}
\noindent where N is the mini-batch size, $\tau$ is the temperature coefficient and \(\text{s}(\cdot)\) denotes the cosine similarity function. Our intra-modal instance discrimination contrastive loss function $\mathcal{L}_{\text{\textit{intra}}}$ for a mini-batch can be described as:
\vspace{-5pt}
\begin{equation}
\mathcal{L}_{\text{\textit{intra}}} = \frac{1}{2N}{\sum_{i=1}^{N} (L_u(i, 1, 2) + L_f(i, 1, 2)})
\end{equation}
\vspace{-5pt}


\subsection{Cross-Modal Contrastive learning}
\label{sec:cross-modal}
In addition to aligning feature embeddings within the video domain (intra-modal contrastive learning), we introduce an auxiliary cross-modal contrastive learning objective that spans both video and sampled frame image modalities. This approach is designed to learn discriminative features across modalities and enhance the video encoder's ability, thereby improving the representation learning capability for videos by aligning frame-level features with their corresponding sampled frame image features. Specifically, we first embed the visible tokens of sampled frames \(f_i\) of \(u_i\) into a feature embedding space using the image encoder \(f_{f}(\cdot)\). We then project the embedded vectors into the frame-level feature invariant space$\mathbb{R}^{|\mathcal{D}|}$ using the image encoder embedding \(g_{f}(\cdot)\), defined as \(\textbf{h}_{f_i}\) where $\textbf{h}_i = g_{f}(f_{f}(f_i))$. The difference between the frame-level representation \(\textbf{h}_{f_i} \) in the cross-modal section and intra-modal section lies in the masking ratio applied to each branch. In contrast to previous cross-modal approaches, we do not explicitly discriminate features between the two modalities (video and image). Instead, we implement feature discrimination in the video domain and distill semantic attributes from sampled frames to videos to improve video understanding. Then, we compute the mean of the projected vectors $\textbf{z}_{u_i}^{1}$ and $\textbf{z}_{u_i}^{2}$, $\textbf{z}_{f_i}^{1}$ and $\textbf{z}_{f_i}^{2}$ to obtain the projected at video-level and frame-level features $\textbf{z}_{u_i}$ and $\textbf{z}_{f_i}$ of $u_i$. 
\vspace{-3pt}
\begin{equation}
\textbf{z}_{u_i} = \frac{1}{2} (\textbf{z}_{u_i}^{1} + \textbf{z}_{u_i}^{2}) ; \quad \textbf{z}_{f_i} = \frac{1}{2} (\textbf{z}_{f_i}^{1} + \textbf{z}_{f_i}^{2})
\end{equation}
In the invariant space, our goal is to maximize the cosine similarity between \(\textbf{z}_{f_i}\) and \(\textbf{h}_{f_i}\), as well as between \(\textbf{z}_{u_i}\) and \(\textbf{h}_{u_i}\), since they correspond to the same instance. Our cross-modal alignment strategy compels the model to learn from more challenging positive and negative samples, thereby enhancing the representation capability beyond what is achieved through intra-modal alignment alone. We compute the contrastive loss functions \(C_u(i, 1, 2)\) and \(C_f(i, 1, 2)\) for the positive pairs \(\textbf{z}_{u_i}\) and \(\textbf{h}_{u_i}\) with \(\textbf{z}_{f_i}\) and \(\textbf{h}_{f_i}\) as follows:
\begin{equation}
\scriptsize
C_u(i, 1, 2) = -\log \frac{\exp(\text{s}(\textbf{z}_{u_i}, \textbf{h}_{u_i})/\tau)}{\sum\limits_{\substack{k=1 \\ k \neq i}}^{N} \exp(\text{s}(\textbf{z}_{u_i}, \textbf{z}_{u_k})/\tau) + \sum\limits_{k=1}^{N}\exp(\text{s}(\textbf{z}_{u_i}, \textbf{h}_{u_k})/\tau)}
\end{equation}
\begin{equation}
\scriptsize
C_f(i, 1, 2) = -\log \frac{\exp(\text{s}(\textbf{z}_{f_i}, \textbf{h}_{f_i})/\tau)}{\sum\limits_{\substack{k=1 \\ k \neq i}}^{N} \exp(\text{s}(\textbf{z}_{f_i}, \textbf{z}_{f_k})/\tau) + \sum\limits_{k=1}^{N}\exp(\text{s}(\textbf{z}_{f_i}, \textbf{h}_{f_k})/\tau)}
\end{equation}
\noindent where \(\text{s}(\cdot)\), N, $\tau$ refers to the same parameters as in Eq.~\ref{equ:equation1},~\ref{equ:equation2} The cross-modal loss function $\mathcal{L}_{\text{\textit{cross}}}$ for a mini-batch is then formulated as:
\vspace{-5pt}
\begin{equation}
\mathcal{L}_{\text{\textit{cross}}} = \frac{1}{2N}{\sum_{k=1}^{N} (C_u(i, 1, 2) + C_f(i, 1, 2)})
\end{equation}

The difference between the frame-level representation in the cross-modal section and the intra-modal section lies in the difference between the masking ratio applied to each branch,with each capturing different aspects of information in each context: cross-modal branch focusing on alignment across modalities and the intra-modal branch focusing on temporal consistency.

\subsection{MSE and Reconstruction Losses}

The MSE and reconstruction losses are tangential to the cross-modal contrastive learning. While the contrastive loss aligns features within and across modalities, the MSE loss focuses specifically on reconstruction fidelity, which helps the model retain finer details in the decoded output and improve the feature embedding of visible tokens in the invariant-space and indirectly contributes to reducing the contrastive loss, and the reconstruction loss on both the original and augmented videos helps the model to generate better feature embedding of visible tokens, which enables more accurate reconstruction of original video and augmented video. With improved feature embedding of visible tokens intra-modal contrastive learning further promotes invariance to augmentations, allowing the model to maintain consistency across augmented views. 

\noindent \textbf{MSE Loss:} The MSE loss is applied at the decoder side to ensure accurate reconstruction of both the original and augmented videos, while minimizing the distance between them. Given the two representations \(f_{e, 1}\) and \(f_{e, 2}\), where \(f_{e, 1} = f_{u}(u_i^{1})\), \(f_{e, 2} = f_{u}(u_i^{2})\), after the decoder attention block, we will obtain two predicted representations, \(f_{d, 1}\) and \(f_{d, 2}\). Then, the decoder prediction is applied between them to get the MSE loss, defined as:
\begin{equation}
\mathcal{L}_{mse} = \frac{1}{N} \sum_{k=1}^{N} (\mathcal{L}_{\textit{pred}}(f_{d, 1}, f_{d, 2}))
\end{equation}
\noindent where N is the batch size and the prediction loss $\ell_{\text{\textit{pred}}}$ is defined as:
\begin{equation}
\mathcal{L}_{\textit{pred}}(f_{d, 1}, f_{d, 2}) = \left\| e_{u}(f_{d, 1}) - e_{u}(f_{d, 2}) \right\|_2^2
\end{equation}

\noindent where the decoder embedding, \(e_{u}(\cdot)\), is a MLP. 

\noindent \textbf{Reconstruction Loss:} The reconstruction loss is calculated using the Mean Squared Error (MSE) between the decoder predicted and target representations. In addition to the MSE (decoder embedding) loss, the decoder performs reconstruction for both the original video and the augmented video. Therefore, the reconstruction loss is defined as:
\begin{equation}
\mathcal{L}_{rl} = \frac{1}{N} \sum_{k=1}^{N} (\left\| u_{i, 1} - \tilde{u}_{i, 1} \right\|_2^2 + \left\| u_{i, 2} - \tilde{u}_{i, 2} \right\|_2^2)
\end{equation}
where N is the batch size, $\tilde{u}_{i, 1}$ and $\tilde{u}_{i, 2}$ are predicted representations.

\subsection{Overall Objective}
\label{sec:objective}

Finally, the overall loss function during pre-training is derived as a combination of contrastive loss c (multiplied by a weight $\lambda_c$), reconstruction loss, and MSE loss. The \(\mathcal{L}_{intra}\) loss ensures invariance to spatiotemporal augmentations, while the \(\mathcal{L}_{cross}\) loss maintains spatiotemporal-spatial correspondence and distills rich semantic information from sampled frames to videos. Additionally, the \(\mathcal{L}_{rl}\) and \(\mathcal{L}_{mse}\) losses are employed to enforce the model to reconstruct the data while preserving intricate relationships within the input. Together, these loss components contribute to a robust and comprehensive pre-training objective, enhancing the model's ability to learn meaningful and discriminative video representations.
\vspace{-5pt}
\begin{equation}
\mathcal{L} = \lambda_c \times (\mathcal{L}_{intra} + \mathcal{L}_{cross}) + \mathcal{L}_{rl} + \mathcal{L}_{mse} 
\end{equation}
\vspace{-5pt}

\begin{table*}[t!]
\vspace{-20pt}
\centering
\scriptsize
\begin{tabular}[t]{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone}} & \textbf{Extra pre-trainining} & \textbf{Param} & \multicolumn{4}{c}{\textbf{Action Full Fine-tuning (Acc@1 (\%))}} \\
& & \textbf{dataset} & \textbf{(M)} & \textbf{UCF101}~\cite{soomro2012ucf101} & \textbf{HMDB51}~\cite{kuehne2011hmdb} & \textbf{SSv2}~\cite{goyal2017something} & \textbf{K400}~\cite{kay2017kinetics} \\
\midrule\midrule
SpeedNet~\cite{benaim2020speednet} & S3D-G & K400 & 9 & 81.1 & 48.8 & — & — \\
Pace Pred~\cite{wang2020self} & R(2+1)D & K400 & 15 & 77.1 & 36.6 & — & — \\
Vi$^2$CLR~\cite{diba2021vi2clr} & S3D & UCF101 & 9 & 82.8 & 52.9 & — & — \\
Vi$^2$CLR~\cite{diba2021vi2clr} & S3D & K400 & 9 & 89.1 & 55.7 & — & — \\
MemDPC~\cite{han2020memory} & R2D3D-34 & K400 & 32 & 86.1 & 54.5 & — & — \\
RSPNet~\cite{chen2021rspnet} & R(2+1)D & K400 & 9 & 81.1 & 44.6 & — & — \\
RSPNet~\cite{chen2021rspnet} & S3D-G & K400 & 9 & 93.7 & 64.7 & — & — \\
VideoMoCo~\cite{pan2021videomoco} & R(2+1)D & K400 & 15 & 78.7 & 49.2 & — & — \\
HiCo~\cite{qing2022learning} & S3D-G & UK400 & — & 91.0 & 66.5 & — & — \\
CVRL~\cite{qian2021spatiotemporal} & SlowOnly-R50 & K400 & 32 & 92.9 & 67.9 & — & — \\
CVRL~\cite{qian2021spatiotemporal} & SlowOnly-R152 & K600 & 32 & 94.4 & 70.6& — & — \\

MIL-NCE~\cite{miech2020end} & S3D-G & HowTo100M & 9 & 91.3 & 61.0 & — & — \\
MMV~\cite{alayrac2020self} & S3D-G & AS+HTM & 9 & 92.5 & 69.6 & — & — \\
CPD~\cite{li2020learning} & ResNet50 & IG300k & — & 92.8 & 63.8 & — & — \\
ELO~\cite{piergiovanni2020evolving} & R(2+1)D & Youtube8M-2 & — & 93.8 & 67.4 & — & — \\
XDC~\cite{alwassel2020self} & R(2+1)D & IG65M & 15 & 94.2 & 67.1 & — & — \\
GDT~\cite{patrick2021multi} & R(2+1)D & IG65M & 15 & 95.2 & 72.8 & — & — \\

\midrule

\multicolumn{6}{l}{\textbf{\textit{Pre-trained Epochs:}}} & \textbf{\textit{800}} & \textbf{\textit{1600}} \\
\midrule\midrule
VIMPAC~\cite{tan2021vimpac} & ViT-L & HowTo100M+DALLE & 307 & 92.7 & 65.9 & 68.1 & 77.4 \\
SVT~\cite{ranasinghe2022self} & ViT-B &IN-21K+K400 & 121 & 93.7 & 67.2 & — & — \\
BEVT~\cite{wang2022bevt} & Swin-B & IN-1K+K400+DALLE & 88 & — & — & 70.6 & 80.6 \\
SpatioTemporalMAE~\cite{feichtenhofer2022masked} & ViT-B & — & 87 & — & — & 68.3 & 81.3 \\
MME~\cite{sun2023masked} & ViT-B & — & 87 & 96.5 & \textcolor{blue}{78.0} & 70.0 & 81.8 \\
VideoMAE~\cite{tong2022videomae} & ViT-B & — & 87 & 90.8 & 61.1 & — & — \\
MAR~\cite{qing2023mar} & ViT-B & — & 87 & 91.0 & 61.4 & — & — \\
\midrule
& & & & \multicolumn{2}{c}{\textbf{\textit{Pre-trained Model: K400}}} & & \\
\midrule
VideoMAE~\cite{tong2022videomae} & ViT-B & — & 87 & 96.1 & 73.3 & 69.3 & 80.9 \\
MAR~\cite{qing2023mar} & ViT-B & — & 87 & 95.9 & 74.1 & 71.0 & 81.0 \\
\midrule
OmniMAE~\cite{girdhar2023omnimae} & ViT-B & IN-1K & 87 & — & — & 69.3 & 80.6 \\
ViC-MAE~\cite{hernandez2023visual} (T.L.) & ViT-B & K(4,6,7)+MiT+IN-1K & 87 & — & — & 69.8 & 80.9 \\
ConvMAE~\cite{gao2022mcmae} & ConvViT-B & — & 86 & — & — & 69.9 & 81.7 \\
AdaMAE~\cite{bandara2023adamae} & ViT-B & — & 87 & — & — & 70.0 & 81.7 \\
MGMAE~\cite{huang2023mgmae} & ViT-B & — & 87 & — & — & 70.6 & 81.2 \\
CMAE-V~\cite{lu2023cmae} & ConvViT-B & — & 87 & — & — & 71.1 & 82.2 \\
MVD-B~\cite{wang2023masked} & Teacher-B & IN-1K & 87 & \textcolor{blue}{97.0} & 76.4 & \textcolor{blue}{72.5} & \textcolor{blue}{82.7} \\
\rowcolor[gray]{0.9}
\textbf{CrossVideoMAE} & \textbf{ViT-B} & \textbf{IN-1K} & \textbf{87*} & \textcolor{red}{97.6} & \textcolor{red}{78.4} & \textcolor{red}{73.7} & \textcolor{red}{83.2} \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{Comparison of our proposed method with state-of-the-art supervised (ref to supplementary material) and self-supervised methods on the UCF101, HMDB51, K400, and SSv2 dataset using the ViT-B/16 backbone. The best results are highlighted in \textcolor{red}{red}, and the second-best results in \textcolor{blue}{blue}. T.L: Transfer Learning. IN: ImageNet dataset. K(4,6,7): Kinetics-400, 600, and 700 datasets. *: shared parameters.}
\label{tab:results}
\vspace{-15pt}
\end{table*}

