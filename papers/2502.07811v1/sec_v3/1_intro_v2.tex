\vspace{-8pt}
\section{Introduction}
\label{sec:intro}




\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{report.jpg}
\captionsetup{font=footnotesize}
\caption{\textbf{Self-attention maps visualization of the proposed approach.}. This demonstrates the efficacy of our method in learning spatiotemporal and semantic representations. The rows depict: original video frames from an action video sequence (\textit{first row}), masked frames with random masking applied (\textit{second row}), reconstructed frames (\textit{third row}), self-attention heatmaps highlighting spatiotemporal representations (\textit{fourth row}), overlaid self-attention heatmaps on reconstructed frames (\textit{fifth row}), and semantic self-attention maps visualizing semantic attributes(\textit{sixth row}). Our approach aim to capture spatiotemporal-spatial feature embedding correspondence of visible tokens across sampled frames and videos,  utilizing differences
between masking ratios (90\% and 95\%), to relate high-level visual and semantic tokens that encode intricate relationships. This joint intra-modal and cross-modal feature embedding at both video and frame level settings enhances invariance to augmentations in the video domain and facilitates effective semantic knowledge distillation from sampled frames to videos. \textit{(ref
supplementary for more visualizations.)}}
\label{fig:report}
\vspace{-20pt}
\end{figure}

Self-supervised learning (SSL) has become a game-changer in reducing reliance on labeled data by harnessing vast quantities of unlabeled information to derive meaningful representations. This approach has shown great promise across domains, including images~\cite{caron2021emerging, radford2021learning, morgado2021audio} and videos~\cite{Jwang2020self, gunawardhana2024effective, dave2024unifying}, establishing a strong foundation for various downstream applications. In particular, video-based action recognition has emerged as an important field with implications for intelligent surveillance, healthcare, and human-computer interaction~\cite{tran2015learning, wang2021tdn, wang2018temporal,zhang2021co}. However, action recognition in video continues to face challenges unique to this medium, such as substantial data redundancy, scene diversity, and the high cost of labeling extensive video datasets~\cite{kumar2023large, kim2024fusion}.

Recent SSL advances for video understanding have partially addressed these challenges through generative models~\cite{chen2020generative, esser2021taming}, reconstruction~\cite{vondrick2016generating,yao2020video} and specialized pretext tasks~\cite{jing2018self, yao2020video, wang2021unsupervised}. Furthermore, contrastive learning, proven highly effective in image-based SSL~\cite{kalantidis2020hard, grill2020bootstrap, he2020momentum, misra2020self}, has been adapted to video data~\cite{huang2021self, qian2021spatiotemporal, pan2021videomoco}. However, many video-based contrastive methods focus on augmentation invariance while overlooking the potential of cross-modal learning, which has demonstrated promise in enriching SSL by integrating complementary data streams from multiple modalities, such as text-image~\cite{desai2021virtex, radford2021learning, sariyildiz2020learning} and audio-video~\cite{morgado2021robust, korbar2018cooperative}.

Existing image- and video-based Masked Autoencoders (MAEs)~\cite{wang2022bevt,girdhar2023omnimae} primarily align low-level visual features with semantic attributes, yet often overlook the intrinsic correlation between sequential frames in video. For example, an existing video-based MAE pre-trained model may misinterpret a Fig.~\ref{fig:report} basketball shot action as a mere arm movement, missing the broader context of the action. This shortcoming highlights a key limitation of current video-based MAEs in action recognition: while they learn spatiotemporal representations, they may focus on isolated spatial details rather than capturing the temporal coherence needed to understand complex actions. Consequently, these models may excel in detecting spatial features but struggle to fully capture the sequential dynamics critical for action-specific contexts.


Encouraged by the intuition of how humans integrate minor scene changes naturally over short video sequences (5â€“10 seconds) without losing context, which is crucial for interpreting dynamic visual content. Cognitive neuroscience research highlights that integrating spatiotemporal information from videos with spatial information from a sequence of frames (from here onward this is referred to as 'spatiotemporal-spatial' unless otherwise stated) is a first capability developed early in infancy~\cite{richards2003development,van1996spatiotemporal}, often before semantic traits are learned~\cite{henderson2003human}. Later, Human visual processing semantically relates action cues across frames, forming a cohesive, action-focused understanding of video content. Inspired by this, we propose \textit{CrossVideoMAE} a cross-modal contrastive learning framework to enhance spatiotemporal and semantic representation learning for video understanding. CrossVideoMAE leverages Masked Image Modeling (MIM)~\cite{bao2022beit, he2022masked} for video to capture correlations between frames and their temporal contexts, integrating semantic attributes without relying on costly language annotations, as evidenced by VARD~\cite{lin2023self}. Through contextual cues within frames, models can detect patterns, objects, interactions, and transitions, achieving high-level semantic understanding even without explicit language data. Temporal relationships among sampled frames further enhance this by learning motion patterns and interactions critical for understanding video dynamics.

CrossVideoMAE extracts mutually correlated spatiotemporal and semantic attributes from videos and sampled frames in a single, end-to-end pre-training phase. Static scene attributes from sampled frames (e.g., "basketball," "hands/arms", "face") complement general action attributes from videos (e.g., "shooting," "trajectory", "arm movement"), enhancing video comprehension. While directly distilling semantic features from state-of-the-art image-based MAEs~\cite{he2022masked, li2022semmae} into video-based models is theoretically possible, significant challenges arise. Unlike image-based MAEs focused on spatial semantics, video-based MAEs must handle temporal complexity, capturing motion, interaction, and continuity. This task is inherently complex, as video-based models must learn from frame sequences, which introduces temporal redundancy and correlation issues absent in static images.

CrossVideoMAE addresses these challenges by independently learning spatial and temporal representations and refining their integration to enhance video content comprehension. Unlike CrossVideo~\cite{liu2024crossvideo}, which emphasizes cross-modal learning, CrossVideoMAE specifically enhances spatiotemporal modelling within a Vision Transformer (ViT)-based MAE framework. To the best of our knowledge, this is the first study to leverage a pre-sampled dataset for SSL in video learning. CrossVideoMAE encodes spatiotemporal-spatial feature correspondences between video sequences and sampled frames, using pre-trained MAEs to embed visible tokens from raw videos, augmented sequences, and frames in a unified feature space with varied masking ratios. This approach enforces spatiotemporal consistency between the two modalities, enabling robust video encoders that capture visual-semantic invariant representations transferable to downstream tasks. Our contributions can be summarized as follows:

\begin{itemize}
\item We show that self-supervised contrastive learning effectively captures spatiotemporal-spatial feature correspondence by enforcing human-like priors on learned concepts, relating video tokens to sampled frames with varied masking.

\item We propose an end-to-end self-supervised contrastive framework that aligns video and frame embeddings, ensuring invariance to augmentations.

\item Our method embeds visible tokens at video and frame levels, capturing correlations and enhancing temporal dynamics understanding.

\item Extensive experiments demonstrate that our approach achieves competitive performance with significantly lower computational resources than existing methods.

\end{itemize}
