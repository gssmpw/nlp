\section{Results}
\label{sec:results}

We evaluate the performance of CrossVideoMAE in action recognition through end-to-end fine-tuning, following established protocols. For the SSv2 and K400 datasets, we apply the methodologies used in previous works~\cite{bao2022beit,he2022masked,feichtenhofer2022masked}, while for UCF101 and HMDB51, we adopt the protocols proposed by Ranasinghe et al.~\cite{ranasinghe2022self}.


\subsection{Comparison with State-of-the-Art Methods}
We compared our method with SOTA video SSL action recognition models on the UCF101, HMDB51, K400, and SSv2 datasets under the full fine-tuning setting (Tab.~\ref{tab:results}). Linear classification results, comparisons with supervised models on K400 and SSv2, and video retrieval results are provided in the supplementary material. Our approach utilizes the ViT-B/16 architecture, with approximately 87 million shared parameters. For inference, we employed multiview testing with \( K \) temporal clips (K = 2 for SSv2 and K = 7 for K400) and 3 spatial views per clip, averaging the results across all views for the final prediction.

CrossVideoMAE consistently outperforms previous methods across all datasets, with the most significant improvements observed on SSv2. This improvement is likely due to the alignment of sampled frames with the dataset's characteristics, which allows for the extraction of rich semantic attributes. On K400, the improvement is less pronounced, potentially due to the random sampling of 5 frames per video, which may not capture temporal dynamics as effectively. We also provide self-attention map visualizations in the supplementary material, illustrating how CrossVideoMAE encourages the model to focus on semantically relevant visual regions.


\begin{table}[h!]
\vspace{-5pt}
\centering
\scriptsize
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \multicolumn{2}{c}{\textbf{Acc@1 (\%)}} \\
& \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
MAE~\cite{he2022masked} / SpatioTemporal MAE~\cite{feichtenhofer2022masked} & 83.60 & 70.0 \\
\rowcolor[gray]{.9}
\textbf{CrossVideoMAE (Ours)} & \textbf{83.62} & \textbf{73.7}\\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{Performance on 1N-1K and SSv2 datasets when combining pre-trained MAE and SpatioTemporalMAE with contrastive learning.}
\label{tab:image_encoder}
\vspace{-10pt}
\end{table}

\noindent \textbf{Image Encoder on Action Recognition:} Experiments on IN-1K~\cite{deng2009imagenet} in Tab.~\ref{tab:image_encoder} demonstrate the capabilities of the pre-trained MAE~\cite{he2022masked}. The performance gain in action recognition on the IN-1K dataset is significantly lower than that on the SSv2 dataset. This difference is likely due to the superior accuracy of temporal information in videos. The integration of spatial representation and motion trajectory in videos provides an advantage in motion analysis for action recognition tasks.

\subsection{Analysis and Ablation Studies}



We conduct ablation studies to validate the effectiveness of CrossVideoMAE. Starting with the pre-trained MAE~\cite{he2022masked} ViT-B/16, we pre-train the video encoder using CrossVideoMAE, then fine-tune it under supervised conditions for all SSv2 experiments. (\textit{See supplementary for additional results})

\noindent\textbf{Number of corresponding sampled frames $(n)$:} 
Tab.~\ref{tab:sampled} examines the impact of the image branch by varying the number of sampled frames. When sampling more than one frame, we compute the mean feature embedding of visible tokens across frames for frame-level cross-modal contrastive learning. CrossVideoMAE effectively captures cross-modal frame-level correspondences with just a single sampled frame, enhancing performance. However, with more than two frames, the added information from the image modality may become redundant.

\begin{table}[h!]
\vspace{-5pt}
\centering
\scriptsize
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{c>{\columncolor[gray]{0.9}}ccccc}
\toprule
\textbf{No. of sampled} & \cellcolor[gray]{0.9} & \multirow{2}{*}{2} & \multirow{2}{*}{3} & \multirow{2}{*}{4} & \multirow{2}{*}{5} \\
\textbf{frame images (n)} & \multirow{-2}{*}{\textbf{1}} \\
\midrule
\textbf{Acc@1 (\%)} & \textbf{73.7} & \textbf{73.7} & 73.5 & 73.4 & 73.1 \\
\bottomrule
\end{tabular}%
}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{Action classification results on SSv2 show that CrossVideoMAE with a single sampled frame (n=1) performs as well or better than using multiple frames. We use n=1 in all experiments.}
\label{tab:sampled}
\vspace{-10pt}
\end{table}

\noindent\textbf{Data Augmentations:}
While self-supervised MAEs~\cite{he2022masked,tong2022videomae,feichtenhofer2022masked} generally use multi-scale cropping alone for pre-training, we explored the effect of additional augmentations, as shown in Tab.~\ref{tab:augment}. We tested random augmentation (resizing, cropping, horizontal flipping), random erasing, mixup, and cutmix. Since masked patches are easier to reconstruct, these augmentations were essential for further performance gains.

\begin{table}[h!]
\vspace{-5pt}
\centering
\scriptsize
\resizebox{\linewidth}{!}{%
\begin{tabular}{cccccc}
\toprule
\textbf{Aug}~\cite{cubuk2020randaugment} & \textbf{Era}~\cite{zhong2020random} & \textbf{MixUp}~\cite{zhang2017mixup} & \textbf{CutMix}~\cite{yun2019cutmix} & \multicolumn{2}{c}{\textbf{Accuracy (\%)}} \\
 & & & & \textbf{Acc@1} & \textbf{Acc@5} \\
\midrule
\ding{55} & $\checkmark$ & $\checkmark$ & $\checkmark$ & 73.72 & 92.67 \\
$\checkmark$ & \ding{55} & $\checkmark$ & $\checkmark$ & 73.51 & \textbf{92.94} \\
$\checkmark$ & $\checkmark$ & \ding{55} & $\checkmark$ & 73.34 & 92.85 \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & \ding{55} & 73.46 & 92.79 \\
\rowcolor[gray]{0.9}
$\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{73.69} & 92.86 \\
\bottomrule
\end{tabular}%
}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{Performance comparison of various data augmentation techniques on the SSv2 dataset for RandomAugment (Aug), Random Erasing (Era), MixUp, and CutMix respectively.}
\label{tab:augment}
\vspace{-10pt}
\end{table}




\noindent\textbf{Masking Ratios:}
As shown in Tab.~\ref{tab:ratios}, CrossVideoMAE achieves optimal performance with masking ratios of 95\% and 90\%, 5\% and 15\% higher than those in SpatioTemporalMAE~\cite{feichtenhofer2022masked} and MAE~\cite{he2022masked}. These higher ratios enhance representation learning by leveraging the added variation from aggressive masking. In contrast, lowering the masking ratio increases visible tokens, limiting the networkâ€™s ability to capitalize on distinctions introduced by high masking, reducing its capacity to capture semantic features from sampled frames.


\begin{table}[h!]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}[t]{cccc}
\toprule
\multicolumn{2}{c}{\textbf{Mask Ratios}} & \multicolumn{2}{c}{\textbf{Acc@1 (\%)}} \\
\textbf{Image Branch} & \textbf{Video Branch} & \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
75\% & 75\% & 83.2 & 72.9 \\
75\% & 90\% & 83.5 & 73.2 \\
75\% & 95\% & 83.4 & 73.4 \\
90\% & 75\% & 83.3 & 72.6 \\
90\% & 90\% & 83.5 & 73.5 \\
\rowcolor[gray]{.9}
\textbf{90\%} & \textbf{95\%} & \textbf{83.6} & \textbf{73.7} \\
\bottomrule
\end{tabular}%
}
\vspace{-5pt}
\caption{Impact of different masking ratios on the image and video branches for action classification accuracy.}
\label{tab:ratios}
\vspace{-10pt}
\end{table}



\noindent\textbf{Impact of Joint Learning Objective:}
As shown in Tab.~\ref{tab:joint} and Section~\ref{sec:method}, our joint feature embedding strategy enhances the model's ability to capture correlations across frame sequences and full videos. By combining intra-modal and cross-modal contrastive learning at both frame and video levels, the model achieves more transferable representations. Ablation studies on SSv2 indicate that removing cross-modal and intra-modal contrastive learning reduces accuracy by 0.7 percentage points (\%p) and 0.5\%p, respectively. Additionally, omitting frame- or video-level objectives results in further accuracy drops of 0.3\%p and 0.4\%p.

\begin{table}[h!]
\vspace{-5pt}
\centering
\scriptsize
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{\textbf{Different Modal}} & \multicolumn{2}{c}{\textbf{Different Level}} & \textbf{Acc@1 (\%)} \\
\textbf{Intra Modal} & \textbf{Cross Modal} & \textbf{Video Level} & \textbf{Frame Level} & \\
\midrule
\ding{55} & \ding{55} & \checkmark & \checkmark & 70.94 \\
\checkmark & \ding{55} & \checkmark & \checkmark & 72.96 \\
\ding{55} & \checkmark & \checkmark & \checkmark & 72.87 \\
\checkmark & \checkmark & \ding{55} & \ding{55} &72.65 \\
\checkmark & \checkmark & \ding{55} & \checkmark & 73.28 \\
\checkmark & \checkmark & \checkmark & \ding{55} & 73.39 \\
\rowcolor[gray]{0.9}
\checkmark & \checkmark & \checkmark &\checkmark & \textbf{73.70} \\
\bottomrule
\end{tabular}%
}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{Effect of different modalities and levels on classification accuracy using a joint learning objective}
\label{tab:joint}
\vspace{-10pt}
\end{table}

\noindent\textbf{Transfer Learning:}
Tab.~\ref{tab:transfer} showcases the transfer learning effectiveness of our CrossVideoMAE pre-trained model across different datasets for action classification. When fine-tuned on SSv2, our K400-pretrained model achieves a state-of-the-art 73.5\% Acc@1. Similarly, with SSv2 pre-training, it attains 83.0\% Acc@1 on K400, outperforming other MAEs.

\begin{table}[h!]
\centering
\scriptsize
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}[t]{lcccc}
\toprule
\textbf{Pre-train Set} & \textbf{\# Pre-train Data} & \textbf{Fine-tune Set} & \textbf{Acc@1 (\%)} \\
\midrule
K400 & 240k & SSv2 & 73.5 \\
SSv2 & 169k & K400 & 83.0 \\
\bottomrule
\end{tabular}%
}
\vspace{-5pt}
\captionsetup{font=footnotesize}
\caption{ Performance comparison of domain adaptation/transfer learning on different datasets using various pre-training methods.}
\label{tab:transfer}
\vspace{-10pt}
\end{table}


