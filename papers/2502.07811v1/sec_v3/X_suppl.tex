\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\setcounter{section}{0}
\setcounter{figure}{0}   
\setcounter{table}{0}   
\renewcommand{\thesection}{\Alph{section}}

\noindent We organize the Supplementary Materials as follows:


\begingroup
\begin{itemize}
\item \textbf{The overall architecture of our proposed method \S~\ref{sec:overall}}
\item \textbf{The implementation details \S~\ref{sec:implement}.}
\item \textbf{Additional experimental results and analysis \S~\ref{sec:additional} }
\end{itemize}
\endgroup


\section{Overall Architecture of CrossVideoMAE}
\label{sec:overall}

\begin{table*}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lcccc}
\toprule
\textbf{Stage} & \multicolumn{2}{c}{\textbf{ViT-Base/16 Configuration}} & \multicolumn{2}{c}{\textbf{Output Sizes}} \\ 
& \textbf{Image Branch} & \textbf{Video Branch} & \textbf{Image Branch} & \textbf{Video Branch} \\
& Pre-trained MAE~\cite{he2022masked} & SpatioTemporalMAE~\cite{feichtenhofer2022masked} & & \\
\midrule
Input Image/Video & \ding{55} & 
\begin{tabular}[c]{c} 
Stride $\textcolor{blue}{4} \times \textcolor{teal}{1} \times \textcolor{teal}{1}$ on K400 \\ 
Stride $\textcolor{blue}{2} \times \textcolor{teal}{1} \times \textcolor{teal}{1}$ on SSv2 
\end{tabular} 
& $\textcolor{red}{3} \times \textcolor{teal}{224} \times \textcolor{teal}{224}$ 
& $\textcolor{red}{3} \times \textcolor{blue}{16} \times \textcolor{teal}{224} \times \textcolor{teal}{224}$ \\ 

Patch Embedding & $\textcolor{red}{3} \times \textcolor{teal}{16} \times \textcolor{teal}{16}$, Embedding Dim. $\textcolor{red}{768}$ & 
\begin{tabular}[c]{c} 
$\textcolor{blue}{2} \times \textcolor{red}{3} \times \textcolor{teal}{16} \times \textcolor{teal}{16}$, Embedding Dim. $\textcolor{red}{768}$ \\ 
Stride $\textcolor{blue}{2} \times \textcolor{teal}{16} \times \textcolor{teal}{16}$ 
\end{tabular} 
& $\textcolor{red}{768} \times \textcolor{teal}{14} \times \textcolor{teal}{14}$ 
& $\textcolor{red}{768} \times \textcolor{blue}{8} \times \textcolor{teal}{14} \times \textcolor{teal}{14}$ \\ 

Mask & 
\begin{tabular}[c]{c} 
Random Mask \\ Mask Ratio = $\textcolor{orange}{\rho}$ 
\end{tabular} 
& 
\begin{tabular}[c]{c} 
Random Mask \\ Mask Ratio = $\textcolor{orange}{\rho}$ 
\end{tabular} 
& $\textcolor{red}{768} \times [\textcolor{teal}{14} \times \textcolor{teal}{14} \times (1 - \textcolor{orange}{\rho})]$ 
& $\textcolor{red}{768} \times \textcolor{blue}{8} \times [\textcolor{teal}{14} \times \textcolor{teal}{14} \times (1 - \textcolor{orange}{\rho})]$ \\ 

Encoder & 
$\left[\begin{array}{c} 
\mathrm{MHA}(\textcolor{red}{768}) \\ 
\mathrm{MLP}(\textcolor{red}{3072}) 
\end{array} \right] \times 12$ 
& 
$\left[\begin{array}{c} 
\mathrm{MHA}(\textcolor{red}{768}) \\ 
\mathrm{MLP}(\textcolor{red}{3072}) 
\end{array} \right] \times 12$ 
& $\textcolor{red}{768} \times [\textcolor{teal}{14} \times \textcolor{teal}{14} \times (1 - \textcolor{orange}{\rho})]$ 
& $\textcolor{red}{768} \times \textcolor{blue}{8} \times [\textcolor{teal}{14} \times \textcolor{teal}{14} \times (1 - \textcolor{orange}{\rho})]$ \\ 

Encoder Embedding & 
\begin{tabular}[c]{c} 
$\mathrm{MLP}(\textcolor{red}{384})$ \\ 
\textit{concat learnable tokens} 
\end{tabular} 
& 
\begin{tabular}[c]{c} 
$\mathrm{MLP}(\textcolor{red}{384})$ \\ 
\textit{concat learnable tokens} 
\end{tabular} 
& $\textcolor{red}{384} \times \textcolor{teal}{14} \times \textcolor{teal}{14}$ 
& $\textcolor{red}{384} \times \textcolor{blue}{8} \times \textcolor{teal}{14} \times \textcolor{teal}{14}$ \\ 

Decoder & \ding{55} & 
$\left[\begin{array}{c} 
\mathrm{MHA}(\textcolor{red}{384}) \\ 
\mathrm{MLP}(\textcolor{red}{1536}) 
\end{array} \right] \times 4$ 
& \ding{55} 
& $\textcolor{red}{384} \times \textcolor{blue}{8} \times [\textcolor{teal}{14} \times \textcolor{teal}{14} \times (1 - \textcolor{orange}{\rho})]$ \\ 

Decoder Embedding & \ding{55} & 
$\mathrm{MLP}(\textcolor{red}{1536})$ 
& \ding{55} 
& $\textcolor{red}{1536} \times \textcolor{blue}{8} \times \textcolor{teal}{14} \times \textcolor{teal}{14}$ \\ 

Reshape & \ding{55} & 
from $\textcolor{red}{1536}$ to $\textcolor{red}{3} \times \textcolor{blue}{2} \times \textcolor{teal}{16} \times \textcolor{teal}{16}$ 
& \ding{55} 
& $\textcolor{red}{3} \times \textcolor{blue}{16} \times \textcolor{teal}{224} \times \textcolor{teal}{224}$ \\ 
\bottomrule
\end{tabular}%
}
\caption{\textbf{Encoder and Decoder Architectural Details of CrossVideoMAE.} We take 16-frame vanilla shared, pre-trained MAE ViT-B/16. "MHA" denotes joint space-time self-attention. The output sizes are denoted by $\{\textcolor{red}{C}\times\textcolor{blue}{T}\times\textcolor{teal}{S}\}$ for channel, temporal, and spatial sizes.}
\label{tab:table1}
\end{table*}


\begin{table*}[h!]
\centering
\scriptsize
\resizebox{0.6\linewidth}{!}{
\begin{tabular}[t]{lccc}
\toprule
\textbf{config} & \textbf{Image Branch} & \multicolumn{2}{c}{\textbf{Video Branch}} \\
& \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{K400}~\cite{kay2017kinetics} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
optimizer & AdamW~\cite{loshchilov2018decoupled} & \multicolumn{2}{c}{AdamW~\cite{loshchilov2018decoupled}} \\
base learning rate & 1.5e-4 & \multicolumn{2}{c}{1.5e-4} \\
weight decay & 0.05 & \multicolumn{2}{c}{0.05} \\
optimizer momentum & $\beta_1, \beta_2=0.9, 0.95$~\cite{chen2020generative} & \multicolumn{2}{c}{$\beta_1, \beta_2=0.9, 0.95$~\cite{chen2020generative}} \\
learning rate schedule & cosine decay~\cite{loshchilov2016sgdr} & \multicolumn{2}{c}{cosine decay~\cite{loshchilov2016sgdr}} \\
warmup epochs~\cite{goyal2017accurate} & 40 & \multicolumn{2}{c}{40} \\
Augmentations: & & & \\
 ShortSideScale & N/A & \multicolumn{2}{c}{256px} \\
 RandomResizedCrop & & & \\
\qquad size & 224px & \multicolumn{2}{c}{224px} \\
\qquad scale & [0.08, 1.0] & \multicolumn{2}{c}{[0.08, 1.0]} \\
\qquad ratio & [0.75, 1.33] & \multicolumn{2}{c}{[0.75, 1.33]} \\
\qquad interpolation & Bicubic & \multicolumn{2}{c}{Bilinear} \\
 RandomHorizontalFlip & $\rho$ = 0.5 & $\rho$ = 0.5 & $\rho$ = 0 \\
 Normalize & yes & \multicolumn{2}{c}{yes} \\
\bottomrule
\end{tabular}
}

\caption{Pre-training setting on IN-1K, K400 and SSv2 datasets.}
\label{tab:pre-train}
\end{table*}

\begin{table*}[h!]
\centering
\scriptsize
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}[t]{lcccc}
\toprule
\textbf{config} & \textbf{Image Branch} & \multicolumn{3}{c}{\textbf{Video Branch}} \\
& \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{K400}~\cite{kay2017kinetics} & \textbf{SSv2}~\cite{goyal2017something} & \textbf{UCF101~\cite{soomro2012ucf101} + HMDB51~\cite{kuehne2011hmdb}} \\
\midrule
optimizer & AdamW & \multicolumn{3}{c}{AdamW} \\
base learning rate & 1e-3 & 5e-4 & 1e-3 & 1.5e-4 \\
weight decay & 0.05 & \multicolumn{3}{c}{0.05} \\
optimizer momentum & $\beta_1=0.9, \beta_2=0.999$ & \multicolumn{3}{c}{$\beta_1=0.9, \beta_2=0.999$} \\
learning rate schedule & cosine decay & \multicolumn{3}{c}{cosine decay} \\
warmup epochs & 5 & \multicolumn{3}{c}{5} \\
Augmentations: & & & \\
 ShortSideScale & N/A & \multicolumn{3}{c}{256px} \\
 RandomResizedCrop & & & \\
\qquad size & 224px & \multicolumn{3}{c}{224px} \\
\qquad scale & [0.08, 1.0] & \multicolumn{3}{c}{[0.08, 1.0]} \\
\qquad ratio & [0.75, 1.33] & \multicolumn{3}{c}{[0.75, 1.33]} \\
\qquad interpolation & Bicubic & \multicolumn{3}{c}{Bilinear} \\
 Repeated Augmentation~\cite{hoffer2020augment} & N/A & \multicolumn{3}{c}{2} \\
 RandomHorizontalFlip & $\rho$ = 0.5 & $\rho$ = 0.5 & $\rho$ = 0 & $\rho$ = 0.5 \\
 RandAugment~\cite{cubuk2020randaugment} & & \\
\qquad magnitude & 9 & \multicolumn{3}{c}{9} \\
\qquad num\_layers & 0.5 & \multicolumn{3}{c}{0.5} \\
 RandomErasing & $\rho$ = 0.25 & $\rho$ = 0 & $\rho$ = 0.25 & $\rho$ = 0.5 \\
 Normalize & yes & \multicolumn{3}{c}{yes} \\
 label smoothing~\cite{szegedy2016rethinking} & 0.1 & \multicolumn{3}{c}{0.1} \\
 mixup~\cite{zhang2017mixup} & 0.8 & \multicolumn{3}{c}{0.8} \\
 cutmix~\cite{yun2019cutmix} & 1.0 & \multicolumn{3}{c}{1.0} \\
 drop path & 0.1 & \multicolumn{3}{c}{0.1} \\
 dropout & 0.1 & \multicolumn{3}{c}{0.1} \\
layer-wise lr decay~\cite{bao2022beit,clark2020electra} & 0.75 & \multicolumn{3}{c}{0.75} \\ 
\bottomrule
\end{tabular}%
}
\caption{End-to-end fine-tuning setting on IN-1K, K400 and SSv2 datasets.}
\label{tab:fine-tune}
\end{table*}


\subsection{Video Branch and Image Branch} 

\subsubsection{Video Branch}

Given a video, we first perform data augmentation to obtain an augmented version of the video. 

\noindent\textbf{Tokenizer:} Given an input video \( u \) of size \( \textcolor{blue}{T} \times \textcolor{red}{C} \times \textcolor{teal}{H} \times \textcolor{teal}{W} \), where \( \textcolor{blue}{T} \) represents the temporal sequence length (frames), \( \textcolor{red}{C} \) is the number of channels, and \( \textcolor{teal}{H}, \textcolor{teal}{W} \) are the spatial dimensions (height and width), we first process it using a patch embedding operation. This involves passing \( u \) through a 3D convolutional layer with a kernel of size \( K = (\textcolor{blue}{t}, \textcolor{red}{C}, \textcolor{teal}{h}, \textcolor{teal}{w}) \), where \( \textcolor{blue}{t} \), \( \textcolor{teal}{h} \), and \( \textcolor{teal}{w} \) define the temporal stride, height, and width dimensions of the kernel, respectively. The convolution uses a stride \( S = (\textcolor{blue}{t}, \textcolor{teal}{h}, \textcolor{teal}{w}) \) and outputs \( \textcolor{red}{D} \) channels. This operation embedding the input video into \( N_u = \textcolor{blue}{\frac{T}{t}} \times \textcolor{teal}{\frac{H}{h}} \times \textcolor{teal}{\frac{W}{w}} \) tokens, each represented as a vector of dimension \( D \).



\noindent\textbf{Positional Encoding:} Positional information is then added to the tokens \( N_u \) to retain their spatial and temporal context.

\noindent\textbf{Masking:} Randomly mask \( M_u \) tokens out of the total \( N_u \) tokens.

\noindent\textbf{Encoder:} Next, we generate feature embedding \(f_{\theta_u}(\cdot)\) of visible tokens by passing \(N_u - M_u\) visible tokens with positional information through the transformer ViTEncoder. 


\noindent\textbf{Decoder:} The feature embedding of the visible tokens is concatenated with a set of fixed, learnable feature embeddings of the masked tokens \( M_u \) to generate the combined embeddings. Positional encodings are then added to both the visible and masked token embeddings. This combined representation is passed through a lightweight transformer-based ViTDecoder, which is trained using the Mean Squared Error (MSE) loss. The loss is computed between the reconstructed tokens of the video and its augmented counterpart, denoted as \( \tilde{u}_i \) and \( \tilde{u}_i^t \), ensuring accurate reconstruction of the input tokens.

\subsubsection{Frame Image Branch} 


Similarly, for the image branch, a set of random frames is manually sampled from the video to generate corresponding sampled frame images.



\noindent\textbf{Tokenizer:} Given an input sampled frame \( f \) of size \(\textcolor{red}{C} \times \textcolor{teal}{H} \times \textcolor{teal}{W}\), where \(\textcolor{teal}{H}\) and \(\textcolor{teal}{W}\) represent the spatial dimensions and \(\textcolor{red}{C}\) denotes the number of channels, we first pass \( f \) through a Patch Embedding layer. This layer is implemented as a 3D convolution with a kernel size of \( K_f = (\textcolor{red}{C}, \textcolor{teal}{h}, \textcolor{teal}{w}) \), producing \(\textcolor{red}{D}\) output channels. This operation embeds \( f \) into \( N_f = \textcolor{teal}{\frac{H}{h}} \times \textcolor{teal}{\frac{W}{w}} \) tokens, each with a dimension of \( D \).


\noindent\textbf{Positional Encoding:} Positional information is then added to the tokens \(N_f\) to retain their spatial context.

\noindent\textbf{Masking:} Randomly mask \( M_f \) tokens out of the total \( N_f \) tokens.

\noindent\textbf{Encoder:} Next, we generate feature embedding \(f_{\theta_f}(\cdot)\) by passing \(N_f - M_f\) visible tokens with positional information through the pre-trained MAE~\cite{he2022masked} transformer ViTEncoder.

\subsection{Architecture Details}
\label{sec:detailed}

Tab.~\ref{tab:table1} details the architecture of the encoder and decoder of our CrossVideoMAE. Specifically, we take the 16-frame vanilla shared, pre-trained ViT-B/16 for all experiments. We use an asymmetric encoder-decoder architecture for self-supervised cross-modal video pre-training and discard the decoder during the fine-tuning phase. We adopt the joint space-time attention~\cite{arnab2021vivit,liu2022video} to capture the rich spatiotemporal representations and semantic attributes in the visible tokens. 

Given a video, we first extract \textcolor{blue}{16} frames (\textcolor{red}{3} $\times$ \textcolor{blue}{16} $\times$ \textcolor{teal}{224} $\times$ \textcolor{teal}{224}). These frames are extracted uniformly at regular intervals for both datasets, as outlined in previous work~\cite{feichtenhofer2022masked}. We use a temporal stride of 4 and 2 for the K400 and SSv2 datasets, respectively. Next, we process this \textcolor {blue}{16} frames through Patch Embedding, which is essentially a convolution layer with a kernel size of \textcolor{blue}{2} $\times$ \textcolor{red}{3} $\times$ \textcolor{teal}{16} $\times$ \textcolor{teal}{16}, the stride of \textcolor{blue}{2}$\times$\textcolor{teal}{16}$\times$\textcolor{teal}{16}, and output embedding dimension of \textcolor{red}{768}.This process results in a total of \textcolor{purple}{1568} tokens, and each token is represented by a \textcolor{red}{768} dimensional vector. A standard positional encoding vector is added to the embedded patches. Next, we mask $M_u = \textcolor{orange}{\rho_u}\times\textcolor{purple}{1568}$ number of tokens and proceed $N_u - M_u = (1 - \textcolor{orange}{\rho_u})\times\textcolor{purple}{1568}$ as the visible tokens. $\rho_u$ denotes the masking ratio applied to the video branch. These visible tokens are then processed through the shared MAE ViT video encoder that comprises 12 cascaded multi-head self-attention blocks (MHA blocks). The shared MAE ViT video encoder outputs are then concatenated with a fixed learnable representation for masked tokens, resulting in the \textcolor{purple}{1568} token representations. This \textcolor{purple}{1568} representations are then processed through an encoder embedding which brings down their embedding dimension from \textcolor{red}{768} to \textcolor{red}{384} by an MLP layer. These embedded representations are then processed through the shared MAE ViT-decoder which consists of 4 MHA blocks followed by an MLP layer to bring the embedding dimension from \textcolor{red}{384} to \textcolor{red}{1536} to compute the MSE loss, and the total number of pixels in a cube which is given by $\textcolor{red}{2}\times\textcolor{blue}{3} \times\textcolor{teal}{16}\times\textcolor{teal}{16} = \textcolor{red}{1536}$. This is finally reshaped back to the original space and used to compute the reconstruction loss. 

Given a sampled frame (\textcolor{red}{3} $\times$ \textcolor{teal}{224} $\times$ \textcolor{teal}{224}), we first process this through patch embedding, which is essentially a convolution layer with a kernel size of \textcolor{teal}{16}$\times$\textcolor{teal}{16}, and output embedding dimension of \textcolor{red}{768}. A standard positional encoding vector is added to the embedded patches and fed into the encoder. This process results in a total of \textcolor{purple}{196} tokens, and each token is represented by a \textcolor{red}{768} dimensional vector. Next, we mask $M_f = \textcolor{orange}{\rho_f}\times\textcolor{purple}{196}$ number of tokens and proceed $N_f - M_f = (1 - \textcolor{orange}{\rho_f})\times\textcolor{purple}{196}$ as the visible tokens. $\rho_f$ denotes the masking ratio applied to the frame image branch. These visible tokens are then processed through the pre-rained MAE~\cite{he2022masked} ViT image encoder that comprises 12 cascaded multi-head self-attention blocks (MHA blocks). These visible tokens are then processed through an encoder embedding which brings down their embedding dimension from \textcolor{red}{768} to \textcolor{red}{384} by an MLP layer. This pre-trained MAE~\cite{he2022masked} ViT image encoder learned visible tokens: $N_f - M_f$ representations are then processed through an encoder embedding which brings down their embedding dimension from \textcolor{red}{768} to \textcolor{red}{384} by an MLP layer. 


These encoder-embedded features facilitate spatiotemporal-spatial feature embedding correspondence by maximizing mutual information between video, augmented video, and sampled frames. Visible tokens in the feature-invariant space are processed in a self-supervised fashion, promoting invariance to augmentations in the video domain. Furthermore, this process distills well-learned knowledge from sampled frames to videos through intra-modal, cross-modal, frame-level, and video-level contrastive learning. This approach enables the model to effectively capture visual concepts, ensure view invariance, and extract semantic attributes analogous to human perception.

\section{Implementation Details}
\label{sec:implement}


We followed the pre-training configurations outlined in previous works, such as MAE ~\cite{he2022masked} and SpatioTemporalMAE ~\cite{feichtenhofer2022masked}. 

\subsection{Datasets}

We evaluated our method on four video datasets commonly used for action recognition: 
Kinetics-400 (K400)~\cite{kay2017kinetics} Something-Something V2 (SSv2)~\cite{goyal2017something}, UCF101~\cite{soomro2012ucf101}, and HMDB51~\cite{kuehne2011hmdb}. 

K400: contains video clips from YouTube, around 240k training videos, and 20k validation videos of 10s from 400 action classes.

SSv2: is also a large-scale video dataset, having around 169k videos for training and 20k videos for validation of 4s, categorized into 174 motion-centric action classes. We conducted ablation studies on the SSv2 dataset and reported results on both K400 and SSv2 datasets.

UCF101: is a relatively small dataset, consisting of $\sim$9.5K training videos and $\sim$3.5K validation videos.

HMDB51: is also a small video dataset that contains around 3.5K/1.5K train/val videos. On UCF101 and HMDB51, we follow the commonly used protocols and evaluate our method across all 3 train/val splits. 

ImageNet-1K (IN-1K)~\cite{russakovsky2015imagenet} We use the ILSVRC 2012 challenge subset, which includes 1.28M training and 50K validation images spanning 1000 classes. 




\begin{table}[!h]
\centering
\begin{tabular}[t]{cc}
\toprule
\textbf{Config} & \textbf{SSv2} \\ 
\midrule
optimizer & SGD \\ 
base learning rate & 0.1 \\ 
weight decay & 0 \\ 
optimizer momentum & 0.9 \\ 
learning rate schedule & cosine decay \\
warmup epochs & 10 \\ 
training epochs & 100 \\ 
augmentation & MultiScaleCrop \\ 
\bottomrule
\end{tabular}%

\caption{Linear probing setting.}
\label{tab:linear}
\end{table}

\begin{table*}[!t]
\centering
\scriptsize
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}[t]{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Modality}} & \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{Extra Data}} & \multicolumn{4}{c}{\textbf{Action Linear Classification (Acc@1 (\%))}} \\
& & & &\textbf{UCF101} & \textbf{HMDB51} & \textbf{K400} & \textbf{SSv2} \\

\midrule
MoCo~\cite{he2020momentum} & V & R50 & UCF101 & 65.4 & — & 34.5 & 7.4 \\
CoCLR-RGB~\cite{han2020self} & V & R(2+1)D & UCF101 & 74.5 & 46.1 & — & — \\
CVRL~\cite{qian2021spatiotemporal} & V & SlowOnly-R50 & K400 & 89.8 & 58.3 & 66.1 & — \\
$\rho$BYOL~\cite{feichtenhofer2021large} & V & SlowOnly-R50 & K400 & 90.1 & 61.1 & 68.3 & 24.5 \\
VideoMoCo~\cite{pan2021videomoco} & V & R(2+1)D & K400 66.3 & — & 31.0 & 19.5 \\
CORP\textsubscript{f}~\cite{hu2021contrast}& V & SlowOnly-R50 & K400 & 90.2 & 58.7 & 66.6 & — \\
Vi$^2$CLR~\cite{diba2021vi2clr} & V & S3D & K400 & 75.4 & 47.3 & 63.4 & — \\
GDT~\cite{patrick2021multi}& V + A & R(2+1)D & IG65M & 75.7 & — & 38.6 & 11.9 \\
TimeSformer~\cite{bertasius2021space} & V & ViT-B & IN-21K & — & — & 14.0 & — \\
SVT~\cite{ranasinghe2022self} & V & ViT-B & IN-21K+K400 & 90.8 & 57.8 & \textcolor{blue}{68.1} & 18.3 \\
ViMPAC~\cite{tan2021vimpac} & V + I & ViT-L & HowTo100M+DALLE & — & — \\
VideoMAE~\cite{tong2022videomae} & V & ViT-B & K400 & 84.6 & 60.5 & 61.2 & 23.1 \\
MME~\cite{sun2023masked} & V & ViT-B & K400 & — & — & — & \textcolor{blue}{29.2} \\
MVD-B~\cite{wang2023masked} & V + I & Teacher-B & IN-1K + K400 & \textcolor{blue}{97.0} & \textcolor{blue}{76.4} & — & — \\
\midrule
\rowcolor[gray]{0.9}
\textbf{CrossVideoMAE} & V + I & ViT-B & IN-1K + K400 & \textcolor{red}{97.6} & \textcolor{red}{76.9} & \textcolor{red}{68.7}& \textcolor{red}{31.2}\\
\bottomrule
\end{tabular}%
}
\caption{Comparison with state-of-the-art methods on UCF101, HMDB51, K400 and SSv2 for linear probing. ‘A’ is audio, and ‘I’ is image. The best and second best results are marked by \textcolor{red}{red} and \textcolor{blue}{blue} colours, respectively.}
\label{tab:action_linear}
\end{table*}



\begin{table*}[!h]

\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}[t]{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone}} & \textbf{Extra pre-training} & \multirow{2}{*}{\textbf{Extra labels}} & \multirow{2}{*}{\textbf{Frames}} & \textbf{GFLOPs (G)} & \textbf{Param} & \textbf{Acc@1} & \textbf{Acc@5} \\
& & \textbf{dataset} & & & \textbf{FLOPs}$\times$\textbf{Clips}$\times$\textbf{Crops} & \textbf{(M)} & \textbf{(\%)} & \textbf{(\%)} \\
\midrule\midrule
\multicolumn{7}{l}{\textbf{\textit{Category: Supervised Pre-training}}} \\
\midrule
TSM\textsubscript{$two$ $stream$}~\cite{lin2019tsm} & ResNet50\textsubscript{$\times$2} & \multirow{4}{*}{IN-1K} & \checkmark & 16+16 & 130$\times$2$\times$3 & 49 & 66.0 & 90.5 \\
TEINet\textsubscript{$En$}~\cite{liu2020teinet} & ResNet50\textsubscript{$\times$2} & & \checkmark & 8+16 & 99$\times$10$\times$3 & 50 & 66.6 & N/A \\
TANet\textsubscript{$En$}/TAM~\cite{liu2021tam}& ResNet50\textsubscript{$\times$2} & & \checkmark & 8+16 & 99$\times$2$\times$3 & 51 & 66.0 & 90.1 \\
TDN\textsubscript{$En$}~\cite{wang2021tdn} & ResNet101\textsubscript{$\times$2} & & \checkmark & 8+16 & 198$\times$1$\times$3 & 88 & 69.6 & 92.2 \\
\midrule
SlowFast~\cite{feichtenhofer2019slowfast} & ResNet101 & \multirow{2}{*}{K-400} & \checkmark & 8+32 & 106$\times$1$\times$3 & 53 & 63.1 & 87.6 \\
MViTv1~\cite{fan2021multiscale} & MViTv1-B & & \checkmark & 64 & 455$\times$1$\times$3 & 37 & 67.7 & 90.9 \\
\midrule
TimeSformer~\cite{bertasius2021space} & ViT-B & \multirow{2}{*}{IN-21K} & \checkmark & 8 & 196$\times$1$\times$3 & 121 & 59.5 & N/A \\
TimeSformer~\cite{bertasius2021space} & ViT-L & & \checkmark & 64 & 5549$\times$1$\times$3 & 430 & 62.4 & N/A \\
\midrule
ViViT FE~\cite{arnab2021vivit} & ViT-L & IN-21K+K400 & \checkmark & 32 & 995$\times$4$\times$3 & N/A & 65.9 & 89.9 \\
TAdaConvNeXt-T~\cite{huang2021tada} & ConvNeXt-T & IN-1K & \checkmark & 32 & 94$\times$3$\times$2 & 38 & 67.1 & 90.4 \\
\midrule
Motionformer~\cite{patrick2021keeping} & ViT-B & \multirow{3}{*}{IN-21K+K400} & \checkmark & 16 & 370$\times$1$\times$3 & 109 & 66.5 & 90.1 \\
Motionformer~\cite{patrick2021keeping} & ViT-L & & \checkmark & 32 & 1185$\times$1$\times$3 & 382 & 68.1 & 91.2 \\
Video Swin~\cite{liu2020teinet} & Swin-B & & \checkmark & 32 & 321$\times$1$\times$3 & 88 & 69.6 & 92.7 \\
\midrule\midrule
\multicolumn{7}{l}{\textbf{\textit{Category: Self-Supervised Pre-training}}} \\
\midrule
\hdashline
\multicolumn{7}{l}{\textbf{\textit{Pre-trained Epochs: 800}}} \\
\hdashline
\rowcolor[gray]{0.9}
\textbf{CrossVideoMAE (Ours)} & \textbf{ViT-B} & IN-1K & \ding{55} & \textbf{16} & 180$\times$2$\times$3 & 87 (Shared) & \textcolor{red}{73.7} & \textcolor{red}{93.4} \\
\bottomrule
\end{tabular}%
}

\caption{Comparison of our proposed method with supervised SOTA methods on SSv2 dataset. We use ViT-B/16 backbone. Extra labels \ding{55} denotes only unlabeled data used for the pre-training phase. The N/A denotes these numbers as not being available/reported in the paper. The best result is marked by \textcolor{red}{red} colour.}
\label{tab:supervised_ssv2}
\end{table*}



We conduct the experiments with the pre-trained models adopted from open-source repositories (
MAE ~\cite{he2022masked} and SpatioTemporalMAE ~\cite{feichtenhofer2022masked}) and fine-tuning on the K400, SSv2, UCF101, HMDB51, and IN-1K datasets.









\subsection{Pre-training} 

The default settings for pre-training and end-to-end finetuning on IN-1K, K400, and SSv2 datasets are shown in Tab.~\ref{tab:pre-train} and Tab.~\ref{tab:fine-tune}. We use the pre-trained model on the Kinetics-400 [1600 epochs] and then transfer it to the UCF101 and HMDB51. The default
settings of fine-tuning for 100 epochs and 50 epochs, respectively, are shown in Tab.~\ref{tab:fine-tune}. 


Tab.~\ref{tab:pre-train} details the pre-training setting on IN-1K, K400, and SSv2 datasets. In addition, we linearly scale the base learning rate w.r.t the overall batch size, $\textit{\text{lr}} = \textit{\text{base\_learning\_rate}} \times \text{\textit{batchsize} / 256}$~\cite{goyal2017accurate}. We adopt the PyTorch and DeepSpeed frameworks for faster training.




\begin{table*}[h!]

\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}[t]{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone}} & \textbf{Extra pre-trainining} & \multirow{2}{*}{\textbf{Extra labels}} & \multirow{2}{*}{\textbf{Frames}} & \textbf{GFLOPs (G)} & \textbf{Param} & \textbf{Acc@1} & \textbf{Acc@5} \\
& & \textbf{dataset} & & & \textbf{FLOPs}$\times$\textbf{Clips}$\times$\textbf{Crops} & \textbf{(M)} & \textbf{(\%)} & \textbf{(\%)} \\
\midrule\midrule
\multicolumn{7}{l}{\textbf{\textit{Category: Supervised Pre-training}}} \\
\midrule
\midrule
NonLocal I3D~\cite{wang2018non} & ResNet101 & \multirow{5}{*}{IN-1K} & \checkmark & 128 & 359$\times$10$\times$3 & 62 & 77.3 & 93.3 \\
TAdaConvNeXt-T~\cite{huang2021tada} & ConvNeXt-T & & \checkmark & 32 & 94$\times$3$\times$4 & 38 & 79.1 & 93.7 \\
TANet/TAM~\cite{liu2021tam} & ResNet152 & & \checkmark & 16 & 242$\times$4$\times$3 & 59 & 79.3 & 94.1 \\
TDN\textsubscript{$En$}~\cite{wang2021tdn} & ResNet101\textsubscript{$\times$2} & & \checkmark & 8+16 & 198$\times$10$\times$3 & 88 & 79.4 & 94.4 \\
Video Swin~\cite{liu2020teinet} & Swin-B & & \checkmark & 32 & 282$\times$4$\times$3 & 88 & 80.6 & 94.6 \\
\midrule
TimeSformer~\cite{bertasius2021space} & ViT-B & \multirow{5}{*}{IN-21K} & \checkmark & 8 & 196$\times$1$\times$3 & 121 & 78.3 & 93.7 \\
TimeSformer~\cite{bertasius2021space} & ViT-L & & \checkmark & 96 & 8353$\times$1$\times$3 & 430 & 80.7 & 94.7 \\
ViViT FE~\cite{arnab2021vivit} & ViT-L & & \checkmark & 128 & 3980$\times$1$\times$3 & N/A & 81.7 & 93.8 \\
Motionformer~\cite{patrick2021keeping} & ViT-B & & \checkmark & 16 & 370$\times$10$\times$3 & 109 & 79.7 & 94.2 \\
Motionformer~\cite{patrick2021keeping} & ViT-L & & \checkmark & 32 & 1185$\times$10$\times$3 & 382 & 80.2 & 94.8 \\
Video Swin~\cite{liu2020teinet} & Swin-L & & \checkmark & 32 & 604$\times$4$\times$3 & 197 & 83.1 & 95.9 \\
\midrule
ViViT FE~\cite{arnab2021vivit} & ViT-L & \multirow{2}{*}{JFT-300M} & \checkmark & 128 & 3980$\times$1$\times$3 & N/A & 83.5 & 94.3 \\
ViViT~\cite{arnab2021vivit} & ViT-H & & \checkmark & 32 & 3981$\times$4$\times$3 & N/A & 84.9 & 95.8 \\
\midrule
ip-CSN~\cite{tran2019video} & ResNet152 & \multirow{3}{*}{\textbf{---}} & \ding{55} & 32 & 109$\times$10$\times$3 & 33 & 77.8 & 92.8 \\
SlowFast~\cite{feichtenhofer2019slowfast} & R101+NL & & \ding{55} & 16+64 & 234$\times$10$\times$3 & 60 & 79.8 & 93.9 \\
MViTv1~\cite{fan2021multiscale} & MViTv1-B & & \ding{55} & 32 & 170$\times$5$\times$1 & 37 & 80.2 & 94.4 \\
\midrule
\multicolumn{7}{l}{\textbf{\textit{Category: Self-Supervised Pre-training}}} \\
\midrule
\multicolumn{7}{l}{\textbf{\textit{Pre-Trained Epochs: 1600}}} \\
\rowcolor[gray]{0.9}
\textbf{CrossVideoMAE (Ours)} & ViT-B & 1N-1K & \ding{55} & 16 & 180$\times$7$\times$3 & 87 (Shared) & \textcolor{red}{83.2} & \textcolor{red}{95.6} \\
\bottomrule
\end{tabular}%
}

\caption{Comparison of our proposed method with supervised SOTA methods on the K400 dataset. We use ViT-B/16 backbone. Extra labels \ding{55} denotes only unlabelled data used for the pre-training phase. The N/A denotes these numbers as not being available/reported in the paper. The best result is marked by \textcolor{red}{red} colour.}
\label{tab:supervised_k400}

\end{table*}




\begin{table*}[!h]
\centering
\scriptsize
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}[t]{lccccc}
\toprule
\textbf{Method} & \textbf{Modality} & \textbf{Backbone} & \textbf{Extra Data} & \multicolumn{2}{c}{\textbf{Video Retrieval (R@1)}} \\
& & & & \textbf{UCF101} & \textbf{HMDB51} \\
\midrule
VCOP~\cite{xu2019self} & V & R(2+1)D & UCF101 & 14.1 & — \\
CoCLR-RGB~\cite{han2020self} & V & S3D-G & K400 & 53.3 & 23.2 \\
Vi$^2$CLR~\cite{diba2021vi2clr} & V & S3D & K400 & 55.4 & 24.6 \\
$\rho$BYOL\textsubscript{$\rho=4$}~\cite{feichtenhofer2021large} & V & SlowOnly-R50 & K400 & 76.8 & 39.6 \\
SVT~\cite{ranasinghe2022self} & V & ViT-B & IN-21K+K400 & \textcolor{blue}{82.9} & \textcolor{blue}{44.4} \\
VideoMAE~\cite{tong2022videomae}& V & ViT-B & K400 & 64.0 & 32.5 \\
\midrule
\rowcolor[gray]{.9}
\textbf{CrossVideoMAE} & V + I & ViT-B & IN-1K + K400 & \textcolor{red}{85.5} & \textcolor{red}{49.7} \\
\bottomrule
\end{tabular}%
}

\caption{Comparison with state-of-the-art methods on UCF101 and HMDB51 forVideo Retrieval. ‘V’ refers to visual, ‘A’ is audio, ‘T’ is text narration, and ‘I’ is the image. The best and second best results are shown in \textcolor{red}{red} and \textcolor{blue}{blue} colours, respectively.}
\label{tab:video_Retrival}
\end{table*}

\subsection{Evaluation}
We evaluate our models under two main methods: End-to-end full fine-tuning and linear evaluation.

\subsubsection{End-to-end full Finetuning} 
Default settings for end-to-end fine-tuning can be found in Tab.~\ref{tab:fine-tune} on IN-1K, K400, SSv2, UCF101, and HMDB51 datasets. Similar to previous work, we use layer-wise learning rate decay~\cite{he2022masked}.

\subsubsection{Linear probing} 
We further evaluate our method under liner probing setting on the UCF101, HMDB51, K400, and SSv2 datasets. We follow SVT~\cite{ranasinghe2022self} to fix the transformer backbone and train a linear layer for 100 epochs. Tab.~\ref{tab:linear} shows the settings that we use for linear evaluation.






\section{Additional Results}
\label{sec:additional}

\subsection{Comparison with State-of-the-Art Methods}
In this section, we provide an extended set of results, evaluating our method on action recognition tasks through linear evaluation and full fine-tuning and comparing it against supervised learning models. We also report comparative results on video retrieval tasks.

\subsubsection{Action Recognition}

\paragraph{Linear Evaluation:}

Table~\ref{tab:action_linear} presents the linear evaluation results for action recognition on the UCF101, HMDB51, K400, and SSv2 datasets. Our model, CrossVideoMAE, consistently outperforms the current state-of-the-art methods across all datasets.

\paragraph{End-to-End Full Fine-Tuning (Supervised Learning Evaluation):}

In Tables~\ref{tab:supervised_ssv2} and~\ref{tab:supervised_k400}, we present a comparison of CrossVideoMAE's performance on the SSv2 and K400 datasets against other state-of-the-art methods that rely on supervised pre-training. Our method demonstrates superior performance in both datasets, highlighting its effectiveness for end-to-end fine-tuning.

\subsubsection{Video Retrieval}

Table~\ref{tab:video_Retrival} showcases the results of video retrieval on the UCF101 and HMDB51 datasets. CrossVideoMAE achieves the highest retrieval accuracy on both datasets, with 85.5\% on UCF101 and 49.7\% on HMDB51, setting a new benchmark for video retrieval performance in these tasks.











\subsection{More analysis and ablation studies}

\subsubsection{Sampled frame selection}

In this study, we investigate the influence of sampled frame selection on the distillation process. We compare the random frame as the sampled frame with either the first or a middle frame, and the result is shown in Tab.~\ref{tab:sampled}. This implies that the random frame is the best, as K400/SSv2 dataset videos are short-range (4-10s) videos.

\begin{table}[h!]
\centering
\scriptsize
\resizebox{0.7\linewidth}{!}{
\begin{tabular}[t]{ccc}
\toprule
\textbf{Sampled frame} & \textbf{Acc@1} & \textbf{Acc@5} \\
\midrule
first frame & 73.0 &92.9 \\
middle frame & 73.3 & 93.1 \\
\rowcolor[gray]{0.9}
\textbf{random frame} & \textbf{73.7} & \textbf{93.4} \\
\bottomrule
\end{tabular}%
}

\caption{\textbf{Sampled frame selection.} We perform an ablation study on SSv2 to select the sampled frame as the first, middle, or random frame}
\label{tab:sampled}
\end{table}

\subsubsection{Masking Types} 
We applied random masking to the image branch and tested frame, tube, and random masking for the video branch (Tab.~\ref{tab:types}). Our results showed that random masking in both branches achieved the best performance. Frame masking, which hides entire tokens in random frames, performed poorly might be due to pixel redundancy across frames. Tube masking~\cite{tong2022videomae}, which masks tokens at the same spatial location over consecutive frames, also underperformed as it might struggle to transfer learned semantics effectively. Random patch masking~\cite{feichtenhofer2022masked} with high ratios (90-95\%) worked well for both images and videos, hence we selected random masking for both modalities.


\begin{table}[h!]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}[t]{cccc}
\toprule
\multicolumn{2}{c}{\textbf{Masking Types}} & \multicolumn{2}{c}{\textbf{Acc@1 (\%)}} \\
\textbf{Image Branch} & \textbf{Video Branch} & \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
Random & Tube & 83.4 & 73.4 \\
Random & Frame & 83.1 & 72.7 \\
\rowcolor[gray]{.9}
\textbf{Random} & \textbf{Random} & \textbf{83.6} & \textbf{73.7} \\
\bottomrule
\end{tabular}%
}

\caption{Performance comparison of various masking strategies on the IN-1K SSv2 dataset using Acc@1, highlighting the impact of different combinations of image and video branches.}
\label{tab:types}

\end{table}

\subsubsection{Decoder Depth}Tab.~\ref{tab:decoder} illustrates the impact of varying decoder depths on action classification accuracy.The results indicate that increasing the number of decoder blocks generally improves accuracy, with four blocks achieving the highest performance. However, using eight blocks slightly decreases top-1 accuracy, suggesting diminishing returns beyond four blocks.

\begin{table}[h!]
\centering
\scriptsize
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{ccc}
\toprule
\textbf{Blocks} & \multicolumn{2}{c}{\textbf{Accuracy (\%)}} \\
& \textbf{Acc@1 (\%)} & \textbf{Acc@5 (\%)} \\
\midrule
1 & 72.52 & 92.65 \\
2 & 72.79 & 92.87 \\
\rowcolor[gray]{0.9}
\textbf{4} & \textbf{73.70} & \textbf{93.40} \\
8 & 71.63 & 93.35 \\
\bottomrule
\end{tabular}%
}
\caption{Impact of varying decoder depth on action classification accuracy.}
\label{tab:decoder}
\end{table}






\subsubsection{Further analysis of the impact of joint learning objective} We emphasize that addressing both intra-modal and cross-modal contrastive learning in a joint manner contributes to richer representation learning than individual objectives alone. Besides, both video and frame-level contrastive learning capture spatial and spatio-temporal prior representations. 

Intra-modal contrastive learning encourages the model to capture the spatiotemporal correspondence by imposing invariance to augmentations, while cross-modal contrastive learning establishes spatiotemporal-spatial correspondence and fine-grained part semantic attributes. Video-level and frame-level contrastive learning capture spatio-temporal prior and spatial prior representations, respectively. 


\begin{table}[h!]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Contrastive Learning Technique} & \textbf{Acc@1. Drop (\%)} \\ 
\midrule
Without Intra-Modal Contrastive Learning & 0.5 \\ 
Without Cross-Modal Contrastive Learning & 0.7 \\ 
Without Cross-Modal + Intra-Modal Contrastive Learning & 1.2 \\ 
Without Frame Level Contrastive Learning & 0.3 \\
Without Video Level Contrastive Learning & 0.4 \\ 
Without Video Level + Frame Level Contrastive Learning & 0.7 \\ 
\bottomrule
\end{tabular}%
}

\caption{Effect of the joint learning objective on intra-modal, cross-modal, frame-level, and video-level tasks. Action recognition performance of pre-trained embeddings evaluated on the SSv2 dataset under the default configuration.}
\label{tab:table8}

\end{table}

We empirically test this by conducting ablation studies on the SSv2 dataset, training the model in all possible settings, and evaluating its performance on action recognition. Our findings, as shown in Tab.~\ref{tab:table8}, illustrate that in all learning settings, the proposed joint learning paradigm outperforms the individual objectives. Notably, the combination of both intra-modal and cross-modal, and both video and frame-level learning objectives, obtain an accuracy gain of 0.8\% over the second best approach in SSv2 with the pre-trained SpatioTemporalMAE~\cite{feichtenhofer2022masked} video encoder.

\subsubsection{Effect of corresponding data.} Since SpatioTemporalMAE is pre-trained with a sampled frame image dataset instead of IN-1K, one concern is whether the gains can be attributed to joint training. To that end, we experiment with a pre-training image branch (pre-trained MAE) with IN-1K instead of the sampled frame dataset. To ensure, we use the exact setup for CrossVideoMAE: ensuring the exact same epochs, number of parameter updates, data, learning rates schedule, etc. As shown in Tab.~\ref{tab:table9}, the SSv2 video action recognition performance drops significantly by almost 2.9\% when trained using the IN-1K dataset. This shows that the performance gains with CrossVideoMAE are not merely due to the IN-1K being used for training. This ensures that the gains are indeed from jointly training on the corresponding two modality datasets rather than simply using more data during training.

\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Setting} & \textbf{Data} & \multicolumn{2}{c}{\textbf{Performance (\%)}} \\
& & \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
\multirow{2}{*}{\textbf{CrossVideoMAE (Ours)}} & IN-1K + SSv2 & 82.8 & 70.8 \\
& sampled frame dataset + SSv2 & 83.1 & 73.7 \\
\bottomrule
\end{tabular}
}
\caption{Effect of corresponding data}
\label{tab:table9}

\end{table}

\subsubsection{Masking Types} 

\begin{table}[h!]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}[t]{cccc}
\toprule
\multicolumn{2}{c}{\textbf{Masking Types}} & \multicolumn{2}{c}{\textbf{Acc@1 (\%)}} \\
\textbf{Image Branch} & \textbf{Video Branch} & \textbf{IN-1K}~\cite{russakovsky2015imagenet} & \textbf{SSv2}~\cite{goyal2017something} \\
\midrule
Random & Tube & 83.4 & 73.4 \\
Random & Frame & 83.1 & 72.7 \\
\rowcolor[gray]{.9}
\textbf{Random} & \textbf{Random} & \textbf{83.6} & \textbf{73.7} \\
\bottomrule
\end{tabular}%
}

\caption{Performance comparison of various masking strategies on the IN-1K SSv2 dataset using Acc@1, highlighting the impact of different combinations of image and video branches.}
\label{tab:types}

\end{table}
We applied random masking to the image branch and explored frame, tube, and random masking for the video branch (Tab.~\ref{tab:types}). Our experiments revealed that random masking for both branches yielded the best performance. Frame masking, which masks entire tokens in randomly selected frames, performed worse due to high pixel redundancy across frames. Tube masking~\cite{tong2022videomae}, which masks tokens at the same spatial location across consecutive frames, was also less effective, as it struggled to transfer well-learned semantic information from the sampled frames to full videos. Consequently, we opted for random masking in both branches. Additionally, random patch masking~\cite{feichtenhofer2022masked}, which masks tokens randomly across space and time, performed well with high masking ratios (90\% and 95\%) in both images and videos. Given its simplicity and effectiveness, we chose random masking for both modalities.






\subsection{Qualitative Results}
\label{sec:visualize}


To further understand how the proposed CrossVideoMAE approach effectively captures rich spatiotemporal representations and semantic attributes in videos, we analyze the self-attention maps for reconstructed samples from randomly selected additional videos in the K400(Fig.~\ref{fig:figure1}–\ref{fig:figure11}) and the SSv2 (Fig.~\ref{fig:figure12}–\ref{fig:figure18}) validation set and additional images in the IN-1K (Fig.~\ref{fig:figure19}) validation set. Even under high masking ratios, CrossVideoMAE demonstrates the ability to produce satisfying reconstruction results. These examples highlight the capability of CrossVideoMAE to learn and preserve complex spatiotemporal structures and semantic attributes in video data, underscoring its robustness and effectiveness in representation learning. 

For instance, in Fig.~\ref{fig:figure1}, the spatiotemporal representations are primarily concentrated in the central and lower regions of each frame, specifically focusing on the girl’s hand and lip movements while playing the guitar. Accurately reconstructing these regions is challenging, as evident in the third and sixth rows. The proposed CrossVideoMAE leverage difference between masking ratios applied to both branches across sampled frames and videos to effectively learn representations. This process allows the model to utilize visible tokens from both the sampled frame and the broader video context. Similar observations can be made for the other examples, further validating the capability of CrossVideoMAE to capture nuanced spatiotemporal and semantic details in video data.



Similar observations can be made for the other examples, further reinforcing the effectiveness of CrossVideoMAE in capturing nuanced spatiotemporal and semantic representations across diverse video samples. Upon acceptance, we plan to release additional \textbf{GIF} visualizations, alongside the code, on GitHub to provide a more comprehensive understanding of the proposed method's capabilities.

\noindent \textit{These results are for the default setting pre-training.}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo0.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset.}
\label{fig:figure1}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo1.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo2.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure3}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo3.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure4}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo4.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure5}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo5.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure6}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo6.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure7}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo7.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure8}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo8.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure9}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo9.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure10}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/K400/demo10.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on the K400 dataset for a masking ratio of 95\%.}
\label{fig:figure11}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo0.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure12}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo1.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure13}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo2.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure14}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo3.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure15}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo7.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure16}
\end{figure}

\begin{figure}[t!]
\includegraphics[width=\linewidth]{figures/SSv2/demo8.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure17}

\centering
\includegraphics[width=\linewidth]{figures/SSv2/demo9.9.jpg}
\caption{An example self-attention maps visualization of our CrossVideoMAE on SSv2 dataset for a masking ratio of 95\%.}
\label{fig:figure18}

\includegraphics[width=\linewidth]{figures/report.jpg}
\caption{\textbf{Additional reconstruction visualizations.} using CrossVideoMAE on the IN-1K image dataset. We show the model predictions for a masking ratio of 90\%.}
\label{fig:figure19}
\end{figure}


