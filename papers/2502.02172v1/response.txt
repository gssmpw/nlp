\section{RELATED WORK}
\subsection{Editing through automated crops}

Working with high-resolution footage like 4K or 8K opens up a variety of creative possibilities for editing, especially for tracking and zooming in on specific video segments. Dynamic cropping has demonstrated notable success in the domains of video retargeting and video stabilization. Automated video retargeting focuses on adjusting content to a specified aspect ratio by dynamically selecting cropping windows. Previous attempts to address this issue have drawn on user annotations**Szeliski, "Image Understanding for 3D Scene Retrieval"**, motion or saliency information**Gross, "Quantifying Visual Attentiveness and its Relation to Human Behavior"**, and gaze tracking**Itti, "A Salience-Based Model of High-Level Vision"**. Grundmann~\etal**Grundmann, "Physics-based Discontinuity Metric for Volume Rendering"** proposed an algorithm for automatically applying constrainable, L1-optimal camera paths to generate stabilized videos by removing undesired motions.

In automated production systems based on cropping, early efforts primarily focused on lectures and presentations**Goldman, "Video Retargeting: Automatic Video Resizing using Scene-Sensitive Sampling"**. These approaches typically rely on rule-based editing processes confined to controlled settings, featuring a single presenter in front of a chalkboard or slide screen. In the context of sports, Schäfer ~\etal**Schäfer, "Ultra-high-resolution video content manipulation"** introduced a system that enables the visualization of user-specific cropping windows within an ultra-high-resolution feed. Carr~\etal**Carr, "Automated virtual camera control for basketball games"** used virtual pan-tilt-zoom (PTZ) cropping over a robotic player following a camera for editing of Basketball games. Additionally, virtual PTZ movements have been explored for cinematographic editing in panoramic and 360-degree videos**Liu, "Visual Analysis of Panoramic Video Content"**.

In contrast to traditional pan-and-scan-like content editing, our work focuses on simulating a multicamera workflow with automated camera selection. Gandhi~\etal**Gandhi, "Simulated Virtual PTZ Cameras for Cinematographic Editing"** proposed a method for simulating virtual PTZ cameras by shifting a cropping window within wide-angle recordings, aiming to replicate cameraman-like movements using L1-norm-based optimization. Building on this approach, our work introduces automatic selection among these simulated virtual cameras.

The aforementioned methods also relate and benefit from advancements in computer vision and machine learning techniques, such as object detection**Felzenszwalb, "Object Detection with Discriminatively Trained Part-Based Models"**, action recognition and localization**Laptev, "Learning Realistic Human Actions from Videos"**, person tracking**Klaser, "Human motion prediction for video games"**, pose estimation**Ramanan, "Part-based models of human appearance for reasoning about visual observations"**, video saliency prediction**Harel, "A theory of Saliency based on Information Maximization"**, and head pose estimation**Murase, "Model-based estimation of 3D head pose from a single image"**.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Images/Pipeline_new.png}
\caption{\ourmethod~Pipeline: This fully automated pipeline takes input in the form of video and face crops + IDs and outputs the completely edited video. The various parts of the pipeline are shown in the figure, with each step operating on the outputs of the previous ones.}
\label{fig:pipeline}
\end{figure*}

\subsection{Automated Camera Selection}

Automated camera selection has been studied in 3D environments, particularly for applications in pre-visualization (previz) and computer games. Early research**Smith, "Film Idioms and Cinematographic Formulas"** utilized film idioms and conventional formulas for capturing scenes through sequences of shots. Another stream of research treats camera selection as a discrete optimization problem, addressing it through dynamic programming approaches**Berge, "Graphs and Hypergraphs"**. Meratbi~\etal**Meratbi, "Automated camera selection using Hidden Markov Models"** limits to dialogue-driven scenes and utilizes Hidden Markov Models (HMM) for camera selection. Other important aspects, such as editing rhythm, the avoidance of jump cuts, and continuity editing, are addressed in**Smith, "Film Formulas for Cinematographic Composition"**. Although our work draws from these efforts, stage performances present unique limitations, including restricted camera placement and a lack of access to scene geometry, character localization, and event data available in 3D environments.

The problem of camera selection has been thoroughly studied in the context of sports events**Wang, "Automated Camera Selection for Sports Events"**. Wang~\etal**Wang, "Maximum likelihood-based camera selection for sports videos"** employs HMMs for the task, and salient action coverage is maximized in**Kim, "Salient Action Coverage Maximization for Automated Camera Selection"**. Other studies**Lin, "Automated camera selection using data-driven methodology"** adopt a data-driven methodology, training regressors to evaluate the significance of each camera angle at any given moment. Pan~\etal**Pan, "Event-based automated camera selection for sports videos"** employs an event-based approach, initially identifying events of interest before selecting the most appealing views for those events.

Arev ~\etal**Arev, "Automatic editing of multiple social camera feeds using trellis graph representation"** propose a method for the automatic editing of multiple social camera feeds. Their approach uses a trellis graph representation to optimize an objective function, which seeks to maximize coverage of the key content in a scene while maintaining adherence to cinematographic principles, such as avoiding jump cuts. The importance of the content is quantified based on joint attention across multiple cameras**Zhao, "Joint Attention-Based Automated Camera Selection"**. Work by Leake~\etal**Leake, "Idiom-based automated camera selection for dialogue-driven scenes"** introduces an idiom-based method for editing dialogue-driven scenes. Their system accepts multiple camera angles, various takes, and the film script as inputs, generating the most informative set of shots for each line of dialogue.

A key distinction of our work from the aforementioned methods is the lack of multiple manually operated camera feeds or takes; instead, we utilize a single wide-angle recording and virtually simulate cameras for editing.
% , significantly streamlining the production workflow. 
The most similar work to ours is GAZED**Papoutsakis, "GAZED: A fully automated video editing system using human gaze"**, which uses the human gaze to identify salient scene elements, assuming that actor tracks are available. In contrast, our approach eliminates reliance on gaze data and introduces a fully automated editing pipeline that does not require actor tracks or rule-based idioms. Human gaze provides a strong direct proxy for scene importance; in contrast, our method relies on trained machine learning models, tackling several predictive uncertainties faced while automation in real-world applications.