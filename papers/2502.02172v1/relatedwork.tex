\section{RELATED WORK}
\subsection{Editing through automated crops}

Working with high-resolution footage like 4K or 8K opens up a variety of creative possibilities for editing, especially for tracking and zooming in on specific video segments. Dynamic cropping has demonstrated notable success in the domains of video retargeting and video stabilization. Automated video retargeting focuses on adjusting content to a specified aspect ratio by dynamically selecting cropping windows. Previous attempts to address this issue have drawn on user annotations~\cite{10.1145}, motion or saliency information~\cite{wang2010motion,liu2006video}, and gaze tracking~\cite{rachavarapu2018watch}. Grundmann~\etal~\cite{5995525} proposed an algorithm for automatically applying constrainable, L1-optimal camera paths to generate stabilized videos by removing undesired motions. 

In automated production systems based on cropping, early efforts primarily focused on lectures and presentations~\cite{heck2007virtual,zhang2008automated}. These approaches typically rely on rule-based editing processes confined to controlled settings, featuring a single presenter in front of a chalkboard or slide screen. In the context of sports, Sch√§fer ~\etal~\cite{schafer2010ultra} introduced a system that enables the visualization of user-specific cropping windows within an ultra-high-resolution feed. Carr~\etal~\cite{carr2013hybrid} used virtual pan-tilt-zoom (PTZ) cropping over a robotic player following a camera for editing of Basketball games. Additionally, virtual PTZ movements have been explored for cinematographic editing in panoramic and 360-degree videos~\cite{su2016pano2vid,tang2019joint,10.1145/2744411}.

In contrast to traditional pan-and-scan-like content editing, our work focuses on simulating a multicamera workflow with automated camera selection. Gandhi~\etal~\cite{vcs} proposed a method for simulating virtual PTZ cameras by shifting a cropping window within wide-angle recordings, aiming to replicate cameraman-like movements using L1-norm-based optimization~\cite{5995525}. Building on this approach, our work introduces automatic selection among these simulated virtual cameras. 

The aforementioned methods also relate and benefit from advancements in computer vision and machine learning techniques, such as object detection~\cite{yolo_repo}, action recognition and localization~\cite{acarnet,slowfast}, person tracking~\cite{botsort,cao2023observation}, pose estimation~\cite{yolov8}, video saliency prediction~\cite{dhf1k,vinet}, and head pose estimation~\cite{kao2023toward}. 

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Images/Pipeline_new.png}
\caption{\ourmethod~Pipeline: This fully automated pipeline takes input in the form of video and face crops + IDs and outputs the completely edited video. The various parts of the pipeline are shown in the figure, with each step operating on the outputs of the previous ones.}
\label{fig:pipeline}
\end{figure*}

\subsection{Automated Camera Selection}

Automated camera selection has been studied in 3D environments, particularly for applications in pre-visualization (previz) and computer games. Early research~\cite{christianson1996declarative, he1996virtual} utilized film idioms and conventional formulas for capturing scenes through sequences of shots~\cite{arijon1976grammar}. Another stream of research treats camera selection as a discrete optimization problem, addressing it through dynamic programming approaches~\cite{lino2011computational, galvane2015continuity, merabti2016virtual}. Meratbi~\etal~\cite{merabti2016virtual} limits to dialogue-driven scenes and utilizes Hidden Markov Models (HMM) for camera selection. Other important aspects, such as editing rhythm, the avoidance of jump cuts, and continuity editing, are addressed in~\cite{galvane2015continuity}. Although our work draws from these efforts, stage performances present unique limitations, including restricted camera placement and a lack of access to scene geometry, character localization, and event data available in 3D environments.


The problem of camera selection has been thoroughly studied in the context of sports events~\cite{wang2008automatic,chen2010personalized,ChenWHCSG13,chen2018camera,pan2021smart}. Wang~\etal~\cite{wang2008automatic} employs HMMs for the task, and salient action coverage is maximized in~\cite{chen2010personalized}. Other studies~\cite{ChenWHCSG13,chen2018camera} adopt a data-driven methodology, training regressors to evaluate the significance of each camera angle at any given moment. Pan~\etal~\cite{pan2021smart} employs an event-based approach, initially identifying events of interest before selecting the most appealing views for those events.

Arev ~\etal~\cite{arev2014automatic} propose a method for the automatic editing of multiple social camera feeds. Their approach uses a trellis graph representation to optimize an objective function, which seeks to maximize coverage of the key content in a scene while maintaining adherence to cinematographic principles, such as avoiding jump cuts. The importance of the content is quantified based on joint attention across multiple cameras~\cite{NIPS2012_1bf2efbb}. Work by Leake~\etal~\cite{leake-speaker} introduces an idiom-based method for editing dialogue-driven scenes. Their system accepts multiple camera angles, various takes, and the film script as inputs, generating the most informative set of shots for each line of dialogue. 

A key distinction of our work from the aforementioned methods is the lack of multiple manually operated camera feeds or takes; instead, we utilize a single wide-angle recording and virtually simulate cameras for editing. 
% , significantly streamlining the production workflow. 
The most similar work to ours is GAZED~\cite{gazed}, which uses the human gaze to identify salient scene elements, assuming that actor tracks are available. In contrast, our approach eliminates reliance on gaze data and introduces a fully automated editing pipeline that does not require actor tracks or rule-based idioms. Human gaze provides a strong direct proxy for scene importance; in contrast, our method relies on trained machine learning models, tackling several predictive uncertainties faced while automation in real-world applications.