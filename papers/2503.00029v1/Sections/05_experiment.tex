\section{Experiments}\label{sec:experiments}


We experiment with the open-sourced model {Llama-3-8B-Instruct}~\citep{DBLP:journals/corr/abs-2407-21783} and its variaces.
We set the hidden layer dimension to 256 for the reward channel.
We set $d$, $k$, and $n$ as 2, 2, and 10, respectively.
We use the ArmoRM-Llama3-8B-v0.1\footnote{\url{https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1}}~\citep{DBLP:conf/emnlp/00030X0024} to simulate the ground truth of human preference $R$.



% \subsection{Experiment Setup}

% \textbf{Models.}
% Our method is compatible with different policy models, whether they are fine-tuned through supervised learning or reinforced learning. We primarily conducted our experiments on {Llama-3-8B}~\citep{DBLP:journals/corr/abs-2407-21783}, involving {Llama-3-8B-Instruct}, {Llama-3-8B-Instruct-DPO}\footnote{\url{https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-DPO}}, and {Llama-3-8B-Instruct-SimPO}\footnote{\url{https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SimPO}}. {Llama-3-8B-Instruct-DPO}, {Llama-3-8B-Instruct-SimPO} are variations of {Llama-3-8B-Instruct} that have been fine-tuned through DPO~\citep{DBLP:conf/nips/RafailovSMMEF23} and SimPO~\citep{DBLP:journals/corr/abs-2405-14734}, respectively. We incorporated our \Red{think channel} into these models, enabling them with the capability of \Red{self-reward}.
% For all models, the hidden layer dimension of the \Red{think channel is 256}.
% We use the ArmoRM-Llama3-8B-v0.1\footnote{\url{https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1}}~\citep{DBLP:conf/emnlp/00030X0024} as our reward model.

\textbf{Training:}
We use prompts from ultrafeedback~\citep{DBLP:conf/icml/CuiY0YH0NXXL0024} dataset as our training data to train the self-reward capability of the models. 
We follow the data collection process of~\citep{DBLP:journals/corr/abs-2405-14734}.
Specifically, we first collect responses generated by the policy model and then label the responses using the ground truth reward model. 
For each input prompt, we generate five responses with a temperature of 0.8, selecting the highest-scoring response as the chosen response and the lowest-scoring response as the rejected response.
We tried two settings: (1) freeze the policy model and (2) update the policy model with DPO.
For each setting, we train for three epochs with a learning rate of 5e-5. We use the Adam optimizer and a cosine learning rate schedule.



\textbf{Evaluation:}
We follow the standard approach to use prompts from three general-domain instruction-following datasets for evaluation: MT-Bench~\citep{DBLP:conf/nips/ZhengC00WZL0LXZ23}, Arena-Hard~\citep{DBLP:journals/corr/abs-2406-11939}, and AlpacaEval 2~\citep{DBLP:journals/corr/abs-2404-04475}. 
MT-Bench encompasses 80 questions across nine categories, requiring the model to engage in two rounds of dialogue. 
Arena-Hard is an enhanced version of MT-Bench, comprising 500 well-defined queries spanning multiple domains.
AlpacaEval-2 is made up of 805 questions sourced from five datasets.
We simulate the actual application scenario to generate outputs in a streaming paradigm. As the baseline, we consider all decoding methods that are efficient enough to support streaming usage and applicable to the general domain.
Specifically, we consider naive \textit{temperature sampling} (with a temperature of 0.8), \textit{top-p} sampling (with a probability threshold of 0.9), \textit{top-k} sampling (with a threshold of 50), and \textit{beam-search} (with a beam size of 4).
Besides that, as discussed by \citep{DBLP:journals/corr/abs-2404-12358}, \textit{DPO}~\citep{rafailov2024direct} could inherently learn the human preference into the model weights such that the greedy output could perform following the reward feedback.
Thus, we also report the performance of all search algorithms combined with the model after \textit{DPO} fine-tuning.
In our experiments, we use the same paired data used by \textit{SLA} to conduct the \textit{DPO} training.
We use the same reward model to score the final outputs and compare outputs with the greedy output to calculate the win rate, which is defined as the percentage of wins plus half of the ties.
% We report the percentage of wins, ties, and losses of each approach and use the average win rate (i.e., percentage of wins plus half of the percentage of ties) as the main evaluation metric.





\begin{table*}[t]
\scriptsize
\centering
\caption{Performance Comparison of different approaches against the baseline greedy decoding. In the bottom line, we also directly compare ``llama-DPO + SLA'' against ``llama-DPO + Greedy'' to better understand their performance. }
\label{table:main_results}
\begin{tabular}{l|ccc|ccc|ccc|c}
\toprule
Model & \multicolumn{3}{c|}{MT\_Bench} & \multicolumn{3}{c|}{ArenaHard} & \multicolumn{3}{c|}{AlpacaEval} & Avg. win\_rate \\
 & win & tie & loss & win & tie & loss & win & tie & loss & \\
\midrule
% llama-instruct + greedy & 0.0 & 100.0 & 0.0 & 0.0 & 100.0 & 0.0 & 0 & 100.0 & 0.0 & 50.0 \\ \midrule
llama-instruct + temperature &51.2 & 0.6 & 48.1 & 48.6 & 0.0 & 51.4 & 48.4 & 0.5 & 51.1 & 49.6\\
llama-instruct + top-P& 50.6 & 0.0 & 49.4 & 50.8 & 0.0 & 49.2 & 51.1 & 0.7 & 48.2 & 51.0 \\
llama-instruct + top-K& 45.0 & 0.0 & 55.0 & 50.8 & 0.0 & 49.2 & 47.8 & 0.4 & 51.8 & 47.9 \\
llama-instruct + Beam& 46.2 & 0.0 & 53.8 & 43.0 & 0.0 & 57.0 & 47.5 & 0.5 & 52.0 & 45.7 \\
llama-DPO + greedy & 77.5 & 0.0 & 22.5 & 87.0 & 0.0 & 13.0 & 83.0 & 0.4 & 16.6 & 82.6 \\
llama-DPO + temperature & 75.6 & 0.0 & 24.4 & 85.0 & 0.0 & 15.0 & 81.2 & 0.2 & 18.5 & 80.7 \\
llama-DPO + top-P & 75.0 & 0.0 & 25.0 & 82.6 & 0.0 & 17.4 & 80.0 & 0.4 & 19.6 & 79.3 \\
llama-DPO + top-K & 72.5 & 0.0 & 27.5 & 80.8 & 0.0 & 19.2 & 79.8 & 0.2 & 20.0 & 77.7 \\
llama-DPO + Beam & 74.4 & 0.0 & 25.6 & 77.4 & 0.0 & 22.6 & 77.3 & 0.2 & 22.5 & 76.4 \\
\midrule
llama-instruct + SLA & 75.0 & 1.9 & 23.1 & 82.6 & 0.2 & 17.2 & 79.3 & 2.6 & 18.1 & 79.7 \\
llama-DPO + SLA & 86.2 & 0.0 & 13.8 & 92.4 & 0.0 & 7.6 & 89.3 & 0.4 & 10.3 & \textbf{89.4} \\
\midrule
llama-DPO + SLA vs. llama-DPO & 71.9 & 0.0 & 28.1 & 77.4 & 0.0 & 22.6 & 73.0 & 1.2 & 25.8 & 74.3 \\ %(71.9+77.4+73.6)
\bottomrule
\end{tabular}



\end{table*}


\subsection{Main results}
We present the performance of different approaches in~\autoref{table:main_results}. 
From the results, we can make the following observations.
First, the advanced algorithms might also select the greedy trajectory as the final output, which is the reason for the tie scenario.
Second, sampling methods such as sampling with \textit{temperature}, \textit{top-P}, and \textit{top-K} might increase the final output's diversity, but they cannot improve overall performance.
Third, \textit{DPO} is an efficient way of learning the reward signal into the model parameters. As a result, the \textit{DPO} model could outperform the baseline model under the same greedy decoding setting.
Lastly, the proposed \textit{SLA} could significantly outperform all baseline methods, and combining the \textit{DPO} technique and \textit{SLA} can further enhance the performance.
To better compare \textit{SLA}'s effect on the \textit{DPO} model, we add one more comparison of ``llama-DPO + SLA'' against ``llama-DPO + Greedy.''
The results show that although the policy model has already learned the reward preference into the parameters through \textit{DPO} training, the greedy outputs are still not optimal.
The fact that ``DPO+SLA'' could achieve an overall 74.3\% win rate against ``DPO+Greedy'' demonstrates that even for a strong and well-calibrated model, doing the looking ahead search during inference is still quite beneficial.

\subsection{Analysis on Different Search Strategy}
\begin{figure*}
    \centering
    \begin{subfigure}{0.3\textwidth} 
        \includegraphics[width=\textwidth]{Figures/SLA_d.png} 
        \caption{Effect of the search depth.}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/SLA_k.png} 
        \caption{effect of the search width.}
        \label{fig:sub2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/SLA_n.png} 
        \caption{effect of the search step size.}
        \label{fig:sub3}
    \end{subfigure}

    \caption{Effect of different search strategies. We set the default search depth and width to be 2 and step size to be 10. }
    \vspace{-0.15in}
    \label{fig:SLA_variations}
\end{figure*}
The streaming looking ahead algorithm has three key hyperparameters. 
The search width $k$, search depth $d$, and step size $n$.
We report the average win rate of SLA against the greedy decoding algorithm on three benchmarks and show the results in \autoref{fig:SLA_variations}.
As shown by \autoref{fig:SLA_variations} (a), the search depth could improve model performance since the model could utilize further future information and thus can make a more accurate selection.
However, results in \autoref{fig:SLA_variations} (b) show that increasing the search width might hurt the performance.
This is mainly because the TRM is not perfect, and extra noise impacts the final performance.
We leave improving the TRM performance for future research.
Lastly, in \autoref{fig:SLA_variations} (c), we show the impact of step size, from which we can see that increasing step size negatively affects final performance.
This phenomenon shows that the finer-grained search gives SLA more freedom, which further justifies the value of our framework against previous MCTS frameworks that can only use pre-defined coarse-grained steps.
Increasing the search depth linearly increases processing time and computations. 
Although the search width and step size theoretically have a minimal impact on efficiency, they still add extra computational load, consume additional GPU memory, and increase the frequency of operations like sequence copying. 
These factors can affect practical efficiency. 
Therefore, when choosing hyperparameters, one should consider the practical scenario to strike a balance between quality and efficiency.

\subsection{Analysis on Token-level Reward Modeling}
\label{sec:analysis_TRM}


Another critical factor is how well the model could learn the token-level reward.
We conduct the ablation study from two perspectives: model architecture and model capacity.
Specifically, we use the widely used adapter approach as the baseline for the model architecture. 
This approach adds MLP on top of the policy model to learn the token-level reward. 
% For the training objective, we compare it with the traditional policy gradient method, which treats all tokens equally. In the positive example, this baseline will encourage all actions in the trajectory, and in the negative example, it will punish all actions.
% Besides that, we also compare with the cross-entropy loss function.
To analyze the impact of the reward channel capacity, we conducted experiments with different hidden dimensions as variations of our model.
We introduce a new evaluation metric, the Area Under Token-level Reward Curve (AuTRC), to better understand the models' TRM capability.
Given a trajectory, we can compute the agreement of the model's prediction at each token versus the ground truth.
We select the percentage as the x-axis since different trajectories might have different lengths to draw the distribution.
We can then use the area under the curve as the quantity evaluation.
We present the results in \autoref{fig:TRM}.

We compare our Reward Transformer with an additional 35.7M parameters with two variations of the adaptor approach. 
One only has a single layer and thus only has 4K additional parameters; the other one has four layers with 50.3M parameters. 
Results show that although more parameters in the adaptor help, RT could still significantly outperform the adaptor model with more parameters because the TRM requires the model to deeply understand information from all layers rather than just the abstract information in the top layer.
% Second, token-level BT performs best on the first 30\% of the tokens. In contrast, sequence-level BT, which treats all tokens in the sequence equally, performs better on later tokens, where the model can observe more complete information.
% This makes sense because when the model can only see early tokens, the prediction tends to have a larger variance, and thus, the fine-grained loss is helpful. 
% On the other hand, when more information is provided, considering the whole sequence equally is more stable than putting more attention on several tokens.
% Since the early-stage decision could impact the whole process, TRM on early tokens is more important in real applications. 
On the other hand, we can see that the hidden dimension does not significantly impact the TRM performance.
We could thus use a relatively small hidden representation to learn the reward as long as we evenly distribute the job into all layers.
% Thanks to this, SLA only introduces 35.7M extra parameters, which is very lightweight.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.37\textwidth}
        \includegraphics[width=\textwidth]{Figures/arch.png} 
        \caption{Architecture design.}
        \label{fig:sub2}
    \end{subfigure}
    % \begin{subfigure}[b]{0.3\textwidth}
    %     \includegraphics[width=\textwidth]{Figures/loss.png} 
    %     \caption{Loss function.}
    %     \label{fig:sub3}
    % \end{subfigure}
    % \hfill
    \begin{subfigure}{0.37\textwidth} 
        \includegraphics[width=\textwidth]{Figures/dim.png} 
        \caption{Hidden dimension.}
        \label{fig:sub1}
    \end{subfigure}


    \caption{Contribution of different TRM designs. RT is the proposed architecture.  }
    \vspace{-0.1in}
    \label{fig:TRM}
\end{figure}