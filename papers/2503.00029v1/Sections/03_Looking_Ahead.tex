\section{Looking Ahead Search Algorithms}
\label{sec:MCTS}


A critical limitation of the aforementioned approaches is that they rely solely on past information, which may not be sufficiently informative for making a wise decision. 
To address this, researchers try to enable the model to look ahead and revisit its choices to make a more informed decision. 
This methodology mirrors how humans plan ahead before making a decision.
Such ideas were widely used in the traditional RL tasks such as the GO game~\citep{silver2016mastering,silver2017mastering} and one of the most widely used algorithms is the Monte Carlo Tree Search (MCTS)~\citep{metropolis1949monte}.

As shown in Algorithm \autoref{algorithm:MCTS}, when making a move, a typical MCTS algorithm typically involves the following steps: (1) selection: following the UCB policy to find a leaf node; (2) expansion: expand it if the located node is not the final state; (3) simulation: calculate the reward for the current state; (4) backpropagation: update the Q-value and visit count for all previous nodes.
This expand-simulation-backpropagation procedure is essentially a way of looking ahead and using future information for the current decision.
Typically, we repeat this procedure for $N$ times/rollouts and then decide based on the future rewards collected.


Recently, MCTS has been introduced into the LLM scenario for both the training and inference stages~\citep{zhang2022efficient,xie2024monte,zhang2024rest,wang2024towards,liu2024don}.
During training, MCTS was known as a power algorithm for sampling good responses, which can be used to optimize the model.
On the other hand, search methods like MCTS have also been proven to be a powerful inference algorithm for improving the model's performance on complex tasks~\citep{feng2023alphazero,lightman2023let}.
However, as the Deepseek technical report~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} discussed, efficiency is still the Achilles' heel of applying MCTS in LLM.
% The main time cost comes from the reward modeling part. 

\begin{algorithm}[t]
\small
\caption{Monte-Carlo Tree Search (Single Step)}
\label{algorithm:MCTS}
\textbf{INPUT}: Policy Model $P$, Reward Model $R$, Root Node $s_0$, Max Iterations $N$ \\
\textbf{OUTPUT}: Optimal Action $a^*$ \\
\begin{algorithmic}[1]
    \STATE \textbf{INITIALIZE}: Create a search tree with root node $s_0$ and initialize $Q(s, a) \leftarrow 0$, $N(s, a) \leftarrow 0$ for all states and actions.
    \FOR{$i = 1$ TO $N$}
        \STATE \textbf{SELECTION}: Start at $s_0$, traverse the tree by choosing child nodes using the UCB policy until a leaf node $s_L$ is reached.

        \STATE \textbf{EXPANSION}: if $s_L$ is not a terminal state, add a child node $s_{L+1}$ for each possible action $a$ and estimate the prior probability $P(s_{L+1}, a)$.

        \STATE \textbf{SIMULATION (ROLLOUT)}: Calculate the cumulative reward $r$ over the trajectory: $r = \sum_{t=0}^{T} R(s_t, a_t)$.
        
        \STATE \textbf{BACKPROPAGATION}: For each node $(s_t, a_t)$ along the path from $s_L$ to $s_0$, Update $Q$-value and visit count.
        % \FOR{each node $(s_t, a_t)$ along the path from $s_L$ to $s_0$}
        %     \STATE Update $Q$-value and visit counts:
        %     \[
        %     Q(s_t, a_t) \leftarrow \frac{N(s_t, a_t) \cdot Q(s_t, a_t) + r}{N(s_t, a_t) + 1}
        %     \]
        %     \STATE Increment visit count:
        %     \[
        %     N(s_t, a_t) \leftarrow N(s_t, a_t) + 1
        %     \]
        % \ENDFOR
    \ENDFOR

    \STATE \textbf{RETURN}: $a^* = \arg\max_{a} Q(s_0, a)$
    % \[
    % a^* = \arg\max_{a} Q(s_0, a)
    % \]
\end{algorithmic}


\end{algorithm}

If we use $N$ and $n$ to indicate the number of sampled trajectories and number of tokens per action.\footnote{We use ``action'' because different algorithms might use different granularities such as tokens and sentences.}
To select each action, the algorithm first expands trajectories, collects rewards for each sampled trajectory, backpropagates, and selects the action.
Since the main time costs in the large language model scenario are related to language model computing and communication, we ignore another time cost for simplicity.
For each action, the computation time complexity is
\begin{equation}
   O( N \cdot (n \cdot t_{d} + 2 \cdot t_{c} + t_{r})),
\end{equation}
where $t_d$, $t_c$, and $t_r$ are the time cost for the policy model to decode a token, communication between two models, and the reward model to generate a numerical score.
And then, if we use $t_p$ to indicate the prefilling time and $T_{max}$ as the maximum trajectory length, the total time cost will become:
\begin{equation}
   O( t_p + \frac{T_{max}}{n} \cdot N \cdot (n \cdot t_{d} + 2 \cdot t_{c} + t_{r})).
\end{equation}
% With the current infrastructure, since $t_d$ is usually way smaller than $t_c$ and $t_r$, the overall compute is bounded by the reward part.
% \begin{equation}
%    O( t_p + \frac{T_{max}}{n} \cdot N \cdot (2 \cdot t_{c} + t_{r})).
% \end{equation}
Given that people often use another LM with the same or larger size as the reward model, $t_r$ is often large.
In actual applications, people reduce this complexity by choosing a relatively larger $n$ and defining each action at coarser granularity, such as a sentence.
This trick makes the MCTS search slightly more affordable but also restricts the generalization capability.