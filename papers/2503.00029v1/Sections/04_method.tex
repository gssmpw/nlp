\section{Streaming Looking Ahead}\label{sec:method}

% In this section.

% \subsection{Algorithm}





We present the proposed streaming looking ahead (\textit{SLA}) algorithm in Algorithm \autoref{alg:streaming_looking_ahead}.
Compared with the naive MCTS, \textit{SLA} has two upgrades.
First, we remove the requirements of pre-defined heuristic steps to increase the generalization capability.
Instead, we directly use tokens as the minimum action granularity.\footnote{Users may choose any token as the step size; for a more in-depth analysis of the impact of the step granularity, please refer to \autoref{sec:analysis_TRM}.}
Second, we remove the separate reward model by proposing a \textit{Reward Transformer} module, an advanced version of the original transformer with an additional reward channel to simultaneously judge the current trajectory's quality.
The reward channel predicts the final reward given incomplete trajectories at the token level.
Thus, we can name this process as token-level reward modeling (TRM).\footnote{A token-level reward is an estimate of the final reward rather than an absolute ground truth. We use the term ``reward'' for clarity and to align with previous work.}
This section introduces details of the proposed \textit{SLA} algorithm and the reward transformer.



% \begin{algorithm*}
% \caption{Rejection Sampling}
% \textbf{INPUT}: Policy Model $P$, Reward Model $R$, Root Node $s_0$, Max trials $N$ \\
% \textbf{OUTPUT}: Optimal Trajectory $\tau^*$ \\
% \begin{algorithmic}[1]
%     \STATE \textbf{INITIALIZE}: Create a search tree with root node $s_0$.
%     \FOR{$i = 1$ TO $N$}
%         \STATE \textbf{SAMPLING}: Start at $s_0$, sample the tokens following $P(s_{L+1}, a)$ for each consecutive states until the termination. The resulting trajectory is denoted as $\tau_i$.
%     \ENDFOR

%     \STATE \textbf{RETURN}: $\tau^* = \arg\max_{\tau \in \mathcal{T}} R(\tau_i)$
%     % \[
%     % a^* = \arg\max_{a} Q(s_0, a)
%     % \]
% \end{algorithmic}

\begin{algorithm}[t]
\small
\caption{Streaming Looking Ahead (Single Step)}\label{alg:streaming_looking_ahead}
\textbf{INPUT}: Joint Policy-Reward Model $J$, Current Node $s_0$, Search Depth $d$, Search Width $k$, Step size $n$. \\
\textbf{OUTPUT}: Optimal Action $a^*$ \\
\begin{algorithmic}[1]
    \STATE \textbf{INITIALIZE}: Create a search tree with root node $s_0$ and initialize $Q(s, a) \leftarrow 0$ for all states and actions.
    \FOR{$i = 1$ TO $d$}
        \STATE \textbf{EXPANSION}: Expand all leaf nodes to $k$ children's. For each child, generate the next maximum $n$ tokens unless meeting a stopping criteria.

        \STATE \textbf{SELF-EVALUATION}: Record the Reward Transformer output $R^\prime$ at the last token of each child.
        
        \STATE \textbf{BACKPROPAGATION}: For each node $(s_i, a_i)$ along the path from $s_i$ to $s_0$, Update $Q$-value as $Q(s_i, a_i) = \max Q(s_i \mid a_i, *)$, where $\mid$ is the transaction rule of the language model concatenation and $*$ indicates all children actions.
    \ENDFOR

    \STATE \textbf{RETURN}: $a^* = \arg\max_{a} Q(s_0, a)$
    % \[
    % a^* = \arg\max_{a} Q(s_0, a)
    % \]
\end{algorithmic}


\end{algorithm}
% \end{algorithm*}

\subsection{Streaming Looking Ahead}
Following the notations in \autoref{sec:MCTS}, we use $t_{p}$, $t_{d}$, $t_{r}$, and $t_{c}$ to indicate the time cost for prefilling, decoding a token, generating rewards for a trajectory, and communication between different models, respectively.
Compared with the naive MCTS, \textit{SLA} makes two changes.
First, \textit{SLA} removes the communication and reward modeling cost by including the self-reward functionality in the policy model.
Second, \textit{SLA} changes the random exploration algorithm with a balanced tree search.

Here, we denote the search depth, search width, and step size as $d$, $k$, and $n$, respectively.
For each step, since there is no external remodel, all the computation come from computing the future tokens in advance. Thus, we could formulate the computation complexity as
\begin{equation}
    O(k^{d} \cdot (n \cdot t_{d})).
\end{equation}
Since the effective searched trajectory of our algorithm is $k^d$. 
Following the setting used by MCTS, we could let $k^{d} = N$ and thus rewrite $d$ as $\log_k(N)$.
Multiplying the time per step with the number of steps $\frac{T_{max}}{n}$ and the prefilling time, we can get the final time complexity as
\begin{equation}
    O(t_{p} + \frac{T_{max}}{n} \cdot (k^{\log_k(N)} \cdot (n \cdot t_{d}))).
\end{equation}

Compared with the exploration in MCTS, the main advantage of tree search is that all children from the same expansion node share the same ancestor, and we could utilize batch computing to compute all children in parallel to significantly reduce the time complexity from $k^{\log_k(N)}$ to $\log_k(N)$.
As a result, the total time complexity becomes
\begin{align}
    &O(t_{p} + \frac{T_{max}}{n} \cdot \log_k(N) \cdot n\cdot t_{d})\\
    = &O(t_{p} + T_{max} \cdot \log_k(N) \cdot t_{d}).
\end{align}
With the KV-cache technique, $t_d$ is often a small value.
Unlike the MCTS method, \textit{SLA}'s theoretical speed is irrelevant to the step size.\footnote{Pratically, frequent expansion operations could increase the time cost from other operations, such as sequence copying, in current inference engines. Hence, in our experiments, the default step size is set to 10, which is also much smaller than the sentence level used by prior works.}
As a result, \textit{SLA} can operate at a much finer granularity, down to the token level (i.e., $n=1$), making it applicable to a broader range of domains.
Compared with the greedy decoding, the prefilling time remains the same, and the decoding time changes from $t_d$ to $\log_k(N) \cdot t_{d}$.
Assuming we set $N$ and $k$ to be 64 and 4, which is a typical MCTS setup, \textit{SLA} will delay the streaming speed by $\log_4(64) = 3$ times.
Given that the typical speed for streaming applications humans could tolerate is five tokens/second~\citep{liu2024andes}, and the current inference speed is above hundreds of tokens per second~\citep{kwon2023efficient}, \textit{SLA} makes the complex looking ahead search possible for streaming applications.



\subsection{Architecture}

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{Figures/architecture.pdf}
    \vspace{-0.1in}
    \caption{Overall framework of the \textit{Reward Transformer}. }
    \vspace{-0.1in}
    \label{fig:architecture}
\end{figure}
The foundation of \textit{SLA}'s success lies in the assumption that the policy model can also function as the TRM with nearly zero additional cost.
To achieve this goal, we propose the \textit{reward transformer}.
As shown in \autoref{fig:architecture}, the network takes a sequence of tokens as input and simultaneously outputs the distribution for the next action and reward for the current input sequence.
At the bottom, we have the input projection layer to project tokens into a sequence of embedding representations.
Then, we have $N$ repeat kernels that follow a dual-channel design.
The policy channel on the left side is responsible for computing current state representation and predicting the distribution of the next actions.
The reward channel on the right side predicts the final reward given the current representations.
In the end, we have two separate output heads to project the hidden representations from two channels to distributions of the next token and rewards for the current input.

Formally speaking, we denote the input representation after the input projection as $\mathbf{X} \in \mathbb{R}^{t \times d}$, where $t$ is the number of input tokens, and $d$ is the initial token representation dimension, we first create a copy of it and then use them as the input for the first \textit{reward transformer} module.
For each layer $j$, we denote the policy and reward representation as $h_p^{j}$ and $h_r^{j}$.
We compute the policy representation for the next layer $h_p^{j+1}$ following the original transformer as
\begin{equation}
h_p^{j+1} = Norm(h_p^j+FFN(Norm(h_p^j+MHA(q_p^j, k_p^j, v_p^j)))),
\end{equation}
where $Norm$ is the normalization layer, FFN is a feed-forward layer, and MHA is a multi-head attention.
$q_p^j$, $k_p^j$, and $v_p^j$ represent query, key, and value projected from $h_p^j$.
On the other side, we compute the reward representation as:
\begin{equation}
h_r^{j+1} = Norm(h_r^j + FFN([h_p^j:h_r^j]),
\end{equation}
where $[:]$ indicates the concatanention.

We have two separate output layers after the $N$ repeated reward kernels.
For the policy channel, we follow the standard setting to get the output distribution.
\begin{equation}
    \pi(a \mid \mathbf{X}) = Softmax(W_\pi h_p^{N-1} + b_\pi),
\end{equation}
where $W_\pi$ and $b_\pi$ are output weights and biases that project the hidden representation to the whole vocabulary.
Similarly, for the reward channel, we get the reward prediction with
\begin{equation}
    R^\prime(\mathbf{X}) = W_R h_p^{N-1} + b_R.
\end{equation}

Such a design has three advantages. 
First, the policy channel inherits the original transformer, so we could utilize any policy model and only update the reward channel.
Second, compared with the adapter design, this fused design preserves the deep communications between two channels at each layer such that the reward channel could better utilize the strong language representation capability of the adopted policy channel.
Third, this fused design can be easily scaled up with bigger models.

\subsection{Optimization}

We face two challenges in optimizing the \textit{reward transformer} model.
The first is the distant supervision challenge because the annotation only applies to the complete trajectories rather than the incomplete ones. 
The second one is that people are poor at providing an absolute score for a trajectory, especially for general domain tasks such as dialogue.
Instead, they are better at comparing different trajectories.
Given these challenges and the goal of expecting the TRM functionality to work,
we propose optimizing the reward channel with the Bradley-Terry loss, where the sequence score aggregates token-level rewards.
% we propose to optimize the model with the Bradley-Terry (Token-BT) loss so that it can be used to compare different trajectories at a token level.

Specifically, We formulate the training data in pairs $(\tau^w, \tau^l) \in \mathcal{D}$ to represent the winning and losing trajectories.
Tokens in the winning and losing trajectories are denoted with $a^w_i$ and $a^l_i$, respectively.
$i$ indicates the position and we use $\tau^w_i$ and $\tau^l_i$ to denote the incomplete trajectories at length $i$.
We then can define the training loss as:
\begin{equation}
    \mathcal{L} = - \mathbb{E}_{(\tau^w, \tau^l) \in \mathcal{D}} \left[ \log \sigma( \frac{1}{T_{w}}\sum_{t=1}^{T_{w}} R^\prime(\tau^w_i) - \frac{1}{T_{l}} \sum_{t=1}^{T_{l}} R^\prime(\tau^l_i) )  \right],
\end{equation}
where $T_{w}$ and $T_{l}$ is the maximum length of $\tau^w$ and $\tau^l$, respectively. $R^\prime$ is the reward output. 
We can then optimize the reward channel with the gradient descent algorithm.

% \begin{equation}
% \mathcal{L}_{\text{Token-BT}} = - \mathbb{E}_{(\tau^w, \tau^l) \in \mathcal{D}} \left[ \sum_{t=1}^{T_{max}} \log \frac{e^{R^\prime(\tau^w_i)}}{e^{R^\prime(\tau^w_i)}+e^{R^\prime(\tau^l_i)}}\right],
% \end{equation}
% where $T_{max}$ is the maximum length of $\tau^w$ and $\tau^l$, and $R^\prime$ is the reward output.
% To simply this equation, we could rewrite it in the format of the sigmoid function as 
% \begin{equation}
% \mathcal{L}_{\text{Token-BT}} = - \mathbb{E}_{(\tau^w, \tau^l) \in \mathcal{D}} \left[ \sum_{t=1}^{T_{max}} \log \sigma(\text{Diff}(\tau^w_i, \tau^l_i))\right],
% \end{equation}
% where $\text{Diff}(\tau^w_i, \tau^l_i)) = R^\prime(\tau^w_i) - R^\prime(\tau^l_i)$.
% We can then optimize the model with the gradient descent algorithm over all parameters in the reward channel.



