\section{Conclusion}\label{sec:conclusion}

% In this paper, we introduce streaming looking ahead (\textit{SLA}), an efficient looking ahead search algorithm for LLMs.
% To make the policy capable of modeling token-level rewards, we propose a new \textit{Reward Transformer} architecture with an additional channel for predicting the final reward at each token.
% Experiments show that \textit{SLA} significantly improves the model's streaming output quality by making looking-ahead searches possible. 
% Experiments on three open-domain benchmarks show that compared to greedy decoding while keeping the model frozen, we achieve an overall win rate of 79.7\%. Furthermore, when combining SLA with reinforcement fine-tuning techniques like DPO, the win rate increases to 89.4\%.

In this paper, we present Streaming Looking Ahead (\textit{SLA}), an efficient lookahead search algorithm for LLMs. 
To enable the policy to model token-level rewards, we introduce a \textit{Reward Transformer} architecture that adds a channel in each transformer module to predict the token-level reward. 
Our experiments demonstrate that SLA markedly improves streaming output quality. On three open-domain benchmarks, compared to greedy decoding, SLA achieves an overall win rate of 79.7\%, which increases to 89.4\% when combined with reinforcement fine-tuning techniques like \textit{DPO}.