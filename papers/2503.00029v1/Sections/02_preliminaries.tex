\section{Preliminaries}\label{sec:problem_formulation}

% \begin{table*}[h!]
% \centering
% \caption{Comparison of Algorithms}
% \label{tab:algorithm_comparison}
% \begin{tabular}{l|ccccc}
% \toprule
% \textbf{Algorithm Name} & \textbf{Performance} & \textbf{Diversity} & \textbf{Generalization} & \textbf{Efficiency} & \textbf{Streaming}\\
% \midrule
% Greedy-decoding & Moderate  &   Low  & High          & High      & Yes \\
% Decoding with Temperature & Low & High & High              & High & Yes   \\
% Top-K Sampling & Moderate      & High & High               & High   & Yes     \\
% Top-P Sampling & Moderate     & High & High              & High   & Yes      \\
% Beam-Search & Moderate     & Moderate & High              & Moderate    & Yes     \\
% Majority Voting & Moderate     & High & Low              & Moderate     & No    \\
% RM Selection & Moderate     & High & High              & Moderate    & No     \\
% RM-guided Tree Search & High     & High & Low              & Low    & No    \\
% \bottomrule
% \end{tabular}
% \end{table*}
% The language model generation process generally selects a sequence of tokens following certain algorithms (e.g., greedy or sampling methods) until a stopping criterion, such as an end-of-sequence token or maximum sequence length, is reached.

In this section, we first introduce how the LM generation process can be formulated as a token-level Markov Decision Process (MDP) and then explain how existing sampling algorithms relate to it.

\subsection{LLM Decoding as Token-level MDP}

The language model generation process takes a sequence of tokens as inputs and generates a sequence of tokens as outputs.
Mainstream transformer-based language models generate the output tokens one by one until the stopping criteria (e.g., an end-of-sequence token or maximum sequence length) are met.
The traditional MDP is usually formulated as a tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, F, R, \gamma)$, where $\mathcal{S}$ is the set of all possible states, $\mathcal{A}$ is the set of actions, $F$ is the transition function, $R$ is the reward function, and $\gamma$ is the decay parameter.
In the language model scenarios, each state in $\mathcal{S}$ is a trajectory that can be denoted as $\tau$.
Each action in $\mathcal{A}$ is selecting a token $x$ from the vocabulary set.
$F$ is the deterministic transition of concatenating the selected action (i.e., a token) with the existing state (i.e., a trajectory) to become a new one.
Traditionally, rewards $r_t$ are defined at every step $t$ and contribute to the return $G_t=\sum_{k=0}^{\infty} \gamma^k R_{t+k}$ through the decay factor $\gamma$.
However, in the LLM scenario, we are only concerned with the quality of the complete trajectory generated, meaning that the reward function $R(s)$ evaluates the final trajectory rather than providing step-by-step feedback.
Thus, the return becomes $G=R_T$, where $R_T$ is the reward associated with the final sequence at step $T$, and $\gamma$ is irrelevant because intermediate rewards are not accumulated. 

\subsection{The Classical Decoding Algorithms}

Formally, given an input \( \mathbf{x}  = (x_1, x_2, \ldots, x_T) \), a reward function $R$ that provides a scalar reward for a trajectory $\tau$, and a language model \( p_\theta(x) \) parameterized by \( \theta \), the goal of decoding algorithms is to find the optimal trajectory $x^\star$ sampled from \( p_\theta(x) \) that could maximize the reward:

\[
\tau^* = \arg\max_{\tau \sim p_\theta(\mathbf{x} )} R(\tau).
\]

% Assumes that the input is a token sequence with $a$ and the foundation language model is a probabilistic model of predicting the likelihood of the next token given previous ones, which is usually formulated as $P(x \mid x_{<i})$, the goal of language model decoding is to utilize this likelihood to get the output trajectory of length $T$ that achieves the highest $R_T$.
This section covers representative decoding algorithms and explains how they are connected.

\textbf{Greedy Decoding}: The naive but most widely used algorithm is \textit{Greedy Decoding}, which uses the language modeling likelihood at each step as guidance.
At each step $i$, this algorithm selects the action token $x_i \in \mathcal{A}$ following:
\begin{equation}
x_i = \arg\max_{x} P(x \mid x_{<i}).
\end{equation}
From the angle of MDP, this method uses the accumulative likelihood predicted by the language as the final reward:
\begin{equation}
 R(\tau) \gets \Pi_{i}^{T}P(x_i \mid x_{<i}),
\end{equation}
where $T$ is the length of $\tau$, and takes a greedy solution to approach this goal.


\textbf{Sampling-based Decoding:} 
On top of greedy decoding, people also try to incorporate diversity in the final output.
For example, the temperature-based method introduces an additional parameter $\lambda$ to control the greedy sampling process by reshaping the likelihood distribution as:
\begin{equation}
x_i \sim P(x \mid x_{<i})^{1/\lambda}.
\end{equation}
From the angle of token-level MDP, we can reinterpret this process as introducing an additional diversity objective:

\begin{equation}
 R(\tau) \gets \Pi_{i}^{T}P(x_i \mid x_{<i}) \cdot D(x_i, x_{<i}),
\end{equation}
where 
\begin{equation}
    D(x_i , x_{<i}) = P(x_i \mid x_{<i})^{\frac{1}{\lambda}-1}.
\end{equation}


To avoid sampling rare tokens and achieve a balance between performance and diversity, researchers have investigated how to dynamically adjust the candidate token pool~\citep{holtzman2019curious,zarriess2021decoding}. 
For example, the \textit{Top-$k$ Sampling} algorithm only considers the top $k$ tokens with the highest probabilities as candidates instead of the whole vocabulary.
Similarly, the \textit{Nucleus Sampling}, which is also known as \textit{Top-p Sampling}, only selects from the smallest possible set $\mathcal{V}_p \subseteq \mathcal{V}$, where the cumulative probability mass exceeds a threshold $p$.

% \paragraph{Nucleus Sampling:} This strategy samples tokens from the smallest possible set $\mathcal{V}_p \subseteq \mathcal{V}$, where the cumulative probability mass exceeds a threshold $p$. Formally,
% \begin{equation}
%     \mathcal{V}_p = \{x_i \mid \sum_{x_j \in \mathcal{V}_p} p(x_j \mid \mathbf{x}_{<t}) \geq p\}.
% \end{equation}

\textbf{Trajectory-level Decoding:} Although these token-level decoding algorithms are efficient, they tend to generate locally coherent outputs that may lack global quality.
To solve this problem, people also developed decoding algorithms that consider partial or whole trajectories.
For example, the \textit{Beam Search Decoding} algorithm keeps track of the top $B$ partial trajectories, expanding them at each step and retaining only the ones with the highest joint likelihood.
Similar to the \textit{Greedy Decoding}, this method also uses the joint likelihood as the trajectory reward function.

\textbf{Advanced Reward Modeling Algorithms:}
A common limitation of the aforementioned algorithms is their fundamental assumption that the joint likelihood could represent $R(\tau)$ might not always hold.
People have been interested in introducing better reward signals as guidance to address this.
For example, in the QA scenario, the \textit{Majority-voting algorithm} assumes that the more frequent answer aligns better with the grounding reward function (i.e., accuracy) and thus selects candidate trajectories following this guidance.
Though this intuitive approach has been shown to be effective on tasks such as QA and math problems, it is restricted to tasks with structured output for voting. It cannot be generalized to more general-purpose applications.
To address this issue, researchers also include an external model $R^\prime$, which is often another transformer-based model, to approximate the ground truth reward model $R$. 
With that, we could sample $K$ trajectories $\mathcal{T}_K$ with sampling-based decoding algorithms and then use $R$ to select the trajectory with the maximum reward:
\begin{equation}
    \tau^\star = \arg \max_{\tau \in \mathcal{T}} R^\prime(\tau).
\end{equation}
Employing an external model to model the reward offers greater flexibility than heuristic rewards. This approach is not constrained by the structured answer format, which improves generality and adaptability in various scenarios. 







