\section{Introduction}\label{sec:introduction}
Language models have demonstrated remarkable capabilities in broad natural language processing tasks, from text generation to question answering~\citep{yang2019xlnet,brown2020language,ouyang2022training,chowdhery2023palm}. 
However, despite their impressive performance, autoregressive decoding algorithms that only use past information lead to suboptimal results, particularly in unseen complex tasks~\citep{song2024good}.

% Advanced search algorithms 

% Inference-time searching is a native solution for guiding language models toward more reliable outputs by reducing variance and randomness. Heuristic-based methods, such as majority voting, offer a simple yet effective way to aggregate multiple trials and enhance robustness. However, despite its effectiveness, inference-time searching has two key limitations. First, the majority voting approach is inherently suited for tasks with well-defined answers, such as multiple-choice questions and arithmetic problems, making it less effective for open-ended generative tasks. Second, each trial in the search process operates independently, meaning that insights gained from one iteration are not used to refine subsequent generations, limiting the overall efficiency and adaptability of the search process.

To further enhance LLM's performance, recent research proposes framing the generation process of large language models as a trajectory optimization problem, similar to reinforcement learning (RL), and optimizing it using advanced search algorithms like Monte Carlo Tree Search (MCTS)~\citep{metropolis1949monte,silver2016mastering}. 
MCTS allows the model to look ahead before making a move by incorporating an independent reward model (RM) to evaluate the quality of future generation trajectories.
The advanced searching techniques are crucial for both the training and the inference phases.
During the training phase, we could utilize MCTS to find an optimal solution and then upgrade the policy model through reinforcement fine-tuning (RFT)~\citep{xie2024monte,zhang2024rest,wang2024towards}.
Given a fixed policy model during the inference phase, such searching algorithms can also help the model find a better trajectory than simply using greedy decoding~\citep{zhang2022efficient,liu2024don}.


% Unlike majority voting, which is restricted to specific task formats with well-defined answers, the reward model is task-agnostic and can adapt to open-ended generative scenarios. Additionally, this method leverages feedback from previous searches to iteratively improve subsequent generations, making the overall process more robust and effective.



Despite its advantages, the RM-based MCTS approach has notable efficiency limitations when applied to the LLM scenario, especially in streaming ones. 
In the LLM domain, the policy and reward models typically have billions, even trillions of parameters~\citep{donisch2024inference,wang2024litesearch}, which costs not only a high computation burden but also a high communication burden since we need to distribute these models to separate machines.
Moreover, the time complexity of these search algorithms is exponential towards the trajectory length.
The overall complexity will become unacceptable if we directly apply search algorithms to the token level.
To address the challenge, existing works mostly reduce the trajectory length by using more coarse-grained action granularity, such as defining each sentence, equation, or code block as an action~\citep{xu2023no,dainese2024generating,zhang2024accessing,brandfonbrener2024vermcts}.
However, these heuristic approaches are limited to specific structured domains and do not apply to general-domain tasks.

This paper aims to unleash the power of looking ahead algorithms by removing the external reward model.
To achieve this goal, we propose to enhance the transformer architecture~\citep{waswani2017attention} with an additional channel to model rewards simultaneously.
This design offers three advantages.  
First, since the architecture can produce reward estimation with near-zero additional computational cost for each token during exploration, it supports simultaneous token-level reward modeling (TRM), greatly expanding the method's applicability to general-domain tasks.
Second, integrating generation and self-evaluation into a single model significantly reduces the communication overhead of multi-model interactions.
Third, distributing the reward modeling into each transformer block could significantly improve the reward modeling performance compared with widely used adapter-based methods.
In the rest of the paper, we denote the proposed architecture as \textit{Reward Transformer}.

% However, this design also introduces unique challenges. 
% In the LLM scenario, the reward for each token is not well defined, and we need to learn TRM from trajectory-level feedback.
% We propose a token-level Bradley-Terry~\citep{bradley1952rank} (Token-BT) loss function to overcome this challenge.
% Experiments show that compared with traditional learning objectives, such as cross-entropy and trajectory level objectives, Token-BT could significantly improve the TRM performance on early tokens.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/framework.pdf}
    \caption{Overall framework of the proposed streaming looking ahead algorithm. We leverage the proposed \textit{Reward Transformer} that combines policy modeling and token-level self-reward modeling to conduct the efficient looking-ahead search. At each step, the policy model will generate future tokens and the associated reward simultaneously and then aggregate back to select the better action.}
    \label{fig:framework}
    \vspace{-0.1in}
\end{figure}

Building upon the trained model, we implement a streaming look-ahead (\textit{SLA}) algorithm, with a demonstration in~\autoref{fig:framework}. 
% \textit{SLA} supports token-level looking ahead searching without incurring significant delays.
At each step, the policy model looks ahead by simultaneously generating future tokens and the associated reward, which it then aggregates to select the better action.
The whole process is streaming since generated future tokens can be directly reused for the next step of decision-making.
Unlike previous MCTS works that require pre-defined steps~\citep{xu2023no,dainese2024generating,zhang2024accessing,brandfonbrener2024vermcts}, \textit{SLA} supports arbitrary action granularity down to a single token to guarantee generalization capability.


Evaluation on three general-domain datasets~\citep{DBLP:conf/nips/ZhengC00WZL0LXZ23,DBLP:journals/corr/abs-2406-11939,DBLP:journals/corr/abs-2404-04475} demonstrates that \textit{SLA} achieves a 79.7\% win rate against the baseline greedy decoding with a frozen policy model.
If the RFT is allowed, the combination of \textit{SLA} and \textit{DPO} could achieve an 89.4\% win rate.
To better monitor the TRM performance, we also propose a new evaluation metric AuTRC, with more details in \autoref{sec:analysis_TRM}.
Empirical experiments show that the proposed architecture could significantly improve the TRM quality compared with existing methods.

The paper is organized as follows. In \autoref{sec:problem_formulation}, we provide the problem formulation and essential background. 
In \autoref{sec:MCTS}, we introduce the importance and limitation of existing looking ahead algorithms in LLM applications.
In \autoref{sec:method}, we introduce the details of \textit{SLA} and \textit{Reward Transformer}. In \autoref{sec:experiments}, we present empirical experiments to demonstrate the effectiveness of SLA and associated design choices.
Lastly, \autoref{sec:conclusion} concludes this paper.
