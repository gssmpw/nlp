\section{Literature Review}
\label{appendix1}

\subsection{MLLM-based Time Series Analysis}

\subsubsection{Multimodal Large Language Model Definition}
A Multimodal Large Language Model (MLLM) is an advanced AI system that extends the reasoning capabilities of Large Language Models (LLMs) by enabling them to process, interpret, and generate information across multiple modalities, including text, images, audio, and time-series data~\cite{wu2023multimodal}. Unlike traditional LLMs that rely solely on textual data, MLLMs integrate multimodal representations through sophisticated deep-learning architectures, allowing them to perceive and reason about complex relationships between different data types. This capability enhances their performance in tasks such as multimodal question answering, image and video captioning, and medical image analysis, where understanding information from multiple sources is essential~\cite{yin2023survey}. By leveraging advanced fusion mechanisms, MLLMs generate contextually rich and coherent outputs that go beyond text-based reasoning, making them highly effective in applications requiring comprehensive multimodal understanding~\cite{zhang2024mm}.

\subsubsection{Time Series Definition} 
A time series refers to a collection of data points organized in chronological order, representing the progression of one or more variables over time. Formally, a univariate time series is denoted as $\mathbf{x} = (x_0, x_1, \dots, x_{T-1}) \in \mathbb{R}^{T}$, where $T$ is the total number of time steps, and each $x_t \in \mathbb{R}$ represents the value of the series at time $t$. This structure captures the temporal evolution of a single variable. In contrast, a multivariate time series extends this definition to multiple dimensions and is represented as $\mathbf{X} = (\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{T-1}) \in \mathbb{R}^{T \times D}$. Here, $D$ represents the number of features, so each $\mathbf{x}_t \in \mathbb{R}^D$ is a vector containing $D$ values at time $t$, reflecting the simultaneous evolution of multiple interrelated variables. Time series is fundamental in numerous domains, including finance, healthcare, and environmental monitoring, due to its ability to capture temporal dependencies and trends.

\subsubsection{Multimodal Time Series}
The increasing availability of heterogeneous data has highlighted the need to better capture the complexity of real-world phenomena. Multimodal time series address this challenge by integrating data from multiple modalities, where each modality represents a distinct type of information, such as images, text, audio, or structured numerical data. By extending traditional single-modal time series analysis, this approach enables the incorporation of diverse and complementary data sources, providing a more comprehensive understanding of complex systems. Formally, a multimodal time series can be represented as $\mathbf{X} = {\mathbf{X}^{(m)}}_{m=1}^M$, where $M$ denotes the total number of modalities, and each $\mathbf{X}^{(m)}$ corresponds to the time series for modality $m$. For instance, $\mathbf{X}^{(m)} = (\mathbf{x}_0^{(m)}, \mathbf{x}_1^{(m)}, \dots, \mathbf{x}_{T-1}^{(m)})$ represents the sequential data of modality $m$ over $T$ time steps, with $\mathbf{x}_t^{(m)}$ varying in form depending on the modality, such as vectors for numerical data, matrices for images, or sequences for text and audio. Multimodal time series analysis enables a more comprehensive understanding of temporal patterns and interactions across modalities by integrating multiple data sources. This is particularly important in applications such as healthcare, autonomous driving, and multimedia analysis~\cite{moor2023foundation,cui2024survey,zhou2024survey}.

\subsubsection{Time Series Classical Tasks}
Time series analysis and its various classical tasks are widely applied across real-world domains, such as financial forecasting, healthcare monitoring, traffic flow analysis, climate modeling, industrial predictive maintenance, and AIOps~\cite{shumway2000time,nie2024survey,yang2021early,yang2023sgdp,liang2024foundation}. Also, time series analysis encompasses a diverse set of tasks aimed at extracting insights and addressing challenges in temporal data~\cite{hamilton2020time,kirchgassner2012introduction}. Among them, common tasks include forecasting, which predicts future values based on historical trends and can be divided into short-term and long-term predictions~\cite{wen2022transformers}, and anomaly detection, which identifies unusual patterns or deviations from expected behavior~\cite{zamanzadeh2024deep}. Imputation addresses missing or corrupted data points to ensure dataset completeness~\cite{du2024tsi}, while generation creates synthetic time series to replicate statistical properties for data augmentation or scenario simulation~\cite{yang2024survey}. Other tasks include classification, which assigns categorical labels based on patterns, and regression, which predicts continuous target values~\cite{mohammadi2024deep,yang2021long}. In recent years, an increasing number of approaches have explored leveraging multimodal data to enhance classical time series analysis tasks, validating the effectiveness and rationality of these methods~\cite{zhou2023one,liutime,gruver2024large,chang2023llm4ts,cao2023tempo,yin2023survey}. For instance, Time-LLM aligns and reprograms LLMs for time series forecasting through textual input alignment~\cite{jin2023time}, while Time-MMD incorporates additional textual data with Transformer-based models using weighted fusion to perform time series forecasting and potentially other tasks~\cite{liutime}. Beyond text modalities, medical data employs supplementary image or tabular data to enhance time series analysis using foundation models~\cite{hollmann2025accurate,moor2023foundation}. Moreover, methods leveraging generative models, such as diffusion models, have also been proposed. These approaches inject multiple modalities into the conditional space, improving the robustness of temporal tasks and enhancing generative diversity~\cite{yang2024survey}.

\begin{table*}[t]
    \caption{Comparison of reasoning structure types with definitions, examples, and mathematical formulations.}
    \vspace{2mm}
    \centering
    \setlength\tabcolsep{6pt}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{p{3.5cm}p{5.5cm}p{4.5cm}p{6cm}}
            \toprule
            \textbf{Reasoning Type} & \textbf{Definition} & \textbf{Example} & \textbf{Mathematical Formula} \\
            \midrule
            End-to-end Reasoning & Allows the model to learn a direct mapping from the inputs $(\mathbf{x}, \mathbf{c})$ to the outputs $\mathbf{y}$ without revealing intermediate reasoning steps. The logic chain is encapsulated in the model's hidden states, making it less interpretable. & Effective in descriptive tasks such as brief analysis where intermediate reasoning is not required. & $P_M(\mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \prod_{t=1}^{|\mathbf{y}|} P_M(y_t \mid y_1, \dots, y_{t-1}, \mathbf{x}, \mathbf{c})$ \\
            \midrule
            Forward Reasoning & A bottom-up approach that starts with existing knowledge and applies inference rules step-by-step to derive the solution. Intermediate steps (“chain of thought”) are explicitly stated. & Solving a math problem step-by-step or predicting time series trends sequentially. & $P_M(\mathbf{r}, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \biggl[\prod_{j=1}^k P_M\bigl(r_j \mid r_{<j}, \mathbf{x}, \mathbf{c}\bigr)\biggr] \times \biggl[\prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}, \mathbf{x}, \mathbf{c}\bigr)\biggr]$ \\
            \midrule
            Backward Reasoning & A top-down approach that begins with the main problem and breaks it into smaller sub-problems, continuing recursively until solvable with existing knowledge. & Diagnosing an anomaly by identifying key data patterns in reverse order (e.g., starting from the anomaly and tracing its cause). & $P_M(\mathbf{r}, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \Bigl[\prod_{j=k}^{1} P_M\bigl(r_j \mid r_{j+1}, \dots, r_k, \mathbf{x}, \mathbf{c}\bigr)\Bigr] \times \Bigl[\prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}, \mathbf{x}, \mathbf{c}\bigr)\Bigr]$ \\
            \midrule
            Forward-Backward Reasoning & Combines forward and backward reasoning. Forward reasoning proposes potential solutions, and backward reasoning verifies or refines them by analyzing dependencies. & Analyzing time series anomalies by first identifying possible causes (forward) and then verifying them against the data (backward). & $P_M(\mathbf{r}^F, \mathbf{r}^B, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \prod_{j=1}^{k} P_M\bigl(r^F_j \mid r^F_{<j}, r^B_{<\ast}, \mathbf{x}, \mathbf{c}\bigr) \times \prod_{m=1}^{\ell} P_M\bigl(r^B_m \mid r^B_{<m}, r^F_{<\ast}, \mathbf{x}, \mathbf{c}\bigr) \times \prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}^F, \mathbf{r}^B, \mathbf{x}, \mathbf{c}\bigr)$ \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:reasoning_types}
\end{table*}


\subsection{Reasoning in NLP and Time Series}

\subsubsection{Reasoning in NLP}
In the field of Natural Language Processing (NLP), reasoning refers to the process of deriving conclusions from textual evidence and logical principles~\cite{besta2025reasoning}. It involves tasks such as understanding implicit information, performing logical inferences, and applying commonsense knowledge. Reasoning capabilities are crucial for addressing complex language tasks like natural language inference, multi-hop question answering, and commonsense reasoning~\cite{yu2024natural}. Types of reasoning include Chain-of-Thought (CoT), which breaks problems into intermediate steps for clarity, deductive reasoning which applies general rules to specific cases, and inductive reasoning which generalizes from observations~\cite{xia2024beyond}. Abductive reasoning identifies the most plausible explanations, while analogical reasoning transfers knowledge based on similarities~\cite{shi2024language,lewis2024using}. Others include commonsense reasoning, probabilistic reasoning, and causal reasoning~\cite{yu2024natural}. These approaches enhance the interpretability and performance of NLP systems. 

\subsubsection{Reasoning in Time Series}
Time series analysis tasks traditionally focus on narrower objectives — like forecasting or anomaly detection — each addressed by its own specialized model, often relying solely on numerical patterns within the data. In contrast, time series reasoning with logic integrates multiple tasks under a single, context-aware framework with human-like reasoning~\cite{chow2024towards}. It readily incorporates domain knowledge and external data sources, providing natural language explanations and causal insights rather than mere numerical outputs~\cite{potosnak2024implicit}. This approach allows time series reasoning to adapt to shifting conditions and novel questions, delving into the “why” behind observed patterns and bridging the gap between automated analysis and real-world decision-making.

Furthermore, reasoning in NLP can enhance time series analysis by enabling models to infer complex temporal patterns and relationships, improving interpretability and decision-making~\cite{potosnak2024implicit,cai2024timeseriesexam}. By incorporating reasoning capabilities, models can better handle ambiguous or incomplete data, leading to more robust predictions and insights. Currently, few studies have explored this area~\cite{chow2024towards,ye2024beyond,xie2024chatts}. However, reasoning in time series analysis remains an underexplored yet promising and impactful field.