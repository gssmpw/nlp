\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{threeparttable}
\usepackage{pifont}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[accepted]{icml2025}

\usepackage[capitalize,noabbrev]{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Position: Empowering Time Series Reasoning with Multimodal LLMs}

\begin{document}
\twocolumn[
\icmltitle{Position: Empowering Time Series Reasoning with Multimodal LLMs}

\icmlsetsymbol{equal}{$\star$}
\icmlsetsymbol{corresponding}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Yaxuan Kong}{Oxford,equal}
\icmlauthor{Yiyuan Yang}{Oxford,equal}
\icmlauthor{Shiyu Wang}{Ant}
\icmlauthor{Chenghao Liu}{Salesforce}
\icmlauthor{Yuxuan Liang}{HKUST}
\icmlauthor{Ming Jin}{Griffith,corresponding} \\
\icmlauthor{Stefan Zohren}{Oxford}
\icmlauthor{Dan Pei}{THU}
\icmlauthor{Yan Liu}{USC}
\icmlauthor{Qingsong Wen}{Oxford,Squirrel,corresponding}
\end{icmlauthorlist}

\icmlaffiliation{Oxford}{University of Oxford, UK}
\icmlaffiliation{Ant}{Ant Group, China}
\icmlaffiliation{Salesforce}{Salesforce, Singapore}
\icmlaffiliation{HKUST}{Hong Kong University of Science and Technology (Guangzhou), China}
\icmlaffiliation{Griffith}{Griffith University, Australia}
\icmlaffiliation{THU}{Tsinghua University, China}
\icmlaffiliation{USC}{University of Southern California, USA}
\icmlaffiliation{Squirrel}{Squirrel Ai Learning, USA}

\icmlcorrespondingauthor{Ming Jin}{mingjinedu@gmail.com}
\icmlcorrespondingauthor{Qingsong Wen}{qingsongedu@gmail.com}

\icmlkeywords{Time Series Reasoning, Multimodal LLMs}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Understanding time series data is crucial for multiple real-world applications. While large language models (LLMs) show promise in time series tasks, current approaches often rely on numerical data alone, overlooking the multimodal nature of time-dependent information, such as textual descriptions, visual data, and audio signals. Moreover, these methods underutilize LLMs’ reasoning capabilities, limiting the analysis to surface-level interpretations instead of deeper temporal and multimodal reasoning. In this position paper, we argue that multimodal LLMs (MLLMs) can enable more powerful and flexible reasoning for time series analysis, enhancing decision-making and real-world applications. We call on researchers and practitioners to leverage this potential by developing strategies that prioritize trust, interpretability, and robust reasoning in MLLMs. Lastly, we highlight key research directions, including novel reasoning paradigms, architectural innovations, and domain-specific applications, to advance time series reasoning with MLLMs.
\end{abstract}

\section{Introduction}
Time series analysis has long been a cornerstone of real-world applications across domains such as finance, healthcare, and energy. Prior to the rise of large language models (LLMs), research in this area predominantly focused on classic tasks such as forecasting and anomaly detection~\cite{nie2022time,yang2023dcdetector}. While tasks such as explainable time series and dependency analysis existed before the emergence of LLMs, they primarily relied on numerical data. The key shift with LLMs lies in their ability to incorporate rich contextual information beyond pure numerical representations~\cite{wang2024tabletime,niu2024multimodal,liu2024picture,aksu2024xforecast,zhang2024llmforecaster}. Additionally, researchers continue exploring classic tasks leveraging LLMs to enhance these traditional approaches~\cite{jin2023time,jin2023large}. However, these efforts are often limited in scope, focusing narrowly on tasks like forecasting rather than advancing broader reasoning and inference capabilities based on extra contextual information~\cite{jin2024position,zhou2024can,su2024large}.

\begin{figure}[!t]
\begin{center}
\includegraphics[width = \linewidth]{1.pdf}
\end{center}
\vspace{-10pt}
\caption{MLLMs integrate multimodal time series and external knowledge, enhancing reasoning and expanding time-series tasks.}
\label{Figure_1}
\vspace{-10pt}
\end{figure}

Deeper reasoning and contextual understanding in time series analysis are critical for identifying patterns, causal relationships, and subtle contextual dynamics~\cite{hamilton2020time,fatemi2024dynamical,hu2024rankprompt}. These subtle contextual dynamics may include shifts in temporal dependencies, latent external influences, or evolving structural patterns that are not easily discernible through conventional numerical analysis. However, most current researches treat time series as purely numerical input, overlooking the inherently multimodal nature of real-world and time-dependent contexts~\cite{zhang2025tempogpt}. In practice, time series are frequently accompanied by complementary data streams (e.g., text and images) that provide additional layers of information. Most existing systems do not fully exploit this multimodal richness, leaving a considerable gap in achieving robust reasoning for more complex time series tasks~\cite{merrill2024language,wang2024news,zhou2025unveiling}. 

To bridge this gap, we believe that it is crucial to develop the next generation of multimodal large language model (MLLM) frameworks that can integrate multiple sources of time-dependent data, thereby unlocking richer insights and more powerful decision-making abilities~\cite{wang2024exploring}. Figure \ref{Figure_1} illustrates this novel integration, where an MLLM fuses multiple modalities and external knowledge to enhance reasoning and tackle various time-series tasks.

\paragraph{Our Position.}
Given the growing need for advanced time series reasoning in real-world applications, we believe that time series reasoning with MLLMs can unlock more powerful and flexible inferences, support more informed decision-making, and drive tangible outcomes. We propose a framework that goes beyond traditional methods and addresses three key points: \textit{\textbf{(1) A New Reasoning Paradigm}} – We define time series reasoning, highlight its essential components, and discuss both current and prospective architectures to enable deeper inference. \textit{\textbf{(2) Beyond Traditional Tasks}} – We illustrate how time series reasoning, coupled with MLLMs, opens doors to novel tasks that go beyond the scope of classical tasks, demonstrating the broader real-world relevance. \textit{\textbf{(3) Resources and Future Directions}} – We review existing resources, identify unresolved challenges among datasets, benchmarks, and evaluations, and emphasize the need for robust multimodal training strategies to further advance time series reasoning.

\paragraph{Contributions.}
The contributions of this work can be summarized in three aspects: \textit{\textbf{(1) New Perspective on Time Series Reasoning}} – We move beyond traditional time series analysis tasks by emphasizing deeper inference and understanding. \textit{\textbf{(2) A Multimodal Reasoning Framework}} – We propose a paradigm that integrates time-dependent data from various modalities, empowering MLLMs to derive richer insights and explanations. \textit{\textbf{(3) Opportunities, Challenges, and Future Directions}} – We explore key research directions and technical challenges, propose solutions for advancing multimodal reasoning architectures, and highlight the importance of designing new datasets and evaluation methods to rigorously assess multimodal time series reasoning.


\section{Time Series Reasoning}

\subsection{What is Time Series Reasoning?}
Time series reasoning refers to the open-ended ability of an MLLM to process and interpret time series with human-like logic. It captures temporal structures, trends, and patterns to generate precise and interpretable results across various time series tasks, delivering insights in clear and natural language. Unlike traditional time series methods focused on specific goals, time series reasoning unifies these tasks into an integrated framework. It combines context-awareness, time series characteristics, and advanced inference to provide deeper insights, enhanced interpretability, and the ability to handle complex tasks requiring external information beyond the time series itself. Please refer to the Appendix~\ref{appendix1}  for more definitions and references.

\begin{table*}[!t]
    \caption{Comparison of reasoning types regarding the task objectives with definitions, examples, and mathematical formulations.}
    \vspace{10pt}
    \centering
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{0.05}{
        \begin{tabular}{p{3.5cm}p{5cm}p{5.6cm}p{6cm}}
            \toprule
            \textbf{Reasoning Type} & \textbf{Definition} & \textbf{Example} & \textbf{Mathematical Formulation} \\
            \midrule
            Deductive Reasoning & Derives specific, logically consistent conclusions from general principles or rules. & Confirming a 25\% sales spike aligns with a known rule that holiday promotions increase sales by 20–30\%. & $P_M(\mathbf{y}^+ \mid \mathbf{x},\mathbf{c}) > P_M(\mathbf{y}^- \mid \mathbf{x},\mathbf{c})$, where $\mathbf{y}^+$ represents a correct conclusion and $\mathbf{y}^-$ an incorrect one. \\
            \midrule
            Inductive Reasoning & Infers general rules or patterns from specific observations. & Hypothesizing that sales spikes every December indicate seasonality. & $P_M(\mathbf{r}^+ \mid \mathbf{x},\mathbf{c}) > P_M(\mathbf{r}^- \mid \mathbf{x},\mathbf{c})$, where $\mathbf{r}^+$ represents a valid generalization and $\mathbf{r}^-$ an invalid one. \\
            \midrule
            Etiological Reasoning & Identifies and explains underlying causes or mechanisms behind observed patterns. & Explaining a sudden drop in energy consumption by analyzing potential causes. & $P_M(\mathbf{d}^+ \mid \mathbf{x},\mathbf{c}) > P_M(\mathbf{d}^- \mid \mathbf{x},\mathbf{c})$, where $\mathbf{d}^+$ represents a correct explanation and $\mathbf{d}^-$ an incorrect one.\\
            \midrule
            Causal Reasoning & Measures causal effect relationships and requires clear demonstration of true causal impacts beyond etiological reasoning. & Evaluating if a new pricing strategy caused sustained sales increases by comparing pre- and post-policy data. & $P_M(\mathbf{h}^+ \mid \mathbf{x},\mathbf{c}) > P_M(\mathbf{h}^- \mid \mathbf{x},\mathbf{c})$, where $\mathbf{h}^+$ denotes a correct causal statement (such as $\mathbf{x}$ causes $\mathbf{y}$) and $\mathbf{h}^-$ a non-causal or incorrect statement. \\
            \midrule
            Analogical Reasoning & Identifies similarities between different time series or contexts. & Comparing sales patterns across stores to infer shared seasonality. & $P_M(\mathbf{a}^+ \mid \mathbf{x},\mathbf{x'},\mathbf{c}) > P_M(\mathbf{a}^- \mid \mathbf{x},\mathbf{x'},\mathbf{c})$, where $\mathbf{a}^+$ is a correct analogy, $\mathbf{a}^-$ a flawed analogy, and $\mathbf{x'}$ the reference time series. \\
            \midrule
            Counterfactual Reasoning & Explores “what-if” scenarios by imagining alternative conditions or interventions. & Analyzing what sales would look like without a promotional discount in June. & $P_M(\mathbf{u}^+ \mid \mathbf{x},\mathbf{c}) > P_M(\mathbf{u}^- \mid \mathbf{x},\mathbf{c})$, where $\mathbf{u}^+$ is plausible outcome under counterfactual condition and $\mathbf{u}^-$ an implausible one. \\
            \midrule
            Mathematical Reasoning & Applies mathematical techniques or logical derivations to interpret data. & Proving stationarity by examining bounded partial sums over time. & $P_M(\mathbf{m}^+ \mid \mathbf{x}, \mathbf{c}) > P_M(\mathbf{m}^- \mid \mathbf{x}, \mathbf{c})$, where $\mathbf{m}^+$ is correct mathematical derivation and $\mathbf{m}^-$ incorrect or logically inconsistent one. \\
            \midrule
            Abductive Reasoning & Forms is the most plausible explanation for observations when evidence is incomplete. & Proposing a supply chain issue as the cause for an unexplained sales dip. & $P_M(\mathbf{a}^+ \mid \mathbf{x}, \mathbf{c}) > P_M(\mathbf{a}^- \mid \mathbf{x}, \mathbf{c})$, where $\mathbf{a}^+$ is a plausible abductive explanation and $\mathbf{a}^-$ an implausible or less likely one. \\
            \bottomrule
        \end{tabular}
    }}
    \label{tab:additional_reasoning_types}
    \vspace{-10pt}
\end{table*}

\subsection{Types of Time Series Reasoning}
\label{section_3_2}
The role of time series reasoning is to understand how sequence patterns change over time and to explain the mechanisms behind these changes for more informed decision-making. Numerous approaches exist to model temporal dependencies. However, for conducting time series reasoning, it is crucial to consider both how the reasoning process is structured and what the analysis aims to achieve. Therefore, we can naturally categorize time series reasoning based on two perspectives: \textit{Reasoning Structure} and \textit{Task Objective}.

Here, we first formally define time series reasoning. Consider a univariate time series $\mathbf{x} = (x_0, x_1, \dots, x_{T-1})$ of length $T$, where $\mathbf{x} \in \mathbb{R}^T$. Let $M$ be an MLLM model that takes as input (i) the time series $\mathbf{x}$ and (ii) a sequence of context tokens $\mathbf{c}$, which may encode additional information such as domain knowledge or prompts. The model $M$ produces an output sequence $\mathbf{y}$, which can include numeric tokens representing time series values, textual tokens providing explanations or descriptions of $\mathbf{x}$, and tokens for other modalities. The model defines a probability distribution over the output sequence $\mathbf{y}$, conditioned on the inputs $\mathbf{x}$ and $\mathbf{c}$, as $P_M(\mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \prod_{t=1}^{|\mathbf{y}|} P_M(y_t \mid y_1, \dots, y_{t-1}, \mathbf{x}, \mathbf{c})$, where each token $y_t$ is predicted based on the preceding tokens $(y_1, \dots, y_{t-1})$ with time series $\mathbf{x}$ and context $\mathbf{c}$. 

\paragraph{Reasoning Structure.} 
A reasoning structure is a set of systematic steps that connects initial observations to final conclusions~\cite{wei2022chain, chu2023survey}. In time series reasoning, it provides a transparent roadmap, showing how each inference is derived and how input factors interact to shape results. Reasoning structure can be categorized into four types. \textit{{(1) End-to-end reasoning}} directly maps inputs to outputs, prioritizing efficiency over interpretability by skipping intermediate steps. \textit{{(2) Forward reasoning}} adopts a bottom-up approach, building solutions step-by-step with explicit intermediate steps, making it suitable for tasks like math problems or trend prediction. \textit{{(3) Backward reasoning}} uses a top-down approach, breaking problems into smaller sub-problems, often applied in diagnostic tasks to trace the causes of anomalies. Finally, \textit{{(4) Forward-backward reasoning}} combines both approaches, employing forward reasoning to propose solutions and backward reasoning to validate or refine them, making it particularly useful for iterative tasks like anomaly detection. Detailed descriptions of the above are provided in the Appendix for further clarity. 

In addition, these reasoning structures can be further organized into chain-, tree-, or graph-based formalisms to represent the reasoning path more explicitly. A \textit{chain-based} approach arranges the reasoning steps in a sequential, linear fashion, making it straightforward to follow how each conclusion is derived. A \textit{tree-based} approach expands reasoning into branches, allowing multiple concurrent paths and a hierarchical breakdown of complex problems. Meanwhile, a \textit{graph-based} approach generalizes these connections and can accommodate interdependencies and cyclic references among different inference steps.

\paragraph{Task Objective.} In time series reasoning, the primary goal is to extract meaningful and actionable insights from temporal and other complementary multimodal data. Depending on the objective - such as explaining anomalies, forecasting, or identifying causal factors - different reasoning types can be applied. For instance, etiological reasoning identifies causes of sudden shifts, while deductive reasoning confirms hypotheses about periodicity. The task objective determines the appropriate reasoning type and its application. Table \ref{tab:additional_reasoning_types} highlights commonly used reasoning types.

\subsection{Key Components for Achieving Time Series Reasoning}
\label{section_3_3}
Robust time series reasoning often demands a comprehensive understanding of temporal patterns and the integration of relevant task-related contextual information. Accordingly, we propose four essential components for achieving effective time series reasoning: \textit{(1) Understanding Time Series Characteristics}, \textit{(2) Contextual Guidance}, \textit{(3) Reasoning Process}, and \textit{(4) Iterative Feedback} (illustrated in Figure \ref{Figure_3_3}).

\begin{figure*}[ht]
\begin{center}
\includegraphics[width = 1\linewidth]{3_3.pdf}
\end{center}
\vspace{-10pt}
\caption{Key components for achieving time series reasoning (illustrated with financial time series example).}
\label{Figure_3_3}
\vspace{-10pt}
\end{figure*}

\paragraph{Understanding Time Series Characteristics.} 
One essential aspect of time series reasoning is identifying patterns such as seasonality, trends, and abrupt fluctuations, which are vital for analyses and decision-making. In MLLMs, numerical time-series data are often converted into textual tokens, enabling integration with textual or multimodal inputs and leveraging the model’s language capabilities. However, this can reduce the ability to recognize temporal patterns due to differences in how LLMs tokenize numerical data \cite{gruver2024large}. A key debate centers on using customized tokenization or encoding methods to represent numerical time-series data. Customized tokenization offers flexibility by aligning representation with the model architecture and integrating well with language tasks but may introduce repetitive symbols for numerical values. Encoding methods, while preserving local fluctuations and long-range dependencies, can disrupt the natural flow of language processing. Choosing between these approaches requires balancing interpretability, computational efficiency, and the preservation of temporal relationships. Addressing these challenges and improving methods remain critical areas for innovation.

To fully leverage multimodal integration in time-dependent data, it is crucial to understand correlated features across multimodal time series. In healthcare, electronic health records (EHR), wearable device data, and patient-reported outcomes often influence each other with time lags. For example, abnormalities in heart rate variability might precede changes in EHR metrics such as blood pressure by days, while self-reported fatigue might occur before alterations in wearable or EHR data. Capturing these lag effects is a key challenge and opportunity for MLLMs. Using mechanisms like temporal attention \cite{rosin2022temporal} or dynamic temporal graph networks \cite{rossi2020temporal}, MLLMs can infer lagged relationships, enabling proactive decision-making and personalized insights across domains. 

\paragraph{Contextual Guidance.} 
Context plays a crucial role in guiding time series reasoning by providing additional knowledge for interpreting patterns and shaping forecasting outcomes. Time-series data rarely exists in isolation, and the same data can lead to vastly different predictions depending on the context \cite{requeima2024llm, williams2024context}. This context may come from internal features like seasonality, trends, or anomalies, or external sources such as economic indicators, news events, or environmental factors. Since internal patterns often interact with external influences, incorporating contextual information could significantly improve data comprehension and decision-making. However, integrating external context presents challenges, as factors like policy changes, market shifts, or global events are often sporadic, unstructured, and difficult to quantify. Even when accessible, such data may be inconsistent or incomplete, potentially undermining analysis. Overcoming these obstacles is essential to advancing time series reasoning and enabling more informed decisions.

\paragraph{Reasoning Process.}
In Section \ref{section_3_2}, we introduced various approaches to time series reasoning, where the choice of method depends on the complexity and goals of the analysis. For instance, combining backward reasoning with etiological reasoning is effective for tracing the causes of anomalies, while forward reasoning with an inductive approach is better suited for identifying recurring seasonal patterns. Moreover, rather than focusing on a single task, time series reasoning aims to unify multiple objectives within an integrated framework, which introduces several challenges. First, maintaining consistent logic across reasoning steps is difficult, as earlier conclusions must remain valid when addressing subsequent goals. Second, incorporating external context without introducing spurious correlations can be complex. Lastly, as reasoning spans multiple tasks, articulating each inference step clearly becomes harder, especially when new information necessitates revising earlier conclusions. 

\paragraph{Iterative Feedback.}
To tackle the above challenges in the reasoning process, iterative feedback and refinement become quite important. It allows the model to incrementally improve its reasoning by identifying inconsistencies, revising intermediate conclusions, and incorporating new information. This process can be facilitated through several methods: leveraging LLM agents to evaluate and critique reasoning steps, embedding self-evaluation mechanisms within the model to detect and resolve potential errors, or integrating a feedback loop directly into the model’s architecture to allow it to adjust based on performance metrics.

\subsection{Promising Model Design}
\label{section_3_4}
There are generally four model design ideas for advanced reasoning tasks that either leverage the built-in reasoning capabilities of LLM/MLLMs, design a time-series MLLM, or fully utilize multimodal inputs and capabilities. We categorize these methods as \textit{Zero-Shot Inference}, \textit{One-Stage Tuning-Based}, \textit{Two-Stage Tuning-Based}, and \textit{Multimodal Time Series} approaches (illustrated in Figure \ref{Figure_3_4}).

\begin{figure}[!t]
\begin{center}
\includegraphics[width = 0.95\linewidth]{3_4.pdf}
\end{center}
\vspace{-10pt}
\caption{Different categories of advanced time series reasoning
task and architectures.}
\label{Figure_3_4}
\vspace{-10pt}
\end{figure}


\paragraph{Zero-Shot Inference.}
LLM/MLLMs inherently possess zero-shot reasoning abilities, enabling them to generate insights into temporal patterns through direct prompting using their built-in knowledge. Incorporating different types of reasoning structures (as discussed in Section \ref{section_3_2}) within prompts can enhance the quality of reasoning and interpretations. Additionally, the in-context learning approach — providing a small set of question-answer pairs with accompanying rationale — can further refine the model’s temporal reasoning capabilities.

\paragraph{One-Stage Tuning-Based.}
A time-series MLLM is fine-tuned using a systematically compiled dataset of ⟨instruction, response⟩ pairs - often involving time-series data - to align model behavior with human objectives. This process, known as instruction tuning, mitigates issues such as untruthful, unhelpful, or unsafe responses by guiding the model to produce accurate and contextually relevant outputs. A key aspect of this strategy involves formulating clear instructions that capture user goals and providing detailed, precise answers. To enhance reasoning capability, the dataset’s instructions may sometimes include different types of reasoning structures to guide the thinking process, and the corresponding responses may feature a detailed explanation of how the final answer was derived. The model parameters are updated under a supervised loss function, computed from the generated response tokens. This approach ensures the model can better generalize across diverse tasks as well as strengthen its reasoning and explanatory capabilities.

\paragraph{Two-Stage Tuning-Based.}
This approach begins by establishing an initial alignment between text and time-series modalities, followed by a supervised fine-tuning stage. In the first step, the model is trained to map textual descriptions to corresponding temporal attributes, ensuring a robust linkage between linguistic concepts and time-series features. Building on this foundation, the second step focuses on supervised fine-tuning, where the model is optimized for question-answering and reasoning tasks over the aligned modalities. By separating the alignment process from the fine-tuning phase, this two-stage approach aims to both equip the model with a strong multimodal representation of time-series data and facilitate contextually accurate, inference-driven responses~\cite{xie2024chatts}.

\paragraph{Multimodal Time Series.}
Multimodal time-series data include not only numerical sequences but also other temporal modalities. Tackling complex tasks that require robust reasoning demands a model design that integrates diverse modalities to enhance the capabilities of an MLLM. Addressing advanced reasoning capabilities in this domain involves three key components. First, a \textit{{modality encoder}} transforms various raw inputs – such as numerical data, images, or audio - into meaningful embeddings. This step often leverages pre-trained models (e.g., CLIP for images) to efficiently capture domain-specific features \cite{radford2021learning}. Second, a \textit{{modality interface}} aligns these embeddings into a unified, text-like representation, ensuring seamless integration with the language-based reasoning engine. Finally, the \textit{{LLM backbone}} serves as the central reasoning system, synthesizing the transformed inputs to perform advanced analysis and decision-making.

\begin{tcolorbox}[top=1pt, bottom=1pt, left=1pt, right=1pt]
  \textbf{Our position:}~\textit{Robust time series reasoning requires integrating external knowledge and incorporating iterative feedback. Future directions should refine data representation, integrate diverse modalities, enhance models, and develop self-correcting mechanisms to unify time series reasoning with MLLMs.}
\end{tcolorbox} 

\section{Beyond Classical Time Series Tasks}
The integration of MLLMs and time series reasoning has inspired new tasks beyond traditional time series tasks (illustrated in Figure \ref{Figure_4}). These include \textit{question answering}, \textit{causal inference \& impact analysis}, and \textit{time series generation \& editing}, which focus on reasoning and creative manipulation of time series. This section introduces these tasks, highlighting their distinct views and applications.

\begin{figure}[!t]
\begin{center}
\includegraphics[width = 0.95\linewidth]{4.pdf}
\end{center}
\caption{Time series tasks in the age of time series reasoning.}
\vspace{-10pt}
\label{Figure_4}
\end{figure}

\subsection{Question Answering}
Time series-based Question Answering (QA) represents a shift from classical analysis to high-level reasoning, where a time series serves as the primary input, optionally enriched with multimodal data like text, images, or structured data for alignment (e.g., event timestamps) or context enrichment (e.g., supplementary details). The core of QA lies in answering open-ended questions posed by users based on input multimodal time series. Moreover, other modalities beyond the time series play an indispensable role in this novel task. This is particularly impactful in healthcare, where multimodal integration (e.g., clinical notes, imaging, wearable device data) facilitates holistic patient assessments, enabling real-time critical care insights or chronic disease trend analysis. In the Appendix, we analyze the QA tasks and evaluate the reasoning using different MLLMs for real healthcare data in Figure \ref{fig:case_healthcare}. Besides, integrating meteorological data with satellite imagery helps uncover patterns in extreme weather, while combining sensor data and maintenance logs to identify causes of system failures in industry. Leveraging MLLMs, this approach bridges raw time-series data and actionable insights by aligning inputs, modeling temporal dependencies, and interpreting patterns.

\subsection{Causal Inference and Impact Analysis}
Time series causal inference and impact analysis focus on uncovering causal relationships and quantifying the effects of specific events or interventions~\cite{moraffah2021causal}. This task often involves integrating time-series data with additional modalities, such as text or tabular data, which can provide alignment or supplementary context and uncover nuanced causal relationships that may not be apparent from time series alone. For instance, in finance, combining stock price sequences with company real-time financial reports enables analysis of how financial performance impacts market fluctuations~\cite{kong2024large,kong2024large2}. In Appendix Figure \ref{fig:case_financial}, we compare the causal inference and impact analysis task and evaluate the reasoning using different MLLMs for real finance data with and without financial reports. Similarly, in healthcare, integrating electronic health records with external environmental data helps evaluate the causal effects of interventions, such as new drug treatments, on patient outcomes. Applications extend to marketing, where promotional timelines are analyzed alongside sales data to assess campaign effectiveness, and public policy, where the impacts of reforms on economic indicators like employment rates are quantified. 


\begin{table*}[!t]
\caption{The time series reasoning multimodal LLM datasets and benchmarks.}
\vspace{2mm}
\resizebox{\linewidth}{!}{
\renewcommand{\arraystretch}{0.5}{
\begin{tabular}{lccccccccc}
\toprule            
\textbf{Dataset} & \textbf{Time Series} & \textbf{Text} & \textbf{Image}  & \textbf{URL} & \textbf{Application Domain} & \textbf{Task Type} & \textbf{Algorithm} & \textbf{Open-source} \\ \midrule
Jolt~\cite{cai2023jolt}  &  \ding{52} &  \ding{52} & \ding{56} & \ding{56} & Healthcare & Representation & LLM-based & \ding{56} \\ \midrule
TS-Insights~\cite{zhang2023insight}\footnotemark[1]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{56} & General & Trend Classification & LLM-based & \ding{52} \\ \midrule

Time-series-reasoning\footnotemark[2]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{56} & General & Q\&A & - & \ding{52} \\ \midrule
ChatTS~\cite{xie2024chatts}\footnotemark[3]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{56} & Weather\&AIOps & Q\&A & MLLM-based & \ding{52} \\ \midrule
TimeMMD~\cite{liutime}\footnotemark[4]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{52} & General & Forecasting & LLM-based & \ding{52} \\ \midrule
Finst~\cite{dong2024fnspid}\footnotemark[5]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{52} & Finance & Forecasting & LLM-based & \ding{52} \\ \midrule
CiK~\cite{williams2024context}\footnotemark[6]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{56} & General & Forecasting & LLM-based & \ding{52} \\ \midrule
ChatTime~\cite{wang2024chattime}\footnotemark[7]  &  \ding{52} &  \ding{56} & \ding{56} & \ding{56} & Traffic & Q\&A & LLM-based & \ding{52} \\ \midrule
TabPFN~\cite{hollmann2025accurate}\footnotemark[8]  &  \ding{52} &  \ding{56} & \ding{56} & \ding{56} & General & Forecasting & LLM-based & \ding{52} \\ \midrule
TimeQA\footnotemark[9]  &  \ding{52} &  \ding{52} & \ding{56} & \ding{52} & General & 
Universal & - & \ding{52} \\ 
\bottomrule
\end{tabular}}}
\label{tab:dataset}
\end{table*}


\subsection{Time Series Generation and Editing}
Time series generation and editing focus on synthesizing or modifying time series, with inputs optionally enriched by other modalities like text, images, or structured data to provide alignment or supplemental information~\cite{narasimhan2024time,jingtowards}. This task is widely applicable in weather forecasting and urban management. For instance, generating synthetic weather data helps simulate extreme climate scenarios and explore the reasons behind extreme weather, while editing traffic flow data allows urban planners to assess the impact of infrastructure changes. Time series reasoning is fundamental to this process, ensuring that generated or edited data aligns with logical temporal dependencies and contextual relationships, such as maintaining seasonal weather patterns or capturing the cause-effect dynamics of urban systems. Multimodal inputs further enhance these tasks by providing additional context-satellite imagery that can guide the generation of weather time series, while map data or demographic statistics inform traffic simulations-ensuring the outputs are realistic and actionable. Moreover, we can leverage generation or editing techniques for improved time series imputation, utilizing conditions provided by information from other modalities. By incorporating reasoning from these modalities, we can achieve more realistic generation and imputation results. We show in Appendix Figure \ref{fig:case_electricity} an example of generating electrical data that demonstrates the usefulness of other modes for time series editing and generation.

\begin{tcolorbox}[top=1pt, bottom=1pt, left=1pt, right=1pt]
  \textbf{Our position:}~\textit{We advocate for continued innovation in this intersection, particularly in high-stakes domains like healthcare, finance, and industrial systems, where robust reasoning and multimodal integration are critical. Key challenges to address include data confidentiality, computational efficiency, and interpretability to ensure trustworthy and actionable outcomes. Future research should focus on domain-specific datasets, efficient training strategies, and evaluation metrics that assess both accuracy and reasoning quality, unlocking transformative capabilities across various sectors.}
\end{tcolorbox} 

\section{Resources and Challenges}
\label{section_5}

\paragraph{Dataset and Benchmark.} 
There is a notable shortage of publicly available datasets and codes in this area of research. To address this, we summarize several existing datasets in Table~\ref{tab:dataset}. However, several opportunities and challenges remain. Many datasets are artificially generated by GPT models or LLMs, and standard evaluation methods for these generated questions are lacking. Additionally, while most datasets pair numerical time series with textual descriptions, they often lack multimodal representations incorporating additional modalities, limiting their broader applicability. Furthermore, datasets that naturally merge time series into textual information are limited. Finally, existing datasets primarily focus on forward reasoning structure, such as chain-of-thought approaches, leaving opportunities to explore more diverse reasoning processes in future research.

\footnotetext[1]{\url{https://drive.google.com/drive/folders/1qGXigxE5GvmF1oLuGXaqLMkRgwoQfZ7V}}
\footnotetext[2]{\url{https://huggingface.co/datasets/mikeam/time-series-reasoning}}
\footnotetext[3]{\url{https://zenodo.org/records/14349206}}
\footnotetext[4]{\url{https://github.com/AdityaLab/Time-MMD}}
\footnotetext[5]{\url{https://huggingface.co/datasets/Zihan1004/FNSPID}}
\footnotetext[6]{\url{https://github.com/ServiceNow/context-is-key-forecasting}}
\footnotetext[7]{\url{https://github.com/ForestsKing/ChatTime/tree/main/dataset}}
\footnotetext[8]{\url{https://zenodo.org/records/13981285}}
\footnotetext[9]{\url{https://huggingface.co/datasets/TimeQA/TimeQA}}

\paragraph{Evaluation Metrics.} 
Reasoning is often intangible and highly subjective, making it relatively difficult to evaluate. Most existing research compares the outcomes of different LLMs and measures their accuracy, which is the approach commonly applied to multiple-choice or true/false questions~\cite{chang2024survey}. However, the methods for quantifying reasoning vary based on the specific tasks, datasets, and types of reasoning involved. For example, in QA tasks requiring inductive reasoning, answers are evaluated using RAGAS, a keyword-matching approach through LLM-based fuzzy matching \cite{es2023ragas}. To assess both forecast accuracy and the integration of contextual information, the Region of Interest CRPS (RCRPS) metric was introduced, which priorities context-sensitive windows in the prediction and accounts for constraint satisfaction \cite{williams2024context}. At present, there is no standard evaluation metric in the time series reasoning field. Future research should address this gap by designing task-specific metrics that can evaluate not only the accuracy of answers but also the underlying reasoning process.

\paragraph{Training Strategy.}
Current and potential model designs for trading strategies are detailed in Section \ref{section_3_4}. One potential area for improvement lies in the integration of explicit reasoning processes into the training phase. Currently, most approaches focus on including reasoning structure only within the question-and-answer pairs, without fully embedding reasoning mechanisms into the training process itself. This leaves open opportunities to explore whether the reasoning embedded in these pairs is optimal and how incorporating more detailed and high-quality reasoning could play a greater role in training. Such advancements could enhance model performance and decision-making capabilities, offering promising directions for future development in trading strategy models.

\begin{tcolorbox}[top=1pt, bottom=1pt, left=1pt, right=1pt]
  \textbf{Our position:}~\textit{Advancing time series reasoning requires progress across datasets, evaluation metrics, and training strategies. We need more realistic and multimodal benchmark datasets to move beyond artificial samples and support diverse reasoning structures. Specialized evaluation metrics should examine both outcomes and underlying reasoning processes. Integrating explicit reasoning into the training phase will strengthen model performance and decision-making, establishing a stronger foundation for interpretable time series reasoning systems.}
\end{tcolorbox} 

\section{Alternative Views}
\paragraph{\textit{Is Single-Modality Data Sufficient to Advance Time-Series Reasoning in Real-World Applications?}}
Single-modality numerical data can sometimes be sufficient - particularly in scenarios where the time series is extremely sparse and relies on simple, clear assumptions. However, this approach often fails to capture the rich contextual factors driving real-world phenomena. For instance, if we only observe a small set of yearly sales data for a product and neglect considerations like untapped markets or slowing innovation cycles, even the most sophisticated models are constrained by the narrow assumptions derived from these limited observations. 

In contrast, LLMs excel at integrating diverse sources of information - including textual descriptions, background knowledge, and multimodal time-series data. By leveraging these varied inputs, LLMs can uncover deeper causal factors that go beyond the time series alone. This capability enables us to integrate richer contextual information and formulate more flexible assumptions. For example, consider how electric vehicles are shaped by shifting government incentives, rapid technological advancements, and evolving consumer attitudes. By factoring in these contextual elements, time-series analysis can yield more reliable predictions and insights. Hence, while single-modality data can suffice in tightly constrained scenarios, employing a multimodal approach and tapping into LLMs’ broader reasoning capabilities enables richer, more accurate time-series analysis for complex, real-world applications.

\paragraph{\textit{Could LLM/MLLMs Truly Contribute to Time Series Reasoning?}}
While LLMs show promise in time series analysis, they have inherent limitations stemming from their design. As text-based sequence predictors, LLMs may lack an intrinsic understanding of mathematical logic, numerical precision, and temporal dynamics. Although they can mimic patterns from training data, they cannot perform true calculations or provide rigorous domain-specific temporal reasoning. Consequently, it is necessary to integrate explicit reasoning mechanisms - such as causal, analogical, and counterfactual reasoning - into MLLMs, as discussed in Sections \ref{section_3_3} - \ref{section_3_4}. 

Another concern is whether real-world time series might be inadvertently included in LLM pretraining datasets - an issue heightened by the opacity of training sources. Although textual-numerical pairs appear in specialized domains, multimodal time-series data are usually absent from standard pretraining corpora. Moreover, real-world deployments often rely on proprietary, domain-specific datasets (e.g., healthcare vitals with clinical annotations), which differ significantly from generic pretraining data. This specificity reduces the likelihood of overlap with undisclosed LLM training sets. However, given the lack of transparency in pretraining, practitioners cannot rule out overlaps entirely, emphasizing the need for rigorous evaluation frameworks for trustworthiness (see Section \ref{section_5}). 

Despite these challenges, MLLMs offer transformative potential by unifying multimodal time series with contextual guidance. This facilitates deeper temporal reasoning - helping to identify causal links, anomalies, and domain insights — rather than mere pattern matching. To achieve this potential, we advocate for strategies that prioritize trust, interpretability, and robust reasoning within MLLMs. Through these advancements, MLLMs could ultimately deliver reliable insights in high-stakes domains such as healthcare and industrial systems.


\section{Further Discussion}
\paragraph{Hallucination.}
It is a persistent issue in MLLMs, leading to inaccurate results in time series analysis, especially in critical fields like finance and healthcare. Incorporating reasoning mechanisms into MLLMs offers a solution by enabling the model to understand causal relationships and context, allowing it to cross-check and validate outputs. This reasoning process can also flag and correct hallucinations, ensuring more reliable and accurate analyses.

\paragraph{Environmental and Computational Cost.}
The integration of MLLMs into time series analysis may introduce challenges related to scalability and computational complexity. Critics highlight the environmental and computational costs, as training and deploying these models require substantial resources, particularly when handling large-scale, high-precision numerical data and in domains like finance and science. Addressing these challenges necessitates strategies to alleviate computational burdens, such as optimizing MLLMs, refining time-series data processing pipelines, and developing efficient alignment and inference mechanisms to enhance scalability while reducing overhead.

\paragraph{Data Confidentiality and Operational Constraints.}
MLLMs face challenges in time series analysis, including data confidentiality, as sensitive data like financial transactions or patient records cannot be shared with external services. Real-time forecasting, such as in wind power management, demands low latency (e.g., under 20 seconds for five-minute-ahead predictions). Additionally, cloud-based MLLMs also struggle in remote areas due to connectivity issues. Local deployment of open-source models ensures data control, real-time processing, and offline operation. Future research should focus on developing high-quality, accessible open-source MLLMs for time series analysis.


\section{Conclusion}
This position paper highlights the potential of time series reasoning with MLLMs for both researchers and practitioners. Our central position is that MLLMs can deliver more powerful and flexible reasoning capabilities for time series analysis, thereby improving decision-making and practical applications. To strengthen our position, we propose novel time series reasoning paradigms and introduce new task frameworks that leverage MLLMs to tackle complex temporal challenges. Despite current limitations - such as scarce datasets and the need for more sophisticated evaluation metrics - the integration of MLLMs with time series reasoning represents a noteworthy advancement. Looking ahead, we encourage researchers to focus on developing innovative architectures, refining training methodologies, and establishing comprehensive benchmarks to unlock MLLMs’ potential in real-world time series contexts.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning, especially MLLM-based time series analysis and reasoning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{ref}
\bibliographystyle{icml2025}

\medskip

\clearpage
\newpage
\appendix

\section*{Appendix}
\section{Literature Review}
\label{appendix1}

\subsection{MLLM-based Time Series Analysis}

\subsubsection{Multimodal Large Language Model Definition}
A Multimodal Large Language Model (MLLM) is an advanced AI system that extends the reasoning capabilities of Large Language Models (LLMs) by enabling them to process, interpret, and generate information across multiple modalities, including text, images, audio, and time-series data~\cite{wu2023multimodal}. Unlike traditional LLMs that rely solely on textual data, MLLMs integrate multimodal representations through sophisticated deep-learning architectures, allowing them to perceive and reason about complex relationships between different data types. This capability enhances their performance in tasks such as multimodal question answering, image and video captioning, and medical image analysis, where understanding information from multiple sources is essential~\cite{yin2023survey}. By leveraging advanced fusion mechanisms, MLLMs generate contextually rich and coherent outputs that go beyond text-based reasoning, making them highly effective in applications requiring comprehensive multimodal understanding~\cite{zhang2024mm}.

\subsubsection{Time Series Definition} 
A time series refers to a collection of data points organized in chronological order, representing the progression of one or more variables over time. Formally, a univariate time series is denoted as $\mathbf{x} = (x_0, x_1, \dots, x_{T-1}) \in \mathbb{R}^{T}$, where $T$ is the total number of time steps, and each $x_t \in \mathbb{R}$ represents the value of the series at time $t$. This structure captures the temporal evolution of a single variable. In contrast, a multivariate time series extends this definition to multiple dimensions and is represented as $\mathbf{X} = (\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{T-1}) \in \mathbb{R}^{T \times D}$. Here, $D$ represents the number of features, so each $\mathbf{x}_t \in \mathbb{R}^D$ is a vector containing $D$ values at time $t$, reflecting the simultaneous evolution of multiple interrelated variables. Time series is fundamental in numerous domains, including finance, healthcare, and environmental monitoring, due to its ability to capture temporal dependencies and trends.

\subsubsection{Multimodal Time Series}
The increasing availability of heterogeneous data has highlighted the need to better capture the complexity of real-world phenomena. Multimodal time series address this challenge by integrating data from multiple modalities, where each modality represents a distinct type of information, such as images, text, audio, or structured numerical data. By extending traditional single-modal time series analysis, this approach enables the incorporation of diverse and complementary data sources, providing a more comprehensive understanding of complex systems. Formally, a multimodal time series can be represented as $\mathbf{X} = {\mathbf{X}^{(m)}}_{m=1}^M$, where $M$ denotes the total number of modalities, and each $\mathbf{X}^{(m)}$ corresponds to the time series for modality $m$. For instance, $\mathbf{X}^{(m)} = (\mathbf{x}_0^{(m)}, \mathbf{x}_1^{(m)}, \dots, \mathbf{x}_{T-1}^{(m)})$ represents the sequential data of modality $m$ over $T$ time steps, with $\mathbf{x}_t^{(m)}$ varying in form depending on the modality, such as vectors for numerical data, matrices for images, or sequences for text and audio. Multimodal time series analysis enables a more comprehensive understanding of temporal patterns and interactions across modalities by integrating multiple data sources. This is particularly important in applications such as healthcare, autonomous driving, and multimedia analysis~\cite{moor2023foundation,cui2024survey,zhou2024survey}.

\subsubsection{Time Series Classical Tasks}
Time series analysis and its various classical tasks are widely applied across real-world domains, such as financial forecasting, healthcare monitoring, traffic flow analysis, climate modeling, industrial predictive maintenance, and AIOps~\cite{shumway2000time,nie2024survey,yang2021early,yang2023sgdp,liang2024foundation}. Also, time series analysis encompasses a diverse set of tasks aimed at extracting insights and addressing challenges in temporal data~\cite{hamilton2020time,kirchgassner2012introduction}. Among them, common tasks include forecasting, which predicts future values based on historical trends and can be divided into short-term and long-term predictions~\cite{wen2022transformers}, and anomaly detection, which identifies unusual patterns or deviations from expected behavior~\cite{zamanzadeh2024deep}. Imputation addresses missing or corrupted data points to ensure dataset completeness~\cite{du2024tsi}, while generation creates synthetic time series to replicate statistical properties for data augmentation or scenario simulation~\cite{yang2024survey}. Other tasks include classification, which assigns categorical labels based on patterns, and regression, which predicts continuous target values~\cite{mohammadi2024deep,yang2021long}. In recent years, an increasing number of approaches have explored leveraging multimodal data to enhance classical time series analysis tasks, validating the effectiveness and rationality of these methods~\cite{zhou2023one,liutime,gruver2024large,chang2023llm4ts,cao2023tempo,yin2023survey}. For instance, Time-LLM aligns and reprograms LLMs for time series forecasting through textual input alignment~\cite{jin2023time}, while Time-MMD incorporates additional textual data with Transformer-based models using weighted fusion to perform time series forecasting and potentially other tasks~\cite{liutime}. Beyond text modalities, medical data employs supplementary image or tabular data to enhance time series analysis using foundation models~\cite{hollmann2025accurate,moor2023foundation}. Moreover, methods leveraging generative models, such as diffusion models, have also been proposed. These approaches inject multiple modalities into the conditional space, improving the robustness of temporal tasks and enhancing generative diversity~\cite{yang2024survey}.

\begin{table*}[t]
    \caption{Comparison of reasoning structure types with definitions, examples, and mathematical formulations.}
    \vspace{2mm}
    \centering
    \setlength\tabcolsep{6pt}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{p{3.5cm}p{5.5cm}p{4.5cm}p{6cm}}
            \toprule
            \textbf{Reasoning Type} & \textbf{Definition} & \textbf{Example} & \textbf{Mathematical Formula} \\
            \midrule
            End-to-end Reasoning & Allows the model to learn a direct mapping from the inputs $(\mathbf{x}, \mathbf{c})$ to the outputs $\mathbf{y}$ without revealing intermediate reasoning steps. The logic chain is encapsulated in the model's hidden states, making it less interpretable. & Effective in descriptive tasks such as brief analysis where intermediate reasoning is not required. & $P_M(\mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \prod_{t=1}^{|\mathbf{y}|} P_M(y_t \mid y_1, \dots, y_{t-1}, \mathbf{x}, \mathbf{c})$ \\
            \midrule
            Forward Reasoning & A bottom-up approach that starts with existing knowledge and applies inference rules step-by-step to derive the solution. Intermediate steps (“chain of thought”) are explicitly stated. & Solving a math problem step-by-step or predicting time series trends sequentially. & $P_M(\mathbf{r}, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \biggl[\prod_{j=1}^k P_M\bigl(r_j \mid r_{<j}, \mathbf{x}, \mathbf{c}\bigr)\biggr] \times \biggl[\prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}, \mathbf{x}, \mathbf{c}\bigr)\biggr]$ \\
            \midrule
            Backward Reasoning & A top-down approach that begins with the main problem and breaks it into smaller sub-problems, continuing recursively until solvable with existing knowledge. & Diagnosing an anomaly by identifying key data patterns in reverse order (e.g., starting from the anomaly and tracing its cause). & $P_M(\mathbf{r}, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \Bigl[\prod_{j=k}^{1} P_M\bigl(r_j \mid r_{j+1}, \dots, r_k, \mathbf{x}, \mathbf{c}\bigr)\Bigr] \times \Bigl[\prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}, \mathbf{x}, \mathbf{c}\bigr)\Bigr]$ \\
            \midrule
            Forward-Backward Reasoning & Combines forward and backward reasoning. Forward reasoning proposes potential solutions, and backward reasoning verifies or refines them by analyzing dependencies. & Analyzing time series anomalies by first identifying possible causes (forward) and then verifying them against the data (backward). & $P_M(\mathbf{r}^F, \mathbf{r}^B, \mathbf{y} \mid \mathbf{x}, \mathbf{c}) = \prod_{j=1}^{k} P_M\bigl(r^F_j \mid r^F_{<j}, r^B_{<\ast}, \mathbf{x}, \mathbf{c}\bigr) \times \prod_{m=1}^{\ell} P_M\bigl(r^B_m \mid r^B_{<m}, r^F_{<\ast}, \mathbf{x}, \mathbf{c}\bigr) \times \prod_{t=1}^{|\mathbf{y}|} P_M\bigl(y_t \mid y_{<t}, \mathbf{r}^F, \mathbf{r}^B, \mathbf{x}, \mathbf{c}\bigr)$ \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:reasoning_types}
\end{table*}


\subsection{Reasoning in NLP and Time Series}

\subsubsection{Reasoning in NLP}
In the field of Natural Language Processing (NLP), reasoning refers to the process of deriving conclusions from textual evidence and logical principles~\cite{besta2025reasoning}. It involves tasks such as understanding implicit information, performing logical inferences, and applying commonsense knowledge. Reasoning capabilities are crucial for addressing complex language tasks like natural language inference, multi-hop question answering, and commonsense reasoning~\cite{yu2024natural}. Types of reasoning include Chain-of-Thought (CoT), which breaks problems into intermediate steps for clarity, deductive reasoning which applies general rules to specific cases, and inductive reasoning which generalizes from observations~\cite{xia2024beyond}. Abductive reasoning identifies the most plausible explanations, while analogical reasoning transfers knowledge based on similarities~\cite{shi2024language,lewis2024using}. Others include commonsense reasoning, probabilistic reasoning, and causal reasoning~\cite{yu2024natural}. These approaches enhance the interpretability and performance of NLP systems. 

\subsubsection{Reasoning in Time Series}
Time series analysis tasks traditionally focus on narrower objectives — like forecasting or anomaly detection — each addressed by its own specialized model, often relying solely on numerical patterns within the data. In contrast, time series reasoning with logic integrates multiple tasks under a single, context-aware framework with human-like reasoning~\cite{chow2024towards}. It readily incorporates domain knowledge and external data sources, providing natural language explanations and causal insights rather than mere numerical outputs~\cite{potosnak2024implicit}. This approach allows time series reasoning to adapt to shifting conditions and novel questions, delving into the “why” behind observed patterns and bridging the gap between automated analysis and real-world decision-making.

Furthermore, reasoning in NLP can enhance time series analysis by enabling models to infer complex temporal patterns and relationships, improving interpretability and decision-making~\cite{potosnak2024implicit,cai2024timeseriesexam}. By incorporating reasoning capabilities, models can better handle ambiguous or incomplete data, leading to more robust predictions and insights. Currently, few studies have explored this area~\cite{chow2024towards,ye2024beyond,xie2024chatts}. However, reasoning in time series analysis remains an underexplored yet promising and impactful field.

\section{Types of Reasoning}
We summarize reasoning structure types in Table~\ref{tab:reasoning_types}. It includes four types, End-to-end Reasoning, Forward Reasoning, Backward Reasoning, and Forward-Backward Reasoning. Each type is defined, exemplified, and mathematically formulated, highlighting their distinct characteristics and applications. End-to-end reasoning is characterized by its direct mapping from inputs to outputs, encapsulating the reasoning process within hidden states, which makes it less interpretable but effective for tasks requiring concise outputs. Forward Reasoning, on the other hand, adopts a bottom-up approach, explicitly stating intermediate steps, making it suitable for tasks like solving math problems or predicting time series trends sequentially. Backward Reasoning employs a top-down strategy, breaking down the main problem into smaller sub-problems, which is particularly useful for diagnostic tasks such as identifying the causes of anomalies. Lastly, Forward-Backward Reasoning combines forward and backward approaches, proposing potential solutions and verifying them, making it ideal for complex tasks like analyzing time series anomalies. The mathematical formulations provided for each reasoning type further elucidate the probabilistic frameworks underlying these reasoning processes, emphasizing their structured and systematic nature. Overall, the table underscores the importance of selecting the appropriate reasoning type based on the task's requirements and the desired level of interpretability.


\section{The Examples of Beyond Classical Time Series Tasks}
This section presents a set of figures that demonstrate the capabilities of different LLMs in handling zero-shot open questions across various application domains, going beyond classical time series tasks. The first figure, Figure~\ref{fig:case_healthcare}, focuses on healthcare applications. By providing an ECG time series recording and relevant background information, it examines how ChatGPT-o1 and Deepseek respond without and with other modal information. The models are tasked with making statistical judgments, determining the presence of anomalies in the time series, and identifying potential illnesses. ChatGPT-o1 and Deepseek analyze the data from different perspectives, considering factors like amplitude swings, baseline wander, and the shape of the QRS complex. They both acknowledge that while the data shows potential anomalies, further analysis, and clinical correlation are necessary to confirm a diagnosis, and noise or artifacts need to be ruled out. In addition, if we add other modal information, such as image information of normal as well as various abnormal ECG species to ChatGPT-o1, then this LLM can generate more logical as well as close to the doctor's answers.

Figure~\ref{fig:case_financial} delves into financial applications. Here, the input is a time series of Nvidia's stock prices from September 9, 2024 to January 24, 2025. The LLMs are required to perform causal inference and impact analysis regarding the stock price rise or fall and prospects. Without other modal information, ChatGPT-o1 and Deepseek analyze the overall trend, daily fluctuations, and the factors driving the price changes, such as market expectations, earnings reports, and macroeconomic conditions. When ChatGPT-o1 has access to other modal information (i.e., Financial Reports from Nvidia's official website), it incorporates detailed financial data from the company's reports to provide a more comprehensive analysis of the stock's performance and future prospects.

The third figure, Figure~\ref{fig:case_electricity}, is centered around electrical applications. It presents an incomplete time series of energy consumption from the London Smart Meters Dataset, with missing values represented by 'X'. The LLMs need to provide statistical analysis and impute the missing values. ChatGPT-o1 and Deepseek without other modal information use linear interpolation to fill the gaps, with explanations about why this method is suitable for maintaining the continuity of the data and supporting subsequent analyses. When ChatGPT-o1 has other modal information from the news of those days and some useful website links about local weather records, specifically knowing that the data was collected during a summer period with high temperatures, it employs quadratic interpolation to better capture the potentially rapid changes in consumption. 

Overall, these figures showcase the diverse ways in which LLMs can handle time series data in different real-world application scenarios, providing valuable insights and analysis for various fields. Moreover, if we add more related modal information, the generated answers will be more accurate, truthful, and realistic.


\begin{figure*}[!t]
\begin{center}
\includegraphics[width = 0.91\linewidth]{Healthcare.pdf}
\end{center}
\caption{Zero-shot open question performances of different LLMs and input settings for healthcare application.}
\label{fig:case_healthcare}
\end{figure*}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width = 0.92\linewidth]{financial.pdf}
\end{center}
\caption{Zero-shot open question performances of different LLMs and input settings for financial application.}
\label{fig:case_financial}
\end{figure*}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width = 0.92\linewidth]{electricity.pdf}
\end{center}
\caption{Zero-shot open question performances of different LLMs and input settings for electrical application.}
\label{fig:case_electricity}
\end{figure*}

\end{document}
