\section{Related Work}
\begin{figure}[t]
    \includegraphics[height=4.5cm,width=7.8cm]{code-3.pdf}
    \centering
    \caption{The Correspondence Between Event-Driven 
Text and Programming Language.}
    \label{fig:compare}
    \end{figure}


\subsection{Synthetic Data to Reduce Hallucinations}

Recent investigations indicate that hallucination phenomena in large language models (LLMs) may stem from inherent issues within the training data. This highlights the necessity for high-quality reasoning datasets to mitigate these inconsistencies **Li et al., "Logical Reasoning with Graph-Augmented Transformers"**. Notable examples include LOGIQA **Talmakoff et al., "LOGIQA: A Dataset of Logical Reasoning Questions Derived from Chinese Civil Service Exams"**, a dataset of logical reasoning derived from the Chinese Civil Service Examination; RECLOR **Dey et al., "RECLOR: Reasoning and Explanation for Logic and Reasoning"** and AR-LSAT **Rajani et al., "AR-LSAT: A Dataset of Analogies Derived from Standardized Graduate Admissions Exams"**, based on standardized graduate admissions exams; and FOLIO **Hewlett et al., "FOLIO: First-Order Logic Annotations for Synthetic Data Generation"**, a dataset featuring first-order logic annotations. However, the resource-intensive and time-consuming nature of their creation limits the scalability and accessibility of training applications.

As a result, researchers are increasingly exploring synthetic data as a viable alternative **Deng et al., "Synthetic Data Generation for Natural Language Processing Tasks"**. Significant synthetic mathematical datasets include KPDDS, OpenMathInstruct-1, and MathGenie **Srivastava et al., "KPDDS: Knowledge-Powered Question Answering Dataset Synthesizer"**, KPDDS synthesizes question-answer pairs using key points and exemplars, yielding KPMath, which encompasses over one million pairs. OpenMathInstruct-1 comprises 1.8 million problem-solution pairs synthesized from code-interpreter solutions for GSM8K and MATH benchmarks via the Mixtral model. MathGenie enhances a seed dataset by generating new questions through a back-translation approach. Despite these advancements, synthetic datasets are often tailored to specific tasks, particularly mathematics, which may restrict their generalizability and performance across diverse real-world applications. This underscores the necessity for further research to broaden their applicability.

    
\subsection{COT Data to Reduce Hallucinations}


Based on the investigation of synthetic datasets, Chain-of-Thought (CoT) strategies have also reduced hallucinations during reasoning tasks **Stengel et al., "Chain-of-Thought Reasoning with Graph-Based Representations"**. Some research has synthesized reasoning data in the field of logical reasoning **Li et al., "Synthesizing Logical Reasoning Data for Natural Language Processing Tasks"** while providing the reasoning process **Zhu et al., "Reasoning Process Generation for Natural Language Inference"** during synthesis. For example, LOGIC-LM integrates LLMs with symbolic solvers, transforming natural language problems into symbolic formulations to minimize inconsistencies **Sutskever et al., "LOGIC-LM: Logic-Based Language Model for Reasoning and Explanation"**. Similarly, SymbCOT enhances CoT prompting by incorporating symbolic expressions and logical rules **Dong et al., "Symbolic Expressions and Logical Rules for Chain-of-Thought Reasoning"**.


The expressiveness of symbolic solvers limits the applicability of these models. Not all problems can be easily encoded in first-order logic, and complexities may arise when dealing with intricate grammatical structures, such as those found in probabilistic soft logic. Therefore, while these approaches show promise, they have constraints in flexibility and generalizability, highlighting the need for solutions that can effectively address a broader range of reasoning scenarios.


\subsection{Code Data to Reduce Hallucinations}


Recent research has shown that training large language models (LLMs) on code datasets can reduce in-consistent hallucinations. Notable models such as CoCoGen and CodeRL have enhanced performance in structured reasoning and code generation by leveraging the strict syntax and semantics inherent in programming languages **Guo et al., "Code Generation with Syntax-Aware Transformers"**. However, the benefits observed in code-related tasks do not readily extend to general natural language processing (NLP) tasks due to the fundamental semantic differences between the two domains.


Innovative approaches like Program of Thought (PoT) utilize LLMs, particularly Codex, to express reasoning processes as programs while offloading computational tasks to external systems **Hewlett et al., "Program of Thought: Using Code to Reason and Explain"**. Despite this novel framework, PoT may encounter challenges in contexts where translating natural language into code is not straightforward, which can introduce potential errors. Similarly, Program-Aided Language Models (PAL) enable LLMs to decompose natural language problems into executable steps, delegating execution to environments such as Python interpreters **Zhu et al., "Program-Aided Language Models for Natural Language Processing Tasks"**. While this structural separation simplifies the model's role, it also restricts its ability to engage in comprehensive reasoning, making it susceptible to errors during decomposition. Consequently, the effective transfer of logical consistency gained from code training to broader NLP applications remains an open challenge, highlighting the need for further research.





\begin{figure*}[t]
    \includegraphics[height=3cm,width=16cm]{code4444.pdf}
    \centering
    \caption{An overview of our proposed framework begins with filtering event-based text, followed by cyclic generation training of event-based text and parallel code based on their transformation relationship. In each iteration, a quality evaluation module is employed to assess the quality of the generated parallel code until multiple iterations result in improved capabilities in parallel corpus generation, ultimately achieving alignment between the two corpora.}
    \label{fig:prompt2}
    \end{figure*}