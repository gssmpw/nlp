\section{Discussion of some open problems}\label{app: open problems}

\subsection{Practical computation of actions and action-set violations}\label{subsec: action feasibility}


In \cref{alg: alg shared,alg: alg shared + approx} we used a black-box solver for an entropic optimal transport problem. This is a computational abstraction and not implementable in practice. Implementing a computationally feasible resolution raises several questions. 

\subsubsection{Numerical resolution of the Kantorovich problem}

Sinkhorn's algorithm is the standard method for solving entropic optimal transport problems. It relies on the dual formulation of the entropic problem, that is
\begin{align}
    \ent(\mu,\nu,c,\ve) = \sup_{\substack{\varphi\in L^1(\mu)\\\psi\in L^1(\nu)\\\varphi\oplus\psi\le c}} \left\{\int \varphi\de\mu + \int \psi\de\nu - \ve\int e^{\ve^{-1}(\varphi+\psi-c)}\de(\mu\tensor\nu) + \ve\right\}\,\notag%\label{eq: entropic dual}
\end{align}
in the case $c\in L^1(\mu\tensor\nu)$, see e.g.\ \cite[Thm.~4.7]{nutz_introduction_2022}. The solution of the dual problem is given by the pair $(\varphi^*,\psi^*)$ which satisfies the SchrÃ¶dinger system
\begin{align*}
    \varphi^*&=-\ve\log\left(\int e^{\frac{\psi^*(y)-c(\cdot,y)}\ve}\de\nu(y)\right) \quad \mu\mbox{-a.s.}\\
    \psi^*&=-\ve\log\left(\int e^{\frac{\varphi^*(x)-c(x,\cdot)}\ve}\de\mu(x)\right) \quad \nu\mbox{-a.s.}\,.
\end{align*}

Sinkhorn's algorithm \citep{sinkhorn_concerning_1967}, in its application to this problem \citep{cuturi_sinkhorn_2013}, is a fixed-point iteration which improves one potential at a time. In other words, for $n\in\Nb$, it computes
\begin{align*}
    \varphi_{2n+1}&=-\ve\log\left(\int e^{\frac{\psi_{2n}(y)-c(\cdot,y)}\ve}\de\nu(y)\right) \\
    \intertext{and}
    \psi_{2n}&=-\ve\log\left(\int e^{\frac{\varphi_{2n-1}(x)-c(x,\cdot)}\ve}\de\mu(x)\right)\,.
\end{align*}

A primal solution to $\ent(\mu,\nu,c,\ve)$ can be recovered from the optimal dual potentials $(\varphi^*,\psi^*)$ via
\[ 
    \de\pi^*=e^{\frac{\varphi^*\oplus\psi^*-c}\ve}\de[\mu\tensor\nu]\,,
\]  
in which $\varphi^*\oplus\psi^*: (x,y)\mapsto \varphi^*(x)+\psi^*(y)$. Through an analogue for ${(\varphi_{2n+1},\psi_{2n})}_{n\in\Nb}$, we can obtain iterates ${(\varpi_n)}_{n\in\Nb}$.

\begin{lemma}[{\cite[Thm.~3.15]{eckstein_quantitative_2022}}]\label{lemma: bound on inf memory sinkhorn}
    If $c$ is Lipschitz on $\supp(\mu)\times\supp(\nu)$, and $\mu,\nu$ are sub-Gaussian measures, then the iterates ${\{\varpi_n(c)\}}_{n\in\Nb}$ of Sinkhorn's algorithm satisfy
    \[
        \entf(c^*,\varpi_n(c)) - \ent(\mu,\nu,c,\ve)\le C_0\ve n^{-\frac14}\,,
    \]
    for every $\ve>0$, in which $C_0$ is a numerical constant independent of $n$.
\end{lemma}
We omit the explicit dependencies in the constant $C_0$ as they are quite technical and require parsing a large part of~\cite{eckstein_quantitative_2022}, which proceeds from within a highly general framework. We should note, however, that consequently their bound is valid under much weaker assumptions than the ones stated here, and that the rate can, in fact, be improved if $c$ has sub-linear growth.


Unfortunately for regret minimisation, $\varpi_n$ need not be a transport plan in $\Pi(\mu,\nu)$, meaning it is not a valid action. Removing the requirement that $\pi_t\in\Pi(\mu,\nu)$ entirely would render the problem meaningless, as the regret can be made negative by finding a single point such that $c(x,y)<\kant(\mu,\nu,c)$, and playing $\delta_{(x,y)}$. 

As an auxiliary remark, this problem is one of the main hurdle to adapting \cref{alg: alg shared} to unknown marginals, as there would be no conceivable way to pick valid transport plans, which renders the analysis a non-starter.


\subsubsection{On action violations}


Two possible directions appear to resolve this issue: one at the level of bandit design, and one at the level of numerical optimal transport. The former revolves around the idea of incorporating action-set violations to regret analysis, the latter around the idea of modifying Sinkhorn's algorithm to produce valid primal iterates at each step, e.g.\ by projecting onto $\Pi(\mu,\nu)$.

The question of violating action sets has been posed before in Bandit Theory and has also arisen in practical use-cases in Reinforcement Learning, see \citep{seurin_im_2020}. It is a staple topic in the context of fairness, see e.g.\ \citep{joseph_fairness_2016} and of contextual bandits (including linear stochastic bandits) in which various other types constraint have also been considered, see e.g. \citep{liu_efficient_2024}. These types of constraints typically, in effect, disable certain arms at certain times, a generic setting which has been considered as well, e.g.\ by~\cite{kleinberg_regret_2010,abensur_productization_2019}. 

These works adopt a range of strategies to formulate the problem in a meaningful way, but their perspectives don't really fit with the real challenge we have with the OT problem. The problem isn't so much that the constraints placed on the action set are complicated: $\Pi(\mu,\nu)$ is a convex, compact set defined by linear inequalities. The problem arises entirely from the facts that $\Pi(\mu,\nu)$ is infinite-dimensional, and that it is a subspace of $\Ps(\state)$, whose geometry is far from straightforward.

A preliminary exploration of this topic would likely require a taxonomy of the different possible violations of $\Pi(\mu,\nu)$. Indeed, $\pi_t$ could violate one or both marginal constraints, or it could even fail to be a probability measure through the total mass or positivity conditions. It appears likely that these will have quite different impacts both on the problem's geometry and on practical usefulness. Thereafter, one might consider whether guaranteeing finitely many violations, as~\cite{liu_efficient_2024} do, or developing a penalised regret is more appropriate.

The alternative would be to design an algorithm which optimises the entropic or Kantorovich problems through while staying within the constraint set $\Pi(\mu,\nu)$ (either for all time, or once it reaches a desired precision). On the one hand, there are finite-dimensional intuitions for this to work as Sinkhorn's algorithm can be viewed as a form of gradient descent \citep{leger_gradient_2021}, which could be projected onto $\Pi(\mu,\nu)$ (which is convex and compact). On the other hand, the geometry of $\Pi(\mu,\nu)$ as an infinite-dimensional probability space is likely to make rigourously doing so (and deriving convergence rates) quite arduous work. 


\subsection{Extensions to the Monge problem}\label{subsec: Monge pb}

The \emph{Monge} optimal transport problem associated to $(\mu,\nu,c)$ is
\begin{align}
    \monge(\mu,\nu,c):= \inf_{T\in\Ts} \int c(x,T(x))\de\mu(x)\,,
    \label{eq: monge def}
\end{align}
in which $\Ts$ is the set of all $\mu$-measurable maps  $T:\Mc_\mu\to\Mc_\nu$ such that $\mu(T^{-1}(\cdot))=\nu$. Chronologically, this is in fact the original formulation of the OT problem \citep{monge1781memoire}. 

The Monge problem is best approached through finite-dimensional practical applications such as \emph{matchings} of students to universities, employees to employers, etc. The requirement that the map $T$ be a function imposes an \emph{indivisibility} of the mass $T$ moves from $\mu$ to $\nu$ (i.e.\ one university per student). This makes the resolution of the problem much more difficult. For example, if $\mu$ and $\nu$ each have two atoms with weights $(1/2,1/2)$ and $(1/3,2/3)$ respectively, then $\Ts=\emptyset$, meaning $\monge(\mu,\nu,\cdot)\equiv +\infty$, and the problem is never solvable. 

If $\mu,\nu$ are non-atomic, $\monge(\mu,\nu,c)$ can be interpreted as the cheapest way (w.r.t. $c$) to transport a $\mu$-shaped pile of infinitesimally small things into a $\nu$-shaped one, but its geometry remains complicated. 
The Kantorovich relaxation drastically simplified the geometry of the problem and remains one of the most effective tools to approach the Monge problem, which is why it is accepted as the standard in modern OT theory.

Note that the relaxation from $\monge(\mu,\nu,c)$ to $\kant(\mu,\nu,c)$ is known to be exact in some cases, such as $c=\norm{\cdot-\cdot}^2/2$ with $\Mc_\mu=\Mc_\nu=\Rb^d$, $(\mu,\nu)$ having second-order moments and $\mu$ being absolutely continuous w.r.t.\ the Lebesgue measure~\cite[Thm.~5.2]{ambrosio_lectures_2021}. See also \citep[Thm.~5.30]{villani_optimal_2009} for weaker conditions. 
But it is also known (e.g.\ via the above example) that this  relaxation is not without loss.

If we want to learn a Monge problem, we must, of course, make sufficient assumptions for it to be solvable, but more importantly we must face the issue that~\eqref{eq: monge def} is now a non-linear functional and that $\Ts$ is not as docile a set as $\Pi(\mu,\nu)$. Here, the recent work in statistical optimal transport on learning Monge maps (i.e.\ the solutions to~\eqref{eq: monge def}) is highly relevant, see e.g.~\cite[Ch.~3]{chewi_statistical_2024} or the paragraph in \cref{app: biblio} below. Though once again most work focuses on the batch sampling of marginals, not on online learning. This line of work would appear to also require more general results about the learning of minima of non-linear functionals, which are not yet available in the literature. Overall, it remains unclear if the Monge problem is on a similar or different level of difficulty to the Kantorovich problem as it is not clear that the techniques to reduce to online least-squares we used will transfer. 

Beyond these statistical issues, one should also expect the problems of effective optimisation from \cref{subsec: action feasibility} to return with a vengeance as the Monge problem is a fully non-linear problem unlike the Kantorovich problem which is an (infinite-dimensional) linear program.

