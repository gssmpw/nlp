\section{Bibliographical complements on statistical optimal transport}\label{app: biblio}

    An excellent detailed history of the development of OT as a mathematical theory, replete with bibliographical notes, can be found in~\cite[Ch.~3]{villani_topics_2003}. Summarising this field's venerable history further would be of little value. Instead, we will expand on relevant research specifically about \textit{learning} optimal transport problems. We touch on key aspects of the literature below, and refer to the forthcoming book~\cite{chewi_statistical_2024}, for a deeper longitudinal overview.
  
        \paragraph{Estimation of Wasserstein distances}
            One of the most important contributions of optimal transport is a family of useful distances between probability measures: the Wasserstein metrics. The study of these distances has allowed major progress on the geometry of spaces of probability measures, and has been used in many applications. It is therefore natural that the estimation of these distances has been a major topic of interest in the learning of optimal transport. 

            The key question here is the convergence in Wasserstein distance of an empirical distribution to the true distribution. Pioneering work on this topic began in the 80s and 90s, see \citep{ajtai_optimal_1984,talagrand_transportation_1994}, with the study of \emph{Matching} (i.e.\ discrete optimal transport). Key statistical analysis of this problem includes finite sample bounds, see \citep{horowitz_mean_1994} and more recently \citep{fournier_rate_2015,weed_sharp_2019} among others, as well as distributional limits, see e.g.\
            \citep{tameling_empirical_2019} and references therein. 

            Sadly, most work has remained limited to Wasserstein distances rather than generic cost functions, owing to a reliance on the pleasant geometric properties that they enjoy.

        \paragraph{Estimation of Entropic OT}
            Motivated by the success of Entropic OT in designing numerical solution to OT problems, see \citep{cuturi_sinkhorn_2013}, work on the Entropic problem has focused on estimating $\ent(\mu,\nu,c,\ve)$ using $\ent(\hat\mu_n,\hat\nu_n,c,\ve)$, for empirical measures $(\hat\mu_n,\hat\nu_n)$. This has often gone together with estimation for the Schr√∂dinger potentials $(\varphi,\psi)$ of~\eqref{eq: entropic dual}.

            While this is very much the same type of study as for the Kantorovich problem in Wasserstein metrics, it should be noted that the entropic problem exhibits qualitatively different behaviour. While learning the Kantorovich problem exhibits a curse of dimensionality, the entropic problem exhibits parametric-rate (dimension-free) convergence, as shown by~\cite{genevay_sample_2019,rigollet_sample_2022}. This was tempered by large dependencies in other problem quantities, which were reduced over time \citep{stromme_minimum_2024} and were complemented by distributional limits, see e.g.\ \citep{gonzalez-sanz_weak_2024}.

        \paragraph{Estimation of Monge maps}
            While the estimation of Wasserstein distances is mostly motivated by statistical applications, the estimation of Monge maps is motived by effectively solving transport problems in an applied context. Here, one sees samples from two marginals $\mu$ and $\nu$, and attempts to estimate $T^*$ the minimiser of~\eqref{eq: monge def}. 
            
            There has been a significant amount of machine learning and statistics literature on this topic, following on from \citep{hutter_minimax_2021,gunsilius_convergence_2022}. Various types of estimators have been constructed, either derived from optimal transport theory \citep{hutter_minimax_2021}, or from plug-in estimates using classical machine learning methods such as $k$-NN \citep{manole_plugin_2024,deb_rates_2021}. 

        % \paragraph{Estimation of Dual potentials}
        %     (related via resolution of the Kantorovich problem, but offline and full information)
        
        \paragraph{Optimal transport applied to learning}

        While these bibliographical notes concern learning in optimal transport let us conclude by underline that the machine learning community has used optimal transport to impressive success in applications. One could highlight in particular Wassertein GANs \citep{arjovsky_wasserstein_2017} and subsequent works, e.g.\ \citep{salimans_improving_2018} as well as the field of domain adaptation \citep{courty_joint_2017,torres_survey_2021}
