\subsection{Provably sub-linear regret}\label{subsec: computability}

Multiple approaches are possible to reduce the dimensionality of the learning problem and control the determinants in the regret. Below we sketch a method inspired by functional regression \citep{morris_functional_2015} which directly leverages the regularity of $c^*$. A complete treatment is deferred to \cref{app: computation}.

By controlling the decay of the coefficients in this basis (see \cref{asmp: basis decay}), we can quantify the regularity properties of $c^*$, and thus the complexity of learning. If only finitely-many coefficients are non-zero, \cref{cor: on finite basis regret} shows that the regret is the same as finite-dimensional linear bandits \citep{abbasi-yadkori_improved_2011}. However, if this is not the case but we have a control on the decay rate of ${(\gamma^*_i)}_{i\in\Nb}$, we can adapt the approximation order with time to ensure we learn optimally, but this will degrade the regret because of a trade-off between approximation error and confidence set size. In \cref{cor: restating bounds for intrinsic regularity}, we balance the trade-off to quantify the overall rate in terms of the regularity of $c^*$. 

\begin{restatable}{assumption}{asmpthree}\label{asmp: basis decay}
    There is a known orthonormal basis ${(\phi_i)}_{i\in\Nb}$ of $L^2(\Rb^d;\varrho)$ in which we write $\fourier c^*:=\sum_{i=1}^n\gamma_i^*\phi_i$ and $\zeta:\Rb_+\to[0,1]$, a known monotonically increasing continuous function satisfying
    \[ 
        \inf_{n\in\Nb}\frac{\sum_{i=1}^{n}\abs{\gamma_i^*}}{\zeta(n)}\ge \norm{c^*}_{L^2(\Rb^d;\varrho)}\,.
    \]
\end{restatable}

\begin{restatable}[\cref{cor: regret for fixed approximation order with bounded basis}]{corollary}{CorOnFiniteBasis}\label{cor: on finite basis regret}
    Under \cref{asmp: basis decay,asmp: L2 case,asmp: estimate + subG}, if $\zeta(n)=\1_{\{n\ge N\}}$ for some $N\in\Nb$, then regret of $\tilde\Oc(\sqrt{NT})$ is achievable by using $N$ dimensions of the basis at each $t\in\Nb$.
\end{restatable}

\Cref{cor: on finite basis regret} applies, e.g., to discrete problems, in which $\abs{\supp(\mu)}=K$ and $\abs{\supp(\nu)}=K'$ yield $\tilde\Oc(\sqrt{KK'T})$ regret, and $p$-dimensional parametric models, in which $\tilde\Oc(\sqrt{pT})$ regret is achievable.

\begin{corollary}[\cref{thm: regret for varying approximation}]\label{cor: restating bounds for intrinsic regularity}
    Assume \cref{asmp: estimate + subG,asmp: L2 case,asmp: basis decay} and $\zeta(n)=1-n^{-q}$ for some $q>0$, then regret of $\tilde\Oc(T^{\frac{q+1}{2q+1}})$ is achievable by using $\ceil{t^{\frac{1}{2q+1}}}$ dimensions of the basis at each step $t\in\Nb$.
\end{corollary}

%If a bound is known on the decay rate, we can change the approximation order $n$ over time to ensure that learning is preserved. Of course, increasing $n$ causes the bounds on the determinant terms of the regret to grow, while it reduces the approximation error. This leads to a trade-off, which we can quantify by specifying a decay speed. Balancing this trade-off leads to \cref{cor: restating bounds for intrinsic regularity}.

 
\Cref{cor: restating bounds for intrinsic regularity} shows that the regret of \cref{alg: alg shared} is controlled by the regularity of the cost function $c^*$, which varies from $\Oc(\sqrt{T})$ for $q\to +\infty$ down to $\Oc(T)$ as $q\downarrow0$. Note that $q=0$ corresponds to $c^*$ being an indicator function, in which learning the optimal plan is clearly infeasible. In this manner the learning complexity is captured directly as a function of the regularity of the cost function. 



