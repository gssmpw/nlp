

\section{Controlling the infinite dimensional terms}\label{app: computation}



The parametric and RKHS estimation methodologies are highly standard in bandit theory, because they seamlessly fit into the general Hilbert Space analysis of~\cite{abbasi-yadkori_online_2012} while giving a control on the resulting regret bounds in terms of finite dimensional quantities. In Fourier analysis and fields which rely on it, such as functional regression \citep{morris_functional_2015}, it is more natural to look for approximations by decomposing $\fourier c^*$ and $a_t$ into an orthonormal basis and truncating it at some finite order. We detail this learning methodology below.

We being in \cref{subsec: Fourier basis representation} by presenting the general concept of basis decomposition as an approximation method. Then, in \cref{subsubsec: finite order} we truncate at a fixed order and derive the regret bounds for this case. Before moving on to changing the truncation order with $t\in\Nb$ in \cref{subsubsec: increasing order}, we give a brief discussion in \cref{subsubsec: parametric rate /matching} of some examples in which a finite basis is sufficient. Finally, we give a brief treatment of kernel methods in \cref{subsec: tikhonov and RKHS} for completeness.



\subsection{Intrinsic regularity and fourier basis decay}\label{subsec: Fourier basis representation}

To simplify notation, let $f^*:=\fourier c^*$. Recall the chosen orthonormal basis ${(\phi_i)}_{i\in\Nb}$ of the space $L^2(\Rb^d;\varrho)$, in which $(f^*,a_t)\in {L^2(\Rb^d;\varrho)}^2$, $t\in\Nb$, admit representations 
\[
    f^*= \sum_{i=0}^\infty \gamma_i^*\phi_i\quad\mbox{ and }\quad a_t = \sum_{i=0}^\infty \vartheta^{(t)}_{i}\phi_i\,, \quad \mbox{ for some }\quad (\gamma^*,\vartheta^{(t)})\in{\ell_2(\Rb)}^2\,.
\] 
Classical choices for ${(\phi_i)}_{i=\in\Nb}$ are wavelet systems such as the Haar or Hermitian systems, and the Fourier basis if $\supp(\mu)\x\supp(\nu)$ is bounded. The choice of a specific basis is made \textit{ad hoc} from knowledge of the structure of the problem; we present the general argument. 

By definition of ${(\phi_i)}_{i\in\Nb}$ as an orthonormal basis, we have 
\[
    \langle f\vert a_t\rangle_{L^2(\Rb^d;\varrho)}=\langle \gamma^*\vert \vartheta^{(t)}\rangle_{\ell_2(\Rb)}=\sum_{n=1}^{+\infty} \gamma_n\vartheta^{(t)}_n\,.
\]

Let $f^*\vert_n:=\sum_{i=0}^n\gamma_i^*\phi_i$ be the truncation of the basis expansion of $f$ at order $n\in\Nb$. By abuse of notation, and only when it is clear from context, we will override notation and denote $c^*\vert_n$ the result of applying the inverse fourier transform to $(f^*)\vert_n$, the basis truncation of $f^*:=\fourier c^*$.
A straightforward derivation yields the approximation bound of \cref{lemma: fourier decay bound}.

\begin{lemma}\label{lemma: fourier decay bound}
    Let ${(\phi_i)}_{i\in\Nb}$ be an orthonormal basis of $L^2(\Rb^d;\varrho)$, and let $f\in L^2(\Rb^d;\varrho)$ with $f:=\sum_{i=0}^\infty \gamma_i\phi_i$. Then, we have
    \[
    \abs{\left\langle f - f\vert_n\vert g\right\rangle }_{L^2(\Rb^d;\varrho)}\le \norm{g}_{L^2(\Rb^d;\varrho)}\sum_{k=n+1}^{+\infty}\abs{\gamma_n^*}\quad \mbox{ for every } g\in L^2(\Rb^d;\varrho).
    \]
\end{lemma}

For our purpose, $g=a_t$ is bounded by $1$ since $\norm{\fourier\pi_t}_\infty\le\pi_t(\Rb^d)=1$ and $\varrho(\Rb^d)=1$, so that the resulting approximation error is controlled entirely by the decay of the coefficients ${(\gamma_i^*)}_{i\in\Nb}$. Consequently, regret analysis can leverage \cref{lemma: fourier decay bound} to move the problem into a finite dimensional regression problem on the coefficients ${(\gamma_i^*)}_{i\in\Nb}$. We begin by setting the stage with a fixed order (i.e.\ $n$ independent of $t$) methodology. Later, we will derive regret guarantees when $n$ is allowed to grow with $t$ in order to control the approximation error. 


\subsection{Fixed order basis truncation}\label{subsubsec: finite order}

In this section, let $n\in\Nb$ be fixed. One can approximately regress $\bm{R}_t$ against $a_t$ up to order $n$ by solving the $n$-dimensional Regularised Least-Squares (RLS) problem
\begin{align}
    \hat \gamma^{n,\lambda}_t:=\argmin_{\gamma\in\Rb^n} \sum_{s=1}^t \norm{R_s - \sum_{i=1}^n\gamma_i\vartheta^{(s)}_i}_2^2 + \lambda \Lambda_n(\gamma)\,,\label{eq: RLS  basis truncation}
\end{align}
in which $\Lambda_n:\Rb^n\to[0,+\infty)$ is a strictly convex continuously Fréchet-differentiable regulariser such that its Fréchet derivative $\De\Lambda_n$ satisfies
\[
    \frac1{M_{\Lambda_n}}\Id\preceq \De\Lambda_n\preceq M_{\Lambda_n}\Id\,.
\] 

For clarity, let $\vartheta^{(s,n)}$ denote the truncation of $\vartheta^{(s)}\in\Rb^\Nb$ at order $n$, so that $\vartheta^{(s,n)}\in\Rb^{n}$ and $\vartheta^{(s,n)}_i=\vartheta^{(s)}_i$ for all $i\in[n]$.
Following the standard arguments for online linear regression (omitted for brevity, see e.g.\ \cite{abbasi-yadkori_improved_2011,abbasi-yadkori_online_2012}), one can construct the (valid, by \cref{cor: probability of confsets order n}) confidence sets
\begin{align}
    \tilde\confset_{t}^{n}(\delta):=\left\{\gamma\in\Rb^{n}: \norm{\gamma -\hat \gamma^{n,\lambda}_t}_{\tilde\design_t^{\lambda,n}}\le \tilde\width_{t,n}(\delta)\right\}\label{eq: confidence set fixed order}\,,
\end{align}
in which $\tilde\design_t^{\lambda,n}:= \lambda\De\Lambda_n + \sum_{s=1}^t \vartheta^{(s,n)}{\vartheta^{(s,n)}}^\top$ and 
\begin{align}
    \tilde\width_{t}^{n}(\delta):=\sigma\sqrt{\log\left(\frac{4\det\left(\De\Lambda_n+\lambda^{-1}\sum_{s=1}^t \vartheta^{(s,n)}{\vartheta^{(s,n)}}^\top\right)}{\delta^2}\right)} +{\left(\frac\lambda{\norm{\De\Lambda_n}_\op}\right)}^{\frac12}\ubnorm\,.
\label{eq: width fixed order}
\end{align}
Notice that $C>\norm{c^*}_{L^2(\Rb^d;\varrho)}$ implies that $C\ge \norm{\gamma^*}_{\ell_2(\Rb)}$ by definition of ${(\phi_i)}_{i\in\Nb}$, so that $\ubnorm$ is a valid upper bound on $\norm{\gamma^*}_{\ell_2(\Rb)}$. To verify the validity of the confidence sets (see \cref{cor: probability of confsets order n}), let 
    \begin{align}
        \tilde{\event_t^{n}}(\delta):=\left\{ \norm{\gamma -\hat \gamma^{n,\lambda}_t}_{\tilde\design_t^\lambda}\le \tilde\width_{t}^{n}(\delta) \right\}\quad \mbox{ for }\quad (t,n)\in\Nb^2\,.\label{eq: event def for fixed order approx}
    \end{align} 

\begin{corollary}\label{cor: probability of confsets order n}
    Under \cref{asmp: estimate + subG,asmp: L2 case}, for every $\delta>0$, $\lambda>0$, $n\in\Nb$, 
    \[
        \Pb\left(\bigcap_{t=1}^\infty \tilde{\event_t^{n}}(\delta)\right)\ge 1-\frac\delta2\,.
    \]
\end{corollary}

\begin{proof}
    Follow the proof method of \cref{lemma: probability of confsets unif in N} (or apply \cref{lemma: confidence sets with varying basis order} below).
\end{proof}


Applying this learning methodology to \cref{alg: alg shared} in place of the infinite-dimensional RLS, and with the optimistic choice of belief-action pairs
\begin{align}
    (\tilde\pi_t,\tilde \gamma_t^n)\in \argmin_{\substack{\pi\in\Pi(\mu,\nu)\\ \gamma\in \tilde\confset_{t}^{n}(\delta)}}\entf\left(\mu,\nu,\sum_{i=1}^n\gamma_{i}\phi_i,\ve\right)\label{eq: optimism for finite order}
\end{align}
yields \cref{alg: alg shared + approx} with ${(n_t)}_{t\in\Nb}={(n)}_{t\in\Nb}$ and the regret bound of \cref{cor: regret for fixed approximation order}.

\begin{algorithm}
    \caption{\namealgtwo{}\label{alg: alg shared + approx}}
    \SetKwFunction{approxpi}{ApproximateAction}
    \KwData{Confidence $\delta$, regularization level $\lambda$, entropy penalisation ${(\ve_t)}_{t\in\Nb}$, orders ${(n_t)}_{t\in\Nb}$.}
    \For{$t\in\Nb$}{
            Compute the RLS estimator $\hat \gamma^{n_t,\lambda}_t$ using~\eqref{eq: RLS  basis truncation}\;
            Construct the confidence set $\tilde\confset_{t}^{n_t}(\delta)$ using~\eqref{eq: confidence set fixed order} and~\eqref{eq: width fixed order}\;
            Optimism:  pick $ (\tilde\pi_t,\tilde\gamma^{n_t}_t)$ according to~\eqref{eq: optimism for finite order}\;
        Play $\pi_t=\tilde\pi_t$ if $t>0$, else $\pi_0=\mu\tensor\nu$; receive feedback $R_t$\;
    }
\end{algorithm}

\begin{restatable}{corollary}{RegretCorrForFixedApproximationOrder}\label{cor: regret for fixed approximation order}
    Under \cref{asmp: estimate + subG,asmp: L2 case}, for any $\delta>0$, $\lambda>0$, $T\in\Nb$, using \cref{alg: alg shared + approx} with ${(\ve_t)}_{t\in\Nb}={(\ve)}_{t\in\Nb}$ and ${(n_t)}_{t\in\Nb}={(n)}_{t\in\Nb}$ (denoted $\Ac_n$) yields
    \begin{align}
        \regret_T^{\entropy,\ve} (\Ac_n)&\le  \sigma\sqrt{2T\log\left(\frac2\delta\right)} + 2\ubnorm\sqrt{nT}\left(\log\left(\frac{M_{\Lambda_n}}\lambda + \frac{t \ubnorm^2}n\right)+\frac{n}{2(1\wedge\lambda\ubnorm)}\log M_{\Lambda_n} \right)\notag\\
        &\quad +2T \sum_{k=n+1}^{+\infty}\abs{\gamma_k^*}\,, \label{eq: entropic regret for fixed approximation order} \\
        \intertext{ while using ${(\ve_t)}_{t\in\Nb}={(\alpha t^{-\alpha})}_{t\in\Nb}$ and ${(n_t)}_{t\in\Nb}={(n)}_{t\in\Nb}$ (denoted $\Bc_n$) yields }
        \regret_T(\Bc) &\le \sigma\sqrt{2T\log\left(\frac2\delta\right)} + 2\ubnorm\sqrt{nT}\left(\log\left(\frac{M_{\Lambda_n}}\lambda + \frac{t \ubnorm^2}n\right)+\frac{n}{2(1\wedge\lambda\ubnorm)}\log M_{\Lambda_n} \right)\notag\\
        &\quad +\frac{\kappa\alpha}{1-\alpha}\left(T^{1-\alpha}\log(T) + \frac{\alpha}{2^\alpha}\log(6)\right) + 2T \sum_{k=n+1}^{+\infty}\abs{\gamma_k^*}\,.  \label{eq: Kantorovich regret for fixed approximation order}
    \end{align}
\end{restatable}

\begin{proof}
    The proof follows the usual decomposition up the following modifications which are the same for both~\eqref{eq: entropic regret for fixed approximation order} and~\eqref{eq: Kantorovich regret for fixed approximation order}. We give the modification for \cref{thm: entropic regret}, the same modifications need only be applied to \cref{thm: regret Kantorovich} to complete the proof of the second bound.  

    At the second step of the proof, let $\bar\pi^{\epsilon}$ be an $\epsilon$-minimiser of $\ent(\mu,\nu,c^*,\ve)$, for $\epsilon>0$, and decompose $\bar r_t := \entf (c^*,\pi_t)-\ent(\mu,\nu,c^*,\ve)$ as
    \begin{align}
        \bar r_t &\le  \epsilon + \entf (c^*,\pi_t) - \entf(c^*, \bar\pi^{\epsilon})\notag \\
        & \le \epsilon +\entf (c^*\vert_n,\pi_t) - \ent(\mu,\nu,c^*\vert_n,\ve) \notag \\
        &\quad +\entf (c^*,\pi_t) -\entf (c^*\vert_n,\pi_t)  + \entf(c^*\vert_n, \bar\pi^{\epsilon})- \entf(c^*, \bar\pi^{\epsilon})\notag \\% \entf(c^*\vert_n,\pi_t) - \ent(\mu,\nu,c^*,\ve)\,.
        &\le \epsilon +\entf (c^*\vert_n,\pi_t) - \ent(\mu,\nu,c^*\vert_n,\ve) + 2\sum_{k=n+1}^{+\infty}\abs{\gamma_k^*}\,,\notag
    \end{align}
    by a double application of \cref{lemma: fourier decay bound} combined with the bound $\norm{a_t}_{L^2(\Rb^2;\varrho)}\le 1$. 
    Sending $\epsilon\to0$ allows one to then continue the proof, up to replacing the events $\event_t(\delta)$ by $\tilde{\event_t^{n}}(\delta)$, and \cref{lemma: probability of confsets unif in N} by \cref{cor: probability of confsets order n}. 

    Finally, let us introduce $\tilde c_t^{\,n}:= \sum_{i=1}^n \tilde\gamma_{t,i}^{n}\phi_i$ for $t\in\Nb$, so that by \cref{lemma: bound on width term}, we can directly derive
    \begin{align}
        \sum_{t=1}^T\langle c^*\vert_n -\tilde c_t^{\,n}\vert \tilde\pi_t\rangle\le 2C\tilde\width_{T,n}(\delta)\sqrt{nT\log\det\left(\idmat+\frac{1}{2\lambda C}\sum_{t=1}^T \vartheta^{(t,n)}_t\De\Lambda_n^{-1}{\vartheta^{(t,n)}}^\top\right)}\,.\notag
    \end{align}

    To obtain the stated bounds, it remains to bound 
    \[
        \det\left(\De\Lambda_n+\lambda^{-1}\sum_{t=1}^T \vartheta^{(t,n)}{\vartheta^{(t,n)}}^\top\right) \quad\mbox{and}\quad \det\left(\idmat+\frac{1}{2\lambda C}\sum_{t=1}^T \vartheta^{(t,n)}\De\Lambda_n^{-1}{\vartheta^{(t,n)}}^\top\right)
    \]
    using \cref{lemma: bounds for the determinants in finite dimension}.
\end{proof}


\begin{lemma}\label{lemma: bounds for the determinants in finite dimension}
    Under \cref{asmp: estimate + subG,asmp: L2 case}, for $(n,t)\in\Nb^2$, we have
    \begin{align}
        \log\det\left(\De\Lambda_n+\lambda^{-1}\sum_{t=1}^T \vartheta^{(t,n)}{\vartheta^{(t,n)}}^\top\right)\le \log\left(\frac{M_{\Lambda_n}}\lambda + \frac{t \ubnorm^2}n\right)+n\log M_{\Lambda_n}\,.\label{eq: bound for the determinindant of beta in finite dimension}\\
        \log\det\left(\idmat+\frac{1}{2\lambda C}\sum_{t=1}^T \vartheta^{(t,n)}_t\De\Lambda_n^{-1}{\vartheta^{(t,n)}}^\top\right)\le \log\left(\frac{M_{\Lambda_n}}\lambda + \frac{t \ubnorm^2}n\right)+\frac{n}{2\lambda\ubnorm}\log M_{\Lambda_n}\,.
        \label{eq: bound for the determinindant of width in finite dimension}
    \end{align}
\end{lemma}

\begin{proof}
    We take the two bounds in turn. First, apply the matrix determinant lemma to obtain
    \[
        \det\left(\De\Lambda_n+\lambda^{-1}\sum_{t=1}^T \vartheta^{(t,n)}{\vartheta^{(t,n)}}^\top\right)\le \det(\De\Lambda_n)\det\left(\idmat+\lambda^{-1}\sum_{t=1}^T \vartheta^{(t,n)}\De\Lambda_n^{-1}{\vartheta^{(t,n)}}^\top\right)\,,
    \]
    which can be readily bounded as in \citep[Lemma E.3]{abbasi-yadkori_online_2012} by noticing that $\snorm{\vartheta^{(t,n)}}_2\le\norm{c^*}_{L^2(\Rb^d;\varrho)}$ (with $\det(\De\Lambda_n)\le M_{\lambda_n}^n\vee 1$) to obtain~\eqref{eq: bound for the determinindant of beta in finite dimension}.

    For the second bound, apply \citep[Lemma E.3]{abbasi-yadkori_online_2012} directly to obtain
    \begin{align}
        \det\left(\idmat+\frac{1}{2\lambda C}\sum_{t=1}^T \vartheta^{(t,n)}\De\Lambda_n^{-1}{\vartheta^{(t,n)}}^\top\right) \le {\left(\frac{2\lambda\ubnorm \Tr(\De\Lambda_n)+t\ubnorm^2 }n\right)}^n\left(2\lambda \ubnorm\det(\De\Lambda_n)\right)\notag
    \end{align}
    wherefrom~\eqref{eq: bound for the determinindant of width in finite dimension} follows.
\end{proof}


\subsection{Finite order bases: matching and parametric models}\label{subsubsec: parametric rate /matching}


At this point, let us recall \cref{asmp: basis decay} which provides the quantification of the regularity of $c^*$ which we will use to set $n$.  We will now discuss some examples in which a finite basis is sufficient to control the approximation error.

\asmpthree*


\begin{proposition}\label{cor: regret for fixed approximation order with bounded basis}
    Under \cref{asmp: basis decay,asmp: estimate + subG,asmp: L2 case}, with $\zeta(n)\1_{\cdot\ge N}$ for some $N\in\Nb$ (i.e.\ if $\gamma_i^*=0$ for every $i>N$), then under the conditions of \cref{cor: regret for fixed approximation order} with $n=N$, $\Lambda_n=\norm{\cdot}_{L^2(\Rb^d;\varrho)}/2$, and $\alpha=1/2$ the bounds of \cref{cor: regret for fixed approximation order} become
    \begin{align}
        \regret_T^{\entropy,\ve} (\Ac_n)&\le \sigma\sqrt{2T\log\left(\frac2\delta\right)} + 2\ubnorm\sqrt{NT}\log\left(\frac{1}\lambda + \frac{T \ubnorm^2}N\right) \label{eq: entropic regret for fixed approximation order with bounded basis}\\
        \intertext{ and }
        \regret_T(\Bc_n)&\le  \sigma\sqrt{2T\log\left(\frac2\delta\right)} + 2\ubnorm\sqrt{NT}\log\left(\frac{1}\lambda + \frac{T \ubnorm^2}N\right) +\kappa(1+\sqrt{T}\log(T))\label{eq: kantorovich regret for fixed approximation order with bounded basis}
    \end{align}
\end{proposition}


Naturally, the assumption that $\gamma_i^*=0$ for any $i>N$ is not satisfactory, but it is verified for several existing models and serves to demonstrate that some learning problems in BOT are learnable at the rate $\tilde\Oc(\sqrt{T})$ given only knowledge of an upper bound on $N$ and $\norm{\gamma^*}_{\ell_2(\Rb)}$ and an appropriate basis ${(\phi_i)}_{i\in\Nb}$.  

Consider a matching problem in which the measures $\mu$ and $\nu$ are supported on $K$ and $K'$ loci respectively. Let $\{x_1,\ldots, x_K\}=\supp(\mu)$ and $\{x'_1,\ldots, x'_{K'}\}=\supp(\nu)$ denote these loci. We can let $c^*$ assume arbitrarily values outside of $\state=\{(x_i,x_j'):(i,)\in[K]\x [K']\}$ without loss of generality. Let $\epsilon<\inf\{\norm{u-v}:(u,v)\in\state^2\,,\; u\neq v\}$ and define the functions 
\[
    \phi_{i,j}:= \frac{6}{\pi\epsilon^3}\1_{\{B_2({(x_i,x_j')}^\top,\epsilon/2)\}} \quad \mbox{ for } (i,j)\in[K]\times[K']\,.
\]
Re-indexing the functions by $k\in[K\times K']$, and adding suitable functions for $k>KK'$, we obtain an orthonormal basis ${(\phi_k)}_{k\in\Nb}$ of $L^2(\Rb^d;\varrho)$, in which $c^*:=\sum_{k=1}^{KK'}\gamma_k^*\phi_k$. Consequently, we can apply \cref{cor: regret for fixed approximation order with bounded basis} with $N=KK'$ to obtain a regret bound of $\tilde\Oc(\sqrt{KK'T})$ for the learning problem.

Alternatively, consider that there is a parametric model for $c^*$, i.e.\ there is $\theta^*\in\Rb^p$ such that 
\[
    c^*(x,y) = \sum_{i=1}^p {\theta^*}_i\Phi_i(x,y)\,,
\]
for some embedding function $\Phi:\Rb^d\x\Rb^d\to\Rb^p$. When the embedding function is known, one can construct a basis through the Gram-Schmidt process. Let $\phi_1:=\Phi_1/\norm{\Phi_1}_{L^2(\Rb^d;\varrho)}$, and for $i\le p$, define $S_i:={\{\phi_k:k<i\}}^\perp$ the orthogonal complement of the sequence this far. Now, repeatedly project the feature dimensions onto $S_i$ to construct $\phi_i:=P_{S_i}\Phi_i/\norm{P_{S_i}\Phi_i}_{L^2(\Rb^d;\varrho)}$. For $i>p$, take any orthonormal basis of $S_p$ to complete the basis, it will not be used anyway. Consequently, we can also apply \cref{cor: regret for fixed approximation order with bounded basis} with $N=p$ to obtain a regret bound of $\tilde\Oc(\sqrt{pT})$ for the learning problem.


These results are summarised in \cref{cor: on finite basis regret}, but notice that higher order polynomial models can be readily considered as well, such as quadratic costs 
\[
    c^*(x,y) = {\Phi(x,y)}^\top\Theta^*\Phi(x,y)\,,
\]
for $\Theta^*\in\Rb^{p\times p}$, by simply reparametrising it as a linear model in dimension $p^2$ and applying the same construction. Many other models can be considered in this manner, and would benefit from further specialised investigation.

\CorOnFiniteBasis*

\subsection{Increasing order basis truncation}\label{subsubsec: increasing order}

In this section, we will extend the results of \cref{subsubsec: finite order} to let $n$ change with $t\in\Nb$ along the learning process. We will denote the corresponding sequence by ${(n_t)}_{t\in\Nb}\subset\Nb$. It is relatively simple to see that the proofs of the key properties of online least-squares estimation will extend, but we include the key proof sketches for completeness. We begin by diagonalising the validity of the confidence sets in \cref{lemma: confidence sets with varying basis order}.

\begin{lemma}\label{lemma: confidence sets with varying basis order}
    Under \cref{asmp: estimate + subG,asmp: L2 case},
\[
    \Pb\left(\bigcap_{t=1}^\infty \tilde{\event}_t^{n_t}(\delta)\right)\ge 1-\frac\delta2\,.
\]
\end{lemma}

\begin{proof}
    The proof only requires diagonalisation of the standard stopping time construction. For $(\delta,t)\in(0,1)\x\Nb$, on the filtered probability space $(\Omega,\Fc_\infty,\Fb,\Pb)$ define 
    \[ 
        B_t(\delta):=\left\{\omega\in\Omega: \norm{\gamma^* -\hat \gamma^{n_t,\lambda}_t}_{\tilde\design_t^{\lambda,n_t}}\le \tilde\width_{t,n_t}(\delta) \right\}\overset{\mbox{a.s.}}{=}\{\omega\in\Omega: c^*\vert_{n_t} \not\in \tilde\confset_{t}^{n_t}(\delta)\}\,,
    \]
    be the $t\textsuperscript{th}$ ``bad event'', and let $\tau_\delta: \omega\in\Omega\to\inf\{t\in\Nb: \omega\in B_t(\delta)\}$, which is a stopping time. We have 
    \[
        \{\tau <+\infty\}= \bigcup_{t\in\Nb} B_t(\delta)\,.
    \]
    By construction, in the classical manner:
    \begin{align*}
        \Pb\left(\bigcup_{t\in\Nb} B_t(\delta)\right)&= \Pb(\tau<+\infty, B_t(\delta))\le \Pb\left(\tilde\event_t^{n_t}(\delta)\right)\le \frac{\delta}2\,.
    \end{align*}
\end{proof}

The confidence sets using for non-constant ${(n_t)}_{t\in\Nb}$ are simply instantiations of~\eqref{eq: confidence set fixed order} and~\eqref{eq: width fixed order} with $n_t$ in place of $n$. This change of basis with time however requires a modification of the proof of \cref{lemma: bound on width term} as the steps summed up in~\eqref{eq: summation of widths in proof of the bound on width term lemma} are no longer homogenous. In particular,~\eqref{eq: sum of log is logdet} is no longer valid. 

\begin{lemma}\label{lemma: bound on width term with varying basis order}
    Under \cref{asmp: estimate + subG,asmp: L2 case}, if $\Lambda_n:=\frac{1}{2}\norm{\cdot}_{2}^2$ with the norm being on $\Rb^n$, then
    \begin{align}
        \sum_{t=1}^T\langle c^*\vert_{n_t} -\tilde c_t^{\,n_t}\vert \tilde\pi_t\rangle\le 2C\sigma\left( \sqrt{2\log\left(\frac{\lambda^{-1}+\frac{T\ubnorm^2}{n_T}}{\delta}\right)}+\sqrt{\lambda}\ubnorm\right) \sqrt{n_T T\log\left(1+ \frac{T}{n_T\ubnorm^2}\right)}\,.\notag
    \end{align}
\end{lemma}

\begin{proof}
    Recall the notation of \cref{lemma: bound on width term}, which adapts to $\varphi_t:=\langle c^*\vert_{n_t}-c_t^{n_t}\vert \tilde\pi_t\rangle$ for $t\in\Nb$ and $\tilde c_t^{\, n_t}:= \sum_{i=1}^{n_t}\tilde\gamma^{n_t}_{t,i}\phi_i$. The proof of \cref{lemma: bound on width term} yields 
    \begin{align}
        \sum_{t=1}^T\varphi_t \le 2C\beta_{T,n_T}(\delta)\sqrt{T\sum_{t=1}^T\log\left(1+\frac1{2\ubnorm}\norm{\vartheta^{(t,n_t)}}_{{({\tilde{D}_t^{\lambda,n_t}})}^{-1}}\right)}\,.
    \end{align}
    
    First, one can bound $\tilde\width_{T,n_T}(\delta)$ by~\eqref{eq: bound for the determinindant of width in finite dimension} in \cref{lemma: bounds for the determinants in finite dimension}. 

    It remains to adapt the logarithmic term into a log-determinant of the desired form by conforming the vectors $\vartheta^{(t,n_t)}$. To do so, let us define the block matrices
    \[
    Z_t:= \begin{pmatrix}
        {(\tilde\design_t^{\lambda,n_t})}^{-1} & \bm{0} \\
        \bm{0} & \bm{0}\\
        \end{pmatrix} \quad \mbox{ for } t\in\Nb\,,
    \]
    so that we may use the rank one update formula to write
    \[
    \prod_{t=1}^T \left(1+\frac1{2\ubnorm}\norm{\vartheta^{(t,n_t)}}_{{({\tilde{D}_t^{\lambda,n_t}})}^{-1}}\right) = \frac{\det\left(\De\Lambda_n+ \sum_{t=1}^T \vartheta^{(t,n_t)}Z_t{\vartheta^{(t,n_t)}}^\top\right)}{\det(\De\Lambda_n)}\,.
    \]
    Taking $\Lambda_n=\frac{1}{2}\norm{\cdot}_{2}^2$ as given, we can bound the determinant of the numerator by
    \[
        {\det\left(\De\Lambda_n+ \sum_{t=1}^T \vartheta^{(t,n_t)}Z_t{\vartheta^{(t,n_t)}}^\top\right)} \le {\left(1+ \frac{T}{n_T\ubnorm^2}\right)}^{n_T}
    \]
    as in~\cite[Lemma E.3]{abbasi-yadkori_online_2012}. Combining with the bound on $\tilde\width_{T,n_T}(\delta)$ completes the proof.
\end{proof}


Having established the technical lemmata, we now turn to the regret guarantees of the varying order basis truncation version of \cref{alg: alg shared + approx}. In particular, recall \cref{asmp: basis decay} to give a quantification of the regularity of $c^*$, which in turn will allow us to tune ${(n_t)}_{t\in\Nb}$ to obtain the best possible regret bounds in \cref{thm: regret for varying approximation}.

\begin{restatable}{theorem}{RegretForVaryingApprox}\label{thm: regret for varying approximation}
    Assume \cref{asmp: estimate + subG,asmp: L2 case,asmp: basis decay} and  $\zeta(n)=1-n^{-q}$ for some $q>0$. For any $\delta\in(0,1)$, $\lambda>0$, $\ve>0$, let $\tilde\Ac$ (resp. $\tilde\Bc$) denote \cref{alg: alg shared + approx} with ${(n_t)}_{t\in\Nb}={(\ceil{t^{\frac1{2q+1}}})}_{t\in\Nb}$, $\Lambda_n=\frac12\norm{\cdot}_{2}^2$, for all $n\in\Nb$, and ${(\ve_t)}_{t\in\Nb}={(\ve)}_{t\in\Nb}$ (resp. ${(\ve_t)}_{t\in\Nb}= {(\alpha t^{-\alpha})}_{t\in\Nb}$). For any $T\in\Nb$, the following regret bounds hold:
    \begin{align}
        \regret_T^{\entropy,\ve} (\tilde\Ac)&\le \sigma\sqrt{2T\log\left(\frac2\delta\right)} + \ubnorm\left(1+\frac{qT^{\frac{q+1}{2q+1}}}{2q+1}\right) \notag\\
        &\quad + 2\ubnorm\sigma T^{\frac{q+1}{2q+1}}\left(\sqrt{2\log\left(\frac{\lambda^-1+2T^{\frac{2q}{2q+1}}\ubnorm^2}{\delta}\right)}+\sqrt\lambda \ubnorm\right)\sqrt{\log\left(1+\frac{2T^{\frac{2q}{2q+1}}}{\ubnorm^2}\right)}\,,\notag
        %\label{eq: entropic regret for varying approximation order with bounded basis}\\
        \intertext{ and }
        \regret_T(\tilde\Bc)&\le  \sigma\sqrt{2T\log\left(\frac2\delta\right)} + \ubnorm\left(1+\frac{qT^{\frac{q+1}{2q+1}}}{2q+1}\right)+\kappa(1+\sqrt{T}\log(T))\notag\\
        &\quad + 2\ubnorm\sigma T^{\frac{q+1}{2q+1}}\left(\sqrt{2\log\left(\frac{\lambda^-1+2T^{\frac{2q}{2q+1}}\ubnorm^2}{\delta}\right)}+\sqrt\lambda \ubnorm\right)\sqrt{\log\left(1+\frac{2T^{\frac{2q}{2q+1}}}{\ubnorm^2}\right)}\,,\notag
        %\label{eq: kantorovich regret for varying approximation order with bounded basis}
    \end{align}
\end{restatable}

\begin{proof}
    The proof requires only two steps from the one of \cref{cor: regret for fixed approximation order}. First, we bound the approximation error term.
    \Cref{lemma: fourier decay bound} readily implies that
    \[
        \abs{\langle c^* - c^*\vert_{n_t}\vert \pi_t\rangle} \le \sum_{k=n_t+1}^{+\infty}\abs{\gamma_i^*}\,.
    \]
    Summing over $t\in\Nb$, one obtains
    \begin{align}
        \sum_{t=1}^T \abs{\langle c^* - c^*\vert_{n_t}\vert \pi_t\rangle}\le \sum_{t=1}^T \sum_{i=n_t+1}^{+\infty}\abs{\gamma_i^*}\,.\label{eq: regret term for truncation, summed}
    \end{align}
    By \cref{asmp: basis decay}, for any $n\in\Nb$, we have
    \[ 
        \sum_{i=n_t+1}^\infty\abs{\gamma_i^*} = \norm{c^*}_{L^2(\Rb^d;\varrho)}- \sum_{i=1}^{n_t}\abs{\gamma_i^*} \le \norm{c^*}_{L^2(\Rb^d;\varrho)}(1-\zeta(n_t))
    \]
    so that for any $u>0$, the choice $n_t:=\ceil{\zeta^{-1}((1-t^{-u}))}=\ceil{t^{\frac uq}}$ ($q>0$) yields
    \[
        \sum_{i=n_t+1}^\infty \abs{\gamma_i^*}\le  \norm{c^*}_{L^2(\Rb^d;\varrho)} t^{-u}\,.
    \]
    This follows from the fact that $\zeta$ can be made a bijection of $\Rb_+\to(0,1]$, and that $\zeta$ is increasing. Injecting $\sum_{s=n_t+1}^{+\infty}\abs{\gamma_s^*}\le \norm{c^*} t^{-u}$ into~\eqref{eq: regret term for truncation, summed} yields
    \[
        \sum_{t=1}^T \abs{\langle c^* - c^*\vert_{n(t)}\vert \pi_t\rangle}\le\norm{c^*}_{L^2(\Rb^d;\varrho)}\left(1+\frac{T^{1-u}}u\right)\,.
    \]
    The second step simply involves applying \cref{lemma: bound on width term with varying basis order} for $n_T:=\ceil{T^{\frac{u}q}}\le 2T^{u/q}$ to obtain a bound of order $\Oc(T^{\frac12+\frac{u}{2q}})$. Setting $u=\frac{q}{2q+1}$ yields the stated bounds.
\end{proof}

%In terms of computational complexity, 

%\lc{In terms of computational complexity, suppose for example that $\zeta(t)=1-t^{-\iota}$ for some $\iota>0$. Note that one may always assume\footnote{Summability of $\{\abs{\gamma_i}\}_{i\in\Nb}$ implies that $\{1-\zeta(n)\}_{n\in\Nb}$ should be summable.} (up to multiplying $\zeta$ by a constant) that $\iota>1$. At step $t\in\Nb$, solving the linear system of \eqref{eq: RLS basis truncation} (of dimension $t^{1/2\iota}\times t$) thus has a complexity of $\Oc(t^{1+1/\iota})$.} 



\subsection{Tikhonov regularisation and RKHS theory}\label{subsec: tikhonov and RKHS}

In this section, we will assume that $\Lambda=\frac12\norm{\cdot}^2_2$ for simplicity. In general any increasing positive function of $\norm{\cdot}_2$ will suffice to use the representer theorem as per our argument. Suppose we are given $(\Hf,K)$ a Reproducing Kernel Hilbert Space\footnote{Understood, of course, up to the identifications necessary for the RKHS to be a space of functions. Recall that $L^2(\Rb^d;\varrho)$ is \emph{not} an RKHS due to a subtlety of this nature.} (RKHS) such that $\Hf\subset L^2(\Rb^d;\varrho)$. We may specialise the RLS estimator (see \cref{prop: least squares estimator}) to this case by noting that $M_t:={(K(a_0,\cdot),\dots,K(a_{t-1},\cdot))}^\top$. 

By the representer theorem, at any step $t\in\Nb$,the solution to the regularised least squares problem in $\Hf$ is given by
\[
    \hat f_t^\lambda = \sum_{i=0}^{t-1} \upsilon_i K(a_i,\cdot)\,,
\]
for some ${(\upsilon_i)}_{i=0}^{t-1}\in\Rb^t$. The problem can therefore be reduced to the finite dimensional optimisation problem 
\[
    \min_{\upsilon\in\Rb^t} \norm{\vec{R}_t - K_t\upsilon}_2^2 + \lambda \upsilon^\top K_t\upsilon\,,
\]
in which $K_t:={[K(a_i,a_j)]}_{i,j}\in\Rb^{t\x t}$ is the kernel (Grammian) matrix. The rest of the standard developments follow, and one arrives at the approximation bound 
\[
    \sum_{t=1}^T \langle c^* - \tilde c_t\vert \tilde\pi_t\rangle \le 2\ubnorm\beta_T(\delta)\sqrt{2T\log\det\left(I+{(\lambda \ubnorm)}^{-1}K_T\right)}
\] 
by \cref{lemma: bound on width term}, and corresponding regret bounds easily follows via \cref{thm: entropic regret,thm: regret Kantorovich}. From here, one can easily recover bounds ad-hoc or by following the general methodology of \cref{subsec: Fourier basis representation}.

One of the main benefits of kernel methods is that they can be used to learn in infinite-dimensional spaces efficiently. While they are inherently efficient thanks to the kernel trick, works in this field have suggested further efficiency refinements such as~\cite{takemori_approximation_2021} which uses approximation theory to reduce learning in an RKHS to a finite-dimensional approximation on a well chosen basis. This resembles the methodology used above, further developments in this direction appear an interesting avenue for research. 