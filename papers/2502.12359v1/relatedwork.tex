\section{Related work}
\noindent\textbf{Large Vision-Language Models}. 
Benefiting from the combination of LLMs~\citep{radford2018improving,radford2019language,BrownMRSKDNSSAA20,peng2023instruction} with visual modules, LVLMs~\citep{LiuLWL23a,openai2023gpt,claude3,reid2024gemini,liu2023improved,NEURIPS2023_9a6a435e,zhu2023minigpt,chen2024sharegpt4v,ye2023mplug,awadalla2023openflamingo,wang2023cogvlm,bai2023qwen,li2023blip,ye2023mplug,ye2023mplug2,yang2024law,sun2023generative} have acquired strong capability to understand and utilize both visual and textual information, benefiting various tasks. Many open-source LVLMs usually contain three components~\citep{LiDZWZW23}: a vision encoder such as CLIP~\cite{radford2021learning} to encode the vision information, an LLM to provide textual analysis and reasoning, and a cross-modal alignment module to align the information from text modality and vision modality.




\noindent\textbf{Hallucination in LVLMs}. Despite the great performance of LVLMs, they suffer from hallucinations, which generally refer to the phenomenon that the generated text responses are inconsistent with the corresponding visual content~\citep{bai2024hallucination}. Many efforts~\citep{yu2024rlhf,han2024skip,sun2023aligning,liu2023improved,liu2023mitigating,zhai2023halle,chen2023mitigating,yue2024less,huang2023opera,wang2023evaluation,tong2024eyes,jiang2023hallucination,leng2023mitigating,yin2023woodpecker,zhang2024reflective,chen2024ict} have been taken to understand the causes of hallucinations, which can be generally categorized into three categories, i.e., \textit{data-level}, \textit{decoding-level}, and \textit{module-level}. From the \textit{data-level} perspective, the text-image data used for training LVLMs is more challenging to scale than the pure text data for training Large Language Models~\citep{sun2023aligning}. Additionally, data collected from the internet or generated by models often contains noise~\citep{liu2023improved} and lacks diversity~\citep{liu2023mitigating}. Recent studies~\citep{zhai2023halle,chen2023mitigating,yue2024less} have also suggested that overly detailed data, which exceeds the visual perception capabilities of LVLMs, can increase hallucinations. From the \textit{decoding-level} perspective, 
analysis on Opera~\citep{huang2023opera} shows that a model's self-attention might focus more on the previously generated text tokens, thus ignoring the image and causing hallucination. Furthermore, mainstream methods such as top-$K$ sampling, while enhancing the diversity of generated text, also amplify the risk of hallucinations~\citep{wang2023evaluation}. Regarding the \textit{module-level}, some researchers~\citep{tong2024eyes} believe that the perceptual capability of the model's visual encoder is deficient, leading to a loss of visual information during encoding. Weaknesses in the alignment~\citep{jiang2023hallucination} module may also hinder perfect modality alignment. Additionally,~\citet{leng2023mitigating} found that the model's language priors can override the visual content, resulting in hallucinations. Therefore, many studies~\citep{leng2023mitigating,chen2024halc,zhang2024debiasing,deng2024seeing,wu2024noiseboost,xiao2024seeing,sarkar2024mitigating,woo2024don,chen2024alleviating} are dedicated to reducing the influence of language priors. 


While this approach enhances the model's performance on benchmarks \citep{liu2023hallusionbench,tong2024eyes,LiDZWZW23} that emphasize visual capability and overlook language priors, we believe that strong language priors are crucial for robust and comprehensive LVLMs. For LVLMs aimed at achieving Artificial General Intelligence, the language priors are not strong enough.

\noindent\textbf{Benchmarks for Evaluating Hallucination}. With the development of research on LVLMs hallucinations, numerous benchmarks~\citep{LiDZWZW23,liu2024phd,liu2023hallusionbench,jiang2024hal,lovenia2023negative,huang2024visual,kaul2024throne,fieback2024metatoken,jing2023faithscore,cui2023holistic,wang2023llm,wang2024mitigating,chen2024unified,cha2024visually,sun2023aligning,zheng2024reefknot,xu2023lvlm,sun2024crosscheckgpt,qu2024look,wang2024understanding} for evaluating hallucinations have been proposed. POPE~\citep{LiDZWZW23} has pioneered the exploration of object hallucinations, discovering that LVLMs tend to generate objects frequently appearing or co-occurring in the training set. They constructed questions based on the frequency of co-occurring words in the corpus, asking LVLMs about the presence of objects. However, with the development of multimodal large models, there is a need for updated and more challenging benchmarks. Hallusionbench~\citep{liu2023hallusionbench} is the first benchmark specifically examining language priors, evaluating the model's ability to handle conflicts between prior knowledge and visual information through manually crafted complex questions.

\noindent\textbf{General Domain LVLMs Benchmarks}. We also introduce some classical general domain LVLMs benchmarks here. General domain LVLM benchmarks aim to holistically measure foundational abilities across vision and language, with examples from early benchmarks like VQAv2~\cite{goyal2017making} and VizWiz~\cite{gurari2018vizwiz}. Representative examples in this category are MME~\cite{abs-2306-13394}, MMBench~\cite{liu2024mmbench}, and SEED-Bench~\cite{li2024seed}, each offering distinct approaches to comprehensive assessment. MME stands out as a benchmark designed to provide a broad evaluation across more than ten perception and cognition tasks, emphasizing real-world scenarios and practical application. MMBench is another comprehensive benchmark, featured by its detailed analysis, allowing for granular insights into model strengths and weaknesses. SEED-Bench and SEED-Bench-2~\cite{abs-2311-17092}, are notable for their large-scale multiple-choice questions spanning a wide range of capabilities. These benchmarks are valuable for measuring overall model performance and tracking progress across various dimensions of multimodal understanding.