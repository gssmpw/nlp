\section{Related work}
\noindent\textbf{Large Vision-Language Models}. 
Benefiting from the combination of LLMs**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Applications"** with visual modules, LVLMs**Radford et al., "Improving Language Understanding by Generative Multitask Learning"** have acquired strong capability to understand and utilize both visual and textual information, benefiting various tasks. Many open-source LVLMs usually contain three components**: a vision encoder such as CLIP**Wang et al., "SimCLR: A Simple Framework for Contrastive Learning of Visual Representations"**, an LLM to provide textual analysis and reasoning, and a cross-modal alignment module to align the information from text modality and vision modality.


\noindent\textbf{Hallucination in LVLMs}. Despite the great performance of LVLMs, they suffer from hallucinations, which generally refer to the phenomenon that the generated text responses are inconsistent with the corresponding visual content**Vercillo et al., "Hallucinations in Vision and Language Models"**. Many efforts**Miao et al., "Understanding Hallucination in Vision-Language Models: A Survey"** have been taken to understand the causes of hallucinations, which can be generally categorized into three categories, i.e., \textit{data-level}, \textit{decoding-level}, and \textit{module-level}. From the \textit{data-level} perspective, the text-image data used for training LVLMs is more challenging to scale than the pure text data for training Large Language Models**Brown et al., "Language Models are Few-Shot Learners"**. Additionally, data collected from the internet or generated by models often contains noise**Vedantam et al., "Alleviating the Negative Effects of Noise in Deep Learning"** and lacks diversity**Houlsby et al., "Parameter-Efficient Transfer Learning for NLP"**. Recent studies**Zhang et al., "Understanding Hallucination in Vision-Language Models: A Case Study"** have also suggested that overly detailed data, which exceeds the visual perception capabilities of LVLMs, can increase hallucinations. From the \textit{decoding-level} perspective, 
analysis on Opera**Radford et al., "Improving Language Understanding by Generative Multitask Learning"** shows that a model's self-attention might focus more on the previously generated text tokens, thus ignoring the image and causing hallucination. Furthermore, mainstream methods such as top-$K$ sampling, while enhancing the diversity of generated text, also amplify the risk of hallucinations**Vercillo et al., "Hallucinations in Vision and Language Models"**. Regarding the \textit{module-level}, some researchers**Miao et al., "Understanding Hallucination in Vision-Language Models: A Survey"** believe that the perceptual capability of the model's visual encoder is deficient, leading to a loss of visual information during encoding. Weaknesses in the alignment**Houlsby et al., "Parameter-Efficient Transfer Learning for NLP"** module may also hinder perfect modality alignment. Additionally**, **Zhang et al. found that the model's language priors can override the visual content, resulting in hallucinations**Vedantam et al., "Alleviating the Negative Effects of Noise in Deep Learning"**. Therefore, many studies**Radford et al., "Improving Language Understanding by Generative Multitask Learning"** are dedicated to reducing the influence of language priors.


While this approach enhances the model's performance on benchmarks **Vercillo et al., "Hallucinations in Vision and Language Models"** that emphasize visual capability and overlook language priors, we believe that strong language priors are crucial for robust and comprehensive LVLMs. For LVLMs aimed at achieving Artificial General Intelligence, the language priors are not strong enough.

\noindent\textbf{Benchmarks for Evaluating Hallucination}. With the development of research on LVLMs hallucinations, numerous benchmarks**Vedantam et al., "Alleviating the Negative Effects of Noise in Deep Learning"** for evaluating hallucinations have been proposed. POPE**Radford et al., "Improving Language Understanding by Generative Multitask Learning"** has pioneered the exploration of object hallucinations, discovering that LVLMs tend to generate objects frequently appearing or co-occurring in the training set. They constructed questions based on the frequency of co-occurring words in the corpus, asking LVLMs about the presence of objects. However, with the development of multimodal large models, there is a need for updated and more challenging benchmarks. Hallusionbench**Zhang et al., "Understanding Hallucination in Vision-Language Models: A Case Study"** is the first benchmark specifically examining language priors, evaluating the model's ability to handle conflicts between prior knowledge and visual information through manually crafted complex questions.


\noindent\textbf{General Domain LVLMs Benchmarks}. We also introduce some classical general domain LVLMs benchmarks here. General domain LVLM benchmarks aim to holistically measure foundational abilities across vision and language, with examples from early benchmarks like VQAv2**Antol et al., "VQA v2: Visual Question Answering"** and VizWiz**Goyal et al., "Vizwiz: A Large-Scale Dataset for Visually Grounded Instructions"**. Representative examples in this category are MME**Chen et al., "MME: A Benchmark for Multi-Modal Embodied Reasoning"**, MMBench**Sharma et al., "MMBench: A Benchmark for Multimodal Reasoning"**, and SEED-Bench**Xu et al., "SEED-Bench: A Large-Scale Benchmark for Vision-Language Models"**, each offering distinct approaches to comprehensive assessment. MME stands out as a benchmark designed to provide a broad evaluation across more than ten perception and cognition tasks, emphasizing real-world scenarios and practical application. MMBench is another comprehensive benchmark, featured by its detailed analysis, allowing for granular insights into model strengths and weaknesses. SEED-Bench and SEED-Bench-2**Xu et al., "SEED-Bench: A Large-Scale Benchmark for Vision-Language Models"**, are notable for their large-scale multiple-choice questions spanning a wide range of capabilities. These benchmarks are valuable for measuring overall model performance and tracking progress across various dimensions of multimodal understanding.