@misc{openai2023gpt,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/GPTV_System_Card.pdf}
}


@misc{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
   URL = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
  author={Anthropic},
  year={2024}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}




@article{ye2023mplug2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}



@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}
@article{li2023monkey,
  title={Monkey: Image resolution and text label are important things for large multi-modal models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}
@article{chen2023internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Muyan, Zhong and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{luo2024codis,
  title={CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models},
  author={Luo, Fuwen and Chen, Chi and Wan, Zihao and Kang, Zhaolu and Yan, Qidong and Li, Yingjie and Wang, Xiaolong and Wang, Siyu and Wang, Ziyue and Mi, Xiaoyue and others},
  journal={arXiv preprint arXiv:2402.13607},
  year={2024}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
pages = {1877--1901},
url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  booktitle    = {Proceedings of Annual Conference on Neural Information Processing Systems, {NeurIPS}},
  year         = {2020}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{LiuLWL23a,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
  booktitle    = {Proceedings of the Annual Conference on Neural Information Processing Systems, {NeurIPS}},
  year         = {2023}
}




@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
url          = {https://proceedings.mlr.press/v202/li23q.html},
  booktitle={Proceedings of the International conference on machine learning, {ICML}},
  pages={19730--19742},
  year={2023}
}

@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@inproceedings{DaiLJSF23,
  author       = {Wenliang Dai and
                  Zihan Liu and
                  Ziwei Ji and
                  Dan Su and
                  Pascale Fung},
  title        = {Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language
                  Pre-training},
  booktitle    = {Proceedings of the 17th Conference of the European Chapter of the
                  Association for Computational Linguistics, {EACL}},
  pages        = {2128--2140},
  year         = {2023},
url = "https://aclanthology.org/2023.eacl-main.156"
}

@inproceedings{LiDZWZW23,
  author       = {Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  JiRong Wen},
  title        = {Evaluating Object Hallucination in Large Vision-Language Models},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.20},
  booktitle    = {Proceedings
                                        of the Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {292--305},
  year         = {2023}
}

@article{liu2024phd,
  title={Phd: A prompted visual hallucination evaluation dataset},
  author={Liu, Jiazhen and Fu, Yuhan and Xie, Ruobing and Xie, Runquan and Sun, Xingwu and Lian, Fengzong and Kang, Zhanhui and Li, Xirong},
  journal={arXiv preprint arXiv:2403.11116},
  year={2024}
}

@article{liu2023hallusionbench,
  title={Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models},
  author={Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2310.14566},
  year={2023}
}

@article{jiang2024hal,
  title={Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models},
  author={Jiang, Chaoya and Ye, Wei and Dong, Mengfan and Jia, Hongrui and Xu, Haiyang and Yan, Ming and Zhang, Ji and Zhang, Shikun},
  journal={arXiv preprint arXiv:2402.15721},
  year={2024}
}
@article{leng2023mitigating,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  journal={arXiv preprint arXiv:2311.16922},
  year={2023}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
url          = {https://doi.org/10.1109/CVPR52729.2023.01436},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR},
  pages={14953--14962},
  year={2023}
}

@inproceedings{shao2023prompting,
  title={Prompting large language models with answer heuristics for knowledge-based visual question answering},
  author={Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR},
  pages={14974--14983},
url          = {https://doi.org/10.1109/CVPR52729.2023.01438},
  year={2023}
}

@inproceedings{zhou2024navgpt,
  title={Navgpt: Explicit reasoning in vision-and-language navigation with large language models},
  author={Zhou, Gengze and Hong, Yicong and Wu, Qi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence, AAAI},
  pages={7641--7649},
  year={2024}
}

@inproceedings{sammani2022nlx,
  title={Nlx-gpt: A model for natural language explanations in vision and vision-language tasks},
  author={Sammani, Fawaz and Mukherjee, Tanmoy and Deligiannis, Nikos},
url          = {https://doi.org/10.1109/CVPR52688.2022.00814},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, {CVPR}},
  pages={8322--8332},
  year={2022}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  pages={1--38},
url          = {https://doi.org/10.1145/3571730},
  year={2023}
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@inproceedings{
liu2023mitigating,
title={Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning},
author={Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=J44HfH4JCg}
}

@article{yue2024less,
  title={Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective},
  author={Yue, Zihao and Zhang, Liang and Jin, Qin},
  journal={arXiv preprint arXiv:2402.14545},
  year={2024}
}
@article{chen2023mitigating,
  title={Mitigating hallucination in visual language models with visual supervision},
  author={Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and Li, Zhaowen and Zhao, Chaoyang and Wang, Jinqiao and Tang, Ming},
  journal={arXiv preprint arXiv:2311.16479},
  year={2023}
}
@article{zhai2023halle,
  title={Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption},
  author={Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun},
  journal={arXiv preprint arXiv:2310.01779},
  year={2023}
}
@article{huang2023opera,
  title={Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation},
  author={Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2311.17911},
  year={2023}
}
@article{wang2023evaluation,
  title={Evaluation and analysis of hallucination in large vision-language models},
  author={Wang, Junyang and Zhou, Yiyang and Xu, Guohai and Shi, Pengcheng and Zhao, Chenlin and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Zhang, Ji and Zhu, Jihua and others},
  journal={arXiv preprint arXiv:2308.15126},
  year={2023}
}

@article{jiang2023hallucination,
  title={Hallucination augmented contrastive learning for multimodal large language model},
  author={Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and Chen, Jiaxing and Ye, Wei and Yan, Ming and Ye, Qinghao and Zhang, Ji and Huang, Fei and Zhang, Shikun},
  journal={arXiv preprint arXiv:2312.06968},
  year={2023}
}
@article{zhu2024ibd,
  title={IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding},
  author={Zhu, Lanyun and Ji, Deyi and Chen, Tianrun and Xu, Peng and Ye, Jieping and Liu, Jun},
  journal={arXiv preprint arXiv:2402.18476},
  year={2024}
}
@article{chen2024halc,
  title={HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding},
  author={Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei},
  journal={arXiv preprint arXiv:2403.00425},
  year={2024}
}
@article{zhang2024debiasing,
  title={Debiasing large visual language models},
  author={Zhang, Yi-Fan and Yu, Weichen and Wen, Qingsong and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu},
  journal={arXiv preprint arXiv:2403.05262},
  year={2024}
}
@article{deng2024seeing,
  title={Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding},
  author={Deng, Ailin and Chen, Zhirui and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.15300},
  year={2024}
}
@article{wu2024noiseboost,
  title={NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models},
  author={Wu, Kai and Jiang, Boyuan and Jiang, Zhengkai and He, Qingdong and Luo, Donghao and Wang, Shengzhi and Liu, Qingwen and Wang, Chengjie},
  journal={arXiv preprint arXiv:2405.20081},
  year={2024}
}
@article{xiao2024seeing,
  title={Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment},
  author={Xiao, Xin and Wu, Bohong and Wang, Jiacong and Li, Chunyuan and Zhou, Xun and Guo, Haoyuan},
  journal={arXiv preprint arXiv:2405.17871},
  year={2024}
}
@article{sarkar2024mitigating,
  title={Mitigating Object Hallucination via Data Augmented Contrastive Tuning},
  author={Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Ar{\i}k, Sercan {\"O} and Pfister, Tomas},
  journal={arXiv preprint arXiv:2405.18654},
  year={2024}
}
@article{woo2024don,
  title={Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models},
  author={Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17820},
  year={2024}
}
@article{chen2024alleviating,
  title={Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization},
  author={Chen, Beitao and Lyu, Xinyu and Gao, Lianli and Song, Jingkuan and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2405.15356},
  year={2024}
}
@article{woo2024ritual,
  title={RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs},
  author={Woo, Sangmin and Jang, Jaehyuk and Kim, Donguk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17821},
  year={2024}
}
@article{lovenia2023negative,
  title={Negative object presence evaluation (nope) to measure object hallucination in vision-language models},
  author={Lovenia, Holy and Dai, Wenliang and Cahyawijaya, Samuel and Ji, Ziwei and Fung, Pascale},
  journal={arXiv preprint arXiv:2310.05338},
  year={2023}
}
@article{huang2024visual,
  title={Visual Hallucinations of Multi-modal Large Language Models},
  author={Huang, Wen and Liu, Hongbin and Guo, Minxin and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2402.14683},
  year={2024}
}
@article{kaul2024throne,
  title={THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models},
  author={Kaul, Prannay and Li, Zhizhong and Yang, Hao and Dukler, Yonatan and Swaminathan, Ashwin and Taylor, CJ and Soatto, Stefano},
  journal={arXiv preprint arXiv:2405.05256},
  year={2024}
}
@article{fieback2024metatoken,
  title={MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification},
  author={Fieback, Laura and Spiegelberg, Jakob and Gottschalk, Hanno},
  journal={arXiv preprint arXiv:2405.19186},
  year={2024}
}
@article{jing2023faithscore,
  title={Faithscore: Evaluating hallucinations in large vision-language models},
  author={Jing, Liqiang and Li, Ruosen and Chen, Yunmo and Jia, Mengzhao and Du, Xinya},
  journal={arXiv preprint arXiv:2311.01477},
  year={2023}
}
@article{cui2023holistic,
  title={Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges},
  author={Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and Wu, Shirley and Zhang, Linjun and Zou, James and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2311.03287},
  year={2023}
}
@article{wang2023llm,
  title={An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
  author={Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
  journal={arXiv preprint arXiv:2311.07397},
  year={2023}
}
@inproceedings{wang2024mitigating,
  title={Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites},
  author={Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng},
  booktitle={International Conference on Multimedia Modeling},
  pages={32--45},
url          = {https://doi.org/10.1007/978-3-031-53302-0\_3},
  year={2024},
  organization={Springer}
}
@article{chen2024unified,
  title={Unified Hallucination Detection for Multimodal Large Language Models},
  author={Chen, Xiang and Wang, Chenxi and Xue, Yida and Zhang, Ningyu and Yang, Xiaoyan and Li, Qiang and Shen, Yue and Gu, Jinjie and Chen, Huajun},
  journal={arXiv preprint arXiv:2402.03190},
  year={2024}
}
@article{cha2024visually,
  title={Visually Dehallucinative Instruction Generation: Know What You Don't Know},
  author={Cha, Sungguk and Lee, Jusung and Lee, Younghyun and Yang, Cheoljong},
  journal={arXiv preprint arXiv:2402.09717},
  year={2024}
}
@article{chen2024we,
  title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2403.20330},
  year={2024}
}
@inproceedings{andriluka14cvpr,
               author = {Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Schiele, Bernt},
               title = {2D Human Pose Estimation: New Benchmark and State of the Art Analysis},
               booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
url= {https://doi.org/10.1109/CVPR.2014.471},
               year = {2014}
}
@inproceedings{shi2014discriminative,
  title={Discriminative blur detection features},
  author={Shi, Jianping and Xu, Li and Jia, Jiaya},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
  pages={2965--2972},
url          = {https://doi.org/10.1109/CVPR.2014.379},
  year={2014}
}

@inproceedings{Zhou2019Stereodeblur,
  author       = {Shangchen Zhou and
                  Jiawei Zhang and
                  Wangmeng Zuo and
                  Haozhe Xie and
                  Jinshan Pan and
                  Jimmy S. Ren},
  title        = {DAVANet: Stereo Deblurring With View Aggregation},
  booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, {CVPR}},
  pages        = {10996--11005},
  year         = {2019},
    url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Zhou\_DAVANet\_Stereo\_Deblurring\_With\_View\_Aggregation\_CVPR\_2019\_paper.html}
}

@inproceedings{rim_2020_ECCV,
 title={Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms},
 author={Jaesung Rim, Haeyun Lee, Jucheol Won, Sunghyun Cho},
 booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
 year={2020},
}
@article{Exdark,
  title = {Getting to Know Low-light Images with The Exclusively Dark Dataset},
  author = {Loh, Yuen Peng and Chan, Chee Seng},
  journal = {Computer Vision and Image Understanding},
  volume = {178},
  pages = {30-42},
  year = {2019},
  doi = {https://doi.org/10.1016/j.cviu.2018.10.010}
}

@misc{M8JQCR2021,
author = {Xiao, Haixia},
publisher = {Harvard Dataverse},
title = {{Weather phenomenon database (WEAPD)}},
year = {2021},
version = {V1},
doi = {10.7910/DVN/M8JQCR},
url = {https://doi.org/10.7910/DVN/M8JQCR}
}


@inproceedings{liu2022imageadaptive,
  title={Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions},
  author={Liu, Wenyu and Ren, Gaofeng and Yu, Runsheng and Guo, Shi and Zhu, Jianke and Zhang, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
pages        = {1792--1800},
url          = {https://doi.org/10.1609/aaai.v36i2.20072},
  year={2022}
}

@article{qi2022occluded,
    title={Occluded Video Instance Segmentation: A Benchmark},
    author={Jiyang Qi and Yan Gao and Yao Hu and Xinggang Wang and Xiaoyu Liu and Xiang Bai and Serge Belongie and Alan Yuille and Philip Torr and Song Bai},
    journal={International Journal of Computer Vision},
pages        = {2022--2039},
url          = {https://doi.org/10.1007/s11263-022-01629-1},
    year={2022},
}
@article{young-etal-2014-image,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
}
@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
url          = {https://doi.org/10.1007/s11263-016-0981-7},
  publisher={Springer}
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}



@inproceedings{NEURIPS2023_9a6a435e,
 author = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems, {NeurIPS}},
 pages = {49250--49267},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 year = {2023},
url={https://openreview.net/forum?id=vvoWPYqZJA}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@inproceedings{LinMBHPRDZ14,
  author       = {Tsung{-}Yi Lin and
                  Michael Maire and
                  Serge J. Belongie and
                  James Hays and
                  Pietro Perona and
                  Deva Ramanan and
                  Piotr Doll{\'{a}}r and
                  C. Lawrence Zitnick},
  title        = {Microsoft {COCO:} Common Objects in Context},
url          = {https://doi.org/10.1007/978-3-319-10602-1\_48},
  booktitle    = {Proceedings of the European Conference on Computer Vision, ECCV},
  pages        = {740--755},
  year         = {2014}
}


@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}
@article{hu2023large,
  title={Do Large Language Models Know about Facts?},
  author={Hu, Xuming and Chen, Junzhe and Li, Xiaochuan and Guo, Yufei and Wen, Lijie and Yu, Philip S and Guo, Zhijiang},
  journal={arXiv preprint arXiv:2310.05177},
  year={2023}
}

@article{liu2023medical,
  title={A medical multimodal large language model for future pandemics},
  author={Liu, Fenglin and Zhu, Tingting and Wu, Xian and Yang, Bang and You, Chenyu and Wang, Chenyang and Lu, Lei and Liu, Zhangdaihong and Zheng, Yefeng and Sun, Xu and others},
  journal={NPJ Digital Medicine},
  volume={6},
url          = {https://doi.org/10.1038/s41746-023-00952-2},
  number={1},
  pages={226},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@inproceedings{WangCPXKZXXDSTA23,
  author       = {Boxin Wang and
                  Weixin Chen and
                  Hengzhi Pei and
                  Chulin Xie and
                  Mintong Kang and
                  Chenhui Zhang and
                  Chejian Xu and
                  Zidi Xiong and
                  Ritik Dutta and
                  Rylan Schaeffer and
                  Sang T. Truong and
                  Simran Arora and
                  Mantas Mazeika and
                  Dan Hendrycks and
                  Zinan Lin and
                  Yu Cheng and
                  Sanmi Koyejo and
                  Dawn Song and
                  Bo Li},
  title        = {DecodingTrust: A Comprehensive Assessment of Trustworthiness in
                  GPT Models},
  booktitle    = {Proceedings of the thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, NeurIPS},
url={https://openreview.net/forum?id=kaHpo8OZw2},
  year         = {2023}
}
@article{yin2023woodpecker,
  title={Woodpecker: Hallucination correction for multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong},
  journal={arXiv preprint arXiv:2310.16045},
  year={2023}
}
@article{fu2023challenger,
  title={A challenger to gpt-4v? early explorations of gemini in visual expertise},
  author={Fu, Chaoyou and Zhang, Renrui and Lin, Haojia and Wang, Zihan and Gao, Timin and Luo, Yongdong and Huang, Yubo and Zhang, Zhengye and Qiu, Longtian and Ye, Gaoxiang and others},
  journal={arXiv preprint arXiv:2312.12436},
  year={2023}
}
@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}
@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}
@article{zong2023fool,
  title={Fool your (vision and) language model with embarrassingly simple permutations},
  author={Zong, Yongshuo and Yu, Tingyang and Zhao, Bingchen and Chavhan, Ruchika and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2310.01651},
  year={2023}
}
@article{zhang2024benchmarking,
  title={Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study},
  author={Zhang, Yichi and Huang, Yao and Sun, Yitong and Liu, Chang and Zhao, Zhe and Fang, Zhengwei and Wang, Yifan and Chen, Huanran and Yang, Xiao and Wei, Xingxing and others},
  journal={arXiv preprint arXiv:2406.07057},
  year={2024}
}

@article{bai2024hallucination,
  title={Hallucination of multimodal large language models: A survey},
  author={Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2404.18930},
  year={2024}
}
@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, CVPR},
  pages={248--255},
  year={2009},
  url          = {https://doi.org/10.1109/CVPR.2009.5206848},
  organization={Ieee}
}
@article{han2024skip,
  title={Skip$\backslash$n: A simple method to reduce hallucination in large vision-language models},
  author={Han, Zongbo and Bai, Zechen and Mei, Haiyang and Xu, Qianli and Zhang, Changqing and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2402.01345},
  year={2024}
}
@article{yang2024law,
  title={Law of Vision Representation in MLLMs},
  author={Yang, Shijia and Zhai, Bohan and You, Quanzeng and Yuan, Jianbo and Yang, Hongxia and Xu, Chenfeng},
  journal={arXiv preprint arXiv:2408.16357},
  year={2024}
}

@article{duan2024vlmevalkit,
  title={VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  journal={arXiv preprint arXiv:2407.11691},
  year={2024}
}
@article{zheng2024reefknot,
  title={Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models},
  author={Zheng, Kening and Chen, Junkai and Yan, Yibo and Zou, Xin and Hu, Xuming},
  journal={arXiv preprint arXiv:2408.09429},
  year={2024}
}

@inproceedings{bhatia-etal-2024-fintral,
    title = "{F}in{T}ral: A Family of {GPT}-4 Level Multimodal Financial Large Language Models",
    author = "Bhatia, Gagan  and
      Nagoudi, El Moatez Billah  and
      Cavusoglu, Hasan  and
      Abdul-Mageed, Muhammad",
    booktitle = "Findings of the Association for Computational Linguistics ACL",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-acl.774",
    pages = "13064--13087"
}

@inproceedings{LiWZULYNPG23,
  author       = {Chunyuan Li and
                  Cliff Wong and
                  Sheng Zhang and
                  Naoto Usuyama and
                  Haotian Liu and
                  Jianwei Yang and
                  Tristan Naumann and
                  Hoifung Poon and
                  Jianfeng Gao},

  title        = {LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine
                  in One Day},
  booktitle    = {Proceedings of the Annual Conference
                  on Neural Information Processing Systems},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/5abcdf8ecdcacba028c6662789194572-Abstract-Datasets\_and\_Benchmarks.html}
}


@InProceedings{pmlr-v225-moor23a,
  title = 	 {Med-Flamingo: a Multimodal Medical Few-shot Learner},
  author =       {Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav},
  booktitle = 	 {Proceedings of the 3rd Machine Learning for Health Symposium},
  pages = 	 {353--367},
  year = 	 {2023},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v225/moor23a.html}
}

@article{wimmer2023leveraging,
  title={Leveraging vision-language models for granular market change prediction},
  author={Wimmer, Christopher and Rekabsaz, Navid},
  journal={arXiv preprint arXiv:2301.10166},
  year={2023}
}

@inproceedings{
sharma2024towards,
title={Towards Understanding Sycophancy in Language Models},
author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Esin DURMUS and Zac Hatfield-Dodds and Scott R Johnston and Shauna M Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
booktitle={Proceedings of the Twelfth International Conference on Learning Representations, ICLR},
year={2024},
url={https://openreview.net/forum?id=tvhaxkMKAn}
}

@article{chang2024xprompt,
  title={XPrompt: Explaining Large Language Model's Generation via Joint Prompt Attribution},
  author={Chang, Yurui and Cao, Bochuan and Wang, Yujia and Chen, Jinghui and Lin, Lu},
  journal={arXiv preprint arXiv:2405.20404},
  year={2024}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
url          = {https://doi.org/10.1109/CVPR52733.2024.01310},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR},
  pages={13807--13816},
  year={2024}
}
@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}
@article{zhang2024reflective,
  title={Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models},
  author={Zhang, Jinrui and Wang, Teng and Zhang, Haigang and Lu, Ping and Zheng, Feng},
  journal={arXiv preprint arXiv:2407.11422},
  year={2024}
}
@article{sun2024crosscheckgpt,
  title={CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models},
  author={Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark},
  journal={arXiv preprint arXiv:2405.13684},
  year={2024}
}
@article{qu2024look,
  title={Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning},
  author={Qu, Xiaoye and Sun, Jiashuo and Wei, Wei and Cheng, Yu},
  journal={arXiv preprint arXiv:2408.17150},
  year={2024}
}
@article{wang2024understanding,
  title={Understanding Multimodal Hallucination with Parameter-Free Representation Alignment},
  author={Wang, Yueqian and Liang, Jianxin and Wang, Yuxuan and Zhang, Huishuai and Zhao, Dongyan},
  journal={arXiv preprint arXiv:2409.01151},
  year={2024}
}
@article{sun2023generative,
  title={Generative pretraining in multimodality},
  author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  journal={arXiv preprint arXiv:2307.05222},
  year={2023}
}

@inproceedings{pishchulin2014fine,
  title={Fine-grained activity recognition with holistic and pose based features},
  author={Pishchulin, Leonid and Andriluka, Mykhaylo and Schiele, Bernt},
  booktitle={German Conference on Pattern Recognition},
  pages={678--689},
  year={2014},
url          = {https://doi.org/10.1007/978-3-319-11752-2\_56},
  organization={Springer}
}


@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  pages={303--338},
  year={2010},
url          = {https://doi.org/10.1007/s11263-009-0275-4},
  publisher={Springer}
}


@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}


@article{yao2024minicpm,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024},
url          = {https://doi.org/10.48550/arXiv.2408.01800}
}
@article{huang2024deciphering,
  title={Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate},
  author={Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2410.07167},
  year={2024}
}
@article{chen2024ict,
  title={ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models},
  author={Chen, Junzhe and Zhang, Tianshu and Huang, Shiyu and Niu, Yuwei and Zhang, Linfeng and Wen, Lijie and Hu, Xuming},
  journal={arXiv preprint arXiv:2411.15268},
  year={2024}
}

@article{fu2024mme,
  title={Mme-survey: A comprehensive survey on evaluation of multimodal llms},
  author={Fu, Chaoyou and Zhang, Yi-Fan and Yin, Shukang and Li, Bo and Fang, Xinyu and Zhao, Sirui and Duan, Haodong and Sun, Xing and Liu, Ziwei and Wang, Liang and others},
  journal={arXiv preprint arXiv:2411.15296},
  year={2024}
}
@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}
@article{geigle2024african,
  title={African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification},
  author={Geigle, Gregor and Timofte, Radu and Glava{\v{s}}, Goran},
  journal={arXiv preprint arXiv:2406.14496},
  year={2024}
}
@article{wu2023q,
  title={Q-bench: A benchmark for general-purpose foundation models on low-level vision},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and others},
  journal={arXiv preprint arXiv:2309.14181},
  year={2023}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}
@article{yue2024mmmu,
  title={Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yu, Botao and Zhang, Ge and Sun, Huan and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}
@article{xu2024lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}
@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}
@inproceedings{liu2025mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2025},
  organization={Springer}
}
@article{li2024seed,
  author       = {Bohao Li and
                  Yuying Ge and
                  Yi Chen and
                  Yixiao Ge and
                  Ruimao Zhang and
                  Ying Shan},
  title        = {SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with
                  Text-Rich Visual Comprehension},
  journal      = {CoRR},
  volume       = {abs/2404.16790},
  year         = {2024}
}

@article{abs-2311-17092,
  author       = {Bohao Li and
                  Yuying Ge and
                  Yixiao Ge and
                  Guangzhi Wang and
                  Rui Wang and
                  Ruimao Zhang and
                  Ying Shan},
  title        = {SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.17092},
  year         = {2023}
}

@article{ying2024mmt,
  title={Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and others},
  journal={arXiv preprint arXiv:2404.16006},
  year={2024}
}
@article{yu2023mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}



@article{tong2024eyes,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2401.06209},
  year={2024}
}
@inproceedings{fu2025blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2025},
  organization={Springer}
}
@article{li2024naturalbench,
  title={Naturalbench: Evaluating vision-language models on natural adversarial samples},
  author={Li, Baiqi and Lin, Zhiqiu and Peng, Wenxuan and Nyandwi, Jean de Dieu and Jiang, Daniel and Ma, Zixian and Khanuja, Simran and Krishna, Ranjay and Neubig, Graham and Ramanan, Deva},
  journal={arXiv preprint arXiv:2410.14669},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proceedings of the International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{lin2024decoding,
  title={Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation},
  author={Lin, Minhua and Chen, Zhengzhang and Liu, Yanchi and Zhao, Xujiang and Wu, Zongyu and Wang, Junxiang and Zhang, Xiang and Wang, Suhang and Chen, Haifeng},
  journal={arXiv preprint arXiv:2410.17462},
  year={2024}
}

@article{schmirler2024fine,
  title={Fine-tuning protein language models boosts predictions across diverse tasks},
  author={Schmirler, Robert and Heinzinger, Michael and Rost, Burkhard},
  journal={Nature Communications},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{MaWGSTRGM24,
  author       = {Pingchuan Ma and
                  Tsun{-}Hsuan Wang and
                  Minghao Guo and
                  Zhiqing Sun and
                  Joshua B. Tenenbaum and
                  Daniela Rus and
                  Chuang Gan and
                  Wojciech Matusik},
  title        = {{LLM} and Simulation as Bilevel Optimizers: {A} New Paradigm to Advance
                  Physical Scientific Discovery},
  booktitle    = {Proceedings of the Forty-first International Conference on Machine Learning, {ICML}},
  year         = {2024},
  url          = {https://openreview.net/forum?id=hz8cFsdz7P}
}

@inproceedings{brahmavar2024generating,
  title={Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback},
  author={Brahmavar, Shreyas Bhat and Srinivasan, Ashwin and Dash, Tirtharaj and Krishnan, Sowmya Ramaswamy and Vig, Lovekesh and Roy, Arijit and Aduri, Raviprasad},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}
@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{gao2024mini,
  title={Mini-InternVL: a flexible-transfer pocket multi-modal model with 5\% parameters and 90\% performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={1--17},
  year={2024},
  publisher={Springer}
}
@inproceedings{chen2024sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={370--387},
  year={2024},
  organization={Springer}
}

@article{xu2024llm,
  title={LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning},
  author={Xu, Junjie and Wu, Zongyu and Lin, Minhua and Zhang, Xiang and Wang, Suhang},
  journal={arXiv preprint arXiv:2406.01032},
  year={2024}
}

@article{lee2024vlind,
  title={VLind-Bench: Measuring Language Priors in Large Vision-Language Models},
  author={Lee, Kang-il and Kim, Minbeom and Yoon, Seunghyun and Kim, Minsung and Lee, Dongryeol and Koh, Hyukhun and Jung, Kyomin},
  journal={arXiv preprint arXiv:2406.08702},
  year={2024}
}


@inproceedings{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{abs-2407-10671,
  author       = {An Yang and
                  Baosong Yang and
                  Binyuan Hui and
                  Bo Zheng and
                  Bowen Yu and
                  Chang Zhou and
                  Chengpeng Li and
                  Chengyuan Li and
                  Dayiheng Liu and
                  Fei Huang and
                  Guanting Dong and
                  Haoran Wei and
                  Huan Lin and
                  Jialong Tang and
                  Jialin Wang and
                  Jian Yang and
                  Jianhong Tu and
                  Jianwei Zhang and
                  Jianxin Ma and
                  Jianxin Yang and
                  Jin Xu and
                  Jingren Zhou and
                  Jinze Bai and
                  Jinzheng He and
                  Junyang Lin and
                  Kai Dang and
                  Keming Lu and
                  Keqin Chen and
                  Kexin Yang and
                  Mei Li and
                  Mingfeng Xue and
                  Na Ni and
                  Pei Zhang and
                  Peng Wang and
                  Ru Peng and
                  Rui Men and
                  Ruize Gao and
                  Runji Lin and
                  Shijie Wang and
                  Shuai Bai and
                  Sinan Tan and
                  Tianhang Zhu and
                  Tianhao Li and
                  Tianyu Liu and
                  Wenbin Ge and
                  Xiaodong Deng and
                  Xiaohuan Zhou and
                  Xingzhang Ren and
                  Xinyu Zhang and
                  Xipin Wei and
                  Xuancheng Ren and
                  Xuejing Liu and
                  Yang Fan and
                  Yang Yao and
                  Yichang Zhang and
                  Yu Wan and
                  Yunfei Chu and
                  Yuqiong Liu and
                  Zeyu Cui and
                  Zhenru Zhang and
                  Zhifang Guo and
                  Zhihao Fan},
  title        = {Qwen2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2407.10671},
  year         = {2024}
}



@article{yang2024qwen2,
  title={Qwen2.5 technical report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}


@article{abs-2306-13394,
  author       = {Chaoyou Fu and
                  Peixian Chen and
                  Yunhang Shen and
                  Yulei Qin and
                  Mengdan Zhang and
                  Xu Lin and
                  Zhenyu Qiu and
                  Wei Lin and
                  Jinrui Yang and
                  Xiawu Zheng and
                  Ke Li and
                  Xing Sun and
                  Rongrong Ji},
  title        = {{MME:} {A} Comprehensive Evaluation Benchmark for Multimodal Large
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2306.13394},
  year         = {2023}
}


@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

