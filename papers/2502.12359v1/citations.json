[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2018improving",
        "author": "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others",
        "title": "Improving language understanding by generative pre-training"
      },
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "BrownMRSKDNSSAA20",
        "author": "Tom B. Brown and\nBenjamin Mann and\nNick Ryder and\nMelanie Subbiah and\nJared Kaplan and\nPrafulla Dhariwal and\nArvind Neelakantan and\nPranav Shyam and\nGirish Sastry and\nAmanda Askell and\nSandhini Agarwal and\nAriel Herbert{-}Voss and\nGretchen Krueger and\nTom Henighan and\nRewon Child and\nAditya Ramesh and\nDaniel M. Ziegler and\nJeffrey Wu and\nClemens Winter and\nChristopher Hesse and\nMark Chen and\nEric Sigler and\nMateusz Litwin and\nScott Gray and\nBenjamin Chess and\nJack Clark and\nChristopher Berner and\nSam McCandlish and\nAlec Radford and\nIlya Sutskever and\nDario Amodei",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "key": "peng2023instruction",
        "author": "Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng",
        "title": "Instruction tuning with gpt-4"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "LiuLWL23a",
        "author": "Haotian Liu and\nChunyuan Li and\nQingyang Wu and\nYong Jae Lee",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "openai2023gpt",
        "author": "OpenAI",
        "title": "GPT-4V(ision) System Card"
      },
      {
        "key": "claude3",
        "author": "Anthropic",
        "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku"
      },
      {
        "key": "reid2024gemini",
        "author": "Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "key": "liu2023improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "NEURIPS2023_9a6a435e",
        "author": "Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      },
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "chen2024sharegpt4v",
        "author": "Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua",
        "title": "Sharegpt4v: Improving large multi-modal models with better captions"
      },
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      },
      {
        "key": "awadalla2023openflamingo",
        "author": "Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",
        "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models"
      },
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      },
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      },
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      },
      {
        "key": "ye2023mplug2",
        "author": "Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren",
        "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration"
      },
      {
        "key": "yang2024law",
        "author": "Yang, Shijia and Zhai, Bohan and You, Quanzeng and Yuan, Jianbo and Yang, Hongxia and Xu, Chenfeng",
        "title": "Law of Vision Representation in MLLMs"
      },
      {
        "key": "sun2023generative",
        "author": "Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative pretraining in multimodality"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "LiDZWZW23",
        "author": "Yifan Li and\nYifan Du and\nKun Zhou and\nJinpeng Wang and\nWayne Xin Zhao and\nJiRong Wen",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bai2024hallucination",
        "author": "Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng",
        "title": "Hallucination of multimodal large language models: A survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yu2024rlhf",
        "author": "Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others",
        "title": "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback"
      },
      {
        "key": "han2024skip",
        "author": "Han, Zongbo and Bai, Zechen and Mei, Haiyang and Xu, Qianli and Zhang, Changqing and Shou, Mike Zheng",
        "title": "Skip$\\backslash$n: A simple method to reduce hallucination in large vision-language models"
      },
      {
        "key": "sun2023aligning",
        "author": "Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others",
        "title": "Aligning large multimodal models with factually augmented rlhf"
      },
      {
        "key": "liu2023improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "liu2023mitigating",
        "author": "Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang",
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"
      },
      {
        "key": "zhai2023halle",
        "author": "Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun",
        "title": "Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption"
      },
      {
        "key": "chen2023mitigating",
        "author": "Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and Li, Zhaowen and Zhao, Chaoyang and Wang, Jinqiao and Tang, Ming",
        "title": "Mitigating hallucination in visual language models with visual supervision"
      },
      {
        "key": "yue2024less",
        "author": "Yue, Zihao and Zhang, Liang and Jin, Qin",
        "title": "Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective"
      },
      {
        "key": "huang2023opera",
        "author": "Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai",
        "title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation"
      },
      {
        "key": "wang2023evaluation",
        "author": "Wang, Junyang and Zhou, Yiyang and Xu, Guohai and Shi, Pengcheng and Zhao, Chenlin and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Zhang, Ji and Zhu, Jihua and others",
        "title": "Evaluation and analysis of hallucination in large vision-language models"
      },
      {
        "key": "tong2024eyes",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms"
      },
      {
        "key": "jiang2023hallucination",
        "author": "Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and Chen, Jiaxing and Ye, Wei and Yan, Ming and Ye, Qinghao and Zhang, Ji and Huang, Fei and Zhang, Shikun",
        "title": "Hallucination augmented contrastive learning for multimodal large language model"
      },
      {
        "key": "leng2023mitigating",
        "author": "Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding"
      },
      {
        "key": "yin2023woodpecker",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong",
        "title": "Woodpecker: Hallucination correction for multimodal large language models"
      },
      {
        "key": "zhang2024reflective",
        "author": "Zhang, Jinrui and Wang, Teng and Zhang, Haigang and Lu, Ping and Zheng, Feng",
        "title": "Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models"
      },
      {
        "key": "chen2024ict",
        "author": "Chen, Junzhe and Zhang, Tianshu and Huang, Shiyu and Niu, Yuwei and Zhang, Linfeng and Wen, Lijie and Hu, Xuming",
        "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sun2023aligning",
        "author": "Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others",
        "title": "Aligning large multimodal models with factually augmented rlhf"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2023improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2023mitigating",
        "author": "Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang",
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhai2023halle",
        "author": "Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun",
        "title": "Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption"
      },
      {
        "key": "chen2023mitigating",
        "author": "Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and Li, Zhaowen and Zhao, Chaoyang and Wang, Jinqiao and Tang, Ming",
        "title": "Mitigating hallucination in visual language models with visual supervision"
      },
      {
        "key": "yue2024less",
        "author": "Yue, Zihao and Zhang, Liang and Jin, Qin",
        "title": "Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "huang2023opera",
        "author": "Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai",
        "title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2023evaluation",
        "author": "Wang, Junyang and Zhou, Yiyang and Xu, Guohai and Shi, Pengcheng and Zhao, Chenlin and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Zhang, Ji and Zhu, Jihua and others",
        "title": "Evaluation and analysis of hallucination in large vision-language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "tong2024eyes",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jiang2023hallucination",
        "author": "Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and Chen, Jiaxing and Ye, Wei and Yan, Ming and Ye, Qinghao and Zhang, Ji and Huang, Fei and Zhang, Shikun",
        "title": "Hallucination augmented contrastive learning for multimodal large language model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "leng2023mitigating",
        "author": "Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "leng2023mitigating",
        "author": "Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding"
      },
      {
        "key": "chen2024halc",
        "author": "Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei",
        "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding"
      },
      {
        "key": "zhang2024debiasing",
        "author": "Zhang, Yi-Fan and Yu, Weichen and Wen, Qingsong and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu",
        "title": "Debiasing large visual language models"
      },
      {
        "key": "deng2024seeing",
        "author": "Deng, Ailin and Chen, Zhirui and Hooi, Bryan",
        "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding"
      },
      {
        "key": "wu2024noiseboost",
        "author": "Wu, Kai and Jiang, Boyuan and Jiang, Zhengkai and He, Qingdong and Luo, Donghao and Wang, Shengzhi and Liu, Qingwen and Wang, Chengjie",
        "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models"
      },
      {
        "key": "xiao2024seeing",
        "author": "Xiao, Xin and Wu, Bohong and Wang, Jiacong and Li, Chunyuan and Zhou, Xun and Guo, Haoyuan",
        "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment"
      },
      {
        "key": "sarkar2024mitigating",
        "author": "Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Ar{\\i}k, Sercan {\\\"O} and Pfister, Tomas",
        "title": "Mitigating Object Hallucination via Data Augmented Contrastive Tuning"
      },
      {
        "key": "woo2024don",
        "author": "Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick",
        "title": "Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models"
      },
      {
        "key": "chen2024alleviating",
        "author": "Chen, Beitao and Lyu, Xinyu and Gao, Lianli and Song, Jingkuan and Shen, Heng Tao",
        "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2023hallusionbench",
        "author": "Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models"
      },
      {
        "key": "tong2024eyes",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms"
      },
      {
        "key": "LiDZWZW23",
        "author": "Yifan Li and\nYifan Du and\nKun Zhou and\nJinpeng Wang and\nWayne Xin Zhao and\nJiRong Wen",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "LiDZWZW23",
        "author": "Yifan Li and\nYifan Du and\nKun Zhou and\nJinpeng Wang and\nWayne Xin Zhao and\nJiRong Wen",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      },
      {
        "key": "liu2024phd",
        "author": "Liu, Jiazhen and Fu, Yuhan and Xie, Ruobing and Xie, Runquan and Sun, Xingwu and Lian, Fengzong and Kang, Zhanhui and Li, Xirong",
        "title": "Phd: A prompted visual hallucination evaluation dataset"
      },
      {
        "key": "liu2023hallusionbench",
        "author": "Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models"
      },
      {
        "key": "jiang2024hal",
        "author": "Jiang, Chaoya and Ye, Wei and Dong, Mengfan and Jia, Hongrui and Xu, Haiyang and Yan, Ming and Zhang, Ji and Zhang, Shikun",
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models"
      },
      {
        "key": "lovenia2023negative",
        "author": "Lovenia, Holy and Dai, Wenliang and Cahyawijaya, Samuel and Ji, Ziwei and Fung, Pascale",
        "title": "Negative object presence evaluation (nope) to measure object hallucination in vision-language models"
      },
      {
        "key": "huang2024visual",
        "author": "Huang, Wen and Liu, Hongbin and Guo, Minxin and Gong, Neil Zhenqiang",
        "title": "Visual Hallucinations of Multi-modal Large Language Models"
      },
      {
        "key": "kaul2024throne",
        "author": "Kaul, Prannay and Li, Zhizhong and Yang, Hao and Dukler, Yonatan and Swaminathan, Ashwin and Taylor, CJ and Soatto, Stefano",
        "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models"
      },
      {
        "key": "fieback2024metatoken",
        "author": "Fieback, Laura and Spiegelberg, Jakob and Gottschalk, Hanno",
        "title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification"
      },
      {
        "key": "jing2023faithscore",
        "author": "Jing, Liqiang and Li, Ruosen and Chen, Yunmo and Jia, Mengzhao and Du, Xinya",
        "title": "Faithscore: Evaluating hallucinations in large vision-language models"
      },
      {
        "key": "cui2023holistic",
        "author": "Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and Wu, Shirley and Zhang, Linjun and Zou, James and Yao, Huaxiu",
        "title": "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges"
      },
      {
        "key": "wang2023llm",
        "author": "Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao",
        "title": "An llm-free multi-dimensional benchmark for mllms hallucination evaluation"
      },
      {
        "key": "wang2024mitigating",
        "author": "Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng",
        "title": "Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites"
      },
      {
        "key": "chen2024unified",
        "author": "Chen, Xiang and Wang, Chenxi and Xue, Yida and Zhang, Ningyu and Yang, Xiaoyan and Li, Qiang and Shen, Yue and Gu, Jinjie and Chen, Huajun",
        "title": "Unified Hallucination Detection for Multimodal Large Language Models"
      },
      {
        "key": "cha2024visually",
        "author": "Cha, Sungguk and Lee, Jusung and Lee, Younghyun and Yang, Cheoljong",
        "title": "Visually Dehallucinative Instruction Generation: Know What You Don't Know"
      },
      {
        "key": "sun2023aligning",
        "author": "Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others",
        "title": "Aligning large multimodal models with factually augmented rlhf"
      },
      {
        "key": "zheng2024reefknot",
        "author": "Zheng, Kening and Chen, Junkai and Yan, Yibo and Zou, Xin and Hu, Xuming",
        "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models"
      },
      {
        "key": "xu2023lvlm",
        "author": "Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping",
        "title": "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models"
      },
      {
        "key": "sun2024crosscheckgpt",
        "author": "Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark",
        "title": "CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models"
      },
      {
        "key": "qu2024look",
        "author": "Qu, Xiaoye and Sun, Jiashuo and Wei, Wei and Cheng, Yu",
        "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning"
      },
      {
        "key": "wang2024understanding",
        "author": "Wang, Yueqian and Liang, Jianxin and Wang, Yuxuan and Zhang, Huishuai and Zhao, Dongyan",
        "title": "Understanding Multimodal Hallucination with Parameter-Free Representation Alignment"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "LiDZWZW23",
        "author": "Yifan Li and\nYifan Du and\nKun Zhou and\nJinpeng Wang and\nWayne Xin Zhao and\nJiRong Wen",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2023hallusionbench",
        "author": "Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "goyal2017making",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "gurari2018vizwiz",
        "author": "Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P",
        "title": "Vizwiz grand challenge: Answering visual questions from blind people"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "abs-2306-13394",
        "author": "Chaoyou Fu and\nPeixian Chen and\nYunhang Shen and\nYulei Qin and\nMengdan Zhang and\nXu Lin and\nZhenyu Qiu and\nWei Lin and\nJinrui Yang and\nXiawu Zheng and\nKe Li and\nXing Sun and\nRongrong Ji",
        "title": "{MME:} {A} Comprehensive Evaluation Benchmark for Multimodal Large\nLanguage Models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "liu2024mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "Mmbench: Is your multi-modal model an all-around player?"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "li2024seed",
        "author": "Bohao Li and\nYuying Ge and\nYi Chen and\nYixiao Ge and\nRuimao Zhang and\nYing Shan",
        "title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with\nText-Rich Visual Comprehension"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "abs-2311-17092",
        "author": "Bohao Li and\nYuying Ge and\nYixiao Ge and\nGuangzhi Wang and\nRui Wang and\nRuimao Zhang and\nYing Shan",
        "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models"
      }
    ]
  }
]