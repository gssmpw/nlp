@inproceedings{BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
pages = {1877--1901},
url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  booktitle    = {Proceedings of Annual Conference on Neural Information Processing Systems, {NeurIPS}},
  year         = {2020}
}

@inproceedings{LiDZWZW23,
  author       = {Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  JiRong Wen},
  title        = {Evaluating Object Hallucination in Large Vision-Language Models},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.20},
  booktitle    = {Proceedings
                                        of the Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {292--305},
  year         = {2023}
}

@inproceedings{LiuLWL23a,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
  booktitle    = {Proceedings of the Annual Conference on Neural Information Processing Systems, {NeurIPS}},
  year         = {2023}
}

@inproceedings{NEURIPS2023_9a6a435e,
 author = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems, {NeurIPS}},
 pages = {49250--49267},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 year = {2023},
url={https://openreview.net/forum?id=vvoWPYqZJA}
}

@article{abs-2306-13394,
  author       = {Chaoyou Fu and
                  Peixian Chen and
                  Yunhang Shen and
                  Yulei Qin and
                  Mengdan Zhang and
                  Xu Lin and
                  Zhenyu Qiu and
                  Wei Lin and
                  Jinrui Yang and
                  Xiawu Zheng and
                  Ke Li and
                  Xing Sun and
                  Rongrong Ji},
  title        = {{MME:} {A} Comprehensive Evaluation Benchmark for Multimodal Large
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2306.13394},
  year         = {2023}
}

@article{abs-2311-17092,
  author       = {Bohao Li and
                  Yuying Ge and
                  Yixiao Ge and
                  Guangzhi Wang and
                  Rui Wang and
                  Ruimao Zhang and
                  Ying Shan},
  title        = {SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.17092},
  year         = {2023}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{bai2024hallucination,
  title={Hallucination of multimodal large language models: A survey},
  author={Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2404.18930},
  year={2024}
}

@article{cha2024visually,
  title={Visually Dehallucinative Instruction Generation: Know What You Don't Know},
  author={Cha, Sungguk and Lee, Jusung and Lee, Younghyun and Yang, Cheoljong},
  journal={arXiv preprint arXiv:2402.09717},
  year={2024}
}

@article{chen2023mitigating,
  title={Mitigating hallucination in visual language models with visual supervision},
  author={Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and Li, Zhaowen and Zhao, Chaoyang and Wang, Jinqiao and Tang, Ming},
  journal={arXiv preprint arXiv:2311.16479},
  year={2023}
}

@article{chen2024alleviating,
  title={Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization},
  author={Chen, Beitao and Lyu, Xinyu and Gao, Lianli and Song, Jingkuan and Shen, Heng Tao},
  journal={arXiv preprint arXiv:2405.15356},
  year={2024}
}

@article{chen2024halc,
  title={HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding},
  author={Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei},
  journal={arXiv preprint arXiv:2403.00425},
  year={2024}
}

@article{chen2024ict,
  title={ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models},
  author={Chen, Junzhe and Zhang, Tianshu and Huang, Shiyu and Niu, Yuwei and Zhang, Linfeng and Wen, Lijie and Hu, Xuming},
  journal={arXiv preprint arXiv:2411.15268},
  year={2024}
}

@inproceedings{chen2024sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={370--387},
  year={2024},
  organization={Springer}
}

@article{chen2024unified,
  title={Unified Hallucination Detection for Multimodal Large Language Models},
  author={Chen, Xiang and Wang, Chenxi and Xue, Yida and Zhang, Ningyu and Yang, Xiaoyan and Li, Qiang and Shen, Yue and Gu, Jinjie and Chen, Huajun},
  journal={arXiv preprint arXiv:2402.03190},
  year={2024}
}

@misc{claude3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
   URL = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
  author={Anthropic},
  year={2024}
}

@article{cui2023holistic,
  title={Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges},
  author={Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and Wu, Shirley and Zhang, Linjun and Zou, James and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2311.03287},
  year={2023}
}

@article{deng2024seeing,
  title={Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding},
  author={Deng, Ailin and Chen, Zhirui and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.15300},
  year={2024}
}

@article{fieback2024metatoken,
  title={MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification},
  author={Fieback, Laura and Spiegelberg, Jakob and Gottschalk, Hanno},
  journal={arXiv preprint arXiv:2405.19186},
  year={2024}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@article{han2024skip,
  title={Skip$\backslash$n: A simple method to reduce hallucination in large vision-language models},
  author={Han, Zongbo and Bai, Zechen and Mei, Haiyang and Xu, Qianli and Zhang, Changqing and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2402.01345},
  year={2024}
}

@article{huang2023opera,
  title={Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation},
  author={Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2311.17911},
  year={2023}
}

@article{huang2024visual,
  title={Visual Hallucinations of Multi-modal Large Language Models},
  author={Huang, Wen and Liu, Hongbin and Guo, Minxin and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2402.14683},
  year={2024}
}

@article{jiang2023hallucination,
  title={Hallucination augmented contrastive learning for multimodal large language model},
  author={Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and Chen, Jiaxing and Ye, Wei and Yan, Ming and Ye, Qinghao and Zhang, Ji and Huang, Fei and Zhang, Shikun},
  journal={arXiv preprint arXiv:2312.06968},
  year={2023}
}

@article{jiang2024hal,
  title={Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models},
  author={Jiang, Chaoya and Ye, Wei and Dong, Mengfan and Jia, Hongrui and Xu, Haiyang and Yan, Ming and Zhang, Ji and Zhang, Shikun},
  journal={arXiv preprint arXiv:2402.15721},
  year={2024}
}

@article{jing2023faithscore,
  title={Faithscore: Evaluating hallucinations in large vision-language models},
  author={Jing, Liqiang and Li, Ruosen and Chen, Yunmo and Jia, Mengzhao and Du, Xinya},
  journal={arXiv preprint arXiv:2311.01477},
  year={2023}
}

@article{kaul2024throne,
  title={THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models},
  author={Kaul, Prannay and Li, Zhizhong and Yang, Hao and Dukler, Yonatan and Swaminathan, Ashwin and Taylor, CJ and Soatto, Stefano},
  journal={arXiv preprint arXiv:2405.05256},
  year={2024}
}

@article{leng2023mitigating,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  journal={arXiv preprint arXiv:2311.16922},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
url          = {https://proceedings.mlr.press/v202/li23q.html},
  booktitle={Proceedings of the International conference on machine learning, {ICML}},
  pages={19730--19742},
  year={2023}
}

@article{li2024seed,
  author       = {Bohao Li and
                  Yuying Ge and
                  Yi Chen and
                  Yixiao Ge and
                  Ruimao Zhang and
                  Ying Shan},
  title        = {SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with
                  Text-Rich Visual Comprehension},
  journal      = {CoRR},
  volume       = {abs/2404.16790},
  year         = {2024}
}

@article{liu2023hallusionbench,
  title={Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models},
  author={Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2310.14566},
  year={2023}
}

@inproceedings{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

@article{liu2024phd,
  title={Phd: A prompted visual hallucination evaluation dataset},
  author={Liu, Jiazhen and Fu, Yuhan and Xie, Ruobing and Xie, Runquan and Sun, Xingwu and Lian, Fengzong and Kang, Zhanhui and Li, Xirong},
  journal={arXiv preprint arXiv:2403.11116},
  year={2024}
}

@article{lovenia2023negative,
  title={Negative object presence evaluation (nope) to measure object hallucination in vision-language models},
  author={Lovenia, Holy and Dai, Wenliang and Cahyawijaya, Samuel and Ji, Ziwei and Fung, Pascale},
  journal={arXiv preprint arXiv:2310.05338},
  year={2023}
}

@misc{openai2023gpt,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://cdn.openai.com/papers/GPTV_System_Card.pdf}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{qu2024look,
  title={Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning},
  author={Qu, Xiaoye and Sun, Jiashuo and Wei, Wei and Cheng, Yu},
  journal={arXiv preprint arXiv:2408.17150},
  year={2024}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proceedings of the International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{sarkar2024mitigating,
  title={Mitigating Object Hallucination via Data Augmented Contrastive Tuning},
  author={Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Ar{\i}k, Sercan {\"O} and Pfister, Tomas},
  journal={arXiv preprint arXiv:2405.18654},
  year={2024}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@article{sun2023generative,
  title={Generative pretraining in multimodality},
  author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  journal={arXiv preprint arXiv:2307.05222},
  year={2023}
}

@article{sun2024crosscheckgpt,
  title={CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models},
  author={Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark},
  journal={arXiv preprint arXiv:2405.13684},
  year={2024}
}

@article{tong2024eyes,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2401.06209},
  year={2024}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{wang2023evaluation,
  title={Evaluation and analysis of hallucination in large vision-language models},
  author={Wang, Junyang and Zhou, Yiyang and Xu, Guohai and Shi, Pengcheng and Zhao, Chenlin and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Zhang, Ji and Zhu, Jihua and others},
  journal={arXiv preprint arXiv:2308.15126},
  year={2023}
}

@article{wang2023llm,
  title={An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
  author={Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
  journal={arXiv preprint arXiv:2311.07397},
  year={2023}
}

@inproceedings{wang2024mitigating,
  title={Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites},
  author={Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng},
  booktitle={International Conference on Multimedia Modeling},
  pages={32--45},
url          = {https://doi.org/10.1007/978-3-031-53302-0\_3},
  year={2024},
  organization={Springer}
}

@article{wang2024understanding,
  title={Understanding Multimodal Hallucination with Parameter-Free Representation Alignment},
  author={Wang, Yueqian and Liang, Jianxin and Wang, Yuxuan and Zhang, Huishuai and Zhao, Dongyan},
  journal={arXiv preprint arXiv:2409.01151},
  year={2024}
}

@article{woo2024don,
  title={Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models},
  author={Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17820},
  year={2024}
}

@article{wu2024noiseboost,
  title={NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models},
  author={Wu, Kai and Jiang, Boyuan and Jiang, Zhengkai and He, Qingdong and Luo, Donghao and Wang, Shengzhi and Liu, Qingwen and Wang, Chengjie},
  journal={arXiv preprint arXiv:2405.20081},
  year={2024}
}

@article{xiao2024seeing,
  title={Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment},
  author={Xiao, Xin and Wu, Bohong and Wang, Jiacong and Li, Chunyuan and Zhou, Xun and Guo, Haoyuan},
  journal={arXiv preprint arXiv:2405.17871},
  year={2024}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@article{yang2024law,
  title={Law of Vision Representation in MLLMs},
  author={Yang, Shijia and Zhai, Bohan and You, Quanzeng and Yuan, Jianbo and Yang, Hongxia and Xu, Chenfeng},
  journal={arXiv preprint arXiv:2408.16357},
  year={2024}
}

@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{ye2023mplug2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{yin2023woodpecker,
  title={Woodpecker: Hallucination correction for multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong},
  journal={arXiv preprint arXiv:2310.16045},
  year={2023}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
url          = {https://doi.org/10.1109/CVPR52733.2024.01310},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR},
  pages={13807--13816},
  year={2024}
}

@article{yue2024less,
  title={Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective},
  author={Yue, Zihao and Zhang, Liang and Jin, Qin},
  journal={arXiv preprint arXiv:2402.14545},
  year={2024}
}

@article{zhai2023halle,
  title={Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption},
  author={Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun},
  journal={arXiv preprint arXiv:2310.01779},
  year={2023}
}

@article{zhang2024debiasing,
  title={Debiasing large visual language models},
  author={Zhang, Yi-Fan and Yu, Weichen and Wen, Qingsong and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu},
  journal={arXiv preprint arXiv:2403.05262},
  year={2024}
}

@article{zhang2024reflective,
  title={Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models},
  author={Zhang, Jinrui and Wang, Teng and Zhang, Haigang and Lu, Ping and Zheng, Feng},
  journal={arXiv preprint arXiv:2407.11422},
  year={2024}
}

@article{zheng2024reefknot,
  title={Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models},
  author={Zheng, Kening and Chen, Junkai and Yan, Yibo and Zou, Xin and Hu, Xuming},
  journal={arXiv preprint arXiv:2408.09429},
  year={2024}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

