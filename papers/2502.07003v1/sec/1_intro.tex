
\section{Introduction}
\label{sec:intro}



\begin{figure}
    \begin{center}
    \includegraphics[width=0.99\columnwidth]{images/teaser.png}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{One model, many space to ground applications.} We train a single model, AstroLoc, that succeeds in multiple space-based image retrieval settings with significant domain gaps.
    We show strong performance on astronaut photography localization, “lost in space” orbit determination, and historical (Space Shuttle) photography localization.
    Each group of 3 images represent a query and its top-2 predictions from searching over a worldwide database of thousands of satellite images. Correct predictions in green, wrong predictions in red.
    }
    \vspace{-5mm}
    \label{fig:teaser}
\end{figure}


Earth observation is the process of collecting information about the Earth's surface, which is vital for monitoring the state of our planet.
Among the multitude of systems used to acquire Earth observations data, most of which rely on automatic collection by nadir-facing satellites,
manual acquisition of photos by astronauts aboard the International Space Station (ISS) is a clear outlier.
This imagery has distinctive properties including (1) a wide range of spatial resolutions (up to 2 meters per pixel), (2) the potential for oblique perspectives to observe topography and height (\eg, measure the height of a volcanic eruption to divert flights), (3) various illumination conditions (whereas satellite imagery is typically sun-synchronous, always capturing the same area at the same time of day), and (4) offering the highest resolution open source Earth observations data.
Furthermore, due to the ISS's 90 minute repeating orbit, astronauts can be tasked with imaging for near real time disaster response, and they can use human intuition to focus on relevant areas and events, making the photographs very information-dense, whereas satellites commonly collect images of areas regardless of semantic value.
All these characteristics make this data source a perfect complement to satellite data and a crucial component for climate science \cite{astro_photos_climate_patterns}, atmospheric phenomena \cite{sprites, TLEs}, urban planning \cite{Sanchez_2022_artificial_lightning, Gaston_2022_environ_impacts_artificial_light, Small_2022_spectrometry_urban_lightscape, Schirmer_2019_nightlight_behavior}, and, most importantly, disaster management and response \cite{IDC_Stefanov}\footnote{\url{https://storymaps.arcgis.com/stories/947eb734e811465cb0425947b16b62b3}}.

Although over 5 million photos have been collected from the ISS since the beginning of its operations, unfortunately their full potential has yet to be unlocked.
The complication is that astronaut photos, unlike satellite imagery, are not automatically localized \wrt the Earth's surface.
Even though the position of the ISS can be determined from the timestamp of the photo and the ISS orbit path, an astronaut can point their camera (a standard hand-held DSLR) toward any location on Earth within their vast visibility range, which spans roughly 20 million square kilometers (sqkm)\footnote{Considering that the ISS orbits at an altitude of 415 kilometers.}.
Therefore, localizing a single photo that can cover an area as small as 100 sqkm (\ie 0.0005\% of the visible/search area) is akin to finding a needle in a haystack.
For example, when an astronaut is above Texas and photographs a wildfire they see, the burning area could be anywhere from Canada to Mexico depending on the camera orientation, and the only way to know where is to geolocate the image.

The potential value these photos hold when geolocated has led to a concerted manual localization effort, with experts and citizen scientists localizing over 300,000 of these images -- a process defined by NASA as a ``monumentally important, but monumentally time-consuming job"\footnote{We highly encourage the reader to view this clip \url{https://www.youtube.com/watch?v=drrP_Iss0gA&t=295s}}. Nevertheless, manually geolocating a single astronaut photo can take hours, effectively making this procedure inadequate given the ever-growing number of images that are being collected (up to 10k per day).
This problem has prompted recent investigation into automating the task of \textbf{Astronaut Photography Localization (APL)} with deep learning models,  combining both image retrieval~\cite{Berton_2024_EarthLoc} and image matching techniques~\cite{Stoken_2023_CVPR, Berton_2024_EarthMatch, Stoken_2024_CVPR}.
Image retrieval is used as a first step, to search within a database of precisely geo-referenced satellite images for those that are most similar to the query astronaut photo. With such a system, a query can be coarsely localized in less than a second, making it a viable solution for such a large scale problem. Afterwards, the retrieval candidates may be refined using more computationally demanding matching techniques.
Although these pipelines have shown satisfactory speeds for APL, their performance show a wide margin for improvement, and we hypothesize that this is due to existing APL models being trained with only satellite images~\cite{Berton_2024_EarthLoc}. This is a critical limitation, considering that the end goal is to geo-reference astronaut photos, not satellite images.

We argue that it would be beneficial to exploit the available 300,000 hand-labeled open-source astronaut photos to train the image retrieval model. The difficulty with doing so is that these photos are only weakly annotated, \ie, the manual label only provides the GPS location of a random point close to the center of the image.
Therefore, these weakly labeled photos cannot be directly used in a metric learning framework, where the model must be provided with pairs of queries (astronaut photos) and database (satellite) images from the same location. In order to meet this criteria, we need these photos to be labeled with their full footprint (\ie precise GPS location of its four corners), so that we can exactly determine images from the database to be used as positive samples (\ie, with a good overlap \wrt the training queries). The concept of weak and full labels is visualized in \cref{fig:weak_full_annotation}.

\begin{figure}
    \begin{center}
    \includegraphics[width=0.8\columnwidth]{images/weak_full_annotation.png}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{Visual example of weak (manual) annotation and full annotation of an astronaut photo.} Weak annotation is the GPS coordinates of a single point, which does not provide information regarding the image's size (\ie it could cover a city or an entire continent), whereas full annotation provides GPS for its 4 corners (called the footprint), from which the coordinate of any pixel within the image can be easily calculated (pixel-wise label).
    }
    \vspace{-5mm}
    \label{fig:weak_full_annotation}
\end{figure}


In light of these considerations, we first produce a precise annotation of these 300,000 photos, leveraging their weak annotation and a state-of-the-art matching pipeline to label their full footprint. 
Then, we demonstrate the importance of using this newly annotated data for training, by extending the state-of-the-art method EarthLoc~\cite{Berton_2024_EarthLoc} in two ways.
First, we create pairs of matching astronaut-satellite images, and use them in a pairwise contrastive loss that takes advantage of the fact that the two images within each pair come from different domains. The creation of these pairs relies on the new annotation, \ie, we select a pair if the two images have enough intersection over union on Earth's surface.
Even with this loss that takes advantage of the newly labeled 300,000 astronaut photos, it is still important to fully leverage the much larger set of 5.3M geo-referenced satellite images, as is done in previous methods. However, we observe that the satellite images are uniformly distributed on the surface of the Earth, whereas the photos from astronauts tend to be unevenly distributed, with higher concentrations in salient areas like volcanoes, glaciers, lakes and coastlines.
Therefore, we use a second contrastive loss that leverages tuples of satellite images only, but sampled according to the distribution of astronaut photos.

We find that our training pipeline, yielding a model we call \textbf{AstroLoc}, leads to a model so powerful that it saturates existing test sets (recall@100 above 99\%), and we therefore propose new, more challenging test sets that better reflect the task of APL in the real world.

To summarize, our contributions are as follows:

\begin{itemize}
    \item we provide precise localization labels to 300,000 weakly labeled astronaut photographs, which will be made available to support further research in this field;
    \item we propose two techniques to effectively adopt this data source into an Astronaut Photography Localization training pipeline;
    \item AstroLoc leads to a staggering 35\% improvement on average of recall@1 over previous SOTA, and practically solves existing test sets (recall@100 around 99\%);
\end{itemize}
Furthermore, AstroLoc is practically useful and has already been used to localize hundreds of thousand of images available here\footnote{Photos available at: \url{https://eol.jsc.nasa.gov/ExplorePhotos/}
}, and shows great potential for tasks like the lost-in-space satellite and historical space image localization.
We finally note that, thanks to AstroLoc, in a few months the backlog of non-localized astronaut photos will be nearly empty for the first time since the launch of the ISS, with the exception of a small fraction that still produce failure cases.










