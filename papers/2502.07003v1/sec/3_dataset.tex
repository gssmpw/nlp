
\section{Data}
\label{sec:datasets}

Our goal is to localize the extent of \textbf{astronaut photos (queries)} given a worldwide map of geo-referenced \textbf{satellite images (database)}.
Astronaut photographs represent one of the most diverse and long standing Earth observations data sources. The dataset contains a wide range in spatial resolution, field of view, illumination (including night imagery), and obliquity (astronauts can tilt the camera and take oblique photos).
These images have many applications in numerous fields
\cite{sprites, TLEs, astro_photos_climate_patterns, Sanchez_2022_artificial_lightning, Gaston_2022_environ_impacts_artificial_light, Small_2022_spectrometry_urban_lightscape, Schirmer_2019_nightlight_behavior}, and
most importantly in disaster management and response \cite{IDC_Stefanov}\footnote{\url{https://eol.jsc.nasa.gov/Collections/Disasters/ShowDisastersCollection.pl}}.

For database images we follow~\cite{Berton_2024_EarthLoc} and use a \textit{composited} set of cloudless open-source satellite yearly imagery from Sentinel-2, named \textit{S2}.
We use tiles from zoom levels 8-12 \cite{osm_zoom_levels} (expanding in both directions on the set used in~\cite{Berton_2024_EarthLoc}), to provide a thorough train and test set that reflects the extents of the query images.
Characteristics of astronaut and satellite imagery are compared in \cref{tab:data}.

\begin{table}
\begin{center}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{c|cc}
\toprule
Type & Queries & Database \\
\midrule
Acquisition & manual, by astronauts & automatic, by satellites \\
Number (Annotation) & 5M (none), 300k (weak) & 5.2M (full) \\
Extent &  40 to 1,357,493 sqkm &  346 to 391,776 sqkm \\
Extent percentile (5 / 95) &  239 / 39,729 sqkm &  465 / 16,026 sqkm \\
Occlusions & Yes (ISS hardware, clouds) & No \\
Obliquity \& distortion & Yes & No \\
Illumination changes & Yes (\eg day/dawn/sunset) & No \\
Years &  2000 to 2024 &  2018 to 2021 \\
Source & \url{https://eol.jsc.nasa.gov/} & \url{https://s2maps.eu} \\
Coverage & scattered, biased 
on glaciers, volcanoes etc. & dense, worldwide \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\vspace{-5mm}
\caption{\textbf{Overview of data.} Information reported about satellite imagery refers specifically to the data used in this project, not satellite imagery in general. The labeling refers to the weak and full labeling shown in \cref{fig:weak_full_annotation}.}
\vspace{-5mm}
\label{tab:data}
\end{table}




\subsection{Training Set}
\label{sec:train_set}
In contrast to previous work, we propose taking advantage of the 300k manually localized astronaut photos for training.
The idea is to pair these photos with matching satellite imagery from the database (\ie with enough Intersection over Union, or IoU) and apply contrastive training to train models that are robust to the domain differences between astronaut and satellite imagery.
Unfortunately, the available manual localization is only a weak annotation (see \cref{fig:weak_full_annotation}), which does not provide enough information to compute IoU. To overcome this limitation, we estimate the precise footprint for each query image.
For each manually localized query, we select
as potential positives all the database images which contain the weak label point at each zoom level, and we rotate these potential positives by 90°, 180° and 270° to maximize the chance of finding a similar image among the potential positives. In practice, given five zoom levels, four rotations, and the fact that each point is covered by four images (there is overlap among the tiles), there are $5 \times 4 \times 4 = 80$ potential positives per query.
Note that different zoom levels are required because it is impossible to estimate an image extent with only a weak label, and a photo could cover an area as small as a city or as big as a continent.
We then perform image matching with SuperPoint \cite{Detone_2018_superpoint} + LightGlue \cite{Lindenberger_2023_lightglue} and the EarthMatch pipeline \cite{Berton_2024_EarthMatch} to get the footprint coordinates of each query.

This localization was successful for 221k queries, with the remaining ones not localizable due to either errors in manual labeling, too much obstruction from clouds, or the image being of the horizon (\ie Earth limb photograph, with two corners representing outer space, hence an invalid footprint).
With this now precisely localized collection of astronaut photography, we pair each query with the all database images with an IoU over $t_{iou}=0.2$ producing 865k query-database training pairs.
We then discard any pair containing a query that is included within the test sets to avoid any data leakage.


\subsection{Evaluation Sets}
\label{sec:evaluation_sets}
Evaluation sets are made of queries and the associated database to localize them.
Six evaluation sets were proposed in \cite{Berton_2024_EarthLoc}, covering geographical areas across five continents that mirror astronaut photography use cases like land change study and flood monitoring.
However, as detailed in their limitations \cite{Berton_2024_EarthLoc}, the test sets only contain queries whose projected area on Earth's surface is between 5,000 and 900,000 sq km, which represents only 22\% of all astronaut photo queries, as shown in \cref{fig:sqkm_distribution}.

\begin{figure}
    \begin{center}
    \includegraphics[width=0.8\columnwidth]{images/sqkm_distribution.jpg}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{Distribution of queries by covered area.} The red mark at 5,000 sqkm shows that 78\% of astronaut photographs cover an area lower than 5,000 sqkm. Thus, the test sets in \cref{tab:main_table_1} do not contain this vast majority of queries, leading us to propose test sets containing all queries used for experiments in \cref{tab:main_table_2}.}
    \vspace{-5mm}
    \label{fig:sqkm_distribution}
\end{figure}
We therefore propose new evaluation sets, to include all available geolocated queries within an evaluation area, as well as add zoom levels 8 and 12 to the database to match the wider range of query areas.
To avoid introducing complexity, we use the same regions as the test sets from \cite{Berton_2024_EarthLoc}. These datasets were named after the geographic location of their center, like Texas, Gobi, and Amazon, so we call these new extended versions Texas-L (L for Large), Gobi-L, etc.
