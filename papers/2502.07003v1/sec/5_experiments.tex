
\input{tables/main_table_1}
\input{tables/main_table_2}

\section{Experiments}
\label{sec:experiments}

\subsection{Implementation Details}
\label{sec:Implementation_details}

\custompar{Training}
We set the hyperparameters as follows: $t_{iou}=0.2, \alpha_1=1, \alpha_2=1, \beta_1=50, \beta_2=50, \lambda_1=1, \lambda_2=1, K=50$, batch size $=$ 48 (48 pairs for pair loss, 48 quadruplets for MUM loss), learning rate $=$ 5e-5, Adam optimizer.
We use as architecture a DINO-v2-base backbone \cite{Oquab_2023_dinov2} with SALAD \cite{Izquierdo_2024_SALAD} and a linear layer to reduce feature dimensionality (from 8448 to 2048) (\ie a similar model to AnyLoc \cite{Keetha_2023_AnyLoc} while being over 10 times lighter as they use DINO-v2-giant).
Training runs for 30k iterations.
As in \cite{Berton_2024_EarthLoc}, the Texas dataset is used as validation.

\custompar{Evaluation}
We follow common procedure \cite{Berton_2024_EarthLoc} by doing image retrieval with an augmented dataset (\ie applying four 90Â° rotations to each image),
and using as metric the recall@N, as the percentage of queries where at least one of the top-N predictions is a correct match to the query.


\subsection{Results}
In \cref{tab:main_table_1} we report experiments on the test sets proposed by \cite{Berton_2024_EarthLoc}.
For fairness, we include results with a more powerful version of EarthLoc that uses the same architecture and training data of AstroLoc, which we refer to as EarthLoc++.

Results clearly show that, whereas previously there was no dominant method between EarthLoc and AnyLoc, the introduction of AstroLoc provides a new state-of-the-art model which significantly outperforms all previous, achieving a near perfect (above 99\%) recall@100 on all six evaluation sets.
Note that the recall@100 is more relevant in the real world than recall@1, since it is common practice to re-rank the top-N predictions with image matching methods \cite{Berton_2024_EarthMatch, Stoken_2023_CVPR}.
Some qualitative results of queries and their predictions are in \cref{fig:teaser}, with many more in the supplementary.


\custompar{New test sets}
Here we compute experiments on the newly extended test sets described in \cref{sec:evaluation_sets}, which provide a setting that is more similar to the real world scenario and more challenging. Results, reported in \cref{tab:main_table_2}, illustrate that AstroLoc is able to outperform previous models by an even wider margin, and presents recall@100 consistently above 96\% even in these more comprehensive and complex cases.

\subsection{Other Space to Ground Use Cases}
In this paper we test the robustness of AstroLoc by performing experiments on two related tasks: the lost-in-space problem and historical space imagery localization.
Given the lack of space, we present a thorough explanation of the tasks, motivations, experimental details and results in the supplementary, while only offering a brief summary here.
\label{sec:lost_in_space}
\input{tables/vinsat}
\custompar{The Lost-in-Space satellite problem} has the goal of identifying the location/orbit of nanosatellites through computer vision \cite{McCleary_2024_vinsat}, requiring photos be searched for over the entire planet.
We use as queries the images from McLeary et al. \cite{McCleary_2024_vinsat}, and a worldwide database at zoom level 9 (12k images).
Results in \cref{tab:vinsat} highlight the robustness and adaptability of AstroLoc, which is the only method to achieve a R@1 over 50\% on the retrieval-formulation of lost in space, outperforming all other methods by at least 45\% of R@1.

\input{tables/sshuttle}
\custompar{Historical space imagery localization}
Historical imagery of Earth from space is a unique source of data to understand how Earth has changed over decadal time spans.
We perform experiments on 704 queries from early Space Shuttle days (1981-1984), on a worldwide database, and report results in \cref{tab:sshuttle}.
We empirically find that AstroLoc achieves strong results even on these old photographs, showcasing its robustness to various types of domain changes.


\subsection{Ablations}
\label{sec:ablations}
\custompar{Training setup}
We compute ablations on the losses and mining in \cref{tab:ablation}, where results clearly show the strong impact from each module in our pipeline.
Most importantly, the combination of these components presents the optimal characteristic of orthogonality, in that their mixture has notably higher results than each of the singular elements.
Features visualizations of the model trained with and without our mining is also shown in \cref{fig:tsne}.

\input{tables/ablation}

\custompar{Feature dimensionality}
Given that feature dimensionality plays a crucial role in any large-scale retrieval task, as it is linearly correlated with the test-time memory footprint and matching time (\ie the time it takes to process the kNN), we present a plot (\cref{fig:ablation_feat_dim}) of how results vary when changing the dimension, as we compute results with two versions of the DINO-v2 backbone, base and small.
Interestingly, we find that results increase up to the highest dimensionality (\ie 8448, equal to the input dimension to the final linear layer), showing no plateau at lower dimensions.
\begin{figure}
    \begin{center}
    \includegraphics[width=0.8\columnwidth]{images/ablation_feat_dim.jpg}
    \end{center}
    \vspace{-6mm}
    \caption{\textbf{Ablation on feature size.} Backbone (DINO-v2 small/base) with different feature size/dimension. Stars
    are models selected for our experiments, blue is AstroLoc-tiny (\cref{sec:lost_in_space}) and orange is AstroLoc in all other tables.
    R@1 computed on Texas-L.
    }
    \vspace{-2mm}
    \label{fig:ablation_feat_dim}
\end{figure}

\subsection{Toward Worldwide Search}
\label{sec:worldwide_search}
Astronaut photographs are associated with a timestamp which, combined with the known orbit of the ISS, can be used to obtain the position of the camera when the photo was taken, allowing the search space to be reduced from the entire extent of the Earth to just 20M sqkm (the area of Earth's surface visible from the ISS).
However, due to high-energy cosmic particles causing bit-flips in cameras, the timestamp can be unreliable, leaving no information about which area of the Earth the photo is taken near.

We therefore propose the task of worldwide astronaut photography localization, where the database covers the entire world with 881k images.
Results (\cref{tab:worldwide}) show that AstroLoc achieves a recall@100 of 96.8\%, proving its robustness even in this highly challenging scenario.
\input{tables/worldwide}

