
\section{Method}
\label{sec:method}

\subsection{Setting}
\label{sec:setting}

\begin{figure*}
    \begin{center}
    \includegraphics[width=0.9\linewidth]{images/architecture.jpg}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{Overview of AstroLoc training pipeline.}
    The upper branch feeds pairs of matching query-database images to the pairwise loss (\cref{sec:query_sat_pairwise_loss}).
    The lower branch (\cref{sec:cluster_sampling}) first creates clusters of satellite images, then queries are assigned to these clusters, and then the clusters are sampled according to how many queries are assigned to each cluster (unsupervised mining).
    A training batch (mined quadruplets, \ie tuples of 4 images) is sampled from a single cluster, and fed to the Multi-similarity Unsupervised Mining (MUM) loss.
    Queries are not fed to the MUM loss, and are not used to compute the clusters ---~they are only used to sample training data from a closer distribution to the queries'.
    }
    \vspace{-5mm}
    \label{fig:architecture}
\end{figure*}

\custompar{Task}
Astronaut Photography Localization (APL) is the task of estimating the geographic location covered by a photo (query) taken by astronauts.
APL can be approached through image retrieval: for each query, estimate its location by finding the most similar match in a large database of geolocated images of Earth.

\custompar{Formalization}
We pose the task as a retrieval problem, and rely on three sets of images:
\begin{itemize}
    \item The \textbf{database} $\mathcal{D}=\{d_1, d_2, \ldots, d_{N_{\mathcal{D}}}\}$ is a set of $N_\mathcal{D}$ \emph{satellite} images, each characterized by their RGB content and the coordinate label of their four corners, called the \textit{footprint}. Formally, the footprint for a generic image $d_i \in \mathcal{D}$ is denoted as $F_i=\{lat_1, lon_1, lat_2, lon_2, lat_3, lon_3, lat_4, lon_4\}$ where $lon_k$ and $lat_k$ denote the longitude and latitude of the $k$-th corner, respectively (\cref{fig:weak_full_annotation}).
    \item The \textbf{training queries} $\mathcal{Q}=\{q_1, q_2, \ldots, q_{N_{\mathcal{Q}}}\}$ is a set of $N_{\mathcal{Q}}$ astronaut photos,  with RGB content and footprints (localized as explained in \cref{sec:train_set}).
    \item The \textbf{test queries} $\mathcal{T}=\{t_1, t_2, \ldots, t_{N_{\mathcal{T}}}\}$ is another set of $N_{\mathcal{T}}$ astronaut photos, disjoint from $\mathcal{Q}$, each characterized by its RGB content and timestamp. The timestamp is crucial in determining the location of the ISS when the photo was taken, which enables narrowing down the search space from a world-wide area (510M sqkm) to the local area visible from the ISS at that time (20M sqkm). Note that due to cosmic radiation bit flips, the timestamp can sometimes be incorrect: in such cases, it is necessary to conduct a ``world-wide" search (see \cref{sec:worldwide_search}).
\end{itemize}


\custompar{Training with Astronaut Photos (Queries)}
While various approaches have been used for APL, no previous work has taken advantage of the huge dataset of 300k astronaut photos labeled by human experts.
We argue that their use at training time can lead to huge improvements in performance, and we present two novel techniques to use this data.
In \cref{sec:query_sat_pairwise_loss} we show how we use pairs of matching query-reference images (\ie, depicting the same location) described in \cref{sec:train_set} to implement a \textit{pairwise loss} for this setup (see \cref{fig:architecture} top).
Additionally, we also use a second loss computed on tuples of satellite images, by sampling them using the distribution of the queries (see an overview in \cref{fig:architecture} bottom), allowing us to use the entire set of database images for training (whereas the \textit{pairwise loss} can be applied only on areas covered by astronaut photos) while still leveraging the queries, as we explain in \cref{sec:cluster_sampling}.



\subsection{Query-Satellite Pairwise Loss}
\label{sec:query_sat_pairwise_loss}

Given the cross-domain nature of the problem, we aim to take matching images from the different distributions and applying contrastive learning to encode their relationship in the feature space.
To this end, we create a batch $\mathcal{P} = \{p_1, p_2, \ldots, p_{B} \}$ of query-database pairs $p_i = (q_i, d_i) \in (\mathcal{Q}, \mathcal{D})$ that have an intersection over union (IoU) higher than a threshold $t_{iou}$\footnote{For the sake of readability, we abused the notation by implying that a matching query and database images from the sets $\mathcal{Q}$ and $\mathcal{D}$ have the same index as the index of the pair they belong to.
This trick to simplify the notation is equivalent to have preliminarily sorted the sets $\mathcal{Q}$ and $\mathcal{D}$.} (see examples in \cref{fig:architecture} top).
We ensure that within a batch there are no two pairs that have geographic overlap, so that for a pair $p_i\in\mathcal{P}$ all the images from other pairs $p_j \in\mathcal{P}\backslash\{p_i\}$ are negative examples.

With this setting, we define a contrastive loss that comprises two terms: an attraction term $\mathcal{L}_{pos}$ that acts between images in a matching pair, pulling their representations closer, and a repulsion term $\mathcal{L}_{neg}$, that is applied between images from different pairs (\ie, non matching), pushing them apart.
Formally, given a similarity measure $\mathcal{S}(I_1, I_2)$ between the features of two image (\eg, the cosine similarity), we define the positive loss term as
{\small
\begin{align}
        \mathcal{L}_{pos} 
        & = \frac{1}{\alpha_1 B} \sum_{i=1}^{B}
      \gamma(\alpha_1, q_i, d_i)
\end{align}
}
where $\alpha_1>0$ is a gain
and
{\small
\begin{equation}
   \gamma(x,y,z) =  \text{log} \left[ 1 + e^{-x \times \mathcal{S}(y,z)} \right]
\end{equation}
}
is the attraction function.

For the negative loss term, let us first denote with $\mathcal{P}_Q  = \{q_1, q_2, \ldots, q_B\}$ the set of all queries in the batch $\mathcal{P}$, and with $\mathcal{P}_D  = \{d_1, d_2, \ldots, d_B\}$ the set of all database images in $\mathcal{P}$. With this notation, we define $\mathcal{L}_{neg}$ as 
{\small
\begin{align}    
\begin{split}
    \mathcal{L}_{neg}
    & =  \frac{1}{\beta_1 B} \sum_{i=1}^B 
    \Big[ \varphi(\beta_1, q_i, \mathcal{P}_Q\backslash\{q_i\}) + 
     \varphi(\beta_1, q_i, \mathcal{P}_D\backslash\{d_i\}) 
    \\
    &+
     \varphi(\beta_1, d_i, \mathcal{P}_Q\backslash\{q_i\})
 + 
     \varphi(\beta_1, d_i, \mathcal{P}_D\backslash\{d_i\})
    \Big]
    \end{split}
\end{align}
}

where $\beta_1>0$ is a gain and
{\small
\begin{equation}
    \varphi(x,y,\mathcal{Z}) = \text{log} \left( 1 + \sum_{j=1}^{|\mathcal{Z}|} e^{x \times \mathcal{S}(y,\mathcal{Z}_j)} \right)
\end{equation}
}
is the repulsion function.
The overall loss is 
{\small
\begin{equation}
    \mathcal{L}_{pairs} = \mathcal{L}_{pos} + \mathcal{L}_{neg}
\end{equation}
}



\subsection{Unsupervised Mining}
\label{sec:cluster_sampling}

\begin{figure}
    \begin{center}
    \includegraphics[width=0.95\columnwidth]{images/cluster_sampling.png}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{Weighted clusters from unsupervised mining.} ~The spatial distribution of images from 50 mined clusters, colored by cluster sampling weight. Higher weighted areas (yellow) are sampled more frequently than lower weighted areas (blue).
    }
    \vspace{-5mm}
    \label{fig:cluster_sampling}
\end{figure}

To fully exploit the much larger corpus of satellite image, we use a second loss inspired by EarthLoc \cite{Berton_2024_EarthLoc}, where quadruplets of satellite images are used with a multi-similarity loss \cite{Wang_2019_multi_similarity_loss} to learn robust feature representations.
Each quadruplet contains four images of the same place, \ie positives toward each other, and negatives \wrt the images from other quadruplets.
To implement this idea, EarthLoc~\cite{Berton_2024_EarthLoc} first uses an offline clustering strategy, to divide the database images into visually similar subsets (\eg, a cluster with images of mountains, one with deserts, one with coastlines, etc.). Formally, it creates a set of $K$ clusters $\mathcal{C}_1, \ldots, \mathcal{C}_K$ such that
{\small
\begin{equation}
    \bigcup_{k=1}^{K} \mathcal{C}_k = \mathcal{D}
\end{equation}
}
The clusters are computed with a k-means algorithm using features computed with the model while it's training, as common practice with mining techniques \cite{Arandjelovic_2018_netvlad}.
Then, at each iteration a batch of quadruplets from one single cluster is created, where the chosen cluster is uniformly sampled among all possible clusters. Formally, we denote this sampled batch of quadruplets as 
{\small
\begin{align}
    \mathcal{H}_k = \{h_1, \ldots, h_H \}
\end{align}
}
where the generic $h_i=(h_{i_1}, \ldots, h_{i_4})\in (\mathcal{C}_k,\mathcal{C}_k,\mathcal{C}_k,\mathcal{C}_k)$ is a quadruplet of images from the same location, and the cluster is chosen according to uniform sampling
{\small
\begin{equation}
    k \sim U(1,K)
    \label{eq:uniform_sampling}
\end{equation}
}
EarthLoc demonstrates that this random sampling of clusters achieves significant improvement over sampling quadruplets from the entire database $\mathcal{D}$. This is because clustering ensures that all the images in the batch, including the ones used as negatives, are visually similar, thus yielding a better signal for a contrastive loss (\ie that the negatives are harder negatives).
However, we argue that the uniform sampling in \cref{eq:uniform_sampling}  leads the model to learn on a data distribution that is geographically very dissimilar from how the photos from astronauts are distributed. Generally, astronauts take few photos of feature-less landscapes like deserts and ice sheets, while often photographing scenes relevant to natural disasters and climate change like glaciers, coastlines and islands.
\begin{figure}
    \begin{center}
    \includegraphics[width=0.7\columnwidth]{images/tsne_comparison.png}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{t-SNE plot of features with and without our mining.} Blue dots represent query set features, and other dots represent features of the training quadruplets (made of database images) with and without our mining, (right and left, respectively). Although neither is a perfect blend,  our mining allows sampling from a distribution much more intertwined with the query distribution.}
    \vspace{-5mm}
    \label{fig:tsne}
\end{figure}
To avoid this distribution shift, we propose sampling the cluster to be used to create the batch $\mathcal{H}_k$ according to the distribution of the query set $\mathcal{Q}$. We first compute the features from our training queries, assign them to clusters, and then apply a \textit{weighted} sampling of the clusters weighted by how many queries belong to it.
Note that the clusters are computed only on the database, and the queries are simply assigned to them to determine the clusters' weighting.
For example, if there are three clusters A, B and C (see \cref{fig:architecture}), and 10\% of queries are assigned to cluster A, 90\% to cluster B, and 0 to cluster C, then the training batches will be sampled 10\% from cluster A and 90\% from cluster B (see \cref{fig:architecture}).
Formally, let's denote with $b_1, \ldots, b_K$ the size of the bins derived from assigning the features of the training queries to the $K$ clusters.
The sampling of the cluster is then replaced by
{\small
\begin{equation}
    k\sim B(Q, 1, k)
    \label{eq:weighted_sampling}
\end{equation}
}
where $B$ denotes the weighted distribution according to the bins $b_1, \ldots, b_K$, \ie such that 
{\small
\begin{equation}
  Pr(k) = \frac{b_k}{\sum_{i=1}^{K}{b_i}}
\end{equation}
}
Unlike what commonly done in contrastive learning \cite{Musgrave_2020_PyTorchML, Arandjelovic_2018_netvlad, AliBey_2022_BMVC, Musgrave_2020_ML_reality}, this mining is unsupervised, as the queries' labels are not used, meaning that we could potentially use all of the 5M existing astronaut photos.
In practice we use only training queries to avoid seeing test queries at train time.
The second loss used in our pipeline is thus
{\small
\begin{equation}
    \begin{split}
        \mathcal{L}_{MUM} = \frac{1}{4H}\sum_{i=1}^{H} \left[\frac{1}{\alpha_2} 
        \sum_{j=1}^4 \text{log} 
        \left( 1+ \sum_{d\in h_i\backslash\{h_{i_{j}}\}}e^{-\alpha_2\mathcal{S}(h_{{i}_j}, d)} \right) \right. \\ 
        + \left. \frac{1}{\beta_2} \sum_{j=1}^4 \text{log} \left( 1+ \sum_{d\in\mathcal{H}_k\backslash \{h_i\}}e^{+\beta_2 \mathcal{S}(h_{{i}_j}, d)} \right) \right]
    \end{split}
\end{equation}
}
where $k$ is sampled according to \cref{eq:weighted_sampling}, $\alpha_2$ and $\beta_2$ are positive hyperparameters, and MUM stands for \textbf{M}ulti-similarity with \textbf{U}nsupervised \textbf{M}ining.

To summarize, our unsupervised mining relies on the following steps:
\begin{enumerate}
    \item extract features from database (\ie satellite imagery);
    \item compute k-means clustering on these features;
    \item  extract features from training queries (\ie astronaut photos), assign them to the clusters;
    \item sample database images in proportion to the number of training queries assigned to that cluster.
\end{enumerate}
These steps are repeated periodically every 5000 iterations, increasing training time by less than 10\%.
This is a data-driven unsupervised mining, where the goal is to sample database images using the training queries distribution (defined in eq. 9).
The practical effect of this mining is visible in \cref{fig:cluster_sampling}, where yellower areas represent more informative clusters that are therefore sampled more often.


Finally, our final loss is the sum of the two losses:{\small
\begin{equation}
    \mathcal{L} = \lambda_{1} \mathcal{L}_{pairs} + \lambda_{2} \mathcal{L}_{MUM}
\end{equation}
}
