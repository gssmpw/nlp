
\section{Related Work}
\label{sec:related_work}
\custompar{Astronaut and satellite imagery localization}
The task of localizing photos taken from Earth's orbit has been studied in a number of domains.
Straub and Christian \cite{Straub_2015_sat_loc} examine the possibility of matching the coastline of imagery from satellites to enhance their autonomous navigation capabilities. Schockley and Bettinger \cite{Shockley_2021_sat_loc} take a similar approach, but instead propose using terrestrial illumination matching.
The problem is more recently addressed by McCleary et al. \cite{McCleary_2024_vinsat}, which selects a set of landmarks on Earth to achieve the same goal. They note that common hardware solutions for localization are expensive ($\sim$10k\$) for nanosatellite developers, making a software solution a game changer for the industry.
For the similar task of localizing astronaut photography, which was historically performed by hand \cite{Stoken_2023_CVPR}, Stoken and Fisher proposed an automated solution leveraging image matching \cite{Stoken_2023_CVPR}, in a framework that was then expanded to localize night-time imagery \cite{Stoken_2024_CVPR}.

While these solutions have advanced the field of Earth from space localization, they are limited, as Berton et al. \cite{Berton_2024_EarthLoc} notes. The matching-based methods can not achieve the scalability and speed required to localize astronaut photographs in real time. Berton et al. proposes approaching the task as an image retrieval problem, where the database is composed of satellite images of known location.
In this setting, AnyLoc \cite{Keetha_2023_AnyLoc}, a universal place recognition model, has been shown to produce strong performance and robustness to the query-database domain gap.

\custompar{Unmanned aerial vehicle localization}
A closely related task is that of unmanned aerial vehicle (UAV) localization. Similar to Astronaut Photography Localization (APL), it is approached through cross-domain image retrieval, where the database is often made of satellite imagery and the queries are photos collected by a drone.
The goal is to efficiently find the best prediction for a given query in order to localize the drone in real time \cite{Zheng_2023_UAV_workshop}.
Among notable examples from the literature, 
Bianchi and Barfoot~\cite{bianchi2021uav} introduced a method utilizing autoencoded satellite images, significantly reducing storage and computation costs while maintaining robust localization performance. Dai et al.~\cite{dai2022finding} proposed an end-to-end framework, FPI, that directly identifies UAV locations by matching UAV-view images with satellite-view counterparts, streamlining the localization process. Li et al.~\cite{li2024transformer} developed a transformer-based adaptive semantic aggregation method, focusing on part-level representations to improve UAV-satellite image matching by capturing detailed semantic information.

Further improvements in UAV localization are achieved by orientation-guided methods, as shown by Deuser et al.~\cite{Deuser_2023_ogcl_uav_view}, who use orientation-guided contrastive learning to improve UAV-satellite image alignment. Zhu et al.~\cite{Zhu_2023_uav_backbone_winnerUAV_mbeg} proposed a modern backbone architecture optimized for efficient UAV geo-localization, enhancing both speed and accuracy for UAV applications.
