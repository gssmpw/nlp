% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{enumitem}

\usepackage{marvosym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{MoM: Linear Sequence Modeling with Mixture-of-Memories}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Jusen Du$^{1,2}$}\textsuperscript{\dag},
 \textbf{Weigao Sun$^{1}$}\textsuperscript{\Letter},
 \textbf{Disen Lan$^{1,3}$}\textsuperscript{\dag},
 \textbf{Jiaxi Hu$^{4}$},
 \textbf{Yu Cheng$^{5}$}\textsuperscript{\Letter}
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}Shanghai AI Laboratory,
 \textsuperscript{2}Nanjing University,
 \textsuperscript{3}South China University of Technology,
\\
 \textsuperscript{4}The Hong Kong University of Science and Technology (Guangzhou),
\\
 \textsuperscript{5}The Chinese University of Hong Kong
\\
}
% \mycite{
%     \begin{center}
%         \textit{"The enemy of memory isn’t time; it’s other memories."} \\
%         --- David Eagleman
%     \end{center}
% }

% \setlength{\belowcaptionskip}{-5pt}

\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}


\begin{document}
\maketitle

\blfootnote{\dag\ interns at Shanghai AI Laboratory; \Letter\  corresponding authors (sunweigao@outlook.com, chengyu@cse.cuhk.edu.hk). }

\begin{abstract}
Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models.

The code is released at \url{https://github.com/OpenSparseLLMs/MoM} and is also released as a part of \url{https://github.com/OpenSparseLLMs/Linear-MoE}.
\end{abstract}

\section{Introduction}
\input{introduction}

\section{Preliminary}
For notations in this work, we use bold lower-case letters for row vectors (e.g., $\bm q_t, \bm k_t$), bold upper-case letters for matrices (e.g., $\bm Q, \bm K$) and the identical letters represent a row in the matrix, e.g., $\bm q_t$ is the $t$-th row of $\bm Q$.

% \subsection*{Neural Basis of Memory Separation}
% The hippocampus employs coupled theta (4–8 Hz) and gamma (30–100 Hz) oscillations to segregate sequential memories \cite{buzsaki2002theta}. Each theta cycle partitions time into discrete gamma subcycles (~25-50 ms), where distinct neuronal ensembles activate through competitive "E\%-max" mechanisms \cite{de2009second}. This temporal separation prevents overlapping representations. By encoding memories in separate gamma windows, interference between them is avoided, while the theta phases coordinate their sequential integration. Crucially, this biological strategy demonstrates how limited memory capacity can maintain high-fidelity representations through structured temporal multiplexing – a principle we adapt computationally through memory compartmentalization.

\subsection*{Softmax Attention}
Given input $X = [x_1,...,x_n]^T \in \mathbb{R}^{n \times d}$,where n is the sequence length and $x_t \in \mathbb{R}^d$ is the $t$-th input vector with $d$ dimensions. Transformer \cite{vaswani2017attention} softmax attention computes:

\begin{equation}
    \bm Q = \bm X\bm W_Q, \bm K = \bm X\bm W_K, \bm V = \bm X\bm W_V,
\end{equation}

\begin{equation}
    \bm O = \mathrm{softmax}(\frac{\bm Q \bm K^T}{\sqrt{d}} \odot \bm A)\bm V,
\end{equation}
where $\bm Q, \bm K, \bm V \in \mathbb{R}^{n \times d}$, $\bm A \in \mathbb{R}^{d \times d}$ is an attention mask, $\odot$ denotes element-wise production.

The training time complexity is $O(n^2)$ due to the $\bm Q \bm K^T$ operation. During inference, Transformers store all the $\bm k, \bm v$ calculated as "$\bm{KV}$ cache" which can be viewed as memory states. As the $\bm{KV}$ cache grows with the input, the Transformers can be considered to separate the memory storage for each input token individually and have an unlimited memory capacity. This also results in a time complexity of $O(n)$ per token during inference.

\subsection*{Linear Attention}
To reduce the time complexity of Transformer attention, various optimization techniques have been proposed. Linear Transformers \cite{katharopoulos2020transformers} replace the softmax attention mechanism with dot-product of feature maps $\phi(\cdot)$:
\begin{equation}
    \bm o_t = \frac{\sum_{i=1}^n \phi(\bm q_t)\phi(\bm k_i)^T\bm v_i}{\sum_{i=1}^n \phi(\bm q_t)\phi(\bm k_i)^T},
\end{equation}
where $\bm q_t,\bm k_t, \bm v_t \in \mathbb{R}^{d}$. While the presence of the denominator may lead to numerical instability \cite{qin2024lightning} and the feature map can utilize an identity function, which we omit for simplicity. In perspective of memory, the formulation can also be written in a recurrent format:
\begin{equation}
    \bm M_t = \bm M_{t-1} + \bm k_t^T\bm v_t, \quad \bm o_t = \bm q_t \bm M_t,
\end{equation}
This indicates that linear attention can function as a linear recurrent layer with a matrix-valued hidden state $\bm M$ which we refer to as memory sate and the output is generated by querying the memory state $\bm M$. This represents the ultimate compression of sequence information, condensing the entire sequence into a single memory state.

Building on the foundational concepts of linear attention and memory perspective, some recent advancements have focused on optimizing memory structure, including gated updates \cite{yang2023gated, qin2024hierarchically, qin2024hgrn2} and memory capacity expansion \cite{peng2024eagle, qin2024hgrn2}.

% Building on the foundational concepts of linear attention and memory perspective, some recent advancements have focused on optimizing memory structure. GLA \cite{yang2023gated} introduces a data-dependent forget gate during memory updates to better maintain long-term dependencies. HGRN \cite{qin2024hierarchically} and HGRN2 \cite{qin2024hgrn2} employs forget gates with monotonically increased lower bound values from bottom layers to upper layers. RWKV \cite{peng2024eagle} introduces muti-head, matrix-valued memory to enhance performance. Delta networks and its gated variant \cite{widrow1960adaptive, schlag2021linear, yang2024gated} dynamically rewrite the old memory value based on the input key with a new value. GSA \cite{zhang2024gated} separates the combined memory of $\bm k$ and $\bm v$ into two distinct parts, one for each. Numerous studies have indicated that expanding memory capacity can enhance the performance of linear attention, which aligns intuitively \cite{qin2024hgrn2, peng2024eagle}.



\begin{figure}[t]
    \centering
    \includegraphics[width=1.05\linewidth]{figs/mom_fig2.pdf}
    \caption{\textbf{Framework of MoM.} Each input token selectively activates and updates $K$ memory states, leaving non-activated memory states unchanged to avoid interference from current input. Additionally, we introduce a continuously activated shared memory. This figure presents the basic memory update mechanism; other mechanisms involving gating or more complex updates follow a similar approach.}
    \label{fig:mom-framework}
\end{figure}

\section{Method}
\subsection{Motivation}
Linear sequence models compress the entire sequence data into a fixed-size memory state. Despite numerous efforts to minimize information loss—such as introducing gating mechanisms and employing more precise control over memory modifications \cite{orvieto2023resurrecting, de2024griffin, beck2024xlstm, yang2023gated, zhang2024gated}—some degradation in this compression process is inevitable. Expanding the memory capacity has been shown to mitigate this issue to some extent, with studies indicating that increasing memory capacity can enhance model performance \cite{qin2024hgrn2, peng2024eagle}.

However, previous approaches that simply increased the size of the RNN state, essentially expanding a single memory state, struggled to capture the full spectrum of information within an entire sequence. We propose that this difficulty arises because sequence information is often multifaceted, and a single, expanded memory may not be capable of simultaneously capturing multiple aspects of the data. Inputs that introduce new or orthogonal information may interfere with existing memory content when using a shared memory. Rather than discarding these inputs through gating mechanisms or overwriting the existing memory state, it may be more effective to consider alternative strategies that allow for the preservation of diverse information without interference.


\subsection{MoM: Mixture-of-Memories}
To address the challenge outlined above, we propose a novel approach inspired by biological mechanisms for encoding multi-item memory such as theta-gamma oscillations \cite{lisman2013theta}, and concepts from Mixture-of-Experts (MoE) \cite{shazeer2017outrageously}, where different experts handle specific tokens. In this approach, we leverage multiple memory states, each of which is selectively updated by different inputs. This increases the memory capacity and enables the model to retain diverse pieces of information by storing various types of inputs in separate memory states.

In our framework, the memory states function similarly to the experts in MoE. However, instead of relying on completely separate networks, these modules are individual RNN states embedded within a linear recurrent mechanism. This design allows for the isolation of memory updates while concurrently managing distinct types of information. It is important to note that MoM fundamentally differs from traditional MoE, as we will discuss in Appendix~\ref{app:diff-moe}. Figure~\ref{fig:mom-framework} provides an overview of the MoM architecture. Below, we introduce the structure of the MoM layer and explain how this biologically-inspired architecture is implemented in the context of linear sequence modeling.

\subsubsection{Router Network}
We use a router to assign inputs to different memory states. Utilizing the top-$k$ concept, each token is routed to the top-$k$ memories based on its importance scores. Specifically, we use a simple linear layer to generate these scores for each input token. After applying a softmax function, we select the top-$k$ scores and normalize them.
\begin{equation}
    \mathbf{scores}_t = \mathrm{TopK}(\mathrm{softmax}(\bm x_t\bm W_g))\in \mathbb{R}^{k},
\end{equation}

\begin{equation}
    \bm g_t = \frac{\mathbf{scores}_t}{\sum \mathbf{scores}_t}\in \mathbb{R}^{k},
    \label{eq:importance_score}
\end{equation}
where $\bm x_t \in \mathbb{R}^{d}$, $k$ is the top-$k$ number, $\bm W_g \in \mathbb{R}^{d \times M}$ is learnable weight, $\bm g_t$ is the normalized importance scores of the input $\bm x_t$.

\subsubsection{Linear Recurrent Memory Module}
After the router network, the input $\bm x_t$ is directed to top-$k$ linear recurrent modules, meaning that the top-$k$ memories are activated while the others remain inactive. For each activated memory module, indexed by $m$, we perform the following operation:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Key and Value Projections}: We project the input $\bm x_t$ to $\bm k^m_t$ and $\bm v^m_t$ using $\bm W^m_k$ and $\bm W^m_v$:
    \begin{equation}
        \bm k^m_t = \bm x_t \bm W^m_k, \bm v^m_t = \bm x_t \bm W^m_v \in \mathbb{R}^d,
    \end{equation}
    where $\bm W^m_k$, $\bm W^m_v$ are learnable projection weights for $kv$ of the $m$-th memory module.
    \item \textbf{Memory Update}: We update the activated memory state using $\bm k^m_t$, $\bm v^m_t$:
    \begin{equation}
        \bm M^m_t = \bm M^m_{t-1} + (\bm k^m_t)^T \bm v^m_t \in \mathbb{R}^{d \times d},
    \end{equation}
    The equation above represents the simplest form of memory update for clarity. Our approach is flexible and does not rely on a specific memory update mechanism. To enhance performance, we can incorporate mechanisms such as forget gates~\cite{sun2023retentive}:
    \begin{equation}
        \bm M^m_t = \gamma \bm M^m_{t-1} + (\bm k^m_t)^T \bm v^m_t \in \mathbb{R}^{d \times d},
    \end{equation}
    where $\gamma$ is a constant forget gate.

    More generally, our method can be adapted to incorporate various memory update methods proposed in previous work. Detailed descriptions of these methods are provided in Table~\ref{tab:memory_update}.
\end{enumerate}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ll}
    \toprule
        \textbf{Method} & \textbf{Memory Update Rule}  \\
        \midrule
        LA & $\bm M_t = \bm M_{t-1} + \bm k_t^T \bm v_t$ \\
        Lightning & $\bm M_t = \gamma\bm M_{t-1} + \bm k_t^T \bm v_t$ \\
        RetNet & $\bm M_t = \gamma\bm M_{t-1} + \bm k_t^T \bm v_t$ \\
        HGRN2 & $\bm M_t = (\bm a_t^T\bm 1) \bm M_{t-1} + (1 - \bm a_t)^T \bm v_t$ \\
        GLA & $\bm M_t = (\bm a_t^T\bm 1) \bm M_{t-1} + \bm k_t^T \bm v_t$ \\
        Mamba2 & $\bm M_t = \alpha_t \bm M_{t-1} + \beta_t \bm k_t^T \bm v_t$ \\
        DeltaNet & $\bm M_t = (\bm I - \bm k_t^T \bm k_t) \bm M_{t-1} + \beta_t \bm k_t^T \bm v_t$ \\
        G-DeltaNet & $\bm M_t = \alpha_t(\bm I - \bm k_t^T \bm k_t) \bm M_{t-1} + \beta_t \bm k_t^T \bm v_t$ \\
        TTT & $\bm M_t = \bm M_{t-1} + \beta_t \nabla l(\bm M_{t-1};\bm k_t, \bm v_t) $ \\
        Titan & $\bm M_t = \alpha_t \bm M_{t-1} + \beta_t \nabla l(\bm M_{t-1};\bm k_t, \bm v_t) $ \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Memory Update Rules.} We demonstrate that several current linear sequence models can be viewed as recurrent models in terms of memory updates, where $\alpha_t, \beta_t \in (0,1)$ are data-dependent scaler, $\bm a_t$ is data-dependent vector, and $\gamma$ is a data-independent constant.}
    \vspace{2mm}
    \label{tab:memory_update}
\end{table}

\noindent\textbf{Memory Mixing.} After updating the activated memory states, we perform a weighted sum of these memory states using the importance scores obtained from Equation(\ref{eq:importance_score}).
\begin{equation}
    \tilde{\bm M}_t = \sum_m g_t^{(m)} \bm M^m_t \in \mathbb{R}^{d \times d},
\end{equation}
where $\bm M_m$ is one activated memory and $g_t^{(m)}$ is the importance score of $\bm M_m$.

We then obtain the output of the MoM by applying query vector $\bm q_t$ to the mixed memory $\tilde{\bm M}_t$:
\begin{equation}
    \bm o_t = \bm q_t \tilde{\bm M}_t \in \mathbb{R}^d,
\end{equation}
Finally, the output of the MoM layer is computed by applying an activation function, normalization, and a linear transformation:
\begin{equation}
    \bm o_t = \mathrm{RMSNorm}(\mathrm{Swish} (\bm o_t)) \bm W_o \in \mathbb{R}^d,
\end{equation}
Throughout the recurrent process, only a subset of memory states is activated and updated at each time step, while memory states that are not routed remain inactive and unchanged. When the input passes through the key-value projection layer, it generates multiple sets of keys and values that are fed into different memory modules. This design enables the model to maintain multiple memory states, each preserving distinct pieces of information. By aggregating the activated memories into a comprehensive mixed memory by weighted summation, the query can effectively retrieve information from this mixed memory, which results the "attention output" followed by other layers.

% 引入类脑后，与MoE的比较似乎并不重要，可以放在附录
% \subsection{Comparison between MoM and MoE} \label{sec:differ_MoE}
% While our approach to implementing the Mixture of Memories (MoM) draws inspiration from the Mixture of Experts (MoE) framework, there are significant differences that set our method apart.
% \begin{itemize}
%     \item \textbf{Purpose}: The MoE was introduced to scale up the number of parameters without significantly increasing computational resources. It address the limitations of dense models in scaling both parameters and computational demands through sparse activation. However, MoM is designed to expand the memory capacity of linear attention models while preserving their linear time complexity. By sparsely activating memories and using weighed summation to create a mixed memory, MoM effectively address the challenge of forgetting historical information in linear attention.
%     \item \textbf{Structure}: In conventional MoE, each expert is a separate neural network within the feedford network (FFN) layer \cite{qwen_moe, wei2024skywork}. In contrast, in MoM, each memory is an RNN state with unique key-value projection weights to generate different key-value pairs. MoE operates during the channel mixing phase, where each token is processed independently by selected exoerts. On the other hand, MoM functions during the token mixing phase, where each memory processes different segments of the sequence, preserving inter-token relationships.
% \end{itemize}

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{llccccccc}
        \toprule
        \textbf{Scale} & \textbf{Model} & \textbf{FDA} & \textbf{SWDE} & \textbf{SQUAD} & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Drop} & \textbf{Avg.} \\
        \midrule
        \rowcolor{gray!30}
        \cellcolor{white}\multirow[t]{4}{2.5cm}{\textit{340M Params} \\ \textit{15B Tokens}\\ \textit{L=24, d=1024}} & Transformer++ & 46.14 & 25.87 & 33.22 & 18.94 & 45.97 & 20.03 & 31.70 \\
        & RetNet & 5.90 & 9.28 & 22.41 & 6.91 & 40.05 & 18.59 & 17.19 \\
        & HGRN2 & 11.53 & 17.34 & 24.08 & 12.67 & 43.84 & 17.35 & 21.14 \\
        & GLA & 11.26 & 16.78 & 27.85 & 12.77 & 43.90 & 17.68 & 21.71 \\
        & GSA & 6.36 & 16.87 & 21.90 & 14.60 & 42.18 & 16.72 & 19.77 \\
        & Gated DeltaNet & 20.53 & 23.24 & 28.55 & \textbf{14.98} & \textbf{44.91} & 16.48 & 24.78 \\
        & MoM & \textbf{30.79} & \textbf{26.05} & \textbf{29.63} & 13.84 & 44.79 & \textbf{20.41} & \textbf{27.59} \\
        \midrule
        \rowcolor{gray!30}
        \cellcolor{white}\multirow[t]{4}{2.5cm}{\textit{1.3B Params} \\ \textit{100B Tokens}\\ \textit{L=24, d=2048}} & Transformer++$^\dagger$ & 44.32 & 32.43 & 42.59 & 24.49 & 58.47 & 21.56 & 37.31 \\
        & RetNet$^\dagger$ & 13.62 & 22.59 & 33.46 & 15.43 & 53.79 & 19.79 & 26.45 \\
        & HGRN2$^\dagger$ & 12.35 & 23.24 & 33.19 & 19.10 & 55.27 & 19.65 & 27.13 \\
        & GLA$^\dagger$ & 27.61 & 30.93 & 35.04 & 22.27 & 56.28 & 19.45 & 31.93 \\
        & GSA$^\dagger$ & 23.25 & 32.80 & 35.57 & 22.96 & 57.05 & 20.65 & 32.05 \\
        & Gated DeltaNet & 30.25 & 27.65 & 34.06 & 23.22 & 58.23 & 20.36 & 32.30 \\
        & MoM & \textbf{41.14} & \textbf{34.30} & \textbf{37.08} & \textbf{24.11} & \textbf{58.59} & \textbf{21.03} & \textbf{36.04} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Results on Recall-Intensive Tasks.} All inputs are truncated to a maximum length of 2K tokens. MoM significantly outperforms all other linear models across both model sizes. In the 1.3B model, MoM even achieves performance very close to that of Transformer models.}
    \label{table:recall}
\end{table*}

\subsection{Optimizations for MoM}

\noindent\textbf{Shared Memory.}\hspace{0.5em}To enhance our model's ability to capture long-term dependencies, we introduce a \emph{shared memory} mechanism. This shared memory has access to the entire sequence information, allowing it to effectively store and retrieve long-term information. By integrating shared memory into our model, we ensure that it can leverage the complete historical context, resulting in significant improvements in performance and robustness.

\begin{flalign}
    & \bm k^s_t = \bm x_t \bm W^s_k, \bm v^s_t = \bm x_t \bm W^s_v \in \mathbb{R}^d \\
    & \bm M^s_t = \bm M^s_{t-1} + (\bm k^s_t)^T \bm v^s_t \in \mathbb{R}^{d \times d} \\
    & \tilde{\bm M}_t = \bm M^s_t + \sum_m g_t^{(m)} \bm M^m_t \in \mathbb{R}^{d \times d}
\end{flalign}

\noindent\textbf{Hardware-efficient Implementation.}\hspace{0.5em} Chunk-wise parallel form of linear attention is a computational optimization strategy that divides the input sequence into smaller chunks to enable partial parallel and recursive computation during model training \cite{hua2022transformer, yang2023gated}, which is well supported in open-source frameworks.

In the implementation of MoM, mixing the memories before multiplying by the query is mathematically equivalent to firstly multiplying each memory by the query and then mixing the results. This property allows us to reuse the efficient Triton-based operator implementation of all previous linear sequence modeling methods, facilitate the hardware-efficient training as well as inference of MoM.

\renewcommand{\thefootnote}{}
\footnote{$^\dagger$Models marked with an asterisk use open-source pretrained weights with identical training configurations.}

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{llcc|ccccccc}
        \toprule
        \textbf{Scale} & \textbf{Model} & \begin{tabular}[c]{@{}c@{}} \textbf{Wiki.} \\ ppl$\downarrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Lamb.} \\ ppl$\downarrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{ARC-e} \\ acc$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{ARC-c} \\ $\mathrm{acc_n}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Hella.} \\ $\mathrm{acc_n}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Lamb.} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{PIQA} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Wino.} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \textbf{Avg.} \\
        \midrule
        \multirow[t]{4}{2cm}{\textit{340M Params} \\ \textit{15B Tokens} \\ \textit{L=24, d=1024}} & Transformer++ & 26.88 & 76.46 & 44.91 & \textbf{25.94} & 34.95 & 26.90 & 64.31 & 51.07 & 41.35 \\
        & RetNet & 31.07 & 87.11 & 44.49 & 23.04 & 33.86 & 23.93 & 63.49 & 52.33 & 40.19 \\
        & HGRN2 & 27.90 & 77.40 & 45.24 & 23.63 & 35.61 & 24.74 & 65.45 & \textbf{54.06} & 41.46 \\
        & GLA & 28.78 & 79.95 & 44.53 & 22.27 & 34.84 & 24.94 & 63.93 & 51.38 & 40.32 \\
        & GSA & 28.17 & 82.50 & 45.50 & 24.23 & 35.00 & 24.02 & 64.85 & 50.43 & 40.67 \\
        & Gated DeltaNet & 26.47 & 58.59 & 46.04 & 23.55 & 35.18 & 27.01 & \textbf{66.05} & 50.83 & 41.44 \\
        & MoM & \textbf{26.00} & \textbf{51.25} & \textbf{46.13} & 24.15 & \textbf{35.91} & \textbf{28.26} & 65.61 & 52.57 & \textbf{42.11} \\
        \midrule
        \multirow[t]{4}{2cm}{\textit{1.3B Params} \\ \textit{100B Tokens} \\ \textit{L=24, d=2048}} & Transformer++$^\dagger$ & 17.61 & 19.29 & 55.01 & 28.07 & 49.21 & 40.95 & 70.08 & 56.27 & 49.93 \\
        & RetNet$^\dagger$ & 18.18 & 21.97 & 57.49 & 26.88 & 48.09 & 37.75 & 69.37 & 53.28 & 48.81 \\
        & HGRN2$^\dagger$ & 17.32 & 15.65 & \textbf{58.33} & 28.07 & \textbf{51.93} & 42.31 & 71.33 & 52.01 & 50.66 \\
        & GLA$^\dagger$ & 17.61 & 19.66 & 55.18 & 27.56 & 48.89 & 40.03 & 69.86 & 53.91 & 49.24 \\
        & GSA$^\dagger$ & 16.69 & 16.02 & \textbf{58.33} & \textbf{28.33} & 50.98 & 42.03 & \textbf{72.25} & 53.43 & 50.89 \\
        & Gated DeltaNet & 17.14 & 18.80 & 56.82 & 27.39 & 49.77 & 39.94 & 71.76 & 51.78 & 49.58 \\
        & MoM & \textbf{16.64} & \textbf{14.83} & 55.35 & 27.99 & 50.95 & \textbf{43.43} & 71.27 & \textbf{56.83} & \textbf{50.97} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Results on Common-Sense Reasoning Tasks.} The performance of linear models and Transformer models is comparable; however, MoM consistently achieves the best average performance across all model sizes.}
    \label{table:common}
\end{table*}

\section{Experiments}

\subsection{Experimental Setups}

\textbf{Models.}\hspace{0.5em} In our experiments, we employ the Gated DeltaNet \cite{yang2024gated} as the memory update mechanism in MoM. The model is configured with four memory states, two of which are activated at each time step, along with a shared memory.

\noindent\textbf{Baselines.}\hspace{0.5em} We evaluate MoM against several linear recurrent models and Transformers, including RetNet \cite{sun2023retentive}, GLA \cite{yang2023gated}, Gated DeltaNet \cite{yang2024gated}, and Transformer++ \cite{touvron2023llama}, which incorporates Rotary Position Embeddings \cite{su2024roformer} and GLU \cite{shazeer2020glu} into the Transformer architecture. To ensure a fair comparison, we train all baseline models from scratch using the exact same number of tokens.

\noindent\textbf{Training.}\hspace{0.5em} We follow the training procedure described by \citet{yang2023gated}, utilizing the SlimPajama dataset \cite{cerebras2023slimpajama} sampled with 100B tokens and tokenized using the Mistral tokenizer \cite{jiang2023mistral}. We train models from scratch with parameter sizes of 340M and 1.3B, respectively. For the 340M models, we train on 15B tokens with a batch size of 0.5M tokens. The warmup tokens count is set to 0.25M. We set the hidden ratio of our model to 3 to keep the overall parameter count approximately the same. For the 1.3B models, we train on 100B tokens with a batch size of 2M tokens. We utilized publicly available pretrained weights from \citet{zhang2024gated} with exactly same configuration. The warmup tokens count is 1B. We employ AdamW optimizer \cite{loshchilov2017fixing,sun2024co2} with learning rate of 3e-4 with cosine learning rate schedule \cite{zhou2020pbsgd}. The weight decay is set to 0.01 and gradient clipping is 1.0.

\subsection{Main Results}

\subsubsection{Recall-intensive Tasks}
Linear sequence models, due to their limited memory capacity, often exhibit a significant performance gap compared to Transformer models, especially in recall-intensive tasks where extensive context is crucial. These tasks highlight notable performance differences among various linear models, making them a more accurate benchmark for evaluating a linear model's capabilities in handling contextual information.

To thoroughly assess our model's proficiency in such scenarios, we test six recall-intensive tasks following \citet{arora2024simple}: FDA \cite{arora2023language}, SWDE \cite{arora2023language, lockard-etal-2019-openceres}, SQuAD \cite{rajpurkar2018know}, NQ \cite{kwiatkowski2019natural}, TriviaQA \cite{joshi2017triviaqa} and Drop \cite{dua2019drop}. These tasks are designed to challenge a model's ability to perform context-based retrieval and comprehension.

As shown in Table~\ref{table:recall}, our proposed approach, benefiting from increased memory capacity and memory mixing mechanism, achieves significant improvements over other linear sequence models. Specifically, our model effectively narrows the performance gap with Transformer models. This improvement underscores the advantage of our method in capturing and utilizing long-range dependencies, thereby enhancing performance on tasks that require extensive contextual understanding.

\subsubsection{Commonsense Reasoning Tasks}
As shown in Table~\ref{table:common}, we report the language modeling perplexity and zero-shot performance of commonsense reasoning tasks following \cite{zhang2024gated} which includes WikiText \cite{merity2016pointer}, LAMBADA \cite{paperno2016lambada}, ARC-easy, ARC-challenge \cite{Clark2018ThinkYH}, HellaSwag \cite{zellers2019hellaswag}, PiQA \cite{Bisk2020} and WinoGrande \cite{sakaguchi2019winogrande}. The evaluation results are based on the lm-evaluation-harness \cite{eval-harness}.

Experimental results show that MoM outperforms other linear models and surpassed the Transformer model as well.

\subsubsection{Long Context Tasks}
Assessing performance on long-context tasks is crucial for linear models, as it reflects their ability to handle long-range dependencies effectively. We evaluated our model's comprehension of long contexts using the LongBench benchmark \cite{bai2024longbenchbilingualmultitaskbenchmark, 2023opencompass}. In Table~\ref{table:long}, we present the average results across various categories, including summarization, few-shot learning, synthetic tasks, and code completion, along with the overall mean across all tasks. The complete detailed results are provided in Appendix~\ref{table:complete-longbench}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Sum} & \textbf{FS} & \textbf{Syn} & \textbf{Code} & \textbf{Avg.} \\
        \midrule
        RetNet$^\dagger$ & 6.30 & 15.76 & \textbf{2.64} & 40.52 & 13.61 \\
        HGRN2$^\dagger$ & 6.51 & 15.50 & 2.61 & 40.11 & 13.02 \\
        GSA$^\dagger$ & \textbf{7.75} & 20.29 & 1.92 & 42.83 & 14.61 \\
        Gated DeltaNet & 7.14 & 18.00 & 2.10 & 41.52 & 13.98 \\
        MoM & 6.89 & \textbf{21.26} & 2.63 & \textbf{47.79} & \textbf{15.64} \\
        \bottomrule
    \end{tabular}
    \begin{minipage}{\textwidth}
    \small
    \end{minipage}
    \caption{\textbf{LongBench Results.} All evaluations were done using \citet{2023opencompass}. \textit{Note:} Sum = Summarization, FS = Few-shot, Syn = Synthetic.}
    \label{table:long}
\end{table}

\begin{table*}[h!]
    \centering
    \small
    \vspace{-4mm}
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Model} & \textbf{FDA} & \textbf{SWDE} & \textbf{SQUAD} & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Drop} & \textbf{Avg.} \\
        \midrule
        GLA \textit{\small{expanded}} & \textbf{15.08} & 20.15 & 28.28 & 13.30 & 41.65 & 18.74 & 22.87 \\
        GLA \textit{\small{MoM}} & 9.90 & \textbf{21.65} & \textbf{29.36} & \textbf{14.16} & \textbf{45.20} & \textbf{20.89} & \textbf{23.53} \\
        \midrule
        Gated DeltaNet \textit{\small{expanded}} & 18.26 & 24.27 & \textbf{30.03} & \textbf{17.74} & \textbf{48.34} & 19.26 & 26.32 \\
        Gated DeltaNet \textit{\small{MoM}} & \textbf{28.07} & \textbf{29.15} & 29.73 & 15.24 & 45.50 & \textbf{20.41} & \textbf{28.02} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Comparison Between Mixed Memory and Single Memory.} We constructed MoM models using different memory update mechanisms. Separate memory segments yielded better performance compared to simply increasing the memory capacity of a single memory.}
    \vspace{-2mm}
    \label{table:vssingle}
\end{table*}

\subsubsection{Mixed Memory vs. Single Memory}
To validate the effectiveness of our mixed memory mechanism, we compare our MoM model with mixed memories to a baseline model that uses an expanded single memory with the same activated memory capacity. We adopt the same memory update method as existing linear models and extend it within our MoM framework. For comparison, we adjust the dimensions of projected $\bm k$ and $\bm v$ in the original model to match the total size of all activated memories in the MoM model. Note that while our MoM model activates multiple memories during retrieval, the mixing process effectively consolidates these into a memory capacity equivalent to a single memory. In contrast, the expanded single memory approach increases the entire retrieval capacity to encompass the size of multiple activated memories. We evaluate its performance on the recall-intensive tasks in Table~\ref{table:vssingle}.

The experimental results demonstrated that using multiple mixed memories leads to a greater improvement than simply expanding the capacity of a single memory. This confirms that mixed memory can effectively reduce interference from different inputs. Assigning inputs specifically to different memories, combined with the use of a forget gate, proves to be a more effective approach for reducing interference than relying solely on a forget gate.

\subsubsection{Efficiency}
We compare the inference speed and memory usage of MoM and Transformer++ with flash attention in Fig~\ref{fig:efficiency}. Our analysis demonstrates that MoM exhibits linear complexity, showcasing significant advantages over the Transformer model when handling long sequences. Specifically, MoM's efficient memory update mechanisms allow it to process longer inputs with reduced computational overhead, positioning it as a more scalable solution for large-scale natural language processing tasks.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figs/efficiency_2.pdf}
    \caption{\textbf{Efficiency of MoM.} We demonstrate the inference time and GPU memory consumption required to generate 1K tokens at specific sequence lengths.}
    \label{fig:efficiency}
\end{figure}

\subsubsection{Training Loss Comparison}
To further assess the learning efficiency of MoM, we compared the training loss curves of MoM with those of other baseline models. As depicted in Figure~\ref{fig:loss}, MoM consistently maintains the lowest loss throughout the entire training phase. Even as training nears convergence, MoM continues to exhibit a clear advantage over other methods.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figs/loss.png}
    \caption{\textbf{Training Loss.} Loss curves for training 340M models on 15B tokens with a fixed random seed of 42.}
    \label{fig:loss}
\end{figure}

\subsubsection{Ablation}
To evaluate the influence of memory hyperparameters in our MoM model, we conducted ablation studies using a 340 million parameter model trained on 15 billion tokens. Our investigation primarily focused on the impact of the number of memories, the number of activations, and the use of shared memory on model performance. The results of these experiments are presented in Table~\ref{table:ablation}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
         & Acc $\uparrow$ \\
        \midrule
        \multicolumn{2}{l}{Number of Memories and Activations} \\
        \multicolumn{2}{l}{\footnotesize (all cases include shared memory by default)} \\
        \midrule
        \begin{tabular}{cc} Memories & Activated \end{tabular} & \\
        \midrule
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.2cm}} 2 & 1 \end{tabular} & 25.05 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.2cm}} 3 & 2 \end{tabular} & 27.31 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.2cm}} 4 & 2 \end{tabular} & \textbf{27.59} \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.2cm}} 8 & 2 \end{tabular} & 25.56 \\
        \midrule
        \multicolumn{2}{l}{Shared Memory} \\
        \midrule
        w/ shared memory & \textbf{27.59} \\
        w/o shared memory & 26.06 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation Study.} We performed ablation studies on the number of memories and the use of shared memory. The table presents the average results across all recall-intensive tasks.}
    \label{table:ablation}
    \vspace{-6mm}
\end{table}

\section{Related Work}

\subsection*{Linear Recurrent Models}
Linear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests \cite{qin2023scaling}. The advancement of SSMs began with the pioneering work on S4 \cite{gu2022efficientlymodelinglongsequences}, which was later optimized through a diagonalized version \cite{NEURIPS2022_9156b0f6}. Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly because they rely solely on data-independent processes. As research progressed, constant forgetting gates were introduced, helping to alleviate some interference issues by uniformly managing memory decay \cite{sun2023retentive, gu2024mambalineartimesequencemodeling}. The next breakthrough involved data-dependent forget gates. These allowed models to dynamically adjust memory updates based on the input data, significantly enhancing performance across various tasks \cite{qin2024various, qin2024hgrn2, yang2023gated, zhang2024gated, yang2024gated,qin2024unlocking}. Sequence parallelism techniques are well adapted on linear recurrent models \cite{sun2024linear,sun2025lasp} for efficient long context training. There have also been recent advancements in scaling law and test-time regression optimization \cite{shen2024scaling,sun2024learning, behrouz2024titans}.

Building on these advancements, our MoM model incorporates data-dependent mechanisms that selectively update memory. By efficiently managing interference through tailored memory updates and leveraging increased memory capacity, MoM represents a further evolution, improving model expressiveness and performance.

\subsection*{Mixture-of-Experts}
Mixture-of-Experts (MoE) is a technique designed to enhance the capacity of deep neural networks while maintaining computational efficiency \cite{fedus2022switch, rajbhandari2020zero, lepikhin2020gshard, tang2023ms, zhu2024llama, qu2024llama}. MoE achieves this by activating a subset of parameters, known as "experts", for each input, which reduces the computational costs. Shazeer first integrated MoE into LSTM layers \cite{shazeer2017outrageously}. The Switch Transformer \cite{fedus2022switch} refined this approach by simplifying the gating mechanism to select only one expert per input. Gshard \cite{lepikhin2020gshard} further advanced this by using a top-2 expert routing strategy to improve performance. Recent MoE models, such as Deepseek-MoE \cite{dai2024deepseekmoe}, introduce shared experts to capture and consolidate common knowledge across different contexts, while designing fine-grained experts to increase combinatorial flexibility.

\section{Conclusion}
In this paper, we propose Mixture-of-Memories (MoM), a novel architecture that enhances memory capacity and eliminates memory interference, inspired by the brain’s mechanisms for long-term memory retention. By leveraging multiple independent memory states, MoM significantly improves performance on recall-intensive tasks while maintaining the efficiency advantages of linear-time training and constant-memory inference. Instead of simply discarding tokens as done in gating mechanisms, our memory separation paradigm provides a more effective way to preserve sequence information. Our experimental results demonstrate that MoM outperforms existing linear sequence modeling methods, particularly on tasks requiring strong recall, and achieves performance comparable to Transformer models. This makes MoM a promising approach for applications need strong efficiency and recall-intensive performance, paving the way for sparse solutions in sequence modeling.


% \section{Limitations}

% % 去掉 Future work
% The MoM model expands single memory into multiple memories, allowing for diverse interactions between memories and linear models. However, our current approach employs straightforward weighted summation-based mixing techniques, which may not fully optimize these interactions. Although MoM shows significant advantages over Transformers on long sequences, on short sequences, the overheads from token distribution, combination, and memory updates leave room for optimization.

% While MoM outperforms all other linear attention models and has significantly narrowed the gap with Transformers on long-text and recall-intensive tasks, there remains a slight disparity. The model still faces challenges in fully matching the performance of Transformers, particularly in tasks that heavily rely on long-term dependencies and precise recall of historical information.

% \section{Acknowledgments}

\bibliography{acl_latex}

\newpage
\appendix

\section{Comparison between MoM and MoE}
\label{app:diff-moe}
While our approach to implementing the Mixture-of-Memories (MoM) draws inspiration from the Mixture-of-Experts (MoE) framework, there are significant differences that distinguish our method from traditional MoE implementations.
\begin{itemize}
    \item \textbf{Purpose}: The MoE was introduced to scale up the number of parameters without significantly increasing computational resources. It address the limitations of dense models in scaling both parameters and computational demands through sparse activation. However, MoM is designed to expand the memory capacity of linear attention models while preserving their linear time complexity. By sparsely activating memories and using weighed summation to create a mixed memory, MoM effectively address the challenge of forgetting historical information in linear attention. Moreover, by separating the memory into distinct states, MoM reduces interference between different pieces of information.
    \item \textbf{Structure}: In conventional MoE, each expert is a separate neural network within the feedforward network (FFN) layer \cite{qwen_moe}. In contrast, in MoM, each memory is an RNN state with unique key-value projection weights to generate different key-value pairs. MoE operates during the channel mixing phase, where each token is processed independently by selected experts. On the other hand, MoM functions during the token mixing phase, where each memory processes different segments of the sequence, preserving inter-token relationships.
\end{itemize}

\section{Dataset and Benchmark}
We pretrained model on SlimPajama dataset. For 340M models, we train on a sample of 15B tokens. For 1.3B models, we train on a sample of 100B tokens.
\begin{itemize}
    \item \textbf{SlimPajama} \cite{cerebras2023slimpajama} is a high-quality, optimized subset of the RedPajama dataset, designed for large-scale language model training. It includes diverse text sources such as Common Crawl, Wikipedia, books, and GitHub code, with a primary focus on English. The dataset is cleaned, deduplicated, and optimized for efficiency and performance.
\end{itemize}
For the benchmark, we tested on these tasks:
\begin{itemize}
    \item \textbf{WikiText} \cite{merity2016pointer}: A dataset consisting of high-quality Wikipedia articles, primarily in English, designed for language modeling tasks with 62 test samples. The text covers a wide range of topics, including history, science, and culture, and is authored by Wikipedia contributors, who come from diverse demographic backgrounds.
    \item \textbf{LAMBADA} \cite{paperno2016lambada}: An English-language dataset for evaluating contextual understanding in language models with 5153 test samples. It consists of narrative texts sourced from books, requiring models to predict the final word of a passage. The data reflects a mix of literary styles and author demographics.
    \item \textbf{ARC-Easy \& ARC-Challenge} \cite{Clark2018ThinkYH}: A set of multiple-choice science questions in English, sourced from standardized exams and educational materials with 2376 and 1172 test samples. The dataset represents the domain of elementary and high school science, with questions authored by educators and test designers. ARC-Easy includes straightforward questions, while ARC-Challenge contains more difficult ones that require advanced reasoning.
    \item \textbf{HellaSwag} \cite{zellers2019hellaswag}: An English-language dataset designed for commonsense reasoning, where models must choose the most plausible continuation of a sentence. The text is derived from activity descriptions (e.g., WikiHow), covering everyday scenarios. The dataset was constructed adversarially to be challenging for language models. It has 10003 test samples.
    \item \textbf{PiQA} \cite{Bisk2020}: A dataset focused on physical commonsense reasoning in English with 3084 test samples. The text consists of everyday tasks and scenarios, requiring models to determine the most practical way to perform an action. The data is sourced from crowdsourced descriptions, reflecting a broad range of common human experiences.
    \item \textbf{WinoGrande} \cite{sakaguchi2019winogrande}: A large-scale English dataset for commonsense reasoning, based on the Winograd Schema Challenge with 1267 test samples. It tests pronoun resolution in ambiguous contexts, with sentences sourced and refined through crowdsourcing. The dataset aims to reduce annotation biases by diversifying sentence structures and topics.
    \item \textbf{FDA} \cite{arora2024simple, arora2023language}: An English-language dataset includes 100 randomly sampled PDF files of FDA 510(k) pre-market notification submissions, each with 16 manually annotated attributes like device classification and predicate device codes. It has 1102 test samples.
    \item \textbf{SWDE} \cite{arora2024simple, arora2023language, lockard-etal-2019-openceres}: An English dataset dataset is used for evaluating information extraction systems. It comprises data from 8 movie websites (e.g., IMDB, Rotten Tomatoes) and 5 university websites (e.g., US News), with each site containing 1063 to 2000 annotated web pages and 1111 test samples.
    \item \textbf{SQuAD} \cite{rajpurkar2018know}: A reading comprehension dataset with English questions based on Wikipedia articles with 2984 test samples. Answers are text segments from the articles or the questions can be unanswerable. SQuAD2.0 adds over 50,000 unanswerable questions to the original 100,000 from SQuAD1.1, requiring systems to both answer questions and identify when no answer is supported.
    \item \textbf{NQ} \cite{kwiatkowski2019natural}: A large-scale English dataset for open-domain question answering, where questions are sourced from real Google search queries. Answers are extracted from Wikipedia articles, reflecting general knowledge across various domains. It has 3157 test samples
    \item \textbf{TriviaQA} \cite{joshi2017triviaqa}: An English-language question-answering dataset consisting of trivia-style questions with 1688 test samples. The dataset includes both human-generated questions and web-sourced evidence, covering a broad range of topics such as history, science, and entertainment.
    \item \textbf{DROP} \cite{dua2019drop}: A reading comprehension benchmark in English that requires discrete reasoning, such as arithmetic and logical operations, over passages. The dataset is sourced from Wikipedia and challenges models to go beyond simple span extraction by performing multi-step reasoning. It has 2087 test samples.
    \item \textbf{LongBench} \cite{bai2024longbenchbilingualmultitaskbenchmark} is a bilingual (Chinese and English) benchmark for evaluating long-context understanding in large language models. It covers 21 tasks across six categories, including QA, summarization, and code completion with 4750 test samples.
\end{itemize}

All datasets used in this work are publicly available and have been released by their original creators, who are responsible for ensuring privacy protection. These datasets are used in accordance with their respective licenses and intended purposes. No modifications or derivative datasets have been created.

\section{Experiments Details}
Our experiments were conducted using 32 NVIDIA A800 GPUs. Training the 340M parameter model required approximately 10 hours, while the 1.3B parameter model took around 6 days. All models were trained and evaluated using a fixed random seed of 42 to ensure reproducibility. The reported results are based on a single run, without aggregation over multiple random seeds.

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{lccccccccc}
        \toprule
        \textbf{Model} & \textbf{SQA} & \textbf{MQA} & \textbf{Sum} & \textbf{FS} & \textbf{Syn} & \textbf{Code} & \textbf{Zh-Avg} & \textbf{En-Avg} & \textbf{Avg.} \\
        \midrule
        RetNet & \textbf{9.23} & \textbf{7.23} & 6.3 & 15.76 & \textbf{2.64} & 40.52 & 15.44 & 13.5 & 13.61 \\
        HGRN2 & 7.38 & 6.02 & 6.51 & 15.5 & 2.61 & 40.11 & 14.28 & 13.12 & 13.02 \\
        GSA & 8.21 & 6.63 & \textbf{7.75} & 20.29 & 1.92 & 42.83 & 15.06 & 15.2 & 14.61 \\
        Gated DeltaNet & 8.52 & 6.61 & 7.14 & 18 & 2.1 & 41.52 & 14.19 & 14.63 & 13.98 \\
        MoM & 8.14 & 7.11 & 6.89 & \textbf{21.26} & 2.63 & \textbf{47.79} & \textbf{17.33} & \textbf{15.71} & \textbf{15.64} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Complete Results of LongBench.} (SQA: Single-doc QA, MQA: Multi-doc QA, Sum: Summarization, FS: Few-shot learning, Syn: Synthetic)}
    \label{table:complete-longbench}
\end{table*}

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{lccccccc}
    \toprule
         & \textbf{FDA} & \textbf{SWDE} & \textbf{SQUAD} & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Drop} & \textbf{Avg.} \\
        \midrule
        \multicolumn{2}{l}{Number of Memories} \\
        \midrule
        \begin{tabular}{cc} Memories & Activated \end{tabular} & \\
        \midrule
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 2 & 1 \end{tabular} & 20.53 & 23.15 & 29.56 & 13.72 & 43.96 & 19.36 & 25.05 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 3 & 2 \end{tabular} & 23.71 & \textbf{29.52} & 28.69 & 15.55 & 46.09 & 20.32 & 27.31 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 4 & 2 \end{tabular} & \textbf{30.79} & 26.05 & 29.63 & 13.84 & 44.79 & \textbf{20.41} & \textbf{27.59} \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 8 & 2 \end{tabular} & 19.71 & 24.27 & \textbf{30.13} & \textbf{15.68} & \textbf{46.15} & 17.39 & 25.56 \\
        \midrule
        \multicolumn{2}{l}{Shared Memory} \\
        \midrule
        w/ shared memory & \textbf{30.79} & 26.05 & \textbf{29.63} & 13.84 & \textbf{44.79} & \textbf{20.41} & \textbf{27.59} \\
        w/o shared memory & 25.61 & \textbf{27.09} & 28.35 & \textbf{14.32} & 41.94 & 19.02 & 26.06 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Complete Ablation Study Results.} The complete results of the ablation studies on recall-intensive tasks.}
    \label{table:full_ablation_recall}
\end{table*}

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{lccccccccc}
    \toprule
         & \begin{tabular}[c]{@{}c@{}} \textbf{Wiki.} \\ ppl$\downarrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Lamb.} \\ ppl$\downarrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{ARC-e} \\ acc$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{ARC-c} \\ $\mathrm{acc_n}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Hella.} \\ $\mathrm{acc_n}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Lamb.} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{PIQA} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Wino.} \\ $\mathrm{acc}$$\uparrow$ \end{tabular} & \textbf{Avg.} \\
        \midrule
        \multicolumn{2}{l}{Number of Memories} \\
        \midrule
        \begin{tabular}{cc} Memories & Activated \end{tabular} & \\
        \midrule
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 2 & 1 \end{tabular} & 26.84 & 66.26 & \textbf{46.25} & 23.21 & 35.54 & 25.40 & 64.69 & 51.30 & 41.07 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 3 & 2 \end{tabular} & 25.86 & 52.78 & 45.62 & 23.46 & \textbf{36.25} & \textbf{28.84} & 65.13 & \textbf{53.67} & \textbf{42.16} \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 4 & 2 \end{tabular} & \textbf{26.00} & \textbf{51.25} & 46.13 & 24.15 & 35.91 & 28.26 & \textbf{65.61} & 52.57 & 42.10 \\
        \begin{tabular}{>{\centering}p{1.3cm}>{\centering}p{1.3cm}} 8 & 2 \end{tabular} & 26.40 & 52.30 & 45.66 & \textbf{24.74} & 35.93 & 28.39 & 64.91 & 49.49 & 41.52 \\
        \midrule
        \multicolumn{2}{l}{Shared Memory} \\
        \midrule
        w/ shared memory & \textbf{26.00} &\textbf{ 51.25} & \textbf{46.13} & \textbf{24.15} & \textbf{35.91} & \textbf{28.26} & \textbf{65.61} & \textbf{52.57} & \textbf{42.10} \\
        w/o shared memory & 27.74 & 85.22 & 45.33 & \textbf{24.15} & 34.91 & 23.52 & 64.09 & 50.28 & 40.38 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Complete Ablation Study Results.} The complete results of the ablation studies on recall-intensive tasks.}
    \label{table:full_ablation_common}
\end{table*}

\end{document}
