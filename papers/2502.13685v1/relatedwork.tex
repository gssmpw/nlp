\section{Related Work}
\subsection*{Linear Recurrent Models}
Linear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests \cite{qin2023scaling}. The advancement of SSMs began with the pioneering work on S4 \cite{gu2022efficientlymodelinglongsequences}, which was later optimized through a diagonalized version \cite{NEURIPS2022_9156b0f6}. Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly because they rely solely on data-independent processes. As research progressed, constant forgetting gates were introduced, helping to alleviate some interference issues by uniformly managing memory decay \cite{sun2023retentive, gu2024mambalineartimesequencemodeling}. The next breakthrough involved data-dependent forget gates. These allowed models to dynamically adjust memory updates based on the input data, significantly enhancing performance across various tasks \cite{qin2024various, qin2024hgrn2, yang2023gated, zhang2024gated, yang2024gated,qin2024unlocking}. Sequence parallelism techniques are well adapted on linear recurrent models \cite{sun2024linear,sun2025lasp} for efficient long context training. There have also been recent advancements in scaling law and test-time regression optimization \cite{shen2024scaling,sun2024learning, behrouz2024titans}.

Building on these advancements, our MoM model incorporates data-dependent mechanisms that selectively update memory. By efficiently managing interference through tailored memory updates and leveraging increased memory capacity, MoM represents a further evolution, improving model expressiveness and performance.

\subsection*{Mixture-of-Experts}
Mixture-of-Experts (MoE) is a technique designed to enhance the capacity of deep neural networks while maintaining computational efficiency \cite{fedus2022switch, rajbhandari2020zero, lepikhin2020gshard, tang2023ms, zhu2024llama, qu2024llama}. MoE achieves this by activating a subset of parameters, known as "experts", for each input, which reduces the computational costs. Shazeer first integrated MoE into LSTM layers \cite{shazeer2017outrageously}. The Switch Transformer \cite{fedus2022switch} refined this approach by simplifying the gating mechanism to select only one expert per input. Gshard \cite{lepikhin2020gshard} further advanced this by using a top-2 expert routing strategy to improve performance. Recent MoE models, such as Deepseek-MoE \cite{dai2024deepseekmoe}, introduce shared experts to capture and consolidate common knowledge across different contexts, while designing fine-grained experts to increase combinatorial flexibility.