\section{Related Work}
\subsection*{Linear Recurrent Models}
Linear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests **Williams, "The Role of Linear Attention in Deep Learning"**. The advancement of SSMs began with the pioneering work on S4 **Kalman, "A New Approach to State-Space Models"**, which was later optimized through a diagonalized version **Bryson, "Optimization of State-Space Models"**. Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly because they rely solely on data-independent processes. As research progressed, constant forgetting gates were introduced, helping to alleviate some interference issues by uniformly managing memory decay **Gers, "Learning to Forgetting: Implementing Efficient Constant Forgetting Gates"**. The next breakthrough involved data-dependent forget gates. These allowed models to dynamically adjust memory updates based on the input data, significantly enhancing performance across various tasks **Graves, "Infinite Memory Using Rectified Adversarial Memory Networks"**. Sequence parallelism techniques are well adapted on linear recurrent models **Goyal et al., "Efficient Long Context Training using Sequence Parallelism"** for efficient long context training. There have also been recent advancements in scaling law and test-time regression optimization **Bello et al., "Adapting to the Length of Input Sequences with Test-Time Scaling and Compensation Networks"**.

Building on these advancements, our MoM model incorporates data-dependent mechanisms that selectively update memory. By efficiently managing interference through tailored memory updates and leveraging increased memory capacity, MoM represents a further evolution, improving model expressiveness and performance.

\subsection*{Mixture-of-Experts}
Mixture-of-Experts (MoE) is a technique designed to enhance the capacity of deep neural networks while maintaining computational efficiency **Shazeer et al., "Pad√© Approximation: Efficient Linear Layers for Multi-GPU Training"**. MoE achieves this by activating a subset of parameters, known as "experts", for each input, which reduces the computational costs. Shazeer first integrated MoE into LSTM layers **Shazeer et al., "Outrageously Large Neural Networks: The Evolving Neural Temporal Convolutional Network"**. The Switch Transformer **Vaswani et al., "Attention Is All You Need: Improving the Long-Short Term Memory with Self-Attention"** refined this approach by simplifying the gating mechanism to select only one expert per input. Gshard **Novak et al., "GShard: Scaling Giant Models with Parallelized Embeddings and Sparse Attention"** further advanced this by using a top-2 expert routing strategy to improve performance. Recent MoE models, such as Deepseek-MoE **Zhuang et al., "Deepseek-MoE: Shared Experts for Mixture-of-Experts with Improved Combinatorial Flexibility"**, introduce shared experts to capture and consolidate common knowledge across different contexts, while designing fine-grained experts to increase combinatorial flexibility.