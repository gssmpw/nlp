\section{Related Work}
\subsection*{Linear Recurrent Models}
Linear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests ____. The advancement of SSMs began with the pioneering work on S4 ____, which was later optimized through a diagonalized version ____. Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly because they rely solely on data-independent processes. As research progressed, constant forgetting gates were introduced, helping to alleviate some interference issues by uniformly managing memory decay ____. The next breakthrough involved data-dependent forget gates. These allowed models to dynamically adjust memory updates based on the input data, significantly enhancing performance across various tasks ____. Sequence parallelism techniques are well adapted on linear recurrent models ____ for efficient long context training. There have also been recent advancements in scaling law and test-time regression optimization ____.

Building on these advancements, our MoM model incorporates data-dependent mechanisms that selectively update memory. By efficiently managing interference through tailored memory updates and leveraging increased memory capacity, MoM represents a further evolution, improving model expressiveness and performance.

\subsection*{Mixture-of-Experts}
Mixture-of-Experts (MoE) is a technique designed to enhance the capacity of deep neural networks while maintaining computational efficiency ____. MoE achieves this by activating a subset of parameters, known as "experts", for each input, which reduces the computational costs. Shazeer first integrated MoE into LSTM layers ____. The Switch Transformer ____ refined this approach by simplifying the gating mechanism to select only one expert per input. Gshard ____ further advanced this by using a top-2 expert routing strategy to improve performance. Recent MoE models, such as Deepseek-MoE ____, introduce shared experts to capture and consolidate common knowledge across different contexts, while designing fine-grained experts to increase combinatorial flexibility.