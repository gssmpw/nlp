
Attention mechanisms have made significant contributions to the field of artificial intelligence, advancing various modalities such as language, vision, audio, video, graphs, and even time series \cite{achiam2023gpt,team2023internlm}. The Transformer \cite{vaswani2017attention}, known for its ability to capture long-range dependencies, has become a foundational architecture in this space. However, traditional Transformers encounter computational challenges due to their quadratic time complexity, $O(n^2)$, with respect to sequence length $n$, making it difficult to scale to long sequences. To overcome this limitation, several linear sequence modeling methods have been proposed, including linear attention \cite{katharopoulos2020transformers,qin2023transnormerllm, li2025minimax}, state space modeling \cite{gu2024mambalineartimesequencemodeling, dao2024transformers}, and linear RNNs \cite{peng2024eagle, qin2024hgrn2}, which offer $O(n)$ training complexity and $O(1)$ inference complexity. These approaches often reduce the input sequence to a fixed-size hidden space, collapsing the information into a single "memory state". While these methods enhance efficiency, they face two main challenges: \textbf{limited memory capacity} and \textbf{memory interference}. When new information overwrites the single fixed-size memory state, previously stored representations may degrade, which negatively impacts its long-term memory performance on recall-intensive tasks.


% "Memory interference" is a well-known issue in cognitive neuroscience. As Stanford neuroscientist David Eagleman articulates: "\textit{The enemy of memory isn't time; it's other memories.}". Overlapping neural representations in the human brain can lead to retrieval conflicts and integration failures \cite{chanales2019interference}. Thus, finding a balance between efficient memory utilization and the preservation of individual memories remains an open challenge in both artificial and biological systems.

We argue that the strong performance of Transformer models on recall-intensive tasks arises from their ability to avoid "memory interference" by maintaining independent key-value caches for each token, thus offering virtually unlimited memory capacity. In contrast, linear sequence modeling relies on extreme compression, consolidating all the input information into a single fixed-size memory state \cite{katharopoulos2020transformers,dao2024transformers}. This approach results in limited memory capacity and inherently leads to memory interference issues.


Interestingly, the human brain has developed mechanisms that enable large memory capacity while reducing memory interference. Neuroscience studies show that in the hippocampus, theta oscillations (4$\sim$8 Hz) and gamma oscillations (30$\sim$100 Hz) work together to support a neural coding mechanism for multi-item memory \cite{buzsaki2002theta, lisman2013theta}. Specifically, each theta cycle is subdivided into multiple gamma subcycles, and within each gamma subcycle, a distinct group of neurons is activated following the "E\%-max" mechanism \cite{de2009second}. This sequential activation temporally separates different memory items, thus preventing interference.


Inspired by these biological insights, we propose a new architecture called \textbf{Mixture-of-Memories (MoM)}, which aims to strike a balance between the explicit token representations in Transformers and the extreme compression found in earlier linear sequence modeling methods. MoM employs multiple independent memory states, with a router network that directs input tokens to specific memory states. The input sequence is divided into a predefined number of subsequences (phase-specific neural assemblies), which are processed in parallel and fed into the corresponding memory projections (dentate microcircuits) to generate key-value pairs. As the linear sequence modeling layer processes each subsequence using an RNN-like update mechanism, it produces multiple memory states that capture different aspects of the input sequence. The final output is computed as a weighted sum of these memories, which we refer to as the mixture-of-memories. This approach expands memory capacity and eliminates memory interference, enabling MoM to significantly outperform existing linear sequence models that rely on a single fixed-size memory state.


Our contributions can be summarized as follows:

\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item We present MoM, a neuroscience-inspired architecture that incorporates multiple independent memory states, significantly enhancing memory capacity and eliminating memory interference, while retaining the efficiency benefits of linear-time training and constant-memory inference.
    \item Distinct with existing gating mechanisms, MoM is a new paradigm to reduce memory interference by separating the memory states. The overall design of MoM is compatible with existing linear sequence modeling methods, making it a straightforward and effective approach to boost task performance.
    \item Through empirical evaluation, we show that MoM outperforms state-of-the-art linear sequence modeling techniques across a variety of language tasks, particularly on recall-intensive tasks. MoM even achieves performance on par with Transformer models, a feat that current linear sequence modeling methods struggle to match.
\end{itemize}


% 引入类脑
% https://chatgpt.com/share/67a321a3-0eb4-8002-9cd8-ff41b0b713a0
% The human brain's memory system offers profound insights for overcoming computational bottlenecks in artificial intelligence. Neuroscience research reveals that in the hippocampus, theta oscillations (4–8 Hz) and gamma oscillations (30–100 Hz) work together to create a neural coding mechanism for multi-item memory.\cite{buzsaki2002theta,lisman2013theta}. Specifically, each theta cycle is divided into several gamma subcycles, and within each subcycle, a distinct neuronal ensemble is activated. This sequential activation of different ensembles across gamma subcycles ensures that multiple memory items are temporally segregated, preventing interference between them. Moreover, through an “E\%-max” mechanism, the most excitable neurons within a gamma cycle are preferentially activated, which helps to select and enhance the most salient information \cite{lisman2013theta}. This biological mechanism allows multiple memories to be stored separately and elegantly solves the \textbf{memory interfernce} problem described by Stanford neuroscientist David Eagleman: \textit{"The enemy of memory isn't time; it's other memories"} - a challenge that paradoxically resurfaces in modern linear attention models.

% Transformer把所有kv全部隔离开，Linear把所有kv聚合成一份记忆，折中的多份记忆更为合适
% Transformer architectures \cite{vaswani2017attention} initially circumvented this interference through brute-force key-value caching for each separate token, a strategy akin to dedicating unique neural ensembles for each memory trace. However, this comes with a quadratic complexity of $O(n^2)$, posing significant computational and memory challenges when handling long sequences. Conversely, linear attention variants \cite{katharopoulos2020transformers, choromanski2020rethinking} adopt extreme compression, collapsing sequences into a single fixed-size vector-valued or matrix-valued memory, also known as the recurrent state, they inherit the biological limitation of single-memory overwriting. Hippocampal recordings show that shared neural substrates critically underlie diverse memory errors, as overlapping representations drive competitive retrieval conflicts and integration failures \cite{chanales2019interference}. In linear attention, overlapping updates degrade memory specificity, precisely illustrating the interference mechanism captured by David Eagleman's aphorism.

% Current improvements to linear attention have adopted monolithic memory expansion \cite{arora2023zoology, qin2024hgrn2, peng2024eagle} and forget gates \cite{orvieto2023resurrecting, de2024griffin, yang2023gated, zhang2024gated} which mirror the 
% brain's basic memory maintenance strategies, yet lack the multi-item memory organization revealed by in vivo neural recordings \cite{lisman2013theta, virga2024activity}.

% While Transformers and existing linear models appear to represent two extremes, we propose a biologically inspired compromise: Mixture of Memories (MoM) which is a neural architecture that implements theta-gamma oscillations-inspired memory compartmentalization in linear attention. MoM divides the memory into multiple independent segments and separate the sequence into a specific number of subsequences (phase-specific neural assemblies) using a router. These are fed in parallel into the corresponding memory projections (dentate microcircuits), which generate the respective key-value pairs. When the linear attention layer processes each subsequence, it encounters different inputs. Due to its RNN-like update mechanism, it generates multiple memories that focus on different aspects of the input information. The final output is based on a weighted sum over these multiple memories which we refer to as the mixture of memories. As a result, this biologically-plausible design achieves: 1. \textbf{Interference suppression} by maintaining isolated memory updates across segments. 2. \textbf{Memory integration} through input-dependent activation that combines information from different memory compartments.


% 初版
% Transformer models \cite{vaswani2017attention} have revolutionized the field of natural language processing, offering unprecedented performance in many sequence-modeling tasks. However, the self-attention mechanism in Transformers has a quadratic complexity of $O(n^2)$, posing significant computational and memory challenges when handling long sequences.  To address these issues, linear attention emerged as a solution that replaces the exponential similarity function with a simplified operation such as matrix multiplication \cite{katharopoulos2020transformers, choromanski2020rethinking}. By redesigning the attention computation process, linear attention reduces the complexity to $O(n)$, effectively extending the applicability of Transformers to longer sequences.

% The gated recurrent form of linear attention which can be reframed as RNNs during inference has garnered research interest \cite{sun2023retentive,yang2023gated,qin2024hgrn2,peng2024eagle}. From a memory perspective, these linear sequence models compress the entire sequence data into a vector-valued or matrix-valued memory, also known as the hidden state. In these models, the size of the hidden state is fixed and is continuously updated based on the input. This leads to a common issue in RNN networks: as the hidden state is updated repeatedly, information can be overwritten or forgotten, making it challenging for the model to capture long-term dependencies in the sequence. Transformer models learn long-term dependencies in sequences by storing all historical information using an expanding key-value cache without compression. This allows them to perform better on tasks that require in-context recall and handling long texts. Can we enable linear attention to capture long-term dependencies in sequences? If so, what are the current limitations of this approach?

% Linear attention cannot maintain key-value memory for every token as Transformers do, due to its limited memory capacity. Research suggests that increasing RNN state size, or memory capacity, within linear attention can improve model performance \cite{arora2023zoology, qin2024hgrn2, peng2024eagle}. However, previous methods have focused on expanding a single memory capacity, where each token updates this memory, leading to interference from irrelevant information. To address this, many approaches have incorporated forget gate mechanisms to eliminate the interference of irrelevant information\cite{orvieto2023resurrecting, de2024griffin, yang2023gated, zhang2024gated}.

% Sequences often contain diverse information, and some inputs, while not closely related to the current memory, are equally important. Simply discarding these tokens with a forget gate or using them to update memory may not be the optimal solution. David Eagleman, a renowned neuroscientist at Stanford University, once remarked, \textit{"The enemy of memory isn't time; it's other memories."} Inspired by this insight, we identified a direction to address the aforementioned issues: leveraging multiple memories. By allowing inputs to activate specific memory updates, we can minimize interference between memories. This concept led to the development of our method: Mixture of Memories.

% To avoid interference between different types of information, we divide the memory into multiple independent segments and separate the sequence into a specific number of subsequences using a router, with each subsequence representing a particular aspect of information. These are fed in parallel into the corresponding experts which generate the respective key-value pairs. When the linear attention layer processes each subsequence, it encounters different inputs. Due to its RNN-like update mechanism, it generates multiple memories that focus on different aspects of the information. The final output is a weighted sum over these multiple memories which we refer to as the mixture of memories. As a result, our model can more effectively focus on key information, enhancing its overall understanding and processing capabilities for complex inputs. This approach fundamentally differs from traditional Mixture of Experts (MoE) \cite{shazeer2017outrageously, lepikhin2020gshard, fedus2022switch, rajbhandari2022deepspeed}, which will be discussed in detail in Section \ref{sec:differ_MoE}.

% Our contributions can be summarized as follows:

% \begin{itemize}
%     \item We propose a Mixture of Memories (MoM), a biologically inspired architecture that introduces memory compartmentalization into linear attention. By maintaining multiple parallel memory states through phase-like segmentation, our model achieves interference suppression while preserving the original $O(n)$ time complexity.
%     \item We develop a novel memory update strategy that enables independent information preservation across compartments. This biologically grounded approach complements existing techniques like forget gates, establishing a new paradigm for interference reduction in linear attention that significantly improves performance on long-context tasks.
%     \item We demonstrate MoM's superiority over conventional linear attention variants through comprehensive evaluation. The model achieves state-of-the-art results on interference-sensitive recall tasks, validating the effectiveness of neural-inspired memory organization. MoM is the first pure linear model to achieve performance comparable to transformer models on recall tasks.
% \end{itemize}