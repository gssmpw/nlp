@book{ROM,
author = {Quarteroni, Alfio and Rozza, Gianluigi},
year = {2014},
month = {01},
pages = {},
title = {Reduced Order Methods for Modeling and Computational Reduction},
isbn = {978-3-319-02089-1},
publisher = {Springer International Publishing},
doi = {10.1007/978-3-319-02090-7}
}

@article{aiforscience,
author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Zitnik, Marinka},
year = {2023},
month = {08},
pages = {47-60},
title = {Scientific discovery in the age of artificial intelligence},
volume = {620},
journal = {Nature},
doi = {10.1038/s41586-023-06221-2}
}
@article{Butcher1996,
  title = {A history of Runge-Kutta methods},
  volume = {20},
  ISSN = {0168-9274},
  url = {http://dx.doi.org/10.1016/0168-9274(95)00108-5},
  DOI = {10.1016/0168-9274(95)00108-5},
  number = {3},
  journal = {Applied Numerical Mathematics},
  publisher = {Elsevier BV},
  author = {Butcher,  J.C.},
  year = {1996},
  month = mar,
  pages = {247–260}
}
@book{numerical_matemathics, author = {Quarteroni, Alfio and Sacco, Riccardo and Saleri, Fausto}, title = {Numerical Mathematics (Texts in Applied Mathematics)}, year = {2006}, isbn = {3540346589}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg} }

@article{UQ,
author = {Iman, Ronald and Helton, Jon},
year = {2006},
month = {05},
pages = {71 - 90},
title = {An Investigation Of Uncertainty And Sensitivity Analysis Techniques For Computer-Models},
volume = {8},
journal = {Risk Analysis},
doi = {10.1111/j.1539-6924.1988.tb01155.x}
}
@article{pinns,
  title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  volume = {378},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2018.10.045},
  DOI = {10.1016/j.jcp.2018.10.045},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Raissi,  M. and Perdikaris,  P. and Karniadakis,  G.E.},
  year = {2019},
  month = feb,
  pages = {686–707}
}

@article{kovachki2021neural,
  title={Neural operator: Learning maps between function spaces},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2108.08481},
  year={2021}
}
@article{DON,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}
@article{CNO,
  title={Are Neural Operators Really Neural Operators? Frame Theory Meets Operator Learning},
  author={Bartolucci, Francesca and de B{\'e}zenac, Emmanuel and Raoni{\'c}, Bogdan and Molinaro, Roberto and Mishra, Siddhartha and Alaifari, Rima},
  journal={arXiv preprint arXiv:2305.19913},
  year={2023}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{gradient_descent,
  title={Gradient descent based optimization algorithms for deep learning models training},
  author={Zhang, Jiawei},
  journal={arXiv preprint arXiv:1903.03614},
  year={2019}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}
@Book{GoodBengCour16,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}
@article{Franco2023,
  title = {Deep learning-based surrogate models for parametrized PDEs: Handling geometric variability through graph neural networks},
  volume = {33},
  ISSN = {1089-7682},
  url = {http://dx.doi.org/10.1063/5.0170101},
  DOI = {10.1063/5.0170101},
  number = {12},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  publisher = {AIP Publishing},
  author = {Franco,  Nicola Rares and Fresca,  Stefania and Tombari,  Filippo and Manzoni,  Andrea},
  year = {2023},
  month = dec 
}
@inproceedings{
yin2023continuous,
title={Continuous {PDE} Dynamics Forecasting with Implicit Neural Representations},
author={Yuan Yin and Matthieu Kirchmeyer and Jean-Yves Franceschi and Alain Rakotomamonjy and patrick gallinari},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=B73niNjbPs}
}
@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{knigge2024space,
  title={Space-Time Continuous PDE Forecasting using Equivariant Neural Fields},
  author={Knigge, David M and Wessels, David R and Valperga, Riccardo and Papa, Samuele and Sonke, Jan-Jakob and Gavves, Efstratios and Bekkers, Erik J},
  journal={arXiv preprint arXiv:2406.06660},
  year={2024}
}
@article{Regazzoni2024,
  title = {Learning the intrinsic dynamics of spatio-temporal processes through Latent Dynamics Networks},
  volume = {15},
  ISSN = {2041-1723},
  url = {http://dx.doi.org/10.1038/s41467-024-45323-x},
  DOI = {10.1038/s41467-024-45323-x},
  number = {1},
  journal = {Nature Communications},
  publisher = {Springer Science and Business Media LLC},
  author = {Regazzoni,  Francesco and Pagani,  Stefano and Salvador,  Matteo and Dede’,  Luca and Quarteroni,  Alfio},
  year = {2024},
  month = feb 
}
@article{Lu2022_afaircomparison,
  title = {A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data},
  volume = {393},
  ISSN = {0045-7825},
  url = {http://dx.doi.org/10.1016/j.cma.2022.114778},
  DOI = {10.1016/j.cma.2022.114778},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  publisher = {Elsevier BV},
  author = {Lu,  Lu and Meng,  Xuhui and Cai,  Shengze and Mao,  Zhiping and Goswami,  Somdatta and Zhang,  Zhongqiang and Karniadakis,  George Em},
  year = {2022},
  month = apr,
  pages = {114778}
}

@article{Geneva2022transformer,
  title = {Transformers for modeling physical systems},
  volume = {146},
  ISSN = {0893-6080},
  url = {http://dx.doi.org/10.1016/j.neunet.2021.11.022},
  DOI = {10.1016/j.neunet.2021.11.022},
  journal = {Neural Networks},
  publisher = {Elsevier BV},
  author = {Geneva,  Nicholas and Zabaras,  Nicholas},
  year = {2022},
  month = feb,
  pages = {272–289}
}
@misc{CNNarithme,
  doi = {10.48550/ARXIV.1603.07285},
  url = {https://arxiv.org/abs/1603.07285},
  author = {Dumoulin,  Vincent and Visin,  Francesco},
  keywords = {Machine Learning (stat.ML),  Machine Learning (cs.LG),  Neural and Evolutionary Computing (cs.NE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {A guide to convolution arithmetic for deep learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@article{butcher_1963, title={Coefficients for the study of Runge-Kutta integration processes}, volume={3}, DOI={10.1017/S1446788700027932}, number={2}, journal={Journal of the Australian Mathematical Society}, publisher={Cambridge University Press}, author={Butcher, J. C.}, year={1963}, pages={185–201}}

@inproceedings{Ascher1998ComputerMF,
  title={Computer methods for ordinary differential equations and differential-algebraic equations},
  author={Uri M. Ascher and Linda R. Petzold},
  year={1998},
  url={https://api.semanticscholar.org/CorpusID:32366732}
}
@inproceedings{
bartolucci2023representation,
title={Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning},
author={Francesca Bartolucci and Emmanuel de Bezenac and Bogdan Raonic and Roberto Molinaro and Siddhartha Mishra and Rima Alaifari},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=7LSEkvEGCM}
}
@article{gholamalinezhad2020pooling,
  title={Pooling methods in deep neural networks, a review},
  author={Gholamalinezhad, Hossein and Khosravi, Hossein},
  journal={arXiv preprint arXiv:2009.07485},
  year={2020}
}
@article{Geneva2022,
  title = {Transformers for modeling physical systems},
  volume = {146},
  ISSN = {0893-6080},
  url = {http://dx.doi.org/10.1016/j.neunet.2021.11.022},
  DOI = {10.1016/j.neunet.2021.11.022},
  journal = {Neural Networks},
  publisher = {Elsevier BV},
  author = {Geneva,  Nicholas and Zabaras,  Nicholas},
  year = {2022},
  month = feb,
  pages = {272–289}
}
@article{Zhu2018,
  title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
  volume = {366},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2018.04.018},
  DOI = {10.1016/j.jcp.2018.04.018},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Zhu,  Yinhao and Zabaras,  Nicholas},
  year = {2018},
  month = aug,
  pages = {415–447}
}
@article{pichi2024graph,
  title={A graph convolutional autoencoder approach to model order reduction for parametrized PDEs},
  author={Pichi, Federico and Moya, Beatriz and Hesthaven, Jan S},
  journal={Journal of Computational Physics},
  pages={112762},
  year={2024},
  publisher={Elsevier}
}
@article{bhattacharya2021model,
  title={Model reduction and neural networks for parametric PDEs},
  author={Bhattacharya, Kaushik and Hosseini, Bamdad and Kovachki, Nikola B and Stuart, Andrew M},
  journal={The SMAI journal of computational mathematics},
  volume={7},
  pages={121--157},
  year={2021}
}
@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@article{FRESCA2022114181,
title = {POD-DL-ROM: Enhancing deep learning-based reduced order models for nonlinear parametrized PDEs by proper orthogonal decomposition},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {388},
pages = {114181},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114181},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521005120},
author = {Stefania Fresca and Andrea Manzoni},
keywords = {Reduced order modeling, Deep learning, Proper orthogonal decomposition, Dimensionality reduction, Parametrized PDEs},
abstract = {Deep learning-based reduced order models (DL-ROMs) have been recently proposed to overcome common limitations shared by conventional reduced order models (ROMs) – built, e.g., through proper orthogonal decomposition (POD) – when applied to nonlinear time-dependent parametrized partial differential equations (PDEs). These might be related to (i) the need to deal with projections onto high dimensional linear approximating trial manifolds, (ii) expensive hyper-reduction strategies, or (iii) the intrinsic difficulty to handle physical complexity with a linear superimposition of modes. All these aspects are avoided when employing DL-ROMs, which learn in a non-intrusive way both the nonlinear trial manifold and the reduced dynamics, by relying on deep (e.g., feedforward, convolutional, autoencoder) neural networks. Although extremely efficient at testing time, when evaluating the PDE solution for any new testing-parameter instance, DL-ROMs require an expensive training stage, because of the extremely large number of network parameters to be estimated. In this paper we propose a possible way to avoid an expensive training stage of DL-ROMs, by (i) performing a prior dimensionality reduction through POD, and (ii) relying on a multi-fidelity pretraining stage, where different physical models can be efficiently combined. The proposed POD-DL-ROM is tested on several (both scalar and vector, linear and nonlinear) time-dependent parametrized PDEs (such as, e.g., linear advection–diffusion–reaction, nonlinear diffusion–reaction, nonlinear elastodynamics, and Navier–Stokes equations) to show the generality of this approach and its remarkable computational savings.}
}
@article{fresca2021comprehensive,
  title={A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs},
  author={Fresca, Stefania and Dede’, Luca and Manzoni, Andrea},
  journal={Journal of Scientific Computing},
  volume={87},
  pages={1--36},
  year={2021},
  publisher={Springer}
}
@article{Bhattacharya2021,
  title = {Model Reduction And Neural Networks For Parametric PDEs},
  volume = {7},
  ISSN = {2426-8399},
  url = {http://dx.doi.org/10.5802/smai-jcm.74},
  DOI = {10.5802/smai-jcm.74},
  journal = {The SMAI journal of computational mathematics},
  publisher = {Cellule MathDoc/CEDRAM},
  author = {Bhattacharya,  Kaushik and Hosseini,  Bamdad and Kovachki,  Nikola B. and Stuart,  Andrew M.},
  year = {2021},
  month = jul,
  pages = {121–157}
}
@article{cracco2022deep,
  title={Deep learning-based reduced-order methods for fast transient dynamics},
  author={Cracco, Martina and Stabile, Giovanni and Lario, Andrea and Larcher, Martin and Casadei, Folco and Valsamos, Georgios and Rozza, Gianluigi},
  journal={arXiv preprint arXiv:2212.07737},
  year={2022}
}
@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
@article{Guo2019,
  title = {Data-driven reduced order modeling for time-dependent problems},
  volume = {345},
  ISSN = {0045-7825},
  url = {http://dx.doi.org/10.1016/j.cma.2018.10.029},
  DOI = {10.1016/j.cma.2018.10.029},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  publisher = {Elsevier BV},
  author = {Guo,  Mengwu and Hesthaven,  Jan S.},
  year = {2019},
  month = mar,
  pages = {75–99}
}
@article{pod_lumley,
author="LUMLEY J. L.",
title="The structure of inhomogeneous turbulent flows",
journal="Atmospheric turbulence and wave propagation",
publisher="Nauka",
year="1967",
pages="166-178",
URL="https://cir.nii.ac.jp/crid/1573387449825294592"
}
@book{RB,
author = {Quarteroni, Alfio and Manzoni, Andrea and Negri, Federico},
year = {2015},
month = {01},
pages = {1-263},
title = {Reduced basis methods for partial differential equations: An introduction},
isbn = {978-3-319-15430-5},
doi = {10.1007/978-3-319-15431-2}
}
@article{Benner2015,
  title = {A Survey of Projection-Based Model Reduction Methods for Parametric Dynamical Systems},
  volume = {57},
  ISSN = {1095-7200},
  url = {http://dx.doi.org/10.1137/130932715},
  DOI = {10.1137/130932715},
  number = {4},
  journal = {SIAM Review},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
  author = {Benner,  Peter and Gugercin,  Serkan and Willcox,  Karen},
  year = {2015},
  month = jan,
  pages = {483–531}
}
@inproceedings{cohen2014learning,
  title={Learning the irreducible representations of commutative lie groups},
  author={Cohen, Taco and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={1755--1763},
  year={2014},
  organization={PMLR}
}
@article{Fefferman2016,
  title = {Testing the manifold hypothesis},
  volume = {29},
  ISSN = {1088-6834},
  url = {http://dx.doi.org/10.1090/jams/852},
  DOI = {10.1090/jams/852},
  number = {4},
  journal = {Journal of the American Mathematical Society},
  publisher = {American Mathematical Society (AMS)},
  author = {Fefferman,  Charles and Mitter,  Sanjoy and Narayanan,  Hariharan},
  year = {2016},
  month = feb,
  pages = {983–1049}
}
@article{goldt2020modeling,
  title={Modeling the influence of data structure on learning in neural networks: The hidden manifold model},
  author={Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Physical Review X},
  volume={10},
  number={4},
  pages={041044},
  year={2020},
  publisher={APS}
}
@article{Placzek2011,
  title = {A nonlinear POD-Galerkin reduced-order model for compressible flows taking into account rigid body motions},
  volume = {200},
  ISSN = {0045-7825},
  url = {http://dx.doi.org/10.1016/j.cma.2011.08.017},
  DOI = {10.1016/j.cma.2011.08.017},
  number = {49–52},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  publisher = {Elsevier BV},
  author = {Placzek,  A. and Tran,  D.-M. and Ohayon,  R.},
  year = {2011},
  month = dec,
  pages = {3497–3514}
}
@article{dumoulin2018feature-wise,
  author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm de and Courville, Aaron and Bengio, Yoshua},
  title = {Feature-wise transformations},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/feature-wise-transformations},
  doi = {10.23915/distill.00011}
}

@article{RF,
author = {Nelsen, Nicholas H. and Stuart, Andrew M.},
title = {The Random Feature Model for Input-Output Maps between Banach Spaces},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {5},
pages = {A3212-A3243},
year = {2021},
doi = {10.1137/20M133957X},

URL = { 
    
        https://doi.org/10.1137/20M133957X
    
    

},
eprint = { 
    
        https://doi.org/10.1137/20M133957X
    
    

}
,
    abstract = { Well known to the machine learning community, the random feature model is a parametric approximation to kernel interpolation or regression methods. It is typically used to approximate functions mapping a finite-dimensional input space to the real line. In this paper, we instead propose a methodology for use of the random feature model as a data-driven surrogate for operators that map an input Banach space to an output Banach space. Although the methodology is quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs and outputs are themselves functions, with the input parameters being functions required to specify the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon discretization, the model inherits several desirable attributes from this infinite-dimensional viewpoint, including mesh-invariant approximation error with respect to the true PDE solution map and the capability to be trained at one mesh resolution and then deployed at different mesh resolutions. We view the random feature model as a nonintrusive data-driven emulator, provide a mathematical framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and engineering applications: the viscous Burgers' equation and a variable coefficient elliptic equation. }
}

@article{lu2022comprehensive,
  title={A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data},
  author={Lu, Lu and Meng, Xuhui and Cai, Shengze and Mao, Zhiping and Goswami, Somdatta and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={393},
  pages={114778},
  year={2022},
  publisher={Elsevier}
}
@article{jin2022mionet,
  title={MIONet: Learning multiple-input operators via tensor product},
  author={Jin, Pengzhan and Meng, Shuai and Lu, Lu},
  journal={SIAM Journal on Scientific Computing},
  volume={44},
  number={6},
  pages={A3490--A3514},
  year={2022},
  publisher={SIAM}
}

@article{chen,
author = {Chen, Tianping and Chen, Hong},
year = {1995},
month = {08},
pages = {911 - 917},
title = {Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its applications to dynamic systems},
journal = {Neural Networks, IEEE Transactions on},
doi = {10.1109/72.392253}
}
@article{improvedDON,
  title={Improved architectures and training algorithms for deep operator networks},
  author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  journal={Journal of Scientific Computing},
  volume={92},
  number={2},
  pages={35},
  year={2022},
  publisher={Springer}
}
@article{Franco2022,
  title = {A deep learning approach to Reduced Order Modelling of parameter dependent partial differential equations},
  volume = {92},
  ISSN = {1088-6842},
  url = {http://dx.doi.org/10.1090/mcom/3781},
  DOI = {10.1090/mcom/3781},
  number = {340},
  journal = {Mathematics of Computation},
  publisher = {American Mathematical Society (AMS)},
  author = {Franco,  Nicola and Manzoni,  Andrea and Zunino,  Paolo},
  year = {2022},
  month = nov,
  pages = {483–524}
}
@article{Brunton2016,
  title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  volume = {113},
  ISSN = {1091-6490},
  url = {http://dx.doi.org/10.1073/pnas.1517384113},
  DOI = {10.1073/pnas.1517384113},
  number = {15},
  journal = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author = {Brunton,  Steven L. and Proctor,  Joshua L. and Kutz,  J. Nathan},
  year = {2016},
  month = mar,
  pages = {3932–3937}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{fft,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2003354},
 author = {James W. Cooley and John W. Tukey},
 journal = {Mathematics of Computation},
 number = {90},
 pages = {297--301},
 publisher = {American Mathematical Society},
 title = {An Algorithm for the Machine Calculation of Complex Fourier Series},
 urldate = {2023-12-12},
 volume = {19},
 year = {1965}
}
@article{pichi2023graph,
  title={A graph convolutional autoencoder approach to model order reduction for parametrized PDEs},
  author={Pichi, Federico and Moya, Beatriz and Hesthaven, Jan S},
  journal={arXiv preprint arXiv:2305.08573},
  year={2023}
}

@article{wang2021learning,
  title={Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
  author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  journal={Science advances},
  volume={7},
  number={40},
  pages={eabi8605},
  year={2021},
  publisher={American Association for the Advancement of Science}
}
@article{Perk2014,
  title = {Grid and basis adaptive polynomial chaos techniques for sensitivity and uncertainty analysis},
  volume = {260},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2013.12.025},
  DOI = {10.1016/j.jcp.2013.12.025},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Perkó,  Zoltán and Gilli,  Luca and Lathouwers,  Danny and Kloosterman,  Jan Leen},
  year = {2014},
  month = mar,
  pages = {54–84}
}
@article{fourierdeformation,
  title={Fourier neural operator with learned deformations for pdes on general geometries},
  author={Li, Zongyi and Huang, Daniel Zhengyu and Liu, Burigede and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2207.05209},
  year={2022}
}
@inproceedings{Pinkus1985nWidthsIA,
  title={n-Widths in Approximation Theory},
  author={Allan Pinkus},
  year={1985},
  url={https://api.semanticscholar.org/CorpusID:119692918}
}
@article{brandstetter2022message,
  title={Message passing neural PDE solvers},
  author={Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
  journal={arXiv preprint arXiv:2202.03376},
  year={2022}
}
@article{gupta2022towards,
  title={Towards multi-spatiotemporal-scale generalized pde modeling},
  author={Gupta, Jayesh K and Brandstetter, Johannes},
  journal={arXiv preprint arXiv:2209.15616},
  year={2022}
}
@article{franco2023deep,
  title={Deep learning-based surrogate models for parametrized PDEs: Handling geometric variability through graph neural networks},
  author={Franco, Nicola Rares and Fresca, Stefania and Tombari, Filippo and Manzoni, Andrea},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={33},
  number={12},
  year={2023},
  publisher={AIP Publishing}
}

@article{cnn,
  title={An introduction to convolutional neural networks},
  author={O'Shea, Keiron and Nash, Ryan},
  journal={arXiv preprint arXiv:1511.08458},
  year={2015}
}
@article{pathak2022fourcastnet,
  title={Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators},
  author={Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and others},
  journal={arXiv preprint arXiv:2202.11214},
  year={2022}
}
@article{li2023geometry,
  title={Geometry-Informed Neural Operator for Large-Scale 3D PDEs},
  author={Li, Zongyi and Kovachki, Nikola Borislavov and Choy, Chris and Li, Boyi and Kossaifi, Jean and Otta, Shourya Prakash and Nabian, Mohammad Amin and Stadler, Maximilian and Hundt, Christian and Azizzadenesheli, Kamyar and others},
  journal={arXiv preprint arXiv:2309.00583},
  year={2023}
}

@article{upnet,
author = {Stender, Merten and Ohlsen, Jakob and Geisler, Hendrik and Chabchoub, Amin and Hoffmann, Norbert and Schlaefer, Alexander},
year = {2023},
month = {03},
pages = {},
title = {Up-Net: a generic deep learning-based time stepper for parameterized spatio-temporal dynamics},
volume = {71},
journal = {Computational Mechanics},
doi = {10.1007/s00466-023-02295-x}
}
@article{koopmanembedding,
author = {Lusch, Bethany and Kutz, J. and Brunton, Steven},
year = {2018},
month = {11},
pages = {},
title = {Deep learning for universal linear embeddings of nonlinear dynamics},
volume = {9},
journal = {Nature Communications},
doi = {10.1038/s41467-018-07210-0}
}
@article{cnnimageclass,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = {may},
pages = {84–90},
numpages = {7}
}

@article{POD,
author = {Berkooz, G and Holmes, PJ and Lumley, John},
year = {2003},
month = {11},
pages = {539-575},
title = {The Proper Orthogonal Decomposition in the Analysis of Turbulent Flows},
volume = {25},
journal = {Annual Review of Fluid Mechanics},
doi = {10.1146/annurev.fl.25.010193.002543}
}
@article{equer2023multi,
  title={Multi-scale message passing neural pde solvers},
  author={Equer, L{\'e}onard and Rusch, T Konstantin and Mishra, Siddhartha},
  journal={arXiv preprint arXiv:2302.03580},
  year={2023}
}
@article{kutz2016koopman,
  title={Koopman theory for partial differential equations},
  author={Kutz, J Nathan and Proctor, Joshua L and Brunton, Steven L},
  journal={arXiv preprint arXiv:1607.07076},
  year={2016}
}
@article{li2024geometry,
  title={Geometry-informed neural operator for large-scale 3d pdes},
  author={Li, Zongyi and Kovachki, Nikola and Choy, Chris and Li, Boyi and Kossaifi, Jean and Otta, Shourya and Nabian, Mohammad Amin and Stadler, Maximilian and Hundt, Christian and Azizzadenesheli, Kamyar and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{kissas2022learning,
  title={Learning operators with coupled attention},
  author={Kissas, Georgios and Seidman, Jacob H and Guilhoto, Leonardo Ferreira and Preciado, Victor M and Pappas, George J and Perdikaris, Paris},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={215},
  pages={1--63},
  year={2022}
}
@inproceedings{hao2023gnot,
  title={Gnot: A general neural operator transformer for operator learning},
  author={Hao, Zhongkai and Wang, Zhengyi and Su, Hang and Ying, Chengyang and Dong, Yinpeng and Liu, Songming and Cheng, Ze and Song, Jian and Zhu, Jun},
  booktitle={International Conference on Machine Learning},
  pages={12556--12569},
  year={2023},
  organization={PMLR}
}
@article{Lu2021,
  title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  volume = {3},
  ISSN = {2522-5839},
  url = {http://dx.doi.org/10.1038/s42256-021-00302-5},
  DOI = {10.1038/s42256-021-00302-5},
  number = {3},
  journal = {Nature Machine Intelligence},
  publisher = {Springer Science and Business Media LLC},
  author = {Lu,  Lu and Jin,  Pengzhan and Pang,  Guofei and Zhang,  Zhongqiang and Karniadakis,  George Em},
  year = {2021},
  month = mar,
  pages = {218–229}
}
@article{Lusch2018,
  title = {Deep learning for universal linear embeddings of nonlinear dynamics},
  volume = {9},
  ISSN = {2041-1723},
  url = {http://dx.doi.org/10.1038/s41467-018-07210-0},
  DOI = {10.1038/s41467-018-07210-0},
  number = {1},
  journal = {Nature Communications},
  publisher = {Springer Science and Business Media LLC},
  author = {Lusch,  Bethany and Kutz,  J. Nathan and Brunton,  Steven L.},
  year = {2018},
  month = nov 
}
@article{SoleraRico2024,
  title = {Variational autoencoders and transformers for reduced-order modelling of fluid flows},
  volume = {15},
  ISSN = {2041-1723},
  url = {http://dx.doi.org/10.1038/s41467-024-45578-4},
  DOI = {10.1038/s41467-024-45578-4},
  number = {1},
  journal = {Nature Communications},
  publisher = {Springer Science and Business Media LLC},
  author = {Solera-Rico,  Alberto and Sanmiguel Vila,  Carlos and Gómez-López,  Miguel and Wang,  Yuning and Almashjary,  Abdulrahman and Dawson,  Scott T. M. and Vinuesa,  Ricardo},
  year = {2024},
  month = feb 
}
@article{romcnn,
  title = {Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders},
  volume = {404},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2019.108973},
  DOI = {10.1016/j.jcp.2019.108973},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Lee,  Kookjin and Carlberg,  Kevin T.},
  year = {2020},
  month = mar,
  pages = {108973}
}

@inproceedings{vaswani2017attention,
  added-at = {2019-01-14T18:39:11.000+0100},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  biburl = {https://www.bibsonomy.org/bibtex/2a08c93d224dfcfb83550246c3d6a178f/stefan.ernst},
  booktitle = {Advances in Neural Information Processing Systems},
  description = {Aktuelleres Paper zur Verwendung von Attention für die Neural Machine Translation},
  interhash = {c9bf08cbcb15680c807e12a01dd8c929},
  intrahash = {a08c93d224dfcfb83550246c3d6a178f},
  keywords = {final thema:attention},
  pages = {5998--6008},
  timestamp = {2019-01-14T18:39:11.000+0100},
  title = {Attention is all you need},
  year = 2017
}
@article{meshfree,
  title = {Mesh-Informed Neural Networks for Operator Learning in Finite Element Spaces},
  volume = {97},
  ISSN = {1573-7691},
  url = {http://dx.doi.org/10.1007/s10915-023-02331-1},
  DOI = {10.1007/s10915-023-02331-1},
  number = {2},
  journal = {Journal of Scientific Computing},
  publisher = {Springer Science and Business Media LLC},
  author = {Franco,  Nicola Rares and Manzoni,  Andrea and Zunino,  Paolo},
  year = {2023},
  month = sep 
}
@inproceedings{lippe2023pderefiner,
  title        = {{PDE-Refiner: Achieving Accurate Long Rollouts with Temporal Neural PDE Solvers}},
  author       = {Phillip Lippe and Bastiaan S. Veeling and Paris Perdikaris and Richard E Turner and Johannes Brandstetter},
  year         = 2023,
  booktitle    = {NeurIPS},
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}

@article{Alsayyari2021,
  title = {A fully adaptive nonintrusive reduced-order modelling approach for parametrized time-dependent problems},
  volume = {373},
  ISSN = {0045-7825},
  url = {http://dx.doi.org/10.1016/j.cma.2020.113483},
  DOI = {10.1016/j.cma.2020.113483},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  publisher = {Elsevier BV},
  author = {Alsayyari,  Fahad and Perkó,  Zoltán and Tiberga,  Marco and Kloosterman,  Jan Leen and Lathouwers,  Danny},
  year = {2021},
  month = jan,
  pages = {113483}
}
@article{sampling,
author = {Katharopoulos, Angelos and Fleuret, Francois},
year = {2018},
month = {03},
pages = {},
title = {Not All Samples Are Created Equal: Deep Learning with Importance Sampling}
}
@article{Lee2020,
  title = {Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders},
  volume = {404},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2019.108973},
  DOI = {10.1016/j.jcp.2019.108973},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Lee,  Kookjin and Carlberg,  Kevin T.},
  year = {2020},
  month = mar,
  pages = {108973}
}
@InProceedings{vcnef-hagnberger:2024,
  title = 	 {{V}ectorized {C}onditional {Ne}ural {F}ields: A Framework for Solving Time-dependent Parametric Partial Differential Equations},
  author =       {Hagnberger, Jan and Kalimuthu, Marimuthu and Musekamp, Daniel and Niepert, Mathias},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {17189--17223},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/hagnberger24a/hagnberger24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/hagnberger24a.html},
  abstract = 	 {Transformer models are increasingly used for solving Partial Differential Equations (PDEs). Several adaptations have been proposed, all of which suffer from the typical problems of Transformers, such as quadratic memory and time complexity. Furthermore, all prevalent architectures for PDE solving lack at least one of several desirable properties of an ideal surrogate model, such as (i) generalization to PDE parameters not seen during training, (ii) spatial and temporal zero-shot super-resolution, (iii) continuous temporal extrapolation, (iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer temporal rollouts. To address these limitations, we propose <em>Vectorized Conditional Neural Fields</em> (VCNeFs), which represent the solution of time-dependent PDEs as neural fields. Contrary to prior methods, however, VCNeFs compute, for a set of multiple spatio-temporal query points, their solutions in parallel and model their dependencies through attention mechanisms. Moreover, VCNeF can condition the neural field on both the initial conditions and the parameters of the PDEs. An extensive set of experiments demonstrates that VCNeFs are competitive with and often outperform existing ML-based surrogate models.}
}
@article{willrom,
    author = {Benner Peter, Albert Cohen, Mario Ohlberger and Karen Willcox},
    title = {Model Reduction and Approximation : Theory and Algorithms},
    journal = {Philadelphia: Society for Industrial and Applied Mathematics},
    year = {2017}
}
@inproceedings{takamoto2023learning,
  title={Learning neural pde solvers with parameter-guided channel attention},
  author={Takamoto, Makoto and Alesiani, Francesco and Niepert, Mathias},
  booktitle={International Conference on Machine Learning},
  pages={33448--33467},
  year={2023},
  organization={PMLR}
}
@article{takamoto2022pdebench,
  title={Pdebench: An extensive benchmark for scientific machine learning},
  author={Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Daniel and Alesiani, Francesco and Pfl{\"u}ger, Dirk and Niepert, Mathias},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1596--1611},
  year={2022}
}
@inproceedings{Eastwood2018AFF,
  title={A Framework for the Quantitative Evaluation of Disentangled Representations},
  author={Cian Eastwood and Christopher K. I. Williams},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:19571619}
}

@article{Higgins2018TowardsAD,
  title={Towards a Definition of Disentangled Representations},
  author={Irina Higgins and David Amos and David Pfau and S{\'e}bastien Racani{\`e}re and Lo{\"i}c Matthey and Danilo Jimenez Rezende and Alexander Lerchner},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.02230},
  url={https://api.semanticscholar.org/CorpusID:54447715}
}
@article{Kidger2022OnND,
  title={On Neural Differential Equations},
  author={Patrick Kidger},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.02435},
  url={https://api.semanticscholar.org/CorpusID:246634262}
}
@article{list2024temporal,
  title={How temporal unrolling supports neural physics simulators},
  author={List, Bjoern and Chen, Li-Wei and Bali, Kartik and Thuerey, Nils},
  journal={arXiv preprint arXiv:2402.12971},
  year={2024}
}
@article{higgins2018towards,
  title={Towards a definition of disentangled representations},
  author={Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1812.02230},
  year={2018}
}
@article{Raissi2019,
  title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  volume = {378},
  ISSN = {0021-9991},
  url = {http://dx.doi.org/10.1016/j.jcp.2018.10.045},
  DOI = {10.1016/j.jcp.2018.10.045},
  journal = {Journal of Computational Physics},
  publisher = {Elsevier BV},
  author = {Raissi,  M. and Perdikaris,  P. and Karniadakis,  G.E.},
  year = {2019},
  month = feb,
  pages = {686–707}
}
@article{Wang2021,
  title = {Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
  volume = {7},
  ISSN = {2375-2548},
  url = {http://dx.doi.org/10.1126/sciadv.abi8605},
  DOI = {10.1126/sciadv.abi8605},
  number = {40},
  journal = {Science Advances},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author = {Wang,  Sifan and Wang,  Hanwen and Perdikaris,  Paris},
  year = {2021},
  month = oct 
}
@article{Alsayyari_2021, title={A fully adaptive nonintrusive reduced-order modelling approach for parametrized time-dependent problems}, volume={373}, ISSN={0045-7825}, url={http://dx.doi.org/10.1016/j.cma.2020.113483}, DOI={10.1016/j.cma.2020.113483}, journal={Computer Methods in Applied Mechanics and Engineering}, publisher={Elsevier BV}, author={Alsayyari, Fahad and Perkó, Zoltán and Tiberga, Marco and Kloosterman, Jan Leen and Lathouwers, Danny}, year={2021}, month=jan, pages={113483} }
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@InProceedings{pmlr-v115-aicher20a,
  title = 	 {Adaptively Truncating Backpropagation Through Time to Control Gradient Bias},
  author =       {Aicher, Christopher and Foti, Nicholas J. and Fox, Emily B.},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {799--808},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/aicher20a/aicher20a.pdf},
  url = 	 {https://proceedings.mlr.press/v115/aicher20a.html},
  abstract = 	 {Truncated backpropagation through time (TBPTT) is a popular method for learning in recurrent neural networks (RNNs) that saves computation and memory at the cost of bias by truncating backpropagation after a fixed number of lags. In practice, choosing the optimal truncation length is difficult: TBPTT will not converge if the truncation length is too small, or will converge slowly if it is too large. We propose an adaptive TBPTT scheme that converts the problem from choosing a temporal lag to one of choosing a tolerable amount of gradient bias. For many realistic RNNs, the TBPTT gradients decay geometrically in expectation for large lags; under this condition, we can control the bias by varying the truncation length adaptively.  For RNNs with smooth activation functions, we prove that this bias controls the convergence rate of SGD with biased gradients for our non-convex loss. Using this theory, we develop a practical method for adaptively estimating the truncation length during training. We evaluate our adaptive TBPTT method on synthetic data and language modeling tasks and find that our adaptive TBPTT ameliorates the computational pitfalls of fixed TBPTT.}
}
@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{coral,
  author={Louis Serrano and Lise Le Boudec and Armand Kassaï Koupaï and Thomas X. Wang and Yuan Yin and Jean-Noël Vittaut and Patrick Gallinari},
  title={Operator Learning with Neural Fields: Tackling PDEs on General Geometries},
  year={2023},
  cdate={1672531200000},
  url={http://papers.nips.cc/paper_files/paper/2023/hash/df54302388bbc145aacaa1a54a4a5933-Abstract-Conference.html},
  booktitle={NeurIPS},
}

@inproceedings{xie2022neural,
  title={Neural fields in visual computing and beyond},
  author={Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
  booktitle={Computer Graphics Forum},
  volume={41},
  number={2},
  pages={641--676},
  year={2022},
  organization={Wiley Online Library}
}

@inproceedings{
cao2021choose,
title={Choose a Transformer: Fourier or Galerkin},
author={Shuhao Cao},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ssohLcmn4-r}
}

@article{
li2023transformer,
title={Transformer for Partial Differential Equations{\textquoteright} Operator Learning},
author={Zijie Li and Kazem Meidani and Amir Barati Farimani},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=EPPqt3uERT},
note={}
}