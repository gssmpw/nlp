% Add Appendix title
\section*{Appendix}
\section{Runge-Kutta schemes}
\label{sec:RK}
Runge-Kutta methods \cite{Ascher1998ComputerMF} are a family of numerical methods for the solution of Ordinary Differential Equations (ODEs). They belong to the category of \textit{one-step} methods, as such they do not use any information from previous time steps. Given $t_i\in\mathbf{T}$, a standard explicit Runge-Kutta method would solve Equation (\ref{eq:ODE}) as:
\begin{equation}
\label{eq:rungekutta_reduced}
    \varepsilon(t_{i+1}|\pmb{\mu}) = \varepsilon(t_{i}|\pmb{\mu}) + \Delta t_{i+1,i}\sum_{j=1}^q h_j b_j,
\end{equation}
where $q$ is called the \textit{stage} of the Runge-Kutta approximation, $\Delta t_{i+1,i} = t_{i+1}-t_i$ and:
\begin{align*}
     b_1 = &f(\varepsilon(t_{i}|\pmb{\mu}),t_{i},\pmb{\mu}),\\
     b_2 = &f(\varepsilon(t_{i}|\pmb{\mu})+(a_{2,1}b_1)\Delta t_{i+1,i},t_i+c_2\Delta t_{i+1,i},\pmb{\mu}),\\
    & \vdots \\
    b_k = & f(\varepsilon(t_{i}|\pmb{\mu})+\sum_{l=1}^{k-1}a_{k,l}b_l\Delta t_{i+1,i}, t_i+c_k\Delta t_{i+1,i},\pmb{\mu}).
\end{align*}
The matrix composed by $a_{ij}$ is known as Runge-Kutta matrix, $h_j$ are the weights and $c_j$ are the nodes, with their values given by the Butcher tableau \cite{butcher_1963}.

\subsection{The effect of the stage of RK on time generalization}
\label{subsubsec:RK_time_gen}
In this work we use a fixed Runge-Kutta time stepper, and to go from the state $i$ to the state $i+1$ we do not step trough intermediate states. We defined the Processor of Equation (\ref{eq:processor}) with a $\Delta t_{i+1,i}$ dependency as we want to perform inference even at temporal discretizations finer than the one used at training. %In other words, we want to be able to approximate the dynamics of the latent space $\mathcal{E}$ by looking at some fixed points in time during training without precluding the possibility to access the dynamics of $\mathcal{E}$ continuously in time. 
Although this task may look trivial as $\pi_\theta$ directly takes $\Delta t_{i+1,i} $ as input, it raises the following issue when solving the ODE. Let us consider a processor $\pi_\theta$ which evolves in time the latent vector $\varepsilon(t|\pmb{\mu})$ using an Euler integration scheme (from here on we omit the $\pmb{\mu}$ dependency for ease of reading):
\begin{equation}
    \varepsilon(t_{i+1})  = \varepsilon(t_{i})+\Delta t_{i+1,i}\,f_\theta(\varepsilon(t_{i})),
\end{equation}
and let us define a moment in time $t_m$ such that $t_i<t_m<t_{i+1}$. We want $f_\theta$ to satisfy the following conditions:
\begin{equation}
    \left\{
    \begin{aligned}
    &\varepsilon(t_{i+1})  = \varepsilon(t_{i})+\Delta t_{i+1,i}\,f_\theta(\varepsilon(t_{i})), \\
    &\varepsilon(t_{i+1})  = \varepsilon(t_{m}) + (\Delta t_{i+1,i}-\Delta t_{m,i})\, f_\theta(\varepsilon(t_{m}))),
    \end{aligned}
    \right.
\label{eq:two_generalization}
\end{equation}
where $\varepsilon(t_{m}) = \varepsilon(t_{i})+\Delta t_{m,i}\,f_\theta(\varepsilon(t_{i}))$.
By taking the difference of the two Equations of (\ref{eq:two_generalization}) we get that 
%the following relationship:
%\begin{equation}
%\begin{aligned}
%    &0 = (\Delta t_{i+1,i}-\Delta t_{m,i}) \times \\
%    &\left[ f_\theta(\varepsilon(t_{i}))-%f_\theta(\varepsilon(t_{i})+\Delta t_{m,i}\,f_\theta(\varepsilon(t_{i})))\right],
%\end{aligned}
%\end{equation}

\begin{equation}
f_\theta(\varepsilon(t_{i})) = f_\theta(\varepsilon(t_{m})),
\end{equation}
i.e., when we use the Euler scheme $f_\theta$ must be a constant if we want $\pi_\theta$ to be coherent with its predictions at the variation of $\Delta t_{i+1,i}$. Notice that a constant $f_\theta$ would imply a linear time dependence of $\varepsilon(t)$ (the dotline of Figure \ref{fig:inference_in_time} would be a line), meaning that the construction of $\mathcal{E}$ would be subjected to a \textbf{strong constraint}, thus limiting the expressiveness of the AE. For a RK method of order 2 instead:
\begin{equation}
    \varepsilon(t_{i+1}) = \varepsilon(t_{i})+\Delta t_{i+1,i}f_\theta\left(\varepsilon(t_{i})+\frac{1}{2}\Delta t_{i+1,i}\,f_\theta(\varepsilon(t_{i}))\right),
\end{equation}
where $f_\theta$ is evaluated at $\Delta t_{i+1,i}/2$. We want $f_\theta$ to respect the following system:
\begin{equation}
    \left\{
    \begin{aligned}
    &\varepsilon(t_{i+1}|\pmb{\mu})  = \varepsilon(t_{i}|\pmb{\mu}) + \Delta t_{i+1,i}f_\theta(\varepsilon(t^{i+1}_i|\pmb{\mu})), \\
    &\varepsilon(t_{i+1}|\pmb{\mu})= \varepsilon(t_{i}|\pmb{\mu})+\Delta t_{m,i} f_\theta(\varepsilon(t^m_i|\pmb{\mu}))+ (\Delta t_{i+1,i}-\Delta t_m)f_\theta(\varepsilon(t^{i+1}_m|\pmb{\mu})),
    \end{aligned}
    \right.
\label{eq:two_generalization_RK_2}
\end{equation}
where  $t_i<t_m<t_{i+1}$, $t^{j}_k = \frac{t_j+t_k}{2}$ and $\varepsilon(t^{j}_k|\pmb{\mu}) =\varepsilon(t_{k}|\pmb{\mu})+\frac{ \Delta t_{j,k}}{2}f_\theta(\varepsilon(t_{k}|\pmb{\mu}))$. By taking the difference of System (\ref{eq:two_generalization_RK_2}) we get:
\begin{equation}
    \Delta t_{i+1,i}f_\theta(\varepsilon(t^{i+1}_i|\pmb{\mu})) = \Delta t_{m,i} f_\theta(\varepsilon(t^m_i|\pmb{\mu}))+ (\Delta t_{i+1,i}-\Delta t_m)f_\theta(\varepsilon(t^{i+1}_m|\pmb{\mu})),
\end{equation}
which, by going back to full notation, results in the following Equation:
\begin{equation}
    \begin{aligned}
    &\Delta t_{i+1,i}f_\theta\left[\varepsilon(t_{i}|\pmb{\mu})+\frac{ \Delta t_{i+1,i}}{2}f_\theta(\varepsilon(t_{i}|\pmb{\mu}))\right]= \Delta t_{m,i} f_\theta\left[\varepsilon(t_{i}|\pmb{\mu})+\frac{ \Delta t_{m,i}}{2}f_\theta(\varepsilon(t_{i}|\pmb{\mu}))\right]+ \\ 
    &+(\Delta t_{i+1,i}-\Delta t_m)f_\theta\left[\varepsilon(t_{i}|\pmb{\mu})+\frac{ \Delta t_{m,i}}{2}f_\theta(\varepsilon(t_{i}|\pmb{\mu}))+\frac{ \Delta t_{i+1,m}}{2}f_\theta\left[\varepsilon(t_{i}|\pmb{\mu})+\frac{ \Delta t_{m,i}}{2}f_\theta(\varepsilon(t_{i}|\pmb{\mu}))\right]\right],
    \end{aligned}
\end{equation}
where $\varepsilon(t_{i}|\pmb{\mu})+\frac{ \Delta t_{m,i}}{2}f_\theta(\varepsilon(t_{i}|\pmb{\mu})) = \varepsilon(t_m|\pmb{\mu})$. The constraint to which $f_\theta$ is now subjected allows for a more complex form of $f_\theta$ which in turns results in a reduced space $\mathcal{E}$ \textbf{more capable of adapting} to the complexity of the original space $\mathcal{S}$. It follows that, if we require $f_\theta$ to generalize to a variable $\Delta t_{i+1,i}$, the higher the stage of the Runge-Kutta scheme used, the more complex $f_\theta$ can be and the more complex the reduced space $\mathcal{E}$ can be.
\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{image_methodology/generalization_in_time.png}
  \caption{The evolution over time of a one dimensional $\varepsilon(t|\pmb{\mu})$ is shown (dot line). The red points indicate the steps in time used during the training, at intervals of $\Delta t_{i+1,1}$, and the green points show the points in time that can be predicted at testing time at distance of $\Delta t_{i+1,1}/\alpha$, where $\alpha\in[1,\infty)$.}
  \label{fig:inference_in_time}
\end{figure}

\section{Architecture details}
\label{subsec:architecture}
As detailed in Section \ref{subsec:AE&NODE}, our model is made up of three components: an Encoder $\varphi_\theta$, a Processor $\pi_\theta$ and a Decoder $\psi_\theta$.\\

\textbf{Encoder}\\
We build $\varphi_\theta$ as a series of Convolutional layers \cite{CNNarithme}  followed by a final Linear layer as in the 2D example of Table \ref{tab:Encoder}.
\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Layer Number & Input size & Output size & Filters & Kernel & Stride \\ \hline
$1$ (Convolutional) & $[m,N,N] $   & $[Fe_1,N,N]$                    & $Fe_1$ & $[Ke_{1},Ke_{1}]$ & $[1,1]$ \\ \hline
$2$ (Convolutional) & $[Fe_1,N,N]$ & $[Fe_2,\frac{N}{2},\frac{N}{2}]$ &  $Fe_2$    &   $[Ke_{2},Ke_{2}]$    &   $[2,2]$    \\ \hline
$\vdots$ &            &                                &      &       &                   \\ \hline
 $j$ (Convolutional)&  $[Fe_{j-1},\frac{N}{2^{j-2}},\frac{N}{2^{j-2}}]$ & $[Fe_{j},\frac{N}{2^{j-1}},\frac{N}{2^{j-1}}]$ &  $Fe_j$&  $[Ke_{j},Ke_{j}]$    &  $[2,2]$      \\ \hline
 $\vdots$ &            &                                &      &       &               \\ \hline
$L-1$  (Flat layer) & & & & &    \\ \hline
$L$ (Linear) &$[Fe_{L-2}\times\frac{N}{2^{L-3}}\times\frac{N}{2^{L-3}}]$ &$[\lambda]$ & & &   \\ \hline
\end{tabular}
\caption{The structure of the Encoder $\varphi_\theta$ layer by layer, for a 2D case. $m$ is the dimension of the solution field $s(\mathbf{x},t|\pmb{\mu})$, $N$ is the size of the spatial discretization of the field in the $x$ and in the $y$ axis, $Fe = [Fe_1,\cdots,Fe_{L-2}]$ is the vector of convolutional filters, $Ke = [Ke_1,\cdots,Ke_{L-2}]$ is the vector of kernels , $j\in\{2,L-2\}$ and $\lambda$ is the latent dimension. The Flat layer takes all the features coming from the last Convolutional layer and flattens them in a 1D vector. This example is easily reduced to the 1D case.}
\label{tab:Encoder}
\end{table}
The first layer has stride $1$ to do a preprocessing of the fields and the subsequent layers up to the Flat layer halve each spatial dimension by $2$. We use as activation function after each Convolutional layer the GELU function \cite{hendrycks2016gaussian} and we \textbf{do not} use any activation function after the final Linear layer to not constrain the values of the latent space. We experimented with Batch Normalization \cite{ioffe2015batch} and Layer Normalization layers \cite{ba2016layer} between the Convolutional layers and the GELU function but we did not notice any improvements in the results. The weights of all the layers are initialized with the Kaiming (uniform) initialization \cite{he2015delving}. Notice that we are not using any Pooling layer \cite{gholamalinezhad2020pooling} to reduce the dimensionality but only strided Convolutions, as Pooling layers would enforce translational invariance which is not always a desired property.

\textbf{Processor}\\
Inside the Processor $\pi_\theta$, in practice only the function $f_\theta$ which approximates $f$ of Equation (\ref{eq:ODE}) is parametrized by a NN with $f_\theta(\varepsilon(t|\pmb{\mu}),\pmb{\mu})$ being a function of both $\varepsilon(t|\pmb{\mu})$ and $\pmb{\mu}$. We experimented with the parameter dependency in two ways:
\begin{itemize}
    \item $\pmb{\mu}$ is simply concatenated to the reduced vector $\varepsilon(t|\pmb{\mu})$. In this case $f_\theta:\mathbb{R}^{\lambda+z}\rightarrow\mathbb{R}^{\lambda}$, where $z$ is the dimensionality of $\pmb{\mu}$ and $\lambda$ the latent dimension. 
    \item The vector $\varepsilon(t|\pmb{\mu})$ is conditioned to $\pmb{\mu}$ through a FiLM layer \cite{perez2018film}. This means defining the function $\alpha:\mathbb{R}^z\rightarrow\mathbb{R}^{\lambda}$ and the function $\tau:\mathbb{R}^z\rightarrow\mathbb{R}^{\lambda}$. The input to $f_\theta$ will thus be $\alpha(\pmb{\mu})\odot\varepsilon(t|\pmb{\mu})$ + $\tau(\pmb{\mu})$, where $\odot$ is the Hadamard product. $\alpha$ and $\tau$ are chosen to be simple Linear layers.
\end{itemize}
In both cases $f_\theta$ is built as a sequence of Linear layers followed by the GELU activation function. Importantly the activation function is not used after the last Linear layer as this is a regression problem.

\textbf{Decoder}\\
\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Layer Number & Input size & Output size & Filters & Kernel & Stride \\ \hline
$1$ (Linear) & $[\lambda] $   & $[Fd_{1}\times\frac{N}{2^{L-3}}\times\frac{N}{2^{L-3}}]$                    &  &  & \\ \hline
$2$ (Reshape layer) &  &  &     &      &      \\ \hline
$\vdots$ &            &                                &      &       &                   \\ \hline
 $j$ (T. Convolutional) & $[Fd_{j-2},\frac{N}{2^{L-j}},\frac{N}{2^{L-j}}]$ &  $[Fd_{j-1},\frac{N}{2^{L-j-1}},\frac{N}{2^{L-j-1}}]$ & $Fd_{j-1}$&  $[Kd_{j-1},Kd_{j-1}]$    &  $[2,2]$      \\ \hline
 $\vdots$ &            &                                &      &       &               \\ \hline
 $L-1$ (T. Convolutional) &$[Fd_{L-3},\frac{N}{2},\frac{N}{2}]$ &$[Fd_{L-2},N,N]$ &$Fd_{L-2}$ &$[Kd_{L-3},Kd_{L-3}]$ & $[2,2]$  \\ \hline
$L$ (T. Convolutional) &$[Fd_{L-2},N,N]$ &$[m,N,N]$ & $[m]$&$[Kd_{L-2},Kd_{L-2}]$ & $[1,1]$  \\ \hline
\end{tabular}
\caption{The structure of the Decoder $\psi_\theta$ layer by layer, for a 2D case. 'T.' stands for 'Transposed'.}
\label{tab:Decoder}
\end{table}
We build $\psi_\theta$ as a Linear layer followed by a series of Transposed Convolutional layers \cite{CNNarithme} as shown in Table \ref{tab:Decoder}. The initial Linear layer and the last Transposed Convolutional layer are \textbf{not} followed by an activation function while the other Transposed Convolutional layers are followed by a GELU function. We do not use any activation function for the Linear layer for symmetry with the Encoder, while for the last layer of the Decoder because this is a regression task. After the Reshape layer each Transposed (T.) Convolutional layer doubles the dimensionality in both $x$ and $y$ dimensions until the layer number $L-1$. The last layer does not increase the dimensionality of the input (stride $=1$) and is just used to go to the final dimensionality $m$ of the solution field $s$. $Fd = [Fd_1,\cdots,Fd_{L-2}]$ is the vector of convolutional filters, $Ke = [Kd_1,\cdots,Kd_{L-2}]$ is the vector of kernels, $j\in\{3,L-1\}$.
\subsection{Normalization of the inputs}
\label{subsec:normalization}
In order to facilitate the training process we normalize the inputs, as standard Deep Learning practice. We use a max-min normalization both for the input fields $s$ and for the parameters $\pmb{\mu}$. We do not normalize $\Delta t_{i+1,i}$. By max-min normalization, we mean the following: given an input $y$, we transform it accordingly to $y\rightarrow \frac{y-min(D_y)}{max(D_y)-min(D_y)}$, where $max(D_y)$ and $min(D_y)$ are computed over the \textit{training} datasets $D_y$ to which $y$ belongs. In our experiments $s$ is a scalar field so we only compute one tuple $(max(D_s),min(D_s))$. In the case of $\pmb{\mu}$ instead, since each parameter $\mu_i$ can belong to a different scale, we compute $z$ tuples $(max(D_{\mu_i}),min(D_{\mu_i}))$ with $i\in\{1,z\}$. We normalize accordingly the parameters for all the datasets while the input fields for all the datasets but the Burgers' Equation and the parametric Advection.

\section{Training and hyperparameter details}
\label{subsec:training_details}
In all the experiments we use the Adam optimizer \cite{adam} and we stop the training if the validation loss has not decreased for $200$ epochs. We use an Exponential Learning Rate Scheduler, with a decay parameter $\gamma_{lr}$. We set $5000$ as the maximum number of epochs. In all the experiments, unless otherwise specified, we used $q=4$ as the stage of the RK algorithm.  We use $\mathcal{L}_{tr}$ for training and $\mathcal{L}_{vl}$ for validation:
\begin{equation}
    \mathcal{L}_{vl} = \mathcal{L}_{tr} + \sum_{i=1}^{F} \frac{||s_r(\mathbf{x},t_i|\pmb{\mu}) - \tilde{s}_r(\mathbf{x},t_i|\pmb{\mu})||_2}{||s_r(\mathbf{x},t_i|\pmb{\mu})||_2}.
\end{equation}
\subsection{PDEs with fixed parameters}
\textbf{1D Advection}\\
We split the datasets  by taking the time series from the $1$st to the $8000$th as training, from the $8001$th to the $9000$th as validation and from the $9001$th to the $10000$th as testing. We use an initial learning rate of $0.001$, $\gamma_{lr} = 0.997$ and a batch size of $16$. We use $F_e =[8,16,32,64,64,64,64]$, $F_d =[64,64,64,64,32,16,1]$, $K_e = [5,5,5,5,5,5,5]$ and  $K_d = [6,6,6,6,6,6,5]$. $f_\theta$ is composed by $2$ hidden layers with $50$ neurons each and $\lambda=30$. $\lambda_{rg} = 0.0$\\
\textbf{1D Burgers}\\
We split the datasets  by taking the time series from the $1$st to the $8000$th as training, from the $8001$th to the $9000$th as validation and from the $9001$th to the $10000$th as testing. We use an initial learning rate of $0.0014$, $\gamma_{lr} = 0.999$ and a batch size of $32$. We use $F_e =[8, 16, 32, 32, 32, 32, 32]$, $F_d =[32, 32, 32, 32, 32, 16, 1,1]$, $K_e = [5,5,3,3,3,3,3]$ and  $K_d = [4,4,4,4,4,4,3,3]$. $f_\theta$ is composed by $4$ hidden layers with $200$ neurons each and $\lambda=30$. $\lambda_{rg} = 0.001$
\\
\textbf{2D Shallow-Water}\\
We split the datasets  by taking the time series from the $1$st to the $800$th as training, from the $801$th to the $900$th as validation and from the $901$th to the $1000$th as testing. We use an initial learning rate of $0.001$, $\gamma_{lr} = 0.999$ and a batch size of $16$. We use $F_e =[8, 32, 32, 32, 32, 32, 32]$, $F_d =[32, 32, 32, 32, 32, 16, 1,1]$, $K_e = [5,5,3,3,3,3,3]$ and  $K_d = [4,4,4,4,4,4,3,3]$. $f_\theta$ is composed by $2$ hidden layers with $50$ neurons each and $\lambda=20$. $\lambda_{rg} = 0.001$
\subsection{PDEs with varying parameters}
\textbf{1D Advection}\\
We use as training parameters the velocities $\zeta\in\{0.2,0.4,0.7,2.0,4.0\}$ and as testing $\zeta\in\{0.1,1.0,7.0\}$. We split the datasets  by taking, \textbf{for each training parameter} $\zeta$, the time series from the $1$st to the $8000$th as training, from the $8001$th to the $9000$th as validation and from the $9001$th to the $10000$th as testing. We use an initial learning rate of $0.0018$, $\gamma_{lr} = 0.995$ and a batch size of $64$. We use $F_e =[8,16,32,32,32,32,32]$, $F_d =[32,32,32,32,32,16,1]$, $K_e = [5,5,3,3,3,3,3]$ and  $K_d = [4,4,4,4,4,4,3]$. $f_\theta$ is composed by $4$ hidden layers with $200$ neurons each and $\lambda=30$. $\lambda_{rg} = 0.0$ and $\gamma_0=1/500$.\\
\textbf{1D Burgers}\\
We use as training parameters the diffusion coefficient $\nu\in\{0.002,0.004,0.02,0.04,0.2,0.4,2.0\}$ and as testing $\nu\in\{0.001,0.01,0.1,1.0,4.0\}$. We split the datasets  by taking the time series from the $1$st to the $8000$th as training, from the $8001$th to the $9000$th as validation and from the $9001$th to the $10000$th as testing. We use an initial learning rate of $0.0018$, $\gamma_{lr} = 0.995$ and a batch size of $124$. We use $F_e =[8,32,32,32,32,32,32]$, $F_d =[32,32,32,32,32,16,1,1]$, $K_e = [5,5,3,3,3,3,3]$ and  $K_d = [4,4,4,4,4,4,3,3]$. $f_\theta$ is composed by $4$ hidden layers with $200$ neurons each and $\lambda=30$. $\lambda_{rg} = 0.0$ and $\gamma_0=1/1000$.
\\
\textbf{2D Molenkamp test}\\
We generate $5000$ training, $200$ validation and $100$ testing samples by sampling from a uniform distribution the parameters described in $\ref{subsec:molenkamp_test}$. We use an initial learning rate of $0.0015$, $\gamma_{lr} = 0.995$ and a batch size of $16$. We use $F_e =[8, 16, 32, 32, 32, 32, 32]$, $F_d =[32, 32, 32, 32, 32, 16, 1, 1]$, $K_e = [5, 5, 3, 3, 3, 3, 3]$ and  $K_d = [4, 4, 4, 4, 4, 4, 3, 3]$. $f_\theta$ is composed by $2$ hidden layers with $100$ neurons each and $\lambda=50$. $\lambda_{rg} = 0.0$ and $\gamma_0=1/500$. Here we apply a GELU function  in the last layer of the Encoder and to the first layer of the Decoder. 
\subsection{Instabilities of the training}
\label{subsubsec:instabilities}
$\mathcal{L}_{2}^{T,k_1} $ and especially $\mathcal{L}_{2}^{A,k_2}$ can involve complex gradients. During the training, this can sometimes lead the NN to be stuck in the trivial minimum for $\mathcal{L}_{2,i}^{T} $ and $ \mathcal{L}_{2,i}^{A}$ which consists in $\varphi_\theta$ and $\pi_\theta$ returning a constant output. We found out that some datasets are particularly sensitive to this problem, while other are not affected by this issue. In the following, a list of measures which help avoiding the trivial solution:
\begin{itemize}
    \item removing the biases from the Encoder;
    \item using (Batch/Layer) Normalization layers in the Encoder (not necessarely after each convolution);
    \item careful tuning of the learning rate (lowering the learning rate or increasing the batch size);
    \item warm up of the learning rate;
    \item if the instabilities come mostly from $\mathcal{L}_{2,i}^{T,k_1} $ and $\mathcal{L}_{2,i}^{A,k_2} $ as they involve more complex gradients, it can be beneficial to turn them off for some initial epochs (for example during the warm up of the learning rate) by setting $\beta$ and $\gamma$ to zero. This allows for an initial construction of $\mathcal{E}$ with simpler constraints.
\end{itemize}
\subsection{Hardware details}
For training we use either an NVIDIA A40 40 GB or an NVIDIA A100 80GB PCIe depending on availabilities. 
\subsection{Number of NNs weights and speed of inference}
\label{subsec:speed_and_numb_comp}
In Table \ref{tab:numb_weights} we show the number of NNs weights associated with our model and the models used for comparison from \cite{vcnef-hagnberger:2024}. In Table \ref{tab:inference_time} we report the inference time for the Burgers' dataset with $\nu=0.001$. We do not consider the time spent for sending the batches from the CPU to the GPU; the time measured is the time taken to do inference on the whole testing dataset of 1000 initial conditions with a batchsize of 64. We use an NVIDIA A100 80GB PCIe to conduct the inference test. Inference time of other methods is from \cite{vcnef-hagnberger:2024} where they use an NVIDIA A100-SXM4 80GB GPU.

\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|l|}{Time resolution} & Model    & Inference time {[}ms{]}  \\ \hline
                                      & Ours     & $466.73^{\pm 68.38}$                             \\ \cline{2-3} 
                                      & FNO      & $917.77^{\pm 2.51}$                 \\ \cline{2-3} 
\multicolumn{1}{|c|}{41}               & VCNeF    & $2244.04^{\pm 6.65}$                 \\ \cline{2-3} 
                                      & Galerkin & $2415.99^{\pm 54.56}$                  \\ \cline{2-3} 
                                      & VCNeF s. & $4853.17^{\pm 75.29}$                 \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $6025.75^{\pm 12.75}$                 \\ \hline
                                      & Ours     & $932.43^{136.588}$                            \\ \cline{2-3} 
                                      & FNO      & $1912.19^{\pm 56.03}$                    \\ \cline{2-3} 
\multicolumn{1}{|c|}{81}               & VCNeF    & $4422.65^{\pm 4.11}$                  \\ \cline{2-3} 
                                      & Galerkin & $4940.80^{\pm 89.44}$                 \\ \cline{2-3} 
                                      & VCNeF s. & $9701.80^{\pm 84.48}$                  \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $12081.98^{\pm 19.39}$                   \\ \hline
                                      & Ours     & $1440.67^{\pm 250.29}$                          \\ \cline{2-3} 
                                      & FNO      & $2808.04^{\pm 82.22}$                     \\ \cline{2-3} 
\multicolumn{1}{|c|}{121}               & VCNeF    & $6606.41^{\pm 3.0}$                  \\ \cline{2-3} 
                                      & Galerkin & $7908.18^{\pm 96.52}$                   \\ \cline{2-3} 
                                      & VCNeF s. & $14577.00^{\pm 112.83}$                    \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $17965.47^{\pm 14.19}$                  \\ \hline
                                      & Ours     & $1846.729^{\pm 270.72}$                            \\ \cline{2-3} 
                                      & FNO      & $3733.10^{\pm 62.94}$                     \\ \cline{2-3} 
\multicolumn{1}{|c|}{161}               & VCNeF    & $6084.04^{\pm 9.37}$                    \\ \cline{2-3} 
                                      & Galerkin & $10295.78^{\pm 116.50}$                    \\ \cline{2-3} 
                                      & VCNeF s. & $19449.80^{\pm 113.73}$                  \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $24108.24^{\pm 6.45}$                 \\ \hline
                                      & Ours     & $2389.07^{386.02}$                           \\ \cline{2-3} 
                                      & FNO      & $4614.21^{\pm 97.52}$                    \\ \cline{2-3} 
\multicolumn{1}{|c|}{201}               & VCNeF    & $7584.48^{\pm 1.86}$                  \\ \cline{2-3} 
                                      & Galerkin & $13151.47^{\pm 93.95}$                   \\ \cline{2-3} 
                                      & VCNeF s. & $24252.38^{\pm 101.41}$                   \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $29986.81^{\pm 6.35}$                   \\ \hline
                                      & Ours     & $2773.019^{406.31}$                          \\ \cline{2-3} 
                                      & FNO      & $5572.07^{\pm 109.23}$                     \\ \cline{2-3} 
\multicolumn{1}{|c|}{240}               & VCNeF    & $8935.28^{\pm 7.08}$                  \\ \cline{2-3} 
                                      & Galerkin & $15600.60^{\pm 262.51}$                     \\ \cline{2-3} 
                                      & VCNeF s. & $29063.89^{\pm 79.58}$                 \\ \cline{2-3} 
\multicolumn{1}{|l|}{}                & OFormer  & $35900.51^{\pm 6.71}$                 \\ \hline
\end{tabular}
\caption{Comparison of the inference time. The dataset used is the Burger's one with $\nu=0.001$.}
\label{tab:inference_time}
\end{table}

\begin{table}[]
\begin{tabular}{llll}
\cline{2-4}
\multicolumn{1}{l|}{}                      & \multicolumn{3}{l|}{\# NNs weights }                                                                    \\ \hline
\multicolumn{1}{|l|}{Model}                & \multicolumn{1}{l|}{Advection}    & \multicolumn{1}{l|}{Burgers'} & \multicolumn{1}{l|}{Molenkamp} \\ \hline
\multicolumn{1}{|l|}{Ours}                 & \multicolumn{1}{l|}{188,549 / 214,461} & \multicolumn{1}{l|}{216,657}     & \multicolumn{1}{l|}{166,825}          \\ \hline
\multicolumn{1}{|l|}{Galerkin T.} & \multicolumn{1}{l|}{530,305}         & \multicolumn{1}{l|}{530,305}     & \multicolumn{1}{l|}{-}          \\ \hline
\multicolumn{1}{|l|}{FNO}                  & \multicolumn{1}{l|}{549,569}         & \multicolumn{1}{l|}{549,569}     & \multicolumn{1}{l|}{-}          \\ \hline
\multicolumn{1}{|l|}{U-Net}                & \multicolumn{1}{l|}{557,137}         & \multicolumn{1}{l|}{557,137}     & \multicolumn{1}{l|}{-}          \\ \hline
\multicolumn{1}{|l|}{MP-PDE}               & \multicolumn{1}{l|}{614,929}         & \multicolumn{1}{l|}{614,929}     & \multicolumn{1}{l|}{-}          \\ \hline
\multicolumn{1}{|l|}{OFormer}              & \multicolumn{1}{l|}{660,814}         & \multicolumn{1}{l|}{660,814}     & \multicolumn{1}{l|}{-}          \\ \hline
\multicolumn{1}{|l|}{VCNeF}                & \multicolumn{1}{l|}{793,825}         & \multicolumn{1}{l|}{793,825}     & \multicolumn{1}{l|}{1,594,005}          \\ \hline                         
\end{tabular}
\caption{Number of NNs' weights of the different architectures. In the first line of the 1D Advection case we show the number of weights of our model for $\zeta=0.01$/$\zeta$ varying.}
\label{tab:numb_weights}
\end{table}
\section{Methods used for comparison}
In Section \ref{sec:results} we compare our model to the following methods:
\\
\textbf{Fourier Neural Operator (FNO)} \cite{li2020fourier}: it is a particular case of a \textit{Neural Operator}, i.e., a class of models which approximate operators and that can thus perform mapping from infinite-dimensional spaces to infinite-dimensional spaces. The name comes from the the assumption that the Kernel of the operator layer is a convolution of two functions, which makes it possible to exploit the Fast Fourier Transform under particular circumstances.
\\
\textbf{Fourier Neural Operator (cFNO)} \cite{takamoto2023learning}: it is an adaptation of the FNO methodology which allows to add the PDE parameters as input.
\\
\textbf{Message Passing Neural PDE Solver (MP-PDE)} \cite{brandstetter2022message}: it leverages Graph Neural Networks (GNNs) for building surrogate models of PDEs. All the components are based on neural message passing which (as they show in the paper) representationally contain classical methods such as finite volumes, WENO schemes and finite differences.
\\
\textbf{U-Net} \cite{ronneberger2015u}: it is a method based on an Encoder-Decoder architecture with skip connections between the downsampling and upsampling procedures. Originally it was born for image segmentation and has been applied to the field of PDE solving \cite{gupta2022towards}.
\\
\textbf{Coordinate-based Model for Operator Learning (CORAL)} \cite{coral}: it is a method which leverages Neural Fields \cite{xie2022neural} for the solution of PDEs on general geometries and general time discretizations.
\\
\textbf{Galerkin Transformer (Galerkin)} \cite{cao2021choose}: it is a Neural Operator based on the self-attention mechanism from Transformers; it is based on a novel layer normalization scheme which mimics the Petrov-Galerkin projection. 
\\
\textbf{Operator Transformer (OFormer)} \cite{li2023transformer}: it is a Neural Operator which leverages the fact that the self-attention layer of Transformers is a special case of an Operator Layer as shown in \cite{kovachki2021neural} to build a PDE solver.
\\
\textbf{cOFormer}: it is an adaptation of the OFormer architecture which allows for the query of PDE parameters as inputs, following what is done in \cite{takamoto2022pdebench}. We 
\\
\textbf{Vectorized Conditional Neural Fields (VCNeF)} \cite{vcnef-hagnberger:2024}: it is a transformer based model which leverages neural fields to represent the solution of a PDE at any spatial point continuously in time. For the Molenkamp test we implemented the VCNeF method from the Git-Hub repository indicated in their paper: as they say in their paper, we use for the training of their model the One Cycle Scheduler with maximum learning rate at 0.2, initial division factor $0.003$ and final division factor $0.0001$; we use $500$ epochs, a batch size of $64$, an embedding size of $96$, $1$ transformer layer and $6$ modulation blocks. 
\label{sec:methods_comparison}
\\
\textbf{Physics-Informed Neural Networks (PINN)} \cite{pinns}: it is a class of methods which uses the physical knowledge of the system (in this case the PDE), in order to approximate the solution of the PDE. 
\\
The above methods have been implemented in \cite{vcnef-hagnberger:2024} (for results concerning 1D Advection and 1D Burgers) and \cite{takamoto2022pdebench} (for results concerning 2D Shallow Water) and we used their results as comparison with ours in Section \ref{sec:results}. 
\section{Ablation studies}
\subsection{The role of the ODE solver and of $\mathcal{L}_3$ in time generalization}
\label{subsec:solver_for_time}
In Figure \ref{fig:ODE_comparison_burger} we show the effect of the stage $q$ of the RK algorithm used to solve Equation (\ref{eq:ODE}) for the Burgers' dataset with $\nu=0.001$. We see that by increasing $q$ not only the nRMSE is lowered, but also the gap between the trajectory of $\Delta t = 0.05$ (used during training)  and $\Delta t = 0.01$ is decreased, i.e., the larger the $q$ the better the generalization in time during inference. This is particularly clear when looking at the nRMSE over time of $q=3$ and $q=4$, since for $\Delta t =0.05$ they are almost the same, while for $\Delta t =0.01$ it is noticeably lower when $q=4$. In Figure \ref{fig:ODE_comparison_advection} we do the same experiment with the Advection dataset: here only for $q=1$ there is a big gap between the prediction at $\Delta t=0.05$ and $\Delta t=0.01$. 
\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{image_results/stage_ODE_comparison_burgers.png}
  \caption{We show the nRMSE vs time when varying the stage $q$ of the RK algorithm to solve the ODE of Equation (\ref{eq:ODE}) on the Burgers' dataset with $\nu=0.001$. The same $q$ is used both at training and at inference. We see that by increasing $q$ not only we improve the predictions when using the same $\Delta t$ used during training ($\Delta t = 0.01)$ but we also get a better generalization in time.} 
  \label{fig:ODE_comparison_burger}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{image_results/stage_ODE_comparison_advection.png}
  \caption{We show the nRMSE vs time when varying the stage $q$ of the RK algorithm to solve the ODE of Equation (\ref{eq:ODE}) on the Advection dataset with $\zeta=0.1$. The same $q$ is used both at training and at inference. We see that by increasing $q$ not only we improve the predictions when using the same $\Delta t$ used during training ($\Delta t = 0.01)$ but we also get a better generalization in time.} 
  \label{fig:ODE_comparison_advection}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{image_results/stage_ODE_comparison_molenkamp.png}
  \caption{We show the nRMSE vs time when varying the stage $q$ of the RK algorithm to solve the ODE of Equation (\ref{eq:ODE}) on the Molenkamp dataset. The same $q$ is used both at training and at inference. We see that by increasing $q$ not only we improve the predictions when using the same $\Delta t$ used during training ($\Delta t = 0.01)$ but we also get a better generalization in time.} 
  \label{fig:ODE_comparison_molenkamp}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{image_results/no_l3_comparison_burger.png}
  \caption{nRMSE over time when using $\mathcal{L}_3$ in the training (red curves) and when not (blue curves, $\delta=0$). The presence of $\mathcal{L}_3$ at training improves the generalization in time. $\Delta t = 0.05$ is the training time-step.} 
  \label{fig:no_l3_burger}
\end{figure}
In Figure \ref{fig:ODE_comparison_molenkamp} once again we show the same pattern for the Molenkamp dataset: increasing the value of $q$ results in a better cabability of the model to generalize in time during inference by taking a smaller $\Delta t$.


In Figure \ref{fig:no_l3_burger} we show a comparison of the nRMSE over time on the Burgers' dataset with $\nu=0.001$ between using the full loss $\mathcal{L}_{tr}$ and switching off $\mathcal{L}_3$ by setting $\delta=0$: while for $\Delta t =0.05$ (the one used at training) the two curves are comparable, for $\Delta t =0.05$ a huge gap is present. This result is in line with the reasoning that $\mathcal{L}_3$ helps the model to generalize in time, as explained in \ref{subsec:generalization_in_time}.
%\subsection{Is a coupled system needed?}
%\label{subsec:coupled_system}
%However training $\varphi_\theta$ and $\psi_\theta$ together with $f_\theta$ pushes the NNs's weights towards a minimum where $\mathcal{E}$ allows for an easier modeling of the latent dynamics through $f_\theta$, since $\mathcal{E}$ and $f_\theta$ are built \textbf{at the same time}.
\section{Datasets}
\label{subsec:datasets}
Unless stated otherwise, the solutions of the PDEs described in this section used in the training come from \cite{takamoto2022pdebench}.
\subsection{1D Advection Equation}
\label{subsubsec:advection_dataset}
The 1D Advection Equation is a linear PDE which transports the initial condition with a constant velocity $\zeta$:
\begin{equation}
\label{eq:advection}
    \left\{
    \begin{aligned}
    &\partial_t s(\mathbf{x},t|\pmb{\mu})+\zeta \partial_x s(\mathbf{x},t|\pmb{\mu})= 0,\quad x\in(0,1),\, t\in(0,2]\\
    &  s(\mathbf{x},0|\pmb{\mu}) = s^0(\mathbf{x},\pmb{\mu}),\, x\in(0,1).
    \end{aligned}
    \right.
\end{equation}
Periodic boundary conditions are considered and as initial condition a super-position of sinusoidal waves is used:
\begin{equation}
    s^0(\mathbf{x},\pmb{\mu}) = \sum_{k_i=k_1,...,k_N}A_i \sin(k_i x+\phi_i),
\end{equation}
where $k_i = 2\pi\,{n_i}/L_x$ with ${n_1}$ being random integers, $L_x$ is the domain size, $A_i$ is a random number from the interval $[0,1]$ and $\phi_i$ is the phase chosen randomly in $(0,2\pi)$. We use 256 equidistant spatial points in the interval $[0,1]$ and for training 41 uniform timesteps in the interval $[0,2]$.
\subsection{1D Burgers' Equation}
The Burgers's equation is a non-linear PDE used in various modeling tasks such as fluid dynamics and traffic flows:
\begin{equation}
\label{eq:burger}
    \left\{
    \begin{aligned}
    &\partial_t s(\mathbf{x},t|\pmb{\mu})+\partial_x(s^2(\mathbf{x},t|\pmb{\mu})/2)-\nu/\pi\partial_{xx}s(\mathbf{x},t|\pmb{\mu})\quad x\in(0,1),\, t\in(0,2]\\
    &  s(\mathbf{x},0|\pmb{\mu}) = s^0(\mathbf{x},\pmb{\mu}),\, x\in(0,1),
    \end{aligned}
    \right.
\end{equation}
where $\nu$ is the diffusion coefficient. The initial conditions and the boundary conditions are the same as in Subsection \ref{subsubsec:advection_dataset}. We use 256 equidistant spatial points in the interval $[0,1]$ and for training 41 uniform timesteps in the interval $[0,2]$.
\subsection{2D Shallow Water Equations}
The 2D Shallow Water Equations are a system of hyperbolic PDEs derived from the Navier Stokes equations and describe the flow of fluids, primarily water, in situations where the horizontal dimensions (length and width) are much larger than the vertical dimension (depth):
\begin{equation}
\label{eq:sw}
    \begin{aligned}
    &\partial_t h+\partial_x h u + \partial_y h v = 0, \\
    &\partial_t h u +\partial_x \left( u^2 h + \frac{1}{2} g_r h^2 \right)+\partial_y u v h = -g_r h\partial_x b,  \\
    & \partial_t h v +\partial_x \left( v^2 h + \frac{1}{2} g_r h^2 \right)+\partial_y u v h = -g_r h\partial_y b,
    \end{aligned}
\end{equation}
where $u,v$ are the horizontal and vertical velocities, $h$ is the water depth and $b$ is a spatially varying bathymetry. $g_r$ is the gravitational acceleration. We use $128\times 128$ equidistant spatial points in the interval $[-1,1]\times[-1,1]$ and for training 21 uniform timesteps in the interval $[0,1]$, while the compared methods use 101 uniform timesteps in the interval $[0,1]$.
\subsection{2D Molenkamp test}
\label{subsec:molenkamp_test}
The Molenkamp test is a two dimensional advection problem, whose exact solution is given by a Gaussian function which is transported trough a circular path without modifying its shape. Here we add a reaction term which makes the Gaussian shape decay over time:
\begin{equation}
\label{eq:molenkamp}
    \begin{aligned}
    &\partial_t q(x,y,t)+u\partial_x q(x,y,t) + v\partial_y q(x,y,t)+\lambda_3 q(x,y,t) = 0 \\
    &q(x,y,0) = \lambda_1\,0.01^{\lambda_2 h(x,y,0)^2},\quad h(x,y,0)=\sqrt{(x-\lambda_4+\frac{1}{2})^2+(y-\lambda_5)^2},
    \end{aligned}
\end{equation}
with $u=-2\pi y$ and $v = 2\pi x$ and $(x,y)\in[-1,1]$. For this problem an exact solution exists:
\begin{equation}
    \begin{aligned}
        &q(x,y,t) = \lambda_1 0.01^{\lambda_2 h(x,y,t)^2}\exp^{-\lambda_3 t},\\
        &h(x,y,t) = \sqrt{(x-\lambda_4+\frac{1}{2}\cos(2\pi t))^2+(y-\lambda_5-\frac{1}{2}\sin(2\pi t))^2}.
\end{aligned}
\end{equation}
\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{image_results/molenkamp.pdf}
  \caption{We show the predictions of our model over time on the Molenkamp test dataset for $4$ different combination of parameters $\pmb{\mu}_1$, $\pmb{\mu}_2$, $\pmb{\mu}_3$ and $\pmb{\mu}_4$. The vertical colorbar refers to the fields $s_r(\mathbf{x},t|\pmb{\mu})$ and $\tilde{s}_r(\mathbf{x},t|\pmb{\mu})$ (prediction and ground truth), while the horizontal one to the relative error $e_r$ of Equation \ref{eq:relative_error}.} 
  \label{fig:molenkamp_fields}
\end{figure}
\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{image_results/SW.pdf}
  \caption{We show the predictions of our model over time on the Shallow-Water test dataset for $4$ different initial conditions $s^0_{r,1}$, $s^0_{r,2}$, $s^0_{r,3}$ and $s^0_{r,4}$. The vertical colorbar refers to the fields $\tilde{s}_r(\mathbf{x},t|\pmb{\mu})$ and $s_r(\mathbf{x},t|\pmb{\mu})$ (prediction and ground truth), while the horizontal one to the relative error $e_r$ of Equation \ref{eq:relative_error}.} 
  \label{fig:SW}
\end{figure}
The PDE depends on $5$ parameters $\lambda_1,...,\lambda_5$, which control the magnitude of the initial Gaussian, the size of the cloud, the spees of decay, the initial x coordinate and the initial y coordinate, respectively. In Table we show the parameters' ranges as it is done in \cite{Alsayyari_2021}: $\lambda_1\in[1,20]$, $\lambda_2\in[2,4]$, $\lambda_3\in[1,5]$, $\lambda_4\in[-0.1,0.1]$, $\lambda_5\in[-0.1,0.1]$. We use $128\times 128$ equidistant spatial points in the interval $[-1,1]\times[-1,1]$ and for training 21 uniform timesteps in the interval $[0,1]$.
\subsection{Metrics}
 We use as testing metric the Normalized-Mean-Squared-Root-Error (nRMSE), defined as
\begin{equation}
\label{eq:nRMSE}
    \text{nRMSE} =\frac{1}{N_u\,N_{\pmb{\mu}}F}\,\sum_{i=1}^{N_u}\sum_{p=1}^{N_{\pmb{\mu}}}\sum_{j=1}^F \frac{||s_r(\mathbf{x},t_j|\pmb{\mu}_p,s_{r,i}^0)-\tilde{s}_r(\mathbf{x},t_j|\pmb{\mu}_p,s_{r,i}^0))||_2}{||s_r(\mathbf{x},t_j|\pmb{\mu}_p,s_{r,i}^0)||_2},
\end{equation}
where $N_u$, $N_{\pmb{\mu}}$ and $F$ are the number of initial conditions, parameter instances and time steps used at testing, respectively; $s^0_{r,i}$ stands for the $i$th initial condition used at testing. We also define the \textit{relative error} $e_r$, i.e., a more spatially descriptive error measure between the prediction of a field $\tilde{s}_r(\mathbf{x},t|\pmb{\mu})$ and the expected field $s_r(\mathbf{x},t|\pmb{\mu})$:
\begin{equation}
\label{eq:relative_error}
    e_r = \frac{|s_r(\mathbf{x},t|\pmb{\mu})-\tilde{s}_r(\mathbf{x},t|\pmb{\mu})|}{||s_r(\mathbf{x},t|\pmb{\mu})||_2},
\end{equation}
where the numerator is the point-wise absolute value of the difference between $\tilde{s}_r(\mathbf{x},t|\pmb{\mu})$ and $s_r(\mathbf{x},t|\pmb{\mu})$ (hence it has the same dimensionality as $s_r(\mathbf{x},t|\pmb{\mu})$), while the denominator is a scalar.
\section{Additional images}

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{image_results/advection_x_t.pdf}
  \caption{We show the predictions of our model on the parametric Advection dataset for $2$ different initial conditions and $4$ different velocities: $\zeta=0.4$, $\zeta=0.7$, $\zeta=2.0$ and $\zeta=4.0$. Each plot is a heat map with time $t$ on the horizontal axis and space $x$ on the vertical axis. The vertical colorbar refers to the fields $\tilde{s}_r(\mathbf{x},t|\pmb{\mu})$ and $s_r(\mathbf{x},t|\pmb{\mu})$ (prediction and ground truth), while the horizontal one refers to the relative error $e_r$ of Equation \ref{eq:relative_error}.} 
  \label{fig:advection_x_t}
\end{figure}
In Figure \ref{fig:molenkamp_fields} we show the predictions of our model on the Molenkamp test for $4$ different parameter values: $\pmb{\mu}_1= [2.4522 ,2.3731 ,  2.7912  ,0.0533,0.01250]$, $\pmb{\mu}_2=[19.8578  ,   2.5791  , 1.9388 ,  0.0959, -0.0857]$, $\pmb{\mu}_3=[11.7423 ,   3.9285   , 2.5638 ,  0.0384,  0.0200]$ and $\pmb{\mu}_4=[16.8555  , 3.4449  , 2.6506  , 0.0502 , 0.0423]$.
In Figure \ref{fig:SW} we show the predictions of our model on the Shallow-Water test case for $4$ different initial conditions $s^0_{r,1}$, $s^0_{r,2}$, $s^0_{r,3}$ and $s^0_{r,4}$. 
\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{image_results/burgers_x_t.pdf}
  \caption{We show the predictions of our model on the Burgers' dataset when $\nu=0.001$. The two rows correspond to two different initial conditions.} 
  \label{fig:burgers_x_t}
\end{figure}
In Figure \ref{fig:advection_x_t} we show the predictions of our model on the 1D Advection test case for $2$ different initial conditions and  $4$ different velocities: $\zeta=0.4$, $\zeta=0.7$, $\zeta=2.0$ and $\zeta=4.0$. Finally, in Figure \ref{fig:burgers_x_t} we show the prediction for the Burgers case when $\nu=0.001$ for two different initial conditions.

