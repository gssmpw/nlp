\section{Methodology}
\label{sec:methodology}
\subsection{Mathematical background}
Let $\mathcal{V}$, $\mathcal{S}$ and $\mathcal{S}^0$ be the functional spaces of the boundary condition functions, PDE solution functions and the initial condition functions of a given PDE, respectively. These functions are required to satisfy some properties, such that:\begin{equation}
 \left\{
\begin{aligned}
& \mathcal{V} = \{v|v:\partial\mathcal{D}_{\mathbf{x}}\times\mathcal{D}_t\rightarrow\mathbb{R}^m\,;\lVert v_i \rVert_2<\infty\,\forall\,i\in[1,\cdots,m]\,\,; v\in\mathcal{C}^2\},\\
&  \mathcal{S} = \{s|s:\mathcal{D}_{\mathbf{x}}\times\mathcal{D}_t\rightarrow\mathbb{R}^m\,;\lVert s_i \rVert_2<\infty\,\forall\, i \,\in[1,\cdots,m]\,\,; s\in\mathcal{C}^2\},\\
& \mathcal{S}^0\subseteq \mathcal{S}^t = \{s(x,t=\tilde{t}|\pmb{\mu})|s(x,t=\tilde{t}|\pmb{\mu}):\mathcal{D}_{\mathbf{x}}\rightarrow\mathbb{R}^m, \forall \tilde{t}\in \mathcal{D}_t, \pmb{\mu}\in \mathcal{D}_{\pmb{\mu}}\},
\end{aligned}
\right.
\end{equation}
where $\mathcal{S}^t$ is the set of all possible states, $ \mathcal{D}_{\mathbf{x}}\subseteq\mathbb{R}^n$ is the domain of independent variables $\mathbf{x}$, $\mathcal{D}_{t}\subseteq\mathbb{R}^+$ is the temporal domain and $\partial\mathcal{D}_{\mathbf{x}}\subseteq\mathbb{R}^n$ the boundary of $\mathcal{D}_{\mathbf{x}}$ and $\mathcal{D}_{\pmb{\mu}}\subseteq\mathbb{R}^z$ is a domain for the vector $\pmb{\mu} = (\mu_1,\mu_2,\cdots, \mu_z)$, containing information about the PDE parameters, the geometry of the problem and whatever quantity defines the physical system. %In principle the space $\mathcal{D}_{t}$ may be considered as an additional dimension of $\mathcal{D}_{\mathbf{x}}$. 
We are interested in solving general PDEs of the form of:
\begin{equation}
    \left\{
    \begin{aligned}
    &\hat{\mathcal{N}}(s(\mathbf{x},t|\pmb{\mu}),\mathbf{x},t,\pmb{\mu})= g(\mathbf{x},t,\pmb{\mu} ) \\
    &s(\tilde{\mathbf{x}},t|\pmb{\mu}) = v(\tilde{\mathbf{x}},t,\pmb{\mu}) \\
    &s(\mathbf{x},t=0|\pmb{\mu}) = s^0(\mathbf{x},\pmb{\mu}),
    \end{aligned}
    \right.
\label{eq:PDEsystem}
\end{equation}
where $\hat{\mathcal{N}}(s(\mathbf{x},t|\pmb{\mu}),\mathbf{x},t,\pmb{\mu})$ is a (typically) nonlinear integro-differential operator, $g(\mathbf{x}, t ,\pmb{\mu})$ is the forcing term, $s\in\mathcal{S}$ is the PDE solution, $v\in\mathcal{V}$ and $s^0\in\mathcal{S}^0$ are the boundary and initial conditions, $\mathbf{x}\in\mathcal{D}_{\mathbf{x}}$, $ \pmb{\mu}\in\mathcal{D}_{\pmb{\mu}}$, $\tilde{\mathbf{x}}\in\partial\mathcal{D}_{\mathbf{x}}$, $ t\in\mathcal{D}_{t}$. The different elements of the system of equations (\ref{eq:PDEsystem}) have either an explicit dependency on $\pmb{\mu}$, signaled by $(\cdot,\pmb{\mu})$, or an implicit dependency, signaled by $(\cdot|\pmb{\mu})$. Although (\ref{eq:PDEsystem}) describes a very general PDE system and our method description addresses this fully general case, in the experiments shown in Section \ref{sec:results} we fix $v(\tilde{\mathbf{x}},t, \pmb{\mu})$, i.e., it is not an input to the NN, we choose $g(\mathbf{x},t,\pmb{\mu} ) = 0$ and $\hat{\mathcal{N}} = \hat{\mathcal{N}}(s(\mathbf{x},t|\pmb{\mu}),\mathbf{x},\pmb{\mu})$, i.e., without explicit $t$ dependence.

In the context of surrogate modeling for parametric PDEs, one usually approximates by means of a NN either the \textit{Solution Operator} $\hat{\mathcal{S}}$ (\textbf{global} approach) or the \textit{Evolution Operator} $\hat{\mathcal{H}}$ (\textbf{autoregressive} approach), where 
\begin{equation}
\hat{\mathcal{S}}:\mathcal{S}^0\times\mathcal{D}_{t}\times\mathcal{D}_{\pmb{\mu}}\rightarrow\mathcal{S}\quad\text{and}\quad \hat{\mathcal{H}}:\mathcal{S}^t\times\mathcal{D}_{\Delta t}\times\mathcal{D}_{\pmb{\mu}}\rightarrow\mathcal{S}^t,
\label{eq:global&auto_op}
\end{equation}
with $\mathcal{D}_{\Delta t}\subseteq\mathbb{R}^+$. When approximating $\hat{\mathcal{S}}$, the NN is given as input $(s^0,t,\pmb{\mu})$ to output $s(\mathbf{x},t|\pmb{\mu})$, while when approximating $\hat{\mathcal{H}}$, the NN is given as input $(s(\mathbf{x},t=\tilde{t}|\pmb{\mu}), \Delta t, \pmb{\mu})$ to output $s(\mathbf{x},\tilde{t}+\Delta t|\pmb{\mu})$. While the former can approximate the solution $s$ at any point in time $t$ with just one call of the solver, the latter requires advancing iteratively in time by predicting the solution at the next time step from the solution at the previous time step as input, starting from $s^0$. Although the global approach has a (potential) advantage in terms of computational speed, we propose an autoregressive method for the following reasons:
\begin{itemize}
    \item Most PDEs represent causal physical phenomena, hence their solution evolution at time $t$ only depends on the system state at $t$. Therefore, as it is done in classical numerical solvers, only the solution $s$ at time $t$ is necessary for the prediction of $s$ at time $t+\Delta t$. This fact is not respected by global approaches.
    \item Global approaches require in general a high number of NN weights, as a mapping for arbitrary $t$ is required, contrary to autoregressive methods, as the state $s(t)$ carries more information than $s^0$ to predict $s(t+\Delta t)$.
    %\item In the global approach, if during the training $t\in[0,\hat{T}]$, the NN will not generalize to $t>\hat{T}$.
\end{itemize}
While the two approaches are clearly different from a theoretical perspective, from a purely \textit{architectural} point of view they are very similar, as an architecture conceived as global can always be used autoregressively and vice-versa. It is primarily the training strategy that determines whether a global or autoregressive logic is followed.

In what follows we show how to approximate with a NN the (latent) Evolution Operator that governs the dynamics of the \textbf{reduced} space to which the full space $\mathcal{S}$ is mapped.
\subsection{Discretization}
In order to work with solution functions $s$ computationally it is necessary
to discretize the independent variable (typically spatial at least), temporal and parametric domains.  We thus define:
$\mathbf{X} = \{x_k|x_k\in \mathcal{D}_{\mathbf{x}}, x_k = (x_k^1,\cdots,x_k^n),k=0,\cdots,N_\mathbf{x}\}$ as the set of points inside the domain of independent variables; %We are thus going from a \textit{continuous} representation $\mathcal{D}_{\mathbf{x}}$ to a \textit{discrete} representation $\mathbf{X}$.
$\partial\mathbf{X} = \{\tilde{x}_k|\tilde{x}_k\in \partial\mathcal{D}_{\mathbf{x}}, \tilde{x}_k = (\tilde{x}_k^1,\cdots,\tilde{x}_k^n), k=0,\cdots,N_{\tilde{\mathbf{x}}}\}$ as the set of points on the boundary of the \textit{independent} variables of the domain $\mathcal{D}_{\mathbf{x}}$;
$\pmb{\mathcal{X}} = \mathbf{X}\cup\partial\mathbf{X}$,
$\pmb{M} = \{\pmb{\mu}\in\mathcal{D}_{\pmb{\mu}},\pmb{\mu}=(\mu_0,\mu_1,\cdots,\mu_z)\}$ as the set of parameter points $\pmb{\mu}$;
$\mathbf{T} = \{t|t\in\mathcal{D}_{t},t=(t_0,t_1,\cdots,t_{F})\}$  as the set of discrete points in time.
We also define the \textit{solution} and the \textit{initial condition} sets, as the sets of functions living in $\mathcal{S}$ and $\mathcal{S}^0$ discretized on $\pmb{\mathcal{X}}$ and $\pmb{T}$:
$\mathcal{S}_r=\{s_{r}(\mathbf{x},t|\pmb{\mu})|\mathbf{x}\in\pmb{\mathcal{X}}, t\in\mathbf{T}, \pmb{\mu}\in\pmb{M}\}\subset{\mathbb{R}^{|\pmb{\mathcal{X}}|\times n\times m}}$ and $\mathcal{S}^{0}_r=\{s^0_{r}(\mathbf{x},\pmb{\mu})|\mathbf{x}\in\pmb{\mathcal{X}},\pmb{\mu}\in\pmb{M}\}\subset\mathcal{S}_r$, where $r$ is a subscript that indicates a discretized representation of $s$. Obviously, $s_r(\mathbf{x},t_0|\pmb{\mu}) = s_r^0(\mathbf{x},\pmb{\mu})$. We will signal the implicit parameter dependency of $s_r$ using the notation $s_r(\mathbf{x},t|\pmb{\mu})$. In principle $s_r(\mathbf{x},t|\pmb{\mu},s_r^0)$ but for notational ease we will drop the implicit dependence on $s_r^0$. Although we are using a finite difference approach for discretization, our methodology is fully general to other discretization schemes too (finite volumes, finite elements, etc.)
\subsection{Reduced space and (latent) Neural ODEs}
\label{subsec:AE&NODE}
We want to build a method that at inference time maps the initial condition $s^0$ into its reduced representation and then evolves it in time (according to the PDE parameters) autoregressively.
The \textbf{first} building block of our methodology is the mapping between the full and the reduced space by an AutoEncoder. Let $\mathcal{E}$ be the \textit{reduced} (\textit{latent}) \textit{set}
\begin{equation}
    \mathcal{E}=\{\varepsilon(t|\pmb{\mu})|\varepsilon(t|\pmb{\mu})=
    (\varepsilon_1(t|\pmb{\mu}), \cdots,\varepsilon_{\lambda}(t|\pmb{\mu})), t\in\mathcal{D}_t, \pmb{\mu}\in\mathcal{D}_{\pmb{\mu}}\}\subset{\mathbb{R}^{\lambda}},
\end{equation}
with $\lambda\ll|\mathcal{X}|\cdot nm$ being the dimension of the latent space. Each time-dependent vector $\varepsilon(t|\pmb{\mu})\in\mathcal{E}$ has a one-to-one correspondence with a given solution function $s\in\mathcal{S}$ (implicitly depending on the parameter $\pmb{\mu}$), so that by computing the dynamics of $\varepsilon(t|\pmb{\mu})$ we can reconstruct the original trajectory of $s(\mathbf{x},t|\pmb{\mu})$. Each dimension $\varepsilon_i(t)$ is an \textit{intrinsic representation} of the corresponding function $s$ and embodies the correlations, symmetries and fundamental information about the original object $s$ (for a deeper understanding of the nature and the desiderata of a latent representation, see \cite{Eastwood2018AFF, Higgins2018TowardsAD}). Although we will work with discretized functions belonging to $\mathcal{S}_r$, each vector $\varepsilon(t|\pmb{\mu})$ is in principle associated with the original \textbf{continuous} function belonging to $\mathcal{S}$ (i.e., $\varepsilon(t|\pmb{\mu})$ should be independent of the discretization of $\mathcal{S}$).

The mathematical operators mapping $\mathcal{S}$ to $\mathcal{E}$ and viceversa are the \textit{Encoder} $\varphi$ and the \textit{Decoder} $\psi$, such that:
\begin{equation}
    \varphi: \mathcal{S}\rightarrow\mathcal{E}\quad\text{and}\quad\psi: \mathcal{E}\rightarrow\mathcal{S},
    \label{eq:AE}
\end{equation}
with $\varphi\circ\psi=\psi\circ\varphi= \mathbbm{1}$, together forming the AutoEncoder. We approximate $\varphi$ and $\psi$ by two NNs, respectively $\varphi_\theta:\mathcal{S}_r\rightarrow\mathcal{E}$ and $\psi_\theta:\mathcal{E}\rightarrow\mathcal{S}_r$.
The \textbf{second} building block concerns the dynamics of the vectors $\varepsilon$ belonging to the reduced set $\mathcal{E}$. We assume that the temporal dynamics of $\mathcal{E}$ follows an ODE:
\begin{equation}
    \frac{d}{dt}\varepsilon(t|\pmb{\mu})=f(\varepsilon(t|\pmb{\mu}),\pmb{\mu}),\quad\text\quad f\in\mathcal{F}:\mathcal{E}\times\mathcal{D}_{\pmb{\mu}}\rightarrow\mathcal{E},
    \label{eq:ODE}
\end{equation}
where $\pmb{\mu}\in\pmb{M}$ is the vector of PDE parameters.$f$ does not depend \textit{explicitly} on $t$ since the PDEs we work with do not have explicit time dependence, making the dynamics of $\mathcal{E}$ an \textit{autonomous system}. If instead $\hat{\mathcal{N}}$ or $g(\mathbf{x},t,\pmb{\mu})$ had an explicit dependence on $t$, we would have $f = f(\varepsilon(t|\pmb{\mu}),\pmb{\mu},t)$ and would treat $t$ in the model simply as an additional dimension of $\pmb{\mu}$. We can now define the \textit{Processor}
\begin{equation}
\label{eq:processor}
    \pi:\mathcal{E}\times\mathcal{F}\times\mathcal{D}_{\pmb{\mu}}\times\mathcal{D}_{\Delta t}\rightarrow \mathcal{E},
\end{equation} as the mathematical operator that advances the latent vector $\varepsilon(t|\pmb{\mu})$ in time according to Equation (\ref{eq:ODE}):
\begin{equation}
    \pi(\varepsilon(t_i|\pmb{\mu}),f,\pmb{\mu},\Delta t_{i+1,i}) = \varepsilon(t_i|\pmb{\mu})+\int_{t_i}^{t_{i+1}}f(\varepsilon(t|\pmb{\mu}),\pmb{\mu}) \,dt,
    \label{eq:NODE}
\end{equation}
with $\Delta t_{i+1,i} = t_{i+1}-t_i$ and the $\pmb{\mu}$ dependency being controlled by $f$. Clearly, $\pi(\varepsilon(t_i|\pmb{\mu}),f,\pmb{\mu},\Delta t_{i+1,i})= \varepsilon(t_{i+1}|\pmb{\mu})$. In summary, $\varphi$ and $\psi$ describe the mapping between the full order and reduced order representation of the system, while $\pi$ describes the \textit{dynamics} of the system. For notational convenience, we will \textbf{drop} the dependence of $\pi$ on $f$.

We now define $f_\theta$ as a NN which approximates $f$ and $\pi_\theta$ as the discrete approximation of $\pi$ which advances in time the vectors belonging to $\mathcal{E}$ by solving the integral of Equation (\ref{eq:NODE}), using known integration schemes (see in~\ref{sec:RK}):
\begin{equation}
   \pi_\theta(\varepsilon(t_{i}|\pmb{\mu}), \pmb{\mu},\Delta t_{i+1,i}) = ODESolve(\varepsilon(t_{i}|\pmb{\mu}),\pmb{\mu},\Delta t_{i+1,i}),
\end{equation}
as it is done in Neural ODEs (NODEs) \cite{chen2018neural,Kidger2022OnND}.
For example, in the case of the explicit Euler scheme \cite{Ascher1998ComputerMF}:
\begin{equation}
   \pi_\theta(\varepsilon(t_{i}|\pmb{\mu}),\pmb{\mu},\Delta t_{i+1,i}) = \varepsilon(t_i|\pmb{\mu})+\Delta t_{i+1,i}\,f_\theta(\varepsilon(t_{i}|\pmb{\mu}),\pmb{\mu}).
\end{equation}
%In Section \ref{subsec:solver_for_time} we will show the effect of the integration scheme used by $\pi_\theta$, comparing different stages of the Runge-Kutta (RK) integration schemes \cite{Ascher1998ComputerMF}. To review the RK schemes we refer to Appendix .
The Processor $\pi$ is the equivalent of the Evolution Operator $\hat{\mathcal{H}}$ of Equation (\ref{eq:global&auto_op}) but acting on the reduced space $\mathcal{E}$ of \textbf{discrete} intrinsic representations: as such $\pi$ does not need to be equipped with the notion of (spatial) discretization invariance as in the case of Neural Operators. 
\begin{figure*}
  \centering
  \includegraphics[width=1.0\textwidth]{image_methodology/inference.pdf}
  \caption{Workings of our proposed method at \textbf{testing} time. The initial condition $s^0_r$ is mapped trough the Encoder $\varphi_\theta$ into its latent representation $\varepsilon_0^{\pmb{\mu}}$. Subsequently the vector $\varepsilon_0^{\pmb{\mu}}$ is advanced in time autoregressively by repeated evaluation of the processor $\pi_\theta$, conditioned to the vector of parameters $\pmb{\mu}$ and to the size of the temporal jump $\Delta t_{i+i,i}$. The Decoder $\psi_\theta$ is used to map back each predicted latent vector $\varepsilon_{i}^{\pmb{\mu},i}$ into the corresponding field $\tilde{s}_r(\mathbf{x},t_i|\pmb{\mu})$. Notice that $\varphi_\theta$ is applied only to the initial condition $s_r^0$.}
  \label{fig:inference}
\end{figure*}
\subsection{Training of the model}
\label{subsec:training}
The model we defined in Section \ref{subsec:AE&NODE} requires the optimization of \textbf{two} training processes which we consider \textbf{coupled}: the training of the AE which regulates the mappings between $\mathcal{S}_r$ and $\mathcal{E}$ and the training of $\pi_\theta$ which regulates the latent dynamics described by Equation (\ref{eq:ODE}). The latter can be approached by combining a \textit{Teacher Forcing (TF)} and an \textit{Autoregressive (AR)} strategy. We thus define: 
\begin{equation}
    \mathcal{L}_{1,i} = \frac{||s_r(\mathbf{x},t_i|\pmb{\mu})-\psi_\theta\circ\varphi_\theta(s_r(\mathbf{x},t_i|\pmb{\mu})||_2}{||s_r(\mathbf{x},t_i|\pmb{\mu})||_2}
\end{equation}
as the term which governs the AE training. By introducing
\begin{equation}
    \left\{
    \begin{aligned}
    &\varepsilon_i^{\pmb{\mu}} = \varphi_\theta(s(\mathbf{x},t_i|\pmb{\mu})),\\
    &\varepsilon_{i}^{\pmb{\mu},k} = \pi_\theta(\cdot,\pmb{\mu},\Delta t_{i,i-1})\circ\cdots\circ\pi_\theta(\varepsilon_{i-k}^{\pmb{\mu}},\pmb{\mu},\Delta t_{i-k+1,i-k}), \\
    \end{aligned}
    \right.
\label{eq:notation_simplified}
\end{equation}
we define the two terms which govern the latent dynamics:
\begin{equation}
    \left\{
    \begin{aligned}
    &\mathcal{L}_{2,i}^{T,k_1} = \frac{||\varepsilon_i^{\pmb{\mu}}-\varepsilon_{i}^{\pmb{\mu},k_1}||_2}{||\varepsilon_i^{\pmb{\mu}}||_2},\\
    &\mathcal{L}_{2,i}^{A,k_2} =  \frac{||\varepsilon_i^{\pmb{\mu}}-\varepsilon_i^{\pmb{\mu},i}||_2}{||\varepsilon_i^{\pmb{\mu}}||_2},
    \end{aligned}
    \right.
\label{eq:L2_loss}
\end{equation}
where $T$ identifies the TF approach and $A$ the AR one. The term $\mathcal{L}_{2,i}^{T,k_1}$ (TF), penalizes the difference between the expected latent vector $\varepsilon_{i}^{\pmb{\mu}}$ and the predicted latent vector $\varepsilon_{i}^{\pmb{\mu},k_1}$ obtained by applying autoregressively $\pi_\theta$ to $\varepsilon_{i-k_1}^{\pmb{\mu}}$, \textit{which comes from encoding} the \textbf{true} \textit{field} $s_r(\mathbf{x},t_{i-k_1}|\pmb{\mu})$ (hence the name \textit{Teacher-Forcing}, as the \textbf{true} input $k_1$ steps earlier, is fed into the NN). Using TF when training autoregressive models is known to cause potential \textit{distribution shift} \cite{brandstetter2022message}, representing a problem at inference time: as depicted in Figure \ref{fig:inference}, at testing time the input of $\pi_\theta$ is the previous output of $\pi_\theta$ starting from $\varepsilon_0^{\pmb{\mu}}$, contrary to what $\mathcal{L}^{T,k_1}_{2,i}$ penalizes (unless $k_1= F$, i.e., the full length of the time series). To avoid this mismatch between training and inference, we introduced $\mathcal{L}^{A,k_2}_{2,i}$ (AR), which penalizes the difference between the expected latent vector $\varepsilon_{i}^{\pmb{\mu}}$ and the predicted latent vector $\varepsilon_i^{\pmb{\mu},i} = \pi_\theta(\cdot,\pmb{\mu},\Delta t_{i,i-1})\circ\cdots\circ\pi_\theta(\varepsilon_{0}^{\pmb{\mu}},\pmb{\mu},\Delta t_{1,0})$ obtained by repeated application of $\pi_\theta$ \textit{starting from the encoded representation of the initial condition} $s_r^0$, as it is done at testing time. $k_2$ denotes the number of steps in time from which the \textbf{gradients of the backpropagation algorithm} flow, i.e., the predicted latent vector at time $t_i$ is obtained by encoding the initial condition $s_r^0$ and fully evolving it autoregressively (by applying $\pi_\theta$ $i$ times), but the gradients of the backpropagation algorithm \textbf{flow only} from the predicted latent vector at time $t_{i-k_2}$ up to $t_i$. It follows that $\mathcal{L}^{T,k_1}_{2,i}$ and $\mathcal{L}^{A,k_2}_{2,i}$ are computed in the same way only if $k_1=k_2= F$. By truncating the gradients flow at time $t_{i-k}$, we are implementing a form of Truncated Backpropagation Through Time (TBPTT) as it is usually done for gradients stability purpuses when training Recurrent Neural Networks (RNNs) \cite{pmlr-v115-aicher20a}.
%In Appendix \ref{subsubsec:instabilities} we explain some technical details such as how we deal with the latent vector at $t_2$ if $k=5$.
%We stress that we decided to train the two processes together while in principle it is perfectly doable to train first the AE and then the NODE, as explained in \ref{subsec:coupled_system}. 
Figure \ref{fig:inference} shows a summary of the method at testing time, where the predicted solution is computed as
$\tilde{s}_r(\mathbf{x},t_i|\pmb{\mu}) =
     \psi_\theta\circ \pi_\theta(\cdot,\pmb{\mu},\Delta t_{i,i-1})\circ\cdots\circ\pi_\theta(\cdot,\pmb{\mu},\Delta t_{1,0})\circ\varphi_\theta(s^0_r)$.
\subsection{Combining Teacher Forcing with Autoregressive}
\label{subsec:TF_&_AR}
\textbf{How} do we combine AR and TF strategies in practice? We start by considering the loss $\mathcal{L}^{T,1}_{2,i}$, the simplest form of TF strategy with the advantage of being computationally efficient and stable, but at the cost of making the training agnostic to the autoregressive nature of the model at testing time and not addressing the \textit{accumulation of errors} which is typical of autoregressive models. $\mathcal{L}^{A,k_2}_{2,i}$ instead, regardless of the chosen $k_2$, already reflects during training the autoregressive modality used at testing; it has however the disadvantage of being computationally more demanding (for $k_2>1$) and more difficult to train the larger $k_2$ is. If $k_1>1$, $\mathcal{L}^{T,k_1}_{2,i}$ introduces a certain degree of 'autoregressiveness' as well, although the latent vector at time $t_{i-k}$ is still provided by the true solution. Among the several possible training strategies, here we list the two we used, with $\mathcal{L}_{2,i}=\beta \mathcal{L}^{T,k_1}_{2,i}+\gamma \mathcal{L}^{A,k_2}_{2,i}$, where $\beta$ and $\gamma$ weigh the importance of the terms:
\begin{enumerate}
    \item set $\beta=1$, $\gamma=0$ and $k_1=1$, using only the TF term.
    \item set $\beta=1$, $\gamma=1$, $k_1=1$ and dynamically increase $k_2$ during the training, starting with $k_2 = 1$. This strategy has the advantage of taking into account the AR term gradually during the training.
\end{enumerate}
Our experiments have shown that although for some systems strategy $1$ is enough, more complex datasets require using strategy $2$, mainly due to two separate behaviors in our observations. First, that in the \textbf{early stages} of the training, $\mathcal{L}_{1,i}$, $\mathcal{L}^{T,1}_{2,i}$ and $\mathcal{L}^{A,1}_{2,i}$ play the important role of building a latent space whose dynamics is described by Equation (\ref{eq:ODE}); and second, that in the \textbf{later stages} of the training, with $k_2$ becoming larger (and the computed gradients more complex), the autoregressive nature of the model is increasingly taken into account, with $\pi_\theta$ becoming more robust to the accumulation of errors.
\subsection{Generalization in the time domain}
\label{subsec:generalization_in_time}

\begin{figure*}
  \centering
  \includegraphics[width=0.8\textwidth]{image_methodology/architecture.pdf}
  \caption{A representation of the \textbf{training} procedure. \textbf{a)} The time series of fields $s_r(\mathbf{x},t_i|\pmb{\mu})$, with $i\in\{0,F\}$, is processed by the Encoder $\varphi_\theta$ and the corresponding latent vectors $\varepsilon_i^{\pmb{\mu}}$ are obtained; these are subsequently mapped back to the full space by means of the Decoder $\psi_\theta$ which generates the time series of reconstructed fields $\tilde{s}_r(\mathbf{x},t_i|\pmb{\mu})$, allowing for the computation of $\mathcal{L}_1$. \textbf{b)} The Processor $\pi_\theta$ receives as input the sequence of latent vectors $\varepsilon_i^{\pmb{\mu}}$ with $i\in\{0,F-1\}$ and predicts the latent vectors $\varepsilon_{i}^{\pmb{\mu},1}$ with $i\in\{1,F\}$. $\mathcal{L}_2^{T,1}$, where $T$ stands for \textit{Teacher-Forcing}, is thus computed with inputs $\varepsilon_i^{\pmb{\mu}}$ and $\varepsilon_{i}^{\pmb{\mu},1}$. \textbf{c)} The Processor $\pi_\theta$ is applied autoregressively to the initial latent vector $\varepsilon_0^{\pmb{\mu}}$ and the whole time series of vectors $\varepsilon_{i}^{\pmb{\mu},i}$ is reconstructed with $i\in\{1,F\}$; $\mathcal{L}_2^{A,k_2}$, where $A$ stands for \textit{Autoregressive}, is thus computed with inputs $\varepsilon_i^{\pmb{\mu}}$ and $\varepsilon_{i}^{\pmb{\mu},i}$. \textbf{d)} The Processor $\pi_\theta$ takes as input the sequence of latent vectors $\varepsilon_i^{\pmb{\mu}}$ with $i\in\{0,F-1\}$ and outputs for each $\varepsilon_i^{\pmb{\mu}}$ an intermediate vector $\varepsilon^{\pmb{\mu},1}_{m}$ with a time-step $\Delta t_{m,i-1}$ randomly sampled from $[0,\Delta t_{i,i-1}]$. Last, $\pi_\theta$ advances in time each $\varepsilon^{\pmb{\mu},1}_{m}$ with a time-step of $\Delta t_{i,i-1}-\Delta t_{m,i}$ to get the predicted vectors $\tilde{\varepsilon}_{i}^{\pmb{\mu}}$; $\mathcal{L}_3$ is thus computed with inputs $\varepsilon_i^{\pmb{\mu}}$ and $\tilde{\varepsilon}_{i}^{\pmb{\mu}}$.}
  \label{fig:architecture}
\end{figure*}

As shown in Figure \ref{fig:inference_in_time} we expect our models to be trained on a given set of time-steps, but we want them to generalize to time-steps not seen during the training phase (such as intermediate times). For this reason, we introduce a \textbf{last term} of the loss function as:
\begin{equation}
\label{eq:L3}
    \left\{
    \begin{aligned}
    &\mathcal{L}_{3,i} =\frac{||\varepsilon_i^{\pmb{\mu}}-\tilde{\varepsilon}_{i}^{\pmb{\mu}}||_2}{||\varepsilon_i^{\pmb{\mu}}||_2}\ \\ &\tilde{\varepsilon}_{i}^{\pmb{\mu}} = \pi_\theta(\cdot,(\Delta t_{i,i-1}-\Delta t_{m,i}))\circ\pi_\theta(\varepsilon_{i-1}^{\pmb{\mu}},\pmb{\mu},\Delta t_{m,i-1}),
    \end{aligned}
    \right.
\end{equation}
where $\Delta t_{m,i-1}\in[0,\Delta t_{i,i-1}]$ is a randomly sampled intermediate time step and $i-1<m<i$. In Appendix \ref{subsubsec:RK_time_gen} we further detail $\mathcal{L}_{3,i}$. In some cases we also found it beneficial to add a \textit{regularization} term $\mathcal{L}_{rg}$ to the latent vectors, such as $\mathcal{L}_{rg} = \lambda_{rg}\sum_{i=0}^F ||\varepsilon_i^{\pmb{\mu}}||_1/\lambda$, with $\lambda_{rg}\in\mathbb{R}^+$. 

Thus, during model training for a given $s_r^0(\mathbf{x},\pmb{\mu})$, the gradients are computed based on the final loss function of:
\begin{equation}
    \begin{aligned}
    &\mathcal{L}_{tr} = \frac{1}{F}\sum_{i=0}^F \alpha \mathcal{L}_{1,i}+\frac{1}{F}\sum_{i=1}^{F}\left[\beta\mathcal{L}_{2,i}^{T,k_1}+\gamma\mathcal{L}_{2,i}^{A,k_2}+\delta\mathcal{L}_{3,i}\right]+ \mathcal{L}_{rg} = \alpha \mathcal{L}_{1}+\beta\mathcal{L}_{2}^{T,k_1}+\gamma\mathcal{L}_{2}^{A,k_2}+\delta\mathcal{L}_{3} + \mathcal{L}_{rg},
    \end{aligned}
\end{equation}
where $k_1$ and $k_2$ depend on the chosen strategy of section \ref{subsec:TF_&_AR} and $\alpha$, $\beta$, $\gamma$ and $\delta$ weigh the importance of each term.  We use $\mathcal{L}_{tr}$ for training and $\mathcal{L}_{vl}$ for validation:
\begin{equation}
    \mathcal{L}_{vl} = \mathcal{L}_{tr} + \sum_{i=1}^{F} \frac{||s_r(\mathbf{x},t_i|\pmb{\mu}) - \tilde{s}_r(\mathbf{x},t_i|\pmb{\mu})||_2}{||s_r(\mathbf{x},t_i|\pmb{\mu})||_2}.
\end{equation}
Figure \ref{fig:architecture} visualizes our training methodology.

