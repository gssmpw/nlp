\section{Introduction}
Physical simulations are crucial to all areas of physics and engineering, such as fluid dynamics, nuclear physics, climate science, etc. Although a lot of work has been done in constructing robust and quick numerical Partial Differential Equations (PDE) solvers \cite{numerical_matemathics}, traditional solvers such as finite element methods are still computationally expensive when the system is complex. This is a problem especially when repeated evaluations of a model for different initial conditions and parameters are needed, which is typical in sensitivity analysis, design optimization or uncertainty quantification studies \cite{UQ,Perk2014}. To overcome such time limitations, decades of extensive research has been put into building so called \textit{surrogate models}, i.e., faster to evaluate but accurate enough approximations of the original complex model describing the physical system of interest. 

%Surrogate modeling follows two different paths: \textit{intrusive modeling}, requiring the set of equations describing the original physical system, and \textit{non-intrusive modeling}, where access to these equations is \textbf{not} required. Non-intrusive approaches are preferable when the physical system is described by a large number of equations - which is typically the case for any real-world engineering problem - or in the obvious case when these are unknown or difficult to access (e.g., due to the use of legacy code). Since the methodology that we propose belongs to the non-intrusive subset, from this point on we will only discuss such methods.

%The research on surrogate modeling follows two different paths: \textit{intrusive modeling}, where the set of equations describing the original physical system are required, and \textit{non-intrusive modeling}, where the set of equations describing the original physical system are \textbf{not} required. Non-intrusive approaches are preferable when the physical system is described by a large number of equations (or in the obvious case when these are unknown). Since the methodology that we propose belongs to the non-intrusive subset, from this point on we will only discuss such methods.

The first studies on surrogate modeling fall under the umbrella of \textit{Reduced Order Modeling} (ROM) \cite{ROM} methods, with the pioneering work on Proper Orthogonal Decomposition (POD) by Lumley in 1967 \cite{pod_lumley}. The main assumption of ROM is that a system determined by $N$ (potentially infinite) degrees of freedom (full space) can instead be projected into a lower dimensional space of dimension $n$ (reduced space), hence its evolution can be calculated by solving a much smaller system of $n\ll N$ equations. 
A common way of proceeding, under the name of reduce basis methods, is by assuming that the solution field $s(x,t)$ of a PDE can be approximated as: $s(x,t)\approx\sum_{k=1}^n a_k(t)V_k(x)$ with $a_k(t)$ being time-dependent coefficients and $V_k(x)$ being independent variable $x$ dependent functions, the latter constituting an orthonormal basis (the reduced basis). Once the optimal basis is found the system is completely described by the $n$ coefficients $a_k(t)$. 
%Many methods (both linear and nonlinear) to find the optimal vector basis have been so far studied \cite{Benner2015, Placzek2011,RB, Guo2019}. 
The same concept of Dimensionality Reduction (DR) is known in the Deep Learning (DL) field under the name of \textit{manifold hypothesis} \cite{Fefferman2016, goldt2020modeling, cohen2014learning, higgins2018towards}, analogously stating that high-dimensional data typically lie in low dimensional manifolds (due to correlations, symmetries, noise in data, etc.). In DL jargon, this reduced space is usually named \textit{latent} space.

Several recent works explore the potential of DL for surrogate modeling, both following the ideas of traditional ROM approaches and proposing new paradigms. A non exhaustive list of methods that integrate DL techniques with ROM concepts is provided in \cite{FRESCA2022114181, bhattacharya2021model, romcnn, SoleraRico2024}. Among these works, DL is used to approximate the mapping between full space and reduced space, to determine the coefficients of the reduced basis and/or to map initial states of the PDE into the PDE solution $s(x,t)$. \cite{Lusch2018} implements concepts from Koopman Operator theory \cite{kutz2016koopman} for dynamical models, where the linearity of the Koopman Operator is exploited to advance in time the dynamical fields in a reduced space. The Sparse Identification of Nonlinear Dynamics (SINDy) is proposed in \cite{Brunton2016}, where the reduced vectors are assumed to follow a dynamics governed by a library of functions determined a priori.

Recently, \textit{Neural Operators} (NO) \cite{kovachki2021neural, bartolucci2023representation}, i.e., DL models whose objective is the \textit{approximation of operators} instead of functions - contrary to what is typical in DL - have found applications in surrogate modeling tasks. As in the case of PDEs we deal with a mapping between infinite-dimensional functional spaces (from the space of initial and boundary conditions to the solution space of the PDE), the approximated operator is called the \textit{solution operator} of the PDE. The (chronologically) first works on Neural Operators are the DeepONets \cite{Lu2021} and the Fourier Neural Operator \cite{li2020fourier}. Since these publications, the literature on NO has flourished, with many theoretical and empirical studies \cite{kovachki2021neural, CNO, Lu2022_afaircomparison, hao2023gnot, kissas2022learning,li2024geometry, gupta2022towards, jin2022mionet}. In some related works Graph Deep Learning has been used for surrogate modeling to generalize to different geometries \cite{brandstetter2022message,pichi2023graph, Franco2023, equer2023multi}. Beside DR, our model also leverages Neural ODEs \cite{chen2018neural} (NODEs), which are a class of Neural Networks (NNs) where the state of the system $h(t)\in\mathbb{R}^D$ behaves according to $\frac{dh(t)}{dt}=f_\theta(h(t),t)$, with $f_{\theta}$ being parametrized by a NN.
NODEs present the advantage of modeling the dynamics of $h(t)$ continuously in time.
\subsection{Related works}
Among the large literature on Neural Operators and methods at the intersection between DL and ROM, the papers closest to our work are: \cite{knigge2024space} where the latent dynamics is described by an ODE, \cite{yin2023continuous} where the latent dynamics is required to respect the equivariant properties of the original PDE, \cite{vcnef-hagnberger:2024} where an architecture featuring Neural Fields and Transformers is used to solve PDEs for different initial conditions and PDE parameters and \cite{takamoto2023learning} where a general method for including the PDE parameter dependency into Neural Operators is proposed.

\subsection{Contributions}
In this work we propose an autoregressive DL-based method for solving \textbf{parametrized}, \textbf{time-dependent} and (typically) \textbf{nonlinear} PDEs exploiting \textit{dimensionality reduction} and \textit{Neural ODEs}. Our contributions are the following:
\begin{itemize}
    \item We construct a model that allows for the variation of both the PDE's \textbf{parameters} and \textbf{initial conditions}. 
    \item We define two mappings parametrized by $2$ NNs: a close-to bijective mapping between the \textbf{full} (high-fidelity) PDE solution space and the \textbf{latent} (low-fidelity) space via an AutoEncoder (AE) and a mapping from the latent vector at time $t_i$ to the next latent vector at time $t_{i+1}$ modeled by a (latent) NODE. 
    \item Training on a \textbf{given} $\Delta t$ we show that our model can generalize at testing time to \textbf{finer} time steps $\Delta t'<\Delta t$. %To this regard we also study the connection between the type of numerical solver used to solve the NODE in latent space and the time generalization capabilities of the model.
    \item We show a simple but effective strategy to train this model combining a \textbf{Teacher Forcing} type of training with an approach which takes into account the \textbf{Autoregressive} nature of this model at testing time. 
    \item We achieve computational speed up compared to standard numerical PDE solvers thanks to $3$ factors: doing inference at a $\Delta t$ \textbf{higher} than what is usually required by standard numerical solvers, solving an \textbf{ODE} instead of a PDE and advancing in time in a \textbf{low} dimensional space instead of the full original space.
    \item We test our methodology on a series of PDEs benchmarks (using \cite{takamoto2022pdebench} among others) and show that thanks to DR, our model is (at least 2 times) \textbf{lighter} and (at least 2 times) \textbf{faster} than current State of the Art (SOTA) methods.
\end{itemize}
 