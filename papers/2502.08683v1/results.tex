\section{Results}
\label{sec:results}
In this section we compare our method with a series of SOTA methods from \cite{vcnef-hagnberger:2024} and \cite{takamoto2022pdebench}. The datasets we use for comparison are taken from \cite{takamoto2022pdebench}. A complete description of the PDEs can be found in Appendix \ref{subsec:datasets}. In Appendix \ref{subsec:training_details} we list all the training and hyperparameter details and in Appendix \ref{sec:methods_comparison} the methods used for comparison. We use as metric the nRMSE error defined in Equation (\ref{eq:nRMSE}). 
\definecolor{lightgray}{gray}{0.9} % 90% white, 10% black

\begin{table}[]
\begin{center}
\begin{tabular}{ll>{\columncolor{lightgray}}ll}
\toprule
PDE   & Model & nRMSE, $\Delta t = 0.05\,s$    & nRMSE, $\Delta t = 0.01\,s$ \\
\hline
& (Ours) & $\mathbf{0.0066}$&$\mathbf{0.0066}$ \\
& FNO & $0.0190$ & $0.0258$\\
& MP-PDE & $0.0195$& \\
& UNet & $0.0079$ & \\
1D Advection& CORAL & $0.0198$&$0.9656$  \\
& Galerkin & $0.0621$ &\\
& OFormer & $0.0118$& \\
& VCNeF & $0.0165$&$0.0165$ \\
& VCNeF-R & $0.0113$& \\ \hline
& (Ours) & $\mathbf{0.0373}$&$\mathbf{0.0399}$ \\
& FNO & $0.0987$ &$0.1154$\\
& MP-PDE & $0.3046$ &\\
& UNet & $0.0566$& \\
1D Burgers & CORAL &$0.2221$&$0.6186$  \\
& Galerkin & $0.1651$& \\
& OFormer & $0.1035$& \\
& VCNeF & $0.0824$&$0.0831$ \\
& VCNeF-R & $0.0784$& \\ 
 \bottomrule
\end{tabular}
\end{center}
\caption{nRMSE on test dataset for fixed $\pmb{\mu}$ and varying $s^0$ for the 1D Advection and Burgers datasets. Gray column refers to testing with the $\Delta t$ of the training, while white one with a smaller $\Delta t$. Cells are empty when comparison was not found in literature.}
\label{table:results_fixed_par}
\end{table}
\begin{table}[]
\begin{center}
\begin{tabular}{ll>{\columncolor{lightgray}}l>{\columncolor{lightgray}}ll}
\toprule
PDE   & Model & nRMSE, $\Delta t = 0.05\,s$    & nRMSE,$\Delta t = 0.01\,s$ & nRMSE,$\Delta t = 0.01\,s$ \\
\hline
& (Ours) & $0.0028$&&$\mathbf{0.0032}$ \\
& FNO & &$0.0044$ &\\
2D SW & U-Net &  &$0.0830$ &\\
 & PINN&&$0.0170$ &\\ 
 \bottomrule
\end{tabular}
\end{center}
\caption{nRMSE on test dataset for fixed $\pmb{\mu}$ and varying $s^0$ for the 2D Shallow-Water dataset. Gray column refers to testing with the $\Delta t$ of the training, while white one with a smaller $\Delta t$. Our model is trained on $\Delta t = 0.05\,s$, while the others on $\Delta t = 0.01\,s$. }
\label{table:results_fixed_par_SW}
\end{table}
\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth]{image_results/advection.pdf}
  \caption{Distribution of the nRMSE across the test sample for the parametric 1D Advection. Regular font on the x axes refers to training parameter values, while bald ones to testing parameters (but in both cases testing initial conditions). We compare our methodology (red on right image) with other published methods (left image, taken from \cite{vcnef-hagnberger:2024}).}
  \label{fig:advection_parametric}
\end{figure*}
\subsection{PDEs with fixed parameter}
\label{subsec:PDE_fixed}
In this Section we are going to apply our method to the 1D Advection Equation (\ref{eq:advection}) ($\zeta =0.1$), to the 1D Burgers' Equation (\ref{eq:burger}) ($\nu=0.001$) and to the 2D Shallow-Water (SW) Equations (\ref{eq:sw}).
In Table \ref{table:results_fixed_par} and \ref{table:results_fixed_par_SW} we compare our results to the ones obtained (on the same dataset), in \cite{takamoto2022pdebench} and \cite{vcnef-hagnberger:2024}. In Table \ref{table:results_fixed_par} we show that our proposed model achieves a \textbf{lower nRMSE} compared to a series of common methods used in Scientific Machine Learning for the $3$ cases. Furthermore, we observe that our model achieves a better \textbf{generalization in time} than the other methods in the Burgers' and Advection cases, meaning that we got little to none increase of the nRMSE when going from testing on the training $\Delta t=0.05\,s$ to testing on a smaller $\Delta t = 0.01\,s$ . For the SW case in Table \ref{table:results_fixed_par_SW}, even if our model is trained with $\Delta t = 0.05\,s$ while the others with $\Delta t = 0.01\,s$, we still get a lower nRMSE when testing on $\Delta t = 0.01\,s$ (thus the comparison on the same number of time-steps is only between our method in the white column and the other methods in the grey column). For the experiments in this section we used Strategy $1$ of Section \ref{subsec:TF_&_AR}, as using $\mathcal{L}_{2,i}^{T,1}$ alone was sufficient to reach acceptable results.
\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth]{image_results/burgers.pdf}
  \caption{Distribution of the nRMSE across the test sample for the parametric 1D Burgers'. Regular font on x axes refers to training parameters, while bald ones to testing parameters (but in both cases testing initial conditions). We compare our methodology (red on right image) with other published methods (left image, taken from \cite{vcnef-hagnberger:2024}).}
  \label{fig:burger_parametric}
\end{figure*}
\subsection{PDEs with varying parameters}
\label{subsec:PDE_varying}
In this section we experiment with $3$ datasets where we both vary the initial conditions and the PDE parameters: 1D Advection Equation (\ref{eq:advection}), the 1D Burgers' Equation (\ref{eq:burger}) and the 2D Molenkamp Test (\ref{eq:molenkamp}). In all three cases we use Strategy $2$ of Section \ref{subsec:TF_&_AR}, since only using $\mathcal{L}_{2,i}^{T,1}$ optimized correctly $\mathcal{L}_{tr}$ but resulted in a larger value of $\mathcal{L}_{vl}$. We start with $k_2 =1$ and we increase it by $1$ every $30$ epochs until the maximum length of the time series is reached. To make the training more stable, we introduce gradually the $\gamma\,\mathcal{L}_{2,i}^{A,k_2}$ term by starting with a $\gamma = \gamma_0<1$ and increasing it every epoch by an amount of $\gamma_0$ until $\gamma=1$. In Figures \ref{fig:advection_parametric} and \ref{fig:burger_parametric} we show a comparison of our methodology (red) with the cFNO, cOFormer and VCNeF from \cite{vcnef-hagnberger:2024}. In both cases, we show the distribution across the test samples of training (regular font) and testing (bald font) parameters when using \textbf{testing initial conditions}. From Figure \ref{fig:advection_parametric}, we see that our model has a lower median than the compared methods on the training velocities $\zeta$, while it struggles with testing parameters, similar to cFNO, cOFormer and VCNeF too. This is likely a dataset issue yielding insufficient generalization, with $0.1$ and $7.0$ both being outside the training range and $1.0$ possessing a dynamics very far from the one that characterizes $0.7$ and $2.0$. Similarly for the Burgers' case in Figure \ref{fig:burger_parametric}, the median of the nRMSE given by our model is lower than the compared methods for all parameters $\nu$ except $\nu=1.0$ and $\nu=4.0$. In this case our model - similar to the ones used for comparison - is able to generalize better than in the Advection example to test parameters, as in the case of $\nu=0.001$ and $\nu=0.01$. Given the discrepancy in the ability of the models to generalize to different testing parameters, more accurate strategies for adaptively selecting parameter points for training should be researched. In Figure \ref{fig:molenkamp} we compare our method with VCNeF on the Molenkamp when testing on $\Delta t = 0.05\,s$ (same as the one in training) and when $\Delta t = 0.02\,s$: our method achieves a lower nRMSE and is able to generalize to intermediate time points better than VCNeF. In Appendix \ref{subsec:speed_and_numb_comp}, we compare the number of parameters and the inference speed of the methods used and we show that our proposed method is \textbf{lighter} and \textbf{faster} at inference.
\subsection{Ablation studies}
In Appendix \ref{subsec:solver_for_time} we conduct ablation studies regarding how the choice of the ODE solver and $\mathcal{L}_{3,i}$ impacts the capabilities of the method and the generalization in time.


\begin{figure*}
  \centering
  \includegraphics[width=0.8\textwidth]{image_results/molenkamp_VCNeF_ours.png}
  \caption{Comparison (on the test dataset) for the Molenkamp application of the nRMSE over time $t$ between our model (red) and the VCNeF (green). We study the difference when applying at inference the same $\Delta t $ used for the training ($\Delta t = 0.05\,s$) and when applying a smaller one $\Delta t = 0.02\,s$. The nRMSE of our model slightly increases when decreasing the $\Delta t $, while VCNeF struggles with inference at intermediate time-steps.}
  \label{fig:molenkamp}
\end{figure*}