\section{Conclusions}
In this work we showed how \textbf{Dimensionality Reduction} and \textbf{Neural ODEs} can be coupled to construct a surrogate model of time-dependent and parametric PDEs. Our model inherits from these two paradigms two important features which are desiderata when building DL models that substitute standard numerical solvers, i.e.,  \textbf{fast computational inference} and \textbf{continuity in time}. The former is achieved thanks to the low dimensionality of the reduced space $\mathcal{E}$, while the latter by the definition of the latent dynamics through the ODE of Equation \ref{eq:ODE}. In Section \ref{sec:results} we showed that our methodology \textbf{surpasses in accuracy} several state of the art methods on different benchmarks used in the Scientific Machine Learning field. In addition, our model requires \textbf{significantly less NN's weights} (thus less memory) and is \textbf{computationally faster at inference} compared to other published methodologies (Tables \ref{tab:inference_time} and \ref{tab:numb_weights}); for these reasons relying on \textit{dimensionality reduction} as opposed to \textit{large and overparametrized} architectures is going to be key in the future for building fast and memory efficient surrogate models of complex physical systems. 

%Finally, through the interplay of the terms $\mathcal{L}_i^{T,k_1}$ and $\mathcal{L}_i^{A,k_2}$ we proposed in Section $\ref{subsec:TF_&_AR}$ a \textbf{simple strategy to deal with the autoregressive nature} of our model which can be applied to any autoregressive method. 

The main limitation of our method is the use of CNNs for Encoding and Decoding, which hinders its applicability to non-uniform meshes and makes it necessary to re-train models if inference needs to be done on grid points not used at training - future research will explore using Neural Operators for the construction of the Encoder and the Decoder. Another aspect which should be improved concerns the definition of an efficient strategy to determine which PDE parameters should be used during training in order to be able to better generalize to new ones, as evident from Section \ref{subsec:PDE_varying}. Finally, while leaving the construction of $\mathcal{E}$ and definition of the function $f_\theta$ general gives flexibility to the fitting of the training dataset, researching into the \textit{interpretability} of both $\mathcal{E}$ and $f_\theta$ can at the same time improve our understanding of NNs and build more accurate surrogate models.

\subsection{CRediT authorship contribution statement}
\textbf{Alessandro Longhi}: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Visualization. \\
\textbf{Danny Lathouwers}: Conceptualization, Methodology, Formal Analysis, Resources, Supervision, Project administration, Funding acquisition.\\
\textbf{Zoltán Perkó}: Conceptualization, Methodology, Formal analysis, Resources, Writing - Review and Editing, Supervision, Project administration, Funding acquisition.