
\paragraph{Graph Neural Networks.}
GNNs are at the current forefront of graph machine learning research. 
They are designed to transform input nodes into compact vector representations, which can be used for node, link and graph-level tasks. GNNs commonly use the layer-wise message-passing design~\citep{kipf2016semi,veličković2018graph,xu2018powerful,Gao2018LargeScaleLG,wu2020comprehensive,ma2021unified}, in which nodes progressively aggregate and process information from immediate neighbors, enabling convolution over the graph with multiple layers. With the message-passing design, the embedding of each node can be computed given its $k$-hop subgraph for a $k$-layer GNN. The process of generating the $k$-hop subgraphs is often referred as subgraph sampling.

\paragraph{GNN Scalability.}
Most GNN applications in the research landscape occur at a single-machine scale, owing to the size of benchmark datasets~\cite{hu2020open}.  In industrial contexts, graphs may surpass such limits in terms of number of nodes, edges and features, which complicates GNN training and inference due to data dependency in the message-passing design and the need for careful minibatching~\citep{duan2022comprehensive,xue2024large}. 
Recent literature discusses multiple directions to train GNNs with large-scale graphs, including sampling methods~\citep{hamilton2017inductive,chen2018fastgcn}, staleness-based methods~\citep{fey2021gnnautoscale,xue2024haste},  distillation~\citep{zhang2021graph,guo2023linkless}, quantization~\citep{ding2021vq}, condensation and coarsening~\citep{jin2021graph, tsitsulin2023graph}, pre-training~\citep{han2022mlpinit, borisyuk2024lignn}, distributed training~\citep{md2021distgnn, lin2023comprehensive}, and many more~\citep{xue2024large}.
These techniques have been commonly adopted as directions in GNN scaling research, and are applied to training in larger benchmark graphs (e.g., {\small\texttt{ogbn-products}}~\citep{hu2020open} with 2.4M nodes and 61M edges). Nonetheless, in large-scale applications, some of these techniques meet various challenges owing to inefficient implementations, misalignment between academic and industrial assumptions, and diverse graph properties \cite{palowitch2022graphworld} -- for example, graphs we handle at Snapchat are routinely have 10-50B edges. 

\paragraph{Industrial Applications.}
Adopting GNNs has been a key focus in multiple industrial contexts: Pinterest discusses using GNNs to learn pin-board embeddings in an early work~\cite{ying2018graph} which adopts GraphSAGE with hard-negative sampling.  Google showcases the use of GNNs for traffic time forecasting~\cite{derrow2021eta}.  LinkedIn applies GNNs for feed and job recommendations~\cite{borisyuk2024lignn, liu2024linksage}. Uber applies GNNs for food and restaurant discovery~\cite{jain2019food}. At Snapchat, we explored the use of GNNs in friend recommendation~\cite{shi2023embedding, kung2024improving, sankar2021graph}, story ranking~\cite{tang2022friend} and engagement forecasting~\cite{tang2020knowing}. 

Solutions at scale broadly fall into two groups: \emph{real-time} subgraph sampling, and \emph{tabularization}: The former requires maintaining a shared graph state, like an in-memory engine or graph database, which is queried during training and inference.  The latter pre-computes all subgraph samples to cloud storage, which are used for subsequent training and inference via an extract-transform-load (ETL) job; this removes need for shared graph state, akin to standard data-parallel setup common in other ML workflow.
GraphLearn~\citep{zhu2019aligraph}, ByteGNN~\cite{zheng2022bytegnn}, DeepGNN~\cite{samylkin2022deepgnn} and GraphStorm~\cite{zheng2024graphstorm} adopt realtime sampling via various graph backends and partitioning strategies, following ideas by \citet{chiang2019cluster, karypis1997metis}. Conversely, AGL~\cite{zhang13agl}, TF-GNN~\cite{ferludin2022tf}, and MultiSAGE~\cite{yang2020multisage} adopt tabularization.  Both strategies have pros and cons, which we will discuss further in \cref{sec:lesson-infra}. 

The library we discuss in this work, GiGL, adopts the tabularization approach by default, but is not constrained to it, and has a configurable real-time backend which allows interfacing with tabularization backends via Spark and graph databases, as well an in-memory graph engine. We have used GiGL successfully internally at Snapchat with different backends, and this flexibility is empowering for challenges at different technical scale, freshness and complexity. We hope our work adds to the literature in this space via \emph{(i)} offering the community a diverse technical perspective, \emph{(ii)} sharing unique lessons learned and business impact earned over years of working on this problem domain, and \emph{(iii)} case studies on billion-scale tasks at Snapchat.  We emphasize that our goal in open-sourcing GiGL and sharing details in this work is to empower the community with helpful tooling and guidance, rather than advocate for GiGL as a superior alternative to other approaches.

