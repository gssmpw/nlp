

GiGL is widely used inside Snapchat to power multiple graph learning usecases. Along with the business applications, we also discuss how we utilize the key advantages of GiGL, such as the flexibility, scalability, and reusability. Additionally, we showcase a couple key features of GiGL that were inspired by our production needs.

\subsection{Friend Recommendation}
\label{sec:friending}

Snapchat's friend recommendation system is a major internal user of GiGL.  Snapchat powers a large social network with hundreds of millions of daily active users, striving to help users discover their real-life friends on the platform. A classic multi-stage retrieval and ranking system is employed to power this large-scale recommendation system which entails recommending potential friends for each user from the entire user base~\citep{shi2023embedding}. Prior to GiGL, the retrieval system primarily exploited principles of graph locality, using discrete graph traversal algorithms to retrieve friend candidates (e.g. friends-of-friends) which serves as candidates in the heavy ranker.

One early usecase of GNNs at Snap was the embedding of this social graph.  In particular, we constructed a friendship graph defined over recently active users, where nodes represent users and edges indicate friendships, with node features indicating  user profile properties, and trained an unsupervised GraphSAGE model with margin loss~\citep{hamilton2017inductive} to generate embeddings for all users on a daily cadence. These embeddings enabled the deployment of a robust Embedding-based Retrieval (EBR) system~\citep{shi2023embedding} powered by efficient Approximated Nearest Neighbor (ANN)~\citep{shi2023embedding} search, which has since become the largest funnel for our friend retrieval stage. 
Additionally, aligned with \citet{sankar2021graph}, we found that incorporating such GNN embeddings as additional features in the ranker further improved online metrics, demonstrating that topology information is beneficial even for precision-focused models. % (further discussed in \cref{sec:lesson-model}).

\input{tables/friending}

The initial launch of graph embeddings using GiGL was a major success for Snapchat's friend recommendation system, leading to continuous investments into this joint effort. In the past two years, GiGL has supported more than 20+ successful product launches in multiple areas of the friend recommendation stack. \cref{tab:friending} highlights several of these launches, showcasing either notable findings on GNN usage in friend recommendation or important feature developments in GiGL with application-based learnings:

\paragraph{Graph Definition Update.} 
Real-world data is often noisy, and graph ML in  production systems rarely comes with a ``ready-to-use'' graph. Instead, we rely on leveraging abundant log data containing various types of information.
% \footnote{All user data at Snap is processed in compliance with privacy regulations and laws, such as the EU's General Data Protection Regulation (GDPR).}. 
Initially, we used the friendship graph, which seemed to align well with the friend recommendation use case. 
However, we soon realized that social circles evolve over time, and recent interactions are more indicative of friend recommendations. To address this gap, we introduced a simple graph sparsification strategy of updating the graph to an engagement graph, with nodes still represent users, but edges are defined only between users who recently interacted. This update not only significantly improved performance but also reduced costs, as the engagement graph is far much sparser. In following updates, we also investigated data augmentation techniques~\citep{zhao2021data,zhao2022graph} to enhance the low-degree users, and our recent research NodeDup~\citep{guo2024node} is also on an internal testing roadmap.

\paragraph{Model Update.} 
As discussed in \cref{sec:framework}, GiGL offers key advantages in modeling flexibility by leveraging existing model implementations from external libraries. This enabled us to experiment with all graph convolution layers available in PyG at the time, and we identified attention-based models (e.g. GAT~\citep{velivckovic2017graph}) as the best-performing for our task. 
Over time, similar experiments were conducted periodically and led to multiple model updates, including transitions to GATv2 layers~\citep{brody2021attentive} and the addition of DCN layers~\citep{wang2017deep}. In particular, we hypothesize such attention-based models are especially useful for two main reasons: \emph{(i)} they can improve model expressivity via added parameterization, \emph{(ii)} they can well-handle scale-free graphs with skewed degree distributions (common in social networks), where not all neighbors have equivalent significance in modeling (e.g. defining convolutions based on strong-engagement user relationships can provide stronger signal vs. compared to randomly selected user relationships).

\paragraph{Loss Update.} 
Similar to modeling flexibility, GiGL also offers custom loss-function definition, with several ready-to-use implementations.  For the friend recommendation EBR usecase, we found the Sampled Softmax-based Retrieval Loss~\citep{jean2014using,yi2019sampling,wu2024effectiveness} to be suitable in application, and achieved outsized business gains given its simplicity. Additionally, inspired by recent research~\citep{ju2022multi}, we further adopted a multi-task learning approach which adopted ideas in self-supervised graph representation learning~\cite{kolodner2024robust}: we combined retrieval loss with feature-reconstruction and whitening-decorrelation losses, based on findings in ParetoGNN~\citep{ju2022multi} that these could yield stronger general-purpose representations. Both auxiliary losses resulted in successful product launches.

\paragraph{Task Update.} 
To further enhance GNN embeddings' quality for the task of friend recommendation, we use GiGL's ``supervised'' link prediction setup (not commonly observed in academic literature), 
and replaced the sampling of training edges with user-defined positive (and optionally negative) supervision edges. For example, we used newly established friendships made over next-$k$ days as positive labels while keeping the engagement graph as the message-passing graph. This approach improved business metrics, improved representation quality compared to a purely self-supervised setup, and opened new possibilities for incorporating hard negative signals, such as user ignores and blocks into the training process.

\paragraph{EBR Scheme Update.} A key finding from our GNN embedding iterations for friend recommendation EBR is that ANN seed selection significantly impacts performance. 
We naturally started with each user's own embedding as the query for retrieving potential friend candidates. However, we observed that this approach could limit candidate diversity and miss relevant connections.
To address this,  we developed stochastic EBR~\citep{kung2024improving}, which retrieves candidates using embeddings of a userâ€™s friends instead of their own. 
This method effectively broadened the search space and captured richer social signals.
This approach delivered substantial performance gains in business metrics and has since become the default scheme for GNN-based friend recommendation EBR at Snapchat.

\input{tables/content-online}

\input{tables/content-offline}

\subsection{Content Recommendation}

Beyond friend recommendation, content recommendation is another key application with abundant relational data that benefits from GNN modeling. The interactions between users and short videos (Spotlight\footnote{\url{https://www.snapchat.com/spotlight}} and Discover\footnote{\url{https://www.snapchat.com/discover}} tab) naturally form bipartite graphs, with user nodes connected to video nodes through various interactions. By incorporating additional metadata, such as content creators, this structure extends to a heterogeneous graph, effectively representing relationship across multiple types of entities. 

\paragraph{Graph Definition Update.}
Before we implemented the GraphDB backend for sampling heterogeneous graphs, we first constructed a video-to-video co-engagement graph to leverage GNN embeddings for content recommendation.
User-video interactions are even more long-tailed in our setting than user-user ones, with many videos receiving millions of positive engagement signals. To address this imbalance, we used the Jaccard index with a threshold (over the sets of co-engaging users) to sparsify the graph, resulting in a more balanced structure which facilitates GNN learning.  
These embeddings were used in a video-to-video EBR system to retrieve similar videos that users had engaged with positively. This application was successfully launched in both of our short video products with measurable business metrics improvements (as shown in \cref{tab:content-online}).

\paragraph{Heterogeneous Support.}
With the heterogeneous graphs support by GraphDB-based subgraph sampling, we then pursued a modeling task designed for a similar use-case, but with a user-engage-video bipartite graph, which allowed us to also leverage user node features and engagement features on the edges.
The embeddings generated by the heterogeneous graph are undergoing both offline and online studies for both video-to-video retrieval and user-to-video retrieval use cases. \cref{tab:content-offline} exemplifies a few observations during our offline iterations, highlighting the improvements brought by feature engineering and increasing number of GNN layers in product usages.  

\input{tables/ads_offline}
\subsection{Ads Recommendation}
User-ad engagement is often sparse, noisy, and influenced by various factors. For instance, in a mobile setting, users may unintentionally click on ads when their actual intention was to skip them. Such complexity is further compounded by the presence of multiple in-app ad inventories (e.g., Story Ads, Discover Ads), diverse advertiser objectives (e.g., swipes, clicks, installs, purchases), and different ad products (e.g., websites, apps, physical products). The primary goal of ads modeling is to maximize the utilization of available signals across these varying products, objectives, and inventories. Given the inherently heterogeneous nature of ads engagement, we explore testing of GNN modeling with GiGL on both homogeneous and heterogeneous graphs in different ads product verticals.

\paragraph{Graph Definition Update.}
For product ads recommendation, similar to content recommendation, we constructed a homogeneous graph with product IDs as nodes, and various types of co-interactions as edges. We also adopted a thresholded Jaccard index to achieve a more balanced structure for effective GNN learning.  Additionally, we incorporated contextual embeddings, such as product text and image embeddings, as well as product metadata (price, currency, brands), enabling the model to capture both co-engagement similarity and product-type similarities. The resulting product embeddings are then utilized in EBR systems for product retrieval.  \cref{tab:ads-offline} compares results with the current productionized model. GNN v1 utilizes product text, image embedding and product metadata as features, and GNN v2-v3 utilizes text embedding with product metadata. The precision and recall are computed for 1\% of the randomly sampled fixed set of users' future conversion events (add-to-cart, purchase, etc.) We can observe that with 10\% of the training data, a homogeneous GNN model achieves precision parity with the control model, but with higher recall. As with other applications, we observed that the performance increases as the number of training edges and neighbor fan-out increases. 

For web ads recommendation, we constructed a heterogeneous graph with both web ads and users as nodes, connected by edges representing different interaction types.
Node features include user demographics,  user past engagements, and web ads metadata (e.g., domains). To mitigate the impact of power nodes, where a few dominant entities disproportionately influence conversions, we pruned edges and capped node degrees to 10K in the graph. The pruning helps maintain a balanced graph structure, which is easier for GNNs to learn from.  In particular, our results consistently support that careful graph data definition is an important knob to tune in training. Both use-cases showed strong offline performances and are under-going further online studies for internal testing.

\paragraph{Learning from Partially Observable Graph.}
As users on Snapchat can opt-out from personalized ads recommendation, the interaction data between users and ads are only partially available for model training and inferencing. Hence, we model multiple partial graphs between users and different types of ads (e.g., product ads, app ads, web ads) into a joined heterogeneous graph with multi-task learning rather than multiple individual partial graphs, allowing us to have more control and learn from cross-type ads engagement signals. Furthermore, we also explored graph data augmentation~\citep{zhao2021data} and structure learning~\citep{jin2020graph} techniques on such partial graphs.

\input{tables/other-online}

\subsection{Broader Representation Learning Applications}
\label{sec:broader}

Through our experience training GNNs, we discovered that the relational information encoded by GNNs is often valuable not only for a target domain but also for related domains within Snapchat. For example, incorporating social-graph user embeddings from our friend recommendation system as additional features into multiple ML tasks within the business (e.g. bad actor detection), we achieved notable performance gains, likely due to the effective social-prior encoded in the embeddings. \cref{tab:other-online} summarizes several such product launches where existing embeddings were successfully reused.

Looking ahead, the multiple entities and relations across different domains on a social network can be captured in one single large-scale heterogeneous graph.
Powered by our recent research~\citep{ju2022multi}, we aim to develop cross-domain multi-task graph learning models that learn embeddings beneficial to many products simultaneously. While such a pipeline can be computationally expensive, it remains cost-effective as the resource costs are shared across multiple product teams, maximizing both efficiency and business impact.