
Graphs are ubiquitous, representing a wide variety of real-world data across domains such as social networks and recommendation systems. In recent years, Graph Neural Networks (GNNs) have emerged as powerful tools for learning from graph data \cite{kipf2016semi, veličković2018graph, ying2018graph}. 

Nevertheless, when deploying GNNs in industrial contexts, such as large-scale user modeling and social recommendation systems, scalability poses a significant challenge. Most commonly used libraries in the research community assume that graph topology, node and edge features, can fit easily within the CPU (or even GPU) memory of a single machine. However, just storing a graph with 100 billion edges as an example (not uncommon in practice \cite{ching2015one}) requires 800 GB of memory assuming 32-bit integer IDs, even before accounting for node and edge features or the need to represent graphs with more nodes than 32-bit limits.  Such limitations make it difficult to scale GNN models in real-world, billion-scale graphs, which we routinely handle at Snapchat.

To address these challenges, we developed GiGL (Gigantic Graph Learning), an internal library that our internal researchers and practitioners use to efficiently train and infer GNNs on large-scale graphs with tens to hundreds of billions of edges. GiGL seamlessly integrates with popular GNN libraries like PyTorch Geometric (PyG)~\citep{Fey2019pyg}, allowing users to leverage  existing modeling codes without introducing new syntax for defining GNN layers. Specifically designed for large-scale graph learning tasks, GiGL offers:
\begin{itemize}[leftmargin=*]
    \item Support for both supervised and unsupervised applications, including node classification, link prediction, and self-supervised representation learning. 
    \item Abstracted interfaces for integration with popular machine learning frameworks such as PyTorch and TensorFlow.
    \item Compatibility with widely-used graph ML libraries like PyG for flexible GNN modeling.
    \item Utilities for efficient data transformation, pipeline management, and orchestration in large-scale deployments and recurrent applications.
\end{itemize}
At its core, GiGL abstracts the complexity of distributed processing for massive graphs, enabling users to focus on model development while the library handles data transformation, subgraph sampling, and persistence. This empowers users to iterate on graph models at scale with limited concern to infrastructure and scale constraints.

We built GiGL to address our internal needs over the years. GiGL currently powers all GNN-based graph machine learning workflows at Snapchat, supporting critical product features such as friend recommendations, lens recommendations, spam and abuse detection, and content recommendations. Across multiple use-cases, we use GiGL to generate either task-specific embeddings or general-purpose user embeddings, which serve as inputs for embedding-based dense retrieval (EBR) systems and/or as added dense-features in lower-funnel ranking models.  Yet, we believe there are opportunities for GiGL to serve a broader community. GiGL addresses a critical need for machine learning researchers, engineers, and practitioners: enabling large-scale exploration and experimentation with state-of-the-art GNN models on industry-scale graphs, while compatibly interfacing with familiar open-source libraries which enable fast model iteration and development like PyG.  While these libraries are more commonly used in contexts with few millions of nodes and edges, they benefit from strong community support and cutting-edge GNN research implementations. GiGL bridges this gap by providing tooling around handling distributed graph scaling challenges, utilities, orchestration and more to handle  heavy lifting, allowing researchers to continue leveraging the modeling tools they know and trust. Hence, this work accompanies an open-source release of GiGL, at \url{https://github.com/snap-research/GiGL}. We will evolve this offering to support the broader graph ML community, and are happy to accept contributions.

In this work, we first discuss GiGL's development, components and capabilities, and scaling considerations. Readers will understand design rationale, technical tools used, and infrastructure considerations in using the library.  We next discuss multiple ways in which GiGL-trained GNN embeddings have facilitated solving large-scale ML problems at Snapchat, spanning critical tasks for social platforms like friend recommendation, content recommendation and ads ranking; across these domains, GiGL has powered over 35 launches which span all elements of GNN modeling, spanning graph definition, modeling levers, loss function design. We discuss key choices in each context, experimental results, and significant topline metrics movements across the business.  Finally, we share lessons learned from working with large-scale GNNs across these applications, and our perspective and understanding on GNNs at scale as the  community has evolved over the years. These may be instructive to academic researchers who are interested in peeking into industrial challenges, as well as practitioners wanting to advance GNN modeling in their own contexts.





