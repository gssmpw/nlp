


\input{tables/dataset}
\input{tables/scale-het}
\input{tables/ablation}

\section{Dataset Details}
\label{appx:dataset}

In order to showcase the impact of graph scale on GiGL runtime, we vary the internal graph in terms of number of nodes/edges and number of node/edge features. Detailed statistics of these datasets are included in \cref{tab:datasets}. The three smaller internal datasets (Internal-small, Internal-less-feat, and Internal-less-edge) are all sub-sampled from Internal-full.

For the experiments in \cref{tab:scale}, we used the same resource configuration to ensure a fair comparison. However, it's worth noting that the resource configuration can and should be further tuned for specific jobs, resulting with optimal cost and time efficiency.

\paragraph{Internal-full}. Snap internal large scale homogeneous graph dataset containing $\sim${900M} nodes, and $\sim${16.8B} edges. With 249 node features and 19 edge features.

\paragraph{Internal-small}. A small sample of the Snap internal dataset with $\sim${3.5M} nodes and $\sim$125M edges, with feature dimension unchanged.

\paragraph{Internal-less-edge}. We randomly remove $50\%$ of the existing edges in the Internal-full dataset, leaving $\sim$8.4B edges. \#nodes left and features unchanged.

\paragraph{Internal-less-feat}. We reduce the node feature dimension to 155, and edge feature dimension to 10. 

\paragraph{MAG240M}\footnote{\url{https://ogb.stanford.edu/docs/lsc/mag240m/}}. 
In order to evaluate the same link prediction task as we did for the internal datasets, the train/validation/test sets are generated by our Split Generator following a standard transductive link prediction split.
% We leverage the original public academic graph dataset \cite{hu2021ogblsc, hu2020ogb} and cast it to a homogeneous graph for our experiments by: (i) using a 768 size zero vector as the feature vector for each author node. (ii) Combine the new hydrated author nodes and original paper nodes with 768 dim float features into a single node table. (iii) Add the node degree of each node as the 769th feature. (iv) Combine the paper-cites-paper and the author-writes-paper edge table to a single edge table.

\section{Additional Offline Experiments}
\label{appx:experiments}

In \cref{tab:scale-het}, we provide runtime and offline evaluations on an internal heterogeneous (bipartite) graph, with the GraphDB backend for Subgraph Sampler. This graph contains $\sim$114.6M nodes in one node type (482 features), $\sim$10.8M nodes in another node type (124), and $\sim$4.6B edges (5 edge features).


\subsection{Varying Level of Parallelism and Configurations}

We also conduct runtime experiments with different resource settings and hyperparameters. \cref{tab:resource} shows their component-level runtimes as well as offline MRR for link prediction. Following are the details of each setting:

\noindent $\bullet$ \textbf{Half training GPU}. We reduce the number of training GPUs from 16 to 8. This affects training only, since inference is done with CPUs.

\noindent $\bullet$ \textbf{Half inference CPU}. We reduce the number of inference CPUs by half. This only affects inferencer and not trainer or other components.

\noindent $\bullet$ \textbf{Earyly stop patience * 2}. Double the early stop patience will result in better loss optimization and metric, but slower training time.

\noindent $\bullet$ \textbf{\# of positive samples * 2}. We double the positive labels sampled for each training sample. This increase Subgraph Sampler \& trainer run time.

\noindent $\bullet$ \textbf{\# of neighbors in subgraph sampling / 2}. We reduce the fanout of the neighborhood subgraph at each hop by half. This improves multiple components' (mainly Subgraph Sampler) run time. 

\noindent $\bullet$ \textbf{Remove edge features}. We specify the config to remove the use of any edge features. This will in turn speed up all components. Removing edge features is more efficient than reducing node features, since edge are at the scale of billions in the internal dataset comparing millions for nodes. 

\section{Config Examples}
\label{appx:config}

\paragraph{Example Task Config} for GiGL pipeline on MAG240M dataset for the task of link prediction: \url{https://github.com/snap-research/GiGL/blob/main/examples/MAG240M/task_config.yaml}. Detailed implementation of GiGL end-to-end pipeline on MAG240M dataset for link prediction is also provided as an example in the GiGL repository\url{https://github.com/snap-research/GiGL/tree/main/examples/MAG240M}.

\paragraph{Example Resource Config} used w/ \verb+task_config.yaml+ specified above: \url{https://github.com/snap-research/GiGL/blob/main/examples/MAG240M/resource_config.yaml}.


\begin{comment}
\begin{minted}[
    frame=single,
    linenos,
    fontsize=\footnotesize
  ]{yaml}
# GraphMetadata specified what edge and node types are present in the graph.
# Note all the edge / node types here should be referenced in the preprocessor_config
graphMetadata:
  edgeTypes:
  - dstNodeType: paper_or_author
    relation: references
    srcNodeType: paper_or_author
  nodeTypes:
  - paper_or_author
taskMetadata:
  nodeAnchorBasedLinkPredictionTaskMetadata:
    # Specifying that we will perform node anchor based link prediction on edge of type: paper_or_author -> references -> paper_or_author
    supervisionEdgeTypes: 
      - srcNodeType: paper_or_author
        relation: references
        dstNodeType: paper_or_author
# Shared config specifies some extra metadata about the graph structure management of orchestration.
sharedConfig:
  isGraphDirected: True
  shouldSkipAutomaticTempAssetCleanup: true
datasetConfig:
  dataPreprocessorConfig:
    dataPreprocessorConfigClsPath: examples.MAG240M.preprocessor_config.Mag240DataPreprocessorConfig
    # our implementation takes no runtime arguments; if provided these are passed to the constructor off dataPreprocessorConfigClsPath
    # dataPreprocessorArgs:
  subgraphSamplerConfig:
    numHops: 2  # Each subgraph that is computed will be of 2 hops
    numNeighborsToSample: 15 # And, we will sample 15 neighbors at each hop for each node
    numUserDefinedPositiveSamples: 1 # We will sample 1 positive sample per anchor node
  splitGeneratorConfig:
    assignerArgs:
      seed: '42'
      test_split: '0.2'
      train_split: '0.7'
      val_split: '0.1'
    # Since the positive labels are user defined we use the following setup.
    # More assigner and split strategies can be found in splitgenerator.lib.assigners and 
    # splitgenerator.lib.split_strategies respectively.
    assignerClsPath: splitgenerator.lib.assigners.UserDefinedLabelsEdgeToLinkSplitHashingAssigner
    splitStrategyClsPath: splitgenerator.lib.split_strategies.UserDefinedLabelsNodeAnchorBasedLinkPredictionSplitStrategy
trainerConfig:
  # GiGL provides a basic implementation of a NABLP trainer; customers are encouraged to extend this class to suit their needs.
  trainerClsPath: gigl.src.common.modeling_task_specs.NodeAnchorBasedLinkPredictionModelingTaskSpec
  trainerArgs: # The following arguments are passed to trainerClsPath's constructor. See class implementation for more details.
    early_stop_patience: '5'
    early_stop_criterion: 'loss'
    main_sample_batch_size: '512' # Reduce batch size if Cuda OOM. Note that train/validation/test loss is associated with this batch size.
    num_test_batches: '400'  # Increase this number to get more stable test loss
    num_val_batches: '192'
    random_negative_sample_batch_size: '512'
    random_negative_sample_batch_size_for_evaluation: '1000' # The validation/test MRR and hit rates are associated with this batch size.
    val_every_num_batches: '100'  # Trains the model for 100 batches, evaluate it, and mark it as the best checkpoint.
    # More data loaders prefetch more data into memory, which significantly saves data read and preprocess time.
    # However, it also significantly increases CPU memory consumption and could lead to CPU memory OOM.
    # The CPU memory consumption depends on both the number of data loaders and the batch size. 
    train_main_sample_num_workers: '10'
    train_random_sample_num_workers: '10'
    val_main_sample_num_workers: '4'
    val_random_sample_num_workers: '4'
    test_main_sample_num_workers: '8'
    test_random_sample_num_workers: '8'
inferencerConfig:
  # inferencerArgs: We don't need to pass any special arguments for inferencer
  # Note: The inferencerClsPath is the same as the trainerClsPath
  # This is because NodeAnchorBasedLinkPredictionModelingTaskSpec implements both BaseTrainer (interface class needs to implement for training) 
  # and BaseInferencer (interface class needs to implement for inference). See their respective definitions for more information:
  # - gigl.src.training.v1.lib.base_trainer.BaseTrainer
  # - gigl.src.inference.v1.lib.base_inferencer.BaseInferencer
  inferencerClsPath: gigl.src.common.modeling_task_specs.NodeAnchorBasedLinkPredictionModelingTaskSpec
\end{minted}
\end{comment}

\begin{comment}
\begin{minted}[    
    frame=single,
    linenos,
    fontsize=\footnotesize
]{yaml}
shared_resource_config:
  resource_labels:
  # These are compute labels that we will try to attach to the resources created by GiGL components.
  # More information: https://cloud.google.com/compute/docs/labeling-resources.
  # These can be mostly used to get finer gained cost reporting through GCP billing on individual component
  # and pipeline costs.

  # If COMPONENT is provided in cost_resource_group_tag, it will be automatically be replaced with one of 
  # {pre|sgs|spl|tra|inf|pos} standing for: {Preprocessor | Subgraph Sampler | Split Generator | Trainer 
  #   | Inference | Post  Processor} so we can get more accurate cost measurements 
  #   of each component.

    cost_resource_group_tag: dev_experiments_COMPONENT
    cost_resource_group: gigl_platform
  common_compute_config:
    project: "USER_PROVIDED_PROJECT"
    region: "us-central1"
    # We recommend using the same bucket for temp_assets_bucket and temp_regional_assets_bucket
    # These fields will get combined into one in the future. Note: Usually storage for regional buckets is cheaper,
    # thus that is recommended.
    temp_assets_bucket: "gs://USER_PROVIDED_TEMP_ASSETS_BUCKET"
    temp_regional_assets_bucket: "gs://USER_PROVIDED_TEMP_ASSETS_BUCKET"
    perm_assets_bucket: "gs://USER_PROVIDED_PERM_ASSETS_BUCKET"
    temp_assets_bq_dataset_name: "gigl_temp_assets"
    embedding_bq_dataset_name: "gigl_embeddings"
    gcp_service_account_email: "USER_PROVIDED_SA@USER_PROVIDED_PROJECT.iam.gserviceaccount.com"
    dataflow_runner: "DataflowRunner"
preprocessor_config:
  edge_preprocessor_config:
    num_workers: 1
    max_num_workers: 256
    machine_type: "n2d-highmem-64"
    disk_size_gb: 300
  node_preprocessor_config:
    num_workers: 1
    max_num_workers: 128
    machine_type: "n2d-highmem-64"
    disk_size_gb: 300
subgraph_sampler_config:
  machine_type: "n2d-highmem-32"
  num_local_ssds: 16
  num_replicas: 240
split_generator_config:
  machine_type: "n2d-standard-16"
  num_local_ssds: 2
  num_replicas: 256
trainer_config:
  vertex_ai_trainer_config:
    machine_type: "n1-highmem-8"
    gpu_type: "nvidia-tesla-v100"
    gpu_limit: 1
    num_replicas: 16
inferencer_config:
  num_workers: 1
  max_num_workers: 256
  machine_type: "c2d-highmem-32"
  disk_size_gb: 100
\end{minted}
\end{comment}
