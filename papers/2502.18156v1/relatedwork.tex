\section{Related work}
\xhdr{Explainability in ML}
The literature on ML explainability is vast. There are several ways to categorize explainability methods, \eg, perturbation \vs gradient-based, feature \vs concept \vs prototype-based, importance \vs counterfactual-based and optimization \vs self-generated.
See \citet{gilpin2018explaining}, \citet{10.1145/3236009}, and \citet{zhao2024explainability} for details and alternative categorizations. 



\xhdr{Counterfactual explanations in ML}
See Section~\ref{sec:intro} for a comparison between counterfactual explanations (CEs) and other forms of explainability.
Generating valid and plausible CEs is a longstanding challenge~\cite{verma2024counterfactual}.
For instance, \citet{delaney2023counterfactual} highlight discrepancies between human-generated and computationally generated CEs. Through two user studies, they find that humans tend to make larger, more meaningful modifications to misclassified images, whereas computational methods prioritize minimal edits.
Prior work has also highlighted the need for generating on-manifold CEs to ensure plausibility and robustness~\cite{tsiourvas2024manifold,slack2021counterfactual}. Modeling the data manifold, however, is a challenging problem, even for non-LLM models~\cite{arvanitidis2016locally}.





\xhdr{Self-explanation by LLMs}
While a large class of explainability methods require whitebox model access SEs can be deployed in a balckbox manner.
SEs can take many forms, \eg, chain-of-thought (CoT) reasoning~\citep{agarwal2024faithfulness} and feature attributions~\cite{tanneru2024quantifying}.
However, research indicates that CoT and feature attributions are not always a faithful representation of the model's decision-making process \citep{turpin2024language,lanham2023measuring,tanneru2024quantifying}.
Our protocol for testing \SCEs is distinct from both CoT and feature-attribution based self-explanations.

\citet{chen2023models} argue that effective explanations should empower users to predict how a model will handle different yet related inputs, a concept referred to as simulatability. 
Their experiments tested whether GPT-3.5's ability to generate CEs depends on the quality of the examples provided. Interestingly, GPT-3.5 was able to produce comparable (to humans) CEs even when presented with illogical examples, suggesting that its CEs generation capabilities stem more from its pre-training than from the specific examples included in the prompt.
Unlike \citet{chen2023models}, our focus is not on human simulatability of \SCEs.


\xhdr{LLMs for explanations}
LLMs are also used to generate explanations for other  models~\cite{bhattacharjee2024towards,slack2023explaining,nguyen2024llms,li2023prompting,gat2023faithful}. Unlike these studies, our focus is on explaining the LLM itself.
Additionally, the approach of \citet{nguyen2024llms} and \citet{li2023prompting} involved explicitly providing the model with the original human gold labels in the prompt, without assessing the model's independent decision or understanding. As argued by \citet{jacovi2020towards}, the evaluation of faithfulness should not involve human-provided gold labels because relying on gold labels is influenced by human priors on what the model should do.