\section{Limitations}
Our work has several limitations. First, explainability and privacy can sometimes be at odds with each other. Even if LLMs are able to provide comprehensive and faithful explanations, this can introduce privacy and security concerns \citep{pawlicki2024explainability, grant2020show}. Detailed explanations may inadvertently expose sensitive information or be exploited for adversarial attacks on the model itself. However, our work focuses on publicly available models and datasets, ensuring that these risks are mitigated. 

Similarly, savvy users can strategically use counterfactual explanations to unfairly maximize their chances of receiving positive outcomes~\cite{tsirtsis2020decisions}. Detecting and limiting this behavior would be an important desideratum before deployment of LLM counterfactuals. 

Our analyses in this paper solely focused on automated metrics to evaluate quality of \SCEs.
Future studies can conduct human surveys to assess how plausible the explanations appear from a human perspective. This feedback can then be used to enhance the model's performance through methods such as direct preference optimization \citep{rafailov2024direct}. 

