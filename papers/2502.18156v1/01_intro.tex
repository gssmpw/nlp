\section{Introduction}
\label{sec:intro}

Large language models (LLMs) are rapidly gaining adoption due to their remarkable capabilities across a wide spectrum of real-world tasks~\citep{bommasani2021opportunities, wei2022emergent, maynez2023benchmarking,bubeck2023sparks}.
Recent analyses show that LLMs can lead to significant productivity gains, and can match or even surpass human-level performance~\cite{luo2024large,yang2024harnessing,peng2023impact}.
These impressive achievements are often attributed to large datasets, model sizes~\cite{kaplan2020scaling,hoffmann2022empirical} and the effect of alignment with human preferences~\cite{guo2025deepseek,ouyang2022training}. 
The resulting model complexity, however, means that the LLM outputs can be difficult to explain.


A number of recent studies have looked into explaining predictions of LLMs~\citep[inter alia]{templeton2024scaling,zhao2024explainability,bricken2023monosemanticity}. Explainability had been a thoroughly studied area of ML even before the advent of modern LLMs~\cite{gilpin2018explaining,10.1145/3236009}. Consequently, a large number of attempts to explain LLM outputs build upon those designed for non-LLM models. For the most part, these techniques operate by computing model gradients or solving specially designed optimization problems to find input features~\cite{cohen2025contextcite}, model neurons~\cite{templeton2024scaling,meng2022locating}, abstract concepts~\cite{kim2018interpretability,xu2025uncovering} or training data points~\cite{park2023trak} that caused the model to depict a certain behavior. 

In parallel, inspired by the reasoning abilities of LLMs~\cite{wei2022chain}, recent work has started exploring whether LLMs can \textit{explain themselves} without the need for costly operations like computing gradients or solving optimization problems. For instance, \citet{bubeck2023sparks} show that GPT-4 can provide reasoning for its answers and in the process, can even admit its mistakes. 
A fast-emerging branch of explainability literature focuses on methods for producing and evaluating \textit{self-generated explanations}~\cite{tanneru2024quantifying,turpin2023language,agarwal2024faithfulness,lanham2023measuring, guo2025deepseek}.

In this work, we focus on a specific type of self-generated explanations called \textit{self-generated counterfactual explanations} (\SCEs). Given an input $\xb$ and the model output $\hat{y}$, the counterfactual $\xbce$ is a modified version of the input that would lead the model to generate another output $\hat{\yce} \neq \hat{y}$~\cite{wachter2017counterfactual}. Prior work argues that the contrastive nature of counterfactual explanations means that they are better aligned with human reasoning~\cite{miller2019explanation}, are better suited to regulation around right-to-explanation~\cite{wachter2017counterfactual} and constitute a better test of knowledge~\cite{sep-knowledge-analysis} than other input-feature based explanations like LIME~\cite{ribeiro2016should} and SHAP~\cite{lundberg2017unified}.


\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.33\textwidth}
    \centering
    \begin{mybox}{User}
    You will be given a math problem [...] Your task is to provide the solution.
    \\
    \\
    Ken created a care package to send to his brother, who was away at boarding school.  Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.  Then, he added enough brownies to cause the weight to triple.  Next, he added another 2 pounds of jelly beans.  And finally, he added enough gummy worms to double the weight once again.  What was the final weight of the box of goodies, in pounds?
    \end{mybox}
    \begin{mybox}{Model}
        16
    \end{mybox}
    \colorbox{ForestGreen!20}{\tt\small Model answers correctly}
    \caption{Model response on original problem.}
    \label{fig:example_pred_original}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
    \centering
    \begin{mybox}{User}
    Now, revise the math problem so your final answer to the revised problem becomes \textcolor{teal}{\textbf{25}}. Share the revised problem.
    \end{mybox}
    \begin{mybox}{Model}
    Ken created a care package to send to his brother, who was away at boarding school.  Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to \textcolor{teal}{\textbf{3 pounds}}.  Then, he added enough brownies to cause the weight to triple.  Next, he added another 2 pounds of jelly beans.  And finally, he added enough gummy worms to double the weight once again.  What was the final weight of the box of goodies, in pounds?
    \end{mybox}
    \colorbox{BrickRed!15}{\tt \small Correct answer would be 22}
    \caption{Self-generated counterfactual}
    \label{fig:example_ce_gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\textwidth}
    \centering
    \begin{mybox}{User}
        You will be given a math problem. [...] Your task is to provide the solution. 
        \\
        \\
        Ken created a care package to send to his brother, who was away at boarding school.  Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to \textcolor{teal}{\textbf{3 pounds}}.  Then, he added enough brownies to cause the weight to triple.  Next, he added another 2 pounds of jelly beans.  And finally, he added enough gummy worms to double the weight once again.  What was the final weight of the box of goodies, in pounds?
    \end{mybox}
    \begin{mybox}{Model}
        26
    \end{mybox}
    \colorbox{BrickRed!15}{\tt \small \SCE doesn't yield target 25}
    \caption{Evaluation of self-explanation}
    \label{fig:example_pred_ce}
    \end{subfigure}
    \caption{LLMs are unable to explain themselves counterfactually. Explanation generation behavior of \texttt{LLaMA-3.1-70B-instruct} on an example from GSM8K data.
    In the left panel, the model \textcolor{ForestGreen}{correctly answers} a math problem. In the second panel, the model is asked to produce a self-generated counterfactual explanation (\SCE) by revising the original problem so that the answer becomes \textcolor{teal}{25}. The resulting \SCE is incorrect. The correct answer would be \textcolor{BrickRed}{22} instead of the targeted answer of \textcolor{teal}{25}. In the third panel, the \SCE is given as a new problem to the model. The model answers  with \textcolor{BrickRed}{26} which does not yield the target \textcolor{teal}{25}. This figure is best viewed in color.}
    \label{fig:example}
\end{figure*}











Our goal in this paper is to study the \textbf{efficacy of LLMs in  generating \SCEs}. We break down our investigation into three research questions (RQs):

\begin{description}
    \setlength{\itemsep}{0em}
    \item [RQ1] Are LLMs able to generate \SCEs at all?
    \item [RQ2] Do LLM predictions on \SCEs actually yield the target label?
    \item [RQ3] Do LLMs need to make large-scale changes to the input to generate \SCEs?
\end{description}

\noindent
To answer these questions, we design the procedure detailed in~\autoref{fig:example}. We ask the model to make a prediction (\autoref{fig:example_pred_original}); then ask it to generate a \SCE (\autoref{fig:example_ce_gen}); and finally compute the model's prediction on the \SCE it generated.

We evaluate the efficacy of \SCEs,
by conducting extensive experiments on seven LLMs (sizes between 7B to 70B parameters) and six datasets that correspond to four unique tasks. 
All except one LLMs are consistently able to generate \SCEs (RQ1). However, in many cases, the model predictions on \SCEs do not yield the target label, meaning that models are unable to generate valid counterfactuals (RQ2). On the GSM8K mathematics dataset, the validity of counterfactuals is especially low.

We find that when generating the model prediction on a \SCE, the presence of the original prediction (\autoref{fig:example_pred_original}) and the instruction to generate the \SCE (\autoref{fig:example_ce_gen}) in the context window has a large impact on the eventual prediction on \SCE,  pointing to flaws in the internal reasoning process of LLMs. Within the same dataset, models show a large spread in the amount of changes they make to the original input when generating the \SCE.
Overall, our results show that \textbf{despite their impressive reasoning abilities, modern LLMs are far from perfect when explaining their own predictions counterfactually}.




