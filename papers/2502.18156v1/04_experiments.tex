\section{Experimental setup} \label{sec:experiments}


In this section, we describe the datasets, models and parameters used in our experiments.

\subsection{Datasets}

To gain comprehensive insights into the counterfactual generation ability of LLMs, we consider datasets from four different domains: decision making, sentiment classification, mathematics and natural language inference. We describe each dataset in detail.

\xhdrnodot{1. DiscrimEval} (decision making) by \citet{tamkin2023evaluating} is a benchmark featuring $70$ hypothetical decision-making scenarios. Each prompt instructs the model to make a binary decision regarding an individual, \eg, whether the individual should be granted a loan, whether the individual should receive medical treatment. The prompts are designed such that a \emph{yes} decision is always desirable.
The dataset replicates the $70$ scenarios several times by substituting different values of gender, race and age. We fix these features to arbitrarily selected values of female, white, and 20 years old.


\xhdrnodot{2. FolkTexts} (decision making) by \citet{cruz2024evaluating} is a Q\&A dataset suite derived from the 2018 US Census PUMS data. 
Each instance consists of a a textual description of an individual's features like their educational status, age, and occupation. The modeling task is to predict whether the yearly income of the individual exceeds $\$50,000$.

\xhdrnodot{3. Twitter financial news} (sentiment classification) by \citet{twitter_news}  provides an annotated corpus of finance-related tweets, specifically curated for sentiment analysis in the financial domain. Each tweet is labeled with one of three sentiment classes: \textit{Bearish}, \textit{Bullish}, or \textit{Neutral}. 

\xhdrnodot{4. SST2} (sentiment classification) or Stanford Sentiment Treebank by \citet{socher-etal-2013-recursive} consists of single sentence movie reviews along with the binary sentiment (positive and negative).

\xhdrnodot{5. GSM8K} (math) or Grade School Math 8K by \citet{cobbe2021gsm8k} consists of mathematical problems. The answer to the problems is always a positive integer. The original dataset also consist of reasoning steps that one could follow to solve the problem. For simplicity, we ask the model to only generate the answer.

\xhdrnodot{6. Multi-Genre Natural Language Inference (MGNLI)} by \citet{N18-1101} consists of pairs of sentences, the premise, and the hypothesis. The model is asked to  classify the relationship between two sentences. The relationship values can be: entailment, neutral  or contradiction.



\subsection{Models, infrastructure, and parameters}
We aim to gain insights into the counterfactual generation ability of models across various architectural aspects. 
The models we study are:


\xhdrnodot{Small models} namely \texttt{Gemma-2-9B-it} henceforth referred to as \gemmaS, \texttt{Llama-3.1-8B-Instruct} (\llamaS),  \texttt{Mistral-7B-Instruct-v0.3} (\mistralS).  

\xhdrnodot{Medium models} consist of \texttt{Gemma-2-27B-it} (\gemmaM), \texttt{Llama-3.3-70B-Instruct} (\llamaM), and \texttt{Mistral-Small-24B-Instruct-2501} (\mistralM).  

\xhdrnodot{Reasoning model.} We only consider \texttt{DeepSeek-R1-Distill-Qwen-32B} (\rd).  

All experiments were run on a single node with 8x NVIDIA H200 GPUs. The machine was shared between multiple research teams.
We ran all the models in their original 16-bit precision and did not employ any size reduction strategies like quantization.
For each model, we consider two temperature values, $T=0$ and $T=0.5$.

For generating the counterfactuals, one needs to provide the model with the target label $\yce$. For classification datasets, we select $\yce$ from the set $\Ycal - \{\hat{y}\}$ at random. For the GSM8K data, we generate $\yce = \hat{y} +
\epsilon$ with $\epsilon$ was sampled from a uniform distribution
$\text{Unif}(1, 2, \ldots, 10)$.%

Given the high cost of LLM inference, we subsample datasets as follows: For classification datasets, we select $250$ samples at random from each class. For the non-classification, GSM8K data, we also randomly select $250$ samples.
While we did not track the precise time, the experiments took several days on multiple GPUs to complete.

We occasionally used ChatGPT for help with coding errors.
