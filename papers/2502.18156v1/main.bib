% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}


@inproceedings{chatzi2025ce,
  title={Counterfactual Token Generation in Large Language Models},
  author={Chatzi, Ivi and Benz, Nina L Corvelo and Straitouri, Eleni and Tsirtsis, Stratis and Rodriguez, Manuel Gomez},
  booktitle={Proceedings of the 4th Conference on Causal Learning and Reasoning},
  year={2025},
}


@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}



@inproceedings{snyder_early_2024,
author = {Snyder, Ben and Moisescu, Marius and Zafar, Muhammad Bilal},
title = {On Early Detection of Hallucinations in Factual Question Answering},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671796},
doi = {10.1145/3637528.3671796},
abstract = {While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2721â€“2732},
numpages = {12},
keywords = {LLM hallucinations, question answering},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{wachter2017counterfactual,
  title={Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{maynez2023benchmarking,
  title={Benchmarking large language model capabilities for conditional generation},
  author={Maynez, Joshua and Agrawal, Priyanka and Gehrmann, Sebastian},
  journal={arXiv preprint arXiv:2306.16793},
  year={2023}
}
@article{deyoung2019eraser,
  title={ERASER: A benchmark to evaluate rationalized NLP models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  journal={arXiv preprint arXiv:1911.03429},
  year={2019}
}
@article{jacovi2020towards,
  title={Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.03685},
  year={2020}
}
@article{agarwal2024faithfulness,
  title={Faithfulness vs. plausibility: On the (un) reliability of explanations from large language models},
  author={Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2402.04614},
  year={2024}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{wu2020perturbed,
  title={Perturbed masking: Parameter-free probing for analyzing and interpreting BERT},
  author={Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  journal={arXiv preprint arXiv:2004.14786},
  year={2020}
}
@inproceedings{di2024explanation,
  title={Is explanation all you need? an expert survey on llm-generated explanations for abusive language detection},
  author={Di Bonaventura, Chiara and Siciliani, Lucia and Basile, Pierpaolo and Penuela, Albert Merono and McGillivray, Barbara},
  booktitle={Tenth Italian Conference on Computational Linguistics (CLiC-it 2024)},
  year={2024}
}
@article{nguyen2024llms,
  title={LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study},
  author={Nguyen, Van Bach and Youssef, Paul and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  journal={arXiv preprint arXiv:2405.00722},
  year={2024}
}
@article{gat2023faithful,
  title={Faithful explanations of black-box nlp models using llm-generated counterfactuals},
  author={Gat, Yair and Calderon, Nitay and Feder, Amir and Chapanin, Alexander and Sharma, Amit and Reichart, Roi},
  journal={arXiv preprint arXiv:2310.00603},
  year={2023}
}
@article{chen2023models,
  title={Do models explain themselves? counterfactual simulatability of natural language explanations},
  author={Chen, Yanda and Zhong, Ruiqi and Ri, Narutatsu and Zhao, Chen and He, He and Steinhardt, Jacob and Yu, Zhou and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2307.08678},
  year={2023}
}
@article{li2023prompting,
  title={Prompting large language models for counterfactual generation: An empirical study},
  author={Li, Yongqi and Xu, Mayi and Miao, Xin and Zhou, Shen and Qian, Tieyun},
  journal={arXiv preprint arXiv:2305.14791},
  year={2023}
}
@article{sachdeva2023catfood,
  title={Catfood: Counterfactual augmented training for improving out-of-domain performance and calibration},
  author={Sachdeva, Rachneet and Tutek, Martin and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2309.07822},
  year={2023}
}
@inproceedings{bhattacharjee2024towards,
  title={Towards llm-guided causal explainability for black-box text classifiers},
  author={Bhattacharjee, Amrita and Moraffah, Raha and Garland, Joshua and Liu, Huan},
  booktitle={AAAI 2024 Workshop on Responsible Language Models, Vancouver, BC, Canada},
  year={2024}
}
@phdthesis{yucel2024metrics,
  title={Metrics to Ascertain the Plausibility and Faithfulness of Counterfactual Explanations},
  author={Y{\"u}cel, Ali Faruk},
  year={2024},
  school={Delft University of Technology}
}
@inproceedings{pawelczyk2022exploring,
  title={Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis},
  author={Pawelczyk, Martin and Agarwal, Chirag and Joshi, Shalmali and Upadhyay, Sohini and Lakkaraju, Himabindu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4574--4594},
  year={2022},
  organization={PMLR}
}
@article{chernyshev2024u,
  title={U-math: A university-level benchmark for evaluating mathematical skills in llms},
  author={Chernyshev, Konstantin and Polshkov, Vitaliy and Artemova, Ekaterina and Myasnikov, Alex and Stepanov, Vlad and Miasnikov, Alexei and Tilga, Sergei},
  journal={arXiv preprint arXiv:2412.03205},
  year={2024}
}
@article{tamkin2023evaluating,
  title={Evaluating and mitigating discrimination in language model decisions},
  author={Tamkin, Alex and Askell, Amanda and Lovitt, Liane and Durmus, Esin and Joseph, Nicholas and Kravec, Shauna and Nguyen, Karina and Kaplan, Jared and Ganguli, Deep},
  journal={arXiv preprint arXiv:2312.03689},
  year={2023}
}
@article{cruz2024evaluating,
  title={Evaluating language models as risk scores},
  author={Cruz, Andr{\'e} F and Hardt, Moritz and Mendler-D{\"u}nner, Celestine},
  journal={arXiv preprint arXiv:2407.14614},
  year={2024}
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of
               the North American Chapter of the
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{mothilal2020explaining,
  title={Explaining machine learning classifiers through diverse counterfactual explanations},
  author={Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={607--617},
  year={2020}
}

@article{verma2024counterfactual,
  title={Counterfactual explanations and algorithmic recourses for machine learning: A review},
  author={Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag},
  journal={ACM Computing Surveys},
  volume={56},
  number={12},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}


@misc{twitter_news,
  author = {ZeroShot},
  title = {Twitter Financial News dataset},
  howpublished = "\url{https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment}",
  year = {2022}, 
  note = "Accessed: Feb 2025"
}


@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}


@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{peng2023impact,
  title={The impact of ai on developer productivity: Evidence from github copilot},
  author={Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  journal={arXiv preprint arXiv:2302.06590},
  year={2023}
}


@article{luo2024large,
  title={Large language models surpass human experts in predicting neuroscience results},
  author={Luo, Xiaoliang and Rechardt, Akilles and Sun, Guangzhi and Nejad, Kevin K and Y{\'a}{\~n}ez, Felipe and Yilmaz, Bati and Lee, Kangjoo and Cohen, Alexandra O and Borghesani, Valentina and Pashkov, Anton and others},
  journal={Nature human behaviour},
  pages={1--11},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{yang2024harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={ACM Transactions on Knowledge Discovery from Data},
  volume={18},
  number={6},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}

@misc{templeton2024scaling,
  title={Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread},
  author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and others},
  year={2024}
}

@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}


@article{park2023trak,
  title={Trak: Attributing model behavior at scale},
  author={Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and Madry, Aleksander},
  journal={arXiv preprint arXiv:2303.14186},
  year={2023}
}

@article{cohen2025contextcite,
  title={Contextcite: Attributing model generation to context},
  author={Cohen-Wang, Benjamin and Shah, Harshay and Georgiev, Kristian and Madry, Aleksander},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={95764--95807},
  year={2025}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}


@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@article{10.1145/3236009,
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
title = {A Survey of Methods for Explaining Black Box Models},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {93},
numpages = {42},
keywords = {Open the black box, explanations, interpretability, transparent models}
}

@inproceedings{gilpin2018explaining,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}


@inproceedings{tanneru2024quantifying,
  title={Quantifying uncertainty in natural language explanations of large language models},
  author={Tanneru, Sree Harsha and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1072--1080},
  year={2024},
  organization={PMLR}
}

@article{turpin2023language,
  title={Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74952--74965},
  year={2023}
}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@InCollection{sep-knowledge-analysis,
	author       =	{Ichikawa, Jonathan Jenkins and Steup, Matthias},
	title        =	{{The Analysis of Knowledge}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/fall2024/entries/knowledge-analysis/}},
	year         =	{2024},
	edition      =	{{F}all 2024},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}


@article{lanham2023measuring,
  title={Measuring faithfulness in chain-of-thought reasoning},
  author={Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and others},
  journal={arXiv preprint arXiv:2307.13702},
  year={2023}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Scott Lundberg and Su-In Lee},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={4765--4774},
  year={2017},
  publisher={Curran Associates, Inc}
}

@article{slack2023explaining,
  title={Explaining machine learning models with interactive natural language conversations using TalkToModel},
  author={Slack, Dylan and Krishna, Satyapriya and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Nature Machine Intelligence},
  volume={5},
  number={8},
  pages={873--883},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{delaney2023counterfactual,
  title={Counterfactual explanations for misclassified images: How human and machine explanations differ},
  author={Delaney, Eoin and Pakrashi, Arjun and Greene, Derek and Keane, Mark T},
  journal={Artificial Intelligence},
  volume={324},
  pages={103995},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{cito2022counterfactual,
  title={Counterfactual explanations for models of code},
  author={Cito, J{\"u}rgen and Dillig, Isil and Murali, Vijayaraghavan and Chandra, Satish},
  booktitle={Proceedings of the 44th international conference on software engineering: software engineering in practice},
  pages={125--134},
  year={2022}
}

@article{gajcin2024redefining,
  title={Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities},
  author={Gajcin, Jasmina and Dusparic, Ivana},
  journal={ACM Computing Surveys},
  volume={56},
  number={9},
  pages={1--33},
  year={2024},
  publisher={ACM New York, NY}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{pearl2009causal,
  title={Causal inference in statistics: An overview},
  author={Pearl, Judea},
  journal={Statistics Surveys},
  year={2009}
}

@book{molnar2020interpretable,
  title={Interpretable machine learning},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu. com}
}

@article{turpin2024language,
  title={Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{hallucination_snowball,
author = {Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A.},
title = {How language model hallucinations can snowball},
year = {2024},
publisher = {JMLR.org},
abstract = {A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we show that LMs sometimes produce hallucinations that they can separately recognize as incorrect. To do this, we construct three question-answering datasets where LMs often state an incorrect answer which is followed by an explanation with at least one incorrect claim. Crucially, we find that GPT-3.5, GPT-4, and LLaMA2-70B-chat can identify 67\%, 87\%, and 94\% of these incorrect claims, respectively. We show that this phenomenon doesn't disappear under higher temperatures sampling, beam search, and zero-shot chain-of-thought prompting. These findings reveal that LM hallucinations can snowball: early mistakes by an LM can lead to more mistakes that otherwise would not be made.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2465},
numpages = {15},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{azaria-mitchell-2023-internal,
    title = "The Internal State of an {LLM} Knows When It`s Lying",
    author = "Azaria, Amos  and
      Mitchell, Tom",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.68/",
    doi = "10.18653/v1/2023.findings-emnlp.68",
    pages = "967--976",
    abstract = "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM`s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71{\%} to 83{\%} accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier`s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios."
}

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}


@inproceedings{tsiourvas2024manifold,
  title={Manifold-Aligned Counterfactual Explanations for Neural Networks},
  author={Tsiourvas, Asterios and Sun, Wei and Perakis, Georgia},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3763--3771},
  year={2024},
  organization={PMLR}
}

@article{slack2021counterfactual,
  title={Counterfactual explanations can be manipulated},
  author={Slack, Dylan and Hilgard, Anna and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={62--75},
  year={2021}
}

@article{arvanitidis2016locally,
  title={A locally adaptive normal distribution},
  author={Arvanitidis, Georgios and Hansen, Lars K and Hauberg, S{\o}ren},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{pawlicki2024explainability,
  title={Explainability versus Security: The Unintended Consequences of xAI in Cybersecurity},
  author={Pawlicki, Marek and Pawlicka, Aleksandra and Kozik, Rafa{\l} and Chora{\'s}, Micha{\l}},
  booktitle={Proceedings of the 2nd ACM Workshop on Secure and Trustworthy Deep Learning Systems},
  pages={1--7},
  year={2024}
}

@article{grant2020show,
  title={Show us the data: Privacy, explainability, and why the law can't have both},
  author={Grant, Thomas D and Wischik, Damon J},
  journal={Geo. Wash. L. Rev.},
  volume={88},
  pages={1350},
  year={2020},
  publisher={HeinOnline}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{tsirtsis2020decisions,
  title={Decisions, counterfactual explanations and strategic behavior},
  author={Tsirtsis, Stratis and Gomez Rodriguez, Manuel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16749--16760},
  year={2020}
}

@article{xu2025uncovering,
  title={Uncovering safety risks of large language models through concept activation vector},
  author={Xu, Zhihao and Huang, Ruixuan and Chen, Changyu and Wang, Xiting},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={116743--116782},
  year={2025}
}