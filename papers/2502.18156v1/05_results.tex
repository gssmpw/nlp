\section{Results}

\input{tables/direct_prompting_temp_0}

\input{tables/rationale_prompting_temp_0}

Tables \ref{table:direct_prompt_temp0} and \ref{table:rationale_prompting_temp0} show the results when using unconstrained prompting and rationale-based prompting, respectively  at $T = 0$. Tables~\ref{table:direct_prompting_temp05} and \ref{table:rationale_prompting_temp05} showing results with a higher temperature $T = 0.5$ are included in \autoref{app:additional_results} and discussed under each RQ.


\subsection*{RQ1: Ability of LLMs to generate SCEs}

\textit{Most models are able to generate \SCEs in a vast majority of cases} 
with  the exception of \gemmaS model for the DiscrimEval, FolkTexts, and GSM8K datasets.
At a higher temperature of $0.5$ the ability of \gemmaS to generate \SCEs goes up slightly (\autoref{table:direct_prompting_temp05}), whereas for \llamaS,  it decreases slightly on average.


\textit{The fraction roughly remains the same for rationale-based prompting}  as shown in Tables \ref{table:rationale_prompting_temp0} and \ref{table:rationale_prompting_temp05}. The affect of temperature on the ability of \gemmaS and \llamaS to generate \SCEs remains the same.

\subsection*{RQ2: Do \SCEs yield the target label?}
\textit{Model generated \SCEs yield the target label in most cases, however, there are large variations.} 
The most prominent variation is along the \textit{task level}. For the more challenging mathematical reasoning tasks in GSM8K, the percentage of times the models generate valid \SCEs in only in single digits for most models. For the FolkTexts tasks which requires the model to reason through the Census-gathered data of individuals, the validity in many cases is low. 

We also see  a somewhat expected variation at \textit{model-size} level. The smaller models, \gemmaS (9B parameters), \llamaS (8B), and \mistralS (7B) tend to generate valid \SCEs at a higher rate than the larger versions \gemmaM (27B), \llamaM (70B), and \mistralM (24B). There are only few exceptions like MGNLI where \gemmaM performs better. The reasoning model \rd (32B) does not always perform better than the similarly sized models \gemmaM and \mistralM. For instance, for FolkTexts, the percentage of valid \SCEs generated by \rd is much lower than for \gemmaM or \mistralM.

\textit{Presence of the original prediction and counterfactual generation in the context window has a large impact on validity} as shown by the comparison of \Val and \ValH in Tables \ref{table:direct_prompt_temp0} and \ref{table:rationale_prompting_temp0}. Most prominently, on the GSM8K dataset, the validity increases significantly meaning that the \textbf{mathematical reasoning ability of the model is impacted by information that should be irrelevant}. On the FolkTexts data too the model deems more \SCEs as valid when the prediction and \SCE generation are provided in the context. However, for some task and model combinations, the validity rate decreases with the presence of additional information in the context. Examples include \llamaS and \llamaM on DiscrimEval and \llamaS on SST2.

\textit{Temperature generally does not impact the \SCE validity by a large amount} as shown by comparing Tables \ref{table:rationale_prompting_temp0} and \ref{table:direct_prompting_temp05}. However, there are exceptions like \llamaS and \llamaM on DiscrimEval and FolkTexts datasets.

\textit{Rationale-based prompting has diverse impact on \SCE validity} as shown by comparing Tables \ref{table:direct_prompt_temp0} and \ref{table:rationale_prompting_temp0}. In some cases like \mistralM on GSM8K, the fraction of \SCEs deemed valid by the model increases significantly from $0\%$ to $72\%$.  In other cases like \llamaM on DiscrimEval, the amount goes down from $90\%$ to $47\%$.


\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.90\columnwidth]{figures/Revised_Scenario_Length_Histogram_Without_History_L70_Math.pdf}
    \vfill
    \includegraphics[width=0.90\columnwidth]{figures/Revised_Scenario_Length_Histogram_With_History_L70_Math.pdf}
    \caption{[\llamaM on GSM8K with $T=0$] Histogram of \SCE length for a model without (top) and with (bottom) original prediction and counterfactual generation instruction in the context. In both cases, the distribution of lengths varies between valid and invalid \SCEs.}
    \label{fig:length_hist_llama70_gsm8k_temp0}
\end{figure}


\input{tables/response_len_diff_temp0_direct_prompting}

\subsection*{RQ3: Changes required to generate \SCEs}

\textit{For a given task and dataset, different LLMs require different amount of changes to generate \SCEs}, even for a similar level of validity. Consider for \mistralM, \llamaS and \llamaM models for DiscrimEval data. The validity of \SCEs ranges between $89\%$ to $100\%$ while the normalized edit distance ranges between $14$ and $61$. 

The required changes also depend on the task and dataset. For example, in SST2, where models achieve some of the highest validity scores, we observe the highest \ED.
Conversely, the lowest \ED is observed in the GSM8K dataset, where models perform the worst.
This relationship between validity and edit distance, however, is not completely linear and also depends on the input length.
In DiscrimEval and MGNLI where the input length can be several tens to hundreds of tokens, the model are able to achieve high \Val score with relatively lower values of \ED.


Temperature also influences \ED. For instance, in unconstrained prompting with a $T=0.5$ (\autoref{table:direct_prompting_temp05}), \ED values across all datasets, except for Twitter Financial News data, are consistently higher compared to $T=0$.

\textit{Rationale-based promoting does not consistently lead to closer \SCEs} as seen when comparing Tables~\ref{table:direct_prompt_temp0} and~\ref{table:rationale_prompting_temp0}. For instance, for DiscrimEval, the \ED values are generally lower with rationale-based prompting but are higher for MGNLI. 















\subsection*{Are invalid \SCEs statistically different?}
In this section, we aim to characterize the differences between valid and invalid \SCEs. Specifically, we investigate if the lengths of \SCEs can provide a clue on their validity. Our question is inspired by previous work on detecting LLM hallucinations~\cite{hallucination_snowball,snyder_early_2024,azaria-mitchell-2023-internal} which shows that incorrect model outputs show statistically different patterns from correct answers. Figure~\ref{fig:length_hist_llama70_gsm8k_temp0} shows that the length distribution of valid and invalid \SCEs is indeed different.

Next, we systematically analyze these difference in \SCE lenghts. Specifically, for each model, datasest and \SCE generation configuration, we compute the \textit{normalized difference in lengths} as
$
\frac{| L_{\text{val}} - L_{\text{inval}} |}{\max(L_{\text{val}}, L_{\text{inval}})} \times 100 
$
where $L_{\text{val}}$ is the average length of valid \SCEs.
The normalization ensures that the metric lies in the range $[0, 100]$. The results in \autoref{table:response_len_diff_direct_prompting_temp0} show that in many dataset and model combinations, the \SCEs lengths can indeed provide a signal on their validity.






