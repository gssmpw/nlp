\section{Conclusion and future work}
In this study, we examined the ability of LLMs to produce self-generated counterfactual explanations (SCEs).
We design a prompt-based setup for evaluating the efficacy of \SCEs.
Our results show that LLMs consistently struggle with generating valid \SCEs. In many cases model prediction on a \SCE does not yield the same target prediction for which the model crafted the \SCE.
Surprisingly, we find that LLMs put significant emphasis on the context---the prediction on \SCE is significantly impacted by the presence of original prediction and instructions for generating the \SCE.
Based on this empirical evidence, we argue that LLMs are still far from being able to explain their own predictions counterfactually.
Our findings add to similar insights from recent studies on other forms of self-explanations~\cite{lanham2023measuring,tanneru2024quantifying}.



Our work opens several avenues for future work. Inspired by counterfactual data augmentation~\cite{sachdeva2023catfood}, one could include the counterfactual explanation capabilities a part of the LLM training process. This inclusion may enhance the counterfactual reasoning capabilities of the LLM. Follow ups should also explore the effect of prompt tuning, specifically, model-tailored prompts for generating \SCEs. These approaches might lead to better quality \SCEs.


We limited our investigation to open source models of upto 70B parameters. Extending our analysis to larger and more recent models, \eg, DeepSeek R1 671B, and closed source models like OpenAI o3 would be an interesting avenue for future work.

Finally, our experiments were limited to relatively simple tasks: classification and mathematics problems where the solution is an integer. This limitation was mainly due to the fact that it is difficult to automatically judge validity of answers for more open-ended language generation tasks like search and information retrieval. Scaling our analysis to such tasks would require significant human-annotation resources, and is an important direction for future investigations.
