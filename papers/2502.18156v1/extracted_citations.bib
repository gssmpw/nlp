@article{10.1145/3236009,
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
title = {A Survey of Methods for Explaining Black Box Models},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {93},
numpages = {42},
keywords = {Open the black box, explanations, interpretability, transparent models}
}

@article{agarwal2024faithfulness,
  title={Faithfulness vs. plausibility: On the (un) reliability of explanations from large language models},
  author={Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2402.04614},
  year={2024}
}

@article{arvanitidis2016locally,
  title={A locally adaptive normal distribution},
  author={Arvanitidis, Georgios and Hansen, Lars K and Hauberg, S{\o}ren},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{bhattacharjee2024towards,
  title={Towards llm-guided causal explainability for black-box text classifiers},
  author={Bhattacharjee, Amrita and Moraffah, Raha and Garland, Joshua and Liu, Huan},
  booktitle={AAAI 2024 Workshop on Responsible Language Models, Vancouver, BC, Canada},
  year={2024}
}

@article{chen2023models,
  title={Do models explain themselves? counterfactual simulatability of natural language explanations},
  author={Chen, Yanda and Zhong, Ruiqi and Ri, Narutatsu and Zhao, Chen and He, He and Steinhardt, Jacob and Yu, Zhou and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2307.08678},
  year={2023}
}

@article{delaney2023counterfactual,
  title={Counterfactual explanations for misclassified images: How human and machine explanations differ},
  author={Delaney, Eoin and Pakrashi, Arjun and Greene, Derek and Keane, Mark T},
  journal={Artificial Intelligence},
  volume={324},
  pages={103995},
  year={2023},
  publisher={Elsevier}
}

@article{gat2023faithful,
  title={Faithful explanations of black-box nlp models using llm-generated counterfactuals},
  author={Gat, Yair and Calderon, Nitay and Feder, Amir and Chapanin, Alexander and Sharma, Amit and Reichart, Roi},
  journal={arXiv preprint arXiv:2310.00603},
  year={2023}
}

@inproceedings{gilpin2018explaining,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}

@article{jacovi2020towards,
  title={Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.03685},
  year={2020}
}

@article{lanham2023measuring,
  title={Measuring faithfulness in chain-of-thought reasoning},
  author={Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and others},
  journal={arXiv preprint arXiv:2307.13702},
  year={2023}
}

@article{li2023prompting,
  title={Prompting large language models for counterfactual generation: An empirical study},
  author={Li, Yongqi and Xu, Mayi and Miao, Xin and Zhou, Shen and Qian, Tieyun},
  journal={arXiv preprint arXiv:2305.14791},
  year={2023}
}

@article{nguyen2024llms,
  title={LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study},
  author={Nguyen, Van Bach and Youssef, Paul and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  journal={arXiv preprint arXiv:2405.00722},
  year={2024}
}

@article{slack2021counterfactual,
  title={Counterfactual explanations can be manipulated},
  author={Slack, Dylan and Hilgard, Anna and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={62--75},
  year={2021}
}

@article{slack2023explaining,
  title={Explaining machine learning models with interactive natural language conversations using TalkToModel},
  author={Slack, Dylan and Krishna, Satyapriya and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Nature Machine Intelligence},
  volume={5},
  number={8},
  pages={873--883},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{tanneru2024quantifying,
  title={Quantifying uncertainty in natural language explanations of large language models},
  author={Tanneru, Sree Harsha and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1072--1080},
  year={2024},
  organization={PMLR}
}

@inproceedings{tsiourvas2024manifold,
  title={Manifold-Aligned Counterfactual Explanations for Neural Networks},
  author={Tsiourvas, Asterios and Sun, Wei and Perakis, Georgia},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3763--3771},
  year={2024},
  organization={PMLR}
}

@article{turpin2024language,
  title={Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{verma2024counterfactual,
  title={Counterfactual explanations and algorithmic recourses for machine learning: A review},
  author={Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag},
  journal={ACM Computing Surveys},
  volume={56},
  number={12},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}

@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

