\subsection{Reasoning-based LLMs}
The pioneering advancements in reasoning-based LLMs, such as OpenAI's O1~\cite{jaech2024openai} and DeepSeek-R1~\cite{deepseek2025r1}, have excelled in many tasks and attracted significant research attentions. While earlier explorations focus on using inference-time reasoning for solving complex tasks such as math and coding~\cite{qin2024o1,zhang2024o1}, there is a trending belief towards utilizing reasoning-based LLMs for general AI tasks. \citet{zhao2024marco} expanded reasoning-based LLMs to open-ended text generation, generalizing them to broader domains where clear standards and quantified rewards are absent. \citet{shen2025vlmr1} propose VLM-R1, a stable and generalizable R1-style LLM for vision-language tasks. In addition, reasoning-based LLMs are utilized for financial tasks~\cite{chu2025domaino1s} and adapted to multilingual reasoning~\cite{ko2025understand}.

Compared with existing endeavors on reasoning-based LLMs, our work focuses on introducing long CoTs into the field of general MT, thereby developing a reasoning-based LLM for MT.

\subsection{LLMs for Machine Translation}
Since the appearance of ChatGPT~\cite{ouyang2022training}, endeavors on applying LLMs for MT continuously emerge. Early studies focus on directly prompting LLMs to translate via prompt strategies~\cite{jiao2023chatgpt,jiao2024gradable,peng2023towards}, or fine-tune open-source LLMs using parallel corpus~\cite{xuparadigm,wu2024adapting,zhang2023machine}. Techniques are further proposed to enhance the performance of LLMs for the tasks of MT, such as continuous pre-training~\cite{boughorbel2024improving,fujiicontinual}, mixture-of-expert modules~\cite{xu2024x,zhu2025overcoming} and multi-task training~\cite{wang-etal-2024-taste,ul2024lkmt}.

Recently, MT with CoT methodology draws attentions of researchers. \citet{feng2024improving} introduced an API-based self-correcting framework for LLMs, where LLMs autonomously call external evaluation models and refine translation hypothesis based on the evaluation result. \citet{wang-etal-2024-taste} designed a multi-task training phase and a multi-stage inference phase for MT, guiding the LLM to first conduct translation task, then a evaluation task and a revision task. DRT~\cite{jiaan2024drt} merged this procedure into a inference-time CoT, utilized multi-agent mechanism to synthesize such long CoTs and examined performance in English-Chinese literature translation.

Compared with existing MT approaches, our R1-T1 incorporates six types of human-aligned CoTs for translation, possesses the ability to self-evolve its CoTs via RL, and is designed for general MT tasks. 