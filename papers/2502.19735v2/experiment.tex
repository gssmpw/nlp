
In this section, we provide a detailed overview of the experimental setup used to evaluate the effectiveness of our proposed approach. We begin by presenting the implementation details, followed by the evaluation dataset, and then move on to the core evaluation on multilingual machine translation (MT) performance. Finally, we discuss our evaluation of the model's ability to preserve general capabilities during training, and analyze the self-evolution of Chain of Thoughts (CoT) in the reinforcement learning (RL) process.

\subsection{Experiment Setting}


\subsubsection{Implement Details}

we selected Qwen2.5-7B-Instruct as backbone model due to its superior multilingual performance among open-source models with fewer than 10 billion parameters~\cite{Yang2024Qwen25TR}. This choice helps minimize potential negative impacts from insufficient base model capabilities. The dataset was randomly split into a training set and a validation set at a ratio of 9:1. In RL implementation, we utilized the modified REINFORCE++ framework ~\cite{Hu2025REINFORCEAS}, which has proven effective with low training costs. The training configuration consists of 3 epochs, a learning rate of $3 \times 10^{-7}$, a batch size of 8, and 16 rollouts. For SFT with CoTs, we used 2 epochs of full parameter fine-tuning with a learning rate of $1 \times 10^{-4}$.
\subsubsection{Evaluation Dataset}

We employed the Flores-101 test set~\cite{Goyal2021TheFE}, which is well-suited for evaluating multilingual MT models due to its extensive coverage of diverse language pairs and domains. The selection of this dataset allows us to measure the robustness and generalization of our approach across different linguistic contexts. In Table \ref{tab:different_training_style}, we summarize the token coverage between the evaluation datasets and our training sets across six languages. This helps to confirm the integrity of our experimental setup and the validity of the evaluation results.

\begin{table}[htbp]
\centering
\resizebox{1.0\linewidth}{!} {%
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset} & \textbf{ru} & \textbf{fr} & \textbf{de} & \textbf{ja} & \textbf{zh} & \textbf{en}\\
\midrule
Flores101  & 0.142 & 0.137 & 0.097 & 0.113 & 0.110 & 0.097 \\
Literature  & - & - & - & - & 0.102 & 0.105 \\

\bottomrule
\end{tabular}
}
\caption{similarity evaluation between test set and our training set.}
\label{tab:different_training_style}
\end{table}


\subsection{Evaluation on Multilingual MT}

Our experiment aims to assess the multilingual MT performance of our model. Specifically, we investigate how well the model handles multiple languages from training, including both seen and unseen languages, and compare it against relevant baselines.

\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c}
            \toprule
    \multirow{2}{*}{\textbf{Models}} & \multicolumn{5}{c}{\hspace{-1.5em}\textbf{xx2en}}       & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{en2xx}} & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{zh2xx}} & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{xx2zh}} & \textbf{avg}\\
    \cmidrule(l{0.3em}r{1em}){2-6} \cmidrule(l{0.3em}r{1em}){7-11} \cmidrule(l{0.3em}r{1em}){12-16} \cmidrule(l{0.3em}r{1em}){17-21} 
        & \textbf{zh} & \textbf{ja} & \textbf{ru} & \textbf{fr} & \textbf{de} & \textbf{zh} & \textbf{ja} & \textbf{ru} & \textbf{fr} & \textbf{de} & \textbf{en} & \textbf{ja} & \textbf{ru} & \textbf{fr} & \textbf{de} & \textbf{en} & \textbf{ja} & \textbf{ru} & \textbf{fr} & \textbf{de} & \textbf{-} \\ \midrule
    \multicolumn{21}{c}{\textbf{General-purpose LLMs}} \\
    \hline

    Q2.5$^{\mathrm{a}}$-7b-base & 0.700 & 0.659 & 0.649 & \textbf{0.800} & 0.747 & 0.624 & 0.435 & 0.538 & \textbf{0.759} & 0.559 &  0.700 & 0.583 & 0.525 & 0.565 & 0.527 & 0.624 & 0.531 & 0.489 & \textbf{0.555} & \textbf{0.475} & 0.602 \\
    Q2.5-7b-Ins$^{\mathrm{b}}$ & 0.673 & 0.641 & 0.556 & 0.736 & 0.604 & 0.622 & 0.452 & 0.343 & 0.495 & 0.453 & 0.673 & 0.550 & 0.465 & 0.531 & 0.473 & 0.622 & 0.450 & 0.088 & 0.152 & 0.280 & 0.493 \\
    DS-R1-D$^{\mathrm{c}}$ & 0.572 & 0.377 & 0.441 & 0.684 & 0.577 & 0.452 & -1.020 & -0.851 & 0.033 & -0.454 & 0.572 & 0.009 & 0.199 & 0.290 & 0.206 & 0.452 & -0.621 & -0.903 & -0.222 & -0.580 & 0.011 \\
    DS-R1-DM$^{\mathrm{d}}$ & 0.572 & 0.321 & 0.437 & 0.689 & 0.577 & 0.504 & -0.662 & -0.508 & 0.077 & -0.299 & 0.572 & -0.014 & 0.266 & 0.292 & 0.193 & 0.504 & -0.474 & -0.744 & -0.279 & -0.538 & 0.074 \\
    \midrule
    \multicolumn{21}{c}{\textbf{SFT LLMs}} \\
    \hline
    Q2.5-7b-SFT (w/o CoT) & 0.702 & \textbf{0.654} & \textbf{0.650} & 0.797 & \textbf{0.749} & 0.645 & 0.588 & \textbf{0.621} & 0.742 & 0.559 & 0.702 & 0.585 & 0.525 & 0.590 & 0.565 & 0.645 & 0.573 & \textbf{0.562} & 0.539 & 0.461 & 0.623 \\
    Q2.5-7b-SFT (CoT) & 0.667 & 0.615 & 0.613 & 0.774 & 0.715 & 0.564 & 0.419 & 0.418 & 0.636 & 0.444 & 0.667 & 0.533 & 0.463 & 0.514 & 0.482 & 0.564 & 0.420 & 0.309 & 0.418 & 0.275 & 0.525 \\
    \midrule
    \multicolumn{21}{c}{\textbf{RL}} \\
    \hline
    \textbf{R1-T1} & \textbf{0.704} & 0.643 & 0.646 & 0.793 & 0.743 & \textbf{0.664} & \textbf{0.611} & 0.592 & 0.745 & \textbf{0.570} & \textbf{0.704} & \textbf{0.602} & \textbf{0.542} & \textbf{0.599} & \textbf{0.575} & \textbf{0.664} & \textbf{0.595} & 0.513 & 0.552 & 0.464 & \textbf{0.626} \\

    \bottomrule 
    \end{tabular}
    }
    \caption{R1-T1's performance on \textbf{Trained} languages in Flores-101. \textbf{xx2en} (or \textbf{xx2zh}) means the translation direction is from language \textbf{xx} to English (or Chinese). \textbf{Q2.5} means Qwen2.5. \textbf{Ins} means Instruction. \textbf{DS-R1-D} means DeepSeek-R1-Distill-Qwen-7B. \textbf{DS-R1-DM} means DeepSeek-R1-Distill-Qwen-7B-Multilingual.}
    \label{tab:trained_flores}
\end{table*}

\begin{table*}[t]

    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c@{\hskip 0.05in}c}
            \toprule
    \multirow{2}{*}{\textbf{Models}} & \multicolumn{5}{c}{\hspace{-1.5em}\textbf{xx2en}}       & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{en2xx}} & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{zh2xx}} & \multicolumn{5}{c}{\hspace{-0.2em}\textbf{xx2zh}} & \textbf{avg}\\
    \cmidrule(l{0.3em}r{1em}){2-6} \cmidrule(l{0.3em}r{1em}){7-11} \cmidrule(l{0.3em}r{1em}){12-16} \cmidrule(l{0.3em}r{1em}){17-21} 
        & \textbf{th} & \textbf{nl} & \textbf{vi} & \textbf{tr} & \textbf{cs} & \textbf{th} & \textbf{nl} & \textbf{vi} & \textbf{tr} & \textbf{cs} & \textbf{th} & \textbf{nl} & \textbf{vi} & \textbf{tr} & \textbf{cs} & \textbf{th} & \textbf{nl} & \textbf{vi} & \textbf{tr} & \textbf{cs} & \textbf{-} \\ \midrule
    \multicolumn{21}{c}{\textbf{General-purpose LLMs}} \\
    \hline

    Q2.5$^{\mathrm{a}}$-7b-base & \textbf{0.668} & 0.482 & \textbf{0.715} & 0.441 & 0.701 & \textbf{0.438} & \textbf{0.672} & \textbf{0.619} & 0.381 & \textbf{0.513} & 0.515 & 0.501 & 0.578 & 0.368 & 0.521 & \textbf{0.355} & \textbf{0.409} & \textbf{0.589} & 0.256 & \textbf{0.410} & 0.507 \\
    Q2.5-7b-Ins$^{\mathrm{b}}$ & 0.566 & 0.580 & 0.536 & 0.391 & 0.601 & 0.148 & 0.026 & 0.226 & 0.185 & 0.333 & 0.434 & 0.418 & 0.456 & 0.301 & 0.470 & 0.012 & -0.046 & -0.065 & -0.237 & 0.075 & 0.270 \\
    DS-R1-D$^{\mathrm{c}}$ & 0.143 & 0.408 & 0.217 & 0.156 & 0.323 & -1.222 & -0.777 & -0.925 & -0.812 & -0.900 & -0.194 & -0.016 & -0.160 & -0.295 & -0.049 & -0.986 & -0.786 & -0.805 & -0.915 & -0.979 & -0.429 \\
    DS-R1-DM$^{\mathrm{d}}$ & 0.161 & 0.415 & 0.181 & 0.023 & 0.331 & -0.930 & -0.593 & -0.538 & -0.715 & -0.767 & -0.478 & -0.072 & -0.467 & -0.445 & 0.063 & -0.868 & -0.739 & -0.691 & -0.901 & -0.840 & -0.394 \\
    \midrule
    \multicolumn{21}{c}{\textbf{SFT LLMs}} \\
    \hline
    Q2.5-7b-SFT (w/o CoT) & 0.660 & \textbf{0.680} & 0.711 & 0.657 & \textbf{0.710} & 0.388 & 0.478 & 0.607 & 0.432 & 0.458 & 0.539 & \textbf{0.529} & 0.558 & 0.494 & 0.553 & 0.282 & 0.367 & 0.549 & 0.171 & 0.337 & 0.508 \\
    Q2.5-7b-SFT (CoT) & 0.625 & 0.642 & 0.664 & 0.610 & 0.668 & 0.178 & 0.271 & 0.327 & 0.043 & 0.146 & 0.475 & 0.434 & 0.516 & 0.404 & 0.462 & 0.104 & 0.155 & 0.244 & -0.093 & 0.037 & 0.345 \\

    \midrule
    \multicolumn{21}{c}{\textbf{RL}} \\
    \hline
    \textbf{R1-T1} & 0.659 & 0.679 & 0.713 & \textbf{0.680} & 0.707 & 0.367 & 0.460 & 0.573 & \textbf{0.513} & 0.461 & \textbf{0.558} & 0.528 & \textbf{0.594} & \textbf{0.503} & \textbf{0.558} & 0.289 & 0.380 & 0.526 & \textbf{0.280} & 0.390 & \textbf{0.521} \\
    % R1-T1 (CS)\\
    \bottomrule 
    \end{tabular}
    }
    \caption{R1-T1's performance on \textbf{Unseen} languages in Flores-101. \textbf{xx2en} (or \textbf{xx2zh}) means the translation direction is from language \textbf{xx} to English (or Chinese). \textbf{Q2.5} means Qwen2.5. \textbf{Ins} means Instruction. \textbf{DS-R1-D} means DeepSeek-R1-Distill-Qwen-7B. \textbf{DS-R1-DM} means DeepSeek-R1-Distill-Qwen-7B-Multilingual.}
    \label{tab:unseen_flores}
\end{table*}




\subsubsection{Setup}

    \paragraph{Metric} we employ COMETScore~\cite{rei-etal-2022-comet}\footnote{\url{https://huggingface.co/Unbabel/wmt20-comet-da}} to measure quality of translation hypotheses given references. COMETScore has been widely adopted due to its strong correlation with human judgment, providing a reliable metric for evaluating machine translation performance. It reflects the translation accuracy of the model, with a higher score indicating better translation quality.

    \paragraph{Baselines} 

    We compare our model against a set of baseline models, including two models from the Qwen series~\cite{Yang2024Qwen25TR}: \textit{Qwen2.5-7b-base}, \textit{Qwen2.5-7B-Instruct}, and two distilled models of DeepSeek-R1~\cite{deepseek2025r1}: \textit{DeepSeek-R1-Distill-Qwen-7B}, \textit{DeepSeek-R1-Distill-Qwen-7B-Multilingual}~\footnote{\url{https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Multilingual}}. These baselines are chosen because they represent a diverse range of approaches to multilingual MT, including general-purpose LLMs, instruction-tuned LLMs, and multilingual distilled models. The inclusion of these baselines allows for a comprehensive comparison across different configurations and architectures, ensuring that our model’s performance is evaluated in the context of current state-of-the-art techniques.

\subsubsection{Result of Trained Languages}

As shown in Table \ref{tab:trained_flores}, the performance of our model on trained languages demonstrates strong results across all evaluation metrics. The model outperforms general-purpose LLMs and multilingual distilled models in terms of translation quality, particularly when compared to the baseline models like DeepSeek-R1-Distill-Qwen-7B. 

\subsubsection{Result of Unseen Languages}

According to the result on Table \ref{tab:unseen_flores}, R1-T1 significantly outperforms baseline models in translation tasks for unseen languages, showcasing its strong generalization capabilities. While general-purpose models like Q2.5-7b-base perform well in some language pairs, they struggle with more challenging ones, especially vi and cs. The incorporation of Chain of Thought (CoT) strategies in fine-tuned models (Q2.5-7b-SFT (CoT)) leads to notable improvements, particularly in xx2en translation tasks. However, the R1-T1 model, which utilizes reinforcement learning, consistently achieves higher scores across all translation directions, including a notable improvement in xx2en (e.g., Thai to English, with a COMET score of 0.659 compared to 0.566 for the baseline). This reinforces the effectiveness of reinforcement learning and CoT integration in enhancing translation performance for unseen languages.


\subsection{Evaluation on Literature Domain MT}


In addition to general translation, we further investigate our model performance in the literary domain. For this experiment, the test set is constructed by DRT~\cite{jiaan2024drt} using Chinese and English texts from the Gutenberg Public Domain Book Repository\footnote{\url{https://www.gutenberg.org/}}. Referred to \citet{jiaan2024drt}, this source obtain advantages of expired copyrights and covers translation units ranging from sentence to paragraph.

We also calculate token-level similarity, which yielded a score of only 0.105 (see Table \ref{tab:different_training_style}). Overall results, as detailed in Table \ref{tab:literature_results}, indicate that our model substantially outperforms both the base model and supervised fine-tuning approach. Although only Chinese <=> English translation is tested, these results still demonstrate strong generalization across translation tasks. 

These findings underscore the effectiveness of our approach in handling complex literary texts and motivate further exploration into multi-lingual literary translation.

\begin{table}[htbp]

% \begin{center}
\centering
\resizebox{0.76\linewidth}{!} {%
\begin{tabular}{lcc}
\toprule
\textbf{models} & \textbf{en2zh} & \textbf{zh2en}\\
\midrule
%Q2.5-7b-base & - & - \\
Q2.5-7b-SFT (w/o CoT) & 0.179 & 0.201 \\
R1-T1 & \textbf{0.322} & \textbf{0.211}  \\
 \bottomrule
\end{tabular}
}
\caption{R1-T1’s performance on Literature test set}
\label{tab:literature_results}
% \end{center}
\end{table}





\subsection{CoT Self-evolution Analysis}

To provide a more intuitive evaluation of the CoT self-evolution mechanism, we conduct a visualization for the translation of a complex sentence from Chinese to English, with and without the integration of self-evolving CoTs in Fig. \ref{fig1}.  In both instances, the translated sentence correctly conveys the meaning, but the version utilizing self-evolution provides more nuanced translations. The description without CoTs includes generic phrasing, such as "the meeting was held," whereas the self-evolved CoT version includes additional context, such as "the critical meeting was held to address urgent concerns."

Through this analysis, it becomes evident that the self-evolving CoT improves the model's ability to adapt over time, refining its translation strategies. The self-evolving CoT allows the model to generate more diverse and accurate translations by incorporating expert-level reasoning strategies and incorporating feedback from prior outputs. This demonstrates that CoT self-evolution is effective in enhancing the model's reliability and its capacity to provide richer, context-aware translations across languages.