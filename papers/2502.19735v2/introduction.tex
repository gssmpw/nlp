Recent advancements in large language models (LLMs) have brought prominent paradigms of O1-like~\cite{jaech2024openai} and R1-like models~\cite{deepseek2025r1}, which leverage inference-time scaling to amplify chain-of-thought (CoT) reasoning~\cite{wei2022chain} for tasks like mathematical proofs and code generation, and employ reinforcement learning (RL) to incentivize autonomous reasoning behaviors without supervised fine-tuning (SFT)~\cite{shao2024deepseekmath}. For instance, DeepSeek-R1-Zero~\cite{deepseek2025r1} demonstrated that pure RL training could unlock self-verification and reflection capabilities, achieving state-of-the-art performance on multiple reasoning benchmarks. These models exemplify how extended CoT reasoning through scaled inference can tackle complex problem-solving. However, the application of extended CoT reasoning to machine translation (MT)—a task demanding both linguistic fidelity and contextual reasoning—remains underexplored.

While LLMs have shown promise in MT, prior work primarily focuses on direct fine-tuning on parallel corpus~\cite{kocmi2024findings} or retrieval-augmented methods~\cite{wang2024retrieval}. Recently, \citet{jiaan2024drt} proposed DRT and introduced long CoT to enhance Chinese-English literary translation, revealing that extended reasoning steps improve handling of culturally nuanced phrases and metaphors. Yet, this approach narrowly targets literature, overlooking the broader spectrum of translation scenarios where professional translators rigorously apply structured reasoning—such as decomposing idioms, resolving contextual conflicts, and adopting techniques (\emph{e.g.}, back translations) to refine outputs~\cite{ordudari2007translation,nida1964toward,brislin2001back}. \citet{nida1964toward} also states that a technical translator should make ``a thorough study of the source language text before making attempts translate it'', suggesting a deep thinking process in human translation. Current MT systems lack mechanisms to reflect these human-like, complex CoT processes, limiting their adaptability to diverse translation challenges.

Moreover, existing approaches for reasoning-based translation utilize purely synthesized CoTs distilled from LLMs (\emph{e.g.}, DRT~\cite{jiaan2024drt}) or design a single fixed procedure (\emph{e.g.}, \textsc{TasTe}~\cite{wang-etal-2024-taste}), which may be suboptimal and unaligned with the hybrid reasoning strategies employed by human experts. Also, their reliance on pre-defined CoTs restricts adaptability to novel translation domains and tasks where high-quality CoT data is scarce. 

To address these, we propose \textbf{R1-Translator (R1-T1)}, a framework that fully incentivizes reasoning-based translation through three innovations:

\textbf{(1) Reasoning for general MT.} Beyond literature translation, we extended reasoning-based MT to general MT and diverse tasks such as document-level translation, idiom translation, and domain translation (\emph{e.g.}, legal, medical), while expanding language support to six languages, including low-resource pairs. This aligns with our core assumption that reasoning is beneficial for general MT.

\textbf{(2) Human-aligned and complex translation CoTs.} Professional translators often interleave complex reasoning procedure such as lexical disambiguation, context-aware paraphrasing, and iterative self-correction. With the help of language experts, we formalized these into six distinct CoT templates mimicking common thinking patterns of human translators and curated a training dataset with hybrid translation CoT trajectories. Unlike single-type CoT in existing approaches, our approach mirrors the multi-layered reasoning observed in human workflows, enhancing translation robustness across domains.

\textbf{(3) Self-evolving translation CoTs.} 

Inspired by R1's RL-driven self-evolution~\cite{deepseek2025r1}, we designed a RL process with specially designed rewards for MT, enabling LLMs to autonomously discover novel CoT trajectories for unseen translation tasks, as shown in Fig.~\ref{fig1}. For example, when translating technical manuals without pre-existing CoT data, the model explores reasoning paths via RL rewards for terminological accuracy and readability, bypassing dependency on curated SFT data. 

To realize this vision, we introduce the concept of \textit{translation reasoning learning}, training a series of R1-T1 models through two complementary strategies: (1) fine-tuning on hybrid CoTs curated with professional translators, and (2) a RL-based exploration to self-discover optimal reasoning paths. Our contributions are threefold:
\begin{itemize}
\item We demonstrate the benefits of reasoning (\emph{i.e.}, long CoTs) for general MT, achieving superior translation performance in six languages and multiple domains.
\item We design a novel methodology for constructing a collection of CoT trajectories combining six distinct CoT templates that reflect human translators’ multi-stage reasoning, validated on challenging tasks like idiom translation and cross-domain adaptation.
\item We pioneer a general-purpose, R1-like MT framework that autonomously evolves translation CoTs via RL.
\end{itemize}
In addition, we open-source R1-T1’s datasets and code, facilitating future research into reasoning-driven MT systems\footnote{Codes and data will be available at \url{https://github.com/superboom/R1-T1}}.