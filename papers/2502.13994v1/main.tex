\documentclass[acmtog,nonacm=true]{acmart}

\input{macros}

\citestyle{acmauthoryear}


\settopmatter{printacmref=false, printccs=false, printfolios=true} %
\renewcommand\footnotetextcopyrightpermission[0]{} %
\pagestyle{plain} %


\acmJournal{TOG}





\begin{document}
\title{Generative Detail Enhancement for Physically Based Materials}

\author{Saeed Hadadan}
\affiliation{%
  \institution{University of Maryland, College Park,}
  \institution{NVIDIA}  
  \state{MD}
  \postcode{20740}
  \country{USA}}
\email{saeedhd@umd.edu}

\author{Benedikt Bitterli}
\affiliation{%
 \institution{NVIDIA}
 \state{WA}
 \country{USA}}
\email{bbitterli@nvidia.com}

\author{Tizian Zeltner}
\affiliation{%
 \institution{NVIDIA}
 \country{Switzerland}}
\email{tzeltner@nvidia.com}

\author{Jan Nov√°k}
\affiliation{%
 \institution{NVIDIA}
 \country{Czech Republic}}
\email{jnovak@nvidia.com}

\author{Fabrice Rousselle}
\affiliation{%
 \institution{NVIDIA}
 \country{Switzerland}}
\email{frousselle@nvidia.com}


\author{Jacob Munkberg}
\affiliation{%
 \institution{NVIDIA}
 \country{Sweden}}
\email{frousselle@nvidia.com}

\author{Jon Hasselgren}
\affiliation{%
 \institution{NVIDIA}
 \country{Sweden}}
\email{frousselle@nvidia.com}

\author{Bartlomiej Wronski}
\affiliation{%
 \institution{NVIDIA}
 \country{USA}}
\email{bwronski@nvidia.com}


\author{Matthias Zwicker}
\affiliation{%
 \institution{University of Maryland, College Park}
 \state{MD}
 \country{USA}}
\email{zwicker@cs.umd.edu}


\begin{abstract}
    We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering.
    Our goal is to increase the visual fidelity of existing materials by adding, for instance, signs of wear, aging, and weathering that are tedious to author.
    To obtain realistic appearance with minimal user effort, we leverage a generative image model trained on a large dataset of natural images.
    Given the geometry, UV mapping, and basic appearance of an object, we proceed as follows:
    We render multiple views of the object and use them, together with an appearance-defining text prompt, to condition a diffusion model. 
    The generated details are then backpropagated from the enhanced images to the material parameters via inverse rendering.
    For inverse rendering to be successful, the generated appearance has to be consistent across all the images. 
    We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space.
    Second, we enforce spatial consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. 
    Our approach does not require any training or finetuning of the diffusion model, is agnostic to the used material model, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. We demonstrate prompt-based material edits exhibiting high levels of realism and detail.
\end{abstract}


\input{figsrc/teaser}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010382.10010384</concept_id>
       <concept_desc>Computing methodologies~Texturing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010372.10010374</concept_id>
       <concept_desc>Computing methodologies~Ray tracing</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Texturing}
\ccsdesc[300]{Computing methodologies~Ray tracing}


\maketitle

\input{introduction}
\input{prior}
\input{background}
\input{method}
\input{implementation}
\input{results}
\input{limitations}
\input{conclusion}


\begin{acks}

This material is based upon work supported by the National Science Foundation under Grant No. IIS2126407. 
We would like to thank Aaron Lefohn for supporting this research, and NVIDIA for funding the work with an NVIDIA academic partnership. 
We would also like to thank Arash Vahdat and Weili Nie for the highly insightful discussions on multi-view generation using diffusion models. 


\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\appendix
\input{suppl_sections.tex}

\end{document}
