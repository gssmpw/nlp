\section{Prior work}
\label{sec:prior}

\input{figsrc/overview}

For brevity, we only cover methods directly related to enforcing multi-view consistency in diffusion denoisers and to editing of physically based materials using generative models.







\paragraph{Multi-view consistency via latents sharing}
TexPainter ____ denoises multi-view latents, and correlates the views in a shared texture space.
Tex4D____ extends the idea to the temporal domain using a video diffusion model.
These methods however do not easily generalize to view dependent PBR materials.
____ and ____ instead operate on intermediate features of the network to enforce 3D consistent transformations in the output images.

Another approach is to reuse the diffusion model's input noise accross views as proposed by____.
Our work extends this idea by anchoring the noise field in UV space for a more robust handling of disocclusions.


\paragraph{Multi-view consistency via view correspondences}
Cross-frame attention modules have been devised for known depth maps____, poses____, or epipolar constraints____.
These methods however require large-scale training.
Our method exploits known geometry and is training-free.

\paragraph{Text-guided 3D generation}
____ pioneered generation of 3D models 
using text-to-image diffusion and score distillation sampling (SDS).
The method has been extended to various representations____, 
with improved objective functions____, 
sampling____, and material decomposition____.

Early methods____ suffer from over-blurring due to the lack of view consistency. 
Follow-up work improved this using spatial attention____, video-models____, and tiled inputs____.
We use the latter idea in our work.

FlashTex____, DreamMat____, and MaPa____ specialize in material reconstruction for a 
known scene. They leverage known priors by training a controlnet____ from geometry buffers 
(e.g. depth and normal), and lighting rendered with known, constant, materials (e.g. fully diffuse and specular). 
This helps greatly with view dependent shading effects, and separating shadows from material albedo. 
____ train a generative model to directly synthesize material maps.
All these methods are costly as they require training with specialized object/material datasets, which we avoid.






\paragraph{Image and appearance editing}
Text-guided diffusion models are often applied to image editing while preserving the semantics of the source image.
This is achieved by manipulating the self-attention layers 
____, often combined with DDIM inversion____.
$\text{RGB}{\leftrightarrow}\text{X}$____ allows editing the content of images by first extracting irradiance and material maps, manually editing them, and then generating the corresponding realistic image.
We consider a dual problem, where the inputs and outputs are PBR maps, and the intermediate step that enhances details involves generation of an image.




\paragraph{Material upscaling}
____ consider a subset of detail enhancement, focusing on increasing the resolution of PBR material textures by inverse rendering upscaled images.
However, they operate on flat-geometry renderings and are therefore unable to synthesize detail in the context of the object geometry and the surrounding scene; this is one of our goals.


\paragraph{Video diffusion models}
Video diffusion models____ generate (view-consistent) video from text and image conditioning. Notably, SV3D____ adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation. However, these models come at significant computational cost, and the generated frames are typically not as detailed as text-to-image models.