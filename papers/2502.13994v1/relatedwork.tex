\section{Prior work}
\label{sec:prior}

\input{figsrc/overview}

For brevity, we only cover methods directly related to enforcing multi-view consistency in diffusion denoisers and to editing of physically based materials using generative models.







\paragraph{Multi-view consistency via latents sharing}
TexPainter \cite{TexPainter} denoises multi-view latents, and correlates the views in a shared texture space.
Tex4D~\cite{bao2024tex4d} extends the idea to the temporal domain using a video diffusion model.
These methods however do not easily generalize to view dependent PBR materials.
\citet{spiderman} and \citet{diffusion_handles} instead operate on intermediate features of the network to enforce 3D consistent transformations in the output images.

Another approach is to reuse the diffusion model's input noise accross views as proposed by~\cite{chang2024how,daras2024warped}.
Our work extends this idea by anchoring the noise field in UV space for a more robust handling of disocclusions.


\paragraph{Multi-view consistency via view correspondences}
Cross-frame attention modules have been devised for known depth maps~\cite{mvdiffusion}, poses~\cite{cerkezi2023multiview}, or epipolar constraints~\cite{spad}.
These methods however require large-scale training.
Our method exploits known geometry and is training-free.

\paragraph{Text-guided 3D generation}
\citet{poole2023dreamfusion} pioneered generation of 3D models 
using text-to-image diffusion and score distillation sampling (SDS).
The method has been extended to various representations~\cite{lin2023magic3d, taoran2023gaussiandreamer}, 
with improved objective functions~\cite{wang2023prolificdreamer}, 
sampling~\cite{zhu2023hifa}, and material decomposition~\cite{chen2023fantasia3d,youwang2024paintit}.

Early methods~\cite{richardson2023texture,chen2023text2tex,cao2023textfusion} suffer from over-blurring due to the lack of view consistency. 
Follow-up work improved this using spatial attention~\cite{shi2023MVDream}, video-models~\cite{voleti2024sv3d, wu2024cat4d}, and tiled inputs~\cite{flashtex}.
We use the latter idea in our work.

FlashTex~\cite{flashtex}, DreamMat~\cite{dreammat}, and MaPa~\cite{mapa} specialize in material reconstruction for a 
known scene. They leverage known priors by training a controlnet~\cite{controlnet} from geometry buffers 
(e.g. depth and normal), and lighting rendered with known, constant, materials (e.g. fully diffuse and specular). 
This helps greatly with view dependent shading effects, and separating shadows from material albedo. 
\citet{Vecchio2024controlmat} train a generative model to directly synthesize material maps.
All these methods are costly as they require training with specialized object/material datasets, which we avoid.






\paragraph{Image and appearance editing}
Text-guided diffusion models are often applied to image editing while preserving the semantics of the source image.
This is achieved by manipulating the self-attention layers 
\cite{tumanyan2023pnp}, often combined with DDIM inversion~\cite{mokady2023nulltext,parmar2023pix2pixzero}.
$\text{RGB}{\leftrightarrow}\text{X}$~\cite{RGBX} allows editing the content of images by first extracting irradiance and material maps, manually editing them, and then generating the corresponding realistic image.
We consider a dual problem, where the inputs and outputs are PBR maps, and the intermediate step that enhances details involves generation of an image.




\paragraph{Material upscaling}
\citet{gauthier2024matup} consider a subset of detail enhancement, focusing on increasing the resolution of PBR material textures by inverse rendering upscaled images.
However, they operate on flat-geometry renderings and are therefore unable to synthesize detail in the context of the object geometry and the surrounding scene; this is one of our goals.


\paragraph{Video diffusion models}
Video diffusion models~\cite{blattmann2023svd,hong2023cogvideo,yang2024cogvideox} generate (view-consistent) video from text and image conditioning. Notably, SV3D~\cite{voleti2024sv3d} adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation. However, these models come at significant computational cost, and the generated frames are typically not as detailed as text-to-image models.