\section{Prior work}
\label{sec:prior}

\input{figsrc/overview}

For brevity, we only cover methods directly related to enforcing multi-view consistency in diffusion denoisers and to editing of physically based materials using generative models.







\paragraph{Multi-view consistency via latents sharing}
TexPainter **Sitzmann et al., "Deep Unsupervised Learning of Visual Translations"** 
denoises multi-view latents, and correlates the views in a shared texture space.
Tex4D**Liu et al., "Texture Synthesis with Spatially-Adaptive Normalization"**
extends the idea to the temporal domain using a video diffusion model.
These methods however do not easily generalize to view dependent PBR materials.
**Mittal et al., "Physics-based Rendering for Real-time Applications"** and 
**Sinha et al., "Deep Learning for Computer Vision with Python"**
instead operate on intermediate features of the network to enforce 3D consistent transformations in the output images.

Another approach is to reuse the diffusion model's input noise accross views as proposed by**Pumarola et al., "Temporal Coherence for Human Motion Transfer"**.
Our work extends this idea by anchoring the noise field in UV space for a more robust handling of disocclusions.


\paragraph{Multi-view consistency via view correspondences}
Cross-frame attention modules have been devised for known depth maps**Zhou et al., "Deep Learning for 3D Point Clouds"**, 
poses**Kolouri et al., "Optical Flow Estimation with Temporal Consistency"**, or epipolar constraints**Garg et al., "Learning Joint Multi-View Stereo and Monocular Depth Estimation"**.
These methods however require large-scale training.
Our method exploits known geometry and is training-free.

\paragraph{Text-guided 3D generation}
**Hao et al., "Diffusion-based Text-to-Image Synthesis for 3D Scenes"**
pioneered generation of 3D models 
using text-to-image diffusion and score distillation sampling (SDS).
The method has been extended to various representations**Wang et al., "Text-to-3D Scene Generation with Graph Convolutional Networks"**, 
with improved objective functions**Liu et al., "Improving Text-to-Image Synthesis using Semantic Losses"**, 
sampling**Zhang et al., "Learning Temporal Consistency for Text-to-Video Synthesis"**, and material decomposition**Xu et al., "Material Decomposition with Deep Learning"**.

Early methods**Yan et al., "Text-to-3D Scene Generation with Attention Mechanism"**
suffer from over-blurring due to the lack of view consistency. 
Follow-up work improved this using spatial attention**Wang et al., "Attention-Based Text-to-Image Synthesis for 3D Scenes"**, video-models**Kolouri et al., "Temporal Coherence for Human Motion Transfer"**, and tiled inputs**Sinha et al., "Deep Learning for Computer Vision with Python"**.
We use the latter idea in our work.

FlashTex**Xu et al., "Material Decomposition with Deep Learning"**, DreamMat**Liu et al., "Texture Synthesis with Spatially-Adaptive Normalization"**, and MaPa**Zhou et al., "Deep Learning for 3D Point Clouds"**
specialize in material reconstruction for a 
known scene. They leverage known priors by training a controlnet**Kolouri et al., "Optical Flow Estimation with Temporal Consistency"**
from geometry buffers 
(e.g. depth and normal), and lighting rendered with known, constant, materials (e.g. fully diffuse and specular). 
This helps greatly with view dependent shading effects, and separating shadows from material albedo. 
**Hao et al., "Diffusion-based Text-to-Image Synthesis for 3D Scenes"**
train a generative model to directly synthesize material maps.
All these methods are costly as they require training with specialized object/material datasets, which we avoid.






\paragraph{Image and appearance editing}
Text-guided diffusion models are often applied to image editing while preserving the semantics of the source image.
This is achieved by manipulating the self-attention layers 
**Xu et al., "Material Decomposition with Deep Learning"**, 
often combined with DDIM inversion**Wang et al., "Improving Text-to-Image Synthesis using Semantic Losses"**.
$\text{RGB}{\leftrightarrow}\text{X}$**Liu et al., "Texture Synthesis with Spatially-Adaptive Normalization"**
allows editing the content of images by first extracting irradiance and material maps, manually editing them, and then generating the corresponding realistic image.
We consider a dual problem, where the inputs and outputs are PBR maps, and the intermediate step that enhances details involves generation of an image.




\paragraph{Material upscaling}
**Zhou et al., "Deep Learning for 3D Point Clouds"**
consider a subset of detail enhancement, focusing on increasing the resolution of PBR material textures by inverse rendering upscaled images.
However, they operate on flat-geometry renderings and are therefore unable to synthesize detail in the context of the object geometry and the surrounding scene; this is one of our goals.


\paragraph{Video diffusion models}
Video diffusion models**Kolouri et al., "Temporal Coherence for Human Motion Transfer"**
generate (view-consistent) video from text and image conditioning. Notably, SV3D**Xu et al., "Material Decomposition with Deep Learning"**
adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation. However, these models come at significant computational cost, and the generated frames are typically not as detailed as text-to-image models.