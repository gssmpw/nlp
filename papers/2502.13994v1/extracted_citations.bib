@inproceedings{RGBX,
   title={RGB-X: Image decomposition and synthesis using material- and lighting-aware diffusion models},
   url={http://dx.doi.org/10.1145/3641519.3657445},
   DOI={10.1145/3641519.3657445},
   booktitle={Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers ’24},
   publisher={ACM},
   author={Zeng, Zheng and Deschaintre, Valentin and Georgiev, Iliyan and Hold-Geoffroy, Yannick and Hu, Yiwei and Luan, Fujun and Yan, Ling-Qi and Hašan, Miloš},
   year={2024},
   month=jul, pages={1–11},
   collection={SIGGRAPH ’24}
}

@article{Vecchio2024controlmat,
      author = {Vecchio, Giuseppe and Martin, Rosalie and Roullier, Arthur and Kaiser, Adrien and Rouffet, Romain and Deschaintre, Valentin and Boubekeur, Tamy},
      title = {ControlMat: A Controlled Generative Approach to Material Capture},
      year = {2024},
      issue_date = {October 2024},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      volume = {43},
      number = {5},
      issn = {0730-0301},
      url = {https://doi.org/10.1145/3688830},
      doi = {10.1145/3688830},
      journal = {ACM Trans. Graph.},
      month = sep,
      articleno = {164},
      numpages = {17},
}

@article{bao2024tex4d,
      title={Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models}, 
      author={Jingzhi Bao and Xueting Li and Ming-Hsuan Yang},
      journal={arXiv preprint arxiv:2410.10821},
      year={2024}
}

@article{blattmann2023svd,
	title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
	author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
	year={2023},
	journal={arXiv:2311.15127}
}

@inproceedings{cao2023textfusion,
      author = { Cao, Tianshi and Kreis, Karsten and Fidler, Sanja and Sharp, Nicholas and Yin, Kangxue },
      booktitle = { 2023 IEEE/CVF International Conference on Computer Vision (ICCV) },
      title = {{ TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models }},
      year = {2023},
      pages = {4146-4158},
}

@misc{cerkezi2023multiview,
      title={Multi-View Unsupervised Image Generation with Cross Attention Guidance}, 
      author={Llukman Cerkezi and Aram Davtyan and Sepehr Sameni and Paolo Favaro},
      year={2023},
      eprint={2312.04337},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2312.04337}, 
}

@inproceedings{chen2023fantasia3d,
  author={Chen, Rui and Chen, Yongwei and Jiao, Ningxin and Jia, Kui},
  title={Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month={October},
  year={2023},
  pages={22246--22256}
}

@inproceedings{chen2023text2tex,
      author = { Chen, Dave Zhenyu and Siddiqui, Yawar and Lee, Hsin-Ying and Tulyakov, Sergey and Niesner, Matthias },
      booktitle = { 2023 IEEE/CVF International Conference on Computer Vision (ICCV) },
      title = {{ Text2Tex: Text-driven Texture Synthesis via Diffusion Models }},
      year = {2023},
      pages = {18512-18522},
}

@misc{controlnet,
      title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
      author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
      year={2023},
      eprint={2302.05543},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.05543}, 
}

@article{daras2024warped,
  title={Warped diffusion: Solving video inverse problems with image diffusion models},
  author={Daras, Giannis and Nie, Weili and Kreis, Karsten and Dimakis, Alex and Mardani, Morteza and Kovachki, Nikola Borislavov and Vahdat, Arash},
  journal={arXiv preprint arXiv:2410.16152},
  year={2024}
}

@misc{diffusion_handles,
      title={Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D}, 
      author={Karran Pandey and Paul Guerrero and Matheus Gadelha and Yannick Hold-Geoffroy and Karan Singh and Niloy Mitra},
      year={2023},
      eprint={2312.02190},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.02190}, 
}

@misc{dreammat,
      title={DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models}, 
      author={Yuqing Zhang and Yuan Liu and Zhiyu Xie and Lei Yang and Zhongyuan Liu and Mengzhou Yang and Runze Zhang and Qilong Kou and Cheng Lin and Wenping Wang and Xiaogang Jin},
      year={2024},
      eprint={2405.17176},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2405.17176}, 
}

@misc{flashtex,
      title={FlashTex: Fast Relightable Mesh Texturing with LightControlNet}, 
      author={Kangle Deng and Timothy Omernick and Alexander Weiss and Deva Ramanan and Jun-Yan Zhu and Tinghui Zhou and Maneesh Agrawala},
      year={2024},
      eprint={2402.13251},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2402.13251}, 
}

@article{gauthier2024matup,
      journal   = {Computer Graphics Forum},
      title     = {MatUp: Repurposing Image Upsamplers for SVBRDFs},
      author    = {Gauthier, Alban and Kerbl, Bernhard and Levallois, Jérémy and Faury, Robin and Thiery, Jean-Marc and Boubekeur, Tamy},
      year      = {2024},
      volume    = {43},
      number    = {4},
}

@inproceedings{hong2023cogvideo,
	title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers},
	author={Wenyi Hong and Ming Ding and Wendi Zheng and Xinghan Liu and Jie Tang},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023}
}

@inproceedings{lin2023magic3d,
  title={Magic3D: High-Resolution Text-to-3D Content Creation},
  author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year={2023}
}

@inproceedings{mapa,
author = {Zhang, Shangzhan and Peng, Sida and Xu, Tao and Yang, Yuanbo and Chen, Tianrun and Xue, Nan and Shen, Yujun and Bao, Hujun and Hu, Ruizhen and Zhou, Xiaowei},
title = {MaPa: Text-driven Photorealistic Material Painting for 3D Shapes},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657504},
doi = {10.1145/3641519.3657504},
abstract = {This paper aims to generate materials for 3D meshes from text descriptions. Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing. Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs. Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts. Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description. Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {4},
numpages = {12},
keywords = {3D asset creation, generative modeling, material painting},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{mokady2023nulltext,
  author={Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Null-text Inversion for Editing Real Images using Guided Diffusion Models}, 
  year={2023},
  volume={},
  number={},
  pages={6038-6047},
}

@misc{mvdiffusion,
      title={MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion}, 
      author={Shitao Tang and Fuyang Zhang and Jiacheng Chen and Peng Wang and Yasutaka Furukawa},
      year={2023},
      eprint={2307.01097},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01097}, 
}

@inproceedings{parmar2023pix2pixzero,
      author = {Parmar, Gaurav and Kumar Singh, Krishna and Zhang, Richard and Li, Yijun and Lu, Jingwan and Zhu, Jun-Yan},
      title = {Zero-shot Image-to-Image Translation},
      year = {2023},
      booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
      articleno = {11},
      numpages = {11},
      series = {SIGGRAPH '23}
}

@inproceedings{poole2023dreamfusion,
	title={DreamFusion: Text-to-3D using 2D Diffusion},
	author={Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023}
}

@inproceedings{richardson2023texture,
      author = {Richardson, Elad and Metzer, Gal and Alaluf, Yuval and Giryes, Raja and Cohen-Or, Daniel},
      title = {TEXTure: Text-Guided Texturing of 3D Shapes},
      year = {2023},
      booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
      articleno = {54},
      numpages = {11},
      series = {SIGGRAPH '23}
}

@article{shi2023MVDream,
  author = {Shi, Yichun and Wang, Peng and Ye, Jianglong and Mai, Long and Li, Kejie and Yang, Xiao},
  title = {MVDream: Multi-view Diffusion for 3D Generation},
  journal = {arXiv:2308.16512},
  year = {2023},
}

@misc{spad,
      title={SPAD : Spatially Aware Multiview Diffusers}, 
      author={Yash Kant and Ziyi Wu and Michael Vasilkovsky and Guocheng Qian and Jian Ren and Riza Alp Guler and Bernard Ghanem and Sergey Tulyakov and Igor Gilitschenski and Aliaksandr Siarohin},
      year={2024},
      eprint={2402.05235},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.05235}, 
}

@inproceedings{spiderman,
author = {Patashnik, Or and Gal, Rinon and Cohen-Or, Daniel and Zhu, Jun-Yan and De La Torre, Fernando},
title = {Consolidating Attention Features for Multi-view Image Editing},
year = {2024},
isbn = {9798400711312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680528.3687611},
doi = {10.1145/3680528.3687611},
abstract = {Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.},
booktitle = {SIGGRAPH Asia 2024 Conference Papers},
articleno = {40},
numpages = {12},
keywords = {Multi-view, Diffusion Models, Image Editing},
location = {
},
series = {SA '24}
}

@article{taoran2023gaussiandreamer,
    title={GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors},
    author={Taoran Yi and Jiemin Fang and Guanjun Wu and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Qi Tian and Xinggang Wang},
    journal={arxiv:2310.08529},
    year={2023}
}

@inproceedings{tumanyan2023pnp,
      author    = {Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
      title     = {Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2023},
      pages     = {1921-1930}
}

@article{voleti2024sv3d,
      title={SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion}, 
      author={Vikram Voleti and Chun-Han Yao and Mark Boss and Adam Letts and David Pankratz and Dmitry Tochilkin and Christian Laforte and Robin Rombach and Varun Jampani},
      year={2024},
      journal={arXiv:2403.12008},
}

@article{wang2023prolificdreamer,
      title={ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation},
      author={Zhengyi Wang and Cheng Lu and Yikai Wang and Fan Bao and Chongxuan Li and Hang Su and Jun Zhu},
      journal={arXiv:2305.16213},
      year={2023}
}

@article{wu2024cat4d,
    title={{CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models}},
    author={Wu, Rundi and Gao, Ruiqi and Poole, Ben and Trevithick, Alex and Zheng, Changxi and Barron, Jonathan T. and Holynski, Aleksander},
    journal={arXiv:2411.18613},
    year={2024}
}

@misc{yang2024cogvideox,
      title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}, 
      author={Zhuoyi Yang and Jiayan Teng and Wendi Zheng and Ming Ding and Shiyu Huang and Jiazheng Xu and Yuanming Yang and Wenyi Hong and Xiaohan Zhang and Guanyu Feng and Da Yin and Xiaotao Gu and Yuxuan Zhang and Weihan Wang and Yean Cheng and Ting Liu and Bin Xu and Yuxiao Dong and Jie Tang},
      year={2024},
      journal={arXiv:2408.06072}
}

@inproceedings{youwang2024paintit,
    title = {Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering},
    author = {Youwang, Kim and Oh, Tae-Hyun and Pons-Moll, Gerard},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2024}
}

@article{zhu2023hifa,
      title={HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance}, 
      author={Junzhe Zhu and Peiye Zhuang},
      journal={arXiv:2305.18766},
      year={2023}
}

