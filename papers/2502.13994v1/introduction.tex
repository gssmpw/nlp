\section{Introduction}
\label{sec:intro}


Depicting rich 3D worlds is a driving goal of computer graphics.
While achieving this goal is possible today for experienced artists with expert tools, the pareto principle applies:
The creative step of authoring the overall look of an asset takes little time in comparison to the disproportionate effort of infusing details and imperfections of the real world.
Our goal is therefore to create a tool that enhances 3D objects with appearance details requiring comparatively minimal effort from the artist.


For this, we turn to diffusion models~\cite{ho2020diffusion} that are capable of producing realistic visuals and can be conditioned using text prompts and guiding images.
A key consideration, however, is the amount of training data available.
While datasets containing 3D objects and materials exist~\cite{objaverse,vecchio2024matsynth}, they cannot compete with natural image datasets in size and diversity, which directly impacts the model capabilities.
We therefore build our tool using an off-the-shelf diffusion model that was trained on an internet-scale image set.



We combine the diffusion model with a physically based renderer to enable two key editing features: 1) specifying the initial look of the object, and 2) outputing a material representation that is complient with traditional authoring workflows.
Our algorithm works as follows.
We start by rendering the original 3D asset from multiple views.
Then we condition the diffusion model on a concatenation of these views, and a text prompt describing the desired detail enhancements.
Since our goal is to merely enhance the appearance, we propose a specific way of using two 
publicly available ControlNets~\cite{controlnet} to condition the model on the asset geometry and initial appearance.
Finally, the differences between the original renderings and the diffusion-generated views are back-propagated to the material parameters via inverse rendering.


The main challenge of multi-view generation with diffusion models is the consistency of individual details in all relevant views.
We address this challenge with two contributions. First, we seed the diffusion model with noise that is itself consistent across the views. 
We adopt the idea of integral noise~\cite{chang2024how} and project a common UV-space noise pattern into each view in a variance-preserving manner.
Second, we bias the attention maps in the diffusion model to encourage pixels to attend to their corresponding locations in different views.
We compute the correspondences by reprojecting points between the views using ray tracing operations.

Our approach has several benefits.
It avoids the impractical creation of a new dataset and/or retraining a large diffusion model; we rely on an off-the-shelf model that can be easily replaced by another one.
We preserve the original geometry and artistic intent, while modifying the material texture maps according to the user's target prompts. 
Because we only enhance the input material, the inverse rendering is more likely to succeed than if starting the optimization from scratch.
Lastly, the input and output of our model are in the form of a classical 3D representation (e.g, triangles, textures) and thus perfectly multi-view consistent. This also allows users to further edit the appearance, integrate it into larger scenes, and render with common renderers.







 







