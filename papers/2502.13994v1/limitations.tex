\section{Limitations and Future work}
\label{sec:limitations}

\begin{figure}[t]
    \centering%
    \setlength{\tabcolsep}{0.002\textwidth}%
    \renewcommand{\arraystretch}{1}%
    \footnotesize%
    \begin{tabular}{cccc}
        \rotatebox{90}{\hspace{1em}Input views}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/initial/4.jpg}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/initial/5.jpg}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/initial/6.jpg}\\
        \rotatebox{90}{\hspace{0.5em}Diffusion result}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/diffused/4.jpg}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/diffused/5.jpg}&
        \includegraphics[angle=90,height=1.82cm,trim=100 0 100 50,clip]{figures/limitation_spoon/diffused/6.jpg}\\
    \end{tabular}
    \vspace{-2mm}
    \caption{
        Environment reflections on the spoon (top) are enhanced with diffuse details that follow the reflections across views (bottom) in a manner that may be inconsistent with user intentions.
    }
    \label{fig:limit_spoon}
\end{figure}



\paragraph*{Additional consistency improvements}
Although our contributions improve multi-view consistency, occasional deviations between views still occur at small scale. 
The inverse rendering typically resolves them by superimposing the conflicting visuals in the material maps.
The final representation is view consistent, by design, but high-frequency view-dependent effects (e.g., mirror reflections) may end up baked in the albedo texture; see \autoref{fig:limit_spoon}.
Since multi-view consistency of diffusion generators is an actively sought property, we believe future work will alleviate this
either in a data-driven fashion, e.g., by employing multi-step optimization and video models, or by imposing additional geometric priors, such as identifying pixel correspondences using manifold walks on specular surfaces~\cite{Jakob2012Manifold}.



\paragraph*{Controlling the diffusion model}
We currently expose only a text prompt as the control over the generated detail. 
Future work could explore more fine-grained user control, such as CLIP priors~\cite{stable_unclip_pipeline, dall_e2} or manipulations using texture exemplars~\cite{texsliders}.
The recently published Stable Diffusion 3.x~\cite{SD3,SD3_5_large} supports more sophisticated text encoders and ControlNets, and swapping them in place of our current diffusion model provides a near-term avenue for better control.
Importantly, artists still retain full editability of the material produced by our tool using traditional workflows.

\paragraph*{Enhancing macro geometry} 
We focused on visual enhancements that can be captured using texture maps
without attempting to refine the input macro geometry. 
While inverse rendering is in principle capable of updating the mesh, 
we leave this for future work.


\paragraph*{Manual hyperparameter tuning}
The attention bias $\bias$ is currently a manually tuned hyperparameter. Although the range of reasonable values is limited ($[0-3.5]$ in our experiments), and $\bias$ can be tuned quickly on low-resolution images, a parameter-free biasing would make the technique more practical.





