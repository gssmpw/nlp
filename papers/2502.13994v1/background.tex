\section{Background}
\label{sec:background}

\paragraph{Diffusion models.} Denoising diffusion leverages a bidirectional process
where the forward pass gradually corrupts training data by iteratively adding Gaussian noise until the data becomes pure noise.
The reverse process then learns to denoise the corrupted data through a neural network, which predicts and removes noise step-by-step.
We use a latent space diffusion model~\cite{rombach2022stablediffusion}, which extracts the latent space using a variational autoencoder and performs the denoising steps with a U-Net architecture operating at different scales (see Figure 3 in~\citet{rombach2022stablediffusion}).

\paragraph{ControlNet.} In order to guide the denoising process, the ControlNet model~\cite{controlnet} implements a dual-network architecture.
The first network---a pretrained diffusion model---is locked down to perform the usual denoising task and cloned.
The clone network is connected to the locked network using zero-initialized convolution layers,
and enables precise spatial conditioning of the pretrained denoiser using images.

\paragraph{Attention.}
The attention operation~\cite{Vaswani2017attention} has been incorporated to diffusion models to capture relationships between different activations in the denoising process.
The operation takes its inputs and embeds them in three learned linear spaces, referring to them as key, query, and value.
Denoting the matrices of these embeddings $\matQ$, $\matK$, and $\matV$, respectively
the attention formula
\[
\text{Attention}(\matQ, \matK, \matV) = \text{softmax}\left(\frac{\matQ \matK^\top}{\sqrt{d_k}}\right)\matV,
\]
provides a mechanism for capturing the similarity between queries and keys (where $d_k$ is the dimensionality of the key embedding).

A typical U-Net diffusion model features two types of attention:
\emph{cross-attention} layers that guide the denoising of image regions using a given text-prompt,
and \emph{self-attention} layers that allow regions within the image to influence each other.
In the self-attention module, the $\matQ \matK^\top$ product forms a large attention score matrix,
where entry $[i,j]$ describes how strongly region $i$ attends to region $j$.




