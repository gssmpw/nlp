\begin{figure*}[t]
    \begin{overpic}[width=0.98\textwidth]{figures/overview/figure.pdf}
        \footnotesize
        \put(11.0,27){   \begin{minipage}{2cm}\centering Fixed\\geometry             \end{minipage}}
        \put(1.1,29.5){  \begin{minipage}{2cm}\centering Initial material\\textures  \end{minipage}}
        \put(86.8,29.5){ \begin{minipage}{2cm}\centering Edited material\\textures   \end{minipage}}
        \put(20.0,25.5){ \begin{minipage}{3cm}\centering Rendered color \&\\normal grids\end{minipage}}
        \put(62.7,18){ \begin{minipage}{3cm}\centering Generated images              \end{minipage}}
        \put(43.7,0.4){  \begin{minipage}{3cm}\centering Text prompt                 \end{minipage}}
        \put(43.7,2.4){  \begin{minipage}{3cm}\centering \prompt{a rusty kettle}     \end{minipage}}
        \put(11.3,10.8){   \begin{minipage}{2cm}\centering \textcolor{white}{\textbf{Render}}          \end{minipage}}
        \put(76.7,10.8){  \begin{minipage}{2cm}\centering \textcolor{white}{\textbf{Inverse\\render}} \end{minipage}}
        \put(36.4,33.8){ \begin{minipage}{3cm}\centering View-correlated noise\\(Section \ref{sec:noise_warping}) \end{minipage}}
        \put(50.7,33.8){ \begin{minipage}{3cm}\centering Attention bias\\(Section \ref{sec:attention_bias})  \end{minipage}}
        \put(43.7,13.7){  \begin{minipage}{3cm}\centering \textcolor{white}{\textbf{Stable Diffusion}}\end{minipage}}
        \put(43.7,11.4){  \begin{minipage}{3cm}\centering \textcolor{white}{\textbf{ControlNet tile}}\end{minipage}}
        \put(43.7,9.1){  \begin{minipage}{3cm}\centering \textcolor{white}{\textbf{ControlNet normal}}\end{minipage}}
        \put(43.7,6.8){ \begin{minipage}{3cm}\centering Diffusion model (Section \ref{sec:second-controlnet}) \end{minipage}}
    \end{overpic}
    \vspace{-2mm}
    \caption{Pipeline overview: Given a 3D asset including fixed geometry and initial material textures, we render color and normal images from multiple viewpoints (4 out of 16 views shown above). We then apply enhancements based on text prompts using a multi-view diffusion model designed to produce view-consistent outputs that edit the input images in a controllable manner. We achieve this by leveraging three distinct techniques, including suitable publicly available ControlNets, view-correlated noise, and cross-view attention bias. We finally obtain the edited material textures using inverse rendering. }
    \label{fig:overview}
\end{figure*}
