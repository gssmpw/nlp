
\section{Implementation}
\label{sec:implementation}



We implemented our system in PyTorch~\cite{pytorch} using publicly available models: the \textit{tile}~\cite{controlnet_tile} and \textit{normal}~\cite{controlnet_normal} variants of ControlNet (for color and normal inputs respectively) and the corresponding Hugging Face implementation~\cite{huggingface} of Stable Diffusion 1.5.%


We use the Mitsuba 3 system~\cite{mitsuba} for GPU accelerated (differentiable) rendering.
During inverse rendering, we apply a tonemapping operator~\cite{Reinhard2002} to the high-dynamic range renderings before comparing them with the (low-dynamic range) diffusion-generated target images using a relative L2 loss.
It is important not to clip high values in order to preserve smooth specular highlights and avoid zero-valued gradients during backpropagation.

The ControlNet occasionally fails to preserve the exact silhouette of the rendered 3D geometry causing some background pixels to ``bleed into the object'' during the diffusion. We therefore stop gradient propagation for pixels that are within a fixed distance of the (precomputed) object boundary and downscale the loss at grazing incident angles based on a cosine-factor. Masked points still receive coverage from other views and are not removed from optimization.

\paragraph{Memory considerations and scalability}

Both the main diffusion process and our attention biasing can easily exhaust available GPU memory when operating on large images. 
One memory bottleneck is the variational autoencoder used to convert between images and the spatially-downsampled latent representation. For this conversion we found it necessary to split images back into tiles consisting of the individual views. The latent sampling inside the autoencoder and the main diffusion \& denoising processes can then internally operate on the (re)concatenated grids.

While the pixel-to-pixel correspondences
can easily be precomputed for a given asset (e.g. by storing matching pixel coordinates between each pair of views) we cannot afford to explicitly store the resulting $N\times N$ bias matrix in memory.
This limits our choice of the attention framework. From the ones we tested~\cite{xFormers,flashattention,flashattention2,memeffattention,memeffattention2} only the recent FlexAttention~\cite{flexattention} allowed us to scale to 16 views at resolution $1024^2$ as it can apply attention biases on the fly.
We illustrate how memory consumption and runtime performance scale with varying number of views and resolutions in Table~\ref{tab:runtime}.










\begin{table}[t]
    \centering
    \small
    \caption{Runtime on NVIDIA RTX 5880 and memory usage for different numbers of conditioning views and resolutions.
    Implementing our attention biasing with xFormers (xF) instead of FlexAttention (FA) runs roughly $2\times$ faster, but exceeds available memory with 9 and more views at $1024 \times 1024$.
    }
    \label{tab:runtime}
    \setlength{\tabcolsep}{0.009\textwidth}%
    \begin{tabular}{llrrrrrr}
        \toprule
        && \multicolumn{2}{c}{4 views} & \multicolumn{2}{c}{9 views} & \multicolumn{2}{c}{16 views} \\
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \cmidrule(lr){7-8}
        && $512^2$ & $1024^2$ & $512^2$ & $1024^2$ & $512^2$ & $1024^2$ \\
        \midrule
        \multirow{2}{*}{FA} & Runtime (s) & 17 & 187 & 67 & 923 & 187 & 2816 \\
        & Peak memory (GB) & 6.2 & 9.2 & 7.6 & 15.2 & 9.7 & 20.4 \\
        \midrule
        \multirow{2}{*}{xF} & Runtime (s) & 8 & 102 & 34 & - & 100 & - \\
        & Peak memory (GB) & 6.9 & 33.2 & 13.8 & >45 & 33.2 & >45 \\
        \bottomrule
    \end{tabular}
\end{table}




