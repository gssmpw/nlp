\section{Conclusion}
\label{sec:conclusion}

We presented a method for enhancing the detail of a classically authored material using a diffusion model.
Our method renders the provided material from multiple views, adds details to the renderings using a diffusion model, and then backpropagates the changes to the material using inverse rendering.
Inverse rendering requires detail to be consistent across views, and we achieve this with two technical contributions: noise correlation by projecting from a reference noise anchored in the UV space, and attention biasing using the known geometry of the object.
This requires no new datasets or expensive retraining and is largely built from off-the-shelf, pre-trained components.

The resulting method serves the important use case of \emph{human-in-the-loop} authoring: Rather than entirely replacing the artist and generating materials from scratch, we allow the artist to maintain creative control using traditional workflows, while reducing the time spent on tedious detailing of assets---analogous to ``auto-complete'' for material detail. Because the input and output of our method are traditional materials, our method can be used at any stage in the authoring process, and the produced enhacements arbitrarily post-processed, blended, and combined. We believe our work builds a solid foundation for future practical tools that will further improve the robustness and controllability of the generative process.

\input{figsrc/ablation}

\input{figsrc/inv_rend}


\input{figsrc/results}