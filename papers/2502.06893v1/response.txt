\section{Related Work}
\label{sec:related_work}

The field of multimodal fake news detection has been significantly advanced through the integration of a variety of modalities, including text, images, and videos. This integration has expanded the scope of detection methods beyond those based entirely on text **Zhang, "Multimodal Fake News Detection"**. A review of the literature on deep learning techniques reveals that while considerable progress has been made in utilising multimodal data, challenges remain in the effective fusion of data and the integration of underutilised modalities, such as news propagation networks **Chen et al., "Multimodal Data Fusion for Fake News Detection"**. Furthermore, research gaps remain in the real-time detection and evaluation of metrics, with further opportunities to enhance model explainability and incorporate more sophisticated machine-learning approaches. Additionally, the interdisciplinary nature of disinformation research, involving computer science and the humanities, underscores the need for unified terminology and a more cohesive research community to address issues such as temporal dynamics in the spread of disinformation **Zhu et al., "Temporal Dynamics of Disinformation"**.

Disinformation detection based on textual content analysis involves examining linguistic attributes, including lexical, syntactic, and semantic features, using Natural Language Processing (NLP) techniques. The primary goal is to identify unique stylistic patterns used by disinformers **Peng et al., "Stylistic Patterns of Disinformers"**,, based on the idea that these entities typically adopt specific writing styles to attract and convince social media users, thereby instilling a false sense of trust. However, disinformers can mimic the writing styles of credible sources. Schuster et al. **Schuster et al., "Detecting Human-Written Disinformation"** found that while models based on stylistic features effectively detect human-written disinformation, they struggle with synthetic texts generated by advanced language models. Research on disinformation in social networks covers various domains. For instance, Peng et al. **Peng et al., "COVID-19 Misinformation"** investigated the dissemination of false information about COVID-19, using a question-based learning approach to evaluate the reliability of online texts. In a separate study, Rastogi and Bansal **Rastogi and Bansal, "Integrated Methodology for Disinformation Detection"** proposed an integrated methodology for the detection of disinformation on social media. This approach combines style-based and social context-based features and demonstrates the efficacy of an ensemble model in differentiating between authentic news, disinformation, and satire.

Due to the multimedia capabilities of social media platforms, malicious users create disinformation using text, images, and videos to enhance the content's credibility. Recently, these multimedia elements have been utilised to improve the accuracy of ML models **Wang et al., "Multimodal Disinformation Analysis"**. This method, known as multimodal disinformation analysis, involves various techniques to address the problem. For instance, Singh et al. **Singh et al., "Multimodal Approach for Disinformation Detection"** proposed a multimodal approach that integrates textual and visual features from labelled news article datasets to detect disinformation automatically, demonstrating that combining classic ML algorithms and multimodal data yields better results than relying on a single data type.
 c
Deep Learning techniques have become increasingly popular in multimodal disinformation detection because they effectively handle sequential data (text) and structured data with known topology (images). For instance, Chai et al. **Chai et al., "Combining Traditional Machine Learning Interpretability with Deep Learning"** introduced a model that combines traditional machine learning interpretability with the representational power of Deep Learning to identify misleading reviews and fraudulent emails. Typically, Deep Learning-based models for multimodal disinformation detection use two neural network architectures: text and images. These architectures extract features combined using a final neural network **Wang et al., "Feature Fusion in Multimodal Disinformation Detection"**. Recent advancements have incorporated the attention mechanism **Lu et al., "Attention Mechanism in Deep Learning"**, which allows the model to focus on the most relevant aspects of the input data, enhancing detection accuracy **Li et al., "Impact of Attention Mechanism on Detection Accuracy"**. Additionally, Hua et al. **Hua et al., "Contrastive Learning Module for Multimodal Disinformation Detection"** incorporated a contrastive learning module alongside attention mechanisms, which learns data representation by contrasting example pairs. This approach compares similar past news articles to improve the performance of the disinformation detection model.

The feature fusion not only occurs at the end of the multimodal disinformation detection process. Meel and Vishwakarma **Meel and Vishwakarma, "Tree Approaches to Fusion in Multimodal Disinformation Detection"** distinguished tree approaches to fusion: early fusion ---feature vectors from each pipeline are merged by concatenating them together **Xu et al., "Early Fusion for Feature Vectors"**--- late fusion ---involves combining the final probability decisions from each pipeline **Kumar et al., "Late Fusion for Probability Decisions"**--- and hybrid multi-level fusion  --- encompasses both early and late fusion, distributing the inputs among these two approaches. 

Other works try to enhance the detection of disinformation by incorporating additional features. For example, Wu et al. **Wu et al., "Human Cognition-based Consistency Inference Networks (HCCIN)"** introduced the Human Cognition-based Consistency Inference Networks (HCCIN) model, which captures consistent and inconsistent information between news content and its comments, incorporating an extra context element. On the other hand, Giachanou et al. **Giachanou et al., "Multimodal System for Disinformation Detection"** and Li et al. **Li et al., "Multimodal System for Disinformation Detection"** presented a multimodal system grounded on a neural network that integrates textual, visual, and semantic data to distinguish between false and trustworthy information. Furthermore, Zhang et al. **Zhang et al., "Semantic Information Extraction from Images"** focused on extracting semantic information from images by analysing visual scenes such as location, seasons, and weather. Similarly, Wang et al. **Wang et al., "Entity-Based Multimodal Disinformation Detection"** incorporated entities extracted from images as additional features in a multimodal approach.

A different approach was proposed by Gôlo et al. **Gôlo et al., "Multimodal Variational Autoencoders (MVAEs)"**. In the work, a novel method that utilises Multimodal Variational Autoencoders (MVAEs) to learn a new representation by combining various modalities such as text embedding, topic, and linguistic information is described. Notably, their employed learning method is One-Class Learning (OCL). This ML approach only requires positive examples to build a classifier, eliminating counterexamples and reducing the effort required for data labelling.

According to Hameleers et al. **Hameleers et al., "Multimodal Disinformation Perceived Credibility"** , multimodal disinformation is perceived as slightly more credible than textual disinformation. Moreover, to enhance the robustness of disinformation detectors, Chen et al. **Chen et al., "Visual Features in Multimodal Disinformation Detection"** recommended giving greater emphasis to visual features, particularly images associated with trending events, as these visual elements represent a shared vulnerability among such detectors.

Conversely, Han et al. **Han et al., "Multifaceted Reasoning Network for Explainable Fake News Detection (MRE-FND)"** presents the Multifaceted Reasoning Network for Explainable Fake News Detection (MRE-FND), which integrates a multitude of data sources, employs a dual graph neural network, and utilises explainable reasoning to enhance interpretability and accuracy. This model demonstrates the capacity for noise elimination and attains a state-of-the-art performance on major datasets. In contrast, Peng et al. **Peng et al., "Contextual Semantic Representation Learning for Multimodal Fake News Detection (CSFND)"** proposes Contextual Semantic Representation Learning for Multimodal Fake News Detection (CSFND), which emphasises the integration of contextual semantics into multimodal data, addressing inconsistencies in detection by incorporating an unsupervised context learning stage and a contextual testing strategy, enhancing detection accuracy and robustness.

In the work of Huda et al. **Huda et al., "Fake-checker: Combating Disinformation with Deep Learning and Texture Features"** , Fake-checker, a method that fuses deep learning with texture features, using the novel DMLHP descriptor and Inception V3 for feature extraction, is introduced. This approach achieves high accuracy in detecting deep fakes and targeting face-swapping and face-re-enactment techniques to combat disinformation