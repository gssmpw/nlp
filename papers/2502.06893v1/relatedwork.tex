\section{Related Work}
\label{sec:related_work}

The field of multimodal fake news detection has been significantly advanced through the integration of a variety of modalities, including text, images, and videos. This integration has expanded the scope of detection methods beyond those based entirely on text \cite{Comito2023}. A review of the literature on deep learning techniques reveals that while considerable progress has been made in utilising multimodal data, challenges remain in the effective fusion of data and the integration of underutilised modalities, such as news propagation networks \cite{Tufchi2023}. Furthermore, research gaps remain in the real-time detection and evaluation of metrics, with further opportunities to enhance model explainability and incorporate more sophisticated machine-learning approaches. Additionally, the interdisciplinary nature of disinformation research, involving computer science and the humanities, underscores the need for unified terminology and a more cohesive research community to address issues such as temporal dynamics in the spread of disinformation \cite{Wilson2023}.

Disinformation detection based on textual content analysis involves examining linguistic attributes, including lexical, syntactic, and semantic features, using Natural Language Processing (NLP) techniques. The primary goal is to identify unique stylistic patterns used by disinformers \cite{MontoroMontarroso2023}, based on the idea that these entities typically adopt specific writing styles to attract and convince social media users, thereby instilling a false sense of trust. However, disinformers can mimic the writing styles of credible sources. Schuster et al. \cite{Schuster2020} found that while models based on stylistic features effectively detect human-written disinformation, they struggle with synthetic texts generated by advanced language models. Research on disinformation in social networks covers various domains. For instance, Peng et al. \cite{PENG2023120501} investigated the dissemination of false information about COVID-19, using a question-based learning approach to evaluate the reliability of online texts. In a separate study, Rastogi and Bansal \cite{Rastogi2022} proposed an integrated methodology for the detection of disinformation on social media. This approach combines style-based and social context-based features and demonstrates the efficacy of an ensemble model in differentiating between authentic news, disinformation, and satire.

Due to the multimedia capabilities of social media platforms, malicious users create disinformation using text, images, and videos to enhance the content's credibility. Recently, these multimedia elements have been utilised to improve the accuracy of ML models \cite{Hangloo2022}. This method, known as multimodal disinformation analysis, involves various techniques to address the problem. For instance, Singh et al. \cite{singh2021} proposed a multimodal approach that integrates textual and visual features from labelled news article datasets to detect disinformation automatically, demonstrating that combining classic ML algorithms and multimodal data yields better results than relying on a single data type.
 c
Deep Learning techniques have become increasingly popular in multimodal disinformation detection because they effectively handle sequential data (text) and structured data with known topology (images). For instance, Chai et al. \cite{CHAI2024121588} introduced a model that combines traditional machine learning interpretability with the representational power of Deep Learning to identify misleading reviews and fraudulent emails. Typically, Deep Learning-based models for multimodal disinformation detection use two neural network architectures: text and images. These architectures extract features combined using a final neural network \cite{Sengan2023, Jing2023, Ghorbanpour2023}. Recent advancements have incorporated the attention mechanism \cite{Guo2023}, which allows the model to focus on the most relevant aspects of the input data, enhancing detection accuracy \cite{Yadav2023}. Additionally, Hua et al. \cite{Hua2023} incorporated a contrastive learning module alongside attention mechanisms, which learns data representation by contrasting example pairs. This approach compares similar past news articles to improve the performance of the disinformation detection model.

The feature fusion not only occurs at the end of the multimodal disinformation detection process. Meel and Vishwakarma \cite{Meel2023} distinguished tree approaches to fusion: early fusion ---feature vectors from each pipeline are merged by concatenating them together \cite{Xiong2023}--- late fusion ---involves combining the final probability decisions from each pipeline \cite{Singh2023}--- and hybrid multi-level fusion  --- encompasses both early and late fusion, distributing the inputs among these two approaches. 

Other works try to enhance the detection of disinformation by incorporating additional features. For example, Wu et al. \cite{Wu2023} introduced the Human Cognition-based Consistency Inference Networks (HCCIN) model, which captures consistent and inconsistent information between news content and its comments, incorporating an extra context element. On the other hand, Giachanou et al. \cite{Giachanou2020} and Li et al. \cite{Li2022} presented a multimodal system grounded on a neural network that integrates textual, visual, and semantic data to distinguish between false and trustworthy information. Furthermore, Zhang et al. \cite{Zhang2022} focused on extracting semantic information from images by analysing visual scenes such as location, seasons, and weather. Similarly, Wang et al. \cite{Wang2023} incorporated entities extracted from images as additional features in a multimodal approach.

A different approach was proposed by GÃ´lo et al. ~\cite{Silva2023}. In the work, a novel method that utilises Multimodal Variational Autoencoders (MVAEs) to learn a new representation by combining various modalities such as text embedding, topic, and linguistic information is described. Notably, their employed learning method is One-Class Learning (OCL). This ML approach only requires positive examples to build a classifier, eliminating counterexamples and reducing the effort required for data labelling.

According to Hameleers et al. \cite{Hameleers2020}, multimodal disinformation is perceived as slightly more credible than textual disinformation. Moreover, to enhance the robustness of disinformation detectors, Chen et al. \cite{Chen2023} recommended giving greater emphasis to visual features, particularly images associated with trending events, as these visual elements represent a shared vulnerability among such detectors.

Conversely, Han et al. \cite{Han2024} presents the Multifaceted Reasoning Network for Explainable Fake News Detection (MRE-FND), which integrates a multitude of data sources, employs a dual graph neural network, and utilises explainable reasoning to enhance interpretability and accuracy. This model demonstrates the capacity for noise elimination and attains a state-of-the-art performance on major datasets. In contrast, Peng et al. \cite{Peng2024} proposes Contextual Semantic Representation Learning for Multimodal Fake News Detection (CSFND), which emphasises the integration of contextual semantics into multimodal data, addressing inconsistencies in detection by incorporating an unsupervised context learning stage and a contextual testing strategy, enhancing detection accuracy and robustness.

In the work of Huda et al. \cite{Huda2024}, Fake-checker, a method that fuses deep learning with texture features, using the novel DMLHP descriptor and Inception V3 for feature extraction, is introduced. This approach achieves high accuracy in detecting deep fakes and targeting face-swapping and face-re-enactment techniques to combat disinformation.