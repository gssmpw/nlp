\section{Related Work}
\subsection{LLM Quantizations and Massive Values in LLM}

Various existing works in quantization have studied the existence of massive value (also called outlier) features in LLMs~\citep{dettmers2022gpt3, ahmadian2023intriguing, guo2024activedormant}. Some research showed that massive value features have large activation values in most of their sequence dimensions. BERT often focuses attention on the "[SEP]" token~\citep{clark-etal-2019-bert, xiao2024efficient}, while LLMs predominantly assign attention to the starting word token~\citep{xiao2024efficient}, Some research focuses on the massive activation value in activation in LLM~\citep{sun2024massive} and identified attention artifacts in ViTs~\citep{darcet2024vision}. Someone trying to understand the mechanism of these massive values in transformer~\citep{guo2024activedormant}.

Research on LLM quantization has predominantly focused on analyzing model weights and activations through two main approaches: (1) W8A8 quantization, which converts both activations and weights to INT8 \citep{wei2022outlier, wei-etal-2023-outlier, xiao2023smoothquant, yao2022zeroquant, dettmers2022gpt3}, and (2) low-bit weight-only quantization (e.g., W4A16), which converts only weights to low-bit integers \citep{frantar-gptq, lin2023awq, sheng2023flexgen, park2024lut}. 
During quantization, some studies have identified massive values in activation values \citep{liukivi, sun2024massive} as a critical factor that can affect quantization. Several approaches have been developed to handle massive values specifically, either specific suppression techniques \citep{wei2022outlier, wei-etal-2023-outlier} or protection mechanisms \citep{lin2023awq, xiao2023smoothquant} to maintain massive values unchanged. Quantization methods specifically address massive values that are typically analyzed within individual attention heads. Our work examines massive values across the depth dimension of multiple attention heads, offering potential new directions for future quantization approaches.

\subsection{Rotary Positional Encoding}
Rotary Positional Encoding (RoPE) has been extensively studied in transformer architectures since its introduction by \citep{su2021roformer} as a relative position encoding method. RoPE has been widely adopted in various LLMs, including Llama \citep{touvron2023llama}, Palm \citep{anil2023palm}, Mistral \citep{jiang2023mistral}, and GLM \citep{du2022glm}. Recent research on long-context Transformers \citep{sun-etal-2023-length, xiong-etal-2024-effective, dubey2024llama} has demonstrated RoPE's effectiveness in input length extrapolation. These studies found that increasing the $\theta$ parameter in RoPE from its original value of 10,000 to larger values (e.g., 500,000) reduces attention to decay and enables more robust learning across extended contexts. These studies demonstrated that increasing RoPE's $\theta$ parameter from its original value of 10,000 to larger values (e.g., 500,000) mitigates attention decay and enhances robust learning over extended context windows. And Qwen2-VL adopts Multimodal Rotary Position Embedding (M-ROPE) that decomposes positional embedding into parts to capture 1D, 2D, and 3D positional information, enhancing its multimodal processing capabilities~\citep{Qwen2VL}. In parallel, research on NoPE (No Positional Encodings) has shown promising results in out-of-distribution (OOD) settings compared to RoPE, suggesting that the causal mechanism alone might sufficiently capture positional information without explicit position encoding \citep{haviv2022transformer, kazemnejad2024impact}. More recently, the analysis of Gemma-7b suggests an essential role of frequency components: the high-frequency part of the embedding vector in LLM encodes positional information, while the low-frequency part carries semantic information \citep{barbero2024round}. This work takes a different approach by investigating how RoPE might contribute to the formation of massive value in LLM of transformer architectures.