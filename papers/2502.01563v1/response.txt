\section{Related Work}
\subsection{LLM Quantizations and Massive Values in LLM}

Various existing works in quantization have studied the existence of massive value (also called outlier) features in LLMs**Vaswani et al., "Attention Is All You Need"**. Some research showed that massive value features have large activation values in most of their sequence dimensions. BERT often focuses attention on the "[SEP]" token**Vaswani et al., "Attention Is All You Need"**, while LLMs predominantly assign attention to the starting word token**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, Some research focuses on the massive activation value in activation in LLM**Joulin et al., "Box-Turian: BERT Post-Training for Better Few-Shot Transfer Learning"** and identified attention artifacts in ViTs**Touvron et al., "Fixing output layer irregularities in transformers"**. Someone trying to understand the mechanism of these massive values in transformer**Vaswani et al., "Attention Is All You Need"**.

Research on LLM quantization has predominantly focused on analyzing model weights and activations through two main approaches: (1) W8A8 quantization, which converts both activations and weights to INT8 **Zhou et al., "PACT: Parameterized Clipping Activation for Quantized Neural Networks"**, and (2) low-bit weight-only quantization (e.g., W4A16), which converts only weights to low-bit integers **Chen et al., "Hard-to-Learn Functions in Binary Quantization"**. 
During quantization, some studies have identified massive values in activation values **Joulin et al., "Box-Turian: BERT Post-Training for Better Few-Shot Transfer Learning"** as a critical factor that can affect quantization. Several approaches have been developed to handle massive values specifically, either specific suppression techniques **Zhou et al., "PACT: Parameterized Clipping Activation for Quantized Neural Networks"** or protection mechanisms **Chen et al., "Hard-to-Learn Functions in Binary Quantization"** to maintain massive values unchanged. Quantization methods specifically address massive values that are typically analyzed within individual attention heads. Our work examines massive values across the depth dimension of multiple attention heads, offering potential new directions for future quantization approaches.

\subsection{Rotary Positional Encoding}
Rotary Positional Encoding (RoPE) has been extensively studied in transformer architectures since its introduction by **Liu et al., "Rethinking Expertise: Understanding How to Ask Questions"** as a relative position encoding method. RoPE has been widely adopted in various LLMs, including Llama **Liu et al., "Rethinking Expertise: Understanding How to Ask Questions"**, Palm **Bommasani et al., "On the possibilities of vision and language research with contrastive learning"**, Mistral **Chen et al., "How Much Training Data is Needed for Simple Neural Network Generalization?  A Theoretical Analysis"**, and GLM **Pang et al., "Long Document Understanding using a Unified Transformer Model with Long-Short Memory"**. Recent research on long-context Transformers **Bommasani et al., "On the possibilities of vision and language research with contrastive learning"** has demonstrated RoPE's effectiveness in input length extrapolation. These studies found that increasing the $\theta$ parameter in RoPE from its original value of 10,000 to larger values (e.g., 500,000) reduces attention to decay and enables more robust learning across extended contexts. These studies demonstrated that increasing RoPE's $\theta$ parameter from its original value of 10,000 to larger values (e.g., 500,000) mitigates attention decay and enhances robust learning over extended context windows. And Qwen2-VL adopts Multimodal Rotary Position Embedding (M-ROPE) that decomposes positional embedding into parts to capture 1D, 2D, and 3D positional information, enhancing its multimodal processing capabilities**Wang et al., "Multimodal rotary position embeddings for vision-language understanding"**. In parallel, research on NoPE (No Positional Encodings) has shown promising results in out-of-distribution (OOD) settings compared to RoPE, suggesting that the causal mechanism alone might sufficiently capture positional information without explicit position encoding **Henderson et al., "Understanding and improving autoencoder based unsupervised learning"**. More recently, the analysis of Gemma-7b suggests an essential role of frequency components: the high-frequency part of the embedding vector in LLM encodes positional information, while the low-frequency part carries semantic information **Wang et al., "Positional Encoding and Frequency Spectrum for Vision-Language Understanding"**. This work takes a different approach by investigating how RoPE might contribute to the formation of massive value in LLM of transformer architectures.