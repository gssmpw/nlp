@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191 (Qwen Team Tech Report)},
  year={2024}
}

@inproceedings{ahmadian2023intriguing,
  title={Intriguing properties of quantization at scale},
  author={Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and Venkitesh, Bharat and Gou, Zhen Stephen and Blunsom, Phil and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  pages={34278--34294},
  year={2023}
}

@article{anil2023palm,
  title={Palm 2 Technical Report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403 (Google Technical Report)},
  year={2023}
}

@inproceedings{barbero2024round,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}`s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828/",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
}

@inproceedings{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={320--335},
  year={2022}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783 (Meta AI Technical Report)},
  year={2024}
}

@inproceedings{frantar-gptq,
  title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  year={2022},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{haviv2022transformer,
  title={Transformer Language Models without Positional Encodings Still Learn Positional Information},
  author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022 (EMNLP)},
  pages={1382--1390},
  year={2022}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825 (Mistral AI Technical Report)},
  year={2023}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2024}
}

@inproceedings{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={Conference on Machine Learning and Systems (MLSys)},
  year={2024}
}

@inproceedings{liukivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle={Forty-first International Conference on Machine Learning (ICML)}
}

@inproceedings{park2024lut,
  title={Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models},
  author={Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{sun-etal-2023-length,
    title = "A Length-Extrapolatable Transformer",
    author = "Sun, Yutao  and
      Dong, Li  and
      Patra, Barun  and
      Ma, Shuming  and
      Huang, Shaohan  and
      Benhaim, Alon  and
      Chaudhary, Vishrav  and
      Song, Xia  and
      Wei, Furu",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)",
    year = "2023",
    address = "Toronto, Canada",
    url = "https://aclanthology.org/2023.acl-long.816/",
    pages = "14590--14604",
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971 (Meta AI Technical Report)},
  year={2023}
}

@inproceedings{wei-etal-2023-outlier,
    title = "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
    author = "Wei, Xiuying  and
      Zhang, Yunchen  and
      Li, Yuhang  and
      Zhang, Xiangguo  and
      Gong, Ruihao  and
      Guo, Jinyang  and
      Liu, Xianglong",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = dec,
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.102/",
    pages = "1648--1665",
}

@inproceedings{wei2022outlier,
    title={Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models},
    author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
    booktitle={Proceedings of the 34th Advances in Neural Information Processing Systems (NeurIPS)},
    year={2021}
}

@InProceedings{xiao2023smoothquant,
    title = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
    author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
    year = {2023}
}

@inproceedings{xiong-etal-2024-effective,
    title = "Effective Long-Context Scaling of Foundation Models",
    author = "Xiong, Wenhan  and
      Liu, Jingyu  and
      Molybog, Igor  and
      Zhang, Hejia  and
      Bhargava, Prajjwal  and
      Hou, Rui  and
      Martin, Louis  and
      Rungta, Rashi  and
      Sankararaman, Karthik Abinav  and
      Oguz, Barlas  and
      Khabsa, Madian  and
      Fang, Han  and
      Mehdad, Yashar  and
      Narang, Sharan  and
      Malik, Kshitiz  and
      Fan, Angela  and
      Bhosale, Shruti  and
      Edunov, Sergey  and
      Lewis, Mike  and
      Wang, Sinong  and
      Ma, Hao",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)",
    year = "2024",
    address = "Mexico City, Mexico",
    url = "https://aclanthology.org/2024.naacl-long.260/",
    pages = "4643--4663",
}

@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={27168--27183},
  year={2022}
}

