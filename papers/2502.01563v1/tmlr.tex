
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[accepted]{tmlr}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{multirow}




\usepackage{amsmath} % For enhanced mathematical formatting
\usepackage[T1]{fontenc} % T1 Font encoding
\usepackage{libertine} % This package provides the Linux Libertine fonts
\usepackage{amsmath} % For mathematical content
\usepackage{amssymb} % For additional mathematical symbols
\usepackage{amsthm}  % For theorem-like environments
\usepackage{pifont}

% Define 'definition' environment
\newtheorem{definition}{Definition}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}

\newtheorem{remark}[theorem]{Remark}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\definecolor{mygray}{RGB}{226, 226, 226}
\definecolor{myred}{RGB}{252, 142, 142}
\definecolor{mygreen}{RGB}{147, 255, 143}
\definecolor{myblue}{RGB}{144, 155, 255}
\definecolor{myyellow}{RGB}{253, 253, 143}
\definecolor{mypurple}{RGB}{255, 142, 250}
\definecolor{softblue}{RGB}{100, 149, 237}
\definecolor{mygreen}{RGB}{62,123,39}
\hypersetup{
    colorlinks,
    linkcolor=mypurple,
    anchorcolor=blue,
    citecolor=softblue
} 

\usepackage[textsize=tiny]{todonotes}




\title{Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.
\author{\name Mingyu Jin \textsuperscript{$1$}\thanks{\quad Equal contribution and shared co-first authorship.} \email mingyu.jin@rutgers.edu
      \AND
      \name Kai Mei \textsuperscript{$1$}\footnotemark[1]\email kai.mei@rutgers.edu 
      \AND
      \name Wujiang Xu \textsuperscript{$1$} \email wujiang.xu@rutgers.edu 
      \AND
      \name Mingjie Sun \textsuperscript{$2$} \email mingjies@andrew.cmu.edu 
      \AND
      \name Ruixiang Tang \textsuperscript{$1$}\email ruixiang.tang@rutgers.edu 
      \AND
      \name Mengnan Du \textsuperscript{$3$} \email mengnan.du@njit.edu 
      \AND
      \name Zirui Liu \textsuperscript{$4$} \footnotemark[2]\email zrliu@umn.edu
      \AND
      \name Yongfeng Zhang\textsuperscript{$1$} \thanks{\quad Corresponding author.}  \email yongfeng.zhang@rutgers.edu\\ \\
\textsuperscript{$1$} \textbf{Rutgers University}
\textsuperscript{$2$} \textbf{Carnegie Mellon University}
\textsuperscript{$3$} \textbf{New Jersey Institute of Technology}\\
\textsuperscript{$4$} \textbf{University of Minnesota}
      }


% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (i.e., knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model’s parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope\_with\_LLM.
\end{abstract}
\section{Introduction}

Large Language Models (LLMs) have achieved remarkable success across various applications \citep{wei2022chain, brown2020language, achiam2023gpt, jin2024disentangling}. However, our understanding of their internal mechanisms and how these mechanisms relate to observable behaviors remains limited, posing a challenge to improving their reliability and performance. Researchers have explored LLM representations \citep{azaria2023internal, rger2024truth, gurnee2024language} and discovered that residual stream activations can exhibit massive values (i.e., magnitudes significantly larger than typical values \citep{dettmers2022gpt3, wei2022outlier, wei-etal-2023-outlier, sun2024massive, heo2024rethinking}). 
Recent studies further \citep{liukivi, hooper2024kvquant, zhao2024atom} observe that massive values appear exclusively in Q and K while are absent in V.
Here, by Q, K, and V, we mean the representations output by the query, key, and value layers in self-attentions, respectively. These massive values have been identified as critical factors influencing quantization \citep{liukivi, lin2023awq}, leading to the development of methods such as suppression techniques \citep{wei2022outlier, wei-etal-2023-outlier} and protection mechanisms \citep{lin2023awq, xiao2023smoothquant}. Although these studies utilize observations of massive values for quantization, they do not explore the rationale behind this counterintuitive phenomenon deeply.



To address this gap, we systematically investigate the formation of massive values and their connection to model behaviors. Our key findings are as follows:
\ding{182} \textbf{\textit{Massive values are concentrated in specific regions of Q and K exclusively.}}, which echos the finding in Round and Round we go~\citep{barbero2024round}. We observe that massive values concentrate in specific regions of Q and K computations, these massive values in each head's dim index are very close, as is shown in \autoref{fig:666}. While self-attention computation dictates that each attention head operates independently, the massive values across different heads consistently cluster at remarkably similar positional indices, which is very counterintuitive. This phenomenon is absent in V computations and absent in models without RoPE, such as GPT-2 \citep{black-etal-2022-gpt} and OPT \citep{zhang2022opt}.
\ding{183} \textbf{\textit{Massive values in Q and K are critical for understanding contextual knowledge over parametric knowledge.}}
Our analysis shows that massive values significantly contribute to contextual knowledge understanding ability(i.e., knowledge augmented through the context window) rather than parametric knowledge retrieval ability(i.e., knowledge encoded during training). Disrupting these values leads to a notable degradation in tasks requiring contextual understanding, such as passkey retrieval~\citep{jin2024llm, mohtashami2023randomaccess}, IMDB review analysis~\citep{maas-etal-2011-learning}, and mathematical reasoning~\citep{cobbe2021training, ling-etal-2017-program}. In contrast, tasks involving parametric knowledge, such as World-Cities~\citep{marks2023geometry} (Is the city of Krasnodar in Russia?), are only subtly affected. Perplexity analysis reveals that disrupting massive values causes a significant decline in reasoning benchmarks, particularly in IMDB tasks, compared to when non-massive values are disrupted.
\ding{184} \textbf{\textit{Quantization techniques targeting massive values preserve contextual knowledge better.}}
Our experiments on various quantization methods show that techniques like AWQ~\citep{lin2023awq} and SmoothQuant~\citep{xiao2023smoothquant}, which scale per-channel weights or activations to reduce the dominance of massive values, effectively maintain LLM's contextual knowledge understanding capabilities. In contrast, methods that do not specifically address massive values~\citep{frantar-gptq} exhibit significant degradation of LLM's performance in contextual knowledge understanding tasks while performance in parametric knowledge retrieval are maintained. These findings align with our analysis, which shows that massive values contribute more to LLM's contextual knowledge understanding capabilities. 
\ding{185} \textbf{\textit{Concentration of massive values is caused by RoPE \citep{su2021roformer} and it appears since very early layers in LLMs. }}
We analyze the root cause of concentrated massive values in the Q and K and investigate layers where massive values become concentrated. Our analysis demonstrates that this phenomenon originates from the mechanisms of RoPE, which makes low-frequency regions in the Q and K less affected by position information, and the concentration of massive values becomes apparent from the very first layers. Our temporal and causal analysis deepens the understanding of how and when massive values are concentrated. 





In summary, our contributions are as follows:

$\bullet$\; We systematically investigate the emergence of massive values in attention modules and identify that their concentrations occur exclusively in Q and K, revealing a distinct structural property of these components.  

$\bullet$\; We analyze the functional role of these massive values in Q and K and conduct extensive experiments on ten datasets to demonstrate that they play a crucial role in LLMs' ability to process contextual knowledge while having subtle impact on parametric knowledge retrieval.  

$\bullet$\; We evaluate three representative quantization methods and show that those explicitly addressing massive values better preserve LLMs’ contextual understanding capabilities. This finding reinforces our analysis and provides insights into designing new quantization strategies.  

$\bullet$\; We conduct the causal and temporal analysis of massive value concentrations, uncovering that they originate from RoPE mechanisms and emerge as early as the initial layers. This deepens the understanding of how massive values form and propagate through the model.  



\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{figs/Llama_K_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{figs/Llama_K_attention_map_layer_20.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{figs/Llama_Q_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{figs/Llama_Q_attention_map_layer_20.pdf}
        \label{fig:image4}   
    \end{subfigure}    
    \vspace{-10pt}
    \caption{Q and K Embedding Vector in Llama-2-7B, we choose Layer 10 and 20, and the input question is shown as \autoref{fig:prompt_in_inference_LLM}. The horizontal axis is the \textit{number of head} and the vertical axis is \textit{head dim}. We can see that the massive value is concentrated at the bottom of the picture.}  
    \label{fig:666}
    \vspace{-10pt}
\end{figure*}

% Consequently, existing studies have only partially investigated how these extreme values emerge and their impact.
% However, more work is to use outliers' impact on quantifying and accelerating LLM.



\section{Preliminary}
\label{Preliminary}
\subsection{Position Encoding}
\textbf{Position Encoding.}
Position encoding in transformers can be categorized into two main approaches: absolute and relative position encodings. Absolute position embeddings assign fixed positional vectors, such as sinusoidal embeddings~\cite{Vaswani2017AttentionIA}, learned embeddings used in models like GPT-3~\citep{brown2020language}, or dot-product-based adjustments ~\cite{kerethinking}. In contrast, relative position encodings focus on the distances between tokens and integrate into attention layers, with examples including logit biases like T5~\cite{raffel2020exploring, Dai2019TransformerXLAL} and rotary position embeddings like RoPE~\cite{su2021roformer}. 

\textbf{RoPE.}
Here, we introduce the basic concept of RoPE. Let’s consider a sequence of tokens represented as $w_1, w_2, \cdots, w_L$, and their corresponding embeddings are denoted as $\mathbf{x}_1, \cdots, \mathbf{x}_L \in \mathbb{R}^{|D|}$, where $|D|$ is the dimension of the embedding. The basic idea of RoPE is to incorporate the positional information into the query $\mathbf{q}$ and the key vectors $\mathbf{k}$, respectively. This integration ensures that their inner product $\mathbf{q}^T\mathbf{k}$ will contain the relative positional embedding information inherently. To achieve this, RoPE employs the following vector transformations:
\begin{equation}
    \mathbf{q}_m = f_q(\mathbf{x}_m, m) \in \mathbb{R}^{|L|}, \mathbf{k}_n = f_k(\mathbf{x}_n, n) \in \mathbb{R}^{|L|}
\end{equation}
where $|L|$ is the hidden dimension of each head. The functions $f_q$ and $f_k$, responsible for injecting positional information, are defined as:
\begin{equation}
    f_q(\mathbf{x}_m, m) = W_q \mathbf{x}_m e^{im\theta}, f_k(\mathbf{x}_n, n) = W_k \mathbf{x}_n e^{in\theta}
\end{equation}
where $\theta_d = b^{-2d/|D|}$, $b = 10,000$, and the projection matrices $W_q, W_k : \mathbb{R}^{|D|} \to \mathbb{R}^{|L|}$. RoPE keeps the real part of the inner product $\mathbf{q}^T\mathbf{k}$, which is $\mathrm{Re}(\mathbf{q}^*\mathbf{k})$. This operation ensures that the dot product of the query and key vectors depends entirely on the relative distance between the tokens, represented by $m - n$, as follows:
\begin{equation}
\begin{split}
    \langle f_q(\mathbf{x}_m, m), f_k(\mathbf{x}_n, n) \rangle_{\mathbb{R}} 
    = \mathrm{Re}(\langle f_q(\mathbf{x}_m, m), f_k(\mathbf{x}_n, n) \rangle_{\mathbb{C}}) \\
    = \mathrm{Re}(\mathbf{x}_m^* W_q^* W_k \mathbf{x}_n e^{i\theta(m-n)}) = g(\mathbf{x}_m, \mathbf{x}_n, m-n)
\end{split}
\end{equation}
where $g(\cdot)$ is an abstract mapping function.

\subsection{LLM Inference Workflow}
\label{inferencellm}
LLM inference consists of a prefilling phase that processes the input context and a decode phase that generates tokens autoregressively, with the prefilling phase establishing the initial context representation and the decode phase iteratively producing new tokens based on this foundation. 

\textbf{Prefilling Phase.} Given the $X \in \mathbb{R}^{l_{\text{prompt}} \times h \times d}$ be the input tensor (we set batch size as 1 by default), $l_{\text{prompt}}$ is the length of the input prompt, $h$ is the number of head, and $d$ is the model hidden size. For convenience, we ignore the layer index here. The key value tensors can be computed by
\begin{equation}
    X_K = X W_K, \quad X_V = X W_V,
\end{equation}
where $W_K, W_V \in \mathbb{R}^{d \times d}$ are the key and value's layer weights, respectively. 
After obtaining $X_K$ and $X_V$, they are cached in the memory for easy decoding. 


\textbf{Decoding Phase.} During the decoding phase, let $t \in \mathbb{R}^{b \times 1 \times d}$ be the current input token embedding. 
Let $t_K = t W_K$ and $t_V = t W_V$ be the key and value layer output, respectively. 
The KV cache is updated by:
\begin{equation}
    X_K \leftarrow \text{Concat}(X_K, t_K), \quad 
    X_V \leftarrow \text{Concat}(X_V, t_V),
\end{equation}
then attention output is calculated as:
\begin{equation}
    t_Q = t W_Q, \quad 
    A = \text{Softmax}(t_Q X_K^\top), \quad 
    t_O = A X_V
\end{equation}


where $W_Q$ is the weight matrix of the query layer. 
For ease of illustration, we ignore the attention output layer and other parts of the inference workflow.




\section{The Effect of Massive Values in Knowledge Understanding}



\subsection{Massive Value}
The attention queries (Q) and keys (K) in mainstream LLMs are typically represented as  
\( Q, K \in \mathbb{R}^{\mathcal{B} \times \mathcal{S} \times \mathcal{H} \times \mathcal{D}} \),  
where \( \mathcal{B} \) is the batch size, \( \mathcal{S} \) is the sequence length,  
\( \mathcal{H} \) is the number of attention heads, and \( \mathcal{D} \) is the head dimension.  
Assuming \( \mathcal{B} = 1 \), we compute the \( L_{2} \) norm along the sequence length dimension,  
reducing the representation to a matrix \( M \in \mathbb{R}^{\mathcal{H} \times \mathcal{D}} \),  
where each element \( M_{h, d} \) represents the norm of the corresponding component in head \( h \)  
and dimension \( d \). Taking \( Q \) as an example, \( M_{h, d} \) is computed as:  

\vspace{-10pt}
\begin{equation} \label{eq:1}
    M_{h, d} = \left\| Q_{:, h, d} \right\|_2 = \sqrt{\sum_{s=1}^{\mathcal{S}} Q_{s, h, d}^2 }.
\end{equation}

\begin{definition}{(Massive Value)}\label{def:massive_value}  
A massive value is an element \( M_{h, d} \) that satisfies:  

\vspace{-10pt}
\begin{equation} \label{eq:2}
    M_{h, d} > \lambda \frac{1}{\mathcal{D}} \sum_{d' = 1}^{\mathcal{D}} M_{h, d'}
\end{equation}

where \( \lambda > 1 \) is a threshold controlling massive value selection.  
In our experiments, we empirically set \( \lambda = 5 \).
\end{definition}   


In each attention head, certain dimensions exhibit notably massive values, and these tend to cluster in specific dimensional regions (shown in the black box in the figure~\ref{fig:666}). Moreover, across different attention heads, these large values often appear at similar positions.


\subsection{Contextual Knowledge Understanding and Parametric Knowledge Retrieval}

Research demonstrates that Large Language Models (LLMs) acquire extensive knowledge through pre-training on large-scale corpora, termed Parametric Knowledge (PK) \citep{cheng2024understanding, xu-etal-2024-knowledge-conflicts}. In practical applications, LLMs augment this parametric knowledge (PK) with Contextual Knowledge (CK)—additional information provided within the input context \citep{xu-etal-2024-knowledge-conflicts, antoniades2024generalization}. This distinction gives rise to two fundamental tasks:


\begin{remark}{\textit{(Contextual Knowledge Understanding)}}\label{def:icl} \textit{refers to understanding the content within a paragraph and using the information it provides to answer questions. For example, it could involve identifying a key amidst a collection of meaningless text or determining whether a movie review is positive or negative based on the content of the review~\citep{xu-etal-2024-knowledge-conflicts, cheng2024understanding, wen2024rnns}.}
\end{remark}

\begin{remark}{\textit{(Parametric Knowledge Retrieval)}}\label{def:p} \textit{refers to questions that can be answered correctly by simply using the query and the knowledge within the model to perform a retrieval match. For example, "What is the capital of the United States?" The answer would be "Washington D.C."~\citep{xu-etal-2024-knowledge-conflicts,cheng2024understanding}}

\end{remark}
% Contextual Knowledge Understanding is crucial for real-world applications~\cite{ram2023context}, requiring information extraction and interpretation from dynamic contexts.

\textbf{Dataset: }
We separate datasets into the two main categories corresponding to Remark~\ref{def:icl} and Remark~\ref{def:p}. For \textbf{\textit{Contextual Knowledge Understanding Tasks}}, we adopt mathematical reasoning benchmarks (i.e., GSM-8K \cite{cobbe2021training}, AQUA \cite{ling-etal-2017-program}), sentiment analysis dataset (i.e., IMDB \citep{maas-etal-2011-learning}) and synthetic passkey retrieval datasets as \autoref{app:synthesis} in different difficulty levels.  
For \textbf{\textit{Parametric Knowledge Retrieval Tasks}}, we adopt factual knowledge QA such as Cities~\cite{marks2023geometry} and our synthetic datasets covering topics in Sports, Arts, Technology and Celebrity. The rationale behind our synthesis choice is that we would like to use more simple and direct queries that focus on direct factual knowledge rather than blended with reasoning. The dataset details and our data synthesis pipeline can be found at \autoref{app:detail}. 





\subsection{Disruption of Massive Value} \label{sec:pre_and_setup}
To investigate the impact of massive values in \textit{Definition \ref{def:massive_value}} on LLM performance, we systematically examine the effects of disrupting both massive values and non-massive values, respectively, in LLMs equipped with RoPE. The disruption is carefully synchronized with the LLM's two inference phases, the prefill stage and decode stage, like Section \ref{inferencellm} content generation process to ensure accurate manipulation of massive values at the appropriate stage.

\textbf{Disruption Setup. }
Our disruption of massive values and non-massive values is specifically targeted at the \textbf{prefilling stage}, as disrupting both stages would compromise the model's fundamental language modeling capabilities. This selective approach ensures that any observed performance changes can be attributed to the impact on context processing rather than impairment of the model's generative abilities. Let $\mathbf{X} \in \mathbb{R}^{l \times h \times d}$ denote the query tensor. We replace the values at the massive value indices with the average value computed over the query tensor. 

\vspace{-6pt}
\begin{equation}
\mathbf{X}_{i;j;k^*} =\left\{
\begin{array}{lll}
\mathrm{Mean}(\mathbf{X})\;, & &  {  k^* = \text{argmax}_{k} x_{i;j;k} }\\
\mathbf{X}_{i;j;k^*}\;, & & {k^* \neq \text{argmax}_{k} x_{i;j;k} }
\end{array} \right.
\label{head_tailX}
\end{equation}

Our investigation reveals that disrupting massive values can be accomplished through several substitution methods—using mean values, zeros, maxima, or minima. We disrupt massive/non-massive values \textit{on both Q and K}. More comprehensive experiments comparing these replacement strategies are detailed in Section \ref{Preliminary}.



\begin{table*}[htbp]
\centering
\caption{Results of LLMs under different settings (vanilla, massive value disrupted, non-massive value disrupted) on different benchmarks. For the Passkey Retrieval Task, the values (max prompt token length, passkey length) represent the maximum number of tokens allowed in the prompt and the length of the passkey to be retrieved, respectively. All values are reported in percentage (\%). }
\vspace{3pt}
\label{tab:main_result}
\adjustbox{max width=1.0\textwidth}{
\begin{tabular}
{lccccccccccc}
\toprule
\multirow{3}{*}{\large \textbf{Model}} & \multicolumn{6}{c}{\large \textbf{Contextual Knowledge Understanding Task}} & \multicolumn{5}{c}{\large \textbf{Parametric Knowledge Retrieval Task}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-12}
& \multicolumn{1}{c}{\multirow{2}{*}{GSM8K}} & \multicolumn{1}{c}{\multirow{2}{*}{AQUA-RAT}} & \multicolumn{3}{c}{Passkey Retrieval Task} & \multicolumn{1}{c}{\multirow{2}{*}{IMDB}} & \multicolumn{1}{c}{\multirow{2}{*}{Cities}} & \multicolumn{1}{c}{\multirow{2}{*}{Sports}} & \multicolumn{1}{c}{\multirow{2}{*}{Art}} & \multicolumn{1}{c}{\multirow{2}{*}{Technology}} & \multicolumn{1}{c}{\multirow{2}{*}{Celebrity}} \\
\cmidrule{4-6}
& & & \multicolumn{1}{c}{(128,6)} & \multicolumn{1}{c}{(256,12)} & \multicolumn{1}{c}{(1024,48)} & & & & & & \\
\midrule
\textbf{\textit{Gemma2-9B}} & 81.30 & 63.80 & 100.00 & 100.00 & 100.00 & 94.70 & 99.70 & 91.00 & 84.00 & 81.00 & 92.50 \\
\texttt{\small + Non-Massive Value Disrupted} & 81.60 & 65.60 & 100.00 & 100.00 & 100.00 & 97.40 & 99.60 & 91.00 & 84.00 & 81.50 & 92.50 \\
\texttt{\small + Massive Value Disrupted} & 15.10 & 16.50 & 2.00 & 0.00 & 0.00 & 1.80 & 76.40 & 73.50 & 68.00 & 72.00 & 82.00 \\
\midrule
\textbf{\textit{Llama3-8B}} & 76.90 & 53.51 & 100.00 & 100.00 & 100.00 & 95.40 & 99.40 & 95.00 & 93.50 & 92.50 & 95.00 \\
\texttt{\small + Non-Massive Value Disrupted} & 77.40 & 53.90 & 100.00 & 100.00 & 100.00 & 95.40 & 99.40 & 94.50 & 93.00 & 92.50 & 95.50 \\
\texttt{\small + Massive Value Disrupted} & 4.00 & 9.68 & 9.00 & 0.00 & 0.00 & 11.00 & 88.20 & 74.50 & 64.00 & 74.90 & 73.00 \\
\midrule
\textbf{\textit{Qwen2.5-7B}} & 86.60 & 56.69 & 100.00 & 100.00 & 100.00 & 96.80 & 97.70 & 95.00 & 96.00 & 90.00 & 93.50 \\
\texttt{\small + Non-Massive Value Disrupted} & 85.40 & 57.28 & 100.00 & 100.00 & 100.00 & 97.60 & 97.50 & 94.00 & 96.50 & 90.00 & 93.50 \\
\texttt{\small + Massive Value Disrupted} & 16.10 & 19.68 & 9.00 & 1.00 & 0.00 & 6.53 & 81.50 & 74.00 & 69.50 & 71.00 & 71.00 \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsection{Massive Values Contribute to Contextual Knowledge Understanding}
As shown in \autoref{tab:main_result}, 
\textit{Parametric Knowledge Retrieval tasks} still maintain \textit{relatively high accuracy} even when massive values are disrupted, demonstrating performance comparable to their original unmodified state and when non-massive values are disrupted. Cities tasks maintain relatively strong performance with scores achieving from 76\% to 88\%, showing a degradation of only 15-20\%. Sports and Art categories show moderate impact, with performance dropping to the 65-75\% range. Technology maintains similar resilience levels, with scores remaining above 70\%. The Celebrity category shows the strongest resilience, maintaining performance above 70\% across all models. Importantly, when \textit{only non-massive values} are disrupted, performance remains remarkably stable across all tasks and models, with variations typically less than $\pm$1\%. This striking contrast suggests that \textit{massive values play a crucial role specifically in \textbf{contextual knowledge understanding tasks}, while \textbf{parametric knowledge retrieval} tasks can maintain reasonable performance even when these values are disrupted.} For mathematical reasoning tasks, the impact is devastating: GSM8K accuracy drops dramatically (Gemma2-9B: 81.30\% to 15.10\%, Llama3-8B: 76.90\% to 4.00\%, Qwen2.5-7B: 86.60\% to 16.10\%), and AQUA-RAT shows similar degradation with performance declining by over 40 percentage points. Passkey Retrieval tasks collapse from 100\% to near-zero accuracy (0-2\%) across all difficulty levels, while IMDB sentiment analysis performance plummets from above 94\% to single digits across all models. 





\textbf{PPL and Diversity Score.}
In addition to accuracy, we evaluate perplexity (PPL) and diversity (2-gram diversity) as complementary metrics to understand the impact of massive values on model performance. \textit{These metrics reinforce our previous findings regarding the crucial role of massive values in contextual knowledge understanding tasks.} Perplexity, which measures a language model's prediction confidence (lower values indicating better language modeling ability), and diversity, quantified through 2-gram diversity scores to assess the richness and variety of model outputs, both demonstrate patterns consistent with our accuracy-based observations.

We analyzed the model's \textit{perplexity (PPL) and diversity scores} before and after disrupting massive values, with particular focus on Llama3-8B as shown in \autoref{fig:three_images1} (additional results can be found in \autoref{table:ppl}). The experimental results reveal striking patterns: When massive values are disrupted, the LLM shows severely degraded performance. Particularly in the IMDB dataset, we observe a dramatic increase in perplexity ($\sim$80) and a decrease in 2-gram diversity scores ($\sim$0.1) when massive values are disrupted, while vanilla and non-massive value disrupted conditions maintain low perplexity and high diversity ($\sim$0.9). \textit{These findings further confirm that massive values are crucial for contextual knowledge understanding tasks. }
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/diver.pdf}
        \label{fig:diver}
    \end{subfigure}
    \hfill
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/ppl.pdf}
        \label{fig:ppl}
    \end{subfigure}
    \hfill
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/destroy.pdf}
        \label{fig:des}
    \end{subfigure}
    \vspace{-10pt}
    \caption{When massive values are disrupted, it significantly reduces diversity and increases perplexity, especially in the IMDB dataset, suggesting poorer performance. Conversely, the vanilla model and the model when non-massive values are disrupted can still achieve lower perplexity and higher diversity.}
    \label{fig:three_images1}
    \vspace{-10pt}
\end{figure*}

\textbf{Ablation Study. }
We conduct our disruption of massive values and non-massive values specifically during the \textbf{prefilling stage}. Let $\mathbf{X} \in \mathbb{R}^{l \times h \times d}$ represent the query tensor and we replace the smallest values with calculated averages. Specifically, we identify the top n smallest values along the last dimension ($k$) and replace them with the mean value computed across the entire query tensor. For presentation clarity, we omit the batch dimension as these operations are applied consistently within each batch.

\begin{equation}
\mathbf{X}_{i;j;k^*} =\left\{
\begin{array}{lll}
\mathrm{Mean}(\mathbf{X})\;, & &  {  k^* = \text{Smallest}_{k} x_{i;j;k} }\\
\mathbf{X}_{i;j;k^*}\;, & & {k^* \neq \text{Smallest}_{k} x_{i;j;k} }
\end{array} \right.
\end{equation}

As shown in \autoref{fig:three_images1}, we systematically varied n from 1 to 20 (represented on the horizontal axis) in our control experiments. The results demonstrate that \textit{replacing these minimum values with averages has minimal impact on model performance across all three contextual knowledge understanding benchmarks}, with GSM8K, AQUA-RAT, and IMDB maintaining relatively stable performance throughout the range of n values tested (additional results in \autoref{app:llm}). This robustness to non-massive value manipulation provides compelling evidence that \textit{the model's contextual knowledge understanding capabilities \textbf{primarily rely on massive value regions}, while other regions in the Q and K play a less crucial role.}


\textbf{Disruption Methods Show Consistent Effects. }
Different approaches to disrupting massive values (using mean, zero, or minimum value substitution like mentioned in Section \ref{sec:pre_and_setup}) yield consistently similar effects across benchmarks. For instance, in GSM8k, all disruption methods reduce performance to single digits (mean: 4.00\%, zero: 1.60\%, minimum: 2.73\%), while non-massive value manipulations maintain performance close to the vanilla baseline (76-77\%). This pattern holds across all benchmarks, demonstrating that the \textit{impact of massive value disruption is robust to the variations of the disruption method}. Additional results across other models can be found in \autoref{app:llm}.

\vspace{-10pt}
\begin{table}[htbp]
\centering
\caption{Impacts of applying different methods to disrupt massive values/non-massive values on model performance (Llama3-8b). }
\adjustbox{max width=0.65\linewidth}{
\begin{tabular}{llcccccc}
\toprule
\multicolumn{2}{l}{\multirow{2}{*}{Disruption}} & \multirow{2}{*}{GSM8k} & \multirow{2}{*}{AQUA} & \multirow{2}{*}{IMDB} & \multicolumn{2}{c}{Passkey Retrieval} &                          \\ \cmidrule(l){6-7} 
\multicolumn{2}{l}{}                           &                        &                       &                       & (128,6)              & (256,12)                \\ \midrule
\multirow{1}{*}{None}  &   &    76.90                 &  53.51                    &     95.40                &       100           &  100                            \\ \midrule
\multirow{2}{*}{Mean}        & Massive Value Disrupted        &    4.00                 &  9.68                    &     11.00               &      9.00           &  0.00                             \\
& Non-Massive Value     & 77.40   & 53.90  & 95.40  & 100 & 100  \\ \midrule
\multirow{2}{*}{Zero}        & Massive Value Disrupted       &        1.60                &            8.07           &       13.40                &  8.00                    &             0.00         &                      \\
                             & Non-Massive Value Disrupted     & 76.40  & 53.83 & 94.70  & 100 & 100 \\ \midrule
\multirow{2}{*}{Minimum}     & Massive Value Disrupted       &   2.73                     &         9.33              &    10.80                   &                9.00      &       0.00                                 \\
                             & Non-Massive Value Disrupted    & 75.20   &  53.98 & 95.40  & 100 & 100   \\ \bottomrule
\end{tabular}
}
\end{table}




\subsection{Effects of Massive Values on Knowledge Conflict. }
% \vspace{3pt}

Our discovery of the differential impact of extreme values on contextual knowledge understanding versus parametric knowledge retrieval tasks motivated a novel experimental investigation. We designed an experiment to \textit{introduce conflicting contextual information} in factual datasets, particularly focusing on geographical knowledge from cities~\citep{marks2023geometry,longpre-etal-2021-entity}. For instance, we modified straightforward questions like "Is New York a city in the United States?" by adding contradictory context: "Geographical knowledge has changed, and New York has become a city in the United Kingdom. Is New York still a city in the United States?"


As shown in \autoref{fig:pda}, we tested this modified cities dataset across three model conditions. Interestingly, while the vanilla models (Llama3-8B and Qwen2.5-7B) achieve approximately 50\% accuracy on this binary classification task - equivalent to random guessing - the models with massive values disrupted demonstrate accuracy significantly above chance level. This \textit{counter-intuitive improvement suggests that when massive values are destroyed, the model loses its ability to process misleading contextual information and \textbf{instead defaults to its parametric knowledge}, effectively ignoring the contradictory context.} This observation aligns with previous findings~\citep{yu2023characterizing} that suggest distinct regions within neural networks are responsible for context understanding versus parametric knowledge storage. 


\begin{figure}[tb!]
  \centering
\includegraphics[width=0.35\textwidth]{figs/know.pdf}
  \vspace{-10pt}
  \caption{We can observe that introducing conflicting background knowledge causes LLM to be misled into making random guesses. However, \textbf{after massive values are disrupted, the model is still able to maintain a certain level of accuracy.}}
  \label{fig:pda}
  \vspace{-15pt}
\end{figure}

\subsection{The Effects of Massive Values on Quantization}

To further validate our findings regarding the importance of massive value in model behavior, we evaluated three well-established quantization methods: AWQ \citep{lin2023awq}, SmoothQuant \citep{xiao2023smoothquant}, and GPTQ \citep{frantar-gptq}. As shown in \autoref{fig:quantization_result}, these methods demonstrate distinctly different patterns across task types.

AWQ and SmoothQuant, which explicitly \textit{preserve massive values} during quantization, maintain strong performance across all tasks. AWQ achieves this by selectively protecting "important" weights during quantization, while SmoothQuant employs a smoothing factor ($S$) to redistribute massive values in activations through mathematically equivalent transformations. In contrast, GPTQ, which does \textit{not specifically protect massive values}, shows significant performance \textit{degradation on contextual knowledge understanding tasks}, particularly on GSM8K and AQUA (dropping to approximately 75\% normalized accuracy), while maintaining comparable performance on parametric knowledge retrieval tasks (Cities, Sports, Celebrity). 
\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.47\linewidth]{figs/model_performance_barchart.pdf}
    \vspace{-12pt}
    \caption{Impacts of different quantization methods on Llama3-8b across different benchmarks. }
    \label{fig:quantization_result}
    \vspace{-15pt}
    
\end{figure}
This performance disparity is particularly revealing: quantization methods that protect massive values maintain good and robust performance on contextual knowledge understanding tasks, while methods that do not protect these values struggle specifically with contextual knowledge understanding tasks. \textit{These results provide additional evidence for our hypothesis about the \textbf{crucial role of massive values in LLM's contextual knowledge understanding capabilities.}} 





Building upon existing research documenting the relationship between RoPE and stripe-like massive values embedding patterns~\citep{barbero2024round}, we continue investigating this phenomenon in \( Q \) and \( K \) embeddings. Based on several observations and analyses, we propose three hypotheses suggesting that these characteristic stripe-like massive values are an inherent consequence of RoPE's implementation.

\textbf{Evidence 1: } \textit{Analysis of High and Low-Frequency Regions in RoPE-based LLMs:} In large language models (LLMs), the Rotary Position Embedding (RoPE) implements a sophisticated dimensional pairing approach to encode positional information. The mechanism divides the embedding dimensionality ($d$) into ($\lceil \frac{d}{2} \rceil$) pairs, where each pair undergoes rotation at a unique frequency determined by its dimensional index.

\textit{Frequency Distribution:} For each dimensional pair ($j$), the rotation frequency is governed by the equation:
\begin{equation}
\theta_j = 10000^{-2j/d}
\end{equation}

This formulation creates an exponential decay in frequencies across dimensional pairs, \textit{resulting in a spectrum from high-frequency to low-frequency rotations.}

\textit{Dimensional Rotation:} Each pair of dimensions ( ($x_{2j}$, $x_{2j+1}$) ) undergoes rotation according to the matrix operation:
\begin{equation}
\begin{bmatrix} 
\cos(m\theta_j) & -\sin(m\theta_j) \\[0.3em]
\sin(m\theta_j) & \cos(m\theta_j)
\end{bmatrix}
\begin{bmatrix}
x_{2j} \\[0.3em]
x_{2j+1}
\end{bmatrix}
\end{equation}

where $m$ represents the position in the sequence.

\textit{Multi-Scale Position Sensitivity:} This frequency distribution serves two crucial purposes: \textit{High-frequency components (small $j$) enable fine-grained position discrimination at local scales}, as the rotation angle changes rapidly with position. Low-frequency components (large $j$) capture long-range positional relationships as their rotation angles change gradually across positions. The resulting position-encoded representations interact with the attention mechanism to produce position-aware similarity scores. It decreases from 0-($\lceil \frac{d}{2} \rceil$) as the dimensional rotation angle decreases, causing the massive values to concentrate at the bottom (i.e., low-frequency regions) due to less position information possessed. This concentration of massive values in low-frequency regions primarily encodes rich semantic content rather than positional information, as evidenced by experimental results showing that \textbf{disrupting these values severely impairs contextual understanding tasks} (with IMDB accuracy dropping from 94\% to single digits) \textbf{while preserving basic parametric knowledge retrieval} (showing only 15-20\% degradation). The dramatic increase in perplexity and decrease in output diversity when massive values are disrupted further confirms their crucial role in semantic comprehension rather than positional encoding.

\textbf{Evidence 2:} \textit{Concentrated Massive Value appears exclusively in the \( Q \) and \( K \) but not \( V \)}: RoPE applies position encoding selectively: it operates exclusively on \( K \) and \( Q \), but not on \( V \). As explained in \autoref{Preliminary}, this design is intentional—RoPE specifically incorporates positional information into the query $\mathbf{q}$ and key vectors $\mathbf{k}$, enabling their inner product $\mathbf{q}^T\mathbf{k}$ to inherently capture relative positional relationships. This selective application has a direct consequence: \textbf{\textit{the concentrated massive value appears exclusively in the \( Q \) and \( K \), while being completely absent in \( V \)}}, as demonstrated in \autoref{fig:777}. This pattern is consistently observed across various architectures—as shown in \autoref{tab:rope_outlier}, all LLMs implementing RoPE exhibit these concentrated massive values in the \( Q \) and \( K \), while \( V \) shows no discernible regular patterns.



\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/OPT_K_attention_map_layer_20.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/OPT_Q_attention_map_layer_20.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/OPT_V_attention_map_layer_20.pdf}
        \label{fig:image3}
    \end{subfigure}
 
    \vspace{-10pt}
    \caption{K, Q, V in Layer 20 of OPT-350M.}  
    \label{fig:777}
    \vspace{-10pt}
\end{figure*}








\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/GEMMA_K_attention_map_layer_20.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/GEMMA_Q_attention_map_layer_20.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{figs/GEMMA_V_attention_map_layer_20.pdf}
        \label{fig:image3}
    \end{subfigure}
 
    \vspace{-10pt}
    \caption{K, Q, and V at Layer 20 in Gemma2-9B. Massive values are notable in low-frequency regions of K and Q, absent in V.}  
    \label{fig:888}
    \vspace{-10pt}
\end{figure*}





\textbf{Evidence 3:} \textit{Concentrated Massive Value appears exclusively in the LLM with RoPE like Gemma}: To validate our hypothesis about the relationship between RoPE and massive values, we conducted a comparative analysis across different model architectures. First, we examined models without RoPE, including OPT~\cite{zhang2022opt} and GPT-NEO~\cite{gpt-neo}. As expected, their embedding vector maps showed no signs of organized massive values, with OPT-2.7B's attention maps appearing particularly disorganized and chaotic. In contrast, we observed consistent patterns of massive values in models employing RoPE, extending beyond traditional language models to multimodal architectures. For instance, both LLaVA-1.5~\cite{liu2024visual} and Qwen2-VL~\cite{Qwen2VL} exhibit similar massive value patterns (as shown in \autoref{fig:three_images_LLaVA-1.5} in Appendix). Notably, Qwen2-VL implements Multimodal Rotary Position Embedding (M-RoPE), which decomposes positional embedding to capture 1D, 2D, and 3D positional information yet maintains the characteristic massive value patterns. Further supporting our hypothesis, \textbf{\textit{models using alternative position encoding mechanisms}}, such as Jamba~\cite{lieber2024jamba} which combines Mamba and Transformer architectures without RoPE, \textbf{\textit{show no evidence of these massive value patterns.}} Detailed visualizations supporting these findings are provided in the appendix.


\section{Causal Mechanisms and Temporal Analysis of Concentrated Massive Values}
In this section, we analyze the rationale behind when and why concentrated massive values appear. 
 \begin{table}[h]
    \centering
    \small
 \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{RoPE} & \textbf{Concentrated} \\
    & & \textbf{Massive Values} \\ \midrule
    Llama 2, 3       & \checkmark    & \checkmark       \\
    Qwen 2, 2.5         & \checkmark    & \checkmark       \\
    Gemma 1, 2           & \checkmark    & \checkmark       \\
    GPT-NeoX          & \checkmark    & \checkmark       \\
    Phi-3       & \checkmark    & \checkmark       \\
    Falcon3      & \checkmark    & \checkmark       \\
    LLAVA               & \checkmark    & \checkmark       \\
    Qwen2-VL               & \checkmark    & \checkmark       \\
    Mistral-v0.3        & \checkmark    & \checkmark   \\
    GPT-2        & \texttimes    & \texttimes \\ 
    GPT-Neo        & \texttimes    & \texttimes \\ 
    OPT all size            & \texttimes    & \texttimes    \\ 
    Jamba               & \texttimes    & \texttimes      \\ \bottomrule
    \end{tabular}
    \caption{Analysis of different models about whether they adopt RoPE and whether massive value stripes can be observed in these models.}
    \label{tab:rope_outlier}
    \end{table}



\textbf{Evidence 4:} Since RoPE divides the dimensionality into pairs and applies similar rotational operations across these pairs, we hypothesized that the pattern of massive values should exhibit a mirrored structure across the two halves of the embedding dimensions. Indeed, our experimental results confirm this prediction: we observe two distinct clusters of massive values in the embedding vector, with one cluster appearing in the first half of the dimensions and a corresponding cluster in the second half, creating a symmetric pattern. This symmetrical distribution of massive values aligns perfectly with RoPE's dimensional pairing mechanism and provides additional evidence that these patterns are not random artifacts but rather emerge from the fundamental architectural design of positional encoding in transformer models.




\subsection{The Occurrence of Concentrated Massive Values}
Through extensive experiments and observations, we found that LLMs such as Llama, Gemma, and so on exhibit massive values in Q and K starting from the very first layer, which is different from massive attentions \citep{sun2024massive} that become significantly apparent only in the later layers. However, there are subtle differences in the patterns between each layer. The layer-wise visual illustrations can be found in \autoref{app:llm}. In each layer, when extracting the Q and K, we consistently observe that there are no significant differences between the results taken before and after applying RoPE. The patterns remain largely consistent across all layers. This indicates that the massive value is gradually formed through training rather than being solely caused by the addition of RoPE (since RoPE itself is not trainable).



 
\section{Related Work}
\subsection{LLM Quantizations and Massive Values in LLM}

Various existing works in quantization have studied the existence of massive value (also called outlier) features in LLMs~\citep{dettmers2022gpt3, ahmadian2023intriguing, guo2024activedormant}. Some research showed that massive value features have large activation values in most of their sequence dimensions. BERT often focuses attention on the "[SEP]" token~\citep{clark-etal-2019-bert, xiao2024efficient}, while LLMs predominantly assign attention to the starting word token~\citep{xiao2024efficient}, Some research focuses on the massive activation value in activation in LLM~\citep{sun2024massive} and identified attention artifacts in ViTs~\citep{darcet2024vision}. Someone trying to understand the mechanism of these massive values in transformer~\citep{guo2024activedormant}.

Research on LLM quantization has predominantly focused on analyzing model weights and activations through two main approaches: (1) W8A8 quantization, which converts both activations and weights to INT8 \citep{wei2022outlier, wei-etal-2023-outlier, xiao2023smoothquant, yao2022zeroquant, dettmers2022gpt3}, and (2) low-bit weight-only quantization (e.g., W4A16), which converts only weights to low-bit integers \citep{frantar-gptq, lin2023awq, sheng2023flexgen, park2024lut}. 
During quantization, some studies have identified massive values in activation values \citep{liukivi, sun2024massive} as a critical factor that can affect quantization. Several approaches have been developed to handle massive values specifically, either specific suppression techniques \citep{wei2022outlier, wei-etal-2023-outlier} or protection mechanisms \citep{lin2023awq, xiao2023smoothquant} to maintain massive values unchanged. Quantization methods specifically address massive values that are typically analyzed within individual attention heads. Our work examines massive values across the depth dimension of multiple attention heads, offering potential new directions for future quantization approaches.

\subsection{Rotary Positional Encoding}
Rotary Positional Encoding (RoPE) has been extensively studied in transformer architectures since its introduction by \citep{su2021roformer} as a relative position encoding method. RoPE has been widely adopted in various LLMs, including Llama \citep{touvron2023llama}, Palm \citep{anil2023palm}, Mistral \citep{jiang2023mistral}, and GLM \citep{du2022glm}. Recent research on long-context Transformers \citep{sun-etal-2023-length, xiong-etal-2024-effective, dubey2024llama} has demonstrated RoPE's effectiveness in input length extrapolation. These studies found that increasing the $\theta$ parameter in RoPE from its original value of 10,000 to larger values (e.g., 500,000) reduces attention to decay and enables more robust learning across extended contexts. These studies demonstrated that increasing RoPE's $\theta$ parameter from its original value of 10,000 to larger values (e.g., 500,000) mitigates attention decay and enhances robust learning over extended context windows. And Qwen2-VL adopts Multimodal Rotary Position Embedding (M-ROPE) that decomposes positional embedding into parts to capture 1D, 2D, and 3D positional information, enhancing its multimodal processing capabilities~\citep{Qwen2VL}. In parallel, research on NoPE (No Positional Encodings) has shown promising results in out-of-distribution (OOD) settings compared to RoPE, suggesting that the causal mechanism alone might sufficiently capture positional information without explicit position encoding \citep{haviv2022transformer, kazemnejad2024impact}. More recently, the analysis of Gemma-7b suggests an essential role of frequency components: the high-frequency part of the embedding vector in LLM encodes positional information, while the low-frequency part carries semantic information \citep{barbero2024round}. This work takes a different approach by investigating how RoPE might contribute to the formation of massive value in LLM of transformer architectures.

\section{Conclusion}

Our study provides novel insights into the role and origin of massive values in Large Language Models (LLMs). Through systematic investigation, we find that massive values play a critical role in contextual knowledge understanding tasks, such as passkey retrieval, IMDB sentiment understanding, and GSM8K reasoning. In contrast, their influence on parametric knowledge retrieval tasks, such as world knowledge retrieval, is limited. This finding emphasizes the importance of preserving massive value to maintain model performance in reasoning and context-dependent tasks. Our investigation reveals that RoPE induces massive value stripes, distinct patterns exclusively in the Q and K while absent in models without RoPE, such as GPT-2 and OPT. This highlights how positional encoding mechanisms contribute to massive values, particularly low-frequency channel dimensions, offering new insights into RoPE's role in modern LLMs. In this way, this study establishes a deeper understanding of massive values in LLMs, their critical role in contextual knowledge understanding, their implications for model optimization techniques such as quantization, and their connection to RoPE-induced patterns. These findings lay the foundation for developing more robust, efficient, and interpretable LLM architectures and optimization strategies.


\section{Impact Statements}

This work advances our understanding of Large Language Models by systematically investigating massive values in attention mechanisms and their relationship with contextual knowledge understanding. Our findings provide valuable insights for future model design and optimization, particularly in model quantization and parameter-efficient fine-tuning areas.
Our discoveries about the relationship between massive values and contextual knowledge understanding could inform more efficient model compression techniques, potentially reducing the computational resources required to deploy these models. This could help make language models more accessible while maintaining their core capabilities. While this technical advancement in understanding model internals has primarily positive implications for model efficiency and effectiveness, we acknowledge that improvements in LLM capabilities may have broader societal impacts that warrant ongoing discussion and careful consideration by the research community. Our work is focused on advancing the fundamental understanding of machine learning systems, particularly in transformer architectures and attention mechanisms. The ethical implications and societal consequences are in line with those generally associated with improvements in LLM understanding and optimization.

\subsubsection*{Acknowledgments}
We thank Taowen Wang, Fei Sun, Wenyue Hua for their valuable discussions and suggestions during
the project.

\bibliography{tmlr}
\bibliographystyle{tmlr}
\newpage
\appendix
\textbf{Appendix: }
This appendix contains additional details for the paper. The appendix is organized as follows:
% \vspace{-4em}

\begin{center}
\begin{itemize}

    \item Section reports \S\ref{app:synthesis} more about \textbf{Data Synthesis Pipeline}.
    \item Section reports \S\ref{app:more} more about \textbf{More visualization results for LLMs without RoPE}.
    \item Section reports \S\ref{app:more2} more about \textbf{More visualization results for LLMs with RoPE}.
    \item Section reports \S\ref{app:result} more about \textbf{Some Results when massive values are disrupted in LLMs.}.
    \item Section reports \S\ref{app:detail} more about \textbf{Experiment Details}.
    \item Section reports \S\ref{app:llm} more about \textbf{More Experiments to Support our Conclusion}.
    
    % \item Section \S\ref{app:agent_benchmarks} reports more \textbf{Details of Agent Benchmarks}.
    % \item Section \S\ref{app:addtion} shows more \textbf{Additional Experimental Results}.
    % \item Section \S\ref{app:discussion} analyzes \textbf{Discussion}.
\end{itemize}
\end{center}
\section{Data Synthesis Pipeline} \label{app:synthesis}
\subsection{Synthesizing parametric knowledge retrieval Dataset}
The pipeline for synthesizing parametric knowledge retrieval datasets as shown as below. Following existing benchmarks, we choose the commonly-used topics covering Sports, Arts, Technology and Celebrity to feed into the LLM to generate factual knowledge QA related to these topics~\citep{hu2023survey}. For the QA pair, the LLM is required to generate true/false following predefined formatting requirements like~\citep{zheng2023judging}. These generated questions are verified by another LLM instance regarding each question's structure, answer, and correctness, making improvements when necessary to meet quality requirements~\citep{zheng2023judging, chiang2023can}. To ensure the highest standards of data quality, a manual inspection stage is incorporated, where human annotators review and validate the generated questions, marking them as either acceptable or requiring revision. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/synthesis.pdf}
    \caption{\textbf{Pipeline of synthesizing parametric knowledge retrieval dataset: }The process starts with a factual knowledge taxonomy as input. An LLM generates true/false factual questions in a predefined format. Another LLM verifies and refines these questions for accuracy. A manual inspection ensures quality, producing a knowledge-intensive dataset for parametric knowledge retrieval tasks.}
    \label{fig:synthesis}
\end{figure}

% \noindent \textbf{LLM Verification. }

% \noindent \textbf{Manual Inspection. }

To ensure the quality of our synthesized question-answer pairs, we implement a two-stage verification pipeline. First, we leverage LLM (i.e., claude-3.5-sonnet) as an initial filter to automatically assess the factuality of the generated content across different categories (Sports, Arts, Technology, and Celebrity). This automated verification serves as an efficient preliminary screening mechanism. Subsequently, we conduct a rigorous manual inspection by human evaluators to further validate the filtered data by the LLM evaluator to avoid hallucinations. After the human evaluation, we sample 200 examples for each category to construct the final synthetic dataset. The verification statistics are shown in \autoref{tab:verification}, and some examples of the factual QA are shown in \autoref{tab:factual_qa_examples}. 


\vspace{-10pt}
\begin{table}[htbp]
\centering
\caption{Verification of whether the synthesized question and answer fulfills the factuality by both LLM and human. } \label{tab:verification}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Category} & \multicolumn{2}{c}{LLM Verification} & \multicolumn{2}{c}{Manual Inspection} \\ \cmidrule(r){2-3} \cmidrule(l){4-5} 
                          & Pass Rate        & Failed Case       & Pass Rate         & Failed Case         \\ \midrule
Sports                    &   100\%               &0\%                   &         92\%            & 8\%                      \\
Arts                      &    98\%               &         2\%          &    89\%                & 11\%                     \\
Technology                &  99\%                 &    1\%               &    90\%                & 10\%                    \\
Celebrity                 &  99\%                 &         1\%          &        90\%           &   10\%                   \\ \bottomrule
\end{tabular}
\end{table}
\vspace{10pt}
Following the two-stage pipeline, we need to use GPT4 and Human to evaluate and check the dataset, to verify whether our synthetic dataset is completely knowledge-based, without any context understanding~\citep{jin-etal-2024-impact, jin-etal-2025-exploring}. We observed a little difference in pass rates between the LLM and human inspections. As shown in \autoref{tab:verification}, while the LLM achieved near-perfect accuracy with a pass rate of 98-100\% across all categories, manual inspection revealed discrepancies, with pass rates ranging from 89\% to 92\%. This highlights the necessity of human evaluation to catch subtle errors, especially in nuanced categories like Arts and Celebrity, where subjective interpretations and context might introduce inaccuracies. The example in our dataset can be seen at \autoref{tab:factual_qa_examples}. These carefully curated question-answer pairs form a robust foundation for downstream tasks such as knowledge-intensive evaluations and parametric knowledge retrieval testing.


\begin{table}[htbp]
\small
    \centering
    \caption{Examples of parameter knowledge retrieval task: factual QA, covering Sports, Arts, Technology, and Celebrity. }
    \label{tab:factual_qa_examples}
    \begin{tabular}{p{0.15\textwidth}p{0.65\textwidth}c}
        \toprule
        \textbf{Category} & \textbf{Example} & \textbf{Ground Truth} \\
        \midrule
        \multirow{3}{*}{Sports} 
        & Is the Olympic Games held every four years? & Yes \\
        & Was Babe Ruth a famous football player? & No \\
        & Is the FIFA World Cup held every two years? & No \\
        \midrule
        \multirow{3}{*}{Arts}
        & Was the painting `Girl with a Pearl Earring' completed during the 18th century? & No \\
        & Is Pablo Picasso one of the founding figures of Cubism? & Yes \\
        & Was Diego Rivera a famous Mexican muralist? & Yes \\
        \midrule
        \multirow{3}{*}{Technology}
        & Is the ASCII character set limited to 256 characters? & No \\
        & Was the first iPhone released in 2007? & Yes \\
        & Is Linux an open-source operating system? & Yes \\
        \midrule
        \multirow{3}{*}{Celebrity}
        & Is Leonardo DiCaprio an Oscar-winning actor? & Yes \\
        & Was Taylor Swift born in Los Angeles? & No \\
        & Was Michael Jackson a member of The Beatles? & No \\
        \bottomrule
    \end{tabular}
    
\end{table}
\subsection{Synthesizing Passkey Retrieval Dataset}
The passkey retrieval task is the same as defined in Landmark Attention~\citep{mohtashami2023randomaccess}, a synthetic long context task. It requires a language model to retrieve a simple passkey (i.e., a 6-digit random number) from a long, meaningless text sequence. The passkey is placed at various document depths and context lengths~\citep{jin2024llm} (maybe ranging from 4k to 24k). The LLM’s performance on this is not sensitive to the prompt (the prompt is all garbled, so the LLM is not sensitive to these different prompts)~\citep{anthropic2023claude}. This is likely because the sentence carrying the passkey is significantly different from the surrounding repeated random text. Empirically, within the effective context window, almost all LLMs, including those without any instruction tuning or alignment, are able to locate the sentence carrying the passkey. 


Although this task is simple and far from real-world scenarios, it tests two fundamental capabilities of LLMs: The model should be able to identify and locate useful information at any position in the input sequence. The model should be able to utilize the perceived information to complete tasks. To summarize the two abilities, this task serves as a good in-context retrieval benchmark, as it does not require understanding the text content (not require contextual knowledge understanding ability), only locating the passkey retrieval. 


\begin{figure*}[ht]
    \centering
    \begin{tcolorbox}[
        title=\texttt{An example of Passkey Retrieval Task (12-256)},
        width=\textwidth % Makes the tcolorbox span the full page width
    ]
    \begin{flushleft}
    \textbf{Prompt:} There is an important info hidden inside a lot of irrelevant text. Find it and memorize it. I will quiz you about the important information there. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The pass key is 201724567512. Remember it. 201724567512 is the passkey.The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.

    \textbf{Ground Truth:} 201724567512
    \end{flushleft}
    \end{tcolorbox}
    \caption{An example in Passkey Retrieval Task, this is the harder case in this kind of task. The pass key length is 12 tokens, and the prompt length is about 256 tokens. The pass key is hidden in the middle of the prompt.}
    \label{fig:prompt_in_passkey_LLM2}
\end{figure*}

Therefore, we chose this as a benchmark. Additionally, we set up three hierarchical levels of tasks with 128 (as \autoref{fig:prompt_in_passkey_LLM}), 256 (as \autoref{fig:prompt_in_passkey_LLM2}), and 1024 tokens, ranging from simple to difficult. Experimental results show that for the 128-token level task if we eliminate all massive values, the model may not even be able to provide a response. In the task of retrieving passwords within just 128 tokens, LLMs perform extremely poorly, which is sufficient to demonstrate that they no longer possess in-context retrieval capabilities.


\begin{figure*}[htbp]
    \centering
    \begin{tcolorbox}[
        title=\texttt{An example of Passkey Retrieval Task (6-128)},
        width=\textwidth % Makes the tcolorbox span the full page width
    ]
    \begin{flushleft}
    \textbf{Prompt:} There is important info hidden inside a lot of irrelevant text. Find it and memorize it. I will quiz you about the important information there. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The pass key is 383816. Remember it. 383816 is the passkey.

    \textbf{Ground Truth:} 383816
    \end{flushleft}
    \end{tcolorbox}
    \caption{An example in Passkey Retrieval Task, this is the easier case in this kind of task. The pass key length is 6 tokens, and the prompt length is less than 128 tokens.}
    \label{fig:prompt_in_passkey_LLM}
\end{figure*}



\subsection{Inequality relation problems}
In this chapter, we detail the process of synthesizing a dataset comprising 100 inequality relation problems. These problems are designed to assess logical contextual knowledge understanding by presenting two premises involving inequalities and querying the relationship between two variables based on these premises. The dataset is structured to facilitate applications such as educational assessments, machine learning model training, and logical contextual knowledge understanding evaluations.

The synthesized dataset consists of 200 entries, each containing:

\begin{itemize}[leftmargin=*]
    \item \textbf{Premise 1}: An inequality between two variables (e.g., \( A > B \)).
    \item \textbf{Premise 2}: An inequality between the second variable of Premise 1 and a third variable (e.g., \( B > C \)).
    \item \textbf{Question}: A query that asks about the relationship between the first and third variables based on the given premises (e.g., \( A > B \), \( B > C \), what is the relation between \( A \) and \( C \)?).
    \item \textbf{Answer}: The inferred relationship between the first and third variables, which could be a definitive inequality or an indication that the relationship cannot be determined based on the given premises.
\end{itemize}

\begin{table}[h!]
\centering
\caption{Sample Inequality Relation Problems}
\scriptsize % Reduce font size for the table
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ccllc}
\toprule
\textbf{Premise 1} & \textbf{Premise 2} & \textbf{Question}                                                               & \textbf{Options}                                                                                                                          & \textbf{Correct Answer} \\ \midrule
(A $>$ B) & (B $>$ C) & (A $>$ B, B $>$ C), what is the relation between (A) and (C)? & \begin{tabular}[c]{@{}l@{}}1. (A $>$ C)\\ 2. Cannot determine the relation between (A) and (C)\\ 3. (A $<$ C)\end{tabular} & 1                       \\ \midrule
(D $<$ E)    & (E $<$ F)    & (D $<$ E, E $<$ F), what is the relation between (D) and (F)?       & \begin{tabular}[c]{@{}l@{}}1. (D $<$ F)\\ 2. Cannot determine the relation between (D) and (F)\\ 3. (D $>$ F)\end{tabular} & 1                       \\ \midrule
(G $>$ H) & (H $<$ I)    & (G $>$ H, H $<$ I), what is the relation between (G) and (I)?    & \begin{tabular}[c]{@{}l@{}}1. Cannot determine the relation between (G) and (I)\\ 2. (G $<$ I)\\ 3. (G $>$ I)\end{tabular} & 1                       \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\begin{figure*}[htbp]
    \centering
    \begin{tcolorbox}[
        title=\texttt{Prompt in LLM to save Embedding Vector},
        width=1.0\textwidth % Makes the tcolorbox span the full page width
    ]
    \begin{flushleft}
A car is being driven, in a straight line and at a uniform speed, towards the base of a vertical tower. The top of the tower is observed from the car and, in the process, it takes 10 minutes for the angle of elevation to change from 45° to 60°. After how much more time will this car reach the base of the tower?
    \end{flushleft}
    \end{tcolorbox}
    \caption{Prompt in Inference LLM}
    \label{fig:prompt_in_inference_LLM}
\end{figure*}

\section{More visualization results for LLMs without RoPE.} \label{app:more}


\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/OPT_K_attention_map_layer_10.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/OPT_Q_attention_map_layer_10.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/OPT_V_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
   
    \caption{Embedding Vector of K Q, V in OPT-350M, we choose Layer 10 and the input question is shown as Figure \ref{fig:prompt_in_inference_LLM}}. 
    \label{fig:three_images_opt}
    \vspace{-5pt}
\end{figure*}




\textbf{Open Pretrained Transformers (OPT)} is a complete suite of large language models based on the Transformer Decoder, serving as a replica of the largest GPT-3 model with 175 billion parameters~\citep{zhang2022opt}. OPT ranges in size from 125 million to 175 billion parameters, matching the scale of GPT-3, and its code has been fully open-sourced. In the structure of OPT, we can see that they did not apply RoPE (Rotary Positional Encoding) in Q and K in the transformer. The decoder in the OPTForCausalLM model is a critical component of its architecture, designed to process sequences in a manner conducive to causal language modeling. It consists of several layers, specifically 24 identical OPTDecoderLayer modules, reflecting a deep transformer architecture.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{appendix_fig/OPT_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of OPT-350M}
    \label{fig:opt_d}
\end{figure}


We tested two LLMs without RoPE, OPT-2.7B and OPT-350M with the prompt~\autoref{fig:prompt_in_inference_LLM}, and visualized the 20th layer for each model as~\autoref{fig:three_images_opt} . We observed a series of OPT models~\citep{zhang2022opt}, which also do not use RoPE. As shown in the figure, they also do not exhibit Massive Values formed by extreme values. These results indicate that the absence of RoPE in the OPT models contributes to a more uniform distribution of attention weights, as seen in the Q, K, and V. Unlike models that use RoPE, which tend to exhibit massive values from concentration, the OPT models demonstrate smoother and more evenly distributed attention scores across all dimensions and heads. 

The uniform distribution of value in each token in OPT-350M, 10 layers, with 0,3,7,11 heads, implies that OPT architecture promotes a more balanced handling of input tokens like \autoref{fig:opt_d}. This is particularly evident in the absence of concentrated massive values that typically emerge in LLMs using RoPE, where attention often spikes in specific dimensions like \autoref{fig:llama2_key}.

\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Jamba_K_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Jamba_Q_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Jamba_V_attention_map_layer_10.pdf}
    \end{subfigure}
    \caption{Embedding Vector of K Q, V in Jamba, we choose Layer 10, and the input question is shown as Figure \ref{fig:prompt_in_inference_LLM}}
    \label{fig:jamba}
\end{figure*}

\textbf{Jamba} introduces a new hybrid Transformer-Mamba mixture-of-experts (MoE) architecture~\citep{lieber2024jamba}. The Jamba architecture incorporates Jamba blocks, which combine Mamba layers~\citep{gu2024mamba} and Transformer modules with MoE layers in between. Jamba uses eight layers of Jamba blocks, with an Attention-to-Mamba ratio of 1:7, and includes MoE layers in between. Jamba's positional encoding does not use ROPE (Rotary Positional Embedding). Therefore, in the appendix, we study the attention queries (Q) and keys (K) components of the Transformer modules within Jamba. Our study reveals that Q and K have only two heads, with no concentrated massive values like ~\autoref{fig:jamba}, which further supports our hypothesis. 




\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{appendix_fig/Jamba_layer_10_Key_Value.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of Jamba-350M}
    \label{fig:synthesis}
\end{figure}



\section{More visualization results for LLMs with RoPE} \label{app:more2}



\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama_K_attention_map_layer_20.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama_Q_attention_map_layer_20.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama_V_attention_map_layer_20.pdf}
        \label{fig:image3}
    \end{subfigure}
    
    \caption{Embedding Vector of Q, K and V in meta-Llama-2-7b-chat-hf, we choose Layer 20, and the input question is shown as  \autoref{fig:prompt_in_inference_LLM}}. 
    \label{fig:three_images_llama}
    \vspace{-5pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama3_K_attention_map_layer_10.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama3_Q_attention_map_layer_10.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Llama3_V_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \caption{Embedding Vector of Q, K and V in Meta-Llama-3-8B-Instruct, we choose Layer 10, and the input question is shown as \autoref{fig:prompt_in_inference_LLM}}. 
    \label{fig:three_images_llama3}
    \vspace{-5pt}
\end{figure*}

\textbf{Llama: } Llama~\citep{dubey2024llama, touvron2023llama} is a cutting-edge transformer model designed with several unique structural features that optimize its performance for language modeling tasks. Below are the key characteristics of LLaMA's architecture: Pre-applied RMSNorm~\citep{jiang2024pre}:\[
    \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2 + \epsilon}}
    \]
RoPE (Rotary Position Embedding)~\citep{su2021roformer} applied to $Q$ and $K$. A causal mask is used to ensure that each position can only attend to previous tokens. LLaMA concatenates earlier $K$ and $V$ values to the current $K$ and $V$ values, allowing $Q$ to retrieve earlier information:\[K' = [K_{\text{prev}},K_{\text{curr}}], \quad
 V' = [V_{\text{prev}}, V_{\text{curr}}] \]
 MLP formulation:
    \[
    \text{MLP}(x) = \text{down}(\text{up}(x) \times \text{SiLU}(\text{gate}(x)))
    \]
    where $\text{down}$, $\text{up}$, and $\text{gate}$ are all linear layers.

  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{appendix_fig/Llama2_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 20 layers of Llama2-7B}
    \label{fig:llama2_key}
\end{figure}





\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{appendix_fig/Llama3_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of Llama3-8B}
    \label{fig:llama3_key}
\end{figure}

Through observations of LLaMA's K Q V map as \autoref{fig:three_images_llama3} and \autoref{fig:three_images_llama}, I noticed a distinct bright line in the high-frequency regions of the Q and K matrices. This phenomenon indicates a consistent pattern of attention concentration along certain dimensions across multiple heads. Such bright lines suggest that these dimensions are heavily weighted during the attention computation process, potentially playing a key role in determining the model's focus on specific tokens or features.

Both LLaMA2-7B and LLaMA3-8B exhibit extreme values in Key and Query about all heads as \autoref{fig:llama3_key} and  \autoref{fig:llama2_key}, but their distribution patterns differ across heads and layers. LLaMA3-8B seems to have smoother and more systematic patterns in certain heads compared to LLaMA2-7B.








\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{appendix_fig/LLAVA_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of LLaVA-V1.5}
    \label{fig:qwen2.5_key}
\end{figure}


\begin{figure*}[htbp]
    \centering
    \begin{tcolorbox}[
        title=\texttt{Prompt in LLaVA to save vector},
        width=0.8\textwidth % Makes the tcolorbox span the full page width
    ]
    \begin{flushleft}
What are the key aspects or factors I should be aware of, and what precautions or considerations should I take to ensure success or avoid potential pitfalls in this image? 
    \end{flushleft}
    \end{tcolorbox}
    \caption{Prompt in Inference LLM}
    \label{fig:prompt_in_inference_mllm}
\end{figure*}

\textbf{LLaVA: }LLaVA (Large Language and Vision Assistant) is a multimodal large model that combines visual and language capabilities, focusing on tasks that involve processing both language and visual inputs~\citep{liu2024visual}. Its goal is to extend the abilities of traditional Large Language Models (LLMs) to understand and generate language descriptions related to visual content. LLaVA is typically built upon architectures like LLaMA~\citep{dubey2024llama,touvron2023llama} and integrates with vision perception models (e.g., CLIP~\citep{radford2021learning}) to create a system with multimodal capabilities. Through structural analysis, we can identify that LLaVA utilizes RoPE in two places. Therefore, we hypothesize that its KV would contain Massive Values composed of extreme values. We found this true upon verification like Figure \ref{fig:three_images_LLaVA-1.5}.

Through observations of LLaVA’s K, Q, and V maps, as shown in Figure \ref{fig:three_images_LLaVA-1.5}, I noticed similar patterns to those observed in Llama, particularly in the Q and K matrices. Specifically, there is a consistent bright line in the high-frequency regions of the Q and K embedding vectors. This phenomenon suggests that LLaVA, like Llama, exhibits a pattern of attention concentration along specific dimensions across multiple heads.

\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/LLAVA_K_attention_map_layer_10.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/LLAVA_Q_attention_map_layer_10.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/LLAVA_V_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \caption{Embedding Vector of K, Q, V in LLaVA-1.5, we choose Layer 10, and the input question is shown as \autoref{fig:prompt_in_inference_LLM}.}. 
    \label{fig:three_images_LLaVA-1.5}
    \vspace{-5pt}
\end{figure*}


\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{LLaVA Output:} The image shows a diagram with several boxes and arrows, which represent different things. The boxes are labeled with various terms, such as "add norm," "add \& norm," "forward," "add norm," "add \& norm," "multi head," "masked attention," "add norm," "add \& norm," "position encoding," "output encoding," and "embedding." The arrows connect these boxes, indicating relationships between them. When analyzing this diagram, it is essential to be cautious about the context and purpose of the diagram. It could be a visual representation of a complex system, a network, or a process. Understanding the relationships between the boxes and arrows will help you understand the overall structure and function of the system. Additionally, paying attention to the labels and their meanings will help you interpret the diagram more accurately.
    \end{minipage}
}
\caption{output in Inference LLaVA}
\label{fig:passkey_retrieval}
\label{fig:prompt_in_LLaVA}
\end{figure}



\textbf{Qwen2-VL: } Qwen2-VL~\citep{Qwen2VL} is the latest enhancement of the Qwen-VL model, marks a significant leap forward in multimodal AI, offering state-of-the-art visual understanding across diverse resolutions and ratios and excelling in benchmarks. For Qwen2-VL, we have adopted a three-stage training methodology similar to LLaVA~\citep{liu2024visual}. In the first stage, we focus on training the Vision Transformer (ViT)~\citep{dosovitskiy2021thomas} component, utilizing a large corpus of image-text pairs to enhance semantic understanding. In the second stage, we unfreeze all parameters and train with a broader range of data for more comprehensive learning, and the backbone LLM is Qwen2 series models~\citep{qwen2}. In the final stage, we lock the ViT parameters and exclusively fine-tune the LLM using instructional datasets.


\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_K_Vision_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_Q_Vision_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_V_Vision_attention_map_layer_10.pdf}
    \end{subfigure}
    \caption{Embedding Vector of K, Q, V in ViT in Qwen2-VL 7B, we choose Layer 10}
    \label{app:qwen2vlvit}
    \vspace{-5pt}
\end{figure*}


Qwen2-VL uniquely incorporates Multimodal Rotary Position Embedding (M-RoPE), which differs from the traditional one-dimensional Rotary Position Embedding~\citep{su2021roformer} (1D-RoPE) used in LLMs, the latter being limited to encoding one-dimensional positional information. M-RoPE effectively simulates the positional information of multimodal inputs by decomposing the original rotary embedding into three components: time, height, and width. For text inputs, these components use identical position IDs, making M-RoPE functionally equivalent to 1D-RoPE. In processing images, the temporal IDs of each visual token remain constant, while distinct IDs are assigned to the height and width components based on the token's position in the image. For videos, which are treated as sequences of frames, the temporal ID increments with each frame, and the height and width components follow the same ID assignment pattern as images. This means that RoPE is used in both VIT and subsequent LLMs. Therefore, the phenomenon of extreme values should be observable both in VIT and Qwen model. However, due to the Multimodal Rotary Position Embedding (M-RoPE), which differs from the traditional one-dimensional Rotary Position Embedding (1D-RoPE) used in LLMs, the patterns on the KQ Embedding Vector in VIT differ slightly from other Embedding Vectors, but the phenomenon still exists like \autoref{app:qwen2vlvit}.



\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_K_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_Q_attention_map_layer_10.pdf}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen2VL_V_attention_map_layer_10.pdf}
    \end{subfigure}
    \caption{Embedding Vector of K, Q, V in large language model in Qwen2-VL 7B, we choose Layer 10}
    \label{app:qwen2vl}
    \vspace{-5pt}
\end{figure*}



\textbf{Qwen and Mistral: }Qwen and Mistral both use RoPE. Through observations of Qwen2.5-7B’s K, Q, and V maps as shown in \autoref{fig:three_images_qwen}, I noticed a similar phenomenon where distinct bright regions appear in the high-frequency areas of the Q and K matrices. In particular, the Key embeddings across different heads show localized peaks with varying intensities, as evident in the 3D visualizations of \autoref{fig:qwen2.5_key}. While certain heads, such as Head 0 and Head 3, display pronounced peaks in specific token ranges.









\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen_K_attention_map_layer_10.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen_Q_attention_map_layer_10.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Qwen_V_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \caption{Embedding Vector of K, Q, V in Qwen2.5-7B, we choose Layer 10, and the input question is shown as \autoref{fig:prompt_in_inference_LLM}}. 
    \label{fig:three_images_qwen}
    \vspace{-5pt}
\end{figure*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_fig/Qwen_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of Qwen2.5-7B}
    \label{fig:qwen2.5_key}
\end{figure}




\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Mistral_K_attention_map_layer_10.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Mistral_Q_attention_map_layer_10.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.3\textwidth]{appendix_fig/Mistral_V_attention_map_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \caption{K, Q and V in Mistral-7B-Instruct-v0.3, we choose Layer 10, and the input question is shown as \autoref{fig:prompt_in_inference_LLM}. }. 
    \label{fig:three_images_Mistral-7B-Instruct-v0.3}
    \vspace{-5pt}
\end{figure*}




\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{appendix_fig/Mistral_layer_10_KEY.pdf}
    \caption{3D Figure of Key Value in different Head in 10 layers of Mistral}
    \label{fig:qwen2.5_key}
\end{figure}

\section{Some Results when Destroying the extreme values of LLMs} \label{app:result}

\subsection{Perplexity Result}



\begin{table}[htbp]
\centering
\caption{Perplexity changes when Massive Values are destroyed and non-Massive Values are destroyed. }
\label{table:ppl}
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lccc}
\toprule
Model           & AQUA-RAT & IMDB & GSM8K \\ \midrule
\textbf{\textit{Llama3-9B}} &  3.21    &  8.38    & 4.17      \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}           &  3.30    &  8.94    & 4.08      \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           & 10.34   &  112.61    &    8.35  \\ \midrule
\textbf{\textit{Gemma2-9B}}  & 4.83   &   14.39   &   6.09    \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}        & 5.22     &  12.66   &   6.14    \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           &   13.93   & 48.06    &    16.78 \\ \midrule
\textbf{\textit{Qwen2.5-7b}} & 2.95   &   8.45 &    1.98  \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}           &  3.12    & 8.64  &  2.21     \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           &   9.53   &   14.71   &    8.62   \\ \bottomrule
\end{tabular}
}
\end{table}



\textbf{Definition of Perplexity}

Perplexity is a common metric used to evaluate the quality of language models. It measures how well a probabilistic model predicts a sequence of text. A lower perplexity indicates better predictions by the model.


In mathematical terms, perplexity is defined as:

\begin{itemize}
    \item \textbf{Input Sequence:} A tokenized sequence \( \mathbf{X} \in \mathbb{R}^{1 \times N} \), where \( N \) is the number of tokens in the input sequence. Each token corresponds to an element \( w_i \) in the vocabulary.
    \item \textbf{Model Predictions:} The model predicts a probability distribution \( P(w_i \mid w_1, w_2, \dots, w_{i-1}) \) over the vocabulary for each token \( w_i \), resulting in a tensor \( \mathbf{P} \in \mathbb{R}^{N \times V} \), where \( V \) is the size of the vocabulary.
\end{itemize}

\textbf{Perplexity Computation}
\begin{enumerate}
    \item \textbf{Log-Probability Computation:} For each token \( w_i \), the log-probability \( \log P(w_i \mid w_1, w_2, \dots, w_{i-1}) \) is extracted from the predicted probability tensor \( \mathbf{P} \) by selecting the value corresponding to the actual token \( w_i \)'s index in the vocabulary.
    \item \textbf{Averaging Log-Probabilities:} Compute the average log-probability over the entire sequence:
    \[
    \frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, w_2, \dots, w_{i-1})
    \]
    \item \textbf{Exponential Transformation:} Take the negative exponential to convert the average log-probability into perplexity:
    \[
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1, w_2, \dots, w_{i-1})\right)
\]

\end{enumerate}

The table presents perplexity (PPL) changes across three datasets (AQUA, IMDB, GSM8K) for three models (Llama3-9B, Gemma2-9B, and Qwen2.5-7B) under three conditions as \autoref{table:ppl}. The perplexity is low in the original (Vanilla) setting, indicating that the models effectively handle these datasets under normal circumstances. For instance, Llama3-9B achieves PPL values of 3.21, 8.38, and 4.17 for AQUA, IMDB, and GSM8K, respectively.


When non-Massive Values are destroyed, perplexity increases slightly across all models and datasets, indicating that their removal has a limited impact on language modeling capabilities. For instance, Llama3-8B's PPL increases from 3.21 to 3.30 on AQUA and from 8.38 to 8.94 on IMDB, while Qwen2.5-7B experiences a marginal rise from 2.95 to 3.12 on AQUA. These results suggest that non-Massive Values represent less critical features in the data, and their disruption does not significantly affect model performance. In contrast, the destruction of Massive Values causes a dramatic increase in perplexity, severely degrading the models' language modeling abilities. For example, Llama3-8B's PPL jumps from 3.21 to 10.34 on AQUA and from 8.38 to 112.61 on IMDB, while Gemma2-9B sees an increase from 4.83 to 13.93 on AQUA and from 14.39 to 48.06 on IMDB. These results highlight the critical role of Massive Values in enabling models to accurately predict tokens, as they likely encode key semantic or syntactic information.

Overall, this experiment underscores the importance of preserving Massive Values in language models to maintain their ability to handle various tasks effectively. While models are relatively robust to the removal of non-Massive Values, Massive Values are integral to their performance. Their destruction fundamentally disrupts the models’ ability to model language effectively, as evidenced by the significant increase in perplexity.









\subsection{Diversity Result}
\textbf{Definition of Diversity: }
Diversity is a metric used to measure the variety in vocabulary and sentence structures in generated texts, often applied to evaluate the quality of outputs in text generation tasks~\cite{tevet-berant-2021-evaluating}. A text with high diversity typically contains a broader range of vocabulary and sentence patterns, reducing repetition and improving naturalness. In natural language processing, diversity is commonly evaluated by calculating the proportion of unique \textit{n}-grams in the generated texts. \textit{n}-grams refer to consecutive sequences of $n$ words or characters in a text, used to capture local structures within the text.

The calculation of diversity can be represented by the following formula.


Let:
\begin{itemize}
    \item $N$: The total number of generated texts (e.g., the number of sentences generated).
    \item $T_i$: The $i$-th generated text.
    \item $\text{total\_n\_grams}$: The total number of $n$-grams across all generated texts.
    \item $\text{unique\_n\_grams}$: The total number of unique (non-repeated) $n$-grams across all generated texts.
\end{itemize}

The diversity is then calculated as:
\[
\text{Diversity (n-gram)} = \frac{\text{unique\_n\_grams}}{\text{total\_n\_grams}}
\]


\textbf{Proportion of Unique \textit{n}-grams}
\begin{itemize}
    \item This is the core part of the metric, measuring the fraction of $n$-grams in the generated texts that are unique.
    \item A higher ratio indicates lower repetition and higher diversity in the generated text.
\end{itemize}

\subsection*{Role of \textit{n}-grams}
\begin{itemize}
    \item For $n=1$ (unigram): Captures diversity at the word level (lexical diversity).
    \item For $n=2$ or higher: Captures diversity at the phrase or sentence level (structural diversity).
\end{itemize}



























\begin{table}[htbp]
\centering
\caption{Diversity changes when massive values are disrupted and non-massive values are disrupted. }
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lccc}
\toprule
Model           & AQUA-RAT & IMDB & GSM8K \\ \midrule
\textbf{\textit{LLAMA3-9B}} & 0.755    &  0.921  &   0.776 \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}        &    0.763 &   0.912     &     0.775  \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           & 0.446  &   0.668   &  0.489     \\ \midrule
\textbf{\textit{Gemma2-9B}} &  0.695  &  0.809  &    0.652   \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}         & 0.757     &   0.863 &  0.666  \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           & 0.675    &   0.085   &    0.720   \\ \midrule
\textbf{\textit{Qwen2.5-7b}} & 0.663   & 0.683    &   0.758  \\
\hspace{0.5em}\texttt{+ Non-Massive Value Disrupted}         &  0.675   &    0.699 &    0.747  \\
\hspace{0.5em}\texttt{+ Massive Value Disrupted}           &  0.561    &  0.546   &  0.633    \\ \bottomrule
\end{tabular}
}
\end{table}
We adopted 2-grams to calculate diversity, which has the advantage of capturing structural diversity at the phrase or sentence level. By analyzing 2-grams, we can measure not just the lexical variation of individual words but also how words are combined to form meaningful expressions. This approach provides a more comprehensive assessment of the model’s ability to generate diverse and coherent outputs. Additionally, using 2-grams helps to detect subtle changes in diversity when certain components, such as massive values or non-massive values, are removed. For example, as shown in Table 8, disrupting massive values significantly reduces diversity, especially in tasks like AQUA-RAT and GSM8K, indicating that Massive Values play a critical role in maintaining the richness of generated content. Conversely, destroying non-Massive Values has a relatively minor impact on diversity, suggesting that the structural patterns encoded by Massive Values are essential for maintaining variation in model outputs.


\subsection{Output in destroyed LLM}
In the previous two chapters, we observed that on the IMDB dataset, the content generated by LLMs tends to deviate significantly from normal values. We selected various examples generated by different LLMs (Llama, Gemma, Qwen) to illustrate this phenomenon. The primary goal is to demonstrate that after Massive Value destruction, the diversity of content generated by LLMs drops significantly, perplexity increases drastically, and the resulting content often falls into a range that is incomprehensible to humans.
\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Gemma2-9B Output:} It seems like you're exploring the idea of making a movie! It's awesome that you're thinking about filmmaking. It sounds like you're playing with words and ideas, which is a great part of the creative process. If you want to talk more about your movie idea, I'm here to listen and help brainstorm.  What kind of movie are you imagining? \\
        \textbf{PPL: }9.125
    \end{minipage}
}
\caption{Result in Inference Gemma}
\end{figure}

\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Llama Output:} Answer: s and Playboy bunnies in your films! Bob Fosse's "Star 80" about Dorothy Stratten, of whom Bogdanovich was obsessed enough to have married her SISTER after her murder at the hands of her low-life husband, is a zillion times more interesting than Dorothy herself on the silver screen. Patty Hansen is no actress either..I expected to see some sort of lost masterpiece a la Orson Welles but instead got Audrey Hepburn cavorting in jeans and a god-awful "poodlesque" hair-do....Very disappointing...."Paper Moon" and "The Last Picture Show" I could watch again and again. This clunker I could barely sit through once. This movie was reputedly not released because of the brouhaha surrounding Ms. Stratten's tawdry death; I think the real reason was because it was so bad! Judge the statement Negative or PositiveYou should explain the reason for your answer.\\
        \textbf{PPL: }356.0
    \end{minipage}
}
\caption{Result in Inference Gemma}
\end{figure}

\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Llama Output:} I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me. \\
        \textbf{PPL: }41.1904411315918
    \end{minipage}
}
\caption{Result in Inference Llama3-8B}
\end{figure}



\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Llama Output:} I can! This movie is not for the typical film snob, unless you want to brush up on your typical cinematic definitions, like "continuity editing" and "geographic match". I couldn't tell where I was in this movie. One second they're in the present, next minute their supposedly in the 70's driving a modern SUV and wearing what looked like to me as 80's style clothing. I think it only got attention because it has a 3d which I did not watch. If you're a b-movie buff, and by b-movie I mean BAD movie, then this film is for you. It's home-movie and all non-sense style will keep you laughing for as long as you can stay awake. If your tastes are more for Goddard and Antonioni, though, just skip this one. \\
        \textbf{PPL: }42.127960205078125
    \end{minipage}
}
\caption{Result in Inference Llama3-8B}
\end{figure}

\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Llama Output:} I am a helpful movie critic who can help me. Answer 'Negative' or 'Positive' I should explain the reason for my answer. \\
        \textbf{PPL: }72.02507019042969
    \end{minipage}
}
\caption{Result in Inference Llama3-8B}
\end{figure}


\begin{figure}[h!]
\centering
\fbox{
    \begin{minipage}{0.9\textwidth}
        \textbf{Qwen Output:} Answer:  to continue the story, we could start with a scene where Lena, the main character, is at a film festival, perhaps in Stockholm, discussing the themes and techniques of Ingmar Bergman's films with other film enthusiasts. This could lead into a broader discussion about the intersection of art, politics, and social commentary in Swedish cinema, setting up the backdrop for Lena's exploration of these themes in her own work.\\
        \textbf{PPL: }21.375
    \end{minipage}
}
\caption{Result in Inference Qwen}
\end{figure}



\section{Experiment Details} \label{app:detail}
\textbf{Large Language Models: }
For the main table as \autoref{tab:main_result} in the body text, we used three classic models with RoPE: Llama-3-8B-Instruct~\cite{dubey2024llama}, google/gemma-2-9b-it~\cite{team2024gemma}, and Qwen2.5-7B-Instruct~\cite{qwen2}. In the appendix, we further supplemented the experiments with models including opt-2.7b~\cite{zhang2022opt}, Qwen2-VL-2B-Instruct~\cite{Qwen2VL}, Llama-2-7b-chat-hf~\cite{touvron2023llama}, Jamba-v0.1~\cite{lieber2024jamba}, and Mistral-7B-Instruct-v0.3~\cite{jiang2023mistral}. Among these, some models have RoPE while others do not. During inference, we consistently used flash\_attention\_2~\cite{dao2022flashattention} for faster inference speeds.

\textbf{Save Embedding Vector: }
We save Embedding Vector after RoPE.

\textbf{Datasets: }

\noindent \textbf{Cities} \cite{marks2023geometry}:  consists of statements about the location of cities and their veracity labels (e.g., The city of Zagreb is in Japan, which is wrong). We use 1496 of these samples.

\noindent \texttt{``True'' example:}
\noindent \texttt{Judge the statement is True or False.} \texttt{\color{blue} The city of Tokyo is in Japan.}

\vspace{0.06cm}

\noindent \texttt{``False'' example:}
\noindent \texttt{Judge the statement is True or False.} \texttt{\color{blue} The city of Lodz is in the Dominican Republic.}

\noindent \textbf{GSM8K} \cite{cobbe2021training}:  
GSM8K (Grade School Math 8K) is a dataset consisting of 8.5K high-quality, linguistically diverse grade school math word problems. The dataset is designed to support question answering tasks for basic mathematical problems that require multi-step reasoning. These problems typically require 2 to 8 steps to solve. The solutions primarily involve performing a sequence of basic arithmetic operations to arrive at the final answer. A bright middle school student should be able to solve all the problems. The paper states: "These problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable. In our experiments, we used the first 1,000 samples from the training set of GSM8K and we use \autoref{fig:prompt_math} as the system prompt.


\begin{figure*}[htbp]
    \centering
    \begin{tcolorbox}[
        title=\texttt{System Prompt in LLM to solve Math problem},
        width=0.95\textwidth 
    ]
    \begin{flushleft}
You are a helpful math expert who can help me. Put the final option and answer at the end of the sentence. Do not show other incorrect options.
    \end{flushleft}
    \end{tcolorbox}
    \caption{Prompt in Inference LLM}
    \label{fig:prompt_math}
\end{figure*}

\noindent \texttt{GSM8K Example:}
\texttt{\color{blue} The ratio of coins that Elsa has to that which Amalie has is 10:45. If the total number of coins they have is 440, and Amalie spends 3/4 of what she has on toys, how many will she remain with?}

\noindent \texttt{GSM8K Example:} \texttt{\color{blue} Carly collected 7 starfish with 5 arms each and one seastar with 14 arms. How many arms do the animals she collected have in total?}


\noindent \texttt{GSM8K Example:} \texttt{\color{blue} Tim has 30 less apples than Martha, and Harry has half as many apples as Tim. If Martha has 68 apples, how many apples does Harry have?}

\noindent \texttt{GSM8K Example:} \texttt{\color{blue} Nancy is filling an aquarium for her fish. She fills it halfway and goes to answer the door. While she's gone, her cat knocks the aquarium over and spills half the water in it. Then Nancy comes back and triples the amount of water in the aquarium. If the aquarium is 4 feet long, 6 feet wide, and 3 feet high, how many cubic feet of water are in the aquarium?}


\noindent \textbf{AQUA-RAT} \citep{ling-etal-2017-program}: 
Dataset Card for AQUA: AQUA is a large-scale dataset consisting of approximately 100,000 algebraic word problems. Each question is accompanied by a step-by-step solution explained in natural language. This dataset is designed to train program generation models that can both generate explanations and create programs to solve the given questions. We treat AQUA the same way as GSM8K in our experiments. Below is an example.

\noindent \texttt{AQUA-RAT Example 1:} \\
\noindent \texttt{Question:} \texttt{\color{blue} Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15\% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?}\\
\noindent \texttt{Options:} \texttt{\color{blue} [ "A)21", "B)21.5", "C)22", "D)22.5", "E)23" ]}\\
\noindent \texttt{Rationale:} \texttt{\color{blue} If Q complete x kilometers, then P completes 1.15x kilometers. x + 1.15x = 43 2.15x=43 x = 43/2.15 = 20 Then P will have have walked 1.15*20=23 km. The answer is E.}



\noindent \texttt{AQUA-RAT Example 2:} \\
\noindent \texttt{Question:} \texttt{\color{blue} In the coordinate plane, points (x, 1) and (5, y) are on line k. If line k passes through the origin and has slope 1/5, then what are the values of x and y respectively?}\\
\noindent \texttt{Options:} \texttt{\color{blue} [ "A)4 and 1", "B)1 and 5", "C)5 and 1", "D)3 and 5", "E)5 and 3" ]}\\
\noindent \texttt{Rationale:} \texttt{\color{blue} Line k passes through the origin and has slope 1/5 means that its equation is y=1/5*x. Thus: (x, 1)=(5, 1) and (5, y) = (5,1) -->x=5 and y=1 Answer: C}



\noindent \textbf{IMDB} \citep{maas-etal-2011-learning}: Large Movie Review Dataset: This dataset is designed for binary sentiment classification and contains significantly more data than previous benchmark datasets. It includes 25,000 highly polar movie reviews for training and 25,000 for testing, along with additional unlabeled data for further use. For our experiments, we selected 1,000 samples from the dataset and instructed the LLM to classify each review as either positive or negative sentiment based on a provided system prompt like \autoref{fig:prompt_imdb}.


\begin{figure*}[htbp]
    \centering
    \begin{tcolorbox}[
        title=\texttt{System Prompt in LLM to judge ''Negative'' or ''Positive'},
        width=0.95\textwidth 
    ]
    \begin{flushleft}
You are a helpful movie critic who can help me. Answer ''Negative'' or ''Positive''
    \end{flushleft}
    \end{tcolorbox}
    \caption{Prompt in Inference LLM}
    \label{fig:prompt_imdb}
\end{figure*}

\noindent \texttt{IMDB Example 1:} \texttt{\color{blue} I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered "controversial" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.}

\noindent \texttt{IMDB Example 2:} \texttt{\color{blue} When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York's portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York's portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He's small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A\&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS' American Masters: Finding Lucy. If you want to see a docudrama, "Before the Laughter" would be a better choice. The casting of Lucille Ball and Desi Arnaz in "Before the Laughter" is much better compared to this. At least, a similar aspect is shown rather than nothing.}

\section{More Experiments to Support the Conclusion.}
\label{app:llm}
\textbf{Greater-Less Experiment: }
We used our synthetic dataset to test Qwen2.5-7B, Llama3-8B, Llama2-7B and Mistral-7B. The synthetic dataset has been introduced previously as \autoref{app:synthesis}, the result is shown as \autoref{fig:<}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\linewidth]{appendix_fig/dayu.pdf}
    \caption{Greater-Less Experiment}
    \label{fig:<}
\end{figure}


Based on the results shown in \autoref{fig:<}, the "Vanilla" configuration consistently outperformed the "Massive Value Destroy" setting across all models in the Greater-Less Experiment. For instance, Qwen2.5-7B achieved the highest accuracy in the Vanilla setting at 0.77\%, while its accuracy dropped significantly to 0.03\% under the Massive Value Destroy setting. Similarly, Mistral-7B displayed a drop from 0.59\% in the Vanilla setting to 0.12\% in the Massive Value Destroy setting, illustrating the impact of Massive Value removal on the model's performance.

\textbf{Experiment for other metrics: }

We believe that perplexity as a metric is somewhat one-dimensional. For instance, when large language models exhibit a "repetition phenomenon," such as in the following example:
"divisible by 9 and 12, which is the product of X, if it is divisible by 9 and 12, which is the product of X..." (repeated endlessly), the perplexity score remains as low as 2.99. Despite the low perplexity, the generated sentence is clearly incomprehensible to humans.

To address this limitation, we combined human judgment with GPT-4 evaluations to assess the readability of these sentences. From this, we introduced a new metric: Readability Rate. This metric aims to better capture the comprehensibility of text, going beyond what perplexity alone can measure.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\linewidth]{appendix_fig/read.pdf}
    \caption{Readability Rate Experiment}
    \label{fig:<}
\end{figure}

For example: the output from Qwen2-7B is not readable: Answer:  you're right, the premise of a "mystic man who eats women" is quite disturbing and inappropriate. Let's shift gears to something more constructive. Could you tell me about a topic you're interested in or need help with? Perhaps we could have a thoughtful discussion on a subject like philosophy, science, or literature instead? That might be more enjoyable and productive.

\textbf{Explore when Massive Values appear: }

By observing the Embedding Vectors of Qwen2.5-7B, Llama3-8B, Llama2-7B, and Mistral-7B, it was found that this phenomenon appears as early as the first layer like \autoref{fig:4_images}.

\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/Llama3_K_layer_1.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/Llama3_K_layer_2.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/qwen_K_layer_1.pdf}
        \label{fig:image3}
    \end{subfigure}
    \hfill
     \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/qwen_K_layer_2.pdf}
        \label{fig:image3}
    \end{subfigure}    
    \caption{Embedding Vector of K in Llama-3-8B and Qwen2.5-7B, we choose Layer 1 and 2, and the input question is shown as  \autoref{fig:prompt_in_inference_LLM}.}. 
    \label{fig:4_images}
    \vspace{-5pt}
\end{figure*}


In each layer, when extracting the QK embedding vector map, we consistently observe that there are no significant differences between the results taken before and after applying RoPE. The patterns remain largely consistent. This phenomenon is the same across all layers. Initially, we assumed that this might not be the case in the first layer. However, we later found that it occurs in every layer. To further investigate, we specifically selected the 20th layer of LLaMA3-8B for observation.




\begin{figure*}[ht]
    \centering
    % 第一张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/llama_before_K_layer_1.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % 第二张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/llama_after_K_layer_1.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % 第三张图片
    \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/llama_before1_K_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}
    \hfill
     \begin{subfigure}{}
        \centering
        \includegraphics[width=0.23\textwidth]{appendix_fig/llama_after_K_layer_10.pdf}
        \label{fig:image3}
    \end{subfigure}    
    \caption{Embedding Vector of K in Llama-3-8B, we choose Layer 1 and 10 before RoPE and after RoPE, and the input question is shown as  \autoref{fig:prompt_in_inference_LLM}}. 
    \label{fig:4_images}
    \vspace{-5pt}
\end{figure*}


\textbf{Disruption choices exert subtle influences in Gemma: }

The table comprehensively demonstrates the impact of various Massive Value handling methods (Mean, Zero, Minimum, and Maximum) on Gemma-9b model's performance across multiple benchmarks including GSM8k, AQUA, IMDB, and Passkey Retrieval with different configurations. The baseline Gemma2-9B model serves as a reference point, showing scores of 81.30 for GSM8k, 63.80 for AQUA, and 94.70 for IMDB, alongside perfect 100\% performance on all Passkey Retrieval configurations. The data reveals that regardless of the Massive Value handling method employed, the model maintains remarkably stable performance, particularly when dealing with non-Massive Values. While there are slight variations in the treatment of Massive Values, these differences are minimal and don't significantly impact the model's overall effectiveness. The Passkey Retrieval tasks consistently maintain perfect performance for non-Massive Values across all configurations, highlighting the model's robustness. This stability across different handling methods suggests that Gemma-9b possesses strong inherent resilience to various data processing approaches, with particularly notable stability in handling non-Massive Value cases, while still maintaining acceptable performance even when processing massive values.
\begin{table}[htbp]
\centering
\caption{Impacts of applying different methods to disrupt massive values/non-massive values on model performance (Gemma-9b).}
\adjustbox{max width=0.6\linewidth}{
\begin{tabular}{@{}llcccccc@{}}
\toprule
\multicolumn{2}{l}{\multirow{2}{*}{Operation}} & \multirow{2}{*}{GSM8k} & \multirow{2}{*}{AQUA-RAT} & \multirow{2}{*}{IMDB} & \multicolumn{3}{c}{Passkey Retrieval} \\
\cmidrule(l){6-8}
\multicolumn{2}{l}{} & & & & (128,6) & (256,12) & (1024,48) \\
\midrule
\multirow{1}{*}{None} & & 81.30 & 63.80 & 94.70 & 100 & 100 & 100 \\
\midrule
\multirow{2}{*}{Mean} & Massive Value & 15.10 & 16.50 & 1.80 & 2.00 & 0.00 & 0.00 \\
& Non-Massive Value & 81.60 & 65.60 & 97.40 & 100 & 100 & 100 \\
\midrule
\multirow{2}{*}{Zero} & Massive Value & 14.30 & 15.90 & 1.60 & 2.00 & 0.00 & 0.00 \\
& Non-Massive Value & 81.20 & 65.10 & 96.90 & 100 & 100 & 100 \\
\midrule
\multirow{2}{*}{Min} & Massive Value & 15.20 & 16.80 & 2.10 & 2.00 & 0.00 & 0.00 \\
& Non-Massive Value & 81.90 & 65.90 & 97.80 & 100 & 100 & 100 \\
\midrule
\multirow{2}{*}{Max} & Massive Value & 14.70 & 16.20 & 1.90 & 2.00 & 0.00 & 0.00 \\
& Non-Massive Value & 81.40 & 65.30 & 97.20 & 100 & 100 & 100 \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Controlled Experiment in Non Massive Value Region Desrupt in Gemma: }
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\linewidth]{appendix_fig/destroy_qwen.pdf}
    \caption{Readability Rate Experiment}
    \label{fig:qwen_des}
\end{figure}

As illustrated in \autoref{fig:qwen_des}, our controlled experiments with Qwen2.5-7B demonstrate consistent patterns across different benchmarks. The performance across all three tasks (GSM8K, AQUA, and IMDB) remains notably stable when varying the number of minimum values replaced (from 1 to 20). GSM8K maintains accuracy between 75-85\%, IMDB consistently performs above 90\%, and AQUA stays within the 51-57\% range. This stability from vanilla performance through various degrees of non-Massive Value manipulation reinforces our hypothesis that these regions play a relatively minor role in the model's capabilities. The consistent performance across different numbers of replaced values suggests that the model's Context Knowledge Understanding abilities remain intact as long as the Massive Values are preserved, regardless of how many non-Massive Values are modified.


\end{document}

