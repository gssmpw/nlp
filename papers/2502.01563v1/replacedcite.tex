\section{Related Work}
\subsection{LLM Quantizations and Massive Values in LLM}

Various existing works in quantization have studied the existence of massive value (also called outlier) features in LLMs____. Some research showed that massive value features have large activation values in most of their sequence dimensions. BERT often focuses attention on the "[SEP]" token____, while LLMs predominantly assign attention to the starting word token____, Some research focuses on the massive activation value in activation in LLM____ and identified attention artifacts in ViTs____. Someone trying to understand the mechanism of these massive values in transformer____.

Research on LLM quantization has predominantly focused on analyzing model weights and activations through two main approaches: (1) W8A8 quantization, which converts both activations and weights to INT8 ____, and (2) low-bit weight-only quantization (e.g., W4A16), which converts only weights to low-bit integers ____. 
During quantization, some studies have identified massive values in activation values ____ as a critical factor that can affect quantization. Several approaches have been developed to handle massive values specifically, either specific suppression techniques ____ or protection mechanisms ____ to maintain massive values unchanged. Quantization methods specifically address massive values that are typically analyzed within individual attention heads. Our work examines massive values across the depth dimension of multiple attention heads, offering potential new directions for future quantization approaches.

\subsection{Rotary Positional Encoding}
Rotary Positional Encoding (RoPE) has been extensively studied in transformer architectures since its introduction by ____ as a relative position encoding method. RoPE has been widely adopted in various LLMs, including Llama ____, Palm ____, Mistral ____, and GLM ____. Recent research on long-context Transformers ____ has demonstrated RoPE's effectiveness in input length extrapolation. These studies found that increasing the $\theta$ parameter in RoPE from its original value of 10,000 to larger values (e.g., 500,000) reduces attention to decay and enables more robust learning across extended contexts. These studies demonstrated that increasing RoPE's $\theta$ parameter from its original value of 10,000 to larger values (e.g., 500,000) mitigates attention decay and enhances robust learning over extended context windows. And Qwen2-VL adopts Multimodal Rotary Position Embedding (M-ROPE) that decomposes positional embedding into parts to capture 1D, 2D, and 3D positional information, enhancing its multimodal processing capabilities____. In parallel, research on NoPE (No Positional Encodings) has shown promising results in out-of-distribution (OOD) settings compared to RoPE, suggesting that the causal mechanism alone might sufficiently capture positional information without explicit position encoding ____. More recently, the analysis of Gemma-7b suggests an essential role of frequency components: the high-frequency part of the embedding vector in LLM encodes positional information, while the low-frequency part carries semantic information ____. This work takes a different approach by investigating how RoPE might contribute to the formation of massive value in LLM of transformer architectures.