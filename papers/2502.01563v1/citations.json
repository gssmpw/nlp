[
  {
    "index": 0,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      },
      {
        "key": "ahmadian2023intriguing",
        "author": "Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and Venkitesh, Bharat and Gou, Zhen Stephen and Blunsom, Phil and {\\\"U}st{\\\"u}n, Ahmet and Hooker, Sara",
        "title": "Intriguing properties of quantization at scale"
      },
      {
        "key": "guo2024activedormant",
        "author": "Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael Jordan and Song Mei",
        "title": "Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in {LLM}s"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "clark-etal-2019-bert",
        "author": "Clark, Kevin  and\nKhandelwal, Urvashi  and\nLevy, Omer  and\nManning, Christopher D.",
        "title": "What Does {BERT} Look at? An Analysis of {BERT}`s Attention"
      },
      {
        "key": "xiao2024efficient",
        "author": "Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xiao2024efficient",
        "author": "Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "sun2024massive",
        "author": "Mingjie Sun and Xinlei Chen and J Zico Kolter and Zhuang Liu",
        "title": "Massive Activations in Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "darcet2024vision",
        "author": "Timoth{\\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski",
        "title": "Vision Transformers Need Registers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "guo2024activedormant",
        "author": "Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael Jordan and Song Mei",
        "title": "Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in {LLM}s"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wei2022outlier",
        "author": "Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong",
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
      },
      {
        "key": "wei-etal-2023-outlier",
        "author": "Wei, Xiuying  and\nZhang, Yunchen  and\nLi, Yuhang  and\nZhang, Xiangguo  and\nGong, Ruihao  and\nGuo, Jinyang  and\nLiu, Xianglong",
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      },
      {
        "key": "yao2022zeroquant",
        "author": "Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
      },
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "frantar-gptq",
        "author": "Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh",
        "title": "{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers"
      },
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "key": "sheng2023flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\\'e}, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "Flexgen: High-throughput generative inference of large language models with a single gpu"
      },
      {
        "key": "park2024lut",
        "author": "Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo",
        "title": "Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liukivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
      },
      {
        "key": "sun2024massive",
        "author": "Mingjie Sun and Xinlei Chen and J Zico Kolter and Zhuang Liu",
        "title": "Massive Activations in Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wei2022outlier",
        "author": "Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong",
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
      },
      {
        "key": "wei-etal-2023-outlier",
        "author": "Wei, Xiuying  and\nZhang, Yunchen  and\nLi, Yuhang  and\nZhang, Xiangguo  and\nGong, Ruihao  and\nGuo, Jinyang  and\nLiu, Xianglong",
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "su2021roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "anil2023palm",
        "author": "Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others",
        "title": "Palm 2 Technical Report"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "du2022glm",
        "author": "Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sun-etal-2023-length",
        "author": "Sun, Yutao  and\nDong, Li  and\nPatra, Barun  and\nMa, Shuming  and\nHuang, Shaohan  and\nBenhaim, Alon  and\nChaudhary, Vishrav  and\nSong, Xia  and\nWei, Furu",
        "title": "A Length-Extrapolatable Transformer"
      },
      {
        "key": "xiong-etal-2024-effective",
        "author": "Xiong, Wenhan  and\nLiu, Jingyu  and\nMolybog, Igor  and\nZhang, Hejia  and\nBhargava, Prajjwal  and\nHou, Rui  and\nMartin, Louis  and\nRungta, Rashi  and\nSankararaman, Karthik Abinav  and\nOguz, Barlas  and\nKhabsa, Madian  and\nFang, Han  and\nMehdad, Yashar  and\nNarang, Sharan  and\nMalik, Kshitiz  and\nFan, Angela  and\nBhosale, Shruti  and\nEdunov, Sergey  and\nLewis, Mike  and\nWang, Sinong  and\nMa, Hao",
        "title": "Effective Long-Context Scaling of Foundation Models"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "Qwen2VL",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "haviv2022transformer",
        "author": "Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer",
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
      },
      {
        "key": "kazemnejad2024impact",
        "author": "Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva",
        "title": "The impact of positional encoding on length generalization in transformers"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "barbero2024round",
        "author": "Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\\v{c}}kovi{\\'c}, Petar",
        "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?"
      }
    ]
  }
]