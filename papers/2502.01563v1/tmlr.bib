
@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{dosovitskiy2021thomas,
  title={Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob Uszkoreit and Neil Houlsby. An image isworth 16$\times$ 16 words: Transformers for image recognition atscale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems (MLSys)},
  volume={6},
  pages={196--209},
  year={2024}
}


@article{jin2024disentangling,
  title={Disentangling Memory and Reasoning Ability in Large Language Models},
  author={Jin, Mingyu and Luo, Weidi and Cheng, Sitao and Wang, Xinyi and Hua, Wenyue and Tang, Ruixiang and Wang, William Yang and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2411.13504},
  year={2024}
}

@article{DBLP:journals/corr/abs-2408-08862,
  publtype={informal},
  author={Guangyan Sun and Mingyu Jin and Zhenting Wang and Cheng-Long Wang and Siqi Ma and Qifan Wang and Ying Nian Wu and Yongfeng Zhang and Dongfang Liu},
  title={Visual Agents as Fast and Slow Thinkers},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2408.08862},
  url={https://doi.org/10.48550/arXiv.2408.08862}
}
@inproceedings{jin-etal-2025-exploring,
    title = "Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?",
    author = "Jin, Mingyu  and
      Yu, Qinkai  and
      Huang, Jingyuan  and
      Zeng, Qingcheng  and
      Wang, Zhenting  and
      Hua, Wenyue  and
      Zhao, Haiyan  and
      Mei, Kai  and
      Meng, Yanda  and
      Ding, Kaize  and
      Yang, Fan  and
      Du, Mengnan  and
      Zhang, Yongfeng",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.37/",
    pages = "558--573",
}

@inproceedings{
hooper2024kvquant,
title={{KVQ}uant: Towards 10 Million Context Length {LLM} Inference with {KV} Cache Quantization},
author={Coleman Richard Charles Hooper and Sehoon Kim and Hiva Mohammadzadeh and Michael W. Mahoney and Sophia Shao and Kurt Keutzer and Amir Gholami},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
year={2024},
url={https://openreview.net/forum?id=0LXotew9Du}
}


@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision(CVPR)},
  pages={4195--4205},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825 (Mistral AI Technical Report)},
  year={2023}
}

@inproceedings{frantar-gptq,
  title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  year={2022},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}


@inproceedings{
heo2024rethinking,
title={Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models},
author={Jung Hwan Heo and Jeonghoon Kim and Beomseok Kwon and Byeongwook Kim and Se Jung Kwon and Dongsoo Lee},
booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
year={2024},
url={https://openreview.net/forum?id=JzG7kSpjJk}
}
@inproceedings{
yin2024outlier,
title={Outlier Weighed Layerwise Sparsity ({OWL}): A Missing Secret Sauce for Pruning {LLM}s to High Sparsity},
author={Lu Yin and You Wu and Zhenyu Zhang and Cheng-Yu Hsieh and Yaqing Wang and Yiling Jia and Gen Li and AJAY KUMAR JAISWAL and Mykola Pechenizkiy and Yi Liang and Michael Bendersky and Zhangyang Wang and Shiwei Liu},
booktitle={Forty-first International Conference on Machine Learning (ICML)},
year={2024},
url={https://openreview.net/forum?id=ahEm3l2P6w}
}

@InProceedings{xiao2023smoothquant,
    title = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
    author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
    year = {2023}
}

@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191 (Qwen Team Tech Report)},
  year={2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671 (Qwen Team Tech Report)},
      year={2024}
}

@inproceedings{kerethinking,
  title={Rethinking Positional Encoding in Language Pre-training},
  author={Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research (JMLR)},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@inproceedings{Dai2019TransformerXLAL,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  booktitle={Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:57759363}
}
@inproceedings{wei2022outlier,
    title={Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models},
    author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
    booktitle={Proceedings of the 34th Advances in Neural Information Processing Systems (NeurIPS)},
    year={2021}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning (ICML)},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={27168--27183},
  year={2022}
}
@inproceedings{ahmadian2023intriguing,
  title={Intriguing properties of quantization at scale},
  author={Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and Venkitesh, Bharat and Gou, Zhen Stephen and Blunsom, Phil and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  pages={34278--34294},
  year={2023}
}
@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}`s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828/",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
}
@inproceedings{wei-etal-2023-outlier,
    title = "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
    author = "Wei, Xiuying  and
      Zhang, Yunchen  and
      Li, Yuhang  and
      Zhang, Xiangguo  and
      Gong, Ruihao  and
      Guo, Jinyang  and
      Liu, Xianglong",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = dec,
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.102/",
    pages = "1648--1665",
}

@inproceedings{park2024lut,
  title={Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models},
  author={Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}


@inproceedings{
darcet2024vision,
title={Vision Transformers Need Registers},
author={Timoth{\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
year={2024},
url={https://openreview.net/forum?id=2dnO3LLiJ1}
}


@inproceedings{
xiao2024efficient,
title={Efficient Streaming Language Models with Attention Sinks},
author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
year={2024},
url={https://openreview.net/forum?id=NG7sS51zVF}
}
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={320--335},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 Technical Report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403 (Google Technical Report)},
  year={2023}
}
@article{cheng2024understanding,
  title={Understanding the interplay between parametric and contextual knowledge for large language models},
  author={Cheng, Sitao and Pan, Liangming and Yin, Xunjian and Wang, Xinyi and Wang, William Yang},
  journal={arXiv preprint arXiv:2410.08414},
  year={2024}
}
@inproceedings{
marks2023geometry,
title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
author={Marks, Samuel and Tegmark, Max},
booktitle={NeurIPS Workshop on Attributing Model Behavior at Scale},
year={2023},
url={https://openreview.net/forum?id=giMJzZIuzr}
}

@inproceedings{sun-etal-2023-length,
    title = "A Length-Extrapolatable Transformer",
    author = "Sun, Yutao  and
      Dong, Li  and
      Patra, Barun  and
      Ma, Shuming  and
      Huang, Shaohan  and
      Benhaim, Alon  and
      Chaudhary, Vishrav  and
      Song, Xia  and
      Wei, Furu",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)",
    year = "2023",
    address = "Toronto, Canada",
    url = "https://aclanthology.org/2023.acl-long.816/",
    pages = "14590--14604",
}

@inproceedings{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Proceedings of the 35th Neural Information Processing Systems Conference (NeurIPS)},
  year={2022}
}

@inproceedings{azaria2023internal,
    title={The Internal State of an LLM Knows When Itâ€™s Lying},
    author={Azaria, Amos and Mitchell, Tom},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023 (EMNLP)},
    pages={967--976},
    year={2023}
}

@inproceedings{
    jin2024llm,
    title={{LLM} Maybe Long{LM}: SelfExtend {LLM} Context Window Without Tuning},
    author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
    booktitle={Forty-first International Conference on Machine Learning (ICML)},
    year={2024},
    url={https://openreview.net/forum?id=nkOMLBIiI7}
}

@inproceedings{xiong-etal-2024-effective,
    title = "Effective Long-Context Scaling of Foundation Models",
    author = "Xiong, Wenhan  and
      Liu, Jingyu  and
      Molybog, Igor  and
      Zhang, Hejia  and
      Bhargava, Prajjwal  and
      Hou, Rui  and
      Martin, Louis  and
      Rungta, Rashi  and
      Sankararaman, Karthik Abinav  and
      Oguz, Barlas  and
      Khabsa, Madian  and
      Fang, Han  and
      Mehdad, Yashar  and
      Narang, Sharan  and
      Malik, Kshitiz  and
      Fan, Angela  and
      Bhosale, Shruti  and
      Edunov, Sergey  and
      Lewis, Mike  and
      Wang, Sinong  and
      Ma, Hao",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)",
    year = "2024",
    address = "Mexico City, Mexico",
    url = "https://aclanthology.org/2024.naacl-long.260/",
    pages = "4643--4663",
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783 (Meta AI Technical Report)},
  year={2024}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2024}
}

@inproceedings{haviv2022transformer,
  title={Transformer Language Models without Positional Encodings Still Learn Positional Information},
  author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022 (EMNLP)},
  pages={1382--1390},
  year={2022}
}
@inproceedings{wang-etal-2024-length,
    title = "Length Generalization of Causal Transformers without Position Encoding",
    author = "Wang, Jie  and
      Ji, Tao  and
      Wu, Yuanbin  and
      Yan, Hang  and
      Gui, Tao  and
      Zhang, Qi  and
      Huang, Xuanjing  and
      Wang, Xiaoling",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.834/",
    doi = "10.18653/v1/2024.findings-acl.834",
    pages = "14024--14040",
    abstract = "Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE`s generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which substantially expands NoPE`s context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible"
}

@inproceedings{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2024}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}
@inproceedings{rger2024truth,
title={Truth is Universal: Robust Detection of Lies in {LLM}s},
author={Lennart B{\"u}rger and Fred A. Hamprecht and Boaz Nadler},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
year={2024},
url={https://openreview.net/forum?id=1Fc2Xa2cDK}
}
@article{achiam2023gpt,
  title={Gpt-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint:2303.08774 (OpenAI Technical Report)},
  year={2023}
}
@article{wen2024rnns,
  title={Rnns are not transformers (yet): The key bottleneck on in-context retrieval},
  author={Wen, Kaiyue and Dang, Xingyu and Lyu, Kaifeng},
  journal={arXiv preprint arXiv:2402.18510},
  year={2024}
}
@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in neural information processing systems (NeurIPS)},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}
@inproceedings{
sun2024massive,
title={Massive Activations in Large Language Models},
author={Mingjie Sun and Xinlei Chen and J Zico Kolter and Zhuang Liu},
booktitle={First Conference on Language Modeling (COLM)},
year={2024},
url={https://openreview.net/forum?id=F7aAhfitX6}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168 (OpenAI Technical Report)},
  year={2021}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971 (Meta AI Technical Report)},
  year={2023}
}

@inproceedings{
gurnee2024language,
title={Language Models Represent Space and Time},
author={Wes Gurnee and Max Tegmark},
booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
year={2024},
url={https://openreview.net/forum?id=jE8xbmvFin}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068 (Meta AI Technical Report)},
  year={2022}
}

@inproceedings{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={Conference on Machine Learning and Systems (MLSys)},
  year={2024}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS)},
  pages={1877--1901},
  year={2020}
}


@inproceedings{
mohtashami2023randomaccess,
title={Random-Access Infinite Context Length for Transformers},
author={Amirkeivan Mohtashami and Martin Jaggi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)},
year={2023},
url={https://openreview.net/forum?id=7eHn64wOVy}
}
@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL)",
    year = "2011",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}


@inproceedings{barbero2024round,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

%% Knowledge-intensive
@inproceedings{chen-etal-2023-beyond,
    title = "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
    author = "Chen, Liang  and
      Deng, Yang  and
      Bian, Yatao  and
      Qin, Zeyu  and
      Wu, Bingzhe  and
      Chua, Tat-Seng  and
      Wong, Kam-Fai",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.390",
}


@article{hu2023survey,
  title={A survey of knowledge enhanced pre-trained language models},
  author={Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  journal={IEEE Transactions on Knowledge and Data Engineering (TKDE)},
  year={2023},
  publisher={IEEE}
}
@inproceedings{
    li2024cok,
    title={Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources},
    author={Xingxuan Li and Ruochen Zhao and Yew Ken Chia and Bosheng Ding and Shafiq Joty and Soujanya Poria and Lidong Bing},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2024},
    url={https://openreview.net/forum?id=cPgh4gWZlz}
}

@inproceedings{li2024corpuslm,
  title={Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks},
  author={Li, Xiaoxi and Dou, Zhicheng and Zhou, Yujia and Liu, Fangchao},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},
  pages={26--37},
  year={2024}
}

@inproceedings{yao2023react,
  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle = {International Conference on Learning Representations (ICLR) },
  year = {2023},
  html = {https://arxiv.org/abs/2210.03629},
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2024}
}

@inproceedings{
chen2024what,
title={What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information},
author={Yiting Chen and Junchi Yan},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
year={2024},
url={https://openreview.net/forum?id=e5Mv7iWfVW}
}
@inproceedings{
xu2024base,
title={Base of Ro{PE} Bounds Context Length},
author={Mingyu Xu and Xin Men and Bingning Wang and Qingyu Zhang and Hongyu Lin and Xianpei Han and weipeng chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
year={2024},
url={https://openreview.net/forum?id=EiIelh2t7S}
}
@inproceedings{liukivi,
  title={KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle={Forty-first International Conference on Machine Learning (ICML)}
}


@inproceedings{wangunderstanding,
  title={Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation},
  author={Wang, Xinyi and Amayuelas, Alfonso and Zhang, Kexun and Pan, Liangming and Chen, Wenhu and Wang, William Yang},
  booktitle={Forty-first International Conference on Machine Learning (ICML)}
}


@inproceedings{allenphysics,
  title={Physics of Language Models: Part 3.1, Knowledge Storage and Extraction},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Forty-first International Conference on Machine Learning (ICML)}
}

@misc{anthropic2023claude,
  title={Long context prompting for Claude 2.1},
  author={Anthropic},
  howpublished={\url{https://www.anthropic.com/news/claude-2-1-prompting}},
  year={2023},
  note={Accessed: date-of-access}
}


@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{
he2024learning,
title={Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},
author={Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
year={2024},
url={https://openreview.net/forum?id=aVh9KRZdRk}
}


@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}

@inproceedings{ling-etal-2017-program,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)",
    year = "2017",
    url = "https://aclanthology.org/P17-1015",
}


@inproceedings{longpre-etal-2021-entity,
    title = "Entity-Based Knowledge Conflicts in Question Answering",
    author = "Longpre, Shayne  and
      Perisetla, Kartik  and
      Chen, Anthony  and
      Ramesh, Nikhil  and
      DuBois, Chris  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.565/",
    doi = "10.18653/v1/2021.emnlp-main.565",
    pages = "7052--7063",
}

@inproceedings{black-etal-2022-gpt,
    title = "{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",
    author = "Black, Sidney  and
      Biderman, Stella  and
      Hallahan, Eric  and
      Anthony, Quentin  and
      Gao, Leo  and
      Golding, Laurence  and
      He, Horace  and
      Leahy, Connor  and
      McDonell, Kyle  and
      Phang, Jason  and
      Pieler, Michael  and
      Prashanth, Usvsn Sai  and
      Purohit, Shivanshu  and
      Reynolds, Laria  and
      Tow, Jonathan  and
      Wang, Ben  and
      Weinbach, Samuel",
    editor = "Fan, Angela  and
      Ilic, Suzana  and
      Wolf, Thomas  and
      Gall{\'e}, Matthias",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models (ACL Workshop)",
    month = may,
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.9/",
}

@inproceedings{
zheng2023judging,
title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS)},
year={2023},
url={https://openreview.net/forum?id=uccHPGDlao}
}

@inproceedings{jin-etal-2024-impact,
    title = "The Impact of Reasoning Step Length on Large Language Models",
    author = "Jin, Mingyu  and
      Yu, Qinkai  and
      Shu, Dong  and
      Zhao, Haiyan  and
      Hua, Wenyue  and
      Meng, Yanda  and
      Zhang, Yongfeng  and
      Du, Mengnan",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024 (ACL)",
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    url = "https://aclanthology.org/2024.findings-acl.108",
    pages = "1830--1842",
}


@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295 (Google DeepMind Technical Report)},
  year={2024}
}


@inproceedings{tevet-berant-2021-evaluating,
    title = "Evaluating the Evaluation of Diversity in Natural Language Generation",
    author = "Tevet, Guy  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL)",
    month = apr,
    year = "2021",
    url = "https://aclanthology.org/2021.eacl-main.25/",
    doi = "10.18653/v1/2021.eacl-main.25",
    pages = "326--346",
}
@inproceedings{chiang2023can,
  title={Can Large Language Models Be an Alternative to Human Evaluations?},
  author={Chiang, Cheng-Han and Lee, Hung-Yi},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={15607--15631},
  year={2023}
}

@inproceedings{
gu2024mamba,
title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
author={Albert Gu and Tri Dao},
booktitle={First Conference on Language Modeling (COLM)},
year={2024},
url={https://openreview.net/forum?id=tEYskw1VY2}
}

@article{jiang2024pre,
  title={Pre-RMSNorm and Pre-CRMSNorm transformers: equivalent and efficient Pre-LN transformers},
  author={Jiang, Zixuan and Gu, Jiaqi and Zhu, Hanqing and Pan, David},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2024}
}


@article{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  volume={11},
  pages={1316--1331},
  year={2023},
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@inproceedings{
guo2024activedormant,
title={Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in {LLM}s},
author={Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael Jordan and Song Mei},
booktitle={NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning},
year={2024},
url={https://openreview.net/forum?id=KWVjN8BL8a}
}

@inproceedings{
antoniades2024generalization,
title={Generalization vs. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data},
author={Antonis Antoniades and Xinyi Wang and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang},
booktitle={International Conference on Learning Representations (ICLR)},
year={2025}
}
@inproceedings{xu-etal-2024-knowledge-conflicts,
    title = "Knowledge Conflicts for {LLM}s: A Survey",
    author = "Xu, Rongwu  and
      Qi, Zehan  and
      Guo, Zhijiang  and
      Wang, Cunxiang  and
      Wang, Hongru  and
      Zhang, Yue  and
      Xu, Wei",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.486/",
    doi = "10.18653/v1/2024.emnlp-main.486",
    pages = "8541--8565",
}

@inproceedings{yu2023characterizing,
  title={Characterizing Mechanisms for Factual Recall in Language Models},
  author={Yu, Qinan and Merullo, Jack and Pavlick, Ellie},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)}
}