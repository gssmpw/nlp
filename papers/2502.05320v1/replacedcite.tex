\section{Related Work}
\subsection{Renal Pathology Segmentation}
\label{sec:title}
With the rapid advancements in deep learning, convolutional neural networks (CNNs) have become the standard approach for image segmentation ____. Bueno et al. ____ proposed SegNet-VGG 16 to detect glomerular structures through multi-class learning, achieving a high Dice Similarity Coefficient (DSC). Lutnick et al. ____ used DeepLab v2 to detect glomerulosclerosis, interstitial fibrosis, and tubular atrophy. Salvi et al. ____ designed a network based on a multi-residual U-Net for quantitative analysis of glomeruli and renal tubules. Bouteldja et al. ____ developed a CNN model capable of automatic multi-class segmentation of renal pathology across different mammalian species and experimental disease models. However, most of these methods still face challenges in the fine segmentation of renal vasculature, particularly under limited annotation conditions, including key structures such as inner and outer vessel walls, arteries, and small lesions.

\subsection{Attention Modules}
\label{sec:title}
Unlike natural images, pathological images contain complex cellular structures, dense tissue backgrounds, and significant noise. Attention modules can automatically adjust weights across different scales of feature maps, effectively integrating global and local information, ensuring that the model performs well in segmenting both small lesions and large structures ____. However, modeling the complex spatial relationships and detailed structures in pathological images for segmentation models remains challenging. Several deep learning-based methods have been developed to capture spatial dependencies between pixels in feature maps ____. These studies introduced attention gates and other mechanisms but primarily focused on general features in natural images. In contrast, our method applies a hierarchical soft attention mechanism explicitly designed for pathological contexts, enhancing core vascular feature segmentation by adaptively reducing noise interference.

\subsection{Multi-scale Segmentation}
\label{sec:title}
In renal pathological image segmentation, significant variation in scale among different objects is a key challenge. Deng et al. ____ proposed a scale-aware dynamic network, which employs multiple segmentation networks or multi-head structures to address these diversities. However, such strategies often increase model complexity and lack explicit modeling of spatial relationships, limiting performance. Recent efforts have shown that combining multi-scale skip connections and attention modules can address these issues. For example, U-Net++ ____ introduced nested skip connections, and Attention U-Net ____ used attention gates, but these studies did not investigate their combined use. To the best of our knowledge, our work is the first to integrate full-scale skip connections with hierarchical soft attention gates in a unified framework for renal vasculature segmentation.

In our proposed network, we explicitly designed the architecture with full-scale skip connections, integrating detailed anatomical information and cross-scale contextual semantics. We proposed a full-scale hierarchical learning framework (FH-Seg) that effectively bridges the gap between structural and pathological contexts. Additionally, we implemented a learnable hierarchical soft attention gate that adaptively reduces interference from non-core information, enhancing the focus on critical vascular features.
% in the Tunica media category
 % such as Lumen and Tunica Intima,
% strategically integrated after full-scale skip connections and before each upsampling step, adaptively 

% \vskip -0.1in
% \subsection{Preliminaries}
% The traditional U-Net architecture utilizes symmetric skip connections to enhance the direct feature transfer from the corresponding encoder level to each decoder level:
% \begin{equation}
% X_{De}^i = \mathcal{U}(X_{De}^{i+1}) \oplus X_{En}^i,
% \end{equation}
% where $X_{De}^i$ and $X_{En}^i$ represent the feature maps at the $i$-th level of the decoder and encoder respectively, $\mathcal{U}$ denotes the upsampling operation and $\oplus$ signifies the concatenation of feature maps.

% The efficiency of U-Net in terms of parameter usage is notable. The encoder and decoder are symmetrical, with each layer $i$ having $32 \times 2^i$ channels. The number of parameters for each decoder stage $i$ is calculated as follows:

% \begin{equation}
% N_{De}^i = K_s \times K_s \times \left[ c(X_{D e}^{i+1}) \times c(X_{D e}^i) + c(X_{D e}^i)^2 + c(X_{En}^i + X_{D e}^i) \times c(X_{D e}^i) \right],
% \end{equation}
% where $K_s$ is the convolution kernel size and $c(\cdot)$ denotes the channel depth, ensuring efficient computation and effective feature retention.

\begin{figure}       
	\centering
    \includegraphics[scale=0.5]{zhutu.pdf}  
	\caption{Full-Scale Hierarchical Learning Segmentation Framework.}   
	\label{FIG:1}
 \vskip -0.2in
\end{figure}

% \begin{figure}       
% 	\centering
%     \includegraphics[scale=0.5]{figure1.png}  
% 	\caption{Examples of image-mask pairs in our LRV dataset.}   
% 	\label{FIG:LRV}
%  \vskip -0.2in
% \end{figure}

% \vskip -0.2in