\section{LEFTOVERS}

\subsection{General lower bound}
We adapt a previous result to obtain a new lower bound on the sample complexity of \decomp sets. The bound is asymptotic with respect to the dimension, and should be contrasted with our lattice-specific bounds presented later on, which are nonasymptotic. % To simplify the analysis, for the following lemma we assume that the configuration space is a $d$-dimensional $R$-ball for some $R\in (0,\infty)$, i.e., $\C:=\B_R$.

\begin{lemma}[Verger-Gaugry's~\cite{verger2005covering} lower bound]\label{thm:sample_lower}
  Suppose that $\X$ is a  $\beta$-cover. Fix a radius $R>0$ such that  $R=\sqrt{d}\beta$ or $R\geq d\beta$, $\beta:=\beta\left(\delta,\epsilon\right)$. Then for $d>0$ large enough there exists $c>0$ such that 
    \begin{equation}\label{eq:general_LB}
        |\X| \geq   cd\left(\frac{R}{\beta}\right)^d =: n_{VG}^{\de}.
    \end{equation}
    % \triangleq \|\X_{LB}\|
\end{lemma}

\begin{proof}
Theorem 3.10 in~\cite{verger2005covering} states that if a sample set $\X^*$ is a $\frac{1}{2}$-cover for $\B_{R^*}$, where $R^*=\frac{\sqrt{d}}{2}$ or $R^*\geq\frac{d}{2}$, then there exists $c>0$ such that for $d>0$ large enough, $|\X^*|\geq cd\cdot \left(2R^*\right)^d$. Next, we use a rescaling argument to apply those findings to our setting. In particular, we wish to cover $\B_R$ with $\beta$-balls. To convert $\beta$-balls into $\frac{1}{2}$-balls we rescale $\C$ by $\frac{1}{2\beta}$, which yields a ball of radius $R^*=\frac{R}{2\beta}$. In this rescaled space, $\B_{R^*}$ is covered by $\frac{1}{2}$-balls. 

For the radius $R^*$ to fit Verger-Gaugry's requirements, we need to ensure that
\[
    \frac{R}{2\beta} = R^* = \frac{\sqrt{d}}{2},
\]
which implies that $R=\sqrt{d}\beta$, 
and similarly  %\footnote{Notice that this lower bound applies to any ball $\B_r$, as long as it is $\sqrt{d}$ times or more than $d$-times the "minimal radius" ball of $\beta$.}
$R\geq d\beta$ for the other case. Returning to Verger-Gaugry's result, we get the lower bound
\[
    |\X|=|\X^*| \geq cd\cdot \left(2R^*\right)^d = cd\cdot \left(\frac{R}{\beta}\right)^d,
\]
which concludes the proof. %\qedsymbol
\end{proof}

%\kiril{Instead of the following discussion, can you generate a plot for the two conditions $R=\delta\beta,R\geq d\cdot \beta$ in dimensions $3,6,9$, where on $x$-axis you fix $\delta$ and on the $y$-axis you plot the allowed $\eps$-range?} \kiril{Alternatively, move into appendix.} \itai{TODO: discuss this. is this part nessecary?}

%\kiril{Start of text to be commented out.}

We discuss the applicability of the above lemma in our setting and show that the requirement $R\geq d\beta$ is not overly restrictive. \kiril{Update the discussion below to $d=5$ or $6$, which is more relevant. Feel free to change the numbers, but try to stick to our setting, i.e., a ball Cspace. No need to include here all the calculations, just the outcome.}\itai{I added some computations, tell me what you think of this version} Using the requirement above, ~\Cref{lem:cover}---in which we defined $\beta(\delta,\epsilon)$, and using $d=6,R=1$, one can reach the following connection:
\begin{align*}
    \epsilon \leq \frac{1}{\sqrt{36\delta^2-1}}
\end{align*}
First, this implies a minimal clearance of ${\delta>\frac{1}{6}\approx0.17}$. Second, this shows that for low enough $\delta$ values we have an unlimited maximal $\epsilon\rightarrow\infty$ (when $\delta\downarrow0.17$). It does pose a limit on the clearance, but it still enables a good approximation in many situations. See \Cref{fig:limit_graph_lower} for the exact figure showing the relationship between $\delta$ and $\epsilon$. \itai{we may need to remove this part, as we now know that this is asymptotic. I added a note that it is asymptotic.} \kiril{let's keep it. It strengthens our lattice bounds.}\itai{done + graph}
\begin{figure}[thb]
\centering  
\includegraphics[width=0.9\columnwidth]{Images/eps_delta_lower_limit.png}
\caption{Dependence of $\epsilon$ on $\delta$ in order to get a valid lower limit (\Cref{thm:sample_lower}).}
\label{:limit_graph_lower}
\end{figure}


%  if our space's radius is $\sqrt{10}$ times bigger than a $\beta$-ball then the volume is $\sqrt{10}^{10}=10^{\frac{10}{2}}$ bigger than a $\beta$-ball. This means we expect to find about $10^5$ smaller $\beta$-balls in it. With a normal cubic $[0,1]^d$ world, we expect to find about $\frac{1}{\beta^d}=\left(\frac{1}{\beta}\right)^d=\left(\frac{1}{\beta^2}\right)^{\frac{d}{2}}$ balls. Taking the same $d=10$, we get that for our spherical space to have less $\beta$-balls than a cubic space, we will need $\frac{1}{\beta^2}\geq 10 \Rightarrow \beta \leq 0.32$. We get:
% \[
%     \beta=\delta\sqrt{\frac{\epsilon^2}{1+\epsilon^2}}\leq0.32 \iff \frac{\epsilon^2}{1+\epsilon^2}\leq \left(\frac{0.32}{\delta}\right)^2
% \]
% But notice that:
% \[
%     \frac{0.32}{\delta}<1 \iff \delta>0.32
% \]
% So for $\delta\leq 0.32$, we get $\frac{0.32}{\delta}\geq1$. But $\frac{\epsilon^2}{1+\epsilon^2}<1$ for all $\epsilon>0$, which makes the above inequality true for all $\epsilon>0$. All this is to say the following: the requirements on $\delta,\epsilon$ aren't harsh and would probably be fulfilled in most use cases, allowing us to rely on this lower bound.

We conclude this part with a comparison of the bound $n_{VG}^{\de}$ with a naive lower-bound, which asserts that the minimal number of points in a set $\X$ that is $\beta$-cover should be at least the volume of $\B_R$ divided by the volume of a $\beta$-ball, i.e., 
\begin{equation}
        |\X|\geq \frac{\partial(\B_R)}{\partial(\B_\beta)}=\frac{\partial(\B_1) R^d}{\partial(\B_ 1)\beta^d}=\left(\frac{R}{\beta}\right)^d:=n_{\text{naive}}^{\de}, 
\end{equation}
where $\partial(A)$ is the volume, or Lebesgue measure, of a set $A\subset \dR^d$. 
Notice that $n_{VG}^{\de}$ is $d$ times tighter than $n_{\text{naive}}^{\de}$, although this result is asymptotic. 


\subsection{a complicated lettice definition (not needed?)}
A lattice $\Lambda$ with a basis $E$ is defined to be:
\[
    \Lambda\triangleq\smashoperator[r]{\bigcup_{\{p=k\cdot e_i | k\in \mathbb{N},e_i\in E\}}}\{v|v\text{ is a vertex of } F_{\Lambda,p}\}
\]

\subsection{something from the lower bound section Im not sure of anymore}
with $D\subset \mathbb{R}^d$ some set. This calculation is equivalent to a sample set taken uniformly on the set $D$, spaced out by balls of radius $\epsilon>0$.  This means we can extrapolate the number of samples this lower bound would've gotten on the same $\frac{\sqrt{d}}{2}$-radius ball. We know the $[0,1]^n$-cube is of volume 1, so the new number of samples is:
\begin{align*}
    \|\chi_{new-LB}\|=\|\chi_{LB}\|*Vol(B_2(0,\frac{\sqrt{d}}{2})) \\
    =\sqrt{\frac{e}{2}}\Big(1-\frac{2\delta}{1-2\delta}\Big)^2\Big(\sqrt{\frac{d-1}{2\pi e}}\cdot \frac{1-2\delta}{\delta}\Big)^d \cdot c_d(\frac{\sqrt{d}}{2})^d \\
    =c_2c_d\cdot (d(d-1))^{\frac{n}{2}}\Big(\frac{1-2\delta}{2\delta}\Big)^d
\end{align*}
for some $c_2>0$. Comparing the two values:
\begin{align*}
    \frac{\|\chi\|}{\|\chi_{new-LB}\|} = \frac{4c_1\delta d\cdot\frac{d^{\frac{d}{2}}}{(2\delta)^d}}{kc_d\cdot (d(d-1))^{\frac{n}{2}}\Big(\frac{1-2\delta}{2\delta}\Big)^d} \\
    =\frac{c_3\delta c_d^{-1}d}{(d-1)^{\frac{n}{2}}}(1-2\delta)^d
\end{align*}
Using the same approximation for $c_d^{-1}\approx\sqrt{\pi d}(\frac{d}{2\pi e})^{\frac{d}{2}}$:
\begin{align*}
    \approx\frac{c_3\delta \sqrt{\pi d}(\frac{d}{2\pi e})^{\frac{d}{2}}d}{(d-1)^{\frac{n}{2}}}(1-2\delta)^d \\
    =c_4\delta d^{1.5}\Big(\frac{d}{d-1}\Big)^{\frac{d}{2}}\Big(\frac{1-2\delta}{\sqrt{2\pi e}}\Big)^d \\
\end{align*}
Again, using the similar calculations as the previous section, we can use $\Big(\frac{d}{d-1}\Big)^\frac{d}{2}\rightarrow \sqrt{e}$:
\begin{align*}
    \leq c_5\delta d^{1.5}\cdot\Big(\frac{1}{\sqrt{2\pi e}}\Big)^d\rightarrow 0
\end{align*}

\subsection{anstar definitions we dont need}
In general, the $A_d^*$ set , is defined through the $A_d$ set:
\begin{align}
    A_d=\{(x_i)_1^{d+1} \in \mathbb{Z}^{d+1} \mid \sum_i x_i = 0\}
\end{align}
He then continues to define the "dual" to $A_d$ as:
\begin{align}
    A_d^*=\bigcup_{i=0}^d ([i]+A_d)
\end{align}
For some vector $[i]$. 

\subsection{anstar upper bound leftovers}

Following this definition, we can use a well known result: if $T:\mathbb{R}^d\rightarrow\mathbb{R}^d$ then \\ ${Vol(T(S))=\det(T)\cdot Vol(S)}$. Let us first calculate $\det(T)$:
\begin{align}
    \det(T) = \sqrt{\det(T)\det(T)} = \sqrt{\det(T^t)\det(T)} =\sqrt{\det(T^tT)} \nonumber \\ \label{det_T}
    = \det((EPG^t)^t(EPG^t)) = \det(G(EP)^t(EP)G^t)
\end{align}
 In ~\cite{conway2013sphere}, the determinant of the lattice is defined as $\det(GG^t)$ and Conway cites it~\cite{conway2013sphere} to be $\frac{1}{d+1}$. Combine these facts with~(\ref{det_T}) and you get that $\det(T)=\sqrt{n+1}$. 


Let us use this fact to compute two things: first, let us find the size of the whole sample set. Conway~\cite{conway2013sphere} gives us the radius of the most optimal ("thinnest" in the book's terms) ball radius, which is said to be:
\begin{align}
    R=\sqrt{\frac{n(n+2)}{12(n+1)}}
\end{align}
So, let us assume we live in a cube of size $[0,w]^n$ with optimal balls of the above radius $R$. We would like the cube size to be such that if we rescale $[0,w]^n$ to $[\delta,1-\delta]^n$ we would also get $\beta$-Balls from $R$-Balls, to fit with the setting in~\cite{dayan2023near}. So:
\begin{equation}
    \begin{cases}
      wx=1-2\delta\\
      Rx=\beta
    \end{cases}
    \Rightarrow 
    \begin{cases}
         x=\frac{\beta}{R} \\
         w=\frac{(1-2\delta)R}{\beta} = \frac{1-2\delta}{\delta}\frac{R}{\alpha}
    \end{cases}
\end{equation}
So using these facts, we need a set $S\subset \mathbb{R}^d$ such that $T(S)=[0,w]^d$, getting:
\begin{align*}
    Vol(S)=\sqrt{d+1}\cdot Vol([0,w]^d)=\sqrt{d+1}\cdot(\frac{1-2\delta}{\delta}\frac{R}{\alpha})^d\\
    =\sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\sqrt{\frac{d(d+2)}{12(d+1)}})^d(\frac{\sqrt{1+\epsilon^2}}{\epsilon})^d \\
    =\sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\frac{d(d+2)}{12(d+1)}(1+\frac{1}{\epsilon^2}))^{\frac{d}{2}}\\
    = \sqrt{d+1}(\frac{1}{\delta} - 2)^d\cdot(\frac{d(d+2)}{12(d+1)}(1+\frac{1}{\epsilon^2}))^{\frac{d}{2}}\\
    \leq \sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\frac{d(d+2)}{12(d+1)})^{\frac{d}{2}}
\end{align*}
Using similar methods, we will compute the number of samples in a ball of radius $\beta$. We want a set $\Tilde{B}\subset \mathbb{R}^d$ such that $T(\Tilde{B})=B(\delta,c)$ for some center point $c\in\mathbb{R}^d$. Remember from the last paragraph that this comes from a ball of radius $R$, so:
\begin{align*}
    \|B_\beta\cap\Lambda\|\leq \frac{Vol(B_\beta)}{\sqrt{d}}+k(\beta)^{d-2}\leq \frac{\beta^d}{\sqrt{d}c_d^{-1}}+k(\beta)^{d-2}\\
    \approx\beta^{d-2}[\frac{\beta^2}{\pi\cdot d}(\frac{d}{2\pi e})^{\frac{d}{2}} + k]\\
    \|\chi_{Ball}\|\leq Vol(\Tilde{B})=\sqrt{d+1}Vol(B(\beta,c))=\sqrt{d+1}c_dR^d\\
    =\sqrt{d+1}(\frac{d(d+2)}{12(d+1)})^{\frac{d}{2}}\frac{1}{\sqrt{\pi d}(\frac{d}{2\pi e})^{\frac{d}{2}}}\\
    =\sqrt{\frac{d+1}{\pi d}}(\frac{2\pi e \cdot (d+2)}{d+1})^{\frac{d}{2}}\rightarrow \frac{(\sqrt{\frac{2\pi e}{12}})^d}{\sqrt{\pi}}\approx\frac{1.19^d}{\sqrt{\pi}}
\end{align*}

Remembering that we want the integer vectors in this space, and that small cubes in that grid are of volume 1, we get that:
\begin{align}
    \|\chi_{A_d^*}\|\leq Vol(S)=\sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\frac{d(d+2)}{12(d+1)})^{\frac{d}{2}}
\end{align}
And Comparing it now to $\|\chi_{SG^*}\|$, we get:
\begin{align*}
    \frac{\|\chi_{A_d^*}\|}{\|\chi_{SG^*}\|}\leq\frac{\sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\frac{d(d+2)}{12(d+1)})^{\frac{d}{2}}}{2(\frac{1-2\delta}{\delta})^n(\frac{2d-1}{16})^{\frac{d}{2}}} \\
    =\frac{\sqrt{d+1}}{2}(\frac{16d(d+2)}{12(d+1)(2d-1)})^{\frac{d}{2}}\leq\frac{\sqrt{d+1}}{2}(\frac{16}{12\cdot2})^{\frac{d}{2}} \\
    = \frac{\sqrt{d+1}}{2}(\frac{2}{3})^{\frac{d}{2}}\rightarrow 0
\end{align*}
This is already good. It is exponentially better than the $SG^*$ set. Let us compare it to the lower bound:
\begin{align*}
    \frac{\|\chi_{A_d^*}\|}{\|\chi_{LB}\|}=\frac{\sqrt{d+1}(\frac{1-2\delta}{\delta})^d\cdot(\frac{d(d+2)}{12(d+1)})^{\frac{d}{2}}}{\sqrt{\frac{e}{2}}\Big(1-\frac{2\delta}{1-2\delta}\Big)^2\Big(\sqrt{\frac{d-1}{2\pi e}}\cdot \frac{1-2\delta}{\delta}\Big)^d} \\
    \leq \sqrt{\frac{2(d+1)}{e}}(\frac{2\pi e\cdot d(d+2)}{12(d^2-1)}))^{\frac{d}{2}} \\
    \rightarrow (\sqrt{\frac{\pi e}{6}})^d\approx1.19^d
\end{align*}
So $\|\chi_{A_d^*}\|\leq1.19^d\|\chi_{LB}\|$, compared to $1.43^d$ for the $SG^*$ set---exponentially better!
\subsection{some remains from the onion-set section}
We need to compare it to the SG*-grid coverage of the original ball. In that, we get:
\begin{align}
    \|\chi_{SG*}\|\approx2\frac{Vol(B_{\beta}(0))}{(2\frac{2}{\sqrt{2n-1}}\delta)^n}=\frac{2c_n(\frac{\sqrt{n}}{2})^n}{(2\frac{2}{\sqrt{2n-1}}\delta)^n}=2c_n(n(2n-1))^\frac{n}{2}(\frac{1}{8\delta})^n
\end{align}
In comparison:
\begin{align*}
    \frac{\|\chi\|}{\|\chi_{SG*}\|}\approx\frac{cn^{\frac{n}{2}+1}ln(n)(\frac{1}{2\delta})^n}{2c_n(n(2n-1))^\frac{n}{2}(\frac{1}{8\delta})^n}=2c\cdot c_n^{-1} \cdot nln(n)(\frac{1}{2n-1})^{\frac{n}{2}}4^n
\end{align*}
Using~\cite{tsao2020sample}, we insert an approximation for $c_n^{-1}\approx\sqrt{\pi n}(\frac{n}{2\pi e})^{\frac{n}{2}}$:
\begin{align*}
    \frac{\|\chi\|}{\|\chi_{SG*}\|}\approx 2c \sqrt{\pi n}(\frac{n}{2\pi e})^{\frac{n}{2}}\cdot nln(n)(\frac{1}{2n-1})^{\frac{n}{2}}4^n \\ 
    =2c \sqrt{\pi} n^{1.5}ln(n)(\frac{n}{2n-1})^{\frac{n}{2}}(\frac{8}{\pi e})^{\frac{n}{2}} \\
    =2c \sqrt{\pi} n^{1.5}ln(n)(\frac{2n}{2n-1})^{\frac{n}{2}}(\frac{4}{\pi e})^{\frac{n}{2}} \\
\end{align*}
On the side, we can see that:
\begin{align}
    (\frac{2n}{2n-1})^{\frac{n}{2}}=(1+\frac{1}{2n-1})^{\frac{n}{2}}=((1+\frac{1}{2n-1})^{2n-1})^{\frac{\frac{n}{2}}{2n-1}}\rightarrow e^{\frac{1}{4}}
\end{align}
So finally:
\begin{align*}
    \frac{\|\chi\|}{\|\chi_{SG*}\|}\approx 2c\sqrt{\pi}e^{\frac{1}{4}}n^{1.5}ln(n)\cdot (0.684)^n\rightarrow0
\end{align*}
So for a high enough $n\in\mathbb{N}$, we get $\|\chi\|<\|\chi_{SG*}\|$. In general, for a high enough dimension, it behaves as $\|\chi\|\approx\|\chi_{SG*}\|\cdot 0.685^n$. It can be checked that we get $\|\chi\|<\|\chi_{SG*}\|$ starting at $n=c\cdot 19$: the $c>0$ is not specified in any of the referenced material, but assuming $c>1$, this makes the onion set impractical for motion planning problems, as we rarely reach robots with $n\geq 19$.

\subsection{ONION set leftovers: taken out of the paper for now}

\kiril{The following hasn't been defined. Why it's here?}
\begin{align*}
    \frac{\|\chi_{Onion, C-Space}\|}{\|\chi_{LB}\|} = O(\ln{d})
\end{align*}
\subsection*{Complexity of sample sizes: the non-constructive "Onion" set}

To show what's theoretically possible, using a non-constructive method, we'll be using in this section a covering set from a paper in~\cite{verger2005covering} that used an "onion" like construction to cover $\B_{\beta}(0)\subset\mathbb{R}^d$ (instead of a cube). Intuitively, this is what he does:
\begin{enumerate}
    \item Cover a sphere $S_{R}(0)$ for some $R>0$, using balls of radius $r=\frac{1}{2}$. 
    \item Show that this collection of spheres actually covers an annulus from $R$ to $R-\frac{1}{2R}$.
    \item Recursively cover smaller and smaller annuluses, until the smaller radius is less than $\frac{1}{2}$, allowing you to cover it with one extra ball.
\end{enumerate}
It can be noted that (1) is done using probabilistic methods, making the whole set non-constructive.
Using this method, he manages to cover a ball of radius $R\geq \frac{\sqrt{d}}{2}$ with $n$ balls of radius \(\frac{1}{2}\), with:
\[
    n \leq c\cdot dln(d)(2R)^d
\]
We need to cover the spherical C-Space $\B_\frac{\sqrt{d}}{2}$ with balls of radius $\beta$, so let us resize the sphere: turn balls of radius $\beta$ into balls of radius $\frac{1}{2}$, so resizing the whole space by a factor of $\frac{1}{2\beta}$. This turns the ball of radius $\frac{\sqrt{d}}{2}$ to a ball of radius $\frac{\sqrt{d}}{4\beta}$.


Our problem is, then: cover a ball of radius $\beta=\frac{\sqrt{d}}{4\delta}$ with balls of radius $\frac{1}{2}$, which falls under~\cite{verger2005covering} if:
\[
    \frac{\sqrt{d}}{4\beta} \geq \frac{\sqrt{d}}{2} \Rightarrow \beta \leq \frac{1}{2}
\]
Which is reasonable. This gives us:
\begin{align}
    \|\chi\|\leq cdln(d)(2\frac{\sqrt{d}}{4\beta})^d
\end{align}
Comparing it to the same lower bound all other lattices were compared to, we get:
\begin{align*}
    \frac{\|\chi_{Onion, C-Space}\|}{\|\chi_{LB}\|} \leq \frac{cdln(d)(2\frac{\sqrt{d}}{4\beta})^d}{k\cdot d (2\frac{\sqrt{d}}{4\beta})^d} =\frac{c}{d}\ln{d}
    \Rightarrow \frac{\|\chi_{Onion, C-Space}\|}{\|\chi_{LB}\|} = O(\ln{d})
\end{align*}
This means that this semi-random set achieves a covering that is only $O(\ln{d})$ times above the lower bound!

\subsection{calcing anstar $\rho$}

 From pages LXII and 10 of Conway~\cite{conway2013sphere}, we know 
    \[
        \rho = (\delta\sqrt{\det{\Lambda}})^{\frac{1}{d}}
    \]
    where $\delta$ is a parameter given in page 115 which is independent of choice of basis. The only thing left to calculate, then, is $\det{\Lambda}=det(G_\Lambda G_\Lambda^t)$. Let us calculate it:
    \begin{align*}
        \det{\Lambda}&=det(G_\Lambda G_\Lambda^t)\\
        &=\det\begin{pmatrix}
            1 & -1 &  0  & \dots & 0 & 0 & 0 \\
            0 & 1  &  -1 & \dots & 0 & 0 & 0 \\
            . & .  &  . & \dots & . & .  & .\\
            0 & 0  &  0 & \dots & 1 & -1 & 0\\
            \frac{-d}{d+1} & \frac{1}{d+1} & \frac{1}{d+1} & \dots & \frac{1}{d+1} & \frac{1}{d+1} & \frac{1}{d+1}
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & \dots & 0 & 0 & \frac{-d}{d+1} \\
            -1 & 1 & \dots & 0 & 0 & \frac{1}{d+1} \\
            0 & -1 & \dots & 0 & 0 & \frac{1}{d+1} \\
            . & . & \dots & . & .  & .\\
            0 & 0 & \dots & 0 & -1 & \frac{1}{d+1}\\
            0 & 0 & \dots & 0 & 0 & \frac{1}{d+1}
        \end{pmatrix}\\
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            -1 & 2  &  -1 & 0 & 0 &\dots & 0 & 0 & 0\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            -1 & 0  &  0 & 0 & 0 &\dots & 0 & 0 & \frac{d}{d+1}\\
        \end{pmatrix},\\
    \end{align*}
    now add all $d-1$ rows to the last row:
    \begin{align*}
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            -1 & 2  &  -1 & 0 & 0 &\dots & 0 & 0 & 0\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 1.5  &  -1 & 0 & 0 &\dots & 0 & 0 & 0.5\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
        &=\frac{1}{2}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
            &=\frac{1}{2}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  \frac{4}{3} & -1 & 0 & \dots & 0 & 0 & \frac{-1}{3}\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}.
    \end{align*}
    we keep going like this until the last row, getting
    \begin{align*}
        &=\frac{1}{(d-1)!}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  4 & -3 & 0 & \dots & 0 & 0 & -1\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & 0 & d & -1\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
        &=\frac{1}{(d-1)!}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  4 & -3 & 0 & \dots & 0 & 0 & -1\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & 0 & d & -1\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 0 & \frac{d}{d+1}-1+\frac{1}{d}\\
        \end{pmatrix}\\
        &=\frac{1}{(d-1)!}\cdot d!(\frac{1}{d(d+1)})
        =\frac{1}{d+1}.
    \end{align*}

    The following short lemma will help us get the diameter.
\begin{lemma}\label{lemma:diameter}
    Let $\{e_i\}$ be vectors such that $\angle\left(e_i,e_j\right)\leq \frac{\pi}{2}$ for all $i\neq j$. Then, the diameter of the FR created by these vectors is $\D=\|\sum_i e_i\|$.
\end{lemma}
\begin{proof}
    \begin{align*}
        \|e_i+e_j\|^2&= \|e_i\|^2+\|e_j\|^2+2\langle e_i,e_j \rangle \\
        &= \|e_i\|^2+\|e_j\|^2+2\cos{\left(\angle\left(e_i,e_j\right)\right)}\|e_i\|\|e_j\|,
    \end{align*}
    and if $\angle\left(e_i,e_j\right)\leq \frac{\pi}{2}$ then $\cos{\left(\angle\left(e_i,e_j\right)\right)}\geq 0$, so \(\|e_i+e_j\|^2 \geq \|e_i\|^2\), giving us \(\|e_i+e_j\| \geq \|e_i\|\).
    
    
    Overall, this means that we can keep adding base FR elements such that the norm keeps getting larger, until we can't add more elements. At the end, the distance between $v=0,w=\sum_i e_i$ is the largest it can be. Seeing as how the vertices of the FR are also its extreme points, we can not make it larger, meaning we found the diameter.
\subsection{LEFTOVERS from the old collision complexity proof}
We start with introducing a few basic definitions that would be used throughout this section. First, we define a \emph{fundamental region} (FR) of a lattice, which is the closed set contained inside the linear combination of that lattice's basis vectors. The geometric shape of a given FR can be viewed as ``lattice tile'', as interior-disjoint translations of it can be used to cover the space. \kiril{Are the following definitions taken from somewhere or brand new? If the former then a citation would be good.}

%This creates something that can be viewed as the "lattice tile" (see Figure~\ref{fig:reflection)), which is because the union of all the FRs covers (tile) the space. 

 \begin{figure}
    \centering
    \includegraphics[width=0.66\linewidth]{Images/FR_2D.png}
    \caption{Examples of FR tiles in $d=2$. \kiril{Explain what we see here. E.g., what are the basis and FRs?} }
    \label{fig:reflection}
\end{figure}

%From this illustration, it can be also seen that FRs are not unique, some even having a completely different basis.

% Also called a \emph{Parallelotope} sometimes. 
\begin{definition}[Fundamental region]
    For a given lattice $\Lambda$ with a basis ${E_\Lambda=\{e_i\in \dR^d\}_{i=1}^m}$ (where $m\leq d$), and a point $p\in \dR^d$, the FR is defined as 
    \begin{align*}
        F_{\Lambda,p}:=\left\{p+\sum_{i=1}^m a_i e_i\middle|\,a\in[0,1]^m\right\}. 
    \end{align*}
    The center of $F_{\Lambda,p}$ is defined to be \[c_{\Lambda,p}=p+\frac{1}{2^{m}}\sum_{a\in \{0,1\}^m}\sum_{i=1}^ma_ie_i.\]
\end{definition}
\kiril{Way may consider removing $p$ from the definition of the FR.}
We mention that the geometric shape of a FR can is not unique, which follows from non-uniqueness of the lattice basis (see Figure~\ref{fig:reflection}). 

\kiril{Ideally, figures should be saved as pdfs (or other vector formats). I usually used \url{http://ipe.otfried.org/}.}.


% A for a given FR, its center is the average of all binary linear combinations of basis vectors, which means $2^{|E_\Lambda|}$ of them. \kiril{add it to the FR figure}
% \begin{definition}[FR center]\label{def:lattice_center}
%     Given a lattice $\Lambda$ with basis ${E_\Lambda=\{e_i\in \dR^d\}_{i=1}^m}$, the center of the FR $F_{\Lambda,p}$ in relation to some point $p\in \mathbb{R}^d$, is
% \end{definition}

\begin{lemma}\label{Lattice-properties}
    An FR center \(c_{\Lambda,p}\) for a  lattice \(\Lambda\) with basis ${E_\Lambda=\{e_i\in \dR^d\}_{i=1}^m}$ (where $m\leq d$) has the following properties: 
    \begin{enumerate}
        \item \(c_{\Lambda,p}=p+\frac{\sum_{i=1}^m e_i}{2}\), i.e., the center of the FR is the average of the base vectors; \kiril{Is average the right word here if we divide by $2$?}
        \item \(\{c_{\Lambda,p_i}|p_i\in\Lambda\}=\Lambda+c_{\Lambda,o}\), i.e., the collection of centers of FR regions whose point offset is a lattice point, is equal to the lattice translated with the center of the FR $F_{\Lambda,o}$, where $o$ is the origin of $\dR^d$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    For the first property, we rely on the definition of $c_{\Lambda,p}$ and expand the summation expression within it. In particular, we decompose it by individually choosing the $a_i$ values, and separating the inner sum according to the dependencies on $a_i$'s. That is,
%    use a combinatorial argument. Fix a value for $a_1$. If the rest of the $a_j$ values for $j\neq i$  are chosen, then we get two "copies" of the series: one where $a_i=0$ and one where $a_i=1$. So, the relative weight of each vector is $\frac{1}{2}$. More formally, notice that
 \kiril{Please check that it makes sense.}    \begin{align*}
  \sum_{a\in \{0,1\}^m}\sum_{i=1}^ma_ie_i &= \sum_{a_1=0}^1\ldots \sum_{a_m=0}^1\sum_{i=1}^ma_ie_i \\ &=
\sum_{a_1=0}^1\ldots \sum_{a_m=0}^1a_1e_1+\sum_{a_1=0}^1\ldots \sum_{a_m=0}^1\sum_{i=\bm{2}}^ma_ie_i \\ &=
\sum_{a_{\bm{2}}=0}^1\ldots \sum_{a_m=0}^1e_1+\sum_{a_1=0}^1\ldots \sum_{a_m=0}^1\sum_{i=\bm{2}}^ma_ie_i                             \\
                                      &=
2^{m-1}e_1+2\sum_{a_{\bm{2}}=0}^1\ldots \sum_{a_m=0}^1\sum_{i=\bm{2}}^ma_ie_i                             \\
                                        &=
2^{m-1}e_1+2\cdot 2^{m-2}e_2+ 2\cdot 2\sum_{a_{\bm{3}}=0}^1\ldots \sum_{a_m=0}^1\sum_{i=\bm{3}}^ma_ie_i                             \\  & = \ldots = 2^{m-1}\sum_{i=1}^m e_i. 
    \end{align*}
    Thus, $c_{\Lambda,p}=p+\frac{\sum_{i=1}^m e_i}{2}$.

    Now consider the second property. From the previous paragraph it follows that $c_{\Lambda,o}=\frac{\sum_ie_i}{2}$. For some $p_i\in \Lambda$ it holds that
    \[c_{\Lambda,p_i}=p_i+\frac{\sum_{i=1}^m e_i}{2}= p_i+o+\frac{\sum_{i=1}^m e_i}{2}=p_i + c_{\Lambda,o},\]
    which completes the proof. 
\end{proof}

\begin{lemma}\label{fr_shifted_sum} Consider an FR $F:=F_{\Lambda,p}$ for a lattice $\Lambda$. Then, 
    \[
        I_{F,p} = I_{F,o} - |P_F|\cdot \|c_{\Lambda,p}\|^2.
    \]
\kiril{The notations here are still rather confusing. In the equation, is $p$ the same as in $F_{\Lambda,p}$, or should be $p'$? There's also ambiguity with respect to $c_{\Lambda,p}$ since it doesn't explicitly reflect a specific FR. I suggest to change it to $c(F_{\Lambda,p})$ for clarity.}
\end{lemma}
\begin{proof}
  The statement follows from expanding the definition of $I_{F,p}$:
    \begin{align}
        I_{F,p} &= \sum_{x\in P_F} \|x-c_{\Lambda,p}\|^2 = \sum_{x\in P_F} \sum_{j=1}^d \left(x(j)-c_{\Lambda,p}(j)\right)^2 \nonumber\\
        &= \sum_{x\in P_F} \sum_{j=1}^d \left(x(j)^2-2x(j)c_{\Lambda,p}(j)+c_{\Lambda,p}(j)^2\right) \nonumber\\
        &=\sum_{x\in P_F} \sum_{j=1}^d x(j)^2 + \sum_{x\in P_F} \sum_{j=1}^d c_{\Lambda,p}(j)^2 - 2\sum_{x\in P_F} \sum_{j=1}^d x(j)c_{\Lambda,p}(j) \nonumber\\
        &= I_{F,o} + |P_F|\cdot\|c_{\Lambda,p}\|^2 - 2 \sum_{j=1}^d \sum_{x\in P_F} x(j)c_{\Lambda,p}(j) \nonumber \\
        &= I_{F,o} + |P_F|\cdot\|c_{\Lambda,p}\|^2 - 2 \sum_{j=1}^d c_{\Lambda,p}(j) \sum_{x\in P_F} x(j) \nonumber \\
        &= I_{F,o} + |P_F|\cdot\|c_{\Lambda,p}\|^2 - 2 \sum_{j=1}^d c_{\Lambda,p}(j) \underbrace{\left(\sum_{x\in P_F} \left(x(j) - c_{\Lambda,p}(j)\right)+\sum_{x\in P_F} c_{\Lambda,p}(j)\right)}_\text{(*)}. \label{eq:IFo}
    \end{align}
    Next, we show that the expression in ($*$) is equal to $|P_F|c_{\Lambda,p}(j)$. In particular, we rely on the fact that for any given $x\in P_F$ it holds that $x=p+\sum_1^m a_ie_i$ for some $a\in\{0,1\}^m$ (and vice versa).
    Thus, 
    \begin{align*}     c_{\Lambda,p}&=p+\frac{1}{2^{m}}\sum_{a\in \{0,1\}^m}\sum_{j=1}^ma_ie_i \\ & =\frac{1}{2^m}\sum_{a\in \{0,1\}^m}\left(p+\sum_{j=1}^m a_ie_i\right)\\
      & =\frac{1}{2^m}\sum_{x\in P_F}x.%\\
%     &=\frac{1}{2^{|E_\Lambda|}}\sum_{j=1}^{|E_\Lambda|} x(j),\\
%        &\Rightarrow 2^{|E_\Lambda|}c_{\Lambda,p}=\sum_{j=1}^{|E_\Lambda|} x(j)\Rightarrow 0=\sum_{j=1}^{|E_\Lambda|} \left(x(j)-c_{\Lambda,p}\right),
    \end{align*}
    \kiril{I fixed some equations, which I believe were incorrect. Please check me.}
    Hence, $c_{\Lambda,p}(j)=\frac{1}{2^m} \sum_{x\in P_F}x(j)$, for $1\leq j\leq d$, 
    which implies that the vertices of the FR are symmetric around the center, intuitively speaking. From here we obtain
    \[(*)=\sum_{x\in P_F} c_{\Lambda,p}(j)=|P_F|c_{\Lambda,p}(j),\]
which we plug into \eqref{eq:IFo} to yield
    \begin{align*}
        I_{F,p}&=  I_{F,o} + |P_F|\|c_{\Lambda,p}\|^2 - 2 |P_F|\sum_{j=1}^d c_{\Lambda,p}(j)^2 \\
        &= I_{F,o} + |P_F|\|c_{\Lambda,p}\|^2 - 2|P_F|\|c_{\Lambda,p}\|^2 = I_{F,o} - |P_F|\|c_{\Lambda,p}\|^2.
    \end{align*}
\end{proof}

Notice that the value of $I_{F,p}$ is independent  of the particular point $p$: this notation talks about the sum-of-edges distance from the center of some cell that is located at an unknown location. The $p$ just denotes where the corner of the cell is, and this property \emph{does not effect} the distance from the center. \kiril{I still don't see why this is true. More care should be taken where explaining this claim.} \itai{I tried a different explanation. let's discuss it if you think it's not enough.} \kiril{Also, we already have $I_\Lambda(r)$, which looks similar.} \kiril{I want to revisit this statement after we improve the notation.} Hence, from now we denote $I_{\Lambda}:=I_{F,p}$ for some FR $F$ and point $p$. Thus, we can simplify the previous lemma statement to 
%\equiv CONST\) for any $F$ you choose, so we will just call it \(I_{\Lambda}\) from now on. Also, in our case since we are using a lattice, it has $|P_F|=2^d$ vertices, so we will just use that from now on. So we rephrase our lemma to appear as such:
\begin{align}
            I_{F,o} = I_\Lambda + 2^d\cdot \|p\|^2,
\end{align}
which holds in our case since  $|P_F|=2^d$ for our three lattices. $\ZN,\DN$ have $d$-dim bases in $\mathbb{R}^d$ and $\AN$ has a $d$-dim base in $\mathbb{R}^{d+1}$.
% \begin{lemma}
%     Let \(D_{i,r}\) be the $D_i$ such that \(\{p_i\in D_i\}\cap \B_r(0)\neq \phi\) (all the cells that contribute a vertex in the ball). Then:
%     \[
%         \lim_{n\rightarrow \infty}I(r)=\lim_{n\rightarrow \infty}{\frac{1}{N}\sum_{D_{i,r}} I_{0,D_i}}
%     \]
% \end{lemma}
% \begin{proof}
%     I know that for most of the ball, each vertex is counted N times (once for each neighbor). The trick here is to show that for the vertices that are on the edge of the ball, that are covered less than N times, the contribution to the sum goes to 0 asymptotically.
% \end{proof}
We now move to first major contribution of this section, which is a general upper-bound on collision complexity.
\begin{thm}[Collision-complexity upper bound]\label{complexity_upper_bound}
    Let $\Lambda$ be a lattice and let $F$ be some FR for the basic $E_\Lambda$. Denote by $\D$ the diameter of $F$, which is the diameter of the smallest sphere that contains $F$. Let $\rho$ be the inradius of $F$, which is the radius of the biggest  sphere that fits within $F$.
For any $r>0$ it holds that
\begin{align*}
        I\left(r\right) \leq \frac{1}{4}\left(\sum_{e\in E_\Lambda}\|e\|^2\right) \btheta^{d-1}\left(\frac{\btheta}{\rho d}+1\right)\left(\frac{\vol(\B_1)}{\sqrt{d}}\btheta+ \alpha\right), 
    \end{align*}
    where $\alpha$ is some positive constant, and
    \[
        \btheta:=\frac{r+\D-\rho}{\beta}f_\Lambda.
    \]
\kiril{We already used $\btheta$ for a different purpose.}\kiril{It would be good to visualize this theorem.} \kiril{Remove the parts related to rescaling.}
    %Then for some radius $r>0$, if we use $\theta\left(\beta,R\right):=\frac{R}{\beta}$ from corollary~\ref{cor:specific_sample_complexity} and define the rescaled radius to be
    %\[
    %    \btheta=\theta\left(\beta,r+\D-\rho\right)f_\Lambda,
    %\]
    %we get
    % \[
    %     g:=\frac{r+D-\rho}{\beta}f_\Lambda
    % \]
    % \begin{align*}
    %     g(r)&:=\frac{r-\rho}{\beta}f_\Lambda,\\
    %     % a_n(r)&:=\frac{I_\Lambda}{2^n}(\frac{Vol(\B_1)}{\sqrt{n}}g(r)^2 + k_1),
    % \end{align*}
    % we get
\end{thm}
\begin{proof}
    %First note that for now, we will use the unscaled space. Later, we will be using $\btheta$ instead of just $r+\D-\rho$, because it performs a rescaling. This is done to enable us to use the lattice point bound in equation (9) above.\kiril{Those statements are quite vague and I don't follow them.}
  We start by showing that
    \begin{align}
        I\left(r\right)\leq \sum_{i=1}^{[\frac{r+\D}{\rho}]-1} \frac{I_\Lambda}{2^d}(r+\D-i\rho)^{d-2}\left(\frac{ \vol\left(\B_1\right)}{2^d}\left(r+\D-i\rho\right)^2+\alpha\right).
    \end{align}
    Let
    \begin{align*}
        J\left(r\right):=\frac{1}{2^d}\smashoperator[r]{\sum_{F\in \F_r}} I_{F,o},
    \end{align*}
     be the partial sum-of-squares approximation, only using FRs fully contained in the $r$-ball (denoted by the set $\F_r$) \kiril{This definition is vague, and should be defined more formally.}).
     For a given FR $F\in \F_r$, denote by $c_F$ its center.
    %$$C:=\{c_F| F\subset \B_r\}.$$
     Then using \Cref{fr_shifted_sum} we obtain   \begin{align*}
        J\left(r\right)&= \frac{1}{2^d}\smashoperator[r]{\sum_{F\in \F_r}} I_{F,o} \underset{[L\ref{fr_shifted_sum}]}{=} \frac{1}{2^d}\smashoperator[r]{\sum_{F\in \F_r}} \left(I_\Lambda + 2^d\|c_F\|^2\right)\\
        &=\smashoperator[r]{\sum_{F\in \F_r}}\frac{I_\Lambda}{2^d} + \smashoperator[r]{\sum_{F\in \F_r}}\|c_F\|^2. %\leq \frac{I_\Lambda}{2^d}|B_{r-\rho}\cap\Lambda|+\smashoperator[r]{\sum_{F\in \F_r}}\|c_F\|^2. 
    \end{align*}
   Next, we observe that $|\F_r|\leq |\B_{r-\rho}\cap \Lambda|$: every FR fully included  which  follows from \kiril{be more explicit here w.r.t to the definition of $I_\Lambda$ and why this property follows. }. We also notice that $\sum_{F\in \F_r}\|c_F\|^2\leq I(r-\rho)$, which follows the second property in~\Cref{Lattice-properties}. In particular, \kiril{be more formal and include all the necessary notations and definitions.} the lemma shows that the collection of centers of is itself is a "smaller copy" of the main lattice. More than that, from the symmetry of the FR we can say that the insphere which defines the inradius is centered around the centroid (\itai{I hope this is right, otherwise I need to make some changes to this lemma}). Seeing as the insphere is included in the FR, and that we picked only the FRs that are included in the ball, we can safely reduce the radius of the ball by the inradius and still have all the centroids we started with. Therefor, using the bound on $|\B_r\cap\Lambda|$ we got at theorem~\ref{general_sample_complexity}, we get
    \begin{align}
        &\overset{\leq}{[L\ref{Lattice-properties}]} \frac{I_\Lambda}{2^d}\cdot|B_{r-\rho}\cap\Lambda| + I\left(r-\rho\right) \nonumber\\
        &\overset{\leq}{[T\ref{general_sample_complexity}]} \frac{I_\Lambda}{2^d}\cdot\left(\frac{Vol\left(\B_{r-\rho}\right)}{\sqrt{d}}+a\left(r-\rho\right)^{d-2}\right) + I\left(r-\rho\right) \nonumber\\
        &=\frac{I_\Lambda}{2^d}\cdot\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\cdot \left(r-\rho\right)^d+a\left(r-\rho\right)^{d-2}\right) + I\left(r-\rho\right)\nonumber\\
        &=\frac{I_\Lambda}{2^d}\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\left(r-\rho\right)^2 + a\right)\left(r-\rho\right)^{d-2} + I\left(r-\rho\right) \nonumber\\
        &=b_d\left(r\right)\left(r-\rho\right)^{d-2} + I\left(r-\rho\right)
    \end{align}
    For $b_d\left(r\right):=\frac{I_\Lambda}{2^d}\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\left(r-\rho\right)^2 + a\right)$. Notice that $J\left(r\right)$ counts some edges too few times: edges near the border of the sphere may appear in less than $2^d$ neighboring cells, due to some cells not being included in $J\left(r\right)$, so dividing by $2^d$ lowers their contribution too much. To solve this, we will notice that for a given FR we can take the diameter $\D$ and expand $B_r$ with it. Doing this, every relevant point of $B_r$ will now contain all $2^d$ neighboring cells in the $B_{r+\D}$ ball. We do have extra points now, but the normalizer $\frac{1}{2^d}$ makes sure their contribution decays to 0. We get
    \[
        I\left(r\right)\leq J\left(r+\D\right),
    \]
    and overall
    \begin{align*}
        I\left(r\right) &\leq J\left(r+\D\right) \overset{\left(13\right)}{\leq} b_d\left(r+\D\right)\left(r+\D-\rho\right)^{d-2} + I\left(r+\D-\rho\right)\\
        &\leq b_d\left(r+\D\right)\left(r+\D-\rho\right)^{d-2} + b_d\left(r+\D-\rho\right)\left(r+\D-2\rho\right)^{d-2} + I\left(r-2\rho\right) \\
        &\leq \dots,
    \end{align*}
    which we continue until
    \[
        r+\D-m\rho < \rho \Rightarrow \frac{r+\D}{\rho} - 1 < m.
    \]
    This means we stop at \(m=[\frac{r+D}{\rho}]-1\), getting
    \begin{align*}
        I\left(r\right) &\leq \sum_{i=1}^{[\frac{r+\D}{\rho}]-1} b_d\left(r+\D-\left(i-1\right)\rho\right)\left(r+\D-i\rho\right)^{d-2}\\
        &= \sum_{i=1}^{[\frac{r+\D}{\rho}]-1} \frac{I_\Lambda}{2^d}\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\left(r+\D-i\rho\right)^2 + k_1\right)\left(r+\D-i\rho\right)^{d-2}\\
        &=\sum_{i=1}^{[\frac{r+\D}{\rho}]-1} \left(\frac{I_\Lambda Vol\left(\B_1\right)}{2^d\sqrt{d}}\left(r+\D-i\rho\right)^d+k_1\frac{I_\Lambda}{2^d}\left(r+\D-i\rho\right)^{d-2}\right)\\
    \end{align*}
    which is because the last $I\left(r+\D-m\rho\right)=0$, due to capturing either nothing or a single point. 
    
    
    Now that we finished our base upper bound, we can now finish the theorem by further developing it. Using $k=[\frac{r+\D}{\rho}]-1$, and utilizing the known comparison between sums and integrals, we develop the following
    % Notice that $J(r)$ counts some edges too few times: edges near the border of the sphere may appear in less than $2^d$ neighboring cells, so dividing by $2^d$ lowers their contribution too much. To solve this, we will notice that for a FR we can take the diameter $D:=\text{Diam}(FR)$ and expand $B_r$ with it. Doing this, every relevant point of $B_r$ will now contain all $2^d$ neighboring cells in the $B_{r+D}$ ball. We do have extra points now, but the normalizer $\frac{1}{2^d}$ makes sure their contribution decays to 0. We get
    % \[
    %     I(r)\leq J(r+D)
    % \]
    % Which is because v is the longest vector out of all the others in the cell (as he is a sum of all of them, and they are positive (<- show this)). Due to this, the larger radius now contains all cells we didn't count before with a few more - but they are counted less, as they are weighted with $\frac{1}{2^n}$.
    \begin{align}
        &A\sum_1^k \left(r+\D-i\cdot \rho\right)^{d-B} \nonumber\\
        &\leq A\Big[\left(r+\D - 1\cdot \rho\right)^{d-B} + \int_1^k \left(r+\D-x\cdot \rho\right)^{d-B} dx\Big] \nonumber\\
        &= A\Big[\left(r+\D-\rho\right)^{d-B} + [\frac{\left(r+\D-k\cdot \rho\right)^{d-B+1}}{-\rho\left(d-B+1\right)}-\frac{\left(r+\D-\rho\right)^{d-B+1}}{-\rho\left(d-B+1\right)}]\Big] \nonumber\\
        &=A\Big[\left(r+\D-\rho\right)^{d-B} + \frac{1}{\rho\left(d-B+1\right)}[\left(r+\D-\rho\right)^{d-B+1} - \left(r+\D-k\cdot \rho\right)^{d-B+1}]\Big].
        %=_{(2)} (r+\|v\|)^n + \frac{1}{\rho(n+1)}[(r+\|v\|)^{n+1} -((n+1)k\rho(r+\|v\|)^n+O(smaller?)+(r+\|v\|)^{n+1})] \\
    \end{align}
    We also know that 
    % \begin{align*}
    %         k=[\frac{r+\|v\|}{\rho}]-1\geq \frac{r+\|v\|}{\rho}-2 \\
    %         \Rightarrow k\rho \geq r+\|v\|-2\rho \\
    %         \Rightarrow 1-k\rho \leq 1 - (r+\|v\|) + 2\rho
    % \end{align*}
    \begin{align*}
            k&=[\frac{r+\D}{\rho}]-1\leq \frac{r+\D}{\rho}-1 \\
            &\Rightarrow k\rho \leq r+\D-\rho \Rightarrow -k\rho \geq -\left(r+\D\right)+\rho \\
            &\Rightarrow r+\D-k\rho \geq \rho,
    \end{align*}
    which means that
    \begin{align*}
        % &=A\Big[(r+D-\rho)^{n-B} + \frac{1}{\rho(n-B+1)}[(r+D-\rho)^{n-B+1} - (r+D-k\cdot \rho)^{n-B+1}]\Big]\\
        \left(13\right)&\leq A\Big[\left(r+\D-\rho\right)^{d-B} + \frac{\left(\left(r+\D-\rho\right)^{d-B+1} - \rho^{d-B+1}\right)}{\rho\left(d-B+1\right)}\Big].
    \end{align*}
    Returning to our original expression and setting $\Tilde{r}:=r+D-\rho$, we get
    \begin{align*}
        &\sum_{i=1}^{[\frac{r+\D}{\rho}]-1} \left(\frac{I_\Lambda Vol\left(\B_1\right)}{2^d\sqrt{d}}\left(r+\D-i\rho\right)^d+a\frac{I_\Lambda}{2^d}\left(r+\D-i\rho\right)^{d-2}\right)\\
        &\leq \frac{I_\Lambda Vol\left(\B_1\right)}{2^d\sqrt{d}}\Big[\Tilde{r}^d\left(1+\frac{\Tilde{r}}{\rho \left(d+1\right)}\right) - \frac{\rho^d}{\left(d+1\right)}\Big] + \\
        &\qquad a\frac{I_\Lambda}{2^d}\Big[\Tilde{r}^{d-2}\left(1+\frac{\Tilde{r}}{\rho\left(d-1\right)}\right) - \frac{\rho^{d-2}}{d-1}\Big] \\
        &\leq \frac{I_\Lambda Vol\left(\B_1\right)}{2^d\sqrt{d}}\Big[\Tilde{r}^d\left(1+\frac{\Tilde{r}}{\rho \left(d+1\right)}\right)\Big] +
        a\frac{I_\Lambda}{2^d}\Big[\Tilde{r}^{d-2}\left(1+\frac{\Tilde{r}}{\rho \left(d-1\right)}\right)\Big].
    \end{align*}
    Because this refers to the unscaled space, we now want to move back to our rescaled space, using $\btheta$ and getting
    \begin{align*}
        &\frac{I_\Lambda Vol\left(\B_1\right)}{2^d\sqrt{d}}\Big[\btheta^d\left(1+\frac{\btheta}{\rho \left(d+1\right)}\right)\Big] +
        a\frac{I_\Lambda}{2^d}\Big[\btheta^{d-2}\left(1+\frac{\btheta}{\rho \left(d-1\right)}\right)\Big] \\
        &\leq \frac{I_\Lambda}{2^d}\btheta^{d-2}\left(1+\frac{\btheta}{\rho \left(d-1\right)}\right)\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\btheta^2 + a\right).
    \end{align*}
    \itai{Here is another way, that reaches a DIFFERENT result for some reason}
    SECOND proof: 
    \begin{align*}
        &A\sum_1^k \left(r+\D-i\cdot \rho\right)^{d-B} \\
        = &A(r+\D-\rho)^{d-B}\sum_0^k \left(1-\frac{i\cdot \rho}{r+\D-\rho}\right)^{d-B} \\
    \end{align*}
    Let $x=\frac{i\cdot \rho}{r+\D-\rho}$. Then, $x>0$ (obvious, bc $\D >\rho\Rightarrow r+\D>\rho$). but also:
    \[
        x<1 \iff \frac{i\cdot \rho}{r+\D-\rho} < 1 \iff i<\frac{r+\D}{\rho}-1
    \]
    which is exactly what $k=[\frac{r+\D}{\rho}]-1$ holds, and $i\leq k$. From here, we use \({(1-x)^N<e^{-xN}}\) for $x\in(0,1)$:
    \begin{align*}
        \leq &A(r+\D-\rho)^{d-B}\sum_0^k e^{-\frac{i(d-B)\cdot \rho}{r+\D-\rho}} \\
        \leq &A(r+\D-\rho)^{d-B}\sum_0^{\infty} e^{-\frac{i(d-B)\cdot \rho}{r+\D-\rho}} \\
        = &A(r+\D-\rho)^{d-B} \left(1+\frac{1}{e^{\frac{(d-B)\cdot \rho}{r+\D-\rho}}}\right)\\
        \leq &2A(r+\D-\rho)^{d-B}
    \end{align*}
    At $r\rightarrow\infty$ we get $A(r+\D-\rho)^{d-B}$ instead of $2A\cdot(\dots)$.
    \itai{And just to contrast, the integral approximation got:}
    \begin{align*}
        \left(13\right)&\leq A\Big[\left(r+\D-\rho\right)^{d-B} + \frac{\left(\left(r+\D-\rho\right)^{d-B+1} - \rho^{d-B+1}\right)}{\rho\left(d-B+1\right)}\Big].
    \end{align*}
    \itai{this is much shorter than the other term, but also much better asymptotically. but I dont logically understand this. this means I can just ignore the sum alltogether. and then I get that the sum of length is pretty much the same order of magnitude as the number of samples, which makes no sense to me.}
    \itai{OK, I made a graph and appearently the number of samples IS proportional to the sum of edges! this is a test for a unit ball, I need to make sure its not a special case}

     \begin{figure}[H]
    \centering
    \includegraphics[width=0.66\linewidth]{Images/new_plot.png}
    \caption{proportional! }
    \label{fig:reflection}
\end{figure}

    Returning to the lattice basis $\{e_i\}$, we will now show that
    \[
        I_\Lambda=2^{d-2}\sum_i\|e_i\|^2.
    \]
    This is because
    \begin{align*}
        I_\Lambda &= \sum_{x\in P_{F_{\Lambda,p}}}\|x - c_{\Lambda,p}\|^2=_{[\ref{Lattice-properties}]} \sum_{x\in P_{F_{\Lambda,p}}}\|x - \frac{\sum e_i}{2}\|^2\\
        &=\sum_{a_i\in\{0,1\}}\|\sum_1^d a_i e_i -\frac{\sum e_i}{2}\|^2=\sum_{a_i\in\{0,1\}}\langle\sum_1^d\left(a_i-\frac{1}{2}\right)e_i,\sum_1^d\left(a_i-\frac{1}{2}\right)e_i\rangle\\
        &=\sum_{b_i\in \{\pm\frac{1}{2}\}} \sum_{i,j} b_ib_j\langle e_i,e_j\rangle= \sum_{b_i\in \{\pm\frac{1}{2}\}}b_i^2\sum_i\|e_i\|^2 + \sum_{b_i\in \{\pm\frac{1}{2}\}}\sum_{i\neq j}\langle e_i,e_j\rangle.
    \end{align*}
    We now notice two things: first, that $b_i^2=\frac{1}{4} \forall i$. Second, that for each $i,j$ there are 4 options: $\frac{1}{2},\frac{1}{2}$ and $-\frac{1}{2},-\frac{1}{2}$ which both lead to $\frac{1}{4}$, and $-\frac{1}{2},\frac{1}{2}$ and $\frac{1}{2},-\frac{1}{2}$ which both lead to $-\frac{1}{4}$. For that reason, all the terms of the second inner-product cancel each other out! We are left with:
    \begin{align*}
        \sum_{b_i\in \{\pm\frac{1}{2}\}}\frac{1}{4}\sum_i\|e_i\|^2=\frac{2^d}{4}\sum_i\|e_i\|^2=2^{d-2}\sum_i\|e_i\|^2,
    \end{align*}
    inserting this into the expression, we get our final version:
    \begin{align*}
        I\left(r\right)&\leq \frac{I_\Lambda}{2^d}\btheta^{d-2}\left(1+\frac{\btheta}{\rho \left(d-1\right)}\right)\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\btheta^2 + a\right)\\
        &= \frac{1}{4}\left(\sum_i\|e_i\|^2\right) \btheta^{d-2}\left(\frac{\btheta}{\rho \left(d-1\right)}+1\right)\left(\frac{Vol\left(\B_1\right)}{\sqrt{d}}\btheta^2+ a\right).
    \end{align*}
\end{proof}
\end{proof}

    First, notice that the first two terms form a telescopic sum, which leaves us with
    \begin{align}
        =r^{N+1} - (r-\frac{[(r-r_0)n]}{rn})^{N+1} - \frac{1}{n}\sum_{i=0}^{[rn]-1}(r-\frac{i+1}{n})^N\nonumber\\        
        \leq r^{N+1} - r_0^{N+1} -\frac{1}{n}\sum_{i=0}^{[(r-r_0)n]-1}(r-\frac{i+1}{n})^N.
    \end{align}
    Looking at the second term, we can say that
    \begin{align*}
        \frac{1}{n}\sum_{i=0}^{[(r-r_0)n]-1}(r-\frac{i+1}{n})^N=\frac{r^N}{n}\sum_{i=0}^{[(r-r_0)n]-1}(1-\frac{i+1}{rn})^N=\frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}(1-\frac{i}{rn})^N.
    \end{align*}
    Letting \(x=\frac{i}{rn}\), we trivially know that $x>0$, but we also know that
    \[
        x<1 \iff \frac{i}{rn} < 1 \iff i < rn,
    \]
    and surely enough \(i<[(r-r_0)n]<(r-r_0)n<rn\). This means we can use the bound of \({(1-x)^N>e^{\frac{-Nx}{1-x}}}\), getting
    \begin{align*}
        &\frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}(1-\frac{i}{rn})^N\geq\frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}e^{-N\frac{\frac{i}{rn}}{1-\frac{i}{rn}}}\\
        =&\frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}e^{-N\frac{i}{rn-i}}.
    \end{align*}
    But we know that $i<(r-r_0)n\Rightarrow r_0n<rn-i$, which means that ${e^{-N\frac{i}{rn-i}}>e^{-N\frac{i}{r_0n}}}$. Using this with the formula for a geometric sum, we get
    \begin{align*}
        >&\frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}e^{-N\frac{i}{r_0n}}= \frac{r^N}{n}\sum_{i=1}^{[(r-r_0)n]}\left(e^{\frac{-N}{r_0n}}\right)^i\\
        =&\frac{r^N}{n}\frac{e^{-\frac{[(r-r_0)n]N}{rn}} - e^{-\frac{N}{r_0n}}}{e^{-\frac{N}{r_0n}} - 1}\\
    \end{align*}
    and since $[(r-r_0)n]<(r-r_0)n$, we get
    \begin{align*}
        >& \frac{r^N}{n}\frac{e^{-\frac{(r-r_0)nN}{rn}} - e^{-\frac{N}{r_0n}}}{e^{-\frac{N}{r_0n}} - 1}
        = \frac{r^N}{n}\frac{e^{-\frac{(r-r_0)N}{r}} - e^{-\frac{N}{r_0n}}}{e^{-\frac{N}{r_0n}} - 1}\\
        =& r^N\frac{\frac{1}{n}}{e^{-\frac{N}{r_0n}} - 1}\left(e^{-\frac{(r-r_0)N}{r}} - e^{-\frac{N}{r_0n}}\right).
    \end{align*}
    First, we can quickly say that
    \[
        \lim_{n\rightarrow\infty}\left(e^{-\frac{(r-r_0)N}{r}} - e^{-\frac{N}{r_0n}}\right)=e^{-\frac{(r-r_0)N}{r}} - 1.
    \]
    Second, notice that
    \[
        \lim_{n\rightarrow\infty}\frac{\frac{1}{n}}{e^{-\frac{N}{r_0n}} - 1}=\frac{0^+}{0^-},
    \]
    meaning we can use the L'H\^ospital rule, getting
    \[
    \lim_{n\rightarrow\infty}\frac{\frac{1}{n}} {e^{\frac{-N}{r_0n}} - 1}
    =\lim_{n\rightarrow\infty}\frac{\frac{-1}{n^2}}{\frac{N}{r_0n^2}e^{\frac{-N}{r_0n}}}=\frac{-r_0}{N}\lim_{n\rightarrow\infty}e^{\frac{N}{r_0n}}=\frac{-r_0}{N}.
    \]
    Summarizing, using (10) with the above we get
    \begin{align}
        \sum_{i=0}^{[(r-r_0)n]-1} \left(r_i^{N+1} - r_ir_{i+1}^N\right)\leq& r^{N+1} - r_0^{N+1} -\frac{1}{n}\sum_{i=0}^{[(r-r_0)n]-1}(r-\frac{i+1}{n})^N\nonumber\\
        \leq&r^{N+1} - r_0^{N+1} - \left(r^N\cdot\frac{-r_0}{N}\left(e^{-\frac{(r-r_0)N}{r}} - 1\right)\right)\nonumber\\
        =& r^{N+1} - r_0^{N+1} - r^N\cdot\frac{r_0}{N}\left(1 - e^{-\frac{(r-r_0)N}{r}}\right).
    \end{align}
    Let $f(r_0):=r_0\left(1 - e^{-\frac{(r-r_0)N}{r}}\right)=r_0\left(1 - e^{\frac{N}{r}r_0-N}\right)$. We need to choose $r_0$ so that $f(r_0)$ gets a maximal value, causing the whole expression get the lowest value - resulting in the tightest bound. Let us solve it:
    \begin{align*}
        0&=\frac{d}{dr_0}f(r_0)=\left(1 - e^{\frac{N}{r}r_0-N}\right) - r_0\cdot e^{\frac{N}{r}r_0-N}\cdot \frac{N}{r}\\
        1&=e^{\frac{N}{r}r_0-N}(\frac{N}{r}r_0+1) \Rightarrow e^{N+1}=e^{\frac{N}{r}r_0+1}\left(\frac{N}{r}r_0+1\right).
    \end{align*}
    This type of equation, i.e.
    \[
        x=ye^y,x>0,
    \]
    has a solution $y=W_0(x)$ called a \emph{Lambert W function}. So, we get
    \begin{align*}
        \frac{N}{r}r_0+1=W_0(e^{N+1})\Rightarrow 
        r_0=\frac{r}{N}\left(W_0(e^{N+1})-1\right)
    \end{align*}
    as the value for the maximum. \itai{add check that it is indeed a maximum..} Let us use this equation again to get
    \begin{align*}
        \frac{N}{r}r_0+1=W_0(e^{N+1}) 
        \Rightarrow& \frac{N}{r}r_0-N=W_0(e^{N+1})-N-1\\
        \Rightarrow& e^{\frac{N}{r}r_0-N}=e^{W_0(e^{N+1})}e^{-(N+1)},
    \end{align*}
    but by definition
    \[
        W_0(e^{N+1})e^{W_0(e^{N+1})}=e^{N+1}\Rightarrow W_0(e^{N+1})=\frac{e^{N+1}}{W_0(e^{N+1})},
    \]
    which means that
    \begin{align*}
        e^{\frac{N}{r}r_0-N}=e^{W_0(e^{N+1})}e^{-(N+1)}=\frac{e^{N+1}}{W_0(e^{N+1})}e^{-(N+1)}=\frac{1}{W_0(e^{N+1})}.
    \end{align*}
    From here we can calculate the maximal $f$ value, getting
    \begin{align*}
        f(r_0) =& r_0\left(1 - e^{\frac{N}{r}r_0-N}\right) \\
        \leq& \frac{r}{N}\left(W_0(e^{N+1})-1\right)(1-\frac{1}{W_0(e^{-(N+1)})})\\
        =&\frac{r}{N}(W_0(e^{N+1})-1 - 1+\frac{1}{W_0(e^{-(N+1)})})\\
        =&\frac{r}{N}(W_0(e^{N+1})+\frac{1}{W_0(e^{-(N+1)})}-2)\\
        =&\frac{r}{N}(a_N+\frac{1}{a_N}-2), a_N:=W_0(e^{N+1}).
    \end{align*}
    Returning to (11), we get the expression
    \[
        (11) \leq r^{N+1} - r_0^{N+1} - \frac{r^N}{N}\left[\frac{r}{N}(a_N+\frac{1}{a_N}-2)\right].
    \]
    Remember that after summing the annulus we add the inner $r_0$-ball, which causes our final expression to be
    \[
        \leq r^{N+1} - \frac{r^{N+1}}{N^2}(a_N+\frac{1}{a_N}-2)=r^{N+1}\left(1-\frac{a_N+\frac{1}{a_N}-2}{N^2}\right)
    \]

In order that we do get better evaluations for each lattice, we will need to calculate $\D, \rho, \sum_i \|e_i\|^2$. Notice that, so far, this was done for a general lattice. Each lattice will differentiate by these values.
Using this theorem, we move on to our second contribution for this section.
\begin{thm}[\Lattices collision complexity]\label{collision_complexity_results}
    Fix $\beta=\beta\left(\delta,\epsilon\right)$ and $\theta:=\theta\left(\beta,R\right)$ as used previously. Consider the configuration space  $\C=\B_R$ and some $r$-ball $\B_r\subseteq\C$. Assume $\beta\leq1$ and $R\leq\sqrt{d}$. Then the following bounds hold:
    \begin{enumerate}[topsep=1pt,itemsep=1ex,partopsep=1ex,parsep=1ex]
        \item 
            $\text{CC}\left(\XZ\cap\B_r\right) \approx O\left(\frac{\beta}{\sqrt{d}}\left(\theta\left(\beta,r\right)\sqrt{\frac{d}{4}}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right)$
        \item 
            $\text{CC}\left(\XD\cap\B_R\right) \approx O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\sqrt{\frac{d}{8}-\frac{1}{16}}+\sqrt{\frac{9}{4}d}-\frac{1}{2}\right)^{d-1}\right)$
        \item 
            $\text{CC}\left(\XD\cap\B_R\right) \approx O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right)$
    \end{enumerate}
\end{thm}
\begin{proof}
    First note that assuming our C-Space has $R\leq \sqrt{d}$ is reasonable, because
        \[
        \sqrt{d}\geq R\geq d\beta\Rightarrow\beta\leq\frac{1}{\sqrt{d}},
    \]
    and taking $d=10$ (similarly to our example in theorem~\ref{sample_lower}) requires us to have $\beta\leq 0.32$---similar to the situation in theorem~\ref{sample_lower}, which we explained makes sense.

    
    As mentioned before, we need to determine $\D, \rho, \sum_i \|e_i\|^2$ for each lattice. We get $I_\Lambda$ using all our previous knowledge. Regarding the inradius, Burchard et al.~\cite{burchard2015perimeter} gives us a lower bound for the inradius:
        \[
        \rho \geq Det\left(e_1,\dots,e_n\right) = \sqrt{\det\left(\Lambda\right)}
    \]
    The first term is the determinant of the lattice's base vectors. This is the volume of $F_\Lambda$, which is also a size given to us by Conway~\cite{conway2013sphere} as $\sqrt{\delta\left(\Lambda\right)}$. Finally, the diameter of each lattice will be determined using \cref{lemma:diameter}. Let us move on to our analysis of the lattices. Note that we will be using the shortened version of the approximation
    \[
        I\left(r\right)\approx O\left(\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}\btheta^{d-1}\right).
    \]
    because, as explained above, $Vol\left(\B_1\right)$ goes to 0 very quickly and becomes irrelevant.


    First, we look at $\ZN$. Using theorem \ref{thm:decomp_lattices} we know that our basis is 
    \[
        c\cdot e_i=c\cdot\left(0,\dots,1,\dots,0\right),
    \]
    for $c=\frac{2\beta}{\sqrt{d}}$. Being that the FRs are squares, the inradius is naturally exactly $\rho=\frac{c}{2}=\frac{\beta}{\sqrt{d}}$. 
    For the diameter, in a cube the longest distance is between the two "corners" (i.e. $(0,\dots,0)$ and $(1,\dots,1)$), so our diameter is
    \begin{align*}
        D&=c\|\left(1,\dots,1\right)\|\\
        &=c\sqrt{d}=\frac{2\beta}{\sqrt{d}}\sqrt{d}=2\beta.
    \end{align*}
    Lastly, we can see that 
    \[
        \sum_i \|e_i\|^2 =c^2d=\frac{4\beta^2}{d}d=4\beta^2,
    \]
    which gives us
    \begin{align*}
        &\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}=
        \frac{4\beta^2}{\frac{\beta}{\sqrt{d}}\left(d-1\right)}=\frac{4\beta\sqrt{d}}{d-1}.
    \end{align*}
    We now calculate $\btheta$ using the previous calculations and theorem~\ref{thm:decomp_lattices}, getting
    \begin{align*}
        \btheta &= \frac{r+D-\rho}{\beta}f_{\ZN}\\
        &= \frac{r+2\beta - \frac{\beta}{\sqrt{d}}}{\beta}\frac{\sqrt{d}}{2}\\
        & = \frac{r}{\beta}\frac{\sqrt{d}}{2}+\sqrt{d}-\frac{1}{2}.
    \end{align*}
    With these two calculations we get the final complexity
    \begin{align*}
        I\left(r\right)&\approx O\left(\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}\btheta^{d-1}\right)\\
        &=O\left(\frac{4\beta\sqrt{d}}{d-1}\left(\frac{r}{\beta}\frac{\sqrt{d}}{2}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\frac{\sqrt{d}}{2}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\theta\left(\beta,r\right)\sqrt{\frac{d}{4}}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right).
    \end{align*}
    Next we look at $\DN$, using the base we got from theorem~\ref{thm:decomp_lattices}---the base vectors are $c\left(1,0,\dots,0\right)$ to $c\left(0,\dots,0,1,0\right)$ and $c\left(\frac{1}{2},\dots,\frac{1}{2}\right)$, with $c=\frac{4\beta}{\sqrt{2d-1}}$ (WLOG, $d$ is odd). We know from Conway~\cite{conway2013sphere} (page 120) that $\rho=\frac{1}{2}$, so after rescaling the space, we get $\frac{2\beta}{\sqrt{2d-1}}$. Next, our base vectors are all in the $[\mathbb{R}_+,\dots,\mathbb{R}_+]$ quadrant, so naturally (similar to the "cube" case!) the longest distance between any two points in the FR has to be between $\overset{\rightarrow}{0}$ and $\sum_i e_i$, getting that our diameter is
    \begin{align*}
        D&=c\|\left(1,0,\dots,0\right) + \dots + \left(0,\dots,1,0\right)+\left(\frac{1}{2},\dots,\frac{1}{2}\right)\|\\
        &=\frac{c}{2}\|\left(3,3,\dots,3,1\right)\|=\frac{c}{2}\sqrt{9\left(d-1\right)+1}\\
        &=\frac{c}{2}\sqrt{9d-8}=2\beta\sqrt{\frac{9d-8}{2d-1}}.
    \end{align*}
    Lastly, we can see that 
    \begin{align*}
        &\sum_i \|e_i\|^2 =c^2\left(d-1\right)+c^2\frac{d}{4}=\frac{c^2}{4}\left(5d-4\right)\\
        &=\frac{4\beta^2}{2d-1}\left(5d-4\right)=
        4\beta^2\frac{\left(5d-4\right)}{2d-1},
    \end{align*}
    which gives us
    \begin{align*}
        &\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}=
        \frac{4\beta^2\frac{\left(5d-4\right)}{2d-1}}{\frac{2\beta}{\sqrt{2d-1}}\left(d-1\right)}\\
        &=2\beta\frac{\sqrt{2d-1}\left(5d-4\right)}{\left(d-1\right)\left(2d-1\right)}.
    \end{align*}
    We now calculate $\btheta$ using the previous calculations and theorem~\ref{thm:decomp_lattices}, getting
    \begin{align*}
        \btheta &= \frac{r+D-\rho}{\beta}f_{\DN}\\
        &= \frac{r+2\beta\sqrt{\frac{9d-8}{2d-1}}-\frac{2\beta}{\sqrt{2d-1}}}{\beta}\frac{\sqrt{2d-1}}{4}\\
        &= \frac{r}{\beta}\frac{\sqrt{2d-1}}{4}+\sqrt{\frac{9}{4}d-2}-\frac{1}{2}.
    \end{align*}
    With these two calculations we get the final complexity
    \begin{align*}
        I\left(r\right)&\approx O\left(\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}\btheta^{d-1}\right)\\
        &=O\left(2\beta\frac{\sqrt{2d-1}\left(5d-4\right)}{\left(d-1\right)\left(2d-1\right)}\left(\frac{r}{\beta}\frac{\sqrt{2d-1}}{4}+\sqrt{\frac{9}{4}d-2}-\frac{1}{2}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta\sqrt{d-\frac{1}{2}}}{d-1}\left(\frac{r}{\beta}\sqrt{\frac{d}{8}-\frac{1}{16}}+\sqrt{\frac{9}{4}d-2}-\frac{1}{2}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\sqrt{\frac{d}{8}-\frac{1}{16}}+\sqrt{\frac{9}{4}d}-\frac{1}{2}\right)^{d-1}\right)\\
    \end{align*}
    Finally, we look at $\AN$. The base we got from theorem~\ref{thm:decomp_lattices} uses the vectors $\left(c,-c,0\dots,0\right)$ to $\left(c,0,\dots,0,-c,0\right)$ and $\left(\frac{-cd}{d+1},\frac{c}{d+1}\dots,\frac{c}{d+1}\right)$ for $c=\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta$. As our method relies on the diameter of the FR, and this FR's diameter is roughly $O\left(d\right)$ (due to the first coordinate being filled on all vectors) in contrast to $O\left(\sqrt{d}\right)$ in the other lattices, we will use a different FR: we will use
    \begin{align*}
        v_1&=\left(c,-c,0,\dots,0\right)\\
        v_2&=\left(0,c,-c,\dots,0\right)\\
        v_d&=\left(0,\dots,0,c,-c,0\right)\\
        w&=\left(\frac{-cd}{d+1},\frac{c}{d+1}\dots,\frac{c}{d+1}\right).
    \end{align*}
    This spans the same lattice, as we can recreate the original basis from this (by adding $i$ vectors in a row).  To calculate the inradius, we refer to pages LXII and 10 of Conway~\cite{conway2013sphere}, citing that 
    \[
        \rho = \left(\delta\sqrt{\det{\Lambda}}\right)^{\frac{1}{d}},
    \]
    where $\delta$ is a parameter given in page 115 which is independent of choice of basis. The only thing left to calculate, then, is $\det{\Lambda}=det\left(G_\Lambda G_\Lambda^t\right)$. Let us calculate it:
    \begin{align*}
        \det{\Lambda}&=det\left(G_\Lambda G_\Lambda^t\right)\\
        &=\det\begin{pmatrix}
            1 & -1 &  0  & \dots & 0 & 0 & 0 \\
            0 & 1  &  -1 & \dots & 0 & 0 & 0 \\
            . & .  &  . & \dots & . & .  & .\\
            0 & 0  &  0 & \dots & 1 & -1 & 0\\
            \frac{-d}{d+1} & \frac{1}{d+1} & \frac{1}{d+1} & \dots & \frac{1}{d+1} & \frac{1}{d+1} & \frac{1}{d+1}
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & \dots & 0 & 0 & \frac{-d}{d+1} \\
            -1 & 1 & \dots & 0 & 0 & \frac{1}{d+1} \\
            0 & -1 & \dots & 0 & 0 & \frac{1}{d+1} \\
            . & . & \dots & . & .  & .\\
            0 & 0 & \dots & 0 & -1 & \frac{1}{d+1}\\
            0 & 0 & \dots & 0 & 0 & \frac{1}{d+1}
        \end{pmatrix}\\
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            -1 & 2  &  -1 & 0 & 0 &\dots & 0 & 0 & 0\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            -1 & 0  &  0 & 0 & 0 &\dots & 0 & 0 & \frac{d}{d+1}\\
        \end{pmatrix},\\
    \end{align*}
    now add all $d-1$ rows to the last row:
    \begin{align*}
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            -1 & 2  &  -1 & 0 & 0 &\dots & 0 & 0 & 0\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}.
    \end{align*}
    We now cancel out the first non-zero coordinate in the second row using the first row, and then multiply it by an integer so it becomes an integer row, getting
    \begin{align*}
        &=\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 1.5  &  -1 & 0 & 0 &\dots & 0 & 0 & 0.5\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
        &=\frac{1}{2}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & -1  &  2 & -1 & 0 & \dots & 0 & 0 & 0\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}.
    \end{align*}
    we keep going like this recursively
    \begin{align*}
            &=\frac{1}{2}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  \frac{4}{3} & -1 & 0 & \dots & 0 & 0 & \frac{-1}{3}\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & -1 & 2 & 0\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix},
    \end{align*}
    until ultimately, we reach the last row, getting
    \begin{align*}
        &=\frac{1}{\left(d-1\right)!}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  4 & -3 & 0 & \dots & 0 & 0 & -1\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & 0 & d & -1\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 1 & \frac{d}{d+1}-1\\
        \end{pmatrix}\\
        &=\frac{1}{\left(d-1\right)!}\det\begin{pmatrix}
            2 & -1 &  0  & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 3  &  -2 & 0 & 0 &\dots & 0 & 0 & -1\\
            0 & 0  &  4 & -3 & 0 & \dots & 0 & 0 & -1\\
            . & .  &  . & . & . & \dots & . & . & .\\
            0 & 0  &  0 & 0 & 0 & \dots & 0 & d & -1\\
            0 & 0  &  0 & 0 & 0 &\dots & 0 & 0 & \frac{d}{d+1}-1+\frac{1}{d}\\
        \end{pmatrix}\\
        &=\frac{1}{\left(d-1\right)!}\cdot d!\left(\frac{1}{d\left(d+1\right)}\right)
        =\frac{1}{d+1}.
    \end{align*}    
    Looking at page 115 of~\cite{conway2013sphere}, we see that the determinant of the classic basis is similar to our FR's, which means we can just use the inradius for the classic basis, which is quoted to be
    \[
        \rho = \frac{1}{2}\sqrt{\frac{d}{d+1}},
    \]
    and after rescaling the space we get
    \[
        \rho=\frac{1}{2}\sqrt{\frac{d}{d+1}}\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta
        =\frac{1}{2}\sqrt{\frac{12}{d+2}}\beta.
    \]
    Next, to calculate the diameter we need to consider distances between all binary sums ("in the sum or not") of base vectors. If we add $v_i,v_{i+1}$ then the $i+1$-coordinate cancels out and we get $\left(\dots,1,0,-1,\dots\right)$. This lowers the possible contribution of this coordinate, as it only appears in these vectors and in $w$, and in $w$ it is $\frac{-1}{d+1}$---resulting in a total of $\frac{1}{d+1}$ (in absolute value). This is in contrast to adding $v_i,v_j$ where $j\geq i+2$, so we get $\left(\dots,1,-1,\dots,1,-1,\dots\right)$. When adding it to $w$ now, we get $1-\frac{1}{d+1}>\frac{1}{d+1}$ (in absolute value). So we see that in general, the longest difference between FR points we can get is when adding $u=w+v_1+v_3+v_5+\dots$, getting
    \begin{align*}
        D^2&=\|u\|^2=c^2\|\left(1,-1,1,-1,\dots,-1,0\right)+\left(\frac{-d}{d+1},\frac{1}{d+1},\dots,\frac{1}{d+1}\right)\|^2\\
        &=c^2\|\left(1-\frac{d}{d+1},-\left(1-\frac{1}{d+1}\right),1-\frac{1}{d+1},\dots,-\left(1-\frac{1}{d-1}\right),\frac{1}{d+1}\right)\|^2\\
        &=c^2\|\left(\frac{1}{d+1},-\left(1-\frac{1}{d+1}\right),1-\frac{1}{d+1},\dots,-\left(1-\frac{1}{d+1}\right),\frac{1}{d+1}\right)\|^2\\
        &=c^2\left(\frac{2}{\left(d+1\right)^2}+\frac{d}{2}\left(1-\frac{1}{d+1}\right)^2\right)\\
        &=\frac{1}{2}c^2\frac{4+d^3}{\left(d+1\right)^2},\\
        \Rightarrow D&=\frac{c}{\sqrt{2}}\frac{d}{d+1}\sqrt{d+\frac{4}{d^2}}\leq\frac{c}{\sqrt{2}}\sqrt{d+2}\\
        &=\frac{1}{\sqrt{2}}\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta\sqrt{d+1}=\sqrt{\frac{6\left(d+1\right)}{d}}\beta\leq\sqrt{12}\beta.
    \end{align*}
    Lastly, we can see that 
    \begin{align*}
        &\sum_i \|e_i\|^2 =c^2\left(d\cdot\left(2\right)+1\cdot\left(\frac{d^2}{\left(d+1\right)^2}+d\frac{1}{\left(d+1\right)^2}\right)\right)\\
        &=c^2\left(2d+\frac{d}{d+1}\right)
        = \frac{12\left(d+1\right)}{d\left(d+2\right)}\beta^2\left(2d+\frac{d}{d+1}\right)\\
        &\leq\frac{12}{d}\beta^2\left(2d+1\right)\leq 36\beta^2,
    \end{align*}
    which gives us
    \begin{align*}
        &\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}\leq
        \frac{36\beta^2}{\frac{1}{2}\sqrt{\frac{12}{d+2}}\beta\left(d-1\right)}\\
        &=\frac{36\beta\sqrt{d+2}}{\sqrt{3}\left(d-1\right)}.
    \end{align*}
    We now calculate $\btheta$ using the previous calculations and theorem~\ref{thm:decomp_lattices}, getting
    \begin{align*}
        \btheta &= \frac{r+D-\rho}{\beta}f_{\DN}\\
        &\leq \frac{r+\sqrt{12}\beta-\frac{1}{2}\sqrt{\frac{12}{d+2}}\beta}{\beta}\sqrt{\frac{d\left(d+2\right)}{12\left(d+1\right)}}\\
        &= \frac{r}{\beta}\sqrt{\frac{d\left(d+2\right)}{12\left(d+1\right)}}+\sqrt{\frac{d\left(d+2\right)}{\left(d+1\right)}}-\frac{1}{2}\sqrt{\frac{d}{\left(d+1\right)}}\\
        &=\sqrt{\frac{d+2}{d+1}}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\sqrt{\frac{d}{d+2}}\right).
    \end{align*}
    Remember that we have seen in corollary~\ref{cor:specific_sample_complexity} that $\left(\sqrt{\frac{d+2}{d+1}}\right)^d\rightarrow\sqrt{e}$. With  the calculations above, we get the final complexity
    \begin{align*}
        I\left(r\right)&\approx O\left(\frac{\sum_i\|e_i\|^2}{\rho\left(d-1\right)}\btheta^{d-1}\right)\\
        &=O\left(\frac{36\beta\sqrt{d+2}}{\sqrt{3}\left(d-1\right)}\left(\sqrt{\frac{d+2}{d+1}}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\sqrt{\frac{d}{d+2}}\right)\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\sqrt{\frac{d+2}{d+1}}\right)^{d-1}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\sqrt{\frac{d}{d+2}}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\sqrt{\frac{d}{d+2}}\right)^{d-1}\right)\\
        &=O\left(\frac{\beta}{\sqrt{d}}\left(\frac{r}{\beta}\sqrt{\frac{d}{12}}+\sqrt{d}-\frac{1}{2}\right)^{d-1}\right)\\
    \end{align*}
\end{proof}
\subsection*{Comparing collision 
complexity for \Lattices}
The collision complexity of the lattices seems to be naturally "carried over" from the sample complexity. In this aspect, all conclusions we drew previously apply here: $\AN$ still rises as having the least total edge length, resulting in a lower amount of collision checks. The higher degree of the power in comparison to sample complexity ($d-1$ against $d-2$) means these differences are even more accentuated. 

\subsection*{Comparing $\AN$ to random sample sets}
 
 First thing we compared was the classic $PRM$ algorithm to our setting, with the purpose of seeing how $\AN$ stands against random sample sets, which are known to generally give good enough results. This test was conducted with a regular PRM algorithm, with the "H" map, at $d=6$ using 3 moving disks, with some augments that we will soon list. Just to note, we used a much higher clearance of $\delta=25$, which was found to empirically work with $\AN$. This high clearance value allows us also to see another facet of the LPRM issue: clearance rates are pivotal to having feasible runtimes in MPs. The following method was used in this test section:
 \begin{enumerate}
     \item Each of the three robots has a set location in a corner of the "H" map. 
     \item We create a test set this way: shift every robot a bit, into a legal position, by $N$ steps. This creates $N^3$ sets. Permute the three robots (we plan in a $d=3\cdot2=6$, so these are different tests), getting 6 test. Overall: $6N^3$ tests. We chose $N=3$ for 162 tests.
     \item On each of these tests, run two problems: first, a run of $\AN$ with LPRM. On finishing, save the length and solution time and get the number of samples used. Second, a run of PRM using a fixed amount of samples---the number we got from the LPRM run.
     \item The second test is then run in two different modes. First, we use LPRM's radius, the one used from Dayan et al.~\cite{dayan2023near}. Then we use PRM*'s radius. 
     \item What we look for are relative qualities (our data divided by their data) and success rates, all averaged over the 162 tests.
 \end{enumerate}
 Results: (all done at $\delta=25$)
 \begin{enumerate}
     \item \underline{Both with same radius:} 
     \begin{enumerate}
        \item Success rate: $\AN$ with LPRM: 100\%, random with PRM: $\epsilon=10$ with success=42\%, $\epsilon=5$ with success=53\%, $\epsilon=2$ with success=100\%.
        \item Total time quality, $\frac{\text{LPRM time}}{\text{rnd PRM time}}$: $\epsilon=10$ with quality=7.77, $\epsilon=5$ with quality=10.87, $\epsilon=2$ with quality=11.93.
        \item $A^*$ \textbf{run only} time quality, $\frac{A^*-\text{LPRM time}}{A^*-\text{rnd PRM time}}$: $\epsilon=10$ with quality=1.96, $\epsilon=5$ with quality=2.39, $\epsilon=2$ with quality=4.55.
        \item Length quality, $\frac{\text{LPRM length}}{\text{rnd PRM time}}$: $\epsilon=10$ with quality=1.25, $\epsilon=5$ with quality=1.22, $\epsilon=2$ with quality=1.13.
     \end{enumerate}
     \item \underline{Each with its own radius:}
          \begin{enumerate}
         \item Success rate: all 100\% at all values.
         \item Total time quality, $\frac{\text{LPRM time}}{\text{rnd PRM time}}$: $\epsilon=10$ with quality=1.12, $\epsilon=5$ with quality=1.3, $\epsilon=2$ with quality=3.96.
        \item $A^*$ \textbf{run only} time quality, $\frac{A^*-\text{LPRM time}}{A^*-\text{rnd PRM time}}$: $\epsilon=10$ with quality=1.2, $\epsilon=5$ with quality=1.64, $\epsilon=2$ with quality=4.13.
         \item Length quality, $\frac{\text{LPRM length}}{\text{rnd PRM time}}$: $\epsilon=10$ with quality=1.25, $\epsilon=5$ with quality=1.22, $\epsilon=2$ with quality=1.13.
     \end{enumerate}
 \end{enumerate}
%  \begin{figure*}[!h]
%   \centering
%   \subfloat[this image is a placeholder]{
%     \includegraphics[width=0.3\textwidth]{Images/CC_relative_by_e.png}
%     \label{fig:exp_3body:hmap}}
%   \hfil
%   \subfloat[this image is a placeholder]{
%     \includegraphics[width=0.3\textwidth]{Images/CC_relative_by_e.png}
%     \label{fig:exp_3body:rndpoly}}
%   \caption{The scenarios we used to test our sample set, in each map we wanted each disk-robot to switch places with the next neighbor.}
%   \label{fig:exp_3body}
% \end{figure*}

\emph{Second section conclusions:} there are several conclusions we can draw. 
\begin{enumerate}
    \item First, that if PRM isn't given a good enough sampling radius, then it will not find a solution a meaningful percentage of the time. This is in contrast to LPRM, which is deterministically guaranteed to get a result, after you pass certain sample density "threshold".
    \item Second, we can see that while rnd-PRM had a better average path quality, it was not substantial (at worse 2x longer).
    \item Lastly, it is clear that rnd-PRM is quicker than LPRM, which is to be expected: we have much more edges in LPRM than in rnd-PRM. This is true also in case we only count the $A^*$ runtime only (without the time to build the graph). In $\epsilon=10$ the runtimes do get very close, but in realistically lower $\epsilon$ values rnd-PRM is much quicker again.
\end{enumerate}

Overall, it is notable that LPRM has not demonstrated a significant improvement over rnd-PRM, at least in our setting. Its strengths lay in its theoretical promise to achieve a good result in finite time, in contrast to rnd-PRM. We will demonstrate in the next section the promise of the $\AN$ lattice over the others.

\subsection*{Comparing runtimes between the lattices}
 
 The second set of tests compared the lattices themselves: \Lattices. This version of LPRM saw two important improvements to the runtime, that greatly improved it.
 \begin{enumerate}
     \item The first one was to move to a fully "lazy" implementation, on a graph that already implicitly exists: we know in advance that each point in the lattice has $2d$ neighbors, and we know their location. Therefor we don't have to build the graph at all, and just consider the implicit neighbors. That is in contrast to a graph with random sampling, in which the graph is incrementally built as you add samples, which makes comparing to an implicit graph impossible.
     \item The second was to recognize that we can calculate a points neighbor's in a general ball, in advance, and then just have a point's neighbors be these same points - only shifted! This is due to the regularity of the lattice structure.
 \end{enumerate}
The results, each averaged over 6 different case, are as follows:

\begin{table*}[t]
\centering
\caption{LPRM vs RPRM, Both with the same radius (from Dayan~\cite{dayan2023near})}
\begin{tabularx}{\linewidth}{|X|X|X|X||X|X|X|X|X|X|X|X|X|} \hline
 Map & ID & Scenario & NN &
 \multicolumn{4}{|c|}{Construction times [S]} &
 \multicolumn{4}{|c|}{$A^*$ times [S]} &
  Success
 \\
 \hline
 & & & & $\ZN$ & $\DN$ & $\AN$ & Random & $\ZN$ & $\DN$ & $\AN$ & Random & \\
 \hline
 \multirow{4}{*}{K}
  & 1 & 2-Swap & No  & $0.0001$    &$9E(-5)$&   $9E(-5)$ & $2.308$ & $5.505$ & $2.212$ & \cellcolor{green} $1.214$ & \cellcolor{pink} $1.569$ & 100\%\\
  \cline{2-13}
  & 1 & 2-Swap & Yes  & $7.643$    &$3.897$&   $3.087$ & $2.308$ & $6.140$ & $2.468$ & \cellcolor{green}$1.363$ & \cellcolor{pink}$1.569$ & 100\%\\
  \cline{2-13}
  & 2& 3-Swap   & No & $0.0042$    &$0.0014$&   $0.0032$ & NA & $78.493$ & $10.391$ &\cellcolor{green} $3.786$ & \cellcolor{pink}NA & 0\% \\
    \cline{2-13}
  & 2& 3-Swap   & Yes & $0.0042$    &$0.0014$&   $0.0032$ & NA & $78.493$ & $10.391$ &\cellcolor{green} $3.786$ & \cellcolor{pink}NA & 0\% \\
 \hline
  \multirow{4}{*}{UM}
  & 3 & 2-Swap  & No & $0.00028$    &$0.00013$&   $0.00014$ & $1.061$ & $54.638$ & $17.463$ & \cellcolor{green}$7.316$ & \cellcolor{pink}$8.650$ & 100\% \\
  \cline{2-13}
    & 3 & 2-Swap  & Yes & $3.277$    &$1.799$&   $1.278$ & $1.061$ & $64.355$ & $19.701$ &\cellcolor{green} $8.346$ & \cellcolor{pink}$8.650$ & 100\% \\
  \cline{2-13}
  & 4& 3-Swap  & No & $0.00637$    &$0.001851$&   $0.000691$ & $40.972$ & $506.254$ & $25.898$ &\cellcolor{green} $7.029$ & \cellcolor{pink}$21.380$ & 95\% \\
    \cline{2-13}
    & 4& 3-Swap  & Yes & $437.237$    &$124.184$&   $68.451$ & $40.972$ & $912.857$ & $56.335$ & \cellcolor{pink}$21.774$ & \cellcolor{green}$21.380$ & 95\% \\
 \hline
 \multirow{4}{*}{RP}
  & 5& 2-Swap  & No & $0.000179$    &$0.000172$&   $0.450$ & $0.465$ & $5.532$ & $1.954$ & \cellcolor{green}$1.612$ & \cellcolor{pink}$2.323$ & 100\%\\
  \cline{2-13}
    & 5& 2-Swap  & Yes & $1.472$    &$0.770$&   $0.554$ & $0.465$ & $6.335$ & $2.185$ & \cellcolor{green}$1.672$ & \cellcolor{pink}$2.323$ & 100\%\\
  \cline{2-13}
  & 6 & 3-Swap  & No & $0.0062$    &$0.0020$&   $0.0007$ & $21.227$ & $812.614$ & $7.207$ & \cellcolor{pink}$7.347$ & \cellcolor{green}$5.833$ & 100\%\\
    \cline{2-13}
  & 6 & 3-Swap  & Yes & $225.829$    &$61.552$&   $33.688$ & $21.227$ & $1127.65$ & $9.771$ & \cellcolor{pink}$10.898$ & \cellcolor{green}$5.833$ & 100\%\\
 \hline
\end{tabularx}
\end{table*}

\begin{enumerate}
    \item \textbf{Test \#1}: d=6, $\delta=9, \epsilon=10$, "H" map, 3 disk robots $(x,y)$. Results:
    \begin{enumerate}
        \item Zn average total runtime: 1141 seconds.
        \item Dn* average total runtime: 55 seconds.
        \item An* average total runtime: 23 seconds.
    \end{enumerate}
    \underline{Conclusion:} $\AN$ runs 50x faster than $\ZN$.
    \item \textbf{Test \#2}: d=6, $\delta=??, \epsilon=10$, "random polygons" map, 3 disk robots $(x,y)$. Results:
    \begin{enumerate}
        \item Zn average total runtime: ?? seconds.
        \item Dn* average total runtime: ?? seconds.
        \item An* average total runtime: ?? seconds.
    \end{enumerate}
    \underline{Conclusion:} $\AN$ runs ?? faster than $\ZN$.
\end{enumerate}
\emph{First section conclusions:} it is clear from these comparisons that $\AN$ performs much better than $\ZN$. Another point to notice is the differences between the lattices: if we wanted an even lower clearance or stretch factor, $\ZN$'s runtime would become infeasible to run. We believe that if you are considering using a deterministic sample set, for any purpose, the set $\AN$ has demonstrated a substantial superiority over other lattices---making it a good choice to use.