\section{Introduction}
% 
Motion planning is a key ingredient in autonomous robotic systems, whose aim is computing collision-free trajectories for a robot operating in environments cluttered with obstacles~\cite{lavalle2006planning}. 
Over the years, various approaches have been developed for tackling the problem, including potential fields~\cite{luo2024potential}, geometric methods~\cite{halperin2017algorithmic}, and optimization-based approaches~\cite{SchulmanDHLABPPGA14,MalyutaEtAl2022,MarcucciEA23}. %, and sampling-based planners~\cite{}. 
In this work, we focus on sampling-based planners (SBPs), which aim to capture the structure of the robot's free space through graph approximations that result from configuration sampling (typically in a random fashion) and connecting nearby samples. 
SBPs have enjoyed popularity in recent years due to their relative scalability, in terms of the number of robot degrees of freedom (DoFs), and the ease of their implementation~\cite{OrtheyCK24}. 

\begin{figure*}[h!]
  \centering
  \subfloat[$\X_{\dZ_2}^{\delta,\epsilon}$ sample set.]{
    \includegraphics[width=0.27\textwidth, trim={2.2cm 1.7cm 0.9cm 1.0cm},clip]{Images/ZN_2D.png}
    %\label{fig:2d_lattices:z}
    }
  \hfil
  \subfloat[$\X_{D_2^*}^{\delta,\epsilon}$ sample set.]{
    \includegraphics[width=0.27\textwidth, trim={2.2cm 1.8cm 0.9cm 1.0cm},clip]{Images/DN_2D.png}
    %\label{fig:2d_lattices:d}
    }
  \hfil
  \subfloat[$\X_{A_2^*}^{\delta,\epsilon}$  sample set.]{
    \includegraphics[width=0.27\textwidth, trim={2.4cm 1.7cm 0.9cm 1.0cm},clip]{Images/AN_2D.png}
    %\label{fig:2d_lattices:a}
    }
  \caption{Sample sets within a fixed disc in $\dR^2$, derived from the lattices $\dZ^2, D_2^*$ and $A^*_2$, which yield \decomp guarantees for the same values of $\delta$ and $\eps$. The set $\X_{\dZ_2}^{\delta,\epsilon}$ can be viewed as a tessellation of space using cubes. The set $\X_{D_2^*}^{\delta,\epsilon}$ is obtained by placing a (rescaled) standard grid, and then placing another point in the middle of each cube. The set $\X_{A_2^*}^{\delta,\epsilon}$ can be viewed as a rescaled hexagonal grid as each point is surrounded by a hexagon whose vertices are points in the set. Note that the density of $\X_{\dZ^2}^{\delta,\eps}$ and $\X_{D^*_2}^{\delta,\eps}$ is the same, and higher than the density of $\X_{A^*_2}^{\delta,\eps}$.}
  \label{fig:2d_lattices}
\end{figure*}

Another key benefit is the ability of SBPs to escape local minima (unlike potential fields) and global solution guarantees (in contrast, optimization-based approaches~\cite{SchulmanDHLABPPGA14}, which typically provide only local guarantees). Earlier work on the theoretical foundations of SBPs has focused on deriving probabilistic completeness (PC) guarantees for methods such as PRM~\cite{kavraki1996probabilistic} or RRT~\cite{LaVKuf01,KunzS14,Kleinbort.Solovey.ea.19}. PC implies that the probability of a given planner finding a solution (if one exists) converges to one as the number of samples tends to infinity. The work of~\citet{karaman2011sampling} initiated studying the quality of the solution returned by SBPs. Specifically, they introduced the planners PRM* and RRT*, and proved that the solution length of those planners converges to the optimum as the number of samples tends to infinity---a property called asymptotic optimality (AO). Subsequent work has introduced even more powerful AO planners for geometric~\cite{JSCP15,GammellBS20} and dynamical~\cite{HauserZ16,LiETAL16} systems.

Unfortunately, the practical relevance of the aforementioned theoretical findings remains limited due to the lack of meaningful finite-time implications. Specifically, when a solution is obtained using a finite number of samples, it is unclear to what extent its quality can be improved with additional computation time. Moreover, in cases where no solution is returned, it is uncertain whether a solution does not exist or if the algorithm simply failed to find one. Developing finite-time bounds through randomized sampling continues to be a significant challenge~\cite{DobsonMB15,shaw2024towards}.

Deterministic sampling methods such as grid sampling or Halton sequences~\cite{lavalle2006planning}, where samples are generated according to a geometric principle, can improve the performance of SBPs in practice and simplify the algorithm analysis. Specifically, some deterministic sampling procedures have a significantly lower dispersion than uniform random sampling, which implies that the former requires fewer samples to cover the search space to a desired resolution~\cite{janson2018deterministic}. 
Recently, Tsao et al.~\cite{tsao2020sample} have leveraged deterministic sampling to disrupt the asymptotic analysis paradigm by introducing a significantly stronger notion than AO, called \decomps, that yields finite-time guarantees for PRM-based algorithms such as PRM*~\cite{karaman2011sampling}, FMT*~\cite{JSCP15}, BIT*~\cite{GammellBS20}, and GLS~\cite{MandalikaCSS19}. Informally, a \emph{finite} sample set is \decomp for a given approximation factor $\eps>0$ and clearance parameter $\delta>0$, if the corresponding planner returns a solution whose length is at most $(1+\eps)$ times the length of the shortest $\delta$-clear solution. If no solution is found using a \decomp sample set then no solution of clearance $\delta$ exists. 

The work of~\citet{tsao2020sample} derived a relation between \decomps and geometric space coverage to obtain lower bounds on the number of samples necessary to achieve \decomps, as well as upper bounds accompanied with explicit (deterministic) sampling distributions. A follow-up work by~\citet{dayan2023near} has introduced an even more compact \decomp sample distribution that is more efficient than the one proposed in~\cite{tsao2020sample} or rectangular grid sampling. In particular, the staggered grid~\cite{dayan2023near} consists of two shifted and rescaled copies of the rectangular grid (see Figure~\ref{fig:2d_lattices} and Figure~\ref{fig:3d_lattices}). 

However, the work~\cite{dayan2023near} still leaves a significant gap between the lower bound in~\cite{tsao2020sample} and the upper bound obtained with the staggered grid. In practice, this gap limits the applicability of the \decomps theory to relatively low dimensions (up to dimension 6) due to the large number of samples currently needed to satisfy this property, which can lead to excessive running times. 

\vspace{5pt}
\noindent \textbf{Contribution.} In this work, we develop a theoretical framework for obtaining highly-efficient \decomp sample sets by leveraging the foundational theory of lattices\footnote{Lattices are point sets exhibiting a regular geometric structure, which are obtained by transforming the integer lattice $\dZ^d$. For instance, the aforementioned rectangular grid and the staggered grid can be viewed as lattices.}~\cite{conway2013sphere}, which has been instrumental in diverse areas from number theory~\cite{siegel_geometry_numbers}, coding theory~\cite{ebeling2013lattices}, and crystallography~\cite{sands1994introduction}. Specifically, we show that lattices can be transformed to obtain \decomp sample sets (Theorem~\ref{thm:decomp_lattices}) and develop tight theoretical bounds on their size (Theorem~\ref{thm:general_sample_complexity}), which allows to compare between different sample sets qualitatively. 
Using this machinery, we not only refine and generalize previous results on the staggered grid~\cite{dayan2023near} but also introduce a new highly efficient \decomp sample set that is based on the $\AN$ lattice, which is famous for its minimalist coverage properties~\cite{conway2013sphere}. We also initiate the study of a new property, which estimates the computational cost resulting from using a given sample set in a more informative manner than sample complexity. In particular, the property called collision-check complexity captures the amount of collision checks, which is typically a computational bottleneck.

From a practical perspective, when solving motion-planning problems using lattice-based sample sets, we show that our $\AN$-based sample sets can result in at least order-of-magnitude improvement in terms of running time over staggered-grid samples and two orders of magnitude improvements over rectangular grids. Moreover, $\AN$-based sample sets are vastly superior in practice to the widely-used uniform random sampling, which is evident in improved running times, success rates, and solution quality.

\vspace{5pt}
\noindent \textbf{Organization.} In Section~\ref{sec:preliminaries} we review basic definitions on motion planning and \decomps, and formally define our objectives. In Section~\ref{sec:lattices}, we develop a general tool for transforming lattices into \decomp sample sets. We obtain sample-complexity bounds for lattice-based sample sets in Section~\ref{sec:sample_complexity}, and generalize those bounds to collision-check complexity in Section~\ref{sec:collision_complexity}. We evaluate the practical implications of our theory in Section~\ref{sec:experiments}, and conclude with a discussion of limitations and future directions in Section~\ref{sec:future}.


% \itai{Added the intro. used some from the thesis-proposal, added different stuff at the end}
%  the field of autonomous robots, the problem of getting a robot “from point A to point B” can be divided into three general stages: estimating the robot's position, planning the robot's path and controlling the robot. We use estimation methods (like the Kalman Filters) to understand where we are in the world, we use planning methods to figure out how to reach the goal, and we use control methods (like PID) to follow the planned path during execution.


% Focusing on the planning part of the problem, instead of using the \emph{workspace} of the robot it is convenient to use a representation of it called a \emph{configuration space}---a parameterization of the robot’s position in space, which turns the set of points defined as a \emph{robot} to a single-point robot. A quick example would be thinking of a polygon in the workspace as three parameters: $(x,y)\in \mathbb{R}^2$ for its location, and $\theta\in[0,2\pi]$ for its rotation, which means the configuration space is $\mathbb{R}^2\times S^1$. Furthermore, we use the term \emph{free space} in both contexts to describe the area of the space with no obstacles. 


% Even though using configuration spaces is much more convenient in terms of the robot being a single point, it quickly becomes apparent that even simple configuration spaces of dimensions $d\geq 3$ can be challenging to properly describe (due to the need to describe the obstacles in the new space, among other reasons). Thus, instead of explicitly representing the whole configuration space, methods were developed to sample the space: sampling-based approaches aim to approximate the space via a graph structure that is induced by sampled configurations. This can drastically reduce the computational effort of path planning.


% One of the most widely used sampling-based algorithms is the \emph{probabilistic roadmap method} (PRM)~\cite{kavraki1996probabilistic}. This approach generates (typically random) samples across the space, and connects nearby samples while checking for collisions with obstacles, which gives rise to a graph data structure---a path between two nodes in the graph yields a collision-free path for the robot connecting between the two configurations corresponding to the end-point nodes. PRM has the theoretical guarantee to return a path with a probability tending to 1 if enough samples are generated~\cite{laddgeneralizing}. 


% Another well known sampling-based planner is the \emph{rapidly-exploring random trees} (RRT)~\cite{lavalle1998rapidly}: It randomly expands towards nearby samples in space, creating in the process a “tree” structure that eventually finds a path to the goal~\cite{kleinbort2018probabilistic}. Later, a notion of \emph{asymptotically optimal} (AO) algorithms was introduced: with infinite samples, the algorithm can converge to an \textbf{optimal} path. Both PRM and RRT  were expanded to AO versions (PRM*, RRT*) in a paper by Karaman et al.~\cite{karaman2011sampling}. RRT itself had seen many expansions, including dRRT/dRRT* to apply to multiple robots~\cite{solovey2015finding, dobson2017scalable}.

% Approximately optimal methods were demonstrated using deterministic sample sets, achieving good results in finite time, in Dayan et al.'s paper~\cite{dayan2023near}, demonstrating superior results over random sets---although those improvements diminish as the desired approximation factor of the optimal path lowers.

% Still, all these methods have a main limitation: the number of points required to guarantee finding a path rises exponentially with the robot's degrees of freedom~\cite{tsao2020sample}.


% Dayan et al.~\cite{dayan2023near}, using a "staggered grid" structure (recognized in this paper as the $\DN$ set), gave guarantess for an approximately-optimal solution in finite time, which outperformed random sets in certain situations. For this, they introduced the concept of a \decomp set, a set that generates such approximate solutions. Still, as we seek better and better approximations for the optimal path, the staggered grid in the paper falls off against random sets. The question that stands, then, is what other sample sets can be used to provide better results?


% In this paper, we would like to utilize \decomp sets and investigate a specific series of deterministic sample sets, using lattices---a generalization of the regular grid structure using a general set of mutually-independent base vectors (not necessarily the usual $(0,\dots,1,\dots,0)$ vectors). We first familiarize the reader with three different lattices \Lattices we intend on investigating, and then move on to using Dayan et al.'s~\cite{dayan2023near} definition of a \decomp set to define lattice sample sets as such sets. This definition tells us that these sets can give us a good approximation for the optimal solution at a finite time. 


% After that, we use our new lattice sample sets to investigate the upper bounds on the number of sample points, and on the sum of edge length in a typical PRM vertices-connecting $r$-Ball---something we use as a measure point to the algorithm's complexity, as it is known that collision checks along the edges are the bottleneck in today's PRM algorithms.


% We will end up demonstrating, theoretically and practically, that one lattice, $\AN$, stands out as performing much better than the regular grid often used in many MP algorithms.