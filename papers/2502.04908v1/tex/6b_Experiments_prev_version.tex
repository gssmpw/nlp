


In this section, we will demonstrate some of the theoretical results achieved in the pape . All experiments were done with the same computer, the "Vivo Asusbook 16x" lapto . \itai{Added computer details} The first section of the paper dealt with finding the sample complexity of lattice . In the following graphs we compare the resulting theoretical sample and collision complexity with an actual count of the number of these values in a simulatio . The counting (both samples and edge lengths) was done using a BFS-like search algorithm (starting from the center of the sphere) to overcome how heavy the calculations become at higher dimensions, as our BFS-like algorithm only visits $O(d\cdot|\XL|)$ nodes.

\begin{algorithm}
\caption{GetNeighbors}\label{alg:getNN}
\KwData{$\Lambda\in\mathbb{R}^d, R>0, p_0,\delta>0,\epsilon>0$}
\KwResult{$B_R(p_0)\cap\Lambda$ with additional data}
  $G \gets GetGenerator(\Lambda)$\;
  $O:=OPEN \gets \{(Sample:p_0,IntVec:\overset{\rightarrow}{0},g:0)\}$\;
  $O_{tmp} \gets \phi$\;
  $V:=VISITED \gets \{p_0: 0$\}\;
  $R:=RESULT \gets (p_0,\overset{\rightarrow}{0},0)$\;
  \While{$O$ not empty OR $O_{tmp}$ not empty}{
    \If{$V$ is empty} {
      $V=V_{tmp}$\;
      clear $V_{tmp}$\;
    }
    $v,V_I,g \gets$ $V.pop()$\;
    \tcp{iterate over all basis vectors, in both directions}
    \For {$e\in\{\pm e_i\}$}  { 
      \tcp{get the lattice point with the generator and the base vector}
      $v^{new}\gets v+ G \cdot e$\; 
      $v^{new}_I\gets v_I$\;
      $v^{new}_I[i] \gets v^{new}_I[i] + sgn(e)$\;
      \eIf{$V$ contains $v_{I,new}$ AND $V[v_{I,new}] < 2d$}
      {
        $V[v^{new}_I]\gets V[v^{new}_I] + 1$\;
        \tcp{save space by erasing points we won't return to}
        \If{$V[v^{new}_I]==2d$}{
          remove $v^{new}_I$ from $V$
        }
      }
      {
        $V[v^{new}_I]\gets 1$\;
        $D\gets\|v^{new}_I-v\|$\;
        \If{$D\leq R$}{
          $R \gets R\cup(v^{new},v^{new}_I,D)$\;
        }
      }
    }
  }
\end{algorithm}

\begin{algorithm}
\caption{Implicit-A*}\label{alg:ImpA}
\KwData{$\Lambda\in\mathbb{R}^d, R>0, p_s,p_e,\delta>0,\epsilon>0$}
\KwResult{a $(\delta,\epsilon)$-path from $p_s$ to $p_e$ on $\Lambda$}
  ${p^{data}_s \gets (Sample:p_s,IntVec:\overset{\rightarrow}{0},g:0,h:\|p_e-p_s\|)}$\;
  $O:=OPEN \gets \{p^{data}_s\}$\;
  $V:=VISITED \gets \{\overset{\rightarrow}{0}: \{p^{data}_s\}$\}\;
  $P:=PREV \gets \{p^{data}_s: null\}$\;
  $INV:=INVALID \gets \{\}$\;
  $NN\gets GetNeighbors(\Lambda,R,p_s,\delta,\epsilon)$\;
  \While{$O$ not empty}{
    $v^{min}\gets O.pop\_min()$\;
    \If{$v^{min}$ is $p_e$}{
      create path RES using PREV array P\;
      \Return RES
    }
    $\overline{NN} \gets NN$\;
    \If{$\|v^{min}-p_e\| < R$} {
      $\overline{NN} \gets \overline{NN} \cup \{p_e-v^{min}\}$\;
    }
    \For{$n$ in $\overline{NN}$} {
      \tcc{shift the neighbors to the current center}
      $n_I \gets n_I + v^{min}_I$\;
      $n \gets n + v^{min}$\;
      \If{$n$ is not goal but is INVALID}{
        \Continue
      }
      \If{n is not VALID}{
        $INV[n]=True$\;
        \Continue
      }
      \If{edge $v^{min}\rightarrow n$ is INVALID}{
        \Continue
      }
      $newG \gets v^{min}_g + n_g$\;
      $currG \gets \infty$\;
      \If{$n$ was VISITED}{
        $currG \gets V[n_I].g$\;
      }
      \If{$newG < currG$}{
        \eIf{$n$ was VISITED}{
          $n^{data} \gets V[n]$
        } {
          $n^{data} \gets (n,n_I,newG, \|n-g_e\|)$\;
          $V[n_I] \gets n^{data}$\;
        }
        $P[n^{data}]\gets v^{min}$\;
        \If{$n^{data}$ is NOT in $OPEN$}{
          $O \gets O \cup \{n^{data}\}$\;
        }
      }
    }
  }
\end{algorithm}

\subsection*{Implementation details}

All of the experiment sections require an implementation of the lattice . To do that we utilize the OMPL library---an environment developed by the Kavraki Lab~\cite{sucan2012the-open-motion-planning-library} that enables both efficient running of c++ algorithms and their testing in the OMPLapp python interfac   The lattices were using a BFS-like algorithm to actively create the sample set, allowing to get all the samples in ball / box regions, after which we run an implicit-A* run with some modification . Let us go over important implementation details in Algorithms~\ref{alg:getNN} and \ref{alg:ImpA}.
\begin{enumerate}
    \item First thing to notice is that both algs use the underlying  $\ZN$ structure that we have in every lattice, before applying the generato . Using a integer vectors to compare points instead of float vectors is quicker and more accurate.
    \item Second thing to notice is that we calculate GetNeighbors once, before the main loop, in line  . This is due to the regularity of lattice . In lines 18-19 we shift the neighbors to the current center.
    \item Last thing to notice is that we keep track of all points that were already invalidate . As each point in the lattice is visited up to $2d$ times, only checking it once saves computation time.
    \item Regarding RND sample sets, Algorithm~\ref{alg:ImpA} is well fitted to easily run them as well. The main changes that occur are: in line 6, $NN$ is calculated using some RND nearest-neighbor structure, to be later used in line 1 . After this, we need to avoid preforming the "shift" in lines 18,1 . At any place we use the integer vector to compare, we instead use a special "Vertex" structure that OMPL uses in its NN-graph . This way RND works as an "implicit-A*" for the purposes of a fair comparison.
\end{enumerate}

The integrated lattice traversal within the implicit-PRM setting will be referred to, in this section, as \emph{Lattice-PRM} (LPRM), and the the random-sample variant will be referred to as \emph{Random-PRM} (RPRM) \kiril{Unclea . Please be more precise about the algorithm her . The same applies to all the different algorithm in this section.}\itai{done + full algorithm explanations }. An important hyper-parameter involves picking the right $\delta$ value i . While the improvement over a regular grid remains, it takes less time to find a path when using the optimal clearance---optimal in the sense of runtime, not optimal path clearanc . The differences between runtimes for low and high clearance values are, as you will see, very sharp, and the right value needs to be picke . The second one is the $\epsilon$ hyper-parameter, which is less important - as we can start with "infinite" clearance to just get some solution (in practice $\epsilon=10$ is already "infinite"), and lower it if we want a better one.


The last thing to note in terms of our testing environment is that we updated the OMPLapp~\cite{sucan2012the-open-motion-planning-library} environment to be able to display paths of any $N$ disk-robots moving around.

\begin{figure}[thb]
\centering  
\includegraphics[width=0.9\columnwidth]{Images/relative_bound_quality.png}
\caption{Comparing the lattices' relative size}
\label{fig:comparative_samples}
\end{figure}

\subsection{Sample complexity}

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/sample_set_absolute_by_d.png}
%         \label{fig:epgt_visual1}
%         \caption{Absolute size comparison}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/sample_set_absolute_by_e.png}
%         \label{fig:epgt_visual1}
%         \caption{Absolute size comparison}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/sample_set_relative_by_d.png}
%         \label{fig:epgt_visual1}
%         \caption{Relative size comparison}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/sample_set_relative_by_e.png}
%         \label{fig:epgt_visual1}
%         \caption{Relative size comparison}
%     \end{subfigure}
%     \caption{Sample complexity comparison in \Lattices}
%     \label{fig:experiments2}
% \end{figure}

We start by checking the quality of our upper limits in relation to the actual count---found by using a c++ simulation of the lattices (more on its implementation in the following experiments . First off, we can return to~\Cref{fig:limit_graph_upper} and recall the theoretical comparison between the lattice sample sets sizes: as the dimension increases, $\AN$ becomes better and better (in relation to $\ZN$)---e.g. in $d=12$ it is already produces a sample set which is around $\approx 129$ times smaller. 


Second, we can look to~\Cref{fig:comparative_samples} and see that all relative quantities are close to the value  . This means that our theoretical upper bound is very close to the real on . It can also be seen that it is mostly $<1$, which makes the upper limit an actual upper limit (with the difference probably compensated by the $P_d(\cdot)$ term).


\subsection{Collision complexity}

\itai{Todo: compare the theoretical CC to the actual one, once we agree that the bounds we got are OK.}

\itai{Todo: insert sum-of-edges checking here}

\begin{figure*}[!h]
  \centering
  \subfloat[H-Map]{
    \includegraphics[width=0.3\textwidth]{Images/Hmap_basic.png}
    %\label{fig:maps_visual1}
    }
  \subfloat[Random-polygon (RP)]{
    \includegraphics[width=0.3\textwidth]{Images/rndpoly_basic.png}
    %\label{fig:maps_visual2}
    }
  \subfloat[Unique-maze (UM)]{
    \includegraphics[width=0.3\textwidth]{Images/uniquemaze_basic.png}
    %\label{fig:maps_visual3}
    }
  \
  \subfloat[Kenny (K)]{
    \includegraphics[width=0.3\textwidth]{Images/kenny_basic.png}
    %\label{fig:maps_visual4}
    }
  \caption{Four unique scenrios. The HMap is an easy map to plan i . The RndPoly map is harder to navigate than H-Map, while still having some easy solutions due to high clearance areas. The UniqueMaze map is the hardest to traverse in terms of clearance, but robots can still move independently of eachothe . Lastly, the Kenny map is the hardest map in all aspect . Not only low clearance, robots have to also move simultaneously, otherwise they would collide.}
  \label{fig:maps}
\end{figure*}

\subsection{Simulations}
% From here, the following important detail can be used in more complicated maps: when planning a multi-robot path with $n$ robots in $\mathbb{R}^{2n}$, if we maintain movement by a single robot at a time, then no matter how close the moving robot is to the obstacles - we still get the sum of clearances for the other "still" robot   This intuitively means we "inflate" the moving robot's free space, helping him move through tunnels easier.\\
% Noticing this detail, certain maps (like this H map) give rise to an easy initial clearance guess: the sum of clearance of any $n-1$ robots, given that they are still in their locatio . Very obstacle-dense maps or maps with so many robots such that they are always close to each other still pose a problem, thoug . We use this distinction in our tests to improve the test runtimes, getting an approximated clearance value (that may be needed to be adjusted . Speficially, this distinction did help me progress to testing three disks, i.e. $d=6$, with $\delta=12.5\cdot 2=25$, which will be our base for the following experiments.


\subsection*{Comparing \Lattices and random sample sets}

We continue to compare the four sample sets that we considered in the scenarios depicted in~\Cref{fig:maps}. We focus on two styles of experiments: 2-swap and 3-swa . In these, the robots have to swap places: $1\leftrightarrow2$ or $1\rightarrow2\rightarrow3\rightarrow1$ . Another aspect of the test was the comparison of $\AN$ with RND using RPRM's method of building the NN graph beforehand (unlike LPRM where we already know the neighbors . These are the rows who's NN column is marked "Yes . This was done to further compare only the \emph{structure} of the PRM graphs we recieve with both methods. 
In~\Cref{table:experiment}, specific 2,3-swap start coordinates were chosen for each ma . It can be seen that in some RND does better than $\AN$, and in some $\AN$ win . While results are summarized in the table itself, here are the main points:
\begin{enumerate}
    \item In most, the deterministic method did at least more or less as well as the probabilistic metho . Even in cases where a random set did better than $\AN$, the difference was not big.
    \item $\AN$ does not only beat $\ZN$ in all of the cases, but in a lot of them we can observe an order of magnitude improvement in runtime.
    \item We recognized a dominance of $\AN$ over RND in maps of very narrow passage . The first example can be seen in the 3-swap UniqueMaze scenario, where it is already 3 times quicker than RN . The more impressive result, though, is in the Kenny map, where RPRM struggles to even find a solutio . This map was made to force all 3 robots to move "together": none of the robots can realistically move without the others also moving, forcing a solution path which can not be broken down into single-robot-movements.
\end{enumerate}

\subsection*{Demonstrating $\AN$ on difficult scenarios}

\itai{added the demonstrative scenarios}
In this final section, we demonstrate the best performing lattice on several situations considered to be especially difficult in the multi-robot planning setting.

\begin{figure}[thb]
\centering  
\includegraphics[width=0.9\columnwidth]{Images/8-swap.png}
\caption{8-swap}
\label{fig:8-swap}
\end{figure}

The first scenario~\ref{fig:8-swap} we managed to plan is one where eight disk robots were asked to perform a swap (similar to previous 3-swaps, this is an 8-swap . An 8-swap scenario is a $d=16$ problem, which is relatively high dimensio . It took 337 seconds to finish planning this.  

\begin{figure}[thb]
\centering  
\includegraphics[width=0.9\columnwidth]{Images/slide-puzzle.png}
\caption{slide puzzle: $(1,2,3,4)\rightarrow(4,3,2,1)$}
\label{fig:slide-puzzle}
\end{figure}

The next scenario~\ref{fig:slide-puzzle} is inspired by the "16 puzzle", introducing 4 sliding pieces in a very tight environmen . Naming the piece "1" to "4" (from left to right), we want $(1,2,3,4)\rightarrow(4,3,2,1)$. The difficulty here is not the high dimension, but the extremely limited area in which the $d=8$ point robot can maneuve . This scenario took 128 minutes.

