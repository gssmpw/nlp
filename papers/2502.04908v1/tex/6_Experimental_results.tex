\section{Experimental results}\label{sec:experiments}
%In the previous two sections, we derived theoretical bounds for the sample complexity and collision-check complexity of lattice-based sample sets. We then tested the tightness of those bounds by comparing them with their actual values in simulations. 
We study practical aspects of our theoretical findings in motion planning problems. We report comparisons between the three lattice-based sets, as well as uniform random sampling. An additional experiment studying the effect of the parameters~$\delta$ and~$\eps$ is reported in\conditionaltext{ the appendix}{ the supplementary material}. 

\begin{figure*}[]
\hspace*{-0.8cm}
  \centering
\subfloat[Kenny ($\uparrow$), Narrow ($\downarrow$)]{
\includegraphics[width=0.364\columnwidth,clip]{Images/Scenarios/K_N_Vert_scenarios.png}
    %\label{fig:3d_lattices:z}
    }
\subfloat[Zigzag]{
\includegraphics[width=0.3\columnwidth,clip]{Images/Scenarios/ZZB1_scenario.png}
    %\label{fig:3d_lattices:z}
    }
\subfloat[Unique Maze]{
\includegraphics[width=0.725\columnwidth,clip]{Images/Scenarios/UM_scenarios.png}
    %\label{fig:3d_lattices:da}
    }
\subfloat[Bugtrap]{\includegraphics[width=0.725\columnwidth,trim={1.1cm 1.1cm 1.1cm 1.1cm},clip]{Images/Scenarios/BT_scenarios.png}
    %\label{fig:3d_lattices:da}
    }

  \caption{A subset of the scenarios used in the experimental results. Some of the figures depict several scenarios, where each scenario consists of a workspace environment, along with an initial configuration specifying the number of robots, their initial positions, and a permutation of their target positions. Each configuration is drawn in a different color, where the scenario name is indicated by the first letters of the workspace name along with a number. (a) (Top) For the "Kenny" workspace, there is a single scenario K1, where robot $1$ starts in the bottom position, robot $2$ in the top position, and robot $3$ in the remaining position, where the disc size corresponds to the robot geometry. In this map, the robots are forced to perform a simultaneous movement. (a) (Bottom)   
  For the "Narrow Room" workspace, we illustrate the scenarios N3,N4, and N5. Due to overlap, the discs in N4 are slightly shrunk for better visualizations. (b) The Zigzag scenario contains narrow, winding passages in which one of the robots can hide in a pocket to let the other robot pass. Swaps can also occur in the top or bottom corners, albeit with greater care. (c),(d) Those workspaces are taken from OMPL~\cite{sucan2012the-open-motion-planning-library}.}
  \label{fig:scenarios}
\end{figure*}

% \begin{figure}[H]
%   \centering
%   
%   \caption{words}
%   \label{fig:3d_lattices}
% \end{figure}

\subsection{Implementation details and planners}
The experiments were performed on an ASUS Vivobook 16x laptop equipped with an Intel Core i9-13900H CPU, 32GB DDR4 RAM, and SSD storage, running Ubuntu 22.04.5 LTS OS.
The planners were implemented in C\texttt{++} within OMPL~\cite{sucan2012the-open-motion-planning-library}, with FCL~\cite{Pan2012FCL} for collision detection, and GNAT~\cite{gipson2013resolution} for nearest-neighbor search (where applicable).

We use a single-query planner where an implicitly-represented PRM graph is explored using a search heuristic similar to FMT*~\cite{JSCP15}, BIT*~\cite{GammellBS20}, and GLS~\cite{MandalikaCSS19}. Those state-of-the-art approaches are well-suited for settings where samples are generated in large batches, as lattice-based sampling facilitates. We focus on the single-query setting, as it allows us to experiment with more complex problem scenarios (e.g., in terms of dimensions and tightness) than in a multi-query setting where the entire configuration space needs to be explored, which requires additional memory and compute time. 

The planner we use, which is termed for simplicity implicit A* (iA*), can be viewed as a simplified version of BIT* with a single sample batch searched using the A* algorithm. iA* generates a sample set $\X$ from a given sample distribution  ($\XZ,\XD,\XA$ or uniform random sampling). Instead of constructing the entire PRM graph $G:=G_{\M(\X,r)}$ resulting from $\X$ and a given radius parameter $r$, iA* constructs a partial graph $G'\subset G$ in an implicit manner, where the construction is guided by the underlying A* search. That is, when a vertex $v$ of $G$ is expanded, its neighbors $N_v$ within an $r$-neighborhood are retrieved from $\X$, and the edges between $v$ and every $u\in N_v$ are collision-checked and added to the explored portion of the graph $G'$. For lattice samples, we set $r:=r^*$. The radius for random samples is described below. 

We consider two flavors of iA*. In the first flavors, 
denoted by \glo, vertex neighbors (as $N_v$ above) are retrieved by calling a \emph{global} nearest-neighbor (NN) data structure. Before starting the A* search, this data structure is initialized with the set $\X$. %The use of NN is standard in sampling-based planners in general and in implicit search planners such as~\cite{JSCP15,GammellBS20,MandalikaCSS19}, in particular. 

Although the benefits of lattice-based sampling over randomized sampling are already apparent for the \glo flavor (especially $\XA$), the performance of iA* can be further improved by exploiting the \emph{local} regular structure of lattices. In the second flavor of iA*, denoted by \loc,  which only applies to lattice-based sets, vertex neighbors are efficiently retrieved without NN data structures. Given a lattice-based sample set $\X_\Lambda$ and a connection radius $r>0$, denote by $N_x:=\B_r(x)\cap \X_\Lambda$ the $r$ nearest-neighbors of a vertex $x\in \X_\Lambda$. Then, for another sample $x'\in \X_\Lambda$ it holds that $N_{x'}:=\B_r(x')\cap \X_\Lambda=(x'-x)+N_x$, i.e., $N_{x'}$ is a translation of $N_x$. Hence, the computation of the neighbor set $N_0$ is only performed from scratch once\footnote{We compute the neighbor set $N_0$ by performing a breadth-first search from the origin vertex, and traversing integer vectors $v\in \dZ^d$ in an increasing radius around the origin, which are then multiplied by the generator matrix to obtain lattice points. The computational cost of this operation is negligible compared to the  A* search itself and hence omitted from the running time results below.} at the beginning of the run of the search algorithm, where the $N_v$ is easily obtained from vector operations.
We associate every sample $x\in \X_\Lambda$ with the integer vector $v\in \dZ^d$ such that $x:=vG_\Lambda$, which allows us to keep track of explored vertices efficiently. 

\subsection{Scenarios}
We test the planners on a variety of motion-planning problems where $d\in \{4,6,8,10,12\}$. Each scenario consists of a multi-robot system of $m$ labeled planar disc robots that need to (simultaneously) exchange positions (i.e., robot $i\in \{0,\ldots,m-1\}$ moves to the start position of robot $i+1 \mod m$) while avoiding collisions with each other and static obstacles. This setting is considered for two reasons. First, such multi-robot systems can be viewed as Euclidean systems, which allows applying our theoretical results (see discussion in Section~\ref{sec:future} of extensions to more general systems). (Specifically, the configuration space of an $m$-robots system is $\dR^{2m}$.) Second, this setting allows us to test systems of various dimensions while still being able to visualize the problem setting (which is in 2D).  Third, it provides a simple approach for determining the value $\delta$ (see below).

A subset of the tested scenarios is found in Figure~\ref{fig:scenarios} (additional scenarios are found in\conditionaltext{~\Cref{fig:scenarios:app}}{ the supplementary material} along with a detailed description). The scenarios present various difficulty levels for the planners, where the most difficult scenarios consist of narrow passages for the individual robots and a significant amount of coordination between the robots, giving rise to narrow passages in the full configuration space. 

Unless stated otherwise, the parameters $\delta$ and $\eps$ were specified in the following manner. We assigned $\eps:=10$ to focus on running time scalability rather than solution quality. The parameter $\delta$ was initially set to be the clearance of the start configuration (capturing both distances from obstacles and between robots), which was decreased until a solution using $\XA$ was found. We discuss the automatic tuning of $\delta$ and $\eps$  in Section~\ref{sec:future}.

\subsection{Comparison between lattice-based sample sets}
In the first set of experiments, we study the running time and solution quality (in terms of path length) of the three lattice-based sample sets $\XZ,\XD$, and $\XA$ within the \loc flavor of iA* for a selected set of scenarios. Although in some scenarios, the performance between the sample sets can be comparable, in terms of running time, especially for $\XD$ and $\XA$, here we highlight situations where large gaps occur. 

The results are reported in Table~\ref{tbl:lattice_comparison}. (Results for additional scenarios are provided in\conditionaltext{~\Cref{tbl:lattice_comparison:app} in the appendix}{ the supplementary material}.) In terms of running time, $\XA$ outperforms the other sample sets, as predicted by our theoretical results with respect to sample complexity and collision-check complexity. $\XZ$ results in at least one order of magnitude (up to 2 orders) slower running times than the other two sample sets. $\XA$ outperforms $\XD$, being at least $3\times$, and sometimes as much as $10\times$, faster. The notations "dnf" and "-" indicate a running time threshold of 1000 seconds was exceeded and failure to find a solution, respectively. 

Regarding solution quality, $\XZ$ outperforms the other sample sets, except for the last scenario (unless it does not manage to find a solution) due to a denser graph. The difference in the solution length suggests that the completeness-cover relation Lemma~\ref{lem:cover} can be tightened by, e.g., reducing the value of ${\beta^*}$, albeit we emphasize this is a worst-case bound. From a practical perspective, the difference in the path length can be reduced via post-processing techniques with negligible computational cost. To summarize, $\XA$-sampling can drastically reduce computational cost while achieving comparable solution quality to the other sample distribution. For this reason, we omit comparisons with $\XZ$ and $\XD$ in the remainder of this section. 

\begin{table}[]
\caption{Comparison of running time and solution length using lattices-based sample sets (where the underlying lattice is denoted in the table) in the iA*-\loc algorithm. Solution length is normalized with respect to the length obtained using $\XA$. } \label{tbl:lattice_comparison}
\centering
\begin{tabular}{|c||ccc|cc|}
\hline
 & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF} Time (s)} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF} Length (r)} \\ \cline{2-6} 
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Scenario\\ (robot \#)\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFC7}$\ZN$} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFC7}$\DN$} & \cellcolor[HTML]{FFFFC7}$\AN$ & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFC7}$\ZN$} & \cellcolor[HTML]{FFFFC7}$\DN$ \\ \hline \hline
\cellcolor[HTML]{ECF4FF}N1(5) & \multicolumn{1}{c|}{165.35} & \multicolumn{1}{c|}{4.59} & 0.36 & \multicolumn{1}{c|}{0.65} & 0.79 \\
\cellcolor[HTML]{ECF4FF}N1B(6) & \multicolumn{1}{c|}{dnf} & \multicolumn{1}{c|}{328.30} & 15.08 & \multicolumn{1}{c|}{dnf} & 0.89 \\ \hline
\cellcolor[HTML]{ECF4FF}BT10(2) & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{1.20} & 0.30 & \multicolumn{1}{c|}{-} & 0.92 \\
\cellcolor[HTML]{ECF4FF}BT5(3) & \multicolumn{1}{c|}{0.54} & \multicolumn{1}{c|}{0.14} & 0.06 & \multicolumn{1}{c|}{0.38} & 0.51 \\
\cellcolor[HTML]{ECF4FF}BT1(4) & \multicolumn{1}{c|}{146.69} & \multicolumn{1}{c|}{50.81} & 3.51 & \multicolumn{1}{c|}{0.95} & 1.03 \\
\hline
\cellcolor[HTML]{ECF4FF}K1(3) & \multicolumn{1}{c|}{32.31} & \multicolumn{1}{c|}{4.97} & 1.37 & \multicolumn{1}{c|}{0.82} & 0.89 \\ \hline
\cellcolor[HTML]{ECF4FF}UM4(2) & \multicolumn{1}{c|}{-} & \multicolumn{1}{c|}{8.47} & 2.43 & \multicolumn{1}{c|}{-} & 0.90 \\
\cellcolor[HTML]{ECF4FF}UM2(3) & \multicolumn{1}{c|}{13.35} & \multicolumn{1}{c|}{1.22} & 0.04 & \multicolumn{1}{c|}{1.04} & 1.52 \\
\hline
\end{tabular}
\end{table}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\caption{Comparison of running time and solution length between $\XA$ (using \loc and \glo) and uniform random sampling. For \rnd we report the average values in terms of running and solution length (the latter is given as normalized value with  respect to the solution length with $\XA$).}
\label{tbl:lattice_vs_random}
\centering
\begin{tabular}{|c||ccc|c|c|}
\hline
 & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF} Time (s)} & \cellcolor[HTML]{EFEFEF}Length (r) & \cellcolor[HTML]{EFEFEF}Success (\%) \\ \cline{2-6} 
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Scenario\\ (robot \#)\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFC7}\begin{tabular}[c]{@{}c@{}}$\AN$\\ \loc \end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFC7}\begin{tabular}[c]{@{}c@{}}$\AN$\\ \glo \end{tabular}} & \cellcolor[HTML]{FFFFC7}\begin{tabular}[c]{@{}c@{}}\rnd\\ \glo\end{tabular} & \cellcolor[HTML]{FFFFC7}\begin{tabular}[c]{@{}c@{}}\rnd\\ \glo\end{tabular} & \cellcolor[HTML]{FFFFC7}\begin{tabular}[c]{@{}c@{}}\rnd\\ \glo\end{tabular} \\ \hline\cellcolor[HTML]{ECF4FF}N1(5) & \multicolumn{1}{c|}{0.36} & \multicolumn{1}{c|}{3.05} & 4.16 & 1.48 & 80 \\
\cellcolor[HTML]{ECF4FF}N1(5) & \multicolumn{1}{c|}{0.36} & \multicolumn{1}{c|}{3.05} & 4.16 & 1.48 & 80 \\
\cellcolor[HTML]{ECF4FF}N2(5) & \multicolumn{1}{c|}{0.41} & \multicolumn{1}{c|}{2.67} & 2.74 & 2.43 & 65 \\
\cellcolor[HTML]{ECF4FF}N3(5) & \multicolumn{1}{c|}{0.59} & \multicolumn{1}{c|}{3.83} & 5.44 & 2.02 & 85 \\ \hline
\cellcolor[HTML]{ECF4FF}BT3(3) & \multicolumn{1}{c|}{5.38} & \multicolumn{1}{c|}{14.15} & 62.22 & 1.05 & 100 \\
\cellcolor[HTML]{ECF4FF}BT8(3) & \multicolumn{1}{c|}{12.17} & \multicolumn{1}{c|}{19.31} & 169.32 & 1.00 & 100 \\
\cellcolor[HTML]{ECF4FF}BT8B(3) & \multicolumn{1}{c|}{3.17} & \multicolumn{1}{c|}{3.60} & 41.63 & 1.16 & 100 \\ \hline
\cellcolor[HTML]{ECF4FF}UM4(2) & \multicolumn{1}{c|}{2.43} & \multicolumn{1}{c|}{2.93} & 12.71 & 0.96 & 70 \\
\cellcolor[HTML]{ECF4FF}UM1(3) & \multicolumn{1}{c|}{6.68} & \multicolumn{1}{c|}{58.62} & 49.35 & 0.98 & 100 \\
\cellcolor[HTML]{ECF4FF}UM2(3) & \multicolumn{1}{c|}{0.04} & \multicolumn{1}{c|}{2.94} & 4.49 & 1.95 & 75 \\ \hline
\cellcolor[HTML]{ECF4FF}ZZB1(2) & \multicolumn{1}{c|}{0.44} & \multicolumn{1}{c|}{0.49} & 10.44 & 0.89 & 100 \\
\cellcolor[HTML]{ECF4FF}ZZB2(2) & \multicolumn{1}{c|}{0.71} & \multicolumn{1}{c|}{2.51} & 272.70 & 0.89 & 100 \\
\cellcolor[HTML]{ECF4FF}ZZB3(2) & \multicolumn{1}{c|}{0.47} & \multicolumn{1}{c|}{7.69} & 341.84 & 0.88 & 100 \\ \hline
\end{tabular}
\end{table}

\subsection{Comparison with randomized sampling}
We compare the performance of iA* when using $\XA$-sampling and uniform random sampling (denoted by \rnd). We show that the advantage of $\XA$ stems not only from algorithmic speedups due to the regular lattice structure (as in \loc) but also from structural properties leading to a more efficient coverage of space. Hence, we run both \glo and \loc using $\XA$, where \rnd is tested only with \glo. 

For a given scenario, we fix the parameters $\delta$ and $\eps$ used to derive $\XA$. To derive a set of random samples $\XR$, we compute the number of $\XA$ samples within the scenario (while ignoring collision with obstacles and between robots) and produce the same number of points via uniform random sampling. Importantly, when running \glo with $\XR$ we specify the standard connection radius  $r_\text{rnd}(n)=\psi \left(\frac{\log n}{n}\right)^{1/d}$, where $n:=|\XR|$ and $\psi$ is a constant, such that asymptotic optimality is guaranteed~\cite{karaman2011sampling}. The results are reported in  Table~\ref{tbl:lattice_vs_random}, where \rnd is averaged over 20 repetitions. (Results for additional scenarios are provided in\conditionaltext{~\Cref{tbl:lattice_vs_random:app} in the appendix}{ the supplementary material}.)
%For \loc using $\XA$ we report the total running, and solution length. For \glo using $\XA$ we report the running time, which consists of the construction of the NN data structure and the search algorithm. The running time of \glo using uniform random sampling (\rnd) is reported in a similar manner, where we provide both the mean and average values across 50 repetitions. We also report the solution length of \rnd as a ratio between it and the length using $\XA$. Finally, we report the success rate of \rnd. \kiril{How do we determine timeouts for \rnd, and what do we report for runtime in such a case?}\itai{I took the largest possible time value I could without the computer crashing (e.g. 10-20 times the runtime of An* allowed, up to 1000 times in scenarios where An*'s time was VERY short), resulting in just a few timeouts in most runs. they are reported as fails and their runtime ignored (I average over successes). }


Note that \rnd achieves a perfect success rate only in some of the scenarios. Furthermore, its running time is typically at least $5\times$ slower than \loc, and in some scenarios, up to three orders of magnitude slower. This time gap can be partially attributed to \rnd constructing and maintaining a nearest-neighbor data structure. However, notice that in some scenarios, $\AN$-\glo significantly outperforms \rnd. One explanation of that is $\AN$ requires a smaller connection radius than \rnd, which leads to a lower sample complexity and collision check complexity. Another reason is that \rnd, due to poor coverage of space, is forced to make detours and so considers additional vertices and collision checks. We provide further evidence for those points in\conditionaltext{~\Cref{tbl:lattice_vs_random:app} in the appendix}{ the supplementary material}. We also report results for \rnd where the connection radius $r_\text{rnd}(n)$ is substituted with $r^*$, which further reduces success rates and emphasizes the efficiency of space coverage using $\XA$.

%In terms of solution quality, $\XA$ consistently obtains shorter paths (with the exception of one scenario where \rnd obtained a better solution, albeit, with a success rate of only $0.42$), at times twice as short, than \rnd. 

%\yaniv{How is RND different exactly from PRM and from your algo? I would expect that using the same number of vertices results in similar runtimes but lower success rate for RND, maybe try to explain why that isn't the case }

Overall, we conclude that locality can substantially speed up performance from an algorithmic perspective, while the structural properties of $\XA$ also improve performance in terms of success rate, running time, and solution quality. %Finally, we mention that before settling on the scenarios reported in this paper, we considered an extended collection, which also includes easier problems than those reported here (requiring a small number of samples due to the high clearance of the solution). We observed on those simpler problems comparable performance between \loc and \rnd, as expected. Overall, we have not encountered problems where \rnd had any benefit compared with \loc in terms of running time, solution quality, or success rate. 




