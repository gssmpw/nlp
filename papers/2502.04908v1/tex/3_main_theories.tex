\section{Sample Complexity}\label{sec:sample_comlpexity}
We present our main theoretical results on sample complexity. First, we adapt Verger-Gaugry's~\cite{verger2005covering} lower bound result to obtain a new lower bound on the sample complexity of \decomp sets. Notice that this bound applies to any sample set, not necessarily a lattice-based one.

\begin{thm}[Sample-complexity lower bound~\protect{\cite[Theorem~3.10]{verger2005covering}}]
    Consider the configuration space $\C$ being a $d$-dimensional R-ball\itai{change here}, $\B_R\subset \dR^d$. Without loss of generality, assume\footnote{These radius ranges are assumed so that they fit into the setting of a theorem we use later.} $R=\sqrt{d}\beta$ or $R\geq d\cdot\beta$, where $\beta:=\beta\left(\delta,\epsilon\right)$ as defined in Lemma 1. Suppose that $\X$ is a sample set that is a $\beta$-cover for $\C$. Then there exists $c>0$ such that:
    \[
        |\X|\geq cd\left(\frac{R}{\beta\left(\de\right)}\right)^d =: n_{LB}^{\de}.
    \]
    % \triangleq \|\X_{LB}\|
\end{thm}
\begin{proof}
Theorem 3.10 in~\cite{verger2005covering} states that if a sample set $\X^*$ is a $\frac{1}{2}$-cover for $\B_{R^*}$, where $R^*=\frac{\sqrt{d}}{2}$ or $R^*\geq\frac{d}{2}$, then there exists $c>0$ such that $|\X^*|\geq cd\cdot \left(2R^*\right)^d$.


Next, we use a rescaling argument to apply those finding to our setting. In particular, we wish to cover $\B_R$ with $\beta$-balls. To convert $\beta$-balls into $\frac{1}{2}$-balls we need to rescale by $\frac{1}{2\beta}$, which makes it so that $R$ becomes $R^*=\frac{R}{2\beta}$ in the rescaled space. In this rescaled space, $\B_{R^*}$ is covered by $\frac{1}{2}$-balls. 


For the radius $R^*$ to fit Verger-Gaugry's requirements we need to ensure  that:
\begin{equation*}
    \frac{R}{2\beta} = R^* = \frac{\sqrt{d}}{2},
\end{equation*}
which implies that:
\[
    R=\sqrt{d}\beta,
\]
and similarly if \footnote{Notice that this lower bound applies to any ball $\B_r$, as long as it is $\sqrt{d}$ times or more than $d$-times the "minimal radius" ball of $\beta$.} $R\geq d\beta$. I want to demonstrate next why we can safely assume that our C-Space's radius is $\sqrt{d}$-times the radius of the $\beta$-ball. \itai{new segment here}


Taking $d=10$ for example, if our space's radius is $\sqrt{10}$ times bigger than a $\beta$-ball then the volume is $\sqrt{10}^{10}=10^5$ bigger than a $\beta$-ball. This means we expect to find about $10^5$ smaller $\beta$-balls in it. With a normal cubic $[0,1]^d$ world, we expect to find about $\frac{1}{\beta^d}=\left(\frac{1}{\beta}\right)^d=\left(\frac{1}{\beta^2}\right)^{\frac{d}{2}}$ balls. Taking the same $d=10$, we get that for our spherical space to have less $\beta$-balls than a cubic space, we will need $\frac{1}{\beta^2}\geq 10 \Rightarrow \beta \leq 0.32$. We get:
\[
    \beta=\delta\sqrt{\frac{\epsilon^2}{1+\epsilon^2}}\leq0.32 \iff \frac{\epsilon^2}{1+\epsilon^2}\leq \left(\frac{0.32}{\delta}\right)^2
\]
But notice that:
\[
    \frac{0.32}{\delta}<1 \iff \delta>0.32
\]
So for $\delta\leq 0.32$, we get $\frac{0.32}{\delta}\geq1$. But $\frac{\epsilon^2}{1+\epsilon^2}<1$ for all $\epsilon>0$, which makes the above inequality true for all $\epsilon>0$. All this is to say the following: the requirements on $\delta,\epsilon$ aren't harsh and would probably be fulfilled in most use cases, allowing us to rely on this lower bound. Returning to Verger-Gaugry's result, we get the lower bound:
\begin{equation*}
    |\X|\ \geq cd\cdot \left(2R^*\right)^d = cd\cdot \left(\frac{R}{\beta}\right)^d,
\end{equation*}
which concludes the proof. \qedsymbol
\end{proof}
\itai{Note to self: either remove the following part or find a suitable comparison between square and spherical CSpaces.}


\itai{I moved this section HERE, Im not sure why it was down below.}
Let us compare this lower-bound to a more naive bound, asserting the minimal number of points in a set $\X_{base}$ should be at least the volume of $\B_R$ divided by the volume of a $\beta$-ball:
\begin{equation}
        |\X_{base}|\geq \frac{Vol\left(\B_R\right)}{Vol\left(\B_\beta\right)}=\frac{c_d R^d}{c_d\beta^d}=\left(\frac{R}{\beta}\right)^d.
\end{equation}
It is immediately clear that the new lower bound is $d$ times better than the classic one. This is another reason to support spherical configuration spaces.

Next, we will define what is a "lattice sample set" and show that we can give explicit representations of them such that they'll be \decomp sets. As we will be talking about covering the space throughout the paper, let us first consider the following definition from Conway and Sloane~\cite{conway2013sphere}:
\begin{definition} (Covering Radius)
    \itai{changes here} For a collection of points $\X$, a covering radius is defined to be:
    \[
        R_{cover}^{\X} = \sup_{y\in\mathbb{R}^d}\inf_{x\in\X} \|x-y\|.
    \]
\end{definition}
\itai{added some clarification here}
This definition makes it more clear that one can not cover $\mathbb{R}^d$ with a radius smaller than $R_{cover}^{\X}$ for a given point set $\X$. In the next theorem, we will be using the covering radii's optimality to construct $\beta$-balls, paving the way to explicitly construct \decomp sample sets.
% \begin{lemma}[Optimal covering radius]
%     Let $\Lambda$ be a lattice in $\mathbb{R}^d$. Then $Cover\left(\Lambda\right)$ is the optimal \left(in terms of density\right) covering radius for this $d$-dimensional lattice. Relying on Conway~\cite{conway2013sphere}, we have:
%     \itai{I'm correcting this. the optimal radius is related to the original lattice, not the rescaled one.}
%     \begin{enumerate}[topsep=1pt,itemsep=1ex,partopsep=1ex,parsep=1ex]
%         \item $Cover\left(\AN\right) = \sqrt{\frac{d}{2}}$
%         \item $Cover\left(\DN\right) = \frac{\sqrt{d}}{2}$
%         \item $Cover\left(\ZN\right) = \sqrt{\frac{d\left(d+2\right)}{12\left(d+1}}$
%     \end{enumerate}
% \end{lemma}
% \itai{changes here.}
% Using the concept of covering radii for lattices, we will explicitly find sample sets defined by our three lattices:
% \begin{definition}[Lattice Sample Set]
%     Let $\Lambda$ be a lattice, and $\delta,\epsilon$ be the clearance and stretching parameters. We define $\XL$ to be the set $\left(c\cdot\Lambda\right)$ for some $c>0$ such that it's a \decomp set.
% \end{definition}
% Specifically for \Lattices we get the following theorem:
\begin{thm}
  Fix $\delta>0,\epsilon>0$. Then the following sets are \decomp:
  \itai{added a new case for Dn (even/odd)}
  \begin{enumerate}
      \item $\XZ=\frac{2\beta\left(\delta,\epsilon\right)}{\sqrt{d}}\ZN$,
      \item $\XD=
      \begin{cases}
        d\text{ is odd: }
        \{x\in\mathbb{R}^d|x=\frac{4\beta\left(\delta,\epsilon\right)}{\sqrt{2d-1}}G_{\DN}^t\cdot v,v\in\mathbb{Z}^d\}:=\frac{4\beta\left(\delta,\epsilon\right)}{\sqrt{2d-1}}\DN,\\
        d\text{ is even: }
        \{x\in\mathbb{R}^d|x=\sqrt{\frac{8}{d}}\beta G_{\DN}^t\cdot v,v\in\mathbb{Z}^d\}:=\sqrt{\frac{8}{d}}\beta\DN,
      \end{cases}$
      \item $\XA=\{\mathbb{R}^d|x=\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta\left(\delta,\epsilon\right) T\cdot v,v\in\mathbb{Z}^d\}:=\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta\left(\delta,\epsilon\right) T(\AN)$,
      
      
      with:
      $T=\begin{pmatrix}
                    1 &  1  & \dots & 1 & x - 1\\
                    -1 & 0  & \dots & 0 & x \\
                    0 & -1  & \dots & 0 & x \\
                    \vdots & \vdots  &  \ddots & \vdots & \vdots \\
                    0 & 0  &  \dots & -1 & x \\
                \end{pmatrix}\in \mathbb{R}^{d \times d}, x=\frac{1}{(d+1) - \sqrt{d+1}}$.
  \end{enumerate}
\end{thm}
\begin{proof}
    \itai{this section onwards has changes} We need to show that we can rescale the lattices \Lattices by some $c>0$ such that they become \decomp. We remember that the lattices were introduced in the first section in the context of using unscaled distances---i.e., in a cube $[0,1]^d$ we have exactly one cube for the lattice $\mathbb{Z}^d$, in contrast with many more cubes if we start rescaling the grid. Since we want the samples to be sufficiently dense as to form a \decomp set, we will need to rescale the space. This is why in each of the following sections, given our lattice $\Lambda$, we use a \emph{rescale coefficient} $f_\Lambda:=f_\Lambda\left(d\right)$, together with a \emph{rescaling factor} $w_\Lambda>0$, defined by:
    \[
        \beta = f_\Lambda w_\Lambda\Rightarrow w_\Lambda = \frac{\beta}{f_\Lambda}.
    \]
    Meaning: we start with the covering radius (i.e. the rescale coefficient $f_\Lambda$), and we rescale it (i.e. multiply it with the rescaling factor) to "the smallest size possible" (i.e. the $\beta$-ball that covers the space), which is sufficient for $\left(\delta,\epsilon\right)$-completeness due to Lemma~\ref{lem:cover}.
    One thing to note is, that using our scaling equation, our space went from some implicit "originating" space of radius $R_{orig}$ (where, as we said, the lattices are still unscaled)---which becomes $R$ with the relation:
    \begin{align*}
        R_{orig}\cdot w_\Lambda = R \Rightarrow \frac{R_{orig}\cdot\beta}{f_\Lambda}=R\Rightarrow R_{orig}=\frac{R}{\beta}\cdot f_\Lambda.
    \end{align*}
    We will find $f_\Lambda$ for each of our three lattices, which will immediately give us our \decomp sets.


    \itai{This whole section was overhauled (no more enumerate, rewritten, etc.)} We first look at $\ZN$. Consider the cube $[0,w_{\ZN}]^d$ which tessellates the space. To find the rescale coefficient, remember that we need to completely cover each of these lattice cubes. Thus, the radius needs to reach the center of the cube. Seeing as the cube has a side of length $w_{\ZN}$, this gives us the scaling equation:
    \[
        \beta = \frac{\sqrt{d}}{2}w_{\ZN} \Rightarrow f_{\ZN}\left(d\right)=\frac{\sqrt{d}}{2},
    \]
    from which we get our set definition:
    \begin{align*}
        \XZ = \{x\in\mathbb{R}^d|x=w_{\ZN} v\cdot G_{\ZN},v\in\mathbb{Z}^{1\times d}\} = \\
        \{x\in\mathbb{R}^d|x=\frac{\beta}{f_{\ZN}\left(d\right)}I_{d\times d}\cdot v,v\in\mathbb{Z}^d\} = \\
        \{x\in\mathbb{R}^d|x=\frac{2\beta}{\sqrt{d}} v,v\in\mathbb{Z}^d\}:=\frac{2\beta}{\sqrt{d}}\ZN.
    \end{align*}
    Next, we look at $\DN$. Here we use the fact that the covering radius for $D_{d\text{-odd}}^*$ is \(\frac{\sqrt{2d-1}}{4}\), and for $D_{d\text{-even}}^*$ is \(\frac{\sqrt{2d}}{4}=\sqrt{\frac{d}{8}}\) (~\cite[page 120]{conway2013sphere}). Using this radius as a baseline for what we want our $\beta$-ball to be, we get the following rescaling equations:
    \begin{align*}
                \beta = \frac{\sqrt{2d-1}}{4}w \Rightarrow f_{D_{d\text{-odd}}^*}\left(d\right) = \frac{\sqrt{2d-1}}{4}, \\
                \beta = \sqrt{\frac{d}{8}}w \Rightarrow f_{D_{d\text{-even}}^*}\left(d\right) = \sqrt{\frac{d}{8}},
    \end{align*}
    from which we get our set definitions:
    \begin{align*}
        D_{d\text{-odd}} = \{x\in\mathbb{R}^d|x=wv\cdot G_{\DN},v\in\mathbb{Z}^{1\times d}\} = \\
        \{x\in\mathbb{R}^d|x=\frac{4\beta}{\sqrt{2d-1}}G_{\DN}^t\cdot v,v\in\mathbb{Z}^d\}:=\\
        \frac{4\beta}{\sqrt{2d-1}}\DN,\\
        D_{d\text{-even}} = \{x\in\mathbb{R}^d|x=wv\cdot G_{\DN},v\in\mathbb{Z}^{1\times d}\} = \\
        \{x\in\mathbb{R}^d|x=\sqrt{\frac{8}{d}}\beta G_{\DN}^t\cdot v,v\in\mathbb{Z}^d\}:=\\
        \sqrt{\frac{8}{d}}\beta\DN.
    \end{align*}
    Finally, we analyze the lattice $\AN$.
    % Using a lattice's generator, we can define the lattice points generated from it as:
    % \[
    %     \Lambda=\{x \in \mathbb{R}^{d} \mid \exists g\in \mathbb{Z}^{d} \text{ s.t. } x=g\cdot G\}
    % \]
    Recall that $A_d^*$ is defined using this \emph{generator matrix}:
    \begin{align} \label{eq:generaror}
        G:=G_{\AN}=
        \begin{pmatrix}
            1 & -1 & 0 & 0 & \dots & 0 & 0 \\
            1 & 0 & -1 & 0 & \dots & 0 & 0 \\
            . & . & . & . & \dots & . & 0 \\
            1 & 0 & 0 & 0 & \dots & -1 & 0 \\
            \frac{-d}{d+1} & \frac{1}{d+1} & \frac{1}{d+1} & \frac{1}{d+1} & \dots & \frac{1}{d+1} & \frac{1}{d+1} \\
        \end{pmatrix}\in \mathbb{R}^{d\times \left(d+1\right)}.
    \end{align}
    As this is a mapping from $d$ to $d+1$, we start with the process of embedding $A_d^*$ in $\mathbb{R}^d$. Afterwards we show that the embedding we created in $\mathbb{R}^d$ shares the same covering radius as the original set in $\mathbb{R}^{d+1}$.

    
    Notice that the rows of $G$ are all in the plane $H_0: \sum_i x_i = 0$, so $A_d^*$ itself is contained in that $d$-dimensional plane. It remains to find a transformation of $\AN$ such that the dimension is reduced to $d$, while maintaining the structure of the points in $\AN$.
    
    
    First, we will rotate $H_0$ so that we zero out the last coordinate. To do that, we will use a \emph{Householder matrix}~\cite{householder1958unitary}, which gives us a way to reflect a point set with a plane. Defining the plane through its normal vector $\hat{n}\in \mathbb{R}^{d+1}$, the Householder matrix is defined as:
    \begin{align}\label{eq:reflection}
        P=I-2\hat{n}\hat{n}^t.
    \end{align}
    We need to find the normal $\hat{n}$ to a plane that is exactly between (angle-wise) the plane $H_0$ and the $X_{d+1}=0$ plane, because reflecting against this plane will give us points that lie in $X_{d+1}=0$. \itai{fixed many typos and rewritten this section. The angle thing wasnt accurate enough IMO, so I proved it another way.}
     Finding an angle between planes is equivalent to finding the angle between their normal vectors, which leads us to the following \emph{intuition} in two dimensions.
     \begin{figure}[H]
        \centering
        \includegraphics[width=0.45\linewidth]{Images/normal_inuition.jpg}
        \caption{Illustraion of finding the reflection normal in $d=2$}
        \label{fig:reflection}
    \end{figure}
     the plane $H_0$ has a normal vector $u=\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)$. We want to find the vector splitting the angle between u and $(0,-1)$. We can look for it on the vertical line descending from $u$, and because $\|u\|=\|(0,-1)\|=1$ then to make sure it splits the angle we just need that 
     \begin{align*}
         n\cdot u = n\cdot (-e_2) \iff \\
         \left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}-\alpha\right)\cdot\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)=\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}-\alpha\right)\cdot(0,-1) \Rightarrow \\
         \frac{1}{2}+\frac{1}{2}-\alpha\frac{1}{\sqrt{2}}=\alpha-\frac{1}{\sqrt{2}} \Rightarrow \\
         1+\frac{1}{\sqrt{2}}=\alpha(1+\frac{1}{\sqrt{2}})\Rightarrow\alpha=1,
     \end{align*}
    getting the normal $n=\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}-1\right)$. It can be shown that the same idea repeats in $d=3$ with $n=\left(\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}}-1\right)$, leading us to the following generalization.
    Let $e_{d+1}=\left(0,\dots,0,1\right)\in\mathbb{R}^{d+1},u=\left(\frac{1}{\sqrt{d+1}},\dots,\frac{1}{\sqrt{d+1}}\right)\in~\mathbb{R}^{d+1}$. Define $n:=\left(-\frac{1}{\sqrt{d+1}},\dots,-\frac{1}{\sqrt{d+1}},1-\frac{1}{\sqrt{d+1}}\right)$ (this is the same as our $d=2,3$ cases, only with an opposite sign). We are now going to show that this vector, when used with the Householder matrix, will rotate $H_0$ to "the floor"---i.e. reflection onto the $[X_1,\dots,X_d]$ plane. Using $\hat{n}=\frac{n}{\|n\|}$ to get a normalized vector, we can calculate the Householder matrix, getting
    \begin{align}
        P=
        \begin{bNiceArray}{cw{c}{1cm}c|c}[margin]
            \Block{3-3}<\Large>{I_d - \frac{1}{D-\sqrt{D}}\mathds{1}} 
            & & & \dfrac{1}{\sqrt{D}} \\
            & & & \Vdots \\
            & & & \dfrac{1}{\sqrt{D}} \\
            \hline
            \dfrac{1}{\sqrt{D}} & \dots& \dfrac{1}{\sqrt{D}} & \dfrac{1}{\sqrt{D}}
        \end{bNiceArray}.
    \end{align}
    We need to show that for all $i\leq d+1$, the last element of $v=PG_{\AN}^t\cdot e_i$ is always zero. First, for all $i<d+1$, we easily get
    \begin{align*}
        PG_{\AN}^t\cdot e_i=P\cdot
        \begin{pmatrix}
            1 \\
            0 \\
            \vdots \\
            0 \\
            -1 \\
            0 \\
            \vdots \\
            0
        \end{pmatrix}.
    \end{align*}
    But, in the matrix $P$, the last elements of all $i<d+1$ columns are all equal. This means that taking $+1$ of one and $-1$ of the other will give us 0 there. We are left with $e_{d+1}$, which gives us
        \begin{align*}
        PG_{\AN}^t\cdot e_{d+1}=P\cdot
        \begin{pmatrix}
            -\frac{D-1}{D} \\
            \frac{1}{D} \\
            \vdots \\
            \frac{1}{D}
        \end{pmatrix}.
    \end{align*}
    Looking specifically at the last element, we get
    \begin{align*}
        \frac{1}{\sqrt{D}}\cdot\frac{1-D}{D} + (D-1)\frac{1}{D}\frac{1}{\sqrt{D}}=\frac{1-D+D-1}{D\sqrt{D}}=0,
    \end{align*}
    showing what we needed.
    % Let $\angle\left(u,v\right)$ denote the angle between the vectors $u,v$. Then we get that $\cos\left({\angle\left(u,n\right)}\right) = \cos\left({\angle\left(-e_{d+1},n\right)}\right)$:
    % \begin{align*}
    %     \cos\left({\angle\left(u,n\right)}\right) = \frac{u\cdot n}{\|u\|\|n\|} = \frac{\frac{-1}{d+1}\cdot d + \frac{1}{\sqrt{d+1}} - \frac{1}{d + 1}}{\|n\|} = \frac{\frac{1}{\sqrt{d+1}} - 1}{\|n\|}, \\
    %     \cos{\angle\left(\left(-e_{d+1}\right),n\right)} = \frac{\left(-e_{d+1}\right)\cdot n}{\|-e_{d+1}\|\|n\|} = \frac{\frac{1}{\sqrt{d+1}} - 1}{\|n\|},
    % \end{align*}
    % In $d=2$ the three vectors $-e_{d+1},u,n$ are on the same 3D-plane, as we have
    % \[
    %     u\times n = n \times (-e_{d+1}) = (\frac{1}{\sqrt{D}}, \frac{1}{\sqrt{D}}, 0).
    % \]
    % For three vectors $-e_{d+1},u,n$ on the same plane to define two angles that have the same cosine value, they need to have angles $\pi+x,\pi-x$. But these three unique vectors define three unique angles, and $\pi+x +\pi-x=2\pi$, which is a contradiction. This means that their angles are equal, finishing our claim for $d=2$. This serves as a motivation for defining this $n$ vector for all $d$ dimensions. I
    
    
    Using this, all we have left to do is use an embedding that ignores the last element, meaning $E:\mathbb{R}^{n+1}\Rightarrow\mathbb{R}^d$ defined by $E\left(x_1,\dots,x_d,x_{d+1}\right)=\left(x_1,\dots,x_d\right)$. We finish with the following definition that helps us explicitly calculate points in $A_d^*$ (using (\ref{eq:generaror}), (\ref{eq:reflection})):
    \begin{align}
        T: \mathbb{Z}^d\Rightarrow\mathbb{R}^d \nonumber \\
        T\left(g\right) = EPG^t\left(g\right).
    \end{align}
    \itai{a new explanation about T. }
    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
            \includegraphics[width=\textwidth]{Images/EPGt_visual_explanation1.png}
            \caption{Sample points in $H_0$}
            \label{fig:epgt_visual1}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
            \includegraphics[width=\textwidth]{Images/EPGt_visual_explanation2.png}
            \caption{$H_0$ rotated to the "floor"}
            \label{fig:epgt_visual2}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
            \includegraphics[width=\textwidth]{Images/EPGt_visual_explanation3.png}
            \caption{The samples as they look in $\mathbb{R}^2$}
            \label{fig:epgt_visual3}
        \end{subfigure}
        \caption{Lowering dimensions in $A_2^*$}
        \label{fig:egpt_visual}
    \end{figure}
    This is what the transformation does: first (Fig.~\ref{fig:epgt_visual1}), $G^t$ takes an integer vector in $\mathbb{R}^d$ and turns it to a vector in $\AN\subset\mathbb{R}^{d+1}$. Next (Fig.~\ref{fig:epgt_visual2}), $P$ takes the element in $\AN$ and reflects it into $[X_1,\dots,X_d]$---effectively lowering its dimension. Lastly (Fig.~\ref{fig:epgt_visual3}), because we are still in $\mathbb{R}^{d+1}$, we use $E$ to embed it into $\mathbb{R}^d$.
    
    \itai{changed all N to D in the following sections and changed a bit the order of things}
    Formally, we start by defining the embedding $E$. Let $D:=d+1$, then
    \begin{align*}
        E=
        \begin{pmatrix}
            1 & 0 & \dots & 0 & 0 \\
            0 & 1 & \dots & 0 & 0 \\
            . & . & \dots & . & 0 \\
            0 & 0 & \dots & 1 & 0
        \end{pmatrix},
    \end{align*}
    which, together with $P$ (in (6)), we use to calculate $EP$ 
    \begin{align*}
        \left(EP\right)^t=
        \begin{bNiceArray}{cw{c}{1cm}c}[margin]
            \Block{3-3}<\Large>{I_d - \frac{1}{D-\sqrt{D}}} 
            & &  \\
            & &  \\
            & &  \\
            \hline
            \dfrac{1}{\sqrt{D}} & \Cdots& \dfrac{1}{\sqrt{D}}
        \end{bNiceArray}\in\mathbb{R}^{d\times(d+1)}.
    \end{align*}
    From here it can be shown that
    \begin{align}
        T^t=G\left(EP\right)^t=
        \begin{pmatrix}
            1 & -1 &  0  & \dots & 0 \\
            1 & 0  &  -1 & \dots & 0 \\
            . & .  &  .  & \dots & . \\
            1 & 0  &  0  & \dots & -1 \\
            \frac{1}{D - \sqrt{D}} - 1 & \frac{1}{D - \sqrt{D}} & \frac{1}{D - \sqrt{D}} & \dots & \frac{1}{D - \sqrt{D}}
        \end{pmatrix}\in\mathbb{R}^{d\times d}
    \end{align}
    Being that reflections are isometries, and so are embeddings, we can use the same covering radius after using $T$. So, we are given the rescaling coefficient through the covering radius (~\cite[page 115]{conway2013sphere}):
    \[    
        \beta = \sqrt{\frac{d\left(d+2\right)}{12\left(d+1\right)}}w_{\AN} \Rightarrow f_{\AN}\left(d\right)=\sqrt{\frac{d\left(d+2\right)}{12\left(d+1\right)}},
    \]
    from which we get our set definition:
    \begin{align*}
        \XA = \{x\in\mathbb{R}^d|x=w_{\AN}T\cdot v,v\in\mathbb{Z}^d\} = \\
        \{x\in\mathbb{R}^d|x=\sqrt{\frac{12\left(d+1\right)}{d\left(d+2\right)}}\beta T\cdot v,v\in\mathbb{Z}^d\}\, \qedsymbol
    \end{align*} 
\end{proof}
% The above result directly follows from~\cite{verger2005covering} which provides a lower bound on the number of $\tfrac{1}{2}$-balls necessary to cover a ball of radius $R \geq \frac{d}{2}$. A simple rescaling argument is used to make it applicable to our setting where $\beta\left(\de\right)$-balls are used for covering the configuration space. \kiril{Does this description give justice to what you derived or is there something more complicated that I'm missing here?}

% This is a fair upper bound on $\beta=\delta\alpha$, as $\alpha \leq 1$ and we already take $\delta$ as small value, usually. For example, for a 10 degree problem, we would need $\beta\leq\frac{1}{6}\approx 0.17$---then for $\delta = 0.1$ we would need $\epsilon < 1.7$. \kiril{I don't understand this discussion. Please clarify.}
% NOTE \itai{I commented out all this part \left(see tex file\right). Its irrelant as I rewrote the theorem.}
% Let us compare this bound with the previous bound~\cite[Theorem~1]{tsao2020sample}, which employed volume arguments. \kiril{Is this the bound you had in mind?} In particular, the previous bound is at least 
% \kiril{please fill in.}
% Thus, the new upper bound is bigger by a factor of \kiril{fill} than the old one. 

% \kiril{The following text needs to be clarified as in the first equation the new bound is stated and I substituted it with the text above.}
% {\color{blue} To compare this LB to the "classic" volume-derived lower bound, let us return to a configuration space defined in a ball of general radius $r\geq \beta d$. \kiril{return from where? let's keep everything consistent and keep the same setting throughout the paper unless absolutely necessary.}
% We get the following lower bound:
% \begin{align}
%     n \geq k d\cdot\left(2r_0\right)^d =k\cdot d \left(\frac{r}{\beta}\right)^d 
% \end{align}
% NOTE \itai{I commented out all this part also \left(see tex file\right). To clarify: this is a comparison to the classic "volume divided by volume" lower bound, I didn't compare this lower bound to the one in tsao. you dealt with a square C-Space there and I found it hard to compare your approximation to this one.}
% \begin{proof}
%   We rely on , which states that if a sample set $\X$ is a for  then
% \begin{align}
%    |\X| \geq k\cdot d \cdot\left(2R\right)^{d},\,k>0. 
% \end{align}
% \kiril{What is the purpose of $k$? Does it hold for any $k$? If so, can we substitute $\geq$ with $>$ and get rid of $k$?}
% In our setting, we wish to construct a $\beta\left(\de\right)$-cover of our configuration space. 
% We want $\frac{1}{2}$-balls in~\cite{verger2005covering}  to be mapped to balls of radius $\beta\left(\de\right)$-balls, so we need a rescaling factor of $2\beta$. This means that if we want to cover a general ball of radius $r$ by $\beta$-balls, then we need a ball of initial radius $R$ \left(before the rescaling\right) such that:
% \[
%     R\cdot 2\beta=rarrow R=\frac{r}{2\beta}
% \]
% Taking $R\geq\frac{d}{2}$ to fit Verger's result, we reach the following bound on $r>0$:
% \begin{align*}
%     \frac{d}{2} \leq R = \frac{r}{2\beta} arrow r \geq \beta d
% \end{align*}
% To get a feeling for this bound, let us choose our configuration space as a ball of radius $r=\frac{\sqrt{d}}{2}$. This is logical because it is the circumsphere of the regular $[0,1]^d$ cube. With this we need to have:
% \[
%     \frac{\sqrt{d}}{2} \geq \beta d arrow \beta \leq \frac{1}{2\sqrt{d}}
% \]
% \end{proof}
\itai{rewritten section}
% We are now going to find new upper bounds for our newly defined sample sets \Lattices. We name an upper bound for a sample set by the name \emph{sample complexity}. 
% Throughout the analysis, we assume that the configuration space is the Euclidean $d$-dimensional $R$-ball denoted by $\B_R$. This assumption allows us to obtain tighter bounds, than for, say, unit hypercube configuration spaces, which were assumed in previous sample-complexity analysis. 
% To this end, we first introduce the \emph{Gram matrix} of a lattice, relevant to the following Theorem:
%   \begin{definition}[Gram matrix and quadratic form]
%       The \emph{Gram matrix} of a lattice $\Lambda$ with generator matrix $G_\Lambda$ is defined to be $Gr_\Lambda:=G_\Lambda G^t_\Lambda$. Additionally the quadratic form of $\Lambda$ is defined as $q_\Lambda\left(a\right):=aGr_\Lambda a^t$.
%   \end{definition}
In this next theorem, we derive analytical upper bounds for \Lattices, a property we call \emph{sample complexity}. To achieve that, we first derive an upper bound for general lattices. Later on, in our experimental results (Section~\ref{sec:experiments}), we compare the sample complexity of those sets for varying values of $\de$, and $d$. 

\begin{thm}[General lattice sample complexity]
\label{general_sample_complexity}
    Consider a lattice $\Lambda$ that defines the \decomp set $\XL$ in a C-Space $\B_R$. Recall that $\beta=\beta\left(\delta,\epsilon\right)$ is explicitly given in equation (1). Then we get the sample complexity:
    \begin{align*}
        |\XL\cap \B_R| = O\left(\frac{1}{d}\left(\left(\frac{R}{\beta}\right)^2\frac{2\pi e}{d}\cdot f_\Lambda^2\left(d\right)\right)^{\frac{d}{2}} + \left(\frac{R}{\beta}\cdot f_\Lambda\left(d\right)\right)^{d-2}\right).
    \end{align*}
\end{thm}
\begin{proof}
  We wish to estimate the size of sample set $\X_\Lambda$ induced by the lattice $\Lambda$ and the scaling factor $f_\Lambda\left(d\right)$. We first show that this is equivalent to estimating the expression $|\B_R\cap \Lambda|$. Let $v=aG_\Lambda,\,a\in\mathbb{Z}^d$ be a lattice point, then by definition of a \emph{Gram matrix} as being $Gr_\Lambda:=G_\Lambda G_\Lambda^t$, we obtain 
  \[
    aGr_\Lambda a^t:=aG_\Lambda G^t_\Lambda a^t=\|aG_\Lambda\|^2=\|v\|^2.
  \]
  This leads to the relation
  \begin{align*}
      \B_R\cap \Lambda &= \{v\in\Lambda |\,\|v\| \leq R\} =\{a\in\dZ^d|a Gr_\Lambda a^t\leq R^2\} \\ & =E_{R^2}\left(Gr_\Lambda\right)\cap \dZ^d,
  \end{align*}
  where $E_{s}\left(A\right):=\{x\in\dR^d|x A x^t\leq s\}$. 
  
  
  Next, observe that the Gram matrix $Gr_\Lambda$ of the lattices \Lattices contains only rational numbers, and hence defines what is called a \emph{"rational quadratic form"}. This allows us to use Landau's \emph{rational ellipsoid bound theorem}~\cite{ivic2004lattice} which asserts that
  \begin{equation*}
      |E_{r^2}\left(Gr_\Lambda\right)\cap \dZ^d|=\frac{\vol\left(\B_{s}\right)}{\sqrt{d}}+\Theta\left(s^{d-2}\right),
  \end{equation*}
hence,
\begin{align}\label{volume_delta}
    |\B_R\cap \Lambda|=\frac{\vol\left(\B_{R}\right)}{\sqrt{d}}+\Theta\left(R^{d-2}\right).
\end{align}
As~(\ref{volume_delta}) deals with the unscaled lattice, we now use the rescaling actions performed in Theorem 1 to move "back" to the "unscaled" version of our spherical C-Space, in order to reach the sample complexity evaluation:
\begin{align}
    \|\XL\cap \B_R\|\underset{[T2]}{=}\|\Lambda\cap\B_{R_{orig}}\|\underset{\left(\ref{volume_delta}\right)}{\leq}\frac{1}{\sqrt{\pi}d}\left(\frac{2\pi e}{d}\right)^{\frac{d}{2}}R_{orig}^d+c\left(R_{orig}^{d-2}\right) \nonumber\\
    \underset{[T2]}{=}\frac{1}{\sqrt{\pi}d}\left(\frac{R}{\beta}\right)^d\left(\frac{2\pi e}{d}\cdot f_\Lambda^2\left(d\right)\right)^{\frac{d}{2}} +  c\left(\frac{R}{\beta}\right)^{d-2}\left(f_\Lambda\left(d\right)\right)^{d-2} \nonumber\\
    \Rightarrow \|\XL\cap \B_R\| \leq \frac{1}{\sqrt{\pi}d}\left(\frac{R}{\beta}\right)^d\left(\frac{2\pi e}{d}\cdot f_\Lambda^2\left(d\right)\right)^{\frac{d}{2}} +  c\left(\frac{R}{\beta}\right)^{d-2}\left(f_\Lambda\left(d\right)\right)^{d-2},
\end{align}
with some $c>0$ from~(\ref{volume_delta}). This finishes the proof.  
\end{proof}

The following corollary leverages the last theorem and the specific structure of \Lattices to derive specific upper bounds.

\begin{cor}[$\XZ,\XD,\XA$ sample complexity]
    Fix $\beta=\beta\left(\delta,\epsilon\right)$ and let $\theta=~\theta\left(\delta,\epsilon\right):=~\frac{R}{\beta}$. Consider the configuration space  $\C=\B_R$ for $R=\sqrt{d}\beta$ or $R \geq d\beta$. Then the following bounds hold:
    \begin{enumerate}[topsep=1pt,itemsep=1ex,partopsep=1ex,parsep=1ex]
        \item 
            $|\XZ\cap\B_R| = O\left(\frac{1}{d}\left[\left(2.07\theta\right)^d+\left(\frac{d}{4}\theta\right)^\frac{d-2}{2}\right]\right)$
        \item 
            $|\XD\cap\B_R| = O\left(\frac{1}{d}\left[\left(1.46\theta\right)^d + \left(\frac{d}{8}\theta - \frac{1}{16}\theta\right)^\frac{d-2}{2}\right]\right)$
        \item 
        $|\XD\cap\B_R| = O\left(\frac{1}{d}\left[\left(1.19\theta\right)^d+\left(\frac{d}{12}\theta\right)^\frac{d-2}{2}\right]\right)$
    \end{enumerate}
\end{cor}
\begin{proof}
    First, we look at $\ZN$. From Theorem~\ref{general_sample_complexity}, we get
    \begin{align*}
        |\XZ\cap \B_R| = O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{d}\cdot f_{\ZN}^2\left(d\right)\right)^{\frac{d}{2}} + \left(\theta\cdot f_{\ZN}\left(d\right)\right)^{d-2}\right) \\
        = O\left(\frac{1}{d}\left(\theta\sqrt{\frac{\pi e}{2}}\right)^d+\left(\theta\frac{\sqrt{d}}{2}\right)^{d-2}\right)\\
        = O\left(\frac{1}{d}\left[\left(2.07\theta\right)^d+\left(\frac{d}{4}\theta\right)^\frac{d-2}{2}\right]\right).
    \end{align*}
    Next, we look at $\DN$. From Theorem~\ref{general_sample_complexity}, we get
    \begin{align*}
        |\XD\cap \B_R| = O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{d}\cdot f_{\DN}^2\left(d\right)\right)^{\frac{d}{2}} + \left(\theta\cdot f_{\DN}\left(d\right)\right)^{d-2}\right) \\
        = O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{d}\cdot \frac{2d-1}{16}\right)^{\frac{d}{2}}+\left(\theta\frac{\sqrt{2d-1}}{4}\right)^{d-2}\right)\\
        =O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{8}\cdot \frac{2d-1}{2d}\right)^{\frac{d}{2}}+\left(\theta\frac{2d-1}{16}\right)^{\frac{d-2}{2}}\right).
    \end{align*}
    But, since
    \begin{align*}
        \left(\frac{2d-1}{2d}\right)^\frac{d}{2}=\left(\left(1+\frac{\left(-1\right)}{2d}\right)^{2d}\right)^\frac{\frac{d}{2}}{2d}\overset{n\rightarrow\infty}{\longrightarrow} e^{\frac{1}{4}},
    \end{align*}
    we obtain:
    \begin{align*}
         \|\XD\cap \B_R\|=O\left(\frac{1}{d}\left(\theta\sqrt{\frac{\pi e}{4}}\right)^d+\left(\theta\frac{2d-1}{16}\right)^{\frac{d-2}{2}}\right)\\
         = O\left(\frac{1}{d}\left[\left(1.46\theta\right)^d + \left(\frac{d}{8}\theta - \frac{1}{16}\theta\right)^\frac{d-2}{2}\right]\right).
    \end{align*}
    Lastly, we look at $\AN$. From equation (7) above it can be seen that
    \[
        T^tT=G\left(EP\right)^t\left(EP\right)G^t=
        \begin{pmatrix}
            2 & 1 &  1  & \dots & 1 & -1 \\
            1 & 2  &  1 & \dots & 1 & -1 \\
            . & .  &  . & \dots & . & .  \\
            1 & 1  &  1 & \dots & 2 & -1 \\
            -1 & -1 & -1 & \dots & -1 & x
        \end{pmatrix}\in\mathbb{R}^{d\times d}, x=\frac{D-1}{D}.
    \]
    Furthermore, it can be shown that $T^tT=GG^t=Gr_\Lambda$. This means that we can use Theorem~\ref{general_sample_complexity} to approximate the number of lattice points in the embedded lattice, as they both share the gram matrix $Gr_\Lambda$---which intuitively makes sense, as they both represent the lattice (only that one of them is in a higher dimension).
    From Theorem~\ref{general_sample_complexity}, we get:
    \begin{align*}
        |\XD\cap \B_R| = O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{d}\cdot f_{\AN}^2\left(d\right)\right)^{\frac{d}{2}} + \left(\theta\cdot f_{\AN}\left(d\right)\right)^{d-2}\right) \\
        = O\left(\frac{1}{d}\left(\theta^2\frac{2\pi e}{d}\cdot \frac{d\left(d+2\right)}{12\left(d+1\right)}\right)^{\frac{d}{2}}+\left(\theta\sqrt{\frac{d\left(d+2\right)}{12\left(d+1\right)}}\right)^{d-2}\right)\\
        =O\left(\theta^2\frac{1}{d^2}\left(\frac{2\pi e}{12}\cdot \frac{d+2}{d+1}\right)^{\frac{d}{2}}+\left(\frac{d}{12}\theta\right)^{\frac{d-2}{2}}\left(\frac{d+2}{d+1}\right)^{\frac{d-2}{2}}\right).
    \end{align*}
    But, since
    \begin{align*}
        \left(\frac{d+2}{d+1}\right)^\frac{d}{2}=\left(\left(1+\frac{1}{d+1}\right)^{d+1}\right)^\frac{\frac{d}{2}}{d+1}\overset{n\rightarrow\infty}{\longrightarrow} e^{\frac{1}{2}},\\
    \end{align*}
    and similarly for the right term, we obtain
    \begin{align*}
         |\XA\cap \B_R| =O\left(\frac{1}{d}\left(\sqrt{\theta\frac{\pi e}{6}})^d+\left(\frac{d}{12}\theta\right)^{\frac{d-2}{2}}\right)\right)\\
         = O\left(\frac{1}{d}\left[\left(1.19\theta\right)^d+\left(\frac{d}{12}\theta\right)^\frac{d-2}{2}\right]\right).
    \end{align*}
\end{proof}
    \itai{made changes here}
\subsection*{Comparing sample complexity for \Lattices}
    What is quickly clear from these results, is that in terms of sample complexity we get increasingly better evaluations going for the lattices: $\mathbb{Z}^d\rightarrow D_d^*\rightarrow A_d^*$. Both the first term and the error term of the sample complexity expression get better: 
    \begin{enumerate}
        \item The first term gets exponentially better, going from $2.07^d$ to $1.46^d$ and $1.19^d$.
        \item The error term also gets better, with its base going from $\frac{d}{4}$ to $\frac{d}{8}$ and $\frac{d}{12}$. 
    \end{enumerate}
    In both comparisons we disregarded the common $\theta$ term.
    Combining these two improvements, we see that $A_n^*$ is the superior lattice here, in terms of lattice sample density.


    Next, we show that $\XA$ leads to even better performance in terms \emph{CC-complexity}.

% We complete this section with an analysis of the CC complexity of our sample sets. 
% \begin{thm}[$\XZ,\XD,\XA$ CC complexity]
%   Consider the configuration space $\B_{r_c}$ for (rc=?). Then the following upper bounds on CC complexity hold: \kiril{Why is this missing information?}
%     \begin{enumerate}
%         \item 
%         \begin{align*}
%             \frac{\|\chi_{D_d^*,C-Space\|}}{\|\chi_{LB}\|}= ?? \\
%             \|\chi_{D_d^*,\beta-ball}\|= ??
%         \end{align*}        
%         \item 
%         \begin{align*}
%             \frac{\|\chi_{A_d^*,C-Space\|}}{\|\chi_{LB}}\|\ = ?? \\
%             \|\chi_{A_d^*,\beta-ball}\|= ??
%         \end{align*}
%     \end{enumerate}
% \end{thm}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
