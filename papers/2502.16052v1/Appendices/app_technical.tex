\section{Proofs of technical lemmas}\label{app:tech}
% \ac{proof of submitting $n^\star_i$ should come first so that we don't need to worry about $|Y_i|$ being a random variable. }

\begin{lemma}
   \label{lem:NIC-part1} 
   Fix any $(\nni,\subfunci)$. Let $f_a:\dataspace\rightarrow\RR^{\nnopti}$ be a function such that $\forall \initdatai\in\dataspace$, $\muhat\rbr{f_a(\initdatai)}=\muhat\rbr{\subfunci(\initdatai)}$. Then,
    \begin{align*}
        \utilci \rbr{M, \rbr{(\nni,\subfunci),\strat_{-i}^{\star} }}
        \leq
        \utilci \rbr{M, \rbr{(\nni,f_a),\strat_{-i}^{\star} }} .
    \end{align*}
\end{lemma}
\begin{proof}
    Under $M$ we have that 
    \begin{align*}
        &\hspace{0.4cm}
        \utilci \rbr{M, \rbr{(\nni,\subfunci),\strat_{-i}^{\star} }}
        \\
        &=
        \inf_{\mu\in\RR}
        \EE\Bigg[
            \mathbb{I}\left( \left| Y_i \right| = \nnopti \right) 
            \rbr{
                \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
            }
        % \\
        % &\hspace{3cm}
            -  d_i\rbr{
                \muhat(\subfunci(\initdatai))-\muhat(\subdatami)
            }^2
            -\costi\nni
        \Bigg]
        \\
        &\leq
        \inf_{\mu\in\RR}
        \EE\Bigg[
            \rbr{
                \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
            }
            -  d_i\rbr{
                \muhat(f_a(\initdatai))-\muhat(\subdatami)
            }^2
            -\costi\nni
        \Bigg]
        \\
        &=
        \utilci \rbr{M, \rbr{(\nni,f_a),\strat_{-i}^{\star} }} .
    \end{align*} 
    The inequality follows from the definition of $f_a$. 
\end{proof}

\begin{lemma}
   \label{lem:NIC-part2} 
   Fix any $\nni$. Consider any $f_a:\dataspace\rightarrow\RR^{\nnopti}$ and let $f_b:\dataspace\rightarrow\RR^{\nnopti}$ be a function such that 
   $\forall \initdatai\in\dataspace$, $\muhat\rbr{f_b(\initdatai)}=\muhat\rbr{\initdatai}$. Then,
   % Fix any $\nni$. Let $f_a:\dataspace\rightarrow\dataspace$ be a function such that $\forall \initdatai\in\dataspace$, $\muhat\rbr{f_a(\initdatai)}=\muhat\rbr{\initdatai}$. Let $f_b:\dataspace\rightarrow\RR^{\nnopti}$ be a function such that
   %  $\forall \initdatai\in\dataspace$, $\muhat\rbr{f_b(\initdatai)}=\muhat\rbr{f_a(\initdatai)}$. Then,
    \begin{align*}
        \utilci \rbr{M, \rbr{(\nni,f_a),\strat_{-i}^{\star} }}
        % \qquad
        \leq
        \utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }} .
    \end{align*}
\end{lemma}
\begin{proof}
    From the definition of $f_a$ we have
    % Since $f_a$ maps to $\RR^{\nnopti}$, the indicator term in the payment function is always 1, so we get
    \begin{align*}
        % &\hspace{0.45cm}
        \utilci \rbr{M, \rbr{(\nni,f_a),\strat_{-i}^{\star} }}
        % \\
        &=
        \inf_{\mu\in\RR}
        \EE\Bigg[
            \rbr{
                \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
            }
            -  d_i\rbr{
                \muhat(f_a(\initdatai))-\muhat(\subdatami)
            }^2
            -\costi\nni
        \Bigg]
        \\
        &=
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        +d_i
        \inf_{\mu\in\RR}
        \EE\sbr{
            -
            \rbr{
                \muhat(f_a(\initdatai))-\muhat(\subdatami)
            }^2
        }
        \\
        &=
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -d_i
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(f_a(\initdatai))-\muhat(\subdatami)
            }^2
        }.
        \numberthis
        \label{eq:b2-largeeq}
    \end{align*} 
    Now notice that because the other agents truthfully submit a combined $\nnopt_{-i}$ points under $\strat_{-i}^{\star}$, we have that 
    \begin{align*}
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(f_a(\initdatai))-\muhat(\subdatami)
            }^2
        }
        &=
        \sup_{\mu\in\RR}
        \rbr{
            \EE\sbr{
                \rbr{
                    \muhat(f_a(\initdatai))-\mu
                }^2
            }
            +
            \EE\sbr{
                \rbr{
                    \muhat(\subdatami)-\mu
                }^2
            }
        }
        \\
        &=
        \sup_{\mu\in\RR}
        \rbr{
            \EE\sbr{
                \rbr{
                    \muhat(f_a(\initdatai))-\mu
                }^2
            }
        }
        +\frac{\sigma^2}{\nnopt_{-i}} .
    \end{align*}
    Therefore, we can rewrite~\eqref{eq:b2-largeeq}, using the fact that the sample mean is minimax optimal~\citep{lehmann2006theory} and the definition of $f_b$, to get
    \begin{align*}
        &\hspace{0.4cm}
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -\frac{d_i\sigma^2}{\nnopt_{-i}}
        -d_i
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(f_a(\initdatai))-\mu
            }^2
        }
        \\
        &\leq
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -\frac{d_i\sigma^2}{\nnopt_{-i}}
        -d_i
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(\initdatai)-\mu
            }^2
        }
        \\
        &=
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -d_i
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(\initdatai)-\muhat(\subdatami)
            }^2
        }
        \\
        &=
        \utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }} .
    \end{align*}
    
\end{proof}
% \ac{change $Y_i$ to $f_a(X_i)$ and $f_b(X_i)$ throughout lemmas!!!}

\begin{lemma}
   \label{lem:NIC-part3} 
   Fix any $\nni$. Let $f_b:\dataspace\rightarrow\RR^{\nnopti}$ be a function such that
    $\forall \initdatai\in\dataspace$, $\muhat\rbr{f_b(\initdatai)}=\muhat\rbr{\initdatai}$.
    Then,
    \begin{align*}
        \utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }}
        \leq
        \utilci \rbr{M, \rbr{\strat_{i}^{\star},\strat_{-i}^{\star} }} .
    \end{align*}
\end{lemma}
\begin{proof}
    From the definition of $f_b$ we have that 
    \begin{align*}
        \utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }}
        &=
        \inf_{\mu\in\RR}
        \EE\Bigg[
            \rbr{
                \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
            }
            -  d_i\rbr{
                \muhat(f_b(\initdatai))-\muhat(\subdatami)
            }^2
            -\costi\nni
        \Bigg]
        \\
        &=
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -d_i
        \sup_{\mu\in\RR}
        \EE\sbr{
            \rbr{
                \muhat(\initdatai)-\muhat(\subdatami)
            }^2
        }
        \\
        &=
        \rbr{
            \profi \frac{\nnopti}{\numdataA} + \cost_i \nnopti  +  \frac{d_i\sigma^2}{\nnopt_{-i}} +  \frac{d_i\sigma^2}{ \nnopti} 
        }
        -\costi\nni
        -\frac{d_i\sigma^2}{\nni}
        -\frac{d_i\sigma^2}{\nnopt_{-i}}
        .
    \end{align*}
    Define the concave function $l:\RR_+\rightarrow\RR$, given by $l(\nni)=-\costi\nni-\frac{d_i\sigma^2}{\nni}$. Observe that $l'(\nni)=-\costi+\frac{d_i\sigma^2}{\nni^2}$ so $l'(\nni)=0\iff \nni=\sigma\sqrt{\frac{d_i}{\costi}}=\nnopti$. Therefore the choice of $\nni$ which maximizes $\utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }}$ is $\nnopti$. Together with the fact that $\muhat(f_b(\initdatai))=\muhat(\initdatai)=\muhat(\identity(\initdatai))$ we conclude
    \begin{align*}
        \utilci \rbr{M, \rbr{(\nni,f_b),\strat_{-i}^{\star} }}
        \leq
        \utilci \rbr{M, \rbr{(\nnopti,f_b),\strat_{-i}^{\star} }}
        =
        \utilci \rbr{M, \rbr{(\nnopti,\identity),\strat_{-i}^{\star} }}
        =
        \utilci \rbr{M, \rbr{\strat_{i}^{\star},\strat_{-i}^{\star} }} .
    \end{align*}
\end{proof}


%\kkcomment{@Keran, Please organize this proof as you have done for other proofs, adding restatable etc. Also, check if the proof is correct. I simplified it a bit.}

% \subsection{Proof of Lemma~\ref{lem:main_prop5}}

%\Lempropfive*

\Lempropfive*


\begin{proof}[Proof of Lemma~\ref{lem:main_prop5}]
First, let $\itemprice' = A\rbr{\cbr{\valj}_{j \in \buyers},\optnone,\eps} $ denote the price curve obtained by applying $A$ to the buyers' valuations $\cbr{\valj}_{j \in \buyers}$, total amount of data $\optnone$, and the approximation parameter $\eps$. 
% We can upper bound $\blprofit$ as follows:
% Let $\itemprice' = A\rbr{\cbr{\valj}_{j \in \buyers},\optnone,\eps} $.
Using the definition of $\blprofit$ in equation~\eqref{eqn:blprofittwo} and the equivalence between envy-free pricing schemes and ordered item pricing curves (see Lemma~\ref{lem:equal_two_problem} and the discussion above it), we have
   \begin{equation*}     
      \blprofit 
      % & 
      = \;\OPTREVmech\left(\optnone\right) - \costone\optnone  = \; \OPTREV(\optnone) - \costone\optnone  .
      % \\ & \leq \; \ExpectREV(\optnone, \itemprice') + |\buyers|\bigO(\epsilon)- \costone\optnone  
      % \; \leq \; \ExpectREV(\numdataA, \itemprice_{\numdataA})- \cost_1 \numdataA  + |\buyers|\bigO(\epsilon) \\ & = \; \efprofit(\efscheme) + |\buyers|\bigO(\epsilon) 
   \end{equation*}
% Here, the first step follows from the definition of $\blprofit$ in equation~\eqref{eqn:blprofittwo}. The second step uses the equivalence between envy-free pricing schemes and ordered item pricing curves (see Lemma~\ref{lem:equal_two_problem} and the discussion above it).
Since algorithm $A$ returns a $|\buyers|\bigO(\eps)$ solution we get
\begin{align*}
    \OPTREV(\optnone) - \costone\optnone
    \leq
    \ExpectREV(\optnone, \itemprice') + |\buyers|\bigO(\epsilon)- \costone\optnone .
\end{align*}
% In the third step we have used the fact that the algorithm $A$ returns a $|\buyers|\bigO(\eps)$ solution.
Finally, from the definition of $\numdataA$: $  \numdataA = \arg\max_{N \in \cbr{1,2,\dots, \frac{|\buyers|}{\numdata}} } {\rm profit}_N $, since $\optnone \leq \frac{|\buyers|}{\numdata}$, we have $ {\rm profit}_{\numdataA} =\ExpectREV(\numdataA, \itemprice_{\numdataA})- \cost_1 \numdataA  \geq  \ExpectREV(\optnone, \itemprice') - \cost_1 \optnone $. Therefore,
\begin{align*}
    \ExpectREV(\optnone, \itemprice') + |\buyers|\bigO(\epsilon)- \costone\optnone  
      \; &\leq \; \ExpectREV(\numdataA, \itemprice_{\numdataA})- \cost_1 \numdataA  + |\buyers|\bigO(\epsilon) 
      \\
      &= \; \efprofit(\efscheme) + |\buyers|\bigO(\epsilon)
\end{align*}
so we conclude that 
\begin{equation*}
    \efprofit(\efscheme) 
    \geq 
    \blprofit
    - 
    |\buyers|\bigO(\epsilon) .
\end{equation*}


% The fourth step is by definition of $\numdataA$: $  \numdataA = \arg\max_{N \in \cbr{1,2,\dots, \frac{|\buyers|}{\numdata}} } {\rm profit}_N $, since $\optnone \leq \frac{|\buyers|}{\numdata}$, we have $ {\rm profit}_{\numdataA} =\ExpectREV(\numdataA, \itemprice_{\numdataA})- \cost_1 \numdataA  \geq  \ExpectREV(\optnone, \itemprice') - \cost_1 \optnone $.
\end{proof} 




% \section{Proof }

% \begin{lemma} \label{lem:nash}
%     The recommended strategy $\strat^{\star}$ as defined in Algorithm~\ref{alg:mechanism} is Nash equilibrium in mechanism $M$.
% \end{lemma}
% \begin{proof}

% Contributor $i$'s payment is
% \begin{align*} 
% & \pay_i (\mech,\strat_i,\strat_{-i}^{\star})  =   \mathbb{I}\left( | Y_i | = \nnopti \right) 
% \bigg( 
% w \frac{\nnopti}{\numdataA} + \cost_i \datanum_i  + d_i \frac{\sigma^2}{\nnopt_{-i}} + d_i \frac{\sigma^2}{ \nnopti} 
% \bigg)  - d_i \bigg( \hat{\mu}(Y_i) - \hat{\mu}(Y_{-i}) \bigg)^2,  \numberthis 
% \end{align*}


% where $d_i= \frac{\cost_i (\nnopti)^2}{ \sigma^2}$. We prove the nash equilibrium in three steps.

% \textbf{First step}: When fixing the number of data collected $n_i$ and the number of data submitted $\left| Y_i \right|$ by contributor $i$ , we see that, according to Lemma \ref{lem:minimax} and Lemma \ref{lem:same_minimax}, $ \hat{\mu}\left(X_i\right)$ is minimax estimator of $\mathbb{E}\rbr{\rbr{\hat{\mu}(Y_i)-\hat{\mu}(Y_{-i})}^2 \;\middle|\; P} $, i.e., 
% \begin{align*}
%   \hat{\mu}(X_i) = \underset{\hat{\mu}}{\arg\min} \sbr{\sup _\distrifamily \mathbb{E}\left[\sum_{j=1}^{m} d_i \rbr{ \hat{\mu}(Y_i) - \hat{\mu}(Y_{-i})}^2 \;\middle|\;  P \right] },
% \end{align*}
% This implies, fixing $n_i$ and $\left| Y_i \right|$ by contributor $i$, contributor $i$ achieves the maximum utility by selecting $\hat{\mu}(Y_i) = \hat{\mu}(X_i)$. 







% \textbf{Second step}: When fixing $n_i$, and assuming that contributor $i$ submit $\hat{\mu}(Y_i)=\hat{\mu}(X_i)$,  the optimal number of data points to submit is $\left| Y_i \right| = \nnopti$. Because the payment 
% $\pay_i $ will be negative if $\left| Y_i \right| \neq \nnopti$. Therefore, for any $n_i$, we combine the first and second step:
% \begin{align*}
% & \utilci \rbr{\mech, (n_i,f_i \text{ s.t. } \hat{\mu}(Y_i)=\hat{\mu}(X_i), \left| Y_i \right| =\nnopti),\strat_{-i}^{\star} }  \geq  \utilci \rbr{M, (n_i,f_i),\strat_{-i}^{\star} }.
% \end{align*}



% \textbf{Third step}: We choose constant $d_i= \frac{\cost_i (\nnopti)^2}{ \sigma^2}$. When fixing $\hat{\mu}(Y_i)=\hat{\mu}(X_i)$ and $\left| Y_i \right| = \nnopti$, collecting $n_i = \nnopti$ amount of data maximize the contributor $i$'s utility. To see this, we write the contributor's utility given $\hat{\mu}(Y_i)=\hat{\mu}(X_i)$ and $\left| Y_i \right| = \nnopti$ as:

% \begin{align*}
%     & \utilci \rbr{\mech, (n_i, f_i \text{ s.t }\hat{\mu}(Y_i)=\hat{\mu}(X_i), \left| Y_i \right| = \nnopti),\strat_{-i}^{\star} }  
%     \\ 
%     = & \rbr{w \frac{\nnopti}{\numdataA} + \cost_i \datanum_{i}^{\star}  + d_i \frac{\sigma^2}{\nnopt_{-i}} + d_i \frac{\sigma^2}{ \nnopti} } -    d_i \rbr{ \frac{\sigma^2}{\nnopt_{-i}} +\frac{\sigma^2}{\datanum_i} } - \cost_i n_i ,
% \end{align*}


% Plug in the value $d_i= \frac{\cost_i (\nnopti)^2}{\sigma^2}$, it follows that $ d_i \frac{\sigma^2}{\datanum_i} + \cost_i \datanum_i \geq 2\cost_i \nnopti$, and this inequality takes equality if and only if  $\datanum_i = \nnopti$. Therefore, the utility is maximized when contributor $i$ collects $\datanum_i = \nnopti$ data points,
% \begin{align*}
%     &\;\utilci \rbr{\mech, \strat_{i}^{\star} ,\strat_{-i}^{\star} }\\  = & \;\utilci \rbr{\mech, (\nnopti,\mathbf{I}),\strat_{-i}^{\star} }\\  = & \;\utilci \rbr{\mech, (\nnopti, f_i \text{ s.t }\hat{\mu}(Y_i)=\hat{\mu}(X_i), \left| Y_i \right|=\nnopti),\strat_{-i}^{\star} }  \\ \geq &   \; \utilci \rbr{\mech, \strat_{i} ,\strat_{-i}^{\star}} .
% \end{align*}



% \end{proof}


 % [Proof of Lemma~\ref{lem:equal_two_problem}]





% \begin{lemma} \label{lem:minimax}
%     \ac{can't most of this proof be cut out by applying to the standard result: the sample mean is minimax optimal}
%     When others following the recommended strategy, $ \hat{\mu}\left(X_i\right)$ is the minimax estimator for the Normal distribution class $\Normaldistrib := \left\{\mathcal{N}(\mu,\sigma^2) \;\middle|\; \mu \in \mathbb{R}\right\}$:

%     \begin{align*}
%     \hat{\mu}(X_i)= \underset{\hat{\mu}}{\arg\min} \sbr{\sup _\mu \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]}
% \end{align*}
% \end{lemma}


% \begin{proof}


% Given that others following the recommended strategy, 
% \begin{align*}
%     & \ \mathbb{E}\left[ \left( \hat{\mu}\left( Y_i \right)-\hat{\mu}\left( Y_{-i} \right)  \right)^2 \right] \\ =  & \ \mathbb{E}\left[ \left( (\hat{\mu}\left( Y_i \right)-\mu) -(\hat{\mu}\left( Y_{-i} \right) -\mu) \right)^2   \right] \\ =  & \  \mathbb{E}\left[ (\hat{\mu}\left( Y_i \right)-\mu)^2  \right] +  \underbrace{\mathbb{E}\left[ (\hat{\mu}\left( Y_{-i} \right)-\mu)^2  \right]}_{\text{denoted as a constant } A_0 } \\ =  & \;  \mathbb{E}\left[ (\hat{\mu}\left( Y_i \right)-\mu)^2  \right] + A_0 ,
% \end{align*}
% \ac{can't we basically cut everything after this any apply the standard result that the sample mean is minimax optimal?}



% Thus the maximum risk can be written as:

% \begin{align*}
%     \sup _\distrifamily \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; P \right] .
% \end{align*}


% We construct a lower bound on the maximum risk using a sequence of Bayesian risks. Let $\Lambda_{\ell}:=\mathcal{N}\left(0, \ell^2\right), \ell=1,2, \ldots$ be a sequence of prior for $\mu$. For fixed $\ell$, the posterior distribution is:
% $$
% \begin{aligned}
% p\left(\mu \;\middle|\;X_i\right) & \propto p\left(X_i \;\middle|\; \mu\right) p(\mu) \\ & \propto \exp \left(-\frac{1}{2 \sigma^2} \sum_{x \in X_i}(x-\mu)^2\right) \exp \left(-\frac{1}{2 \ell^2} \mu^2\right) \\
% & \propto \exp \left(-\frac{1}{2}\left(\frac{n_i}{\sigma^2}+\frac{1}{\ell^2}\right) \mu^2+\frac{1}{2} 2 \frac{\sum_{x \in X_i} x}{\sigma^2} \mu\right) .
% \end{aligned}
% $$

% This means the posterior of $\mu$ given $X_i$ is Gaussian with:

% \begin{align*}
%     \mu \lvert\, X_i & \sim \mathcal{N}\left(\frac{n_i \hat{\mu}\left(X_i\right) / \sigma^2}{n_i / \sigma^2+1 / \ell^2}, \frac{1}{n_i / \sigma^2+1 / \ell^2}\right) 
%     \\ & =: \mathcal{N}\left(\mu_{\ell}, \sigma_{\ell}^2\right).
% \end{align*}



% Therefore, the posterior risk is:
% $$
% \begin{aligned}
% & \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2}  \;\middle|\; X_i\right] \\ = & \mathbb{E}\left[A_0 +  \left(\left(\hat{\mu}\left(Y_i\right)-\mu_{\ell}\right)-\left(\mu-\mu_{\ell}\right)\right)^{2 j} \;\middle|\; X_i\right] \\ =
% & A_0+\int_{-\infty}^{\infty} \underbrace{\left(e-\left(\hat{\mu}\left(Y_i\right)-\mu_{\ell}\right)\right)^2}_{=: F_1\left(e-\left(\hat{\mu}\left(Y_i\right)-\mu_{\ell}\right)\right)} \underbrace{\frac{1}{\sigma_{\ell} \sqrt{2 \pi}} \exp \left(-\frac{e^2}{2 \sigma_{\ell}^2}\right)}_{=: F_2(e)} d e
% \end{aligned}
% $$

% Because:
% \begin{itemize}
%     \item $F_1(\cdot)$ is even function and increases on $[0, \infty)$;
%     \item $F_2(\cdot)$ is even function and decreases on $\left[0, \infty\right.$, and $\int_{\mathbb{R}} F_2(e) d e<\infty$
%     \item For any $a \in \mathbb{R}, \int_{\mathbb{R}} F_1(e-a) F_2(e) d e<\infty$
% \end{itemize}

% By the corollary of Hardy-Littlewood inequality in Lemma \ref{lemmaHardy},
% $$
% \int_{\mathbb{R}} F_1(e-a) F_2(e) d e \geq \int_{\mathbb{R}} F_1(e) F_2(e) d e
% $$
% which means the posterior risk is minimized when $\hat{\mu}\left(Y_i\right)=\mu_{\ell}$. We then write the Bayes risk as, the Bayes risk is minimized by the posterior mean $\mu_{\ell}$:

% \begin{align*}    R_{\ell}:= & \mathbb{E}\left[ A_0+\mathbb{E}\left[\left(\mu-\mu_{\ell}\right)^{2 } \;\middle|\;  X_i\right]\right] \\ = & A_0 + \sigma_{\ell}^{2}
% \end{align*}

% and the limit of Bayesian risk as $\ell \rightarrow \infty$ is
% $$
% R_{\infty}:=\lim _{\ell \rightarrow \infty} \sum_{j=0}^{k_\epsilon} A_j(2 j-1)!!\frac{\sigma^{2 j}}{n_i^j}
% $$

% When $\hat{\mu}\left(Y_i\right)=\hat{\mu}\left(X_i\right)$, the maximum risk is:

% \begin{align*}
% & \sup _\mu \mathbb{E}\left[A_0+\left(\mu- 
% \hat{\mu}\left(Y_i\right) \right)^{2 } \;\middle|\;  \mu \right] \\
% = & \sup _\mu \mathbb{E}\left[A_0+\left(\mu- 
% \hat{\mu}\left(X_i\right) \right)^{2 } \;\middle|\;  \mu \right]  \\
% = & A_0+ \sigma^{2 } n_i^{-1}  \\
% = &  R_{\infty} .
% \end{align*}

% This implies that,

% \begin{align*}
%     & \underset{\mu}{\sup}\; \mathbb{E} \sbr{ \rbr{\hat{\mu}\left( Y_i \right)-\hat{\mu}\left( Y_{-i} \right)  }^2 \;\middle|\;  \mu}  \\ \geq & \mathbb{E}_{\Lambda_{\ell}} \sbr{ \mathbb{E} \sbr{ \rbr{\hat{\mu}\left( Y_i \right)-\hat{\mu}\left( Y_{-i} \right)  }^2 \;\middle|\;  \mu}} \geq  R_{\ell}
% \end{align*}

% \begin{align*}
%     & \underset{\mu}{\sup}\; \mathbb{E} \sbr{ \rbr{\hat{\mu}\left( Y_i \right)-\hat{\mu}\left( Y_{-i} \right)  }^2 \;\middle|\;  \mu}  \\ \geq & \; R_{\infty} =  \sup _\mu \; \mathbb{E}\left[A_0+\left(\mu- 
% \hat{\mu}\left(X_i\right) \right)^{2 } \;\middle|\;  \mu \right]
% \end{align*}

% Therefore, the recommended strategy $\hat{\mu}( Y_i) =\hat{\mu}( X_i) $ has a smaller maximum risk than other strategies for Normal distribution class $\left\{ \mathcal{N}(\mu,\sigma^2) \;\middle|\; \mu \in \mathbb{R}\right\}$.

% \end{proof}

% We then prove....... \ac{what is this?}
% \begin{lemma} \label{lem:same_minimax}
%     $ \hat{\mu}\left(X_i\right)$ is the minimax estimator for the distribution class $\distrifamily =  \left\{ \forall p \in \distrifamily,\; \mathrm{Var}(P) \leq \sigma^2 \right\}$,
%      \begin{align*}
%     \hat{\mu}(X_i)= \underset{\hat{\mu}}{\arg\min} \sbr{\sup _\Pcal \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; P \right]}.
%     \end{align*}
% \end{lemma}

% \begin{proof}
%     1. Upper bound of $\underset{\hat{\mu} }{\inf} \; \underset{\distrifamily}{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]$: 
%     \begin{align*}
%     & \underset{\hat{\mu} }{\inf} \; \underset{\distrifamily}{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right] \\  \leq & \; \underset{\distrifamily }{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(X_i\right)-\mu\right)^{2} \;\middle|\; \mu\right] \\  = & \; A_0 + \frac{\sigma^2}{n_i}.
%     \end{align*}

%     2. Lower bound of $\underset{\hat{\mu} }{\inf} \; \underset{\distrifamily}{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]$:

%     \begin{align*}
%     & \underset{\hat{\mu} }{\inf} \; \underset{\distrifamily }{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right] \\  \geq & \; \underset{\hat{\mu} }{\inf} \; \underset{\Normaldistrib }{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]  \\  = & \; A_0 + \frac{\sigma^2}{n_i}.
%     \end{align*}
    
%     Therefore, we have 
%     \begin{align*}
%     \underset{\hat{\mu} }{\inf} \; \underset{\distrifamily }{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(Y_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]  =  A_0 + \frac{\sigma^2}{n_i} = \underset{\hat{\mu} }{\inf} \; \underset{\distrifamily }{\sup} \;   \mathbb{E}\left[A_0 + \left(\hat{\mu}\left(X_i\right)-\mu\right)^{2} \;\middle|\; \mu\right]  .
%     \end{align*}

    
% \end{proof}
