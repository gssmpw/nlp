\section{Introduction}

With the ubiquity of AI, \emph{data} has become a critical economic resource
driving operations and innovation across many domains.
However, not everyone has the means to
collect data on their own
 and thus must rely on \emph{data marketplaces} to acquire the data they
need. For instance, materials data platforms (e.g.~\citep{citrine}) aggregate datasets
from proprietary sources; smaller organizations and academic researchers, who do not have
access to extensive experimental infrastructure, can purchase this
data to advance their research.
% \ac{redundant sentence ``materials''?}
In recent years, data marketplaces have emerged across various domains,
including 
% materials science~\cite{citrine},
advertising~\cite{googleads,databrickssharing},
computer systems~\cite{azuredatashare,awshub},
insurance~\cite{datarade},
freight~\cite{freightdat},
logistics~\cite{bloombergeap}, 
and others~\citep{awsdataexchange,ibmdatafabric,snowflake}. 
As shown in~\fig~\ref{fig:datamarketplace},
these platforms act as intermediaries,
facilitating data purchases between buyers and contributors.
%\kkcomment{Insert a figure here illustrating a marketplace.}


% \begin{figure}[h!] \label{fig:datamarketplace}
%     \centering
%     \includegraphics[width=0.4\textwidth]{figure1.png} % Replace with the path to your image
%     \caption{Data Marketplace}
%     \label{fig:your_label} 
% \end{figure}

\begin{figure*}[b]
\vspace{-0.06in}
\centering
\includegraphics[width=0.99\linewidth]{figs/data_marketplace_v2}
\vspace{-0.12in}
\caption{\small%
An illustration of a data marketplace.
(1) Data contributors collect and submit their data.
(2) A \emph{broker} operating the marketplace evaluates
 the quality of their submissions and prices the data.
(3) The broker sells subsets of this dataset to the buyers.
(4) The revenue is redistributed back to the contributors.
\label{fig:datamarketplace}
}
\end{figure*}



\parahead{Untruthful data reporting} 
In data marketplaces, buyers seek high-quality data for their learning tasks, while contributors aim to maximize their profits. Aligning these incentives of buyers and contributors presents a significant challenge.
Strategic contributors seek to minimize their data collection costs, while maximizing their compensation from buyers.
Buyers, on the other hand, pay for data but may question its reliability, particularly when contributors act in self-interest.
These dynamics raise two critical and closely connected questions which we wish to address in this work:
First, are buyers receiving reliable data commensurate with the price they pay?
%Are buyers being treated fairly, relative to other buyers?
%Are contributors being adequately compensated for their efforts?
Second, are contributors incentivized to provide reliable data rather than prioritizing personal profit, say, for instance, by submitting fabricated (fake) data instead of genuinely collecting it?



%Second para: Something lacking in current literature: focus on the incentives of buyers and contributors jointly.

%\parahead{Motivating challenges}
%However, designing mechanisms that maximize profit, provide value to buyers, incentivize truthful data reporting from contributors, and 

% \parahead{Incentives of Buyers and Contributors} 
% In data marketplaces buyers seek high-quality data for their learning tasks, while contributors aim to maximize their profits. Aligning these incentives of buyers and contributors presents a significant challenge.
% Strategic contributors seek to minimize their data collection costs, while maximizing their compensation from buyers.
% Buyers, on the other hand, pay for data but may question its reliability, particularly when contributors act in self-interest.
% These dynamics raise several critical questions:
% Are buyers receiving reliable data commensurate with the price they pay?
% Are buyers being treated fairly, relative to other buyers?
% Are contributors being adequately compensated for their efforts?
% Are contributors incentivized to provide reliable data rather than prioritizing personal profit, say, for instance, by submitting fabricated (fake) data instead of genuinely collecting it?





\parahead{Positioning}
Most prior work on data marketplaces sets prices for buyers and payments to contributors based on \emph{quantity of data}.
This overlooks the fact that buyers ultimately care about performance on a given learning task.
Quantity-based pricing fails when strategic contributors can report untruthfully.
For instance, naively paying contributors based on the size of the dataset they contribute, incentivizes them to report large fabricated (fake) datasets to inflate earnings.

Addressing this requires a \emph{joint} analysis of buyer and contributor incentives.
For instance, in the naive mechanism above, contributors may benefit from fabricating data, but if buyers lose trust in data quality, they may exit the market, reducing revenue and  eventually harming contributors.
To retain buyers, pricing must be adjusted to reflect data quality.
These adjustments should then be passed on to contributors to incentivize truthful (high quality) data submissions.
While there is prior work on incentivizing truthful data reporting, they do not consider broader market constraints, e.g. contributor payments must originate from buyers who derive value from the data they purchase.
%We discuss these distinctions further %in~\S\ref{sec:relatedwork}.

\parahead{Model}
We study these questions in a mean estimation problem. A finite set of data contributors $\contributors$, collect data from the same but unknown normal distribution $\Ncal(\mu,\sigma^2)$, with each contributor  incurring a different cost  per sample.
A finite set of buyers $\buyers$ aim to estimate the mean $\mu$ of this distribution.
Each buyer has a different valuation  based on her estimation error. A broker facilitates data transactions between contributors and buyers.
The contributors collect data and report it (not necessarily truthfully) to the broker.
The broker evaluates these submissions,  and sells different amounts of data to different buyers at different prices to maximize the cumulative profit, i.e. total revenue from buyers minus contributors' costs. 
In doing so, the broker should ensure that both buyers and contributors benefit from participating in the market, each buyer's price is fair relative to the others (envy-free), contributors are incentivized to collect a sufficient amount of data and report it truthfully, and that revenue from buyers balances payments to contributors.


% While we focus on the relatively narrow task of  mean estimation,
% we advance the literature in data marketplaces in two key ways (we discuss detailed differences in~\S\ref{sec:relatedwork}): 

% \emph{(1) Accounting for strategic contributor behavior:} Most prior work models both the prices to buyers and payments to contributors based on the \emph{quantity of data}.
% While this works well when contributors submit truthful data, it is vulnerable
% to manipulation if strategic contributors can report untruthfully. For instance, a naive mechanism that pays contributors based on the size of their datasets incentivizes contributors to report large fabricated (fake) datasets to inflate payments. Existing approaches addressing untruthful reporting often neglect broader market constraints. In contrast, our method explicitly addresses strategic behaviors while adhering to these constraints.  

% \emph{(2) Jointly studying buyer and contributor incentives:} Most prior work often studies the incentives of buyers and contributors in isolation. We analyze them \emph{jointly}, exploring how incentives on one side influence the other. For instance, in the naive mechanism above, contributors are incentivized to fabricate data. However, if buyers question the integrity of the data, they may withdraw from the market, which reduces potential revenue and subsequently harms contributors.





\subsection{Summary of contributions}
\label{sec:contributions}

\parahead{Problem Formalism} 
In~\S\ref{sec:setup}, we formalize the problem. A broker designs a mechanism specifying:  
(a) how much data each contributor should collect,  
(b) how much data is sold to each buyer,  
(c) the prices charged to buyers, and  
(d) the payments distributed to contributors.  
Each contributor $i$'s strategy determines how much data she collects (which may differ from the broker’s specification), and what she submits. For instance, a contributor may only collect a small amount of data, and fabricate the rest to mislead the broker, say by fitting a distribution to the small dataset she collected, and then sampling many points from it.


\subparahead{Requirements for the mechanism} 
The mechanism must satisfy:  
\emph{(i) Individually rational for buyers (IRB):} buyers' prices should not exceed their valuation.  
\emph{(ii) Envy-free for buyers (EFB):} no buyer prefers anothers' data allocation and price.  
\emph{(iii) Individually rational for contributors (IRC):} contributor payments cover data collection costs.  
\emph{(iv) Incentive-compatible for contributors (ICC):} collecting the specified amount of data in (a) and submitting it truthfully is a Nash equilibrium (NE), i.e. the best strategy for a contributor, when others are doing the same.
\emph{(v) Budget balance (BB):} total revenue from buyers equals total contributor payments.  
\emph{(vi) Profit-optimality (PO):} the mechanism should maximize profit among mechanisms satisfying
\emph{(i)--(v)}.  



\subparahead{Accounting for uncertainty of underlying distribution}
A key challenge in formulating this problem is in accounting for the \emph{unknown} underlying distribution. 
Evaluating a contributor's truthfulness requires comparing her submission to others. As this comparison is based on data, it necessarily depends on the 
unknown distribution.
Consequently, the buyer and contributor utilities, as well as the profit may also depend on it.
To address this, we consider the worst-case utilities and profit over all normal distributions with the given variance.


\parahead{Profit-optimal envy-free pricing with non-strategic contributors}
In~\S\ref{sec:buyerside}, we first study a simpler setting where contributors are non-strategic, i.e. they follow the broker’s specification and report data truthfully.  
Our first goal is to establish a profit-maximization baseline, $\blprofit$, considering only buyer incentives, which we will later try to approximate when contributors are strategic.  
Notably, in this baseline, the lowest-cost agent collects all the data---a natural outcome in the truthful setting but one that fails ICC with strategic contributors (see our hardness results).  

Our second goal is to design algorithms for this non-strategic setting. We will later build on this procedures when designing mechanisms with strategic contributors.  
A key insight is that designing revenue-optimal IRB and EFB pricing schemes reduces to optimizing a posted pricing curve for ordered items~\citep{chawla2022pricing}, allowing us to leverage those algorithms for our problem.  


\parahead{Hardness results}
In~\S\ref{sec:hardness}, we establish two key hardness results.
First, there is no nontrivial dominant-strategy incentive-compatible mechanism for this problem,
i.e. collecting the specified amount and reporting it truthfully is the best strategy regardless of others' strategies.
Second,  in any NE of any mechanism, the maximum achievable profit is  $\blprofit - (\cost_2 - \cost_1)$ where $\cost_1,\cost_2$ are the costs of the cheapest and second cheapest agents.
Both results stem from a key insight: when others collect no data, an agent can fabricate data at no cost from an any normal distribution.

\parahead{Mechanism design}
In~\S\ref{sec:mechanism}, we design mechanisms that satisfy the six requirements outlined earlier.  
While collecting all data from one agent is not ICC, we show that using two agents suffices.  
Intuitively, the broker can verify truthfulness by comparing their reported data with each other.  
Thus, to maximize profit, the mechanism assigns data collection to the two cheapest agents.  
Moreover, the cheapest contributor collects almost all of the points. The second cheapest agent collects only a minimal amount (just one point for normal mean estimation), as her primarily role is to help the broker verify the reliability of the cheapest contributor's submissions.

We employ the envy-free pricing algorithm from~\S\ref{sec:buyerside} to determine the total data collection amount, buyers' dataset sizes, and expected prices.  
Buyers' actual prices also include a term dependent on the difference
between the means of the two contributors' reported datasets, with this variation passed on to the contributors' payments.
This design helps us achieve three of our requirements;
ICC: as each contributor should report truthfully when the other is doing so in order to minimize this difference,
IRB: as buyers will pay less if there are significant discrepancies contributors' submissions, which may indicate unreliable data,
and
BB (with probability 1): as the market must be feasible for every random realization of data.


%\emph{Analysis techniques.}
Our proofs build on minimax lower bound techniques for normal mean estimation.  
Using these approaches we show that when other contributors collect and contribute data truthfully, it is best for any contributor to contribute her data truthfully regardless of how much she has collected. Furthermore, by carefully designing contributor payments, we show that the optimal amount of data for contributors to collect is exactly the amount recommended by the mechanism.
% \kkcomment{@Alex: can we say something interestign about the proof here.}
% \ac{is what I wrote fine?}



\subsection{Related work}
\label{sec:relatedwork}

\parahead{Data pricing}
In recent years, a significant body of work has focused on designing markets and auctions for data. Some studies assume settings where buyers either purchase (or are allocated) the entire dataset or none at all~\citep{agarwal2020towards,agarwal2019marketplace,bergemann2019markets,chen2023equilibrium}. However, this assumption does not align with real-world markets (e.g.~\citep{snowflake,awsdataexchange,citrine,bloombergeap}), where buyers can purchase smaller subsets of data. This limitation also leaves untapped revenue from buyers willing to pay smaller amounts for small datasets. Other works develop approaches for pricing \emph{information}~\citep{bergemann2018design,babaioff2012optimal,mehta2021sell,pei2020survey}, but these approaches do not reflect real-world marketplaces, where brokers sell datasets directly from various sources.


\subparahead{Pricing Ordered Items}
General multi-item pricing is a notoriously difficult problem, and several works have developed faster algorithms under structural assumptions on items. One pertinent line of work concerns pricing ordered items, where unit-demand buyers share a common preference ranking over goods~\citep{chawla2022pricing,hartline2005near}. Recently,~\citet{chen2024learning} developed methods for pricing ordered items in the context of data markets, assuming additional properties such as smoothness. As mentioned earlier, we build upon this line of research to design profit-optimal, envy-free pricing schemes in our problem.

\subparahead{Principal-agent models for data collection}
Some studies have explored principal-agent models in data collection, where a principal incentivizes agents to collect data through payments or other means when data collection is costly~\citep{cai2015optimum,huang2023evaluating}.
These methods assume specific forms for the valuation of the data for the principal. our approach is more general: we only assume that buyers' valuation decreases with their estimation error.
Moreover, in these works, and the works on \emph{Data pricing} it is assumed that  contributors will report data truthfully. Indeed, in many cases, they are able to design DSIC mechanisms for contributors, which, as we demonstrate, is impossible in our problem.



\parahead{Incentivizing truthful reporting}
Previous work on incentivizing truthful reporting of already collected data~\citep{zheng2024truthful,chen2020truthful} do not consider many of the market constraints we study, such as PO, IRB, IRC, and EFB. In their model agents do not incur costs to collect data.
Moreover, they assume that the  principal has access to a large budget for incentivization, but do not address the source of these funds. In contrast, our setting requires that payments originate from buyers who derive value from the data contributed by contributors.




\subparahead{Truthful Contributions in Collaborative Mean Estimation}
Recent work has explored collaborative mean estimation, where a group of strategic agents collaborate to estimate the mean of a  distribution~\citep{chen2023mechanism,clinton2024collaborative,dorner2023incentivizing}. A common technique in our method and these works is comparing an agent’s reported data mean against the mean of other agents to incentivize truthful reporting. However, our analysis techniques are different from these methods, in part because we must adhere to market constraints that do not arise in collaborative learning, and in part because all three methods assume
specific forms for the agents' valuation of data.


\subparahead{Moral Hazard}
The issue of contributors submitting fabricated data can also be analyzed under the framework of \emph{moral hazard}, where agents (contributors) deviate from agreed-upon tasks (data collection) with a principal (broker)~\citep{laffont2009theory,holmstrom1979moral,mirrlees1999theory,helpman1975moral}. As in classical moral hazard settings, the outcome of a contributor’s work (the collected dataset) is stochastic. However, unlike typical models, our broker lacks knowledge of the signal distribution given contributors' effort (the learning problem would be trivial if the data distribution were known) and cannot directly observe the outcomes of these efforts, relying instead on potentially dishonest reports from the contributors themselves.



