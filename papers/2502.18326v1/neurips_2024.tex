\documentclass{article}
\usepackage[numbers]{natbib}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% ready for submission
% \usepackage{compositional_learning}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath, amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % figures
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{siunitx}
\crefname{figure}{Fig.}{Figs.}
\definecolor{kleinblue}{RGB}{0, 47, 167} 
\definecolor{kleinblue2}{RGB}{20, 20, 125} 
\definecolor{kleinred}{HTML}{bc1919}

\title{Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks}

\author{
    Thaddäus Wiedemer$^{\,1,2,3}$\footnotemark[1] \quad Yash Sharma$^{\,1,2,3}$\footnotemark[1] \quad Ameya Prabhu$^{\,2,3}$ \\
    \\
    \textbf{Matthias Bethge}$^{\,2,3,4}$ \quad \textbf{Wieland Brendel}$^{\,1,2,4}$\\
    \\
    $^{1\,}$Max-Planck-Institute for Intelligent Systems \quad $^{2\,}$Tübingen AI Center\\
    $^{3\,}$University of Tübingen \quad $^{4\,}$ELLIS Institute Tübingen\\
    \\
    {\tt\small thaddaeus.wiedemer@gmail.com, ysharma1126@gmail.com}
}

\begin{document}
% change footnotes to symbols for asterisk in authors
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution.}

\maketitle

\begin{abstract}
We investigate the success conditions for compositional generalization of CLIP models on real-world data through performance prediction.
Prior work shows that CLIP requires exponentially more pretraining data for linear performance gains on individual concepts.
This sample-inefficient scaling could be mitigated if CLIP systematically understood new inputs as compositions of learned components, allowing rare observation to be mapped to common concepts.
To explore CLIP's compositional generalization ability, we filter retrieval corpora for samples with object combinations not present in the pretraining corpus.
We show that CLIP's performance on these samples can be accurately predicted from the pretraining frequencies of individual objects.
Our findings demonstrate that CLIP learns to disentangle objects observed in its pretraining data and can recompose them straightforwardly.
Additionally, we are the first to show how this ability scales with pretraining data.
For data curation in practice, our results suggest that balancing object occurrences improves generalization, which should benefit CLIP's efficiency and accuracy without scaling data volume.
\end{abstract}

% Switch footnotes back to numbers and reset the counter
\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

\section{Introduction}
Vision Language Models (VLMs) like CLIP~\citep{radford2021learning} have seen widespread adoption for downstream tasks like classification, image retrieval, and image generation due to their transferability and impressive zero-shot performance.
However, CLIP models are data-hungry, requiring exponentially more pretraining data for linear performance gains on downstream samples~\citep{udandarao2024zeroshotexponentialdatapretraining}.
Similar sample-inefficient scaling has been reported for Large Language Models (LLMs)~\citep{kandpal2023large, antoniades2024generalization}, raising doubts about the feasibility of improving zero-shot performance of foundation models by scale alone.

A systematic way to overcome inefficient scaling is thought to be compositional generalization---the ability to understand and form novel combinations of learned concepts~\citep{fodor1988connectionism,hupkes2020compositionality,wiedemer2023compositional}.
A model that can generalize in this way should more effectively combine learned concepts to understand new inputs, ultimately leading to increased zero-shot performance.
Yet, the compositional abilities of large VLMs, such as CLIP, remain poorly understood:
Existing studies in the visual domain are either theoretical, operate on synthetic data, or fail to verify whether compositions used for evaluation are truly novel given the pretraining data (see Sec.~\ref{sec:related}).
Moreover, \emph{the relationship between a VLM's compositional abilities and its pretraining data is entirely uncharacterized}.

To address this gap, we aim to investigate CLIP's compositional generalization on real-world data as a function of its pretraining corpus.
Specifically, we make the following contributions:
\begin{itemize}
    \item We leverage the scalable concept-extraction pipeline proposed by \citet{udandarao2024zeroshotexponentialdatapretraining} to curate text-to-image (T2I) and image-to-text (I2T) retrieval test sets for compositional generalization (Sec.~\ref{sec:compgen_filter}). Each test set is created for the pretraining corpus of the tested model such that it contains only samples with novel combinations of known object classes.
    % \item We propose a simple predictor
    \item We show that CLIP models perform consistently well on our curated test sets, regardless of the architecture or scale of the backbone.
    \item We demonstrate that across architectures, parameter counts, and pretraining data scales, CLIP's ability to compose objects can be accurately predicted from the independent pretraining frequencies of each object in the composition (Sec.~\ref{sec:results}).
\end{itemize}

Our results are the first to establish a firm connection between the compositional generalization of a VLM and its pretraining data. The nature of this connection shows that CLIP obtains an independent understanding of object classes from web-scale data.



\section{Related Work}\label{sec:related}

\paragraph{Theoretical Works \& Synthetic Data}
A growing body of works~\citep{montero2021role,monteroLostLatentSpace2022,schott2022visual,lewis2022does,wiedemer2023compositional,wiedemer2024provable,okawaCompositionalAbilitiesEmerge2023, jung2024learning} provides significant theoretical understanding of compositional generalization results in the vision domain.
Similar works exist for compositionally in language~\citep{fodor1988connectionism,hupkes2020compositionality,berlot2023attribute}, often under the more specific term \emph{systematicity}~\citep{fodor1988connectionism,hupkes2020compositionality,berlot2023attribute}.
In the language domain, promising progress has been made~\citep{lake2023human}, but results in both domains nonetheless remain confined to synthetic datasets~\citep{lake2018generalization,kim2020cogs}l; \citet{sun-etal-2023-validity} actively questions the transferability of insights to real-world data. In contrast, our work analyzes compositional generalization using real-world retrieval datasets.

\paragraph{VLM Benchmarks \& Contamination}
Several compositionality benchmarks have been proposed for VLMs~\citep{thrush2022winoground, lewis2022does, zhao2022vl, yuksekgonul2023when, ma2023crepe, hsieh2023sugarcrepe, ray2024cola, wangEnhancingCompositionalGeneralization2024, abbasi2024deciphering}. However, these studies do not consider the overlap of concept combinations with web-scale pretraining data.
Data contamination of this kind has been shown to significantly impact CLIP's zero-shot performance~\cite{mayilvahanan2024search}, making it difficult to distinguish between genuine generalization and mere memorization.
A notable exception is \citet{abbasi2024deciphering}, who generate test images of novel attribute-object pairs, but as a result, their benchmark resorts to synthetic data. Our work controls for data contamination by only considering combinations that do not occur in the pretraining data but do occur in real-world benchmarks.



\section{Predicting Compositional Generalization from Pretraining Frequency}\label{sec:experiments}
We adapt the pipeline from \citet{udandarao2024zeroshotexponentialdatapretraining} in two steps to study the success conditions for compositional generalization.
First, we use it to curate retrieval test sets that contain novel combinations of objects with respect to a pretraining set (Sec.~\ref{sec:compgen_filter}). Second, we propose a simple modification to predict downstream performance in terms of samples rather than concepts (Sec.~\ref{sec:f_sample}). Finally, we evaluate CLIP models with varying architectures, parameter counts, and pretraining data scales and show consistent scaling behavior (Sec.~\ref{sec:results}).


\subsection{Curating Compositional Generalization Test Sets}\label{sec:compgen_filter}
We consider two standard retrieval datasets: Flickr-1K~\citep{young2014image} and COCO-5K~\citep{lin2014microsoft}. Both can be used for benchmarking text-to-image (T2I) or image-to-text (I2T) retrieval.

To measure compositional generalization, we follow \citet{hupkes2020compositionality} and retain only test samples containing multiple concepts $o_1, ..., o_n$, where
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
    \item the model has been familiarized with each concept $o_i$ only in the absence of $o_{j \neq i}$,
    \item the combination $o_1, ..., o_n$ is plausible.
\end{enumerate}

\citet{udandarao2024zeroshotexponentialdatapretraining} compile a list of 945 nouns in the text captions for these 2 retrieval datasets as possible concepts.
The presence of a concept in a pretraining sample is established if it is part of the caption (after lemmatization) \emph{and} it is found in the image using RAM++~\citep{huang2023open}.
Consequently, our analysis of concepts is limited to tangible objects; more abstract concepts like actions or stylistic information are harder to annotate in the visual domain and can, therefore, not be reliably quantified using this pipeline.
With this setup, the frequency $f_{\mathcal D}(o)$ of object class $o$ in a pretraining corpus $\mathcal D$ simply counts the number of samples it occurs in.\footnote{We use the term frequency instead of count since we only compare quantities for a given, fixed-size pretraining set, in which case normalization can be omitted.}

To address (i), we first consider how often objects $o_1, ..., o_n$ in a test sample $x$ co-occur in the pretraining dataset. We call this quantity the co-occurrence frequency, formally given by
\begin{equation}\label{eq:f_cap}
    f_{\cap, \mathcal D}(x) = \left\| \left\{d \in \mathcal D \;|\; \text{for all } o \in x: o \in d \right\} \right\|.
\end{equation}
Condition (i) above is then satisfied by test samples $x$ which contain a novel combination of objects, i.e., $f_{\cap, \mathcal D}(x) = 0$, but each object has been observed at least once, i.e., $f_{\mathcal D} > 0$ for all $o \in x$.

Condition (ii) is hard to verify in general but is trivially met here since we filter real-world data.

Since all frequencies are dependent on the pretraining corpus, the size of our compositional generalization test sets differ. The number of samples in each test set (total and percentage) is shown in Figs.~\ref{fig:lines_t2i_k10}~and~\ref{fig:lines_i2t_k10}.


\subsection{Per-Sample Prediction}\label{sec:f_sample}
\paragraph{Metrics}
We measure performance on each sample using Recall@$k$ for $k \in \{1, 5, 10\}$ following \citet{radford2021learning}. Figs.~\ref{fig:lines_t2i_k10}~and~\ref{fig:lines_i2t_k10} show results for Recall@10, other results are listed in App.~\ref{app:extra_plots}.

\paragraph{Sample Frequency}
We compute the average pretraining frequency $f_\text{avg}$ of each test sample $x$ as the geometric mean of the frequencies of the objects $o_1, ..., o_n$ in the sample $x$, i.e.,
\begin{equation}
    f_\text{avg}(x) = \left(\prod_{o \in x}^n f(o)\right)^\frac{1}{n}.
\end{equation}
The choice of geometric mean is motivated by the assumption that the model's performance on a combination of objects depends on the quality of the model's independent understanding of each object in the combination. For example, a simple retrieval engine might find samples containing two objects $o_1, o_2$ without considering their interaction by first finding samples containing $o_1$ and then filtering the results for samples containing $o_2$. In this case, the probability of retrieving a correct sample based on the prompt $\mathcal P$ can be written as
\begin{equation}
    P(y=1 | o_1, o_2 \in \mathcal P) = P(y=1 | o_1 \in \mathcal P) P(y=1 | o_2 \in \mathcal P).
\end{equation}
The geometric mean reflects the multiplicative impact of each object on the whole~\citep{okawaCompositionalAbilitiesEmerge2023}.

\paragraph{Fitting a Predictor}
Our evaluation yields $(y, f_\text{avg})$ for each test sample, where $y=1$ (0) indicates correct (wrong) retrieval. For each test set, we drop noisy outliers via IQR-removal on $f_\text{avg}$, following \citet{kandpal2023large, udandarao2024zeroshotexponentialdatapretraining}. We fit a logistic regression model with bootstrapping to predict $P(y = 1)$ given $f_\text{avg}$.


\subsection{Results}\label{sec:results}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_t2i_k=10.pdf}
    \caption{\textbf{T2I Recall@10}. CLIP's performance on unknown combinations (bottom) matches that on known combinations (top) and can be consistently predicted as a linear function of the average pretraining frequency of the constituent objects. All regression fits are significant at $p<0.01$.}
    \label{fig:lines_t2i_k10}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_i2t_k=10.pdf}
    \caption{\textbf{I2T Recall@10}. CLIP's performance on unknown combinations (bottom) almost matches that on known combinations (top) and can be consistently predicted as a linear function of the average pretraining frequency of the constituent objects. All regression fits are significant at $p<0.01$.}
    \label{fig:lines_i2t_k10}
\end{figure}

Fig.~\ref{fig:lines_t2i_k10} collects results for the T2I task, Fig.~\ref{fig:lines_i2t_k10} for I2T. More results can be found in App.~\ref{app:extra_plots}.

\paragraph{Models}
We test CLIP~models~\citep{radford2021learning} with both ResNet~\citep{he2016deep} and Vision~Transformer~\citep{dosovitskiy2020image} architectures. Specifically, we evaluate ViT-B-16~\citep{mu2022slip} and RN50~\citep{goel2022cyclip,nguyen2022quality} trained on CC-3M~\citep{sharma2018conceptual} and CC-12M~\citep{changpinyo2021conceptual}; ViT-B-16, RN50, and RN101~\citep{ilharco2021openclip} trained on YFCC-15M~\citep{thomee2016yfcc100m}; ViT-B-16, ViT-B-32, and ViT-L-14 trained on LAION400M~\citep{schuhmann2021laion}; and ViT-B-16 trained on SynthCI-30M~\citep{hammoud2024synthclip}. We follow \texttt{open\_clip}~\citep{ilharco2021openclip}, \texttt{slip}~\citep{mu2022slip} and \texttt{cyclip}~\citep{goel2022cyclip} for implementation details.

\paragraph{Fraction of Compositional Generalization Samples}
Looking at the histogram percentages, \SI{17}-\SI{44}{\%} of samples are unknown compositions of known concepts for the T2I task. For I2T, the fraction is much higher with \SI{89}-\SI{98}{\%}. The discrepancy stems from many images containing background objects that are inconsistently reflected in their captions. We also find that the fraction generally decreases with the size of the pretraining set.

\paragraph{Overall Performance}
We find that CLIP's performance on the T2I task does not differ greatly between samples with known combinations (top row) and samples with novel combinations of known concepts (bottom row). The difference is slightly more pronounced on the I2T task, but performance is still high overall, indicating that CLIP generalizes well to novel object compositions.

\paragraph{Predicting Compositional Generalization}
We show a clear and consistent relationship between the average pretraining sample frequency $f_\text{avg}$ and CLIP's retrieval performance, even on samples requiring compositional generalization. The relation is approximately linear, except for the best-performing models, where it flattens as retrieval recall approaches 1.
Since the contribution of each object's pretraining frequency to the average pretraining sample frequency $f_\text{avg}$ is multiplicative, this consistent relationship implies that underrepresented objects are the bottleneck for compositional generalization.

\paragraph{Control on Synthetic Data} SynthCI-30M~\citep{hammoud2024synthclip} consists of synthetically generated images designed to cover a diverse combination of concepts.
Due to this process, we treat SynthCI-30 as a control to see if our results hold for a pretraining dataset sourced differently. We find that more test set combinations are unseen in SynthCI-30 than the pretraining sets derived from the real world, but the scaling of compositional generalization observed on real-world pretraining corpora also holds here.

% \paragraph{Conclusion}
Taken together, our results show that CLIP generalizes successfully to novel combinations of objects if it has observed the constituents sufficiently often during pretraining.
Note that objects in the pretraining data do not occur independently. In fact, many training samples contain multiple objects, and some object classes co-occur much more frequently than others.
The consistent scaling of CLIP's compositional generalization implies that the model can nonetheless disentangle objects and obtain an independent understanding of each object class.

For practitioners, our findings underline the importance of balancing object occurrences during data curation, as generalization is bottlenecked by the occurrence of each object.


\section{Next Steps}
\paragraph{Model Selection} While we control for architecture, parameter count, and pretraining scale, our experiments could be extended to other CLIP variants~\citep{yuksekgonul2023when}, diffusion models, or even LLMs.

\paragraph{Type of Compositionality} We only consider object compositions. We expect that our results may extend to attribute-object, foreground-background, texture-shape compositions in single-object scenes, since the independence assumption from Sec.~\ref{sec:f_sample} approximately holds. The bottleneck for these experiments is the concept-extraction pipeline. On the other hand, the scaling behavior of complex compositions, like attribute-binding with multiple objects, may not be as readily predictable.

\paragraph{Composition Granularity} Our definition of the co-occurence frequency in Eq.~\ref{eq:f_cap} only considers whether \emph{all} objects have jointly been observed during pretraining. For samples containing more than two objects, it might also be interesting to consider pair-wise object co-occurence and other partial co-occurences. How to integrate this information in the selection of test samples for compositional generalization remains is an open question.

\section{Conclusion}
Identifying conditions for successful real-world compositional generalization is a first step towards a future where models can be relied upon to generate new ideas, as \emph{``an idea is nothing more nor less than a new combination of old elements''}~\citep{young1975technique}. The ability to forecast when such capabilities will be unlocked is valuable not only to understand the compositional abilities of existing models but also to guide the development and scaling of future methods.
We take a first step in this direction by demonstrating how CLIP's ability to disentangle and recompose objects scales with the the frequency with which each object has been observed in the pretraining data.

\newpage
% \subsection*{Reproducibility}

\subsection*{Acknowledgements}
We thank Vishaal Udandarao for helpful discussions and clarifying details regarding his work that we build our analysis on. We thank Rylan Schaeffer for helpful feedback on this manuscript. This work was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philanthropy Foundation funded by the Good Ventures Foundation. WB is a Machine Learning Cluster of Excellence member, EXC number 2064/1 – Project number 390727645. This research utilized compute resources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.  MB and AP acknowledge financial support by the Federal Ministry of Education and Research (BMBF), FKZ: 011524085B and Open Philanthropy Foundation funded by the Good Ventures Foundation. We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting TW.

\subsection*{Author Contributions}
The project was jointly led and coordinated by TW and YS. YS collected the data with input from TW and AP. TW and YS did the final analysis with input from AP. WB and MB participated in several helpful discussions during the project. TW, YS, and AP wrote the manuscript; TW made the figure with contributions from YS and AP.
{
    \small
    \bibliographystyle{plainnat}
    \bibliography{neurips_2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Additional Evaluations}\label{app:extra_plots}
~\Cref{fig:lines_t2i_k5,fig:lines_i2t_k5,fig:lines_t2i_k1,fig:lines_i2t_k1} plot trends seen in~\Cref{fig:lines_t2i_k10,fig:lines_i2t_k10} for Recall@5 and Recall@1. We observe similar trends.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_t2i_k=5.pdf}
    \caption{\textbf{T2I Recall@5} We see that on combinations that are both known and unknown to the model, across architectures and pretraining sets, there exists a predictive relationship between the sample frequency, i.e. the aggregated frequencies of objects in the combination, and the performance.}
    \label{fig:lines_t2i_k5}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_i2t_k=5.pdf}
    \caption{\textbf{I2T Recall@5} We see that on combinations that are both known and unknown to the model, across architectures and pretraining sets, there exists a predictive relationship between the sample frequency, i.e. the aggregated frequencies of objects in the combination, and the performance.}
    \label{fig:lines_i2t_k5}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_t2i_k=1.pdf}
    \caption{\textbf{T2I Recall@1} We see that on combinations that are both known and unknown to the model, across architectures and pretraining sets, there exists a predictive relationship between the sample frequency, i.e. the aggregated frequencies of objects in the combination, and the performance.}
    \label{fig:lines_t2i_k1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figs_workshop/fig_i2t_k=1.pdf}
    \caption{\textbf{I2T Recall@1} We see that on combinations that are both known and unknown to the model, across architectures and pretraining sets, there exists a predictive relationship between the sample frequency, i.e. the aggregated frequencies of objects in the combination, and the performance.}
    \label{fig:lines_i2t_k1}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \section*{NeurIPS Paper Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
% limit. 

% Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% \begin{itemize}
%     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
%     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
%    % \item {\bf The papers not including the checklist will be desk rejected.}
% \end{itemize}

% {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% IMPORTANT, please:
% \begin{itemize}
%     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% \end{itemize} 
 

% %%% END INSTRUCTIONS %%%


% \begin{enumerate}

% \item {\bf Claims}
%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
%         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
%         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
%     \end{itemize}

% \item {\bf Limitations}
%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
%         \item The authors are encouraged to create a separate "Limitations" section in their paper.
%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
%     \end{itemize}

% \item {\bf Theory Assumptions and Proofs}
%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include theoretical results. 
%         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.
%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
%         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
%     \end{itemize}

%     \item {\bf Experimental Result Reproducibility}
%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
%         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
%         \begin{enumerate}
%             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
%         \end{enumerate}
%     \end{itemize}


% \item {\bf Open access to data and code}
%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that paper does not include experiments requiring code.
%         \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
%     \end{itemize}


% \item {\bf Experimental Setting/Details}
%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
%         \item The full details can be provided either with the code, in appendix, or as supplemental material.
%     \end{itemize}

% \item {\bf Experiment Statistical Significance}
%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
%         \item The assumptions made should be given (e.g., Normally distributed errors).
%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
%     \end{itemize}

% \item {\bf Experiments Compute Resources}
%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
%         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
%     \end{itemize}
    
% \item {\bf Code Of Ethics}
%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
%     \end{itemize}


% \item {\bf Broader Impacts}
%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that there is no societal impact of the work performed.
%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
%     \end{itemize}
    
% \item {\bf Safeguards}
%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper poses no such risks.
%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
%         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
%     \end{itemize}

% \item {\bf Licenses for existing assets}
%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not use existing assets.
%         \item The authors should cite the original paper that produced the code package or dataset.
%         \item The authors should state which version of the asset is used and, if possible, include a URL.
%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
%     \end{itemize}

% \item {\bf New Assets}
%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not release new assets.
%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
%         \item The paper should discuss whether and how consent was obtained from people whose asset is used.
%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
%     \end{itemize}

% \item {\bf Crowdsourcing and Research with Human Subjects}
%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
%         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
%     \end{itemize}

% \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%     \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: \justificationTODO{}
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
%         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
%         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
%     \end{itemize}

% \end{enumerate}


\end{document}