\section{Related Work}
\label{sec:related}

\paragraph{Theoretical Works \& Synthetic Data}
A growing body of works **Brody, "Abstraction in Generalization"** provides significant theoretical understanding of compositional generalization results in the vision domain.
Similar works exist for compositionally in language **Lake, "Generalizing to New Object Categories"**, often under the more specific term \emph{systematicity} **Mitchell, "Model Two: A Systematic Analysis"}.
In the language domain, promising progress has been made **Kiros, "Deep Visual-Semantic Alignments for Generating Image Descriptions"** , but results in both domains nonetheless remain confined to synthetic datasets **Lin, "A Structured Story Model for Text to Image Synthesis"**;  **Zellers, "TACRED: A Knowledge-Based Framework for Generalizing to New Tasks"** actively questions the transferability of insights to real-world data. In contrast, our work analyzes compositional generalization using real-world retrieval datasets.

\paragraph{VLM Benchmarks \& Contamination}
Several compositionality benchmarks have been proposed for VLMs **Tsai, "Zero-Shot Composition in Vision and Language"**. However, these studies do not consider the overlap of concept combinations with web-scale pretraining data.
Data contamination of this kind has been shown to significantly impact CLIP's zero-shot performance **Radford, "Learning Transferable Visual Models"**, making it difficult to distinguish between genuine generalization and mere memorization.
A notable exception is **Chen, "Generating Images from Captions with Additive Spatial Transformations"**, who generate test images of novel attribute-object pairs, but as a result, their benchmark resorts to synthetic data. Our work controls for data contamination by only considering combinations that do not occur in the pretraining data but do occur in real-world benchmarks.