\section{Proposed Tasks and Benchmarks}


The inclusion of multiple agents and annotations in our datasetâ€”in the form of 3D bounding boxes with track IDs enables the development of methods for various collaborative perception tasks, such as object detection, tracking, and motion forecasting. Given the importance of object detection in autonomous driving, we focus on collaborative detection tasks in this paper.

\subsection{Definition of Tasks}
We define two tasks that are distinguished by the collaboration setting: \textit{Collaborative Object Detection} and \textit{Single-Vehicle Object Detection enhanced by communication to RSU}, which we describe in the following sections.

\mypara{Collaborative Object Detection.} 
This is the classical collaborative object detection task \cite{wang2020v2vnet, li2021learning} where every connected agent (i.e., vehicles and RSUs) uses a shared model to (i) extract features from their point clouds, (ii) generate messages to send to other agents, and (iii) fuse the features of their point clouds with messages received from others.
The goal is to detect every visible object in a region of interest. 
We define visibility by comparing the number of \lidar points contained within an object's bounding box to a threshold. In this task, these \lidar points are sourced from any agents present within the region of interest.


\mypara{Object Detection Enhanced by Communication to RSU.}
This task assumes that the RSU model is designed and trained by a different provider than the one responsible for the CAVs' models.
In this task, the RSU model is pre-trained in the single-vehicle detection setting to detect objects visible to its LiDARs. After the pre-training process, the RSU model is fixed. CAVs in the proximity of the RSU receive messages from the RSU to enhance their detection capabilities. The objective of this task is to detect all objects in a region of interest that are visible to either the CAV or the RSU.

The differences between this task and \textit{Collaborative Object Detection} are twofold. 
First, there is no communication among connected vehicles in this task, making it similar to Vehicle-to-Infrastructure (V2I) detection \cite{yu2022dair, zimmer2024tumtraf,xiang2024v2x}. 
Second, instead of having a single model shared among all connected agents like prior works on V2I collaboration, we have one model for the CAVs and another independent model for the RSU. 
This introduces a different challenge, as the CAV's model must adapt to messages from the RSU, which may contain domain gaps due to differences in model architecture, types of \lidar, and viewpoints.

\input{tables/v2x-default}
\input{tables/v2x-freeze-rsu}

\subsection{Benchmark}
\mypara{Evaluation Settings.} Since the annotations are made in the coordinate system of \rsutop, we define the region of interest for the two detection tasks as the range $\left[-51.2, 51.2\right]$ meters along both the x and y axes of this coordinate system.
For evaluation, we transform objects detected by each agent into this coordinate system. The visibility threshold is set to 5 points.
Since timestamp mismatches and localization errors are inherent in real-world applications and consequently present in our dataset, we do not artificially introduce them into the message exchanged among connected agents, as is often done in synthetic datasets \cite{xu2022opv2v, xu2022v2x}.

We measure object detection performance using Average Precision (AP). Detected objects are matched with ground truth based on their Intersection over Union (IoU) in the bird's-eye view plane. A detection and a ground truth object are considered a match if their IoU exceeds thresholds of 0.3, 0.5, or 0.7.

In addition to AP, we measure the bandwidth consumption of each collaborative method to gauge their practicality. The total bandwidth consumption is calculated by multiplying the number of agents in the collaboration network by the size of the message each agent sends. 
While the number of agents is not dependent on the collaboration method of choice, the message size is.
Therefore, we compute the bandwidth consumption by averaging the size of the messages that agents send, measured in Megabytes (MB). 
While some intermediate collaboration methods \cite{wang2020v2vnet, li2021learning, xu2022v2x} employ specialized compressing algorithms to reduce the message size, other methods \cite{xu2022opv2v, lu2023robust, dao2024practical} do not.
To obtain a fair comparison, we measure the size of uncompressed messages.

\mypara{Methods.}
Our benchmark covers three conventional collaboration frameworks, namely Early fusion, Intermediate fusion \cite{wang2020v2vnet, xu2022opv2v, chen2019f, xu2022v2x, li2023learning, hu2022where2comm}, and Late fusion, and the recent \textit{Laly} fusion \cite{dao2024practical}. We detail implementation specifics in the Appendix of this report.

\subsection{Results}
We show the benchmark of \textit{Collaborative Object Detection} task in \autoref{tab:v2x-default}.
The result in this table clearly demonstrates the advantage of collaboration perception over single-agent perception as all fusion methods largely outperform No Fusion on every class.
The comparison of three conventional fusion methods, including Early, Intermediate, and Late, shows that a higher precision is attained at the cost of a larger bandwidth consumption.
In contrast, \textit{Laly} fusion achieves comparable precision on Bike and Pedestrian compared to Early Fusion and Intermediate Fusion while consuming an order magnitude less bandwidth.
The ability to achieve high performance for less bandwidth of \textit{Laly} fusion coupled with its simplicity make this method a strong candidate for real-world deployment. However, we note that state-of-the-art methods still exhibit a performance gap on our dataset, suggesting the need for future algorithm design. 


\autoref{tab:v2x-freeze-rsu} presents the performance of different fusion methods on \textit{Object Detection Enhanced by Communication to RSU} task. 
In this setting, detector training is more challenging, as each vehicle-centric detector must adapt to a frozen RSU detector. Nevertheless, results show that communication with RSU is still advantageous, as evidenced by the substantial performance improvement over the No Fusion baselines.
Furthermore, the performance of the Laser car is better than the performance of the two EVs.
This is because the \lidar of the Laser car has a 360-degree coverage of its surroundings.
On the other hand, the tilted angle of the \lidar on the two EVs makes the region behind them unobservable.
Additionally, the {\lidar}s on the two EVs do not capture the intensity information, resulting in a domain gap between their features and those from the RSU.
These observations point to future research directions for developing methods that could work well with diverse sensor configurations.

