\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\theequation}{S\arabic{equation}}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}

\noindent
In the appendix, we include extra details about the dataset and provided devkit, as well as annotation details and instructions given to annotators. We also provide additional sensor details.

\input{tables/class-definitions}

\section{Data and Devkit}
Please see \url{https://mixedsignalsdataset.cs.cornell.edu/} for the dataset download instructions and the provided devkit. Below, we add a brief description of the devkit and visualize a dataset sample.

\subsection{Devkit Description}
% \todo{add more description}
We integrate our dataset into the framework OpenCOOD \cite{xu2022opv2v}, which offers the implementation of various state-of-the-art collaborative perception methods. 
As OpenCOOD only provides single-class models, we adapt its implementation of Early, Intermediate, and Late Fusion models to detect three classes, including vehicles, bikes, and pedestrians.
We added detection heads of 1-by-1 convolution layers to existing architectures to achieve this.
In addition, we add the recent \textit{Laly} fusion \cite{dao2024practical} to this framework.
Every model in our benchmark uses PointPillar \cite{lang2019pointpillars} as the backbone.
Interested readers can refer to our code release for further details on architectures and training settings. 

\subsection{Sample Data}
\autoref{fig:data_collection} shows an example of the collected data, where the points are colour-coded to represent the different {\lidar}s. %In the centre of the image, a vehicle can be seen, represented by a dense collection of points from different lidars. 
%
% \begin{figure}[h!]
% \centering
% \includegraphics[width=\columnwidth]{figures/data_collection.png}
% \caption{\small Top-down view of the data collected.}
% \label{fig:data_collection}
% \end{figure}
\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{figures/top-down_lidar.jpg}
\caption{\textbf{Top-down view of the data collected at the location.} \lidar point clouds are colored by the vehicle and RSU that collected them, consisting of the 3 vehicle agents ({\color{red}red}, {\color{Goldenrod}yellow}, and {\color{Purple}purple}) and the Top and Dome LiDAR sensors of the RSU ({\color{ForestGreen}green}, {\color{blue}blue}). Best viewed in colour.}
\label{fig:data_collection}
\end{figure}
%
The dataset aims to replicate realistic urban scenarios that reflect the complexities of real-world implementations by using multiple vehicles with diverse sensor configurations and a roadside unit. 
Real-world deployments of autonomous vehicles on streets incorporate {\lidar}s, which are becoming more affordable. Roadside infrastructure, such as roadside units, is also gaining popularity for traffic monitoring and data analytics, now often equipped with LiDAR, traffic light timing information, and communication systems to enhance robustness and applicability.

Our dataset consists of \lidar point clouds, which offer the advantage of not capturing identifiable information like faces or license plates, thus preserving data privacy. This contrasts with camera images, which often require post-processing to anonymize sensitive details, potentially affecting data quality. Our dataset includes tracking IDs for each bounding box, and this information will be released alongside this paper. Benchmarks will be made available at a later date.
%

\section{Annotation Instructions}

We provide the instructions given to the Segments.ai\footnote{https://segments.ai/} annotators on the dataset website under \texttt{annotation\_instructions.pdf}.
We selected to invest in the quality of the annotations,
applying rigorous quality control measures to guarantee accurate and consistent labeled data, minimizing errors, and maintaining high standards.
In \autoref{table:v2x_label}, we provide the definitions of the 10 fine-grained annotation classes in the \ours dataset. The breakdown of the fine-grain classes into the benchmarked classes can be found in the main text.


\section{Sensor Details}

\subsection{Hardware and Synchronization Details}

\begin{table}[th!]
\vspace{-8px}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l c c c c}
\toprule
\textbf{Sensor} & \textbf{Agent} & \textbf{Range$^*$} & \textbf{Channels} & \textbf{Vertical FOV} \\ \midrule
Ouster OS1-128         & Vehicles       & 170 m          & 128               & 45                    \\ %\hline
Ouster OS1-64          & RSU            & 100 m          & 64                & 45                    \\ %\hline
Ouster OS Dome         & RSU            & 45 m           & 128               & 180                   \\ \bottomrule
\multicolumn{5}{l}{\small $^*$Based on 80\% Lambertian reflectivity in the sensors' official datasheets.} \\
\end{tabular}}

\vspace{-4px}
\caption{\small\textbf{Hardware specifications} }
\label{tab:sensor-details}

\vspace{-8px}
\end{table}

\noindent
The sensors in our multi-agent system were timestamped using GPS time as a common reference, and sensor details are provided in \autoref{tab:sensor-details}.
The maximum time gap for matching sensor readings between 10 Hz rotational sensors is 50 ms. 
Since sensors rotate fully in 100 ms, angular positions differ by at most 180 degrees.
If the time difference between readings were larger than 50 ms, it would be 
matched with the next or previous rotation instead.
As shown in the original manuscript, precise sensor synchronization, robust multi-agent localization, and clearly defined annotation protocols produced high-quality data association across all sensors.

\subsection{Definition of Heterogeneity in Sensor Suite}
Heterogeneity in our context refers to the variability between LiDAR sensors and platform geometry within a single dataset. Heterogeneity can appear in multiple forms \cite{jung2023helipr}; our dataset represents it in five LiDARs that span three models, each mounted in four configurations. In line with the feedback, Tab. 1 of the original manuscript has been updated accordingly.
%
Our dataset demonstrates a realistic setting where collaborative agents have different LiDAR models and position them in different configurations. 