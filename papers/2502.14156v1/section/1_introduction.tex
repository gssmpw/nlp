

\section{Introduction}
\label{sec:intro}

% transportation is increasingly autonomous (and assisted)
% previous research + data focuses primarily on individual autonomous vehicles 
% as autonomous vehicles become increasingly prevalent, it is high time to focus on multi-agent settings. 
% in the future vehicles can communicate with each other and wired road site units
% future of intelligent transportation
% in this paper we introduce ...
 

 % Leveraging emerging
% technologies to anticipate, prevent, and mitigate unsafe roadway conditions could augment traditional
% safety engineering in roadway design and intersection control. These emerging technologies are
% expected to include machine vision, machine perception, sensor fusion, real-time decision-making,
% artificial intelligence, and vehicle-to-everything (V2X) communications. \cite{nhtsa_report}

% \todo{katie}

% \todo{lean on availability. make it accessable to the reviewers.}
% \todo{include videos.}
% \todo{include subset of datasets.}
% \todo{not publicly available outside of geo regions}
% \todo{or not difficult in different way}
% \todo{first publically available, within version, subset of dataset is in supp material. very high quality. check videosssss}
% \todo{point them somehow to playing with the data!}
% \todo{mention it's ready to be released}
% \todo{Results: visualizations of hard cases.}
% \todo{what are the challenges?}
In recent years, driver assistance~\citep{khan2019comprehensive,nidamanuri2021progressive} and autonomous driving~\citep{yurtsever2020survey,nhtsa_report} technologies have advanced significantly, equipping vehicles with promising capabilities in perception~\citep{lang2019pointpillars,yin2021center}, planning~\citep{hu2022st,hu2023planning}, and control~\citep{amini2020learning,di2021survey}. Most of these developments, however, focus on scenarios involving a single autonomous vehicle, where challenges remain in complex or unpredictable situations~\citep{xu2022opv2v}. For instance, important traffic participants can be occluded from view, or sensors can fail unexpectedly. With more autonomous vehicles being deployed, new possibilities emerge to address these issues: Multiple vehicles can communicate with each other and with nearby infrastructure so that each vehicle can use shared information to reliably detect road users even when its own sensors miss them. This approach is commonly referred as \textbf{vehicle-to-everything (V2X)} collaborative perception.

% This advancement has been driven by the release of numerous large-scale public datasets~\citep{kitti,lyft,waymo,nuscenes,argoverse,once,ithaca365}, offering real-world data across diverse geographical locations, environments and traffic conditions. However, most existing research has concentrated on scenarios involving a single autonomous vehicle. With more of such vehicles being deployed, new possibilities emerge for them to communicate with each other and with nearby infrastructure, to jointly enhance perception and safety. This setting is commonly referred as vehicle-to-everything (V2X) collaborative perception. 

% While single-vehicle perception datasets are abundant and cover diverse driving conditions~\citep{kitti,lyft,waymo,nuscenes,argoverse,once,ithaca365}, real-world V2X datasets are limited and largely focused on three geographical locations in right-hand driving countries~\citep{yu2022dair,yu2023v2x,xu2023v2v4real,xiang2024v2x,zimmer2024tumtraf}. Additionally, in these datasets, the connected autonomous vehicles (CAVs) share identical \lidar sensor setups, under the assumption that the communicating vehicles are operated by the same company. However, as collaborative perception becomes more widespread, it would be valuable for vehicles of different models and manufacturers to communicate. These vehicles are likely equipped with different sensor configurations, introducing additional complexities that need to be considered.

While single-vehicle perception datasets are abundant and cover diverse driving conditions~\citep{kitti,lyft,waymo,nuscenes,argoverse,once,ithaca365,pandaset,apolloscape,oxfordrobotcar,cadc}, real-world V2X datasets remain limited in availability, diversity, and quality. Only a handful of publicly available V2X datasets exist~\citep{yu2022dair,yu2023v2x,
xu2023v2v4real,xiang2024v2x,zimmer2024tumtraf}, with some of them accessible only within specific geographical regions~\citep{yu2022dair,yu2023v2x}. These data are collected exclusively from three right-hand driving locations, overlooking the unique traffic dynamics in left-hand driving countries which make up about a third of the world~\citep{xu2023left}. Furthermore, in these datasets, the connected autonomous vehicles (CAVs) share identical \lidar sensor setups, under the assumption that the communicating vehicles are operated by the same company. However, as collaborative perception becomes more widespread, it would be valuable for vehicles of different models and manufacturers to communicate. These vehicles are likely equipped with different sensor configurations, introducing additional complexities that need to be considered. Finally, as the V2X setting involves multiple agents and sensors, data collection and alignment present additional challenges. 
Often times, difficulty with pose estimation and faulty localization systems result in poor alignment (\autoref{fig:localization-quality}). 
Such inaccuracies can lead to suboptimal performance for detector training~\citep{xu2022v2x}.

To address current limitations, we introduce the \ours dataset, designed to support diverse real-world V2X research scenarios with clean, high-quality data. Notably, \ours is the first V2X dataset that provides heterogeneous CAV \lidar configurations and features a left-handing driving country, Australia. The dataset includes 45.1k point clouds and 240.6k bounding boxes, collected from three CAVs equipped with two types of \lidar sensors, along with a roadside unit with two {\lidar}s. It captures a diverse range of traffic participants across 10 different classes, including 4 vulnerable road user categories. Furthermore, to our best knowledge, \ours is the \textit{highest quality large-scale V2X dataset publicly available}, providing precisely aligned and consistently annotated data across both time and different viewpoints. We emphasize that our dataset is \textit{\textbf{ready-to-use}}; the data and annotations are available on the dataset website (\url{https://mixedsignalsdataset.cs.cornell.edu/}), along with the corresponding video visualization showcasing their quality.
To summarize, our contributions are as follows:
\begin{itemize} 
    \item We introduce the \ours dataset, a high quality, large-scale, publicly available V2X dataset created through careful processing and precise annotations.
    \item To the best of our knowledge, we are the first real-world V2X dataset that encompasses heterogeneous CAV \lidar setups and left-hand traffic scenarios.
    \item We provide detailed analysis of the dataset's statistics, and conduct comprehensive benchmarking of existing V2X methods across various settings.
\end{itemize}

