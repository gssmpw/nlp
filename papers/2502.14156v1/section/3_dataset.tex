\section{Mixed Signals Dataset}
In this section, we describe the data collection process of the \ours dataset. We have included a devkit and a sample subset of our dataset in the supplementary materials, to allow interested readers to explore it in more detail.

\vspace{-2px}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hardware}

The data collection was carried out using three vehicles and a roadside unit. 

\mypara{Vehicles.}
The three vehicles included two small electric vehicles (EVs) and one urban vehicle, each equipped with OS1 128-beam {\lidar}s, as shown in \autoref{fig:vehicles}. 
The \lidar on the urban vehicle is located horizontally with respect to the ground, while for the EV, the \lidar is tilted downwards 15 degrees. We transformed the EVs' point cloud to have a horizontal reference frame as shown in \autoref{fig:evs}. Although all the vehicles are equipped with the same type of LiDAR sensor, their configurations differ in terms of sensor position and orientation. This variation introduces additional complexity, creating a domain gap between the data collected from different vehicles.

\begin{figure*}
\centering
\begin{subfigure}[]{0.3\linewidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figures/Evs.png}
\label{fig:ev}
\caption{\small Electric vehicle (EV) with a OS1-128 beams \lidar.}
\label{fig:evs}
\end{subfigure}
\quad
\begin{subfigure}[]{0.4\linewidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figures/lasercar.png}
\label{fig:passat}
\caption{\small Urban vehicle with a OS1-128 beams \lidar.}
\end{subfigure}
\quad
\begin{subfigure}[]{0.2\linewidth}
\centering
\includegraphics[width=\columnwidth]{figures/rsu_frame_fix.png}
\label{fig:rsu}
\caption{\small Roadside Unit (RSU) hardware.}
\end{subfigure}
\caption{\small \textbf{Vehicles used for data collection.} (a) is a small electric vehicle outfitted with an OS1-128 beams \lidar system. The \lidar is mounted at a 15Â° angle relative to the vehicle's body and stands at a height of 1.63 meters. (b) is an urban vehicle equipped with an OS1-128 beam \lidar system located at a height of 1.9 meters. (c) is the RSU which consists of two LiDARs: an OS1-64 beam (\rsutop) and an OSDome-128 (\rsudome) LiDAR mounted on a pole at the intersection at a height of 2.5 meters.}
\label{fig:vehicles}

\vspace{-10px}
\end{figure*}

\mypara{Roadside Unit.}
The roadside unit is equipped with two different LiDAR sensors: an OS-Dome 128-beam for long-range detection and an OS1 64-beam LiDAR for detecting nearby objects. It was located at a fixed geographical position, 2.5 meters above the ground.
The intersection where the roadside unit was installed experiences moderate vehicular traffic and features pedestrian crosswalks along with a bike lane that crosses the intersection. This setup allows us to capture diverse agents during data collection. The placement of the roadside unit is illustrated in \autoref{fig:rsu_location}. 


\begin{figure}[h!]
\centering
\includegraphics[width=0.99\columnwidth]{figures/roadside_unit_location.png}
\caption{\small \textbf{Geographical location of the roadside unit.}}
\label{fig:rsu_location}
\vspace{-16px}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Acquisition}

\subsubsection{Location}
The data collection took place at the 
intersection between Abercrombie Street and Myrtle Street (Sydney, Austria),
where the roadside unit is located. The vehicles recorded LiDAR data for two hours during peak rush hour. Throughout this period, the three vehicles repeatedly passed through the intersection. This allowed them to capture interactions between the vehicles and other agents on the road, such as pedestrians, cyclists, and other vehicles. 

\subsubsection{Synchronization and Localization}

Synchronization and localization are crucial for cross-sensor point cloud alignment. Our dataset employs proven robotic techniques to achieve precise sensor synchronization and agent localization, resulting in superior point cloud alignment compared to previous V2X datasets (\autoref{fig:localization-quality}). We describe the details below.


\mypara{Synchronization} refers to the temporal alignment of data streams, ensuring that synchronized sensors capture the same events simultaneously within their overlapping fields of view (FOV). 
This is especially important in dynamic environments, as any introduced time shifts can lead to positional inconsistencies, resulting in multiple detections of the same object.


We use GPS time to timestamp point clouds captured by our LiDARs at a frequency of 10 Hz.
Rotational LiDARs continuously scan the environment in 360 degrees, thus, different portions of the surroundings are captured at slightly different times during a full rotation. When vehicles are in motion, their positions and orientations change dynamically between LiDARs sweeps. This means that even if two vehicles are GPS-synchronized, the data they collect will represent slightly different moments in time and from different spatial perspectives.
We defined data samples by setting a time window to match the closest available timestamps from each LiDARs. A maximum timestamp mismatch of 50 milliseconds between point clouds was set to achieve minimal spatial discrepancies. 


\mypara{Localization} is one of the most critical tasks for CAV, estimating their position relative to a global reference frame. 
One of the most commonly used sensors for localization is the Global Navigation Satellite System (GNSS). GNSS offers access to a satellite constellation that provides global positioning via triangulation. However, despite its widespread use, GNSS has several drawbacks, particularly in urban environments. Its accuracy can be reduced in urban canyons, where tall buildings block or reflect signals, leading to degraded positioning accuracy. 

To overcome this problem, we use dense and accurate point cloud maps \cite{liosam2020shan} as references for our localization algorithm. Both the vehicles and the roadside units are localized within a common reference frame, referred to as the $map\_frame$, which serves as the origin of our map. The localization algorithm employs a scan-matching technique \cite{ndt} to estimate the vehicles' poses within this map, achieving a maximum positioning error of 15 cm and a heading error of 0.4 degrees.
This allows for consistent spatial alignment between the vehicles and the roadside infrastructure. The vehicles' localization estimates their positions within the $map\_frame$, while the roadside unit is static.


\subsubsection{Scene Selection}

In total, 37 scenes were carefully selected for inclusion in the dataset due to their rich diversity of vehicles, pedestrians, and cyclists. The primary goal was to capture various vehicles and vulnerable road users. These scenes encompass a broad spectrum of interactions, including between different types of vehicles and between vehicles and vulnerable road users.
The selected scenes feature intersections of the FOV of the LiDARs of 3 vehicles and the RSU.

Among 37 scenes of our dataset, we select 33 scenes for training and 4 scenes of testing.
The size of the training set and test set are, respectively, 9553 and 1164 data samples.
Our selection ensures that there is not any temporal overlapping between the training set and test set and among scenes of the test set.


\subsection{Dataset Annotation}

The task of 3D object detection for autonomous vehicles requires annotations in the form of 3D bounding boxes, usually parameterized by the center location, three dimensions (length, width, height), and rotation (represented as a quaternion).
% , yaw angle.
To generate such annotations for each data sample, we first aggregate the point clouds of every agent in the coordinate of the roadside unit's top (\rsutop) \lidar.
Then, professional annotators from FlipSideAI \cite{flipside} employ the SegmentsAI \cite{segments} annotation tool to label objects and localize them with 3D bounding box. Classes labeled belong to 10 categories, consisting of: car, truck, pedestrians, bus, electric vehicle, trailer, motorcycle/bike, bicycle, portable personal mobility, and emergency vehicle. 
\autoref{fig:teaser} depicts the annotations applied to the dataset, where each object is enclosed within a cuboid. 
Our annotation process involved cycles of monitoring, reviewing, and adjusting labels to meet defined quality objectives.
This allows \ours dataset to extend the quality of the pioneering datasets in the field, which are generally labeled by lay annotators, as shown in \autoref{fig:label-quality}. Here, we reproject the bounding box of a vehicle, as observed from other sensors, back onto its coordinate frame to visualize label consistency. 
Details of the class descriptions and labeling instructions are presented in supplementary materials.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/localization-quality.png}
    \caption{\small \textbf{Localization and synchronization quality of \ours and existing datasets.} Different colors correspond to different sensors. In the lateral view, existing datasets visually exhibit vertical inconsistencies, where one point cloud is tilted due to localization errors. In contrast, point clouds in \ours are all accurately aligned.
    }
    \label{fig:localization-quality}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/label-quality.png}
    \caption{\small \textbf{Label quality comparison of \ours and existing datasets.} Labels in \ours are highly consistent across different time steps and viewpoints.}
    \label{fig:label-quality}
\end{figure}

The annotation process for this multi-sensor dataset involves handling joint scenes and synchronization discrepancies between sensors. Due to time synchronization, fast-moving objects might appear slightly offset across the data collected from different sensors. To address these discrepancies, annotators were instructed to prioritize the roadside unit point cloud for bounding box creation, following a set hierarchy. When there is a mismatch, bounding boxes should be aligned with the point cloud in the following order: roadside unit,  EVs, and the urban vehicle. For example, if there is a difference between the roadside unit and the vehicles' point cloud, the bounding box should only be fitted around the roadside unit points. This ensures consistency in object localization across frames despite synchronization lags.
%
While agents in our dataset are synchronized at 10 Hz, we sample keyframes at 1 Hz for manual annotation.
To obtain annotations in a non-key frame, we linearly interpolate the pose of annotations of its closest preceding and succeeding keyframes based on their timestamp.

\paragraph{Category Labels.}
The Mixed Signals dataset categories road agents in different vehicle types and pedestrians. Categories such as ``Car" and ``Truck" encompass common passenger and large transport vehicles, while ``Emergency Vehicle" covers ambulances, fire trucks, and police cars, highlighting their importance in urban scenarios. ``Bus" labels are designated for large passenger vehicles typically used in public transportation. The dataset also distinguishes between ``Motorcycle" and ``Motorized Bike," and ``Portable Personal Mobility Vehicle," which includes modern personal transport devices like electric scooters and hoverboards. Traditional ``Bicycle" labels account for both standard and electric bikes. Labels for ``Electric Vehicle" and ``Trailer" ensure that smaller, often data-collection vehicles and towable units are accurately represented. Finally, we labeled humans as ``Pedestrians".
\autoref{table:v2x_label} provides the detailed definition of each category.





\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/lidar_distribution.png}
\caption{\small \textbf{Distribution of \lidar intensities from RSU \rsutop, \rsudome, and \laser car sensors.} Each sensor shows different intensity ranges and distributions. \evone and \evtwo \lidar sensors do not have intensity readings.}
\label{fig:lidar-intensities}

\vspace{-16px}
\end{figure}



\subsection{Dataset Analysis}  \label{sec:dataset_analysis}

Fig. \ref{fig:lidar-intensities} shows LiDAR intensity distributions from three sensors: RSU \rsutop, \rsudome, and \laser car sensors. 
\rsudome and \rsutop sensors record higher intensities because there is a large number of static objects (e.g., buildings, traffic lights) that are near them.
In contrast, the \laser car sensor presents a smoother decline in intensity values because of its location on the vehicle, which allows the detection of objects at greater distances. 
EV-1 and EV-2 sensors do not capture intensity readings.

In our benchmark, we group 10 categories into 3 detection classes according to \autoref{tab:anno_cls_to_det_cls}. \autoref{fig:percentage_each_class_in_polar_coord} shows the distribution of annotations of three classes with respect to their polar coordinate in the coordinate system of \rsutop. \autoref{fig:classes_size_distribution} shows the distribution of dimensions and yaw angle of annotations of three classes. 
\autoref{fig:num_obj_per_class_in_train_test} shows the number of annotations of each class in the training set and test set.

\autoref{fig:track-length} analyzes track lengths in the training and test set. For both splits, most tracks are under 10 seconds. This is due to the dynamic and typical speeds at the intersection environment. A sharp peak at 30 seconds indicates the presence of static objects detected primarily by the RSU for the entire sequence duration.

\autoref{fig:tracks-quality} depicts the aggregation of point clouds from 5 agents and ground truth annotations in the coordinate system of \rsutop during a 4-second time span, which amounts to 40 time steps. 
The consistent pose of static objects and the smooth trajectory of dynamic objects visually demonstrate the quality of our annotation.

\input{tables/detection-classes}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/class_distribution_in_traintest.png}

\vspace{-6px}
\caption{\small \textbf{Number of objects by class.} The y-axis is displayed in log scale.}
\label{fig:num_obj_per_class_in_train_test}

\vspace{-4px}

\end{figure}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/polar_distrib_car_all.pdf}
        \caption{Vehicle}
    \end{subfigure}
    %
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/polar_distrib_bike_all.pdf}
        \caption{Bike}
    \end{subfigure}
    %
    \begin{subfigure}{.32\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/polar_distrib_ped_all.pdf}
        \caption{Pedestrian}
    \end{subfigure}%
    \caption{\small \textbf{Distribution of annotated object locations.} Locations are shown in polar coordinates relative to the RSU \rsutop sensor.}
    \label{fig:percentage_each_class_in_polar_coord}
\end{figure*}



\begin{figure*}[t!]
    \centering
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/size_distrib_dx_all.png}
        \caption{Length (m)}
    \end{subfigure}
    %
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/size_distrib_dy_all.png}
        \caption{Width (m)}
    \end{subfigure}
    %
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/size_distrib_dz_all.png}
        \caption{Height (m)}
    \end{subfigure}%
    \begin{subfigure}{.24\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/heading_all_three_all.pdf}
    \caption{Yaw Angle}
    \end{subfigure}%
    \caption{\small \textbf{Distribution of bounding box dimensions and yaw angles.} Vehicles exhibit a wide range of sizes.}
    \label{fig:classes_size_distribution}
\end{figure*}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/track_length.png}
    \caption{\small \textbf{Distribution of track lengths.} The peak at 30 seconds corresponds to static objects.}
    \label{fig:track-length}

    \vspace{-16px}
\end{figure}

\begin{figure}
    \centering
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[trim={1.4cm 0 1.4cm 0},clip,width=0.95\linewidth]{figures/tracks-quality.png}
    \end{subfigure}

    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[trim={0cm 0 0cm 0},clip,width=0.95\linewidth]{figures/colorbar_track_quality.pdf}
    \end{subfigure}
    \caption{\small \textbf{Visualization of object tracks in \ours.} Dynamic objects display smooth trajectories, while static objects maintain consistent poses over time, highlighting the high quality of our annotations.}
    \label{fig:tracks-quality}

    \vspace{-10px}
\end{figure}


