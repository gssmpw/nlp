\begin{abstract}
Vehicle-to-everything (V2X) collaborative perception has emerged as a promising solution to address the limitations of single-vehicle perception systems. However, existing V2X datasets are limited in scope, diversity, and quality.
To address these gaps, we present \ours{}, a comprehensive V2X dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from three connected autonomous vehicles (CAVs) equipped with two different types of LiDAR sensors, plus a roadside unit with dual LiDARs. 
Our dataset provides precisely aligned point clouds and bounding box annotations across 10 classes, ensuring reliable data for perception training.
We provide detailed statistical analysis on the quality of our dataset and extensively benchmark existing V2X methods on it.
\ours{} is \textbf{ready-to-use}, making it one of the highest quality, large-scale datasets publicly available for V2X perception research. Details on the website: \href{https://mixedsignalsdataset.cs.cornell.edu/}{https://mixedsignalsdataset.cs.cornell.edu/}.

\vspace{-10px}
\end{abstract}