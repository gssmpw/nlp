\section{Related Work}
\label{sec2}
\subsection{Deep Learning-Based Knowledge Tracing}
Knowledge tracing is essentially a time series classification task that predicts student performance based on their historical learning records. Early knowledge tracing research primarily focused on combining Bayesian Knowledge Tracing (BKT)\cite{bkt} or using Item Response Theory (IRT)\cite{deepirt}. With the flourishing development of deep learning, numerous innovative models have emerged in this field. Piech et al. proposed Deep Knowledge Tracing (DKT), as the first model to introduce deep learning into knowledge tracing, employing a Recurrent Neural Network (RNN) architecture to capture temporal relationships in student interaction data\cite{dkt}. This pioneering work not only significantly improved prediction performance but also laid a crucial foundation for subsequent research. Compared to traditional methods, DKT can automatically extract features from large-scale learning behavior data and effectively handle complex relationships among multiple knowledge concepts. Chen et al. proposed QIKT based on the DKT model, introducing question-centric knowledge acquisition, knowledge state, and problem-solving modules, breaking the traditional homogeneous question assumption\cite{qikt}. The model innovatively combines IRT-based interpretable prediction layers, providing better interpretability while maintaining high prediction performance.

Following DKT, memory-based models gradually emerged, with Dynamic Key-Value Memory Networks (DKVMN) proposed by Zhang et al. being the most representative. This model enhances the representation of student historical records through a key-value pair mechanism, more accurately capturing students' latent knowledge state\cite{dkvmn}. DKVMN utilizes explicit memory representations to precisely track students' mastery of different knowledge concepts, and its unique memory mechanism optimizes the modeling of long-term dependencies.

However, when facing sparse learning data scenarios (such as students interacting with only a few knowledge concepts), traditional models struggle to effectively capture implicit relationships within the data. To address this issue, attention-based models were developed. Liu et al. proposed AT-DKT based on the DKT model, introducing Question Tag (QT) prediction and Individual prior Knowledge (IK) prediction as auxiliary learning tasks\cite{atdkt}. The model uses Transformer encoders and masked attention mechanisms for QT tasks and student ability networks for IK tasks. Pandey et al. proposed Self-Attentive Knowledge Tracing (SAKT), which through its multi-head attention mechanism and feed-forward network, can identify knowledge concepts (KCs) related to the target KC from historical activities and predict student mastery based on limited historical data\cite{sakt}. Compared to RNN-based approaches, SAKT demonstrates significant advantages in handling data sparsity issues. Huang et al. proposed sparseKT, a simple yet effective framework aimed at improving the robustness of attention-based knowledge tracing models\cite{sparsekt}. The model's core innovation lies in introducing k-sparse attention mechanisms, making predictions by explicitly selecting the most relevant historical interactions.

Taken together, the development of knowledge tracing models has evolved from traditional probabilistic models to deep learning approaches. However, how to better balance model performance, interpretability, and computational efficiency, as well as how to effectively integrate students' personalized characteristics, remain important directions for further exploration in this field. In traditional knowledge tracing models, predictions are mostly based on students' learning history records, such as problem IDs, knowledge concept IDs, and whether students are correct, without incorporating difficulty features into the training process. Therefore, it is impossible to combine students' knowledge state and potential difficulty features of questions to build a more refined prediction model. Similarly, the lack of difficulty features also weakens the model's interpretability, specifically manifested in: 1) educators cannot accurately evaluate students' knowledge mastery level based on their performance on questions of different difficulties, leading to a lack of targeted teaching strategy adjustments; 2) the model cannot distinguish students' performance differences on questions with the same knowledge points but different difficulties, affecting the precise analysis of learning trajectories; 3) when model predictions deviate from actual performance, it is difficult to explain the reasons for such deviations from the perspective of difficulty, reducing the credibility and practicality of prediction results.

\subsection{Difficulty in Knowledge Tracing}
Question difficulty, as a crucial evaluation metric in the educational process, can serve as a special predictive feature in knowledge tracing\cite{difficulty_intro}. In recent years, research incorporating additional information such as difficulty has gradually increased, though most approaches employ single difficulty information\cite{sideinfo}.

DIMKT is a difficulty-aware knowledge tracing model derived from response correctness rates, which improves knowledge tracing performance by establishing relationships between students' knowledge state and question difficulty levels to measure the difficulty effect\cite{dimkt}. The model's innovation lies in enhancing question representation by simultaneously considering both question-specific difficulty and knowledge concept difficulty, and designing three stages to capture the difficulty effect: first calculating students' subjective difficulty perception before practice, then estimating students' personalized knowledge acquisition when answering questions of different difficulty levels, and finally updating students' knowledge state according to question difficulty.

Similarly focusing on difficulty, Liu et al. proposed the QDCKT model, which combines first-attempt correctness rate-based difficulty with graph attention mechanism, innovatively replacing traditional question IDs with question difficulty levels\cite{dcl4kt}. The model employs an LSTM sublayer to generate representations of historical learning sequences and uses a feed-forward neural network as the prediction layer. QDCKT introduces two key techniques: first using the Hann function to combine embeddings of nearby difficulty levels, and second introducing difficulty consistency constraints to ensure prediction results align with question difficulty levels.

Zhang et al. proposed GDPKT based on the Graph-based Knowledge Tracing (GKT) model\cite{gdpkt}. It enhances knowledge tracing capabilities through heterogeneous graph neural networks and personalized difficulty modeling. The model introduces difficulty nodes into the heterogeneous graph, uses Meta-path to construct node representations, and combines difficulty perception and learning gain modules to model students' knowledge state personalized. The main innovation lies in modeling exercise difficulty as independent nodes in the graph while considering students' personalized perception and learning gains from exercises of different difficulties. Qiu et al. proposed MGEKT is a knowledge tracing model based on multi-graph embedding\cite{megkt}. It adopts a dual-channel architecture, with one channel using node2vecWalk and Meta-path to enhance question representation, and the other channel using AGCN to process directed graphs of learning interactions. The model innovatively considers question difficulty from three dimensions (correctness rate, attempt count, and response time) and introduces reverse knowledge distillation to integrate information from both channels. Experiments demonstrate that this model outperforms existing methods across four datasets, significantly improving knowledge tracing performance.

However, relying solely on statistically derived difficulty as a question difficulty indicator often lacks objectivity and comprehensiveness. This manifests in the data when limited student responses to certain questions result in extremely high or low difficulty values. Moreover, statistical difficulty fails to effectively capture intrinsic question characteristics such as concept complexity and solution steps. This single-dimensional difficulty measurement approach limits the precise modeling of students' learning state in knowledge tracing models. Due to these limitations, existing models often encounter cold-start problems when dealing with questions having few interaction records, making it challenging for difficulty features to serve as objective indicators for learning the intrinsic connections between difficulty and questions. This not only affects model prediction accuracy but also limits its practical application in educational scenarios. Furthermore, the multi-dimensional nature of difficulty assessment has not received sufficient attention in current research. Question difficulty depends not only on statistical data but should also consider solution complexity, knowledge structure complexity, and other dimensions, which opens possibilities for introducing new technologies like large language models to enhance the objectivity and multi-dimensionality of difficulty assessment.

\subsection{Large Language Model Application in Knowledge Tracing}
In recent years, the field of Natural Language Processing has witnessed significant technological advancement with the emergence of increasingly sophisticated tools. Large Language Models (LLMs), as representatives of new-generation pre-trained models, have demonstrated exceptional performance across various domains. Models like ChatGPT and Claude, utilizing techniques such as Chain-of-Thought (COT), have substantially enhanced their capabilities in understanding and generating deep semantic information in natural language. However, as general-purpose language understanding and generation tools, LLMs show limitations in handling strong sequential problems in knowledge tracing, thus primarily serving as auxiliary tools in knowledge tracing tasks\cite{llmnotime}.
\begin{table}[htbp]  
    \centering  
        \begin{tabular}{|c|c|}
        \hline
        \textbf{Symbol} & \textbf{Meaning} \\ \hline
        $q_i$ &  Question $i$\\
        $c_i$ & Concept of question $i$ \\
        $r_i$ & Student's response of question $i$ \\
        $var_t^{indicator}$ & Indicator of variable in timestamp $t$, e.g. Embedding\\
        $var_i^t$ & Variable of question $i$ in timestamp $t$ \\
        $ks_t$ & Student's knowledge state in timestamp $t$\\ 
        $d_t^{type}$ & Difficulty of question sequence in timestamp $t$, e.g. LLM-based Difficulty \\
        $dmr_t$ & Difficulty Mastery Ratio of question sequence in timestamp $t$ \\
        $dpbs_t$ & Difficulty Perception Bias Sequence of question sequence in timestamp $t$ \\
        $ddai_t$ & Dynamic Difficulty Adaptability Index of question sequence in timestamp $t$ \\
        \hline  
        \end{tabular}  
    \caption{\label{tab:widgets}Mathematical symbol and its meanings.}
\end{table}
Yu et al. proposed ECKT, a knowledge tracing model based on large language models and CodeBERT\cite{eckt}. It generates question descriptions and knowledge concepts through chain-of-thought reasoning and few-shot learning, uses BERT for embedding, and combines AST and attention mechanisms for code representation. ECKT innovates by using LLM to generate question descriptions and knowledge concepts, introducing difficulty embeddings to enhance representation, and employing stacked GRU to improve sequence learning capabilities. Guo et al. proposed EAKT (Enhanced Attribute-aware Knowledge Tracing), a LLM-based cold-start solution for knowledge tracing that leverages LLMs to extract additional information such as required problem-solving capabilities\cite{eakt}. EAKT introduces three key innovations: first, it designs an attribute estimation module that utilizes models like GPT-4 with grouping strategies and chain-of-thought prompting to analyze questions and estimate multi-dimensional attributes including difficulty, ability requirements, and expected response time; second, it develops a question embedding module that employs graph attention networks to dynamically adjust attribute values for alignment with the target population's cognitive characteristics; finally, it enhances model interpretability through multi-dimensional attribute representations.

Lee et al. introduced the DCL4KT+LLM (Difficulty-focused Contrastive Learning for Knowledge Tracing with LLM) model, which integrates difficulty-focused contrastive learning with LLM-based difficulty prediction\cite{dcl4kt}. The model comprises three core components: first, it constructs positive and negative embedding layers, where positive embeddings contain question, concept, question difficulty, concept difficulty, and student response information, while negative embeddings contain corresponding "hard negative" information; second, it employs four encoder modules to calculate binary cross-entropy loss and contrastive learning loss separately; third, it introduces a difficulty-focused contrastive learning framework that optimizes model representations through concept and question similarity computations. Yang et al. proposed DPKT, a programming knowledge tracing model based on large language models and CodeBERT\cite{dpkt}\cite{codebert}. It uses large language models to evaluate both the text comprehension difficulty and knowledge concept difficulty of programming questions, extracts semantic features through CodeBERT, and combines graph attention networks with update gate mechanisms to dynamically adjust students' knowledge state. Its innovation lies in being the first to use large language models to assess programming question difficulty, dividing difficulty into text comprehension and knowledge concept dimensions for more precise difficulty assessment.

While current research has shown promising results in utilizing LLMs to extract supplementary information for knowledge tracing, many existing methods simply incorporate LLM-extracted additional information directly into the input, overlooking individual students' difficulty perception. Therefore, how to effectively integrate objective difficulty assessment with personal difficulty perception to enhance knowledge tracing model performance remains an important research direction. Incorporating personal difficulty perception into objective question difficulty could enable more fine-grained student modeling and improve model prediction accuracy.