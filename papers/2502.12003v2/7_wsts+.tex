
\section{The WSTS+ Benchmark}
\label{sec:wsts+}

Our results on the WSTS benchmark indicated that relatively simple models performed best, such as those based upon a ResNet-18, rather than models utilizing larger encoders (e.g., ResNet-50) or those utilizing attention (e.g., SwinUnet).  This contrasts sharply with the broader vision literature where larger models tend to perform best, given sufficient quantities of training data.  Therefore, we hypothesize that collecting more training data would facilitate the use of larger models, yielding superior modeling performance. To investigate this hypothesis, we expand the original WSTS benchmark by curating four additional years of historical wildfire data: 2016, 2017, 2022, and 2023. Our extended dataset, termed WSTS+, contains twice the number of years of historical wildfire data, expands the geographic diversity of the benchmark, and is -- to our knowledge -- the largest public benchmark for time-series next-day wildifre spread prediction. We visualize the geographic distribution of WSTS+ events in \cref{fig:wsts+map} and observe that it is mostly similar to WSTS around the Western United States, with some distinction in more eastern states. \cref{tab:wsts+} summarizes the differences between both datasets in terms of numbers of years, fire events, total images, and active fire pixels. Further collection details can be found in the Supplement.

\begin{table}[h!]
    \centering
    \caption{Comparison between the original WSTS dataset and our extension. We double the number of years and total images and drastically increase the number of fire events and active fire pixels.}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \textbf{Dataset}        & \textbf{WSTS} & \textbf{WSTS+} & \textbf{Increase} \textbf{(\%)} \\
    \midrule
    Years     & 4 (2018-2021) & 8 (2016-2023) & $+100$ \\
    Fire Events & 607 & 1,005 & $+65.6$ \\
    Total Images & 13,607 & 24,462 & $+79.8$ \\
    Active Fire Px & 1,878,679 & 2,638,537 & $+40.4$ \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:wsts+}

\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{fire_events_map_2016_2023_cut.png}
    \caption{Geographic distribution of the fire events in each year of WSTS (blue) and WSTS+ (red)}
    \label{fig:wsts+map}
\end{figure}

\subsection{Benchmarking Models with WSTS+}
% 1. present the goals first: increase data, reduce computational cmplexity; and retain resemblance to reality (hold out two years at a time)  more accessible to people while retaining the rigor; less computation
% 2. the validation set need to similar to the training set as the test set 
As compared to WSTS, we propose a new scheme for evaluating models using WSTS+, which exploits its greater size to significantly reduce computational complexity compared to WSTS -- thereby making the benchmark more accessible to researchers -- while maintaining a similar level of real-world rigor.  For WSTS+, we propose to divide the available data into four folds that each contain two consecutive years of historical wildfire data.  We then evaluate models using four-fold cross-validation, where in each iteration, one fold of data is used for testing, one fold for validation, and two folds for training, as illustrated in \cref{fig:wsts+folds}. To ensure that the testing and validation sets have the same relative temporal distance to the training set, we always select them so that they are non-consecutive.  This results in four-fold cross-validation instead of the twelve-fold cross-validation utilized in WSTS, making it much more accessible to researchers. At the same time, this approach doubles the quantity of data in the training and validation sets, ideally allowing researchers to train larger and more accurate models. Lastly, because two consecutive years of data are included in the test set, the benchmark still evaluates models under highly realistic and rigorous testing conditions, 
% add figure for folds :my folds vs gerard folds --> addition of two years

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{wsts_folds.png}
    \caption{New cross-validation folds used for WSTS+. Each pair of consecutive years is used as validation/testing once. Color code: blue: training, orange: validation, green: test}
    \label{fig:wsts+folds}
\end{figure}


\begin{table}[h]
    \centering
    \caption{Mean test AP $\pm$ standard deviation using vegetation features only (Veg), vegetation, land cover, topography and weather (Multi) and All features, when training on the WSTS+ dataset Results style: \textbf{best} }
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Days} & \textbf{Veg} & \textbf{Multi} & \textbf{All}  \\
        \midrule
        Res18-Unet & 1 & $0.349 \pm 0.109$ & $0.351 \pm 0.105$ & $\mathbf{0.351 \pm 0.122}$ \\
        Res50-Unet & 1 & ${0.345 \pm 0.096}$ & ${0.353 \pm 0.122}$ & $\mathbf{0.351 \pm 0.122}$\\
        Res18-Unet-LTAE & 5 & $\mathbf{0.354 \pm 0.113}$ & $\mathbf{0.363 \pm 0.129}$ & ${0.350 \pm 0.117}$  \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:WSTS+results}
\end{table}

% 2. why doesnt it help: look at the different years folds, there is lot of heterogeneity of X and Y|X; point to the UMAP and to the Geo Map + % add barplot or table of each test year 
\subsection{Experimental Results with WSTS+}
Using our updated cross-validation scheme, we train our best $T=1$ models and our best $T=5$ model on WSTS+ and report the results in terms of mean average precision across all three feature sets in \cref{tab:WSTS+results}.  We see that the performance rank-order of our three models is still similar on WSTS+ as compared to WSTS. However, the overall performance is significantly lower for these models on WSTS+ as compared to WSTS (by roughly 0.1 AP), and the differences in AP between the models are lower. These results seem to contradict our initial hypothesis that more training data would enable larger models and improve accuracy. 

To investigate further, in \cref{fig:wsts-plot}, we measure the performance (in AP) across all eight years of data of our Res18-Unet model trained on WSTS or WSTS+, respectively.  To estimate the WSTS+ models, we simply report the cross-validation test performance, stratified by year.  We apply the same approach to the WSTS benchmark, obtaining performance metrics for 2018 to 2021.  We select the best-performing Res18-Unet (as judged by its test fold error) and evaluate it to the four new years of WSTS data.  The results in \cref{fig:wsts-plot} indicate that, when viewed this way, the performances of the WSTS and WSTS+ models are very similar.  In particular, models trained on each benchmark obtain similar AP on the original four years of the WSTS benchmark (the bolded years in \cref{fig:wsts-plot}), indicating that the two additional years of data that were available in each training fold of WSTS+ were not harmful; however, they were not helpful either, contradicting our initial hypothesis.  \cref{fig:wsts-plot} also reveals that the overall difficulty of the new years of data - 2016, 2017, and 2022 in particular - are higher, as evidenced by the fact that all models perform much worse on those years, explaining why the overall AP on the WSTS+ is lower than WSTS.  

Crucially, the results in \cref{fig:wsts-plot} reveal substantial variance in model prediction accuracy across the eight years in WSTS+, regardless of which of our two models were utilized: WSTS or WSTS+. A similar pattern was observed on the WSTS benchmark \cite{gerard2023wildfirespreadts}, where the authors found similar variation in performance, which was stable across models trained with different years of training data.  These findings could admit several explanations: label noise variations across different years, making some harder than others; each year may exhibit different feature distributions (i.e., domains shifts), or wildfire behavior may vary across years (i.e., concept shifts).  \cref{fig:umap} presents a UMAP visualization of the input features of WSTS years (in blue) and the new WSTS+ years (in red) at the deepest level of our best Res18-Unet encoder. While there is substantial overlap in the features obtained from each year of data, we also observe that many years seem to exhibit unique feature patterns. If some years exhibit especially unique feature distributions that are not present in any other years (or few other years) then it could contribute to low accuracy on those years.  

\paragraph{An Emerging Wildfire Modeling Challenge?} Our results suggest that naively combining historical wildfire data may not scale well across longer time frames (and perhaps space), if our goal is to train more accurate data-driven models. Yet substantial evidence from the broader computer vision literature indicates that developing larger training datasets is crucial to improve modeling.  Therefore, an important emerging challenge for the wildfire modeling community may be to develop means of harmonizing, and/or otherwise effectively utilizing diverse historical wildfire data to train data-driven models.  We hypothesize that developing such methods may be key to obtaining better accuracy on the WSTS+ benchmark, and therefore that the benchmark will be a useful resource for developing solutions. 


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{test_year_breakdown_cropped.png}
    \caption{Performance breakdown by test year. Blue bars represent models trained on the original WSTS data, while red bars represent those trained on WSTS+. The bolded x-axis ticks highlight original test years from WSTS.}
    \label{fig:wsts-plot}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{umap_plot_all_cropped.png}
    \caption{UMAP visualization of the inputs features across years. Each point represents the encoded features at the deepest layer of our best Res18-Unet encoder, with blue indicating original WSTS year and red newly added years in WSTS+}
    \label{fig:umap}
\end{figure}
