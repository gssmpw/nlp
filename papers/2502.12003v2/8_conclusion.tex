\section{Conclusion}
\label{sec:conclusion}
In this work, we focus on the problem of next-day wildfire spread prediction, where we are provided with current and/or historical information about a particular wildfire, and then tasked with predicting its spatial extent on the following day.  We introduce a variety of modeling improvements in two scenarios: single-day ($T=1$) and time-series ($T>1$) input scenarios. Our results show that these improvements lead to substantial increases in accuracy over the existing state-of-the-art, as demonstrated on the WildfireSpreadTS (WSTS) benchmark \cite{gerard2023wildfirespreadts}, resulting in significant increases in the current state-of-the-art accuracy on WSTS.  Lastly, we introduce WSTS+, an extended benchmark for next-day wildfire spread, constructed by doubling the number of years of historical wildfire events in WSTS and resulting, to our knowledge, in the largest public benchmark for \textit{time-series} next-day wildfire spread prediction. The WSTS+ also reveals an emerging challenge for the wildfire modeling community: historical data heterogeneity. We hypothesize that developing models that can handle more heterogeneity (e.g., domain or concept shifts) across data may be key to obtaining better accuracy on the WSTS+ benchmark, and therefore that the WSTS+ benchmark will be a useful resource for developing such solutions. 

% In this work, we evaluated the benefits of using transformer-based models for  wildfire spread prediction on the WildfireTS dataset. Using a SwinUnet model with ImageNet pre-trained Swin blocks significantly outperformed fully convolutional baselines, establishing a new state-of-the-art benchmark performance. These results highlight the suitability of transformers for this task, particularly in the multi-temporal setting. 

% Additionally, we established a new state-of-the-art benchmark performance, by replacing the traditional absolute positional encodings of the UTAE model with relative encodings. 

% Future work should address the challenges of labeled data scarcity and domain shift by using Self-Supervised Learning (SSL) to train transformers, which are designed to handle missing input features. Another important area of future research is the development of models that integrate both spatial and temporal attention. 

% Our vision includes building robust fire prediction models across different geographic and environmental contexts, using any available subset of input features. This would reduce the modelâ€™s dependency on extensive ground truth labels and unify the existing fire spread benchmarks, as the model would be able to make predictions even when some input modalities are missing.
