\section{Improving Wildfire Spread Prediction}
\label{sec:experiments}

In this section, we describe our methods for $T=1$ and $T=5$ scenarios, respectively, as well as experiments to support them (e.g., ablations). Results for our developed benchmark models are reported in \cref{tab:results}, in terms of Average Precision (AP) using 12-fold leave-one-year-out cross-validation on the WSTS benchmark, following prior work \cite{gerard2023wildfirespreadts}. Also following \cite{gerard2023wildfirespreadts}, we report model performance for three feature sets: vegetation features only (Veg), a combination of vegetation and topographic features (Multi), and all features (All), which includes additional weather forecast features.  Full experimental details are provided in the supplemental information.  Models in \cref{tab:results} with citations correspond to the three current best models on WSTS, as reported in \cite{gerard2023wildfirespreadts}. All other models reported in \cref{tab:results} were developed in this work. 

% \paragraph{Improvement Strategy} Since WSTS is a relatively new benchmark, we found that there was substantial opportunity to improve wildfire modeling by introducing appropriate and well-established existing methods from the computer vision literature, and that it was more productive to focus on this strategy rather than immediately seeking to develop novel (and often much more complex) domain-specific models.

\subsection{Single-Day Input ($T=1$)}
\label{sec:results_single_day_input}

The current $T=1$ SOTA utilizes a U-Net architecture with a ResNet-18 encoder, and is denoted Res18-Unet\cite{gerard2023wildfirespreadts}. Therefore we focus our investigation on improving the Res18-Unet\cite{gerard2023wildfirespreadts}.     

\paragraph{Modeling Improvements} We next describe our improvements to the Res18-Unet\cite{gerard2023wildfirespreadts} at a high level.  More details can be found in the supplement. 

\textit{(i) Improved Encoders.} We hypothesize that better performance may be obtained with larger encoders, or those that utilize attention mechanisms. Recent studies have indicated attention-based models to be superior to convolutional models for wildfire spread \cite{li2024wildfire, zou2023attention, shah2023wildfire, xiao2024wildfire}, although these studies did not utilize the rigorous 12-fold leave-one-year out cross-validation in the WSTS benchmark that we use here.  Therefore we investigate the use of a ResNet50 \cite{he2016deep} encoder, as well as the attention-based SwinUnet-Tiny encoder \cite{cao2022swin}.  

\textit{(ii) Utilizing Pre-trained Parameters.} Utilizing pre-trained weights to initialize training is a well-established technique to improve model accuracy in data-limited scenarios, such as the WSTS benchmark. We introduce pre-trained weights into each of the encoders that we consider (i.e., ResNet18, ResNet50, and SwinUnet), while the decoders are trained from scratch.  

\textit{(iii) Improved Loss Functions.} The existing SOTA Res18-Unet \cite{gerard2023wildfirespreadts} is trained using weighted binary cross-entropy loss. However, it has been established that Jaccard/Dice losses are often superior alternatives for segmentation tasks \cite{eelbode2020optimization}, and focal loss has been shown effective for class imbalance \cite{lin2017focal} (the WSTS benchmark exhibits severe class imbalance), and for wildfire spread in particular \cite{fitzgerald2023paying}. Therefore we investigate and compare the aforementioned losses in our experiments.      

\textit{(iv) Improved Hyperparameter Optimization.} The existing SOTA Res18-Unet \cite{gerard2023wildfirespreadts} was trained by selecting the model with the highest F1 score on the validation, however, all models on WSTS are evaluated utilizing the average precision (AP) metric \cite{gerard2023wildfirespreadts}.  We investigate aligning the validation and testing metrics by using AP for both.    

For our experiments, we consider a U-Net with a ResNet-18 encoder (denoted \textit{Res18-Unet}), and a ResNet-50 encoder (denoted \textit{Res50-Unet}), and a SwinUnet-Tiny (denoted \textit{SwinUnet}).  For each of these models, we perform a grid search over all combinations of learning rates ($[1e-1,1e-2,1e-3,1e-4, 1e-5]$), loss functions (BCE, Focal, Dice, Jaccard), and the use of pre-training or not (a binary choice).  Following \cite{gerard2023wildfirespreadts}, we use a single fold of the 12-fold cross-validation, and only one of the three feature sets (the "All" set) for this optimization. As discussed, in contrast to previous work, we utilize AP during validation to select the best models instead of F1.  The focal loss has two hyperparameters: $\alpha$, set as the inverse frequency of positive class pixels, and $\gamma$, set to its default value of two.  

\paragraph{Experimental Results} We found that pre-training was nearly always beneficial, and that Focal Loss usually yielded substantial improvements compared to our other candidate losses. Therefore, for the WSTS benchmark, we included both pre-training and focal loss in all three of our models: \textit{Res18-Unet}, \textit{Res50-Unet}, and \textit{SwinUnet}. As an ablation study, \cref{tab:ablation_t1} reports the performance of our \textit{Res18-Unet} on the full WSTS benchmark, where we progressively remove each of our improvements to assess its impact. Our results indicate that each improvement is highly beneficial, or at least not significantly harmful.     

\cref{tab:results} reports the performance of our three models on the WSTS benchmark,  compared to the best existing $T=1$ model, \textit{Res18-Unet\cite{gerard2023wildfirespreadts}}. Our \textit{Res18-Unet} is identical to the \textit{Res18-Unet\cite{gerard2023wildfirespreadts}}, except for our aforementioned modifications, and obtains substantially higher AP across all candidate input features considered: a 37\% improvement on average.    

Our other two models, \textit{Res50-Unet} and \textit{SwinUnet}, also substantially outperform the existing \textit{Res18-Unet\cite{gerard2023wildfirespreadts}}. However, despite having approximately twice the number of trainable model parameters of \textit{Res18-Unet}, we find that our model outperforms the two larger models in most cases. \textit{Our Res18-Unet also obtains the highest overall AP (0.468) for the $T=1$ models when utilizing the "Multi" feature set, establishing a new SOTA on WSTS for $T=1$}.  

Several recent studies have reported that large and/or attention-based models achieve SOTA accuracy for $T=1$ wildfire spread prediction \cite{li2024wildfire, zou2023attention, shah2023wildfire, xiao2024wildfire}.  However, we find here that with simple improvements and appropriate optimization, \textit{Res18-Unet} outperforms such models. We also suspect that the more rigorous (and potentially more real-world) leave-one-year-out cross-validation adopted by the WSTS benchmark may penalize more complex models for overfitting, compared to the \textit{Res18-Unet}.     

In \cref{fig:example_predictions}, we qualitatively evaluate our \textit{Res18-Unet} and \textit{Res50-Unet} against the Res18-Unet \cite{gerard2023wildfirespreadts}. Each row corresponds to a fire event and the columns show the current fire, the next day label, and the predictions of each model. Yellow represents the fire extent, green shows correctly predicted burned areas, and red shows false positives. 
We observe that the original model tends to overpredict fire spread, leading to multiple red patches where no fire actually occurs. However, the model also underpredicts in areas where the fire spreads, capturing some, but not the full extent of the fire. On the other hand, we observe that our models make consistently more accurate predictions, with far fewer false positives, and slightly better matching green areas.

\begin{table}[]
\caption{Ablation showing the impact of the successive removal of each of our improvements on a \textit{Res18-Unet} trained on Vegetation features}
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Test AP} & \textbf{Percent Decrease} \\
        \midrule
        Res18-Unet (ours) & $0.455 \pm 0.092$ & $-$ \\
        No pretraining & $0.456 \pm 0.086$ & $-0.22$ \\
        No focal loss & $0.345 \pm 0.084$ & $24.18$ \\
        No AP as validation & $0.321 \pm 0.078$ & $29.45$ \\
        \bottomrule
    \end{tabular}
    
    \label{tab:ablation_t1}
\end{table}

\begin{table*}[h]
    \centering
    \caption{Mean test AP $\pm$ standard deviation using vegetation features only (Veg), vegetation, land cover, topography and weather (Multi) and All features, when training with 1 and 5 input days. Models with citations represent accuracy reported on our benchmark from previous publications; all other models reported are developed in this work.  Results style: \textbf{best} }
    \begin{tabular}{clcccccc}
        \toprule
        {\textbf{Fusion Level}} & \textbf{Model} & \textbf{Input days} & \textbf{Veg} & \textbf{Multi} & \textbf{All} & \textbf{\# Params} \\
        \midrule
        \multirow{4}{*}{-} & Res18-Unet\cite{gerard2023wildfirespreadts} & 1 & $0.328 \pm 0.090$ & $0.341 \pm 0.085$ & $0.341 \pm 0.086$ & 14.3M \\
                            & Res18-Unet & 1 & ${0.455 \pm 0.090}$ & $\mathbf{0.468 \pm 0.087}$ & $\mathbf{0.460 \pm 0.084}$ & 14.3M \\
                            % Res18-Unet* & 1 & ${0.423 \pm 0.079}$ & ${0.429 \pm 0.083}$ & $0.425 \pm 0.083$ & 14.3M \\
                            & Res50-Unet & 1 & $\mathbf{0.457 \pm 0.089}$ & ${0.459 \pm 0.090}$ & ${0.451 \pm 0.093}$ & 32.5M \\
                            & SwinUnet & 1 & $0.432 \pm 0.088$ & $0.437 \pm 0.082$ & $0.424 \pm 0.090$ & 27.2M \\
                            %SwinUnet* & 1 & $0.384 \pm 0.076$ & $0.395 \pm 0.077$ & $0.388 \pm 0.077$ & 27.2M \\
                            % TransUnet & 1 & $0. \pm 0.$ & $0. \pm 0.$ & $0. \pm 0.$ & 105M \\
        \cmidrule{2-7} 
        \multirow{3}{*}{Data} & Res18-Unet\cite{gerard2023wildfirespreadts} & 5 & $0.333 \pm 0.079$ & $0.344 \pm 0.076$ & ${0.325 \pm 0.108}$ & 14.4M \\
                              & Res18-Unet & 5 & $\mathbf{0.472 \pm 0.083}$ & $\mathbf{0.469 \pm 0.087}$ & $\mathbf{0.460 \pm 0.084}$ & 14.4M \\
                              &  SwinUnet & 5 & $0.447 \pm 0.087$ & $0.453 \pm 0.083$ & $0.435 \pm 0.079$ & 27.3M \\
        \cmidrule{2-7} 
        \multirow{4}{*}{Feature} 
                              % & Res18-Unet-HTAE & 5 & ${0.459 \pm 0.092}$ & ${0.460 \pm 0.087}$ & ${0.453 \pm 0.092}$ & 14.9M \\
                                % Res18-Unet* & 5 & ${0.437 \pm 0.076}$ & ${0.441 \pm 0.081}$ & ${0.426 \pm 0.086}$ & 14.4M \\
                              &  UTAE\cite{gerard2023wildfirespreadts} & 5 & $0.372 \pm 0.088$ & $0.350 \pm 0.113$ & $0.321 \pm 0.135$ & 1.1M \\
                              &  UTAE & 5 & $0.452 \pm 0.082$ & $0.459 \pm 0.088$ & $0.433 \pm 0.099$ & 1.1M \\
                              &  UTAE(Res18) & 5 & $\mathbf{0.478 \pm 0.085}$ & $\mathbf{0.477 \pm 0.089}$ & $\mathbf{0.475 \pm 0.091}$ & 14.6M \\
                              
                                %SwinUnet* & 5 & $0. \pm 0.$ & $0. \pm 0.$ & $\mathbf{0. \pm 0.}$ & 27.3M \\
                                % TransUnet & 5 & $0. \pm 0.$ & $\mathbf{0. \pm 0.}$ & $0. \pm 0.$ & 105M \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{preds_comparison.png}
    \caption{Sample predictions made by the Res18-Unet \cite{gerard2023wildfirespreadts}, our \textit{Res18-Unet}, and \textit{Res50-Unet}. The two leftmost columns show the current fire spread $y(t-1)$ and the next-day label $y(t)$. True positive pixels are colored in green, while false positives are colored in red}
    \label{fig:example_predictions}
\end{figure}



\subsection{Time-Series Input, $T=5$}

Existing models for the time-series scenario generally adopt one of two approaches: (i) a data-level fusion, or (ii) a feature-level fusion.  In data-level fusion, the features for each day, $\tilde{x}(t) \in \mathbb{R}^{H \times W \times C}$ are concatenated along the feature dimension into one input tensor, $x(t)= [ \tilde{x}(t-1) |, ... ,| \tilde{x}(t-T) ] \in \mathbb{R}^{H \times W \times CT}$, after which they can be processed in the same manner as single-day input (see \cref{sec:problem_setting} for problem notation).  Therefore, we adopt our best-performing $T=1$ models from \cref{sec:results_single_day_input}, and their hyperparameter settings, and evaluate them for data-level fusion.  As a reference, we also include the \textit{reported} results of the Res18-Unet \cite{gerard2023wildfirespreadts} when it was applied for data-level fusion.   

In this context, feature-level fusion implies that we use a shared encoder to first extract features (or embeddings) independently for each day of our input, $\tilde{z}(t) = f_{\theta_{En}}(\tilde{x}(t))$ so that we have a collection of features, $z(t)=\{ \tilde{z}(t-i) \}_{i=1}^{T}$, which are utilized as input into a subsequent model for joint processing (i.e., fusion).  The current SOTA accuracy on WSTS, both for the time-series setting, and overall, was obtained with a UTAE model \cite{garnot2021panoptic}, as reported in \cite{gerard2023wildfirespreadts}. Furthermore, the UTAE achieved superior accuracy despite having just 1.1M parameters - significantly fewer than many other models considered (e.g., the Res18-Unet has 14.3M). Therefore, we focus our modeling improvements on the UTAE from \cite{gerard2023wildfirespreadts}. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{iccv2023AuthorKit/figures/data_vs_feature_fusion_v1.PNG}
%     \caption{Illustration of (a) feature-level fusion, and (b) data-level fusion as we define it here. Further description is provided in the main text, and mathematical notation is described in \cref{sec:problem_setting}}
%     \label{fig:data_vs_feature_fusion}
% \end{figure}

\paragraph{Improvements to the UTAE} We develop two improved UTAE models, referred to as \textit{UTAE} and \textit{UTAE(Res18)}.  We discuss the design of each model next. 

\textit{Our UTAE Model.} Our \textit{UTAE} includes two major improvements over the \textit{UTAE\cite{gerard2023wildfirespreadts}}. The first improvement is to adopt all of the changes investigated for the single-day models from \cref{sec:results_single_day_input}. Pursuant to this, following previous work convention, we did a joint search over the following hyperparameters using a single fold of the WSTS benchmark: pre-training (or not), learning rates ($[1e-2,1e-3,1e-4,1e-5]$), and the type of loss (Focal, BCE, Jaccard, and Dice loss).  The second improvement is the introduction of relative positional encodings in the temporal fusion utilized by the \textit{UTAE}, to replace the day-of-year embeddings (see \cref{sec:preliminaries} for review of topic, and notation). Specifically, instead of using positional encodings as in \cite{garnot2021panoptic, gerard2023wildfirespreadts} that are based upon the absolute day of the year, where $\bar{t} \in [1,365]$, we propose to use a variant of relative positional encoding that indicates where in the daily sequence each feature is located, so that $\bar{t} \in [1,...,T]$ for a T-day input.  We hypothesize that the features (especially the fire mask) from the most recent day of the fire will be most important for making predictions, and therefore the relative position of each feature embedding in the sequence is much more important than its position in the year. Furthermore, it may be difficult for the models to infer relative positional information from absolute encodings, potentially undermining performance.   

\textit{Our UTAE(Res18) Model.} This model is obtained by making one additional improvement to our \textit{UTAE} model.  The encoder utilized in the \textit{UTAE\cite{gerard2023wildfirespreadts}} is relatively small (in terms of free parameters).  Therefore, in similar fashion to our investigation in \cref{sec:results_single_day_input}, we replace the existing UTAE's encoder with a pre-trained ResNet-18.  

\paragraph{Experimental Results} \cref{tab:results} reports the accuracy (in terms of AP) of our time-series models on the WSTS benchmark, categorized by the type of fusion performed: data-level or feature-level. Regarding data-level fusion, our \textit{Res18-Unet} and \textit{Swin-Unet} both substantially outperform the existing \textit{Res18-Unet\cite{gerard2023wildfirespreadts}} across all combinations of input features, with the \textit{Res18-Unet} providing the best overall AP (AP=0.472, on Vegetation features).  Regarding feature-level fusion, our two UTAE models (\textit{UTAE} and \textit{UTAE(Res18)}) substantially outperform the existing \textit{UTAE\cite{gerard2023wildfirespreadts}}, which is the current SOTA model on WSTS, both for time-series input ($T>1$), and overall.  Our \textit{UTAE(Res18)} model achieves the highest overall performance for each combination of input features, across both single-day and time-series models.  \textit{In particular, our UTAE(Res18) achieves the highest overall AP with the Vegetation (Veg) feature subset, leading to a new overall SOTA performance on WSTS of AP=0.478.}  

Notably, our results indicate that models receiving time-series input generally outperform those with single-day input.  This is especially apparent when comparing data-level fusion models, such as \textit{Res18-Unet} and \textit{SwinUnet}, with their single-day counterparts, since they have few architectural differences.  Most existing wildfire spread prediction in the literature has focused on the single-day input, however our findings here corroborate those from \cite{gerard2023wildfirespreadts}, and suggest that time-series modeling is a promising emerging modeling strategy.  

Our results also provide evidence that each of our modeling improvements is beneficial.  As discussed, our \textit{UTAE} included several applicable improvements discussed for our single-day models in \cref{sec:results_single_day_input}, as well as our improved temporal encodings described in this sub-section.  We therefore conducted an ablation experiment, reported in \cref{tab:ablation_pos_enc}, to demonstrate that our modified positional encodings provide additional benefits. To demonstrate that the pre-trained ResNet-18 encoder is beneficial, we can compare the performance of \textit{UTAE(Res18)} and \textit{UTAE} in \cref{tab:results}: the pre-trained ResNet-18 is the only difference between these two models.   

\begin{table}[]
    \centering
    \caption{Test AP of UTAE trained on Vegetation features using the original Absolute positional encodings from \cite{gerard2023wildfirespreadts}, versus our proposed Relative positional encodings}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Pos. Encodings} &  \textbf{Absolute} & \textbf{Relative}\\ %  & \textbf{SwinUnet} \\
    \midrule
    UTAE & $0.419 \pm 0.101$ & $\mathbf{0.452 \pm 0.082}$\\
    %Absolute & $0.419 \pm 0.101$ \\  %& $0.372 \pm 0.093$ \\
    %Relative & $\mathbf{0.452 \pm 0.082}$ \\ %& $\mathbf{0.444 \pm 0.086}$ \\
    \bottomrule
    \end{tabular}
    \label{tab:ablation_pos_enc}
\end{table}

