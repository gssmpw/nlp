\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{data_vs_feature_fusion_v1.PNG}
    \caption{Illustration of (a) feature-level fusion, and (b) data-level fusion as we define it here. Further description is provided in the main text, and mathematical notation is described in \cref{sec:problem_setting}}
    \label{fig:data_vs_feature_fusion}
\end{figure}


\section{Experimental Details}
\paragraph{SwinUnet} SwinUnet \cite{cao2022swin} is a pure transformer-based Unet-shaped model that was first proposed for medical imagery segmentation. The model replaces the convolution blocks of the Unet with Swin Transformer blocks \cite{liu2021swin}, including them throughout the encoder, bottleneck, and decoder. They also rely on patch merging and patch expansion layers in the encoder and decoder, respectively, to downsample the input features and then upsample the extracted features and produce the segmentation mask. Finally, they preserve skip connections to concatenate shallow and deep features. The SwinUnet outperformed the Unet \cite{ronneberger2015u}, ViT \cite{chen2021transunet}, Att-Unet \cite{oktay2018attention}, and TransUnet \cite{chen2021transunet} on two medical benchmark datasets. Its state-of-the-art performance, ability to learn both global and long-range dependencies, and use of the more efficient Swin blocks make it a good candidate for our task. Since the model was developed for RGB images, we modify the \texttt{in\_chans} parameter to take in the number of channels of our multi-modal inputs (Veg: 7, Multi: 33, All: 40) instead of 3.   

\paragraph{Model pre-training} To evaluate the effect of pre-training on the SwinUnet model, we load the \texttt{swin-tiny-patch4-window7-224} weights from HuggingFace onto each of our Swin blocks. These weights correspond to a Swin Transformer trained on ImageNet at 224x224 resolution. We zero-pad our input images (128x128) to match the expected input dimensions and benefit from the pre-trained weights. As for the Unet models, we follow \cite{gerard2023wildfirespreadts} and use the \texttt{segmentation\_models\_pytorch} implementation, and set \texttt{encoder\_weights} to \texttt{imagenet}, which loads a model with ImageNet pre-trained weights. Finally, the UTAE pre-training uses the PASTIS weights, released with the original paper \cite{garnot2021panoptic}. We use the 4th fold checkpoint, as it was the one with the highest performance. 


\paragraph{Training details}
To train our models, we adopt the implementations shared by \cite{gerard2023wildfirespreadts}, which can be found \hyperlink{https://github.com/SebastianGer/WildfireSpreadTS/tree/main/src}{in this GitHub repository}. The implementation relies on PyTorch Lightning for model creation, training, and testing and Weights \& Biases for model logging and metric visualization. All our models use a fixed batch size of 64, the AdamW optimizer, and a fixed optimized learning rate, as described in \cref{sec:experiments}. Also following \cite{gerard2023wildfirespreadts}, we train our models for 10,000 iterations. Increasing the number of iterations to 15,000 and 20,000 did not yield any notable increases in performance.  For all runs in \cref{tab:results}, we report the mean test AP averaged over the 12 folds, and the standard deviation. During the hyperparameter search, we only use a single data fold (id = 2), train for 50 epochs, and pick the combination that yields the highest validation AP.

\begin{table}[]
    \centering
    \caption{Proposed cross-validation splits for WSTS+}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccc}
        \textbf{Fold} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\
        \midrule
        1 & (2016, 2017, 2020, 2021) & (2018, 2019) & (2022, 2023) \\
        2 & (2018, 2019, 2022, 2023) & (2016, 2017) & (2016, 2017) \\
        3 & (2016, 2017, 2020, 2021) & (2022, 2023) & (2018, 2019) \\
        4 & (2018, 2019, 2022, 2023) & (2020, 2021) & (2016, 2017)
    \end{tabular}
    }
 
    \label{tab:my_label}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{distribution.png}
    \caption{Distribution of fire events per year }
    \label{fig:dist}
\end{figure}

\section{WSTS+ Details}

To ensure our added wildfire events are most similar to the original ones, we follow the exact same collection procedure in \cite{gerard2023wildfirespreadts}. Namely, we only collect wildfires that are larger than $10$ km$^2$, and use the GlobFire dataset \cite{artes2019global} to identify wildfire events in the United States for 2016 and 2017. However, given GlobFire's temporal availability ends at 2021, we use MTBS Burned Areas Boundaries Dataset \cite{MTBS_2017} to identify wildfires in 2022 and 2023.

The main differences between both datasets used for fire event identification are that GlobFire relies on MODIS \cite{giglio2021modis} as a data source, which has a resolution of 500 meters, while MTBS uses Landsat imagery, which has 30 meter resolution. This should not impact the dataset homogeneity, as these datasets are only used to find wildfire coordinates, therefore having higher resolution is neither advantageous nor disadvantageous. Furthermore, GlobFire returns burned area maps with start and end dates, while MTBS returns fire perimeters with start dates only. This also has little effect, since we only use the centroid coordinate for both area maps and perimeters to download the fire masks. To account for the lack of fire end dates in MTBS, we collect 30 days of samples after the start date, with an additionnal buffer of 4 days before and after the fire events, similar to \cite{gerard2023wildfirespreadts}.

