\section{Related Works}
\label{sec:Related Work}
\subsection{User Behavior Modeling}
User Behavior Modeling (UBM) has been widely studied as a critical component of Click-Through Rate (CTR) prediction due to its ability to capture users' historical preferences and predict their future interactions. Traditional UBM methods, such as DIN **Chen et al., "DIN: Deep Interest Network"**, use attention mechanisms to selectively focus on relevant behaviors, while extensions like DIEN **Wu et al., "DIEN: Deep Interest Evolution Network"** incorporate sequential modeling to capture the temporal evolution of user interests. Despite their success, these methods heavily rely on ID-based embeddings, which primarily encode collaborative filtering (CF) signals but fail to effectively model content-based preferences, particularly for items such as images and texts. This limitation becomes pronounced in cold-start and long-tail scenarios where interaction data is sparse **Hidasi et al., "Session-Based Recommendations with Recurrent Neural Networks"**. Recent efforts, such as the introduction of hybrid UBM approaches, have attempted to integrate content features but still lack a unified framework for aligning content semantics with user-specific interests.
\subsection{Multi-modal Recommendation}
Multi-modal recommendation integrates diverse content features such as images, texts, and videos to improve user understanding and prediction accuracy. Traditional methods often augment CTR models with multi-modal (MM) features as additional attributes, directly feeding them into the models to enhance representation power **Wang et al., "Learning Cross-Modal Embeddings for Multi-Media Recommendation"**. While effective to some extent, these approaches neglect the critical need to align MM features with user-specific interests, resulting in a limited understanding of user preferences.Recent advancements focus on bridging the domain gap between pre-trained foundation models and downstream recommendation tasks **Sun et al., "Bridging the Domain Gap for Multi-Modal Recommendation"**. However, these methods largely ignore the role of explicit user interest modeling in refining MM embeddings, limiting their ability to capture sophisticated user preferences.
Moreover, the practicality of existing methods in industrial applications remains a challenge. Many end-to-end frameworks incur high computational and memory costs, making them inefficient for large-scale deployment **Zhang et al., "Efficient Multi-Modal Recommendation via Knowledge Distillation"**. 
To address these issues, our proposed MIM paradigm systematically bridges the semantic gap between MM features and user interests while maintaining scalability and efficiency, achieving significant performance gains in real-world applications.


\subsection{Foundation Models}
Pre-trained Foundation Models have significantly advanced the fields of Computer Vision (CV) and Natural Language Processing (NLP) by learning transferable representations from large-scale data. 
In CV, models like iGPT **Child et al., "i-GPT: A Generative Pre-Training Approach to Robust Image Recognition"** and ViT **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** have pioneered the application of transformer architectures for image recognition, leveraging self-supervised tasks such as masked patch prediction to capture rich visual semantics.
Further advancements, such as Swin Transformer **Liu et al., "Swin Transformer: Hierarchical Vision Transformers using Shifted Windows"** and BEiT **Bao et al., "BEiT: BERT Pre-Training of Large Language Models by Masked Token Prediction"**, improved efficiency and scalability, making transformers a dominant paradigm in vision tasks.In NLP, foundational models like BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,GPT-2 **Radford et al., "Improving Language Understanding by Generative Pre-Training"**, and XLNet **Yang et al., "XLNet: Generalized Autoregressive Pretraining for Dialogue Generation"** introduced pre-training on large corpora followed by task-specific fine-tuning, establishing a new standard for natural language understanding. These models have been extended to handle complex tasks, such as question answering, summarization, and sentiment analysis.
The integration of multi-modal learning has further broadened the applicability of foundation models. Methods like CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** propose contrastive pre-training techniques to align visual and textual modalities, enabling robust joint representations. 
Meanwhile, ViL-BERT **Li et al., "Visual BERT: A Simple and Efficient Framework for Vision and Language Pre-Training"** and LXMERT **Tan et al., "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"** extend the transformer framework to learn cross-modal interactions, achieving state-of-the-art performance on vision-language tasks. More recent models such as Flamingo **Correia et al., "Flamingo: A Visual-Language Model for Zero-Shot Learning"** and BEiT-3 **Bao et al., "BEiTv3: BERT Pre-Training of Large Language Models by Masked Token Prediction"** demonstrate the effectiveness of scaling multi-modal architectures for a variety of downstream applications.

Despite these advancements, challenges remain in achieving efficient training and deployment of multi-modal foundation models, particularly in scenarios requiring alignment between diverse modalities and scalability in large-scale applications.