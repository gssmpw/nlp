@inproceedings{DBLP:cv_3,
  author       = {Mathilde Caron and
                  Hugo Touvron and
                  Ishan Misra and
                  Herv{\'{e}} J{\'{e}}gou and
                  Julien Mairal and
                  Piotr Bojanowski and
                  Armand Joulin},
  title        = {Emerging Properties in Self-Supervised Vision Transformers},
  booktitle    = {{ICCV}},
  pages        = {9630--9640},
  publisher    = {{IEEE}},
  year         = {2021}
}

@article{DBLP:cv_2,
  author       = {Zhenda Xie and
                  Yutong Lin and
                  Zhuliang Yao and
                  Zheng Zhang and
                  Qi Dai and
                  Yue Cao and
                  Han Hu},
  title        = {Self-Supervised Learning with Swin Transformers},
  journal      = {CoRR},
  volume       = {abs/2105.04553},
  year         = {2021}
}

@inproceedings{DBLP:cv_1,
  author       = {Xinlei Chen and
                  Saining Xie and
                  Kaiming He},
  title        = {An Empirical Study of Training Self-Supervised Vision Transformers},
  booktitle    = {{ICCV}},
  pages        = {9620--9629},
  publisher    = {{IEEE}},
  year         = {2021}
}

@inproceedings{2022Disentangled,
  title = {Disentangled {{Multimodal Representation Learning}} for {{Recommendation}}},
  year = {2022},
  urldate = {2023-06-14},
  keywords = {\#ref\_of\_mix\_dense},
  annotation = {TMM},
  file = {/Users/yasui/Zotero/attachment/2022/2022_Disentangled Multimodal Representation Learning for Recommendation/2022_Disentangled Multimodal Representation Learning for Recommendation.pdf}
}

@misc{brown2020Language,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-28},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \textendash{} something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#LLM,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2020/2020_Language Models are Few-Shot Learners/2020_Language Models are Few-Shot Learners.pdf}
}

@article{covington2016Deep,
  title = {Deep {{Neural Networks}} for {{YouTube Recommendations}}},
  author = {Covington, Paul and Adams, Jay and Sargin, Emre},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 10th ACM Conference on Recommender Systems},
  pages = {191--198},
  publisher = {{ACM}},
  address = {{Boston Massachusetts USA}},
  doi = {10.1145/2959100.2959190},
  urldate = {2023-08-14},
  abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.},
  isbn = {9781450340359},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense}
}

@misc{devlin2019BERT,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-08-14},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {\#CV,\#ref\_of\_mix\_dense}
}

@article{dosovitskiy2020Image,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, G. and Gelly, S. and Uszkoreit, Jakob and Houlsby, N.},
  year = {2020},
  month = oct,
  journal = {ArXiv},
  urldate = {2023-08-14},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  keywords = {/unread,\#CV,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2020/2020_An Image is Worth 16x16 Words/2020_An Image is Worth 16x16 Words.pdf}
}

@inproceedings{guo2021Embedding,
  title = {An {{Embedding Learning Framework}} for {{Numerical Features}} in {{CTR Prediction}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Guo, Huifeng and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Li, Zhenguo and He, Xiuqiang},
  year = {2021},
  month = aug,
  eprint = {2012.08986},
  primaryclass = {cs},
  pages = {2910--2918},
  doi = {10.1145/3447548.3467077},
  urldate = {2023-07-17},
  abstract = {Click-Through Rate (CTR) prediction is critical for industrial recommender systems, where most deep CTR models follow an Embedding \& Feature Interaction paradigm. However, the majority of methods focus on designing network architectures to better capture feature interactions while the feature embedding, especially for numerical features, has been overlooked. Existing approaches for numerical features are difficult to capture informative knowledge because of the low capacity or hard discretization based on the offline expertise feature engineering. In this paper, we propose a novel embedding learning framework for numerical features in CTR prediction (AutoDis) with high model capacity, end-to-end training and unique representation properties preserved. AutoDis consists of three core components: meta-embeddings, automatic discretization and aggregation. Specifically, we propose meta-embeddings for each numerical field to learn global knowledge from the perspective of field with a manageable number of parameters. Then the differentiable automatic discretization performs soft discretization and captures the correlations between the numerical features and meta-embeddings. Finally, distinctive and informative embeddings are learned via an aggregation function. Comprehensive experiments on two public and one industrial datasets are conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has been deployed onto a mainstream advertising platform, where online A/B test demonstrates the improvement over the base model by 2.1\% and 2.7\% in terms of CTR and eCPM, respectively. In addition, the code of our framework is publicly available in MindSpore1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#CTR,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2021/2021_An Embedding Learning Framework for Numerical Features in CTR Prediction/2021_An Embedding Learning Framework for Numerical Features in CTR Prediction.pdf}
}

@article{gururangan2020Don,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages = {8342--8360},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.740},
  urldate = {2023-08-14},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2020/2020_Don’t Stop Pretraining/2020_Don’t Stop Pretraining.pdf}
}

@article{han2022Modality,
  title = {Modality {{Matches Modality}}: {{Pretraining Modality-Disentangled Item Representations}} for {{Recommendation}}},
  shorttitle = {Modality {{Matches Modality}}},
  author = {Han, Tengyue and Wang, Pengfei and Niu, Shaozhang and Li, Chenliang},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM Web Conference 2022},
  pages = {2058--2066},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3512079},
  urldate = {2023-06-15},
  abstract = {Recent works have shown the effectiveness of incorporating textual and visual information to tackle the sparsity problem in recommendation scenarios. To fuse these useful heterogeneous modality information, an essential prerequisite is to align these information for modality-robust features learning and semantic understanding. Unfortunately, existing works mainly focus on tackling the learning of common knowledge across modalities, while the specific characteristics of each modality is discarded, which may inevitably degrade the recommendation performance. To this end, we propose a pretraining framework PAMD, which stands for PretrAining Modality-Disentangled Representations Model. Specifically, PAMD utilizes pretrained VGG19 and Glove to embed items' both visual and textual modalities into the continuous embedding space. Based on these primitive heterogeneous representations, a disentangled encoder is devised to automatically extract their modality-common characteristics while preserving their modality-specific characteristics. After this, a contrastive learning is further designed to guarantee the consistence and gaps between modality-disentangled representations. To the best of our knowledge, this is the first pretraining framework to learn modality-disentangled representations in recommendation scenarios. Extensive experiments on three public real-world datasets demonstrate the effectiveness of our pretraining solution against a series of state-of-the-art alternatives, which results in the significant performance gain of 4.70\%-17.44\%.},
  isbn = {9781450390965},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense}
}

@article{lei2021SEMI,
  title = {{{SEMI}}: {{A Sequential Multi-Modal Information Transfer Network}} for {{E-Commerce Micro-Video Recommendations}}},
  shorttitle = {{{SEMI}}},
  author = {Lei, Chenyi and Liu, Yong and Zhang, Lingzi and Wang, Guoxin and Tang, Haihong and Li, Houqiang and Miao, Chunyan},
  year = {2021},
  month = aug,
  journal = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages = {3161--3171},
  publisher = {{ACM}},
  address = {{Virtual Event Singapore}},
  doi = {10.1145/3447548.3467189},
  urldate = {2023-08-11},
  abstract = {The micro-video recommendation system becomes an essential part of the e-commerce platform, which helps disseminate micro-videos to potentially interested users. Existing micro-video recommendation methods only focus on users' browsing behaviors on micro-videos, but ignore their purchasing intentions in the e-commerce environment. Thus, they usually achieve unsatisfied e-commerce micro-video recommendation performances. To address this problem, we design a sequential multi-modal information transfer network (SEMI), which utilizes product-domain user behaviors to assist micro-video recommendations. SEMI effectively selects relevant items (i.e., micro-videos and products) with multi-modal features in the micro-video domain and product domain to characterize users' preferences. Moreover, we also propose a cross-domain contrastive learning (CCL) algorithm to pre-train sequence encoders for modeling users' sequential behaviors in these two domains. The objective of CCL is to maximize a lower bound of the mutual information between different domains. We have performed extensive experiments on a large-scale dataset collected from Taobao, a world-leading e-commerce platform. Experimental results show that the proposed method achieves significant improvements over state-of-the-art recommendation methods. Moreover, the proposed method has also been deployed on Taobao, and the online A/B testing results further demonstrate its practical value.},
  isbn = {9781450383325},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense}
}

@inproceedings{li2020Adversarial,
  title = {Adversarial {{Multimodal Representation Learning}} for {{Click-Through Rate Prediction}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Li, Xiang and Wang, Chao and Tan, Jiwei and Zeng, Xiaoyi and Ou, Dan and Zheng, Bo},
  year = {2020},
  month = apr,
  pages = {827--836},
  doi = {10.1145/3366423.3380163},
  urldate = {2023-06-10},
  abstract = {For better user experience and business effectiveness, Click-Through Rate (CTR) prediction has been one of the most important tasks in E-commerce. Although extensive CTR prediction models have been proposed, learning good representation of items from multimodal features is still less investigated, considering an item in E-commerce usually contains multiple heterogeneous modalities. Previous works either concatenate the multiple modality features, that is equivalent to giving a fixed importance weight to each modality; or learn dynamic weights of different modalities for different items through technique like attention mechanism. However, a problem is that there usually exists common redundant information across multiple modalities. The dynamic weights of different modalities computed by using the redundant information may not correctly reflect the different importance of each modality. To address this, we explore the complementarity and redundancy of modalities by considering modality-specific and modality-invariant features differently. We propose a novel Multimodal Adversarial Representation Network (MARN) for the CTR prediction task. A multimodal attention network first calculates the weights of multiple modalities for each item according to its modality-specific features. Then a multimodal adversarial network learns modalityinvariant representations where a double-discriminators strategy is introduced. Finally, we achieve the multimodal item representations by combining both modality-specific and modality-invariant representations. We conduct extensive experiments on both public and industrial datasets, and the proposed method consistently achieves remarkable improvements to the state-of-the-art methods. Moreover, the approach has been deployed in an operational E-commerce system and online A/B testing further demonstrates the effectiveness.},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,\#阿里},
  annotation = {WWW,阿里},
  file = {/Users/yasui/Zotero/attachment/2020/2020_Adversarial Multimodal Representation Learning for Click-Through Rate Prediction/2020_Adversarial Multimodal Representation Learning for Click-Through Rate Prediction.pdf}
}

@article{li2022Grounded,
  title = {Grounded {{Language-Image Pre-training}}},
  author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  year = {2022},
  month = jun,
  journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {10955--10965},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01069},
  urldate = {2023-08-14},
  abstract = {This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines.11Supervised baselines on COCO object detection: Faster-RCNN w/ ResNet50 (40.2) or ResNet101 (42.0), and DyHead w/ Swin-Tiny (49.7). 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be released at https://github.com/microsoft/GLIP.},
  isbn = {9781665469463},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2022/2022_Grounded Language-Image Pre-training/2022_Grounded Language-Image Pre-training.pdf}
}

@misc{lin2022Couldb,
  title = {Could {{Giant Pretrained Image Models Extract Universal Representations}}?},
  author = {Lin, Yutong and Liu, Ze and Zhang, Zheng and Hu, Han and Zheng, Nanning and Lin, Stephen and Cao, Yue},
  year = {2022},
  month = nov,
  number = {arXiv:2211.02043},
  eprint = {2211.02043},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.02043},
  urldate = {2023-08-14},
  abstract = {Frozen pretrained models have become a viable alternative to the pretraining-then-finetuning paradigm for transfer learning. However, with frozen models there are relatively few parameters available for adapting to downstream tasks, which is problematic in computer vision where tasks vary significantly in input/output format and the type of information that is of value. In this paper, we present a study of frozen pretrained models when applied to diverse and representative computer vision tasks, including object detection, semantic segmentation and video action recognition. From this empirical analysis, our work answers the questions of what pretraining task fits best with this frozen setting, how to make the frozen setting more flexible to various downstream tasks, and the effect of larger model sizes. We additionally examine the upper bound of performance using a giant frozen pretrained model with 3 billion parameters (SwinV2-G) and find that it reaches competitive performance on a varied set of major benchmarks with only one shared frozen base network: 60.0 box mAP and 52.2 mask mAP on COCO object detection test-dev, 57.6 val mIoU on ADE20K semantic segmentation, and 81.7 top-1 accuracy on Kinetics-400 action recognition. With this work, we hope to bring greater attention to this promising path of freezing pretrained image models.},
  archiveprefix = {arxiv},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2022/2022_Could Giant Pretrained Image Models Extract Universal Representations/2022_Could Giant Pretrained Image Models Extract Universal Representations2.pdf}
}

@misc{lin2023How,
  title = {How {{Can Recommender Systems Benefit}} from {{Large Language Models}}: {{A Survey}}},
  shorttitle = {How {{Can Recommender Systems Benefit}} from {{Large Language Models}}},
  author = {Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu, Weiwen and Chen, Bo and Li, Xiangyang and Zhu, Chenxu and Guo, Huifeng and Yu, Yong and Tang, Ruiming and Zhang, Weinan},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05817},
  eprint = {2306.05817},
  publisher = {{arXiv}},
  urldate = {2023-06-21},
  abstract = {Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the ``WHERE'' question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the ``HOW'' question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, and whether to involve conventional recommendation model (CRM) for inference. Detailed analysis and general development trajectories are provided for both questions, respectively. Then, we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. We also actively maintain a GitHub repository for papers and other related resources in this rising direction1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#LLM,\#ref\_of\_mix\_dense,\#RS,\#survey},
  annotation = {arXiv},
  file = {/Users/yasui/Zotero/attachment/2023/2023_How Can Recommender Systems Benefit from Large Language Models/2023_How Can Recommender Systems Benefit from Large Language Models.pdf}
}

@misc{liu2021Pretrain,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  number = {arXiv:2107.13586},
  eprint = {2107.13586},
  publisher = {{arXiv}},
  urldate = {2023-07-04},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#LLM,\#ref\_of\_mix\_dense,\#survey},
  annotation = {arXiv},
  file = {/Users/yasui/Zotero/attachment/2021/2021_Pre-train, Prompt, and Predict/2021_Pre-train, Prompt, and Predict.pdf}
}

@misc{liu2022ConvNeta,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  year = {2022},
  month = mar,
  number = {arXiv:2201.03545},
  eprint = {2201.03545},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.03545},
  urldate = {2023-08-10},
  abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  archiveprefix = {arxiv},
  keywords = {\#CV,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2022/2022_A ConvNet for the 2020s/2022_A ConvNet for the 2020s.pdf;/Users/yasui/Zotero/attachment/2022/2022_A ConvNet for the 2020s/2022_A ConvNet for the 2020s2.pdf;/Users/yasui/Zotero/storage/ZACXIG36/2201.html}
}

@article{liu2023Multimodal,
  title = {Multimodal {{Recommender Systems}}: {{A Survey}}},
  shorttitle = {Multimodal {{Recommender Systems}}},
  author = {Liu, Qidong and Hu, Jiaxi and Xiao, Yutian and Gao, Jingtong and Zhao, Xiang},
  year = {2023},
  journal = {ArXiv},
  urldate = {2023-06-15},
  abstract = {A comprehensive survey of the MRS models is given, mainly from technical views, according to three categories, i.e., Feature Interaction, Feature Enhancement and Model Optimization. The recommender system (RS) has been an integral toolkit of online services. They are equipped with various deep learning techniques to model user preference based on identifier and attribute information. With the emergence of multimedia services, such as short video, news and etc., understanding these contents while recommending becomes critical. Besides, multimodal features are also helpful in alleviating the problem of data sparsity in RS. Thus, Multimodal Recommender System (MRS) has attracted much attention from both academia and industry recently. In this paper, we will give a comprehensive survey of the MRS models, mainly from technical views. First, we conclude the general procedures and major challenges for MRS. Then, we introduce the existing MRS models according to three categories, i.e., Feature Interaction, Feature Enhancement and Model Optimization. To make it convenient for those who want to research this field, we also summarize the dataset and code resources. Finally, we discuss some promising future directions of MRS and conclude this paper.},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,\#survey,MM,RS},
  annotation = {Arxiv},
  file = {/Users/yasui/Zotero/attachment/2023/2023_Multimodal Recommender Systems/2023_Multimodal Recommender Systems.pdf}
}

@inproceedings{lyu2022OptEmbed,
  title = {{{OptEmbed}}: {{Learning Optimal Embedding Table}} for {{Click-through Rate Prediction}}},
  shorttitle = {{{OptEmbed}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Lyu, Fuyuan and Tang, Xing and Zhu, Hong and Guo, Huifeng and Zhang, Yingxue and Tang, Ruiming and Liu, Xue},
  year = {2022},
  month = oct,
  eprint = {2208.04482},
  primaryclass = {cs},
  pages = {1399--1409},
  doi = {10.1145/3511808.3557411},
  urldate = {2023-07-17},
  abstract = {Learning embedding table plays a fundamental role in Click-through rate(CTR) prediction from the view of the model performance and memory usage. The embedding table is a two-dimensional tensor, with its axes indicating the number of feature values and the embedding dimension, respectively. To learn an efficient and effective embedding table, recent works either assign various embedding dimensions for feature fields and reduce the number of embeddings respectively or mask the embedding table parameters. However, all these existing works cannot get an optimal embedding table. On the one hand, various embedding dimensions still require a large amount of memory due to the vast number of features in the dataset. On the other hand, decreasing the number of embeddings usually suffers from performance degradation, which is intolerable in CTR prediction. Finally, pruning embedding parameters will lead to a sparse embedding table, which is hard to be deployed. To this end, we propose an optimal embedding table learning framework OptEmbed, which provides a practical and general method to find an optimal embedding table for various base CTR models. Specifically, we propose pruning the redundant embeddings regarding corresponding features' importance by learnable pruning thresholds. Furthermore, we consider assigning various embedding dimensions as one single candidate architecture. To efficiently search the optimal embedding dimensions, we design a uniform embedding dimension sampling scheme to equally train all candidate architectures, meaning architecture-related parameters and learnable thresholds are trained simultaneously in one supernet. We then propose an evolution search method based on the supernet to find the optimal embedding dimensions for each field. Experiments on public datasets show that OptEmbed can learn a compact embedding table which can further improve the model performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#CTR,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2022/2022_OptEmbed/2022_OptEmbed.pdf}
}

@misc{mcauley2015Imagebased,
  title = {Image-Based {{Recommendations}} on {{Styles}} and {{Substitutes}}},
  author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
  year = {2015},
  month = jun,
  number = {arXiv:1506.04757},
  eprint = {1506.04757},
  publisher = {{arXiv}},
  urldate = {2023-06-14},
  abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense},
  annotation = {SIGIR},
  file = {/Users/yasui/Zotero/attachment/2015/2015_Image-based Recommendations on Styles and Substitutes/2015_Image-based Recommendations on Styles and Substitutes.pdf}
}

@article{moreira2021Transformers,
  title = {Transformers with Multi-Modal Features and Post-Fusion Context for e-Commerce Session-Based Recommendation},
  author = {Moreira, G. and Rabhi, Sara and Ak, R. and Kabir, Md Yasin and Oldridge, Even},
  year = {2021},
  month = jul,
  journal = {ArXiv},
  urldate = {2023-08-11},
  abstract = {Session-based recommendation is an important task for e-commerce services, where a large number of users browse anonymously or may have very distinct interests for different sessions. In this paper we present one of the winning solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce Data Challenge. Our solution was inspired by NLP techniques and consists of an ensemble of two Transformer architectures - Transformer-XL and XLNet - trained with autoregressive and autoencoding approaches. To leverage most of the rich dataset made available for the competition, we describe how we prepared multi-model features by combining tabular events with textual and image vectors. We also present a model prediction analysis to better understand the effectiveness of our architectures for the session-based recommendation.},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2021/2021_Transformers with multi-modal features and post-fusion context for e-commerce/2021_Transformers with multi-modal features and post-fusion context for e-commerce.pdf}
}


@inproceedings{sun2019How,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  booktitle = {Chinese {{Computational Linguistics}}},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  editor = {Sun, Maosong and Huang, Xuanjing and Ji, Heng and Liu, Zhiyuan and Liu, Yang},
  year = {2019},
  volume = {11856},
  pages = {194--206},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32381-3_16},
  urldate = {2023-08-14},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  isbn = {978-3-030-32380-6 978-3-030-32381-3},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2019/2019_How to Fine-Tune BERT for Text Classification/2019_How to Fine-Tune BERT for Text Classification.pdf}
}

@inproceedings{xiao2022Abstract,
  title = {From {{Abstract}} to {{Details}}: {{A Generative Multimodal Fusion Framework}} for {{Recommendation}}},
  shorttitle = {From {{Abstract}} to {{Details}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Xiao, Fangxiong and Deng, Lixi and Chen, Jingjing and Ji, Houye and Yang, Xiaorui and Ding, Zhuoye and Long, Bo},
  year = {2022},
  month = oct,
  pages = {258--267},
  publisher = {{ACM}},
  address = {{Lisboa Portugal}},
  doi = {10.1145/3503161.3548366},
  urldate = {2023-06-10},
  isbn = {978-1-4503-9203-7},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,\#京东},
  annotation = {MM,京东},
  file = {/Users/yasui/Zotero/attachment/2022/2022_From Abstract to Details/2022_From Abstract to Details.pdf}
}

@article{yuan2021One,
  title = {One {{Person}}, {{One Model}}, {{One World}}: {{Learning Continual User Representation}} without {{Forgetting}}},
  shorttitle = {One {{Person}}, {{One Model}}, {{One World}}},
  author = {Yuan, Fajie and Zhang, Guoxiao and Karatzoglou, Alexandros and Jose, Joemon and Kong, Beibei and Li, Yudong},
  year = {2021},
  month = jul,
  journal = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {696--705},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462884},
  urldate = {2023-08-11},
  abstract = {Learning user representations is a vital technique toward effective user modeling and personalized recommender systems. Existing approaches often derive an individual set of model parameters for each task by training on separate data. However, the representation of the same user potentially has some commonalities, such as preference and personality, even in different tasks. As such, these separately trained representations could be suboptimal in performance as well as inefficient in terms of parameter sharing. In this paper, we delve on research to continually learn user representations task by task, whereby new tasks are learned while using partial parameters from old ones. A new problem arises since when new tasks are trained, previously learned parameters are very likely to be modified, and as a result, an artificial neural network (ANN)-based model may lose its capacity to serve for well-trained previous tasks forever, this issue is termed catastrophic forgetting. To address this issue, we present Conure the first continual, or lifelong, user representation learner --- i.e., learning new tasks over time without forgetting old ones. Specifically, we propose iteratively removing less important weights of old tasks in a deep user representation model, motivated by the fact that neural network models are usually over-parameterized. In this way, we could learn many tasks with a single model by reusing the important weights, and modifying the less important weights to adapt to new tasks. We conduct extensive experiments on two real-world datasets with nine tasks and show that Conure largely exceeds the standard model that does not purposely preserve such old "knowledge'', and performs competitively or sometimes better than models which are trained either individually for each task or simultaneously by merging all task data.},
  isbn = {9781450380379},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2021/2021_One Person, One Model, One World/2021_One Person, One Model, One World.pdf}
}

@article{yuan2023Where,
  title = {Where to {{Go Next}} for {{Recommender Systems}}? {{ID-}} vs. {{Modality-based Recommender Models Revisited}}},
  shorttitle = {Where to {{Go Next}} for {{Recommender Systems}}?},
  author = {Yuan, Zheng and Yuan, Fajie and Song, Yu and Li, Youhua and Fu, Junchen and Yang, Fei and Pan, Yunzhu and Ni, Yongxin},
  year = {2023},
  month = jul,
  journal = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {2639--2649},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3539618.3591932},
  urldate = {2023-08-07},
  abstract = {Recommendation models that utilize unique identities (IDs for short) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT [9] and Vision Transformer [11], have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this 'old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and warm item scenarios where IDRec has a strong advantage? does this hold for items with different modality features? (ii) can the latest technical advances from other communities (i.e., natural language processing and computer vision) translate into accuracy improvement for MoRec? (iii) how to effectively utilize item modality representation, can we use it directly or do we have to adjust it with new data? (iv) are there any key challenges that MoRec needs to address in practical applications? To answer them, we conduct rigorous experiments for item recommendations with two popular modalities, i.e., text and vision. We provide the first empirical evidence that MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, even for warm item recommendation. Our results potentially imply that the dominance of IDRec in the RS field may be greatly challenged in the future. We release our code and other materials at https://github.com/westlake-repl/IDvs.MoRec.},
  isbn = {9781450394086},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,MM,RS},
  annotation = {titleTranslation: 推荐系统下一步将走向何方? ID - vs .重新审视了基于模态的推荐模型},
  file = {/Users/yasui/Zotero/attachment/2023/2023_Where to Go Next for Recommender Systems/2023_Where to Go Next for Recommender Systems.pdf}
}

@misc{yuan2020ParameterEfficient,
  title = {Parameter-{{Efficient Transfer}} from {{Sequential Behaviors}} for {{User Modeling}} and {{Recommendation}}},
  author = {Yuan, Fajie and He, Xiangnan and Karatzoglou, Alexandros and Zhang, Liguang},
  year = {2020},
  month = jun,
  number = {arXiv:2001.04253},
  eprint = {2001.04253},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-20},
  abstract = {Inductive transfer learning has had a big impact on computer vision and NLP domains but has not been used in the area of recommender systems. Even though there has been a large body of research on generating recommendations based on modeling user-item interaction sequences, few of them attempt to represent and transfer these models for serving downstream tasks where only limited data exists.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#pretrain,\#ref\_of\_mix\_dense,\#RS},
  file = {/Users/yasui/Zotero/attachment/2020/2020_Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and/2020_Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and.pdf}
}

@misc{zhou2019Resembedding,
  title = {Res-Embedding for {{Deep Learning Based Click-Through Rate Prediction Modeling}}},
  author = {Zhou, Guorui and Wu, Kailun and Bian, Weijie and Yang, Zhao and Zhu, Xiaoqiang and Gai, Kun},
  year = {2019},
  month = jun,
  number = {arXiv:1906.10304},
  eprint = {1906.10304},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-17},
  abstract = {Recently, click-through rate (CTR) prediction models have evolved from shallow methods to deep neural networks. Most deep CTR models follow an Embedding\&MLP paradigm, that is, rst mapping discrete id features, e.g. user visited items, into low dimensional vectors with an embedding module, then learn a multi-layer perception (MLP) to t the target. In this way, embedding module performs as the representative learning and plays a key role in the model performance. However, in many real-world applications, deep CTR model o en su ers from poor generalization performance, which is mostly due to the learning of embedding parameters. In this paper, we model user behavior using an interest delay model, study carefully the embedding mechanism, and obtain two important results: (i) We theoretically prove that small aggregation radius of embedding vectors of items which belongs to a same user interest domain will result in good generalization performance of deep CTR model. (ii) Following our theoretical analysis, we design a new embedding structure named res-embedding. In res-embedding module, embedding vector of each item is the sum of two components: (i) a central embedding vector calculated from an item-based interest graph (ii) a residual embedding vector with its scale to be relatively small. Empirical evaluation on several public datasets demonstrates the e ectiveness of the proposed res-embedding structure, which brings signi cant improvement on the model performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#CTR,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2019/2019_Res-embedding for Deep Learning Based Click-Through Rate Prediction Modeling/2019_Res-embedding for Deep Learning Based Click-Through Rate Prediction Modeling.pdf}
}

@misc{zhou2023Comprehensive,
  title = {A {{Comprehensive Survey}} on {{Multimodal Recommender Systems}}: {{Taxonomy}}, {{Evaluation}}, and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Multimodal Recommender Systems}}},
  author = {Zhou, Hongyu and Zhou, Xin and Zeng, Zhiwei and Zhang, Lingzi and Shen, Zhiqi},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04473},
  eprint = {2302.04473},
  publisher = {{arXiv}},
  urldate = {2023-06-19},
  abstract = {Recommendation systems have become popular and effective tools to help users discover their interesting items by modeling the user preference and item property based on implicit interactions (e.g., purchasing and clicking). Humans perceive the world by processing the modality signals (e.g., audio, text and image), which inspired researchers to build a recommender system that can understand and interpret data from different modalities. Those models could capture the hidden relations between different modalities and possibly recover the complementary information which can not be captured by a uni-modal approach and implicit interactions. The goal of this survey is to provide a comprehensive review of the recent research efforts on the multimodal recommendation. Specifically, it shows a clear pipeline with commonly used techniques in each step and classifies the models by the methods used. Additionally, a code framework has been designed that helps researchers new in this area to understand the principles and techniques, and easily runs the SOTA models. Our framework is located at: https://github.com/enoche/MMRec. CCS Concepts: \textbullet{} Information systems \textrightarrow{} Recommender systems; Multimedia and multimodal retrieval.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#MM,\#ref\_of\_mix\_dense,\#RS,\#survey},
  annotation = {Arxiv},
  file = {/Users/yasui/Zotero/attachment/2023/2023_A Comprehensive Survey on Multimodal Recommender Systems/2023_A Comprehensive Survey on Multimodal Recommender Systems.pdf}
}


@inproceedings{zhang2022towards,
  title={Towards Understanding the Overfitting Phenomenon of Deep Click-Through Rate Models},
  author={Zhang, Zhao-Yu and Sheng, Xiang-Rong and Zhang, Yujing and Jiang, Biye and Han, Shuguang and Deng, Hongbo and Zheng, Bo},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2671--2680},
  year={2022}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhu2021open,
  title={Open benchmarking for click-through rate prediction},
  author={Zhu, Jieming and Liu, Jinyang and Yang, Shuai and Zhang, Qi and He, Xiuqiang},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2759--2769},
  year={2021}
}

@inproceedings{zadeh2017tensor,
  title={Tensor Fusion Network for Multimodal Sentiment Analysis},
  author={Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={1103--1114},
  year={2017}
}

@inproceedings{sun2020circle,
  title={Circle loss: A unified perspective of pair similarity optimization},
  author={Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6398--6407},
  year={2020}
}


@article{fawcett2006introduction,
  title={An introduction to ROC analysis},
  author={Fawcett, Tom},
  journal={Pattern recognition letters},
  volume={27},
  number={8},
  pages={861--874},
  year={2006},
  publisher={Elsevier}
}



@inproceedings{song2019autoint,
  title={Autoint: Automatic feature interaction learning via self-attentive neural networks},
  author={Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  booktitle={CIKM},
  pages={1161--1170},
  year={2019}
}


@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}

@inproceedings{wang2023missrec,
  title={MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation},
  author={Wang, Jinpeng and Zeng, Ziyun and Wang, Yunxiao and Wang, Yuting and Lu, Xingyu and Li, Tianxiang and Yuan, Jun and Zhang, Rui and Zheng, Hai-Tao and Xia, Shu-Tao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6548--6557},
  year={2023}
}

@article{yan2022apg,
  title={Apg: Adaptive parameter generation network for click-through rate prediction},
  author={Yan, Bencheng and Wang, Pengjie and Zhang, Kai and Li, Feng and Deng, Hongbo and Xu, Jian and Zheng, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24740--24752},
  year={2022}
}


@article{wang2020practical,
  title={A practical incremental method to train deep ctr models},
  author={Wang, Yichao and Guo, Huifeng and Tang, Ruiming and Liu, Zhirong and He, Xiuqiang},
  journal={arXiv preprint arXiv:2009.02147},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@inproceedings{Chen2022hccm,
   title={Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling},
   url={http://dx.doi.org/10.1145/3477495.3531854},
   DOI={10.1145/3477495.3531854},
   booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   publisher={ACM},
   author={Chen, Xin and Tang, Qingtao and Hu, Ke and Xu, Yue and Qiu, Shihang and Cheng, Jia and Lei, Jun},
   year={2022},
   month=jul, collection={SIGIR ’22} 
}

@misc{ge2018image,
      title={Image Matters: Visually modeling user behaviors using Advanced Model Server}, 
      author={Tiezheng Ge and Liqin Zhao and Guorui Zhou and Keyu Chen and Shuying Liu and Huimin Yi and Zelin Hu and Bochao Liu and Peng Sun and Haoyu Liu and Pengtao Yi and Sui Huang and Zhiqiang Zhang and Xiaoqiang Zhu and Yu Zhang and Kun Gai},
      year={2018},
      eprint={1711.06505},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={1691--1703},
  year={2020},
  organization={PMLR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}


@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}


@article{sarzynska2021detecting,
  title={Detecting formal thought disorder by deep contextualized word representations},
  author={Sarzynska-Wawer, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz},
  journal={Psychiatry Research},
  volume={304},
  pages={114135},
  year={2021},
  publisher={Elsevier}
}


@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{wu2021mm,
  title={Mm-rec: multimodal news recommendation},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
  journal={arXiv preprint arXiv:2104.07407},
  year={2021}
}

@misc{han2022vlsnrvisionlinguistics,
      title={VLSNR:Vision-Linguistics Coordination Time Sequence-aware News Recommendation}, 
      author={Songhao Han and Wei Huang and Xiaotian Luan},
      year={2022},
      eprint={2210.02946},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{wu2021empowering,
  title={Empowering news recommendation with pre-trained language models},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1652--1656},
  year={2021}
}

@misc{woo2023convnext,
      title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders}, 
      author={Sanghyun Woo and Shoubhik Debnath and Ronghang Hu and Xinlei Chen and Zhuang Liu and In So Kweon and Saining Xie},
      year={2023},
      eprint={2301.00808},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{He2019MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={9726-9735},
  url={https://api.semanticscholar.org/CorpusID:207930212}
}


@article{Chen2020ASF,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05709},
  url={https://api.semanticscholar.org/CorpusID:211096730}
}


@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231591445}
}


@article{DBLP:journals/mta/LongDL24,
  author       = {Xianzhong Long and
                  Han Du and
                  Yun Li},
  title        = {Two momentum contrast in triplet for unsupervised visual representation
                  learning},
  journal      = {Multim. Tools Appl.},
  volume       = {83},
  number       = {4},
  pages        = {10467--10480},
  year         = {2024},
  url          = {https://doi.org/10.1007/s11042-023-15998-3},
  doi          = {10.1007/S11042-023-15998-3},
  timestamp    = {Wed, 24 Jan 2024 17:49:24 +0100},
  biburl       = {https://dblp.org/rec/journals/mta/LongDL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cvpr/WuXYL18,
  author       = {Zhirong Wu and
                  Yuanjun Xiong and
                  Stella X. Yu and
                  Dahua Lin},
  title        = {Unsupervised Feature Learning via Non-Parametric Instance Discrimination},
  booktitle    = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages        = {3733--3742},
  publisher    = {Computer Vision Foundation / {IEEE} Computer Society},
  year         = {2018},
  url          = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Wu\_Unsupervised\_Feature\_Learning\_CVPR\_2018\_paper.html},
  doi          = {10.1109/CVPR.2018.00393},
  timestamp    = {Fri, 24 Mar 2023 00:02:53 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/WuXYL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/ClarkLLM20,
  author       = {Kevin Clark and
                  Minh{-}Thang Luong and
                  Quoc V. Le and
                  Christopher D. Manning},
  title        = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
                  Generators},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=r1xMH1BtvB},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ClarkLLM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/MikolovSCCD13,
  author       = {Tom{\'{a}}s Mikolov and
                  Ilya Sutskever and
                  Kai Chen and
                  Gregory S. Corrado and
                  Jeffrey Dean},
  editor       = {Christopher J. C. Burges and
                  L{\'{e}}on Bottou and
                  Zoubin Ghahramani and
                  Kilian Q. Weinberger},
  title        = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle    = {Advances in Neural Information Processing Systems 26: 27th Annual
                  Conference on Neural Information Processing Systems 2013. Proceedings
                  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages        = {3111--3119},
  year         = {2013},
  url          = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/kdd/QiuCDZYDWT20,
  author       = {Jiezhong Qiu and
                  Qibin Chen and
                  Yuxiao Dong and
                  Jing Zhang and
                  Hongxia Yang and
                  Ming Ding and
                  Kuansan Wang and
                  Jie Tang},
  editor       = {Rajesh Gupta and
                  Yan Liu and
                  Jiliang Tang and
                  B. Aditya Prakash},
  title        = {{GCC:} Graph Contrastive Coding for Graph Neural Network Pre-Training},
  booktitle    = {{KDD} '20: The 26th {ACM} {SIGKDD} Conference on Knowledge Discovery
                  and Data Mining, Virtual Event, CA, USA, August 23-27, 2020},
  pages        = {1150--1160},
  publisher    = {{ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1145/3394486.3403168},
  doi          = {10.1145/3394486.3403168},
  timestamp    = {Mon, 01 May 2023 13:01:11 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/QiuCDZYDWT20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ijon/LiuMXYTPG22,
  author       = {Yue Liu and
                  Junqi Ma and
                  Yufei Xie and
                  Xuefeng Yang and
                  Xingzhen Tao and
                  Lin Peng and
                  Wei Gao},
  title        = {Contrastive predictive coding with transformer for video representation
                  learning},
  journal      = {Neurocomputing},
  volume       = {482},
  pages        = {154--162},
  year         = {2022},
  url          = {https://doi.org/10.1016/j.neucom.2021.11.031},
  doi          = {10.1016/J.NEUCOM.2021.11.031},
  timestamp    = {Wed, 11 Jan 2023 13:03:49 +0100},
  biburl       = {https://dblp.org/rec/journals/ijon/LiuMXYTPG22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{pi2020search,
  title={Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction},
  author={Pi, Qi and Zhou, Guorui and Zhang, Yujing and Wang, Zhe and Ren, Lejian and Fan, Ying and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={2685--2692},
  year={2020}
}

@inproceedings{he2019bag,
  title={Bag of tricks for image classification with convolutional neural networks},
  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={558--567},
  year={2019}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@misc{fang2023eva02,
      title={EVA-02: A Visual Representation for Neon Genesis}, 
      author={Yuxin Fang and Quan Sun and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
      year={2023},
      eprint={2303.11331},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{li2021explicit,
  title={Explicit semantic cross feature learning via pre-trained graph neural networks for CTR prediction},
  author={Li, Feng and Yan, Bencheng and Long, Qingqing and Wang, Pengjie and Lin, Wei and Xu, Jian and Zheng, Bo},
  booktitle={Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval},
  pages={2161--2165},
  year={2021}
}

@article{zhang2021deep,
  title={Deep learning for click-through rate estimation},
  author={Zhang, Weinan and Qin, Jiarui and Guo, Wei and Tang, Ruiming and He, Xiuqiang},
  journal={arXiv preprint arXiv:2104.10584},
  year={2021}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}

@inproceedings{chen2019behavior,
  title={Behavior sequence transformer for e-commerce recommendation in alibaba},
  author={Chen, Qiwei and Zhao, Huan and Li, Wei and Huang, Pipei and Ou, Wenwu},
  booktitle={Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data},
  pages={1--4},
  year={2019}
}

@article{feng2019deep,
  title={Deep session interest network for click-through rate prediction},
  author={Feng, Yufei and Lv, Fuyu and Shen, Weichen and Wang, Menghan and Sun, Fei and Zhu, Yu and Yang, Keping},
  journal={arXiv preprint arXiv:1905.06482},
  year={2019}
}

@article{chen2021end,
  title={End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. CoRR abs/2108.04468 (2021)},
  author={Chen, Qiwei and Pei, Changhua and Lv, Shanshan and Li, Chao and Ge, Junfeng and Ou, Wenwu},
  journal={arXiv preprint arXiv:2108.04468},
  year={2021}
}

@inproceedings{gu2020deep,
  title={Deep multifaceted transformers for multi-objective ranking in large-scale e-commerce recommender systems},
  author={Gu, Yulong and Ding, Zhuoye and Wang, Shuaiqiang and Zou, Lixin and Liu, Yiding and Yin, Dawei},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={2493--2500},
  year={2020}
}

@inproceedings{gao2019neural,
  title={Neural multi-task recommendation from multi-behavior data},
  author={Gao, Chen and He, Xiangnan and Gan, Dahua and Chen, Xiangning and Feng, Fuli and Li, Yong and Chua, Tat-Seng and Jin, Depeng},
  booktitle={2019 IEEE 35th international conference on data engineering (ICDE)},
  pages={1554--1557},
  year={2019},
  organization={IEEE}
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad...},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@inproceedings{chang2023twin,
  title={TWIN: TWo-stage interest network for lifelong user behavior modeling in CTR prediction at kuaishou},
  author={Chang, Jianxin and Zhang, Chenbin and Fu, Zhiyi and Zang, Xiaoxue and Guan, Lin and Lu, Jing and Hui, Yiqun and Leng, Dewei and Niu, Yanan and Song, Yang and others},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3785--3794},
  year={2023}
}

@inproceedings{mo2015image,
  title={Image feature learning for cold start problem in display advertising},
  author={Mo, Kaixiang and Liu, Bo and Xiao, Lei and Li, Yong and Jiang, Jie},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  year={2019}
}

@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  year={2022}
}

@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}