@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  year={2022}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={1691--1703},
  year={2020},
  organization={PMLR}
}

@misc{devlin2019BERT,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-08-14},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {\#CV,\#ref\_of\_mix\_dense}
}

@article{dosovitskiy2020Image,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, G. and Gelly, S. and Uszkoreit, Jakob and Houlsby, N.},
  year = {2020},
  month = oct,
  journal = {ArXiv},
  urldate = {2023-08-14},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  keywords = {/unread,\#CV,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2020/2020_An Image is Worth 16x16 Words/2020_An Image is Worth 16x16 Words.pdf}
}

@article{liu2023Multimodal,
  title = {Multimodal {{Recommender Systems}}: {{A Survey}}},
  shorttitle = {Multimodal {{Recommender Systems}}},
  author = {Liu, Qidong and Hu, Jiaxi and Xiao, Yutian and Gao, Jingtong and Zhao, Xiang},
  year = {2023},
  journal = {ArXiv},
  urldate = {2023-06-15},
  abstract = {A comprehensive survey of the MRS models is given, mainly from technical views, according to three categories, i.e., Feature Interaction, Feature Enhancement and Model Optimization. The recommender system (RS) has been an integral toolkit of online services. They are equipped with various deep learning techniques to model user preference based on identifier and attribute information. With the emergence of multimedia services, such as short video, news and etc., understanding these contents while recommending becomes critical. Besides, multimodal features are also helpful in alleviating the problem of data sparsity in RS. Thus, Multimodal Recommender System (MRS) has attracted much attention from both academia and industry recently. In this paper, we will give a comprehensive survey of the MRS models, mainly from technical views. First, we conclude the general procedures and major challenges for MRS. Then, we introduce the existing MRS models according to three categories, i.e., Feature Interaction, Feature Enhancement and Model Optimization. To make it convenient for those who want to research this field, we also summarize the dataset and code resources. Finally, we discuss some promising future directions of MRS and conclude this paper.},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,\#survey,MM,RS},
  annotation = {Arxiv},
  file = {/Users/yasui/Zotero/attachment/2023/2023_Multimodal Recommender Systems/2023_Multimodal Recommender Systems.pdf}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  year={2019}
}

@inproceedings{mo2015image,
  title={Image feature learning for cold start problem in display advertising},
  author={Mo, Kaixiang and Liu, Bo and Xiao, Lei and Li, Yong and Jiang, Jie},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@inproceedings{wang2023missrec,
  title={MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation},
  author={Wang, Jinpeng and Zeng, Ziyun and Wang, Yunxiao and Wang, Yuting and Lu, Xingyu and Li, Tianxiang and Yuan, Jun and Zhang, Rui and Zheng, Hai-Tao and Xia, Shu-Tao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6548--6557},
  year={2023}
}

@inproceedings{wu2021empowering,
  title={Empowering news recommendation with pre-trained language models},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1652--1656},
  year={2021}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{yuan2020ParameterEfficient,
  title = {Parameter-{{Efficient Transfer}} from {{Sequential Behaviors}} for {{User Modeling}} and {{Recommendation}}},
  author = {Yuan, Fajie and He, Xiangnan and Karatzoglou, Alexandros and Zhang, Liguang},
  year = {2020},
  month = jun,
  number = {arXiv:2001.04253},
  eprint = {2001.04253},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-20},
  abstract = {Inductive transfer learning has had a big impact on computer vision and NLP domains but has not been used in the area of recommender systems. Even though there has been a large body of research on generating recommendations based on modeling user-item interaction sequences, few of them attempt to represent and transfer these models for serving downstream tasks where only limited data exists.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {\#pretrain,\#ref\_of\_mix\_dense,\#RS},
  file = {/Users/yasui/Zotero/attachment/2020/2020_Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and/2020_Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and.pdf}
}

@article{yuan2021One,
  title = {One {{Person}}, {{One Model}}, {{One World}}: {{Learning Continual User Representation}} without {{Forgetting}}},
  shorttitle = {One {{Person}}, {{One Model}}, {{One World}}},
  author = {Yuan, Fajie and Zhang, Guoxiao and Karatzoglou, Alexandros and Jose, Joemon and Kong, Beibei and Li, Yudong},
  year = {2021},
  month = jul,
  journal = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {696--705},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462884},
  urldate = {2023-08-11},
  abstract = {Learning user representations is a vital technique toward effective user modeling and personalized recommender systems. Existing approaches often derive an individual set of model parameters for each task by training on separate data. However, the representation of the same user potentially has some commonalities, such as preference and personality, even in different tasks. As such, these separately trained representations could be suboptimal in performance as well as inefficient in terms of parameter sharing. In this paper, we delve on research to continually learn user representations task by task, whereby new tasks are learned while using partial parameters from old ones. A new problem arises since when new tasks are trained, previously learned parameters are very likely to be modified, and as a result, an artificial neural network (ANN)-based model may lose its capacity to serve for well-trained previous tasks forever, this issue is termed catastrophic forgetting. To address this issue, we present Conure the first continual, or lifelong, user representation learner --- i.e., learning new tasks over time without forgetting old ones. Specifically, we propose iteratively removing less important weights of old tasks in a deep user representation model, motivated by the fact that neural network models are usually over-parameterized. In this way, we could learn many tasks with a single model by reusing the important weights, and modifying the less important weights to adapt to new tasks. We conduct extensive experiments on two real-world datasets with nine tasks and show that Conure largely exceeds the standard model that does not purposely preserve such old "knowledge'', and performs competitively or sometimes better than models which are trained either individually for each task or simultaneously by merging all task data.},
  isbn = {9781450380379},
  langid = {english},
  keywords = {/unread,\#ref\_of\_mix\_dense},
  file = {/Users/yasui/Zotero/attachment/2021/2021_One Person, One Model, One World/2021_One Person, One Model, One World.pdf}
}

@article{yuan2023Where,
  title = {Where to {{Go Next}} for {{Recommender Systems}}? {{ID-}} vs. {{Modality-based Recommender Models Revisited}}},
  shorttitle = {Where to {{Go Next}} for {{Recommender Systems}}?},
  author = {Yuan, Zheng and Yuan, Fajie and Song, Yu and Li, Youhua and Fu, Junchen and Yang, Fei and Pan, Yunzhu and Ni, Yongxin},
  year = {2023},
  month = jul,
  journal = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {2639--2649},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3539618.3591932},
  urldate = {2023-08-07},
  abstract = {Recommendation models that utilize unique identities (IDs for short) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT [9] and Vision Transformer [11], have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this 'old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and warm item scenarios where IDRec has a strong advantage? does this hold for items with different modality features? (ii) can the latest technical advances from other communities (i.e., natural language processing and computer vision) translate into accuracy improvement for MoRec? (iii) how to effectively utilize item modality representation, can we use it directly or do we have to adjust it with new data? (iv) are there any key challenges that MoRec needs to address in practical applications? To answer them, we conduct rigorous experiments for item recommendations with two popular modalities, i.e., text and vision. We provide the first empirical evidence that MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, even for warm item recommendation. Our results potentially imply that the dominance of IDRec in the RS field may be greatly challenged in the future. We release our code and other materials at https://github.com/westlake-repl/IDvs.MoRec.},
  isbn = {9781450394086},
  langid = {english},
  keywords = {\#ref\_of\_mix\_dense,MM,RS},
  annotation = {titleTranslation: 推荐系统下一步将走向何方? ID - vs .重新审视了基于模态的推荐模型},
  file = {/Users/yasui/Zotero/attachment/2023/2023_Where to Go Next for Recommender Systems/2023_Where to Go Next for Recommender Systems.pdf}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{zhou2019deep,
  title={Deep interest evolution network for click-through rate prediction},
  author={Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5941--5948},
  year={2019}
}

