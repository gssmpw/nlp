\section{Related Works}
\label{sec:Related Work}
\subsection{User Behavior Modeling}
User Behavior Modeling (UBM) has been widely studied as a critical component of Click-Through Rate (CTR) prediction due to its ability to capture users' historical preferences and predict their future interactions. Traditional UBM methods, such as DIN\cite{zhou2018deep}, use attention mechanisms to selectively focus on relevant behaviors, while extensions like DIEN\cite{zhou2019deep} incorporate sequential modeling to capture the temporal evolution of user interests. Despite their success, these methods heavily rely on ID-based embeddings, which primarily encode collaborative filtering (CF) signals but fail to effectively model content-based preferences, particularly for items such as images and texts. This limitation becomes pronounced in cold-start and long-tail scenarios where interaction data is sparse\cite{yuan2021One,yuan2020ParameterEfficient}. Recent efforts, such as the introduction of hybrid UBM approaches, have attempted to integrate content features but still lack a unified framework for aligning content semantics with user-specific interests.
\subsection{Multi-modal Recommendation}
Multi-modal recommendation integrates diverse content features such as images, texts, and videos to improve user understanding and prediction accuracy. Traditional methods often augment CTR models with multi-modal (MM) features as additional attributes, directly feeding them into the models to enhance representation power \cite{mo2015image}. While effective to some extent, these approaches neglect the critical need to align MM features with user-specific interests, resulting in a limited understanding of user preferences.Recent advancements focus on bridging the domain gap between pre-trained foundation models and downstream recommendation tasks\cite{yuan2023Where, wang2023missrec, wu2021empowering, liu2023Multimodal}. However, these methods largely ignore the role of explicit user interest modeling in refining MM embeddings, limiting their ability to capture sophisticated user preferences.
Moreover, the practicality of existing methods in industrial applications remains a challenge. Many end-to-end frameworks incur high computational and memory costs, making them inefficient for large-scale deployment \cite{yuan2023Where}. 
To address these issues, our proposed MIM paradigm systematically bridges the semantic gap between MM features and user interests while maintaining scalability and efficiency, achieving significant performance gains in real-world applications.


\subsection{Foundation Models}
Pre-trained Foundation Models have significantly advanced the fields of Computer Vision (CV) and Natural Language Processing (NLP) by learning transferable representations from large-scale data. 
In CV, models like iGPT\cite{chen2020generative} and ViT\cite{dosovitskiy2020Image} have pioneered the application of transformer architectures for image recognition, leveraging self-supervised tasks such as masked patch prediction to capture rich visual semantics.
Further advancements, such as Swin Transformer\cite{liu2021Swin} and BEiT\cite{bao2021beit}, improved efficiency and scalability, making transformers a dominant paradigm in vision tasks.In NLP, foundational models like BERT\cite{devlin2019BERT},GPT-2\cite{radford2019language}, and XLNet\cite{yang2019xlnet} introduced pre-training on large corpora followed by task-specific fine-tuning, establishing a new standard for natural language understanding. These models have been extended to handle complex tasks, such as question answering, summarization, and sentiment analysis.
The integration of multi-modal learning has further broadened the applicability of foundation models. Methods like CLIP\cite{radford2021learning} propose contrastive pre-training techniques to align visual and textual modalities, enabling robust joint representations. 
Meanwhile, ViL-BERT\cite{lu2019vilbert} and LXMERT\cite{tan2019lxmert} extend the transformer framework to learn cross-modal interactions, achieving state-of-the-art performance on vision-language tasks. More recent models such as Flamingo\cite{alayrac2022flamingo} and BEiT-3\cite{wang2022image} demonstrate the effectiveness of scaling multi-modal architectures for a variety of downstream applications.

Despite these advancements, challenges remain in achieving efficient training and deployment of multi-modal foundation models, particularly in scenarios requiring alignment between diverse modalities and scalability in large-scale applications.