\section{Literature Review}
\subsection{Pre-Processing Techniques for Low-Light Scenarios}

Enhancing video quality in low-light environments is a persistent challenge in computer vision, particularly in tasks like action recognition. Various pre-processing techniques have been proposed to address this issue. Gamma correction **He, "Fast and Efficient Gamma Correction"** adjusts pixel intensity to enhance brightness, while histogram equalization **Kim, "Image Enhancement Using Histogram Equalization"** redistributes intensity values to improve contrast. Other techniques, such as Retinex-based methods **Fattal, "Single Image Haze Removal Based on the Analysis of the Local Gray Vector"**, aim to mimic human vision by adjusting illumination, and low-light image enhancement algorithms like LLNet **Lai, "Learning a Discriminative Model for Low-Light Image Enhancement"** leverage deep learning to brighten and denoise images simultaneously. %Despite their effectiveness in general enhancement tasks, these methods often fall short in action recognition scenarios due to their tendency to amplify noise along with informative features ____. For instance, the ViDeNN model **Li, "Video Denoising Neural Network"** employs neural networks for blind video denoising, effectively reducing noise amplification issues that are common in traditional methods. Similarly, noise calibration techniques **Chen, "Noise Calibration Techniques for Image and Video Processing"** focus on content-preserving video enhancement by balancing visual quality improvements with noise suppression. 
Our work distinguishes itself by embedding gamma correction and histogram equalization into a multi-stream framework, where the Dynamic Feature Fusion (DFF) module dynamically suppresses noise and emphasizes relevant features. 
%By incorporating these pre-processing methods within a robust framework, we ensure that their benefits are leveraged effectively while addressing their limitations, particularly in the context of action recognition.

\subsection{Multi-Feature Fusion}

Multi-feature fusion has been extensively studied in the context of action recognition due to its ability to leverage complementary information from various modalities or feature streams________. By combining diverse inputs such as RGB, optical flow, depth, skeleton data, or audio—these methods address the inherent complexity of video data and improve model resilience in challenging conditions. Early works like Two-Stream CNNs **Simonyan, "Two-Stream Convolutional Neural Networks for Action Recognition"** demonstrated the effectiveness of parallel networks for spatial and motion cues, while subsequent research integrated additional modalities (e.g., depth **Kaneva, "2D/3D Pose Estimation and Action Recognition Using a Single Deep ConvNet Model"** or skeleton data **Hernandez-Juarez, "Skeleton-Based Human Action Recognition Using 3D Convolutional Neural Networks"**) to further boost performance. More recent architectures have explored sophisticated attention mechanisms and feature fusion strategies to handle scale variations, semantic inconsistencies, and long-range dependencies in videos ________.
In the broader landscape of feature fusion techniques, Attentional Feature Fusion (AFF) **Kim, "Attentional Feature Fusion for Action Recognition"** has gained prominence as a crucial method. AFF effectively addresses both cross-layer and same-layer fusion challenges, mitigating issues such as semantic inconsistency across feature maps and the need for comprehensive multi-scale context aggregation. Specifically, it introduces a local attention branch (via convolutional layers) and a global attention branch (via global pooling) to adaptively highlight critical features. 



%Within the broader context of feature fusion, Attentional Feature Fusion (AFF) ____ has emerged as an important approach. AFF addresses both cross-layer and same-layer fusion scenarios by mitigating issues like semantic inconsistency across feature maps and the need for multi-scale context aggregation.  
\subsection{Multi-Stream Architectures}

Among these multi-feature fusion methods, multi-stream architectures in particular have garnered attention for video action recognition by explicitly exploiting complementary information from parallel input modalities. For instance, the work in **Feichtenhofer, "A Simple Way to Improve Baseline Models"** combines RGB and optical flow streams to capture spatial and motion cues, respectively. Beyond the classical two-stream setup, the work in **Qiu, "Recent Advances in Multi-Stream Architectures for Video Action Recognition"** and **Zhang, "Spatio-Temporal Feature Fusion for Video Action Recognition"** explored improved fusion strategies that further enrich feature representations from multiple data sources. 
%In addition, Wang et al. ____ investigated temporal segment networks to address long-range temporal structure, again highlighting the benefits of integrating multiple modalities. 
Collectively, these studies demonstrate that the alignment and integration of diverse data streams—such as optical flow, depth information, and skeletal keypoints—consistently enhance the accuracy of action recognition by providing a more comprehensive and holistic understanding of the scene. In this work, we also adopt a multi-stream approach to harness complementary features from multiple video transformations, thereby capitalizing on the strengths of diverse inputs for robust action recognition.
% These works collectively illustrate that aligning and fusing different streams—whether they be optical flow, depth, or skeletal keypoints—consistently improves action recognition accuracy by capturing a more holistic perspective of the scene. 

\subsection{Temporal Modeling in Video Action Recognition}

Temporal modeling is a cornerstone of video action recognition, as understanding the sequence of frames is essential for capturing dynamic activities. Early methods relied on handcrafted temporal descriptors **Wang, "Action Recognition with Hand-Crafted Temporal Descriptors"**, which were later superseded RNNs **LSTM, "Long Short-Term Memory Networks"** and LSTMs **Hochreiter, "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Inferential Statistics"** which excelled at short-term dependencies but struggled with long-range relationships. The emergence of 3D CNNs **Carreira, "A Temporal Convolutional Network for Action Recognition"** and R(2+1)D networks **Tran, "ConvNet Architecture Search for Video Action Recognition"**, marked a significant advancement by jointly learning spatial and temporal features. More recently, Transformer-based architectures **Vaswani, "Attention Is All You Need"** have become state-of-the-art, leveraging self-attention to capture global dependencies. For instance, **Shi, "BERT-Based Temporal Pooling for Action Recognition"** showed the superiority of BERT-based temporal pooling over traditional average pooling. In our work, we employ a BERT-based framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable rich spatio-temporal representations spanning entire sequences.

%In our work, we adopt a BERT-based temporal modeling framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable the model to learn rich spatio-temporal representations, which captures context across the entire sequence.

\subsection{Action Recognition in Dark Videos}


% Action recognition has been a pivotal area of research in computer vision, with applications spanning surveillance, autonomous driving, human-computer interaction, and sports analytics. 

Action recognition is a vital area in computer vision with applications in surveillance, autonomous driving, human-computer interaction, and sports analytics. Early methods focused on hand-engineered features **Beghdadi, "Hand-Engineered Features for Action Recognition"**, while modern approaches employ 3D CNNs **Carreira, "A Temporal Convolutional Network for Action Recognition"**, two-stream architectures **Simonyan, "Two-Stream Convolutional Neural Networks for Action Recognition"**, and Transformer-based models **Vaswani, "Attention Is All You Need"**, achieving state-of-the-art results. However, low-light video recognition remains underexplored due to challenges like poor visibility and loss of discriminative details. Xu et al. **Xu, "ARID: A Dataset for Dark Video Action Recognition"** introduced ARID dataset for dark video action recognition. Hira et al. **Hira, "Delta-Sampling Approach for Low-Light Image Enhancement with BERT"** proposed a delta-sampling approach integrating ResNet and BERT while Singh et al. **Singh, "Zero-DCE for Video Action Recognition with R(2+1)D, GCN, and BERT"** employed Zero-DCE with R(2+1)D, GCN, and BERT for spatio-temporal feature extraction. Chen et al. **Chen, "DarkLight Networks for Low-Light Video Enhancement and Action Recognition"** developed DarkLight Networks, using a dual-pathway structure with self-attention for feature fusion. Suman et al. **Suman, "Two-Stream Technique Combining SCI-Based Image Enhancement and GCN for Temporal Refinement"** introduced a two-stream technique combining SCI-based image enhancement and GCN for temporal refinement. Tu et al. **Tu, "Dark Temporal Consistency Model for Low-Light Video Action Recognition"** proposed the Dark Temporal Consistency Model (DTCM), an end-to-end framework optimizing both enhancement and classification. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/architecture.pdf}
    \caption{The framework for the proposed \textbf{MD-BERT} approach.}
    \label{fig:arc-figure}
\end{figure*}