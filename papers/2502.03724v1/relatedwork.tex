\section{Literature Review}
\subsection{Pre-Processing Techniques for Low-Light Scenarios}

Enhancing video quality in low-light environments is a persistent challenge in computer vision, particularly in tasks like action recognition. Various pre-processing techniques have been proposed to address this issue. Gamma correction \cite{poynton2012digital} adjusts pixel intensity to enhance brightness, while histogram equalization \cite{trahanias1992color} redistributes intensity values to improve contrast. Other techniques, such as Retinex-based methods \cite{rahman1996multi}, aim to mimic human vision by adjusting illumination, and low-light image enhancement algorithms like LLNet \cite{lore2017llnet} leverage deep learning to brighten and denoise images simultaneously. %Despite their effectiveness in general enhancement tasks, these methods often fall short in action recognition scenarios due to their tendency to amplify noise along with informative features \cite{chen2018learning}. For instance, the ViDeNN model \cite{claus2019videnn} employs neural networks for blind video denoising, effectively reducing noise amplification issues that are common in traditional methods. Similarly, noise calibration techniques \cite{yang2025noise} focus on content-preserving video enhancement by balancing visual quality improvements with noise suppression. 
Our work distinguishes itself by embedding gamma correction and histogram equalization into a multi-stream framework, where the Dynamic Feature Fusion (DFF) module dynamically suppresses noise and emphasizes relevant features. 
%By incorporating these pre-processing methods within a robust framework, we ensure that their benefits are leveraged effectively while addressing their limitations, particularly in the context of action recognition.

\subsection{Multi-Feature Fusion}

Multi-feature fusion has been extensively studied in the context of action recognition due to its ability to leverage complementary information from various modalities or feature streams\cite{feichtenhofer2016convolutional}\cite{carreira2017quo}. By combining diverse inputs such as RGB, optical flow, depth, skeleton data, or audio—these methods address the inherent complexity of video data and improve model resilience in challenging conditions. Early works like Two-Stream CNNs \cite{simonyan2014two} demonstrated the effectiveness of parallel networks for spatial and motion cues, while subsequent research integrated additional modalities (e.g., depth \cite{yang2012recognizing} or skeleton data \cite{li2010action}) to further boost performance. More recent architectures have explored sophisticated attention mechanisms and feature fusion strategies to handle scale variations, semantic inconsistencies, and long-range dependencies in videos \cite{zhang2020multi}\cite{alamri2019audio}.
In the broader landscape of feature fusion techniques, Attentional Feature Fusion (AFF) \cite{dai2021attentional} has gained prominence as a crucial method. AFF effectively addresses both cross-layer and same-layer fusion challenges, mitigating issues such as semantic inconsistency across feature maps and the need for comprehensive multi-scale context aggregation. Specifically, it introduces a local attention branch (via convolutional layers) and a global attention branch (via global pooling) to adaptively highlight critical features. 



%Within the broader context of feature fusion, Attentional Feature Fusion (AFF) \cite{dai2021attentional} has emerged as an important approach. AFF addresses both cross-layer and same-layer fusion scenarios by mitigating issues like semantic inconsistency across feature maps and the need for multi-scale context aggregation.  
\subsection{Multi-Stream Architectures}

Among these multi-feature fusion methods, multi-stream architectures in particular have garnered attention for video action recognition by explicitly exploiting complementary information from parallel input modalities. For instance, the work in \cite{simonyan2014two} combines RGB and optical flow streams to capture spatial and motion cues, respectively. Beyond the classical two-stream setup, the work in \cite{feichtenhofer2016convolutional} and \cite{carreira2017quo} explored improved fusion strategies that further enrich feature representations from multiple data sources. 
%In addition, Wang et al. \cite{wang2016temporal} investigated temporal segment networks to address long-range temporal structure, again highlighting the benefits of integrating multiple modalities. 
Collectively, these studies demonstrate that the alignment and integration of diverse data streams—such as optical flow, depth information, and skeletal keypoints—consistently enhance the accuracy of action recognition by providing a more comprehensive and holistic understanding of the scene. In this work, we also adopt a multi-stream approach to harness complementary features from multiple video transformations, thereby capitalizing on the strengths of diverse inputs for robust action recognition.
% These works collectively illustrate that aligning and fusing different streams—whether they be optical flow, depth, or skeletal keypoints—consistently improves action recognition accuracy by capturing a more holistic perspective of the scene. 

\subsection{Temporal Modeling in Video Action Recognition}

Temporal modeling is a cornerstone of video action recognition, as understanding the sequence of frames is essential for capturing dynamic activities. Early methods relied on handcrafted temporal descriptors \cite{wang2013action}\cite{laptev2008learning}, which were later superseded RNNs \cite{donahue2015long} and LSTMs \cite{srivastava2015unsupervised} which excelled at short-term dependencies but struggled with long-range relationships. The emergence of 3D CNNs \cite{tran2015learning} and R(2+1)D networks \cite{tran2018closer}, marked a significant advancement by jointly learning spatial and temporal features. More recently, Transformer-based architectures \cite{dosovitskiy2020image}\cite{vaswani2017attention} have become state-of-the-art, leveraging self-attention to capture global dependencies. For instance, \cite{kalfaoglu2020late} showed the superiority of BERT-based temporal pooling over traditional average pooling. In our work, we employ a BERT-based framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable rich spatio-temporal representations spanning entire sequences.

%In our work, we adopt a BERT-based temporal modeling framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable the model to learn rich spatio-temporal representations, which captures context across the entire sequence.

\subsection{Action Recognition in Dark Videos}


% Action recognition has been a pivotal area of research in computer vision, with applications spanning surveillance, autonomous driving, human-computer interaction, and sports analytics. 

Action recognition is a vital area in computer vision with applications in surveillance, autonomous driving, human-computer interaction, and sports analytics. Early methods focused on hand-engineered features \cite{laptev2008learning}, while modern approaches employ 3D CNNs \cite{tran2015learning}, two-stream architectures \cite{simonyan2014two}, and Transformer-based models \cite{dosovitskiy2020image}, achieving state-of-the-art results. However, low-light video recognition remains underexplored due to challenges like poor visibility and loss of discriminative details. Xu et al. \cite{xu2021arid} introduced ARID dataset for dark video action recognition. Hira et al. \cite{hira2021delta} proposed a delta-sampling approach integrating ResNet and BERT while Singh et al. \cite{singh2022action}  employed Zero-DCE with R(2+1)D, GCN, and BERT for spatio-temporal feature extraction. Chen et al. \cite{chen2021darklight} developed DarkLight Networks, using a dual-pathway structure with self-attention for feature fusion. Suman et al. \cite{suman2023two} introduced a two-stream technique combining SCI-based image enhancement and GCN for temporal refinement. Tu et al. \cite{tu2023dtcm} proposed the Dark Temporal Consistency Model (DTCM), an end-to-end framework optimizing both enhancement and classification. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/architecture.pdf}
    \caption{The framework for the proposed \textbf{MD-BERT} approach.}
    \label{fig:arc-figure}
\end{figure*}