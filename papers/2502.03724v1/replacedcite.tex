\section{Literature Review}
\subsection{Pre-Processing Techniques for Low-Light Scenarios}

Enhancing video quality in low-light environments is a persistent challenge in computer vision, particularly in tasks like action recognition. Various pre-processing techniques have been proposed to address this issue. Gamma correction ____ adjusts pixel intensity to enhance brightness, while histogram equalization ____ redistributes intensity values to improve contrast. Other techniques, such as Retinex-based methods ____, aim to mimic human vision by adjusting illumination, and low-light image enhancement algorithms like LLNet ____ leverage deep learning to brighten and denoise images simultaneously. %Despite their effectiveness in general enhancement tasks, these methods often fall short in action recognition scenarios due to their tendency to amplify noise along with informative features ____. For instance, the ViDeNN model ____ employs neural networks for blind video denoising, effectively reducing noise amplification issues that are common in traditional methods. Similarly, noise calibration techniques ____ focus on content-preserving video enhancement by balancing visual quality improvements with noise suppression. 
Our work distinguishes itself by embedding gamma correction and histogram equalization into a multi-stream framework, where the Dynamic Feature Fusion (DFF) module dynamically suppresses noise and emphasizes relevant features. 
%By incorporating these pre-processing methods within a robust framework, we ensure that their benefits are leveraged effectively while addressing their limitations, particularly in the context of action recognition.

\subsection{Multi-Feature Fusion}

Multi-feature fusion has been extensively studied in the context of action recognition due to its ability to leverage complementary information from various modalities or feature streams________. By combining diverse inputs such as RGB, optical flow, depth, skeleton data, or audio—these methods address the inherent complexity of video data and improve model resilience in challenging conditions. Early works like Two-Stream CNNs ____ demonstrated the effectiveness of parallel networks for spatial and motion cues, while subsequent research integrated additional modalities (e.g., depth ____ or skeleton data ____) to further boost performance. More recent architectures have explored sophisticated attention mechanisms and feature fusion strategies to handle scale variations, semantic inconsistencies, and long-range dependencies in videos ________.
In the broader landscape of feature fusion techniques, Attentional Feature Fusion (AFF) ____ has gained prominence as a crucial method. AFF effectively addresses both cross-layer and same-layer fusion challenges, mitigating issues such as semantic inconsistency across feature maps and the need for comprehensive multi-scale context aggregation. Specifically, it introduces a local attention branch (via convolutional layers) and a global attention branch (via global pooling) to adaptively highlight critical features. 



%Within the broader context of feature fusion, Attentional Feature Fusion (AFF) ____ has emerged as an important approach. AFF addresses both cross-layer and same-layer fusion scenarios by mitigating issues like semantic inconsistency across feature maps and the need for multi-scale context aggregation.  
\subsection{Multi-Stream Architectures}

Among these multi-feature fusion methods, multi-stream architectures in particular have garnered attention for video action recognition by explicitly exploiting complementary information from parallel input modalities. For instance, the work in ____ combines RGB and optical flow streams to capture spatial and motion cues, respectively. Beyond the classical two-stream setup, the work in ____ and ____ explored improved fusion strategies that further enrich feature representations from multiple data sources. 
%In addition, Wang et al. ____ investigated temporal segment networks to address long-range temporal structure, again highlighting the benefits of integrating multiple modalities. 
Collectively, these studies demonstrate that the alignment and integration of diverse data streams—such as optical flow, depth information, and skeletal keypoints—consistently enhance the accuracy of action recognition by providing a more comprehensive and holistic understanding of the scene. In this work, we also adopt a multi-stream approach to harness complementary features from multiple video transformations, thereby capitalizing on the strengths of diverse inputs for robust action recognition.
% These works collectively illustrate that aligning and fusing different streams—whether they be optical flow, depth, or skeletal keypoints—consistently improves action recognition accuracy by capturing a more holistic perspective of the scene. 

\subsection{Temporal Modeling in Video Action Recognition}

Temporal modeling is a cornerstone of video action recognition, as understanding the sequence of frames is essential for capturing dynamic activities. Early methods relied on handcrafted temporal descriptors ________, which were later superseded RNNs ____ and LSTMs ____ which excelled at short-term dependencies but struggled with long-range relationships. The emergence of 3D CNNs ____ and R(2+1)D networks ____, marked a significant advancement by jointly learning spatial and temporal features. More recently, Transformer-based architectures ________ have become state-of-the-art, leveraging self-attention to capture global dependencies. For instance, ____ showed the superiority of BERT-based temporal pooling over traditional average pooling. In our work, we employ a BERT-based framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable rich spatio-temporal representations spanning entire sequences.

%In our work, we adopt a BERT-based temporal modeling framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable the model to learn rich spatio-temporal representations, which captures context across the entire sequence.

\subsection{Action Recognition in Dark Videos}


% Action recognition has been a pivotal area of research in computer vision, with applications spanning surveillance, autonomous driving, human-computer interaction, and sports analytics. 

Action recognition is a vital area in computer vision with applications in surveillance, autonomous driving, human-computer interaction, and sports analytics. Early methods focused on hand-engineered features ____, while modern approaches employ 3D CNNs ____, two-stream architectures ____, and Transformer-based models ____, achieving state-of-the-art results. However, low-light video recognition remains underexplored due to challenges like poor visibility and loss of discriminative details. Xu et al. ____ introduced ARID dataset for dark video action recognition. Hira et al. ____ proposed a delta-sampling approach integrating ResNet and BERT while Singh et al. ____  employed Zero-DCE with R(2+1)D, GCN, and BERT for spatio-temporal feature extraction. Chen et al. ____ developed DarkLight Networks, using a dual-pathway structure with self-attention for feature fusion. Suman et al. ____ introduced a two-stream technique combining SCI-based image enhancement and GCN for temporal refinement. Tu et al. ____ proposed the Dark Temporal Consistency Model (DTCM), an end-to-end framework optimizing both enhancement and classification. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/architecture.pdf}
    \caption{The framework for the proposed \textbf{MD-BERT} approach.}
    \label{fig:arc-figure}
\end{figure*}