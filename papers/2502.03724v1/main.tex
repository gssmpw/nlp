\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{colortbl}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
    \IEEEauthorblockN{
        1\textsuperscript{st} Sharana Dharshikgan Suresh Dass\textsuperscript{1},  
        2\textsuperscript{nd} Hrishav Bakul Barua\textsuperscript{2,3},  
        3\textsuperscript{rd} Ganesh Krishnasamy\textsuperscript{1},\\  
        4\textsuperscript{th} Raveendran Paramesran\textsuperscript{1},  
        5\textsuperscript{th} Raphaël C.-W. Phan\textsuperscript{1}
    }
    
    \IEEEauthorblockA{
        \textsuperscript{1}School of Information Technology, Monash University, Malaysia \\
        \textsuperscript{2}Faculty of Information Technology, Monash University, Australia \\
        \textsuperscript{3}Robotics and Autonomous Systems Lab, TCS Research, India
    }

    \IEEEauthorblockA{
        Emails: \{sharana.sureshdass, hrishav.barua, ganesh.krishnasamy, raveendran.paramesran, raphael.phan\}@monash.edu
    }
}
\maketitle

\begin{abstract}

Action recognition in dark, low-light (under-exposed) or noisy videos is a challenging task due to visibility degradation, which can hinder critical spatiotemporal details. This paper proposes MD-BERT, a novel multi-stream approach that integrates complementary pre-processing techniques such as gamma correction and histogram equalization alongside raw dark frames to address these challenges. We introduce the Dynamic Feature Fusion (DFF) module, extending existing attentional fusion methods to a three-stream setting, thereby capturing fine-grained and global contextual information across different brightness and contrast enhancements. The fused spatiotemporal features are then processed by a BERT-based temporal model, which leverages its bidirectional self-attention to effectively capture long-range dependencies and contextual relationships across frames. Extensive experiments on the ARID V1.0 and ARID V1.5 dark video datasets show that MD-BERT outperforms existing methods, establishing a new state-of-the-art performance. Ablation studies further highlight the individual contributions of each input stream and the effectiveness of the proposed DFF and BERT modules. The official website of this work is available at: https://github.com/HrishavBakulBarua/DarkBERT

\end{abstract}

\begin{IEEEkeywords}
Video action recognition, dark video, image enhancement, BERT, self-attention, feature fusion, Convolutional Neural Networks. 
\end{IEEEkeywords}

\section{Introduction}

Action recognition in videos plays a vital role in applications such as surveillance, human-computer interaction, and automatic video tracking \cite{dass2025actnetformer}\cite{dass2023schatten}. Although action recognition has been widely explored among the vision community, recognizing actions in low-light conditions, such as night-time under-exposed surveillance or poorly lit environments, remains an under-explored area with significant challenges. These conditions often lead to reduced visibility, amplified noise, and loss of spatial and temporal details, significantly hindering model performance.

Traditional action recognition methods, which rely on handcrafted features or shallow architectures, struggle in such challenging scenarios. While deep learning models like R(2+1)D \cite{tran2018closer} and I3D \cite{carreira2017quo} have advanced the field through spatiotemporal feature learning, they typically assume well-lit environments, limiting their effectiveness in dark video scenarios. The challenges of action recognition in dark videos have prompted researchers to explore various strategies to address the limitations posed by low-light environments.

For example, Chen et al. \cite{chen2021darklight} proposed a dual-stream approach that combines raw dark frames and gamma-corrected frames to improve feature representation in low-light settings. While effective, their approach relies on static concatenation of features, potentially underutilizing complementary information. Similarly, Singh et al. \cite{singh2022action} introduced an Image Enhancement Module (IEM) with Zero-DCE to enhance dark frames and coupled it with advanced temporal modeling techniques, demonstrating significant improvements. However, these methods often involve fixed preprocessing steps or lack the flexibility to adaptively fuse features.

Building on these advancements, we propose \textbf{MD-BERT}, a novel framework for action recognition in dark videos. MD-BERT introduces a multi-stream architecture that integrates three complementary representations: raw dark frames, gamma-enhanced frames, and histogram-equalized frames. Each input stream captures unique characteristics: raw frames preserve structural information, gamma-enhanced frames highlight brightness, and histogram-equalized frames enhance contrast.

To effectively combine diverse features, we draw inspiration from the Attentional Feature Fusion (AFF) \cite{dai2021attentional} module, which provides a unified approach to feature fusion in various scenarios (e.g., same-layer, short skip, and long skip connections). AFF dynamically fuses two input feature maps by computing attention weights using a Multi-Scale Channel Attention Module (MS-CAM), balancing local and global contexts. However, AFF was originally proposed for 2D image-based tasks and is limited to scenarios with two input streams (e.g., low- vs. high-level features), making it less adaptable for integrating multiple complementary inputs.

Building on this foundation, we propose Dynamic Feature Fusion (DFF), which extends AFF from a two-input, 2D fusion approach to a three-input, 3D video scenario. Specifically, DFF handles dark, gamma-enhanced, and histogram-equalized frames by leveraging 3D attention to aggregate these distinct streams. This setup enables us to capture a more comprehensive range of low-light enhancements and ensures that each stream’s complementary information is effectively highlighted (local attention) or globally weighted (global attention). 
%By integrating AFF’s unified attention approach into a multi-stream, 3D pipeline, DFF effectively addresses semantic inconsistencies and multi-scale challenges in spatiotemporal data. 
Ultimately, we fuse multiple features from our three streams into a single, enriched representation, facilitating robust action recognition under low-light conditions, complex backgrounds, or significant variations in scene dynamics.

For temporal modeling, we employ a BERT-based architecture \cite{kalfaoglu2020late} with bidirectional attention to capture long-range spatiotemporal dependencies. By incorporating positional encoding and multi-head attention, the framework learns comprehensive action representations, further enhancing recognition accuracy. By integrating complementary streams and leveraging dynamic feature fusion, MD-BERT establishes a new state-of-the-art for action recognition in low-light environments, addressing the challenges of visibility, noise, and feature integration.

The main contributions of this work are fourfold and summarized as follows:
\begin{itemize}
    \item We propose a \textbf{multi-stream framework} that combines dark, gamma-enhanced, and histogram-equalized inputs to address the challenges of low-light action recognition.
    \item We introduce \textbf{Dynamic Feature Fusion (DFF)}, a novel attention-based module that adaptively fuses features from multiple streams by balancing local and global contextual information.
    \item We leverage a \textbf{BERT-based temporal modeling} approach to capture long-range dependencies across frames, ensuring robust recognition in dark video scenarios.
    \item We conduct extensive experiments on the ARID V1.0 and ARID V1.5 datasets and demonstrate that our approach outperforms state-of-the-art methods for action recognition in low-light conditions.
\end{itemize}

\section{Literature Review}

\subsection{Pre-Processing Techniques for Low-Light Scenarios}

Enhancing video quality in low-light environments is a persistent challenge in computer vision, particularly in tasks like action recognition. Various pre-processing techniques have been proposed to address this issue. Gamma correction \cite{poynton2012digital} adjusts pixel intensity to enhance brightness, while histogram equalization \cite{trahanias1992color} redistributes intensity values to improve contrast. Other techniques, such as Retinex-based methods \cite{rahman1996multi}, aim to mimic human vision by adjusting illumination, and low-light image enhancement algorithms like LLNet \cite{lore2017llnet} leverage deep learning to brighten and denoise images simultaneously. %Despite their effectiveness in general enhancement tasks, these methods often fall short in action recognition scenarios due to their tendency to amplify noise along with informative features \cite{chen2018learning}. For instance, the ViDeNN model \cite{claus2019videnn} employs neural networks for blind video denoising, effectively reducing noise amplification issues that are common in traditional methods. Similarly, noise calibration techniques \cite{yang2025noise} focus on content-preserving video enhancement by balancing visual quality improvements with noise suppression. 
Our work distinguishes itself by embedding gamma correction and histogram equalization into a multi-stream framework, where the Dynamic Feature Fusion (DFF) module dynamically suppresses noise and emphasizes relevant features. 
%By incorporating these pre-processing methods within a robust framework, we ensure that their benefits are leveraged effectively while addressing their limitations, particularly in the context of action recognition.

\subsection{Multi-Feature Fusion}

Multi-feature fusion has been extensively studied in the context of action recognition due to its ability to leverage complementary information from various modalities or feature streams\cite{feichtenhofer2016convolutional}\cite{carreira2017quo}. By combining diverse inputs such as RGB, optical flow, depth, skeleton data, or audio—these methods address the inherent complexity of video data and improve model resilience in challenging conditions. Early works like Two-Stream CNNs \cite{simonyan2014two} demonstrated the effectiveness of parallel networks for spatial and motion cues, while subsequent research integrated additional modalities (e.g., depth \cite{yang2012recognizing} or skeleton data \cite{li2010action}) to further boost performance. More recent architectures have explored sophisticated attention mechanisms and feature fusion strategies to handle scale variations, semantic inconsistencies, and long-range dependencies in videos \cite{zhang2020multi}\cite{alamri2019audio}.
In the broader landscape of feature fusion techniques, Attentional Feature Fusion (AFF) \cite{dai2021attentional} has gained prominence as a crucial method. AFF effectively addresses both cross-layer and same-layer fusion challenges, mitigating issues such as semantic inconsistency across feature maps and the need for comprehensive multi-scale context aggregation. Specifically, it introduces a local attention branch (via convolutional layers) and a global attention branch (via global pooling) to adaptively highlight critical features. 



%Within the broader context of feature fusion, Attentional Feature Fusion (AFF) \cite{dai2021attentional} has emerged as an important approach. AFF addresses both cross-layer and same-layer fusion scenarios by mitigating issues like semantic inconsistency across feature maps and the need for multi-scale context aggregation.  
\subsection{Multi-Stream Architectures}

Among these multi-feature fusion methods, multi-stream architectures in particular have garnered attention for video action recognition by explicitly exploiting complementary information from parallel input modalities. For instance, the work in \cite{simonyan2014two} combines RGB and optical flow streams to capture spatial and motion cues, respectively. Beyond the classical two-stream setup, the work in \cite{feichtenhofer2016convolutional} and \cite{carreira2017quo} explored improved fusion strategies that further enrich feature representations from multiple data sources. 
%In addition, Wang et al. \cite{wang2016temporal} investigated temporal segment networks to address long-range temporal structure, again highlighting the benefits of integrating multiple modalities. 
Collectively, these studies demonstrate that the alignment and integration of diverse data streams—such as optical flow, depth information, and skeletal keypoints—consistently enhance the accuracy of action recognition by providing a more comprehensive and holistic understanding of the scene. In this work, we also adopt a multi-stream approach to harness complementary features from multiple video transformations, thereby capitalizing on the strengths of diverse inputs for robust action recognition.
% These works collectively illustrate that aligning and fusing different streams—whether they be optical flow, depth, or skeletal keypoints—consistently improves action recognition accuracy by capturing a more holistic perspective of the scene. 

\subsection{Temporal Modeling in Video Action Recognition}

Temporal modeling is a cornerstone of video action recognition, as understanding the sequence of frames is essential for capturing dynamic activities. Early methods relied on handcrafted temporal descriptors \cite{wang2013action}\cite{laptev2008learning}, which were later superseded RNNs \cite{donahue2015long} and LSTMs \cite{srivastava2015unsupervised} which excelled at short-term dependencies but struggled with long-range relationships. The emergence of 3D CNNs \cite{tran2015learning} and R(2+1)D networks \cite{tran2018closer}, marked a significant advancement by jointly learning spatial and temporal features. More recently, Transformer-based architectures \cite{dosovitskiy2020image}\cite{vaswani2017attention} have become state-of-the-art, leveraging self-attention to capture global dependencies. For instance, \cite{kalfaoglu2020late} showed the superiority of BERT-based temporal pooling over traditional average pooling. In our work, we employ a BERT-based framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable rich spatio-temporal representations spanning entire sequences.

%In our work, we adopt a BERT-based temporal modeling framework to capture long-range dependencies in dark video scenarios. By feeding fused features from the DFF module into BERT, we enable the model to learn rich spatio-temporal representations, which captures context across the entire sequence.

\subsection{Action Recognition in Dark Videos}


% Action recognition has been a pivotal area of research in computer vision, with applications spanning surveillance, autonomous driving, human-computer interaction, and sports analytics. 

Action recognition is a vital area in computer vision with applications in surveillance, autonomous driving, human-computer interaction, and sports analytics. Early methods focused on hand-engineered features \cite{laptev2008learning}, while modern approaches employ 3D CNNs \cite{tran2015learning}, two-stream architectures \cite{simonyan2014two}, and Transformer-based models \cite{dosovitskiy2020image}, achieving state-of-the-art results. However, low-light video recognition remains underexplored due to challenges like poor visibility and loss of discriminative details. Xu et al. \cite{xu2021arid} introduced ARID dataset for dark video action recognition. Hira et al. \cite{hira2021delta} proposed a delta-sampling approach integrating ResNet and BERT while Singh et al. \cite{singh2022action}  employed Zero-DCE with R(2+1)D, GCN, and BERT for spatio-temporal feature extraction. Chen et al. \cite{chen2021darklight} developed DarkLight Networks, using a dual-pathway structure with self-attention for feature fusion. Suman et al. \cite{suman2023two} introduced a two-stream technique combining SCI-based image enhancement and GCN for temporal refinement. Tu et al. \cite{tu2023dtcm} proposed the Dark Temporal Consistency Model (DTCM), an end-to-end framework optimizing both enhancement and classification. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/architecture.pdf}
    \caption{The framework for the proposed \textbf{MD-BERT} approach.}
    \label{fig:arc-figure}
\end{figure*}

\section{Methodology}

\subsection{Overview}

This section presents MD-BERT, a methodology for action recognition in dark videos. The approach addresses low-light challenges through a multi-stage framework comprising input preprocessing, feature extraction, and dynamic feature fusion (DFF), inspired by Attentional Feature Fusion (AFF) \cite{dai2021attentional}—followed by BERT-based temporal modeling. The method begins with preprocessing, extracting three complementary input streams: dark, gamma-enhanced, and histogram-equalized frames. Unlike Chen et al. \cite{chen2021darklight}, which used only dark and gamma-enhanced images, we introduce histogram equalization to enhance contrast, improving robustness in action recognition. Each stream is processed independently using the R(2+1)D network \cite{tran2018closer} to extract spatial and temporal features. The DFF module then integrates these features, balancing local and global attention to retain fine-grained and high-level contextual details. Finally, the fused feature map is passed into a BERT-based architecture, where bidirectional attention captures long-range temporal
dependencies within the fused feature sequence, enhancing action recognition in dark videos. The overall framework is illustrated in Fig. \ref{fig:arc-figure}. The following sections detail each component's role within the pipeline.

% The methodology addresses the challenges posed by low-light environments by employing a multi-step process that includes input preprocessing, advanced feature extraction, and dynamic feature fusion through Dynamic Feature Fusion (DFF), inspired by Attentional Feature Fusion (AFF) \cite{dai2021attentional}, followed by temporal modeling with a BERT-based architecture.

%The method begins with preprocessing to extract three complementary input streams: dark, gamma-enhanced, and histogram-equalized frames, which emphasize different visual characteristics of the scene. Unlike prior work such as Chen et al. (2020), which utilized only two input streams (dark and gamma-enhanced images), this study takes inspiration from their approach and extends it by introducing an additional input stream—histogram-equalized frames—to provide richer contrast information. This augmentation builds on their methodology while addressing limitations in capturing contrast details, thereby enhancing the robustness of action recognition.

%These streams are then processed independently using the R(2 + 1)D network \cite{tran2018closer} to extract spatial and temporal features. To integrate the diverse information from the three streams, the DFF module is utilized. DFF dynamically fuses features by balancing local and global attention, ensuring that both fine-grained and high-level contextual details are retained.

%Finally, the fused feature map is passed into a BERT-based architecture for temporal modeling and classification. The bidirectional attention mechanism of BERT enables the model to capture sequential dependencies and context across frames, resulting in a comprehensive understanding of actions in dark video scenarios. The overall framework is illustrated in Fig. \ref{fig:arc-figure}. The subsequent sections provide a detailed examination of each component of the proposed methodology, emphasizing its specific role and contribution to the overall pipeline.


% The following sections detail each component of the methodology and highlight its contribution to the overall pipeline.

\subsection{Input Processing and Feature Extraction}

The model processes individual video frames through three transformations:

\begin{itemize}
    \item \textbf{Dark Image:} Retains the original low-light characteristics. 
    \item \textbf{Gamma-Enhanced Image:} Enhances visibility using gamma correction:
    \begin{equation}\label{equ.gamma_correction}
    GIC(p) = p_{\text{max}} \left(\frac{p}{p_{\text{max}}}\right)^{\frac{1}{\gamma}}
    \end{equation}
    Where, \( p \): Input pixel intensity. \( p_{\text{max}} \): Maximum pixel intensity (e.g., 255). \( \gamma \): Parameter controlling luminance adjustment.

    \item \textbf{Histogram-Equalized Image:} Improves contrast via histogram equalization:
    \begin{equation}\label{equ.histo}
    HE(I) = \frac{CDF(I) - CDF_{\text{min}}}{(M \times N) - CDF_{\text{min}}} \cdot (L - 1)
    \end{equation}
    Where, \( CDF(I) \): Cumulative distribution function for intensity \( I \). \( CDF_{\text{min}} \): Minimum CDF value. \( M \times N \): Total number of pixels. \( L \): Number of intensity levels (e.g., 256 for 8-bit images).
    \end{itemize}

This results in a multi-stream enhanced mono-modal setup, where all inputs originate from the same video but emphasize different characteristics: dark input retains original low-light details ($F_{\text{dark}}=f(I_{\text{dark}})$), gamma-corrected input enhances brightness 
($F_{\text{gamma}}=f(I_{\text{GIC}})$) and histogram-equalised input enhances contrast ($F_{\text{he}}=f(I_{\text{HE}})$). 

%This approach results in a multi-stream enhanced mono-modal setup. Although all three inputs are derived from the same video, they represent different transformations that emphasize diverse visual characteristics: The \textbf{dark input} retains the original low-light information, \textit{i.e}, $F_{\text{Dark}} = f(I_{\text{Dark}})$. The \textbf{gamma-corrected input} highlights brightness-enhanced details, \textit{i.e}, $F_{\text{Gamma}} = f(I_{\text{GIC}})$. The \textbf{histogram-equalized input} focuses on high-contrast features, \textit{i.e}, $F_{\text{HE}} = f(I_{\text{HE}})$.

   


\begin{figure}[ht]
    \centering
    \includegraphics[width=8.5cm,height=2.5cm]{Figures/histo.png}
    \caption{Average histograms for (a) Dark frames, (b) Gamma-enhanced frames (\( \gamma = 2.5\)), and (c) Histogram-equalized frames. The dark frame histogram (left) illustrates the dominance of low-intensity values, reflecting the inherent challenges of low-light videos. The gamma-enhanced histogram (middle) shifts pixel intensities towards higher brightness, effectively enhancing visibility in dark regions. The histogram-equalized frame (right) shows a broader distribution of intensity values, improving contrast by redistributing intensities.}
    \label{fig:histograms}
\end{figure}

To better understand the impact of these preprocessing techniques, we analyzed the pixel intensity distribution across the dataset for the dark, gamma-enhanced, and histogram-equalized frames. Fig. \ref{fig:histograms} shows the average histograms of the respective transformations aggregated across 5 different classes of images shown in Fig. \ref{fig:wide-figure}. The histograms confirm that each transformation captures unique visual characteristics, collectively enhancing feature extraction in low-light conditions. Unlike multimodal setups that fuse different data modalities (e.g., text and video), our multi-stream approach remains within the visual modality, leveraging complementary representations for more robust action recognition.

%These histograms confirm that the three preprocessing steps complement each other, capturing distinct visual characteristics necessary for action recognition in low-light environments.By leveraging these complementary representations, the methodology captures a richer and more robust set of features for action recognition. This multi-stream approach is distinct from multimodal setups, which typically combine different data modalities (e.g., text and video), as all inputs here are derived from the same video and remain within the visual modality.

\begin{figure*}[ht]
    \centering    \includegraphics[width=14cm,height=5cm]{Figures/DFF.pdf}
    \caption{The proposed DFF architecture.}
    \label{fig:DFF}
\end{figure*}

\subsection{Dynamic Feature Fusion (DFF)}

\subsubsection{Overview and Motivation}
The Dynamic Feature Fusion (DFF) module extends the Attentional Feature Fusion (AFF) \cite{dai2021attentional} to address low-light video action recognition. By merging multiple enhanced streams (e.g., dark, gamma-corrected, histogram-equalized), DFF adaptively re-weights each stream's contribution via local and global attention. In particular, DFF provides two key benefits:

\paragraph{Enhanced Information Fusion and Utilization}
DFF integrates complementary perspectives from multiple streams by emphasizing salient features and minimizing redundancy. The attention-weighted streams are efficiently combined into a compact yet rich representation that captures essential contributions from all inputs.

\paragraph{Dynamic Robustness to Noise and Adaptability}
DFF’s local-global attention suppresses irrelevant or noisy features while dynamically adjusting to varying lighting and contrast, ensuring the most relevant information consistently dominates.

\subsubsection{Feature Integration}
Each stream is processed via local (\textsc{bn} + \textsc{ReLU} + \textsc{Conv}) and global (\textsc{pool} + \textsc{bn} + \textsc{ReLU} + \textsc{Conv}) pathways. Local captures spatial details; global encodes holistic context. Their outputs are summed to form a combined feature map.

\subsubsection{Attention Mechanism}
Local and global maps are added, then passed through a convolution and sigmoid to obtain attention weights for each stream. After normalization, these weights are used to fuse the original inputs (via weighted summation), ensuring that each stream's unique information is effectively combined.
\subsubsection{Feature Fusion}

The final fused feature map is computed using the attention weights to balance the contributions from different streams. This ensures that the most relevant features are emphasized while less important ones are suppressed.

Let the three input features be:
\(\mathbf{F}_{\mathrm{dark}}, \mathbf{F}_{\mathrm{gamma}}, \mathbf{F}_{\mathrm{he}} \in \mathbb{R}^{C \times T \times H \times W}\).
We detail the fusion steps as follows:

\begin{enumerate}
    \item \textbf{Combine Input Features:}
    \begin{equation}
    \mathbf{F}_{\mathrm{comb}} 
    \;=\;
    \mathbf{F}_{\mathrm{dark}}
    \;+\;
    \mathbf{F}_{\mathrm{gamma}}
    \;+\;
    \mathbf{F}_{\mathrm{he}},
    \label{eq:comb}
    \end{equation}

    \item \textbf{Local Feature Extraction:}
    \begin{equation}
        \mathbf{L}
        \;=\;
        \mathrm{Conv}_{C}\Bigl(\mathrm{ReLU}\bigl(\mathrm{BN}(\mathbf{F}_{\mathrm{comb}}) \bigr)\Bigr)
        \;\\
        \in 
        \mathbb{R}^{C \times T \times H \times W}.
        \label{eq:local}
    \end{equation}

    \item \textbf{Global Feature Extraction:}
     \begin{equation}
        \begin{aligned}
    \mathbf{G} &= \mathrm{Conv_C} \Bigl( \mathrm{ReLU} \bigl( \mathrm{BN} (\mathrm{GAP} (\mathbf{F}_{\mathrm{comb}})) \bigr) \Bigr) \\
    &\in \mathbb{R}^{C \times T \times H \times W}.
        \end{aligned}
        \label{eq:global}
    \end{equation}


    \item \textbf{Combine Local and Global Features:}
    \begin{equation}
        \mathbf{C}
        \;=\;
        \mathbf{L} + \mathbf{G}
        \;\in \mathbb{R}^{C \times T \times H \times W}.
        \label{eq:combine-local-global}
    \end{equation}

    \item \textbf{Attention Map Generation:}\\
    Produce a three-channel attention map (one channel per input feature) via BN, ReLU, and a \(3\)-channel convolution:
    \begin{equation}
        \mathbf{A}
        \;=\;
        \mathrm{Conv}_{3}\Bigl(\mathrm{ReLU}\bigl(\mathrm{BN}(\mathbf{C})\bigr)\Bigr)
        \;\in \mathbb{R}^{3 \times T \times H\times W}.
        \label{eq:attention-map}
    \end{equation}
    Then apply the sigmoid function to get unnormalized attention weights:
    \begin{equation}
        \mathbf{W} \;=\; \sigma(\mathbf{A})
        \;\in \mathbb{R}^{3 \times T \times H \times W}.
        \label{eq:attention-weights}
    \end{equation}

    \item \textbf{Weight Normalization:}\\
    Normalize along the three input-feature dimension:
    \begin{equation}
        \mathbf{W}_{\text{sum}} \;=\; \sum_{k=1}^{3} \mathbf{W}_{k} \;+\; 10^{-6},
        \quad
        \widetilde{\mathbf{W}}_{k} \;=\; \frac{\mathbf{W}_{k}}{\mathbf{W}_{\text{sum}}},
        \label{eq:weight-norm}
    \end{equation}
    for \(k \in \{1,2,3\}\). Thus, 
    \(\widetilde{\mathbf{W}} \in \mathbb{R}^{3 \times T \times H \times W}\).

    \item \textbf{Feature Fusion:}\\
    Finally, fuse the three original features using the normalized attention weights:
    \begin{equation}
    \begin{aligned}
    \mathbf{F}_{\mathrm{fused}}
    &= \widetilde{\mathbf{W}}_{1} \odot \mathbf{F}_{\mathrm{dark}}
   \;+\; \widetilde{\mathbf{W}}_{2} \odot \mathbf{F}_{\mathrm{gamma}} \\
    &\quad + \widetilde{\mathbf{W}}_{3} \odot \mathbf{F}_{\mathrm{he}}
   \;\in\; \mathbb{R}^{C \times T \times H \times W}.
    \end{aligned}
    \label{eq:fusion}
    \end{equation}

    where \(\odot\) denotes element-wise multiplication.
\end{enumerate}


%\subsubsection{Importance of DFF}

%The Dynamic Feature Fusion (DFF) module is central to the success of this proposed framework. This approach builds upon the concept of Attentional Feature Fusion (AFF) \cite{dai2021attentional}, which has been adapted and extended to address the specific challenges associated with action recognition in low-light video scenarios (as listed below?).
% It builds upon the concept of Attentional Feature Fusion (AFF) \cite{dai2021attentional} but has been adapted and extended to better suit the challenges of action recognition in dark videos.

%\begin{itemize}
    %\item \textcolor{red}{These below points are disconnected from the above paragraph...}
    %\item \textbf{Heterogeneous Information Fusion:} DFF integrates complementary perspectives from dark, gamma-enhanced, and histogram-equalized images. Each input emphasizes distinct aspects of the video, and the module dynamically balances their contributions through attention weights.
    %\item \textbf{Robustness to Noise and Feature Emphasis:} While enhancements like gamma correction and histogram equalization may amplify noise, DFF employs local and global attention mechanisms to suppress irrelevant features and emphasize informative ones, ensuring robust and noise-resilient recognition. 
    %\item \textbf{Adaptability and Dynamic Weighting:}
    %By dynamically adjusting attention weights based on input characteristics, DFF adapts to variations in lighting, contrast, and noise levels. This ensures the most relevant features dominate while redundant information is minimized, maintaining consistent performance across diverse scenarios.
    %%\item \textbf{Efficient Utilization of Information:}
    %Balancing contributions from all streams, DFF effectively utilizes complementary information, resulting in richer and more robust feature representations for action recognition.
%\end{itemize}

%\subsubsection{Feature Integration}

%The feature maps from the three input streams are processed independently through local and global attention mechanisms. These mechanisms compute the importance of each input stream, emphasizing distinct characteristics while integrating complementary information. The weighted feature maps are then fused by summing them to create a unified representation, preserving the contributions of all streams.

%\subsubsection{Attention Mechanism}

%DFF applies local and global attention mechanisms independently to each input stream. These mechanisms work together to capture spatially fine-grained and globally holistic information:

%\begin{itemize}
    %\item \textbf{Local Attention:} Focuses on spatial details through convolutional operations. This operation enables the module to capture patterns such as edges and textures that are crucial for identifying actions.
    %\item \textbf{Global Attention:} Captures a more comprehensive view by applying adaptive average pooling, followed by a fully connected layer for feature transformation. Both local and global features are combined and passed through a sigmoid activation to generate attention weights.
%\end{itemize}

%\textbf{Step 1: Attention Weights for Individual Streams}

%The attention weights for each input stream (dark, gamma-enhanced, histogram-equalized) are computed by combining local and global attention outputs. For each stream:

%\begin{multline}\label{equ.dff_weights_dark}
%M_{\text{Dark}} = \sigma \left( W_2^{\text{Dark,Local}} \text{ReLU} \left( W_1^{\text{Dark,Local}} F_{\text{Dark}} \right) \right) \\
%+ \sigma \left( W_2^{\text{Dark,Global}} \text{ReLU} \left( W_1^{\text{Dark,Global}} \text{GAP}(F_{\text{Dark}}) \right) \right)
%\end{multline}

%\begin{multline}\label{equ.dff_weights_gamma}
%M_{\text{Gamma}} = \sigma \left( W_2^{\text{Gamma,Local}} \text{ReLU} \left( W_1^{\text{Gamma,Local}} F_{\text{Gamma}} \right) \right) \\
%+ \sigma \left( W_2^{\text{Gamma,Global}} \text{ReLU} \left( W_1^{\text{Gamma,Global}} \text{GAP}(F_{\text{Gamma}}) \right) \right)
%\end{multline}

%\begin{multline}\label{equ.dff_weights_he}
%M_{\text{HE}} = \sigma \left( W_2^{\text{HE,Local}} \text{ReLU} \left( W_1^{\text{HE,Local}} F_{\text{HE}} \right) \right) \\
%+ \sigma \left( W_2^{\text{HE,Global}} \text{ReLU} \left( W_1^{\text{HE,Global}} \text{GAP}(F_{\text{HE}}) \right) \right)
%\end{multline}

%Here,
%\( F_{\text{Dark}}, F_{\text{Gamma}}, F_{\text{HE}} \): Feature maps from the dark, gamma-enhanced, and histogram-equalized streams, respectively. \( W_1^{\text{stream,Local}}, W_2^{\text{stream,Local}} \): Weight matrices for local attention computations. \( W_1^{\text{stream,Global}}, W_2^{\text{stream,Global}} \): Weight matrices for global attention computations. \( \text{GAP}(\cdot) \): Global Average Pooling operation. \( \sigma \): Sigmoid activation function. \( \text{ReLU} \): Rectified Linear Unit activation introducing non-linearity.
    
%\textbf{Step 2 : Weighted Inputs}

%The computed attention weights are applied to their respective feature maps as follows:
%\begin{align}
%F_{\text{Weighted}}^{\text{Dark}} &\in \mathbb{R}^{C \times T \times H \times W} = M_{\text{Dark}} \cdot F_{\text{Dark}} \\
%F_{\text{Weighted}}^{\text{Gamma}} &\in \mathbb{R}^{C \times T \times H \times W} = M_{\text{Gamma}} \cdot F_{\text{Gamma}} \\
%F_{\text{Weighted}}^{\text{HE}} &\in \mathbb{R}^{C \times T \times H \times W} = M_{\text{HE}} \cdot F_{\text{HE}} 
%\end{align}
%where, \( F_{\text{Weighted}}^{\text{stream}} \): Weighted feature map for the corresponding stream (\( \text{stream} \in \{\text{Dark}, \text{Gamma}, \text{HE}\} \)).  \( F_{\text{stream}} \): Original feature map from the corresponding stream (\( \text{stream} \in \{\text{Dark}, \text{Gamma}, \text{HE}\} \)).

%\textbf{Step 3: Final Fused Feature Map}  


%The weighted feature maps from all streams are fused to form the final representation:
%\begin{equation}\label{equ.dff_fusion}
%F_{\text{fused}} = F_{\text{Weighted}}^{\text{Dark}} + F_{\text{Weighted}}^{\text{Gamma}} + F_{\text{Weighted}}^{\text{HE}}
%\end{equation}

%where,  \( F_{\text{fused}} \): The fused feature map combining contributions from all three streams.


%\begin{figure}[ht]
  %  \centering
    %\includegraphics[width=0.48\columnwidth]{Figures/Self attention pic.png}
   % \caption{Visual representation of the self-attention mechanism}
   % \label{fig:self_attention}
%\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/updated_3rows.pdf}
    \caption{Visual illustration of Gamma enhanced images and histogram equalised images based 5 different classes of ARID dataset.}
    \label{fig:wide-figure}
\end{figure}

\subsection{BERT-based Temporal Modeling and Classification}

BERT is a bidirectional self-attention model that has achieved remarkable performance across various downstream natural language processing (NLP) tasks \cite{singh2022action}. BERT’s bidirectional nature allows it to capture contextual information from both directions, instead of relying on just one direction, making it well-suited for sequential data.
%For sequential data, BERT's bidirectional capability enables it to incorporate contextual information from both directions instead of relying on a single direction. 
In the BERT architecture, the self-attention module computes the response at a given position by attending to all positions in the sequence and deriving a weighted average in the embedding space. Inspired by \cite{kalfaoglu2020late}, we utilize BERT’s bidirectional self-attention mechanism, as illustrated in Fig. \ref{fig:arc-figure} to effectively capture long-range temporal dependencies within the fused feature sequence.

%Inspired by \cite{kalfaoglu2020late}, we utilize BERT’s bidirectional self-attention mechanism, as illustrated in Fig. \ref{fig:self_attention}, to effectively capture long-range temporal dependencies and model relationships between temporally distant frames within the fused feature map.

%The BERT model is specifically adapted to capture long-range temporal dependencies within the fused feature map, which inherently encodes spatially refined features.
% The BERT model is adapted to capture long-range temporal dependencies in the fused feature map, which already encodes spatially refined features. 
%Inspired by \cite{kalfaoglu2020late}, we utilize BERT’s bidirectional self-attention mechanism, as illustrated in Fig. \ref{fig:self_attention} to effectively model relationships between temporally distant frames. The key steps include:

%The BERT model is adapted to capture long-range temporal dependencies in the fused feature map, which already encodes spatially-refined features.  Inspired by \cite{kalfaoglu2020late} we utilize BERT’s bidirectional self-attention mechanism to model relationships between temporally distant frames effectively. The key steps include:

%\begin{itemize}
    %\item \textbf{Sequence Preparation:} The fused feature map is reshaped into a sequence, with each feature augmented by a learnable positional embedding to encode temporal context.
    %\item \textbf{Temporal Pooling:} After feature fusion, average pooling is applied to reduce spatial dimensions.
    %\item \textbf{Self-Attention Mechanism:} The BERT model computes attention weights for each position in the sequence, enabling it to capture long-range temporal dependencies across frames.
    %\item \textbf{Classification Token:} A learnable CLS token is used to aggregate information from the entire sequence. The final classification output is generated using a fully connected layer.
%\end{itemize}

 The input to BERT is the fused feature map \( F_{\text{fused}} \) with dimensions \( \mathbb{R}^{C \times T \times H \times W} \), reshaped for sequential processing.

\subsubsection{Sequence Preparation for Temporal Encoding}

The fused feature map is first pooled and reshaped:
\begin{equation}
F_{\text{seq}} \in \mathbb{R}^{T \times D}
\end{equation}
where, \( T \): Temporal dimension (number of frames). \( D \): Number of short-term characteristics of consecutive frames.

Each temporal feature \( F_{\text{seq}}[i] \) is augmented with positional embeddings \( b_{\text{pos}}^i \) to encode temporal order:
\begin{equation}
b_i = F_{\text{seq}}[i] + b_{\text{pos}}^i, \quad (i = 1, 2, \dots, T)
\end{equation}
where, \( b_i \): The \( i \)-th encoding vector, providing location information. \( F_{\text{seq}}[i]\): Input feature for the \( i \)-th position. \( b_{\text{pos}}^i \): The \( i \)-th learnable positional embedding, where \( b_{\text{pos}} \in \mathbb{R}^{D \times T} \).

A learnable classification token (\( \text{CLS} \)) is also included in the encoding as \(b_0 \), resulting in:
\begin{equation}
b_o,,b \in \mathbb{R}^{D \times (T+1)}
\end{equation}

\subsubsection{Self-Attention for Temporal Dependency Modeling}

For each layer \( l \), BERT computes the Query (\( Q \)), Key (\( K \)), and Value (\( V \)) matrices as:
\begin{equation}
q_i^{(l,h)} = W_q^{(l,h)} \mathcal{L} \left(b_i^{(l-1)}\right), \quad k_i^{(l,h)} = W_k^{(l,h)} \mathcal{L} \left(b_i^{(l-1)}\right),
\end{equation}
\begin{equation}
v_i^{(l,h)} = W_v^{(l,h)} \mathcal{L} \left(b_i^{(l-1)}\right)
\end{equation}
where, \( q_i^{(l,h)}, k_i^{(l,h)}, v_i^{(l,h)} \in \mathbb{R}^{d} \): Query, Key, and Value vectors for the \( i \)-th position in the sequence, for attention head \( h \). \( W_q^{(l,h)}, W_k^{(l,h)}, W_v^{(l,h)} \in \mathbb{R}^{D \times d} \): Learnable weight matrices for Query, Key, and Value. \( \mathcal{L} \): Layer normalization applied to the input embeddings \( e_i^{(l-1)} \). \( d = D / H \): Dimensionality of each attention head, where \( H \) is the number of attention heads.

\subsubsection{Attention Scores}

Scaled dot-product attention is used to compute attention scores for each pair of positions:
\begin{equation}
\alpha^{(l,h)} = \textit{softmax} \left( \frac{q_i^{(l,h)}T \cdot k_i^{(l,h)}}{\sqrt{d}} \right), \quad (i = 0, 1, \dots, m)
\end{equation}

\subsubsection{Attention Output}

The weighted sum of value vectors using \( \alpha \) from each attention head is used to obtain encoding \( b_i^{(l)} \) at block \( l \), as shown below:
\begin{equation}
O_i^{(l,h)} = \sum_{0}^{m} \alpha_{i}^{(l,h)} v_i^{(l,h)}
\end{equation}

\subsubsection{Multi-Head Attention Output}

The combined attention output with residual connection is expressed as:
\begin{equation}
b_i^{\prime (l)} = W_o 
\begin{bmatrix}
O_i^{(l,1)} \\
\vdots \\
O_i^{(l,H)}
\end{bmatrix}
+ b_i^{(l-1)}
\end{equation}

\subsubsection{Final Encoding and Residual Connection}

The final output after applying LayerNorm and MLP is expressed as:
\begin{equation}
b_i^{(l)} = \text{MLP} \big( \mathcal{L} (b_i^{\prime (l)}) \big) + b_i^{\prime (l)}
\end{equation}
where, \( b_i^{(l)} \): Final encoded representation for layer \( l \). \( \mathcal{L} \): Layer normalization operation. \( \text{MLP} \): Multi-layer perceptron applied to the normalized output.

\subsubsection{Classification}


The final classification token \( Y_{\text{CLS}} \), derived from the last block, is processed through a fully connected (FC) layer. The \textit{argmax} function is then applied to determine the final predicted class:
\begin{equation}
\text{Result} = \text{Argmax}(\text{FC}(Y_{\text{CLS}})).
\end{equation}

\section{Experiments}

\subsection{ARID dataset}

The availability of real-life dark video datasets is extremely limited. To demonstrate the effectiveness of our proposed approach, we utilized the ARID dataset \cite{xu2021arid}, which is specifically designed for low-light conditions. The ARID dataset has two versions: ARID V1.0 and ARID V1.5. All the videos in this dataset were recorded in low-light settings or at night time. ARID V1.0 contains 3,784 video clips, while ARID V1.5 expands to 5,572 clips with each action class containing over 320 clips, spanning a total of 11 action categories. The eleven action classes include drinking, jumping, picking, pouring, pushing, running, sitting, standing, turning, walking, and waving.  

\subsection{Implementation details}
The proposed approach is implemented on an Advanced Computing Platform
(HPC) powered by NVIDIA A100 GPU utilizing the open-source machine learning framework PyTorch \cite{paszke2019pytorch}. We conducted experiments on both versions of the ARID benchmark datasets, ARID V1.0 and ARID V1.5, to evaluate action recognition in dark environments. We follow the approach outlined in \cite{chen2021darklight} to report the average Top-1 and Top-5 accuracies across three splits. The input frame dimensions are set to 3 × 64 × 112 × 112. The input frames are enhanced using gamma correction and histogram equalization. Additionally, the original frames are included to create a multi-stream approach. For feature extraction, we utilized the R(2+1)D-34 model, pre-trained on the IG65M [6] dataset, with the average temporal pooling layer removed. The R(2+1)D-34 architecture decomposes 3D convolutions into separate 2D spatial and 1D temporal convolutions, allowing for more efficient feature learning. The output from the feature extractor has a dimensionality of 512 × 8 × 7 × 7. Next, an average pooling layer is applied, resulting in an output with dimensions of 512 × 8 which serves as the input to the DFF (Dynamic Feature Fusion) module. This output is then transposed to a dimension of 8×512, which serves as the input to the BERT module that provides the feature vector of
dimension 9×256. Which is then reduced  to a single 256-dimensional vector, which is passed to the classification head to predict the action class. The training process employs the ADAMW optimizer [19] with a learning rate of 
$10^{-5}$. 

We carried out experiments to tune the hyper-parameter $\gamma$, testing values ranging from 1 to 5.5 in increments of 0.5, with each experiment running for 30 epochs, as illustrated in Fig. \ref{fig:gamma_ablation} The results indicated that $\gamma = 2.5$ delivered the best performance. Consequently, $\gamma = 2.5$ was adopted for all subsequent experiments.

\begin{figure}[ht]
    \centering
    \includegraphics[width=7.5cm,height=3.5cm]{Figures/gamma_abla.png}
    \caption{The experiment to determine the optimal value of the hyperparameter $\gamma$ in the proposed MD-BERT architecture.}
    \label{fig:gamma_ablation}
\end{figure}



\begin{table}[!t]
\caption{The Top-1 and Top-5 accuracy results on ARID V1.0 for several competitive methods and our proposed approach}
\label{tab:accuracy_comparison}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c}
\toprule[0.5mm]
\textbf{Method} & \textbf{Top-1 Accuracy (\%)} & \textbf{Top-5 Accuracy (\%)} \\ \midrule[0.25mm]
VGG-TS (\textit{ICLR}) \cite{simonyan2014very}  & 32.08 & 90.76 \\ 
TSN (\textit{ECCV}) \cite{wang2016temporal}  & 57.96 & 94.17 \\ 
R(2+1)D (\textit{CVPR}) \cite{tran2018closer} & 62.87 & 96.64 \\ 
I3D-RGB (\textit{CVPR}) \cite{carreira2017quo}  & 68.29 & 97.69 \\ 
3D-ResNet-50 (\textit{CVPR)} \cite{hara2018can}  & 71.08 & 99.39 \\ 
3D-ResNet-101 (\textit{CVPR}) \cite{hara2018can}  & 71.57 & 99.03 \\ 
Pseudo-3D-199 (\textit{ICCV}) \cite{qiu2017learning} & 71.93 & 98.66 \\ 
I3D Two-stream (\textit{CVPR}) \cite{carreira2017quo}  & 72.78 & 99.39 \\ 
DarkLight-ResNeXt-101 (\textit{CVPRW}) \cite{chen2021darklight} & 87.27 & 99.47 \\ 
MRAN (\textit{CVPRW}) \cite{hira2021delta} & 93.73 & - \\ 
DarkLight-R(2+1)D-34 (\textit{CVPRW}) \cite{chen2021darklight} & 94.04 & 99.87 \\ 
SCI + R(2+1)D-GCN (\textit{AAAI}) \cite{suman2023two}  & 95.86 & 99.87 \\ 
R(2+1)D-GCN+BERT (\textit{IEEE TAI}) \cite{singh2022action} & 96.60 & 99.88 \\ 
DTCM (\textit{IEEE TIP}) \cite{tu2023dtcm} & 96.36 & \textbf{99.92} \\ \midrule[0.25mm]
\textbf{Proposed: MD-BERT} & \textbf{96.89} & 99.88 \\ \bottomrule[0.5mm]
\end{tabular}}
\end{table}


\begin{table}[!t]
\centering
\caption{The Top-1 and Top-5 accuracy results on ARID V1.5 for several competitive methods and our proposed approach}
\label{tab:accuracy_comparison1.5}
% Resize the table to fit the column width
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c}
\toprule[0.5mm]
\textbf{Method} & \textbf{Top-1 Accuracy (\%)} & \textbf{Top-5 Accuracy (\%)} \\ \midrule[0.25mm]
3D-ResNet-18 (\textit{CVPR}) \cite{hara2018can}     & 31.16 & 90.49 \\ 
I3D-RGB (\textit{CVPR}) \cite{carreira2017quo}      & 48.75 & 90.61 \\ 
I3D Two-stream (\textit{CVPR}) \cite{carreira2017quo} & 51.24 & 90.95 \\ 
DarkLight-R(2+1)D-34 (\textit{CVPRW}) \cite{chen2021darklight} & 84.13 & 97.34 \\ 
R(2+1)D-GCN+BERT (\textit{IEEE TAI}) \cite{singh2022action}       & 86.93 & \textbf{99.35} \\ 
\midrule[0.25mm]
\textbf{Proposed: MD-BERT} & \textbf{87.43} & 98.99 \\ \bottomrule[0.5mm]
\end{tabular}%
}
\end{table}


\subsection{Results and Discussion}

The results of our method, along with those of current competitive methods for action recognition in dark videos, are presented in Table \ref{tab:accuracy_comparison} and Table \ref{tab:accuracy_comparison1.5}. These results include evaluations on both ARID V1.0 and ARID V1.5 datasets. Most of the data are sourced from \cite{singh2022action}. Our proposed method achieves state-of-the-art results on both versions of the Top-1 accuracy ARID dataset. The evaluation is conducted using all eleven classes of the ARID dataset, measuring both Top-1 and Top-5 accuracy.
MD-BERT achieves 96.89\% Top-1 accuracy on ARID V1.0, surpassing the closest competitor, R(2+1)D-GCN+BERT (96.60\%), by 0.29\%. Though small, this improvement is significant given the high accuracy baseline, where marginal gains are hard to achieve. While MD-BERT and R(2+1)D-GCN+BERT share some architectural similarities, MD-BERT’s multi-stream network—integrating dark, gamma-enhanced, and histogram-equalized frames—along with the Dynamic Feature Fusion (DFF) module, enables adaptive feature weighting and better balancing of local and global contexts, giving it a slight edge.
Compared to DTCM, MD-BERT improves Top-1 accuracy by 0.53\% (96.89\% vs. 96.36\%). DTCM’s joint optimization framework integrates dark enhancement and action recognition through spatio-temporal consistency and lightweight enhancement but lacks advanced temporal modeling. MD-BERT’s BERT-based architecture captures long-range dependencies and contextual relationships across frames, offering a significant advantage in modeling complex action dynamics in dark videos. MD-BERT also outperforms DarkLight-R(2+1)D-34 and SCI + R(2+1)D-GCN by 3.03\% and 1.07\%, respectively.

In Table \ref{tab:accuracy_comparison1.5}, evaluated on the ARID V1.5 dataset, 3D-ResNet-18 \cite{hara2018can} lags with 31.16\% Top-1 accuracy, highlighting the challenges of low-light video recognition using basic 3D CNNs. I3D-RGB and I3D Two-stream \cite{carreira2017quo} improve to 48.75\% and 51.24\%, with the latter benefiting from optical flow, but both still struggle with underexposed frames. DarkLight-R(2+1)D-34 \cite{chen2021darklight} makes a notable leap to 84.13\% by deploying targeted enhancement strategies for dark frames in conjunction with (2+1)D convolutions, highlighting the importance of low-light pre-processing.  R(2+1)D-GCN+BERT \cite{singh2022action} further improves to 86.93\%, leveraging graph-based modeling and BERT for nuanced temporal feature extraction. However, our proposed MD-BERT achieves a 0.5\% improvement (87.43\% vs. 86.93\%) by incorporating multi-stream inputs and the DFF module, which enhances feature diversity and provides richer input representations for BERT-based temporal modeling.

%Our proposed MD-BERT achieves state-of-the-art performance on the ARID V1.0 dataset with 96.89\% Top-1 accuracy, surpassing the closest competitor, R(2+1)D-GCN+BERT, by 0.29\%, which achieved 96.60\% Top-1 accuracy. While the percentage increase appears minimal, it is significant given the already high accuracy levels, where even marginal improvements are challenging to achieve. 

%While our approach and R(2+1)D-GCN+BERT share a few similarities in terms of overall architecture, the addition of a multi-stream network, which integrates dark, gamma-enhanced, and histogram-equalized frames to capture a richer set of complementary features, along with the Dynamic Feature Fusion (DFF) module, ensures adaptive weighting of these features and a more effective balance between local and global contexts, giving MD-BERT the slight edge.

%When compared with DTCM, MD-BERT achieves a 0.53\% improvement in Top-1 accuracy 96.89\% vs. 96.36\%. While DTCM employs a joint optimization framework that integrates dark enhancement and action recognition through spatio-temporal consistency preservation and lightweight enhancement strategies, it does not utilize advanced temporal modeling techniques like BERT. In contrast, MD-BERT incorporates a BERT-based architecture, which excels at capturing long-range temporal dependencies and contextual relationships across frames. This demonstrates that BERT provides a significant advantage in modeling the complex dynamics of actions in dark videos, further enhancing the performance of our multi-stream framework. Our approach also performed better than DarkLight-R(2 + 1)D-34 and SCI + R(2 + 1)D-GCN by 3.03\% and 1.07\%. 

%In Table \ref{tab:accuracy_comparison1.5} 3D-ResNet-18 \cite{hara2018can} lags significantly at 31.16\% Top-1 accuracy, underscoring the difficulty of low-light video recognition when relying solely on a straightforward 3D CNN. I3D-RGB \cite{carreira2017quo} and I3D Two-stream \cite{carreira2017quo} fare better (48.75\% and 51.24\% respectively), primarily due to the incorporation of optical flow in the latter, yet they still fail to compensate for the loss of salient features in underexposed frames. 
%DarkLight-R(2+1)D-34 \cite{chen2021darklight} makes a notable leap to 84.13\% by deploying targeted enhancement strategies for dark frames in conjunction with (2+1)D convolutions, highlighting the importance of low-light pre-processing. 
%R(2+1)D-GCN+BERT \cite{singh2022action} further raises the bar to 86.93\%, showcasing how combining graph-based modeling of skeleton joints with BERT can capture more nuanced temporal dependencies. 

\subsection{Ablation Study}

%This section presents an ablation study analyzing the various blocks utilized in the architecture of the proposed \textbf{MD-BERT}-based action recognition techniques. The DFF module is explicitly designed to fuse features from multiple inputs, each emphasizing different aspects of the video frames. By combining features through a combination of local and global spatiotemporal attention mechanisms, DFF generates a refined weight map that dynamically adjusts the importance of each input stream. This results in a fused feature map that better captures the complementary information across the inputs. 

%\begin{table}[htbp]
%\caption{Ablation Study on ARID V1.0 and V1.5 Datasets with Different Inputs}
%\label{table:ablation_inputs}
%\centering
%\resizebox{\columnwidth}{!}{%
%\begin{tabular}{|p{4.5cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|}
%\hline
%\textbf{Inputs} & \textbf{Top-1 Accuracy V1.0 (\%)} & \textbf{Top-5 Accuracy V1.0 (\%)} & \textbf{Top-1 Accuracy V1.5 (\%)} & \textbf{Top-5 Accuracy V1.5 (\%)} \\ \hline
%Gamma Enhanced + Dark & 94.72 & 99.85 & 84.91 & 98.24 \\ \hline
%Histo Equalized + Dark & 95.15 & 99.91 & 85.03 & 98.21 \\ \hline
%Gamma Enhanced + Histo Equalized & 96.01 & 99.84 & 86.52 & 98.76 \\ \hline
%All Three Inputs (Dark, Gamma, Histo) & \textbf{96.89} & \textbf{99.88} & \textbf{87.43} & \textbf{98.99} \\ \hline
%\end{tabular}%
%}
%\end{table}

\begin{table}[t]
\caption{Ablation Study on ARID V1.0 and V1.5 Datasets with different Inputs}
\label{table:ablation_inputs}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lll|ll|ll}
\toprule[0.5mm]
\multicolumn{3}{c|}{\textbf{Inputs}}                            & \multicolumn{2}{c|}{\textbf{ARID V1.0}}    & \multicolumn{2}{c}{\textbf{{ARID V1.5}}}    \\ \cline{1-7}
\multicolumn{1}{c|}{\textbf{Dark}} & \multicolumn{1}{c|}{\textbf{Gamma}} & \textbf{Histogram} & \multicolumn{1}{c|}{\textbf{Top-1 Acc (\%)}} & \textbf{Top-5 Acc (\%)} & \multicolumn{1}{c|}{\textbf{Top-1 Acc (\%)}} &  \textbf{Top-5 Acc (\%)}\\ \midrule[0.25mm]
\multicolumn{1}{l|}{\checkmark} & \multicolumn{1}{l|}{\checkmark} & - & \multicolumn{1}{l|}{94.72} & 99.85  & \multicolumn{1}{l|}{84.91} & 98.24  \\ 
\multicolumn{1}{l|}{\checkmark} & \multicolumn{1}{l|}{-} & \checkmark  & \multicolumn{1}{l|}{95.15} & 99.61  & \multicolumn{1}{l|}{85.03} & 98.21 \\ 
\multicolumn{1}{l|}{-} & \multicolumn{1}{l|}{\checkmark} & \checkmark & \multicolumn{1}{l|}{96.01} & 99.84  & \multicolumn{1}{l|}{86.52} & 98.76  \\ 
\multicolumn{1}{l|}{\checkmark} & \multicolumn{1}{l|}{\checkmark} & \checkmark & \multicolumn{1}{l|}{\textbf{96.89}} &  \textbf{99.88} & \multicolumn{1}{l|}{\textbf{87.43}} & \textbf{98.99}  \\ \bottomrule[0.25mm]
\end{tabular}}
\end{table}

This section presents an ablation study of the blocks used in the proposed MD-BERT architecture. 
Table \ref{table:ablation_inputs} compares various two-stream input configurations on the ARID V1.0 and V1.5 datasets, highlighting the contributions of Dark, Gamma, and Histogram enhancements to action recognition in low-light conditions. Notably, DarkLight-R(2+1)D-34 \cite{chen2021darklight} also uses a two-stream approach (Dark+Gamma), but our method achieves higher Top-1 accuracy: 94.72\% vs. 94.04\% on ARID V1.0 and 84.91\% vs. 84.13\% on ARID V1.5. These gains demonstrate the effectiveness of our Dynamic Feature Fusion (DFF) and BERT-based temporal modeling in better integrating streams and preserving temporal consistency. Among the two-stream combinations in Tables \ref{table:ablation_inputs}, (Gamma+Histogram) typically provides the best results due to effective brightness and contrast enhancements. However, the three-stream configuration (Dark+Gamma+Histogram) consistently achieves the highest Top-1 accuracy: 96.89\% on ARID V1.0 and 87.43\% on ARID V1.5. This confirms that each input contributes unique low-light cues, and their dynamic integration through our fusion framework is key to outperforming existing two-stream methods in underexposed video scenarios.


%Table \ref{table:ablation_inputs} provide a detailed comparison of various two stream input configurations on the ARID V1.0 and V1.5 datasets, highlighting how each form of enhancement (Dark, Gamma, Histogram) contributes to action recognition under low-light conditions. Notably, DarkLight-R(2+1)D-34 \cite{chen2021darklight} also employs a two-stream strategy using (Dark+Gamma) inputs. However, even under these same dual-stream constraints (Dark+Gamma), our approach achieves marginally higher Top-1 accuracy 94.72\% vs 94.04\% on ARID V1.0, and 84.91\% vs 84.13\% on ARID V1.5. These improvements underscore the effectiveness of our Dynamic Feature Fusion (DFF) and BERT-based temporal modeling in better integrating both streams, capturing diverse lighting details while preserving temporal consistency. Among the dual-stream combinations in Tables \ref{table:ablation} and \ref{table:ablation2}, (Gamma+Histogram) typically yields the best accuracy, indicating an effective integration of brightness and contrast focused enhancements. Nevertheless, the three-stream configuration (Dark+Gamma+Histogram) consistently delivers the highest Top-1 accuracies—96.89\% on ARID V1.0 and 87.43\% on ARID V1.5—demonstrating that each enhancement provides partially unique low-light cues. Overall, these ablation results confirm that incorporating complementary visual transformations and integrating them through a fusion framework that dynamically assigns importance to different features is key to surpassing existing methods and other two-stream approaches, particularly in challenging underexposed video scenarios.









%These gains highlight how our Dynamic Feature Fusion (DFF) and BERT-based temporal modeling more effectively integrate both streams, capturing different lighting details while maintaining temporal consistency. 


%\begin{table}[htbp]
%\caption{Component Ablation Study with Three Inputs (Dark, Gamma, Histogram) for ARID V1.0 and V1.5}
%\label{table:ablation2}
%\centering
%\resizebox{\columnwidth}{!}{%
%\begin{tabular}{|p{0.7cm}|p{0.7cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
%\hline
%\textbf{DFF} & \textbf{BERT} & \textbf{Top-1 Accuracy V1.0 (\%)} & \textbf{Top-5 Accuracy V1.0 (\%)} & \textbf{Top-1 Accuracy V1.5 (\%)} & \textbf{Top-5 Accuracy V1.5 (\%)} \\ \hline
%- & \checkmark& 94.72& 99.85& 84.91& 98.24\\ \hline
%\checkmark & -& 95.15& 99.91& 85.03& 98.21\\ \hline
%\checkmark&\checkmark& 96.01& 99.84& 86.52& 98.76 \\ \hline
%\end{tabular}%
%}
%\end{table}

\begin{table}[t]

\caption{Component Ablation Study with Three Inputs (Dark, Gamma, Histogram) for ARID V1.0 and V1.5}
\label{table:ablation2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ll|ll|ll}
\toprule[0.5mm]
\multicolumn{2}{c|}{\textbf{Components}}    & \multicolumn{2}{c|}{\textbf{ARID V1.0}}    & \multicolumn{2}{c}{\textbf{ARID V1.5}}    \\ \cline{1-6}
\multicolumn{1}{c|}{\textbf{DFF}} & \textbf{BERT} & \multicolumn{1}{c|}{\textbf{Top-1 Acc (\%)}} & \textbf{Top-5 Acc (\%)}  & \multicolumn{1}{c|}{\textbf{Top-1 Acc (\%)}} & \textbf{Top-5 Acc (\%)} \\ \midrule[0.25mm]
\multicolumn{1}{l|}{-} & \checkmark & \multicolumn{1}{l|}{95.08} & 99.83 & \multicolumn{1}{l|}{85.54} & 98.38 \\ 
\multicolumn{1}{l|}{\checkmark} & - & \multicolumn{1}{l|}{95.79} & 99.71 & \multicolumn{1}{l|}{86.62} & 98.11 \\ 
\multicolumn{1}{l|}{\checkmark} & \checkmark & \multicolumn{1}{l|}{\textbf{96.89}} &\textbf{99.88} & \multicolumn{1}{l|}{\textbf{87.43}} & \textbf{98.99} \\ \bottomrule[0.25mm]
\end{tabular}}
\end{table}

Table \ref{table:ablation2} highlights the impact of the Dynamic Feature Fusion (DFF) module and BERT-based temporal modeling on action recognition accuracy across the ARID V1.0 and V1.5 datasets using three inputs (Dark, Gamma, and Histogram). The DFF module effectively integrates features from multiple inputs by dynamically adjusting their importance using local and global spatiotemporal attention. This enhances complementary information fusion, improving Top-1 accuracy from 95.08\% to 95.79\% on ARID V1.0 and 85.54\% to 86.62\% on ARID V1.5 without BERT. When combined with BERT, which excels at capturing long-range temporal dependencies, the framework achieves the highest performance: 96.89\% Top-1 and 99.88\% Top-5 accuracy on ARID V1.0, and 87.43\% Top-1 and 98.99\% Top-5 accuracy on ARID V1.5. These results demonstrate the complementary roles of DFF in enhancing short-range temporal correlations and BERT in capturing long-range dependencies. This fusion framework sets a new benchmark for multi-input action recognition under low-light conditions.

%Table \ref{table:ablation2} demonstrates the impact of the Dynamic Feature Fusion (DFF) module and BERT-based temporal modeling on action recognition accuracy across ARID V1.0 and V1.5 datasets, focusing on a three-input setting (Dark, Gamma, and Histogram Equalization). The results clearly illustrate how each component contributes to the overall system's performance. 
%The DFF module effectively addresses the challenge of integrating features from multiple inputs, including dark frames and enhanced versions (Gamma and Histogram). By combining local and global spatiotemporal attention mechanisms, DFF dynamically adjusts the importance of each input stream through a refined weight map. This improves the integration of complementary information across inputs. For instance, introducing DFF without BERT increases Top-1 accuracy from 95.08\% to 95.79\% on ARID V1.0 and from 85.54\% to 86.62\% on ARID V1.5. These results emphasize DFF's ability to enhance spatiotemporal feature fusion, particularly through the short-range temporal correlations captured by 3D convolutions.When BERT-based temporal modeling is included, the framework leverages its strength in modeling long-range temporal dependencies, significantly improving accuracy. Combining BERT with DFF achieves the best results across both datasets, with Top-1 accuracy of 96.89\% and Top-5 accuracy of 99.88\% on ARID V1.0, and Top-1 accuracy of 87.43\% and Top-5 accuracy of 98.99\% on ARID V1.5. These results validate the complementary effects of DFF's spatiotemporal attention mechanisms and BERT's temporal modeling capabilities, creating a robust framework for understanding both short-range and long-range temporal patterns. This ablation study highlights the individual and combined contributions of DFF and BERT in advancing action recognition under low-light conditions. The results demonstrate that the proposed fusion framework effectively captures both short-range and long-range spatiotemporal dependencies, setting a new benchmark for multi-input action recognition systems in low-light setting.


\section{Conclusion}

In this work, we present MD-BERT, a novel framework for action recognition in dark and low-light or under-exposed videos. By leveraging three input streams, \textit{i.e.}, raw dark frames, gamma-enhanced frames, and histogram-equalized frames, our approach captures complementary visual information to address the challenges of low visibility and noise. The proposed DFF module adaptively combines these features, balancing local and global contexts, while the BERT-based temporal modeling captures long-range dependencies across frames. Experimental results on the ARID V1.0 and V1.5 datasets show that MD-BERT achieves state-of-the-art performance in challenging low-light scenarios. This work highlights the potential of combining dynamic feature fusion and temporal modeling for improving action recognition. Future research could explore integrating advanced image enhancement to further improve the clarity of low-light video features and extend the framework to other low-light video domains.

%\section*{Acknowledgment}



\bibliographystyle{IEEEbib}
{\small
\bibliography{bibliography}}

\end{document}
