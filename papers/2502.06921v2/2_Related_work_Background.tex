\section{Related Work}\label{sec:prior_art}
% \textcolor{black}{GNNs excel at structural tasks due to their ability to extract features from graph topology~\cite{gnn_survey_wu}, yet they require substantial computational power~\cite{g_cos}. As explained in \sectionautorefname~\ref{sec:intro}, 
% % For example, deploying DGCNN~\cite{gcode} on a Raspberry Pi 3B results in only 0.3 fps, insufficient for real-time applications, 
% deploying  GNNs on resource-constrained edge environments presents serious difficulties. 
% Efforts to optimize GNNs for edge devices include simplifying model structures~\cite{gcn_point_cloud} and using hardware-aware neural architecture search (NAS) methods like HGNAS~\cite{gnn_fpga} and others~\cite{gnn_edge_1}. However, these still fall short, with HGNAS improving point cloud processing speed to only 2 fps on the Raspberry Pi~\cite{gnn_fpga}.
GNNs excel at structural tasks due to their ability to extract features from graph topology~\cite{gnn_survey_wu}, yet they require substantial computational power~\cite{g_cos}. As detailed in \sectionautorefname~\ref{sec:intro}, deploying GNNs in resource-constrained edge environments presents serious difficulties. To tackle this, strategies to optimize GNNs for edge devices include simplifying model architectures~\cite{gcn_point_cloud} and employing hardware-aware neural architecture search (NAS) techniques like HGNAS~\cite{gnn_fpga} among others~\cite{gnn_edge_1}. Nonetheless, these approaches still fall short; for instance, HGNAS boosts point cloud processing speed to merely 2 fps on a Raspberry Pi~\cite{gnn_fpga}.
On the other hand, previous optimization approaches for DNN accelerators focused on techniques such as model fine-tuning, memory optimization, and standard quantization~\cite{fast_gnn, hls_gnn, sharedGNN}. Although they improved efficiency, they often required extensive retraining or hardware-specific code modifications, limiting portability.
Furthermore, existing GNN mapping methods do not fully leverage NPU-specific features like efficient sparsity handling, static data shapes, and optimized memory access, leading to suboptimal performance~\cite{EnGN}. These methods also struggle with the irregular computation patterns and memory intensity of GNNs, limiting their deployment on real-time edge devices.
GraNNite addresses these challenges by introducing NPU-tailored optimizations that enable efficient, high-performance GNN execution on resource-constrained accelerators for real-time deployment.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GNN_execution_flow_sing_col.pdf}% This is a *.eps file
\end{center}
\caption{Execution flow of a GCN: graph preprocessing followed by iterative aggregation and combination phases for GNN computation~\cite{raha_book_chapter}.}\label{fig:GNN_agg_comb}
\end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Plots/Motivation_GraphSplit.png}% This is a *.eps file
\end{center}
\caption{Execution Latency Breakdown of GraphConv and GraphAttn Layers (1433 input features and 64 output features) on Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU across graph preprocessing (DPU/DSP) and GNN computation (DPU/DSP) for a graph with 1354 nodes and 5429 edges.}\label{fig:motivation_graphsplit}
\end{figure}


% Previous efforts to optimize GNNs for specialized hardware, such as DNN accelerators, have primarily focused on general-purpose techniques developed for traditional neural networks. These include fine-tuning models for specific architectures, optimizing memory usage, and applying standard quantization to reduce computation and memory overhead. While approaches like dataflow architectures, high-level synthesis (HLS) descriptions, and degree-aware mixed-precision quantization improved execution efficiency~\cite{fast_gnn, hls_gnn, sharedGNN}, they often required extensive retraining or hardware-specific code modifications, limiting portability and scalability.
% Despite these advancements, existing methods fail to address critical challenges unique to NPUs, such as handling static data shapes and mitigating the limited data-parallel performance of DSP modules for GNN-specific operations. Standard compression techniques underutilized the inherent sparsity in GNN data, leading to inefficiencies, especially on edge devices. Additionally, the irregular and input-dependent computation patterns of GNNs exacerbated latency issues on CPUs, GPUs, and accelerators like TPUs, restricting their deployment to offline inference scenarios. The memory-intensive nature of GNNs further amplified bottlenecks, particularly in resource-constrained environments~\cite{EnGN}.
% GraNNite bridges these gaps with novel optimizations tailored to NPU-specific challenges, enabling efficient and high-performance GNN execution on resource-constrained accelerators.

% Graph Neural Networks (GNNs) are widely used for structural tasks due to their ability to extract features from graph topology~\cite{gnn_survey_wu}. However, this capability often comes at the expense of computational efficiency~\cite{reference6}. For instance, deploying the widely-used point cloud processing model DGCNN~\cite{gcode} on a Raspberry Pi 3B achieves a mere 0.3 fps, far below practical requirements for real-time applications. Such inefficiencies limit the applicability of GNNs in resource-constrained edge environments.
% Several research efforts have attempted to address the inefficiency of GNNs on edge devices. Approaches such as manually simplifying model structures~\cite{gcn_point_cloud} have reduced computational overhead, while hardware-aware neural architecture search (NAS) methods, including HGNAS~\cite{gnn_edge_1} and others~\cite{gnn_fpga}, have been used to design hardware-friendly GNNs. Despite these advancements, performance improvements remain constrained by limited edge hardware resources. For example, HGNAS improves point cloud processing speed to only 2 fps on the Raspberry Pi~\cite{gnn_edge_1}, which is still insufficient for real-time scenarios.
% Furthermore, existing GNN mapping techniques often overlook the architectural intricacies of accelerators like NPUs, which are designed for data-parallel operations. These methods fail to exploit hardware-specific capabilities, such as efficient handling of sparsity, static data shapes, and optimized memory access patterns, resulting in suboptimal performance and energy efficiency. This gap underscores the need for frameworks like GraNNite, which address these limitations by introducing hardware-aware optimizations tailored to the unique requirements of NPUs.



% Version #1
% Previous efforts to optimize GNNs for specialized hardware, such as DNN accelerators, have largely focused on general-purpose optimizations designed for traditional neural networks. These approaches often included fine-tuning models for specific hardware architectures, adjusting memory usage, and employing standard quantization techniques to reduce computation and memory overhead. For instance, model mapping techniques adapted GNNs to accelerators but frequently required extensive retraining or hardware-specific code modifications to achieve satisfactory performance. Other solutions leveraged optimizations like dataflow architectures and high-level synthesis (HLS) descriptions to enhance data access and processing element utilization~\cite{fast_gnn, hls_gnn}. Additionally, techniques such as degree-aware mixed-precision quantization and processing-in-memory (PIM) systems were explored to improve the efficiency of GNN execution~\cite{sharedGNN, EnGN}.
% While these methods brought advancements in memory and computation efficiency, they failed to address challenges unique to NPUs, such as the need for static data shapes and the suboptimal performance of DSP modules in executing GNN-specific operations. Retraining models for each hardware platform remained a time-consuming and labor-intensive process, limiting the portability of pre-trained GNNs across devices. Furthermore, hardware-specific code dependencies hindered the transferability of optimizations to new architectures without significant rework. Standard compression techniques often fell short in fully exploiting the inherent sparsity in GNN data, leading to wasted resources and increased energy consumption, particularly on edge devices.
% Moreover, the irregular computation patterns and input-dependent execution of GNNs exacerbated inefficiencies on traditional CPUs, GPUs, and even specialized DNN accelerators like TPUs, causing higher inference latency compared to other neural network types. These inefficiencies restricted the practical deployment of GNNs to scenarios where offline inference was viable. Additionally, the memory-intensive nature of GNNs imposed a major bottleneck, as data movement between memory and processors became increasingly challenging in resource-constrained environments~\cite{EnGN}.
% GraNNite addresses these gaps by introducing a set of novel optimizations tailored to overcome NPU-specific challenges, providing a robust solution for deploying GNNs on resource-constrained accelerators.

% Version #0
% Previous solutions for deploying GNNs on specialized hardware, such as DNN accelerators, relied on general-purpose optimizations typically used for traditional neural networks. These approaches included fine-tuning models for specific hardware architectures, adjusting memory usage, and employing standard quantization techniques to reduce computation and memory overhead. For example, model mapping techniques were used to adapt GNNs to accelerators, though these often-required extensive retraining or hardware-specific code modifications to achieve acceptable performance. Additionally, solutions for enabling high-performance execution of GNNs on resource-constrained DNN accelerators focused on optimizing dataflows and leveraging specialized hardware architectures. Techniques such as high-level synthesis (HLS) descriptions and dataflow architectures optimized data access and processing element utilization~\cite{fast_gnn, hls_gnn}. Moreover, methods like degree-aware mixed-precision quantization and processing-in-memory (PIM) systems were explored to enhance the efficiency of GNN execution~\cite{sharedGNN, EnGN}.
% However, these methods presented several significant challenges. Retraining models for each hardware platform was time-consuming and restricted the portability of pre-trained GNNs across different devices. Additionally, the reliance on hardware-specific code made it difficult to transfer optimizations to new architectures without significant rework. Memory and computation efficiency were often suboptimal, as standard compression techniques did not fully exploit the inherent sparsity in GNN data, resulting in wasted resources on accelerators. This inefficiency was especially problematic for edge devices with limited computational power, leading to increased energy consumption and longer processing times. Furthermore, the irregular and input-dependent computation patterns of GNNs often resulted in inefficient acceleration on traditional CPUs, GPUs, and even specialized DNN accelerators like TPUs. This inefficiency caused higher inference latency compared to other types of neural networks, limiting their practical application to scenarios where inference could be precomputed offline. Moreover, the memory-intensive nature of GNNs posed a major bottleneck, as data movement between memory and processors became particularly challenging in resource-constrained environments~\cite{EnGN}.


\section{Background \& Motivation}\label{sec:background}

% \subsection{Execution of a GNN}
% Version #2
Understanding the execution of GNNs involves analyzing their core computational stages: \textit{Node Embedding}, \textit{Aggregation}, \textit{Combination}, and \textit{Decode}~\cite{raha_book_chapter}. Fig.~\ref{fig:GNN_agg_comb} demonstrates this process using a GCN~\cite{gcn} as an example.
The process begins with loading the graph structure and node embeddings via a data loader. Graph edges are typically represented as tuples of connected node indices. To enhance computational efficiency, the graph can be \textit{preprocessed} into a structured format, such as an adjacency matrix. This binary matrix indicates edge connections and includes self-loops to incorporate node-specific features. Additionally, a normalization matrix is derived from node degrees to ensure a balanced computation.
During the \textit{Node Embedding} stage, raw graph data is converted into feature vectors that serve as inputs to subsequent stages. The \textit{Aggregation} phase then collects features from neighboring nodes, leveraging operations such as pooling or reduction to capture relationships within the graph structure. However, this phase often incurs irregular memory access due to the variable number of neighbors. Next, the \textit{Combination} phase applies neural transformations, such as fully connected layers or attention mechanisms, to the aggregated features, producing higher-level representations. Finally, in the \textit{Decode} phase, these refined features are processed through layers like MLPs or SoftMax to generate predictions.
The Aggregation and Combination phases (main \textit{GNN compute}) are the most computationally intensive, as they are performed repeatedly throughout the model, emphasizing their critical role in GNN execution. This iterative nature underscores the need for efficient preprocessing and computational strategies to optimize performance.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Plots/Motivation_EffOp.png}% This is a *.eps file
\end{center}
\caption{Execution latency breakdown of GNN computation of a single GraphConv and GraphAttn layer (1433 input features and 64 output features) on Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU across operations~\cite{openvino_ops} for a graph with 1354 nodes and 5429 edges.}\label{fig:motivation_effop_grax}
\end{figure}

% The breakdown of execution time for a single GraphConv and GraphAttn layer mapped on the Intel Lunar Lake NPU (Fig.~\ref{fig:motivation_graphsplit}) reveals that preprocessing dominates the overall latency. Specifically, preprocessing contributes to ~55\% of the time in GraphAttn and ~99\% in GraphConv, with a significant portion of this time spent on control-heavy tasks, primarily executed on the DSP. This control-flow dominance in preprocessing poses a major bottleneck for efficient GNN execution on NPUs.
\textcolor{black}{Fig.~\ref{fig:motivation_graphsplit} presents the latency breakdown for a single GraphConv and GraphAttn layer mapped out-of-the-box on the Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPU. The breakdown highlights two major components: \textbf{graph preprocessing} and \textbf{GNN compute} (illustrated in Fig.~\ref{fig:GNN_agg_comb}), which includes operations such as combination and aggregation. Additionally, the figure provides a detailed view of how these components are distributed across the NPU's DPU and DSP units. It is evident from this breakdown that \textbf{preprocessing} plays a dominant role, contributing approximately 55\% of the execution time in GraphAttn and nearly 99\% in GraphConv. The preprocessing tasks, being control-flow heavy, are primarily executed on the DSP (relatively slower than DPU), further exacerbating the latency issue.
Addressing this control-flow challenge is critical for improving GNN performance. In particular, GraphSplit, which is introduced in Section~\ref{sec:Design}, is designed to mitigate this issue, optimizing preprocessing and enhancing overall execution efficiency. Fig.~\ref{fig:motivation_effop_grax} further highlights the breakdown of \textbf{GNN compute} operations across different units of the NPU, with GraphConv benefiting from efficient matrix multiplication (MatMul) on the DPU. While this operation suits NPUs well due to their strength in data-parallel tasks, GraphAttn still presents opportunities for improvement. In particular, around 30\% of the GNN compute execution time in GraphAttn is spent on operations such as Select, Greater, Softmax, and Elu, which are control-heavy and executed on the DSP. These control-flow-intensive sections are prime targets for optimization, which GraNNite addresses through \textit{EffOp}, as discussed in Section~\ref{sec:Design}.
Additionally, GNNs benefit from sparse input graphs and do not require full precision (FP32) for compute. This opens up further opportunities for optimization, where approximate methods can be deployed to reduce computation at the cost of minimal quality loss. GraNNite leverages these characteristics to enable high-speed GNN execution on NPUs, pushing the boundaries of real-time performance in edge environments.}

% Version #1
% Understanding GNN execution requires examining its key computational stages (see Fig.~\ref{fig:GNN_agg_comb}). GNNs operate through a sequence of steps~\cite{raha_book_chapter}: Node Embedding, Aggregation, Combination, and Decode. Initially, the Node Embedding step transforms raw graph data into feature vectors for processing. In the Aggregation phase, features from neighboring nodes (e.g., one-hop connections) are gathered using functions like pooling or reduction. This step captures graph relationships but introduces irregular memory access due to varying connections. Next, the Combination phase applies neural transformations, such as fully connected layers or attention mechanisms, to aggregated features, extracting higher-level representations. Finally, in the Decode phase, the model generates predictions by processing these features through layers like MLPs or Softmax.
% The Aggregation and Combination phases dominate execution time, with minimal overhead from Node Embedding. As illustrated in Fig.~\ref{fig:GNN_agg_comb}, Aggregation involves gathering neighbor features, which can grow exponentially in multi-hop strategies, increasing feature map dimensions. The Combination phase multiplies aggregated features by weight matrices, followed by ReLU activation, normalization, and pooling operations to refine feature representations. These stages highlight the transition from sparse to dense computations, which are central to GNN execution.


% Version-0
% Before delving into the specific challenges associated with executing GNNs on resource-constrained DNN accelerators, it is important to take a bird’s eye view of how these networks operate. 
% GNNs operate through a structured sequence of computation phases, beginning with sparse and irregular operations in the Aggregation phase and shifting to dense, regular computations in the Combination phase. For the remainder of this discussion, we will refer to a simplified version of the graph algorithm execution structure, as illustrated in Figure~\ref{fig:GNN_execution}. Initially, the Node Embedding step transforms input data into feature vectors, preparing raw graph data for further processing. The Aggregation phase follows, gathering features from neighboring nodes—typically one-hop connections—using functions like pooling or reducing mean, often enhanced by sampling techniques to improve model generalization. This phase captures relationships across the graph structure but results in irregular access patterns due to varying graph connections. Next, in the Combination phase, neural transformations are applied over these aggregated features to derive higher-level relationships, often using fully connected layers or attention mechanisms. Finally, in the Decode or Readout phase, the model outputs predictions by processing the derived features, transforming them into usable results through operators like MLPs and Softmax. This step-by-step evolution through GNN phases allows for a detailed understanding of each stage, from sparse data gathering to dense computations and final output generation.

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/GNN_execution.png}% This is a *.eps file
% \end{center}
% \caption{GNN execution flow~\cite{raha_book_chapter}}\label{fig:GNN_execution}
% \end{figure}

% From an execution time perspective, most of the compute time in a GNN execution is spent on the Aggregation and Combination stages, while significant overhead during the node embedding phase is less common. Figure~\ref{fig:GNN_agg_comb} illustrates the computation flow for the Aggregation and Combination phases of a simple bidirectional Graph Network with seven nodes.
% In the Aggregation phase, feature vectors from the immediate neighbors of a node, such as node 1 with three neighbors, are gathered to form a one-hop neighbor feature map. In practical applications, the number of neighboring nodes can be much higher, and some GNNs utilize multi-hop neighbor aggregation strategies, potentially leading to an exponential increase in feature map dimensions.
% In the subsequent Combination phase, the aggregated features are multiplied by a layer weight matrix followed by ReLU activation and normalization, as shown in Figure~\ref{fig:GNN_agg_comb}. This phase may also include a pooling operation, which refines the feature representation, consolidating information for further processing in the GNN architecture.



% The majority of GNNs explored in current research are composed of three primary layer types: Graph Convolution, Graph Attention, and Sample and Aggregate (SAGE) layers. Fig.~\ref{fig:GNNs} illustrates the first GNN architectures that introduced each of these foundational layers, alongside a summary of the distinct computational properties and structural advantages characteristic to each layer type.







% \subsection{Execution of GCN, GAT and GraphSAGE}
% Modern GNNs primarily utilize three foundational layer types: Graph Convolution, Graph Attention, and Sample and Aggregate (SAGE). These layers underpin architectures like GCNs~\cite{gcn}, GATs~\cite{gat}, and GraphSAGE~\cite{sage}, each addressing specific limitations of its predecessors (Fig.~\ref{fig:GNNs}).
% GCNs aggregate features by averaging those of immediate neighbors, capturing local structure but treating all neighbors equally, which can limit representation quality. GATs enhance this by using attention mechanisms to assign importance weights to neighbors, capturing more nuanced relationships. GraphSAGE addresses scalability by sampling a subset of neighbors for aggregation, enabling efficient processing of large graphs.
% Both GATs and GraphSAGE extend the GCN framework with attention and sampling, refining representations while balancing expressiveness and efficiency.
% We will explore how GraNNite enables and optimizes these layers for efficient execution on NPUs.


% Graph Convolutional Networks (GCNs)~\cite{gcn}, Graph Attention Networks (GATs)~\cite{gat}, and GraphSAGE~\cite{sage} represent significant advancements in the development of GNNs, each addressing specific limitations of its predecessors (refer Figure~\ref{fig:gcn_gat_sage}). GCNs~\cite{gcn} perform neighborhood aggregation by applying a convolutional operation that averages features from immediate neighbors, effectively capturing local graph structure. However, GCNs face challenges in handling graphs with varying node degrees, as they treat all neighbors equally during aggregation, which can lead to suboptimal feature representation. This limitation paved the way for GATs~\cite{gat}, which introduce an attention mechanism to assign different weights to neighboring nodes based on their importance, enabling the model to focus on more relevant features while aggregating. This attention-based aggregation allows GATs to capture more nuanced relationships between nodes, enhancing their expressiveness compared to GCNs. GraphSAGE~\cite{sage} (Graph Sample and Aggregation) was developed to address scalability issues in GCNs, particularly in large graphs where the entire neighborhood cannot be processed due to memory constraints. Instead of aggregating all neighbors, GraphSAGE employs a sampling strategy to collect a fixed-size subset of neighbors, facilitating efficient computation. In both GATs and GraphSAGE, the aggregation step collects features from the sampled or attended neighbors, while the combination step applies a learnable weight matrix and non-linear activation functions, similar to GCNs. This flexibility in aggregation and the introduction of attention mechanisms make GATs and GraphSAGE powerful tools for effectively learning from complex graph structures.

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/GCN_GAT_SAGE.png}% This is a *.eps file
% \end{center}
% \caption{GNN architectures: Aggregation and combination in GCN~\cite{gcn}, GAT~\cite{gat}, \& GraphSAGE~\cite{sage}}\label{fig:gcn_gat_sage}
% \end{figure}




