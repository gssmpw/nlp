\section{Experimental Methodology}\label{sec:expt_methodology}
% Version 3
As illustrated in Fig.~\ref{fig:expt_meth}, we evaluated GNNs on Intel\textregistered\ NPUs using the Cora dataset (2,708 nodes, 5,429 edges, 7 classes, 1,433 features) and Citeseer dataset (3,327 nodes, 4,732 edges, 6 classes, 3,703 features) for node classification. The benchmarked models included Graph Convolutional Networks, Graph Attention Networks, and GraphSAGE, achieving baseline Top-1 classification accuracies of 80.80\% (GCN), 81.30\% (GAT), 79.30\% (SAGE-max), and 75.50\% (SAGE-mean). We trained these models using PyTorch and PyTorch Geometric (PyG) with a learning rate of 0.01, weight decay of $5 \times 10^{-4}$, batch size of 64 for 100 epochs. After training, the models were converted to an OpenVINO  \cite{openvino} compatible format for execution on the NPU. Experiments were conducted on two systems: Intel\textregistered\ Core\texttrademark\ Ultra Series 2~\cite{lnl} (ASUS Zenbook S 14 with 16GB RAM, 256V NPU) and Intel\textregistered\ Core\texttrademark\ Ultra Series 1~\cite{mtl} (ASUS NUC 14 Pro with 16GB RAM, 165H NPU). We measured inference latency, throughput, and energy efficiency using OpenVINO’s \texttt{benchmark\_app} tool, configuring performance hints and input/output precision. For the NodePad technique, we augmented the Cora dataset by adding 292 nodes, making the static input size 3,000 nodes. In the GraphSAGE model, we limited the aggregation to a maximum of 10 randomly selected neighbor nodes. Energy consumption analysis was conducted using the HWINFO tool to assess the efficiency of the NPU in comparison to CPU and GPU implementations.
\textcolor{black}{All results were collected using public frameworks (OpenVINO, HWINFO, PyTorch) and can be replicated with the optimized models provided at the link (given in abstract).}


% Version 2
% The experiments evaluated Graph Neural Networks (GNNs) on Intel’s Neural Processing Unit (NPU) using the Cora (2,708 nodes, 5,429 edges, 7 classes, 1,433 features) and Citeseer (3,327 nodes, 4,732 edges, 6 classes, 3,703 features) datasets for node classification. Models tested included Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE, with baseline top-1 classification accuracies of 80.80\% (GCN), 81.30\% (GAT), 79.30\% (SAGE-max), and 75.50\% (SAGE-mean). Training was performed using PyTorch and PyTorch Geometric (PyG) with a learning rate of 0.01, weight decay of $5 \times 10^{-4}$, batch size of 64, and 100 epochs. Trained models were converted to OpenVINO format for optimized NPU execution. The experiments were conducted on two systems: Lunar Lake (ASUS Zenbook S 14, 16GB RAM, Intel Core Ultra 7, 256V NPU) and Meteor Lake (ASUS NUC 14 Pro, 16GB RAM, Intel Core Ultra 165H, 256V NPU). Inference latency, throughput, and energy efficiency were measured using OpenVINO’s \texttt{benchmark\_app} tool with performance hints and input/output precision settings. For the NodePad technique, 292 additional nodes were added to the Cora dataset, making the static input size 3,000 nodes. In GraphSAGE, a maximum of 10 randomly selected neighbor nodes were used during aggregation. Energy consumption was analyzed to compare the efficiency of the NPU against CPU and GPU implementations.


% The experimental methodology for deploying Graph Neural Networks (GNNs) on Intel’s Neural Processing Unit (NPU) followed a systematic and detailed approach that leveraged popular datasets and widely recognized GNN models. The primary datasets utilized in this study were the Karate Club, which consists of 34 nodes and 78 edges, and the Cora dataset, featuring 2,708 nodes and 5,429 edges. These datasets are commonly used for benchmark testing in node classification tasks, allowing for a standardized evaluation of model performance. The focus of the experiments was on implementing GCN and GAT architectures, both known for their efficiency and effectiveness in handling graph data.
% Training hyperparameters were reused from existing literatures to ensure optimal performance. The learning rate was set to 0.01, weight decay was adjusted to 5e-4, and a batch size of 64 was used during the training process. The experiments were conducted on Intel’s Meteor Lake and Lunar Lake machines, equipped with cutting-edge hardware designed to maximize the capabilities of deep learning accelerators. These systems provided the necessary computational power and memory bandwidth to facilitate efficient GNN training and inference.
% The experimental workflow involved several key steps. Initially, the GNN models were trained using the PyTorch framework in conjunction with PyTorch Geometric (PyG) for graph-specific operations. Once training was completed, the models were converted into an OpenVINO-compatible format to enable optimized execution on the NPU. During this conversion, specific optimizations were applied to the model graph to enhance runtime efficiency, including reducing computation and memory requirements. 
% Following the conversion, the optimized models were deployed on the NPU, where their performance was rigorously benchmarked. The benchmarking process involved utilizing a range of available tools, with a focus on measuring inference latency, throughput, and overall efficiency. Performance traces were generated using Perfetto to facilitate detailed debugging and analysis of the execution process. Additionally, NPU compiler flags were strategically selected to fine-tune the model execution, maximizing performance while minimizing resource consumption.
% Throughout the experimentation, various optimization techniques were implemented to improve the GNN's execution on the NPU. These included employing memory-efficient graph representations to take advantage of the inherent sparsity of the data, as well as leveraging hardware-specific features of the NPU to accelerate computations. By carefully orchestrating each step of the methodology, the study aimed to demonstrate the effectiveness of deploying GNNs on specialized hardware while addressing the challenges associated with efficiency and performance. The comprehensive approach not only highlights the potential of GNNs in practical applications but also underscores the importance of tailored methodologies for optimizing deep learning models in resource-constrained environments.

% Experiments with Graph Neural Networks (GNNs) on Intel’s Neural Processing Unit (NPU) utilized the Cora dataset (2,708 nodes, 5,429 edges) to benchmark Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE for node classification (see Figure~\ref{fig:expt_meth}). Training used standard parameters: learning rate of 0.01, weight decay of 5e-4, and batch size of 64. Models were trained with PyTorch and PyTorch Geometric (PyG) dataloader, then converted to an OpenVINO-compatible format for NPU-optimized execution. Intel Lunar Lake systems were used to measure inference latency, throughput, and energy efficiency, with NPU compiler flags fine-tuning performance. Key optimizations leveraged memory-efficient graph representations that exploit data sparsity and hardware features to enhance computational efficiency. 
% HWINFO tracked power metrics, including "CPU Package Power," "IA Cores Power," "DRAM Power," and "System Agent Power," the latter capturing NPU power consumption.


% \end{wrapfigure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Plots/Result_progression_v5.png}% This is a *.eps file
\end{center}
\caption{Progressive performance improvement of GNN through different GraNNite optimizations.}\label{plot:gnn_progression}
\end{figure}



