\section{GraNNite Design Methodology}\label{sec:Design}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth]{Figures/End-to-end.pdf}% This is a *.eps file
\end{center}
\caption{End-to-end GraNNite methodology to efficiently enable GNNs on NPUs through model partitioning and optimizations.}\label{fig:end_to_end}
\end{figure}

GraNNite provides an end-to-end framework (as shown in Fig.~\ref{fig:end_to_end}) for deploying pre-trained GNNs on NPUs without retraining.
% The methodology ensures efficient execution while preserving model performance by partitioning inference tasks: control-intensive operations like graph preprocessing are handled by the CPU, while parallelizable GNN computations run on the NPU, reducing latency (Fig.~\ref{fig:end_to_end}).
% Before converting models to a uniform model graph (a.k.a. intermediate representation, IR), software optimizations like node padding and mapping DSP-heavy tasks to the DPU are applied. The NPU compiler then refines execution through techniques such as sparse storage, pipelined operation fusion, and INT8 quantization, which improves performance per watt without sacrificing accuracy. These integrated optimizations create a streamlined, high-performance workflow for executing GNNs on specialized hardware.
We consider an output-stationary NPU architecture inspired by Ref.~\cite{flexnn}. The core component is the DPU, an $M\times M$ grid of Versatile Processing Elements,  each comprising an $N\times N$ array of MAC Processing Elements (MPEs) designed for efficient Multiply-and-Accumulate (MAC) operations. This DPU is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations.
The architecture includes a local SRAM for storing activations and weights, a tensor distribution network for data flow to and from the DPU, and control logic for managing computation, accumulation, and output extraction. MAC operations, integral to DNNs, calculate dot products of weights and activations to produce output feature maps. Each MPE leverages a local data path with register files, multipliers, and accumulators to perform these tasks. Additionally, a DSP handles non-linear activation functions and control-flow operations, complementing the data-parallel DPU.
\textcolor{black}{Although our case study considers an output-stationary NPU architecture, the proposed techniques are generic and can be applied to other NPUs without loss of generality.}
\textcolor{black}{
GraNNite proposes a \textit{generic step-by-step methodology (Fig.~\ref{fig:GraNNite_tech})} to optimize emerging neural networks on existing AI accelerators. While demonstrated on GNNs using FlexNN-like~\cite{flexnn} NPUs, the methodology is generalizable to other models and hardware platforms. It consists of three key steps:
\textbf{(1) Enabling the Model on the NPU.}
This step ensures the model runs efficiently on the NPU while maintaining flexibility. For GNNs, GraNNite introduces workload partitioning (\textit{GraphSplit}), precomputed static graph processing (\textit{StaGr}), and dynamic graph handling (\textit{GrAd} and \textit{NodePad}) to support real-time updates and adaptive memory management. These techniques enable execution with minimal overhead.
\textbf{(2) Optimizing GNN Performance.}
Once enabled, the model undergoes further optimizations to maximize efficiency without degrading accuracy. \textit{EffOp} accelerates execution and reduces memory bandwidth usage, while \textit{PreG}, \textit{SymG}, and \textit{CacheG} optimize memory access for Graph Convolution layers. \textit{GraSp} exploits sparsity to lower memory and compute costs, improving throughput and energy efficiency.
\textbf{(3) Trading Accuracy for Performance and Energy Gains.}
For applications prioritizing speed and efficiency over quality, GraNNite offers \textit{QuantGr} for INT8 quantization and approximation techniques (\textit{GrAx1}, \textit{GrAx2}, \textit{GrAx3}) to further enhance throughput with minimal quality loss.
These steps provide a systematic framework for deploying GNNs efficiently on NPUs, addressing resource constraints while ensuring scalability, performance, and energy efficiency.
}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GraNNite_techniques_2.pdf}% This is a *.eps file
\end{center}
\caption{Suite of GraNNite Optimization Techniques for Efficient GNN Inference on NPUs.}\label{fig:GraNNite_tech}
\end{figure}

% Figure~\ref{fig:end_to_end} illustrates a comprehensive, end-to-end strategy for efficient GNN deployment, requiring no modifications to the existing NPU hardware. This streamlined approach maximizes compatibility and performance by fully utilizing the NPU’s capabilities with optimized software and compiler techniques, enabling effective execution of GNNs without altering hardware design.

% \subsection{Partitioned GNN Inference and Optimized Execution of Control-Heavy Tasks}
% \subsection{GraphSplit: Partitioned GNN inference between CPU and NPU to minimize inference latency}
\subsection{Step-1: Enabling GNNs on the NPU}
% \subsubsection{GraphSplit}
\textbf{GraphSplit:} To enable efficient execution of GNNs on NPUs, the first challenge is to address the mismatch between the hardware's strengths and the computational demands of graph-based workloads. NPUs excel at data-parallel tasks like matrix multiplications in neural networks, but are less efficient for control-heavy tasks involving frequent decision-making. CPUs, on the other hand, excel at these control-intensive tasks, using techniques such as predictive execution and out-of-order processing to maximize instruction-level parallelism.
Given these contrasting strengths, one might assume it’s best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig.~\ref{fig:motivation_graphsplit}) for aggregation or calculating intermediate attention scores, to the CPU. However, a challenge arises when control-flow tasks exhibit a Read-after-Write (RAW) dependency on previous data-parallel tasks, necessitating the transfer of data back to the CPU. This results in considerable communication overhead.
To overcome this, GraNNite introduces an offline profiling phase during model calibration. In this phase, we build a cost model that measures real-time latencies of various operations on both the CPU and NPU. This cost model also factors in the overhead from data transfer and communication between the CPU and NPU. Using this information, \textbf{GraphSplit} identifies the most effective partition points to minimize communication and latency.
GraphSplit’s partitioning strategy is designed to play to the strengths of each processing unit. Control-flow tasks, which require complex decision-making, are assigned to the CPU. Computationally heavy, data-parallel tasks, such as matrix multiplications, are sent to the NPU. This careful distribution improves graph processing performance by reducing the need for frequent data exchanges. For example, offloading initial input preprocessing to the CPU requires minimal communication with the NPU, resulting in better performance.
As shown in Fig.~\ref{fig:GraphSplit}, this partitioned inference setup for models such as GCN, GAT, and GraphSAGE effectively balances workload between CPU and NPU.
% , achieving efficient task execution and reducing latency across the entire model.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Graph_partitioning_v2.png}% This is a *.eps file
\end{center}
\caption{GraphSplit, partitioned GNN inference using CPU and NPU: CPU handles graph preprocessing; NPU accelerates data-parallel GNN computation.}\label{fig:GraphSplit}
\end{figure}

% \subsubsection{StaGr}
\textbf{StaGr:} For applications involving static graph structures,
% , such as monitoring systems in computer hardware where components remain fixed but their utilization or status fluctuates, 
GraNNite proposes an efficient methodology (\textbf{StaGr}) for implementing GNNs on hardware accelerators. Using a precomputed mask tailored to a fixed input graph, StaGr transforms the aggregation of node features in Graph Convolution into a streamlined matrix multiplication operation (refer to GCN in Fig.~\ref{fig:StaGr}), fully utilizing the capabilities of the NPU.
This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications. For Graph Attention and GraphSAGE, GraNNite leverages precomputed masks—an attention mask for efficient attention score calculation and a sampled adjacency matrix for reuse during inference (see Fig.~\ref{fig:StaGr}).
This methodology achieves highly efficient inference, minimizing computational overhead and latency while optimizing NPU performance under fixed-structure conditions.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/StaGr.png}% This is a *.eps file
\end{center}
\caption{StaGr: execution of GNNs on a static graph structure with dynamic node features.}\label{fig:StaGr}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Dynamic_nature_of_graph.png}% This is a *.eps file
\end{center}
\caption{One of the challenges to efficiently enable GNNs on NPUs: Dynamic input graph (An example of on-device knowledge graph).}\label{fig:dyn_graph}
\end{figure}


% \subsubsection{GrAd \& NodePad}
\textbf{GrAd \& NodePad:} To handle dynamic input graphs (refer Fig.~\ref{fig:dyn_graph}), GraNNite proposes a new approach (\textbf{GrAd}) that uses a mask as input rather than a precomputed weight, allowing dynamic updates to edges without the need to recompile the model.
Real-time graphs often undergo structural changes with nodes and edges dynamically added or removed (Fig.~\ref{fig:dyn_graph}). However, NPUs typically support static input shapes, as DNN models are precompiled for fixed input shapes, with optimizations such as tiling based on corresponding input configuration. This limitation requires recompilation when the input graph shape changes.
% NodePad vs. Batching
% One could argue that compiling the model for a static input shape and processing graphs in mini-batches—feeding portions of the graph to the GNN for inference one at a time—might be a viable approach. However, this method risks information loss, as edges connecting nodes outside the current subgraph are excluded during minibatch inference. Additionally, determining an optimal batch size for dynamic graphs poses significant challenges; a poorly chosen size can result in underutilized NPU resources. Consequently, for small input graphs, it is more efficient and accurate to process the entire graph at once. For extremely large graphs that exceed the NPU's capacity, minibatching becomes necessary despite its trade-offs.
Compiling the model for a static input shape and using mini-batches for inference may seem viable, but risks information loss by excluding edges connecting nodes outside the subgraph. Additionally, selecting an optimal batch size is challenging and may lead to underutilized NPU resources.
% While processing the entire graph is more efficient for small graphs, mini-batching becomes necessary for extremely large graphs that exceed NPU capacity, despite its trade-offs.
Our approach introduces a node-padding technique (\textbf{NodePad}) that compiles the entire model with a higher node capacity than immediately needed for the whole input graph. For smaller graphs, embeddings for unused nodes are zero-padded, while absent edges are represented by zeroes in the adjacency matrix, following the conventional interpretation of ``0" as no edge and ``1" as an active connection. This node padding strategy minimizes the need for frequent recompilation and eliminates the need to store multiple precompiled model versions for different graph sizes. Fig.~\ref{fig:GrAd_NodePad} illustrates how a GNN with GraphConv layers can handle a time-varying input graph on an NPU. This approach applies zero padding to the input features and utilizes a ``norm" matrix (mask), precomputed on the CPU, which is then fed into the main GNN computation on the NPU. By dynamically updating the mask at runtime, GrAd and NodePad allow the GNN to efficiently adapt to evolving graph structures. These techniques significantly improve performance and energy efficiency of GNN inference by reducing the overhead tied to model recompilation.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Node_padding.png}% This is a *.eps file
\end{center}
\caption{GrAd + NodePad: Dynamic input graph support for GNNs via node padding: Eliminates multiple precompiled blobs, saving memory and removing the need for frequent recompilation with varying input node counts.}\label{fig:GrAd_NodePad}
\end{figure}


% \subsection{EffOp: Efficient execution of Control-Heavy Operations on NPUs}
% GraphSplit assigns control-flow-heavy graph preprocessing tasks to the CPU and data-parallel tasks to the NPU, optimizing the strengths of each unit. 
% NPUs are primarily equipped with data-parallel Multiply-Accumulate (MAC) units, arranged to handle intensive, parallelized tasks efficiently. This high-performance array, known as the DPU, is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations. 
% Example of control-heavy tasks—such residing deep within GNNs include conditional logic, selection, or gathering operations, they are typically assigned to the NPU’s DSP, which is specifically designed for such tasks. Unfortunately, the DSP operates at a lower frequency than the DPU, which can create bottlenecks and increase overall latency, especially in deep, sequential sections of GNNs. 

\subsection{Step-2: Optimizing GNN Performance on NPU}
% \subsubsection{EffOp}
\textbf{EffOp:} After enabling GNNs on the NPU, the next challenge lies in optimizing their performance without compromising application quality. A significant bottleneck arises from the control-heavy operations, such as conditional logic, \textit{Select}, or \textit{Gather}, residing deep in the GNNs being executed on the DSP within the NPU (as shown in Fig.~\ref{fig:motivation_effop_grax}). 
% Control-heavy tasks in GNNs, such as conditional logic, \textit{Select}, or \textit{Gather}, are handled by the NPU's DSP. 
The DSP is designed for these operations, but runs at a lower frequency than the DPU. This difference often causes bottlenecks and increases latency in deep, sequential GNN sections.
% To address this limitation, GraNNite introduces \textbf{EffOp}, a novel approach to optimize these operations during GNN model computation on NPUs.
To address this limitation, GraNNite proposes a novel approach, \textbf{EffOp}, that converts these control-heavy operations into equivalent data-parallel tasks, allowing them to be executed on the faster DPU rather than the DSP. The core idea is to restructure sequential tasks, such as Select and Gather, to be processed as simple, elementwise/reduction operations on the DPU. By redefining these tasks using operations like multiplication and addition, combined with precomputed masks, we transform inherently sequential processes into parallel-friendly ones. This allows the DPU to handle tasks that would traditionally rely on the slower DSP, reducing the need for sequential processing and, consequently, lowering overall execution time.
As shown in Fig.~\ref{fig:EffOp}, this method is particularly beneficial for operations in Graph Attention Networks, specifically in sections where intermediate attention scores are computed. EffOp demonstrates how this computation can be achieved using elementwise multiplication, followed by elementwise addition with a slightly modified ``connectivity mask." In EffOp, tasks that typically involve complex control logic are optimized to utilize the DPU’s strengths, transforming them into matrix and elementwise operations that can be efficiently parallelized.
% For instance, as shown in Fig.~\ref{fig:EffOp}, this method is particularly beneficial for operations found in Graph Attention Networks (GAT). Here we observe model section involving the intermediate attention score computation. EffOp demonstrates how the output of the model section can be achieved using elementwise multiplication, followed by elementwise addition with a slightly different mask the initial ``connectivity mask". In EffOp, tasks that would typically involve complex control logic are optimized to leverage the DPU’s strengths, converting them into matrix and elementwise operations that are easily parallelized. 
% GraphConv layers, on the other hand, are naturally aligned with matrix multiplication tasks and do not require additional reconfiguration, as they lack the type of internal control-heavy operations seen in GAT and GraphSAGE models.
% This reallocation of tasks to the DPU takes full advantage of its speed and parallelism, resulting in a significant reduction in inference latency. By minimizing reliance on sequential control flows and maximizing the DPU’s data-parallel efficiency, this approach enhances both computational performance and energy efficiency. This technique capitalizes on the DPU’s high throughput while maintaining the functionality of the GNN, making it a powerful solution for real-time applications where reduced latency and optimized resource use are crucial.


% \subsection{Efficient GNN Inference for Static and Dynamic Graphs}
% \subsection{StaGr: Efficient Implementation of GNNs for Static Input Graphs}


% GraNNite proposes an efficient methodology (StaGr) for implementing GNNs on hardware accelerators when using static input graphs. By leveraging a precomputed mask tailored to a fixed input graph, it transforms the aggregation of node features in Graph Convolution into a streamlined matrix multiplication operation (refer to GCN in Fig.~\ref{fig:StaGr}), fully utilizing the NPU’s optimized capabilities. This precomputed mask establishes the necessary node connections beforehand, significantly reducing irregular memory accesses and improving memory latency, inference speed, and energy efficiency—all without extensive hardware modifications. For Graph Attention, GraNNite suggests using a precomputed attention mask, saved as a model parameter, to facilitate efficient calculation of intermediate attention scores with basic addition operations (see GAT in Fig.~\ref{fig:StaGr}). Similarly, in GNNs that implement the sample and aggregate approach, like GraphSAGE, a sampled adjacency matrix can be saved as a parameter and reused for aggregation during inference (refer to GraphSAGE in Fig.~\ref{fig:StaGr}).
% This approach is particularly beneficial when the graph structure is static and only node characteristics vary over time, such as in monitoring applications for systems like computer hardware, where components remain fixed, but their utilization or status fluctuates. By leveraging this technique, highly efficient inference is achieved, significantly reducing computational overhead and latency and delivering optimal performance on NPUs under fixed-structure conditions.




% \subsection{GrAd \& NodePad: Handling time-varying input graphs using dynamic mask and compiled model with padded nodes}
% In the previous section, we introduced an innovative GNN inference method (StaGr) tailored to static input graphs. 






\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/EffOp.pdf}% This is a *.eps file
\end{center}
\caption{Effop, efficient GNN computation by substituting control-intensive DSP operations with equivalent DPU operations: Utilizing the DPU’s higher frequency and increased parallel compute units reduces end-to-end latency.}\label{fig:EffOp}
\end{figure}






% \begin{figure}[t!]
% % \begin{wrapfigure}{r}{0.4\textwidth}
% \begin{center}
% \includegraphics[width=0.4\columnwidth]{Plots/GraSp.png}% This is a *.eps file
% \end{center}
% \caption{Normalized throughput and latency comparison between the baseline GCN and GCN with GraSp optimization. An 11\% increase in throughput highlights the impact and effectiveness of the optimization}\label{fig:GraSp_result}
% \end{figure}






% \begin{wrapfigure}{r}{0.2\columnwidth}
%     \centering
%     \includegraphics[width=0.18\columnwidth]{Plots/GraSp.png}
%     \caption{Normalized throughput and latency comparison between the baseline GCN and GCN with GraSp optimization. An 11\% increase in throughput highlights the impact and effectiveness of the optimization}\label{fig:GraSp_result}
% \end{wrapfigure}


% \subsection{VerGe: Vertical Fusion Optimization for Concurrent Execution}
% GraNNite introduces a vertical fusion optimization technique (VerGe) to enable efficient overlap of operations between the DPU and DSP units in NPUs. In Graph Neural Networks (GNNs), each layer typically followed by an activation function; for instance, the final linear layer is often followed by a Softmax function. To leverage this pattern, VerGe introduces a vertical fusion approach where linear transformations, matrix multiplications (MatMul), and element-wise operations are executed by the high-throughput DPU, while functions like Softmax are simultaneously assigned to the DSP unit, which handles control-heavy tasks.
% In VerGe, as each block of data is processed by the DPU (with block size determined by the NPU compiler), it is immediately handed off to the DSP unit to compute the subsequent operation, such as an activation function. By pipelining tasks this way, the DPU and DSP can work in tandem: the DPU continues processing new data for linear and MatMul operations while the DSP simultaneously handles activation functions like Softmax on previous blocks. This concurrent execution reduces overall inference latency and enhances throughput, allowing the NPU to achieve superior performance without requiring hardware changes.
% Fig.~\ref{fig:VerGe} demonstrates how vertical fusion reduces the end-to-end latency of a MatMul-Softmax block by overlapping execution through the independent compute units (DPU and DSP) in the NPU. The result is a streamlined inference process that maximizes resource utilization within the NPU, significantly boosting speed and efficiency. This method also makes it possible to capitalize on the distinct strengths of the DPU and DSP, enhancing the NPU’s capacity to handle complex neural network workloads effectively.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.98\columnwidth]{Figures/GraSp.png}% This is a *.eps file
\end{center}
\caption{GraSp, exploiting input graph sparsity for faster execution: zero elements in node embeddings and adjacency matrices are compressed (ZVC), and a sparsity bitmap is used to bypass computation.}\label{fig:GraSp}
\end{figure}

% Figure~\ref{fig:QuantGr} highlights the impact of QuantGr, showing that INT8 precision can yield up to a 4X improvement in performance per watt over FP16. Furthermore, with minimal impact on model generalizability, precision can be reduced to 4 bits, delivering a 16X performance per watt boost—demonstrating the potential of low-precision execution to enhance NPU efficiency for GNN workloads.


% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/Quantization.png}% This is a *.eps file
% \end{center}
% \caption{QuantGr: Low-precision GNN inference on the DNA's INT8 engine delivers a 4X performance-per-watt increase while maintaining accuracy within acceptable limits}\label{fig:QuantGr}
% \end{figure}

% \subsubsection{GraSp}
\textbf{GraSp:} In the context of GNN optimization on NPUs, activation sparsity offers a powerful mechanism to significantly boost performance by skipping unnecessary calculations. Given that input graphs are often highly sparse, with up to 99\% of values being zero, GraNNite leverages this sparsity to optimize both memory usage and computational efficiency. The adjacency matrix in real-world graphs typically exhibits this extreme sparsity, containing many zero-valued entries where no direct connection exists between nodes. By capitalizing on this inherent sparsity, NPUs~\cite{lnl, mtl} can streamline computations by skipping zero values, reducing the workload without affecting the accuracy of model inference.
To efficiently manage these sparse values, GraNNite proposes \textbf{GraSp}, which utilizes a storage format known as Zero Value Compression (ZVC)~\cite{zvc}. In this approach, only the non-zero values in the input graphs are stored explicitly, while the zero values are omitted, allowing the system to allocate memory and computational resources effectively. For GraSp implementation, sparsity bitmaps are used alongside compressed data to denote the locations of non-zero values within the matrix. This bitmap directs the NPU to focus solely on meaningful data while bypassing zero entries.
% By skipping the processing of zeros, GraSp can drastically reduce memory access frequency, cut down on data traffic, and save on storage—all of which contribute to faster and more efficient inference. 
Fig.~\ref{fig:GraSp} illustrates how sparsity bitmaps are integrated within the NPU’s processing pipeline to skip zero-valued computations leading to latency speedup.






% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/Vertical_fusion.png}% This is a *.eps file
% \end{center}
% \caption{VerGe: Vertical Fusion of Operations: Pipelined execution of MatMul and Softmax operations across DPU and DSP compute units within an NPU}\label{fig:VerGe}
% \end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GCN_efficient_compute.png}% This is a *.eps file
\end{center}
\caption{PreG: GraphConv normalization factors rely only on the graph structure, enabling precomputation and bypassing costly square-root and division operation on the NPU's slower DSP units.}\label{fig:PreG}
\end{figure}



\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Symmetric_adj_mat.png}% This is a *.eps file
\end{center}
\caption{SymG + CacheG: Symmetric “norm” matrix in GraphConv layers is reused across all layers, allowing partial storage for memory savings and increased reuse.}\label{fig:SymG_CacheG}
\end{figure}

% \subsection{GraphConv Acceleration on NPUs: Leveraging Precomputation, Symmetry, and Caching}
% \subsection{PreG: Streamlined Computation for GNNs with Graph Convolution (GraphConv) Layers}
% \subsubsection{PreG, SymG \& CacheG}
\textbf{PreG, SymG \& CacheG:} GraNNite presents a streamlined approach tailored for GNNs that use GraphConv layers as core components. \textbf{PreG} leverages a precomputed, constant normalization matrix to accelerate processing. Since GraphConv is foundational and commonly used in more advanced GNN architectures, this enhancement offers broad applicability and efficiency gains. 
In GraphConv, the normalization factors required after neighborhood aggregation depend solely on the degrees of neighboring nodes, not on their specific features. Here, a node's degree represents the count of its neighboring nodes, including itself. By exploiting this feature independence, PreG precomputes these normalization factors once, storing them in a constant matrix (refer Fig.~\ref{fig:PreG}). By precomputing the normalization matrix on the CPU, the aggregation and normalization steps are combined into a single matrix multiplication. This approach leverages the NPU’s strength in matrix multiplication while avoiding the slower DSP unit, which typically handles division, thus streamlining execution and optimizing performance. 
% This allows the aggregation and normalization steps to be combined into a single matrix multiplication, streamlining execution. This approach aligns well with the NPU’s architecture, as NPUs are not optimized for division, which is typically handled by the slower DSP unit. Precomputing the normalization matrix on the CPU converts the aggregation process into a simple matrix multiplication with node features, a task for which the NPU is highly optimized. 
% By matching this process to the NPU’s strengths, this method enhances execution efficiency, leading to marked performance improvements without requiring changes to the underlying accelerator.
% \subsection{SymG \& CacheG: Memory-Efficient Technique for Adjacency Matrix Storage for GraphConv}
In addition, GraNNite introduces a memory-efficient technique (\textbf{SymG}) that capitalizes on the symmetry of the normalization matrix (see Fig.~\ref{fig:SymG_CacheG}) in GraphConv layers, allowing only half of the adjacency matrix and its diagonal elements to be stored. This optimization effectively reduces memory complexity, translating into substantial savings in memory usage. SymG also minimizes memory traffic, especially during Direct Memory Access (DMA) transfers from DRAM to NPU’s local memory, which can be a bottleneck.
Finally, GraNNite introduces \textbf{CacheG} that caches the constant normalization matrix within the NPU’s local memory and \textit{reuses it across all GraphConv layers in the model}, significantly reducing memory access overhead. This caching strategy not only boosts runtime efficiency, but also lowers inference latency, making the overall execution of GNNs on the NPU more streamlined and resource-efficient. Fig.~\ref{fig:SymG_CacheG} demonstrates the portion of the normalization matrix stored in memory and illustrates how it is cached within the NPU’s local memory.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GAT_approx_1.png}% This is a *.eps file
\end{center}
\caption{GrAx1, GraphAttn approximation 1: Removing compute-intensive multiplications boosts performance while preserving accuracy.}\label{fig:GrAx1}
\end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.7\columnwidth]{Figures/GAT_approx_2.png}% This is a *.eps file
\end{center}
\caption{GrAx2, GraphAttn approximation 2: Removing a transpose and a broadcast operation reduces execution latency with negligible quality loss.}\label{fig:GrAx2}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GrAx3.pdf}% This is a *.eps file
\end{center}
\caption{GrAx3, SAGE-max approximation: Replaces sequential DSP operations in SAGE-max aggregation with parallel element-wise multiplication and max pooling on the DPU, improving efficiency while maintaining correct feature aggregation for non-negative values.}\label{fig:GrAx3}
\end{figure}




% \subsection{Approximation-Based Optimizations for GNN Processing on NPU}
% \subsection{GrAx1: Optimizations for GNNs using Graph Attention: Elimination of Element-Wise Multiplication for Intermediate Attention Score Computation}
\subsection{Step-3: Trading Accuracy for Performance \& Energy Gains}
\textbf{QuantGr:} In the pursuit of optimizing GNN performance and energy efficiency, we first explore traditional methods of reducing model precision before introducing GraNNite’s novel approach. \textbf{QuantGr}, a quantization technique for GNNs is integrated in GraNNite that reduces numerical precision to achieve significant performance gains while preserving model accuracy. NPUs, typically designed with low-precision capabilities, support both INT8 and FP16 datapaths, allowing notable performance gains over traditional FP32 computation. Specifically, INT8 precision provides a $2\times$ boost in performance (TOPs) and a $4\times$ improvement in performance per watt (TOPs/Watt) compared to FP16. By carefully configuring the quantization parameters, such as setting an optimal zero point and scale, QuantGr can achieve competitive accuracy at lower precision. QuantGr uses symmetric, static quantization, meaning both weights and activations are quantized around a zero point, with equal scaling factors for positive and negative values. Static quantization, which precomputes scaling and zero-point parameters during model calibration, enables consistent and faster inference, as these values remain fixed throughout execution. Symmetric quantization simplifies processing by ensuring consistent scaling and compatibility across all hardware layers, minimizing conversion overhead. Leveraging the NPU’s support for static quantization of activations and weights, this approach unlocks higher efficiency for GNNs, making it well-suited for performance-sensitive, resource-limited environments.

% \subsubsection{GrAx1, GrAx2 \& GrAx3}
\textbf{GrAx1:} Application of all previously discussed techniques in GraNNite can significantly improve GNN performance and energy efficiency on NPUs compared to the initial out-of-the-box implementation. However, further improvements can be achieved through approximate computing. This approach trades off minimal DNN accuracy for better computational efficiency, enabling faster processing and reduced resource usage \cite{axis_tecs, drax}.
GNNs with Graph Attention (such as GAT) are well-regarded for their ability to generate attention maps that assign varying importance to nodes within a graph. However, these networks face significant computational challenges, particularly in managing non-existent edges. To prevent these edges from influencing the final attention values, they are typically masked by assigning them a large negative number before being processed through a SoftMax function. This masking step effectively ensures that attention coefficients for non-existent edges are rendered negligible during the aggregation phase.
% To improve the efficiency of this process in hardware implementations, GraNNite introduces a hardware-friendly optimization (\textbf{GrAx1}) focused on the operations carried out by the DPU.
In GAT implementation with EffOp, an element-wise multiplication is performed between the attention map and the mask to eliminate the influence of non-existent edges. However, this multiplication is computationally intensive and not well-suited for the DPU.
To mitigate this inefficiency, GraNNite proposes a novel approximation technique (\textbf{GrAx1}). Instead of multiplying the attention map by the mask, it suggests directly adding a large negative value to the positions in the attention map that correspond to non-existent edges. This modification effectively bypasses the multiplication step (as shown in Fig.~\ref{fig:GrAx1}), leading to a substantial reduction in computational burden on the DPU. As a result, throughput is increased without sacrificing the final attention map quality. 
% By eliminating the unnecessary multiplications, this optimization enhances the hardware's efficiency in processing attention maps, ultimately improving the performance of GAT implementations on NPUs.

% \subsection{GrAx2: Optimizations for GNNs using Graph Attention: Replacement of Broadcast-Add Operation on DPU}
\textbf{GrAx2:} Another significant bottleneck in GAT arises during the broadcast-add operation (refer Fig.~\ref{fig:motivation_effop_grax}), which is essential for calculating the intermediate attention map. The traditional implementation of the ``broadcast-add" operation requires adding the same value to multiple nodes, a process that involves broadcasting and transposing the data (refer left of Fig.~\ref{fig:GrAx2}). This step can lead to inefficiencies when executed on the DPU, hindering overall performance.
To address this inefficiency, GraNNite proposes another novel approximate solution (\textbf{GrAx2}) that replaces the conventional ``broadcast-add" operation with just an addition followed by broadcasting. As shown in Fig.~\ref{fig:GrAx2}, this approach eliminates one transpose and one broadcast operation, significantly reducing the computational load for addition and minimizing memory copy/reference operations, which enables the DPU to execute it more efficiently.
% By simplifying this process, the DPU can lower inference latency and allocate resources more effectively for other tasks. This optimization not only enhances the speed of GAT computations but also contributes to improved overall efficiency in processing graphs on NPUs.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/Experimental_methodology_2.pdf}% This is a *.eps file
\end{center}
\caption{Experimental setup for GNN evaluation on Intel AI PCs.}\label{fig:expt_meth}
\end{figure}

% \textcolor{black}{For SAGE-max, we can approximate this model section (shown in figure) as elementwise multiplication followed by max pooling.}
\textbf{GrAx3:} For GNNs using the ``Sample and Aggregate" (SAGE) layers with a ``max" aggregation strategy, the feature selection for each neighborhood is traditionally processed sequentially on the DSP, leading to inefficiency. \textbf{GrAx3} replaces this sequential operation with parallel element-wise multiplication using a mask (sampled adjacency matrix), followed by max pooling on the DPU. As shown in Fig.~\ref{fig:GrAx3}, GrAx3 simplifies the aggregation process, ensuring the correct aggregation of neighborhood features for most cases where feature values are greater than 0.
% This optimization speeds up the computation by exploiting parallelism.


% \subsection{GrAx3: Optimizations for GNNs using SAGE Layers: Parallelization of Neighborhood Feature Selection on DPU}
% GrAx3, introduced by GraNNite, leverages the abundant data parallelism present in the DPU to optimize the computation for GNNs using the "Sample and Aggregate" (SAGE) layers with a "max" aggregation strategy. Traditionally, the feature selection for each neighborhood in the SAGE layer is sequentially processed on the DSP, which can be slow and inefficient. GraNNite replaces this sequential operation with a parallel element-wise multiplication using a mask on the DPU, effectively speeding up the computation by processing multiple features simultaneously. This parallelization reduces inference latency and boosts throughput, leading to significant improvements in energy efficiency. As demonstrated in Fig.~\ref{fig:EffOp}, the GrAx3 optimization simplifies the computation by ensuring that the aggregated neighborhood features are equal for the majority of cases, which further streamlines the process and minimizes unnecessary complexity. This hardware-friendly approach enhances the performance of GNNs, particularly in resource-constrained environments where low latency and high energy efficiency are crucial.

% \subsection{Enhancing GNN Efficiency on NPUs through Activation Sparsity and Quantization}
% \subsection{GraSp: Activation Sparsity Exploitation for GNN Performance}

% When the NPU encounters a sparse adjacency matrix or node embedding in ZVC form, it references the sparsity bitmap to identify positions of non-zero elements and directly performs computations on these values alone. This method not only minimizes redundant memory usage but also reduces the need for extensive memory transfers, thereby improving the NPU’s overall energy efficiency.
% \textcolor{black}{Fig.~\ref{fig:GraSp_result} demonstrates the normalized throughput and latency for the baseline GCN compared to GCN with GraSp. By combining activation sparsity and Zero Value Compression, GraSp enables high performance, especially in resource-limited environments. The reduced computation load and memory savings allow for more efficient hardware utilization, delivering an 11\% increase in throughput and making this approach particularly advantageous in power- and memory-constrained scenarios that require real-time performance.}





% \subsection{QuantGr: Quantization Technique to Improve Performance per Watt}



% \textcolor{black}{ 
% GraNNite proposes a \textit{generic step-by-step methodology (as shown in Fig.~\ref{fig:GraNNite_tech})} to enable and optimize the deployment of emerging neural networks on existing AI accelerators. While we demonstrate its effectiveness using Graph Neural Networks (GNNs) on FlexNN-like NPU architectures, the methodology is generalizable and can be extended to other models and hardware platforms without loss of generality. The methodology consists of three key steps. 
% \textbf{(1) Enabling the Model on the NPU.} 
% The first step focuses on enabling the model to run efficiently on the NPU while maintaining flexibility. This involves techniques tailored to the specific characteristics of the model and hardware. For GNNs, GraNNite introduces \textit{GraphSplit}, which enhances workload distribution and parallelism by partitioning the workload between the CPU and the NPU; \textit{StaGr}, which minimizes latency in static graph processing through precomputation and optimized partitioning; and \textit{GrAd} and \textit{NodePad}, which enable efficient handling of dynamic graphs by supporting real-time updates and adaptive memory management. These techniques ensure that the model is not only executable on the NPU but also achieves reasonable performance and flexibility for diverse graph-based applications.
% \textbf{(2) Optimizing Performance Without Quality Loss.} 
% Once the model is running on the NPU, the second step applies optimizations to further boost performance without compromising application quality. GraNNite achieves this through \textit{EffOp}, which accelerates execution and reduces memory bandwidth usage by optimizing control-heavy operations; \textit{PreG}, \textit{SymG}, and \textit{CacheG}, which reduce redundancy and latency through advanced memory management strategies, such as pre-fetching, symmetry exploitation, and caching; and \textit{GraSp}, which exploits sparsity in graph data to reduce memory usage and computational demands. These optimizations collectively enhance throughput and energy efficiency while maintaining the accuracy and reliability of the model.
% \textbf{(3) Trading Accuracy for Performance and Energy Gains.} 
% If further performance improvements are required and the user is willing to tolerate some loss in application-level quality, GraNNite provides a third set of techniques that trade accuracy for performance and energy gains. These include \textit{QuantGr}, which accelerates computation through reduced precision (e.g., INT8 quantization), balancing speed and accuracy; and \textit{GrAx1}, \textit{GrAx2}, and \textit{GrAx3}, which employ approximation strategies to improve throughput with minimal quality degradation. These techniques are particularly useful for scenarios where real-time processing or energy efficiency is prioritized over absolute precision.
% Together, these three steps form a robust and systematic optimization framework that addresses the challenges of deploying GNNs on resource-constrained NPUs while providing flexibility, performance, and energy efficiency.}
% Now, we detail the suite of optimization techniques (as shown in Fig.~\ref{fig:GraNNite_tech}) that form the core of the GraNNite framework. GraphSplit enhances workload distribution and parallelism, while EffOp focuses on faster execution and reducing memory bandwidth usage. StaGr minimizes latency in static graph partitioning through precomputation. Techniques like GrAd and NodePad are designed for efficient handling of dynamic graphs, whereas PreG, SymG, and CacheG aim to reduce redundancy and latency through optimized memory management. The GrAx1, GrAx2, and GrAx3 techniques improve throughput via approximation strategies, and GraSp reduces memory usage and computational demands by exploiting sparsity. Lastly, QuantGr accelerates computation through reduced precision, ensuring a balance between speed and accuracy. Together, these techniques form a robust optimization framework for diverse graph-based applications.
% We present an end-to-end methodology that enables pre-trained GNNs to run on a DNN Accelerator (DNA), specifically Intel’s Neural Processing Unit (NPU), without retraining, ensuring efficient deployment while preserving model performance. The process leverages CPU and NPU partitioned inference, which distributes control-intensive tasks like graph preprocessing to the CPU and parallelized GNN computations to the NPU, significantly reducing overall latency (see Figure~\ref{fig:end_to_end}).  Prior to converting the GNN models into an intermediate representation (IR), we apply essential software optimizations such as node padding and mapping control-heavy DSP operations to the DPU. Once in IR format, the NPU compiler further enhances execution efficiency by introducing hardware-optimized strategies, including sparse storage and computation, which reduce memory use, and vertical fusion of operations, which enables pipelined execution of tasks like matrix multiplication and Softmax. Additionally, quantization enables low-precision INT8 operations, optimizing performance per watt while maintaining accuracy. Together, these optimizations provide a streamlined, high-performance pathway for deploying GNNs on specialized hardware.
% Figure~\ref{fig:npu_datapath} shows a high-level diagram of this architecture, emphasizing a DNN accelerator’s datapath. The main component of the accelerator is the Processing Element Array (PEA) also known as the Sparse Cell Array (SCA), consisting of an M×M square grid of Sparse Cells (SCL). Each SCL is constructed using an N×N square grid of MAC Processing Elements (MPE) that performs Multiply-and-Accumulate (MAC) operation. In addition, the DPU also has local SRAM memory (CMX) to store and load activations and weights for each DNN layer, a tensor distribution network that consists of load and drain data path to/from the SCA, and finally, the control logic that orchestrates the loading, computation, partial sum accumulation, and extraction of the output points to and from the SCA. In DNNs, MAC operations are used to compute the dot product of many weights and hidden-layer activations to produce the output feature maps for the next layer. Each MPE can perform MAC operations using a local data-path consisting of register files, multipliers, and accumulators.

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/NPU_datapath.png}% This is a *.eps file
% \end{center}
% \caption{NPU datapath architecture with Sparse Cell (SCL) and PE}\label{fig:npu_datapath}
% \end{figure}