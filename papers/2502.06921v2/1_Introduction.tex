\section{Introduction}\label{sec:intro}
% Graph Neural Networks (GNNs) are designed to learn and reason over graph-structured data, excelling in applications like social network analysis, drug discovery, and recommendation systems~\cite{gnn_survey_wu}. Unlike traditional neural networks such as CNNs and LLMs, GNNs leverage graph topology to capture complex relationships, making them essential for tasks requiring node interaction understanding.
% Their growing significance on edge is underscored by MLPerf's inclusion of R-GAT, a prominent GNN model, in its inference benchmarks, reflecting the rising demand for efficient GNN deployment.
% Deploying GNNs on edge devices, such as laptops and client PCs, offers benefits like real-time performance, privacy, and energy efficiency. For instance, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs~\cite{gnn_rag, g_retriever} in personal assistant software or support event-based vision tasks~\cite{aegnn, evgnn}, ensuring rapid, local processing without cloud dependency. These advantages highlight the need for high-performance, power-efficient DNN Accelerators (NPUs) to manage persistent real-time GNN workloads, as illustrated in Fig.~\ref{fig:Applications}.
% However, deploying GNNs on resource-constrained devices faces challenges like irregular memory access, dynamic computation, and limited parallelism. Sparse graphs~\cite{gnn_survey_wu} worsen these issues by causing memory latency and underutilized resources, while dynamic graph structures (Fig.~\ref{fig:dyn_graph}) require frequent recompilation, disrupting real-time applications. Additionally, control-heavy computations and reliance on slower DRAM due to limited SRAM lead to high inference latency, increased energy consumption, and reduced battery life.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.95\columnwidth]{Figures/Applications.pdf}% This is a *.eps file
\end{center}
\caption{Applications of GNNs on Client PCs: showcasing GNN-driven tasks like recommendations and event-driven vision, mapped onto Intel\textregistered\ Core\texttrademark\ Ultra processors for faster response and lower power.}\label{fig:Applications}
\end{figure}

\textcolor{black}{Graph Neural Networks (GNNs) have become essential for learning and reasoning over graph-structured data, with applications in areas like network analysis, recommendation systems~\cite{gnn_survey_wu}, and speech analytics~\cite{gnn_emotion}. Their ability to capture complex relationships through graph topology distinguishes them from traditional neural networks such as CNNs and LLMs. Recently, the inclusion of R-GAT, a prominent GNN model, in MLPerf's inference benchmarks emphasizes their growing importance in real-world applications.}
\textcolor{black}{GNNs are particularly important compared to LLMs and newer architectures like State Space Models (SSMs) due to their ability to explicitly model relational and structural information, which is critical for tasks involving interconnected data such as social networks, molecular structures, and knowledge graphs~\cite{bronstein2017geometric}. While LLMs excel in sequential data processing and SSMs offer efficiency in modeling long-range dependencies~\cite{gu2022efficiently}, GNNs uniquely capture complex relationships through graph topology, making them indispensable for tasks where data is inherently non-Euclidean.}
\textcolor{black}{Running GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces dependency on cloud infrastructure. For instance, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs~\cite{gnn_rag, g_retriever}, enabling efficient personal assistant applications. In addition, they are integral to event-based vision tasks~\cite{aegnn, evgnn}, allowing rapid processing of irregular data streams (as shown in Fig.~\ref{fig:Applications}). The rising popularity of sensors in mobile devices further drives the deployment of GNNs to the wireless network edge for tasks such as sensing and interaction, including collision prediction in self-driving vehicles~\cite{gnn_autonomous} and speech analytics~\cite{gnn_emotion}. These benefits make edge deployment crucial for achieving low-latency and energy-efficient solutions while addressing the growing demand for intelligent local inference.
Despite their potential, deploying GNNs on resource-constrained edge devices presents challenges. Irregular memory access patterns, dynamic graph structures, and limited parallelism hinder computational efficiency. Sparse graphs exacerbate memory latency and lead to underutilized resources~\cite{gnn_survey_wu}. For example, deploying the DGCNN model on a Raspberry Pi 3B achieves less than 0.3 frames per second (fps), far below practical requirements~\cite{gcode}. Additionally, edge devices often rely on slower DRAM due to limited SRAM, resulting in high inference latency, increased energy consumption, and reduced battery life. These limitations highlight the need for optimized techniques to efficiently map GNN workloads onto edge platforms.}







% Neural Processing Units (NPUs) are specialized processors optimized for data-parallel matrix multiplication, the backbone of neural networks. They use Data Processing Unit (DPU)~\cite{flexnn} for data-parallel matrix operations and digital signal processors (DSPs) for non-linear activation functions and sequential tasks. NPUs outperform traditional CPUs and GPUs in speed and power efficiency, making them ideal for continuous GNN workloads.
% However, mapping GNNs onto NPUs is challenging due to dynamic input graphs and sparsity in computations, which reduce efficiency. To address this, we introduce \textbf{\textit{GraNNite}}, the first framework designed to optimize GNN deployment on NPUs for high-performance processing.
\textcolor{black}{Modern edge processors, such as AI PCs from Intel, Qualcomm, and AMD, integrate heterogeneous computing units, including CPUs, GPUs, and NPUs, to efficiently support diverse AI workloads. Among these, NPUs or Neural Processing Units are specialized processors optimized for data-parallel operations, particularly matrix multiplication, which is the foundation of most neural network computations. NPUs typically include Data Processing Units (DPUs)~\cite{flexnn} for parallelized matrix operations and Digital Signal Processors (DSPs) for sequential tasks like non-linear activation functions.
NPUs are well-suited for AI workloads due to their high throughput and energy efficiency, outperforming traditional CPUs and GPUs. These advantages make NPUs ideal for continuous and resource-intensive GNN workloads on edge devices. However, the aforementioned challenges hinder their efficient utilization for GNN processing.
Although prior research has proposed methods to optimize GNN processing~\cite{gcn_point_cloud}, these efforts remain insufficient for real-time edge deployments~\cite{gnn_edge_1, gnn_fpga}. To address these gaps, we introduce \textbf{\textit{GraNNite}}, a framework specifically designed to optimize the deployment of GNNs on NPUs, enhancing performance and efficiency. GraNNite leverages hardware-aware techniques to mitigate the challenges, ensuring scalable and efficient GNN execution on edge platforms.}
\textcolor{black}{Modern GNNs primarily rely on three foundational layer types: Graph Convolution (GraphConv), Graph Attention (GraphAttn), and Sample and Aggregate (SAGE), which form the basis of architectures such as Graph Convolution Network (GCN)~\cite{gcn}, Graph
Attention Network (GAT)~\cite{gat}, and GraphSAGE~\cite{sage} (Fig.~\ref{fig:GNNs}). These layers were selected for our study as they address distinct challenges: GCNs capture local structure through neighbor averaging, GATs improve representation quality by assigning importance weights via attention mechanisms, and GraphSAGE enhances scalability by sampling neighbors for efficient large-graph processing. GraNNite optimizes these layers to achieve efficient execution by introducing a systematic 3-step methodology: (1) enabling GNNs on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance gains. \textit{While we evaluate GraNNite on GNNs using NPUs, the methodology is generic and can be extended to other models and hardware platforms without loss of generality.} Our key contributions are:}
\begin{itemize}
\color{black}
    \item \textbf{Step 1: Enabling GNNs on NPUs.} GraNNite introduces \textit{GraphSplit} to optimize sequential and irregular compute tasks by assigning graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead. For static graphs, \textit{StaGr} transforms node aggregation into matrix multiplication using a precomputed mask, while for dynamic graphs, \textit{GrAd} and \textit{NodePad} enable real-time updates with preconfigured node capacities.

    \item \textbf{Step 2: Optimizing GNN performance.} GraNNite enhances efficiency with \textit{EffOp}, which substitutes control-heavy DSP operations (e.g., select, gather) with equivalent data-parallel operations for DPU execution, reducing latency and improving energy efficiency. Additionally, \textit{GraSp} exploits sparsity bitmaps to skip zero values, reducing memory usage and improving energy efficiency. For GNNs with GraphConv layers, \textit{PreG}, \textit{SymG}, and \textit{CacheG} reduce redundancy: \textit{PreG} offloads normalization factor computation to the CPU, \textit{SymG} stores only half the normalization matrix, and \textit{CacheG} reuses precomputed matrices to minimize memory transfers.

    \item \textbf{Step 3: Trading accuracy for performance gains.} GraNNite introduces \textit{QuantGr}, which shifts computations from FP16 to INT8, achieving performance gains with reduced memory and computation time for negligible quality loss. For further throughput improvements, three approximation techniques are employed: \textit{GrAx1} simplifies attention score computation, \textit{GrAx2} optimizes broadcast-add operations, and \textit{GrAx3} accelerates SAGE-max aggregation using parallelized DPU operations.

    \item Experiments on Intel\textregistered\ AI PCs show that GraNNite achieves speedups of $2.6\times$\nobreakdash--$7.6\times$ over out-of-the-box NPU mappings, with energy efficiency up to $8.6\times$ higher than CPUs and GPUs. Across GNN models, it outperforms CPUs by $3.3\times$\nobreakdash--$10.8\times$ and GPUs by $2.3\times$\nobreakdash--$6.7\times$. Intel\textregistered\ Core\texttrademark\ Ultra Series 2 NPUs deliver up to $1.7\times$ higher throughput than Intel\textregistered\ Core\texttrademark\ Ultra Series 1 NPUs.
\end{itemize}

% \textcolor{black}{Modern GNNs primarily utilize three foundational layer types: Graph Convolution, Graph Attention, and Sample and Aggregate (SAGE). These layers underpin architectures like GCNs~\cite{gcn}, GATs~\cite{gat}, and GraphSAGE~\cite{sage}, each addressing specific limitations of its predecessors (Fig.~\ref{fig:GNNs}).
% GCNs aggregate features by averaging those of immediate neighbors, capturing local structure but treating all neighbors equally, which can limit representation quality. GATs enhance this by using attention mechanisms to assign importance weights to neighbors, capturing more nuanced relationships. GraphSAGE addresses scalability by sampling a subset of neighbors for aggregation, enabling efficient processing of large graphs.
% Both GATs and GraphSAGE extend the GCN framework with attention and sampling, refining representations while balancing expressiveness and efficiency.
% We will explore how GraNNite enables and optimizes these layers for efficient execution on NPUs.}





% \begin{itemize}
%     \item To optimize sequential and irregular compute tasks in GNN models, GraNNite introduces two techniques, GraphSplit and EffOp. GraphSplit partitions the model by assigning control-flow-heavy graph preprocessing tasks to the CPU and data-parallel tasks to the NPU, based on an offline cost model to minimize communication overhead and boost system performance. EffOp offloads control-heavy operations (other than preprocessing), such as select, gather, non-linear activations, and element-wise computations, from the DSP to the NPU's high-speed Multiply-Accumulate (MAC) units in the DPU. This reduces system inference latency and improves performance and energy efficiency by exploiting the DPU's higher frequency and parallelism.

%     \item GraNNite introduces optimizations for both static and dynamic graph scenarios. For static graphs, StaGr leverages the NPU's matrix multiplication capabilities by transforming node feature aggregation into a matrix multiplication using a precomputed mask.
%     For dynamic graphs, GrAd and NodePad enable efficient handling of graph updates by configuring the NPU to accept structural matrices as inputs for real-time updates and compiling models with padded node capacities to minimize recompilation. Together, these techniques enhance performance and energy efficiency across diverse graph workloads.
    
%     \item Through a combination of optimizations (PreG, SymG \& CacheG), GraNNite improves the efficiency of GNNs with Graph Convolutional (GraphConv) layers. PreG offloads costly normalization factor calculations, involving square root and division operations, to the CPU during input preprocessing, reducing the burden on NPUs and improving computational efficiency. SymG takes advantage of the symmetry in the normalization matrix, storing only half of the adjacency matrix and its diagonal elements, significantly reducing memory consumption and enhancing runtime efficiency. CacheG leverages the reuse of the precomputed constant normalization matrix across all GraphConv layers by caching it, minimizing redundant memory transfers and reducing inference latency.

%     \item GraNNite introduces three approximate techniques (GrAx1, GrAx2, and GrAx3) to enhance GNN efficiency with minimal quality impact. GrAx1 approximates attention score computation to reduce DPU overhead. GrAx2 optimizes broadcast-add operations by reordering them. GrAx3 accelerates SAGE-max aggregation using parallel element-wise multiplication and max pooling on the DPU.

%     \item GraNNite presents optimizations (GraSp \& QuantGr) that enhance GNN performance by leveraging NPU capabilities. GraSp exploits both-sided sparsity using sparsity bitmaps, reducing memory usage and improving energy efficiency by skipping zero values. 
%     QuantGr uses NPUs' support for INT8 precision to shift from FP16, achieving performance gains while reducing memory usage, data transfer, and computation time.

%     \item Experimental validation on an Intel Lunar Lake AI PC demonstrates that GraNNite achieves a speedup ranging from $2.6\times$ to $7.6\times$ compared to out-of-the-box NPU mappings, with energy efficiency up to $8.6\times$ higher than CPUs and GPUs. Across various GNN models, GraNNite outperforms CPU and GPU executions, delivering speedups of $3.3\times$ to $10.8\times$ over CPUs and $2.3\times$ to $6.7\times$ over GPUs. Performance comparisons between Meteor Lake and Lunar Lake NPUs highlight Lunar Lake’s advantage, offering up to $1.7\times$ higher throughput across configurations.
    
% \end{itemize}



% \begin{itemize}
% \color{blue}
%     \item GraNNite introduces GraphSplit and EffOp to optimize sequential and irregular compute tasks in GNN models. GraphSplit assigns graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead. EffOp offloads control-heavy operations like select, gather, and activations to the DPU's high-speed MAC units, reducing inference latency and improving energy efficiency.

%     \item GraNNite supports both static and dynamic graphs with StaGr, GrAd, and NodePad. StaGr transforms node aggregation into matrix multiplication using a precomputed mask for static graphs. GrAd and NodePad handle dynamic graphs by enabling real-time updates with preconfigured node capacities, minimizing recompilation and enhancing performance.

%     \item For GNNs with GraphConv layers, GraNNite combines PreG, SymG, and CacheG to improve efficiency. PreG offloads normalization factor computation to the CPU, SymG reduces memory usage by storing only half the normalization matrix, and CacheG reuses precomputed normalization matrices, minimizing redundant memory transfers.

%     \item GraNNite introduces approximate techniques (GrAx1, GrAx2, GrAx3) to boost efficiency with minimal quality loss. GrAx1 simplifies attention score computation, GrAx2 optimizes broadcast-add operations, and GrAx3 accelerates SAGE-max aggregation using parallelized DPU operations.

%     % \item GraSp and QuantGr leverage NPU capabilities to enhance GNN performance. GraSp reduces memory usage by exploiting sparsity bitmaps, while QuantGr shifts computations from FP16 to INT8, improving performance and energy efficiency.

%     \item GraNNite presents optimizations (GraSp \& QuantGr) that enhance GNN performance by leveraging NPU capabilities. GraSp exploits both-sided sparsity using sparsity bitmaps, reducing memory usage and improving energy efficiency by skipping zero values. 
%     QuantGr uses NPUs' support for INT8 precision to shift from FP16, achieving performance gains while reducing memory usage, data transfer, and computation time.

%     \item Experiments on an Intel Lunar Lake AI PC show that GraNNite achieves speedups of $2.6\times$ to $7.6\times$ over out-of-the-box NPU mappings, with energy efficiency up to $8.6\times$ higher than CPUs and GPUs. Across GNN models, it outperforms CPUs by $3.3\times$ to $10.8\times$ and GPUs by $2.3\times$ to $6.7\times$. Lunar Lake NPUs deliver up to $1.7\times$ higher throughput than Meteor Lake NPUs.
% \end{itemize}

\noindent The paper is organized as follows: Section~\ref{sec:prior_art} reviews prior work on GNN optimization for specialized hardware. Section~\ref{sec:background} covers GNN execution and computational challenges. Section~\ref{sec:Design} describes GraNNite methodology for optimizing GNNs on NPUs. Section~\ref{sec:expt_methodology} explains the experimental setup, including datasets, models, and hardware. Section~\ref{sec:results} presents performance and energy efficiency results. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines future directions.
% \noindent \textcolor{black}{The rest of the paper is structured as follows: Section~\ref{sec:prior_art} reviews prior works related to GNN optimization and deployment on specialized hardware. Section~\ref{sec:background} provides the necessary background on GNN execution and its computational challenges. Section~\ref{sec:Design} details the GraNNite design methodology, outlining the proposed optimizations for efficient GNN execution on NPUs. Section~\ref{sec:expt_methodology} describes the experimental methodology, including datasets, models, hardware setup, and measurement techniques. Section~\ref{sec:results} presents the results, demonstrating the performance and energy efficiency improvements achieved by GraNNite’s optimizations. Finally, Section~\ref{sec:conclusion} concludes the paper and discusses potential future directions.}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\columnwidth]{Figures/GNNs_v2.pdf}% This is a *.eps file
\end{center}
\caption{Three fundamental GNNs: GCN, GAT, and GraphSAGE, emphasizing their unique approaches—convolutional aggregation, attention-based weighting, and neighbor sampling for scalability.}\label{fig:GNNs}
\end{figure}



% Graph Neural Networks (GNNs) are powerful tools for learning and reasoning over graph-structured data, excelling in applications like social network analysis, drug discovery, and recommendation systems~\cite{gnn_survey_wu}. Unlike traditional neural networks such as Convolutional Neural Networks (CNNs) and Large Language Models (LLMs), GNNs are designed to capture complex relationships between entities by leveraging graph topology. This capability makes GNNs invaluable for tasks requiring an understanding of how nodes interact, positioning them as a major advancement in neural network architectures.

% Deploying GNNs on edge devices, such as laptops and client PCs, offers significant advantages, including real-time performance, privacy, and energy efficiency. For example, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs~\cite{gnn_rag, g_retriever} in personal assistant software, enabling intelligent local reasoning without cloud dependency. GNNs are also well-suited for event-based vision tasks~\cite{aegnn, evgnn}, where rapid processing is crucial for real-time decision-making. Running GNNs locally not only preserves user privacy but also reduces energy consumption and latency, essential for battery-powered devices. Fig.~\ref{fig:Applications} highlights two applications of GNNs deployed on a client device (AI PC), where GNN tasks are routinely executed in the background. 
% This persistent demand for real-time processing emphasizes the need for a high-performance, power-efficient DNN Accelerator (NPU) to handle these tasks effectively.
% % However, deploying GNNs on resource-constrained client PCs presents several unique challenges, as described below, including irregular memory access patterns, dynamic computation workloads, and the need for effective parallelism, which hinder optimal performance and efficiency.
% However, deploying GNNs on resource-constrained client PCs introduces challenges such as irregular memory access patterns, dynamic computation workloads, and the need for effective parallelism, all of which hinder optimal performance.
% \textbf{Sparse} graphs~\cite{gnn_survey_wu} exacerbate these issues by causing memory latency and underutilization of computational resources as accelerators optimized for dense data struggle to handle gaps in sparse structures, resulting in idle hardware and wasted bandwidth. 
% Additionally, the \textbf{input graphs are dynamic}, meaning their structures change frequently (see Fig.~\ref{fig:dyn_graph}). This creates significant overhead for accelerators that are designed for static models. Each time the structure changes, recompilation is required. This delay affects real-time applications, such as personal assistants.
% Control-heavy computations and inefficient memory access cause \textbf{high inference latency}. This problem worsens when data must be transferred to a slower DRAM because of limited SRAM capacity. The resulting latency leads to prolonged processing times, which increases energy consumption. This strains the battery life of client devices. 
% It is especially critical for applications that need continuous, real-time responsiveness, such as personal assistants and event-based vision systems.

% \textbf{Graphs are sparse:} Sparse graphs create challenges for efficient GNN execution, as the lack of connections results in irregular memory access patterns. This causes memory latency and underutilization of computational resources, as accelerators like NPUs, optimized for dense data, struggle with the gaps in sparse structures. As a result, portions of hardware remain idle, wasting memory bandwidth and computational cycles, and leading to reduced performance. These issues highlight the need for advanced techniques to optimize data handling and improve hardware utilization with sparse graph inputs.

% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/Sparse_adj_matrix.png}% This is a *.eps file
% \end{center}
% \caption{Input graphs are sparse ($\ge 50\%$ of the elements in the adjacency matrix is 0)}\label{fig:sparse_adj_mat}
% \end{figure}

% \textbf{Dynamic nature of input graphs:} GNNs are often employed to process time-varying, dynamic graphs where the structure—including nodes and edges—can frequently change (as illustrated in Figure~\ref{fig:dyn_graph}). However, most DNAs are optimized for static models with fixed input shapes, resulting in considerable overhead when dealing with dynamic graphs. Each structural change, such as the addition of new nodes or edges in a knowledge graph, necessitates recompilation, incurring delays and resource inefficiencies. This challenge is especially critical for applications like personal assistants, which depend on continuously updated, on-device knowledge graphs to deliver accurate, real-time information.





% \textbf{High inference latency:} GNNs typically involve control-heavy computations during the aggregation phase, especially in sparse graphs where nodes are not fully connected. This irregularity results in inefficient memory access patterns, exacerbating latency issues. The dynamic memory footprint of GNNs often exceeds local SRAM capacity, necessitating data transfer to slower DRAM, further contributing to latency. 
% For example, in event-based vision tasks that demand real-time processing, such delays can diminish responsiveness and overall reliability of the system.

% High energy consumption: Frequent background execution and high inference latency in GNNs lead to prolonged processing times, which increase energy consumption on client devices. Many applications, such as personal assistants and event-based vision systems, rely on continuous processing to remain responsive and deliver real-time insights. This constant background activity raises energy demands, straining battery life and device performance—particularly critical for battery-powered devices where efficient energy use is essential.

% Figure~\ref{fig:gnn_challenge} illustrates the core properties of GNNs and the associated challenges encountered when deploying these networks on client devices. 


% \begin{figure}[t!]
% \begin{center}
% \includegraphics[width=\columnwidth]{Figures/GNN_challenges.png}% This is a *.eps file
% \end{center}
% \caption{Challenges in mapping GNNs on an AI accelerator}\label{fig:gnn_challenge}
% \end{figure}


% Deep Neural Network Accelerators, also referred to as Neural Processing Units (NPUs), are specialized processors integrated into client PCs for efficient execution of neural network workloads. These accelerators are optimized for data-parallel matrix multiplication operations, a core component of most neural networks. To manage these computations, DNAs utilize specialized matrix multiplication units, here termed as Data Processing Units (DPUs). Since neural networks also involve non-linear activation functions and sequential, non-data-parallel tasks, DNAs incorporate dedicated digital signal processors (DSPs)—often vector processors—optimized for handling these specific computations.
% DNAs, designed specifically for deep learning workloads, offer significant performance advantages over traditional CPUs and GPUs, enabling faster execution with lower power consumption. They achieve high performance per watt, which is ideal for neural network applications requiring continuous background processing, such as those involving GNNs. This power efficiency and performance scalability make DNAs well-suited for handling GNN workloads, which often run in the background on client devices.
% However, mapping GNNs directly onto DNAs presents several challenges. The dynamic, time-varying nature of input graphs and inherent sparsity in GNN computations make naive deployment on DNAs sub-optimal compared to CPU- or GPU-only implementations. This highlights the need for a framework that leverages both GNN-specific properties and DNA capabilities through targeted optimization strategies. To address these challenges, we introduce GraNNite, the first framework dedicated to optimizing GNN deployment on DNAs, effectively unlocking their potential for efficient and high-performance GNN processing.
% GraNNite addresses these bottlenecks through a series of novel optimizations that enable efficient GNN execution on DNAs. By reducing memory overhead, optimizing dynamic computation workloads, and leveraging hardware capabilities—such as sparsity and vertical fusion—GraNNite significantly enhances GNN performance and resource efficiency. These improvements make it feasible to seamlessly integrate GNNs into resource-constrained edge devices. This patent outlines an end-to-end strategy for deploying pre-trained GNNs on a DNN Accelerator without requiring any hardware modifications. This ensures rapid adoption of the proposed optimizations enabling immediate performance improvements.


% This work explicitly makes the following key contributions:




% Neural Processing Units (NPUs), are specialized processors for efficient neural network workloads. They are optimized for data-parallel matrix multiplication, a key component of most neural networks. NPUs use Data Processing Units (DPUs) for matrix multiplication. Since neural networks also include non-linear activation functions and sequential tasks, NPUs include digital signal processors (DSPs), often vector processors, for these tasks.
% NPUs are designed for deep learning and offer better performance than traditional CPUs and GPUs. They provide faster execution with lower power consumption, making them ideal for applications like GNNs that require continuous background processing. This power efficiency and performance scalability make NPUs well-suited for GNN workloads.
% However, mapping GNNs onto NPUs presents challenges. The dynamic nature of input graphs and the sparsity in GNN computations make direct deployment on NPUs less efficient compared to CPUs or GPUs. To overcome this, we introduce \textbf{\textit{GraNNite}}, the first framework designed to optimize GNN deployment on NPUs for high-performance processing.



% \begin{enumerate}
    % \item StaGr: Leveraging the DNA’s optimized matrix multiplication capability, the GraNNite framework transforms node feature aggregation for static input graphs into a matrix multiplication operation using a precomputed mask stored as a parameter within the GNN. This approach reduces irregular memory access and minimizes memory latency, thereby lowering inference time and energy consumption.
    
    % \item Enabling GNN inference for dynamic graphs on DNAs:
    % \begin{enumerate}
    %     % \item GrAd: By configuring the DNA to accept the adjacency/normalization matrix as an input, the GraNNite framework enables real-time updates to dynamic graphs, allowing edge modifications without requiring model recompilation. This approach effectively mitigates the need for frequent recompilations, which are typically caused by changes in time-varying graph structures.
    %     % \item NodePad: The GraNNite framework introduces a node-padding technique that compiles the model with a larger node capacity and applies zero-padding for smaller inputs, reducing recompilation frequency. Combined with the dynamic adjacency/normalization matrix input, this method minimizes compilation overhead, enhancing both performance and energy efficiency in dynamic graph scenarios.
    %     % \item GraphSplit: The GraNNite framework proposes a partitioning strategy that assigns control-flow-heavy graph preprocessing tasks to the CPU and data-parallel tasks to the DNA, optimizing the strengths of each unit. Using an offline cost model, the optimal partition point is identified to minimize communication overhead, leading to a significant boost in system performance.
    % \end{enumerate}
    
    % \item GNN optimizations:
    % \begin{enumerate}
        % \item EffOp: Leveraging the DNA’s high-speed Multiply-Accumulate (MAC) units in the DPU, the GraNNite framework offloads control-heavy operations deep within the GNN model compute (not initial input graph preprocessing) from DSP to DPU. These operations include tasks such as select, gather, non-linear activations, and certain element-wise computations. They are moved from the general-purpose DSP to the DPU’s processing elements (PEs). By utilizing the DPU’s higher frequency and parallelism, this approach reduces system inference latency and improves both performance and energy efficiency.
    %     \item GraSp: Taking advantage of the DNA's support for both-sided (weight and activation) sparsity, the GraNNite framework enhances GNN performance by exploiting the natural sparsity (~99\%) of input graphs. GraNNite proposes utilizing sparsity bitmaps for both weights and activations, enabling DNAs to skip storage and computation for zero values. This reduces memory usage, minimizes memory traffic, and decreases compute operations, accelerating inference speed and boosting energy efficiency by avoiding unnecessary processing.
    %     \item VerGe: Utilizing the sequential structure of linear layers followed by non-linear activations in GNNs, GraNNite introduces vertical fusion of operators to concurrently execute matrix multiplication (MatMul) on the DPU and non-linear activation functions on the DSP. Through pipelining, the DPU processes MatMul in chunks, while the DSP computes the non-linear activation for previously computed chunks simultaneously. This approach reduces latency, increases throughput, and enhances performance without hardware modifications.
    %     \item QuantGr: Leveraging DNAs' native support for INT8 precision and GNNs' inherent error resilience, GraNNite introduces a quantization technique that reduces precision from FP16 to INT8, achieving substantial performance gains while preserving accuracy. This shift to INT8 reduces memory usage, data transfer demands, and computation time compared to FP16, optimizing GNN performance and efficiency on DNAs.
    % \end{enumerate}

    % \item Optimizations targeting GNNs with Convolution:
    % \begin{enumerate}
    %     \item PreG: GraNNite optimizes GNNs with Graph Convolutional (GraphConv) layers by shifting constant normalization factor calculations, involving square root and division operations, to the CPU during input preprocessing. By handling these costly operations on the CPU, GraNNite reduces the burden on the DNAs, which is less efficient for non-MatMul tasks. This approach boosts computational efficiency and improves inference performance, making it suitable for resource-constrained environments.
    %     \item SymG: Leveraging the symmetry of the normalization matrix in GraphConv, the GraNNite framework introduces a memory-efficient technique by storing only half of the adjacency matrix along with its diagonal elements. This reduces memory storage complexity from $N^2$ to $(N^2+N)/2$, significantly lowering memory consumption. By reducing memory traffic, particularly during Direct Memory Access (DMA) transfers from DRAM to the DNA’s local SRAM memory, this approach enhances runtime efficiency and mitigates memory bottlenecks.
    %     \item CacheG: Leveraging the reuse of the precomputed constant normalization matrix across all GraphConv layers in a GNN, the GraNNite framework caches this matrix, minimizing memory access overhead by avoiding redundant memory transfers for different layers. This reduces inference latency and addresses the bottleneck of high memory access and latency in GNN inference.
    % \end{enumerate}

    % \item Optimizations targeting GNNs with Attention:
    % \begin{enumerate}
    %     \item GrAx1: GraNNite proposes an approximate technique to address the low DPU utilization problem in Graph Attention (GraphAttn) layers of GNNs. It eliminates the element-wise multiplication with the attention mask by approximating the attention score matrix with a large negative value at non-existent edge positions, bypassing multiplication. This reduces computation overhead, enhances DPU utilization, and lowers end-to-end inference latency.
    %     \item GrAx2: GraNNite introduces a coarse-grain approximation strategy to simplify GraphAttn layer computation. The broadcast-add operation, which computes pairwise intermediate attention scores and acts as a bottleneck in the DPU due to low DPU utilization from its element-wise addition, is replaced by an addition followed by broadcasting. This approach reduces latency, improves throughput, and maintains minimal accuracy impact, leading to faster and more efficient GraphAttn execution.
    % \end{enumerate}

    % \item Optimizations targeting GNNs with Sample and Aggregate:
    % \begin{enumerate}
    %     \item GrAx3: Leveraging the abundant data parallelism in the DPU, GraNNite introduces a hardware-friendly compute strategy for GNNs using "Sample and Aggregate" (SAGE) layers with a "max" aggregation strategy. This approach approximates the sequential neighborhood feature selection in the DSP by replacing it with a parallel element-wise multiplication with a mask in the DPU. As a result, inference latency is reduced, improving throughput and energy efficiency.
    % \end{enumerate}

    % \item Results:
    % \begin{enumerate}
    %     \item Experimental validation of these contributions reveals significant performance improvements across various GNN models (using GraphConv, GraphAttn, SAGE layers) when executed on a DNA (Intel NPU), outperforming both CPU and GPU executions. Compared to GPU execution, the results show a 17.3X speedup for GraphConv (GCN), 2.3X for GraphAttn (GAT), 6.7X for SAGE with mean aggregation, and 4.2X for GraphSAGE with max aggregation. When compared to CPU execution, GraphConv achieves a 4.6X speedup, GraphAttn shows a 3.8X speedup, SAGE-mean achieves a 10.8X speedup, and SAGE-max achieves a 5.3X speedup.
    %     \item The performance evaluation across various GNN models (using GraphConv, GraphAttn, and SAGE layers) highlights the improvements brought by GraNNite's optimization techniques, comparing the initial out-of-the-box implementations to the fully optimized versions tailored for Intel NPU. Specifically, GraphConv (GCN) achieves a 2.7X speedup, GraphAttn (GAT) attains an impressive 7.6X speedup, and SAGE with max aggregation shows a 3.2X speedup.
    % \end{enumerate}
% \end{enumerate}