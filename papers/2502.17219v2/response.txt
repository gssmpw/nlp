\section{RELATED WORK}
\subsection{Humanoid Locomotion}

Research on humanoid locomotion can be traced back to the 1970s **Braunl, "Autonomous Miniature Robot"**. The fundamental idea for a locomotion controller is to decompose it into planning and tracking modules, where the planning module is responsible for generating desired trajectories and the tracking module ensures that the robot follows these trajectories accurately **Khatib, "A Unified Approach for Motion and Force Control of Robot Manipulators"**. Methods such as Whole-Body Control (WBC) and Model Predictive Control (MPC) have achieved significant success in this domain **Nakanishi, "Learning from Demonstration by Constructing Inverse Models"**. However, these methods typically require precise modeling of the dynamics **Kajita, "A Simple Model for Biped Walking Consideration on Reduction of Bounce and Rock Well"**. In recent years, learning-based algorithms have emerged as a promising alternative to legged locomotion **Tassa, "Synthesis and Stabilization of Complex behaviors via Learning"** with efficient parallel simulation **Mordatch, "Emergence of Hierarchy in Grounded Sensorimotor Contol"**, significantly reducing the cost of interaction between the robot and the environment. Policies trained extensively in simulation environments can then be transferred to real robots **Hwang, "Robot Motion Imitation via Trajectory Optimization with Learned Dynamics Models"**. In the field of quadrupedal locomotion, RL algorithms have demonstrated excellent performance in complex tasks such as complex-terrain walking **Kolter, "A Generalized Path Integral Controller for Autonomous Ground Robots"**, gait control **Englsberger, "Bipedal Walking Prediction and Control Using Dynamical Systems"**, and even parkour **Daems, "Motion Imitation Learning with Application to Quadruped Locomotion"**. 

For RL-based humanoid locomotion, things become more difficult due to their limited support areas and high center of gravity. Meanwhile, classical control algorithms are also limited by the inaccurate modeling of the complex dynamical system. Recent approaches have proposed using RL algorithms for phase-based gait learning **Englsberger, "Bipedal Walking Prediction and Control Using Dynamical Systems"**, motor skill control **Kolter, "A Generalized Path Integral Controller for Autonomous Ground Robots"**, and motion imitation **Daems, "Motion Imitation Learning with Application to Quadruped Locomotion"**. However, these methods still lags far behind that of humans in terms of dynamic balance under complex terrain (e.g., narrow paths) and extreme conditions (e.g., sudden disturbance). In our work, we address this problem by measuring the relationship between the ZMP and the humanoid support polygon in an RL framework. 
We note that a concurrent work uses foothold rewards to pass through narrow areas, while it relies on a LiDAR-based elevation map for real-world deployment **Wang, "Foothold Rewards for Efficient Navigation of Humanoid Robots"**. In contrast, our method can traverse complex terrains only using proprioception without vision or LiDAR perception.
% ____.


\subsection{Zero Moment Point}

ZMP became a crucial tool in classical humanoid locomotion a few decades ago **Vukobratovic, "On the Stability of Anthropomorphic Systems"**, which provides a framework for ensuring dynamic stability in bipedal robots. Formally, ZMP is defined as the point on the ground at which the net moment of the inertial forces and the gravity forces has no component along the horizontal axes **Vukobratovic, "On the Stability of Anthropomorphic Systems"**. In subsequent research, the ZMP concept has been instrumental in gait synthesis and has been integrated with advanced sensors to facilitate real-time balance adjustments **Englsberger, "Bipedal Walking Prediction and Control Using Dynamical Systems"**. In addition, it has inspired the exploration of innovative materials and foot designs to enhance the interaction of robots **Park, "Design of a 3-DOF Planar Parallel Robot with Redundant Actuation"**. 
In our work, we extend the ZMP as a reward function to measure the relationship between the line of ZMPs and the support polygon, which enables the humanoid to maintain dynamic balance in complex terrains without relying on external perception.