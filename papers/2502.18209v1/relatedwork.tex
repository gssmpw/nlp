\section{Related Work}
\paragraph{LLM for Scientific Research.}
In the realm of LLMs, several studies have explored using LLMs for improving work efficiency in scientific research.
\citet{Baek2024ResearchAgentIR} and \citet{Yang2023LargeLM} proposed a multi-agent-based scientific idea generation method to boost AI-related research. To evaluate the quality of LLM-generated ideas, \citet{Si2024CanLG} introduced a comprehensive human evaluation metric. 
\citet{Wang2023SciMONSI} proposed SciMON, a method that uses LLMs for scientific literature retrieval.
\citet{Wang2024AutoSurveyLL} proposed an AutoSurvey to automatically generate scientific surveys based on the given research topic. The AI Scientist, \citet{Lu2024TheAS} introduced a fully automated, prompt-driven research pipeline. To make LLM-generated ideas more diverse and practical, \citet{Weng2024CycleResearcherIA} proposed CycleResearcher, an iterative self-rewarding framework that allows the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation.
However, no previous research focused on the leaderboard generation for researchers to search, organize, and compare the state-of-the-art methods rapidly and fairly based on a certain research topic.

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{figs/framework.pdf}
\caption{\label{framework}The LAG framework for leaderboard automatic generation. In Stage 1, we automatically crawl scientific papers from arXiv. In Stage 2, we retrieve, extract, and classify tables from the latex code. In Stage 3,  we select the main results tables and extract datasets, metrics, results, and experiment settings from the main results table. In Stage 4, we generate Leaderboards from the selected results and evaluate the quality.}
\vspace{-.1in}
\end{figure*}

\paragraph{Leaderboard Construction.}
Table \ref{tab:related_work} illustrates the differences between the previous work and LAG.
First of all, previous work builds leaderboards by using data sources such as NLP-progress or Papers-With-Code. However, these sources lack rigorous quality assurance, such as standardizing scientific entities across different leaderboards and ensuring complete coverage of relevant publications. Instead, we choose arXiv, which is a free distribution service and an open-access archive for nearly 2.4 million scholarly articles in different domains, providing a large amount of publications for researchers.
Similar to our work, \citet{hou-etal-2019-identification}, \citet{kardas-etal-2020-axcell}, and \citet{Singh2024LEGOBenchSL} extract ``Task'', ``Dataset'', ``Model'' along with the experiment result entities as TDM triples to build a leaderboard. 
\citet{yang-etal-2022-telin} and \citet{KABENAMUALU2023ORKGLeaderboardsAS} leverage the pre-defined TDM triples in an extraction process similar to \citet{hou-etal-2019-identification}.  
Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with realistic task definitions. In short, none of the previous work is adaptable to constantly emerging benchmarks driven by new research and innovation. 
Moreover, none of the studies extract the experiment settings as additional information to generate leaderboards, which results in a lack of fair comparison. In scientific research, experiment settings are important for educators or users to reproduce the experimental results claimed in scientific publications. 
In this work, we address the
aforementioned problems. Specifically, we (1) dynamically download scientific publications and generate up-to-date leaderboards based on the given scientific topic and the specific date; (2) extract experiment settings as part of leaderboards for fair comparison; (3) apply a Multi-Agent-as-Judge to evaluate leaderboard quality.