% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[normalem]{ulem}
\usepackage{booktabs}
\useunder{\uline}{\ul}{}
\usepackage{times}
\usepackage{bbding}
\usepackage {adjustbox}
\usepackage{amsmath,amsfonts,amssymb,amsthm, bm}
\usepackage[T1]{fontenc}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{colortbl} 
\usepackage{inconsolata}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{changepage}
\usepackage{rotating}
\usepackage{adjustbox}

%==============================================================================
\usepackage{newfloat}
\usepackage{tikz}
\newcommand{\framedbox}[2][0.96\textwidth]{
 \centering
 \tikzstyle{mybox} = [draw=black,line width=1.2pt,inner sep=8pt]
 \begin{tikzpicture}
  \node [mybox] (box){%
   \begin{minipage}{#1}{#2}\end{minipage}
  };
 \end{tikzpicture}
}
\DeclareFloatingEnvironment[listname=Box,name=Box]{story}
\captionsetup[story]{listformat=simple}

\title{LAG: LLM agents for Leaderboard Auto Generation on Demanding}

\author{Jian Wu\textsuperscript{1}$^{\ast}$, \ Jiayu Zhang\textsuperscript{2}\thanks{
The authors contributed equally to this work.}, \ Dongyuan Li\textsuperscript{3}, \ Linyi Yang\textsuperscript{4},  \ Aoxiao Zhong\textsuperscript{5}, \\
\textbf{Renhe Jiang\textsuperscript{3}}, \ \textbf{Qingsong Wen\textsuperscript{5}}, \ \textbf{Yue Zhang\textsuperscript{6}} \\
  \textsuperscript{1} Institute of Science Tokyo 
  \textsuperscript{2} Peking University, China
  \textsuperscript{3} University of Tokyo \\
  \textsuperscript{4} University College London
  \textsuperscript{5} AI Research Institute, Squirrel AI Learning, China \\
  \textsuperscript{6} School of Engineering, Westlake Univeristy\\ 
  } 

\begin{document}
\maketitle
\begin{abstract}
This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving
fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in
automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG
solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.
\end{abstract}

\section{Introduction}
%介绍研究现状，AI领域每天有大量的论文上传，让研究者跟上最前沿的方法变得很困难，这里图片说明论文的爆炸式增长和leaderboard缺乏维护的现状。
The explosive growth of scientific publications has created both unprecedented opportunities and significant challenges for researchers seeking to stay abreast of state-of-the-art methods~\cite{Bornmann2020GrowthRO, Wang2024AutoSurveyLL, ahinu2024EfficientPT}. Leaderboard platforms, such as NLP-progress\footnote{https://nlpprogress.com/} and Papers-With-Code\footnote{https://paperswithcode.com/} have become invaluable by offering comprehensive overviews of recent research developments, highlighting ongoing trends, and identifying future directions. 
However, the large amount of daily papers makes it increasingly difficult to update these leaderboards automatically and promptly. Figure~\ref{submission} illustrates two pressing issues: First, the number of LLM-related articles submitted to arXiv has surged dramatically—from 2022 to 2025, with over 20,000 submissions in 2024 alone.  Second, even as new methods continuously emerge, leaderboards, such as the one for Multi-hop Question Answering on the HotpotQA\cite{yang2018hotpotqa} dataset, remain stagnant, with the latest method dating back to 2023. 
These observations highlight a serious issue: the rapid accumulation of daily scientific publications often outpaces the capability of researchers to keep up with cutting-edge research and state-of-the-art methods, emphasizing the growing need for more efficient methods to generate the latest and useful leaderboards. 
%介绍先前工作，存在哪些问题
\begin{figure*}[htb]
\centering
\includegraphics[width=0.48\linewidth]{figs/paper_submission.jpeg}
\hfill
\includegraphics[width=0.48\linewidth]{figs/leaderboard_example.png}
\caption{Left: Growth trend of paper submission on LLMs from 2022 to 2025-02. Right: An Example of a Multi-hop QA dataset leaderboard (HotpotQA Homepage), the latest method is still stuck in 2023.}
\label{submission}
\end{figure*}
%\footnotetext{https://hotpotqa.github.io/}

Prior efforts have attempted to address this gap. A line of work \citep{hou-etal-2019-identification, kardas-etal-2020-axcell} has proposed leaderboard construction methods that directly extract scientific entities from individual NLP papers, and construct a static leaderboard without updating and maintenance. Semi-supervised scientific NER, proposed by \citet{Li2023AllDO}, focuses on extracting scientific entities from both tables and text.
\citet{ahinu2024EfficientPT} introduce SCILEAD, a manually-curated Scientific Leaderboard dataset, including 27 leaderboards derived from 43 NLP papers.
However, all previous methods have been limited to the extracted scientific entities and only give a static snapshot after extracting information from a narrow selections.
%大模型的出现，提供了一个可以可以解决的方法。

We consider using LLMs such as GPT-4 ~\cite{Achiam2023GPT4TR}, Qwen ~\cite{Yang2024Qwen25TR}, and O1-preview which have demonstrated exceptional performance across diverse NLP tasks, especially in the long-context scenario~\cite{Chen2023ExtendingCW, Chen2023LongLoRAEF, Wang2023AugmentingLM} to automatically generate leaderboards based on given research topic.
% The advent of Large Language Models (LLMs), such as GPT-4 ~\cite{Achiam2023GPT4TR}, Qwen ~\cite{Yang2024Qwen25TR}, and LlaMA~\cite{Touvron2023LLaMAOA} has opened a promising avenue for overcoming these limitations.  These models have demonstrated exceptional performance across diverse NLP tasks, especially in the long-context scenario~\cite{Chen2023ExtendingCW, Chen2023LongLoRAEF, Wang2023AugmentingLM}. In particular, LLM agents—sophisticated systems that leverage LLMs with distinct roles to collaborate and execute complex tasks \citep{He2024TheES}—offer a feasible path for automating the extraction of experimental information from scientific publications to build continuously updated leaderboards.


%然而单纯的使用大模型仍然面临三个细节的挑战————
Directly applying LLMs to this task still faces several key challenges.
%, several challenges remain unsolved. 
First, \textbf{Limited Paper Coverage}: It is challenging for human to search for all papers on a certain scientific topic, due to the overwhelming number of constantly emerging publications. Second, \textbf{Unfair Comparison}: Current studies do not consider fair experiment settings when making comparisons. For example, in NLP research, key experimental components, model size, train dataset size, and hyperparameter selection, vary significantly across publications, highlighting the need for automatic alignment. Finally, \textbf{Low Timeliness}: A leaderboard, which lacks regular updates and continuous maintenance, cannot provide researchers with sufficient useful information.

%介绍我们的方法
To this end, we introduce LAG, a novel agent framework for dynamically and automatically generating leaderboards. Figure \ref{framework} illustrates the framework of our method, which is organized into four stages: (1) \textbf{Paper Collection and split}: Initially, LAG  automatically download all relevant LaTex code based on the given research topic from arXiv %\footnote{https://arxiv.org/} 
and filter out papers published before certain date and those unrelated to the topic, ensuring proper paper coverage and timeliness.
(2) \textbf{Table Extraction and Classification}:
We use LLMs to extract and classify experiment tables based on accompanying table descriptions. 
(3) \textbf{Table Unpacking and Integration}:
LAG extracts the datasets, metrics, experiment settings, and experiment results from the tables in the form of a quintuple, including paper title. Experiment setting extraction is crucial for enabling fair comparisons across different baselines.
(4) \textbf{Leaderboard Generation and Evaluation}:
The extracted quintuples are recombined and re-ranked to form candidate leaderboards.

To evaluate the performance of LAG, we propose two key quality dimensions for assessment:
(1) Topic-related Quality: paper coverage assessment, determining whether each quintuple in the LAG-generated leaderboards is related to the given topic. (2) Content Quality: We adopt the LLM-as-Judge method for leaderboard quality evaluation on four aspects, including Coverage, Structure, Latest, and Multiaspect. We also introduce human experts to manually evaluate LAG-generated leaderboards and compute the Pearson Correlation Coefficient between human- and LLMs-assigned scores.
\begin{table*}[!t]
\centering
 \begin{adjustbox}{scale=0.8,center}
\begin{tabular}{lcccc}
\toprule
\textbf{Related Work} & \textbf{Data Source} & \textbf{Experiment Settings} &\textbf{Multi Document} & \textbf{Dynamic}\\
\midrule 
TDMS\citep{hou-etal-2019-identification} & NProg. & \XSolidBrush & \XSolidBrush & \XSolidBrush\\
Axcell~\citep{kardas-etal-2020-axcell} & PwC & \XSolidBrush & \XSolidBrush & \XSolidBrush\\
TELIN~\citep{yang-etal-2022-telin}  & PwC & \XSolidBrush& \XSolidBrush & \XSolidBrush\\
ORKG\citep{KABENAMUALU2023ORKGLeaderboardsAS} & PwC & \XSolidBrush & \XSolidBrush & \XSolidBrush\\
LEGO~\citep{Singh2024LEGOBenchSL} & PwC & \XSolidBrush &  \XSolidBrush & \XSolidBrush  \\
SciLead~\citep{ahinu2024EfficientPT} & NLP papers& \XSolidBrush & \checkmark & \XSolidBrush\\
LAG (Ours) & arXiv & \checkmark & \checkmark & \checkmark\\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of related work and ours. \textbf{Data Source}: Source of leaderboards:  NProg.: \emph{NLP-progress}, PwC: \emph{paperswithcode}. \textbf{Experiment Settings}: whether the experiment settings are extracted as part of leaderboards or not. \textbf{Multi Document}: whether the leaderboards are constructed from multiple papers or not. \textbf{Dynamic}: whether the generated leaderboards can be updated dynamically or not.}
\label{tab:related_work}
\end{table*}

Extensive experiments across different leaderboard lengths (5, 10, 15, and 20 items) show that LAG consistently achieves high topic-related and content quality scores. With 20 items, a LAG-generated Leaderboard represents 20 baselines for researchers, achieving 67.58\% recall and 70.33\% precision scores in topic-related quality. In content quality with 20 items, LAG achieves 4.12 coverage, 3.96 latest, 4.16 structure, and 4.08 multiaspect scores, approaching human performance (4.72 coverage, 4.68 latest, 4.34 structure, and 4.58 multiaspect scores). Although the manually created leaderboard achieves higher content quality, it is much more time-consuming than LAG, deeming the efficiency. With fewer items, LAG gets even higher performance, slightly lower than human performance. These results highlight the effectiveness of LAG, providing a reliable proxy for human judgment across varying leaderboard items. 
%Furthermore, the Pearson correlation coefficient values indicate a moderate positive correlation between the scores assigned by the LLMs and those given by human experts.
Furthermore, the Pearson correlation coefficient values indicate a moderate positive correlation between the human-assigned and LLM-assigned scores. 

To our best knowledge, LAG is the first method to explore the potential of LLM agents for automatic leaderboard generation, proposing evaluation criteria that align with human preferences and offering valuable reference for future related research.

\section{Related Work}
\paragraph{LLM for Scientific Research.}
In the realm of LLMs, several studies have explored using LLMs for improving work efficiency in scientific research.
\citet{Baek2024ResearchAgentIR} and \citet{Yang2023LargeLM} proposed a multi-agent-based scientific idea generation method to boost AI-related research. To evaluate the quality of LLM-generated ideas, \citet{Si2024CanLG} introduced a comprehensive human evaluation metric. 
\citet{Wang2023SciMONSI} proposed SciMON, a method that uses LLMs for scientific literature retrieval.
\citet{Wang2024AutoSurveyLL} proposed an AutoSurvey to automatically generate scientific surveys based on the given research topic. The AI Scientist, \citet{Lu2024TheAS} introduced a fully automated, prompt-driven research pipeline. To make LLM-generated ideas more diverse and practical, \citet{Weng2024CycleResearcherIA} proposed CycleResearcher, an iterative self-rewarding framework that allows the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation.
However, no previous research focused on the leaderboard generation for researchers to search, organize, and compare the state-of-the-art methods rapidly and fairly based on a certain research topic.

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{figs/framework.pdf}
\caption{\label{framework}The LAG framework for leaderboard automatic generation. In Stage 1, we automatically crawl scientific papers from arXiv. In Stage 2, we retrieve, extract, and classify tables from the latex code. In Stage 3,  we select the main results tables and extract datasets, metrics, results, and experiment settings from the main results table. In Stage 4, we generate Leaderboards from the selected results and evaluate the quality.}
\vspace{-.1in}
\end{figure*}

\paragraph{Leaderboard Construction.}
Table \ref{tab:related_work} illustrates the differences between the previous work and LAG.
First of all, previous work builds leaderboards by using data sources such as NLP-progress or Papers-With-Code. However, these sources lack rigorous quality assurance, such as standardizing scientific entities across different leaderboards and ensuring complete coverage of relevant publications. Instead, we choose arXiv, which is a free distribution service and an open-access archive for nearly 2.4 million scholarly articles in different domains, providing a large amount of publications for researchers.
Similar to our work, \citet{hou-etal-2019-identification}, \citet{kardas-etal-2020-axcell}, and \citet{Singh2024LEGOBenchSL} extract ``Task'', ``Dataset'', ``Model'' along with the experiment result entities as TDM triples to build a leaderboard. 
\citet{yang-etal-2022-telin} and \citet{KABENAMUALU2023ORKGLeaderboardsAS} leverage the pre-defined TDM triples in an extraction process similar to \citet{hou-etal-2019-identification}.  
Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with realistic task definitions. In short, none of the previous work is adaptable to constantly emerging benchmarks driven by new research and innovation. 
Moreover, none of the studies extract the experiment settings as additional information to generate leaderboards, which results in a lack of fair comparison. In scientific research, experiment settings are important for educators or users to reproduce the experimental results claimed in scientific publications. 
In this work, we address the
aforementioned problems. Specifically, we (1) dynamically download scientific publications and generate up-to-date leaderboards based on the given scientific topic and the specific date; (2) extract experiment settings as part of leaderboards for fair comparison; (3) apply a Multi-Agent-as-Judge to evaluate leaderboard quality.


\section{Methods}
 Figure \ref{framework} depicts LAG, which consists of four stages: Paper Collection and Split, Table Extraction and Classification, Table Unpacking and Integration, and Leaderboard Generation and Evaluation. Each stage is meticulously designed to address specific challenges associated with leaderboard generation, thereby enhancing the efficiency and quality of the resulting leaderboards. The whole process is iterated several times (e.g., five times) to generate a high-quality leaderboard. 


\subsection{Paper Collection and Split}
Utilizing the off-the-shelf tools \footnote{https://github.com/lukasschwab/arxiv.py}, LAG first searches and retrieves a set of papers $P_{\text{init}} = \{P_{1}, P_{2}, ..., P_{N}\}$ from arXiv and downloads LaTeX code files related to a specific scientific research topic $T$. Then, we specify a certain date and filter out all papers published before the date. The filtering stage is important for ensuring that the generated leaderboards are grounded in the most relevant and recent research. 
Moreover, since the search tool just identifies only the keywords in the paper title and abstract, which can lead to a significant amount of noisy data, we also introduce a retrieval model to filter out papers that are irrelevant to the given topic and retrieve topic-related papers.
The set of filtered papers $P_{\text{filtered}} = \{\text{Retrieval}\{P_{1}, P_{2}, ..., P_{U}\}\}$ is used to generate the leaderboards, ensuring comprehensive coverage of the topic and logical structure. 
Due to the extensive number of relevant papers retrieved and filtered during this stage,
the total input length of $P_{\text{filtered}}$ often exceeds the maximum input length of LLMs. Since most of the LaTex content is unproductive for generating leaderboards, we split the LaTeX code into several sections based on the structure of each paper. Most tables, table-related descriptions, experiment results, and experiment settings are located in the ``Experiment'' section, which contains the key information for generating leaderboards. Consequently, we select all ``Experiment'' sections as well as all tables $\{\text{Table}_{1}, \text{Table}_{2},...,\text{Table}_{U}\}$ and all table-related descriptions $\{D_{1}, D_{2},...D_{U}\}$, extracted from all papers, as input for the next stage.

\subsection{Table Extraction and Classification}
Typically, a scientific paper, such as those in the natural language processing domain, contains several types of tables, including ``Main Results'', ``Ablation Study'',  and ``Others''. %The ``Main Results'' tables are the tables that represent the main experiment results, comparing the results of the proposed method with other baselines. 
The ``Main Results'' tables are the most important tables in the paper, which illustrate the novelty, contributions, and effectiveness of the proposed methods or models by comparing the experiment results of the proposed method with other baselines. We utilize these tables for leaderboard generation.  The ``Ablation Study'' tables examine the effect of damaging or removing certain components in a controlled setting to investigate all possible outcomes of system failure. The ``Others'' tables are the tables that illustrate the supplementary information of the experiments. For example, some tables illustrate the dataset statistics of the benchmark used in the experiments, while other tables list the results of ``Case Study'' and ``Error Analysis''. To address this, we propose an agent that uses the In-Context Learning method \cite{Dong2022ASO} to manually select one table from each of the three different types. %three different types of tables, with one table for each type. 
The agent then prompts LLMs to classify the table types and only keeps the ``Main Experiments'' tables and their descriptions as the final input. The $i_{th}$ table types can be described as: 
$\texttt{LLM}(\text{Table}_{i},D_{i};\text{Prompt})\rightarrow \texttt{Table type}$.  

In practice, the most intrinsic approach is to divide Stage 2 into the following sequential steps:
(1) Extract all tables and their associated captions from the LaTeX code.
(2) Classify the extracted tables according to predefined table types.
(3) Extract metrics, performance values, and experimental settings related to the proposed model from tables categorized as ``Main Results''. Moreover, each of these three steps necessitates the use of LLM APIs, and repeated reference to certain table contents further exacerbates the substantial waste of tokens. To address this issue, we create the agent following the few-shot Chain of Thought (CoT) prompting process, enabling it to classify and extract information from identified ``Main Results'' tables in a single dialogue round. Specifically, in the requested JSON output, we additionally set the key points as follows: ``number of tables (Int)", ``classification of tables (Dict)" and ``selected table's index (Int)".

\subsection{Table Unpacking and Integration}
Following the table extraction and classification phase, each table $Table_{i}$ is sent into the LLM to extract the core information. To build a useful and high-quality leaderboard, we define four types of scientific terms: Datasets, Metrics, Experiment Results, and Experiment Settings. For datasets, we use LLMs to count the frequency in all filtered papers $P_{\text{filtered}}$ of each dataset under a certain research topic and retain the top-K (K=5) datasets with the highest frequency of occurrence in scientific papers.
For the rest of the three scientific terms, we utilize LLMs to extract from given \text{$Table_{i}$} with a related table description $D_{i}$. After scientific term extraction, we recombined them into a quintuple, including the paper title as the unique identification ID. Each paper can produce one quintuple and finally we get a raw leaderboard with $M$ quintuples from $M$ filtered papers. The raw leaderboard is reranked on the basis of the experiment results. 

\subsection{Leaderboard Generation and Evaluation}
After we obtain $K$ leaderboards based on top-K frequent datasets, the final stage involves a quality evaluation based on our pre-defined four criteria, which is shown in Table \ref{table: criteria} in Appendix. Each leaderboard is assigned three scores based on ``Coverage'', ``Latest'' and ``Structure''.
Since a research topic may contain several datasets, 
the ``Multi-Aspect'' is the average quality score that is used to evaluate the LLM-generated leaderboards for each dataset. 
The best leaderboard is chosen from $N$ candidates. LLMs
critically examine the leaderboards in several aspects.
The final output of Leaderboard is $L_\text{best} = \text{Evaluate}({L_\text{ca1}, L_\text{ca2}, . . . , L_\text{caN} })$.

The methodology outlined here, from paper collection to leaderboard evaluation, ensures that LAG effectively addresses the complexities of leaderboard generation in the AI domain using advanced LLM agents. We provide Pseudo-code for easily understanding, which is shown in Algorithm \ref{alg:lag}.
\begin{algorithm}[th]
\caption{\label{alg:lag}Leaderboard Automatic Generation.}
\centering
\footnotesize

\begin{algorithmic}[1]

\STATE \textbf{Input:} Scientific topic $T$, open-access platform arXiv $A$
\STATE \textbf{Output:} Final refined and evaluated leaderboard $L$

%\STATE 
\textcolor{blue}{\textit{ \# Stage 1: Paper Collection and Document Split}}
\STATE Crawl topic $T$ related $N$ publications $P_{\text{init}} = \{P_{1}, ... P_{N}\} \leftarrow \text{Retrieve}(T, A)$
\STATE Filter out topic-unrelated and old papers, $P_{\text{filtered}}=\{P_{1}, ... P_{M}\} \leftarrow \text{Retrieve}(P_{\text{init}}, \text{date}, \text{topic})$

\textcolor{blue}{\textit{\# Stage 2: Table Extraction and Classification}}
\FOR{each Leaderboard iteration $i = 1$ to $Iters$}
\STATE Count frequency of all datasets and retain top-K datasets from $U$ papers.
\FOR{each dataset $j = 1$ to $K$}
\STATE Split $P_{i}$, Extract $U$ Tables \{$\text{Table}_{1},...,\text{Table}_{U}$\} and table-related description \{$D_{1},...D_{U}$\}.
\STATE Classify each table and keep ``Main Results Table''.

\FOR{each main table and table description}
\STATE Extract Paper title, Dataset, Metrics, Experiment Settings, and Experiment Results as quintuple.
\ENDFOR

\textcolor{blue}{\textit{\# Stage 3: Leaderboard Generation}}
\STATE Recombine all quintuples and rank the quintuples by performance scores.
\STATE Output the Candidate Leaderboard $L_{ca}$
\ENDFOR
\ENDFOR

\textcolor{blue}{\textit{\# Stage 4: Quality Evaluation and Iteration}}
\STATE Evaluate and select the best leaderboard $L_{\text{best}} \leftarrow \text{Evaluate}({L_{ca1}, L_{ca2}, \ldots, L_{caN}})$
\STATE \textbf{Output:} Refined and evaluated leaderboard $L_{\text{best}}$
\end{algorithmic}
\end{algorithm}

\begin{table*}[ht!]
\centering

\begin{adjustbox}{width=1.0\textwidth}
\renewcommand\arraystretch{0.8}
\begin{tabular}{c|cc|c|c|cccc}
\toprule
% Survey Length (\#tokens) 
\multirow{2}{*}{\makecell{Leaderboard Length (items)}}  & \multicolumn{2}{c|}{Topic-related Quality} & \multirow{2}{*}{Model} & \multirow{2}{*}{Speed$_{/s}$} & \multicolumn{4}{c}{Content Quality} \\ 
& Recall & Precision & &  & Coverage & Latest & Structure & Multiaspect \\ \midrule
\multirow{5}{*}{5}  & \multirow{5}{*}{$76.57_{\pm 11.65}$}   & \multirow{5}{*}{$79.43_{\pm 8.86}$}                    
                        & Qwen2.5-7B     & $131.43$   & $3.60_{\pm 0.48}$     & $3.46_{\pm 0.49}$      & $3.18_{\pm 0.32}$                   & $3.41$     \\
                        & & & Qwen2.5-14B          & $129.51$   & $4.23_{\pm 0.38}$     & $4.14_{\pm 0.31}$      & $3.68_{\pm 0.29}$                   & $4.02$     \\ 
                        & & & GPT4-o  & $49.64$ &  $4.52_{\pm 0.42}$   &  $4.70_{\pm 0.32}$      & $4.32_{\pm 0.38}$  & $4.41$     \\
                       
                        & & & O1-preview & $79.67$   & $4.63_{\pm 0.48}$     & $4.71_{\pm 0.71}$      & $4.40_{\pm 0.33}$              & $4.58$\\ 
                        & & & Human Writing & $355$ & 4.89 & 4.83& 4.91& 4.88 \\
                        \midrule
\multirow{5}{*}{10}  & \multirow{5}{*}{$75.19_{\pm 9.81}$}   & \multirow{5}{*}{$80.05_{\pm 6.76}$}                    
                        & Qwen2.5-7B     & $156.41$   & $3.22_{\pm 0.48}$     & $3.41_{\pm 0.49}$      & $4.11_{\pm 0.39}$                   & $3.57$     \\
                        & & & Qwen2.5-14B    & $163.54$   & $3.91_{\pm 0.48}$     & $3.55_{\pm 0.49}$      & $3.41_{\pm 0.39}$                   & $4.61$     \\ 
                        & & & GPT4-o  & $88.96$ &  $4.68_{\pm 0.39}$   &  $4.59_{\pm 0.33}$ &  $4.45_{\pm 0.41}$  & $4.56$     \\
                        & & & O1-preview & $98.44$   & $4.40_{\pm 0.48}$     & $4.46_{\pm 0.71}$      & $4.31_{\pm 0.33}$   & $4.39$\\ 
                         & & & Human Writing & $612$ & $4.81$ & $4.72$ & $4.65$ & $4.72$\\
                        \midrule
\multirow{5}{*}{15}  & \multirow{5}{*}{$71.34_{\pm 8.39}$}   & \multirow{5}{*}{$74.58_{\pm 7.35}$}                    
                        & Qwen2.5-7B     & $183.45$   & $3.11_{\pm 0.28}$     & $3.23_{\pm 0.26}$      & $3.15_{\pm 0.27}$                   & $3.16$     \\
                        & & & Qwen2.5-14B  & $195.63$   & $3.68_{\pm 0.28}$     & $3.32_{\pm 0.19}$      & $3.18_{\pm 0.24}$                   & $3.39$     \\ 
                        & & & GPT4-o     & $105.61$  &  $4.47_{\pm 0.22}$   &  $4.16_{\pm 0.27}$      &  $4.32_{\pm 0.24}$           & $4.28$     \\
                       
                        & & & O1-preview & $109.33$   & $4.21_{\pm 0.48}$     & $4.06_{\pm 0.21}$      & $4.28_{\pm 0.31}$              & $4.18$\\ 
                        & & & Human Writing & $839$ & $4.71$ & $4.65$ & $4.44$ & $4.60$\\\midrule
\multirow{5}{*}{20}  &\multirow{5}{*}{$67.58_{\pm 9.12}$}   & \multirow{5}{*}{$70.33_{\pm 6.89}$}         
                        & Qwen2.5-7B     & $196.33$   & $3.03_{\pm 0.25}$     & $3.11_{\pm 0.31}$      & $2.98_{\pm 0.25}$                   & $3.16$     \\
                        & & & Qwen2.5-14B  & $208.64$   & $3.49_{\pm 0.34}$     & $3.17_{\pm 0.26}$      & $3.03_{\pm 0.28}$                   & $3.39$     \\ 
                        & & & GPT4-o     & $120.52$  &  $4.28_{\pm 0.28}$   &  $3.92_{\pm 0.22}$      &  $4.21_{\pm 0.25}$           & $4.13$     \\
                       
                        & & & O1-preview & $117.45$   & $4.12_{\pm 0.38}$     & $3.96_{\pm 0.25}$      & $4.16_{\pm 0.29}$              & $4.08$\\ 
                        & & & Human Writing & $1128$ & $4.72$ & $4.68$ & $4.34$ & $4.58$\\\midrule
\end{tabular}
\end{adjustbox}
\caption{\label{table:main-result} Results of leaderboard quality generated by LLMs in the first iteration. \textbf{Leaderboard Length}: The number of items in the leaderboard. For example, a 5-item leaderboard contains 5 baselines. \textbf{Topic-related Quality}: The precision and recall of each paper in relation to its relevance to the topic. \textbf{Speed}: The average time required to generate a single leaderboard. \textbf{Content Quality}: The evaluation results of the leaderboard content.}
\end{table*}

\section{Experiments}
We designed experiments for LAG, aiming to answer four questions: 
% RQ-1: Whether LAG can solve the paper coverage problem? RQ-2: Whether LAG-generated leaderboards can help researchers compare baselines fairly? RQ-3: Whether publications represented on the LAG-generated leaderboards are the latest?
RQ-1: Can LAG address the paper coverage issue and generate fair leaderboards by incorporating the latest baselines?
RQ-2: Can LAG reduce time consumption?
RQ-3: Is the evaluation consistent between LAG and human experts?
RQ-4: Is each proposed component of LAG useful?

\subsection{Experimental Setup}

We evaluated LAG's performance by testing its ability to generate leaderboards for specific topics across various complex settings. 

\subsubsection{Evaluation Metrics}
We use two metrics to evaluate the quality (topic-related and leaderboard content) and speed of leaderboard generation, respectively, in response to the three challenges mentioned in the introduction.

\noindent \textbf{(1) Topic-related Quality:} The aforementioned arXiv crawler employs regular expression matching in the abstract section to identify papers related to specified topics. While this method is efficient, it is relatively rudimentary and cannot guarantee that all retrieved papers meet our requirements. The quality of these papers not only directly affects the final leaderboard, but low-quality candidate papers can also significantly prolong the time required for construction. Therefore, it is essential to evaluate the quality of the retrieved articles. We evaluate the quality of content from the following two aspects.
\textbf{(i) Recall:} It measures whether all items in the generated leaderboard are related to the given research topic. \textbf{(ii) Precision:} It identifies irrelevant items, ensuring that the items in the generated leaderboards are pertinent and directly support the given research topic.

\noindent \textbf{(2) Leaderboard Content Quality:}
The evaluation metric of leaderboard content quality includes four aspects. Each aspect is judged by LLMs according to a 5-point, calibrated by human experts. 
The evaluation criteria are listed in Table \ref{table: criteria}. \textbf{(i) Coverage}: Assess each paper represented on the LAG-generated leaderboards encapsulates all aspects of the topic. \textbf{(ii) Latest}: Test whether all papers represented on the LAG-generated leaderboards are latest. \textbf{(iii) Structure}: Evaluate the logical organization and determine whether LAG leaderboards are missing any items. \textbf{(iv) Multiaspect}: Average score of the previous three criteria for LAG-generated leaderboards. 

\noindent \textbf{(3) Leaderboard Construction Speed:} Manually building a leaderboard is a time-consuming and laborious task. This process can be divided into the following main components: $T_{r}$ (search for papers on a specific topic), $T_{b}$ (browse all retrieved articles and develop several highly frequent datasets), $T_{f}$ (filter candidate articles based on the selected datasets), $T_{e}$ (read and extract information), and $T_{c}$ (the integration and construction time). And the total time consumption can be calculated as:
\begin{equation}
    T_\text{manual} = T_{r} + T_{b} + T_{f} + T_{e} + T_{c}.
\end{equation}

% Introducing the following variables helps to understand these time components: 
Given $L$ denotes the length of the leaderboard, $N_\text{retrieved}$ number of retrieved articles, $N_\text{filtered}$ number of articles retained, and $P$ the proportion of valid articles with $P=\frac{N_\text{filtered}}{N_\text{retrieved}}$. 
%It can be concluded that $T_{b}$ and $T_{f}$ are strongly correlated with leaderboard length $L$ and the crawl quality:
We find that $T_{b}$ and $T_{f}$ are strongly correlated with leaderboard length $L$ and the Topic-related quality:
\begin{equation}
    \{T_{b}, T_{f}\} \propto \frac{L}{P} = \frac{L \cdot N_\text{retrieved}}{N_\text{filtered}}.
\end{equation}

While $T_{r}$ is relatively fixed, $T_{e}$ and $T_{c}$ usually only have a positive correlation with $L$.

For LAG, we barely account for all the invocation time of the agents' API calls. Compared to manual work,  which often takes several days, LAG reduces the total time cost in the minute level. This is largely attributed to the task decomposition conducted in this paper, the division of labor and scheduling among agents, and the superior performance of the LLMs.

\subsubsection{Baselines}
% We employ the proprietary and open-source LLMs in our experiments and to enhance
% reproducibility, we set the temperature to 0.7 for proprietary models. 
We employ proprietary and open-source LLMs in our experiments and set the temperature to 0.7 for proprietary models.
For proprietary models, we adopt GPT-4o \cite{Achiam2023GPT4TR}, and the O1-preview. For open-source LLMs, we adopt Qwen2.5-7B and Qwen2.5-14B \cite{Yang2024Qwen25TR}. We provide a detailed illustration of our designed prompts for different stages in Appendix \ref{example:prompts}.

\subsection{Experiment Results}

\subsubsection{Performance Comparison (RQ-1)}
\textbf{Topic-related Quality Evaluation}: Table \ref{table:main-result} illustrates the Topic-related Quality\ LAG achieved a recall of 67.58\% and a precision of 70.33\% with 20 items, indicating that it successfully retrieved a large proportion of relevant papers while maintaining a low rate of irrelevant ones. This performance is crucial for ensuring that the generated leaderboards are both comprehensive and accurate. The high precision and recall scores illustrate that LAG could help solve the paper coverage problem.

\noindent \textbf{Fair Comparison}: To ensure fair comparison, LAG extracted all experiment settings as part of the LAG-generated leaderboards. We provide a detailed case study of LAG-generated leaderboards with experiment settings in Appendix \ref{example:leaderboards}. 

\noindent \textbf{Content Quality Evaluation}: Table \ref{table:main-result} presents the results of leaderboard quality generated by LAG and the baselines. LAG consistently achieved high scores across all evaluation metrics, particularly in terms of Coverage and Latest, indicating its ability to include a wide range of relevant and recent papers. For example, at a leaderboard length of 20 items, LAG achieved a Coverage score of 4.12 and a Latest score of 3.96, approaching human performance (4.72 and 4.68, respectively). While manual leaderboards scored slightly higher in content quality, LAG significantly reduced the time required for leaderboard generation, demonstrating its efficiency.


\begin{figure}[htb]
\centering
\includegraphics[width=1.0\linewidth]{figs/iteration.png}
\caption{\label{iteration} Impact of Iteration on LAG Performance.}
\end{figure}

\begin{table*}[t!]
\centering

\begin{adjustbox}{width=1.0\textwidth}
\renewcommand\arraystretch{0.8}
\begin{tabular}{c|c|c|cccc}
\toprule
% Survey Length (\#tokens) 
\multirow{2}{*}{\makecell{Methods}} & \multirow{2}{*}{\makecell{Leaderboard Length (items)}}  & \multirow{2}{*}{Speed$_{/s}$} & \multicolumn{4}{c}{Content Quality} \\ 
& &  & Coverage & Latest & Structure & Multiaspect \\ \midrule                 
LAG \textit{w/o} Table Classification & \multirow{3}{*}{5} & $42.35$   & $4.43$  & $4.52$  & $3.95$                   & $4.30$     \\
LAG \textit{w/o} Refinement & & $43.58$   & $4.41$     & $4.46$      & $4.05$ & $4.31$     \\ 
LAG & & $49.64$   & $4.52$     & $4.70$      & $4.32$              & $4.51$\\ \midrule


LAG \textit{w/o} Table Classification & \multirow{3}{*}{10} & $81.37$   & $4.31$  & $4.36$  & $4.01$                   & $4.33$     \\
LAG \textit{w/o} Refinement & & $80.52$   & $4.24$     & $4.17$      & $3.88$ & $4.10$     \\ 
LAG & & $88.96$   & $4.68$     & $4.59$      & $4.45$   & $4.56$\\
\midrule


LAG \textit{w/o} Table Classification & \multirow{3}{*}{15} & $93.28$   & $4.13$  & $4.08$  & $3.61$                   & $3.94$     \\
LAG \textit{w/o} Refinement & & $91.32$   & $4.19$     & $4.13$      & $3.72$ & $4.01$     \\ 
LAG & & $105.61$  & $4.47$   &  $4.16$      &  $4.32$           & $4.28$\\
\midrule


LAG \textit{w/o} Table Classification & \multirow{3}{*}{20} & $105.31$   & $3.92$  & $3.88$  & $3.51$                   & $3.77$     \\
LAG \textit{w/o} Refinement & & $99.35$   & $3.85$     & $3.71$      & $3.62$ & $3.72$     \\ 
LAG &  & $120.52$  &  $4.28$ &  $3.92$  &  $4.21$  & $4.13$\\ \midrule
\end{tabular}
\end{adjustbox}
\caption{\label{table:ablation}Ablation Study for LAG with different components removed. We use the GPT-4o as the backbone of LAG.}
\end{table*}
\textbf{Iteration Evaluation}: 
To ensure the high-quality of LAG-generated leaderboards, we iterated the process to evaluate the performance change during the whole iteration.
Figure \ref{iteration} presents the effect of different iteration counts on the performance of LAG. The results show
that increasing the number of iterations from 1 to 5 provides
a significant improvement in structure quality and Coverage quality scores.  However, the latest score remains at a relatively high level, which is because in stage 1 of LAG, the old papers are filtered out.


Our experiments demonstrate that LAG is highly effective in generating high-quality, up-to-date leaderboards across various research topics. The framework's ability to dynamically update leaderboards and extract detailed experiment settings ensures a fair comparison between state-of-the-art baselines. While LAG's content quality scores are slightly lower than those of manually created leaderboards, its efficiency and scalability make it a valuable tool for researchers in rapidly evolving fields like AI and NLP.

\subsubsection{Efficiency Analysis (RQ-2)}

\textbf{Construction Speed}: LAG dramatically reduced the time required to generate leaderboards compared to manual methods. For instance, generating a 20-item leaderboard with LAG took approximately 120 seconds, while manual construction took over 18 minutes. This speed advantage makes LAG a practical tool for researchers who need up-to-date leaderboards in rapidly evolving fields. The high speed of LAG illustrates that LAG could help generate high-quality leaderboards timely.

\subsubsection{Meta Evaluation (RQ-3)}
\begin{figure}[htb]
\centering
\includegraphics[width=1.0\linewidth]{figs/chart.png}
\caption{\label{evaluation} Pearson Correlation Coefficient values given by four LLMs and human experts. Note that the Pearson Correlation Coefficient is between -1 and 1, the larger value indicates more positive correlations.}
\end{figure}

To verify the consistency between our proposed LLM evaluation strategy and human evaluation, we conduct a correlation evaluation involving human experts and our automated evaluation method. Human experts judge pairs of generated leaderboards to determine which one is superior. 
We compare the judgments made by our method against those made by human experts. Specifically, we provide experts with the same scoring criteria used in our evaluation for reference. The experts rank the 20 LAG-generated leaderboards, and we compare these rankings with those generated by the LLM using the Pearson Correlation Coefficient to measure consistency between human and LLM evaluations.

The results of this meta-evaluation are presented in Figure \ref{evaluation}. The table shows the Pearson Correlation Coefficient values, indicating the degree of correlation between the rankings given
by each LLM and the human experts. The Pearson correlation coefficient values indicate a strong positive correlation between the quality scores provided by the LLM and those given by human experts, with the O1-preview achieving the highest correlation at \textbf{0.76}. These results suggest that our evaluation method aligns well with human preferences, providing a reliable proxy for human evaluation.


\subsubsection{Ablation Study (RQ-4)}
To understand the contribution of each component in LAG, we conducted an ablation study by removing key components of LAG as follows: (1) LAG w/o Table Classification: We removed the table classification step, which led to a slight decrease in Structure and Multiaspect scores, indicating that classifying tables is essential for maintaining a logical and well-organized leaderboard. (2) LAG w/o Refinement: We disabled the Refinementing step, which resulted in a minor drop in Coverage and Latest scores, suggesting that Refinementing helps refine the leaderboard by ensuring that only the most relevant and recent papers are included. As shown in Table \ref{table:ablation}, the results of the ablation study confirm that each component of the LAG plays a crucial role in achieving the generation of the high-quality leaderboard.



\section{Conclusion}
We introduce LAG, a novel agent framework leveraging large language models to automatically generate the latest, and high-quality leaderboards based on given research topics. LAG addresses key challenges including paper coverage, fair comparison, and timeliness through a systematic approach
involving paper collection and split, table extraction and classification, table unpacking and integration, and leaderboard generation and evaluation.  Experiments showed that LAG can automatically generate new, high-quality leaderboards in a relatively short time and match human performance topic-related quality and content qulaity. This advancement offers a scalable and effective solution for synthesizing the latest leaderboards, providing a valuable tool for researchers in rapidly evolving fields like artificial intelligence.


\section*{Limitations}
One limitation of LAG is its reliance on the quality of the retrieved papers. While our topic-related quality metrics are strong, there is still room for improvement in ensuring that all relevant papers are included. Future work could explore more sophisticated retrieval models to further enhance the coverage of the generated leaderboards. Another limitation is, a specific dataset may contain several evaluation metrics, and different papers may use different metrics to evaluate proposed models' performance, bringing challenges for leaderboard generation and baseline comparison.

\bibliography{custom}

\appendix

\begin{table*}[ht!]
\centering
\caption{Leaderboard Quality Criteria.}
\label{table: criteria}
\tiny
\begin{tabular}{p{1cm} p{15cm}}
\toprule
\textbf{Criteria} & \textbf{Scores} \\
\midrule
\textbf{Coverage} & The ratio of the number of papers used for leaderboard generation to the total number of papers searched. $(P_{used}/P_{total})*5$\\
\midrule
\textbf{Latest} & The ratio of the number of papers published after the certain date to the total number of papers searched. $(P_{new}/P_{total})*5$\\
\midrule
\textbf{Structure} & 

                \textit{Score 1}: The structure of the leaderboard lacks logic, making it difficult to understand and navigate. The table header and each row are not clearly organized and connected. \\
                & \textit{Score 2}: The structure of the leaderboard have some contents arranged in a disordered or unreasonable manner. However, the overall structure is reasonable and coherent. \\
                & \textit{Score 3}: The survey is generally comprehensive in coverage but still misses a few key points that are not fully discussed. \\
                & \textit{Score 4}: The structure of the leaderboard is generally reasonably logical, with most header items arranged orderly, though some header items may be repeated or redundant. \\
                & \textit{Score 5}: The structure of the leaderboard has good logical consistency, with each line strictly related to the header items and the previous line. But it can be optimized in terms of easy understanding. \\
\midrule
\textbf{Multi-Aspect} & The evaluation metric for multi-leaderboard. Specifically, a research topic $T$ may have $N$ different datasets, and thus we can get $N$ leaderboards, the score of the Multi-Aspect is computed based on the average of all the $N$ scores. $(N_{Coverage}+N_{Latest}+N_{Structure})/(3*N)$. \\
\bottomrule
\end{tabular}
\end{table*}
\section{Example Prompts}
\label{example:prompts}
The prompts of instructing LLMs in different stages of LAG are illustrated in Prompts \ref{box:prompt-example1}, \ref{box:prompt-example2}, and \ref{box:prompt-example3}.

\begin{story*}[ht]
 \framedbox[\textwidth]{
\textbf{<instruction>}

You are an expert in summarizing and extracting key content from LaTeX-formatted academic papers on computers and artificial intelligence. Please output your reply in the following JSON format: 

\textbf{<format>}\textcolor{blue}{\textbf{[EXAMPLE JSON]}}\textbf{</format>}

========================================================================
There are some key points to note:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item In the "selected table's core results", other models' results are of no concern and should be omitted.
    \item The table's header metrics should be the same as the evaluation metrics chosen.
    \item The number of items in the "classification of tables" dict should be equal to the "number of tables" int value. These two items help you to identify the main result tables better.
    \item All three items about the settings in the JSON output should be corresponding to the proposed method's best performance in the selected table. 
    \item Sometimes in the selected table, the proposed method's performance may not be unique (e.g., different hyperparameters or training strategies), you need to choose the best one and it usually appears in the last row of the table.
    \item If there are multiple tables that meet the requirements (both being the main result table and based on the specified dataset), choose the one with richer information.
\end{itemize}

========================================================================
Here, I provide you with an example of the complete process to help you understand your task. 

First, I provide you an article: 

\textbf{<article>}\textcolor{blue}{\textbf{[EXAMPLE ARTICLE]}}\textbf{</article>}

Afterwards, I specify the dataset as \textcolor{blue}{\textbf{[EXAMPLE DATASET]}}, you should output:

\textbf{<format>}\textcolor{blue}{\textbf{[EXAMPLE RESPONSE]}}\textbf{</format>}

\textbf{</instruction>}
}
 \caption{The prompt of the table extraction agent, with the table classification COT procedure.}
 \label{box:prompt-example1}
\end{story*}

\begin{story*}[ht]
 \framedbox[\textwidth]{
\textbf{<instruction>}

You are an expert in summarizing and extracting key content from LaTeX-formatted academic papers on computers and artificial intelligence. Please output your reply in the following JSON format: 

\textbf{<format>}\textcolor{blue}{\textbf{[EXAMPLE JSON]}}\textbf{</format>}

========================================================================
There are some key points to note:
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
    \item In the "main result table's core results", other models' results are of no concern and should be omitted.
    \item The main result table's header metrics should be the same as the evaluation metrics chosen.
    \item All three items about the settings in the JSON output should correspond to the proposed method's best performance in the selected table. 
    \item Sometimes in the main result table, the proposed method's performance may not be unique (e.g., different hyperparameters or training strategies), you need to choose the best one and it usually appears in the last row of the table.
    \item If there are multiple tables that meet the requirements (both being the main result table and based on the specified dataset), choose the one with richer 
          information.
\end{itemize}

========================================================================
Here, I provide you with an example of the complete process to help you understand your task. 

First, I provide you an article: 

\textbf{<article>}\textcolor{blue}{\textbf{[EXAMPLE ARTICLE]}}\textbf{</article>}

Afterwards, I specify the dataset as \textcolor{blue}{\textbf{[EXAMPLE DATASET]}}, you should output:

\textbf{<format>}\textcolor{blue}{\textbf{[EXAMPLE RESPONSE]}}\textbf{</format>}

\textbf{</instruction>}
}
 \caption{The prompt of the table extraction agent, w/o the table classification COT procedure.}
 \label{box:prompt-example2}
\end{story*}

\begin{story*}[ht]
 \framedbox[\textwidth]{
\textbf{<instruction>}

You are an expert in constructing the Artificial Intelligence leaderboard. Please refer to the content I  provide you to answer the user's questions. The contents I provide you are a number of structured summaries extracted from computer/artificial intelligence papers. 

You need to build a markdown format leaderboard (showcase the performance of the models on the same dataset, each line representing a specific model) based on the titles, experimental settings, and evaluation metrics of these articles. Please output your reply in the Markdown format. 

Here, I list a complete example of the question and the answer to help you understand your task. For example, I provide you a list of JSON files containing the extracted content of the articles:
\textcolor{blue}{\textbf{[JSON LIST]}}

The expected leaderboard that you generate should be: \textcolor{blue}{\textbf{[EXAMPLE LEADERBOARD]}}

========================================================================

Pay attention: The leaderboard should be in the Markdown format and reflect all the articles provided! 

The leaderboard in the dictionary format is forbidden!

In the above case, selecting Pre and Rec as the metrics in the final leaderboard is not appropriate because in most articles the corresponding performance values are absent.

========================================================================
Here, the target list of extracted content of the articles is as follows: \textcolor{blue}{\textbf{[TARGET JSON LIST]}}

Please give me the well-organized leaderboard of the provided articles. The leaderboard should have consistent performance metrics, github links, settings and the title of the article, etc. The leaderboard should be in the Markdown format and reflect all the articles provided.

Warning: 

\begin{itemize}
    \item I need a well-organized markdown-format leaderboard containing all the articles' information. 
   The leaderboard's max serial number in the "No." column should equal to the number of articles provided.
   \item When selecting metrics, you need to consider their text descriptions. The same metric may have multiple different abbreviations. In the final table, there must not be any duplicate metrics (it is unacceptable to have duplicates where different abbreviations represent the same meaning).
   \item Large-scale omissions are not allowed! For each model, only a small portion of the results are missing under the selected metrics. The vast majority of the metrics have corresponding values.
   The abbreviations for the same metric may be different, but you need to avoid being misled by the abbreviations.
   \item Use approximate intersections to select metrics from the given articles, while avoiding a large amount of data waste. Allow some models to have a certain degree of data missing under the selected metrics.
   \item The content in the "Experimental Setting" column should be concise and non-descriptive, just a few words.
   \item When different articles use different units for the same metric, please note that you need to convert them when integrating them so that the units in the final leaderboard are consistent. 
   For example, 50\% is equal to 0.5. "50" and "0.5" should not be presented in the same column of a leaderboard. 
   \item Check each column corresponding to the selected metrics in the final leaderboard. If more than 60\% of the values in that column are missing or represented by placeholders, the metric should be discarded.
\end{itemize}

\textbf{</instruction>}
}
 \caption{The prompt of the leaderboard construction agent.}
 \label{box:prompt-example3}
\end{story*}

% Define JSON language for listings
\lstdefinelanguage{json}{
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\scriptsize,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{gray!5},
    showstringspaces=false,
    string=[db]{"},
    stringstyle=\color{green!50!black},
    morestring=[s][\color{black}]{\ \ "}{":},
    keywordstyle=\color{blue},
    keywords={true,false,null},
}

\newpage
\begin{table*}[ht!] % Adjust the width as needed
\begin{lstlisting}[language=json]
{
    "title": "The title of the paper (String)",
    "number of tables": "The number of tables in the paper, denoted as n (Int)",
    "classification of tables": "The classification of tables in the paper, including 4 types: main result/comparison tables(0), ablation tables(1), hyperparameter tables(2), and others(3). You should output a dictionary with the number of tables and their corresponding types, formed as {"0":0, "1":2, ..., "n-1":3} (Dict)",
    "selected table's index": "The index of the main result table focused on the specified dataset [SPECIFIED DATASET], denoted as i (Int)",
    "metrics": "The evaluation metrics chosen to assess performance of the method proposed in this paper. This information is extracted from the textual portion of the 'Experimental' related section (String)",
    "selected table's metrics": "Metrics used in the selected main result table, it should be almost the same as the metrics extracted from the textual. Remove the latex format syntax (String)",
    "selected table's core results": "A dictionary only containing this paper's model best performance on the selected dataset, with the metrics as keys and the corresponding values (Dict)",
    "selected table's settings (model & size)": "In computer vision, the model usually means the backbone architecture of the network, such as ResNet, ViT, and so on. The size can be omitted if not specified. In NLP, the model and size are usually organized as a string, such as 'LLAMA-7B', 'GPT-3', and so on (String)",
    "selected table's settings (training strategy)": "Training strategy usually refers to the concepts like: fine-tuning, transfer learning, linear-probing, reinforce learning, one-shot, few-shot, prompt-learning, semi/self supervised and so on (String)",
    "selected table's settings (hyperparameter selection)": "The hyperparameters used in the model, such as learning rate, batch size, and so on. You should output a dictionary with the hyperparameters and their values (Dict)",
    "github": "The link to the gitHub repository containing the code for this paper, if available (String)"
}
\end{lstlisting}
\caption{The example JSON file of the table extraction agent with table classification COT.}
\end{table*}

\begin{figure*}
    \centering
    \adjustbox{center}{\includegraphics[width=1.0\linewidth]{Leaderboard-SSMIS-20.pdf}}
    \caption{A leaderboard (20 lines) of semi-supervised medical image segmentation on the LA dataset, using GPT-4o for table extraction and Qwen2.5-14B for leaderboard construction \& refinement.}
    \label{fig:leaderboard1}
\end{figure*}

\begin{figure*}
    \centering
    \adjustbox{center}{\includegraphics[width=1\linewidth]{Leaderboard-IQA-20.pdf}}
    \caption{A leaderboard (20 lines) of image quality assessment on the LIVE dataset, using GPT-4o for both table extraction and leaderboard construction \& refinement.}
    \label{fig:leaderboard2}
\end{figure*}

\begin{figure*}
    \centering
    \adjustbox{center}{\includegraphics[width=1\linewidth]{Leaderboard-IQA-5.pdf}}
    \caption{A leaderboard (5 lines) of image quality assessment on the LIVE dataset, using GPT4-o for both table extraction and leaderboard construction \& refinement.}
    \label{fig:leaderboard3}
\end{figure*}

\section{Example LAG-generated Leaderboards}\label{example:leaderboards}
Figure \ref{fig:leaderboard1} and \ref{fig:leaderboard2} illustrate two examples generated by LAG. The papers in the first leaderboard are the latest methods of semi-supervised medical image segmentation on the LA dataset from March to December in 2024. The second leaderboard collects the most recent methods for image quality assessment conducted on the LIVE dataset from February 2022 to November 2024. To ensure that the table content is fully displayed, the "model \& size" and "hyperparameters selection" within the experimental settings are presented beneath the paper titles.

First and foremost, when viewed holistically, both leaderboards with 20 entries, whether utilizing qwen2.5-14B or GPT-4o as the construction model, exhibit a notably high level of completeness. Upon specific analysis of the missing information, in Leaderboard 1, LAG failed to successfully extract the HD value from "Self-Paced Sample Selection for Barely-Supervised Medical Image Segmentation" (No. 11) because the metric was referred to as 95HD in the original text. Although our design accounts for such situations: our designed agent is required to extract metrics from both text and tables to avoid confusion caused by abbreviations. This design has successfully resolved most of the issues arising from abbreviations, but such errors still occur with a small probability. The absence of metrics in entries No. 13 and No. 20 is acceptable because the original text indeed lacks these metrics.  The situation in Leaderboard 2 is similar; the only two missing items (No. 4 and No. 14) are also due to the absence of corresponding results in the original texts. 

The higher missing rate in the 5-row leaderboard compared to the 20-row leaderboard for the LIVE dataset can be attributed to the following reasons: When only 5 papers are included, LAG extracts a larger number of metrics, including RMSE, mIoU, and mAcc. The missing values for these metrics are tolerable in a 5-row leaderboard. However, when expanding to a 20-row leaderboard, the excessive number of missing values forces LAG to discard these metrics to ensure that the leaderboard conveys meaningful information.

Secondly, regarding the experimental settings, we observe that in Leaderboard 1, the information on "model \& size", "hyperparameters", and "training strategy" is both accurate and comprehensive. Notably, there is a consistent thread throughout the hyperparameters: the portion of labeled data. In contrast, Leaderboard 2 discards the hyperparameter information compared to Leaderboard 3. This is because we require LAG to extract hyperparameter information in a way that not only maintains completeness but also focuses on the intrinsic connections between different items. If the deviation is too large (i.e., if it cannot provide users with a concise and effective summary), the information should be discarded. Therefore, when the number of input papers for LAG increases from 5 to 20, the hyperparameter settings in the topic of image quality assessment do not have a clear and unified theme and thus are ultimately ignored.

\section{Cost Analysis}\label{tab:cost analysis}
We calculate the average number of input \& output tokens required to generate a 20-entry leaderboard, along with the cost analysis using different LLMs, as shown in Table \ref{tab:cost}. The computational cost of all models remains within 14\$, indicating that LAG is also economically efficient. Overall, the LAG framework consumes more input tokens, while the output tokens represent only a small proportion. OpenAI prices output tokens significantly higher than input tokens, often reaching 4-5 times the cost of input tokens. However, considering the disparity in token numbers, the overall cost remains acceptable. 

\begin{table*}
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{3cm}>{\centering\arraybackslash}p{2.5cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}}
    \hline
         \textbf{Input tokens} &  \textbf{Output tokens} & \textbf{Qwen2.5-7/14B} &  \textbf{kimiAI-128k} &  \textbf{GPT4-o} & \textbf{O1-preview} \\ \hline
         834.7K &  8.9K & 0 &  50.616 ¥ & 2.176 \$ & 13.055 \$ \\ \hline
    \end{tabular}
    \caption{Cost of LAG}
    \label{tab:cost}
\end{table*}

\end{document}
