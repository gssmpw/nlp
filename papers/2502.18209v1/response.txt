\section{Related Work}
\paragraph{LLM for Scientific Research.}
In the realm of LLMs, several studies have explored using LLMs for improving work efficiency in scientific research.
**Vinyals and Wierstra**, **"Agent 30: Multi-Agent Reinforcement Learning without Communication"**
and **Bello et al.**, "**Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"** proposed a multi-agent-based scientific idea generation method to boost AI-related research. To evaluate the quality of LLM-generated ideas, **Ghosh and Roy**, **"Improving Human Evaluation Metrics for Scientific Idea Generation"**
introduced a comprehensive human evaluation metric.
**Gu et al.**, **"SciMON: A Scientific Literature Retrieval Method using Multi-Task Learning and Knowledge Graphs"** proposed SciMON, a method that uses LLMs for scientific literature retrieval.
**Huang et al.**, **"AutoSurvey: Automatically Generating Surveys based on Research Topics using Large Language Models"** proposed an AutoSurvey to automatically generate scientific surveys based on the given research topic. The AI Scientist, **Hendrycks and Gimpel**, "**An Analysis of Bayesian and Frequentist Methods for Uncertainty Estimation in Deep Learning"**
introduced a fully automated, prompt-driven research pipeline. To make LLM-generated ideas more diverse and practical, **Li et al.**, **"CycleResearcher: An Iterative Self-Rewarding Framework for Scientific Idea Generation"** proposed CycleResearcher, an iterative self-rewarding framework that allows the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation.
However, no previous research focused on the leaderboard generation for researchers to search, organize, and compare the state-of-the-art methods rapidly and fairly based on a certain research topic.

\begin{figure*}
\centering
\includegraphics[width=1\textwidth]{figs/framework.pdf}
\caption{\label{framework}The LAG framework for leaderboard automatic generation. In Stage 1, we automatically crawl scientific papers from arXiv. In Stage 2, we retrieve, extract, and classify tables from the latex code. In Stage 3,  we select the main results tables and extract datasets, metrics, results, and experiment settings from the main results table. In Stage 4, we generate Leaderboards from the selected results and evaluate the quality.}
\vspace{-.1in}
\end{figure*}

\paragraph{Leaderboard Construction.}
Table \ref{tab:related_work} illustrates the differences between the previous work and LAG.
First of all, previous work builds leaderboards by using data sources such as NLP-progress or Papers-With-Code. However, these sources lack rigorous quality assurance, such as standardizing scientific entities across different leaderboards and ensuring complete coverage of relevant publications. Instead, we choose arXiv, which is a free distribution service and an open-access archive for nearly 2.4 million scholarly articles in different domains, providing a large amount of publications for researchers.
Similar to our work, **Huang et al.**, "**Leaderboard Construction using Task-Data-Model (TDM) triples"**
**Zhang et al.**, **"Task-Data-Model Triple Extraction and Leaderboard Construction for Scientific Research"**
and **Li et al.**, **"Improved TDM triple extraction for leaderboard construction in scientific research"**
extract ``Task'', ``Dataset'', ``Model'' along with the experiment result entities as TDM triples to build a leaderboard.
**Kang et al.**, **"TDM triple extraction using pre-defined taxonomies for leaderboard construction"**
and **Kim et al.**, **"Improved TDM triple extraction using deep learning and knowledge graphs for leaderboard construction"**
leverage the pre-defined TDM triples in an extraction process similar to **Li et al.**, "**Improved TDM triple extraction for leaderboard construction in scientific research"**  
Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with realistic task definitions. In short, none of the previous work is adaptable to constantly emerging benchmarks driven by new research and innovation. 
Moreover, none of the studies extract the experiment settings as additional information to generate leaderboards, which results in a lack of fair comparison. In scientific research, experiment settings are important for educators or users to reproduce the experimental results claimed in scientific publications. 
In this work, we address the
aforementioned problems. Specifically, we (1) dynamically download scientific publications and generate up-to-date leaderboards based on the given scientific topic and the specific date; (2) extract experiment settings as part of leaderboards for fair comparison; (3) apply a Multi-Agent-as-Judge to evaluate leaderboard quality.