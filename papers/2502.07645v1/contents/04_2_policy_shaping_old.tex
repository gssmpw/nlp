\section{Policy shaping}
\label{sec:Policy_shaping}


In this section, we show the connection between a policy and a desired action space.
In section \ref{section:prob_desired_action_space}, we formulate the Bayes estimates of the desired action space, where the probability of how an action can explain the received feedback and the probability of how actions can be sampled from the desired action space enforced by the received feedback are detailed.
In section \ref{},
we formulate the maximum likelihood estimation, where the policy is 



% \begin{align}
% \ell (\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \frac{\exp (\pi_{\bm \theta}(\bm a^{-}|\bm s) )}{\exp (\pi_{\bm \theta}(\bm a^{+}|\bm s) ) + \exp (\pi_{\bm \theta}(\bm a^{-}|\bm s) )}
% \end{align}


% \begin{align}
%    \mathbb{E}_{(\bm a^{+},  \bm a^{-}, \bm s) \sim  \mathcal{D}} \mathbb{E}_{\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}}[\log \pi_{\theta}(\bm a | \bm s)]
% \end{align}

\subsection{Probabilistic formulation of desired action space}

Then we can introduce the observation model as

\begin{align*}
    p (I | \bm a, \bm s) =   \left\{ {\begin{array}{*{20}{c}}
	 \sigma_T(\| \bm a- \bm a_1\| - \| \bm a- \bm a_2\|), I = 1\\
	 \sigma_T(\| \bm a- \bm a_2\| - \| \bm a- \bm a_1\|), I = 0
	\end{array}} \right. 
\end{align*}

$p (I  = 1| \bm a, \bm s) = 1-p (I=0 | \bm a, \bm s)$ could also remove 

where $\sigma_T(x) = \frac{1}{1 + \exp (- x/T)}$ is the sigmoid function and the temperature parameter $T > 0$ controls how sharply the sigmoid function transitions between 0 and 1. 

$p (I = 1 | \ \bm a, \bm s) = p(\bm a \in \mathcal{A}{(\bm a_1, \bm a_2)} |
 \bm a, \bm s) $  defines 
 % how the action $\bm a$ being the optimal action can explain the data $(\bm a^-, \bm a^+)$ and the corresponding action space $\mathcal{A}^{(\bm a^-, \bm a^+)}$ is the desired action space $\mathcal{A}^d{(\bm a^-, \bm a^+)}$ .  
 how likely the action set $\mathcal{A}{(\bm a_1, \bm a_2)}$ is to contain the action $\bm a$.
 We are interested in how likely the desired action space $\mathcal{A}^d{(\bm a^-, \bm a^+)}$ is to contain the action $\bm a$, which is defined as follows:
% \begin{align}
%     p (\mathcal{A}^{(\bm a^-, \bm a^+)} | \bm a^* = \bm a) = \frac{1}{1 + \exp (\| \bm a- \bm a^+\| - \| \bm a- \bm a^-\|)} 
% \end{align}
\begin{align*}
    p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm a , \bm s) =   \sigma_T(\| \bm a- \bm a^-\| - \| \bm a- \bm a^+\|) 
\end{align*}
The value of the observation model for different actions is visualized in Fig. \ref{fig:Fig2_illustration_2d_measurement_model}. As shown in the figure, the implicit information of one corrective feedback can be utilized as a way of data augmentation to remove some actions from the desired action space. 
Therefore, we define the observation model including the data augmentation as
\begin{align*}
   p ( \bm a \in \mathcal{A}^{C} (\bm s, \bm a, e\bm h) |  \bm a, \bm s) = 
    p \left(\bm a \in \bigcap_{i=0}^n \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})} | \bm a , \bm s \right)
    \\
    = \prod_{i=1}^n p (\bm a \in \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})}| \bm a , \bm s) 
\end{align*}
\textcolor{red}{explain clearly to reuse the concept}
Define the prior as $p(\bm a | \bm s)$,
% Assume a uniform distribution of $p(\bm a | \bm s)$,
the posterior belief of the action $ p (  \bm a |\bm a^+ \succeq \bm a^-,  \bm s)$ can be obtained as 


\begin{align}
p ( \bm a |  \bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)},  \bm s) =\frac{ p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm a , \bm s) p(\bm a | \bm s)}{p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm s)} \\
    p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm s) = \int_{\bm a'} p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm a' , \bm s) p(\bm a'  | \bm s) d \bm a'
\end{align}
By assuming a uniform distribution of $p(\bm a | \bm s)$, the term $p ( \bm a |  \bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)},  \bm s)$ represents the probability that $\bm a$ is the optimal action, given the feedback $\bm a^-$ and $\bm a^+$.
The probability can also be interpreted as the likelihood of $\bm a$ being sampled from $\mathcal{A}^{(\bm a^-, \bm a^+)}$, defined by the given feedback.

We can set the prior $p(\bm a | \bm s)$ as $\pi(\bm a| \bm s)$, then
the posterior can be denoted as $ p^{\pi} (  \bm a |\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)},  \bm s)$, which represents the probability of the action being sampled by the policy $\pi$ filtered by $\mathcal{A}^{(\bm a^-, \bm a^+)}$. 
\begin{align}
p^\pi ( \bm a |  \bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)},  \bm s) \propto { p (\bm a \in \mathcal{A}^d{(\bm a^-, \bm a^+)} | \bm a ,\bm s) \pi(\bm a | \bm s)} 
\end{align}


\subsection{Probabilistic formulation of desired action space}
\label{section:prob_desired_action_space}

From section \ref{sec:Desired_action_space} we have 
\begin{align}
\bm a^* \in \mathcal{A} {(\bm a^r, \bm a^h)} \Leftrightarrow \mathcal{A} {(\bm a^r, \bm a^h)}  = \mathcal{A}^d {(\bm a^r, \bm a^h)}  	\Leftrightarrow \bm a^h \succeq \bm a^r
\end{align}
By using $\bm a^h \succeq \bm a^r$, we are refering to that $\bm a^+ = \bm a^ h$ and  $\bm a^- = \bm a^r$.


Inspired by the formulation of learning from preference feedback, we define the observation/decision variable as $I = \mathbf{1}_{\bm a_h \succeq \bm a_r}  = \mathbf{1}_{\bm a^* \in \mathcal{A}^{(\bm a^r, \bm a^h)}} \in \{0, 1\}$ , where $I = 1$ means that $\bm a^{+} = \bm a_h$ and $I = 0$ means that $\bm a^+ = \bm a_r$.
Then we can introduce the observation model as
% \begin{align*}
%     p (I | \bm a^*(\bm s) = \bm a) =   \left\{ {\begin{array}{*{20}{c}}
% 	{ \frac{\exp (\| \bm a- \bm a_1\|)}{\exp (\| \bm a- \bm a_2\|) + \exp (\| \bm a- \bm a_1\|) }}, I = 1\\
% 	{ \frac{\exp (\| \bm a- \bm a_2\|)}{\exp (\| \bm a- \bm a_2\|) + \exp (\| \bm a- \bm a_1\|) }}, I = 0
% 	\end{array}} \right. 
% \end{align*}


\begin{align*}
    p (I |  \bm a^* = \bm a, \bm s) =   \left\{ {\begin{array}{*{20}{c}}
	 \sigma_T(\| \bm a- \bm a_r\| - \| \bm a- \bm a_h\|), I = 1\\
	 \sigma_T(\| \bm a- \bm a_h\| - \| \bm a- \bm a_r\|), I = 0
	\end{array}} \right. 
\end{align*}


where $\sigma_T(x) = \frac{1}{1 + \exp (- x/T)}$ is the sigmoid function and the temperature parameter $T > 0$ controls how sharply the sigmoid function transitions between 0 and 1. 

For corrective feedback, $I$ is always 1, as we prefer human action over robot action.
The term $p (I = 1 | \bm a^*= \bm a, \bm s) = p(\bm a^* \in \mathcal{A}^{(\bm a^-, \bm a^+)} |
\bm a^*=   \bm a, \bm s) =  p (\bm a^+ \succeq \bm a^- | \bm a^*= \bm a, \bm s )$  defines how  likely the corresponding action space $\mathcal{A}{(\bm a^-, \bm a^+)}$ is the desired action space $\mathcal{A}^d{(\bm a^-, \bm a^+)}$, if the action $\bm a$ is the optimal action.  
The probability also represents how likely the $\mathcal{A}{(\bm a^-, \bm a^+)}$ is to contain the action $\bm a$.
For simplicity, this probability is expressed as $ p (\bm a^+ \succeq \bm a^- | \bm a, \bm s)$, which is defined as follows:
% \begin{align}
%     p (\mathcal{A}^{(\bm a^-, \bm a^+)} | \bm a^* = \bm a) = \frac{1}{1 + \exp (\| \bm a- \bm a^+\| - \| \bm a- \bm a^-\|)} 
% \end{align}
\begin{align*}
    p (\bm a^+ \succeq \bm a^- | \bm a , \bm s) =   \sigma_T(\| \bm a- \bm a^-\| - \| \bm a- \bm a^+\|) 
\end{align*}
The value of the observation model for different actions is visualized in Fig. \ref{fig:Fig2_illustration_2d_measurement_model}. As shown in the figure, the implicit information of one corrective feedback can be utilized as a way of data augmentation to remove some actions from the desired action space. 
Therefore, we define the observation model including the data augmentation as
\begin{align*}
   p ( \bm a^* \in \mathcal{A}^{C} (\bm s, \bm a, e\bm h) |  \bm a) = 
    p \left(\bm a^* \in \bigcap_{i=0}^n \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})} | \bm a \right)
    \\
    = \prod_{i=1}^n \mathbb{P} (\bm a^* \in \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})}| \bm a) \\
    = \prod_{i=1}^n \mathbb{P} (\bm a^{I+} \succeq \bm a^{I-}_i| \bm a)
\end{align*}
\textcolor{red}{explain clearly to reuse the concept}

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.42]{figs/Fig2_illustration_2d_measurement_model.png}
	\caption{ \textcolor{red}{Specify that which variable are fixed and which are not. (only $\bm a$ is changling when draw this plot)} Illustration of the observation model $p (\mathcal{A}^{(\bm a^-, \bm a^+)})$.  The green circle denotes the positive action and the yellow denotes the negative. The orange denotes the implicit negative actions, which are adjusted by $\alpha$ as detailed in section \ref{sec:sub:sub:implicit_info_CLIC}. These implicit actions serve as data augmentation to define the combined desired action space.}
	\label{fig:Fig2_illustration_2d_measurement_model}
\end{figure}


Define the prior as $p(\bm a | \bm s)$,
% Assume a uniform distribution of $p(\bm a | \bm s)$,
the posterior belief of the action $ p (  \bm a |\bm a^+ \succeq \bm a^-,  \bm s)$ can be obtained as 


\begin{align}
p ( \bm a ^* = \bm a |  \bm a^+ \succeq \bm a^-,  \bm s) =\frac{ p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s) p(\bm a | \bm s)}{p (\bm a^+ \succeq \bm a^- | \bm s)} \\
    p (\bm a^+ \succeq \bm a^- | \bm s) = \int_{\bm a'} p (\bm a^+ \succeq \bm a^-| \bm a', \bm s) p(\bm a'  | \bm s) d \bm a'
    \label{eq:posterior_data}
\end{align}
By assuming a uniform distribution of $p(\bm a | \bm s)$, the term $p ( \bm a |  \bm a^+ \succeq \bm a^-,  \bm s)$ represents the probability that $\bm a$ is the optimal action, given the feedback $\bm a^-$ and $\bm a^+$.
The probability can also be interpreted as the likelihood of $\bm a$ being sampled from $\mathcal{A}^{(\bm a^-, \bm a^+)}$, defined by the given feedback.

We can set the prior $p(\bm a | \bm s)$ as $\pi(\bm a| \bm s)$, then
the posterior can be denoted as $ p^{\pi} (  \bm a |\bm a^+ \succeq \bm a^-,  \bm s)$, which represents the probability of the action being sampled by the policy $\pi$ filtered by $\mathcal{A}^{(\bm a^-, \bm a^+)}$. 
\begin{align}
p^\pi ( \bm a |  \bm a^+ \succeq \bm a^-,  \bm s) \propto { p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s) \pi(\bm a | \bm s)} 
\end{align}



\subsection{Implicit Policy shaping}
% \subsubsection{Maximum likehood estimation}

The intuition is that at the given state $\bm s$, 
one feedback signal $(\bm a^-, \bm a^+)$ indicates that the probability of selecting actions belonging to $\mathcal{A}^{(\bm a^-, \bm a^+)}$, denoted as $\pi_{\bm \theta}(\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)} | \bm s)$,  should be increased.
Consequently, $\pi_{\bm \theta}(\bm a \notin \mathcal{A}^{(\bm a^-, \bm a^+)}  | \bm s)$ will be decreased.
Formally, $\pi_{\bm \theta}(\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}  | \bm s) = \int_{\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}}  \pi_{\bm \theta}(\bm a|\bm s)    d \bm a$.  And it can be increased by minimizing 
\begin{align*}
    \ell_{KL}(\bm \theta) = \!\! \underset{(\bm{a}^{+}, \bm{a}^{-}, \bm{s}) \sim p_{\mathcal D}}{\mathbb{E}} \left[ \mathrm{KL}\left(p^{\pi^*}\!(\bm a|\bm a^+ \succeq \bm a^-, \bm s) \big\| \pi_{\theta}(\bm a | \bm s) \right) \right] 
\end{align*}


The above loss is intractable as $p^{\pi^*}(\bm a|\bm a^+ \succeq \bm a^-, \bm s)$ is unknown before the policy $\pi$ converges. 
In the following sections, we introduce assumptions to make the problem tractable.

\subsubsection{ TO find a good title}
We assume a uniform distribution of $p(\bm a | \bm s) = \pi^0 (\bm a| \bm s)$, then $p^{\pi^*}(\bm a|\bm a^+ \succeq \bm a^-, \bm s)$ reduces to $p^{\pi^0}(\bm a|\bm a^+ \succeq \bm a^-, \bm s)$, which is policy-independent and can be calculated by Eq. \eqref{eq:posterior_data}.

\begin{align*}
    \ell_{KL}(\bm \theta) = \!\! \underset{(\bm{a}^{+}, \bm{a}^{-}, \bm{s}) \sim p_{\mathcal D}}{\mathbb{E}} \left[ \mathrm{KL}\left(p^{\pi^0}\!(\bm a|\bm a^+ \succeq \bm a^-, \bm s) \big\| \pi_{\theta}(\bm a | \bm s) \right) \right] 
\end{align*}

To calculate the loss, we use samples of $\bm a$ from $\pi_{\theta}(\bm a | \bm s)$ and $\{ \bm a^{+},  \bm a^{-} \}$ to estimate $p (\bm a|\bm a^+ \succeq \bm a^-, \bm s)$ and $\pi_{\theta}(\bm a | \bm s)$.
The samples can be obtained by MCMC method.
We denote the set that includes all the action samples as $\mathbb A = \{ \bm a^+, \bm a^-,  \bm a_j, j = 1, \dots, N \}$. 
Then the policy evaluated at each sampled action can be approximated via 
\begin{align*}
    \pi_{\theta}(\bm a | \bm s) & = \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\sum_{\bm a' \in \mathbb A} e^{-E_{\theta}(\bm s, \bm a')}}, \bm a \in \mathbb A
\end{align*}

\textbf{Connections to maximum likelihood estimation}
The KL loss is equivalent to maximizing the MLE loss
\begin{align}
   \mathbb{E}_{(\bm a^{+},  \bm a^{-}, \bm s)} \mathbb{E}_{\bm a \sim p^{\pi^0} (\bm a|\bm a^+ \succeq \bm a^-, \bm s)}[\log \pi_{\theta}(\bm a | \bm s)],
\end{align}
which can be interpreted as sampling actions from $\mathcal{A}^{(\bm a^-, \bm a^+)}$ first (each action from the set has the same probability to be selected as the uniform prior assumption), and then increase the probability of the policy selecting these action samples. 


% which is equivalent to minimizing the KL loss 
% \begin{align*}
%     \ell_{mle}(\bm \theta) = \mathbb{E}_{(\bm a^{+},  \bm a^{-}, \bm s)} \left[  D_\text{KL} \left(p (\bm a|\bm a^+ \succeq \bm a^-, \bm s) || \pi_{\theta}(\bm a | \bm s) \right) \right] 
% \end{align*}





\subsubsection{Iterative refinement of estimation of desired action space}

However, the above MLE formulation relies on the accurate target distribution, which is not available in corrective feedback, as the actions belonging to $\mathcal{A}^{(\bm a^{-},  \bm a^{+})}$ could still be undesired actions.
Instead, the target can be policy-dependent. 

% \begin{align}
%    p^{\pi^k} (\bm a|\bm a^+ \succeq \bm a^-, \bm s) = \frac{\exp(-E_{\theta}(\bm s, \bm a) + \log p( \bm a^+ \succeq \bm a^- | \bm a))}{\sum\limits_{{\bm{a}' \in \mathbb{A}}} \exp(-E_{\theta}(\bm s, \bm a') + \log p( \bm a^+ \succeq \bm a^- | \bm a'))}
% \end{align}

\begin{align}
   p^{\pi^k}\!(\bm{a}|\bm{a}^+ \!\succeq\! \bm{a}^-, \bm{s}) = 
   \frac{e^{-E_{\theta}(\bm{s},\bm{a}) + \log p(\bm{a}^+ \succeq \bm{a}^- | \bm{a}, \bm s)}}{\sum_{\bm{a}' \in \mathbb{A}} e^{-E_{\theta}(\bm{s},\bm{a}') + \log p(\bm{a}^+ \succeq \bm{a}^- | \bm{a}', \bm s)}}
\end{align}


\begin{align*}
    \ell_{KL}(\bm \theta, j) = \!\!\!\underset{(\bm{a}^{+}, \bm{a}^{-}, \bm{s}) \sim p_{\mathcal D}\!\!}{\mathbb{E}} \left[ \mathrm{KL}\!\left(p^{\pi^j}\!(\bm a|\bm a^+ \succeq \bm a^-, \bm s) \big\| \pi_{\theta}^j(\bm a | \bm s) \right) \! \right] 
\end{align*}

The probability $p^{\pi} (  \bm a |\bm a^+ \succeq \bm a^-,  \bm s)$ acts as a filtering for the action distribution of the policy $\pi$. In other words, for actions sampled from $\pi$, actions belonging to $\mathcal{A}^{(\bm a^-, \bm a^+)}$ tend to have higher probability.  
Convergence: 
By minimizing $\ell_{KL}(\bm \theta, j)$ and update the policy $\pi^j$ for several iterations, we have
\begin{align}
    p^{\pi}_{data} (\bm a | \bm s) = \frac{1}{|\mathcal{D}|}\sum_{\mathcal{D}} p^{\pi}(\bm a | \bm a^+ \succeq \bm a^-, \bm s) \cdot \mathbf{1}(\bm s_i =\bm s) \cdot \\
    \pi(\bm a | \bm s) \simeq p^{\pi}_{data} (\bm a | \bm s)
\end{align}


\textbf{Analogy with Behavior Cloning (BC)}
We present a probabilistic formulation of Behavior Cloning (BC). For BC with a dataset \(\mathcal{D}^{BC} = \{(\bm{s}_i, \bm{a}_i)\}\), the data distribution \(p_{\text{data}}(\bm{a} | \bm{s})\) can be expressed as:
\[
p_{\text{data}}(\bm{a} | \bm{s}) = \frac{1}{|\mathcal{D}|} \sum_{i} p(\bm{a} | \bm{a}_i, \bm{s}_i) \cdot \mathbf{1}(\bm{s}_i = \bm{s}),
\]
where \(p(\bm{a} | \bm{a}_i, \bm{s}_i)\) represents the likelihood of action \(\bm{a}\) given the labeled action \(\bm{a}_i\) and state \(\bm{s}_i\). 
A simple example of this likelihood is assigning a value of 1 if \(\bm{a} = \bm{a}_i\), and 0 otherwise. This captures the deterministic nature of many BC formulations.
The main difference between CLIC and BC is that CLIC does not assume an action label, which makes the target data policy $p_{data} (\bm a | \bm s)$ undefined. 
Therefore, it's necessary for CLIC to create a temporary target $ p^{\pi}_ {data} (\bm a | \bm s)$ that guides the policy on which direction to improve.


\textbf{Connections to InfoNCE loss} The MLE loss has close connections to InfoNCE loss used in IBC eq.~\eqref{eq:ibc_info_NCE}.
By neglecting the term irrelevant to the parameter $\theta$, the MLE loss can be reorganized as a weighted sum of the InfoNCE loss, where the InfoNCE loss tends to decrease the energy value of selected action $\bm a$ while increasing the energy value of other sampled actions. 
The posterior probability $p (\bm a|\bm a^+ \succeq \bm a^-, \bm s)$ adjusts how confident the selected action  $\bm a$ is a positive action. By taking the weighted average of the InfoNCE loss, the energy of actions with higher probability will get decreased, while the energy of actions with smaller probability (such as $\bm a^-$) will get increased. 
\begin{align*}
    \ell_{mle}(\bm \theta) &\Leftrightarrow
    \sum_{\bm a } -p (\bm a|\bm a^+ \succeq \bm a^-, \bm s) \log \pi_{\theta}(\bm a | \bm s) \\
    % &= \sum_{\bm a } -p (\bm a|\bm a^{+},  \bm a^{-}) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\displaystyle\sum_{\bm a' \in \mathcal{A}^{-}} e^{-E_{\theta}(\bm s, \bm a'_j)} + \sum_{\bm a' \notin \mathcal{A}^{+}} e^{-E_{\theta}(\bm s, \bm a'_j)}} \\
    &= \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^+ \succeq \bm a^-, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \backslash  \bm a)
\end{align*}

\textbf{Connections to desired action space $\mathcal{A}^{\mathcal{D}}$}
We denote the data buffer as $\mathcal{D}_{k-1} = \{[\bm s_i, \bm a_i, e \bm h_i], i = 1, \dots, k \}$ that contains all the received corrective feedback, where $k$ is the total number of the data tuples. 
The policy trained by data $\mathcal{D}_{k}$ 
 via loss Eq. \eqref{} can generate actions that belong to the overall desired action space enforced by $\mathcal{D}_{k}$, which can be represented as $ \pi_{\theta}(\bm a | \bm s) =p (\bm a | \bm s, \mathcal{D}_{k})$, which denotes the probability of an action $\bm a$ being optimal given the data $ \mathcal{D}_{k}$.
For a new feedback tuple $(\bm a^-_{k+1}, \bm a^+_{k+1}, \bm s_{k+1})$, by the Bayesian rule, we have
\begin{align}
    p (\bm a | \bm s, \mathcal{D}_{k+1}) \propto p (\bm a | \bm s, \mathcal{D}_{k}) \cdot p( \bm a^{+}_{k+1} \succeq \bm a^{-}_{k+1} | \bm a. \bm s),
\end{align}
where $p (\bm a | \bm s, \mathcal{D}_{k+1})$ is the posterior distribution of $\bm a$ given the new feedback. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/Fig6_illustration_loss_EBMs.png}
	\caption{Illustration of the loss function for training the energy-based model.}
 \label{fig:Fig6_illustration_loss_EBMs}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.59]{figs/Fig4_illustration_loss_withGaussianpolicy.png}
	\caption{Illustration of the MLE loss and Bayes loss with a Gaussian policy. For a fixed variance, the loss is actually updating the mean of the Gaussian, by the weighted average of actions in $A$, weighted by $p (\bm a|\bm a^{+},  \bm a^{-})$ for MLE loss and $\pi^{\text{target}}(\bm a | \bm s)$  for Bayes loss. }
 \label{fig:Fig4_illustration_loss_withGaussianpolicy}
\end{figure}


\begin{align}
p^{\pi} ( \bm a |  \bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)},  \bm s) =\frac{ p (\bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)} | \bm a ,\bm s) \pi(\bm a | \bm s)}{p^\pi (\bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)} | \bm s)} \\
    p^{\pi} (\bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)} | \bm s) = \int_{\bm a'} p (\bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)}| \bm a', \bm s) \pi(\bm a'  | \bm s) d \bm a'
\end{align}

$ p^{\pi} (\bm a^* \in \mathcal{A}^{\mathcal{D}_k(\bm s)} | \bm s)$ is the probability of sampling actions from policy $\pi$ that belongs to $\mathcal{A}^{\mathcal{D}_k(\bm s)}$.

\begin{align}
p( \bm a |  \bm a^* \in \mathcal{A}^{\mathcal{D}_{k}(\bm s)},  \bm s) =\frac{ p ( \bm a |  \bm a^* \in \mathcal{A}^{\mathcal{D}_{k-1}} , \bm s) p (\bm a^* \in \mathcal{A}^{(\bm a^-_{k},\bm a^+_{k} )} | \bm a, \bm s) }{p^\pi (\bm a^* \in \mathcal{A}^{(\bm a^-_{k},\bm a^+_{k} )} |\bm a^* \in \mathcal{A}^{\mathcal{D}_{k-1}(\bm s)} , \bm s)} \\
    p( \bm a |  \bm a^* \in \mathcal{A}^{\mathcal{D}_{K}(\bm s)},  \bm s) \text{is the policy that converges under the loss of KL.}
\end{align}

\textbf{Why Bayes loss is stable compared with MLE loss?}
(1) As shown in Fig. \ref{fig:Fig4_illustration_loss_withGaussianpolicy},  for MLE loss, the larger the difference between the target distribution and the current policy, the greater the influence. This can lead to the instability and make mode oscillating. 
(2) Instead, in Bayes loss, as visualized in Fig. \ref{fig:Fig6_illustration_loss_EBMs}, the target policy is incremental, which prevents the case where the large difference between the target and current policy leads to training instability.  



\subsection{Explicit Policy shaping}

In this subsection, we assume that the policy follows a Gaussian distribution with fixed covariance, $\pi_{\bm \theta}(\bm a | \bm s) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma)$.

$\Rightarrow$

As the policy can only model uni-modal data, the goal can be adjusted to 
% increase $ \pi_{\bm \theta}(\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}) \text{ if } \pi_{\bm \theta}(\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}) < \frac{1}{2}$, which is that the policy should 
satisfy the inequality 
$\pi_{\bm \theta}(\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}) >\pi_{\bm \theta}(\bm a \notin \mathcal{A}^{(\bm a^-, \bm a^+)}) $.
We can show that, with the Gaussian policy assumption, this inequality can be simplified as 
\begin{align}
    % \pi^*(\bm s, \bm a^{-}) \leq \pi^*(\bm s, \bm a^{+}), 
    \pi_{\bm \theta}(\bm a^{-}|\bm s) \leq \pi_{\bm \theta}(\bm a^{+}|\bm s).
    \label{eq:policy_improvement}
\end{align}

To make $\pi_{\bm \theta}$ satisfy the above inequality, the hinge loss can be utilized:
\begin{align}
\ell (\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \max (0,  \log \pi_{\bm \theta}(\bm a^{-}|\bm s) - \log\pi_{\bm \theta}(\bm a^{+}|\bm s)  ), 
\label{eq:loss_policy_improvement}
\end{align}
