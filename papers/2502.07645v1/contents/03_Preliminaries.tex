\section{Preliminaries}
\label{sec:Preliminaries}

\subsection{Interactive Imitation Learning}
\label{sec:Preliminaries:IIL}

In a typical Interactive IL (IIL) problem, a Markov Decision Process (MDP) is used to model the decision-making of an agent taught by a human teacher. 
An MDP is a 5-tuple \((\mathcal  S, \mathcal A, T, R, \gamma)\), where \(\mathcal  S\) represents the set of all possible states in the environment, \(\mathcal  A\) denotes the set of actions the agent can take, \(T(\bm s' |\bm s, \bm a)\) is the transition probability, \(R(\bm s, \bm s', \bm a)\) is the reward function, giving the reward received for transitioning from state \(\bm s\) to state \(\bm s'\) via action \(\bm a\), and \(\gamma \in [0, 1]\) is the discount factor.
The reward typically reflects the desirability of the state transition.
A policy in an MDP defines the agent's behavior at a given state, denoted by \(\pi\).
In general, $\pi$ can be represented by the conditional probability $\pi(\bm a|\bm s)$ of the density function \(\pi: \mathcal S \times \mathcal A \rightarrow [0, 1]\). Consequently, given a state, $\pi$ is employed to select an action. The objective in an MDP is to find the optimal policy \(\pi^*\) that maximizes the expected sum of discounted future rewards $J(\pi) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^{t} R(\bm s_t, \bm a_t) \right]$. 
% In this work, we model $\pi$ using a Deep Neural Network (DNN). To achieve this, we consider a Gaussian policy with fixed covariance $\bm \Sigma$ and with mean $\bm \mu_{\theta}(\bm s)$, represented by the DNN. Hence, we obtain the parameterized policy $\pi_{\bm \theta}(\bm a | \bm s) \sim \mathcal{N}\left (\bm \mu_{\theta}(\bm s), \bm \Sigma\right)$, where $\bm \theta$ denotes the DNN's parameter vector.

In IIL, a human instructor, known as the \emph{teacher}, aims to improve the behavior of the learning agent, referred to as the \emph{learner}, by providing feedback on the learner's actions. 
IIL does not rely on a predefined reward function for specific tasks, thanks to the direct guidance provided by the teacher \cite{2022_IIL_survey}.
In this paper, we consider the teacher feedback to act in the state-action space, which is described by the function \( \bm h = {H}(\bm s, \bm a)\).
%%% Explain the teacher's feedback H
The feedback $\bm h$ can be defined according to the feedback type.
For instance, in demonstration/intervention feedback, $\bm h$ represents the action the learner should execute at a given state. In contrast, for relative corrective feedback, 
% $\bm h$ is a directional signal guiding the learner's action towards a more favorable one.
 $\bm h$ is a normalized vector indicating the direction in which
the learner's action should be modified, i.e., 
${\bm h \in \mathcal{H} = \{ \bm d \in \mathcal{A} \mid  ||\bm d|| = 1\}}$.
% ${\bm h \in \mathcal{H} = \{ \bm d = \bm a_2 - \bm a_1, \bm a_1\in \mathcal{A}, \bm a_2 \in \mathcal{A} \mid  ||\bm d|| = 1\}}$.
In our context, the reward function is unknown; therefore, we cannot directly maximize the expected accumulated future rewards $J(\pi)$.
% Instead,
% an observable surrogate loss $l_{\pi}( \bm s)$ can be formulated as $l_{\pi}( \bm s) = \mathbb{E}_{\bm a \sim \pi(\bm s)} \left[ l_{\pi}(\bm s, \bm a, \mathcal{H}(\bm s, \bm a)) \right]$ 
% and the minimization of 
% $l_{\pi}( \bm s)$ 
% Instead, 
% an observable surrogate loss $l_{\pi}( \bm s)$ can be defined to indicate how well the learner's policy $\pi$ follows the teacher's feedback $\bm h$
% and the minimization of   $l_{\pi}( \bm s)$ 
% indirectly minimizes $J(\pi)$ \cite{2011_DAgger}.
Instead, an observable surrogate loss, \(\ell_{\pi}(\bm{s})\), is formulated. This loss measures the alignment of the learner's policy \(\pi\) with the teacher's feedback. In IIL, it is assumed that minimizing \(\ell_{\pi}(\bm{s})\) indirectly maximizes \(J(\pi)\) \cite{2011_DAgger,2022_IIL_survey}.
% IIL aims to find a learner's policy $\pi^l$ by solving 
As a result, the objective is to determine an optimal learner's policy $\pi^{l*}$ by solving the following equation:
\begin{equation}
    \pi^{l*} = \underset{\pi\in\Pi}{\arg\min} \mathbb{E}_{\bm s\sim d_{\pi}(\bm s)} \left[ \ell_{\pi}( \bm s) \right]
    \label{eq:IIL_formulation}
\end{equation}
%%% explain the data distribution?
where $d_{\pi}(\bm s)$ is the state distribution induced by the policy $\pi$.
In practice, the expected value of the surrogate loss in Eq.~\eqref{eq:IIL_formulation} is approximated using the data collected by a policy that interacts with the environment and the teacher.



\subsection{Implicit Behavior Cloning}

% Implicit BC \cite{2022_implicit_BC} reformulates the traditional behavioral cloning approach by using implicit models, specifically energy-based models (EBM), to represent policies. Instead of directly mapping observations to actions with an explicit function, 
Implicit BC (IBC) \cite{2022_implicit_BC} defines the policy through an energy function \( E_\theta(s, a) \) that takes state \( s \) and action \( a \) as inputs and outputs a scalar energy value. 
This formulation allows IBC to handle complex, multi-modal, and discontinuous behaviors more effectively than explicit models.
The energy function is trained using maximum likelihood estimation by minimizing the InfoNCE loss \cite{2018_InfoNCE_representation_learning, 2024_revisting_IBC}:
\begin{align}
\label{eq:ibc_info_NCE}
\ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb{A}^{neg})
\!\! = \!\! - \! \log \! \left[ \! \frac{e^{-E_\theta(\bm s, \bm a)}}{e^{-\!E_\theta(\bm s, \bm a)} \!\! + \!\!\sum_{j=1}^{N_{\text{neg}}} e^{-\!E_\theta(\bm s, \tilde{\bm a}_j)}} \!\right] \!\!,\!\!
\end{align}
where the action label \(\bm a\) is the teacher action,  \(\tilde{\bm a}_j \in \mathbb{A}^{neg} (j = 1, \dots, N_{\text{neg}}) \) are negative samples, and $ \mathbb{A}^{neg}$ is the set that includes negative samples. 
The InfoNCE loss encourages the model to assign low energy to action labels and high energy to negative samples. 
To ensure the EBM learns an accurate data distribution, the negative samples should be close to the action label, avoiding overly obvious distinctions that hinder effective learning \cite{2020_flow_constrastive_estimation_EBM}. 
% This can be achieved by obtaining
%  negative samples by sampling from the current EBM \cite{2019_EBM_Du_Yilun}, which can be achieved via MCMC (Markov Chain Monte Carlo) sampling with stochastic gradient Langevin dynamics \cite{2011_Langevin_dynamics}:
This can be achieved by generating negative samples from the current EBM using MCMC sampling with stochastic gradient Langevin dynamics \cite{2011_Langevin_dynamics, 2019_EBM_Du_Yilun}:
\begin{equation}
\tilde{\bm a}_j^i = \tilde{\bm a}_j^{i-1} - \lambda \nabla_{\bm a} E_\theta(\bm s, \tilde{\bm a}_j^{i-1}) + \sqrt{2\lambda}   \omega^i,    
\label{eq:mcmc_sampling}
\end{equation}
where $\{ \tilde{\bm a}_j^0 \}$ is initialized using the uniform distribution and $\omega^i$ is the standard normal distribution.
For each $\tilde{\bm a}_j^0 $, we run $N_{\text{MCMC}}$ steps of the MCMC chain, with $i = 0, \dots, N_{\text{MCMC}}$ denoting the step index. The step size $\lambda > 0$ can be adjusted using a polynomially decaying schedule.

% During inference, the optimal action \(\hat{\bm a}^*\) is estimated as the action with the lowest energy value and is determined by minimizing the energy function among samples from  Langevin MCMC:
During inference, the estimated optimal action \(\hat{\bm a}^*\) is obtained by minimizing the energy function, and can be approximated through Langevin MCMC:
\[
\hat{\bm a}^* = \underset{\bm a}{ \arg\min} E_\theta(\bm s, \bm a).
\]


One core assumption of IBC is that the action label is optimal and all other actions are not \cite{2022_implicit_BC}.
This assumption simplifies the classification of action samples into positive and negative categories. 
Specifically, 
the action label is considered positive, and all other sampled actions are considered negative.
However, actions considered as negative may still be valid and should not be overly penalized.
This makes selecting appropriate negative samples challenging and introduces instability during the IBC's training process.
In contrast, our CLIC method addresses this issue by labeling sampled actions within the desired action space as positive and those outside it as negative, ensuring a more stable and effective training process.
 % limiting Implicit BC's ability to learn a consistent action representation across different trails.



\subsection{Problem Formulation}

% Breifly introduce how our policy is modeled, how action is sampled (using IBC's method), how the feedback looks like

% The objective is to estimate the optimal action $\bm a^*$ for every state $\bm s \sim d_\pi(\bm s)$ via an occasional feedback signal $\bm h$.
The objective is to learn a policy $\pi$ that accurately estimates the optimal action $\bm a^*$ for every state $\bm s \sim d_\pi(\bm s)$, using occasional corrective feedback $\bm h$.
This feedback is provided by a human teacher in the robot's state-action space, placing the problem in the context of IIL.
% in the action space provided by the human teacher in the IIL framework. 
The feedback $\bm h$ can be either absolute or relative corrections, as defined in Section~\ref{sec:Preliminaries:IIL}.
% Although $\bm h$ has different meanings for absolute or relative corrections, 
% the pair of actions can be defined, which are the robot action $\bm a^{r}$ and human action $\bm a^{h}$.  
Accordingly, the \textit{observed action pair} $(\bm a^r, \bm a^h)$ can be defined, where $\bm a^r$ denotes the robot action and $\bm a^h$ denotes the human feedback action, referred to as human action for simplicity. 
For absolute correction, we have that $\bm a^h = \bm h$. 
In contrast, for relative correction, we have that $\bm a^h = \bm a^r + e \bm h$, where the magnitude hyperparameter $e$ is set to a small value.
% compared to the difference between $\bm a^r$ and the optimal action $\bm a^*$.
% % briefly show different types of feedback
% In addition to accurate absolute and relative corrections, Table \ref{tab:feedback-definitions} summarizes other common types of feedback humans provide, which are also used to evaluate the algorithms in the experiment section.

% \begin{table}[h!]
% \caption{Various feedback in the action space}
% \centering
% % \begin{tabular}{@{}ll@{}}
% \begin{tabularx}{0.49\textwidth}{@{}lX@{}}
% \toprule
% \textbf{Type of Feedback Data}      & \textbf{Definition} \\ \midrule
% Accurate absolute correction              & \( \bm a^h = \bm h, \bm h = \bm a^* \) \\
% Gaussian noise             & \( \bm a^h \sim \mathcal{N}(\bm a^*, ||\bm a^* - \bm a^r|| I) \) \\
% Partial feedback           & \( \bm a^h \in \{[\bm a^*_{r1}, \bm a_{r2}], [\bm a_{r1}, \bm a_{r2}^*]\} \) \\ 
% \hline
% Accurate relative correction              & \(\bm  a^h = \bm a^r + e\bm h^*, \bm h^*= \frac{\bm a^* - \bm a^r}{||\bm a^* - \bm a^r||}  \) \\
% Direction noise            & \( \bm a^h = \bm a^r + e \bm h_r \), \( \angle (\bm h_r, \bm h^*) = \beta \in [0, 90^\circ) \) \\
% \bottomrule
% \end{tabularx}
% \label{tab:feedback-definitions}
% \end{table}


% briefly show what is the goal of our paper, and the corresponding general formulation
Although the optimal action may not be directly extracted from human feedback for a given state, it includes information on which subset of $\mathcal{A}$ is more likely to contain optimal actions. 
Based on this insight, assuming the Euclidean action space $\mathcal{A}$, we introduce the \textit{desired action space} $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$ as the set of desired actions, defined as a function of the observed action pair $(\bm a^r, \bm a^h)$.
Formally, ${\hat{\mathcal{A}}:\mathcal{A} \times \mathcal{A} \twoheadrightarrow\mathcal{A}}$, where the symbol $\twoheadrightarrow$ defines a set-valued function, i.e., elements of $\mathcal{A} \times \mathcal{A} $ are mapped to subsets of $\mathcal{A}$.
Actions within the desired action space are more likely to be optimal than those outside it. The construction of this set is detailed in Section~\ref{sec:Desired_action_space}.  
% The desired action set should include $\bm a^h$ and exclude $\bm a^r$; it also includes other desired actions implied by $(\bm a^r, \bm a^h)$.
% Instead of assuming $\bm a^* = \bm a^h$, we assume that the desired action space includes at least one optimal action. 
% And there exists a function $ I(\bm a, \bm a^r, \bm a^h) = \mathbf{1}_{\bm a \in \bar{\mathcal{A}}(\bm a^r, \bm a^h)}$ that determines whether an action belongs to this set.
% Then we can have
% \[
% \bar{\mathcal{A}}(\bm a^r, \bm a^h) = \{ \bm a \in \mathcal{A} \mid I(\bm a, \bm a^r, \bm a^h) = 1 \}.
% \]

% To learn a policy from the set $\bar{\mathcal{A}}(\bm a^r, \bm a^h)$, there are three important aspects tackled in this paper:
% (1) how to construct this set from feedback,
% (2) how to design a loss function to encourage policy to generate actions within the desired action set,  
% and (3) how to guarantee that the policy can generate optimal actions with sufficient feedback data.
% For question (1) and (3), we tackle it in in Section~\ref{sec:Desired_action_space}, which detail
% the definition and property of $\mathcal{A}(\bm a^r, \bm a^h)$.
% For question (2), we tackle it in Section~\ref{sec:Policy_shaping}, where we first give a probablisitc formulation the desired action set, then design the loss function to train the policy. 

% The desired action space can shape the policy to generate actions within this space, which we will introduce in the Section~\ref{sec:Policy_shaping}.
% We first introduce the policy model 
In this work, we model the policy $\pi$ using a Deep Neural Network (DNN). 
To achieve this, following IBC \cite{2022_implicit_BC}, for multi-modal tasks with multiple optimal actions, our policy is defined by the energy function 
\[\pi_{\theta}(\bm a | \bm s) = \frac{\exp(-E_{\theta}(\bm s, \bm a))}{Z},
\]
where $Z$ is a normalizing constant and can be approximated by $Z = \sum_{j=1}^{N_{\text{MCMC}}} \exp(-E_{\theta}(\bm s, \bm a'_j))  $.
The samples $\{\bm a'_j\}$ are obtained via Langevin MCMC sampling (see Eq.~\eqref{eq:mcmc_sampling}), and $\bm \theta$ denotes the DNN's parameter vector.

For simpler tasks with a single optimal action for every state, we can consider an explicit Gaussian policy with fixed covariance $\bm \Sigma$ and with mean $\bm \mu_{\theta}(\bm s)$, which we model using a DNN. Hence, we obtain the parameterized policy $\pi_{\bm \theta}(\bm a | \bm s) \sim \mathcal{N}\left (\bm \mu_{\theta}(\bm s), \bm \Sigma\right)$.
Both explicit and implicit policies can be estimated using our method based on the desired action space, and we will introduce them in Section~\ref{sec:Policy_shaping}.

% \textcolor{red}{Put this framework at the end of intro}

% This paper addresses three key questions for learning a policy from $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$:
% \begin{enumerate}
%     \item \textbf{Constructing the set}: How to derive $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$ from feedback.
%     \item \textbf{Designing the loss}: How to design a loss function that encourages the policy to generate actions within $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$.
%     \item \textbf{Guaranteeing optimality}: How to ensure the policy generates optimal actions with sufficient feedback.
% \end{enumerate}

% Questions (1) and (3) are addressed in Section~\ref{sec:Desired_action_space}, detailing the definition and properties of $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$. Question (2) is covered in Section~\ref{sec:Policy_shaping}, which introduces a probabilistic formulation of the desired action set and a corresponding loss function for policy training.




% We denote the data buffer of corrective data as $\mathcal{D}_{t-1} = \{[\bm s_i, \bm a_i, e \bm h_i], i = 1, \dots, k \}$ that contains all the received corrective feedback, where $k$ is the total number of the data tuples. 
% Our policy is defined by the energy function $\pi_{\theta}(\bm a | \bm s) = \frac{\exp(-E_{\theta}(\bm s, \bm a))}{Z}$, where $Z$ is the normalizing constant and is approximated by $Z = \sum_{j=1}^{N} \exp(-E_{\theta}(\bm s, \bm a'_j))  $, where the samples $\bm a'_j$ are obtained via Langevin sampling in Eq.~\eqref{eq:mcmc_sampling}.





