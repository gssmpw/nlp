\section{Related Work}
\label{sec:related_work}
In this section, we provide an overview of the related work on policy learning from various feedback types, such as demonstration, relative correction, and preference.
We also summarize methods for training policies represented by EBMs.
%  The energy-based model can be trained in many different approaches, depending on the assumption of the type of data (demonstration, correction, reward, etc).
% Besides, other types of models, such as diffusion, have been applied to policy representation, which outperforms its EBM counterpart (IBC). 
% In this section, we briefly overview the related works and the IBC's training instability issue. At the end, we draw connection between IBC and our method, which utlize the corrective feedback for EBM traning. 

\subsection{Learning from Demonstration}
%1 introduce big picture of learning from Demonstration, deep generative model, diffusion, EBM
Learning from demonstration aims to teach robot behavior models from demonstration data, which provides the robot with examples of desired actions. 
Traditional methods often struggled with capturing complex data distributions, especially when multiple optimal actions exist for a given state, such as the task of pushing a T-shape object to a goal configuration \cite{2023_diffusionpolicy, 2024_RSS_pushT_Traj_optimization}.
Deep generative models, including EBMs \cite{2019_EBM_Du_Yilun, 2021_how_to_train_EBM},  diffusion models \cite{2015_diffusion, 2020_diffusion}, have been introduced to better capture such multi-modal data distributions \cite{2024_survey_deep_generative_model_in_robotics}. EBMs learn unnormalized energy values for inputs and have been applied to learn the energy of the entire state-action space via IBC \cite{2022_implicit_BC}.  
Diffusion models, which learn to denoise noise-corrupted data \cite{2015_diffusion, 2020_diffusion}, have also been utilized to represent robot policies, resulting in diffusion policies \cite{2023_diffusionpolicy, 2023_score_diffusion_policy}. These policies effectively learn the gradient (score) of the EBM \cite{2020_Score_based_diffusion} and offer improved training stability \cite{2023_diffusionpolicy}. Implicit models, such as EBMs and diffusion policies, have demonstrated superior capability in handling long-horizon, multi-modal tasks compared to explicit policies and have been successfully applied to various real-world applications \cite{2024_EBM_planning_air_hockey_application, 2024_IBC_RL_planning, 2024_survey_deep_generative_model_in_robotics}.
These implicit models have been extended into an online interactive imitation learning (IIL) framework 
\cite{2023_IIFL_implicit_interactive_BC, 2024_Diffusion_dagger}.  
However, as mentioned in the introduction section, the powerful encoding capability of these models can also cause overfitting behavior with behavior cloning loss, especially when the demonstration data deviates from the optimal action. 
To address this issue while still leveraging their encoding capability, 
we propose a new perspective on iteratively estimating optimal actions in the IIL framework. 
Instead of relying on behavior cloning loss, we introduce a novel loss function that updates the policy to align with desired action spaces. 


% \cite{2024_IBC_RL_planning}

\subsection{Learning from Preference Feedback}
Preference-based feedback involves the comparison of different robot trajectory segments.
% , which can be treated as a relative version of the evaluative feedback.
From this,
a reward/objective model is usually estimated and employed to obtain a policy \cite{2017_DRL_preference, 2015_IJRR_Preference_traj_ranking, 2021_Pebble_preference_RL, 2020_openai_RLHF, 2024_comparative_language}.
% The sub-optimal demonstration data can also be transformed into preference data to learn a high-quality reward model \cite{2019_IRL_ranked_demonstration}.
Some approaches enable directly learning a policy by the contrastive learning approach proposed by \cite{2023_Contrastive_Prefence_Learning, zhao2022calibrating} or direct preference optimization approach \cite{2023_DPO} to improve efficiency.
Moreover, sub-optimal demonstrations can be transformed into preference data to learn a high-quality reward model  \cite{2019_IRL_ranked_demonstration}. 
% Although this feedback modality is promising, it is not very data-efficient, requiring more data and time from the teacher to learn a good policy.
Although this feedback modality is promising, it is not very data-efficient. This is because the feedback is given over complete trajectories, requiring the learner to infer per-state behavior and requiring more data and teacher effort.
While there are works to make it data efficient via active learning \cite{2024batch_acitve_learning_Preference}  or utilizing prior knowledge of state \cite{2024hindsight_preference}, our paper focuses on human feedback in the state-action space.

\subsection{Learning from Relative Corrective Feedback}
% Relative corrective feedback provides incremental information on how to improve an action, balancing information richness and simplicity for the teacher \cite{2022_IIL_survey}. 
% % The data consists of pre-correction and post-correction actions.
% This correction feedback can be transferred into preference data with trajectory pairs and
% the objective function can be learned from the preference data, as in \cite{2015_IJRR_Preference_traj_ranking, 2017_pHRI_correction_learning_objective, 2018_uncertainty_correction_objective}.
% Alternatively, \cite{2022_TRO_correction_objective_function} proposed directly inferring the objective function without preference transformation.
% However, these objective functions are linear combinations of features, which may struggle with complex tasks.
% Another line of work is the COACH-based framework (Corrective Advice Communicated by Humans), which directly learns a policy from relative corrections
% \cite{2018_D_COACH, 2019_Carlos_COACH, 2021_BDCOACH}.
% This framework has been extended to 
%  utilize the feedback from the state space instead of the action space 
% \cite{2020_COACH_state_Space} and combined with reinforcement learning to increase the RL efficiency\cite{2019_Carlos_IJRR}.
% However, as we mentioned in Section~\ref{sec:introduction}, the previous COACH-based framework fails to utilize the history data, making it inefficient compared with demonstration-based methods. 
% Instead, our CLIC method can utilize the history data as it will not harm the policy under our assumption of desired action space. 


Relative corrections provide incremental information on how to improve an action, balancing information richness and simplicity for the teacher \cite{2022_IIL_survey}. 
% The data consists of pre-correction and post-correction actions.
This correction feedback can be transferred into preference data with trajectory pairs and
objective functions can be learned from the preference data, as in \cite{2015_IJRR_Preference_traj_ranking, 2017_pHRI_correction_learning_objective, 2018_uncertainty_correction_objective}.
Alternatively, \cite{2022_TRO_correction_objective_function} proposed directly inferring objective functions without preference transformation.
However, these objective functions are linear combinations of features, which may struggle with complex tasks.

Another line of work is the COACH-based framework (Corrective Advice Communicated by Humans), which directly learns a policy from relative corrections
\cite{2018_D_COACH, 2019_Carlos_COACH, 2021_BDCOACH}.
This framework has been extended to 
 utilize the feedback from the state space instead of the action space 
\cite{2020_COACH_state_Space} and combined with reinforcement learning to increase the RL efficiency\cite{2019_Carlos_IJRR}.
However, COACH-based methods rely on the over-optimistic assumption that the action labels derived from relative corrections are optimal, allowing the policy to be refined by imitating them via the BC loss \cite{2019_Rodrigo_D_COACH, 2019_Carlos_IJRR, 2019_Carlos_COACH}. 
This assumption becomes a critical limitation when feedback is aggregated into a reply buffer.  
As the robot's policy continuously improves, previous feedback may no longer be valid, causing incorrect policy updates \cite{2021_BDCOACH}. 
As a result, the buffer size is limited to being small, ensuring it contains only recent corrections. This leads to policies that tend to overfit data collected from recently visited trajectories, making it inefficient compared with demonstration-based methods. 
In contrast, our CLIC method can utilize the history data, as the desired action spaces created from it will not mislead the policy. 

The COACH-based framework utilizes explicit policies\cite{2019_Carlos_IJRR, 2019_Rodrigo_D_COACH}, limiting its ability to handle multi-modal tasks. 
Implicit policies, encoded by
EBMs, can also be learned using methods like Proxy Value Propagation (PVP)\cite{2023_NIPS_PVP}.
PVP uses a loss function that only considers the energy values of recorded robot and human actions. As a result, the loss provides limited information and fails to train an EBM effectively.
In contrast, our approach generates action samples from the EBM and classifies them into desired and undesired actions based on the desired action space. These classified samples are then used to compute the loss, which can effectively train EBMs.

% using desired action spaces to clarify action samples from the EBM into desired and undesired, which are then used to calculate the loss. 
% proposing that each correction data defines a desired and undesired action set, which are used to train an EBM and enable a more efficient learning process compared to PVP.
% Our method then trains an EBM by pushing down the energy within the desired action set and pushing up the energy within the undesired action set, which is more effective than the PVP method.  





% 2 transition, talk about EBM's advantage and applications
\subsection{Learning Policies Represented as Energy-Based Models}
 % The energy-based model can be trained in many different approaches, depending on the assumption of the type of data (demonstration, correction, reward, etc).
EBMs have been widely used across different types of feedback data.
In reinforcement learning, where data is typically scalar rewards, the energy function is used to encode the action-value function Q, with the relationship $ Q_{soft}(\bm s, \bm a) = - \alpha E(\bm s, \bm a) $ \cite{2017_soft_Q_learning, 2018_SAC, 2024_RAL_imperfect_demon}. 
Reward-conditioned policies can also be learned through Bayesian approaches \cite{2023_Bayesian_reprameterized_RCRL}. For preference feedback, EBMs can be aligned with human preferences via inverse preference learning \cite{2023_Inverse_Preference_learning}. 
In scenarios involving corrective feedback, where both the robot and the human actions are given, methods such as PVP have been proposed to learn EBMs that assign low energy values to human actions and high energy values to robot actions \cite{2023_NIPS_PVP}.
For demonstration or absolute correction data, EBMs can be trained directly using the objective that demonstrated actions have lower energy than other actions  \cite{2020_RSS_expert_interventio_learning, 2022_implicit_BC}.
In discrete action spaces, EBMs can be straightforward to train \cite{2020_RSS_expert_interventio_learning}, though the discrete nature of the action space limits the scope of the method. 
For continuous action spaces, EBMs can be trained
via the InfoNCE loss in IBC \cite{2022_implicit_BC}, through which the energy of human actions is decreased, and the energy of other actions is increased. 

Although IBC achieves better performance than explicit policies \cite{2022_implicit_BC}, it is known to suffer from training instability. The process of training EBMs involves selecting counterexample actions, and the quality of these counterexamples significantly impacts the learning outcomes \cite{2021_how_to_train_EBM,2020_flow_constrastive_estimation_EBM}. Empirically, counterexamples that are near data labels are often preferred \cite{2020_hard_negative_mixing_contrastive}, but these selections may contribute to instability during training \cite{2022_arxiv_IBC_gaps, 2023_diffusionpolicy}. Our method addresses this issue by relaxing the BC assumption and using the proposed desired action space to train EBMs, leading to a stable training process.


