\section{Discussion}
\label{sec:discussion}
\textbf{What will happen when the teacher makes mistakes and gives wrong corrections?}
Incorrect corrections can negatively impact both CLIC and other methods compared in this paper, as they violate the assumptions of both CLIC loss and behavior cloning loss. 
However, errors are common when the teacher is a human, as evidenced by our real-robot experiments where some feedback was incorrect. 
 The teacher's mistake can be a problem if the optimal action is solved analytically (or makes the policy satisfy the constraint strictly in Eq.~\eqref{eq:action_relatinship}), where one wrong feedback can make the desired action space empty. 
Instead, our CLIC method remains effective if sufficient corrective feedback is provided to counterbalance the erroneous corrections.  
As shown in the Algorithm \ref{alg:online_IIL}, a batch of relative corrective feedback data is sampled to calculate the policy improvement loss, which is used to update the parameters of the learner's policy.
As long as the number of the correct feedback is much more than the wrong feedback, the averaged gradient of the parameters should still point to the direction that improves the policy.
Therefore, if the teacher makes some mistakes occasionally, it will not make the policy collapse.

% This further indicates the robustness of our proposed policy improvement loss in Section~\ref{sec:policy_improvement_loss}.
% \todo[inline]{rodrigo: modify this discussion}


\textbf{What will happen when the task has multiple modes (optimal actions)?}
Our CLIC method currently cannot handle data with multiple modes because of the Gaussian distribution assumption of the policy in Section~\ref{sec:Methodology}, and we require the teacher to give consistent corrective feedback during the experiments. 
Therefore, in the case of multiple modes, CLIC will converge to one of the modes under the teacher's guidance.
Besides, in practice, whether the algorithm can handle tasks with multiple modes depends on how we parameterize the policy. 
For the policy $\pi_{\theta}(\bm s,\bm a) \sim 
\mathcal{N}(\bm \mu_{\theta}(\bm s), \bm \Sigma)$, it can only learn one optimal action.
Nevertheless, we believe CLIC can be generalized to the multiple modes situation either explicitly \cite{2022_Behavior_transformer} or implicitly \cite{2022_implicit_BC}, which is not the focus of this paper and we leave this in one of the future directions.

% \todo[inline]{zhaoting: add one section to discuss the limitation}

% As in Section~\ref{sec:policy_improvement_loss}, to make it easy to illustrate the main idea, we made the assumption that there is only one $\bm a^*$ in the action space at a given state $\bm s$.
% In theory, when there are multiple optimal actions $\bm a^*_i, i = 1, \dots, n_a$ , 
% we assign one optimal action $\bm a^*_i$ to the action pair $(\bm a^+, \bm a^-)$ and
% Eq.~\eqref{eq:action_relatinship} is still valid if $(\bm a^*_i, \bm a^+, \bm a^-)$ are not contradicting, which is $\mathbb{D}(\bm a^*_i, \bm a^-) \geq \mathbb{D}(\bm a^*_i, \bm a^+)$.
% The same applies to Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples}.
% Our proposition \ref{proposition:convex_feasible_space}
% and \ref{proposition:open_convex} remain the same, 
% while the optimal action used in proposition \ref{propositoin:alpha_beta} needs to be changed to $\bm a^*_i$.
% The proposition \ref{proposition:convergences} remains the same as we can have the corresponding data buffer $\mathcal{D}_i$ to save the relative corrective feedback w.r.t $\bm a^*_i$, which doesn't influence the convergence of this algorithm.
% However, this assumption is difficult to hold as it requires an additional method to classify which modes the action pair $(\bm a^+, \bm a^-)$ belongs to.