\section{Policy shaping via Desired action spaces}
% \section{Policy shaping}
\label{sec:Policy_shaping}

In this section, we show how to train a policy using desired action spaces. 
First, Section~\ref{sec:sub:algorithm_implicit_policy_shaping} outlines the algorithm for implicit policy shaping, providing an overview of its key components.
Next, in Section~\ref{section:sub:prob_desired_action_space}, we introduce a probabilistic formulation of the desired action space. 
This formulation defines an observation model of how an action can explain the observed action pair. 
Based on this model, we derive a posterior distribution, which serves as a target for updating the robot's policy.
Then, based on this idea, in Section~\ref{sec:sub:implicit_policy_shaping}, we present a novel loss function used to train an implicit policy, represented by an energy-based model.
Finally, in Section~\ref{sec:sub:explicit_policy_shaping},  we provide a simplified formulation, which assumes a Gaussian-parameterized policy. This formulation can be useful for simpler tasks, where data efficiency is more critical than policy complexity.

% In this section, we show the connection between a policy and a desired action space.
% In Section~\ref{section:sub:prob_desired_action_space}, we formulate the Bayes estimates of the desired action space, where the probability of how an action can explain the received feedback and the probability of how actions can be sampled from the desired action space enforced by the received feedback are detailed.
% In Section~\ref{},
% we formulate the maximum likelihood estimation, where the policy is 



% \begin{align}
% \ell (\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \frac{\exp (\pi_{\bm \theta}(\bm a^{-}|\bm s) )}{\exp (\pi_{\bm \theta}(\bm a^{+}|\bm s) ) + \exp (\pi_{\bm \theta}(\bm a^{-}|\bm s) )}
% \end{align}


% \begin{align}
%    \mathbb{E}_{(\bm a^{+},  \bm a^{-}, \bm s) \sim  \mathcal{D}} \mathbb{E}_{\bm a \in \mathcal{A}^{(\bm a^-, \bm a^+)}}[\log \pi_{\theta}(\bm a | \bm s)]
% \end{align}

\subsection{Algorithm for Implicit Policy Shaping}
\label{sec:sub:algorithm_implicit_policy_shaping}
% The algorithm of using our proposed loss to train the implicit policy, represented as the energy-based model, is shown in Algorithm \ref{alg:implicit_policy_shaping}.
The algorithm for training the implicit policy, represented as an EBM, is outlined in Algorithm \ref{alg:implicit_policy_shaping}. 
This algorithm leverages a novel loss function (line \ref{alg:line:estimate_policy}-\ref{alg:line:KL_loss}), which will be detailed in the following sections, to effectively shape the policy and align it with desired action spaces.
In each update step, action samples are drawn from the current EBM using MCMC sampling (line \ref{alg:line:MCMC}).
These sampled actions are then used to estimate both the current policy distribution and the target policy distribution (lines \ref{alg:line:estimate_policy}- \ref{alg:line:estimate_target}).
Finally, the KL loss is computed based on these estimates, and the EBM parameters ${\bm \theta}$ are updated accordingly (line \ref{alg:line:KL_loss}).
An example of the training process using Algorithm \ref{alg:implicit_policy_shaping} is shown in Fig. \ref{fig:Fig9_illustration_implicit_policy_loss_effects}, which illustrates how the algorithm adjusts the density of an EBM and effectively aligns it with a desired action space.  
% In this example, the batch size is set to 1, and the same observed action pair $(\bm a^r, \bm a^h)$ is used to update the EBM over three consecutive steps. 
% Initially, the EBM has the most density outside the desired action space $\hat{\mathcal{A}} (\bm a^r, \bm a^h)$. After the first update step, the peak density shifts toward the desired action space but still retains significant density outside it. 
% With two additional update steps, the EBM is fully inside the desired action space. 


\begin{algorithm}[]
        \caption{Implicit Policy shaping}
        \label{alg:implicit_policy_shaping}
        \begin{algorithmic}[1]
        \Require  Buffer batch \( \mathcal B \), energy-based model $E_{\bm \theta}$
    % \For{Update step = 1, 2, $\dots$, S}
        % \State Sample a batch $\mathcal{B}$ from $\mathcal{D}$
        \State Generate action samples $\mathbb A$ from $E_{\bm \theta}(\bm s, \cdot), \bm s \in \mathcal{ B}$ \label{alg:line:MCMC}
        \State Estimate policy $\pi_{\bm \theta}(\bm a | \bm s) \! \propto {\exp(-E_{\bm \theta}(\bm s, \bm a))}, \bm a \in \mathbb A$  \label{alg:line:estimate_policy}
        \State Calculate observation model $p(\bm a^h \succeq \bm a^r | \bm a, \bm s), \bm a \in \mathbb A$
        \State Estimate target distribution $\pi^{\text{target}}(\bm a| \bm s) , \bm a \in \mathbb A $\label{alg:line:estimate_target}
        \State Calculate KL loss $\mathbb{E}_{\mathcal{B}} \left[ \mathrm{KL}\!\left( \pi^{\text{target}}(\bm a| \bm s) \big\| \pi_{\bm \theta}(\bm a | \bm s) \right) \! \right] $\label{alg:line:KL_loss}
    % \EndFor
        \end{algorithmic}
        \end{algorithm}

        \begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figs/Fig9_illustration_implicit_policy_loss_effects_2.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig9_illustration_implicit_policy_loss_effects.svg} 
    \vspace*{-5mm}
	\caption{2D example of the training process of the energy-based model using Algorithm \ref{alg:implicit_policy_shaping}. The gray-shaped area represents the desired action space enforced by the observed action pair. The contour map shows the value of the EBM. In this example, the batch size is set to 1, and the same observed action pair $(\bm a^r, \bm a^h)$ is used to update the EBM over three consecutive steps. 
Initially, the EBM has the most density outside the desired action space $\hat{\mathcal{A}} (\bm a^r, \bm a^h)$. After the first update step, the peak density shifts toward the desired action space but still retains significant density outside it. 
With two additional update steps, the EBM is fully inside the desired action space.  }
 \label{fig:Fig9_illustration_implicit_policy_loss_effects}
\end{figure}
        
% \textbf{Connections to desired action space $\mathcal{A}^{\mathcal{D}}$} \textcolor{red}{Keep this or remove?}
% % We denote the data buffer as $\mathcal{D}_{k-1} = \{[\bm s_i, \bm a_i, e \bm h_i], i = 1, \dots, k \}$ that contains all the received corrective feedback, where $k$ is the total number of the data tuples. 
% % The policy trained by data $\mathcal{D}_{k}$ 
% %  via loss Eq. \eqref{} can generate actions that belong to the overall desired action space enforced by $\mathcal{D}_{k}$, which can be represented as $ \pi_{\theta}(\bm a | \bm s) =p (\bm a | \bm s, \mathcal{D}_{k})$, which denotes the probability of an action $\bm a$ being optimal given the data $ \mathcal{D}_{k}$.
% % For a new feedback tuple $(\bm a^-_{k+1}, \bm a^+_{k+1}, \bm s_{k+1})$, by the Bayesian rule, we have
% % \begin{align}
% %     p (\bm a | \bm s, \mathcal{D}_{k+1}) \propto p (\bm a | \bm s, \mathcal{D}_{k}) \cdot p( \bm a^{+}_{k+1} \succeq \bm a^{-}_{k+1} | \bm a. \bm s),
% % \end{align}
% % where $p (\bm a | \bm s, \mathcal{D}_{k+1})$ is the posterior distribution of $\bm a$ given the new feedback. 



% \begin{figure}
%     \centering
%     \includegraphics[scale=0.59]{figs/Fig4_illustration_loss_withGaussianpolicy.png}
% 	\caption{\textcolor{red}{could remove this if not necessary} Illustration of the MLE loss and Bayes loss with a Gaussian policy. For a fixed variance, the loss is actually updating the mean of the Gaussian, by the weighted average of actions in $A$, weighted by $p (\bm a|\bm a^{+},  \bm a^{-})$ for MLE loss and $\pi^{\text{target}}(\bm a | \bm s)$  for Bayes loss. }
%  \label{fig:Fig4_illustration_loss_withGaussianpolicy}
% \end{figure}



% \textbf{Why Bayes loss is stable compared with MLE loss?}
% (1) As shown in Fig. \ref{fig:Fig4_illustration_loss_withGaussianpolicy},  for MLE loss, the larger the difference between the target distribution and the current policy, the greater the influence. This can lead to the instability and make mode oscillating. 
% (2) Instead, in Bayes loss, as visualized in Fig. \ref{fig:Fig6_illustration_loss_EBMs}, the target policy is incremental, which prevents the case where the large difference between the target and current policy leads to training instability.  






\subsection{Probabilistic Modeling of Desired Action Spaces}
\label{section:sub:prob_desired_action_space}


Our objective is to infer the optimal action from multiple desired action spaces.
To do so, we seek to define a probability (or utility) function that measures how likely any given random action is to be optimal, conditioned on the information of a desired action space.
As a first step, we introduce the observation model $\text{Pr} [\bm a \in \hat{\mathcal{A} }{(\bm a^r, \bm a^h)} | \bm a , \bm s] $. 
% For a specific action $\bm a$ under state $\bm s$, this model gives
% the probability that the set $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$ includes this action $\bm a$.
This model outputs the probability of a given action $\bm a$ belonging to set $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$, conditioned on state $\bm s$.



\subsubsection{Observation model for CLIC-Half}
For $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$ defined by the intersection of desired half-spaces in Eq. \eqref{eq:def_desired_action_space_From_Halfspaces}, by leveraging conditional independence, we have
\begin{align*}
    \text{Pr} [\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}| \bm a , \bm s] =  \prod_{i=1}^{N_I} \text{Pr} [\bm a \in \mathcal{A}^H{ (\bm a^-_i, \bm a^{+}_i)}| \bm a, \bm s],
\end{align*}
where $(\bm a^-_i, \bm a^{+}_i)$ are the contrastive action pairs obtained from the observed action pair $(\bm a^r, \bm a^h)$, as explained in Section~\ref{sec:sub:CLIC_one_corrective_feedback}. The observation model of one desired half-space $\mathcal{A}^H {(\bm a^-, \bm a^+)}$ is defined as:
\begin{align*}
    \text{Pr} [\bm a \in \mathcal{A}^H {(\bm a^-, \bm a^+)} | \bm a , \bm s] \!= \!\!  \left\{ {\begin{array}{*{20}{c}}
	 1, \| \bm a- \bm a^-\| - \| \bm a- \bm a^+\| \geq 0\\
	  0, \| \bm a- \bm a^-\| - \| \bm a- \bm a^+\| < 0
	\end{array}} \right. 
\end{align*}
The above observation model outputs 1 if the given action lies within the desired half-space, as defined by Eq.~\eqref{eq:desired_half_space}, and 0 otherwise.
To account for the potential noise in human feedback and the possible violation of underlying assumptions in real-world scenarios, the hard decision boundary can be smoothed by a sigmoid function:
\begin{align}
    \text{Pr} [\bm a \in \mathcal{A}^H {(\bm a^-, \bm a^+)} | \bm a , \bm s] \!= \!  \sigma_T(\| \bm a- \bm a^-\| \!- \!\| \bm a- \bm a^+\|) \!\!
    \label{eq:observation_Model_half}
\end{align}
where $\sigma_T(x) = \frac{1}{1 + \exp (- x/T)}$ is the sigmoid function and $T > 0$ is a temperature parameter that determines how sharply the function transitions between 0 and 1. 
As $T \rightarrow 0$, $\sigma_T(x)$ behaves more like a step function: the probability becomes 1 if $\bm a$  is inside $ \mathcal{A}^H {(\bm a^-, \bm a^+)}$ and 0 otherwise. 
% For completeness, we also define $  \text{Pr} [\bm a \notin \mathcal{A} ^H{(\bm a^-, \bm a^+)} | \bm a , \bm s] = 1-  \text{Pr} [\bm a \in \mathcal{A}^H {(\bm a^-, \bm a^+)} | \bm a , \bm s]$.
One example of the observation model of the desired half-space in 2D is shown in Fig. \ref{fig:Fig2_illustration_2d_measurement_model}(a). 
% Similarly, one example of the observation model of the desired action space $\hat{\mathcal{A}} {(\bm a^r, \bm a^h)}$ with intersected desired half-spaces is shown in Fig. \ref{fig:Fig2_illustration_2d_measurement_model} (b). 
Similarly, Fig. \ref{fig:Fig2_illustration_2d_measurement_model}(b) provides an example of the observation model for the desired action space $\hat{\mathcal{A}} {(\bm a^r, \bm a^h)}$, which is the intersection of multiple desired half-spaces.


\subsubsection{Observation model for CLIC-Circular}
As introduced in Section~\ref{sec:sub:desired_action_space_absolute},
the desired action space can be more restricted if the feedback is demonstration data. In this case, similar to the way that we define Eq.~\eqref{eq:observation_Model_half}, the observation model for the circular desired action space is defined as: 
\begin{align*}
    \text{Pr} [\bm a \in \hat{\mathcal{A}} {(\bm a^r, \bm a^h)} | \bm a , \bm s] =   \sigma_T( r(\bm a^r, \bm a^h) - \| \bm a- \bm a^h\|), 
\end{align*}
where $r(\bm a^r, \bm a^h)  =  (1-\varepsilon)\| \bm a^h - \bm a^r\|$ is the radius of the circular desired action space, as in Eq.~\eqref{eq:desired_circular_space}.
The temperature $T$ controls the steepness of the sigmoid function, for $T \rightarrow 0$, the probability becomes 1 for all action inside the ball defined by $r(\bm a^r, \bm a^h)$, and 0 otherwise. 


\subsubsection{Posterior distribution of desired actions}
% As a desired action space $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$ includes actions that could be the optimal action (with actions outside it considered undesirable), the term $\text{Pr} [\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)} | \bm a , \bm s]$ is equivalent to $\text{Pr} [\bm a^* \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}| \bm a^* = \hat{\bm a}, \bm s]$. 
% % This represents the probability that the current believed optimal action supports $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$ to include optimal action. 
% This represents the probability that, for an estimated optimal action value $\bm a^* = \hat{\bm a}$, the desired action space includes this estimated action. 
% In simpler terms, it measures how likely the estimated action $\hat{\bm a}$ is to be the actual optimal action.
% % be $\bm a^+ \succeq \bm a^-$ instead of $\bm a^+ \prec \bm a^-$. 

The observation model $\text{Pr} [\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)} | \bm a , \bm s]$ indicates how likely a given action $\bm a$ can explain the observed action pair $(\bm a^r, \bm a^h)$ and also how likely it belongs to the corresponding desired action space ${\mathcal{A}} {(\bm a^r, \bm a^h)}$. 
For simplicity, we use $\bm a^h \succeq \bm a^r$ to indicate that $\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$. This notation $\bm a^h \succeq \bm a^r$ indicates that an action $\bm a$ is considered to be within the set $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$, defined by the condition that $\bm a^h$ is more preferred than $\bm a^r$ by the human teacher. 
We then rewrite the observation model as
$p (\bm a^h \succeq \bm a^r | \bm a ,\bm s)$.



\begin{figure}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{figs/Fig2_illustration_2d_measurement_model.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig2_illustration_2d_measurement_model.svg} 
	\caption{ Illustration of the observation model $\text{Pr} [\bm a \in \hat {\mathcal{A}} {(\bm a^r\!\!,\! \bm a^h)}| \bm a , \bm s]$ for all $\bm a \in \! \mathcal{ A}$. 
    In each figure, the state $\bm s$, human action $\bm a^h$, and robot action $\bm a^r$ are fixed, while the action $\bm a$ varies across the action space. The black dotted line denotes the boundary of $\hat {\mathcal{A}} {(\bm a^r\!\!, \!\bm a^h)}$, which is smoothed by a sigmoid function. (a) Desired half-space; (b) Desired action space from relative correction; (c) Circular desired action space.}
	\label{fig:Fig2_illustration_2d_measurement_model}
\end{figure}

%  As shown in section , the implicit information of one corrective feedback can be utilized as a way of data augmentation to remove some actions from the desired action space. 
% Therefore, we define the observation model including the data augmentation as
% \begin{align*}
%    \text{Pr} [\bm a \in \mathcal{A}^{C} (\bm s, \bm a, e\bm h) |  \bm a, \bm s] = 
%     \text{Pr} \left[\bm a \in \bigcap_{i=0}^n \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})} | \bm a , \bm s\right]
%     \\
%     = \prod_{i=1}^n \text{Pr} [\bm a \in \mathcal{A}^{ (\bm a^{I-}_i, \bm a^{I+})}| \bm a, \bm s] 
%     = \prod_{i=1}^n p (\bm a^{I+} \succeq \bm a^{I-}_i| \bm a, \bm s)
% \end{align*}
% For simplicity and make the notation clear, we redefine the observation model $p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s)$ to include this augmentation. Specifically, $p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s) \coloneqq   p ( \bm a \in \mathcal{A}^{C} (\bm s, \bm a, e\bm h) |  \bm a, \bm s)$.
% The value of the observation model $p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s)$ for all actions $\bm a \in \mathcal{A}$ is visualized in Fig. \ref{fig:Fig2_illustration_2d_measurement_model}.


Furthermore, we define the prior distribution $p(\bm a | \bm s)$ as the probability of selecting action  $\bm a$ given state $\bm s$.
This distribution represents the initial belief about the likelihood of selecting actions before incorporating any new observed action pairs. 
By the Bayes' rule, the posterior belief $ p (  \bm a |\bm a^h \succeq \bm a^r,  \bm s)$ can be obtained as 
\begin{align}
p ( \bm a |  \bm a^h \succeq \bm a^r,  \bm s) =\frac{ p (\bm a^h \succeq \bm a^r | \bm a ,\bm s) p(\bm a | \bm s)}{p (\bm a^h \succeq \bm a^r | \bm s)}, 
% \\    p (\bm a^h \succeq \bm a^r | \bm s) = \int_{\bm a'} p (\bm a^h \succeq \bm a^r | \bm a', \bm s) p(\bm a'  | \bm s) d \bm a'
    \label{eq:posterior_data}
\end{align}
where $ p (\bm a^h \succeq \bm a^r | \bm s) = \int_{\bm a'} p (\bm a^h \succeq \bm a^r | \bm a', \bm s) p(\bm a'  | \bm s) d \bm a'$ is the normalization term.
  The posterior $p ( \bm a |  \bm a^h \succeq \bm a^r,  \bm s)$ represents the probability of $\bm a$ being selected, given the observed action pair $(\bm a^r, \bm a^h)$  at state $\bm s$.
  This posterior can be used to estimate optimal actions thanks to the definition of the desired action space. 
  Specifically, the desired action space $\hat{\mathcal{A}}{(\bm a^r, \bm a^h)}$ is defined to include actions that could potentially be the optimal actions, while actions outside this space are considered undesirable. As a result, the posterior $p(\bm a | \bm a^h \succeq \bm a^r, \bm s)$ indicates the likelihood that $\bm a$ is an optimal action, given the observed data.
Therefore, a natural way of learning a policy from data $(\bm s, \bm a^r, \bm a^h)$ is to align it with this posterior distribution, as we will detail in the following section. 



% By assuming a uniform distribution of $p(\bm a | \bm s)$,

% We can set the prior $p(\bm a | \bm s)$ as $\pi(\bm a| \bm s)$, then
% the posterior can be denoted as $ p^{\pi} (  \bm a |\bm a^+ \succeq \bm a^-,  \bm s)$, which represents the probability of the action being sampled by the policy $\pi$ filtered by $\mathcal{A}^{(\bm a^-, \bm a^+)}$. 
% \begin{align}
% p^\pi ( \bm a |  \bm a^+ \succeq \bm a^-,  \bm s) \propto { p (\bm a^+ \succeq \bm a^- | \bm a ,\bm s) \pi(\bm a | \bm s)} 
% \end{align}




\subsection{Loss function for Implicit Policy Shaping}
\label{sec:sub:implicit_policy_shaping}
% \subsubsection{Maximum likehood estimation}

% The received action pair feedback can be used to shape the policy. Specifically, at a given state $\bm{s}$, a feedback signal $(\bm{a}^r, \bm{a}^h)$ suggests that the policy $\pi_\theta$ should increase the probability of selecting actions within the set $\hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h)$, denoted as $\pi_{\bm \theta}(\bm{a} \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) | \bm{s})$. Consequently, the probability of selecting actions outside this set, $\pi_{\bm \theta}(\bm{a} \notin \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) | \bm{s})$, should decrease.


The desired action space constructed by the observed action pair can be utilized to shape the robot's policy. 
The intuition is that at a given state $\bm s$, 
one observed action pair $(\bm a^r, \bm a^h)$ indicates that the policy $\pi_\theta$ should increase the probability of selecting actions within the set $\hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h)$, denoted as $\pi_{\bm \theta}(\bm{a} \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) | \bm{s})$. Consequently, the probability of selecting actions outside this set, $\pi_{\bm \theta}(\bm{a} \notin \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) | \bm{s})$, should decrease.
% Formally, $\pi_{\bm \theta}(\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}  | \bm s) = \int_{\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}}  \pi_{\bm \theta}(\bm a|\bm s)    d \bm a$. 
To increase $\pi_{\bm \theta}(\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)} | \bm s)$, the policy $\pi_{\bm \theta}$ can be updated to align with a target distribution that assigns a high probability to actions within $\hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}$.  The posterior distribution defined in Section~\ref{section:sub:prob_desired_action_space} can be this target distribution $\pi^{\text{target}}(\bm a| \bm s ) \coloneqq p(\bm a|\bm a^h \succeq \bm a^r, \bm s) $. 
As a result, the policy can then be optimized by minimizing the KL divergence between the estimated posterior and the policy distribution:
\begin{align}
    \ell_{KL}(\bm \theta) = \!\! \!\!\underset{(\bm{a}^{h}, \bm{a}^{r}, \bm{s}) \sim p_{\mathcal D}}{\mathbb{E}} \!\!\left[ \mathrm{KL}\left(\pi^{\text{target}}(\bm a| \bm s ) \big\| \pi_{\bm \theta}(\bm a | \bm s) \right) \right] 
    \label{eq:KL_loss_general}
\end{align}


To calculate the above loss, we need to evaluate the posterior distribution $p(\bm a|\bm a^h \succeq \bm a^r, \bm s) $.
% , referred to as the target distribution.
This posterior depends on the prior distribution $p(\bm a | \bm s)$, which has yet to be specified.
In the following sections, we introduce two different assumptions for this prior distribution.

\subsubsection{ Uniform Bayes loss}
\label{sec:sub:sub:uniform_bayes_loss}
% \textcolor{red}{Add more ablation, to show the advantages of the ibc and CLIC-circular}
We assume a uniform distribution of $p(\bm a | \bm s)$, then the posterior distribution is directly proportional to the prior distribution:
\begin{equation*}
   \pi^{\text{target}}(\bm a| \bm s )  \propto p (\bm a^h \succeq \bm a^r | \bm a ,\bm s). 
\end{equation*}
% $p(\bm a|\bm a^h \succeq \bm a^r, \bm s) \propto p (\bm a^h \succeq \bm a^r | \bm a ,\bm s)$.
To approximate the loss in Eq.~\eqref{eq:KL_loss_general}, 
we estimate both $\pi^{\text{target}}(\bm a| \bm s ) $ and $\pi_{\bm \theta}(\bm a | \bm s)$ using a set of action samples, defined as follows:
\begin{equation*}
    \mathbb A = \{ \bm a^h, \bm a^r\} \cup   \{\bm a_j | j = 1, \dots, N_{\text{a}} \},
\end{equation*}
where $N_a$ samples $\{\bm a_j \}$ can be obtained by Langevin MCMC as described in Eq.~\eqref{eq:mcmc_sampling}. 
Given this set of actions $\mathbb A $,
 the policy evaluated at each sampled action can be approximated via 
\begin{align}
\label{eq:estiamtion_policy_EBM}
    \pi_{\bm \theta}(\bm a | \bm s) & = \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\sum_{\bm a' \in \mathbb A} e^{-E_{\bm \theta}(\bm s, \bm a')}}, \forall \bm a \in \mathbb A
\end{align}


Using a uniform prior leads to a target distribution that treats sampled actions within the desired action space, $\bm a \in \mathbb{A} \cap \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}$, almost equally.
However, this target distribution is inaccurate as some of these actions are suboptimal.
Consequently, the uniform Bayes loss can become overly optimistic by updating the policy to generate these suboptimal actions, potentially steering the policy in the wrong direction.
In the following section, we introduce a conservative update loss to address this issue.
% Instead, we propose a conservative update loss in the next section. 

% \textbf{Connections to maximum likelihood estimation}
% Minimizing the KL loss in Eq.~\eqref{eq:KL_loss_general} is equivalent to maximizing the MLE loss
% \begin{align}
%    \mathbb{E}_{(\bm a^{h},  \bm a^{r}, \bm s)} \mathbb{E}_{\bm a \sim p (\bm a|\bm a^h \succeq \bm a^r, \bm s)}[\log \pi_{\theta}(\bm a | \bm s)]
% \end{align}
% % since the policyâ€™s probability of selecting actions in the desired action space is given by $\pi_{\bm \theta}(\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}  | \bm s) = \int_{\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}}  \pi_{\bm \theta}(\bm a|\bm s)    d \bm a$, the above MLE loss is equivalent to maximize $\mathbb{E}_{(\bm a^{h},  \bm a^{r}, \bm s)} [\pi_{\bm \theta}(\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}  | \bm s) ]$.
% This loss can be interpreted as a process where actions are first sampled from the set $\hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}$.
% Under the assumption of a uniform prior, each action in the set has an equal chance of being chosen. 
% The loss then increases the likelihood that the policy selects these sampled actions. 
% However,  many of the sampled actions may be suboptimal, potentially misleading the policy toward incorrect improvements.

% which is equivalent to minimizing the KL loss 
% \begin{align*}
%     \ell_{mle}(\bm \theta) = \mathbb{E}_{(\bm a^{+},  \bm a^{-}, \bm s)} \left[  D_\text{KL} \left(p (\bm a|\bm a^+ \succeq \bm a^-, \bm s) || \pi_{\theta}(\bm a | \bm s) \right) \right] 
% \end{align*}





\subsubsection{Policy-weighted Bayes loss}
\label{sec:sub:sub_poicy_weighted_Bayesloss}
The above uniform Bayes loss treats action samples within $\mathbb{A} \cap \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}$ almost equally and may over-optimistically lead the policy toward incorrect updates.
To address this, we propose a conservative update approach when increasing $\pi_{\bm \theta}(\bm a \in \hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}  | \bm s)$.
This can be achieved by defining a policy-dependent target distribution $\pi^{\text{target}}$ that remains close to the current policy, assuming $p(\bm a| \bm s) = \pi_{\bm \theta}(\bm a| \bm s)$:
\begin{align}
\pi^{\text{target}}(\bm a| \bm s)  \propto \! { p (\bm a^h \!\succeq\! \bm a^r | \bm a ,\bm s) \pi_{\bm \theta}(\bm a | \bm s)}.\!\! 
\end{align}
The policy $\pi_{\bm \theta}(\bm a | \bm s)$ acts as a filter for the observation model  $p (\bm a^h \succeq \bm a^r | \bm a,\bm s)$. 
For action samples within $\hat{ \mathcal{A}}{(\bm a^r, \bm a^h)}$, the target policy distribution $\pi^{\text{target}}$ tends to assign higher probabilities to actions favored by the current policy $ \pi_{\bm \theta}$, and lower probabilities to actions that $\pi_{\bm \theta}$ considers unlikely.
Similarly to Eq.~\eqref{eq:estiamtion_policy_EBM}, this target distribution can be estimated using the sampled actions in $\mathbb A$:
\begin{align}
  \pi^{\text{target}}(\bm a| \bm s) \!= \!
   \frac{e^{-E_{\bm \theta}(\bm{s},\bm{a}) + \ln p(\bm{a}^h \succeq \bm{a}^r | \bm{a}, \bm s)}}{\sum_{\bm{a}' \in \mathbb{A}} e^{-E_{\bm \theta}(\bm{s},\bm{a}') + \ln p(\bm{a}^h \succeq \bm{a}^r | \bm{a}', \bm s)}}, \! \forall\bm a \!\in  \! \mathbb A.\!\!\!
\end{align}
Then the loss can be calculated as in Eq. \eqref{eq:KL_loss_general}. 
% then becomes 
% \begin{align*}
%     \ell_{KL}(\bm \theta) = \!\!\!\underset{(\bm{a}^{h}, \bm{a}^{r}, \bm{s}) \sim p_{\mathcal D}\!\!}{\mathbb{E}} \left[ \mathrm{KL}\!\left( \pi^{\text{target}}(\bm a| \bm s) \big\| \pi_{\theta}(\bm a | \bm s) \right) \! \right] 
% \end{align*}
 


% Convergence: 
% By minimizing $\ell_{KL}(\bm \theta)$ and update the policy $\pi$ for several iterations, we have
% \begin{align}
%     p^{\pi}_{data} (\bm a | \bm s) = \frac{1}{|\mathcal{D}|}\sum_{\mathcal{D}} p^{\pi}(\bm a | \bm a^+ \succeq \bm a^-, \bm s) \cdot \mathbf{1}(\bm s_i =\bm s) \cdot \\
%     \pi(\bm a | \bm s) \simeq p^{\pi}_{data} (\bm a | \bm s)
% \end{align}


% \textbf{Analogy with Behavior Cloning (BC)}
% We present a probabilistic formulation of Behavior Cloning (BC). For BC with a dataset \(\mathcal{D}^{BC} = \{(\bm{s}_i, \bm{a}_i)\}\), the data distribution \(p_{\text{data}}(\bm{a} | \bm{s})\) can be expressed as:
% \[
% p_{\text{data}}(\bm{a} | \bm{s}) = \frac{1}{|\mathcal{D}|} \sum_{i} p(\bm{a} | \bm{a}_i, \bm{s}_i) \cdot \mathbf{1}(\bm{s}_i = \bm{s}),
% \]
% where \(p(\bm{a} | \bm{a}_i, \bm{s}_i)\) represents the likelihood of action \(\bm{a}\) given the labeled action \(\bm{a}_i\) and state \(\bm{s}_i\). 
% A simple example of this likelihood is assigning a value of 1 if \(\bm{a} = \bm{a}_i\), and 0 otherwise. This captures the deterministic nature of many BC formulations.
% The main difference between CLIC and BC is that CLIC does not assume an action label, which makes the target data policy $p_{data} (\bm a | \bm s)$ undefined. 
% Therefore, it's necessary for CLIC to create a temporary target $ p^{\pi}_ {data} (\bm a | \bm s)$ that guides the policy on which direction to improve.


\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figs/Fig6_illustration_loss_EBMs.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig6_illustration_loss_EBMs.svg} 
	\caption{Illustration of various loss functions for training an EBM in a 1D action space. Orange dots denote action samples, with orange arrows indicating increased energy values and green arrows showing decreased energy values.}
 \label{fig:Fig6_illustration_loss_EBMs}
\end{figure}



\textbf{Connections to InfoNCE loss} The KL loss has close connections to the InfoNCE loss used in IBC, defined in Eq.~\eqref{eq:ibc_info_NCE}.
By neglecting the constant term $c$ irrelevant to the parameter $\bm \theta$, our KL loss can be reorganized as a weighted sum of the InfoNCE loss({See Appendix \ref{appendix:connection_INFONCE}}):
% \begin{align*}
%     \ell_{KL}(\bm \theta) &\Leftrightarrow
%     \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \log \pi_{\theta}(\bm a | \bm s) \\
%     % &= \sum_{\bm a } -p (\bm a|\bm a^{+},  \bm a^{-}) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\displaystyle\sum_{\bm a' \in \mathcal{A}^{-}} e^{-E_{\theta}(\bm s, \bm a'_j)} + \sum_{\bm a' \notin \mathcal{A}^{+}} e^{-E_{\theta}(\bm s, \bm a'_j)}} \\
%     &\overset{Eq. \eqref{eq:estiamtion_policy_EBM}}{\simeq} \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \backslash  \bm a)
% \end{align*}
\begin{align*}
    \ell_{\!KL}\!(\bm \theta) \!
    % &= \sum_{\bm a } -p (\bm a|\bm a^{+},  \bm a^{-}) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\displaystyle\sum_{\bm a' \in \mathcal{A}^{-}} e^{-E_{\theta}(\bm s, \bm a'_j)} + \sum_{\bm a' \notin \mathcal{A}^{+}} e^{-E_{\theta}(\bm s, \bm a'_j)}} \\
    % \simeq 
   \simeq \!\!\!\!
    \!\!\!\! \underset{(\bm{a}^{h}\!, \bm{a}^{r}\!, \bm{s})  \sim p_{\mathcal D}}{\mathbb{E}} \!\! \sum_{\bm a \in \mathbb A} \!\! -p (\bm a|\bm a^h \!\succeq \! \bm a^r\!, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \! \backslash  \! \{\bm a\}) \!\!+ \!c
\end{align*}
For each selected positive action $\bm a \in \mathbb{A}$, the InfoNCE loss decreases its energy value while increasing the value of the other sampled actions $\bm a' \in \mathbb{A} \backslash  \{\bm a \}$.
The key difference between InfoNCE loss and our KL loss lies in how these action samples within $\mathbb{A}$ are classified.
InfoNCE loss treats all the sampled actions as negative, and only human actions as positive based on the optimal action assumption. 
In contrast, our KL loss classifies each action sample based on the desired action space.
Specifically, the posterior probability $p (\bm a|\bm a^h \succeq \bm a^r, \bm s)$ acts as a weight that quantifies how likely the selected action $\bm a$ is to be optimal according to the observed action pair. 
To combine the effects of each InfoNCE loss, the weighted average is computed for each selected action $\bm a \in \mathbb{A}$.
As a result, actions with higher posterior probability have their energy reduced, while actions with lower posterior probability (such as $\bm a^r$) see their energy increased. 
 This weighting strategy avoids overfitting by preventing the loss from being dominated by a single action label. 

Here, we summarize the effects of the different loss functions. The InfoNCE loss reduces the energy of human actions while increasing the energy of all sampled actions.
In contrast, our KL loss (both uniform and policy-weighted Bayes) lowers the energy of sampled actions that lie within the desired action space and raises the energy of other sampled actions.
Since the action samples within the desired action space may still be suboptimal, the uniform Bayes loss can mislead the policy in the wrong update direction and may make the training process unstable. In contrast, the policy-weighted Bayes loss generates a target energy label that is closer to the existing EBM. 
This conservative update can mitigate the negative effects of suboptimal action samples within the desired action space, leading to a more stable training process. The differences between these loss functions are illustrated in Fig. \ref{fig:Fig6_illustration_loss_EBMs}.


\subsection{Explicit Policy Shaping}
\label{sec:sub:explicit_policy_shaping}

While the implicit policy model can encode multi-modal feedback data, it requires longer training and inference times than explicit policies \cite{2022_implicit_BC, 2023_diffusionpolicy}.
Therefore, we introduce a simplified version of CLIC that trains an explicit policy using the desired action space.
In this subsection, we assume that the policy follows a Gaussian distribution with fixed covariance, $\pi_{\bm \theta}(\bm a | \bm s) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma)$,  which is a common assumption for explicit models.
Under this assumption, the task is considered uni-modal, meaning there is only one optimal action for each state. 
Therefore, to update the policy using the observed action pair data $(\bm a^r, \bm a^h)$ at state $\bm s$, instead of increasing the probability of selecting actions within $\hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h)$ through  $\pi_{\bm \theta}$, we can directly enforce this probability exceeds 0.5.
Formally, the objective described in Section~\ref{sec:Policy_shaping} (increasing $\pi_{\bm \theta}(\bm{a} \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) | \bm{s})$) is adjusted to satisfy the inequality 
\begin{equation}
    \pi_{\bm \theta}(\bm a \in \hat{\mathcal{A}}{(\bm a^r, \bm a^h)}| \bm{s}) \geq \pi_{\bm \theta}(\bm a \notin \hat{\mathcal{A}}{(\bm a^r, \bm a^h)}| \bm{s}).
    \label{eq:policy_improvement_distribution}
\end{equation}
Explicit policies have been extensively used for absolute corrections in unimodal tasks \cite{2011_DAgger, 2019_HG_DAgger, 2023_Survey_LfD}. However, their application to relative corrections has been less explored. Therefore, we focus on relative corrections for this simplified version of CLIC. 
Notably, with the Gaussian policy assumption and a small covariance, the above inequality can be satisfied by enforcing this simplified inequality (See Appendix \ref{apppendix:reduce_policy_improvemnt_inequality_Gaussian_assumption}):
\begin{align}
    % \pi^*(\bm s, \bm a^{-}) \leq \pi^*(\bm s, \bm a^{+}), 
    \pi_{\bm \theta}(\bm a^{-}_i|\bm s) \leq \pi_{\bm \theta}(\bm a^{+}_i|\bm s), i = 1, \dots, N_I,
    \label{eq:policy_improvement}
\end{align}
where $(\bm a^{-}_i, \bm a^{+}_i)$ are the contrastive action pairs obtained from $(\bm a^r, \bm a^h)$, and are defined in Section~\ref{sec:Desired_action_space}.  
To make $\pi_{\bm \theta}$ satisfy the above inequality, the hinge loss can be utilized:
\begin{align*}
\ell (\bm\theta ) =  \!\!\!\! \underset{(\bm{a}^{h}, \bm{a}^{r}, \bm{s}) \sim p_{\mathcal D}}{\mathbb{E}} \sum_{i = 1}^{N_I} \max (0,  \log \pi_{\bm \theta}(\bm a^{-}_i|\bm s) - \log\pi_{\bm \theta}(\bm a^{+}_i|\bm s)  ), 
\label{eq:loss_policy_improvement}
\end{align*}
% \begin{align*}
% \ell (\bm\theta ) =  \!\!\!\! \underset{(\bm{a}^{h}, \bm{a}^{r}, \bm{s}) \sim p_{\mathcal D}}{\mathbb{E}} \sum_{i = 1}^{N_I} \max (0,  m - \log\pi_{\bm \theta}(\bm a^{+}_i|\bm s)  ), 
% \label{eq:loss_policy_improvement}
% \end{align*}
In the case of a Gaussian, this loss function simplifies to
\begin{equation*}
     \ell(\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \max (0,  || \bm a^{+} -\mu_{\bm \theta}(\bm s) ||^2 -  || \bm a^{-} - \mu_{\bm \theta}(\bm s) ||^2 ).
\end{equation*}
If Eq.~\eqref{eq:policy_improvement} is satisfied, the mean of the Gaussian falls within the desired action space defined by the relative correction, making the loss zero. Otherwise, the loss can be used to update the parameters $\bm \theta$. We refer to this approach as \emph{CLIC-Explicit}. 