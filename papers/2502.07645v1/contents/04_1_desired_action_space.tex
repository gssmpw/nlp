\section{Desired action space}
\label{sec:Desired_action_space}
In this section, we detail how to construct desired action spaces using corrective feedback and show how an overall desired action space, formed jointly by these individual spaces, converges to optimal actions.
We begin by outlining the common properties of a general desired action space. Next, we describe the process of constructing the desired action space for two types of feedback: relative and absolute corrections.
We then define the overall desired action space and introduce the CLIC algorithm.  Finally, we provide theoretical proof of the convergence of the overall desired action space.

\subsection{General Definition of a Desired Action Space}
\label{sec:sub:general_def_desiredA}
For a given state, the objective of defining the desired action space is to partition the entire action space into two mutually exclusive categories: {desired actions} and {undesired actions}. 
For an observed action pair \((\bm{a}^r, \bm{a}^h)\), we define the \textit{desired action space} \(\hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h)\) such that the following properties hold:

\begin{enumerate}
    \item {Inclusion of human action; exclusion of robot action:}
    \[
    \bm{a}^h \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h) \quad \text{and} \quad \bm{a}^r \notin \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h).
    \]
    % This ensures that the human-provided action \(\bm{a}^h\) is always within the desired action space, while the reference action \(\bm{a}^r\) is excluded.
    % \item {Inclusion of optimal action:} $\bm{a}^* \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h).$
    % \[
    % \bm{a}^* \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h).
    % \]
    % where \(\bm{a}^*\) denotes the true optimal action within the given context.
    \item {Acceptance of suboptimal actions:}
    % If feedback provides limited information (e.g., relative correction) or is uncertain (e.g., noisy feedback), 
    There may exist other suboptimal actions \(\bm{a}' \) such that:
    \[
    \bm{a}' \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h), \bm{a}' \neq \bm{a}^*.
    \]
    % \textcolor{red}{be careful about mentioning uncertainty, super set, optimal actions is a subset}
    % This accounts for the possibility that multiple actions, apart from the optimal action, can belong to the desired action space under uncertain or incomplete feedback conditions.
\end{enumerate}

To ensure that we can estimate the optimal action by employing the concept of desired action space, we assume that this space includes at least one optimal action $\bm a^*$.
% which is $\bm{a}^* \in \hat{\mathcal{A}}(\bm{a}^r, \bm{a}^h).$
% If this assumption holds, the desired action space is the superset of 
% As the optimal action is unknown in our context, there is

% \textcolor{red}{put in front of the section}
% Learning a policy from desired action space requires that $\hat{\mathcal{A}}{(\bm a^r, \bm a^h)} $ includes the optimal actions, otherwise the policy fails to improve.


% include optimal action depending on whether the assumption aligns with the feedback type.

% include undesired actions when we are uncertain about the feedback (noisy feedback) or the feedback has limited information (relative correction)  

% \begin{figure*}[t!]
% 	\centering
% 	\includegraphics[scale=0.235]{figs/Fig8AndFig9_explain_feasible_space.jpg}
% 	\caption{
%  (a) Visualization of generating pairs of positive and negative actions. Circles denote explicit actions, and squares denote implicit actions.
%  (b)
%  Examples of Proposition 2.
%   The combined desired action space (blue) is the intersection of the desired action space defined by each action pair (shown as grey and purple).
%   (c) Examples of different $\alpha$ for the same $\bm h^*$, shown as the orange dashed line. 
%   The action $\bm a+e\bm h^*$ is denoted by the orange star. 
%   With $\varepsilon = 0$,  when $\alpha \geq 2 \beta$ (left and middle), $\bm a+e\bm h^*$ is inside the desired action space. When $\alpha < 2 \beta$ (right), $\bm a+e\bm h^*$ is outside the desired action space. 
%   \textcolor{red}{symbol need change}}
% \label{fig:Fig8AndFig9_explain_feasible_space}
% \end{figure*}




\subsection{Desired Action Space from Relative Corrections}
\label{sec:sub:desired_action_space_relative}

In this section, we detail how to construct desired action spaces from relative corrective feedback.
For the observed action pair $(\bm a^r, \bm a^h)$ at state $\bm s$, contrastive action pairs can be obtained. 
Each of these contrastive action pairs divides the action space in half.
By intersecting all the half-spaces, we can obtain the desired action space $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$.
We first introduce the definition of the half-space: 

% \subsubsection{Definition}
\subsubsection{Desired half-space}
\label{sec:sub:desired_halfspace}


% For the Euclidean action space $\mathcal{A}$, 
Consider a \textit{contrastive action pair} $(\bm  a^{-}, \bm a^{+})$ at state $\bm s$, 
% which we denote as $(\bm s, \bm a^{-}, \bm  a^{+})$,
where $\bm a^{+}$ is referred to as the \emph{positive} action and $\bm a^{-}$ as the \emph{negative} one. 
The positive action $\bm a^{+}$ is defined as a \emph{better} action than $\bm a^{-}$. 
Here, \emph{better} indicates that $\bm a^{+}$ is closer to an optimal action $\bm a^*$ than $\bm a^{-}$. 
% In other words,
Formally, the distance of these two actions is defined as $\mathbb{D}(\bm a^+, \bm a^-) = \|\bm a^+ - \bm a^- \|$, and 
we assume that
% that there is one optimal action $\bm a^*$ in the action space given the state $\bm s$, and 
% that the positive action $\bm a^+$ is closer to the optimal action $\bm a^*$ than the negative action $\bm a^-$: 
\begin{equation}
\mathbb{D}(\bm a^*, \bm a^-) \geq \mathbb{D}(\bm a^*, \bm a^+),   \label{eq:action_relatinship}
\end{equation}
% where $\mathbb{D}$ is a distance function.
% where distance function $\mathbb{D}(\bm a_1, \bm a_2) = \|\bm a_1 - \bm a_2 \|^2 $.
% From this equation, the action pair $(\bm  a^{-}, \bm a^{+})$ can be used to define the \emph{desired action space}
% The set of action closer to $\bm a^+$ than $\bm a^_-$ is denoted as $\mathcal{A}^{H}{(\bm a^-, \bm a^+)}$. 
From this inequality, the \emph{desired half-space} implied by the action pair $(\bm  a^{-}, \bm a^{+})$ can be defined as
% \begin{equation}
%     \mathcal{A}^{(\bm a^+, \bm a^-)} = \{ \bm a \in \mathcal{A} | \mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+) \}.
% \end{equation}
\begin{equation}
    \mathcal{A}^H{(\bm a^-, \bm a^+)} = \{ \bm a \in \mathcal{A} | \mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+) \}.
    \label{eq:desired_half_space}
\end{equation}
% where $\mathcal{A}^H: \mathcal{A} \times \mathcal{A}  \twoheadrightarrow \mathcal{A}$ is a set-valued function. 
% Conversely, the undesired action space can then be defined as $\widetilde{\mathcal{A}}^{(\bm a^-, \bm a^+)} = \mathcal{A} \backslash  \mathcal{A}^{(\bm a^-, \bm a^+)} $.
The desired half-space satisfies the general definition of desired action space in Section~\ref{sec:sub:general_def_desiredA}.
Specifically, it is unbounded and includes undesired actions. It also includes the optimal action $\bm a^*$ if Eq. \eqref{eq:action_relatinship} holds.

% The desired half-space has the following property:
% \begin{proposition}
% \label{proposition:convex_feasible_space}
%      $ \mathcal{A}^H{(\bm a^-, \bm a^+)}$ is unbounded and includes the optimal action $\bm a^*$ if Eq. \eqref{eq:action_relatinship} holds.
%     \footnote{Proof can be found in  Appendix \ref{proof:proposotion_1}.}
%     % \footnotemark[1]
% \end{proposition}

 
 
 \subsubsection{Obtaining contrastive action pairs}
\label{sec:sub:CLIC_one_corrective_feedback}
In this subsection, we detail how we generate contrastive action pairs $(\bm a^{-}, \bm a^{+})$ from one relative corrective feedback to create desired half-spaces. 
% The relative correction assumes the human teacher does not have access to the optimal action $\bm a^*$ at state $\bm s$.
% Instead, we assume (1) the human teacher has the ability to assess whether the learner's current action $\bm a$ is suboptimal, denoted by the function $G(\bm s, \bm a^r) \in \{0, 1\}$; 
% (2) the teacher can provide a 
%   directional signal, i.e., ${\bm h \in \{ \bm d \in \mathcal{A} \mid  ||\bm d|| = 1\}}$
% if $G(\bm s, \bm a^r) = 1$,
% guiding $\bm a^r$\ towards $\bm a^*$.
% The directional signal $\bm h$ has a unit norm ($\| \bm h \| = 1$), and it is multiplied by a hyperparameter with a small magnitude $e$, resulting in a corrective signal.
% Then, for a given state where the teacher provides feedback, one relative corrective feedback tuple $[ \bm s, \bm a^r, e\bm h ]$ is generated.
 For the robot action $\bm a^r$ at state $\bm s$, the teacher provides the directional signal $\bm h$ to create one observed action pair  $(\bm a^r, \bm a^h)$ (Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}a), where $\bm a^h = \bm a^r + e \bm h.$
% To deal with the magnitude noise of $e$, 
We introduce the hyperparameter 
$\varepsilon \in [0, 1)$, which 
controls how much the robot action gets modified from $\bm h$.
% reflects the teacher's confidence in the correction magnitude $e$.
Accordingly, we define one contrastive action pair
as $(\bm a^-, \bm a^+) = (\bm a^r, \bm a^r + \varepsilon e\bm h)$, which are denoted by the red and green circle in Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}a, respectively.


\begin{figure}[t!]
	\centering
	\includegraphics[width=0.49\textwidth]{figs/Fig11_new_illustration_desired_action_space_2.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig11_new_illustration_desired_action_space_2.svg}
	\caption{
 Illustration of desired action space from relative correction: (a) Generating contrastive action pairs for one observed action pair $(\bm a^r, \bm a^h)$. 
 % Circles denote explicit actions, and
 Squares denote implicit information.
 (b) Examples of different $\alpha$ for the same $\bm h^*$. 
  % The action $\bm a+e\bm h^*$ is denoted by the orange star. 
  With $\varepsilon = 0$,  when $\alpha \geq 2 \beta$ (left), $\bm a+e\bm h^*$ is inside the desired action space. When $\alpha < 2 \beta$ (right), $\bm a+e\bm h^*$ is outside the desired action space. (c) Example of the intersection of the desired half-spaces defined by each contrastive action pair.}
\label{fig:Fig8AndFig9_explain_feasible_space}
\end{figure}
% \textbf{Data augmentation}
% \label{sec:sub:sub:implicit_info_CLIC}


Furthermore, there can be more information contained within relative corrective feedback. This \emph{implicit information} is also utilized, as a way of data augmentation, to exclude some undesired actions in $\mathcal{A}^H{ (\bm a^r, \bm a^r \!\!+ \varepsilon e \bm h)}$. 
We define the positive correction as $\bm h^{+} \!\!\!=\! \bm h$. Additionally, we define the implicit negative correction $\bm h^{-}$ as a unit vector that points in a different direction from $\bm h^{+}$.
$\bm h^{-}$ is sampled from the set: 
% $\mathcal{H}^{-}$:
\begin{equation*}
 \mathcal{H}^{-}(\bm h^{+}, \alpha) = \{ \bm h^{-} \in \mathcal{H} | \, \, \| \bm h^{-} \| = 1,  \angle(\bm h^{-}, \bm h^{+}) = \alpha \}, 
 \label{eq:assumption_negative_correction}
\end{equation*}
where angle $\alpha \in (0^\circ, 180^\circ]$ is a hyperparameter that indicates how much certainty we have on the directional information provided by the human. 
There are infinitely many possible $\bm h^{-}$ if the dimension of $\mathcal{A}$ is greater than 2. 
In this case, we can sample $\bm h^{-}$ by generating a unit vector orthogonal to \(\bm h^{+}\) and combining it with \(\bm h^{+}\) at the specified angle \(\alpha\).
With sampled $\bm h^{-}_i \in \mathcal{H}^{-}$, we can obtain corresponding action pairs $(\bm a^{I-}_i, \bm a^{I+}), i=0,\dots, N_I$, where $N_I$ represents the total number of implicit actions.
In Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}a, the green arrow denotes $\bm h^{+}$ and the red arrow denotes one sample of $\bm h^{-}$.
The action pairs ${(\bm a^{I-}_i, \bm a^{I+})}$ are defined as follows, illustrated by the squares in Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}a:
% $
%     \bm a^{I+} = \bm a^h,
%      \bm a^{I-}_i = \bm a^{r} + \varepsilon e  \bm h^{+} + (1-\varepsilon) e \bm h^{-}_i.
%      \label{eq:implicit_action_pairs}
% $
\begin{equation*}
    \bm a^{I+} = \bm a^h,
     \bm a^{I-}_i = \bm a^{r} + \varepsilon e  \bm h^{+} + (1-\varepsilon) e \bm h^{-}_i.
     \label{eq:implicit_action_pairs}
\end{equation*}


\subsubsection{Desired action space by combining implicit desired half-spaces}
\label{sec:sub:sub:interpretation_CLIC}
% In this subsection, we give the interpretation of the CLIC method from one relative corrective feedback. 
Since the desired half-space defined by each contrastive action pair is convex and includes the human action, their intersection results in a non-empty combined space. This combined space also has a smaller volume compared to each individual half-space. 
As a result, for one relative corrective feedback, we define the \emph{desired action space} $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$ as the intersection of the desired half-spaces enforced by each contrastive action pair ${(\bm a^{I-}_i, \bm a^{I+})}$: 
\begin{equation}
\label{eq:def_desired_action_space_From_Halfspaces}
\hat{\mathcal{A}}^H(\bm a^r, \bm a^h) =  \bigcap_{i=0}^{N_I} \mathcal{A}^H{ (\bm a^{I-}_i, \bm a^{I+})}. 
\end{equation}
% The desired action space is still unbounded and convex, as shown in Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}b, where a 2D action space example is shown.
One example of the desired action space in a 2D action space is shown in Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}c.
Importantly, to exclude some undesired actions in $\mathcal{A}^H { (\bm a^r, \bm a^r + \varepsilon e \bm h)}$ while ensuring $\bm{a}^*$ remains within $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$, the hyperparameter $\alpha$ and $\varepsilon$ need careful selection. 
If $\alpha$ is too small, $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$ may fail to include $\bm a^*$, as illustrated in
Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}b.
% where $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$ with different $\alpha$ values are shown for the same $\bm h^*$. 
Similarly, if $\varepsilon$ is set too large,  it could also exclude $\bm a^*$ from the desired space. 
% Fig. \ref{fig:3D_actionspace} visualizes $\mathcal{A}^{C}$ in 3D.

% \textit{Example of proposition \ref{propositoin:alpha_beta}}:
% Fig. \ref{fig:Fig8AndFig9_explain_feasible_space}c illustrates $\mathcal{A}^{[\bm s, \bm a, e\bm h]}$ 
% for different $\alpha$ values in 2D. 
% To make the relative corrective feedback able to provide useful information, the optimal action must be within the desired space. 
% This requires careful selection of $\alpha$ and $\varepsilon$, given a known directional error $\beta$ and magnitude error $\sigma$ of the relative corrective feedback.


% \textit{Example of $\mathcal{A}^{[\bm s, \bm a, e\bm h]}$ in 3D action space}:
% Fig. \ref{fig:3D_actionspace} visualize $\mathcal{A}^{[\bm s, \bm a, e\bm h]}$ in 3D.
% The value of the angle $\alpha$ and the magnitude certainty parameter $\varepsilon$ are varied to show the effects of the hyperparameters.
% The implicit negative actions $\bm a^{I-}$ are sampled uniformly from the circle determined by $\alpha$ and $\varepsilon$.


\subsection{Desired Action Space from Demonstration Feedback}
\label{sec:sub:desired_action_space_absolute}
% In Section~\ref{sec:sub:desired_action_space_relative}, we described constructing the desired action space using relative corrective feedback. 
% To construct the desired action space using demonstration feedback, 
% a straightforward approach is to transform the demonstration data into relative corrections.
% Alternatively, if the feedback quality is reliable, a stronger assumption can be made by directly defining a circular desired action space centered on the demonstrated actions.
% We detail these two options below:

In Section~\ref{sec:sub:desired_action_space_relative}, we described constructing the desired action space using relative corrective feedback. Similarly, the desired action space can be constructed from demonstrations or absolute corrections. We introduce two methods to achieve this. One approach transforms demonstration data into relative corrections. Alternatively, 
% if the demonstration feedback is reliable, 
a stricter assumption can be made by defining a circular desired action space centered around the demonstrated actions. These two methods are detailed below:

% In this section, we detail two methods that construct desired action space from demonstration feedback: 
% one way is to transform the demonstration feedback into relative corrective feedback and apply the method in Section~\ref{sec:sub:desired_action_space_relative};
% another way is to create a circular desired action space if the demonstration data is high-quality.

\subsubsection{Treating demonstration as relative correction}
For the robot action $\bm a^r$ at state $\bm s$,
if the feedback received is demonstration $\bm a^h$, one way to utilize the desired action space $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$  defined in Section~\ref{sec:sub:desired_action_space_relative} is to transform the demonstration data into relative corrective data. 
% This is useful when the demonstration data is noisy or suboptimal, as directly imitating it could lead to reduced performance.  
% We assume that $\bm a^h$ is better than the current robot's action $\bm a^r$ and create the relative corrective feedback $(\bm s, \bm a^r, \bm a^h)$.
The directional signal is obtained as $\bm h = (\bm a^h - \bm a^r) /e$.
Here, instead of being a hyperparameter, the correction magnitude $e$ is known as the distance between the robot action and human action: $e = \| \bm a^h - \bm a^r \|$.
% The difference between relative and absolute corrections lies in how the feedback is provided by the human teacher.
% For relative correction, the human teacher first provides a directional signal $\bm h$, and then the corresponding human action $\bm a^h$ is obtained. 
% In contrast, for absolute correction, the human teacher directly provides the action $\bm a^h$. The corresponding directional signal $\bm h$ can be derived when converting absolute corrections into relative corrections.


\subsubsection{Circular desired action space}
Besides the unbounded desired action space $\hat{\mathcal{A}}^H(\bm a^r, \bm a^h)$, we can make stricter assumptions by redefining the desired action space concept.
This allows us to leverage additional information from the demonstration data.
Specifically, the \emph{circular desired action space} defined by the demonstration data can be 
\begin{equation}
    \hat{\mathcal{A}}^C{(\bm a^r, \bm a^h)} = \{ \bm a \in \mathcal{A} |(1-\varepsilon) \cdot \mathbb{D}(\bm a^h, \bm a^r) \geq \mathbb{D}(\bm a, \bm a^h) \},  
    \label{eq:desired_circular_space}
\end{equation}
which is the ball centered at $\bm a^h$ with radius $(1-\varepsilon) \cdot \mathbb{D}(\bm a^h, \bm a^r)$.
The hyperparameter $\varepsilon \in [0, 1)$ adjusts the radius of the ball. 
For $\varepsilon \rightarrow 1$, the ball reduces to one action point $\bm a^h$. 
By this definition, we assume the optimal action is within the ball, and all the actions within the ball are potential candidates of the optimal action. 
If the feedback consists only of the teacher action and the robot action is unavailable, we can introduce a hyperparameter $r$ to define the radius.  However, tuning $r$ can be challenging. A small value of $r$ could reduce CLIC into IBC, as shown in an experiment in Section~\ref{sec:exp:toy_exp}, and a large value of $r$ could hinder convergence. 

To differentiate between the desired action spaces constructed in different ways, we refer to the CLIC method using a circular desired action space $\hat{\mathcal{A}}^C{(\bm a^r, \bm a^h)}$ as \emph{CLIC-Circular}. And \emph{CLIC-Half} refers to the method that uses the half-space desired action space $\hat{\mathcal{A}}^H{(\bm a^r, \bm a^h)}$, which is defined by intersecting desired half-spaces.
% To distinguish between the desired action space constructed by intersecting desired half-spaces, we name the CLIC method using circular desired space as \emph{CLIC-Circle} and for CLIC we by default refer to the algorithm using $\hat{\mathcal{A}}^H{(\bm a^r, \bm a^h)}$.



\begin{algorithm}[t]
        \caption{CLIC Algorithm}
        \label{alg:online_IIL}
        \begin{algorithmic}[1]
        \Require  buffer \( \mathcal D \), update frequency \( b \)
        \For{episode = 1, 2, $\dots$}
        \For{\( t = 1, 2, \dots \)}
            % \State Observe \( \bm s_t \), execute $\bm a_t$, receive $\bm h_t$
            \State Observe \( \bm s_t \), execute $\bm a^r_t$
            \State Receive feedback \(  \bm a^ h_t \) for \( \bm a^r_t\), if $\bm a^r_t$ is suboptimal
            \State Append \( [\bm s_t, \bm a^r_t, \bm a^ h_t] \) to \( \mathcal D \), if \(  \bm a^ h_t \) is provided \label{alg:line:aggregate_data_to_buffer}
            \If{\( t \% b = 0 \) or \(  \bm a^ h_t \) is provided} \label{alg:line:update_feq_b}
                \State Sample batch $\mathcal{B}$ from \( \mathcal D \) \label{alg:line:sample_batch}
                \State Obtain desired action spaces (Sec. \label{alg:line:desired_action_space}\ref{sec:sub:desired_action_space_relative}, \ref{sec:sub:desired_action_space_absolute})               
                \State Update policy \( \pi_{\bm \theta} \) (Algorithm \ref{alg:implicit_policy_shaping}, Sec. \ref{sec:Policy_shaping}) \label{alg:line:update_policy_once}
            \EndIf
        \EndFor
        \State Update policy \( \pi_{\bm \theta} \) as line \ref{alg:line:sample_batch}-\ref{alg:line:update_policy_once} for $N_{\text{training}}$ steps \label{alg:line:end_of_episode}
        \EndFor
        \end{algorithmic}
        \end{algorithm}



\subsection{CLIC Algorithm and Overall Desired Action Space}


        

% Proposition \ref{proposition:open_convex} shows that a single human feedback results in $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$ that includes the optimal action but also other undesired actions. 
The desired action space $\hat{\mathcal{A}}(\bm a^r, \bm a^h)$, resulting from a single corrective feedback,  includes the optimal action but also other undesired actions. 
By incorporating multiple feedback inputs, the desired action spaces from these inputs can collectively define an \textit{overall desired action space} for each state. 
CLIC is designed with the idea of aggregating multiple corrections: every time new feedback input is received for a given state, the overall desired action space for that state gets refined (see Fig. \ref{fig:Fig12_illustration_convergence_including_circle_2}).
This fits seamlessly with the IIL framework, where the teacher provides feedback when the robot performs suboptimally.
We first introduce the CLIC algorithm in Section~\ref{sec:sub:sub:algorithm_CLIC} to provide a high-level overview, then elaborate on the overall desired action space and its convergence in Section~\ref{sec:sub:sub:overall_desired_space}.

\subsubsection{Algorithm of CLIC}
\label{sec:sub:sub:algorithm_CLIC}
Algorithm \ref{alg:online_IIL} presents our CLIC method within the IIL framework, shaping the learner's policy through multiple corrections.
We assume the existence of a policy update mechanism at this stage, with details deferred to  Section~\ref{sec:Policy_shaping}.
We define the data buffer as $\mathcal{D} = \{[\bm s_t, \bm a^r_t, \bm a^h_t], t = 1, \dots \}$, which contains all the corrective feedback received (line \ref{alg:line:aggregate_data_to_buffer}).
The core part of this algorithm is from line \ref{alg:line:sample_batch} to \ref{alg:line:update_policy_once}.
In line \ref{alg:line:sample_batch}, a batch of data $\mathcal{B}$ is sampled from the buffer $\mathcal{D}$.
If new data is available, it is included in $\mathcal{B}$ to ensure the robot quickly adapts to the teacher's instructions.
In line \ref{alg:line:desired_action_space}, the desired action spaces are generated for each feedback signal in the sampled buffer.
Line \ref{alg:line:update_policy_once} calculates the loss based on the desired action space to update the policy $\pi_{\bm \theta}$ (this loss will be introduced in Section~\ref{sec:Policy_shaping}).
At the end of each episode, the policy is updated for $N_{\text{training}}$ training steps (line \ref{alg:line:end_of_episode}).

\subsubsection{Overall desired action space}
\label{sec:sub:sub:overall_desired_space}
Formally,
we denote the feedback received at state \(\bm{s}\) by \(\mathcal{D}(\bm{s}) = \{[\bm{s}, \bm{a}^r_i, \bm{a}^h_i], i = 1, \dots, k\}\), where \(k\) indicates the number of feedback samples.
We denote the \textit{overall desired action space} enforced by $\mathcal{D}(\bm s)$ as $\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_k$. 
Define the set of optimal actions at state $\bm s$ as $\mathcal{A}^*(\bm s) = \{\bm a^*_i, i = 1, \dots, N_{*} \}$, where $N_{*}$ is the number of optimal actions at state $\bm s$.
% If only one optimal action exists at state $\bm s$,
If $N_{*}  = 1$ at state $\bm s$,
$\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_k = \cap_i^k \hat{\mathcal{A}}(\bm a^r_i, \bm a^h_i)$. 
For the general multiple optimal actions case ($N_{*}  > 1$), we define 
% \begin{equation}
%    \hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_k \!\!   =\! \!   
% \begin{cases}
%     \hat{\!  \mathcal{A}}^{\mathcal{D}(\bm s)}_{k-1}  \cap  \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}), \! \! & \! \! \! \! \!  \text{if } \hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_{k-1}   \cap  \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}) \! \neq \! \varnothing   \\
%    \!  \hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_{k-1}   \cup  \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}),              & \! \! \! \! \!\text{otherwise} 
% \end{cases}
% \end{equation}
\begin{numcases}{\hat{\mathcal{A}}^{\mathcal{D}(\bm{s})}_k \!\!=}
  \hat{\mathcal{A}}^{\mathcal{D}(\bm{s})}_{k-1} \!\cap \!\hat{\mathcal{A}}(\bm{a}^r_{k}, \bm{a}^h_{k}), 
       \text{if } 
        \hat{\mathcal{A}}^{\mathcal{D}(\bm{s})}_{k-1} \cap \hat{\mathcal{A}}(\bm{a}^r_{k}, \bm{a}^h_{k}) 
          \neq\varnothing
        \label{eq:overallDesiredA_1}
        \\[0pt]
  \hat{\mathcal{A}}^{\mathcal{D}(\bm{s})}_{k-1} \!\cup\! \hat{\mathcal{A}}(\bm{a}^r_{k}, \bm{a}^h_{k}),
        \text{otherwise}
        \label{eq:overallDesiredA_2}
\end{numcases}





\begin{figure}[t!]
	\centering
	\includegraphics[width=0.49\textwidth]{figs/Fig12_illustration_convergence_including_circle_2.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig12_illustration_convergence_including_circle_2.svg} 
    \vspace*{-5mm}
	\caption{
 Overall desired action space obtained from multiple feedback inputs within one state. (1) A human action $\bm a^h_1$ is provided, forming an inital overall desired action space $\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_1 = \hat{\mathcal{A}}{(\bm a^r_1, \bm a^h_1)} $. The robot policy is updated to generate actions within this space. 
 (2) $\bm a^r_2$ is selected, and the human feedback $\bm a^h_2$ is provided, shrinking the overall desired space to $\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_2$.
The policy is then updated accordingly.
 (3) The process continues with further feedback (e.g., $\bm a^h_3$), leading to a smaller overall desired space $\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_3$.  }
\label{fig:Fig12_illustration_convergence_including_circle_2}
\end{figure}
We then provide a theoretical guarantee that as $k\rightarrow \infty$, $\hat{ \mathcal{A}}^{\mathcal{D}(\bm s)}_k$ converges to optimal actions: 

\begin{proposition}
   For any state $\bm s$, assume a trained policy $\pi_{\bm \theta}$ always select actions from $\hat{\mathcal{A}}^{\mathcal{D}(\bm s)}_k$,
   % i.e.,  $\pi_{\bm \theta}({\bm a\in \hat{\mathcal{A}}_k^{\mathcal{D}(\bm s)}}| \bm s ) = 1$,
   and there are a finite number of optimal actions.
   % assume each suboptimal action $\bm a^r_{k+1} \sim \pi_{\bm \theta}(\cdot | \bm s)$ has a nonzero probability of receiving feedback. 
   Assume that there is a nonzero probability for the teacher to provide feedback to each suboptimal action $\bm a^r_{k+1} \sim \pi_{\bm \theta}(\cdot | \bm s)$. 
    Then, as $k \rightarrow \infty$, $\hat{\mathcal{A}}^{\mathcal{D}(\bm s)}_k$ converges to $\mathcal{A}^*(\bm s)$ or a subset of $\mathcal{A}^*(\bm s)$.
   % \footnote{ A complete proof is provided in Appendix \ref{appendix:proof_convergence_DesiredA}}.
   \label{proposition:convergence}
\end{proposition}
\begin{proof}
    In Appendix \ref{appendix:proof_convergence_DesiredA}.
\end{proof}


Fig. \ref{fig:Fig12_illustration_convergence_including_circle_2} illustrates the convergence process for both the half-space and circular desired action spaces.
Beginning with an initial set $\mathcal{A}^{\mathcal{D}(\bm s)}_{k-1}$, the robot samples actions from the policy and selects one action, denoted as $\bm a^r_{k}$. 
According to the assumption stated in Proposition \ref{proposition:convergence},
$\bm a^r_{k} \in \mathcal{A}^{\mathcal{D}(\bm s)}_{k-1}$. 
The teacher will provide new feedback $\bm a^h_{k}$ if $\bm a^r_{k}$ is suboptimal. 
For a single optimal action case, 
the observed action pair $(\bm a^r_{k}, \bm a^h_{k})$ leads to a reduction in the volume of the overall desired action space
(Eq.~\eqref{eq:overallDesiredA_1}). 
With an infinite amount of feedback, $\mathcal{A}^{\mathcal{D}(\bm s)}_k$ converges to the optimal action $\bm a^*$.
When multiple optimal actions exist, the difference is that $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ can also expand its volume (Eq.~\eqref{eq:overallDesiredA_2}). Nevertheless, because only a finite number of optimal actions exists, the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_k$ cannot increase indefinitely. Instead, its volume ultimately decreases and converges to $\mathcal{A}^*(\bm s)$ or a subset of $\mathcal{A}^*(\bm s)$ given an infinite amount of feedback.
In practice, for continuous state-action spaces, it is unlikely to receive multiple feedback inputs for the exact same state. Therefore, we assume that for similar states, the generalization capabilities of DNNs allow for this process of aggregating multiple feedback signals.



% \textcolor{red}{Add text description for Fig. \ref{fig:Fig12_illustration_convergence_including_circle_2} to explain convergence}


% We denote the data buffer $\mathcal{D} = \{[\bm s_t, \bm a^r_t, \bm a^h_t], t = 1, \dots \}$ that contains all the received corrective feedback.






