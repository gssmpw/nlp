% \section{Limitations and Future work} 
% \label{sec:Limitation_future_work}
% Despite the demonstrated effectiveness of CLIC in simulations and real-world experiments, there are limitations that future work can improve.
% Firstly, our method shares a similar limitation with HG-DAgger in that it does not utilize data lacking corresponding human feedback. 
% The naive integration with approaches like those discussed in \cite{2022_Expert_Intervention_Learning}, \cite{2023_RSS_Robot_Learning_on_the_job} have been studied in Section~\ref{sec:experiments} but failed to result in significant improvement.
%  Secondly, as discussed in Section~\ref{sec:discussion}, our CLIC method fails to learn from multi-modal data because of the assumption of the Gaussian distribution policy.
% Thirdly, our method's reliance on learners' errors to provide 
%  corrective feedback poses a challenge: if the learner initially performs well, the method may not effectively enhance the robot's policy. To address this, evaluative feedback could be integrated to reinforce successful actions.
% Future research directions include 1) developing algorithms capable of utilizing data without human feedback; 2) creating algorithms for shaping policies suitable for multi-modal tasks; and 3) combining relative corrective feedback with evaluative or preference feedback to further improve the method's efficiency.


\section{Conclusion} 
\label{sec:conclusion}

In this paper, we introduce CLIC, a novel approach to learning policies from interactive human corrections. 
To achieve this, the desired action space concept and its probabilistic formulation are presented. 
These are employed to design a novel loss function to align the robot's policy with desired action spaces.
Our extensive experiments in both simulation and real-world experiments demonstrate the advantages of CLIC over state-of-the-art methods. 
 Notably, CLIC's loss function overcomes the overfitting problem inherent in behavior cloning. 
 As a result, CLIC is applicable to a broader range of feedback types and efficiently achieves robust and stable performance with an EBM-based policy.


Despite the demonstrated effectiveness of CLIC in simulations and real-world experiments, there are limitations that future work can improve.
Firstly, similar to HG-DAgger, our method does not utilize the robot's state-action data when the teacher provides no feedback \cite{2020_RSS_expert_interventio_learning}.
In future work, we plan to investigate how to incorporate these non-intervention data into CLIC. 
Secondly, 
CLIC relies on learner's errors to trigger corrective feedback. 
This can be a limitation because if the learner initially performs well, our method may not effectively improve the robot's policy. One potential solution is to incorporate evaluative feedback to reinforce learner's successful behaviors.
Beyond these limitations, several promising directions for future research include offline training of CLIC and applying the CLIC loss to train diffusion or flow-based models.

% Despite the demonstrated effectiveness of CLIC in simulations and real-world experiments, there are limitations that future work can improve.
% Firstly, our method shares a similar limitation with HG-DAgger in that it does not utilize data lacking corresponding human feedback. 
% The naive integration with approaches like those discussed in \cite{2022_Expert_Intervention_Learning}, \cite{2023_RSS_Robot_Learning_on_the_job} have been studied in Section~\ref{sec:experiments} but failed to result in significant improvement.
%  Secondly, as discussed in Section~\ref{sec:discussion}, our CLIC method fails to learn from multi-modal data because of the assumption of the Gaussian distribution policy.
% Thirdly, our method's reliance on learners' errors to provide 
%  corrective feedback poses a challenge: if the learner initially performs well, the method may not effectively enhance the robot's policy. To address this, evaluative feedback could be integrated to reinforce successful actions.
% Future research directions include 1) developing algorithms capable of utilizing data without human feedback; 2) creating algorithms for shaping policies suitable for multi-modal tasks; and 3) combining relative corrective feedback with evaluative or preference feedback to further improve the method's efficiency.

