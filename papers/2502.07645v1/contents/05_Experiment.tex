% Tips: for each results section, when you write each paragraph, try to follow this:
% (1) motivation, why are you doing this?
% (2) preset result (pure data)
% (3) give interpretation


\begin{figure}[t]
	\centering
	\includegraphics[width=0.49\textwidth]{figs/Fig5_exp_platforms.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig5_exp_platforms.svg} 
	\caption{Tasks for the simulation experiments. Each task is tested with various feedback types, including accurate demonstrations, noisy demonstrations, and relative corrective feedback. For the TwoArm-Lift task, partial feedback is also tested by applying feedback only to one of the robots.}
	\label{fig:tasks}
\end{figure}



\input{tables/sim_exps_1}
% \vspace{-20pt}

\section{Experiments}
\label{sec:experiments}

In the experiment section, we demonstrate the effectiveness of our CLIC method through a series of simulations and real-world experiments. In Section \ref{sec:exp:simulation}, we compare CLIC with state-of-the-art methods under various types of feedback in the robot action space. Section \ref{sec:exp:ablation} presents an ablation study, analyzing the impact of key parameters and design choices. In Section \ref{sec:exp:toy_exp}, a 2D toy experiment highlights how CLIC prevents overfitting. Finally, Section \ref{sec:exp:real_rotbo} showcases the performance of CLIC in real-robot experiments.


\subsection{Simulation Experiments}
\label{sec:exp:simulation}
\textbf{Baselines} We compare CLIC with multiple baselines.
For explicit policies, we consider HG-DAgger \cite{2019_HG_DAgger} and D-COACH \cite{2019_Rodrigo_D_COACH}, which are IIL algorithms that learn from demonstration data and relative correction data, respectively. These two methods are refined for better performance, as reported in Appendix \ref{appendix:baselines}.
For implicit policies, the baselines include IBC \cite{2022_implicit_BC}, PVP \cite{2023_NIPS_PVP}, and Diffusion Policy \cite{2023_diffusionpolicy}. 
As IBC and Diffusion Policy are originally offline IL methods, to make fair comparisons, we adapt them to the IIL framework to ensure fair comparisons. 
Within this IIL framework, all methods share the same structure, differing only in their specific policy update methods.
PVP \cite{2023_NIPS_PVP} employs a loss function to assign low energy values to human actions $\bm a^h$ and high energy values to robot actions $\bm a^r$, given an observed action pair $(\bm a^r, \bm a^h)$ at state $\bm s$.
IBC, detailed in Section \ref{sec:Preliminaries}, and PVP are closely related to our method because they both involve training energy-based models. 
The Diffusion Policy is a counterpart method to IBC when learning from demonstrations. It outperforms IBC because of the improved training stability offered by the diffusion model. 
To ensure fair comparisons, we use a consistent velocity control scheme across all methods. Specifically, each method outputs a velocity command for the robot's end-effector and, if applicable, the gripper as the action at each time step.

% \textbf{Baselines} We compare CLIC with multiple baselines.
% For implicit policies, the baselines include Implicit BC (IBC) \cite{2022_implicit_BC}, PVP \cite{2023_NIPS_PVP}, and Diffusion Policy \cite{2023_diffusionpolicy}. For explicit policies, the baselines include HG-DAgger \cite{2019_HG_DAgger} and D-COACH \cite{2019_Rodrigo_D_COACH}.
% HG-DAgger and D-COACH are Interactive IL algorithms that learn from demonstration data and relative corrective data, respectively, both assuming an explicit Gaussian policy\footnote{The methods are refined for better performance, reported in Appendix \ref{appendix:HGDAgger}.}.
% When adapting offline IL baselines to the Interactive IL framework, we modify the policy update process while retaining the overall IIL structure.
% The Proxy Value Propagation (PVP) \cite{2023_NIPS_PVP} method utilizes a Proxy Value loss to assign high Q-values to human actions $\bm a^h$ and low Q-values to robot actions $\bm a^r$ that trigger human intervention.
% The IBC method, detailed in Section \ref{sec:Preliminaries}, and PVP are closely related to our method, as they both involve training energy-based models. 
% The Diffusion Policy is a counterpart method to IBC when learning from demonstrations, which outperforms IBC because of improved training stability offered by the diffusion model. 
% To ensure fair comparisons, we use a consistent velocity control scheme across all methods. Specifically, each method outputs a velocity command for the robot's end-effector as the action at each time step.



\textbf{Tasks and metrics} We compared these methods across four simulated tasks, including a Push-T task introduced in \cite{2023_diffusionpolicy} and three manipulation tasks from the robosuite benchmark \citep{2020_robosuite}, as illustrated in Fig. \ref{fig:tasks} and described in Appendix \ref{appendix:simulated_experiments_task details}.
The agent is trained by each method using an IIL framework, where the agent interacts with the environment and receives feedback from a \textbf{simulated teacher}.
This simulated teacher is employed to guarantee repeatability and fairness in training, because human teachers may not provide consistent feedback across different experimental trials. 
This is an expert policy that compares its actions with those of the learner every $n$ time steps. If the distance between these actions exceeds a threshold (set at 0.2 for all tasks), the simulated teacher provides feedback to the learner.  For all the simulation tasks, we set $n=2$.
 Each method was run for 160 episodes in every experiment, with this entire procedure repeated 3 times to calculate average final success rates and convergence time steps. 
Specifically, for each individual experiment, we calculated the final success rate by averaging the success rates of the last 8 episodes, each determined by evaluating the learned policy 10 times at the end of that episode.
We defined the convergence time step as the earliest time step when the success rate exceeded 90\% of the final success rate.

\begin{table}[t!]
\caption{Various feedback in the action space}
\centering
% \begin{tabular}{@{}ll@{}}
\begin{tabularx}{0.49\textwidth}{@{}lX@{}}
\toprule
\textbf{Type of Feedback Data}      & \textbf{Definition} \\ \midrule
Accurate absolute correction              & \( \bm a^h = \bm a^* \) \\
Gaussian noise             & \( \bm a^h = \bm a^* + \bm \omega, \bm \omega \sim \mathcal{N}(\bm0, ||\bm a^* - \bm a^r||) \) \\
Partial feedback           & \( \bm a^h \in \{[\bm a^*_{r1}, \bm a_{r2}], [\bm a_{r1}, \bm a_{r2}^*]\} \) \\ 
\hline
Accurate relative correction              & \(\bm  a^h = \bm a^r + e\bm h^*, \bm h^*= \frac{\bm a^* - \bm a^r}{||\bm a^* - \bm a^r||}  \) \\
Direction noise            & \( \bm a^h = \bm a^r + e \bm h_r \), \( \angle (\bm h_r, \bm h^*) = \beta \in [0, 90^\circ) \) \\
\bottomrule
\end{tabularx}
\label{tab:feedback-definitions}
\end{table}
% briefly show different types of feedback
\textbf{Feedback types}
In addition to accurate absolute and relative corrections, Table \ref{tab:feedback-definitions} summarizes other common types of feedback humans provide. These feedback types are also utilized in the simulation experiments. Here, the optimal action $\bm a^*$ is the original action taken by the simulated teacher. The partial feedback is utilized in the TwoArm-Lift task, where $\bm a_{r\,i}, i\in\{1, 2\}$ denotes each robot's action, and $\bm a_{ri}^*$ denotes its optimal action. 


\input{tables/sim_exps_3_noisydata}
\subsubsection{Experiments with accurate feedback}
\label{sec:exp:accurate_feedback}
% briefly introduce the tasks (or in appendix)

Table \ref{tab:sim_exp_accurate} shows the results when the teacher's feedback has no noise. 

\textbf{CLIC-Half outperforms IBC, PVP, and performs on par with Diffusion Policy} 
The results shown in Table \ref{tab:sim_exp_accurate} indicate that CLIC-Half constantly outperforms IBC and PVP in terms of success rate and convergence timesteps. 
PVP fails at the robosuite tasks because its loss function only considers the energy value of observed action pairs and cannot effectively shape the EBM. 
The optimal action assumption of IBC and Diffusion Policy is valid when the teacher's demonstration feedback is noise-free.
Under such ideal conditions, this assumption should provide more informative guidance than the assumption used by CLIC-Half.
However, IBC performance decreases as the action dimension of the task increases, with zero success rate in the TwoArm-Lift task. 
Notably, CLIC-Half achieves a $37.6 \%$ higher average success rate compared to IBC. 
Besides, CLIC-Half achieves a similar performance to that of Diffusion Policy.
These results highlight the effectiveness of training EBMs using half-space desired action spaces.

% IBC performance decreases as the action dimension of the task increases, with zero success rate in the TwoArm-Lift task. 
% CLIC-Half achieves a $37.6 \%$ higher average success rate compared to IBC. 
% Besides, CLIC-Half achieves a similar performance to that of Diffusion Policy.
% These results are remarkable because the optimal action assumption of IBC and Diffusion Policy holds true when the teacher's demonstration feedback is noise-free.
% In such ideal conditions, this assumption should provide more informative guidance than the assumption used by CLIC-Half.


 
% Therefore, this result further highlights a promising alternative approach to training policies through the estimation of the optimal actions using desired action spaces.


% motivation, why are you doing this?
% preset result (pure data)
% give interpretation
\textbf{CLIC-Circular outperforms all the baselines} 
% motivation, why are you doing this?
CLIC-Circular is the version of CLIC that mostly closely resembles IBC, as it utilizes a circular desired action space under the assumption of demonstration data. 
It reduces to IBC if all the three following conditions are met: (1) a very small radius defines the circular desired action space, (2) the temperature of the sigmoid function goes to zero, and (3) the uniform Bayes loss is used instead of policy-weighted Bayes loss.
However, CLIC-Circular outperforms IBC by a large margin. Notably,  CLIC-Circular can achieve a 99$\%$ success rate in the TwoArm-Lift task while IBC achieves zero.
CLIC-Circular also outperforms CLIC-Half as it has a more strict assumption. 
Moreover, the fact that CLIC-Circular outperforms Diffusion Policy underscores the capacity of policies represented by EBMs to surpass their diffusion-based counterparts.
Therefore, while previous studies \cite{2022_arxiv_IBC_gaps, 2023_diffusionpolicy} highlight the challenges of training EBMs, our results indicate that EBM-based policies can be trained reliably using demonstration data, by leveraging the concept of the desired action space.






\input{tables/sim_exps_2}
\textbf{CLIC-Explicit achieves good results in uni-modal tasks, whereas PVP performs poorly}
The losses used by PVP and CLIC-Explicit share a critical similarity: they both increase the probability of human actions and decrease that of robot actions.
We denote this loss type as \textit{point-based loss} as it calculates the loss only on observed action pairs. In contrast, the losses for CLIC-Half, CLIC-Circular, and IBC are termed \textit{set-based loss}.
These losses utilize not only observed action pairs but also additional actions sampled from the EBM for loss calculation. 
Notably, PVP has zero success rate at robosuite tasks, whereas CLIC-Explicit performs well in Pick-Can and TwoArm-Lift—tasks that are both unimodal and align with the Gaussian policy assumption. 
Since PVP employs an EBM as its policy and CLIC-Explicit uses a Gaussian-parametrized policy, 
the result suggests that point-based loss is only effective for policies with simple forms, such as those based on Gaussian distributions, and fails to shape complex policies like EBMs. 
In contrast, the set-based loss provides richer information and is more effective for training EBMs. 





\subsubsection{Experiments with noisy demonstration feedback}
% motivation, why are you doing this?
% preset result (pure data)
% give interpretation

Noise is common when human teachers provide feedback to robots, either for absolute or relative corrections.
This can arise due to factors such as human fatigue or the limitations of teleoperation devices. Therefore, it is essential for algorithms to account for such noise.
To evaluate the ability of CLIC and baseline methods to learn from noisy feedback, we implemented two types of noise in simulation, as defined in Table \ref{tab:feedback-definitions}. 
For absolute corrections, we use a Gaussian distribution to model the noise, which is added to the original demonstration data. 
For relative corrections, the feedback is derived from absolute correction with a known magnitude. To introduce noise, we perturb the original direction signal by $45^\circ$ while maintaining its magnitude.
% we add direction noise to the original direction signal, resulting in a noisy direction signal with the same magnitude but has a $ 45 ^ \circ$ angle between the accurate signal. 
% To make the direction noise less challenging for the baselines, the relative correction is derived from absolute correction with known magnitude. 

% \textcolor{red}{detail more}

% \textbf{Implicit CLIC also outperforms Diffusion Policy when feedback is noisy or partial} 
\textbf{CLIC remains robust while baselines degrade under noisy feedback}
The results in Table \ref{tab:sim_exp_noise} show that, as feedback transitions from accurate to noisy, CLIC-Half and CLIC-Circular experience much smaller performance drops compared to the other baselines.
This can be observed by comparing Table \ref{tab:sim_exp_noise} with Table \ref{tab:sim_exp_accurate}.
In contrast, methods like Diffusion Policy and IBC perform significantly worse under noisy conditions.
This difference arises because these baselines depend on the strict assumption of having accurate demonstrations, making them unable to handle noisy feedback effectively. In comparison, CLIC allows adjusting the desired action space through hyperparameters, ensuring that the true optimal action remains within the desired action space even under noisy feedback.
This capability helps maintain robust performance under noisy conditions. 

\subsubsection{Experiments with relative or partial feedback}
\label{sec:exp:simulation_relative_partial}
% motivation, why are you doing this?
% Previous behavior cloning methods rely on accurate, high-quality demonstrations, limiting their use to scenarios with experts and precise teleoperation devices.  
When providing demonstrations is not possible, humans can provide feedback in more flexible ways, offering valuable information to guide improvement.
One such scenario involves partial feedback, where limitations in the control interface or a large action space make it challenging to provide complete demonstrations. In this case, we evaluated all methods on the TwoArm-Lift task, in which the teacher provides demonstration feedback to only one robot at a time.
Another scenario involves relative corrective feedback.
This feedback type is easier to provide than demonstration, because it does not require the teacher to know precisely which action should be taken; instead, it only necessitates an understanding of the general behavior the robot should exhibit.
To assess how effectively the methods handle this feedback type, we conducted experiments across four simulation tasks.


\textbf{CLIC-Half and CLIC-Explicit effectively learn from partial feedback}
The results of partial feedback experiments are shown in Table \ref{tab:sim_exp_relative_partial}.
In these experiments, human actions consist of two parts: actions on the \textit{feedback dimensions} (where human feedback is provided) and actions on the \textit{non-feedback dimensions} (which may be suboptimal).
While CLIC-Half maintains its success rate when transitioning from accurate demonstration to partial feedback, Diffusion Policy suffers from lower success rates and longer convergence times.  
This difference arises because the BC loss in Diffusion Policy attempts to imitate the entire teacher action, including non-feedback dimensions, potentially leading to suboptimal behaviors.
In contrast, CLIC-Half imitates a desired action space rather than a single action label.
It focuses on improving actions on the feedback dimensions while leaving the non-feedback dimensions unconstrained. 
As a result, CLIC-Half is robust to partial feedback as long as the optimal action lies within the desired action space.
The same reasoning explains the results of CLIC-Explicit outperforming HG-DAgger.
On the other hand, CLIC-Circular's performance drops because its circular desired action space might not include the optimal action. 
This result highlights the importance of ensuring the assumption of CLIC aligns with the data. 
% By using an intersected half-space that includes the optimal action, CLIC-Half avoids such issues, demonstrating consistent performance in partial feedback scenarios.

\textbf{CLIC-Half and CLIC-Explicit can learn from relative corrective feedback}
The results of relative corrective feedback are reported in Table  \ref{tab:sim_exp_relative_partial}. 
In these experiments, human actions improve upon robot actions but are not optimal.
CLIC-Half and CLIC-Explicit show only small performance drops compared to results in absolute corrections in Table \ref{tab:sim_exp_accurate}, because the correction magnitude information is unknown in relative correction.
In contrast, all demonstration-based baselines fail completely, achieving near-zero success rates. 
This failure occurs because the BC loss can mislead policy updates, especially when the current policy’s actions are better than the human actions in the dataset. 
Meanwhile, CLIC-Half and CLIC-Explicit construct desired action spaces to update the policy; as the policy improves, these spaces constructed from the dataset do not conflict with it and are still useful for policy improvement. 
Additionally, CLIC-Circular fails with a zero success rate, as its circular desired action space fails to include the optimal action.
Overall, the ability of CLIC-Half and CLIC-Explicit to learn from relative corrective feedback highlights their distinct advantages in such scenarios.
% As previously discussed in the analysis of partial feedback failures, the same limitations of behavior cloning methods contribute to their inability to handle relative corrective feedback effectively. CLIC’s ability to adapt highlights its distinct advantage in such scenarios.


\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{figs/Fig10_effects_of_alpha.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig10_effects_of_alpha.svg} 
	\caption{Hyperparameter analysis of the directional certainty parameter 
$\alpha$ for CLIC-Half. The right figure visualizes how different values of 
$\alpha$ adjust the desired action space in 3D.}
 \label{fig:Fig10_effects_of_alpha}
\end{figure}


\subsection{Ablation Study}
\label{sec:exp:ablation}
In this section, we analyze the impact of various hyperparameters and loss design choices on the performance of the CLIC method.  Specifically, we focus on the directional certainty parameter $\alpha$, the temperature $T$ used in the sigmoid function of the observation model, and the different assumptions regarding the prior probability $p(\bm{a}|\bm{s})$.
During these experiments, CLIC-Half is utilized.


\subsubsection{Effects of directional certainty $\alpha$ }
The angle $\alpha$ is the main parameter utilized to control the shape of the desired action space for CLIC-Half, as shown in the right part of Fig. \ref{fig:Fig10_effects_of_alpha}.
In this experiment, we carried out experiments in the Square task. Two feedback types were considered, one with accurate feedback and another with direction noise (noise angle $\beta=45^{\circ}$). The results are reported in the left part of Fig. \ref{fig:Fig10_effects_of_alpha}.
For accurate feedback cases, the success rate decreases when $\alpha$ is larger than $120^\circ$. 
 This occurs because increasing $\alpha$ expands the desired action space to include more undesired actions, thereby providing less useful information for updating the EBM.
For direction noise, the success rate decreases for $\alpha < 2 \beta = 90^\circ$. 
This is because for any given feedback with $\alpha < 2 \beta$, the desired action space fails to include the optimal action and misguides the EBM in its update process.
These findings highlight the importance of carefully selecting $\alpha$ to balance the trade-off: maintaining an informative desired action space and ensuring that it includes the optimal action.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{figs/Fig13_exp_ablation_2.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig13_exp_ablation_2.svg}
	\caption{Ablation study: (1) effects of the temperature parameter $T$. (2) Policy-weighted Bayes loss vs uniform Bayes loss.}
 \label{fig:Fig13_exp_ablation_2}
\end{figure}

\begin{figure*}[t]
	\centering
	% \includegraphics[width=\textwidth]{figs/Fig1_Illustration_generalization_across_state_2.png}
    \includegraphics[width=\textwidth]{figs/Fig1_Illustration_generalization_across_state_3.pdf}
    % \includesvg[width=\textwidth, inkscapelatex=false]{figs/Fig1_Illustration_generalization_across_state_3.svg} 
	\caption{ \textbf{Learned EBM landscapes across different trials}. The figure compares the energy landscapes learned by CLIC, PVP, and IBC after training in a 2D action space. Each row corresponds to the resulting EBMs of each trial. 
    In the middle part, we visualize the process of how CLIC-Circular reduces to IBC as $\varepsilon$ increases.
    CLIC-Circular ( with $\varepsilon=0.5$) effectively trains EBM across different trials, leading to consistent minima close to the true optimal action. In contrast, IBC overfits human actions and fails to estimate the true optimal action. Three evaluation metrics are shown in the right part of the figure.}
 \label{Fig1_Illustration_generalization_across_state}
\end{figure*}


\subsubsection{Effects of temperature $T$}
In Section \ref{section:sub:prob_desired_action_space}, the temperature $T$ is utilized to control the sharpness of the probability distribution $\text{Pr} [\bm a \in \mathcal{A} {(\bm a^r, \bm a^h)} | \bm a , \bm s] $.
We study the effects of $T$ in this experiment on the performance of CLIC, where four different values of $T$ are tested in the Square task. 
The results are presented in the left side of Fig. \ref{fig:Fig13_exp_ablation_2}.
When $T$ is very small ($\log_{10} T = -3$), the success rate drops sharply. At this extreme, the observation model becomes binary (0/1), creating a sharp boundary that is difficult for the neural network to learn. Conversely, when $T$ is too large ($T = 1$), the success rate also declines. In this case, the probabilities of actions belonging or not belonging to $\mathcal{A} {(\bm a^r, \bm a^h)}$ become nearly indistinguishable, offering limited information for policy improvement.
$T = 0.1$ proves to be a good balance between these extremes and is selected across all experiments for CLIC-Half. 


% motivation, why are you doing this?
% preset result (pure data)
% give interpretation
\subsubsection{Policy-weighted Bayes loss vs Uniform Bayes loss}
As described in Section \ref{sec:sub:sub:uniform_bayes_loss}, the uniform Bayes loss treats all actions within the desired action space equally, whereas the policy-weighted Bayes loss prioritizes actions closer to the current robot's policy.
To evaluate the effectiveness of the policy-weighted Bayes loss, we compared it against the uniform Bayes loss.  We implement the uniform variant of CLIC and evaluate it across four simulation tasks with accurate demonstration feedback. The results, shown in the right side of Fig. \ref{fig:Fig13_exp_ablation_2}, demonstrate that the uniform Bayes loss leads to significantly poorer performance compared to the policy-weighted Bayes loss.
This highlights the importance of incremental policy updates. Since the desired action space may include some undesired actions, staying close to the current policy helps avoid imitating unintended behaviors, resulting in a more stable training process.

\subsection{Toy Experiments on Noisy Feedback}
\label{sec:exp:toy_exp}




In this section, we present an example to illustrate the improved performance of CLIC over IBC. The toy task in this example consists of a single constant state with a 2D action space, where the optimal action is set to $\bm{0}$ (see Fig. \ref{Fig1_Illustration_generalization_across_state}). 
The objective is to estimate the optimal action through multiple corrective feedback.
We carried out experiments over 10 trials. In each trial, we generated a randomly sampled dataset consisting of 6 or 7 data points $(\bm s, \bm a^r, \bm a^h)$, where human actions were drawn from a Gaussian distribution centered at the optimal action. 
Each method was trained for 1,000 steps in an offline IL setting, and we visualized the trained EBMs for each method for the first two trials in Fig. \ref{Fig1_Illustration_generalization_across_state}.

To evaluate the methods, we introduced three metrics: (1) the mean square error (MSE) to optimal action: this measures MSE between each local minimum action of the EBM and the optimal action. (2) MSE to human action: this calculates the average MSE between each local minimum action of the EBM and its nearest human action. A smaller value indicates that the EBM is overfitting to the human action. (3) Variance across trials: this evaluates the variance of the EBM values over the entire action space across ten different trials.
These metrics are computed by averaging the results over the 10 trials and are reported in the right side of Fig. \ref{Fig1_Illustration_generalization_across_state}.



\textbf{CLIC learns consistent EBM landscapes across different trials}
% \cite{2017_NIPS_understanding_noise_generalization}
 The PVP-trained EBM tends to be over-optimistic about favorable actions by outputting low energy values for a large region of actions that are not present in the dataset, as shown in the left side of Fig. \ref{Fig1_Illustration_generalization_across_state}.
 This occurs because PVP calculates its loss using only observed action pairs, leaving the energy values of other actions in the action space uncontrolled. 
  On the other hand, IBC's loss function encourages human actions and discourages all other actions, even actions that are very similar to human actions. Consequently, the IBC-trained EBM overfits the data, learning minima corresponding to individual human actions. 
  This overfitting results in EBM landscapes with high variance across different trials, as shown in the bottom-right figure of Fig. \ref{Fig1_Illustration_generalization_across_state}.
 In contrast, the CLIC-trained EBM maintains consistent landscapes, with minima close to the true optimal action and low variance across different trials.
 This explains the superior performance of  CLIC over IBC and PVP, as observed in Section \ref{sec:exp:accurate_feedback}.
 


\textbf{CLIC-Circular reduces to IBC under stricter assumptions}
To illustrate how CLIC-Circular reduces to IBC, we progressively decrease the radius of the circular desired action spaces by increasing the value of $\varepsilon$ (see the middle part of Fig. \ref{Fig1_Illustration_generalization_across_state}). 
As $\varepsilon$ increases, the CLIC-trained EBM starts to split into several clusters and overfit data labels with smaller MSE to human actions, as reported in the right part of Fig. \ref{Fig1_Illustration_generalization_across_state}.
 This overfitting also leads to a larger MSE to the optimal action, indicating that the EBM becomes less effective at identifying the optimal action.
Eventually, as $\varepsilon \rightarrow 1$,  the EBM landscape closely resembles the one trained using IBC.
When the radius becomes nearly zero, imitating a circular desired action space reduces to imitating a human action label, leading to overfitting and performance drops. 
This observation highlights the key distinction between CLIC-Circular and IBC: imitating a circular desired action space rather than a single action. This distinction is crucial for training EBMs stably.
% We demonstrate the transition from CLIC-Circular to IBC by progressively adjusting its hyperparameters and analyzing their impact on the energy-based model.
% (1) For CLIC-Circular with $\varepsilon = 0.5$, by changing the policy-weighted Bayes loss to uniform Bayes loss, the learned EBM tends to imitate the whole desired action space and leads to the EBM landscape quite flat and over-optimistic.





\begin{figure*}[t]
    \centering
    \includegraphics[width = 1.0\textwidth]{figs/Fig7_InsertT_exp_results_traj_3.pdf}
    % \includesvg[width=1.0\textwidth, inkscapelatex=false]{figs/Fig7_InsertT_exp_results_traj_2.svg}
	\caption{Examples of CLIC-Half policy rollout for the Insert-T task after training. At each step, the transparent figure shows the initial state, and the orange arrow indicates the end-effector’s trajectory. The solid figure illustrates the resulting end state, which becomes the initial state for the next step.}
 \label{fig:Fig7_InsertT_exp_results_traj}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{figs/Fig7_InsertT_exp_results.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig7_InsertT_exp_results.svg}
	\caption{Experiment results for the Insert-T task, categorized by difficulty levels (easy, medium, and hard). Each column shows the performance metrics for a given difficulty level, along with examples of initial states for that level.
    ``CLIC-Half (offline)'' denotes results for CLIC-Half trained offline.}
 \label{fig:Fig7_InsertT_exp_results}
\end{figure}

\subsection{Real-robot Validations}
\label{sec:exp:real_rotbo}

% \textcolor{red}{explain why we use CLIC-simplified }

Here, we use three tasks to demonstrate the practical applicability of CLIC.
The experiments include a long-horizon multi-modal Insert-T task, a dynamic ball-catching task, and a water-pouring task that necessitates precise control of the robot's end effector position and orientation. 
For the Insert-T task, we employ CLIC-Half and compare its performance against IBC and Diffusion Policy.
For the ball-catching and water-pouring tasks, we use CLIC-Explicit because it performs well in uni-modal tasks, as demonstrated in Section \ref{sec:exp:simulation}, and is more time-efficient compared to CLIC with an EBM policy (Details in Appendix \ref{appendix:time_efficiency_comparision}.).
% \footnote{Details on the time efficiency comparison between CLIC-Explicit and CLIC with an EBM policy are provided in the Appendix \ref{appendix:time_efficiency_comparision}.}
The experiments were carried out using a 7-DoF KUKA iiwa manipulator. When required, an underactuated robotic hand (1-dimensional action space) was attached to its end effector. A 6D space mouse was employed to provide feedback on the pose of the robot's end effector. Furthermore, in the ball-catching task, a keyboard provided feedback on the gripper's actuation. 
The setup of each task is detailed in Appendix \ref{appendix:real_robot_experiments_task details}, and the time durations used are reported in Appendix \ref{appendix:time_duration}. 
% The learned policy is evaluated every 5 episodes for the water-pouring task, every 10 episodes for the ball-catching task, and every 20 episodes for the Insert-T task.
% Results in Fig. \ref{fig:real_exp_figs_combined_all} show that the success rate for all tasks
% exhibits an overall improving trend as the timestep increases, demonstrating the practical applicability of CLIC. 
The experiment results are reported as follows:



\subsubsection{Insert-T—a comparison between state-of-the-art methods}
The Insert-T task requires the robot to insert a T-shaped object into a U-shaped object by pushing to adjust their positions and orientations.
Compared to the Push-T task in simulation experiments, Insert-T is more complex due to two factors: (1) it involves two objects, introducing multi-modal decisions about which object to manipulate first; and (2) it has an increased task horizon. This makes Insert-T a valuable benchmark for evaluating CLIC's performance in long-horizon, multi-modal tasks compared to state-of-the-art methods. 
To better analyze the performance of each method, we categorize the task into three difficulty levels based on the initial state of the objects and the number of contact changes required to complete the task (according to the teacher’s policy).  Examples of these categories are shown in the upper part of Fig. \ref{fig:Fig7_InsertT_exp_results}.
Tasks requiring fewer than 1 contact change are classified as ``easy”, fewer than 5 as ``medium”, and 5 or more as ``hard”. 
During each evaluation, 10 different initial states are tested for each category. To ensure a fair comparison, all methods are evaluated using the same set of initial states.
In the experiment, human-provided demonstration feedback is used to train CLIC-Half within the IIL framework. The collected data is also used to train baseline methods (Diffusion and IBC) offline. For a fair comparison, CLIC is additionally trained offline on the same dataset as the baselines.

Results for different difficulty levels are shown in Fig. \ref{fig:Fig7_InsertT_exp_results}. For easy tasks, baseline methods perform similarly to CLIC but converge more slowly. For medium and hard tasks, CLIC achieves significantly higher success rates. This is particularly evident for hard tasks, where CLIC achieves 80$\%$ success compared to 30$\%$ for Diffusion and 10$\%$ for IBC. 
The results demonstrate CLIC's ability to handle complex multi-modal tasks, thanks to the powerful encoding capabilities of EBMs and CLIC's stable EBM training. 
Furthermore, as the task difficulty increases, CLIC outperforms Diffusion and IBC by a large margin. This suggests that, for training policies, using a desired action space is more robust and efficient in real-robot tasks than relying on a single action label.
 Examples of post-training policy rollouts for CLIC in hard tasks are shown in Fig. \ref{fig:Fig7_InsertT_exp_results_traj}.

Note that CLIC is an interactive learning method, whereas Diffusion and IBC are offline methods in this experiment. Figure \ref{fig:Fig7_InsertT_exp_results} also includes results for CLIC trained with offline data, showing a similar final success rate to the online version. This indicates that CLIC can be employed to learn from offline data as well.
While CLIC is primarily based on the IIL framework, the core ideas proposed here could also benefit offline methods. We believe exploring offline training of CLIC is a promising direction for future work.



\subsubsection{Ball catching—quick coordination and partial feedback}
 The ball-catching task is challenging because of its highly dynamic nature. 
This complexity makes it difficult to provide successful demonstrations of the task to the robot, thus ruling out demonstration-based IIL methods for solving it\footnote{This limitation could be overcome with a highly reactive and precise teleoperation device. However, this also makes the solution more expensive.}.
Instead, relative corrective feedback is more intuitive and easier for this task as humans can provide direction signals occasionally to improve the robot's policy \cite{2020_DCOACH_temporal}.  
Besides, for a successful grasp, the robot must coordinate precisely the ball's motion, the end-effector's motion, along with the gripper's actuation.
This requirement makes it challenging to provide feedback on the complete action space at any given moment, and makes partial feedback suitable for this task.
With partial feedback, relative corrections can be independently provided for either the end-effector's motion or the gripper's actuation.
Therefore, this task enables testing CLIC's ability to learn effective policies from relative corrective feedback that is also partial. 
% The robot is also expected to react after an unsuccessful attempt by trying to grasp the ball again. 
% As a result, this task is particularly interesting for evaluating CLIC's performance, as it allows for the analysis of its ability to quickly coordinate multiple variables in a problem.
% Additionally, the necessary coordination between end-effector motion and gripper actuation makes it challenging to provide feedback on the complete action space at any given moment. 
% Therefore, this enables testing CLIC's ability to learn effective policies from partial feedback in real-world problems, where feedback is independently provided for either the end-effector's motion or the gripper's actuation.

% \textcolor{red}{add results analysis.}
 \begin{figure}
    \centering
    \includegraphics[width = 0.46\textwidth]{figs/Fig8_CatchBall02.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig8_CatchBall02.svg}
	\caption{Experiment results for the ball-catching task.}
 \label{fig:Fig8_CatchBall}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = 0.45\textwidth]{figs/Fig9_waterpouring.pdf}
    % \includesvg[width=0.49\textwidth, inkscapelatex=false]{figs/Fig9_waterpouring.svg}
	\caption{Experiment results for the water-pouring task.}
 \label{fig:Fig9_waterpouring}
\end{figure}

Fig. \ref{fig:Fig8_CatchBall} shows the experiment results of the ball-catching task, reporting the success rate of catching the ball within one, two, and three attempts. 
By the end of training, the robot achieves a 1.0 success rate for catching the ball within two attempts, and its first-attempt success rate continues to improve to 0.4.
One post-training policy rollout of a successful first-attempt catch is shown in Fig. \ref{fig:Fig8_CatchBall}, where the ball is caught within 1.5 seconds, an impressive result given the actuation delay of the robot hand.
This experiment demonstrates that CLIC can leverage both the relative corrective feedback and partial feedback effectively to learn challenging high-frequency tasks. 




% \begin{figure}
%     \centering
%     \includegraphics[width = 0.49\textwidth]{figs/Fig7_InsertT_exp_results_traj.png}
% 	\caption{Example of one CLIC-implicit policy rollout for the InsertT task. At each step, The transparent figure represents the initial state, the green arrow visualizes the trajectory of the end-effector, and the solid figure shows the corresponding end state, which becomes the initial state for the next step.}
%  \label{fig:Fig7_InsertT_exp_results_traj}
% \end{figure}



\subsubsection{Water pouring—learning full pose control with CLIC}
% In this experiment, the CLIC-Absolute and CLIC-Relative methods are combined to teach the robot, allowing the human teacher to select (by pressing a button) which teaching mode (absolute or relative correction) to use. 
% This approach showcases the flexibility of CLIC, which can learn from relative or absolute corrections depending on which is more appropriate at each moment. 
The water-pouring task requires the robot to control the pose of a bottle to precisely pour liquid (represented with marbles) into a small bowl. 
CLIC-Explicit is utilized in this experiment. 
The human teacher can provide either absolute or relative corrective feedback and has the flexibility to switch between these modes by pressing a specific keyboard button. 
Initially, absolute feedback was preferred as the policy was learned from scratch, and it was easier to intervene in a 6D action space. As the policy improved, relative corrections made it easier to refine the policy in specific regions of the state space.
% Furthermore, since CLIC allows partial feedback, the corrective signal could be provided in three ways: (1) position only, (2) rotation only, and (3) both position and rotation. This is useful when one part of the robot's action is correct while the other still needs improvement.
% An example
% of the learned policy and the results are shown in Fig. \ref{fig:Fig9_waterpouring}.

The experimental data is shown in Fig. \ref{fig:Fig9_waterpouring}. From Episode $1$ to Episode $16$, the teacher's feedback is provided in an absolute correction format. From Episode $16$ onward, the teacher's feedback is given as relative corrections to make small adjustments to the robot's policy. The success rate exhibits an overall improving trend, consistently increasing from 0.6 in Episode 26 to 0.9 by Episode 41. An example of the policy rollouts after training is illustrated in Fig. \ref{fig:Fig9_waterpouring}.
This experiment demonstrates the effectiveness of CLIC for learning precise control over position and orientation.














% \begin{figure*}[t]
%  \captionsetup{skip=0.2pt} % Adjust skip parameter for this figure
% 	\centering
% 	\includegraphics[width = 0.99\textwidth]{figs/real_exp_figs_combined_all.jpg}
%  % \vspace{2pt}
% 	\caption{Experiment results of the three real-world tasks, with success rate evolution in time and timesteps.
%  In the ball-catching task, success rates are recorded over different numbers of attempts.
%  }	\label{fig:real_exp_figs_combined_all}
% \end{figure*}

 




