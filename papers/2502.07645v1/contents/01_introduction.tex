\section{Introduction}
\label{sec:introduction}

% introduce the feedback in action space and their advantages
% also introduces the field
% i.e. Policy learning from demonstration and 

% 1. policy learning for multi-modal task
% Learning from demonstration into.
% multi-modality issue and solved by deep generative models
% however, these expressive model is more prone to overfitting to data labels

% 2. explain the reason of why overfitting
% 2.1 figure
% 2.2 [?] relax the assumption to relative correction

% 3. problem of relative correction

% 4. our solution

%%%%%%%%%%%%% New story
% 1. Challenge of IBC or EBM: overfitting (two parts)
% 2. Solution by Diffusion, but still fails when data is noisy
% 3. Our solution using desired action space
% 4. Our solution also can be perfectly applied into relative correction and solve their problems
Behavior cloning (BC) enables robots to acquire complex skills simply by imitating human demonstrations \cite{2018_review_IL, 2020_Survey_LfD, 2023_Survey_LfD, 2024_IJRR_survery_feedback_types}. 
It casts the policy learning problem into a supervised learning framework, under the assumption that human demonstrations are optimal.
Implicit BC (IBC) extends BC and achieves better performance in multi-modal tasks, with the policy represented by an energy-based model (EBM).
However, IBC suffers from training instability in practice, leading to unreliable results  \cite{2023_diffusionpolicy, 2022_arxiv_IBC_gaps}. 
We identify that the training instability of IBC stems from an overfitting behavior. This behavior can occur when the policy directly imitates action labels through the \emph{behavior cloning loss}, especially if these labels deviate from the optimal action.
Such deviations often arise due to teacher noise or the existence of multiple feasible actions near the optimal one—factors that are common in real-world robot learning tasks.
As illustrated in Fig. \ref{fig:framework} (b2), an EBM trained via IBC can overfit each noisy action label and deviate from the optimal action, resulting in training instability.

Prior work attempts to address the challenges of implicit policy models by exploring alternative representations, such as diffusion models \cite{2023_diffusionpolicy, 2023_score_diffusion_policy}, or flow-based models \cite{2024_arxiv_flow_matching_policy}.  
While these models often prove more stable during training than EBMs, they do not fundamentally resolve the overfitting issue caused by the BC loss. In fact, although these deep generative models \cite{2024_survey_deep_generative_model_in_robotics, 2023_diffusionpolicy, 2022_implicit_BC} can capture multiple optimal actions in the data, they are also prone to overfitting subtle variations around one optimal action. Consequently, this overfitting issue continues to restrict BC methods to tasks where high-quality demonstration data is available.


In this work, we aim to address this challenge by taking a new perspective that is overlooked in the literature—the loss function and its underlying assumption about the data. 
To do so, we situate our method within an Interactive Imitation Learning (IIL) framework \cite{2022_IIL_survey}. 
In IIL, the human teacher provides demonstrations, also known as absolute corrections or interventions, when the robot actions are suboptimal. 
% We relax the standard BC assumption that human actions are always optimal. Instead, we adopt a weaker assumption: the optimal action at a given state can be inferred by aggregating multiple corrective feedback signals.
Rather than adhering to the standard BC assumption that human actions are always optimal, we adopt a weaker assumption: the optimal action at a given state can be inferred by aggregating multiple corrective feedback signals. 
Specifically, we assume that one corrective feedback defines a \textit{desired action space}—a set of actions that could be optimal.
 We then propose our CLIC method to estimate the potential optimal action, by combining and intersecting multiple desired action spaces defined by the dataset.
 Concretely, we develop a probabilistic formulation of the desired action space and introduce a Bayesian-inspired loss function that updates the policy to generate actions within the desired space.
We use an EBM to represent an implicit policy and also introduce a simplified variant to train an explicit policy.

Beyond demonstration or absolute correction data, our CLIC method can be applied to relative corrections \cite{2022_IIL_survey, 2019_Carlos_IJRR, 2024_IJRR_survery_feedback_types}. This feedback indicates the direction of the optimal action relative to the robot's current action but does not specify the distance.
From this signal, a suboptimal human action label can be derived and used to update the policy via the BC loss in prior methods \cite{2019_Rodrigo_D_COACH, 2019_Carlos_IJRR, 2019_Carlos_COACH}. However, imitating suboptimal action labels via the BC loss can lead to incorrect policy updates, and our method addresses this issue by leveraging the desired action space concept, enabling more robust policy learning.


% Humans can teach robots complex skills by directly providing feedback in the robot’s action space.
% Such feedback generally falls into two categories:  demonstration, also known as intervention or absolute correction \cite{2018_review_IL, 2020_Survey_LfD, 2023_Survey_LfD, 2024_IJRR_survery_feedback_types}, and relative correction \cite{2022_IIL_survey, 2019_Carlos_IJRR, 2024_IJRR_survery_feedback_types}, which indicates the direction of the optimal action relative to the robot's current action but does not specify the distance.
% % which is a directional signal indicating how the robot's action should be improved.
% Current methods use feedback in the action space to generate action labels, which are assumed to be optimal. 
% However, this assumption can be problematic for both absolute and relative corrections. 

% % introduce the drawback of the optimal action assumption(overfitting), give example for both 
% In the case of absolute corrections, overfitting can occur when the model imitates action labels directly with the behavior cloning loss, especially when these labels deviate from the optimal action. Such deviations often arise due to teacher noise or the existence of multiple acceptable actions near the optimal action, which is common in real-world robot learning tasks.
% % In practice, demonstration can be close to and surrounding the optimal solutions.
% The issue is further amplified when the policy is represented by deep generative models \cite{2024_survey_deep_generative_model_in_robotics, 2023_diffusionpolicy, 2022_implicit_BC} with powerful modeling capabilities.
% % These models can effectively capture the multi-modal nature of the dataset but are also prone to overfitting small variations within a single mode. 
% These models can capture the presence of multiple optimal actions in the data but are also prone to overfitting to subtle variations around one optimal action.
% Fig. \ref{fig:framework} (b2) shows an example of an energy-based model trained via Implicit BC.
% The learned energy-based model (EBM) overfits to each action label, deviating from the optimal action and resulting in training instability.




% % For relative corrections, the data involves the robot’s current action and a directional signal indicating how to improve. 
% For relative corrections, the human action label can be derived from this signal. Although the human action label is better than the robot’s current action, it is not necessarily optimal (Fig. \ref{fig:framework}A).
% To learn a policy from relative corrections, existing methods rely on the over-optimistic assumption that these action labels are optimal, allowing the policy to be refined by imitating them via behavior cloning loss \cite{2019_Rodrigo_D_COACH, 2019_Carlos_IJRR, 2019_Carlos_COACH}. 
% However, this assumption becomes a critical limitation when feedback is aggregated into a reply buffer.  
% As the robot's policy continuously improves, previous feedback may no longer be valid, causing incorrect policy updates \cite{2021_BDCOACH}. 
% As a result, the buffer size is limited to being small, ensuring it contains only recent corrections. This leads to policies that tend to overfit data collected from recently visited trajectories.

%% briefly introduce our method in a very-high-level

% In this work, we relax the assumption that the optimal action can be directly extracted from single feedback in the action space. Instead, we assume that one corrective feedback can define a \textit{desired action space}—a set of actions that could be the optimal action.
%  Building on this perspective, we propose our CLIC algorithm to estimate the potential optimal action, using multiple desired action spaces defined by the dataset.
%  Specifically, we develop a probabilistic formulation of the desired action space and use a Bayesian framework to update the policy to generate actions within the desired space.
% In particular, we use an energy-based model to parameterize an implicit policy with a deep neural network and also introduce a simplified variant to train an explicit policy.

% \textcolor{red}{explain the advantages inherited from IIL, covarite shift}

% briefly show the advantage of our method
We demonstrate the advantages of our method through a series of experiments. 
First, our method can train an energy-based model stably, which was believed to be challenging in the literature \cite{2023_diffusionpolicy, 2022_arxiv_IBC_gaps, 2019_EBM_Du_Yilun}.  
Second, our method is able to learn complex multi-modal behavior, thanks to the stable training of energy-based models.
Finally, our method can learn from various types of feedback in the action space. 
Besides typical accurate absolute corrections, it can effectively leverage relative corrections and corrections with noisy or partial information—capabilities that state-of-the-art methods do not offer.
This flexibility expands the range of applicable tasks our method can address.
% This flexibility not only expands the range of applicable tasks but also makes the method accessible to non-expert human teachers.
% We also provide insights through experiments on 

The paper is organized as follows:
Section~\ref{sec:related_work} introduces the related work.
Section~\ref{sec:Preliminaries} shows the preliminaries, including the formulations of IIL, IBC, and our method.
Sections \ref{sec:Desired_action_space} and \ref{sec:Policy_shaping} details our CLIC method. Section~\ref{sec:Desired_action_space} focuses on the construction of the desired action space, and Section~\ref{sec:Policy_shaping} focuses on learning a policy from this desired space.
Section~\ref{sec:experiments} presents the experimental results in both simulation and the real world.
Conclusions are provided in Section~\ref{sec:conclusion}. 

% This paper addresses three key questions for learning a policy from the desired action space:
% \begin{enumerate}
%     \item \textbf{Constructing the set}: How to derive the desired action space from feedback, in Section~\ref{sec:Desired_action_space}.
%     \item \textbf{Designing the loss}: How to design a loss function that encourages the policy to generate actions within the desired action space, in Section~\ref{sec:Policy_shaping}.
%     \item \textbf{Guaranteeing optimality}: How to ensure the policy generates optimal actions with sufficient feedback, in Section~\ref{sec:Desired_action_space}.
% \end{enumerate}