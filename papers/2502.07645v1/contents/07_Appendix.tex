  
\section{Appendix}

\subsection{Proof for the Convergence of Overall Desired Action Space}
\label{appendix:proof_convergence_DesiredA}
Proposition \ref{proposition:convergence}:
   For any state $\bm s$, assume a trained policy $\pi_{\bm \theta}$ always select actions from $\hat{\mathcal{A}}^{\mathcal{D}(\bm s)}_k$,
   % i.e.,  $\pi_{\bm \theta}({\bm a\in \hat{\mathcal{A}}_k^{\mathcal{D}(\bm s)}}| \bm s ) = 1$,
   and there are a finite number of optimal actions.
   % assume each suboptimal action $\bm a^r_{k+1} \sim \pi_{\bm \theta}(\cdot | \bm s)$ has a nonzero probability of receiving feedback. 
   Assume that there is a nonzero probability for the teacher to provide feedback to each suboptimal action $\bm a^r_{k+1} \sim \pi_{\bm \theta}(\cdot | \bm s)$. 
    Then, as $k \rightarrow \infty$, $\hat{\mathcal{A}}^{\mathcal{D}(\bm s)}_k$ converges to $\mathcal{A}^*(\bm s)$ or a subset of $\mathcal{A}^*(\bm s)$.
   % \footnote{ A complete proof is provided in Appendix \ref{appendix:proof_convergence_DesiredA}}.


\begin{proof}
    For any state $\bm s$, the combined desired action space is denoted as $\mathcal{A}^{\mathcal{D}(\bm s)}_k$.
    As we assume $\pi_{\bm \theta}(\bm a \in \mathcal{A}^{\mathcal{D}(\bm s)}_k| \bm{s}) = 1$,
    for any action $a^r_{k+1}$ sampled from the policy $ \pi_{\bm \theta}(\cdot | \bm s)$, it is also inside the combined desired action space, i.e., $a^r_{k+1} \in \mathcal{A}^{\mathcal{D}(\bm s)}_k$. If $a^r_{k+1} $ is suboptimal, the feedback $\bm a^h_{k+1}$ is provided accordingly, resulting in a new desired action space  $\hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) $.

    If  $\hat{ \mathcal{A}}^{\mathcal{D}_{k}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) \! \neq \! \varnothing$, then
    the new feedback $\bm a^h_{k+1}$ reduce the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k+1}$ as
    \begin{align}
        \mathcal{A}^{\mathcal{D}(\bm s)}_{k+1} = \hat{ \mathcal{A}}^{\mathcal{D}_{k}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) \subset \mathcal{A}^{\mathcal{D}(\bm s)}_{k}
    \end{align}

    If $\hat{ \mathcal{A}}^{\mathcal{D}_{k}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) = \varnothing$, the new feedback $\bm a^h_{k+1}$ is associate to an optimal action that is outside $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$, therefore increasing the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k+1}$ as 
    \begin{align}
        \mathcal{A}^{\mathcal{D}(\bm s)}_{k} \subset \mathcal{A}^{\mathcal{D}(\bm s)}_{k+1} = \hat{ \mathcal{A}}^{\mathcal{D}_{k}(\bm s)} \cup \hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) 
    \end{align}    
    However, the total number of times of doing expansion of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ is bounded by the number of optimal actions $N_*$, which is finite by assumption.

    Therefore, as the feedback number goes to infinite, the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ continues to decrease. 
    Since each desired action space includes at least one optimal action, by applying intersection, the combined desired action space still contains at least one the optimal action. By expansion, the number of included optimal actions increases. 
    In other words, this process of interactive corrections leads to a non-empty $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ that includes at least one optimal action.
    In the limit, all actions in $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ are optimal and will not receive any feedback. Hence,
    \begin{align*}
        \lim_{k\rightarrow \infty} \mathcal{A}^{\mathcal{D}(s)}_k  \subset \mathcal{A}^*(\bm s).
    \end{align*}
    % $\lim_{k\rightarrow \infty} \mathcal{A}^{\mathcal{D}(s)}_k  \subset \mathcal{A}^*(\bm s)$.
    
    % If  $\hat{ \mathcal{A}}^{\mathcal{D}_{k}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k+1}, \bm a^h_{k+1}) \! \neq \! \varnothing$, as $\bm a^h_{k+1} \in \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k})$.
    % The new feedback $\bm a^h_{k+1}$ therefore reduce the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k+1}$ as $\mathcal{A}^{\mathcal{D}(\bm s)}_{k+1} = \hat{ \mathcal{A}}^{\mathcal{D}_{k-1}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}) \subset \mathcal{A}^{\mathcal{D}(\bm s)}_{k}$.

    % If $\bm a^h_{k+1} \notin \mathcal{A}^{\mathcal{D}(\bm s)}_k$, then $\hat{ \mathcal{A}}^{\mathcal{D}_{k-1}(\bm s)} \cap \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}) = \varnothing$, the new feedback $\bm a^h_{k+1}$ is associate to an optimal action that is outside $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$, therefore increasing the volume of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k+1}$ as $\mathcal{A}^{\mathcal{D}(\bm s)}_{k} \subset \mathcal{A}^{\mathcal{D}(\bm s)}_{k+1} = \hat{ \mathcal{A}}^{\mathcal{D}_{k-1}(\bm s)} \cup \hat{\mathcal{A}}(\bm a^r_{k}, \bm a^h_{k}) $.
    % However, the total number of times of doing expansion of $\mathcal{A}^{\mathcal{D}(\bm s)}_{k}$ is bounded by the number of optimal actions $n$.

    
    % We then have $\lim_{k\rightarrow \infty} \mathcal{A}^{\mathcal{D}(s)}_k  \subset \mathcal{A}^*(\bm s)$.
\end{proof}

\subsection{Proof of the Simplified CLIC-Explicit Objective}
\label{apppendix:reduce_policy_improvemnt_inequality_Gaussian_assumption}

 We want to prove that, with the Gaussian distribution assumption, Eq. \eqref{eq:policy_improvement_distribution} can be satisfied by satisfying Eq. \eqref{eq:policy_improvement}.
\begin{proof}
As the probability of sampling actions within $\hat{\mathcal{A}}{(\bm a^r, \bm a^h)}$ using $\pi_{\bm \theta}$ can be defined as  
\begin{align*}
    \pi_{\bm \theta}(\bm a \in \hat{\mathcal{A}}{(\bm a^r, \bm a^h)}| \bm{s}) 
    &=  \!\!\int_{\bm a \in \mathcal{A}}  \!\!\!\!   \!\!\!\!\pi_{\bm \theta}(\bm a | \bm s)  \text{Pr} [\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}| \bm a , \bm s] d \bm a  \\
    &\overset{T\rightarrow0}{=} \int_{\bm a \in \mathcal{A}}  \pi_{\bm \theta}(\bm a | \bm s)   \cdot \mathbf{1}_{\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}} d \bm a,
\end{align*}
Then Eq.~\eqref{eq:policy_improvement_distribution} in equivalent to 
\begin{align}
    \int_{\bm a \in \mathcal{A}}  \pi_{\bm \theta}(\bm a | \bm s)  \left( 2 \cdot \mathbf{1}_{\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}} - 1\right) d \bm a\geq 0 
    \label{eq:proof_goal_simplifed_inequality}
\end{align}
% We can prove the following:
% % Despite this simplification, by constraining the policy with Gaussian distribution assumption to satisfy  Eq. \eqref{eq:policy_improvement}, it also satisfies Eq. \eqref{eq:policy_improvement_general_case}, which is 
% \begin{align}
% &\pi_{\bm \theta}( \bm a^{-}_i | \bm s) < \pi_{\bm \theta}( \bm a^{+}_i | \bm s) , i = 1, \dots, N_I,
% \\&\pi_{\bm \theta}(\bm a | \bm s) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma) \\
% \Rightarrow
%     &\int_{\bm a \in \mathcal{A}}  \pi_{\bm \theta}(\bm a | \bm s)  \left( 2 \cdot \mathbf{1}_{\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}} - 1\right) d \bm a\geq 0 .
% \end{align}

% First step-> mean of gaussian is inside the desired action space
% Second step-> assume a very small variance, then
    As $\pi_{\bm \theta}(\bm a | \bm s)  \sim\mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$, we have 
\begin{align}
\pi_{\bm \theta}(\bm a | \bm s)  \propto \exp \left( {-\frac{1}{2}( \bm a-\bm \mu_{\bm \theta}(\bm s))^{\mathsf{T}} {\bm \Sigma^{-1}}( \bm a-\bm \mu_{\bm \theta}(\bm s))} \right), 
\label{eq:gaussian}
\end{align}
Without loss of generality, we set $\bm \Sigma = \sigma \bm I$ proportional to the identity matrix. 
Then, from $\pi_{\bm \theta}( \bm a^{-}_i | \bm s) < \pi_{\bm \theta}( \bm a^{+}_i | \bm s)$, we have 
\begin{equation*} 
\| \bm a^{+}_i-\bm  \mu_{\bm \theta}(\bm s) \|^2 < \| \bm a^{-}_i-\bm \mu_{\bm \theta}(\bm s) \|^2,  
\end{equation*}
% which is
% \begin{equation*}
%    % \mathbb{D}(\bm \mu^*(\bm s), \bm a^{-}) \geq  \mathbb{D}(\bm \mu^*(\bm s), \bm a^{+}) 
%    (\bm a^{+} - \bm a^{-})^T \bm \mu^*(\bm s)  \geq \frac{1}{2} (\bm a^{+} - \bm a^{-})^T(\bm a^{+} + \bm a^{-})
% \end{equation*}
which means that $\bm \mu_{\bm \theta}(\bm s) \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)} $. 
For $\alpha = 180^\circ$, $\hat {\mathcal{A}} {(\bm a^r, \bm a^h)}$ is a half-space and $2 \cdot \mathbf{1}_{\bm a \in \hat {\mathcal{A}} {(\bm a^r, \bm a^h)}} - 1 > 0$ always holds, thus Eq.~\eqref{eq:proof_goal_simplifed_inequality} always holds.
For $\alpha \in (0^\circ, 180^\circ)$, extra assumption needs to be made regarding the variance $\sigma$ to make Eq.~\eqref{eq:proof_goal_simplifed_inequality} holds.
For the extreme case, we can choose a $\sigma \rightarrow 0$ such that the policy outputs only the mean. 
In this case, Eq.~\eqref{eq:proof_goal_simplifed_inequality} holds.
% Now, we consider actions with the same distance to $\bm \mu^*(\bm s)$, which is $\bm a \in \mathcal{B}(p)= \{  \bm a \in \mathcal{A} | \|\bm a -\bm \mu^*(\bm s) \|^2 = p \}$.
% The number of actions on this ball that belong to the desired action space is larger than that that do not belong to the desired space. 
% Note that actions on the ball also have the same value of $\pi^*( \bm a | \bm s) $ by definition.
% Therefore, we have
% $\int_{\bm a \in \mathcal{B}(p) } \pi^*( \bm a | \bm s)\left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a\geq 0$
% Thus, 
% \begin{align*}
%     \int_{\bm a \in \mathcal{A}}  \pi^*( \bm a | \bm s) \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a \\
%     = \int_p \int_{\bm a \in \mathcal{B}(p) } \pi^*( \bm a | \bm s) \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a \\
%     \geq 0
% \end{align*}
\end{proof}

\subsection{Policy-weighted Bayes Loss and InfoNCE Loss}
\label{appendix:connection_INFONCE}
In this section, we detail the connections between Policy-weighted Bayes Loss and InfoNCE loss:
\begin{align*}
    &\mathrm{KL}\left(p(\bm a|\bm a^h \succeq \bm a^r, \bm s) \big\| \pi_{\theta}(\bm a | \bm s)\right)  \! \\ &= -\underset{\bm a \sim p(\bm a|\bm a^h \succeq \bm a^r, \bm s)}{\mathbb{E}} \!\! \! \!  \log \pi_{\theta}(\bm a | \bm s) + \! \! \! \! \! \! \! \underset{\bm a \sim p(\bm a|\bm a^h \succeq \bm a^r, \bm s)}{\mathbb{E}} \! \!\! \!  \! \! \! \!  \log p(\bm a|\bm a^h \! \succeq\!  \bm a^r, \! \bm s)  \\
    &= -\underset{\bm a \sim p(\bm a|\bm a^h \succeq \bm a^r, \bm s)}{\mathbb{E}} \!\! \! \!  \log \pi_{\theta}(\bm a | \bm s) + c\\
    &\simeq \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \log \pi_{\theta}(\bm a | \bm s) + c \\
    &\overset{Eq. \eqref{eq:estiamtion_policy_EBM}}{\simeq} \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\sum_{\bm a' \in \mathbb A} e^{-E_{\theta}(\bm s, \bm a')}} + c\\
    &\overset{Eq. \eqref{eq:ibc_info_NCE}}{=} \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \backslash  \{\bm a\}) +c,
\end{align*}
where $c$ denotes the constant that does not depend on $\bm \theta$.
By plugging the above equation into Eq.~\eqref{eq:KL_loss_general},
the KL loss becomes 
\begin{align*}
    \ell_{\!KL}\!(\bm \theta) \!
    % &= \sum_{\bm a } -p (\bm a|\bm a^{+},  \bm a^{-}) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\displaystyle\sum_{\bm a' \in \mathcal{A}^{-}} e^{-E_{\theta}(\bm s, \bm a'_j)} + \sum_{\bm a' \notin \mathcal{A}^{+}} e^{-E_{\theta}(\bm s, \bm a'_j)}} \\
    % \simeq 
   \simeq \!\!\!\!
    \!\!\!\! \underset{(\bm{a}^{h}\!, \bm{a}^{r}\!, \bm{s})  \sim p_{\mathcal D}}{\mathbb{E}}\sum_{\bm a \in \mathbb A} \!\! -p (\bm a|\bm a^h \!\succeq \! \bm a^r\!, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \! \backslash  \!\{\bm a\}) + \!c
\end{align*}

% \begin{align*}
%     \ell_{KL}(\bm \theta) &\Leftrightarrow
%     \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \log \pi_{\theta}(\bm a | \bm s) \\
%     &\overset{Eq. \eqref{eq:estiamtion_policy_EBM}}{\simeq} \sum_{\bm a } -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \log \frac{e^{-E_{\bm \theta}(\bm s, \bm a)}}{\sum_{\bm a' \in \mathbb A} e^{-E_{\theta}(\bm s, \bm a')}}\\
%     &\overset{Eq. \eqref{eq:ibc_info_NCE}}{=} \sum_{\bm a \in \mathbb A} -p (\bm a|\bm a^h \succeq \bm a^r, \bm s) \ell_{\text{InfoNCE}}(\bm s, \bm a, \mathbb A \backslash  \bm a)
% \end{align*}

\subsection{Details of the Implementation of Baselines}
\label{appendix:baselines}
\subsubsection{D-COACH}
In \mbox{D-COACH}, the teacher shapes policies $\pi(\bm a|\bm s) \!\sim \! \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$ by giving occasional relative corrective feedback $\bm h$. 
% Note that the robot action $\bm a$ is parameterized as $\bm \mu_{\bm \theta}(\bm s)$.%, specifically with the parameters $\bm \theta$ as they were at the moment the feedback was given.
% This signal $\bm h$ is used to create a corrected action $\bm a^{+}$ whose relative magnitude with respect to $\bm a$ is defined as the hyperparameter $e$, where  $e$ is a smaller value compared with the difference between $\bm a$ and the optimal action $\bm a^*$.
The human action $\bm a^{h} = \bm a^r  + e  \bm h$ is employed to update the policy parameters $\bm \theta$ in a behavior cloning manner:
\begin{equation}
    %\label{eq:dcoach_update}
    \ell^{\text{COACH}}_{\pi}(\bm s) = \min_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^{h} \|^2}.
    \label{eq:dcoach}
\end{equation}
However, when learning from past experiences by using a replay buffer, as $\bm a^{h} \neq \bm a^*$, old feedback can lead the policy in the wrong direction. 
% More specifically, if $\bm \mu_{\bm \theta}(\bm s)$ is already closer than $\bm a^{h}$ to $\bm a^*$, i.e., $ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^{*}  \|^2 <  \| \bm a^{h}- \bm a^{*} \|^2$, employing Eq. \ref{eq:dcoach_update} become harmful.
Consequently, to avoid this, \mbox{D-COACH} keeps a small data buffer $\mathcal{D}$, using only recent feedback for policy updates.
This approach leads to overfitting to recent trajectories and reduced learning efficiency.
% \subsubsection{BD-COACH}

To address this issue, batched D-COACH \cite{2021_BDCOACH}, \mbox{BD-COACH} for short, was proposed, which is used as the baseline in Section~\ref{sec:exp:simulation_relative_partial}. Here, besides learning a policy,
\mbox{BD-COACH} learns a human model $H_{\bm \phi}(\bm a, \bm s) = \bm h$. This model estimates the human's relative corrective feedback $\bm h$ given the robot's action $\bm a^r$ and state $\bm s$.
%All the history data $[\bm s, \bm a, \bm h]$ is saved to the data buffer $\mathcal{D}$.
Then, the human model is trained by minimizing the loss 
$
    \ell_H = \min_{\bm \phi}{ \| H_{\bm \phi}(\bm a^r, \bm s) - \bm h \|^2}
$,
 and the policy model is trained by minimizing the loss 
\begin{align*}
\ell_{\pi}^{\text{BD-COACH}}(\bm s) = \min_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \hat {\bm a}^{h} \|^2},
\end{align*}
where $\hat{\bm a}^{h} = \bm a^r+ e \cdot  H_{\phi}(\bm a^r , \bm s)$. 
Therefore, since $\hat{\bm a}^{h}$ is estimated in relation to the current robot's policy, the correction data is no longer outdated. 
% However, this approach depends on an accurate estimation of $\bm h$, which is learned simultaneously to $\bm \mu_{\bm \theta}$. 
% Consequently, the learning process becomes slower because the policy can only improve after $H_{\bm \phi}$ generates coherent results. Additionally, precise hyperparameter tuning is essential for a stable learning process, given that two models are being optimized simultaneously.
\subsubsection{HG-DAgger}
\label{appendix:HGDAgger}
HG-DAgger is an intervention-based IIL algorithm that aggregates human demonstrations into a dataset and updates its behavior at the end of each episode by minimizing the distance between its current policy and the actions stored in the dataset.
In contrast, methods like CLIC and D-COACH update the policy continuously during training episodes, as shown in Algorithm \ref{alg:online_IIL} (line \ref{alg:line:update_policy_once}).
To make a fair comparison and make the IIL framework consistent across different methods, we introduce a slight modification to HG-DAgger,  allowing it to update its policy during each training episode, similar to CLIC. 
This modification enables HG-DAgger to converge faster than its original version. 

HG-DAgger assumes an explicit Gaussian policy and updates it using behavior cloning loss (see Eq.~\eqref{eq:dcoach}). For other offline BC baselines, such as Diffusion Policy and IBC, we adopt the HG-DAgger framework while replacing only the policy update step with their respective methods.
\subsection{The Setup of Experiments}
% \label{appendix:real_robot_experiments_task details}



In all the experiments, we used state-based observation as input for the policy across all methods. 
The summary of the tasks in both the simulation and the real world is reported in Table \ref{tab:appendix:setup_exp}.
The term `multi-modal' denotes whether the task has multiple solutions inside the dataset. 



\begin{table}[]
\caption{Tasks summary }
\label{tab:appendix:setup_exp}
\begin{center}
\begin{tabular}{cccccc}
\Xhline{0.75pt}
\multicolumn{1}{c}{\multirow{2}{*}{{Tasks}}} & \multicolumn{1}{c}{\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} State\\ dim \end{tabular}}}} & \multicolumn{1}{c}{\multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}} Action\\ dim\end{tabular}}}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Multi-\\ modal\end{tabular}}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Contact\\ Rich\end{tabular}}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} High\\ freq\end{tabular}}}  \\
\multicolumn{1}{c}{}                       & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}                           & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{} \\
\hline
Push-T         &   24         & 2         &        \ding{51}  & \ding{51}  &  \ding{55}  \\
Square        &     48      & 7         &         \ding{51}    & \ding{55} & \ding{55} \\
Pick-Can       &     40      & 7         &          \ding{55}  & \ding{55} & \ding{55} \\
TwoArm-Lift    &     48     & 14        &         \ding{55}    & \ding{55}  & \ding{55}\\
Ball-catching &      12     & 3         &         \ding{55}   & \ding{55} &  \ding{51} \\
Water-pouring &      7     & 6         &          \ding{55}  & \ding{55} & \ding{55} \\
Insert-T       &    52       & 2         & \ding{51} &  \ding{51}& \ding{55}\\
\Xhline{0.75pt}           
\end{tabular}
\end{center}
\end{table}

\subsubsection{Simulated tasks}
\label{appendix:simulated_experiments_task details}
\label{}
The task descriptions are as follows:
\begin{enumerate}[label=\roman*), leftmargin=0.6cm]
  \item \textbf{Push-T:} This task, introduced by \cite{2023_diffusionpolicy}, involves the robot pushing a T-shape object to a fixed target using its circular end effector. 
 \item \textbf{Square:} The robot must place a square-shaped nut onto a fixed square peg. 
   \item \textbf{Pick-Can:} The objective is to pick up a can object and transport it to a fixed target bin. 
    \item \textbf{TwoArm-Lift:} This task involves two robots working together to lift a shared object. 
\end{enumerate}
For each task, the object's position is randomly initialized at the beginning of each episode.
\subsubsection{Real-world tasks}
\label{appendix:real_robot_experiments_task details}
The learned policy is evaluated every 5 episodes for the water-pouring task, every 10 episodes for the ball-catching task, and every 20 episodes for the Insert-T task. The details of each real-world task are detailed as follows:
\begin{enumerate}[label=\roman*), leftmargin=0.6cm]
  \item \textbf{Ball catching:} This task involves the robot catching a ball that's swinging on a string attached to a fixed point. 
  % The challenge lies in catching the ball while it is still moving. 
  The robot's end effector operates within the same plane where the ball swings, maintaining a fixed orientation. The {action space} consists of the robot's end effector linear velocity in the specified plane and a one-dimensional continuous actuation command for controlling the robot's gripper, where 0 represents fully closed and 1 fully open. The {state space} includes the end-effector's relative position and velocity with respect to the ball, the angle between the gravity vector and the ball's string along with its corresponding angular velocity, and the poses of both the ball and the fixed point.
  The poses of the ball and the fixed point are measured using an OptiTrack system.
  % The CLIC-Relative is utilized in this experiment.

  \item \textbf{Water pouring:} The robot controls the pose of a bottle to precisely pour liquid (represented with marbles) into a small bowl. The {action space} is 6D, consisting of the robot's end-effector linear and angular velocities. The {state space} is defined by the robot's end-effector pose, which consists of its Cartesian position and its orientation, represented with a unit quaternion. The initial pose of the robot is randomized at the start of each episode within certain position and orientation limits to ensure safety.
  % Both the CLIC-Relative and CLIC-Absolute are utilized in this experiment.

  \item \textbf{Insert-T:} The robot must insert a T-shape object into a U-shape object by pushing them. 
  % It doesn't matter which object it manipulates first. 
  The {action space} is defined as the linear velocity of the end-effector in a 2D plane over the table.
  The robot's end-effector orientation and z-axis position (the one aligned with the table's normal vector) are fixed throughout the task.
  The positions of the objects are measured by an OptiTrack system.
  The state space consists of the positions of 10 key points on a T-shape object, the positions of 10 key points on a U-shape object, and the position and velocity of the end-effector.
\end{enumerate}

\subsubsection{Time duration}
\label{appendix:time_duration}
The total time duration of the real-world experiments within the IIL framework is reported in Table \ref{tab:appendix:Time_duration_real_world}, excluding the time spent resetting the robot or performing evaluations.


\begin{table}[h]
\caption{Total time duration of real-world experiments}
\label{tab:appendix:Time_duration_real_world}
\begin{center}
\begin{tabular}{cccc}
\Xhline{0.75pt}
                    & Ball-catching & Water-pouring & Insert-T \\ \hline
Time duration (minutes) & 74        & 40            & 140            \\
\Xhline{0.75pt}
\end{tabular}
\end{center}
\end{table}

% In Fig. \ref{fig:real_exp_figs_combined_all}, the {total time} is used as the x-axis, which refers to the entire experiment duration, excluding resetting time. 

\subsection{Time Efficiency Comparison}
\begin{table}[h]
\caption{Time efficiency comparison per step}
\label{tab:appendix:Time_efficience_comparison}
\begin{center}
\begin{tabular}{cccc}
\Xhline{0.75pt}
                    & CLIC-Half & CLIC-Circular & CLIC-Explicit \\ \hline
Inference time (ms) & 28.61        & 28.52            & 1.13             \\
Training time (ms)  &    201.32        &    176.64          &      12.66           \\ \Xhline{0.75pt}
\end{tabular}
\end{center}
\end{table}
\label{appendix:time_efficiency_comparision}
For all CLIC methods with a batch size of 32, the inference and training times per step on the Square task were recorded and averaged.
As reported in Table \ref{tab:appendix:Time_efficience_comparison}, although implicit models have better encoding capability, they require more time for both training and inference compared to the explicit model used in CLIC-Explicit.
This presents a trade-off when selecting an algorithm for practical use.  For uni-modal tasks, CLIC-Explicit can be used for its time efficiency. For multi-modal tasks, CLIC-Half and CLIC-Circular should be used instead. 
Additionally, Table \ref{tab:appendix:Time_efficience_comparison} shows that CLIC-Half has a slightly longer training time per step than CLIC-Circular, as it involves an additional step of sampling implicit negative actions.





\subsection{Implementation Details}
% \textcolor{red}{Update this information}
\subsubsection{Network structure}
For implicit policies, CLIC, IBC, and PVP use the same neural network structure for the EBM. 
The neural network consists of five fully connected layers with [512, 512, 512, 256, 1] units, respectively.
The ReLU activation function is applied between all layers except for the last layer, which has no activation function.
The input is the concatenation of the state and action vectors, and the output is a scalar. 
For Diffusion Policy, the neural network follows a similar structure, except that the last layer has a number of units equal to the dimensionality of the action space. 


For the explicit policy $\pi_{\bm \theta}(\bm s,\bm a) \sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$, the mean $\bm \mu_{\bm \theta}(\bm s)$ is parameterized by a neural network consisting of five fully connected layers. The layer units are the same as the Diffusion Policy's model; except that the final layer applies a sigmoid activation, followed by scaling and shifting to produce values between -1 and 1. This neural network takes a state vector as input and maps it to an action.
This action is obtained via $\bm a_t = \bm \mu_{\bm \theta}(\bm s_t)$.
The covariance matrix $\bm \Sigma$ can be chosen to control the variability of actions sampled from the Gaussian distribution. In our implementation, it is ignored entirely, in which case actions are always taken as $\bm \mu_{\bm \theta}(\bm s_t)$.
% The values of other hyperparameters for different methods are shown in Table \ref{table:appendx_Hyperparameters}.



\subsubsection{Gradient Penalty}
As described in Appendix B.3.1 of IBC \cite{2022_implicit_BC}, we incorporate a gradient penalty loss to improve the training stability of CLIC. This penalty is computed using only the action samples from the final MCMC step. In practice, we found that the gradient penalty used in IBC is more effective than the L2 norm penalty.

\subsubsection{Hyperparameters for Training}
At each training episode, the network parameters will be updated with an update frequency of $b=5$ for all methods, as shown in line \ref{alg:line:update_feq_b} of Algorithm \ref{alg:online_IIL}.
The batch size is set to 32 for accurate, relative, and partial feedback, and 10 for noisy feedback. 
The learning rate is 0.003. 
The optimizer for the neural network is Adam with $\beta_1 = 0.1, \beta_2 = 0.999$ and $\epsilon=1e-7$.
The number of training steps at the end of each episode, $N_{training}$, is set to 500 for all methods except diffusion policy, for which it is set to 1000.

For CLIC-Circular, $\varepsilon$ is set to $0.5$, and the temperature $T$ is set to 0.05 for all tasks.
For CLIC-Half, the hyperparameters $\alpha$ and $\varepsilon$ are set to $\alpha=30^\circ$ and $\varepsilon = 0.3$ for accurate and relative feedback; and set to $100^\circ$ and $\varepsilon = 0.1$  for partial and noisy feedback. 
For the Insert-T task, $\alpha=30^\circ$ and $\varepsilon = 0.1$.
The number of implicit negative actions, $N_I$, is 128. 
The magnitude hyperparameter is $e = 0.2$.
The number of sampled actions from EBM is $N_{\text{a}} = 512$.
$N_{\text{MCMC}} = 25$ during training and $N_{\text{MCMC}} = 50$ during inference.  

For IBC, $N_{\text{neg}}=512$, $N_{\text{MCMC}} = 25$ during training and $N_{\text{MCMC}} = 50$ during inference.  

% \begin{table}[]
% \caption{Hyperparameters for the algorithms}
% \label{table:appendx_Hyperparameters}
% \begin{center}
% \begin{tabular}{l|l|c}
% \hline
% Algorithm                           & \multicolumn{1}{c|}{Hyperparameters}        & Value  \\ \hline
% \multirow{3}{*}{CLIC-Correction}    & Magnitude parameter $e$  & 0.02   \\& Learning rate & 0.001  \\
%                                     & End-of-episode training iterations & 1000   \\ \hline
% \multirow{2}{*}{CLIC-Demonstration} & Learning rate                               & 0.0003 \\
%                                     & End-of-episode training iterations & 1000   \\ \hline
% \multirow{3}{*}{BD-COACH}           & Magnitude parameter $e$                       & 0.02   \\
%                                     & Learning rate                               & 0.0003 \\
%                                     & End-of-episode training iterations & 1000   \\ \hline
% \multirow{2}{*}{Modified HG-DAgger} & Learning rate                               & 0.0003 \\
%                                     & End-of-episode training iterations & 1000   \\ \hline
% \multirow{2}{*}{Original HG-DAgger} & Learning rate                               & 0.0003 \\
%                                     & End-of-episode training iterations & 2000   \\ \hline
% \end{tabular}
% \end{center}
% \end{table}


