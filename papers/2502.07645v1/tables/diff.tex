\documentclass[conference]{IEEEtran}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL rss24_old_flatten_withoutexp.tex   Fri Apr 19 19:57:21 2024
%DIF ADD rss24_new_flatten.tex              Fri Apr 19 19:41:37 2024
\usepackage{times}

%DIF 4a4-5
\let\labelindent\relax %DIF > 
\usepackage{enumitem} %DIF > 
%DIF -------
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]
{hyperref}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}

\usepackage{gensymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
%DIF 19a21
\usepackage{todonotes} %DIF > 
%DIF -------


%DIF 21-27d24
%DIF < \pdfinfo{
%DIF <    /Author (Homer Simpson)
%DIF <    /Title  (Robots: Our new overlords)
%DIF <    /CreationDate (D:20101201120000)
%DIF <    /Subject (Robots)
%DIF <    /Keywords (Robots;Overlords)
%DIF < }
%DIF -------

%DIF 29a25
 %DIF > 
%DIF -------
\newtheorem{proposition}{Proposition}
\usepackage{multirow}

\usepackage{caption}
\captionsetup{labelfont=bf, textfont=normal}

\usepackage{svg}  

\usepackage{amssymb}
%DIF < \bibliographystyle{IEEEtran}
%DIF -------
 %DIF > 
 %DIF > 
\newcommand{\issue}[1]{\vspace{0.1em}\noindent \textbf{#1 \hspace{0.2em}}}  %DIF > 
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}


\title{Contrastive Policy Shaping via Human Corrective Feedback }

\author{Author Names Omitted for Anonymous Review. Paper-ID 119}










\maketitle

\begin{abstract}
Interactive Imitation Learning (IIL) is a promising approach for humans to teach behaviors to robots data-efficiently. Specifically, relative corrective feedback has shown to be a powerful teaching modality for robots to learn complex behaviors when, at any given time step, only partial information about a task's solution can be provided by humans.
However, existing methods make assumptions that hinder the robots' learning capabilities. Namely, they 1) cannot incorporate uncertainty in the feedback signal, and 2) they are limited to small replay buffer sizes, overfitting to data collected in recently visited trajectories. As a result, the full potential of relative corrective feedback remains unexplored.
To address these issues, we propose a novel contrastive loss function that enables learning from this feedback while removing the assumptions made by previous approaches.
We present the Contrastive Learning via Interactive Corrections (CLIC) framework, which defines a \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend solution set in the action space in every state where feedback is provided. Then, it enforces the policy to converge towards this set through its loss function.
Moreover, we propose a special case of CLIC where it can be used to learn interactively from human demonstrations, also known as absolute corrections.
Our method is compared with state-of-the-art methods across multiple simulated environments. Furthermore, we validate it using a 7-DoF robot in two real-world tasks.
The results show that CLIC can effectively learn policies in the real world and outperforms previous methods based on relative corrections. 
For tasks where demonstrations are feasible, CLIC obtains results comparable to state-of-the-art demonstration-based IIL methods.
Furthermore, CLIC is also more robust to noisy teacher feedback, compared with both demonstration-based and previous correction-based IIL methods. 
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}






Developing general-purpose robots for assisting with everyday tasks in various settings like houses, offices, factories, and greenhouses is a major ambition for roboticists.
In pursuit of this, significant progress has been made in areas such as gaming \cite{2015_atari_deepmind_RL}, locomotion \cite{2020_DRL_locomotion_scirobotics}, and manipulation tasks \cite{2023_SciRobotics_Visual_dexterity},
with data-driven approaches such as Reinforcement Learning (RL) and Imitation Learning (IL) playing a crucial role.


In this context, IL has been increasingly gaining popularity due to its multiple practical advantages over RL. Notably, IL does not require a reward function. This eliminates the challenges associated with designing the function and either adding extra sensing for its computation in real-world learning or bridging the reality gap in simulator-based learning. Furthermore, IL is more data-efficient than RL, as it does not necessitate exhaustive exploration of the action space to discover successful strategies.  Nevertheless, challenges remain before IL can become a practical methodology for solving multiple real-world problems. One key challenge is that, although IL is more data-efficient than RL, it often still requires large datasets to achieve reliable behaviors \cite{2023_RT2, 2023_Mimicgen}.
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.23]{figs/Fig1_framework_new_new.jpg}
\caption{A visualization of the CLIC method.
 The task is to harvest the tomato.
 In Fig. (a), the robot at state $\bm s_1$ takes action $\bm a^{-}_1$ and moves away from the goal, as shown in Fig. (c). 
 The optimal action $\bm a^*_1$ is shown in Fig. (b).
 The teacher observes the effects of $\bm a^{-}_1$, and gives relative corrective feedback to change it to $\bm a^{+}_1$, leading the robot to $\bm s_2$ in Fig. (d).
 The robot then takes action $\bm a^{-}_2$, which is changed to $\bm a^{+}_2$ by the teacher if necessary.
 Iteratively, the robot's policy can be improved to the optimal policy.
The core assumption of CLIC is that the positive action $\bm a^{+}$ is closer to $\bm a^*$ than the negative action $\bm a^{-}$, where we remove the assumption that $\bm a^* = \bm a^+$, used by previous correction-based and demonstration-based methods.}
	\label{fig:framework}
\end{figure}

Previous work has highlighted that one critical issue that harms IL's data efficiency is \emph{covariate shift} \cite{2011_DAgger}.
The covariate shift problem arises when the agent, acting on its learned policy during evaluation, drifts away from the distribution of states present in the demonstration dataset due to compounding errors. 
This results in encountering unseen states and, consequently, taking suboptimal or incorrect actions.
Remarkably, a more recent branch of IL, namely, Interactive Imitation Learning (IIL), mitigates this challenge by incorporating humans into the learning loop. 
Here, a human teacher provides real-time feedback to the robot, incrementally improving its policy \cite{2022_IIL_survey}.
In this case, the robot learns directly from the distribution of states it generates itself, thereby eliminating potential mismatches between training and evaluation state distributions. As a result, the covariate shift problem is addressed \cite{2011_DAgger, 2022-BCz-DataCollectionVia_HG-DAgger, 2022_IIL_survey}.





In IIL, a variety of methods exist for data collection. Among these, one promising approach is based on \emph{corrective feedback}. This involves sending directional signals, which may or may not include magnitude information, to guide the robot from a suboptimal action (pre-correction action) to a more optimal one (post-correction action). When this signal includes magnitude information, it is termed as \emph{absolute correction}, becoming equivalent to a demonstration \cite{2022_IIL_survey}. Often referred to as \emph{human interventions} \cite{2020_RSS_expert_interventio_learning}, this feedback modality involves the human taking control of the agent when it makes incorrect decisions. These interventions are then recorded and utilized for learning \cite{2020_RSS_expert_interventio_learning, 2019_HG_DAgger}. However, intervention/demonstration-based IIL methods require precise and high-quality demonstrations, posing challenges in tasks that are difficult to demonstrate \cite{2019_IRL_ranked_demonstration}. For instance, a robot's dynamics might change more rapidly than a human's reaction time, rendering direct control unfeasible, such as maintaining an inverted pendulum in an upright position. Other challenging scenarios include robots with more degrees of freedom than a human can effectively control, like bimanual manipulators.


In such situations, it becomes more appealing to employ \emph{relative corrections} \cite{2019_Carlos_IJRR}. Relative corrections do not contain magnitude information; thus, they only indicate the direction in which an action should be modified, but not the extent of the modification. For example, in a driving scenario, this signal could instruct the robot to increase speed, yet it does not specify the precise speed to be reached. By iteratively using this feedback, the robot's actions can get arbitrarily close to the optimal action. Notably, this feedback modality does not require the teacher to know precisely which action should be taken. It only necessitates an understanding of the general behavior the robot should exhibit. Furthermore, it does not require taking control of the robot; occasional signals are sufficient for learning successful policies. These two aspects combined make relative corrections easier to employ than absolute corrections. As a result, they require less effort from the teacher and reduce the number of errors they make, enabling the robot to learn a broader range of problems successfully \cite{2020_DCOACH_temporal}.







Nonetheless, work on relative corrective feedback has typically treated the post-correction action as optimal \cite{2018_D_COACH, 2019_Carlos_IJRR}, in that these methods aim to minimize the distance between the policy's action at a given state and the post-correction action. This assumption becomes a critical limitation when feedback is aggregated into a dataset/buffer, from which samples are continuously drawn for updating the policy. The issue is that post-correction actions are not necessarily optimal; they are improvements over the robot's actions at a specific moment in time. Hence, as the robot's policy continuously improves, older corrections may no longer be valid, potentially leading to incorrect policy updates \cite{2021_BDCOACH}. As a result, the buffer size is limited to being small, ensuring it contains only recent corrections. This leads to policies that tend to overfit to data collected from recently visited trajectories.

Moreover, demonstration-based and previous correction-based IIL methods do not account for uncertainty in the teacher's feedback. This uncertainty can arise from various sources, such as noisy human feedback or limitations of the feedback interfaces. 
For noisy human feedback, these methods tend to result in suboptimal behavior \cite{2020_better_than_demonstrator_imitation}. 
By \emph{limited feedback interfaces}, we refer to situations where the interface used only provides partial information to the robot. 
For example, when a robot moving on a plane receives direction via a keyboard's arrow keys, pressing the \emph{right arrow} suggests an increase in rightward movement but offers no guidance on the \emph{up/down} axis, leaving a broad range of valid responses. This significant issue of partial feedback has yet to be addressed in existing methods.

In this work, we tackle both aforementioned issues simultaneously by introducing a novel policy improvement loss. 
This loss, at any given state, enforces the policy to converge towards a \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend set of actions. 
This set is constructed using relative corrective feedback, employing a contrastive approach that generates pairs of positive and negative action pairs. 
We have named our method \textbf{C}ontrastive \textbf{L}earning via \textbf{I}nteractive \textbf{C}orrections (CLIC). 
Importantly, this method can be seamlessly employed with either relative or absolute corrections, rendering it a versatile approach suitable for solving a wide range of problems.


We validated CLIC through simulations using \emph{Meta-World} \cite{2020_metaworld_simulation} and real-world tests with a 7-DoF KUKA iiwa robot. 
Our results show that CLIC can be effectively used in the real world and that it surpasses existing methods that rely on relative corrections, both in terms of success rate and convergence time. 
In scenarios where demonstrations are feasible, CLIC obtains results on par with state-of-the-art demonstration-based IIL methods.
Furthermore, CLIC is also more robust to noisy teacher feedback, compared with both demonstration-based and previous correction-based IIL methods. 




The paper is organized as follows:
Section \ref{sec:Preliminaries} shows the preliminaries of the CLIC method, where the formulations of IIL and previous correction-based methods are given.
Section \ref{sec:Methodology} details our CLIC method.
Section \ref{sec:experiments} presents the experimental results in both simulation and the real world.
Section \ref{sec:related_work} introduces the related work of our method.
Discussion and conclusions are given in Sections \ref{sec:discussion}
and \ref{sec:conclusion}, respectively. 






\section{Preliminaries}
\label{sec:Preliminaries}
\subsection{Problem formulation}

In a typical IIL problem, a Markov Decision Process (MDP) is used to model the decision-making of an agent taught by a human teacher. 
An MDP is a 5-tuple \((\mathcal  S, \mathcal A, T, R, \gamma)\), where \(\mathcal  S\) represents the set of all possible states in the environment, \(\mathcal  A\) denotes the set of actions the agent can take, \(T(\bm s' |\bm s, \bm a)\) is the transition probability, \(R(\bm s, \bm s', \bm a)\) is the reward function, giving the reward received for transitioning from state \(\bm s\) to state \(\bm s'\) via action \(\bm a\), and \(\gamma \in [0, 1]\) is the discount factor.
The reward typically reflects the desirability of the state transition.
A policy in an MDP defines the agent's \DIFdelbegin \DIFdel{way of behaving }\DIFdelend \DIFaddbegin \DIFadd{behavior }\DIFaddend at a given state, denoted by \(\pi\).
\DIFdelbegin \DIFdel{It is a probabilistic function that maps states to actions, }\DIFdelend \DIFaddbegin \DIFadd{In general, $\pi$ can be represented by the conditional probability $\pi(\bm a|\bm s)$ of the density function }\DIFaddend \(\pi: S \times \mathcal A \rightarrow [0, 1]\)\DIFdelbegin \DIFdel{, where \(\sum_{\bm a \in \mathcal A} \pi(\bm s, \bm a) = 1\) for all \(\bm s \in \mathcal  S\)}\DIFdelend \DIFaddbegin \DIFadd{. Consequently, given a state, $\pi$ is employed to select an action}\DIFaddend . The objective in an MDP is to find the optimal policy \(\pi^*\) that maximizes the expected sum of discounted future rewards $J(\pi) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^{t} R(\bm s_t, \bm a_t) \right]$. 
\DIFdelbegin \DIFdel{We consider the policy parameterized by a Gaussian distribution }\DIFdelend \DIFaddbegin \DIFadd{In this work, we model $\pi$ using a Deep Neural Network (DNN). To achieve this, we consider a Gaussian policy }\DIFaddend with fixed covariance \DIFdelbegin \DIFdel{, denoted as $\pi_{\bm \theta}(\bm s,\bm a) \sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$ for simplicity,
}\begin{align*}
    \DIFdel{\pi_{\bm \theta}(\bm s,\bm a) \propto \exp \left( }{\DIFdel{-\frac{1}{2}( \bm a-\bm \mu_{\bm \theta}(\bm s))^{\mathsf{T}} }{\DIFdel{\bm \Sigma^{-1}}}\DIFdel{( \bm a-\bm \mu_{\theta}(\bm s))}} \DIFdel{\right), 
    %DIFDELCMD < \label{eq:gaussian}%%%
}\end{align*}%DIFAUXCMD
\DIFdel{where $\bm \mu_{\bm \theta}(\bm s) \in \mathcal{A}$ is the mean that we parameterized using a neural network, and the covariance }\DIFdelend $\bm \Sigma$ \DIFdelbegin \DIFdel{is predefined}\DIFdelend \DIFaddbegin \DIFadd{and with mean $\bm \mu_{\theta}(\bm s)$, represented by the DNN. Hence, we obtain the parameterized policy $\pi_{\bm \theta}(\bm a | \bm s) \sim \mathcal{N}\left (\bm \mu_{\theta}(\bm s), \bm \Sigma\right)$, where $\bm \theta$ denotes the DNN's parameter vector}\DIFaddend .



In \DIFdelbegin \DIFdel{Interactive Imitation Learning (IIL)}\DIFdelend \DIFaddbegin \DIFadd{IIL}\DIFaddend , a human instructor, known as the \emph{teacher}, aims to improve the behavior of the learning agent, referred to as the \emph{learner}, by providing feedback on observed actions. 
IIL does not rely on a predefined reward function for specific tasks, thanks to the direct guidance provided by the teacher \cite{2022_IIL_survey}.
The teacher's feedback is denoted by the function \( \bm h = {H}(\bm s, \bm a)\)\DIFdelbegin \DIFdel{, where $\bm h \in \mathcal{H}$ and $\mathcal{H}$ is the feedback set}\DIFdelend .
The feedback $\bm h$ can be defined according to the feedback type.
For instance, in demonstration/intervention feedback, $\bm h$ represents the action the learner should execute at a given state. In contrast, for relative corrective feedback, 
$\bm h$ is a directional signal guiding the learner's action towards a more favorable one.
\DIFdelbegin \DIFdel{As }\DIFdelend \DIFaddbegin \DIFadd{In our context, }\DIFaddend the reward function is unknown\DIFaddbegin \DIFadd{; therefore}\DIFaddend , we cannot directly \DIFdelbegin \DIFdel{optimize }\DIFdelend \DIFaddbegin \DIFadd{maximize }\DIFaddend the expected accumulated future rewards $J(\pi)$.
Instead, an observable surrogate loss, \(\ell_{\pi}(\bm{s})\), is formulated. This loss measures the alignment of the learner's policy \(\pi\) with the teacher's feedback. In IIL, its is assumed that minimizing \(\ell_{\pi}(\bm{s})\) indirectly \DIFdelbegin \DIFdel{minimizes }\DIFdelend \DIFaddbegin \DIFadd{maximizes }\DIFaddend \(J(\pi)\) \cite{2011_DAgger,2022_IIL_survey}.
As a result, the \DIFdelbegin \DIFdel{goal of IIL }\DIFdelend \DIFaddbegin \DIFadd{objective }\DIFaddend is to determine an optimal learner's policy $\pi^{l*}$ by solving the following equation:
\begin{equation}
    \pi^{l*} = \arg\min_{\pi\in\Pi} \mathbb{E}_{\bm s\sim d_{\pi}(\bm s)} \left[ \ell_{\pi}( \bm s) \right]
    \label{eq:IIL_formulation}
\end{equation}
where $d_{\pi}(\bm s)$ is the state distribution induced by the policy $\pi$.
In practice, the expected value of the surrogate loss in Eq.~\eqref{eq:IIL_formulation} is approximated using the data collected by a policy that interacts with the environment and the teacher.










\subsection{\DIFdelbegin \DIFdel{D-COACH}\DIFdelend \DIFaddbegin \DIFadd{Learning from relative corrections}\DIFaddend }
\label{sec:sub:COACH}






\DIFdelbegin \DIFdel{Deep-COACH \mbox{%DIFAUXCMD
\cite{2018_D_COACH}}\hskip0pt%DIFAUXCMD
, or }\DIFdelend \DIFaddbegin \DIFadd{To learn from relative corrections, we draw inspiration from the Corrective Advice Communicated by Humans (COACH) framework \mbox{%DIFAUXCMD
\citep{2019_Carlos_COACH}}\hskip0pt%DIFAUXCMD
, specifically its extensions for compatibility with DNNs: Deep COACH (}\DIFaddend D-COACH\DIFdelbegin \DIFdel{for short, is the deep learning implementation of the COACHmethod \mbox{%DIFAUXCMD
\cite{2019_Carlos_COACH}}\hskip0pt%DIFAUXCMD
.
}\DIFdelend \DIFaddbegin \DIFadd{) \mbox{%DIFAUXCMD
\citep{2018_D_COACH, 2019_Rodrigo_D_COACH} }\hskip0pt%DIFAUXCMD
and Batched Deep COACH (BD-COACH) \mbox{%DIFAUXCMD
\citep{2021_BDCOACH}}\hskip0pt%DIFAUXCMD
.
}

\subsubsection{\DIFadd{D-COACH}}
\DIFaddend In \mbox{D-COACH}, the teacher shapes policies \DIFdelbegin \DIFdel{, }\DIFdelend by giving occasional relative corrective feedback $\bm h$ \DIFdelbegin \DIFdel{for }\DIFdelend \DIFaddbegin \DIFadd{as a function of }\DIFaddend the action $\bm a$ executed by the agent at \DIFaddbegin \DIFadd{a }\DIFaddend state $\bm s$. 
Note that $\bm a$ is defined as $\bm \mu_{\bm \theta}(\bm s)$\DIFdelbegin \DIFdel{, specifically with the parameters $\bm \theta$ as they were at the moment the feedback was given}\DIFdelend .The feedback \DIFdelbegin \DIFdel{$\bm h \in \mathcal{A}$ }\DIFdelend \DIFaddbegin \DIFadd{$\bm h$ }\DIFaddend is a normalized vector indicating the direction in which
the action should be modified\DIFaddbegin \DIFadd{, i.e., ${\bm h \in \{ \bm d \in \mathcal{A} \mid  ||\bm d|| = 1\}}$}\DIFaddend .
This signal is used to create a corrected action \DIFdelbegin \DIFdel{$\bm a^\mathrm{target}$ }\DIFdelend \DIFaddbegin \DIFadd{$\bm a^{+}$ }\DIFaddend whose relative magnitude with respect to $\bm a$ is defined as the hyperparameter $e$, where  $e$ is a smaller value compared with the difference between $\bm a$ and the optimal action $\bm a^*$.
The target/corrected action is then defined as \DIFdelbegin \DIFdel{$\bm a^\mathrm{target}(\bm s) = \bm a  + e  \bm h$}\DIFdelend \DIFaddbegin \DIFadd{$\bm a^{+}(\bm s) = \bm a  + e  \bm h$}\DIFaddend , which is employed for updating the policy parameters $\bm \theta$ in a supervised learning manner via the loss
\begin{equation}
\label{eq:dcoach_update}
    \ell^{\text{COACH}}_{\pi}(\bm s) = \min\DIFdelbegin \DIFdel{_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^\mathrm{target}(\bm s) \|^2}}\DIFdelend \DIFaddbegin \DIFadd{_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^{+}(\bm s) \|^2}}\DIFaddend .
\end{equation}
However, when learning from past experiences, e.g., by using a replay buffer, as \DIFdelbegin \DIFdel{$\bm a^\mathrm{target}$ }\DIFdelend \DIFaddbegin \DIFadd{$\bm a^{+}$ }\DIFaddend is not equal to the optimal action, old feedback can lead the policy in the wrong direction. More specifically, if $\bm \mu_{\bm \theta}(\bm s)$ is already closer than \DIFdelbegin \DIFdel{$\bm a^\mathrm{target}$ }\DIFdelend \DIFaddbegin \DIFadd{$\bm a^{+}$ }\DIFaddend to $\bm a^*$, i.e., \DIFdelbegin \DIFdel{$ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^{*}  \|^2 <  \| \bm a^\mathrm{target}(\bm s)- \bm a^{*} \|^2$}\DIFdelend \DIFaddbegin \DIFadd{$ \| \bm \mu_{\bm \theta}(\bm s) - \bm a^{*}  \|^2 <  \| \bm a^{+}(\bm s)- \bm a^{*} \|^2$}\DIFaddend , employing Eq. \ref{eq:dcoach_update} become harmful.
As a consequence, \mbox{D-COACH} keeps a very small data buffer $\mathcal{D}$, therefore only using recently received feedback for updating its policy.
However, this approach leads the algorithm to neglect historical feedback, resulting in the policy overfitting to data from recent trajectories and/or rendering the learning process less efficient.

\DIFaddbegin \subsubsection{\DIFadd{BD-COACH}}
\DIFaddend To address this issue, batched D-COACH \cite{2021_BDCOACH}, \mbox{BD-COACH} for short, was proposed. Here, besides learning a policy \DIFdelbegin \DIFdel{$\pi_{\bm \theta}(\bm s,\bm a) \sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$}\DIFdelend \DIFaddbegin \DIFadd{$\pi(\bm a|\bm s)\sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$}\DIFaddend ,
\mbox{BD-COACH} learns a human model $H_{\bm \phi}(\bm a, \bm s) = \bm h$. This model estimates the human's relative corrective feedback $\bm h$ given the robot's action $\bm \mu_{\bm \theta}$ and state $\bm s$.
Then, the human model is trained by minimizing the loss 
$
    \ell_H = \min_{\bm \phi}{ \| H_{\bm \phi}(\bm a, \bm s) - \bm h \|^2}
$,
 and the policy model is trained by minimizing the loss 
\begin{align*}
\ell_{\pi}^{\text{BD-COACH}}(\bm s) = \min\DIFdelbegin \DIFdel{_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \hat {\bm a}^\mathrm{target} \|^2}}\DIFdelend \DIFaddbegin \DIFadd{_{\bm \theta}{ \| \bm \mu_{\bm \theta}(\bm s) - \hat {\bm a}^{+} \|^2}}\DIFaddend ,
\end{align*}
where \DIFdelbegin \DIFdel{$\hat{\bm a}^\mathrm{target} = \bm \mu_{\bm \theta} (\bm s) + e \cdot  H_{\phi}(\bm \mu_{\bm \theta}(\bm s) , \bm s)$}\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bm a}^{+} = \bm \mu_{\bm \theta} (\bm s) + e \cdot  H_{\phi}(\bm \mu_{\bm \theta}(\bm s) , \bm s)$}\DIFaddend . 
Therefore, since \DIFdelbegin \DIFdel{$\hat{\bm a}^\mathrm{target}$ }\DIFdelend \DIFaddbegin \DIFadd{$\hat{\bm a}^{+}$ }\DIFaddend is estimated in relation to the current robot's policy, the correction is no longer outdated. However, this approach depends on an accurate estimation of $\bm h$, which is learned simultaneously to $\bm \mu_{\bm \theta}$. Consequently, the learning process becomes slower because the policy can only improve after $H_{\bm \phi}$ generates coherent results. Additionally, precise hyperparameter tuning is essential for a stable learning process, given that two models are being optimized simultaneously.


\section{Methodology}
\label{sec:Methodology}

In this section, we outline our CLIC method. 
Initially, in Section \ref{sec:policy_improvement_loss}, we introduce the policy improvement loss, which is used to shape a policy given pairs of positive and negative actions. 
In Section \ref{sec:sub:CLIC_one_corrective_feedback}, we present an approach for utilizing explicit and implicit information when generating pairs of positive and negative actions from one relative corrective feedback.
Then, in Section \ref{sec:sub:CLIC}, we present the algorithm of the CLIC method where the policy can be shaped from multiple relative corrective feedback.
Finally, in Section \ref{sec:sub:learning_from_demonstration} we discuss the case where the CLIC method can be employed to learn from demonstration feedback.

\subsection{Policy improvement loss}
\label{sec:policy_improvement_loss}





Considering a pair of actions $(\bm  a^{+}, \bm a^{-})$ at  state $\bm s$, which we denote as $(\bm s, \bm a^{-}, \bm  a^{+})$ for simplicity, where $\bm a^{+}$ is the \emph{positive} action and $\bm a^{-}$ is the \emph{negative} one. 
For a given state, a positive action $\bm a^{+}$ is defined as a more effective or suitable action in comparison to the negative action $\bm a^{-}$. 
\DIFdelbegin \DIFdel{Define the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend distance of these two actions \DIFaddbegin \DIFadd{is defined }\DIFaddend as $\mathbb{D}(\bm a^+, \bm a^-) = \|\bm a^+ - \bm a^- \|^2$. \DIFdelbegin \DIFdel{Without loss of generality, we }\DIFdelend \DIFaddbegin \DIFadd{We }\DIFaddend assume that there is one optimal action $\bm a^*$ in the action space given the state $\bm s$\DIFdelbegin \DIFdel{.
If there are multiple optimal actions, we can assign the pair of actions to only one of the optimal actions.
We assume }\DIFdelend \DIFaddbegin \DIFadd{, and }\DIFaddend that the positive action $\bm a^+$ is closer to the optimal action $\bm a^*$ than the negative action $\bm a^-$: 
\begin{equation}
\mathbb{D}(\bm a^*, \bm a^-) \geq \mathbb{D}(\bm a^*, \bm a^+).   \label{eq:action_relatinship}
\end{equation}
\DIFdelbegin \DIFdel{We define the feasible action space shaped by the action }\DIFdelend \DIFaddbegin \DIFadd{From this equation, the action }\DIFaddend pair $(\bm  a^{+}, \bm a^{-})$ \DIFdelbegin \DIFdel{as $\mathcal{A}^{(\bm a^+, \bm a^-)} = \{ \bm a \in \mathcal{A} | \mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+) \}$.
This subspace }\DIFdelend \DIFaddbegin \DIFadd{can be used to define the }\emph{\DIFadd{desired action space}} 
\begin{equation}
    \DIFadd{\mathcal{A}^{(\bm a^+, \bm a^-)} = \{ \bm a \in \mathcal{A} | \mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+) \}.
}\end{equation}
\DIFadd{Conversely, the undesired action space can then be defined as $\widetilde{\mathcal{A}}^{(\bm a^+, \bm a^-)} = \mathcal{A} / \mathcal{A}^{(\bm a^+, \bm a^-)} $.
The desired action space }\DIFaddend has the following \DIFdelbegin \DIFdel{properties}\DIFdelend \DIFaddbegin \DIFadd{property}\DIFaddend :
\begin{proposition}
\label{proposition:convex_feasible_space}
    The subspace $\mathcal{A}^{(\bm a^+, \bm a^-)}$ includes the optimal action $\bm a^*$ and is unbounded.\footnote{Proof can be found in  Appendix \ref{proof:proposotion_1}}
\end{proposition}

We now show the connection between a policy and a \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend action space.
The intuition is that \DIFdelbegin \DIFdel{an optimal policy should be more likely to generate actions inside the corresponding feasible action space.
In other words, }\DIFdelend at the given state $\bm s$, actions belonging to $\mathcal{A}^{(\bm a^+, \bm a^-)}$ should be more likely to be selected by the optimal policy $\pi^*$ than those not belonging to this set.
Hence, we have\footnote{Details can be found in Appendix \ref{appendix:policy_improvement_loss}.}
\DIFdelbegin \begin{displaymath}
    \DIFdel{\int_{\bm a \in\mathcal{A}}  \pi^*(\bm s, \bm a)   \mathbf{1}_{\bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}} d \bm a\geq \int_{\bm a \in\mathcal{A}}  \pi^*(\bm s, \bm a)  \mathbf{1}_{\bm a \notin \mathcal{A}^{(\bm a^+, \bm a^-)}}d \bm a.
}\end{displaymath}%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{equation}
   \DIFadd{\underbrace{ \int_{\mathcal{A}}  \pi^*(\bm a|\bm s)   \mathbf{1}_{\bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}} d \bm a}_{\mathbb{P}({\mathcal{A}^{(\bm a^+, \bm a^-)}})} \geq 
   \underbrace{\int_{\mathcal{A}}  \pi^*(\bm a|\bm s) \mathbf{1}_{\bm a \notin \mathcal{A}^{(\bm a^+, \bm a^-)}}d \bm a}_{\mathbb{P}({\widetilde{\mathcal{A}}^{(\bm a^+, \bm a^-)}})}.
    \label{eq:policy_improvement_general_case}
}\end{equation}\DIFaddend 

\DIFdelbegin \DIFdel{To evaluate the above inequality, it is necessary to consider the entire action space. 
However, such an approach is impractical for implementation purposes. Therefore, the evaluation is confined to the action set $\{ \bm a^+, \bm a^-\}$.
We then have the following equation, which means
at any given state, a positive action is more likely to be selected over a negative action by the optimal policy $\pi^*$:
}\begin{align*}
    \DIFdel{\pi^*(\bm s, \bm a^{-}) \leq \pi^*(\bm s, \bm a^{+}) \label{eq:policy_improvement}
}\end{align*}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Eq. ~\eqref{eq:policy_improvement} }\DIFdelend \DIFaddbegin \DIFadd{Notably, Eq. \eqref{eq:policy_improvement_general_case} }\DIFaddend can be utilized to shape the learner's policy \DIFdelbegin \DIFdel{$\pi_{\bm \theta}$. 
This can be achieved by evaluating the policy $\pi_{\bm \theta}$ at $(\bm s, \bm a^{-})$ and $(\bm s, \bm a^{+})$, and optimizing it to satisfy this inequality.
}\DIFdelend \DIFaddbegin \DIFadd{$\pi_{\theta}$. 
}\DIFaddend For a policy $\pi_{\bm \theta}$ represented by a function approximator such as a neural network, 
to make it satisfy \DIFdelbegin \DIFdel{Eq.~\eqref{eq:policy_improvement}}\DIFdelend \DIFaddbegin \DIFadd{the above inequality}\DIFaddend , the policy improvement loss can be designed as follows:
\begin{align}
\ell (\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \max (0, \DIFdelbegin \DIFdel{\log \pi_{\bm \theta}}\DIFdelend \DIFaddbegin \DIFadd{\mathbb{P}}\DIFaddend (\DIFdelbegin %DIFDELCMD < \bm %%%
\DIFdel{s, }%DIFDELCMD < \bm %%%
\DIFdel{a^{-}}\DIFdelend \DIFaddbegin {\widetilde{\mathcal{A}}\DIFadd{^{(\bm a^+, \bm a^-)}}}\DIFaddend )- 
\DIFdelbegin \DIFdel{\log\pi_{\bm \theta}}\DIFdelend \DIFaddbegin \DIFadd{\mathbb{P}}\DIFaddend (\DIFdelbegin %DIFDELCMD < \bm %%%
\DIFdel{s, }%DIFDELCMD < \bm %%%
\DIFdel{a^{+}}\DIFdelend \DIFaddbegin {\DIFadd{\mathcal{A}^{(\bm a^+, \bm a^-)}}}\DIFaddend )  ) 
\DIFdelbegin %DIFDELCMD < \label{eq:loss_policy_improvement}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{eq:loss_policy_improvement_general_case}
\DIFaddend \end{align}
\DIFaddbegin \DIFadd{However, to calculate the loss in Eq. \eqref{eq:loss_policy_improvement_general_case}, it is necessary to consider the entire action space, which can be impractical.
}\DIFaddend 

\DIFdelbegin \DIFdel{For the policy }\DIFdelend \DIFaddbegin \DIFadd{To address this challenge, we assume that the policy is }\DIFaddend parameterized by a Gaussian distribution with fixed covariance, \DIFdelbegin \DIFdel{$\pi_{\bm \theta}(\bm s,\bm a) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma)$, the }\DIFdelend \DIFaddbegin \DIFadd{$\pi_{\bm \theta}(\bm a | \bm s) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma)$. Then  Eq. \ref{eq:policy_improvement_general_case} can be achieved by fulfilling this simplified inequality }\footnote{\DIFadd{Proof can be found in Appendix \ref{apppendix:reduce_policy_improvemnt_inequality_Gaussian_assumption}}}
\begin{align}
\DIFadd{\pi^*(\bm a^{-}|\bm s) \leq \pi^*(\bm a^{+}|\bm s).
    \label{eq:policy_improvement}
}\end{align}
\DIFadd{In other words, only enforcing the positive action $\bm  a^{+}$ to be more likely than the negative action $\bm a^{-}$
}

\DIFadd{the pair $(\bm  a^{+}, \bm a^{-})$ is required to enforce Eq. \ref{eq:loss_policy_improvement_general_case} for the entire action space.
which means
at any given state, a positive action is more likely to be selected over a negative action by the optimal policy $\pi^*$.
The }\DIFaddend loss in Eq. \DIFdelbegin \DIFdel{~\eqref{eq:loss_policy_improvement} is computed as:
}\DIFdelend \DIFaddbegin \DIFadd{\eqref{eq:loss_policy_improvement_general_case} can then be reduced accordingly:
}\begin{align}
\DIFadd{\ell (\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) =  \max (0,  \log \pi_{\bm \theta}(\bm a^{-}|\bm s) - \log\pi_{\bm \theta}(\bm a^{+}|\bm s)  ). 
\label{eq:loss_policy_improvement}
}\end{align}
\DIFadd{For $\pi_{\bm \theta}(\bm a | \bm s) \sim\mathcal{N} (\mu_{\bm \theta}(\bm s), \bm \Sigma)$, Eq. \eqref{eq:loss_policy_improvement} can be further reduced:
}\DIFaddend \begin{align*}
    \ell(\bm \theta;\bm s, \bm a^{-}, \bm  a^{+}) &=  \max (0,  || \bm a^{+} -\mu_{\bm \theta}(\bm s) ||^2 -  || \bm a^{-} - \mu_{\bm \theta}(\bm s) ||^2 )\DIFaddbegin \DIFadd{. 
}\DIFaddend \end{align*}
We can use the gradient descent algorithm to minimize the loss. 
If Eq.~\eqref{eq:policy_improvement} is satisfied, then the loss is zero.
If not satisfied, the loss can be used to update the parameters $\bm \theta$.









































\subsection{Learning from one relative corrective feedback}
\label{sec:sub:CLIC_one_corrective_feedback}


In this subsection, we detail how we generate action pairs $(\bm a^{-}, \bm a^{+})$, from one relative corrective feedback, for \DIFaddbegin \DIFadd{creating the desired action space and }\DIFaddend calculating the policy improvement loss defined in Section \ref{sec:policy_improvement_loss}. 
Similarly to D-COACH, the human teacher does not have access to the optimal action $\bm a^*$ in our setting. 
Instead, the teacher can provide a directional signal $\bm h$, guiding the learner's current action  $\bm a$ towards $\bm a^*$.
The robot takes action $\bm a$ at state $\bm s$ and receives a correction $\bm h$ from the teacher if the teacher believes $\bm a$ is suboptimal.
The correction $\bm h$ only contains the directional information, with its length $\| \bm h \| = 1$, and the magnitude of the correction is defined as $e$.
For the small incremental corrective feedback, the magnitude $e$ is pre-defined by the teacher to a small value.
For the case where the teacher knows the magnitude of the correction, the magnitude $e$ can be specified by the teacher for each correction.
This process leads to one relative corrective feedback tuple $[ \bm s, \bm a, e\bm h ]$.


Formally, we denote our CLIC method from one relative corrective feedback $[ \bm s, \bm a, e\bm h ]$ as generating pairs of actions via a function $F: [ \bm s, \bm a, e\bm h ] \rightarrow (\bm a^{-},\bm a^{+})_j (j = 0, \dots)$, and using them to shape the policy via the policy improvement loss in Eq.~\eqref{eq:policy_improvement}.
We first make some assumptions about $\bm h$ in Section \ref{sub:sub:section:feedback_noise}, then we detail the implementation of the function $F$ of how to sample action pairs for comparison in Section \ref{sec:sub:sub:explicit_info_CLIC} and \ref{sec:sub:sub:implicit_info_CLIC}.
At last, we give the interpretation of this implementation in \ref{sec:sub:sub:interpretation_CLIC}.


\begin{figure}[t]
	\centering
	\includesvg[scale=0.115]{figs/Fig4_policy_Contrastive_explain.svg}
	\caption{Visualization of generating pairs of positive and negative actions. The green and red colors denote the positive and negative actions, respectively.}
	\label{fig:policy_Contrastive_explain}
\end{figure}

\subsubsection{Assumption of relative corrective feedback}
\label{sub:sub:section:feedback_noise}




The human feedback can be noisy for many reasons, such as human error or limitations of the feedback interface (e.g., a keyboard's directional constraints). 
To formally analyze the noise level of the feedback, 
we first define the normalized accurate correction as 
$\bm h^* = (\bm a^* - \bm a)/ \| \bm a^* - \bm a\|$.
We make the following assumptions about the uncertainty of human corrections:
(1) the angle between the human correction $\bm h$ and the accurate correction $\bm h^*$ remains under a threshold $\beta$,
(2) the norm of $\bm h$ relative to $\bm h^*$ lies within $[\sigma_1, \sigma_2]$, and
(3) the average of the human correction for the $\bm a$ at state $\bm s$ should be close to the accurate correction.
Formally, these assumptions are expressed as
\begin{align}
    \angle (\bm h, \bm h^*) \leq \beta, 
\sigma_1 \| \bm h^* \| \leq  e\| \bm h \| \leq \sigma_2 \| \bm h^* \|,
    \mathbb{E}(\bm h) = \bm h^*,
    \label{eq:assumption_corrective_feedback}
\end{align}
where $\beta \in [0^\circ, 90^\circ)$ is the directional error and $\sigma_2 > \sigma_1 >0$ denotes the error in magnitude.

\subsubsection{Explicit information of relative corrective feedback}
\label{sec:sub:sub:explicit_info_CLIC}




From the relative corrective feedback $\bm h$ received at $(\bm s, \bm a)$, the teacher wants the agent to take actions towards the direction of correction $\bm h$, as shown in Fig. \ref{fig:policy_Contrastive_explain}.
However, as outlined in Section \ref{sub:sub:section:feedback_noise}, these corrections 
$\bm h$ can be prone to inaccuracies, including directional and magnitude-related noise.
To deal with magnitude-related noise, we introduce the hyperparameter 
$\varepsilon \in [0, 1]$, which reflects the teacher's confidence in the magnitude of the correction $e\| \bm h \|$.
Consequently, we define one pair of actions in Section \ref{sec:policy_improvement_loss} as $\bm a^{-} = \bm a$ and $\bm a^{+} = \bm a + \varepsilon e\bm h$, which are denoted by the red and green circle in Fig. \ref{fig:policy_Contrastive_explain}, respectively.
According to Eq.~\eqref{eq:policy_improvement} we have the following inequality:
\begin{align}
\mathbb{D} (\bm a^*, \bm a) \geq \mathbb{D}(\bm a^*,\bm a + \varepsilon e \cdot \bm h)
    \label{eq:contrastive_constraint_original}
\end{align}

\subsubsection{Implicit information of relative corrective feedback}
\label{sec:sub:sub:implicit_info_CLIC}
In addition to explicitly comparing one pair of actions, our method utilizes the implicit information contained within relative corrective feedback $\bm h$.
We define the positive correction $\bm h^{+} = \bm h$ and the implicit negative correction $\bm h^{-}$, defined as the wrong corrective feedback that has the same magnitude as $\bm h^{+}$, but points to a different direction.
The relationship between 
$\bm h^{+}$ and $\bm h^{-}$ is quantified by a hyperparameter $\alpha \in (0^\circ, 180^\circ]$, representing the angle between them.
A larger $\alpha$ suggests larger directional difference between $\bm h^{+}$ and $\bm h^{-}$. 
The angle $\alpha$ indicates how certain we are about the positive correction $\bm h^{+}$ by pointing out what are negative corrections $\bm h^{-}$. 
To formalize this, $\bm h^{-}$ is sampled from the negative feedback set $\mathcal{H}^{-}$:
\begin{align}
 \mathcal{H}^{-}(\bm h^{+}, \alpha) = \{ \bm h^{-} \in \mathcal{H} | \| \bm h^{-} \| = 1,  \angle(\bm h^{-}, \bm h^{+}) = \alpha \}
 \label{eq:assumption_negative_correction}
\end{align}

With sampled $\bm h^{-}_i \in \mathcal{H}^{-}, i = 1, \dots, n$, we can obtain the corresponding sampled action pairs, denoted as $(\bm a^{I-}_i, \bm a^{I+})$.
In Fig. \ref{fig:policy_Contrastive_explain}, the green arrow denotes $\bm h^{+}$ and the red arrow denotes one sample of $\bm h^{-}$.
We add both $\bm h^{+}$ and $\bm h^{-}$ to the action $\bm a^{+} = \bm a^{-} + \epsilon e \bm h^{+}$.
The magnitude of the correction is re-scaled by $( 1-\varepsilon ) e$ to make $\bm a^{I+} = \bm a^{-} + e \bm h^{+}$.
Then we have the action pairs defined as follows, which are shown by the squares in Fig. \ref{fig:policy_Contrastive_explain}:
\begin{align}
    \bm a^{I+} = \bm a^{-} + \varepsilon e   \bm h^{+} + (1-\varepsilon) e \bm  h^{+} = \bm a^{-} + e \bm h^{+} \\
     \bm a^{I-}_i = \bm a^{-} + \varepsilon e  \bm h^{+} + (1-\varepsilon) e \bm h^{-}_i,
     \label{eq:implicit_action_pairs}
\end{align}


By constructing implicit action pairs, we assume that the implicit positive action $\bm a^{I+}$ is closer to the optimal action $\bm a^*$ than implicit negative actions $\bm a^{I-}_i$.
By Eq.~\eqref{eq:policy_improvement}, we have: 
\begin{align}
\mathbb{D}(\bm a^*, \bm a^{I-}_i) \geq \mathbb{D}(\bm a^*,\bm a^{I+}), i = 1, \dots, n 
    \label{eq:contrastive_contrasint_samples}
\end{align}

In summary, the function $F$ that generates action pairs for comparison is implemented as $F: [\bm s, \bm a, e\bm h] \rightarrow (\bm a, \bm a + \epsilon e \bm h), (\bm a_i^{I-}, \bm a^{I+}), i = 1, \dots, n$.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.29]{figs/Fig8_explain_convex_space.jpg}
	\caption{Examples of the overall \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired action }\DIFaddendFL space enforced by Eq.~\eqref{eq:contrastive_contrasint_samples} in 2D action space.
 On the left, the black shaded area denotes the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL space $\mathcal{A}^{(\bm a^{I-}_i, \bm a^{I+})}$ enforced by the action pair $(\bm a^{I-}_i, \bm a^{I+})$.
 In the middle, the purple shaded area denotes the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL space $\mathcal{A}^{(\bm a^{I-}_j, \bm a^{I+})}$ enforced by the action pair $(\bm a^{I-}_j, \bm a^{I+})$.
 The right side displays the intersected \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL space in blue.}
	\label{fig:Fig8_explain_convex_space}
\end{figure}


\subsubsection{Interpretation of CLIC method from one relative corrective feedback}
\label{sec:sub:sub:interpretation_CLIC}
In this subsection, we give the interpretation of the CLIC method from one relative corrective feedback. 
Define the overall \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space enforced by action pairs generated from one relative corrective feedback as 
$\mathcal{A}^{[\bm s, \bm a, e\bm h]}$, which is the intersection of the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend spaces enforced by each action pair $\mathcal{A}^{ (\bm a, \bm a + \epsilon e \bm h)}, \mathcal{A}^{ (\bm a_i^{I-}, \bm a^{I+})}, i = 1, \dots, n$.



\begin{figure}[t]
	\centering
	\includesvg[scale=0.081]{figs/Fig9_explain_alpha_beta.svg}
	\caption{Examples of different $\alpha$ for the same normalized accurate corrective feedback,  denoted by the orange star and dash line. $\varepsilon$ is set to zero in this example. The action generated by the accurate correction is denoted by the orange star, which is $\bm a+e\bm h^*$.  When $\alpha \geq 2 \beta$, as shown in the left and middle figures, $\bm a+e\bm h^*$ is inside the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL action space. However, when $\alpha < 2 \beta$, $\bm a+e\bm h^*$ is outside the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL action space.}
	\label{fig:Fig9_explain_alpha_beta}
\end{figure}

\begin{figure}[t]
	\centering
	\DIFdelbeginFL %DIFDELCMD < \includegraphics[scale=0.28]{figs/Fig2_sampling_counterexamples.jpg}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[scale=0.28]{figs/Fig2_sampling_counterexamples_use_correct_math_symbol.jpg}
	\DIFaddendFL \caption{Examples of the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired action }\DIFaddendFL space enforced by one relative corrective feedback for different $\alpha$ and $\varepsilon$ in 3D action space.
The circle denotes where $\bm a^{I-}$ are sampled, resulting in the \DIFdelbeginFL \DIFdelFL{feasible }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{desired }\DIFaddendFL action space denoted with the same color.}
	\label{fig:3D_actionspace}
\end{figure}

\begin{proposition}
The overall \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space of one relative corrective feedback $\mathcal{A}^{[\bm s, \bm a, e\bm h]}$ enforced by Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples} is non-empty, convex, and unbounded. 
\footnote{Proof can be found in Appendix \ref{proof:proposotion_2}}
     \label{proposition:open_convex}
\end{proposition}




\textit{Example of proposition \ref{proposition:open_convex}}:
Fig. \ref{fig:Fig8_explain_convex_space} presents an example of proposition 2 in 2D action space. 
As shown in the right part of Fig. \ref{fig:Fig8_explain_convex_space}, the intersection of the two \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend spaces $\mathcal{A}^{(\bm a^{I-}_i, \bm a^{I+})}$ and $\mathcal{A}^{(\bm a^{I-}_j, \bm a^{I+})}$ is also convex.
Another thing to note is that the overall \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend space is unbounded for one relative corrective feedback, which requires more relative corrective feedback to keep the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend space smaller and bounded eventually.

\begin{proposition}
The overall \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space of one relative corrective feedback $\mathcal{A}^{[\bm s, \bm a, e\bm h]}$ enforced by Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples}  includes the optimal actions by selecting the appropriate parameters $\alpha$ and $\varepsilon$.
     \label{propositoin:alpha_beta}
     \footnote{Proof can be found in Appendix \ref{proof:proposotion_3}}
\end{proposition}



\textit{Example of proposition \ref{propositoin:alpha_beta}}:
Fig. \ref{fig:Fig9_explain_alpha_beta} shows the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space of different $\alpha$  in 2D action space. 
To make the relative corrective feedback able to provide useful information, the post-correction action by the accurate feedback must be included in the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend space. 
This requires carefully selecting the value of $\alpha$ and $\varepsilon$, given a known directional error $\beta$ and magnitude error $\sigma_1, \sigma_2$ of the relative corrective feedback.


\textit{Example of CLIC from one relative corrective feedback in 3D action space}:
Fig. \ref{fig:3D_actionspace} visualize the overall \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space induced by one relative corrective feedback in 3D space.
The value of the angle $\alpha$ and the magnitude parameter $\varepsilon$ are varied to show the effects of the hyperparameters.
The implicit negative actions $\bm a^{I-}$ are sampled uniformly from the circle determined by $\alpha$ and $\varepsilon$.



\subsection{CLIC: Contrastive Learning via Interactive Corrections}
\label{sec:sub:CLIC}


\begin{algorithm}
\caption{CLIC algorithm for relative corrective feedback}
\label{alg:online_IIL}
\begin{algorithmic}[1]
\Require data buffer \( \mathcal D \), update frequency \( b \), magnitude $e$
\For{ episode = 1, 2, $\dots$}
\For{\( t = 1, 2, \dots \)}
    \State Observe state \( \bm s_t \)
    \State Select and execute action $\bm a_t$ from  agent or expert
    \State Provide feedback \( e \bm h_t \) for \( \bm a_t\), if necessary
    \State Aggregate \( [\bm s_t, \bm a_t, e \bm h_t] \) to \( \mathcal D \)
    \label{alg:line:aggregate_data_to_buffer}
    \If{\( \text{mod}(t, b) = 0 \) or $\bm h_t \text{ is not } \bm 0$}
    \label{alg:line:oneline_update}
        \State Sample a batch of $[\bm s, \bm a, e\bm h]$ from $\mathcal{D}$  \label{alg:line:sample_batch}
        \State Generate action pairs for each $[\bm s, \bm a, e\bm h]$  \label{alg:line:generate_action_pairs}
        \State Update policy \( \pi_{\bm \theta} \) by the loss of all action pairs\label{alg:line:update_policy_once}
    \EndIf
\EndFor
\State Update policy \( \pi_{\bm \theta} \) from \( \mathcal D \) $n$ times as line \ref{alg:line:sample_batch} to line \ref{alg:line:update_policy_once}
\EndFor
\end{algorithmic}
\end{algorithm}

In Section \ref{sec:sub:CLIC_one_corrective_feedback}, we detail the CLIC method from one relative corrective feedback.
Proposition \ref{proposition:open_convex} demonstrates that one relative 
 corrective feedback results in an unbounded \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired action }\DIFaddend space, which also includes undesirable actions. 
Therefore, multiple additional corrective feedbacks are necessary to reduce the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend space and exclude these undesirable actions.
In this section, we present the algorithm of the CLIC method in Algorithm \ref{alg:online_IIL}, where we shape the learner's policy through multiple corrections.
We denote the data buffer $\mathcal{D} = \{[\bm s_t, \bm a_t, e \bm h_t], t = 0, \dots \}$ that contains all the received relative  corrective feedback.
The way of collecting relative corrective feedback is shown in the line \ref{alg:line:aggregate_data_to_buffer}.
The core part of this algorithm is from line \ref{alg:line:sample_batch} to \ref{alg:line:update_policy_once}.
In line \ref{alg:line:sample_batch}, a batch of data can be sampled from the buffer $\mathcal{D}$.
In line \ref{alg:line:generate_action_pairs}, the action pairs are generated for each relative corrective feedback, as in Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples}.
Line \ref{alg:line:update_policy_once} calculate the loss of parameters $\bm \theta$ using Eq.~\eqref{eq:loss_policy_improvement} for each update of the policy $\pi_{\bm \theta}$.
We show that as the data buffer $\mathcal{D}$ contains sufficient relative corrective feedback, 
the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend action space becomes smaller and smaller in the following proposition: 
\begin{proposition}
\label{proposition:convergences}
As the number of relative corrective feedback approaches infinity, the \DIFdelbegin \DIFdel{feasible }\DIFdelend \DIFaddbegin \DIFadd{desired }\DIFaddend space $\mathcal{A}^{\mathcal{D}}$ enforced by the buffer $\mathcal{D}$ is bounded and converges to optimal actions
   \footnote{Proof can be found in Appendix \ref{proof:proposotion_4}}.
\end{proposition}



\subsection{Converting demonstration to relative correction}
\label{sec:sub:learning_from_demonstration}




In this subsection, we show that demonstration feedback, known as absolute correction, can be transformed into relative corrective feedback, which can then be utilized by the CLIC method.
Given the human demonstration $\bm a^d$ received at $(\bm s, \bm a)$, the demonstration-based IIL is to minimize 
$||\bm a^d - \bm \mu_{ \bm \theta}(\bm s) ||^2$.
However, if the human demonstration is non-optimal, that is given $\bm s, \bm a_d \neq \bm a^*$.
In this case, directly imitating the non-optimal action can lead to suboptimal performance. 
Instead, we can assume that the human demonstration $\bm a^d$ is better than the current robot's action $\bm a$.
From this, we transform the demonstration data $[\bm s, \bm a^d]$ to the relative 
 corrective feedback $[\bm s, \bm a, \bm h]$, where
we redefine the relative correction $\bm h = \frac{\bm a^d - \bm a}{\| \bm a^d - \bm a \|}$ and the magnitude $e = \| \bm a^d - \bm a \|$.
The difference from the typical demonstration feedback is that we also record the robot's action, besides the teacher's action.
Then, the CLIC method can be directly applied.
We name this method using demonstration \DIFdelbegin \DIFdel{data as CLIC-Demonstration (CLIC-D }\DIFdelend \DIFaddbegin \DIFadd{(absolute correction) data as CLIC-Absolute (CLIC-A }\DIFaddend for short), while the method uses small incremental \DIFaddbegin \DIFadd{relative }\DIFaddend correction data as \DIFdelbegin \DIFdel{CLIC-Correction (CLIC-C }\DIFdelend \DIFaddbegin \DIFadd{CLIC-Relative (CLIC-R }\DIFaddend for short).
The only difference between \DIFdelbegin \DIFdel{CLIC-D and CLIC-C }\DIFdelend \DIFaddbegin \DIFadd{CLIC-A and CLIC-R }\DIFaddend is that the \DIFdelbegin \DIFdel{CLIC-D }\DIFdelend \DIFaddbegin \DIFadd{CLIC-A }\DIFaddend method is more certain about the magnitude of the correction feedback $e$, and the magnitude $e$ is determined from the demonstration feedback. 
In contrast, $e$ in \DIFdelbegin \DIFdel{CLIC-C }\DIFdelend \DIFaddbegin \DIFadd{CLIC-R }\DIFaddend is a small fixed value specified by the teacher before the learning process.







 
\DIFaddbegin \section{\DIFadd{Experiments}}
\label{sec:experiments}

\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.328]{figs/Fig5_tasks_v2.png}
	\caption{\DIFaddFL{Tasks for the experiments: drawer opening, hammer, assembly, multi-line following, real-robot box pushing, water pouring, real-robot ball catching. For the figure of the ball-catching task, the transparent ball can be seen inside the blue box.}}
	\label{fig:tasks}
\end{figure*}




\DIFadd{In this section, we aim to answer the following questions:
}

\DIFadd{(1) How does CLIC perform compared with the latest relative correction-based method (BD-COACH)?
(2) How does CLIC perform compared with the state-of-the-art demonstration-based IIL methods, especially in cases of noisy teacher feedback?
(4) What is the effect of the hyperparameters of the CLIC method?
(3) What kind of tasks can CLIC solve that other methods cannot?
(5) Can CLIC be applied to real-world manipulation tasks?
}

\DIFadd{A summary of the platforms used for our experiments is depicted in Fig. \ref{fig:tasks}.
}


\subsection{\DIFadd{Simulated Experiments}}
\DIFadd{In this section, we evaluate our proposed algorithm using simulation-based tasks, benchmarking it against current state-of-the-art methods. Our objective is to study the performance of CLIC in both of its modalities: CLIC-Absolute and CLIC-Relative. We aim to observe how CLIC behaves in ideal scenarios and when the teacher feedback is noisy, a situation likely to occur in reality. Moreover, we also conduct experiments to evaluate the capacity of CLIC to be employed to teach policies when feedback is provided over a subspace of the action space, a situation also likely to occur in robotics, for instance, when teaching multiple robots simultaneously.
Additionally, the effects of the hyperparameters of the CLIC method are studied.
}


\DIFadd{To achieve this, we use two metrics: the obtained }\textbf{\DIFadd{success rate}} \DIFadd{and the }\textbf{\DIFadd{number of time steps}} \DIFadd{required by the learner to converge to a well-performing policy. The success rate directly indicates the quality of the agent's policy, while the number of convergence steps reflects the time efficiency of the learning algorithm, an essential aspect of interactive learning.
}

\DIFadd{To obtain consistent and comparable results between different methods, we employed a }\textbf{\DIFadd{simulated teacher}}\DIFadd{. The simulated teacher is an expert policy that, every $n$ time steps, compares the action it would take in a state with the one the learner aims to take. If the difference between both actions exceeds a threshold (set at 0.2 for all experiments), the expert action is used to provide feedback to the learner. For the Meta-World experiments, we set $n=10$, and for the multi-line following, $n=1$.
}

\DIFadd{All experiments were conducted on a laptop computer equipped with an }\emph{\DIFadd{NVIDIA GeForce RTX 4080 GPU}}\DIFadd{.
}








\begin{table*}[]
\caption{\DIFaddFL{Simulated experiment results of algorithms with intervention/correction data only. SR denotes the success rate, and CT $(\times 10^3)$ denotes the convergence time steps, defined as the first time step that reaches 90\% of the final success rate. $\diagdown$ denotes that the algorithm fails to converge.
}}
\label{table:simulated_exp_results}
\begin{center}
\begin{tabular}{c|c|cc|cc|cc|cc|cc}
\hline
\multirow{2}{*}{Task}                                                       & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Teacher \\ noise\end{tabular}} & \multicolumn{2}{c|}{CLIC-Absolute} & \multicolumn{2}{c|}{CLIC-Relative} & \multicolumn{2}{c|}{HG-DAgger$^{+}$} & \multicolumn{2}{c|}{HG-DAgger} & \multicolumn{2}{c}{BD-COACH}                   \\
    &                                                                           & \DIFaddFL{SR                          }& \DIFaddFL{CT                      }& \DIFaddFL{SR                        }& \DIFaddFL{CT                         }& \DIFaddFL{SR              }& \DIFaddFL{CT           }& \DIFaddFL{SR }& \DIFaddFL{CT  }& \DIFaddFL{SR }& \DIFaddFL{CT   }\\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Drawer \\ Opening\end{tabular}} & \DIFaddFL{0$^\circ$  }& \DIFaddFL{0.99 }&  \DIFaddFL{$3.3 $ }& \DIFaddFL{0.99 }& \DIFaddFL{$3.1 $ }& \DIFaddFL{0.99 }& \DIFaddFL{$4.5$ }& \DIFaddFL{0.99 }& \DIFaddFL{$8.9$ }& \DIFaddFL{0.99 }& \DIFaddFL{$6.4 $ }\\
    & \DIFaddFL{45$^\circ$ }& \DIFaddFL{0.99 }& \DIFaddFL{5.1 }& \DIFaddFL{0.99 }& \DIFaddFL{4.2 }& \DIFaddFL{0.99 }& \DIFaddFL{6.4 }& \DIFaddFL{0.99 }& \DIFaddFL{12.3 }& \DIFaddFL{0.64 }& \DIFaddFL{22.5 }\\
    & \DIFaddFL{60$^\circ$ }& {\DIFaddFL{0.99}} & \DIFaddFL{6.0 }& \DIFaddFL{0.99 }& {\DIFaddFL{5.3}} & \DIFaddFL{0.98 }& \DIFaddFL{8.7 }& \DIFaddFL{0.95 }& \DIFaddFL{16.0 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\
    & \DIFaddFL{75$^\circ$ }& {\DIFaddFL{0.99}}  & \DIFaddFL{10.9  }& {\DIFaddFL{0.99}}  & \DIFaddFL{10.7 }& \DIFaddFL{0.86 }&  \DIFaddFL{33.0  }& \DIFaddFL{0.18 }& \DIFaddFL{55.9 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ \hline
\multirow{4}{*}{Hammer} &  \DIFaddFL{0$^\circ$ }& \DIFaddFL{0.99 }& \DIFaddFL{6.3 }& \DIFaddFL{0.99 }& \DIFaddFL{7.6 }& \DIFaddFL{0.99 }& \DIFaddFL{8.6 }& \DIFaddFL{0.99 }& \DIFaddFL{17.0 }& \DIFaddFL{0.99 }& \DIFaddFL{21.2 }\\
    & \DIFaddFL{45$^\circ$ }& \DIFaddFL{0.98 }& {\DIFaddFL{10.9}} & \DIFaddFL{0.98 }& \DIFaddFL{11.2 }& \DIFaddFL{0.96 }& \DIFaddFL{12.5 }& \DIFaddFL{0.96 }& \DIFaddFL{23.0 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\
    & \DIFaddFL{60$^\circ$ }& {\DIFaddFL{0.97}} & {\DIFaddFL{16.5}} & {\DIFaddFL{0.97}} & \DIFaddFL{18.6 }& \DIFaddFL{0.89 }& \DIFaddFL{20.6 }& \DIFaddFL{0.83 }& \DIFaddFL{37.1 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ 
    & \DIFaddFL{75$^\circ$ }& {\DIFaddFL{0.95}}  & \DIFaddFL{29.3 }& \DIFaddFL{0.76 }& \DIFaddFL{38.2 }& \DIFaddFL{0.07 }& \DIFaddFL{58.7  }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$  }\\ \hline
\multirow{4}{*}{Assembly} & \DIFaddFL{0$^\circ$ }& \DIFaddFL{0.99 }& {\DIFaddFL{7.9}} & \DIFaddFL{0.97 }& \DIFaddFL{8.6 }& {\DIFaddFL{0.99}} & \DIFaddFL{8.4 }& \DIFaddFL{0.99 }& \DIFaddFL{15.7 }& \DIFaddFL{0.97 }& \DIFaddFL{23.1 }\\
    & \DIFaddFL{45$^\circ$ }& {\DIFaddFL{0.98}} & \DIFaddFL{14.9 }& \DIFaddFL{0.95}& {\DIFaddFL{13.2}} & \DIFaddFL{0.91 }& \DIFaddFL{18.3 }& \DIFaddFL{0.93 }& \DIFaddFL{26.5 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\
    & \DIFaddFL{60$^\circ$ }& {\DIFaddFL{0.91}} & {\DIFaddFL{19.0}} & \DIFaddFL{0.89 }& {\DIFaddFL{18.5}} & \DIFaddFL{0.62 }& \DIFaddFL{30.7 }& \DIFaddFL{0.52 }& \DIFaddFL{38.1 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ 
    & \DIFaddFL{75$^\circ$ }&  {\DIFaddFL{0.82}} & \DIFaddFL{38.3  }& \DIFaddFL{0.44  }& \DIFaddFL{46.1 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ \hline
\multirow{4}{*}{\textbf{Average}}
&  \DIFaddFL{0$^\circ$ }& \textbf{\DIFaddFL{0.99}} & \textbf{\DIFaddFL{5.8}} & \DIFaddFL{0.98 }& \DIFaddFL{6.4 }& \textbf{\DIFaddFL{0.99}} & \DIFaddFL{7.2 }& \textbf{\DIFaddFL{0.99}} & \DIFaddFL{13.9 }& \DIFaddFL{0.98 }& \DIFaddFL{16.9 }\\ 
&  \DIFaddFL{45$^\circ$ }& \textbf{\DIFaddFL{0.98}} & \DIFaddFL{10.3 }& \DIFaddFL{0.97 }& \textbf{\DIFaddFL{9.5}} & \DIFaddFL{0.95 }& \DIFaddFL{12.4 }& \DIFaddFL{0.96 }& \DIFaddFL{20.6 }& \DIFaddFL{0.21 }& \DIFaddFL{$\diagdown$ }\\ 
&  \DIFaddFL{60$^\circ$ }& \textbf{\DIFaddFL{0.96}} & \textbf{\DIFaddFL{13.8}} & \DIFaddFL{0.95 }& \DIFaddFL{14.1 }& \DIFaddFL{0.81 }& \DIFaddFL{19.7 }& \DIFaddFL{0.77 }& \DIFaddFL{30.4 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ 
& \DIFaddFL{75$^\circ$ }& \textbf{\DIFaddFL{0.92}} & \textbf{\DIFaddFL{26.2}} & \DIFaddFL{0.73 }& \DIFaddFL{31.7 }& \DIFaddFL{0.31 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.06 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ \hline
\end{tabular}
\end{center}
\end{table*}
 \begin{table}[htbp]
\caption{\DIFaddFL{Simulated experiment results of algorithms with extra regularization term using additional non-intervention data.}}
\label{table:simulated_exp_results_IWR}
\begin{center}
\setlength{\tabcolsep}{3pt} \begin{tabular}{c|c|cc|cc|cc|cc}
\hline
\multirow{2}{*}{Task} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Teacher \\ noise\end{tabular}} & \multicolumn{2}{c|}{CLIC-CEILing} & \multicolumn{2}{c|}{CLIC-IWR} & \multicolumn{2}{c|}{CEILing} & \multicolumn{2}{c}{IWR} \\
& & \DIFaddFL{SR }& \DIFaddFL{CT }& \DIFaddFL{SR }& \DIFaddFL{CT }& \DIFaddFL{SR }& \DIFaddFL{CT }& \DIFaddFL{SR }& \DIFaddFL{CT }\\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Drawer \\ Opening\end{tabular}} & \DIFaddFL{0$^\circ$ }& \DIFaddFL{1.00 }& \DIFaddFL{3.2 }& \DIFaddFL{0.99 }& \DIFaddFL{3.4 }& \DIFaddFL{0.99 }& \DIFaddFL{4.5 }& \DIFaddFL{0.99 }& \DIFaddFL{4.5 }\\
& \DIFaddFL{45$^\circ$ }& \DIFaddFL{1.00 }& \DIFaddFL{5.0 }& \DIFaddFL{0.99 }& \DIFaddFL{5.8 }& \DIFaddFL{0.99 }& \DIFaddFL{6.3 }& \DIFaddFL{0.99 }& \DIFaddFL{7.8 }\\
& \DIFaddFL{60$^\circ$ }& \DIFaddFL{0.99 }& \DIFaddFL{7.5 }& \DIFaddFL{0.99 }& \DIFaddFL{6.4 }& \DIFaddFL{0.99 }& \DIFaddFL{9.7 }& \DIFaddFL{0.99 }& \DIFaddFL{14.9 }\\
& \DIFaddFL{75$^\circ$ }& \DIFaddFL{0.99 }& \DIFaddFL{11.3 }& \DIFaddFL{0.99 }& \DIFaddFL{11.6 }& \DIFaddFL{0.96 }& \DIFaddFL{37.6 }& \DIFaddFL{0.25 }& \DIFaddFL{47.3 }\\ \hline
\multirow{4}{*}{Hammer} & \DIFaddFL{0$^\circ$ }& \DIFaddFL{0.99 }& \DIFaddFL{6.1 }& \DIFaddFL{0.99 }& \DIFaddFL{6.3 }& \DIFaddFL{0.99 }& \DIFaddFL{6.9 }& \DIFaddFL{0.99 }& \DIFaddFL{10.0 }\\
& \DIFaddFL{45$^\circ$ }& {\DIFaddFL{0.99}} & \DIFaddFL{10.0 }& \DIFaddFL{0.98 }& \DIFaddFL{9.9 }& \DIFaddFL{0.98 }& \DIFaddFL{11.9 }& \DIFaddFL{0.95 }& \DIFaddFL{22.6 }\\
& \DIFaddFL{60$^\circ$ }& {\DIFaddFL{0.95}} & \DIFaddFL{15.7 }& \DIFaddFL{0.93 }& \DIFaddFL{16.4 }& \DIFaddFL{0.91 }& \DIFaddFL{23.4 }& \DIFaddFL{0.67 }& \DIFaddFL{36.8 }\\
& \DIFaddFL{75$^\circ$ }& {\DIFaddFL{0.92}} & \DIFaddFL{28.7 }& \DIFaddFL{0.90 }& \DIFaddFL{29.6 }& \DIFaddFL{0.01 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ \hline
\multirow{4}{*}{Assembly} & \DIFaddFL{0$^\circ$ }& {\DIFaddFL{0.99}} & \DIFaddFL{7.2 }& \DIFaddFL{0.97 }& \DIFaddFL{7.7 }& \DIFaddFL{0.99 }& \DIFaddFL{7.5 }& \DIFaddFL{0.96 }& \DIFaddFL{12.4 }\\
& \DIFaddFL{45$^\circ$ }& {\DIFaddFL{0.98}} & \DIFaddFL{15.0 }& \DIFaddFL{0.95 }& \DIFaddFL{13.8 }& \DIFaddFL{0.92 }& \DIFaddFL{15.0 }& \DIFaddFL{0.65 }& \DIFaddFL{24.0 }\\
& \DIFaddFL{60$^\circ$ }& \DIFaddFL{0.89 }& \DIFaddFL{19.6 }& {\DIFaddFL{0.92}} & \DIFaddFL{22.8 }& \DIFaddFL{0.70 }& \DIFaddFL{33.7 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\
& \DIFaddFL{75$^\circ$ }& \DIFaddFL{0.81 }& \DIFaddFL{39.8 }& {\DIFaddFL{0.86}} & \DIFaddFL{38.9 }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.00 }& \DIFaddFL{$\diagdown$ }\\ \hline
\multirow{4}{*}{\textbf{Average}} & \DIFaddFL{0$^\circ$ }& \textbf{\DIFaddFL{0.99}} & \textbf{\DIFaddFL{5.5}} & \DIFaddFL{0.98 }& \DIFaddFL{5.8 }& \textbf{\DIFaddFL{0.99}} & \DIFaddFL{6.3 }& \DIFaddFL{0.98 }& \DIFaddFL{9.0 }\\
 & \DIFaddFL{45$^\circ$ }& \textbf{\DIFaddFL{0.99}} & \DIFaddFL{10.0 }& \DIFaddFL{0.97 }& \DIFaddFL{9.8 }& \DIFaddFL{0.96 }& \DIFaddFL{11.1 }& \DIFaddFL{0.86 }& \DIFaddFL{18.1 }\\
 & \DIFaddFL{60$^\circ$ }& \textbf{\DIFaddFL{0.94}} & \textbf{\DIFaddFL{14.3}} & \textbf{\DIFaddFL{0.94}} & \DIFaddFL{15.2 }& \DIFaddFL{0.87 }& \DIFaddFL{22.3 }& \DIFaddFL{0.55 }& \DIFaddFL{25.9 }\\
 & \DIFaddFL{75$^\circ$ }& \DIFaddFL{0.91 }& \textbf{\DIFaddFL{26.6}} & \textbf{\DIFaddFL{0.92}} & \DIFaddFL{26.7 }& \DIFaddFL{0.32 }& \DIFaddFL{$\diagdown$ }& \DIFaddFL{0.08 }& \DIFaddFL{$\diagdown$ }\\
\hline
\end{tabular}
\end{center}
\end{table} 
\subsubsection{\DIFadd{Learning exclusively from corrections: state-of-the-art comparison}}\label{sec:exclusive_corr}
\DIFadd{First, we examine the performance of CLIC against other state-of-the-art methods that exclusively learn from corrective feedback (either relative or absolute). The methods compared include: CLIC--Absolute and Relative, HG-DAgger \mbox{%DIFAUXCMD
\cite{2019_HG_DAgger}}\hskip0pt%DIFAUXCMD
, HG-DAgger$^{+}$, and BD-COACH. Among these, HG-DAgger and HG-DAgger$^{+}$ have not been introduced so far. HG-DAgger is an intervention-based algorithm that aggregates intervention demonstrations into a dataset and updates its behavior at the end of each episode by minimizing the distance between its current policy and the actions stored in the dataset. Unlike HG-DAgger, which updates the parameters of the learner's policy at the end of each episode, methods based on relative corrections, such as CLIC and BD-COACH, update the policy during episodes, as indicated in line \ref{alg:line:oneline_update} of Algorithm \ref{alg:online_IIL}. To better contrast the results of CLIC with those of HG-DAgger, we introduce a slight modification to HG-DAgger, updating its policy during each episode, similarly to CLIC. We denote this variation as HG-DAgger$^{+}$.
}

\DIFadd{We compare these methods in scenarios that consider both ideal teacher feedback and noisy feedback. The noisy feedback is generated by introducing noise to the actions that the simulated teacher takes. This noise is controlled by the parameter $\beta$, detailed in Eq. \eqref{eq:assumption_corrective_feedback}.Three challenging tasks from the Meta-Word simulator \mbox{%DIFAUXCMD
\cite{2020_metaworld_simulation}
}\hskip0pt%DIFAUXCMD
were selected. These tasks are: }\emph{\DIFadd{Drawer Opening}}\DIFadd{, }\emph{\DIFadd{Hammer}}\DIFadd{, and }\emph{\DIFadd{Assembly}}\DIFadd{. For all tasks, the action space of the robot is four-dimensional, which includes the end effector's 3D linear velocity and a 1-dimensional torque command to actuate a gripper. The specifics of each task are outlined in Appendix \ref{appendix:metaworld_simulation_exp_details}.
The results of these tasks}\footnote{\DIFadd{Experiment details are reported in Appendix \ref{appendix:metaworld_simulation_exp_details}.}} \DIFadd{are reported in Table \ref{table:simulated_exp_results}.
}




\textbf{\DIFadd{CLIC outperforms BD-COACH}}
\DIFadd{In Table \ref{table:simulated_exp_results}, we observe that CLIC consistently outperforms BD-COACH across various tasks and noise levels, in both success rate and convergence time, particularly in experiments with higher noise levels. The extended convergence time for BD-COACH can be attributed to its dual model learning requirement: the policy model and the human model. This dual requirement limits the learner's performance, since during the initial learning phase, before the human model becomes accurate, it estimates incorrect feedback signals. These are then fed into the policy model, impairing its early performance. Furthermore, we note that BD-COACH also fails to converge under different levels of teacher noise, a likely consequence of the human model's difficulty in converging to a well-performing solution from noisy labels.
}



\textbf{\DIFadd{CLIC is more robust than HG-DAgger to noisy teacher feedback}}
\DIFadd{We also observe that, without noise in the teacher's feedback, both CLIC-Absolute and Relative, and HG-DAgger$^{+}$ achieve the highest success rates ($\geq 97\%$). However, when faced with noisy feedback, CLIC exhibits superior robustness over HG-DAgger$^{+}$. CLIC consistently maintains higher success rates and shorter convergence time steps across various noise levels and tasks. Moreover, CLIC's performance remains stable across different noise levels, unlike HG-DAgger$^{+}$, which shows significant performance drops, particularly at higher noise levels. These observations are expected, as HG-DAgger$^{+}$ treats feedback as optimal. Hence, when feedback is sub-optimal, this assumption breaks, and the method attempts to imitate incorrect behavior. In contrast, CLIC demonstrates robustness to noisy feedback as its loss function does not assume optimal feedback.
}

\subsubsection{\DIFadd{Going beyond corrections: state-of-the-art comparison}}
\DIFadd{So far, we have compared CLIC with other methods that exclusively learn from corrective feedback. We chose this focus to specifically study the behavior of different approaches that utilize similar types of feedback but apply it differently to optimize behavior. This approach is important because it allows us to draw conclusions while minimizing the influence of external variables that could skew the results. However, specifically in the case of HG-DAgger, improvements to this method, which require additional training data, have been introduced in the literature \mbox{%DIFAUXCMD
\cite{2020_IWR,2022_RAL_Correct_me_If_Wrong}}\hskip0pt%DIFAUXCMD
. Therefore, it would also be interesting to observe how CLIC behaves under such conditions. For this purpose, we introduce two methods:
}

\begin{enumerate}[label=\roman*), leftmargin=0.6cm]
  \item \textbf{\DIFadd{Intervention Weighted Regression (IWR) \mbox{%DIFAUXCMD
\cite{2020_IWR}}\hskip0pt%DIFAUXCMD
}}\DIFadd{: IWR is an extension of HG-DAgger that assumes the learner chose the correct actions in states where the teacher did not intervene. Consequently, these state-action pairs are stored in the learning dataset, encouraging the policy to maintain its behavior in regions where no interventions occurred.
  }

  \item \textbf{\DIFadd{Corrective and Evaluative Interactive Learning (CEILing) \mbox{%DIFAUXCMD
\cite{2022_RAL_Correct_me_If_Wrong}}\hskip0pt%DIFAUXCMD
}} \DIFadd{IWR's assumption may be too strong in some scenarios, especially during the initial episodes of a learning process. In such cases, it might be unfeasible for a human to provide feedback at every timestep; however, this does not imply that the learner's decisions were correct. Consequently, CEILing introduces an evaluative component to the IWR approach. Therefore, additional data, derived from the learner's policy, is stored in the dataset only if the human indicates that the behavior was correct. In our CEILing implementation, we apply episode-level evaluative feedback, where a single evaluative signal at the end of each episode classifies every state-action pair within that episode as either correct or incorrect.
}\end{enumerate}

\DIFadd{In our implementations, IWR and CEILing are built upon HG-DAgger$^{+}$, which showed better results than HG-DAgger in Section \ref{sec:exclusive_corr}. Note that both IWR and CEILing use the same learning loss as HG-DAgger; they mainly introduce strategies for collecting additional training data. Consequently, the same extensions can be incorporated into our CLIC counterpart of HG-DAgger, CLIC-Absolute. As a result, we include experiments where we extend CLIC-Absolute with IWR and CEILing, which we call }\emph{\DIFadd{CLIC-IWR}} \DIFadd{and }\emph{\DIFadd{CLIC-CEILing}}\DIFadd{, respectively. Using the introduced methods, we repeated the experiments of the previous section. The results are reported in Table \ref{table:simulated_exp_results_IWR}.
}


\textbf{\DIFadd{IWR might not always improve HG-DAgger}}
\DIFadd{From Table \ref{table:simulated_exp_results_IWR}, we observe that IWR does not improve the performance of HG-DAgger$^{+}$ in any scenario. Although this result is not of primary relevance to our work, it is particularly surprising in scenarios without teacher noise, as previous studies have reported that IWR enhances the performance of HG-DAgger under such conditions \mbox{%DIFAUXCMD
\cite{2020_IWR,2022_RAL_Correct_me_If_Wrong,liu2022robot}}\hskip0pt%DIFAUXCMD
. However, these studies }\emph{\DIFadd{warmstarted}} \DIFadd{their policies, i.e., they utilized an initial dataset of demonstrations to initialize their learning algorithms. Consequently, even during the initial episodes, the learner is more likely to take reasonable actions, supporting IWR's assumption that the absence of feedback indicates satisfactory behavior. In contrast, to eliminate confounding variables, we did not use warmstarting in our experiments, which might explain why IWR fails to improve performance, as erroneous actions are likely stored in the learning dataset.
}

\textbf{\DIFadd{CLIC alone outperforms CEILing}}
\DIFadd{Unlike IWR, we observe that CEILing consistently outperforms HG-DAgger$^{+}$ in terms of success rate in every scenario, for every percentage of noisy feedback. This result is logical, as in this case, only state-action pairs generated by the learner policy are stored in the dataset when they result in successful episodes, thus reducing the amount of incorrect data when compared to IWR. Nevertheless, despite this improvement, CLIC alone performs better than CEILing. This demonstrates the importance of having an algorithm capable of learning from noisy feedback, as this alone, without requiring additional data or feedback signals, significantly improves learning performance.
}

\textbf{\DIFadd{CLIC-IWR and CLIC-CEILing do not clearly demonstrate improvements over CLIC}}
\DIFadd{Lastly, from our experiments, it is not possible to conclude that extending CLIC to CLIC-IWR and CLIC-CEILing improves its performance. This can be attributed to CLIC already achieving well-performing policies without the extensions. Hence, it is already difficult to further improve its performance in the employed tasks.
}




\begin{table}[t]
\caption{\DIFaddFL{Effects of magnitude certainty parameter $\varepsilon$.
SR denotes the success rate. CT denotes the convergence steps.}}
\label{table:effects_gamma}
\begin{tabular}{cc|cc|cc}
\hline
\multicolumn{2}{l|}{Teacher noise}                                                                                                & \multicolumn{2}{c|}{0$^\circ$} & \multicolumn{2}{c}{45$^\circ$} \\
      &                 & \DIFaddFL{SR         }& \DIFaddFL{CT   $(\times 10^3)$    }& \DIFaddFL{SR         }& \DIFaddFL{CT   $(\times 10^3)$      }\\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}CLIC-\\ Absolute\end{tabular}}} & \DIFaddFL{$\varepsilon = 0.0 $ }&    \DIFaddFL{0.99       }&    \DIFaddFL{9.7       }&       \DIFaddFL{0.99    }&   \DIFaddFL{21.4        }\\
\multicolumn{1}{l|}{}                                                                                 & \DIFaddFL{$\varepsilon = 0.1 $ }&         \DIFaddFL{0.99   }&   \DIFaddFL{6.7        }&       \DIFaddFL{0.99     }&   \DIFaddFL{10.9        }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.3 $ }&       \DIFaddFL{0.99     }&      \DIFaddFL{6.5     }&      \DIFaddFL{0.95      }&    \DIFaddFL{10.3       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.5 $ }&     \DIFaddFL{0.97       }&      \DIFaddFL{7.1     }&      \DIFaddFL{0.84     }&   \DIFaddFL{12.1        }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.7 $ }&       \DIFaddFL{0.93     }&   \DIFaddFL{7.7        }&       \DIFaddFL{0.81    }&    \DIFaddFL{12.7       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.9 $ }&     \DIFaddFL{0.92       }& \DIFaddFL{8.3          }&         \DIFaddFL{0.64   }&    \DIFaddFL{13.6       }\\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}CLIC- \\ Relative\end{tabular}}}  & \DIFaddFL{$\varepsilon = 0.0 $ }&         \DIFaddFL{0.99   }&     \DIFaddFL{8.9      }&     \DIFaddFL{0.99  }&     \DIFaddFL{12.6    }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.1 $ }&         \DIFaddFL{0.99   }&     \DIFaddFL{7.6      }&   \DIFaddFL{0.98       }&      \DIFaddFL{12.3         }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.3 $ }&   \DIFaddFL{0.99        }&       \DIFaddFL{6.9    }&     \DIFaddFL{0.98       }&    \DIFaddFL{10.8       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.5 $ }&  \DIFaddFL{0.99          }&   \DIFaddFL{7.2      }&      \DIFaddFL{0.99      }&     \DIFaddFL{10.8      }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.7 $ }&    \DIFaddFL{0.99       }&     \DIFaddFL{7.0      }&     \DIFaddFL{0.99    }&     \DIFaddFL{9.9      }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\varepsilon = 0.9 $ }&   \DIFaddFL{0.99         }&  \DIFaddFL{7.0         }&      \DIFaddFL{0.99     }&      \DIFaddFL{9.7     }\\ \hline
\end{tabular}
\end{table} \begin{table}[t]
\caption{\DIFaddFL{Effects of directional uncertainty parameter $\alpha$.
}}
\label{table:effects_alpha}
\begin{tabular}{ll|ll|ll}
\hline
\multicolumn{2}{l|}{Teacher noise}                                                                                                & \multicolumn{2}{c|}{$\beta = 0 ^\circ$} & \multicolumn{2}{c}{$\beta = 45^\circ$} \\
  &                 & \DIFaddFL{SR         }& \DIFaddFL{CT  $(\times 10^3)$      }& \DIFaddFL{SR         }& \DIFaddFL{CT  $(\times 10^3)$      }\\ \hline
\multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}CLIC-\\ Absolute\end{tabular}}} & \DIFaddFL{$\alpha = 15^\circ $  }&       \DIFaddFL{0.60     }&         \DIFaddFL{34.2  }&   \DIFaddFL{0.0         }&        \DIFaddFL{$\diagdown$   }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 45^\circ $  }&      \DIFaddFL{0.99      }&\DIFaddFL{9.1     }&       \DIFaddFL{0.68     }&   \DIFaddFL{30.0        }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 90^\circ$   }&     \DIFaddFL{0.99       }&    \DIFaddFL{6.9       }&      \DIFaddFL{0.99    }&   \DIFaddFL{10.9      }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 120^\circ $ }&   \DIFaddFL{0.99         }&        \DIFaddFL{7.2   }&     \DIFaddFL{0.97       }&   \DIFaddFL{12.6         }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 150^\circ$  }&  \DIFaddFL{0.98          }&      \DIFaddFL{8.6     }&      \DIFaddFL{0.97       }&    \DIFaddFL{12.0       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 180^\circ $ }&    \DIFaddFL{0.97        }&        \DIFaddFL{10.2   }&      \DIFaddFL{0.94      }&   \DIFaddFL{14.3        }\\ \hline
\multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}CLIC- \\ Relative\end{tabular}}}  & \DIFaddFL{$\alpha = 15^\circ $  }&    \DIFaddFL{0.72        }&     \DIFaddFL{34.8      }&     \DIFaddFL{0.02       }&  \DIFaddFL{$\diagdown$          }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 45^\circ $  }&       \DIFaddFL{0.99      }&      \DIFaddFL{9.8     }&        \DIFaddFL{0.04    }&     \DIFaddFL{$\diagdown$       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 90^\circ$   }&   \DIFaddFL{0.98         }&       \DIFaddFL{7.5    }&        \DIFaddFL{0.98    }&    \DIFaddFL{11.3       }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 120^\circ $ }&   \DIFaddFL{0.99          }&     \DIFaddFL{8.1      }&        \DIFaddFL{0.95    }&   \DIFaddFL{12.8        }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 150^\circ$  }&   \DIFaddFL{0.96         }&        \DIFaddFL{10.1   }&         \DIFaddFL{0.94   }&  \DIFaddFL{14.3         }\\
\multicolumn{1}{l|}{}                                                                                           & \DIFaddFL{$\alpha = 180^\circ $ }&  \DIFaddFL{0.94         }&         \DIFaddFL{11.9  }&       \DIFaddFL{0.92     }&   \DIFaddFL{16.5        }\\ \hline
\end{tabular}
\end{table} 
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.29]{figs/Fig6_multi_line_follwer_results_0419.png}
	\caption{\DIFaddFL{Experiment results of the multi-line following task.}}
	\label{fig:multi_line_follower}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.45]{figs/Fig3_ball_cathcing_demo.png}
	\caption{\DIFaddFL{One experiment result of the ball-catching task.}}
	\label{fig:ball_cathcing_demo}
\end{figure*}


\begin{table*}[]
\caption{\DIFaddFL{Experiment results for the ball-catching task. The term }\emph{\DIFaddFL{algorithm running time}} \DIFaddFL{refers to the training duration excluding overheads associated with robot restarting, whereas }\emph{\DIFaddFL{total time}} \DIFaddFL{encompasses these overheads. The }\emph{\DIFaddFL{success rate}} \DIFaddFL{column presents this metric across multiple catching attempts.}}
\label{table:exp_results_ball_Catching}
\begin{center}
\begin{tabular}{c|cc|c|cccc}
\hline
\multirow{2}{*}{Episode} & \multirow{2}{*}{{\begin{tabular}[c]{@{}c@{}}Total time\\ (HH:MM:SS)\end{tabular}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Algorithm\\ running time\end{tabular}} & \multirow{2}{*}{Amount of feedback} & \multicolumn{4}{c}{Success rate}                                             \\
  &                             &                             &                                     & \DIFaddFL{First catch }& \DIFaddFL{$\leq$ second catch }& \DIFaddFL{$\leq$ third catch       }& \DIFaddFL{Catch at end }\\ \hline
\DIFaddFL{11                       }& \DIFaddFL{0:20:41                     }& \DIFaddFL{0:16:33                                                                           }& \DIFaddFL{501                                 }& \DIFaddFL{0           }& \DIFaddFL{0                   }& \multicolumn{1}{c|}{0.2} & \DIFaddFL{0.7          }\\
\DIFaddFL{21                       }& \DIFaddFL{0:29:22                     }& \DIFaddFL{0:25:17                                                                           }& \DIFaddFL{1321                                }& \DIFaddFL{0.1         }& \DIFaddFL{0.4                 }& \multicolumn{1}{c|}{0.6} & \DIFaddFL{0.9          }\\
\DIFaddFL{31                       }& \DIFaddFL{0:40:46                     }& \DIFaddFL{0:31:36                                                                           }& \DIFaddFL{2401                                }& \DIFaddFL{0.1         }& \DIFaddFL{0.4                 }& \multicolumn{1}{c|}{0.8} & \DIFaddFL{0.9          }\\
\DIFaddFL{41                       }& \DIFaddFL{0:46:02                     }& \DIFaddFL{0:34:30                                                                           }& \DIFaddFL{2648                                }& \DIFaddFL{0.2         }& \DIFaddFL{0.6                 }& \multicolumn{1}{c|}{0.9} & \DIFaddFL{1            }\\
\DIFaddFL{51                       }& \DIFaddFL{0:52:05                     }& \DIFaddFL{0:36:46                                                                           }& \DIFaddFL{3016                                }& \DIFaddFL{0.3         }& \DIFaddFL{0.8                 }& \multicolumn{1}{c|}{0.9} & \DIFaddFL{1            }\\
\DIFaddFL{61                       }& \DIFaddFL{0:57:20                     }& \DIFaddFL{0:39:29                                                                           }& \DIFaddFL{3257                                }& \DIFaddFL{0.4         }& \DIFaddFL{0.6                 }& \multicolumn{1}{c|}{0.9} & \DIFaddFL{1            }\\
\DIFaddFL{71                       }& \DIFaddFL{1:04:10                     }& \DIFaddFL{0:42:00                                                                           }& \DIFaddFL{3410                                }& \DIFaddFL{0.4         }& \DIFaddFL{1                   }& \multicolumn{1}{c|}{1}   & \DIFaddFL{1            }\\
\DIFaddFL{81                       }& \DIFaddFL{1:14:04                     }& \DIFaddFL{0:47:30                                                                           }& \DIFaddFL{3850                                }& \DIFaddFL{0.3         }& \DIFaddFL{1                   }& \multicolumn{1}{c|}{1}   & \DIFaddFL{1            }\\ \hline
\end{tabular}
\end{center}
\end{table*} 
\subsubsection{\DIFadd{Learning from action-space-subset feedback}}
\DIFadd{Another interesting feature of CLIC when learning from relative corrections, i.e., to as CLIC-Relative, is its ability to learn from feedback defined over a subset of the complete action space. In such cases, it allows the policy to take any action in the dimensions of the action space where the feedback does not provide any information, as shown in the policy improvement loss in Section \ref{sec:policy_improvement_loss}. One scenario where this is essential is when learning a policy for controlling multiple robots. In such situations, we might want to provide feedback to each robot independently, while still being able to learn a global policy. 
}

\DIFadd{To study the performance of CLIC in these scenarios, we developed a novel simulated task focused on multi-robot coordination, named }\emph{\DIFadd{Multi-line following}}\DIFadd{. This task requires feedback to be applied to a single robot at a time, where two robots must follow independent lines at a similar rate. The specifics of this task are outlined in Appendix \ref{appendix:multi_line_exp_details}. The experiment results are shown in Figure \ref{fig:multi_line_follower}.
}

\textbf{\DIFadd{CLIC allows learning policies from partial feedback}}
\label{sec:sub:sub:sub_actionspace}
\DIFadd{Figure \ref{fig:multi_line_follower} illustrates that CLIC effectively utilizes partial action space feedback to quickly learn a high-performing policy. 
In contrast, HG-DAgger converged slowly with lower, less stable success rates due to confusion caused by zero-action labels.
We can also observe that, although the performance of \mbox{BD-COACH} is inferior, this approach still manages to employ this type of feedback to some extent. This can be attributed to its capacity to learn from relative corrections. Nevertheless, since it enforces a stricter condition on the action space solution than CLIC, its overall performance is worse.
}



\subsubsection{\DIFadd{Hyperparameter study}}
\DIFadd{With respect to previous methods based on corrective feedback, we have introduced two new hyperparameters in CLIC that must be considered during learning, which depend on the certainty we have about the teacher's feedback. These are the magnitude certainty hyperparameter, $\varepsilon$, and the directional certainty hyperparameter, $\alpha$. For details, the reader is referred to Section \ref{sec:sub:CLIC_one_corrective_feedback}. We evaluated the behavior of CLIC using multiple values of these hyperparameters in the hammer task from Meta-world. Here, two scenarios were considered, one with zero feedback noise, and another with $\beta=45^{\circ}$.
}

\DIFadd{The results for $\varepsilon$ on CLIC-Absolute and CLIC-Relative are shown in Table \ref{table:effects_gamma}, where $\alpha$ is set to $90^\circ$ and $e=0.02$ for CLIC-Relative. For CLIC-Absolute, we observe that increasing $\varepsilon$ under noise of $45^\circ$ leads to a reduction in success rate. This trend suggests that increasing $\varepsilon$ negatively impacts its performance when the feedback is noisy. In contrast, CLIC-Relative exhibits a more consistent success rate across various $\varepsilon$ levels, for both $0^\circ$ and $45^\circ$ teacher noise, indicating its robustness to changes in $\varepsilon$. This is due to the magnitude of the correction being much smaller than that of the teacher's action, which makes it more robust to the magnitude of errors. The results also show that $\varepsilon = 0.0$ leads to large convergence time steps for both CLIC-Absolute and CLIC-Relative methods, which is expected, as the solution space becomes larger.
}

\DIFadd{The results of the directional certainty parameter $\alpha$ are shown in Table \ref{table:effects_alpha}, where $\varepsilon$ is set to $0.1$. We can observe that the smallest value of $\alpha$ leads to a low success rate and longer convergence times for both CLIC variations. In this case, the desired action space $\mathcal{A}^{(\bm{a}^+, \bm{a}^-)}$ is very restricted, which makes it challenging for the neural network to find a solution within this space. In contrast, for larger values of $\alpha$, the success rates increase significantly. Interestingly, in the results considering values of $\alpha$ between $45^\circ$ and $180^\circ$, the larger the $\alpha$, the longer the convergence time, which can be expected, as the solution space becomes larger for larger values of $\alpha$.
}



\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.32]{figs/Fig11_water_pouring.png}
	\caption{\DIFaddFL{Water-pouring task. }}
	\label{fig:water_pouring_demo}
\end{figure*}
\subsection{\DIFadd{Real robot experiments}}
\DIFadd{In the real robot experiments, three tasks were used to demonstrate the practical applicability of our algorithm, as shown in Fig. \ref{fig:tasks}. The experiments include a dynamic ball-catching task, a water-pouring task that necessitates precise control of the robot's end effector position and orientation, and a box-pushing task. The latter is used to study the performance of CLIC in tasks well-suited for demonstration-based/absolute feedback. The experiments were carried out using a 7-DoF KUKA iiwa manipulator arm, and when required, an underactuated robotic hand (1-dimensional action space) was attached to its end effector. To provide feedback on the robot's pose, a 6D space mouse was employed. Furthermore, in the ball-catching experiment, a keyboard was used to provide binary feedback on the gripper's actuation. 
}

\DIFadd{The details of each task are detailed as follows:
}\begin{enumerate}[label=\roman*), leftmargin=0.6cm]
  \item \textbf{\DIFadd{Ball catching:}} \DIFadd{This task involves the robot catching a ball that's swinging on a string attached to a fixed point. The challenge lies in catching the ball while it is moving. The robot's end effector is positioned on the same plane where the ball swings, with a fixed orientation. The action space consists of the robot's end effector linear velocity in the specified plane and a one-dimensional continuous actuation command for controlling the robot's gripper, where 0 indicates fully closed, and 1 indicates fully open. The state space includes the end-effector's relative position and velocity to the ball, the angle defined between the gravity vector and the ball's string, and its corresponding velocity, and the ball's pose and the fixed point's pose.
}

  \item \textbf{\DIFadd{Water pouring:}} \DIFadd{The robot controls the pose of a bottle to precisely pour liquid (represented with marbles) into a small bowl. The action space is 6-dimensional, consisting of the robot's end-effector linear and angular velocities. The state space is defined by the robot's end-effector pose, which consists of its Cartesian position and its orientation, represented with a unit quaternion. The initial pose of the robot is randomized at the start of each episode within certain position and orientation limits to ensure safety.
}

  \item \textbf{\DIFadd{Box pushing:}} \DIFadd{The robot must push a box across a table to a specified goal pose, aligning its position and orientation. At each episode, the box's position is randomly initialized from one of 10 predefined positions. The action space is defined as the linear velocity of the end-effector in a 2D plane over the table. The robot's end-effector orientation and z-axis position (the one aligned with the table's normal vector) are fixed throughout the task. The state space includes the relative position between the end-effector and the box, the orientation of the box, and the relative position between the box and the goal.
}\end{enumerate}

\DIFadd{Each learned policy is evaluated every 10 episodes for the ball-catching task and every 5 for the box-pushing task and the water-pouring task. 
}


\begin{table}[]
\caption{\DIFaddFL{Experiment results for the water-pouring task.}}
\label{table:exp_results_water_pouring}
\begin{center}
\begin{tabular}{c|cc|c|c}
\hline 
{\DIFaddFL{Episode}} & {\begin{tabular}[c]{@{}c@{}}\DIFaddFL{Total time}\\ \DIFaddFL{(HH:MM:SS)}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}}\DIFaddFL{Algorithm}\\ \DIFaddFL{running time}\end{tabular}} &
{\begin{tabular}[c]{@{}c@{}}\DIFaddFL{Amount of}\\ \DIFaddFL{feedback}\end{tabular}} & {\DIFaddFL{Success rate}} \\
\hline
\DIFaddFL{6       }& \DIFaddFL{0:07:05 }& \DIFaddFL{0:05:32      }& \DIFaddFL{1892               }& \DIFaddFL{0.4       }\\
\DIFaddFL{11      }& \DIFaddFL{0:13:00 }& \DIFaddFL{0:10:12      }& \DIFaddFL{2148               }& \DIFaddFL{0.7         }\\
\DIFaddFL{16      }& \DIFaddFL{0:17:37 }& \DIFaddFL{0:13:53      }& \DIFaddFL{2450               }& \DIFaddFL{0.4         }\\
\DIFaddFL{21      }& \DIFaddFL{0:22:09 }& \DIFaddFL{0:17:29      }& \DIFaddFL{2819               }& \DIFaddFL{0.7         }\\
\DIFaddFL{26      }& \DIFaddFL{0:26:55 }& \DIFaddFL{0:21:10      }& \DIFaddFL{3302               }& \DIFaddFL{0.6         }\\
\DIFaddFL{31      }& \DIFaddFL{0:32:09 }& \DIFaddFL{0:24:35      }& \DIFaddFL{3430               }& \DIFaddFL{0.7         }\\
\DIFaddFL{36      }& \DIFaddFL{0:36:32 }& \DIFaddFL{0:28:03      }& \DIFaddFL{3643              }& \DIFaddFL{0.8          }\\
\DIFaddFL{41      }& \DIFaddFL{0:40:38 }& \DIFaddFL{0:31:14       }& \DIFaddFL{3715              }& \DIFaddFL{0.9          }\\
\hline
\end{tabular}
\end{center}
\end{table} 
\subsubsection{\DIFadd{Ball catching: quick coordination and partial feedback}}
\DIFadd{The ball-catching task is challenging due to its highly dynamic nature. This complexity makes it difficult to provide successful demonstrations of the problem to the robot, thus ruling out demonstration-based IIL methods for solving it}\footnote{\DIFadd{This limitation could be overcome with a highly reactive and precise teleoperation device. However, this also makes the solution more expensive.}}\DIFadd{.
For a successful grasp, the robot must coordinate precisely the ball's motion, the end-effector's motion, and the gripper's actuation. 
Moreover, the robot is expected to react after an unsuccessful attempt by trying to grasp the ball again. 
As a result, this task is particularly interesting for evaluating CLIC's performance, as it allows for the analysis of its ability to quickly coordinate multiple variables in a problem. Additionally, the necessary coordination between end-effector motion and gripper actuation makes it challenging to provide feedback on the complete action space at any given moment. Therefore, this also enables testing CLIC's ability to learn effective policies from partial feedback in real-world problems, where feedback is independently provided for either the end-effector's motion or the gripper's actuation }\footnote{\DIFadd{Details are reported in Appendix \ref{appendix:Ball-catching_task_strategy }}}\DIFadd{.
}


\DIFadd{Table \ref{table:exp_results_ball_Catching} details the experimental results for the ball-catching task, demonstrating progressive improvement in the robot's performance across episodes. After $11$ episodes, there were no successful first catches; however, a success rate of $0.7$ was achieved for final catches, already showcasing the robot's ability to recover from failures. However, as time evolves, catching the ball becomes simpler as its speed decreases. Consequently, for the robot to demonstrate more dynamic behavior, it must be able to catch the ball after only a few attempts. This was achieved by episode 71, where the success rate improved to $0.4$ for first catches and reached $1.0$ for catches within the second attempt.
Interestingly, the dataset did not contain any demonstrations showcasing successful first catches at non-zero velocity, which was learned entirely from partial relative corrections. 
Fig. \ref{fig:ball_cathcing_demo} depicts a successful first catch, which only takes around 1.4 seconds.
}



\subsubsection{\DIFadd{Water pouring: learning full pose control with CLIC}}






\DIFadd{The water-pouring task demonstrates the effectiveness of the CLIC method in scenarios requiring precise control over position and orientation. In this experiment, the CLIC-Absolute and CLIC-Relative methods are combined to teach the robot, allowing the human teacher to select (by pressing a button) which teaching mode (absolute or relative correction) to use at each time step. This approach showcases the flexibility of CLIC, which can learn from relative or absolute corrections depending on which is more appropriate at each moment. In this task, absolute feedback was more appropriate in the initial episodes since the policy was learned from scratch, and, initially, it was easier to intervene in a 6-dimensional action space rather than to indicate directions in which the actions should be modified. However, as the policy improved, relative corrections made it easier to refine and fine-tune the policy in specific regions of the state space. Furthermore, since CLIC allows partial feedback, this signal could be provided in three ways: (1) position only, (2) rotation only, and (3) both position and rotation. This is useful when one part of the robot's action is correct but the the other still needs improvement.
}


\DIFadd{The experimental data is shown in Table \ref{table:exp_results_water_pouring}. From Episode $1$ to Episode $11$, the teacher's feedback is provided in an absolute correction format. From Episode $11$ onward, the teacher's feedback is given as relative corrections to make small adjustments to the robot's policy. The success rate exhibits an overall improving trend, consistently increasing from 0.6 in Episode 26 to 0.9 by Episode 41. An example of the learned policy is illustrated in Fig. \ref{fig:water_pouring_demo}.
}



\begin{figure}[t]
	\centering
	\includegraphics[scale=0.28]{figs/Fig10_box_pushing_results_0419.png}
	\caption{ \DIFaddFL{Experiment results of the real-world box-pushing task.}}
	\label{fig:box_pushing_results}
\end{figure}

\subsubsection{\DIFadd{Box pushing: a comparison between intervention-based approaches}}
\DIFadd{In the box-pushing task, the robot faces the challenge of a pusher-slider system \mbox{%DIFAUXCMD
\cite{2020_pusher_slider_system}}\hskip0pt%DIFAUXCMD
. This task requires precise control and adaptability, as it involves an underactuated system where the robot must push a box to a target pose on the table. This task is well-suited for demonstration-based IIL methods; hence, we employ it to compare CLIC-Absolute and HG-DAgger$^{+}$. Both tasks were repeated three times on the real robot, where their averaged success rates are shown in Figure \ref{fig:box_pushing_results}. Similar to what we observed in Section \ref{sec:exclusive_corr}, both methods showcase similar performance, ultimately reaching a success rate near 1.0. This demonstrates CLIC's competitiveness in tasks suited for demonstration-based IIL methods.
}





 
\DIFaddend \section{Related Work}
\label{sec:related_work}
Interactive imitation learning involves a human teacher inside the learning process of the robot \cite{2022_IIL_survey}.
The human teacher can provide various types of feedback to the robot, including demonstration feedback, relative corrective feedback, evaluative feedback, preference feedback, etc.


\textbf{Learning from demonstration feedback:}
When humans provide demonstration feedback, they give the robot information on how to act.
The demonstration feedback is also denoted as absolute corrective feedback.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2011_DAgger} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2011_DAgger} }\hskip0pt%DIFAUXCMD
}\DIFaddend presented the DAgger algorithm that tackled the issue of distribution mismatch in imitation learning by iteratively updating policies based on expert feedback. 
However, its reliance on continuous expert involvement in every state-action pair makes it impossible for real application.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2019_HG_DAgger} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2019_HG_DAgger} }\hskip0pt%DIFAUXCMD
}\DIFaddend proposed the Human-gated DAgger which enabled humans to teach the robot.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2022-BCz-DataCollectionVia_HG-DAgger} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2022-BCz-DataCollectionVia_HG-DAgger} }\hskip0pt%DIFAUXCMD
}\DIFaddend collected a large dataset in the HG-Dagger manner, and shows the learned policy has the ability to generalize.
To make the learning more efficient, \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2022_Expert_Intervention_Learning} }\hskip0pt%DIFAUXCMD
and \mbox{%DIFAUXCMD
\cite{2023_RSS_Robot_Learning_on_the_job} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2022_Expert_Intervention_Learning} }\hskip0pt%DIFAUXCMD
and \mbox{%DIFAUXCMD
\citet{2023_RSS_Robot_Learning_on_the_job} }\hskip0pt%DIFAUXCMD
}\DIFaddend further exploited the implicit information for the robot actions which do not receive the human demonstration feedback.
However, this implicit information is only valid for expert teachers, which is unsuitable for non-experts.
The mentioned works require near-optimal demonstration, which puts a large burden on human teachers.
It may also require complicated interfaces so that teachers can give the demonstration \cite{2023_master_to_robots_teleoperation}. 

The demonstration feedback can work quite good for certain tasks such as autonomous driving and part of manipulation tasks, but its requirement of near-optimal action constrains the teacher to be experts for some tasks. 
The other three types of feedback can be used instead for non-expert users, which might sacrifice the learning efficiency.

\textbf{Learning from evaluative feedback:}
The evaluative feedback gives the robot information on how it performs.
The human teacher can provide a positive or negative scalar to reinforce the robot, which is less informative than demonstration feedback but easier to provide.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2009_TAMER} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2009_TAMER} }\hskip0pt%DIFAUXCMD
}\DIFaddend learns a human model to predict the estimated immediate reward at the given state-action pair.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2013_Policy_shaping_RL} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2013_Policy_shaping_RL} }\hskip0pt%DIFAUXCMD
}\DIFaddend learns a policy directly from the evaluative feedback.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2017_policy_dependent_human_feedback} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2017_policy_dependent_human_feedback} }\hskip0pt%DIFAUXCMD
}\DIFaddend proposed to treat the evaluative feedback as the advantage function, which can be used in policy gradient methods to gradually improve the performance of a policy.
For noisy demonstrations,  \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{peng2023learning} }\hskip0pt%DIFAUXCMD
and 
\mbox{%DIFAUXCMD
\cite{luo2023rlif} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{peng2023learning} }\hskip0pt%DIFAUXCMD
and 
\mbox{%DIFAUXCMD
\citet{luo2023rlif} }\hskip0pt%DIFAUXCMD
}\DIFaddend extract the evaluative information from the noisy demonstration information.
The evaluative information can also be combined with the demonstration feedback \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2022_RAL_Correct_me_If_Wrong}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2022_RAL_Correct_me_If_Wrong}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
However, the evaluative feedback can be policy-dependent and teacher-dependent, which makes it challenging to scale to large datasets.

\textbf{Learning from preference feedback:}
The preference feedback involves the comparison of different robot trajectory segments, which can be treated as a relative version of the evaluative feedback.
The reward model or objective function is usually learned through the preference feedback \cite{2017_DRL_preference}
\cite{2021_Pebble_preference_RL} \cite{2015_IJRR_Preference_traj_ranking}.
The sub-optimal demonstration data can also be transformed into preference data to learn a high-quality reward model \cite{2019_IRL_ranked_demonstration}.
The policy can also be learned directly by the contrastive learning approach proposed by \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2023_Contrastive_Prefence_Learning} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2023_Contrastive_Prefence_Learning} }\hskip0pt%DIFAUXCMD
}\DIFaddend to improve efficiency.
However, as the preference feedback is less informative than evaluative feedback, it requires more data and time from the teacher to learn a good policy.

\textbf{Learning from relative corrective feedback}:
The relative corrective feedback provides incremental information on how the action can be improved.
It is a good balance between the richness of the feedback information and the simplicity with which the teacher can offer feedback.
The human-preferred trajectory can be solved given the correction feedback and
the objective function can be learned by comparing two pairs of trajectories, as in \cite{2017_pHRI_correction_learning_objective} \cite{2015_IJRR_Preference_traj_ranking} \cite{2018_uncertainty_correction_objective}.
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2022_TRO_correction_objective_function} }\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{2022_TRO_correction_objective_function} }\hskip0pt%DIFAUXCMD
}\DIFaddend proposed a cutting-plane method to directly infer the objective function, without postprocessing the human-preferred trajectory.
However, these objective functions are linear combinations of features, which may lack the ability to represent complex tasks.
Another line of approach is directly learning a policy from post-correction actions \cite{2019_Carlos_COACH}
\cite{2018_D_COACH}
\cite{2021_BDCOACH}.
This COACH-based framework (Corrective Advice Communicated by Humans) has been extended to complex time-dependent tasks \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2020_DCOACH_temporal}
}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{by
\mbox{%DIFAUXCMD
\citet{2020_DCOACH_temporal}
}\hskip0pt%DIFAUXCMD
}\DIFaddend and utilize the feedback from the state space instead of the action space \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2020_COACH_state_Space}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{by
\mbox{%DIFAUXCMD
\citet{2020_COACH_state_Space}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
This framework can also be combined with reinforcement learning to increase the efficiency of the RL algorithm \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{2019_Carlos_IJRR}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{by \mbox{%DIFAUXCMD
\citet{2019_Carlos_IJRR}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
However, as we mentioned in Section \ref{sec:introduction}, the previous COACH-based framework fails to utilize the history data, making it inefficient compared with demonstration-based methods.
Our CLIC method solves this problem by comparing negative and positive actions, which are pre- and post-correction actions.
Although the relative corrective feedback at each state is less informative than the demonstration feedback, our experiment results show that our CLIC method has a competitive performance with the demonstration-based method.
Our CLIC method can also utilize the demonstration data by converting it to relative correction, as long as we record the learner's action besides the teacher's demonstration.
Because the CLIC method uses comparisons to calculate the policy improvement loss, instead of treating the (probably noisy) post-correction action as labels, the CLIC method is more robust to the teacher noise.









































































 
\section{Discussion}
\label{sec:discussion}
\textbf{What will happen when the teacher makes mistakes and gives wrong corrections?}
\DIFdelbegin \DIFdel{Making mistakes is }\DIFdelend \DIFaddbegin \DIFadd{Incorrect corrections can negatively impact both CLIC and other methods compared in this paper, as they violate the assumptions of both CLIC loss and behavior cloning loss. 
However, errors are }\DIFaddend common when the teacher is a human\DIFdelbegin \DIFdel{operator, and we also gave some wrong feedback during the }\DIFdelend \DIFaddbegin \DIFadd{, as evidenced by our }\DIFaddend real-robot \DIFdelbegin \DIFdel{experiments. 
}\DIFdelend \DIFaddbegin \DIFadd{experiments where some feedback was incorrect. 
 The teacher's mistake can be a problem if the optimal action is solved analytically (or makes the policy satisfy the constraint strictly in Eq.~\eqref{eq:action_relatinship}), where one wrong feedback can make the desired action space empty. 
Instead, our CLIC method remains effective if sufficient corrective feedback is provided to counterbalance the erroneous corrections.  
}\DIFaddend As shown in the Algorithm \ref{alg:online_IIL}, a batch of relative corrective feedback data is sampled to calculate the policy improvement loss, which is used to update the parameters of the learner's policy.
As long as the \DIFaddbegin \DIFadd{number of the }\DIFaddend correct feedback is much more than the wrong feedback, the averaged gradient of the parameters should still point to the direction that improves the policy.
Therefore, if the teacher makes some mistakes occasionally, it will not make the policy collapse.
\DIFdelbegin \DIFdel{However, the teacher's mistake can be a problem if the optimal action is solved analytically (or makes the policy satisfy the constraint strictly in Eq.~\eqref{eq:action_relatinship}), where one wrong feedback can make the feasible space empty. 
This further indicates the robustness of our proposed policy improvement loss in Section \ref{sec:policy_improvement_loss}. 
}\DIFdelend 




\textbf{What will happen when the task has multiple modes (optimal actions)?}
\DIFdelbegin \DIFdel{As in Section \ref{sec:policy_improvement_loss}, to make it easy to illustrate the main idea, we made the assumption that there is only one $\bm a^*$ in the action space at a given state $\bm s$.
In theory, when there are multiple optimal actions $\bm a^*_i, i = 1, \dots, n_a$ , 
we assign one optimal action $\bm a^*_i$ to }\DIFdelend \DIFaddbegin \DIFadd{Our CLIC method currently cannot handle data with multiple modes because of the Gaussian distribution assumption of the policy in Section \ref{sec:Methodology}, and we require the teacher to give consistent corrective feedback during the experiments. 
Therefore, in }\DIFaddend the \DIFdelbegin \DIFdel{action pair $(\bm a^+, \bm a^-)$ and
Eq. 
~\eqref{eq:action_relatinship} is still valid if $(\bm a^*_i, \bm a^+, \bm a^-)$ are not contradict.
The same applies to Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples}.
Our proposition \ref{proposition:convex_feasible_space}
and \ref{proposition:open_convex} remain the same, while the optimal action used in proposition \ref{propositoin:alpha_beta} needs to be changed to $\bm a^*_i$.
The proposition \ref{proposition:convergences} remains the same as we can have the corresponding data buffer $\mathcal{D}_i$ to save the relative corrective feedback w.r.t $\bm a^*_i$, which doesn't influence the convergence of this algorithm.
However}\DIFdelend \DIFaddbegin \DIFadd{case of multiple modes, CLIC will converge to one of the modes under the teacher's guidance.
Besides}\DIFaddend , in practice, whether the algorithm can handle tasks with multiple modes depends on how we parameterize the policy. 
For the policy $\pi_{\theta}(\bm s,\bm a) \sim 
\mathcal{N}(\bm \mu_{\theta}(\bm s), \bm \Sigma)$, it can only learn one optimal action.
\DIFdelbegin \DIFdel{Therefore, we require the teacher to give consistent corrective feedback during the real-robot experiment.
Nevertheless, other types of policy can be used }\DIFdelend \DIFaddbegin \DIFadd{Nevertheless, we believe CLIC can be generalized to the multiple modes situation either explicitly \mbox{%DIFAUXCMD
\cite{2022_Behavior_transformer} }\hskip0pt%DIFAUXCMD
or implicitly }\DIFaddend \cite{2022_implicit_BC}, which is not the focus of this paper and we leave this \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend one of the future directions.
\DIFaddbegin 



%DIF > 




\DIFaddend \section{Conclusion} 
\label{sec:conclusion}

In this paper, we have presented a novel approach in the area of interactive imitation learning (IIL), which focuses on enhancing the way humans teach robots through relative corrective feedback. 
Our proposed CLIC method can utilize the information of the relative corrective feedback to shape the learner's policy efficiently and robustly.
The CLIC method leverages a policy improvement loss to learn from the comparison between positive and negative actions.
The experiments of our methods against established correction-based and demonstration-based IIL methods in various simulated environments have yielded promising results. 
Our method has demonstrated superiority over traditional correction-based approaches and has exhibited remarkable robustness in scenarios characterized by noisy feedback.
An important aspect of our research lies in identifying specific task domains where our methods surpass demonstration-based approaches. 
This includes high-frequency tasks that progress too rapidly for effective human demonstration, and multi-robot tasks, where the large action space exceeds the capabilities of human-led demonstrations. 
In these areas, our method demonstrates particular advantages. 
Importantly, even in scenarios where demonstration-based methods perform well, our approach achieves competitive results, underscoring its versatility and effectiveness across a broad spectrum of applications.

\DIFdelbegin \DIFdel{Our }\DIFdelend \DIFaddbegin \DIFadd{Despite the demonstrated effectiveness of CLIC in simulations and real-world experiments, there are limitations that future work can improve.
Firstly, our }\DIFaddend method shares a \DIFaddbegin \DIFadd{similar }\DIFaddend limitation with HG-DAgger in that it does not utilize data lacking corresponding human feedback. 
\DIFdelbegin \DIFdel{Integration }\DIFdelend \DIFaddbegin \DIFadd{The naive integration }\DIFaddend with approaches like those discussed in \cite{2022_Expert_Intervention_Learning}, \cite{2023_RSS_Robot_Learning_on_the_job} \DIFdelbegin \DIFdel{could potentially leverage such data . Additionally}\DIFdelend \DIFaddbegin \DIFadd{have been studied in Section \ref{sec:experiments} but failed to result in significant improvement.
 Secondly, as discussed in Section \ref{sec:discussion}, our CLIC method fails to learn from multi-modal data because of the assumption of the Gaussian distribution policy.
Thirdly}\DIFaddend , our method's reliance on learners' errors to provide 
 corrective feedback poses a challenge: if the learner initially performs well, the method may not effectively enhance the robot's policy. To address this, evaluative feedback could be integrated to reinforce successful actions.
Future research directions include 1) developing algorithms capable of utilizing data without human feedback; 2) creating algorithms for shaping policies suitable for multi-modal tasks; and 3) combining relative corrective feedback with evaluative or preference feedback to further improve the method's efficiency.




% 
\section*{Acknowledgments}
Omitted for Anonymous Review.



\bibliographystyle{plainnat}
\bibliography{references}

\newpage




















\section{Appendix}

\subsection{Simplification of the Policy Improvement Assumption}
\label{appendix:policy_improvement_loss}
We want the action from the learner's policy to have a high possibility of being inside $\mathcal{A}^{(\bm a^+, \bm a^-)}$.
Our assumption is that, at any given state, actions inside $\mathcal{A}^{(\bm a^+, \bm a^-)}$ are more likely to be selected over other actions by the teacher policy $\pi^*$.
Therfore, for the learner's action \DIFdelbegin \DIFdel{$\bm a \sim \pi^*(\bm s, \bm a)$}\DIFdelend \DIFaddbegin \DIFadd{$\bm a \sim \pi^*( \bm a | \bm s)$}\DIFaddend , the possibility density of selecting the action $\bm a \notin \mathcal{A}^{(\bm a^+, \bm a^-)}$ is smaller than the possibility density of selecting $\bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}$.
Formally, for the learner's action \DIFdelbegin \DIFdel{$\bm a \sim \pi^*(\bm s, \bm a)$}\DIFdelend \DIFaddbegin \DIFadd{$\bm a \sim \pi^*( \bm a | \bm s)$}\DIFaddend , we have
\begin{equation}
   \int \mathbb{P}( \bm a \notin \mathcal{A}^{(\bm a^+, \bm a^-)})  d\bm a \leq  \int \mathbb{P}( \bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}) d\bm a 
\end{equation}
As \DIFdelbegin \DIFdel{$\mathbb{P}( \bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}) = \pi^*(\bm s, \bm a)   \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-)\geq \mathbb{D}(\bm a, \bm a^+)} $}\DIFdelend \DIFaddbegin \DIFadd{$\mathbb{P}( \bm a \in \mathcal{A}^{(\bm a^+, \bm a^-)}) = \pi^*( \bm a | \bm s)   \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-)\geq \mathbb{D}(\bm a, \bm a^+)} $}\DIFaddend , we then have
\begin{equation*}
    \int  \pi^*( \bm \DIFdelbegin \DIFdel{s, }%DIFDELCMD < \bm %%%
\DIFdelend a \DIFaddbegin \DIFadd{| }\bm \DIFadd{s}\DIFaddend ) \left(  \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) < \mathbb{D}(\bm a, \bm a^+)}\right) d \bm a\geq 0
\end{equation*}
As $\mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) < \mathbb{D}(\bm a, \bm a^+)} = 1 - \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-)\geq \mathbb{D}(\bm a, \bm a^+)} $, we have 
\begin{equation}
    \int_{\bm a \in \mathcal{A}}  \pi^*( \bm \DIFdelbegin \DIFdel{s, }%DIFDELCMD < \bm %%%
\DIFdelend a \DIFaddbegin \DIFadd{| }\bm \DIFadd{s}\DIFaddend ) \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a\geq 0
\end{equation}

\DIFaddbegin \subsection{\DIFadd{Proof of the reduced policy improvement inequality with the Gaussian distribution assumption}}
\label{apppendix:reduce_policy_improvemnt_inequality_Gaussian_assumption}
\DIFadd{The conclusion we want to prove in this section is that, with the Gaussian distribution assumption, Eq. \eqref{eq:policy_improvement_general_case} can be simplified to Eq. \eqref{eq:policy_improvement}. 
Despite this simplification, by constraining the policy with Gaussian distribution assumption to satisfy  Eq. \eqref{eq:policy_improvement}, it also satisfies Eq. \eqref{eq:policy_improvement_general_case}, which is 
}\begin{align*}
\DIFadd{\pi^*( \bm a^{-} | \bm s) < \pi^*( \bm a^{+} | \bm s) , \pi^*(\bm a | \bm s) \sim\mathcal{N} (\mu^*(\bm s), \bm \Sigma) }\\
\DIFadd{\Rightarrow
    \int_{\bm a \in \mathcal{A}}  \pi^*(\bm a | \bm s)  \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a\geq 0 .
}\end{align*}


\begin{proof}
    \DIFadd{As $\pi^*(\bm a | \bm s)  \sim\mathcal{N} (\bm \mu^*(\bm s), \bm \Sigma)$, we have 
    }\begin{align}
    \DIFadd{\pi^*(\bm a | \bm s)  \propto \exp \left( }{\DIFadd{-\frac{1}{2}( \bm a-\bm \mu^*(\bm s))^{\mathsf{T}} }{\DIFadd{\bm \Sigma^{-1}}}\DIFadd{( \bm a-\bm \mu^*(\bm s))}} \DIFadd{\right), 
    \label{eq:gaussian}
}\end{align}
\DIFadd{Without loss of generality, we set $\bm \Sigma$ to the identity matrix. 
Then, from $\pi^*( \bm a^{-} | \bm s) < \pi^*( \bm a^{+} | \bm s)$, we have 
}\begin{equation*} 
\DIFadd{\| \bm a^{+}-\bm  \mu^*(\bm s) \|^2 < \| \bm a^{-}-\bm \mu^*(\bm s) \|^2,  
}\end{equation*}
\DIFadd{which means that $\bm mu^*(\bm s)$ is closer to $\bm a ^{+}$.
Now, we consider actions with the same distance to $\bm \mu^*(\bm s)$, which is $\bm a \in \mathcal{B}(p)= \{  \bm a \in \mathcal{A} | \|\bm a -\bm \mu^*(\bm s) \|^2 = p \}$.
The number of actions on this ball that belong to the desired action space is larger than that that do not belong to the desired space. 
Note that actions on the ball also have the same value of $\pi^*( \bm a | \bm s) $ by definition.
Therefore, we have
$\int_{\bm a \in \mathcal{B}(p) } \pi^*( \bm a | \bm s)\left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a\geq 0$
Thus, 
}\begin{align*}
    \DIFadd{\int_{\bm a \in \mathcal{A}}  \pi^*( \bm a | \bm s) \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a }\\
    \DIFadd{= \int_p \int_{\bm a \in \mathcal{B}(p) } \pi^*( \bm a | \bm s) \left( 2 \cdot \mathbf{1}_{\mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+)} - 1\right) d \bm a }\\
    \DIFadd{\geq 0
}\end{align*}



\end{proof}


\DIFaddend \subsection{Proof of Proposition 
\label{proof:proposotion_1}
\ref{proposition:convex_feasible_space}} 
Proposition \ref{proposition:convex_feasible_space}:
The subspace $\mathcal{A}^{(\bm a^+, \bm a^-)}$ includes the optimal action $\bm a^*$ and is unbounded.

\begin{proof}
    The feasible action space shaped by the action pair $(\bm  a^{+}, \bm a^{-})$ is defined as
    \begin{equation}
        \mathcal{A}^{(\bm a^+, \bm a^-)} = \{ \bm a \in \mathcal{A} | \mathbb{D}(\bm a, \bm a^-) \geq \mathbb{D}(\bm a, \bm a^+) \}
    \end{equation}
    By the definition of the action pair $(\bm  a^{+}, \bm a^{-})$ in Eq.~\eqref{eq:action_relatinship}, we can have $\bm a^* \in \mathcal{A}^{(\bm a^+, \bm a^-)}$.
    As $\mathbb{D}(\bm a_1, \bm a_2) = \| \bm a_1 - \bm a_2 \|^2$, $\mathcal{A}^{(\bm a^+, \bm a^-)}$ can be simplified to 
     \begin{equation*}
        \mathcal{A}^{(\bm a^+, \bm a^-)} = \{ \bm a \in \mathcal{A} | (\bm a^{+} - \bm a^{-})^T  \bm a \geq \frac{1}{2} (\bm a^{+} - \bm a^{-})^T(\bm a^{+} + \bm a^{-})  \},
    \end{equation*}
    which is a convex set. 
    We select $\bm a_1 = \frac{1}{2} (\bm a^{+} + \bm a^{-})  + \eta (\bm a^{+} - \bm a^{-}), \eta \rightarrow +\infty$. 
    We can show that $\bm a_1 \in  \mathcal{A}^{(\bm a^+, \bm a^-)} $ by
    \begin{align*}
        (\bm a^{+} - \bm a^{-})^T  \bm a_1 = \frac{1}{2} (\bm a^{+} - \bm a^{-})^T(\bm a^{+} + \bm a^{-}) + \eta  \| \bm a^{+} - \bm a^{-} \|^2 \\
        \geq \frac{1}{2} (\bm a^{+} - \bm a^{-})^T(\bm a^{+} + \bm a^{-})  
    \end{align*}
   $\| \bm a^1 \| \rightarrow +\infty$ as $\eta \rightarrow +\infty$. Therefore, $ \mathcal{A}^{(\bm a^+, \bm a^-)}$ is unbounded.
\end{proof}







\subsection{Proof of Proposition \ref{proposition:open_convex}} 
\label{proof:proposotion_2}
Proposition \ref{proposition:open_convex}:
The overall feasible space of one relative corrective feedback $\mathcal{A}^{[\bm s_t, \bm a_t, e\bm h_t]}$ enforced by Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples} is non-empty, convex, and unbounded. 
\begin{proof}
By proposition \ref{proposition:convex_feasible_space},
the feasible space enforced by one pair of action  $(\bm a^{I+}, \bm a^{I-}_i )$ is a convex set 
$\mathcal{A}^{(\bm a^{I+}, \bm a^{I-}_i )} = \{ \bm a \in \mathcal{A} |  \mathbb{D}(\bm a, \bm a^{I-}_i) \geq \mathbb{D}(\bm a,\bm a^{I+}) \}$, which can be simplified to  $ \mathcal{A}^{(\bm a^{I+}, \bm a^{I-}_i )} = \{ \bm a \in \mathcal{A} | (\bm a^{I+} - \bm a^{I-}_i)^T  \bm a \geq \frac{1}{2} (\bm a^{I+} - \bm a^{I-}_i)^T(\bm a^{I+} + \bm a^{I-}_i)  \}$.
As $\bm a^{I+} \in  \mathcal{A}^{(\bm a^{I+}, \bm a^{I-}_i )}$ for $i = 1, \dots, n$ and $\bm a^{I+} \in \mathcal{A}^{ (\bm a_t, \bm a_t + \epsilon e \bm h_t)}$,
we have $\bm a^{I+} \in \mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]} =\mathcal{A}^{ (\bm a_t, \bm a_t + \epsilon e \bm h_t)} \cap \mathcal{A}^{(\bm a^{I+}, \bm a^{I-}_1 )} \cap \cdots \cap \mathcal{A}^{(\bm a^{I+}, \bm a^{I-}_n )}$.
Therefore, the overall feasible space $\mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]} $ is a non-empty convex space.

To prove the feasible space is unbounded, we can select  $\bm a_2 =  \bm a^{I+} + \eta \bm h^{+}, \eta \rightarrow +\infty$.  
We can show that $\bm a_2 \in \mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]} $ by 
\begin{align}
&(\bm a^{I+} - \bm a^{I-}_i)^T  \bm a_2 - \frac{1}{2} (\bm a^{I+} - \bm a^{I-}_i)^T(\bm a^{I+} + \bm a^{I-}_i) \\  
    &= \| \bm a^{+} - \bm a^{-}_i \|^2 + (\bm a^{I+} - \bm a^{I-}_i)^T  \eta  \bm h^{+} \\
    &\geq (\bm a^{I+} - \bm a^{I-}_i)^T  \eta  \bm h^{+} \\
    &= \eta (\bm h^{+} - \bm h^{-}_i)^T  \bm h^{+} \\
    &=  \eta (1 - \cos{\alpha}) \geq 0
\end{align}
$\| \bm a^2 \| \rightarrow + \infty$ as $\eta \rightarrow + \infty$. Therefore, $\mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]} $ is unbounded.
\end{proof}

\subsection{Proof of Proposition \ref{propositoin:alpha_beta}}
\label{proof:proposotion_3}
Proposition \ref{propositoin:alpha_beta}:
The overall feasible space of one relative corrective feedback $\mathcal{A}^{[\bm s_t, \bm a_t, e\bm h_t]}$ enforced by Eq.~\eqref{eq:contrastive_constraint_original} and Eq.~\eqref{eq:contrastive_contrasint_samples}  includes the optimal actions by selecting the appropriate parameters $\alpha$ and $\varepsilon$.


\begin{proof}
    The optimal action $\bm a^*$ is defined as
    \begin{equation}
        \bm a^* = \bm a_t +  \bm h^*_t = \bm a_t + m \cdot e \widehat{\bm h}^*,
    \end{equation}
where $\widehat{\bm h}^* = \frac{\bm h^*_t}{\|\bm h^*_t\|}$ and $m = \frac{1 }{e}  \|\bm h^*_t\| \geq 1 $.

    To ensure $\exists \varepsilon, \alpha, \text{ s.t. } \bm a^* \in \mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]}$, we can solve  \begin{equation*}
    \exists \varepsilon, \alpha, \text{ s.t. }\forall i,(\bm a^{I+} - \bm a^{I-}_i)^T  \bm a ^* >\frac{1}{2} (\bm a^{I+} - \bm a^{I-}_i)^T   (\bm a^{I+} + \bm a^{I-}_i).
    \end{equation*}
    With Eq.~\eqref{eq:implicit_action_pairs}, we can simplify the problem to $\exists \varepsilon, \alpha, \text{ s.t. }\forall i,$
    \begin{equation*}
      2 (\bm h^{+} - \bm h^{-}_i)^T \widehat{\bm h}^* >  (\bm h^{+} - \bm h^{-}_i)^T((1+\varepsilon)  \bm h^{+} + (1-\varepsilon)  \bm h^{-}_i).
    \end{equation*}
    We set $\varepsilon = 0$, than we have the inequality   
   \begin{equation*}  
   2(\bm h^{+} - \bm h^{-})^T \widehat{\bm h}^*>  (\bm h^{+} - \bm h^{-})^T( \bm h^{+} +   \bm h^{-}_i) = 0.
    \end{equation*}
    With Eq.~\eqref{eq:assumption_corrective_feedback} and Eq.~\eqref{eq:assumption_negative_correction}, the inequality can be further simplified to 
    \begin{align*}
     \|\bm h^{+} \| \|\widehat{\bm h}^* \|  \cos{\angle(\bm h^{+}_i, \widehat{\bm h}^*)} > \|\bm h^{-} \| \|\widehat{\bm h}^*\|  \cos{\angle(\bm h^{-}_i,\widehat{\bm h}^*)} \\
    \Rightarrow \beta = \angle(\bm h^{+}_i,\widehat{\bm h}^*) < \angle(\bm h^{-}_i, \bm h^{*})
    \end{align*}
    As
    $\angle(\bm h^{-}_i, \bm h^{*}) = \alpha - \beta \text{ or } \alpha + \beta$,
    we have $\alpha \geq 2\beta$.

    Therefore, $\exists \varepsilon = 0, \alpha \geq 2\beta, \text{ s.t. } \bm a^* \in \mathcal{A}^{[\bm s_t, \bm a_t, \bm h_t]}$.
\end{proof}


\subsection{Proof of Proposition \ref{proposition:convergences}} 
\label{proof:proposotion_4}
Proposition \ref{proposition:convergences}:
As the number of relative corrective feedback approaches infinity, the feasible space $\mathcal{A}^{\mathcal{D}}$ enforced by the buffer $\mathcal{D}$ is bounded and converges to optimal actions.

\begin{proof}
For simplicity, we only consider one state $\bm s$, where the corresponding feedback received at this state $\bm s$ can be grouped into $\mathcal{D}(\bm s) = \{ [\bm s_t,\bm a_t, \bm h_t] \in \mathcal{D}, t=0, \dots  | \bm s_t = \bm s \}$. 
For other states, the proof is the same.
We consider at the $k$-th step, the overall feasible action space enforced by all the feedback inside $\mathcal{D}(\bm s)$ is denoted by $\mathcal{A}^{\mathcal{D}(s)}_k$.
At $k+1$ step, the learner takes an action from the feasible action space $\bm a_{k+1} \in \mathcal{A}^{\mathcal{D}(s)}_k$.
It will receive a correction $\bm h_{k+1}$ as long as $ \mathbb{D}( \bm a_{k+1}, \bm a^*) \geq \sigma$, where $\sigma$ is a small constant value.
We define the set of near-optimal actions as $\mathcal{A}^*(\bm s) = \{ \bm a \in \mathcal{A} | \mathbb{D}( \bm a_{k+1}, \bm a^*) < \sigma \}$

We show that as $k \rightarrow \infty$, $\mathcal{A}^{\mathcal{D}(s)}_k  \subset \mathcal{A}^*(\bm s)$.

We first demonstrate that a necessary condition for the  bounded $\mathcal{A}^{\mathcal{D}(s)}_k$ is the existence of two corrective vectors $\bm h_1$ and $\bm h_2$, which must satisfy the criterion $\angle(\bm h_1, \bm h_2) > \frac{\pi}{2}$.
Define the action pairs w.r.t $\bm h_1, \bm h_2$ as $(\bm a_1^-, \bm a_1^+)$, $(\bm a_2^-, \bm a_2^+)$, respectively.
Similar to the proof of proposition \ref{proposition:open_convex}, we define the action that has infinite norm in the feasible action space $\mathcal{A}^ {(\bm a_1^-, \bm a_1^+)}$  as 
\begin{equation}
    \bm a_1^\infty = \bm a_1^+ + \eta_1 \bm h_1 \in \mathcal{A}^ {(\bm a_1^-, \bm a_1^+)}
\end{equation}
As 
$\mathcal{A}^ {(\bm a_2^-, \bm a_2^+)}= \{ \bm a \in \mathcal{A} | \bm h_2^T  \bm a \geq \frac{1}{2} \bm h_2^T(\bm a^{+}_2 + \bm a^{I-}_2)  \}$, we assume $\bm a_1^\infty \in \mathcal{A}^ {(\bm a_2^-, \bm a_2^+)}$, which leads the intersected feasible space $\mathcal{A}^ {(\bm a_1^-, \bm a_1^+)} \cap \mathcal{A}^ {(\bm a_2^-, \bm a_2^+)}$ unbounded.
Therefore, we have 
\begin{align}
    \bm a_1^\infty \in \mathcal{A}^ {(\bm a_2^-, \bm a_2^+)} \Leftrightarrow \bm h_2^T  \bm a_1^\infty \geq \frac{1}{2} \bm h_2^T(\bm a^{+}_2 + \bm a^{I-}_2) \\
    \Leftrightarrow \bm h_2^T (\bm a_1^+ + \eta_1 \bm h_1 )\geq \frac{1}{2} \bm h_2^T(\bm a^{+}_2 + \bm a^{I-}_2) \\
    \Leftrightarrow \bm h_2^T (\bm a_1^+ - \bm a^{+}_2 - \bm a^{I-}_2) + \eta_1 \bm h_2^T \bm h_1 \geq 0 \\
    \Leftrightarrow \bm h_2^T (\bm a_1^+ - \bm a^{+}_2 - \bm a^{I-}_2) + \eta_1 \cos{\angle(\bm h_2, \bm h_1)} \geq 0 \\
    \Leftrightarrow \angle(\bm h_2, \bm h_1) \leq \frac{\pi}{2}
\end{align}
Therefore, by contradiction, we have:
a necessary condition for $\mathcal{A}^{\mathcal{D}(s)}_k$ to be bounded is that two corrective feedback, $\bm h_1$ and $\bm h_2$, exist with $\angle(\bm h_1, \bm h_2) > \frac{\pi}{2}$.

Then we show that space $\mathcal{A}^{\mathcal{D}(s)}_k$ is shrinking as $k\rightarrow \infty$.
At $k+1$ step, the action $\bm a_{k+1} \in \mathcal{A}^{\mathcal{D}(s)}_k$ receives a correction $\bm h_{k+1}$, which leads to an action pair $(\bm a^-_{k+1}, \bm a^+_{k+1}) = (\bm a_{k+1}, \bm a_{k+1} +  \varepsilon e \bm h_{k+1})$.
We have $\mathcal{A}^{\mathcal{D}(s)}_{k+1} = \mathcal{A}^{\mathcal{D}(s)}_k \cap \mathcal{A}^{(\bm a^-_{k+1}, \bm a^+_{k+1}) }$.
As $\bm a_{k+1} \in \mathcal{A}^{\mathcal{D}(s)}_k $ but 
$\bm a_{k+1} \notin \mathcal{A}^{\mathcal{D}(s)}_{k+1} $ as 
$\bm a_{k+1} \notin \mathcal{A}^{(\bm a^-_{k+1}, \bm a^+_{k+1}) } $, we can have 
\begin{equation}
    \mathcal{A}^{\mathcal{D}(s)}_{k+1} \subset \mathcal{A}^{\mathcal{D}(s)}_k
\end{equation}

To prove convergence, assume for contradiction that \( \mathcal{A}^{\mathcal{D}(s)}_k \) does not converge to \( \mathcal{A}^*(\bm s) \). 
This means there exists an \( \epsilon > 0 \) such that for all \( k \), there is an action \( \bm a_k \in \mathcal{A}^{\mathcal{D}(s)}_k \) with \( \mathbb{D}(\bm a_k, \bm a^*) \geq \sigma + \epsilon \). 
However, due to a teacher's existence, such \( \bm a_k \) would be removed from the feasible action space, contradicting the assumption. Therefore:
\begin{equation*} 
\forall \epsilon > 0, \exists K \text{ such that } \forall k > K, \forall \bm a \in \mathcal{A}^{\mathcal{D}(s)}_k, \mathbb{D}(\bm a, \bm a^*) < \sigma + \epsilon, 
\end{equation*}
which means  $\lim_{k\rightarrow \infty} \mathcal{A}^{\mathcal{D}(s)}_k  \subset \mathcal{A}^*(\bm s)$.
\end{proof}


\subsection{Implementation details}

\DIFdelbegin \DIFdel{For the policy $\pi_{\bm \theta}(\bm s,\bm a) \sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$, the }\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{Metaworld simulation experiment details}}
\label{appendix:metaworld_simulation_exp_details}
\DIFadd{For Meta-World tasks, each algorithm was run for 120 episodes in every experiment (except for the Assembly task with $75^\circ$ noise, the algorithms were run for 160 episodes), with these trials repeated 30 times to calculate average final success rates and convergence time steps. 
The success rate for each trial was calculated as the average of the last 25 episodes' success rates. 
We defined the convergence time step as the time step at which the success rate first exceeded 90\% of the final success rate.
}

\DIFadd{The specifics of each task are outlined as follows:
}\begin{enumerate}[label=\roman*), leftmargin=0.6cm]
  \item \textbf{\DIFadd{Drawer opening}}\DIFadd{: The robot must open a closed drawer, which is randomly positioned on a table at the start of each episode. The state space is 39 dimensional
, including the end effector's pose, the gripper's state, the object's pose, and the goal.
  }

  \item \textbf{\DIFadd{Hammer}}\DIFadd{: The robot must pick up a hammer, initially placed randomly on a table, and then use it to drive a nail into a designated spot.
  }

  \item \textbf{\DIFadd{Assembly}}\DIFadd{: The robot is tasked with grasping a circular, open-centered nut from the table and accurately placing it onto a column. The position of the nut is randomly initialized at the start of the episode.
}\end{enumerate}

\subsubsection{\DIFadd{Muli-line following task description}}
\label{appendix:multi_line_exp_details}
\DIFadd{Two point-mass robots, each allowed to move in a 2D space, must simultaneously follow two distinct lines, one line per robot. The action space of the policy consists of the combined action spaces of each robot, consisting of their 2-dimensional velocities.
The state includes the position of both robots, the distance of each robot to its starting and end points, and the shortest distance to its corresponding line. This task assumes a single feedback interface: at any given time step, feedback can only be applied to a single robot. This assumption is intended to mimic scenarios involving multiple robots or feedback channels where a human teacher cannot simultaneously provide feedback across the entire action space.
}

\subsubsection{\DIFadd{Network structure}}
\DIFadd{For the policy $\pi_{\bm \theta}( \bm a | \bm s) \sim \mathcal{N} (\bm \mu_{\bm \theta}(\bm s), \bm \Sigma)$, the }\DIFaddend mean $\bm \mu_{\bm \theta}(\bm s)$ is parameterized by a neural network consisting of two fully connected layers. The first layer contains 256 units with a tanh activation function, followed by a second layer with 128 units and a tanh activation. The final layer is a sigmoid output layer, which scales and shifts its output to produce values between -1 and 1. This model takes input state representations and maps them to policy outputs.
In practice, the learner's action is then obtained by the deterministic policy $\bm a_t = \bm \mu_{\bm \theta}(\bm s_t)$, where $\bm \Sigma$ can be chosen to make the action drawn from the Gaussian distribution close to its mean.
During the episode, the network parameters will be updated with the update frequency $b=5$ for all the algorithms except the original HG-DAgger, \DIFdelbegin \DIFdel{as explained in Section \ref{sub:section:HG-DAgger}}\DIFdelend \DIFaddbegin \DIFadd{which only updates the network parameters at the end of each episode}\DIFaddend .
The values of other hyperparameters for different methods are shown in Table \ref{table:appendx_Hyperparameters}.

\begin{table}[]
\caption{Hyperparameters for the algorithms}
\label{table:appendx_Hyperparameters}
\begin{center}
\begin{tabular}{l|l|c}
\hline
Algorithm                           & \multicolumn{1}{c|}{Hyperparameters}        & Value  \\ \hline
\multirow{3}{*}{CLIC-Correction}    & Magnitude parameter $e$  & 0.02   \\& Learning rate & 0.001  \\
                                    & End-of-episode training iterations & 1000   \\ \hline
\multirow{2}{*}{CLIC-Demonstration} & Learning rate                               & 0.0003 \\
                                    & End-of-episode training iterations & 1000   \\ \hline
\multirow{3}{*}{BD-COACH}           & Magnitude parameter $e$                       & 0.02   \\
                                    & Learning rate                               & 0.0003 \\
                                    & End-of-episode training iterations & 1000   \\ \hline
\multirow{2}{*}{Modified HG-DAgger} & Learning rate                               & 0.0003 \\
                                    & End-of-episode training iterations & 1000   \\ \hline
\multirow{2}{*}{Original HG-DAgger} & Learning rate                               & 0.0003 \\
                                    & End-of-episode training iterations & 2000   \\ \hline
\end{tabular}
\end{center}
\end{table}
\DIFaddbegin 

\subsection{\DIFadd{Strategies used in real-world tasks}}

\subsubsection{\DIFadd{Ball-catching task}}
\label{appendix:Ball-catching_task_strategy }
\DIFadd{We leveraged the CLIC method's ability to provide partial feedback within the entire action space, as discussed in section~\ref{sec:sub:sub:sub_actionspace}, to develop two strategies for generating high-quality data: firstly, teleoperation of the end-effector with corrections applied only to the soft hand;
and secondly, providing feedback on the entire action space to enhance the coordination between the end effector and the hand.
For the first strategy, we used teleoperation to guide the robot’s hand to a favorable position near the target area. It is crucial to note that most end-effector actions taken during this teleoperation phase are not required to be optimal; they are primarily used to put the soft hand in a good position where the chance of obtaining useful grasping data is high. These teleoperation actions of the end effector are not recorded in our data buffer. Instead, we only record the corrective feedback on the soft hand provided by the human operator during the teleoperation.
}







%DIF > 
 \DIFaddend 



\end{document}
