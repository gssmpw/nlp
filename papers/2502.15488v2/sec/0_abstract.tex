\begin{abstract}
% PETR-based methods have dominated benchmarks in 3D perception and are increasingly becoming a key component in modern autonomous driving systems. 
% However, their quantization performance significantly degrades when INT8 inference is required, 
% with a degradation of 58.2\% in mAP and 36.9\% in NDS on the NuScenes dataset. 
% To address this issue, we propose a quantization-aware position embedding transformation for multi-view 3D object detection, termed Q-PETR.
% Q-PETR offers a  deployment-friendly architecture while maintaining the original floating-point performance of PETR. 
% It substantially narrows the accuracy gap between INT8 and FP32 inference for PETR-series methods. 
% Without bells and whistles, our approach reduces the mAP and NDS drop to within 1\% under standard 8-bit per-tensor post-training quantization. 
% Furthermore, our method exceeds the performance of the original PETR in terms of floating-point precision. 
% Extensive experiments across a variety of PETR-series models demonstrate its broad generalization.


Camera-based multi-view 3D detection has emerged as an attractive solution for autonomous driving due to its low cost and broad applicability. However, despite the strong performance of PETR-based methods in 3D perception benchmarks, their direct INT8 quantization for onboard deployment leads to drastic accuracy dropsâ€”up to 58.2\% in mAP and 36.9\% in NDS on the NuScenes dataset. In this work, we propose Q-PETR, a quantization-aware position embedding transformation that re-engineers key components of the PETR framework to reconcile the discrepancy between the dynamic ranges of positional encodings and image features, and to adapt the cross-attention mechanism for low-bit inference. By redesigning the positional encoding module and introducing an adaptive quantization strategy, Q-PETR maintains floating-point performance with a performance degradation of less than 1\% under standard 8-bit per-tensor post-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR achieves a two-fold speedup and reduces memory usage by three times, thereby offering a deployment-friendly solution for resource-constrained onboard devices. Extensive experiments across various PETR-series models validate the strong generalization and practical benefits of our approach.
\vspace{-0.8cm}
\end{abstract}