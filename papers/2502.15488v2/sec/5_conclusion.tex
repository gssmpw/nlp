

\section{Conclusion}
% \vspace{-0.3cm}
In this paper, we address the significant performance drops of PETR models during quantization by identifying two main issues: the imbalance between positional encoding and image feature magnitudes, and uneven scalar dot-products in cross-attention. To resolve these, we introduce Q-PETR, a quantization-friendly positional encoding transformation that redesigns positional encoding and improves scalar dot-product quantization without sacrificing the original floating-point performance. We also propose DuLUT, a dual-table lookup mechanism for efficiently quantizing nonlinear functions, further enhancing deployment suitability on edge AI chips. Our experiments show that Q-PETR limits mAP and NDS drops to below 1\% under standard 8-bit post-training quantization and even surpasses the original PETR in floating-point precision. Extensive tests across various PETR models demonstrate the methodâ€™s strong generalization and deployment suitability.