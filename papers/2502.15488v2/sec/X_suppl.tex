\clearpage
\appendix
\setcounter{page}{1}
% \maketitlesupplementary
{\section*{Supplementary}\huge }

\section{Preliminaries}
\label{sec:petr_preliminaries}

\paragraph{PETR} enhances 2D image features with 3D position-aware properties using camera-ray positional encoding (PE), enabling refined query updates for 3D bounding box prediction. Specifically, surround-view images $\mathbf{I}$ pass through a backbone to generate 2D features $\mathbf{f}_{2D}$, while camera-ray PE $\mathbf{p}_c$ is computed using camera intrinsics and extrinsics. The learnable query embeddings $q$ serve as the initial queries $\mathbf{Q}$ for the decoder. Here, $\mathbf{f}_{2D}$ serves as the values $\mathbf{V}$, and adding $\mathbf{p}_c$ to $\mathbf{f}_{2D}$ element-wise forms the 3D position-aware keys $\mathbf{K}$.

The decoder updates the queries using these key-value pairs through self-attention, cross-attention, and feed-forward network (FFN) modules. The updated query vectors are passed through an MLP to predict 3D bounding box categories and attributes, repeating for $L$ cycles. The entire PETR process is summarized in Algorithm~\ref{algo:algorithm_petr}.
\begin{algorithm}[htb]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
\SetInd{1em}{1em} % <-- 关键：取消缩进
\footnotesize

\KwData{Surround-view images $\mathbf{I}$, camera intrinsics and extrinsics}
\KwResult{3D bounding boxes $\mathbf{b}^l$, categories $\mathbf{c}^l$ for $l = 1$ to $L$}

Compute image features: $\mathbf{f}_{2D} = \text{Backbone}(\mathbf{I})$\\
Compute camera-ray PE $\mathbf{p}_c$ using camera intrinsics and extrinsics\\
Form 3D position-aware keys: $\mathbf{K} = \mathbf{f}_{2D} + \mathbf{p}_c$ \tcp{Element-wise addition}
Set values: $\mathbf{V} = \mathbf{f}_{2D}$ \\
Initialize queries: $\mathbf{Q} = q$ (For simplicity, omit \(\mathbf{Q}\)'s encoding.)\\
\For{$l = 1$ to $L$}{
  $\mathbf{Q} \gets \texttt{QProj}(\mathbf{Q})$; 
  $\mathbf{K} \gets \texttt{KProj}(\mathbf{K})$; 
  $\mathbf{V} \gets \texttt{VProj}(\mathbf{V})$ \\ 
  $\mathbf{A}_s = \texttt{MultiHeadAtt}(\mathbf{Q}, \mathbf{Q}, \mathbf{Q})$ \tcp{Self-Attn}
  $\mathbf{A}_c = \texttt{MultiHeadAtt}(\mathbf{A}_s, \mathbf{K}, \mathbf{V})$ \tcp{Cross-Attn}
  $\mathbf{Q} \gets \texttt{FFN}(\mathbf{Q} + \mathbf{A}_c)$ \\
  $\mathbf{b}^{l} \gets \texttt{MLP}(\mathbf{Q})$; 
  $\mathbf{c}^{l} \gets \texttt{MLP}(\mathbf{Q})$ \\
}
\Return{$(\mathbf{b}^l, \mathbf{c}^l)$ for $l = 1$ to $L$}

\caption{Pseudo-code of PETR.}
\label{algo:algorithm_petr}
\end{algorithm}



\iffalse
\section{Quantization Failure of PETR}
\label{sec:quantization_failure_of_petr}

We evaluate the performance of several PETR configurations~\cite{liu2022petr} using the official code. Under standard 8-bit symmetric per-tensor post-training quantization (PTQ), PETR suffers significant performance degradation, with an average drop of 58.2\% in mAP and 36.9\% in NDS on the nuScenes validation dataset (see Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}). 

\begin{table}[htb] %{0.45\linewidth}
    %\tiny
    %\scriptsize
    \footnotesize
    \setlength{\tabcolsep}{1.4mm}
    %\small
    %\setlength{\tabcolsep}{2.5mm}
    %\normalsize
    %\large
    \centering
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule[1.5pt]
    \multirow{2}{*}{Bac} & \multirow{2}{*}{Size} & \multirow{2}{*}{Feat} & \multicolumn{2}{c|}{FP32 Acc}                               & \multicolumn{2}{c}{INT8 Acc}   \\ \cline{4-7}
     & & & \multicolumn{1}{c|}{mAP} & \multicolumn{1}{c|}{NDS} & \multicolumn{1}{c|}{mAP} & \multicolumn{1}{c}{NDS} \\ 
    \midrule
    \textcolor{white}{0}R50\textcolor{white}{0} & 1408$\times$512 & c5 & 30.5 & 35.0 & 18.4(12.1$\downarrow$) & 27.3(\textcolor{white}{0}7.7$\downarrow$) \\
    \textcolor{white}{0}R50\textcolor{white}{0} & 1408$\times$512 & p4 & 31.7 & 36.7 & 15.7(16.0$\downarrow$) & 26.1(10.6$\downarrow$) \\
    V2-99 & \textcolor{white}{0}800$\times$320 & p4 & 37.8 & 42.6 & 10.9(26.9$\downarrow$) & 23.6(19.0$\downarrow$) \\
    V2-99 & 1600$\times$640 & p4 & 40.4 & 45.5 & 11.3(29.1$\downarrow$) & 23.9(21.6$\downarrow$) \\
    \bottomrule[1.5pt]
    \end{tabular}
    \vspace{-0.3cm}
    \caption{
    PETR's performance of 3D object detection on nuSences val set, directly utilizing the pre-trained parameters from the official repository.
    }
    %\vspace{-0.5cm}
    \label{tab:performance_drop_for_ptq_on_raw_petr}
\end{table}

\iffalse
Enlightened by the existing state-of-the-art post-training quantization methods~\cite{dong2019hawq1,dong2020hawq2,hubara2021adaq,li2021brecq,liu2021ptqvit,nagel2020up,yao2021hawq3}, the adaptive rounding proxy objective is introduced to measure the quantization performance degradation:
\begin{equation}
\begin{aligned}
    \label{eqnAdaptiveEll}
    \textstyle
    \ell(\widetilde W)
    &=\mathbf{E}_x\left[ \Vert{ (W- \widetilde W) x \Vert}^2 \right] \\
    &=tr\left( (W - \widetilde W) H (W - \widetilde W)^T \right)
\end{aligned}
\end{equation}
Where $W$ is the float weight tensor of a learnable operator, $\widetilde W$ are the quantized weight, 
$x$ is an input tensor sampled randomly from a calibration set, 
and $H$ is the second moment matrix of these vectors, interpreted as a proxy Hessian.
\fi

\paragraph{Layer-wise Quantization Error Analysis.} Quantizing a pre-trained network introduces output noise, degrading performance. To identify the root causes of quantization failure, we employ the signal-to-quantization-noise ratio (SQNR), inspired by recent PTQ advancements~\cite{pandey2023practical, yang2023efficient, pagliari2023plinio}:

\begin{equation}\label{eq:sqnr}
SQNR_{q,b} = 10\log_{10} \left( \frac{ \sum_{i=1}^N \mathbb{E}[{\mathcal{F}}{\theta}(x_{i})^{2} ] }{ \sum_{i=1}^N \mathbb{E}[ e(x_{i})^{2} ] } \right)
\end{equation}

Here, $N$ is the number of calibration data points; $\mathcal{F}{\theta}$ denotes the full-precision network; the quantization error is $e(x_i) = \mathcal{F}{\theta}(x_i) - \mathcal{Q}{q,b}(\mathcal{F}{\theta}(x_i))$; and $\mathcal{Q}_{q,b}(\mathcal{F}_\theta)$ denotes the network output when only the target layer is quantized to $b$ bits, with all other layers kept at full precision.

Since 8-bit weight quantization results in only a minor loss of precision, we focus on quantization errors arising from operator inputs. Using the PETR configuration from the first row of Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}, we obtain layer-wise SQNRs, depicted in Fig.~\ref{fig:ASQNR}. From these results, we identify three main factors contributing to quantization errors:

\paragraph{Observation 1: Position Encoding Design Flaws Lead to Quantization Difficulties.}
Our in-depth analysis reveals that the quantization issues in PETR fundamentally stem from a design flaw in the positional encoding module, which manifests in two interrelated aspects. \textbf{(a) The inverse-sigmoid operator disrupts feature distribution balance.} As indicated by the red arrow in Fig.\ref{fig:ASQNR}, quantization difficulties stem from PETR's positional encoding module. Analyzing its construction (Fig.\ref{fig:pe_compare}(a)), we find that the inverse-sigmoid operation induces an imbalanced feature distribution. Specifically, Fig.~\ref{fig:distribution_before_and_after_insigmoid} shows that before applying inverse-sigmoid, the feature distribution is balanced and quantization-friendly, whereas afterward, it exhibits significant outliers. \textbf{(b) Magnitude Disparity between Camera-ray PE and Image Features.} As highlighted by the purple arrow in Fig.~\ref{fig:ASQNR}, applying 8-bit symmetric linear quantization to the 3D position-aware key $\mathbf{K}$ leads to significant performance degradation. To investigate this phenomenon, we conduct a statistical analysis of the magnitude distributions between image features and camera-ray positional encodings (PE). As illustrated in Fig.~\ref{fig:pe_img_compare}, both token-wise and channel-wise comparisons reveal that camera-ray PE exhibits an order-of-magnitude larger dynamic range (typically within $\pm$120) compared to image features (confined to $\pm$3). This severe imbalance creates a critical issue during quantization. When applying symmetric linear quantization with an 8-bit integer range (-128 to 127), the scaling factor \( s \) in Eq~\ref{e:eq2} becomes dominated by the extreme values of PE. Consequently, image features---occupying only ~2.5\% of the total dynamic range—are compressed into merely 7 discrete quantization bins (-3 to +3). As visualized in Fig.~\ref{fig:magnitude_distributions_of_image_feature_and_camera_ray_PE}, over 95\% of the original image feature variations collapse into the zero-centered bins, resulting in catastrophic information loss. This quantization artifact directly explains the observed performance drop in Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}. To mitigate this issue, we propose two essential modifications: 1) eliminating the inverse-sigmoid operation that exacerbates outlier magnitudes, and 2) redesigning the positional encoding architecture to align its magnitude distribution with that of image features. These adaptations ensure balanced quantization resolution allocation, preserving critical information in both PE and image features. To mitigate this issue, we propose two essential modifications: 1) eliminating the inverse-sigmoid operation that exacerbates outlier magnitudes, and 2) redesigning the positional encoding architecture to align its magnitude distribution with that of image features. These adaptations ensure balanced distribution for quantization, preserving critical information in both PE and image features.
% To address these challenges, it is imperative to avoid the inverse-sigmoid operation and to redesign the positional encoding so that its magnitude distribution aligns better with that of the image features, thereby enhancing overall quantization-friendliness.


\begin{figure}[htb]
\centering
	\includegraphics[width=0.8\linewidth]{./figs/coor3d_before_and_after_inversesigmoid.pdf}
    %\vspace{-0.5cm}
	\caption{Feature distribution before and after the inverse-sigmoid operator. Red arrows highlight outliers.}
	\label{fig:distribution_before_and_after_insigmoid}
\end{figure}
\vspace{-0.5cm}
\begin{figure}[htb]
\centering
	\includegraphics[width=0.75\linewidth]{./figs/pe_img_compare.pdf}
    %\vspace{-0.3cm}
	\caption{Magnitude Distribution of Image Features and Positional Encodings: A Token-wise and Channel-wise Comparison}
	\label{fig:pe_img_compare}
\end{figure}


\begin{figure}[htb]
\centering
	\includegraphics[width=0.8\linewidth]{./figs/img_feat_and_pe_distribution.pdf}
    \vspace{-0.3cm}
	\caption{The distributions of image features and camera-ray position encodings after symmetric quantization using the quantization parameters derived from the 3D position-aware $\mathbf{K}$.}
	\label{fig:magnitude_distributions_of_image_feature_and_camera_ray_PE}
\end{figure}
\vspace{-0.5cm}


\paragraph{Observation 2: Dual-Dimensional Heterogeneity in Cross-Attention Leads to Quantization Bottlenecks.}

As evidenced by the green arrow in Fig.~\ref{fig:ASQNR} and further clarified in Fig.~\ref{fig:scaled_dot_product}, the scaled dot-product in cross-attention exhibits pronounced heterogeneity on two levels. First, the inter-head variance spans 2–3 orders of magnitude, while within each head, the value distribution is extremely broad (e.g., ranging beyond [$-10^3$, $10^3$]). We merge the head and query dimensions to directly reveal the row-wise feature distribution. The results show that regardless of whether quantization is performed per head, per token, or on the entire tensor, the excessively large softmax inputs result in significant quantization errors. This confirms that existing quantization paradigms are fundamentally inadequate for handling the severe amplitude disparities in the cross-attention mechanism.



\begin{figure}[htb]
\centering
	\includegraphics[width=0.8\linewidth]{./figs/softmax_input.pdf}
    %\vspace{-0.3cm}
	\caption{The distributions of scaled dot-product in cross-attention. There are significant amplitude fluctuations along the head dimension.}
	\label{fig:scaled_dot_product}
\end{figure}


\fi

\section{Experimental Setup}\label{sec:exp_setup}
\textbf{Benchmark.}
We use the nuScenes dataset, a comprehensive autonomous driving dataset covering object detection, tracking, and LiDAR segmentation. The vehicle is equipped with one LiDAR, five radars, and six cameras providing a 360-degree view. The dataset comprises 1,000 driving scenes split into training (700 scenes), validation (150 scenes), and testing (150 scenes) subsets. Each scene lasts 20 seconds, annotated at 2 Hz.

\textbf{Metrics.}
Following the official evaluation protocol, we report the nuScenes Score (NDS), mean Average Precision (mAP), and five true positive metrics: mean Average Translation Error (mATE), Scale Error (mASE), Orientation Error (mAOE), Velocity Error (mAVE), and Attribute Error (mAAE).


\textbf{Experimental Details.}
Our experiments encompass both floating-point training and quantization configurations. For floating-point training, we follow PETR series settings, using PETR with an R50dcn backbone unless specified, and utilize the C5 feature (1/32 resolution output) as the 2D feature. Input images are at $1408 \times 512$ resolution. Both the lidar-ray PE and QD-aware lidar-ray PE use a pixel-wise depth of 30m with three anchor embeddings per axis. The 3D perception space is defined as $[-61.2, 61.2]$m along the X and Y axes, and $[-10, 10]$m along the Z axis. We also compare these positional encodings on StreamPETR, using a V2-99 backbone and input images of $800 \times 320$ resolution.

Training uses the AdamW optimizer (weight decay 0.01) with an initial learning rate of $2.0 \times 10^{-4}$, decayed via a cosine annealing schedule. We train for 24 epochs with a batch size of 8 on four NVIDIA RTX 4090 GPUs. No test-time augmentation is applied.

For quantization, we adopt 8-bit symmetric per-tensor post-training quantization, using 32 randomly selected training images for calibration. When quantizing the scaled dot-product in cross-attention, we define a candidate set of 20 scaling factors.


\section{Theoretical Analysis of Magnitude Bounds in Position Encodings}
\label{sec:mag_analysis}

\subsection{Normalization Framework and Input Conditioning}
\label{subsec:normalization}
To establish a unified analytical framework, we first formalize the spatial normalization process for various ray-based position encodings. Let $\mathbf{p} = (x, y, z)$ denote the 3D coordinates within the perception range $x, y \in [-51.2, 51.2]$ meters and $z \in [-5, 3]$ meters. The normalized coordinates $\mathbf{v} \in [0, 1]^3$ are computed as:

\begin{equation}
    \mathbf{v} = \left( \frac{x + 51.2}{102.4}, \frac{y + 51.2}{102.4}, \frac{z + 5.0}{8.0} \right)
    \label{eq:normalization}
\end{equation}

Noting that $\mathbf{v}$ is clamped to $\mathbf{v}_c$ within the range $[0, 1]$, the distribution ranges of the normalized sampled points in positional encodings are characterized as follows:
\begin{itemize}
    \item For the sampled point of Camera-Ray PE, denoted as $\mathbf{v}_c^{CR}$, the distribution spans the unit cube, i.e., $[0, 1] \times [0, 1] \times [0, 1]$.
    \item For the sampled points of LiDAR-Ray PE and QDPE, denoted as $\mathbf{v}_c^{LR}$ and $\mathbf{v}_c^{QD}$ respectively, the distributions are constrained to $[0, 0.79] \times [0, 0.79] \times [0, 1]$.
\end{itemize}
Here, the value $0.79$ is derived from the ratio $30/51.2$, where $30$ corresponds to the fixed depth setting in the encoding process. This distinction highlights the inherent differences in spatial coverage and normalization strategies employed by these positional encodings.

\subsection{Magnitude Propagation Analysis}
\label{subsec:magnitude_propagation}

\subsubsection{Camera-Ray Position Encoding}
\label{subsubsec:camera_pe}
As illustrated in Fig.~\ref{fig:pe_compare} (a), the encoding pipeline consists of two critical stages:

\textbf{Stage 1: Inverse Sigmoid Transformation}
\begin{equation}
    \hat{\mathbf{v}}^{CR} = \ln\left(\frac{\mathbf{v}_c^{CR} + \epsilon}{1 - (\mathbf{v}_c^{CR} + \epsilon)}\right), \quad \epsilon = 10^{-5}
    \label{eq:logit_transform_cr}
\end{equation}
Empirical analysis reveals a maximum magnitude $\eta_{\text{max}} = \max(\|\hat{\mathbf{v}}^{CR}\|_\infty) \approx 11.5$.

\textbf{Stage 2: MLP Projection} (Through Two Fully-Connected Layers)
\begin{equation}
    \text{PE}_{\text{CR}} = \mathbf{W}_2 \sigma(\mathbf{W}_1 \hat{\mathbf{v}}^{CR} + \mathbf{b}_1) + \mathbf{b}_2
    \label{eq:mlp_transform_cr}
\end{equation}
where $\sigma$ denotes the ReLU activation function. Let $\Gamma = \max(\|\mathbf{W}_1\|_{\max}, \|\mathbf{W}_2\|_{\max})$ be the maximum weight magnitude. We derive the upper bound:
\begin{equation}
    \| \text{PE}_{\text{CR}} \|_\infty \leq 256 \cdot 192 \cdot \Gamma^2 \cdot 11.5
    \label{eq:cr_bound}
\end{equation}
where $192$ and $256$ denote the input tensor channels for $\mathbf{W}_1$ and $\mathbf{W}_2$, respectively.

\subsubsection{LiDAR-Ray Position Encoding}
\label{subsubsec:lidar_pe}
Unlike Camera-Ray PE, the encoding process of LiDAR-Ray PE introduces sinusoidal modulation between the inverse sigmoid transformation and MLP projection, as shown in Fig.~\ref{fig:pe_compare} (b). The magnitude propagation for LiDAR-Ray PE is as follows:

\textbf{Stage 1: Inverse Sigmoid Transformation}
\begin{equation}
    \hat{\mathbf{v}}^{LR} = \ln\left(\frac{\mathbf{v}_c^{LR} + \epsilon}{1 - (\mathbf{v}_c^{LR} + \epsilon)}\right), \quad \epsilon = 10^{-5}
    \label{eq:logit_transform_lr}
\end{equation}
Empirical analysis reveals a maximum magnitude $\eta_{\text{max}} = \max(\|\hat{\mathbf{v}}^{LR}\|_\infty) \approx 1.8$.

\textbf{Stage 2: Spectral Embedding}
\begin{equation}
    \phi(\hat{\mathbf{v}}^{LR}) = \bigoplus_{k=1}^{32} \left[\sin(\omega_k \hat{\mathbf{v}}^{LR}), \cos(\omega_k \hat{\mathbf{v}}^{LR})\right]
    \label{eq:sinusoidal}
\end{equation}
where $\bigoplus$ denotes concatenation. This ensures:
\begin{equation}
    \| \phi(\hat{\mathbf{v}}^{LR}) \|_\infty \leq 1.0
    \label{eq:sin_bound}
\end{equation}

\textbf{Stage 3: MLP Projection} (Following setting in Camera-Ray PE)
\begin{equation}
    \| \text{PE}_{\text{LR}} \|_\infty \leq 256 \cdot 192 \cdot \Gamma^2 \cdot 1.0
    \label{eq:lr_bound}
\end{equation}

\subsubsection{Ours QD-PE}
\label{subsubsec:qd_pe}
The proposed encoding introduces anchor-based constraints, as depicted in Fig.~\ref{fig:pe_compare} (c):

\textbf{Stage 1: Anchor Interpolation} (For Each Axis $\alpha \in \{x, y, z\}$)
\begin{equation}
    \mathbf{e}_\alpha = \frac{p_\alpha - L_\alpha^i}{\Delta L_\alpha} \mathbf{E}_\alpha^{i+1} + \frac{L_\alpha^{i+1} - p_\alpha}{\Delta L_\alpha} \mathbf{E}_\alpha^i
    \label{eq:anchor_interp}
\end{equation}
where $\mathbf{E}_\alpha^i$ denotes learnable anchor embeddings.
Via Theorem~\ref{thm:anchor}, the magnitude is constrain to:
\begin{equation}
    \| \mathbf{e}_\alpha \|_\infty \leq \gamma \quad 
    \label{eq:anchor_bound}
\end{equation}

\textbf{Stage 2: MLP Projection}
\begin{equation}
    \| \text{PE}_{\text{QD}} \|_\infty \leq 256 \cdot 192 \cdot \Gamma^2 \cdot 0.8
    \label{eq:qd_bound}
\end{equation}

\subsection{Comparative Magnitude Analysis}
\label{subsec:comparative}
The derived bounds reveal fundamental differences in magnitude scaling:
\begin{align}
    \frac{\| \text{PE}_{\text{CR}} \|}{\| \text{PE}_{\text{LR}} \|} &\approx \frac{11.5}{1.0} = 11.5 
    \label{eq:ratio_lidar} \\
    \frac{\| \text{PE}_{\text{CR}} \|}{\| \text{PE}_{\text{QD}} \|} &\approx \frac{11.5}{0.8} = 14.3
    \label{eq:ratio_qd}
\end{align}
This analysis demonstrates that QD-PE requires $14\times$ less quantization range than Camera-Ray PE.

\subsection{Theoretical Guarantee of Magnitude Constraints}
\label{subsec:theorem}
\begin{theorem}[Anchor Embedding Magnitude Bound]
\label{thm:anchor}
Let $\mathbf{E}_\alpha^i, \mathbf{E}_\alpha^{i+1}$ be adjacent anchor embeddings with $\|\mathbf{E}_\alpha^i\|_\infty \leq \gamma$. For any point $p_\alpha \in [L_\alpha^i, L_\alpha^{i+1}]$, its interpolated embedding satisfies:
\begin{equation}
    \| \mathbf{e}_\alpha \|_\infty \leq \gamma
\end{equation}
\end{theorem}

\begin{proof}
Let $\lambda = \frac{p_\alpha - L_\alpha^i}{\Delta L_\alpha} \in [0, 1]$. The interpolated embedding becomes:
\begin{equation}
    \mathbf{e}_\alpha = \lambda \mathbf{E}_\alpha^{i+1} + (1 - \lambda) \mathbf{E}_\alpha^i
\end{equation}
For any component $k$:
\begin{equation}
    |e_{\alpha,k}| \leq \lambda |E_{\alpha,k}^{i+1}| + (1 - \lambda) |E_{\alpha,k}^i| \leq \lambda \gamma + (1 - \lambda) \gamma = \gamma
\end{equation}
Thus, $\|\mathbf{e}_\alpha\|_\infty \leq \gamma$ holds for all dimensions.
\end{proof}
Through the application of regularization (e.g., L2 constraint) on the anchor embeddings $\mathbf{E}_\alpha^i$
during training, the magnitude of $\gamma$ can be explicitly controlled.
Empirically, we find that this value converges to approximately 0.8 in our experiments.


\section{More Ablation Study}\label{sec:more_ablation}

\begin{figure*}[htb]
\centering
	\includegraphics[width=0.8\linewidth]{./figs/pe_visual.pdf}
    %\vspace{-0.8cm}
	\caption{Qualitative comparison of the local similarity.}
	\label{fig:pe_visual_compare}
\end{figure*}

\subsection{Local Similarity of Position Encoding Features}
%\textbf{Local Similarity of Position Encoding Features.} 
Fig.~\ref{fig:pe_visual_compare} shows that QD-PE significantly outperforms 3D point PE and cameraray PE in local similarity of position encoding. Its similarity distribution appears more compact and concentrated, validating the method's superiority in local spatial information modeling and its capability to precisely capture neighborhood spatial relationships around target pixels.
% is a qualitative comparison of QD-PE, 3D point PE, and cameraray PE from the back perspective of a 3D surround-view system, focusing on the local similarity of position encoding features. The red box in the top row marks a selected pixel, and the similarity maps illustrate how each method encodes spatial relationships around that pixel. A more concentrated similarity distribution indicates a stronger position encoding. As shown, our method achieves a more compact and focused similarity, highlighting its effectiveness in capturing local spatial information compared to other approaches.

\section{Limitations}
Although our method incurs almost no quantization accuracy loss, 
users need to replace the camera-ray in the original PETR series with our proposed QDPE. 
The only drawback is that this requires retraining. 
However, from the perspective of quantization deployment,
this retraining is beneficial, 
and the floating-point precision can even be improved.
