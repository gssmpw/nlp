\vspace{-2mm}
\section{Related Work}
\label{sec:related}

\textbf{Multi-View 3D Object Detection.}
Surround-view 3D object detection is essential for autonomous driving and is generally categorized into LSS-based~\cite{bevdet,bevdepth,bevdet4d} and transformer-based~\cite{liu2022petr,shu20233DPPE} approaches. LSS-based methods project multi-camera features onto dense BEV (Bird's Eye View) representations~\cite{lss}, but their high memory consumption hinders efficient long-range perception. Transformer-based methods leverage sparsity to enhance long-distance perception. Among these, the PETR series has gained significant attention. PETR~\cite{liu2022petr} transforms 2D image features into 3D representations using 3D positional encoding. PETRv2~\cite{liu2022petrv2} introduces temporal feature indexing, while StreamPETR~\cite{streampetr} extends temporal query processing. Some works~\cite{wang2022focal, wang2023object, chu2024rayformer} accelerate processing by incorporating 2D detection priors. CMT~\cite{cmt} fuses vision and LiDAR point clouds. Improvements to PETR's positional encoding have also been explored~\cite{shu20233DPPE, hou2024open}. Additionally, PETR has been integrated into the Omnidrive framework~\cite{wang2024omnidrive} to enhance 3D perception with large models.

\textbf{Quantization.}
Quantization compresses models by converting weights and activations from floating-point to lower-bit integer representations~\cite{PACT_arxiv2018,LQNets_eccv2018,PTQ_4bit_rapid_deployment_nips2019,Low_bit_quant_iccvw2019}. Among various methods~\cite{shao2023omniquant,wei2022outlier,liu2024spinquant,ashkboos2024slicegpt,ashkboos2024quarot,xiao2023smoothquant}, we focus on uniform symmetric quantization, mapping floating-point values $x_f$ to discrete $k$-bit integer values $x_q$ as:
\begin{equation}\label{e:eq1}
x_q = \text{clamp}\left( \left\lfloor \frac{x_f}{s} \right\rceil, -2^{k-1}, 2^{k-1}-1 \right),
\end{equation}
where $s$ is the scaling factor computed as:
\begin{equation}\label{e:eq2}
\vspace{-3mm}
s = \frac{x_f^{\text{max}} - x_f^{\text{min}}}{2^k},
\end{equation}
with $x_f^{\text{max}}$ and $x_f^{\text{min}}$ being the maximum and minimum floating-point values from the calibration dataset. Quantization methods are categorized into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT~\cite{esser2019learned,bhalgat2020lsq+} introduces quantization-aware losses during training, enhancing robustness but requiring resource-intensive retraining. Compare to QAT, PTQ offers rapid deployment without retraining. While PTQ methods have been successful on CNNs~\cite{nagel2019data,nagel2020up,li2021brecq}, they often perform poorly on transformer-based 3D detectors due to structural differences. For ViTs, practical PTQ algorithms have been developed~\cite{yuan2022ptq4vit,lin2021fq,tai2023tsptq,li2023repq}. In the context of transformer-based object detection models, Q-DETR~\cite{xu2023q} and AQ-DETR~\cite{wang2024aq} use QAT and knowledge distillation to mitigate performance degradation in low-bit quantization of DETR models. These methods primarily focus on quantizing GEMM operations. For nonlinear activation functions, lookup table (LUT) techniques~\cite{wang2018look} are commonly used. Additionally, methods like I-BERT~\cite{kim2021bert} and I-ViT~\cite{li2023vit} employ integer approximation to achieve fixed-point computation.

\textbf{Quantization for 3D Object Detection.}
Quantization methods have been applied to accelerate 3D object detection in autonomous driving and robotics. Leveraging advances in image quantization, QD-BEV~\cite{zhang2023qd} employs QAT and distillation in multi-camera 3D detection, achieving smaller models and faster inference than the \textit{BEVFormer} baseline~\cite{li2022bevformer}. For LiDAR-based detection, LIDAR-PTQ~\cite{zhou2024lidar} achieves state-of-the-art quantization on \textit{CenterPoint}~\cite{centerpoint}, with performance close to FP32 and 3$\times$ speedup. To our knowledge, there are no PTQ solution tailored for transformer-based 3D detection in autonomous driving.

