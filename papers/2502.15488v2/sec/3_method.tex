\section{Quantization and Deployment-Friendly Adaptation of PETR}
\label{sec:method}

In this section, we aim to improve PETR's quantization performance. 
We begin by elaborating the principles of PETR in Sup. \ref{sec:petr_preliminaries}, 
identify its quantization failures (\S\ref{sec:quantization_failure_of_petr}), and provide strategies to address these challenges (\S\ref{sec:3.3}).


\begin{figure*}
\centering
	\includegraphics[width=0.8\linewidth]{./figs/sqnr.pdf}
    \vspace{-0.3cm}
	\caption{The layer-wise SNQR for classification and regression respectively. For clarity in the illustration, the layers in backbone are omitted, as its quantization does not cause any performance degradation.}
	\label{fig:ASQNR}
    \vspace{-0.5cm}
\end{figure*}


% \iffalse
\subsection{Quantization Failure of PETR}
\label{sec:quantization_failure_of_petr}

% We have identified two phenomena that lead to the failure of PETR quantization:
% \paragraph{Observation 1: Position Encoding Design Flaws Lead to Quantization Difficulties.}
% The design flaws in the encoding can be further attributed to:
% (a) The inverse-sigmoid operator disrupts the balance of feature distribution.
% (b) Magnitude disparity between camera-ray positional encoding (PE) and image features.
% \paragraph{Observation 2: Dual-Dimensional Heterogeneity in Cross-Attention Leads to Quantization Bottlenecks.}
% For a detailed analysis, please refer to Supp. \ref{}.
% % \fi
% \section{Quantization Failure of PETR}
% \label{sec:quantization_failure_of_petr}

We evaluate the performance of several PETR configurations~\cite{liu2022petr} using the official code. Under standard 8-bit symmetric per-tensor post-training quantization (PTQ), PETR suffers significant performance degradation, with an average drop of 58.2\% in mAP and 36.9\% in NDS on the nuScenes validation dataset (see Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}). 

\begin{table}[htb] %{0.45\linewidth}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \footnotesize
    \begin{tabular}{l|c|c|c|c|c|c}
    % \toprule[1.5pt]
    \hline
    \multirow{2}{*}{Bac} & \multirow{2}{*}{Size} & \multirow{2}{*}{Feat} & \multicolumn{2}{c|}{FP32 Acc}                               & \multicolumn{2}{c}{INT8 Acc}   \\ \cline{4-7}
     & & & \multicolumn{1}{c|}{mAP} & \multicolumn{1}{c|}{NDS} & \multicolumn{1}{c|}{mAP} & \multicolumn{1}{c}{NDS} \\ 
    \midrule
    \textcolor{white}{0}R50\textcolor{white}{0} & 1408$\times$512 & c5 & 30.5 & 35.0 & 18.4(12.1$\downarrow$) & 27.3(\textcolor{white}{0}7.7$\downarrow$) \\
    \textcolor{white}{0}R50\textcolor{white}{0} & 1408$\times$512 & p4 & 31.7 & 36.7 & 15.7(16.0$\downarrow$) & 26.1(10.6$\downarrow$) \\
    V2-99 & \textcolor{white}{0}800$\times$320 & p4 & 37.8 & 42.6 & 10.9(26.9$\downarrow$) & 23.6(19.0$\downarrow$) \\
    V2-99 & 1600$\times$640 & p4 & 40.4 & 45.5 & 11.3(29.1$\downarrow$) & 23.9(21.6$\downarrow$) \\
    % \bottomrule[1.5pt]
    \hline
    \end{tabular}}
    \vspace{-0.2cm}
    \caption{
    PETR's performance of 3D object detection on nuSences, utilizing the pre-trained parameters from the official repository.
    }
    %\vspace{-0.5cm}
    \label{tab:performance_drop_for_ptq_on_raw_petr}
    \vspace{-8mm}
\end{table}

\iffalse
Enlightened by the existing state-of-the-art post-training quantization methods~\cite{dong2019hawq1,dong2020hawq2,hubara2021adaq,li2021brecq,liu2021ptqvit,nagel2020up,yao2021hawq3}, the adaptive rounding proxy objective is introduced to measure the quantization performance degradation:
\begin{equation}
\begin{aligned}
    \label{eqnAdaptiveEll}
    \textstyle
    \ell(\widetilde W)
    &=\mathbf{E}_x\left[ \Vert{ (W- \widetilde W) x \Vert}^2 \right] \\
    &=tr\left( (W - \widetilde W) H (W - \widetilde W)^T \right)
\end{aligned}
\end{equation}
Where $W$ is the float weight tensor of a learnable operator, $\widetilde W$ are the quantized weight, 
$x$ is an input tensor sampled randomly from a calibration set, 
and $H$ is the second moment matrix of these vectors, interpreted as a proxy Hessian.
\fi

\paragraph{Layer-wise Quantization Error Analysis.} Quantizing a pre-trained network introduces output noise, degrading performance. To identify the root causes of quantization failure, we employ the signal-to-quantization-noise ratio (SQNR), inspired by recent PTQ advancements~\cite{pandey2023practical, yang2023efficient, pagliari2023plinio}:
\vspace{-3mm}
\begin{equation}\label{eq:sqnr}
\begin{aligned}
SQNR_{q,b} = 10\log_{10} \left( \frac{ \sum_{i=1}^N \mathbb{E}[{\mathcal{F}}{\theta}(x_{i})^{2} ] }{ \sum_{i=1}^N \mathbb{E}[ e(x_{i})^{2} ] } \right)
\end{aligned}
\end{equation}

Here, $N$ is the number of calibration data points; $\mathcal{F}{\theta}$ denotes the full-precision network; the quantization error is $e(x_i) = \mathcal{F}{\theta}(x_i) - \mathcal{Q}{q,b}(\mathcal{F}{\theta}(x_i))$; and $\mathcal{Q}_{q,b}(\mathcal{F}_\theta)$ denotes the network output when only the target layer is quantized to $b$ bits, with all other layers kept at full precision.

Since 8-bit weight quantization results in only a minor loss of precision, we focus on quantization errors arising from operator inputs. Using the PETR configuration from the first row of Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}, we obtain layer-wise SQNRs, depicted in Fig.~\ref{fig:ASQNR}. From these results, we identify three main factors contributing to quantization errors:

% \paragraph{Observation 1: Position Encoding Design Flaws Lead to Quantization Difficulties.}
% Our in-depth analysis reveals that the quantization issues in PETR fundamentally stem from a design flaw in the positional encoding module, which manifests in two interrelated aspects. \textbf{(a) The inverse-sigmoid operator disrupts feature distribution balance.} As indicated by the red arrow in Fig.\ref{fig:ASQNR}, quantization difficulties stem from PETR's positional encoding module. Analyzing its construction (Fig.\ref{fig:pe_compare}(a)), we find that the inverse-sigmoid operation induces an imbalanced feature distribution. Specifically, Fig.~\ref{fig:distribution_before_and_after_insigmoid} shows that before applying inverse-sigmoid, the feature distribution is balanced and quantization-friendly, whereas afterward, it exhibits significant outliers. \textbf{(b) Magnitude Disparity between Camera-ray PE and Image Features.} As highlighted by the purple arrow in Fig.~\ref{fig:ASQNR}, applying 8-bit symmetric linear quantization to the 3D position-aware key $\mathbf{K}$ leads to significant performance degradation. To investigate this phenomenon, we conduct a statistical analysis of the magnitude distributions between image features and camera-ray positional encodings (PE). As illustrated in Fig.~\ref{fig:pe_img_compare}, both token-wise and channel-wise comparisons reveal that camera-ray PE exhibits an order-of-magnitude larger dynamic range (typically within $\pm$120) compared to image features (confined to $\pm$3). This severe imbalance creates a critical issue during quantization. When applying symmetric linear quantization with an 8-bit integer range (-128 to 127), the scaling factor \( s \) in Eq~\ref{e:eq2} becomes dominated by the extreme values of PE. Consequently, image features---occupying only ~2.5\% of the total dynamic range—are compressed into merely 7 discrete quantization bins (-3 to +3). As visualized in Fig.~\ref{fig:magnitude_distributions_of_image_feature_and_camera_ray_PE}, over 95\% of the original image feature variations collapse into the zero-centered bins, resulting in catastrophic information loss. This quantization artifact directly explains the observed performance drop in Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}. To mitigate this issue, we propose two essential modifications: 1) eliminating the inverse-sigmoid operation that exacerbates outlier magnitudes, and 2) redesigning the positional encoding architecture to align its magnitude distribution with that of image features. These adaptations ensure balanced quantization resolution allocation, preserving critical information in both PE and image features.
\paragraph{Observation 1: Position Encoding Design Flaws Lead to Quantization Difficulties.}
We find that PETR’s quantization issues primarily arise from its positional encoding module in two key ways. 
\textbf{(a) Inverse-sigmoid disrupts feature balance.} As shown in Fig.~\ref{fig:ASQNR}, the inverse-sigmoid operation skews an otherwise balanced distribution (Fig.~\ref{fig:distribution_before_and_after_insigmoid}) toward significant outliers. 
\textbf{(b) Magnitude disparity between camera-ray PE and image features.} As highlighted by the purple arrow in Fig.~\ref{fig:ASQNR}, applying 8-bit quantization to the 3D position-aware key $\mathbf{K}$ yields severe performance drops. Statistical analysis (Fig.~\ref{fig:pe_img_compare}) shows that camera-ray PE spans approximately $\pm120$, while image features remain within $\pm3$. Consequently, when using Eq.~\ref{e:eq2} and an 8-bit symmetric range of $[-128,127]$, PE dominates the scaling factor. Image features then collapse into merely seven bins (Fig.~\ref{fig:magnitude_distributions_of_image_feature_and_camera_ray_PE}), causing catastrophic information loss and sharp accuracy degradation (Table~\ref{tab:performance_drop_for_ptq_on_raw_petr}). To address these flaws, we (1) remove the inverse-sigmoid step that drives outlier magnitudes, and (2) redesign the positional encoding to align its scale with that of image features. This balanced approach preserves critical information during quantization.
% To address these challenges, it is imperative to avoid the inverse-sigmoid operation and to redesign the positional encoding so that its magnitude distribution aligns better with that of the image features, thereby enhancing overall quantization-friendliness.


\begin{figure}[htb]
\vspace{-3mm}
\centering
	\includegraphics[width=1.0\linewidth]{./figs/coor3d_before_and_after_inversesigmoid.pdf}
    \vspace{-0.6cm}
	\caption{Distribution before and after inverse-sigmoid operator.}
	\label{fig:distribution_before_and_after_insigmoid}
    \vspace{-0.3cm}
\end{figure}

%\vspace{-0.5cm}
\begin{figure}[htb]
\vspace{-2mm}
\centering
	\includegraphics[width=1.0\linewidth]{./figs/pe_img_compare.pdf}
    \vspace{-0.6cm}
	\caption{Magnitude Distribution of Image Features and Positional Encodings: A Token-wise and Channel-wise Comparison}
	\label{fig:pe_img_compare}
    \vspace{-0.3cm}
\end{figure}


\begin{figure}[htb]
\vspace{-2mm}
\centering
	\includegraphics[width=0.8\linewidth]{./figs/img_feat_and_pe_distribution.pdf}
    \vspace{-0.3cm}
	\caption{The distributions of image features and camera-ray position encodings after symmetric quantization using the quantization parameters derived from the 3D position-aware $\mathbf{K}$.}
    \label{fig:magnitude_distributions_of_image_feature_and_camera_ray_PE}
    \vspace{-0.5cm}
\end{figure}


\paragraph{Observation 2: Dual-Dimensional Heterogeneity in Cross-Attention Leads to Quantization Bottlenecks.}

As evidenced by the green arrow in Fig.~\ref{fig:ASQNR} and further clarified in Fig.~\ref{fig:scaled_dot_product}, the scaled dot-product in cross-attention exhibits pronounced heterogeneity on two levels. First, the inter-head variance spans 2–3 orders of magnitude, while within each head, the value distribution is extremely broad (e.g., ranging beyond [$-10^3$, $10^3$]). We merge the head and query dimensions to directly reveal the row-wise feature distribution. The results show that regardless of whether quantization is performed per head, per token, or on the entire tensor, the excessively large softmax inputs result in significant quantization errors. This confirms that existing quantization paradigms are fundamentally inadequate for handling the severe amplitude disparities in the cross-attention mechanism.



\begin{figure}[htb]
\centering
	\includegraphics[width=1.0\linewidth]{./figs/softmax_input.pdf}
    \vspace{-0.6cm}
	\caption{Distributions of scaled dot-product in cross-attention. There are significant amplitude fluctuations along head dimension.}
	\label{fig:scaled_dot_product}
    \vspace{-0.3cm}
\end{figure}



\begin{figure*}[htb]
\centering
	\includegraphics[width=0.8\linewidth]{./figs/pe_comparsion.pdf}
    \vspace{-0.3cm}
	\caption{
 The overall architecture comparison of camera-ray PE, lidar-ray PE and our QD-aware lidar-ray PE.
	}
	\label{fig:pe_compare}
\end{figure*}
\vspace{-2mm}


\subsection{Quantization and Deployment Friendly Improvement.}
\label{sec:3.3}

% Based on the analysis in Section~\ref{sec:quantization_failure_of_petr}, we identify two key areas for improvement: (1) designing a positional encoding with magnitude and distribution similar to image features, and (2) balancing the scaled dot-products in cross-attention across heads. We propose specific methods to address these challenges respectively.
Drawing on the analysis in Section~\ref{sec:quantization_failure_of_petr} and the deployment challenges noted in the Introduction, we identify three critical issues. \textbf{Firstly}, the positional encoding module mismatches the magnitude and distribution of image features, causing severe quantization loss. \textbf{Secondly}, imbalanced scaled dot-products in cross-attention further compound quantization errors. \textbf{Thirdly}, the high computational cost of nonlinear functions impedes efficient edge inference. We propose targeted solutions for above challenges.

\vspace{-6mm}
\paragraph{Positional Encoding Adaptation.}
% To address the excessive magnitude of positional encoding in PETR, we conduct a statistical analysis of its construction mechanism. As shown in Fig. \ref{fig:pe_compare} (a), PETR samples 64 3D points along each camera ray, aggregating features through nonlinear transformations containing inverse-sigmoid and sinusoidal operations. According to Theorem \ref{thm:pe} in Appendix, the variance of positional encoding grows linearly with the number of sampled 3D points under independence assumption, and super-linearly with positive correlations – a critical issue given the inherent spatial correlations among ray samples.
% When analyzing the positional encoding in PETR, we find that the root cause of its excessively large amplitude lies in the design of the encoding itself. As shown in Fig.~\ref{fig:pe_compare}(a), PETR samples 64 3D points along each camera ray and aggregates the features through nonlinear transformations involving inverse-sigmoid and sinusoidal functions. According to Theorem~\ref{thm:pe} in the Appendix, from the amplitude-bound perspective, if we assume these sampling points are independent, the amplitude of the positional encoding increases linearly with the number of sampled 3D points; moreover, it grows super-linearly when there are positive correlations. Due to inherent spatial correlations among adjacent samples on each ray, the amplitude magnification effect is particularly pronounced. Consequently, the positional encodings in PETR can be far larger than the image features, posing significant challenges for subsequent quantization and inference deployment.

From the derivations in Appendix~\ref{subsec:magnitude_propagation}, we establish that the amplitude of camera-ray PE can theoretically reach up to 11.5 times that of its LiDAR-based counterpart. This stark discrepancy directly explains why PETR’s camera-ray encoding often overshoots the dynamic range of image features, thereby hampering quantization. Although LiDAR-ray PE alleviates the amplitude issue, its reliance on high-frequency sinusoidal functions remains problematic for low-bit deployments on edge devices. 

To overcome both amplitude and implementation obstacles, we propose a \textbf{q}uantization-\textbf{d}eployable LiDAR-ray \textbf{p}osition \textbf{e}mbedding (QDPE) that sharply curtails magnitudes while avoiding complex nonlinearities. Our design contains two main modifications: 
\begin{enumerate}
    \item (Single-point sampling via LiDAR prior). 
    Drawing inspiration from the physical properties of LiDAR sensors, we sample only one 3D point per pixel along each depth ray (Fig.~\ref{fig:pe_compare}~(b)), in contrast to the multi-sample scheme in PETR’s camera-ray PE. By discarding the iterative inverse-sigmoid and sinusoidal transformations, we reduce the overall encoding variance.

    \item (Anchor-based bounded embedding with convex-combination constraints). 
    As illustrated in Fig.~\ref{fig:pe_compare}~(c), we learn three axis-aligned anchor embeddings $\{E_\alpha^i\}_{i=1}^3$ for each spatial axis $\alpha \in \{x,y,z\}$, with corresponding anchor locations $\{L_\alpha^i\}_{i=1}^3$. For a LiDAR-sampled 3D point $(x_j,y_j,z_j)$, we compute the embedding along each axis by linear interpolation between the nearest two anchors:
    \begin{equation}
    \begin{aligned}
        e_x^j &= \frac{x_j - L_x^{i_x}}{L_x^{i_x+1} - L_x^{i_x}}\,E_x^{i_x+1} 
        \;+\;\frac{L_x^{i_x+1} - x_j}{L_x^{i_x+1} - L_x^{i_x}}\,E_x^{i_x}, \\ 
        e_y^j &= \frac{y_j - L_y^{i_y}}{L_y^{i_y+1} - L_y^{i_y}}\,E_y^{i_y+1} 
        \;+\;\frac{L_y^{i_y+1} - y_j}{L_y^{i_y+1} - L_y^{i_y}}\,E_y^{i_y}, \\ 
        e_z^j &= \frac{z_j - L_z^{i_z}}{L_z^{i_z+1} - L_z^{i_z}}\,E_z^{i_z+1} 
        \;+\;\frac{L_z^{i_z+1} - z_j}{L_z^{i_z+1} - L_z^{i_z}}\,E_z^{i_z}.
    \end{aligned}
    \end{equation}
    Theorem~1 guarantees that each axis-wise component $e_\alpha^j$ is strictly confined within the convex hull of its adjacent anchors. We concatenate the three axis-wise embeddings and feed them to a lightweight MLP to obtain the final positional encoding vector.
\end{enumerate}

These two innovations ensure our QD-aware LiDAR-ray PE remains both bounded in magnitude and free of difficult-to-quantize nonlinearities. Fig.~\ref{fig:pe_img_compare} and Fig.~\ref{fig:cameraPE_and_lidarPE} visually demonstrate the elimination of outliers. Further, the dynamic range of our QD-aware encoding ($\pm 29.7$) is only marginally wider than that of standard image features ($\pm 3.4$)—in stark contrast to PETR’s original ($\pm 127.3$). The proposed design eliminates complex nonlinear operations (inverse-sigmoid) and spectral components (high-frequency sinusoids), achieving hardware-compatible computation without compromising geometric fidelity.%By forgoing both inverse-sigmoid and high-frequency sinusoidal functions, the proposed design achieves hardware-friendly numerical characteristics while preserving robust geometric representation.

% Our quantization-deployable (QD-aware) lidar-ray PE achieves magnitude reduction through two key innovations: (1) Single-point sampling following 3D LiDAR physical prior (Fig. \ref {fig:pe_compare} (b)), and (2) Anchor-based bounded encoding with convex combination constraints. As visualized in Fig. \ref {fig:pe_compare} (c), we learn three axis-aligned anchor embeddings $ \{ E_ \alpha ^i \} _{i=1}^3$ per coordinate axis $ \alpha \in \{ x,y,z \} $ , strategically positioned at $ \{ L_ \alpha ^i \} _{i=1}^3$ . For any LiDAR point $(x_j, y_j, z_j)$ , we compute axis-wise embeddings via linear interpolation between the two nearest anchors:

% To reduce the magnitude of positional encoding in PETR, we analyze its construction process (Fig.\ref{fig:pe_compare}(a)). PETR samples 64 3D points along each camera ray per pixel, increasing magnitude variance. According to statistical theory, the variance of a sum increases with the number of variables. Inspired by 3DPPE~\cite{shu20233DPPE}, which introduces a lidar-centered lidar-ray PE using only one 3D point (Fig.\ref{fig:pe_compare}(b)), we design a quantization and deployment-friendly (QD-aware) lidar-ray PE (Fig.\ref{fig:pe_compare}(c)).

% Our QD-aware lidar-ray PE avoids both the inverse-sigmoid operator and sinusoidal calculations, making it suitable for quantization and deployment on edge devices. Specifically, we learn three anchor embeddings along each axis $\alpha \in {x, y, z}$, denoted $E_\alpha^i$ with anchor locations $L_\alpha^i$ for $i = 1,2,3$. For each lidar-ray point $(x_j, y_j, z_j)$, we identify the two closest anchors and compute interpolated embeddings:
% \begin{equation}
% \begin{aligned}
%     e_x^j &= \frac{x_j - L_x^{i_x}}{L_x^{i_x+1} - L_x^{i_x}}E_x^{i_x+1} +
%              \frac{L_x^{i_x+1} - x_j}{L_x^{i_x+1} - L_x^{i_x}}E_x^{i_x}, \\  
%     e_y^j &= \frac{y_j - L_y^{i_y}}{L_y^{i_y+1} - L_y^{i_y}}E_y^{i_y+1} +
%              \frac{L_y^{i_y+1} - y_j}{L_y^{i_y+1} - L_y^{i_y}}E_y^{i_y}, \\  
%     e_z^j &= \frac{z_j - L_z^{i_z}}{L_z^{i_z+1} - L_z^{i_z}}E_z^{i_z+1} +
%              \frac{L_z^{i_z+1} - z_j}{L_z^{i_z+1} - L_z^{i_z}}E_z^{i_z}.
% \end{aligned}
% \end{equation}

% \noindent where $ \Delta L_ \alpha = L_ \alpha ^{i_ \alpha +1} - L_ \alpha ^{i_ \alpha }$ . This formulation guarantees $e_ \alpha ^j$ resides in the convex hull of adjacent anchor embeddings, strictly bounding each component's magnitude (see Theorem 1). The final encoding is obtained through a lightweight MLP: $MLP([e_x^j, e_y^j, e_z^j])$ , where $[\cdot]$ denotes channel-wise concatenation.

% Visual comparisons in Fig.~\ref{fig:cameraPE_and_lidarPE} demonstrate the elimination of extreme outliers through our anchor interpolation mechanism. Crucially, the dynamic range of our QD-aware PE ($\pm$29.7) exhibits only a minimal difference compared to the image features ($\pm$3.4), in stark contrast to PETR’s encoding ($\pm$127.3) which shows a 33$\times$ disparity that underlies its quantization failure. By removing unbounded nonlinearities (inverse-sigmoid) and high-frequency components (sinusoids), our design ensures hardware-friendly numerical characteristics without sacrificing geometric precision.

% The QD-aware lidar-ray PE is then obtained via $MLP([e_x^j, e_y^j, e_z^j])$, where $[\cdot]$ denotes concatenation along the channel dimension, and the MLP has a conv-relu-conv structure.

\vspace{-4mm}
\paragraph{Quantization Strategy for Scaled Dot-Product in Cross-Attention.}

In softmax operations, numerical stabilization (NS) subtracts the maximum value to prevent overflow. Traditional quantization quantizes before NS, leading to issues in \textbf{Observation2}. We propose quantizing after NS (Fig.~\ref{fig:illustration_for_quant_before_after_stabilization}), and adaptively determining the optimal truncation lower bound to minimize softmax error.

\begin{figure}[htb]
\centering
	\includegraphics[width=0.9\linewidth]{./figs/softmax.pdf}
    \vspace{-4mm}
	\caption{Illustration for quant before/after stabilization.}
	\label{fig:illustration_for_quant_before_after_stabilization}
    \vspace{-7mm}
\end{figure}

After NS, inputs for softmax are non-positive. Values below $-20$ approach zero after exponentiation, so we define a candidate set of scaling factors $S = {s_1, s_2, ..., s_N}$ with $s_i = \frac{i}{2^{k-1}}$ for $k$-bit quantization. The dequantized input is:
\vspace{-3mm}
\begin{equation}
\vspace{-3mm}
\begin{aligned}
\hat{x}_s^i = s_i \cdot \text{clamp}\left( \text{round}\left( \frac{x_s}{s_i} \right), -2^{k-1}, 2^{k-1}-1 \right)
\end{aligned}
\end{equation}

ensuring $\hat{x}_s^i \in [-i, 0]$. We compute the softmax distributions $p_f = \text{softmax}(x_s)$ and $p_q^i = \text{softmax}(\hat{x}s^i)$, and select the optimal scaling factor $s{\hat{i}}$ minimizing the error:

\begin{equation}
\begin{aligned}
\hat{i} = \operatorname*{argmin}_{i} | p_f - p_q^i |, \quad i = 1,2,...,N.
\end{aligned}
\vspace{-4mm}
\end{equation}

\paragraph{DuLUT for Non-linear Functions.} 
% \paragraph{DuLUT for Non-linear Functions with Curvature Analysis}
% The error bound of linear LUT can be formally expressed by the maximum interpolation error theorem. Given a twice-differentiable function $f(x)$ over interval $[x_i, x_{i+1}]$, the maximum approximation error using linear interpolation satisfies:

% \begin{equation}
% \max_{x\in[x_i,x_{i+1}]} |f(x)-P(x)| \leq \frac{(x_{i+1}-x_i)^2}{8} \max_{x\in[x_i,x_{i+1}]} |f''(x)|
% \label{eq:error_bound}
% \end{equation}

% where $f''(x)$ represents the curvature. Conventional linear LUT uses uniform intervals ($x_{i+1}-x_i = \Delta$), leading to error accumulation in high-curvature regions. Our DuLUT addresses this through curvature-adaptive segmentation:

% \begin{theorem}[DuLUT Error Bound]
% For a function partitioned into $K$ segments with interval lengths $\{\Delta_k\}_{k=1}^K$, the total approximation error satisfies:
% \begin{equation}
% \mathcal{E}_{\text{total}} \leq \frac{1}{8}\sum_{k=1}^K (\Delta_k)^2 \cdot \max_{x\in S_k} |f''(x)|
% \label{eq:dulut_error}
% \end{equation}
% where $S_k$ denotes the $k$-th segment. By strategically allocating shorter intervals $\Delta_k$ in high-curvature regions, DuLUT achieves comparable precision to full-resolution LUT with fewer entries.
% \end{theorem}

% \begin{proof}
% Let the original LUT require $N=2^{16}$ entries for 16-bit inputs. Through curvature analysis, we partition the domain into three regions:

% 1. \textbf{Shrink region} ($|f''(x)| < \kappa_{\text{low}}$): Compress into 1 interval
% 2. \textbf{Enlarge region} ($\kappa_{\text{low}} \leq |f''(x)| \leq \kappa_{\text{high}}$): Allocate $M$ intervals
% 3. \textbf{Unchanged region} ($|f''(x)| > \kappa_{\text{high}}$): Maintain 1 interval

% For SiLU function with $x\in[-100,100]$, our analysis shows $\kappa_{\text{low}}=0.1$ and $\kappa_{\text{high}}=0.5$ provide optimal segmentation. This reduces the effective interval count from $N$ to:

% \begin{equation}
% N_{\text{eff}} = \underbrace{1}_{\text{Shrink}} + \underbrace{M}_{\text{Enlarge}} + \underbrace{1}_{\text{Unchanged}} \ll N
% \end{equation}

% Using two 256-entry tables ($M=253$), we achieve equivalent precision to 22,900-entry LUT in the critical region $x\in[-4,3]$, with total error bounded by:

% \begin{equation}
% \mathcal{E}_{\text{DuLUT}} \leq \frac{(7/253)^2}{8} \cdot \max_{x\in[-4,3]} |\text{SiLU}''(x)| \approx 5.2\times10^{-5}
% \end{equation}

% This matches the error bound of full 65,536-entry LUT ($\mathcal{E}_{\text{Full}} \approx 4.8\times10^{-5}$) while reducing SRAM usage by 98.5\%.
% \end{proof}
% In \cite{dao2023flashattention}, the authors highlight that on modern GPUs such as the NVIDIA A100, specialized Tensor Cores can provide substantial throughput (up to 312 TFLOPs/s in FP16/BF16 for matrix multiplication), whereas the separate compute unit for non-matmul operations only achieves about 19.5 TFLOPs/s in FP32—approximately one-fifteenth of the matmul throughput. Consequently, each non-matmul FLOP (e.g., Softmax, Dropout, activation functions) is about 16 times more “expensive,” and can quickly become a performance bottleneck if not carefully optimized. Meanwhile, on edge devices with more limited hardware resources, look-up table (LUT) is likewise favored for approximating nonlinear functions efficiently.

% Linear LUT can compute functions losslessly when the input quantization bit width is less than or equal to the number of LUT entries. However, for higher-bit inputs, conventional linear LUT become impractical due to exponentially increasing table sizes—requiring 256 entries for 8-bit inputs and 65,536 entries for 16-bit inputs—leading to significant storage and computational overhead.

% To improve precision without excessive resource usage, non-linear LUT have been proposed in \cite{yu2022nn}. These methods allocate more entries to regions where the function changes rapidly and fewer entries where it remains flat, thus enhancing quantization precision in critical areas. However, non-linear LUT are complex to implement in hardware, necessitating additional processing to define input regions based on function characteristics, and are not efficiently supported by most hardware architectures. Approximation methods like I-BERT~\cite{kim2021bert} and I-ViT~\cite{li2023vit} attempt to simplify computations but introduce approximation errors and increased computational steps, offsetting the benefits of quantization.

The error bound of linear LUT can be formally expressed by the maximum interpolation error theorem. Given a twice-differentiable function $f(x)$ over interval $[x_i, x_{i+1}]$, the maximum approximation error using linear interpolation satisfies:
\vspace{-3mm}
\begin{equation}
\begin{aligned}
\max_{x\in[x_i,x_{i+1}]} |f(x)-P(x)| \leq \frac{(x_{i+1}-x_i)^2}{8} \max_{x\in[x_i,x_{i+1}]} |f''(x)|
\label{eq:error_bound}
\end{aligned}
\end{equation}
where $f''(x)$ represents the curvature. This result indicates that if the second derivative (i.e., curvature) $\max \lvert f''(x) \rvert$ is large within a sub-interval, $(x_{i+1} - x_i)$ must be shortened to control the approximation error. Conversely, if the curvature is small, the sub-interval can be lengthened. Consequently, more interpolation points (LUT entries) should be assigned where the function changes rapidly, while flatter or near-saturated regions may be merged into fewer intervals.

Building on this principle, DuLUT partitions the input domain into three types of sub-intervals—\textit{shrink} (near-saturated), \textit{enlarge} (steep change), and \textit{unchange} (near-linear)—defined as follows:
\begin{itemize}
\item \textit{shrink}: for regions where the function is close to saturation or changes very little, multiple quantization steps are “compressed” into a single or few LUT entries;
\item \textit{enlarge}: for high-curvature regions, more LUT entries are assigned to preserve accuracy;
\item \textit{unchange}: for intervals that appear nearly linear, further subdivision is unnecessary.
\end{itemize}

As a result, extra entries can be concentrated in critical intervals (e.g., [-9, 8] for SiLU) to capture rapid nonlinear variations, while intervals far from the main dynamic range (e.g., $\lvert x\rvert > 9$, where the function is saturated) are merged. Let the high-curvature region have length \(\textit{enlarge\_length}\), the full input domain be \(\bigl(-x_{\max},\, x_{\max}\bigr)\), and the total number of LUT entries be \(\textit{table\_n}\). In a \emph{single} linear LUT scheme, the number of entries assigned to the high-curvature region follows:
\begin{equation}
    \frac{\textit{enlarge\_length}}{2\,x_{\max}} \times\ \textit{table\_n}.
\end{equation}
For example, if the SiLU function spans \([-500,\,500]\) (hence \(x_{\max} = 500\)) and we employ a 512-entry linear LUT, this formula indicates that only \(\approx 8\) entries would fall within the high-curvature region.

By contrast, DuLUT retains the same total of 512 entries but splits them into two smaller 256-entry LUTs. The first LUT maps the input into a “nonlinear index,” while the second LUT stores the actual function values. Continuing the SiLU example, if we allocate 256 entries following the above principle, the \textit{enlarge} region might occupy 4 entries, the \textit{shrink} region 126 entries (with only 1 used explicitly), and thus the \textit{enlarge} region ultimately gains 129 entries. In effect, this yields a lookup resolution equivalent to using approximately 7588 entries in a single-table design.


% As a result, extra entries can be concentrated in critical intervals (e.g., [-9, 8] for SiLU) to capture rapid nonlinear variations, while intervals far from the main dynamic range (e.g., $\lvert x\rvert > 9$, where the function is saturated) are merged. 假设高曲率空间的范围为enlarge\_length, 输入范围为(-x\_max, x\_max), 表项为 tabel\_N, 高曲率空间分配的表项的计算公式：$\frac{enlarge\_length}{2*2\_max} * {table\_N}$，假设一个数的Silu的输入范围为 [-500,500]，那么表项为512的线性表中高曲率空间分配的表项只有 8 个。DuLUT为了解决这个问题，同样使用512的表项，但是将分为两个256表项的线性表，第一个线性表用来将线性映射转为非线性映射，也就是第一个表项中存储的是索引，第二个表中，存储的是非线性函数的值。以上面的silu为例，我们将 256 代入公式中，计算可得 enlarge区域占 4 个表项shrink区域占 126 个表项，shrink区域保留 1 个表项，那么enlarge区域占用 129 个表项，相当于7588 个表项的查表精度。
% By contrast, \textbf{DuLUT} uses two smaller LUTs of 256 entries each, focusing on the high-curvature segment [-4, 3]. This strategy yields an “effective resolution” equivalent to thousands or even tens of thousands of entries in that sub-range—roughly 25,300 effective points—thereby approaching the accuracy of a 65,536-entry LUT while using significantly fewer entries in flatter parts of the function.

A example, with 8-bit quantization, DuLUT uses two tables of 32 entries each without compromising precision (see Fig. \ref{fig:dulut} and Algorithm~\ref{algo:algorithm_dulut}). We applied DuLUT to common activation functions like softmax, GELU, and SiLU. By utilizing DuLUT, we achieve the same precision as larger single-table lookups while significantly reducing SRAM overhead and maintaining computational efficiency.

\begin{figure*}[htb]
\centering
	\includegraphics[width=0.9\linewidth]{./figs/LUT_vs_DuLUT.pdf}
    \vspace{-0.3cm}
	\caption{Illustration for LUT and DuLUT}
	\label{fig:dulut}
    \vspace{-0.5cm}
\end{figure*}
%\vspace{-0.8cm}


\begin{algorithm}[htb]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
\footnotesize

For a nonlinear function \textbf{f}:
Determine segmentation points based on the curvature of \textbf{f};

Partition the input domain into \textit{shrink}, \textit{enlarge}, and \textit{unchanged} regions;

Compress the \textit{shrink} region's table entries into one, reallocating saved entries to the \textit{enlarge} region;

As an illustrative example, construct $\boldsymbol{table1}$ and $\boldsymbol{table2}$, each with 32 entries for 8-bit input (\texttt{int8}); \

\For{each quantized input $i_x$}{
Compute the index: \

$i_x = i_x + 128$; \

$\text{index} = ((\boldsymbol{table1}[i_x[0{:}5]] \times (8 - i_x[5{:}]) + \boldsymbol{table1}[i_x[0{:}5] + 1] \times i_x[5{:}] + (1 \ll 2)) \gg 3$) + 128; \

Compute the quantized output: \
$out = (\boldsymbol{table2}[\text{index}[0{:}5]] \times (8 - \text{index}[5{:}]) + \boldsymbol{table2}[\text{index}[0{:}5] + 1] \times \text{index}[5{:}]) \div 8$;
}

Return: $out$;

\caption{Pseudo-code of DuLUT.}
\label{algo:algorithm_dulut}
% \vspace{-4mm}
\end{algorithm}

\input{table/overall}

