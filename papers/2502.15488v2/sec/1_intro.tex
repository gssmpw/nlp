% \section{Introduction}
% \label{sec:intro}

% 3D perception is crucial for autonomous driving and embodied intelligence~\cite{bevdet,bevformer,liu2022bevfusion,bevdet4d,yu2024panoptic}. PETR-based methods~\cite{streampetr,liu2022petr,liu2022petrv2,shu20233DPPE,cmt,wang2024omnidrive,wang2023object} have gained significant attention due to their effectiveness in this domain. Unlike previous detection methods relying on dense convolutional features~\cite{bevdet,yu2023flashocc,yu2024ultimatedo,bevdepth,lss} or utilizing 3D-to-2D projections as in DETR3D-based approaches~\cite{wang2022detr3d,lin2022sparse4d,Liu2023SparseBEVHS}, PETR adapts the paradigm of 2D object detection with transformers (DETR)~\cite{Carion2020EndtoEndOD} by incorporating 3D positional encoding, achieving seamless end-to-end 3D obstacle detection.

% Despite its advantages, PETR requires substantial storage and computational resources during inference, posing challenges for on-device deployment. To enhance inference efficiency, considerable efforts have been dedicated to efficient model compression~\cite{Shu2020ChannelwiseKD,Li2020HierarchicalKS,Liu2018DARTSDA,Li2016PruningFF,gupta2015deep,gysel2018ristretto}, with quantization being particularly favored for deploying models on AI chips by representing networks in low-bit formats~\cite{shao2023omniquant,liu2024spinquant,xiao2023smoothquant,Low_bit_quant_iccvw2019}.

% Nevertheless, even on high-performance GPUs like the NVIDIA A100, specialized Tensor Cores deliver only about one-fifteenth the throughput (in TFLOPs/s) for non-matrix-multiplication operations compared to matmul~\cite{dao2023flashattention}. This discrepancy makes each nonlinear FLOP (\emph{e.g.}, Softmax, activation functions) substantially more “expensive” and poses a severe performance bottleneck if left unoptimized. On edge AI chips, the challenge is heightened further: hardware typically lacks floating-point efficiency for nonlinear operators, so \emph{lookup table} (LUT) ~\cite{wang2018look}  is frequently used to accelerate such functions in integer form.

% When the LUT’s size matches or exceeds the input bit width’s dynamic range, a \emph{linear} LUT can approximate nonlinear functions with negligible error. However, table size grows exponentially with bit width (e.g., 256 entries for 8-bit vs.65536 for 16-bit), making a single linear LUT impractical in terms of both memory and computational overhead. \emph{Non-linear} LUT methods~\cite{yu2022nn} address this by assigning more entries to high-curvature regions while reducing entries for flatter regions, yet they add hardware complexity for subrange detection. Approaches like I-BERT~\cite{kim2021bert} and I-ViT~\cite{li2023vit} further simplify operator emulation under integer arithmetic but often introduce additional approximation steps or errors.


% % However, deploying complex models like PETR on edge devices presents additional challenges. Autonomous vehicles rely on edge AI chips with limited computational capabilities. Nonlinear functions such as softmax, sigmoid, SiLU, and GeLU are commonly used during inference, but most edge AI chips lack sufficient floating-point performance for these operations, exacerbating deployment difficulties.

% Moreover, quantization awareness is often overlooked in neural architecture design, leading to significant performance degradation during deployment. Notably, models like MobileNet~\cite{sheng2018quantization,yun2021all,howard2017mobilenets,sandler2018mobilenetv2}, EfficientNet~\cite{bhalgat2020lsq+,habi2021hptq,tan2019efficientnet}, and RepVGG~\cite{Chu2022MakeRG} face quantization issues requiring additional design efforts or advanced strategies to mitigate deployment challenges. PETR is no exception; when deployed on in-vehicle AI chips, standard 8-bit per-tensor post-training quantization (PTQ) results in a significant performance drop, with a 58.2\% decrease in mAP and a 36.9\% decrease in NDS.

% In this paper, we address the quantization issues of PETR to facilitate its deployment on edge AI chips in autonomous vehicles. We conduct an in-depth analysis of the mechanisms behind PETR's quantization failure, revealing that the significantly larger magnitudes of positional encodings compared to image features and the highly imbalanced scaled dot-products in cross-attention are the primary causes of quantization collapse. Based on these insights, we propose \textbf{Q-PETR} (Quantization-aware PETR), a model that mitigates quantization collapse, improves floating-point performance, \emph{and} leverages a hardware-friendly dual-LUT strategy to handle nonlinear functions efficiently on edge devices.

% Our contributions are summarized as follows:

% \begin{itemize}
% \item \textbf{Identifying the root causes of quantization collapse in PETR:} We uncover that the imbalanced output of the inverse-sigmoid function and excessively large values in positional encoding, as well as the imbalanced scaled dot-products in cross-attention, are key issues.
% \item \textbf{Redesigning the positional encoding and quantization strategy:} We reformulate the positional encoding and improve the quantization approach for the scaled dot-product in cross-attention, enhancing both floating-point performance and quantization friendliness.
% \item \textbf{Introducing DuLUT for efficient nonlinear function inference:} We propose DuLUT, an improved LUT approach that reduces the number of table entries without compromising precision, optimizing nonlinear function inference on edge AI chips.
% \item \textbf{Demonstrating generalizability and deployment readiness:} Our method generalizes well across different model scales, achieving exceptional post-quantization performance suitable for deployment on edge devices.
% \end{itemize}


\section{Introduction}
\label{sec:intro}

3D object detection~\cite{bevformer,zhou2023fastpillars,zhou2024pillarhist,liu2022bevfusion} has been a longstanding topic in computer vision. Compared to LiDAR, cameras have gained increasing popularity in autonomous systems due to their ability to provide dense texture information at a lower cost, making camera-based 3D object detection more favored~\cite{bevdet,bevdet4d,yu2024panoptic}. Among these studies, the mainstream PETR frameworks~\cite{streampetr,liu2022petr,liu2022petrv2,shu20233DPPE,cmt,wang2024omnidrive,wang2023object} have gained prominence by adapting the 2D transformer-based DETR paradigm~\cite{Carion2020EndtoEndOD} with 3D positional encodings. In comparison to dense feature methods~\cite{bevdet,yu2023flashocc,yu2024ultimatedo,bevdepth,lss} or DETR3D-style 3D-to-2D projections~\cite{wang2022detr3d,lin2022sparse4d,Liu2023SparseBEVHS}, PETR achieves end-to-end 3D detection while performing superior performance. Despite its effective, the deployment of PETR on resource-limited edge devices presents a critical challenge that significantly hinders their widespread application in autonomous vehicles and robotics.

Quantization~\cite{zhou2024lidar,zhang2023qd,Low_bit_quant_iccvw2019,nagel2020up} is an efficient model compression approach that reduces computational burden by converting high-bit floating-point into low-bit integer formats. Compared to quantization-aware training (QAT) methods, which require access to all labeled training data and substantial computation resources, post-training quantization (PTQ) is more suitable for rapid and efficient paratical applications. This is because PTQ only needs a small number of unlabeled samples for calibration. Furthermore, PTQ does not require retraining the network with all available labeled dataset, resulting in a shorter implementation time. Although several advanced PTQ methods have been proposed for 2D detection tasks\citep{li2021brecq, wei2022qdrop, liu2023pd} and ViT~\cite{li2023vit,liu2021ptqvit}, directly applying them to multi-view 3D Detection tasks inevitably leads to severe performance degradation due to the structures and task-specific differences. For instance, when standard 8-bit per-tensor post-training quantization (PTQ) is applied to PETR, leading up to 58.2\% mAP and 36.9\% NDS performance collapse.

Furthremore, nonlinear operators such as softmax, gelu, and silu are indispensable in 3D detection models, but their performance is often hindered by hardware constraints, akin to the ``short board'' in a barrel. Even specialized Tensor Cores in high-end GPUs like the NVIDIA A100 exhibit significantly lower throughput for these nonlinear operators compared to matrix multiplications~\cite{dao2023flashattention}. Moreover, many edge AI chips rely on a lookup table (LUT)~\cite{wang2018look} for integer activation functions, but these typically only support integer inputs and are often linear LUTs. While a linear LUT can faithfully approximate nonlinearities if its size covers the entire dynamic range, its capacity grows exponentially (e.g., from 256 entries at 8-bit to 65536 entries at 16-bit), making it impractical. Non-linear LUTs~\cite{yu2022nn} allocate more entries to steeper regions of the function and fewer to flatter regions, but introduce additional hardware complexity. Other integer-only methods~\cite{kim2021bert,li2023vit} further simplify operator emulation at the cost of increased approximation error. Consequently, effective quantization and hardware-friendly acceleration of these nonlinear operators is crucial and unexplored for PETR in resource-constrained deployment.

%Moreover, many neural network architectures are designed without explicit consideration for quantization. Although models such as MobileNet~\cite{sheng2018quantization,yun2021all,howard2017mobilenets,sandler2018mobilenetv2} and EfficientNet~\cite{bhalgat2020lsq+,habi2021hptq,tan2019efficientnet} achieve state-of-the-art performance in various tasks, they typically require specialized design or re-parameterization to avoid large accuracy drops under low-bit quantization.  For example, QARepVGG~\cite{Chu2022MakeRG} reconfigures the original RepVGG~\cite{ding2021repvgg} for more quantization-friendly deployment, and similar design insights are necessary for PETR-based 3D detectors.

In this paper, we address these quantization challenges for PETR-based 3D Detection. Firstly, through an in-depth analysis, we find that disproportionately large positional encodings and imbalanced scaled dot-products in cross-attention significantly degrade quantized performance. Building on these findings, we propose \textbf{Q-PETR}, a quantization-friendly variant of PETR that not only mitigates performance collapse but also enhances floating-point accuracy. To further resolve the bottleneck of nonlinear operators, we introduce a lightweight dual-LUT (\textbf{DuLUT}) mechanism that maintains high approximation fidelity with fewer table entries. Our main contributions are summarized as follows:
\begin{itemize}
    \item \textbf{Diagnosis of quantization failures in PETR:} We show that large positional encodings, imbalanced inverse-sigmoid outputs, and skewed cross-attention dot-products are key factors causing significant accuracy loss under low-bit quantization.
    \item \textbf{Redesign of positional encodings and cross-attention quantization:} We reformulate the positional encoding module and employ a more balanced scaling strategy for cross-attention dot-products, improving both floating-point and quantized performance.
    \item \textbf{Introduction of DuLUT for hardware-friendly nonlinear functions:} By splitting LUT entries based on the curvature of the function, DuLUT efficiently approximates nonlinearities with fewer table entries, greatly facilitating edge deployment.
    \item \textbf{Broad applicability and deployment readiness:} Our approach generalizes to various model scales, achieving minimal performance loss under standard 8-bit PTQ while even boosting full-precision accuracy, thus meeting the demands of resource-limited scenarios in real-world.
\end{itemize}



% but real-world performance depends heavily on hardware support. Notably, specialized Tensor Cores on high-end GPUs like the NVIDIA A100 deliver far lower throughput for nonlinear operators than for matrix multiplications~\cite{dao2023flashattention}, and many edge AI chips resort to a lookup table (LUT)~\cite{wang2018look} for integer activation functions. Although a linear LUT can faithfully approximate nonlinearities if its size covers the entire dynamic range, its exponential growth (e.g., 256 entries for 8-bit vs. 65536 for 16-bit) quickly becomes impractical. Non-linear LUT~\cite{yu2022nn} alleviates this by assigning more fine-grained entries to steep function regions and fewer to flatter regions, but introduce extra hardware complexity. Other integer-only methods~\cite{kim2021bert,li2023vit} further simplify operator emulation at the risk of increased approximation error.

% Moreover, many neural architectures are designed without quantization-awareness. Structures like MobileNet~\cite{sheng2018quantization,yun2021all,howard2017mobilenets,sandler2018mobilenetv2}, EfficientNet~\cite{bhalgat2020lsq+,habi2021hptq,tan2019efficientnet}, and QARepVGG~\cite{Chu2022MakeRG} often require special modifications to avoid severe quantization drops, and PETR is no exception. 

% We address these quantization challenges for PETR-based 3D detectors on edge AI platforms. Through a detailed analysis, we find that the disproportionately large positional encodings and imbalanced scaled dot-products in cross-attention significantly degrade quantized performance. Building on these insights, we propose \textbf{Q-PETR}, a quantization-friendly variant of PETR that not only mitigates quantization collapse but also brings floating-point improvements. Moreover, we introduce a hardware-efficient dual-LUT (DuLUT) mechanism for nonlinear operators. Our main contributions include:
% \begin{itemize}
%     \item \textbf{Diagnosis of quantization failures in PETR:} We show that large positional encodings, imbalanced inverse-sigmoid outputs, and skewed cross-attention dot-products are key factors causing substantial accuracy loss under low-bit quantization.
%     \item \textbf{Redesign of positional encodings and cross-attention quantization:} We reformulate PETR's positional encoding and adopt a more balanced approach for scaled dot-products, which enhances both floating-point performance and quantization robustness.
%     \item \textbf{Introduction of DuLUT for efficient nonlinear function inference:} By splitting table lookups based on the function's curvature, DuLUT achieves near-lossless approximation with fewer LUT entries, which is crucial for edge AI deployments.
%     \item \textbf{Broad applicability and deployment readiness:} Our approach generalizes across different model scales, delivering minimal performance degradation under standard 8-bit PTQ while even improving float-precision results, thus meeting the demands of resource-constrained environments.
% \end{itemize}