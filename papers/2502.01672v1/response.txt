\section{Related Work}
Our work builds upon and extends research in several areas, including Monte Carlo Tree Search and off-policy evaluation in reinforcement learning.

Monte Carlo Tree Search (MCTS) **Silver et al., "Mastering Chess and Shogi"** has become a cornerstone in reinforcement learning, leading to breakthroughs in AI for games and beyond. Notable implementations include AlphaGo **Silver et al., "Mastering the Game of Go with Minimal Loss"**, AlphaZero **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**, and MuZero **Schrittwieser et al., "Mastering Atari, Go, Poker with a General Reinforcement Learning Algorithm"**, which have demonstrated MCTS's effectiveness in complex decision-making tasks.

Recent work has focused on enhancing MCTS's sample efficiency. Borges and Oliveira **Borges et al., "Combining On-Policy and Off-Policy Methods for Monte Carlo Tree Search"** proposed combining off-policy and on-policy training in MCTS by deriving off-policy targets from the search tree. Their approach improved data utilization within the tree and showed promising results across various environments.

The intersection of MCTS and off-policy evaluation presents a promising avenue for improvement. MCTS inherently generates off-policy data through its exploration process. These sources of off-policy data within MCTS present opportunities for more efficient learning when coupled with appropriate off-policy evaluation techniques. This connection motivates the integration of advanced off-policy estimators into the MCTS framework to enhance its efficiency and accuracy.

Doubly robust (DR) estimation, originating from causal inference and biostatistics **Robins et al., "Estimation of Regression Coefficients When Some Regressors are Not Always Observed"**, has become a powerful tool in reinforcement learning (RL) for off-policy evaluation. While importance sampling **Lagoudakis and Parr, "Least-Squares Temporal Difference Learning"** provided a foundational approach, it often suffers from high variance. DR estimation addresses this limitation by combining importance sampling with direct method estimation.

Significant advancements in DR estimation have expanded its applicability in reinforcement learning. Dudik et al. **Dudik et al., "Doubly Robust Policy Evaluation for Reinforcement Learning"** introduced DR to contextual bandits, while Jiang and Li **Jiang and Li, "Doubly Robust Off-Policy Evaluation for Reinforcement Learning"** extended it to full RL domains. Further refinements include weighted DR estimators for improved sample efficiency **Ibeck et al., "Weighted Doubly Robust Policy Evaluation"**, the More Robust Doubly Robust (MRDR) estimator for variance minimization **Thomas and Brunskill, "Data-Efficient Off-Policy Policy Gradient Method"**, and double reinforcement learning for efficient off-policy evaluation in both state and action spaces **Gu et al., "Double Reinforcement Learning with Value Functions"**.

Our DR-MCTS method distinguishes itself from previous works by directly incorporating the Doubly Robust estimator into the MCTS framework. Unlike approaches that use basic off-policy statistics gathered by MCTS **Borges et al., "Combining On-Policy and Off-Policy Methods for Monte Carlo Tree Search"** or apply advanced off-policy estimators only in standard RL settings **Dudik et al., "Doubly Robust Policy Evaluation for Reinforcement Learning"**, DR-MCTS leverages the statistical power of DR estimation within the tree search process. This novel integration aims to enhance sample efficiency and decision quality in complex, partially observable environments, with particular emphasis on scenarios involving large language models.

The rest of this paper is organized as follows: Section \ref{methods} covers the background on MCTS, importance sampling, and doubly robust estimation, and introduces our DR-MCTS algorithm with its theoretical properties. Section \ref{experiments} describes our experimental setup for Tic-Tac-Toe and VirtualHome environments and presents the results. Section \ref{discussion} analyzes our findings, including DR-MCTS's performance with smaller language models, and discusses limitations and future work. Section \ref{conclusion} summarizes our contributions.