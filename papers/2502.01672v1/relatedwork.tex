\section{Related Work}
Our work builds upon and extends research in several areas, including Monte Carlo Tree Search and off-policy evaluation in reinforcement learning.

Monte Carlo Tree Search (MCTS) \cite{coulom2006efficient} has become a cornerstone in reinforcement learning, leading to breakthroughs in AI for games and beyond. Notable implementations include AlphaGo \cite{silver2016mastering}, AlphaZero \cite{silver2018general}, and MuZero \cite{schrittwieser2020mastering}, which have demonstrated MCTS's effectiveness in complex decision-making tasks.

Recent work has focused on enhancing MCTS's sample efficiency. Borges and Oliveira \cite{borges2021combiningonpolicytrainingmodelbased} proposed combining off-policy and on-policy training in MCTS by deriving off-policy targets from the search tree. Their approach improved data utilization within the tree and showed promising results across various environments.

The intersection of MCTS and off-policy evaluation presents a promising avenue for improvement. MCTS inherently generates off-policy data through its exploration process. These sources of off-policy data within MCTS present opportunities for more efficient learning when coupled with appropriate off-policy evaluation techniques. This connection motivates the integration of advanced off-policy estimators into the MCTS framework to enhance its efficiency and accuracy.

Doubly robust (DR) estimation, originating from causal inference and biostatistics \cite{robins1995semiparametric}, has become a powerful tool in reinforcement learning (RL) for off-policy evaluation. While importance sampling \cite{precup2000eligibility} provided a foundational approach, it often suffers from high variance. DR estimation addresses this limitation by combining importance sampling with direct method estimation.

Significant advancements in DR estimation have expanded its applicability in reinforcement learning. Dudik et al. \cite{dudik2011doubly} introduced DR to contextual bandits, while Jiang and Li \cite{jiang2016doubly} extended it to full RL domains. Further refinements include weighted DR estimators for improved sample efficiency \cite{thomas2016data}, the More Robust Doubly Robust (MRDR) estimator for variance minimization \cite{farajtabar2018more}, and double reinforcement learning for efficient off-policy evaluation in both state and action spaces \cite{kallus2020double}.

Our DR-MCTS method distinguishes itself from previous works by directly incorporating the Doubly Robust estimator into the MCTS framework. Unlike approaches that use basic off-policy statistics gathered by MCTS \cite{borges2021combiningonpolicytrainingmodelbased} or apply advanced off-policy estimators only in standard RL settings \cite{jiang2016doubly, thomas2016data, farajtabar2018more, kallus2020double}, DR-MCTS leverages the statistical power of DR estimation within the tree search process. This novel integration aims to enhance sample efficiency and decision quality in complex, partially observable environments, with particular emphasis on scenarios involving large language models.


The rest of this paper is organized as follows: Section \ref{methods} covers the background on MCTS, importance sampling, and doubly robust estimation, and introduces our DR-MCTS algorithm with its theoretical properties. Section \ref{experiments} describes our experimental setup for Tic-Tac-Toe and VirtualHome environments and presents the results. Section \ref{discussion} analyzes our findings, including DR-MCTS's performance with smaller language models, and discusses limitations and future work. Section \ref{conclusion} summarizes our contributions.