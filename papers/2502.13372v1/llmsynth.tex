



%% \begin{figure}[t]
%%     \centering
%%     \includegraphics[width=\linewidth]{figs/fig_pipeline.pdf}
%%     \caption{Our synthesis and verication pipeline takes a text prompt as input.  The animation synthesizer generates a corresponding motion graphics animation, while the the \dslname{} synthesizer independently generates a corresponding verification program. 
%%       Executing the \dslname{} program on the animation produces a verification report that can automatically be fed back to the animation synthesizer for iterative correction. 
%%       }
%%     \label{fig:pipeline}
%% \end{figure}


\section{LLM-Based Motion Program Synthesis and Verification}
\label{sec:llmsynth}
We leverage \dslname{} within an motion graphics synthesis and
verification pipeline. It takes as input a text prompt describing
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_pipeline.pdf}
  \vspace{-1.5em}
    \label{fig:pipeline}
\end{figure}
\input{resultsFig}
the desired animation along with 
a static SVG that sets the scene context for the prompt.
%
%Our pipeline takes a text prompt describing the desired
%animation, along with the code of a static SVG, as input. 
It uses LLM-based program synthesis to produce two programs;
(1) the {\em animation synthesizer} produces a motion graphics animation program written in a high-level
animation API (GSAP\,\cite{gsap} in our implementation)
%\maneesh{could we demonstrate a physics based API or other APIs?},
and (2) the {\em \dslname{} synthesizer} generates a corresponding \dslname{} verification program.
%
In both cases we follow the LLM-based in-context learnning approach
for program synthesis\,\cite{gupta2023visprog,
  surismenon2023vipergpt}, where we prompt GPT-4o\,\cite{hurst2024gpt}
with annotated API/DSL documentation and examples of text prompts
paired with corresponding output programs
%(Appendix~\ref{appendix:sysprompts} gives the prompts for our
%implementation). \maneesh{fix ref to Appendix.}
(Section A in supplemental materials gives the prompts for our
implementation).

We render the resulting motion graphics animation into the generic
per-frame SVG motion program representation (Section~\ref{sec:execution}).
%
Then we execute the \dslname{} program on
the SVG motion program to check that spatio-temporal properties mentioned
in the prompt appear in the animation.
%
%Note that before running the verification, we convert the motion
%graphics animation program from the high-level API to the generic
%frame-level SVG motion program representation described in
%Section~\ref{sec:execution}, using a custom converter based on
%knowledge of the API. 
%
The output is a verification report
specifying which spatio-temporal properties appear in the animation and which do not.
%specifying which logic
%statements and predicates in the verification program the animation
%successfully passes and which it fails.
%
If the verification report indicates a failure, our pipeline can automatically send the 
report back to the animation synthesizer
and request it to update the motion graphics
animation, thereby supporting iterative correction.
%
Note that when the feedback is automatically sent back to the
LLM, we include the documentation of our \dslname{} DSL in the prompt so it
can interpret the error report (Section A.3 in supplemental materials). 
%\maneesh{update ref to appendix}
%
In addition, a human user can examine the verification report and either
manually fix the motion program, or filter the feedback sent to the
LLM. 
%
%\maneesh{Probably don't need next sentence.}
On each iteration, the LLM animation synthesizer has access to all
previous animations it has generated and the corresponding
verification reports in its chat history to help it make corrections.
%









% LocalWords:  LLM GSAP APIs VISPROG ViperGPT DSL SVG verifier GPT
