\section{Introduction}
\label{sec:introduction}

%\maneesh{Need to lace in the fact that we need to give a static SVG scene description as part of the prompt.}

%Large language models (LLMs) have rapidly become a viable/powerful tool for
%program synthesis in the context of visual content creation
Advances in large vision-language models have yielded
powerful tools for visual content creation via program synthesis.
%
%Large language models (LLMs) are powerful tools for
%program synthesis in the context of visual content creation\,\cite{zhang2024scenelanguage}
% for visual content generation via program synthesis. 
These models can take a natural language text prompt as input and generate
program code in a domain specific language (e.g., SVG, CAD, etc.) that is then executed to
produce visual output including images\,\cite{gupta2023visprog}, vector
graphics\,\cite{xing2024llm4svg,polaczek2025neuralsvg},
3D scenes and CAD\,\cite{hu2025scenecraft,zhang2024scenelanguage,khan2024text2cadgeneration}
and animation\,\cite{gal2024breathing,tseng2024keyframer,liu2024logomotion}.
%
%A common approach is to use in-context learning where the LLM is given
%the DSL documentation and example code.
%\maneesh{One sentence on LLM-based program synthesis that references VISPROG and ViperGPT -- using API documentation and input/output examples. }
%
But automatically verifying that the resulting program produces 
{\em correct} visual output---output that matches the
specification given in the prompt---remains a challenging task.

%Human verification, where a person manually looks at the output and
%compares it to the input prompt is the gold standard approach for
%visual content creation tasks. But such manual verification is time-consuming
%and therefore cannot be applied at scale. \maneesh{cut part of human verification.}
%
In some visual domains like image generation, researchers have
explored automatic verification via concept matching between the input
prompt and the output image\,\cite{Johnson_2017_ICCV,Hessel2021CLIPScoreAR,lee2023holisticevaluationtexttoimagemodels,hu2023tifa,JaeminCho2024}.
%\maneesh{HELM paper from Stanford - uses
%  CLIPScore = cos similarity of CLIP emebeddings of caption and image,
%  TIFA -- From text generate QA pairs using GPT-3, verify text answers
%  using UnifiedQA model, and use BLIP2 for VQA with GPT-3 generated
%  question. -- only seem to have a very few spatial relationship
%  concepts. Davidsonian Scene Grammars extend TIFA but use similar
%  approach. DreamSync shares overall goals with our paper.}
%Note that FiD measures quality of images by comparing distributions.
%Inception score also measures quality, not alignment.
This approach relies on access to
concept detectors for both the natural language prompt
and for the output visual asset.
%
More importantly, the detected concepts must be relevant to the desired
properties of the generated content.
%
%(e.g. verifying the prompt ``A picture of a
%Toyota RAV4 in front of a Gothic Cathedral'' might require concept
%detectors for specific types of cars (Toyota RAV4) and buildings
%(Gothic Cathedral) as well as a spatial concept (in front of).)
%
Verifying the prompt ``A horse sitting on an astronaut" might
require concept detectors for the entities ``horse'' and``astronaut''
and the spatial relationship ``sitting on''.
 


Here
%In this paper,
we focus on such automated verification for 2D motion
graphics animations.
%
%In this paper,
%we focus on the domain of 2D motion graphics animations and
%verifying the basic space-time trajectories specified with an input prompt.
We first present {\bf \dslname{}}, a {\bf Mo}tion {\bf Ver}ification DSL, 
that is based on first-order logic
%\maneesh{NeoDavidsonian event semantics (do we use the event semantics
%  really?) and Allen Interval Algebra and Spatial Algebra}
%
%that can be used to express constraints on the space-time trajectories
%(motions) of the objects in the animation.
and can be used to check properties of the spatio-temporal trajectories of objects in an animation.
%check properties of the objects and their motions.
%
More specifically, in \dslname{}, checks are written as logic
statements constructed from predicates that verify the presence of
specific spatio-temporal properties such as the direction, the
magnitude, or the type (e.g., translation, rotation, scale) of motion
of an object. 
%
%
%The predicates in MoVer are based on prior work
%from cognitive psychology and
%linguistics that has investigated how people think about and
%describe relationships between objects and their motions in
%animation\,\cite{Talmy,Tversky,Allen,Balbiani}.
%
We identify relevant spatio-temporal concepts
%and implement them as predicates in MoVer,
based on prior work from cognitive psychology
%and linguistics
that has investigated how people think about relationships between
objects and their motions in 
animation\,\cite{talmy1983language,talmy1975motion,tversky1998space}.
%
We show how to implement these concepts as  \dslname{} predicates 
using Allen's\,\shortcite{allen1983interval} temporal interval algebra
and its 2D spatial counterpart, the rectangle
algebra\,\cite{balbiani1998model,navarrete2013spatial}.


We then demonstrate how \dslname{} can be applied in an LLM-based 
synthesis and verification pipeline to iteratively produce motion graphics animation that accurately
match an input text prompt.
%
Given a text prompt specifying a desired animation (with a static SVG scene for context),
we use in-context LLM-based program
synthesis\,\cite{gupta2023visprog,surismenon2023vipergpt} to produce
(1) a motion graphics animation (as an SVG motion program) and (2) a corresponding \dslname{} 
verification program.
%
%Given an input natural language prompt specifying a desired
%animation, we use LLM-based program synthesis to generate a
%corresponding motion graphics animation.
%
%
%We simultaneously parse the input prompt, into a verification program
%written with our DSL.  We compare two parsing approaches; (1) a
%rule-based semantic parsing approach that is efficient but less robust
%and (2) a LLM-based program synthesis approach that is we
%experimentally show performs better, but can fail in unpredictable
%ways.
%
Executing the \dslname{} program on the corresponding
motion graphics animation results in a report pinpointing
the spatio-temporal predicates the animation successfully passes and fails (Figure~\ref{fig:teaser}).
%
Users can use the report to manually fix the animation program, or
automatically feed it back to our LLM-based pipeline for iterative correction.
%motion program and better
%match the text prompt. 

To evaluate our synthesis and verification pipeline, we build a
synthetic text dataset of 5600 natural language text prompts describing
motion graphics animations, paired with ground truth \dslname{}
verification programs. We find that our LLM-based pipeline can
synthesize correct \dslname{} programs for 95.1\% of the test
prompts. We also find that using the automatic correction iteration capability of
our pipeline increases the number of correct animations from 58.8\%
(with no iterations) to 93.6\% (up to 50 iterations). 






  
  



% LocalWords:  LLMs SVG LLM VISPROG ViperGPT HEIM VQA DSL parsers GPT
% LocalWords:  NeoDavidsonian CLIPScore emebeddings TIFA UnifiedQA
% LocalWords:  Davidsonian DreamSync Scalable MoVer tion Ver spatio
% LocalWords:  ification iteratively allen algorithmically
