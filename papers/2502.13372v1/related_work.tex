\section{Related Work}
\label{sec:related}

\maneesh{12-31-24: Going to skip over this section for now except for note in FOL subsection. But we should think about where we might be able to shorten it.}

\subsection{Classic Text-to-Animation Systems}
% \maneesh{I would try to make this section shorter by writing sentences that can reference multiple papers together.}
% survey paper
Before the prevalence of LLMs, classic text-to-animation systems uses various natural language processing techniques to convert natural language descriptions into animations~\cite{bouali2023review}.
%
Early text-to-animation systems like CarSim~\cite{aakerberg2003carsim} use regular expressions to find verbs and classify them using WordNet.
It then maps categorized actions to animations.
%
Cardinal~\cite{marti2018cardinal} adds co-reference resolution and extracts triplets of subjects, actions, and objects and verb modifiers.
% It uses the Unity 3D engine for animation rendering.
%
Zhang et al.~\shortcite{zhang2019generating} extends Cardinal by adding a text simplification stage that breaks down complex clauses into simpler ones for downstream processing to support a wider variety of input language.
It also uses a semantic role labeler to extract key-value pairs of visual elements and actions, referred to as Action Representation Fields (ARF).

% \maneesh{Need to always close RW paragraphs by saying how we differ or why these approaches cannot solve the problem. Need to state their limitations.}
An important aspect that this line of work overlooks is the \textit{conflation} of expressions in motion descriptions (e.g. ``contract'' is a single word that describes a scaling motion with a decrease in size)~\cite{talmy1975motion}.
In our work, we fine-tune a sentence transformer to assign multiple classification labels to a phrase to properly process conflated expressions.
Moreover, we process input natural language prompts for the purpose of animation verification, instead of animation generation.


% \maneesh{I might have one subsection on text-to-animation without LLMs and one for work that uses gen AI for this. Are KeyFramer and LogoMotion the only two that use LLMs. I think we need to check more thoroughly. Also need to problem consider video generation and perhaps things like ToonCrafter. Will need to figure out how t organize these sections with respect to the Program Synthesis stuff.}
\subsection{Foundation Model-Based Text-to-Animation Systems}
%% text-to-video
A body of work has trained end-to-end models to generate animations conditioned on an input image and a text prompt.
ModelScope~\cite{wang2023modelscope} is a latent video diffusion model built on image diffusion models.
VideoCrafter~\cite{chen2024videocrafter2} focuses on preserving the structure and style of the input image.
Videofusion~\cite{luo2023videofusion} conditions the generation with captions extracted from the input image.
%% text-to-video as priors
Gal et al.~\shortcite{gal2024breathing} leveraged these text-to-video models as priors to animate input vector sketches conditioned on textual prompts.
% bad because of frame to frame representation
However, these methods do not have a mechanism for users to iteratively improve the generation results.
More importantly, all of these methods produce animations as a sequence of frames in pixels or vector vertices, making it difficult to edit the results~\cite{zhang2023motion}.

%% LLM
%% keyframer
On the other hand, some recent work leverages LLMs to produce animations as programs.
KeyFramer~\cite{tseng2024keyframer} prompts an LLM with SVG code and an animation description to generate an animation program written in CSS.
%% logoMotion
LogoMotion~\cite{liu2024logomotion} extends this work by using a vision language model (VLM) capable of processing SVGs as images.
%
Both works have reported that LLMs often produce results misaligned with the users' requirements.
% \maneesh{Seems like we will need to compare to these methods. Need to state their limitations.}
While KeyFramer did not address this limitation, LogoMotion adds a program repair process to help users debug wrongly generated programs, but their process is confined to only static comparisons between the target and last frames of a rendered animation.
%
% \maneesh{Why is our work better than the previous tools.}
In our work, we propose a FOL-based verification DSL consisting of motion and temporal relation predicates that can be composed to express complex constraints stated in the textual prompts.
We inform the user with the verification results, supporting them to fix issues in the program directly or feed them back to the LLM for automatic improvement.
% propose a deterministic semantic parsing pipeline to produce verification programs that represent constraints needed to be satisfied instead of directly outputting animations.


% \maneesh{May need to come before the section on Davidsonian graphs.}
\subsection{Semantic Parsing into First-order Logic Forms}
\maneesh{We probably need a background section explaining FOL in our context and Allen intervals in our context -- or perhaps they can be folded into the methods description. But some (or maybe all) of this text would then go there.}
Neo-Davidsonian event semantics~\cite{parsons1990events,
davidson2001logical} is a formalism that can express natural language descriptions of events in the form of first-order logic statements.
It treats events, such as motions, as a unary predicate, and additional modifiers (e.g., adverbs) can be added with conjunctions and disjunctions to allow for flexible representation of complex sentence structures.
For example, ``translate upward'' can be expressed as $\exists \texttt{e.} \texttt{translate(e)} \land \texttt{upward(e)}$.
%% weakly surpervised paper
Artzi and Zettlemoyer~\shortcite{artzi2013mapping} applied this formalism to interpret and execute natural language instructions for navigation.
They use a weighted linear CCG grammar to parse natural language into logical forms. 
%% programport
Wang et al.~\shortcite{wang2023programmatically} similarly uses a CCG parser to convert natural language instructions for a robotic arm into domain-specific manipulation programs.
%% google transforming paper
Reddy et al. proposed a dependency-parsing based approach for semantic parsing into Neo-Davidsonian logical forms~\cite{reddy2016transforming}.
After obtaining a dependency tree of an input sentence, they flatten the tree via binarization and convert the binarized tree into logical statements via a set of conversion rules.
%% what's left
More recently, Hsu et al.~\cite{hsu2024left} uses an LLM as a domain-independent semantic parser to produce logical forms of input natural language questions for visual reasoning.

% \maneesh{Why are subordinate clauses an issue? If this is important we should stress that we handle them.}
% \maneesh{Why are simple sentences a problem?}
However, inputs in these works are mostly simple sentences without complex subordinate clauses.
This means that they do not need to consider intra- and inter-sentence relationships that are prevalent and essential in the descriptions of motions, such as object co-references and motion sequencing.
%
In our work, we construct a semantic parser capable of handling complex inter-sentence relations like object and motion co-references and event sequencing expressed through various subordinate clauses.


\subsection{Program Synthesis and Verification}
% VisProg and ViperGPT
VISPROG~\cite{gupta2023visprog} and ViperGPT~\cite{surismenon2023vipergpt} are neuro-symbolic methods demonstrating the LLMs' abilities to generate programs for visual question answering tasks without the need of additional training.
Given a description of available functions in a DSL, corresponding usage examples, and a natural language prompt of the visual question, both of these works can generate a program that returns answers to the input question upon execution.
% \maneesh{We should talk about the limitations of these methods in more detail.}
However, in case of wrong results being produced by the programs, the user needs to manually inspect the programs for debugging.

% TIFA
Prior works in image generation have used LLMs and VLMs to verify if the generated outputs faithfully follow the input descriptions.
For a prompt like ``a person sitting on a horse in air over gate in grass with people and trees in background,'' TIFA~\cite{hu2023tifa} uses an LLM to generate question-answer pairs (e.g. [``Is the horse in air'', ``Yes'']).
It then uses a VLM to answer these questions about the generated images.
The more questions are correctly answered, the higher the faithfulness is for that image. 
%%VP T2I
VPEval~\cite{cho2023vpt2i} uses an LLM to generate visual programs to evaluate text-to-image generation results.
% Davidsonian
Cho et al.~\shortcite{cho2024davidsonian} expanded on this approach by using a davisonian scene graph to keep track of dependencies among evaluation questions (e.g. to answer ``is the horse in air'', the question ``is there a horse'' needs to hold true first). 
This approach ensures that invalid questions do not affect the faithfulness result. 
% \maneesh{Do we use Davidsonian graphs in our work. If so we should probably describe them in more detail using a figure.}
% \maneesh{How does our work differ from these. What are the limitations of these methods?}
In our work, we account for dependencies among constraints by decomposing complex expressions into atomic predicates chained together with logical operators (Figure~\ref{fig:teaser}).


% scenecraft
SceneCraft~\cite{hu2025scenecraft} utilizes LLMs to both synthesize and verify 3D scene programs.
They use an LLM as a ``planner'' to convert input scene descriptions into a scene graph that expresses constraints between objects.
The LLM then act as a ``coder'' to produce a scene program based on the graph.
% \maneesh{Not sure what an ``LLM coder'' is. Should explain.}
Later, another LLM instance verifies if the generated program satisfies the constraints described in the scene graph as a reviewer.
If not, the reviewer asks the LLM coder to generate a revised programs for improvement.
% \maneesh{Does it ask for a ``new'' program or a ``revision''. Should state limitations and why we are better.}
%
% \maneesh{Should explain why semantic parsing is preferable to using an LLM here and in the introduction. What can we handle that previous approaches cannot handle?}
In our work, we adopt the program synthesis with verification
approach. 
However, the non-deterministic nature of LLMs and VLMs makes them less reliable as verifiers~\cite{brown2024large}.
Therefore, we build a rule-based semantic parser based on syntactic parsing, pattern matching, and text classification to convert input motion graphics prompts into a verification program written in our proposed FOL-based DSL.


%% scene language related work

% ~\cite{sueyoshi2024predicated}
% frameNet: https://framenet.icsi.berkeley.edu/

% LocalWords:  LLMs CarSim WordNet Zhang et al pre labeler ARF LLM
% LocalWords:  KeyFramer SVG LogoMotion VLM SVGs ToonCrafter VISPROG
% LocalWords:  ViperGPT neuro DSL VLMs TIFA Cho cho davidsonian Artzi
% LocalWords:  davisonian SceneCraft disjunctions Zettlemoyer CCG Hsu
% LocalWords:  shortcite Reddy binarization binarized zhang artzi
% LocalWords:  ModelScope VideoCrafter Videofusion iteratively intra
% LocalWords:  verifiers
