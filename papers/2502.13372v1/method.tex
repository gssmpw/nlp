\section{Method}

\maneesh{I don't follow the organization of this section. Perhaps we need to quickly walk through an overall pipeline figure (1 paragraph max). But the figure will have to be carefully designed. We should talk about that.}

\subsection{FOL-based Verification DSL}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/predicate_table.png}
    \caption{
        Our first-order logic DSL 
    }
    \label{fig:table_predicate}
\end{figure*}

Our verification programs are written in our verification DSL based on first-order logic (FOL).
The FOL-based DSL consists of functions and predicates (functions return boolean values) designed based on prior cognitive psychology literature that analyzes how people think about and describe objects and motions (see the full list in Table~\ref{fig:table_predicate}).
We can use these functions and predicates, along with logical operators, to build up logical statements that express complex descriptions of motions.
For example, we can express the natural language sentence ``translate the square upward by 100 px'' with $\exists \texttt{m. type(m, "translate")} \land \texttt{direction(m, [0, 1])} \land \texttt{magnitude(m, 100.0)} \land \texttt{agent(m, o)}$, where \texttt{o} is a variable representing the square.
%
% What are the entities (objects and motions)
% A single motion has a type, direction, etc
In our DSL, there are two basic variable types: objects that each has properties like shape, color, position, and size, and motions that each has attributes like type, direction, agent, etc (full list in Table~\ref{fig:table_predicate}).

% Describe the DSL (look at Whatâ€™s Left)
% Given a set of motions and objects, can resolve the FOL predicates
Given an animation program, we resolve the logical statements and return verification results that describe which part of the animation fails the constraints expressed by the logical statements.
%
For an input animation program, we first convert it into a 2D array data representation that our DSL operates on.
Each row of the 2D array corresponds to an object in the animation program, and each column represents a frame.
For each cell $(i,j)$ of the 2D array, we store the transformation information applied to object $i$ at frame $j$.

\jiaju{TODO: describe in more detail, maybe per predicate}
Each predicate in our DSL then looks at every cell and checks if the content in that cell satisfy its input parameters.
If so, it marks that cell as true, and false otherwise.
As the result, each cell returns a 2D array of boolean values, and each logical operator, such as and and or, joins the results of different predicates by performing element-wise boolean operations.

% How you execute the predicate on the 2D table
% Give the table, describe each predicate


% \maneesh{Not sure what this subsection is for.}

% Object predicates:
% \begin{description}
%       \item[\texttt{shape(obj, shape\_str)}]
%       \item[\texttt{color(obj, color\_str)}]
% \end{description}

% \noindent
% Motion predicates:
% \begin{description}
%       \item[\texttt{type(mot, type\_str)}]
%       \item[\texttt{direction(mot, dir)}]
%       \item[\texttt{magnitude(mot, mag)}]
%       \item[\texttt{origin(mot, org)}]
%       \item[\texttt{duration(mot, dur)}]
%       \item[\texttt{agent(mot, obj)}]
%       \item[\texttt{post(mot, expr)}]
% \end{description}

% \noindent
% Interval predicates based on~\cite{allen1983interval}
% \begin{description}
%       \item[\texttt{preceds(motA, motB)}]
%       \item[\texttt{meets(motA, motB)}]
%       \item[\texttt{overlaps(motA, motB)}]
%       \item[\texttt{finished\_by(motA, motB)}]
%       \item[\texttt{contains(motA, motB)}]
%       \item[\texttt{starts(motA, motB)}]
%       \item[\texttt{preceded\_by(motA, motB)}]
%       \item[\texttt{met\_by(motA, motB)}]
%       \item[\texttt{overlapped\_by(motA, motB)}]
%       \item[\texttt{finishes(motA, motB)}]
%       \item[\texttt{during(motA, motB)}]
%       \item[\texttt{started\_by(motA, motB)}]
%       \item[\texttt{equals(motA, motB)}]
% \end{description}

\subsection{Semantic Parser}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig_parser.pdf}
    \caption{
        The processing pipeline of our semantic parser. 
    }
    \label{fig:parser}
\end{figure*}
    
% \maneesh{Will be a lot easier to understand with a concrete example of a prompt and figures illustrating the main ideas. It will be easier to update this section as well once figures have been added -- rough, even handdrawn is best for now. Opening should explain the overall goals and a high-level walkthrough of the steps of the approach and assumptions made at each step.}

% \maneesh{The current version of the methods section reads like a list of things we do and mostly suggests that we string together lots of previous tools. I think it will be much stronger if it is written to emphasize the key ideas that we are bringing to make everything work. For example why the six categories of motion attributes. How did we choose those? These like this should be emphasized at the very beginning of this section if possible.}

%% motion attributes
% \maneesh{Why these six categories?}
\jiaju{Might be better to talk about these motion attributes when
  introducing the FOL DSL}
By analyzing parameters in existing animation software and
API~\cite{TODO} and summarizing relevant prompts found in the
VidProm~\cite{wang2024vidprom} dataset, we identify seven types of
motion attributes onto which constraints can be placed: \textit{type}
(type of motion, one of translation, rotation, and scaling),
\textit{agent} (object that performs the motion), \textit{direction},
\textit{magnitude}, \textit{origin} (for rotation and scaling),
\textit{timing} (duration and sequencing), and \textit{post state}
(e.g. end at a specific position).
\maneesh{I don't follow why we are explaining what is in this paragraph. It seems out of place. Goal needs to be explained first. I also don't understand how these things are constraints. We do we mean by the term ``constraint''?}

The goal of our semantic parser is to extract these types of
constraints placed on motions and their sequencing from an input
natural language description of a motion graphics animation and to
assemble these constraints into a verification program in the
FOL-based DSL described above.
\maneesh{I don't follow this goal because I don't know what we mean by ``constraint'' and I don't understand how the DSL is FOL-based, or what it even means to be FOL-based.}
%
% \maneesh{Should motivate each part of the pipeline. An example prompt can help with this. In general each part/step/stage of the work should be motivated.}
%
As shown in Figure~\ref{fig:parser}, our semantic parsing pipeline
consists of four stages: (1) we resolve object and motion
co-references (e.g. identifying that ``it'' refers to ``the black
square'', (2) we break the input prompt into atomic sentences
without complex clauses such that each sentence describes one
continuous motion (e.g. breaking down ``it rotates, while expanding''
into ``it rotates'' and ``it expands'', (3) we build a list of
constraint predicates for information contained within each atomic
sentence (e.g. \texttt{type(m, ''rotate'')} and \texttt{magnitude(m,
  90)} from ``rotates by 90 degrees'', and (4) we compose the final
verification program by adding timing predicates that describe
inter-sentence sequencing relationships.
\maneesh{Instead of intra- and inter- sentence processing stages we should give those two stages more descriptive names. Stage 4 seems to be about sequencing and stage 3 about single motions, or some such.}

In contrast to prior approaches \maneesh{citations here}, our parser can extract multiple
constraints added by a single expression (conflation) with a
multi-label text classification model in Stage 3 (e.g. \texttt{type}
and \texttt{direction} from the verb ``expand'' in
Figure~\ref{fig:parser}).  We describe each stage of our pipeline in
detail below. \maneesh{This paragraph seems out of place. I don't follow what a conflation is from the previous discussion and because we haven't explained how our stages work I can't follow the differences with prior work. Needs to move elsewhere.}

\subsubsection{Stage 1: Object and motion co-reference resolution}
The goal of this stage is to identify segments of text that refer to
the same object or motion.
%so that we do not add duplicate predicates
%later in the pipeline.
%
To resolve co-references to objects, we use the fastcoref
model~\cite{otmazgin2022fastcoref}.  As shown in
Figure~\ref{fig:parser}, this places``The black square'' and
``it'' in the same co-reference cluster.
%
For motion co-reference resolution, we first identify event triggers
(words that introduce a motion) based a set of heuristics.
Specifically, for each noun and verb in the input prompt, we check if
its lemmatized form matches words that describe a motion in our
vocabulary (e.g. ``spin'' is a word that introduces a rotation
motion). \maneesh{What does ``in our vocabulary'' mean. Where does
  that vocabulary come from. Need to explain this in more detail
  because this sounds very brittle -- it sounds as if we are giving
  the system a pre-specified vocabulary.}  If there is a match, then
we mark it as an event trigger.  We then match event triggers to each
other by comparing their modifiers and their positions in a
sentence~\cite{ahmed2023lemmamatching, kenyondean2018lemmaheuristic}.
\maneesh{Need  more explanation of how this matching occurs. What is the algorithm. Can come after the example.}
For example, in the sentence ``The square spins. While rotating, it
also scales'', ``spins'' and ``rotating'' are identified as event
triggers.  Moreover, because they both operate on the same object, and
``rotating'' sits within a subordinate clause, then we identify them
as co-references referring to the same motion of rotation.
% \maneesh{We should motivate each step. Why do we need three different prior techniques to do the co-reference resolution. Need to include detail about what the trigger words are. Paper needs to be re-implementable after reading. Right now the text is a bit too vague for that.}

\subsubsection{Stage 2: Text simplification}
At this stage, we aim to break down the original input prompt into
atomic sentences that each describes a single continuous motion.
This stage sets up the extractions of intra- and inter-sentence
constraints later. \maneesh{The motivation sentence is not that useful. Should be motivated more in terms why it is necessary to do the simplification. The current motivation begs the question of why intra- and inter- sentence stages are necessary.}

%
Prior works on text
simplification~\cite{zhang2019generating, ferres2016yats} have identified six types of syntactic structures for
simplification: passive constructions, appositive phrases, relative
clauses, coordinated clauses, correlated clauses, and adverbial
clauses. \maneesh{Simplification in what sense? Should explain why these six structures are relevant to breaking the prompt into simple sentences -- e.g. sentences that describe a single motion.}
%
We do not simplify appositive phrases as they modify nouns and do
not often introduce new motions.
\maneesh{I don't understand the previous sentence because I don't know what an ``appositive phrase is''. Also why should I as a reader believe you that they do not introduce new motions.}
We also do not process correlated
clauses (e.g. either...or...) as they are rare when describing
motions.  \maneesh{Again, why should I believe you?} For passive constructions, we process them directly in the
next stage. \maneesh{Why are are forcing passive constructions into the next stage? If the passive construction includes multiple motions it should be handled here -- no? Otherwise I'm not sure what simplification is about breaking sentences into single motions.}

Therefore, we simplify three syntactic constructions at this stage:
\textit{Coordinated Clauses}, \textit{Adverbial Clauses}, and
\text{Relative Clauses}.  We first identify if any of these
constructions exists in the input prompt based on dependency relations
obtained with a roberta-based transformer
model~\cite{liu2019roberta, honnibal2020spacy} and then recursively perform the
following set simplification operations based on clause type, until no
more complex structures are present.

\paragraph{Coordinate clauses} We identify this type of clause by the presence of the conjunctive ``and''. 
We split the sentence based on the number of conjuncts and copy relevant sentence parts. For example, given ``Rotate the blue and black square.'', we would edit it to become ``Rotate the blue square'' and ``Rotate the black square.''

\paragraph{Adverbial Clauses}
This type of clauses usually joins two motions together, such as the
second sentence in Figure~\ref{fig:parser}.  We identify this
construction if a verb has the ``advcl'' dependency tag and a ``mark''
word (such as ``while'' and ''before'') is present. \maneesh{Where are the dependency tags coming from? What do they mean?}
%
We first check if the motion contained in the adverbial clause (``while expanding by 2'' in Figure~\ref{fig:parser}) is a co-reference or not.  \maneesh{How is the check done?}
If not, then we split the adverbial clause into a standalone sentence from the main clause as it introduces a new motion (sentences with yellow and blue underlines in Figure~\ref{fig:parser}).
Otherwise, we leave it as-is since it is adding a timing constraint to the motion described in the main clause.

\paragraph{Relative Clauses}
This construction is less common but we process it similarly as
adverbial clauses.  We identify this type of clauses by the presence
of a noun labeled as ``relcl.''
\maneesh{I don't understand this description. Where are the dependency labels coming from and what do they mean.}
We similarly check if the motion
contained in the relative clause is a co-reference or not, and only
split the sentence if it is not. 


\subsubsection{Stage 3: Intra-sentence processing}
% \maneesh{What is a ``single motion''?} 
% \maneesh{Why is this assumption a safe assumption at this point? Should talk about these assumptions in opening of this section.}
% We assume that each input is an atomic sentence that describes and modifies a motion, and the motion is introduced by a verb (e.g. the black square rotates by 90 degrees).
At this stage, the original input prompts has been processed into
atomic sentences where each one introduces a single continuous motion
(Figure~\ref{fig:parser}).  The goal of intra-sentence processing is to extract
motion constraint predicates from each sentence, including from
\textit{conflated} expressions. \maneesh{Need to explain what a ``conflated expression'' is earlier and provide an example.}

For each sentence, we first use a roberta-based transformer model~\cite{liu2019roberta} \maneesh{what is a Roberta based transformer model. Should cite it} to
obtain its syntactic dependency tree via
spaCy~\cite{honnibal2020spacy}, whose root node is the motion verb
(see Stage 3 in Figure~\ref{fig:parser}).
The root node and each of
its subtrees can be seen as a modifier that adds constraints to a
motion (e.g. ``by 90 degrees'' and ``around its center'' attached to
``rotates'' in Figure~\ref{fig:parser}). 

For each subtree, we identify what types of motion attributes the phrase is adding constraints to by classifying it with a fine-tuned sentence transformer. \maneesh{prev sentence is awk.}
Specifically, we use SetFit~\cite{tunstall2022setfit} to fine-tune a sentence transformer on a multi-label vocabulary dataset of example attribute descriptions we curated manually. \maneesh{Where does the vocabulary come from. How brittle is this. Need more detail.}
% conflation!!
In this way, we can account for conflation cases where a single phrase
introduces more than one constraint by adding predicates for each
label. \maneesh{I don't understand the previous sentence as I don't follow how well SetFit does the classification or knows to treat words that expand into multiple constraints.} As shown in Figure~\ref{fig:parser}, the verb ``expanding'' is
parsed into two constraints: \texttt{type(m\_3, ``scale'')} and
\texttt{direction(m\_3, [1.0, 1.0])}.

% clausIE
% \maneesh{I;m a bit lost. Not sure where we introduced parse trees and root nodes. Should illustrate these things. More importantly should motivate why we are doing each step before we describe the step. Why do we need the finetuned SetFit transformer and how exactly do we finetune it.}
%
% describe object predicates
%

For each type of constraint that a phrase adds, we further process the content in the subtree to parse out arguments for the corresponding predicate.
For example, we identify ``by 90 degrees'' as adding a constraint on the \texttt{magnitude} attribute of the rotation motion.
Within the subtree of the phrase ``by 90 degrees'', we then look for a node with part of speech NUM (i.e. the node of the text ``90'') and use it to construct the predicate \texttt{magnitude(m\_2, 90)}.
\maneesh{What is the algorithm for this parsing procedure?}
\jiaju{Need to add more details on how we process direction/magnitude for scale}

% \maneesh{How do we know if a subtree falls in the categories. What is the algorithm for identifying the constraint. Examples need to be accompanied by a general algorithm.}
% \jiaju{TODO: write this in more detail}

\subsubsection{Stage 4: Inter-sentence processing}
An important part of describing motion graphics is the sequencing of
motions.  In the last stage of our parsing pipeline, we iterate
through every atomic sentence and look for expressions that add
sequencing constraints between motions.  We again use a fine-tuned
transformer model for classification.
\maneesh{For classification into what? What exactly do we do? What is the algorithm?}
As shown in
Figure~\ref{fig:parser}, the word ``then'' signals that the rotation
motion needs to happen after the translation motion, and ``while''
indicates that the scaling needs to take place at the same time as the
rotation.  We map each of these words to their corresponding timing
predicates as described in Allen's interval
algebra~\cite{allen1983interval}.
\maneesh{This interval algebra needs to be described in more detail and likely with a figure.}
The final output verification
program is shown at the bottom of Figure~\ref{fig:parser}.
% For example, if a sentence contains the word ``then'' as an adverb modifying the verb, we create a timing predicate that checks whether if the motion described in this sentence takes place after the previous sentence (e.g. \texttt{preceds(m\_1, m\_2)}).
% \jiaju{TODO: write this in more detail}

% LocalWords:  DSL str dir dur preceds motA motB fastcoref Intra NUM
% LocalWords:  spaCy subtree SetFit finetuned finetune TODO expr pre
% LocalWords:  VidProm intra lemmatized roberta advcl relcl subtrees
% LocalWords:  prev awk
