\section{Results}
\label{sec:results}

Figures~\ref{fig:teaser}, \ref{fig:verification}, \ref{fig:haha} and~\ref{fig:wowmom}  show
examples of motion graphics animations generated with our
synthesis and verification pipeline (more examples can be found in the
video and supplemental materials).
%
%In both examples, the text prompt describes a variety of
%spatio-temporal properties that the animation should include.  The
%LLM-generated \dslname{} program encodes these desired properties as
%logical statements.
%
%Executing this \dslname{} program on the LLM-generated motion graphics animation
%produces a verification report with boolean values for each
%logical statement and its constituent predicates.

%% example in teaser
In Figure~\ref{fig:teaser}, the prompt asks for an upward
translation of the orange circle above the rectangle and a 90-degree
rotation of the letter H with both motions overlapping in
time. The intention is that the animation should end after the shapes
form the word Hi.
While the LLM-generated \dslname{} program captures the spatio-temporal
properties described in the prompt, 
the LLM-generated animation 
does not.
%
As shown in the animation frames and found by the verification
report, there is no valid motion \texttt{m}$_1$,
where the circle
translates to the top of the rectangle, and so both \texttt{top()}
and \texttt{post()} are false.
%
A valid \texttt{m}$_2$ is found as the letter H was
rotated by 90 degrees clockwise.
%
But, since \texttt{m}$_1$ fails, the temporal
predicate \texttt{while()} also fails since one of its input motions is
invalid.
%
%Note that, even if both \texttt{m}$_1$ and \texttt{m}$_2$ were valid,
%the \texttt{while()} predicate could still fail if the motions did not
%overlap in time.
%



%% example in results
In Figure~\ref{fig:verification}, the prompt describes a
series of motions that aim to move the letters Y and S adjacent to the letter 
E.
% \maneesh{The prompt is also unclear about where the S should be. Should fix this.}
The \dslname{} program captures these desired properties and
the verification report
% generated by executing the \dslname{} program on the LLM-generated animation
indicates that \texttt{m}$_1$, the motion of the letter Y, 
is false because \texttt{post()} failed,
even though its input predicate \texttt{left\_border()}
evaluates to true.
This is because 
at frame 80, the letter Y does border
E on its left side.  However, because the Y continues its translational motion
\texttt{m}$_1$ until frame 120, the \texttt{post()} predicate fails.
Since \texttt{m}$_1$ is invalid, the temporal
predicate \texttt{before()} also fails.
% Further, the translation of the letter S takes place over the same period of time as the translation of the letter Y, which fails the requirement that these two translations should happen sequentially.
% mention the human user can also make corrections too?
Figure~\ref{fig:iteration} shows the next two automatic correction iterations of our
pipeline for the example in
Figure~\ref{fig:verification}.
% In this case, we automatically fed the verification error report back to the LLM animation synthesizer.
In correction iteration 1, the verifier reports that the letter Y
now properly borders the letter E at the end of the translation motion \texttt{m}$_1$.  Although
both \texttt{m}$_1$ and \texttt{m}$_2$ are valid, the temporal predicate
\texttt{before()} still fails because the motions do not
occur sequentially.
%
In the second iteration, the animation synthesizer fixes this
problem, and every statement and predicate in the \dslname{}
program evaluates to true, confirming that the final animation
correctly matches the input prompt.
%
Figures~\ref{fig:haha} and~\ref{fig:wowmom} similarly walk through examples that
require multiple correction iterations.

%Once we have the verification result against an animation, we can
%directly feed the result messages, along with the \dslname{} DSL
%documentation and the verification program, back into the LLM
%animation synthesizer to ask it for correction.  As the LLM
%synthesizer has access to all the previous animations it has generated
%and the verification results for each of them, we are in essence
%supplying the LLM synthesizer with negative in-context examples to
%help it make corrections in an iterative manner~\cite{CITE}.  As shown
%in Figure~\ref{fig:iteration},in the LLM synthesizer's first attempt
%to improve the animation, running the verification program indicates
%that the letter Y now properly border the letter E after being
%translated, but their respective translations still did not happen
%sequentially.  We run the feedback loop again, and, in the second
%attempt, the remaining problem of non-sequential translations was
%fixed.  As all logical statements have evaluated to true, we exit the
%feedback loop and output the final animation.


\section{Evaluation}
\label{sec:evaluation}

%We evaluate our synthesis and verification pipeline in
%two ways. (1) We first test the accuracy of our LLM-based
%\dslname{} program synthesizer in generating \dslname{} code from
%text descriptions of a motion graphics animation.  (2) We
%then test how many iterations our fully automatic pipeline requires to
%generate an animation that successfully passes the the corresponding
%verification program.

\vspace{0.5em}
\noindent
{\bf \em Test dataset.}
To evaluate our LLM-based synthesis and verification pipeline, we
require a test dataset consisting of text prompts describing a motion
graphics animation and the corresponding ground truth \dslname{}
programs.
%
Since no such dataset is readily available, we synthetically
build one using a template-based approach in combination with an
LLM. Section B in supplemental materials details our
dataset contruction. %\maneesh{ref to appendix}
%
%
We divide the test dataset into four categories based on the
types of \dslname{} predicates they exercise.
\begin{enumerate}[leftmargin=0.5cm]
    \item {\bf \em Single atomic motion.}  Each prompt describes
      one atomic motion with up to seven of its attributes as in Table~\ref{tab:predicates} (e.g., ``Expand the square by 2.'').
      
    \item {\bf \em Spatially relative motion.}  Each prompt
      describes one atomic motion that is relative to the spatial
      position of another object to exercise predicates from the
      rectangle algebra (Figure~\ref{fig:allen_rectangle} bottom) (e.g., ``Move the square next to the circle.'').
    
    \item {\bf \em Temporally relative motions.}  Each prompt describes 
      multiple atomic motions with relative temporal sequencing relations to
      exercise predicates from the Allen temporal interval algebra (Figure~\ref{fig:allen_rectangle} top). (e.g., ``Move the square,
      while scaling the circle'').
    
    \item {\bf \em  Spatio-temporally relative motions}: Each prompt
      describes multiple atomic motions with both spatially and temporally relative
      relationships (e.g., ``Move the circle next to the square, while it rotates'').
\end{enumerate}
\noindent
Our test dataset includes a total of 5600 pairs of prompts and \dslname{} programs (1200 single
atomic, 1400 spatially relative, 1200 temporally
relative, and 1800 spatio-temporally relative).
%
%\maneesh{May want to talk about break down between fully LLM generated and template generated if there is a difference due to that.}
%\jiaju{talk about it in appendix that we produced more}
%
We will release this dataset to further promote research in the field.


%% We evaluate 1) the LLM-based program synthesizer's ability to parse a
%% natural language motion prompt into a \dslname{} verification program
%% and 2) the LLM-based motion graphics generation pipeline's ability to
%% output an animation that pass all checks in the \dslname{}
%% verification program from an input prompt.

%% \subsection{Motion prompt dataset}
%% We build a dataset consisting of motion prompts and their
%% corresponding ground truth \dslname{} motion verification
%% programs. \jiaju{should we explain why not ground truth animation?}
%% %
%% % \jiaju{TODO: dataset composition}
%% We generate motion prompts in four categories, parametrized by the presence of types of predicates in our DSL.
%% \jiaju{The category names are temporary.}
%% We present the categories below, and more details about how we generate prompts can be found in Section~\ref{sec:prompt_generation}.
%% \begin{enumerate}[leftmargin=0.5cm]
%%     \item \textit{Single atomic motions.}
%%     Each motion prompt within this category describes only one atomic motion with its seven attributes (Table~\ref{tab:predicates}).
%%     No reference to other objects or motions appears in this category of prompts.
    
%%     \item \textit{Object-relative motions.}
%%     Each prompt in this category describes a single atomic motion in reference to another object (e.g. ``Move the square next to the circle.''). 
%%     Each prompt involves concepts related to the Rectangle Algebra (Figure~\ref{fig:allen_rectangle})
    
%%     \item \textit{Motion-relative motions.}
%%     Each prompt of this category describes at least two atomic motions with temporal sequencing relations (e.g. ``Move the square, and then scale the circle'').
%%     Each prompt invokes concepts related to the Allen' Interval Algebra (Figure~\ref{fig:allen_rectangle}).
    
%%     \item \textit{Object-and-motion-relative motions}:
%%     Each prompt includes multiple motions with temporal relations and references to other objects in the scene (e.g. ``Move the square next to the circle, and then scale it'').
%% \end{enumerate}

\input{table_eval_parsing}

\input{table_eval_pipeline}

\begin{figure*}[t]
  \centering
    \includegraphics[width=\linewidth]{figs/fig_discussion.pdf}
    \vspace{-2em}
    \caption{\dslname{} can help user's identify ambiguous
      prompts. Our pipeline correctly generates a \dslname{} program
      reflecting the user's original prompt and it generates a motion that successfully
      passes verification. However, the user's intention was for the
      blue square to make an arcing motion rather
      than rotating about its center and simultaneously
      translating. Understanding why the animation passed verification
      but did not meet their intentions, the user revised the prompt to
      ask for a rotation only motion and produced the desired result.
%      Note that the \dslname{} program for the revised prompt includes
      %      a check that the motion is a rotation only.
    }
    \vspace{-1em}
    \label{fig:discussion}
\end{figure*}

\vspace{0.5em}
\noindent {\bf \em Accuracy of LLM-based \dslname{} synthesizer.}
Table~\ref{tab:eval_parsing} reports the success rate for our
LLM-based \dslname{} synthesizer across our test dataset.
%
Overall we find that it 
generates correct \dslname{} programs for 95.1\% of the test prompts.  It is
especially successful for prompts across the first three categories
(above 96\%), but less so for prompts in the spatio-temporally
relative category at 90.9\%.
%
Analyzing failure cases, we find that the LLM tends to
confuse relative spatial and temporal concepts.  For example, with a
prompt of the form ``The square translates. After that, the
circle scales'' the LLM \dslname{} synthesizer sometimes produces the predicate
\texttt{after(m$_1$,m$_2$)} instead of \texttt{before(m$_1$,m$_2$)}.
%
%If a prompt says to move the square to touch the circle, the LLM sometimes
%generates \texttt{intersect()} instead of \texttt{border()}, even though
%system prompt documentation gives ``to touch'' as an example of the
%\texttt{border()} predicate.
%
There are also cases where the LLM confuses relative
positioning with absolute directions.  For the prompt ``Move the
square to the right of the circle,'' the LLM can produce
\texttt{dir(m,[1,0])} and \texttt{right(o$_1$,o$_2$)} instead of just
the later, confusing rightward motion with the right-side
positioning.
%\maneesh{Still aren't really saying why spatio-temporal is low
%  compared to the others.}  \jiaju{0119: As relative spatial and
%  temporal concepts always appear together in the same prompt in the
%  spatio-temporally relative category, the LLM synthesizer struggles
%  the most.}
%
Nevertheless, the overall success rate of the LLM \dslname{}
synthesizer suggests that it can produce correct programs
across a variety of prompts.


%% comparsion against the semantic parser
To better understand the generalization capabilities of our LLM-based
\dslname{} synthesizer, we implemented a classical rule-based semantic
parser~\cite{kamath2018survey} for converting a text prompt into a
\dslname{} program. This parser serves as a baseline.
%
Section C in supplemental materials describes the parser in
detail.
%
%% parsing results
% \maneesh{Probably need something here about the categories and differences between them them.}
%
Table~\ref{tab:eval_parsing} shows that, although the semantic
parser achieves a high overall success rate of 83.5\%, it is
not as strong as our LLM-based approach.
%across the four
%prompt categories with 94.3\%.
% \maneesh{Next example should focus on understanding the LLM rather than the semantic parser. What do the failures in the four categories tell us about the LLMs capabilities with respect to the semantic parser. We don't have enough context to understand the problem mentioned below about motion type as a noun instead of a verb. Similarly for the LLM based explanation, we just don't have enough context I think. Let's talk about this.}
%% semantic parser brittle
Examining the unsuccessful prompts, we find that the %rule-based
semantic parser
has trouble generalizing to the variety of sentence structures found in 
our test set prompts.
%synthesized by an LLM.
For example, to describe an upward motion of
an object, a prompt might say ``add an upward translation to ...'' or ``propel
the object to climb ...''  The former embeds the motion type
as a noun, while the latter uses multiple verbs to express the
same atomic motion. Our semantic parser does not contain the rules necessary to handle the former
case and produces two motions for the latter.
%
In general, rule-based methods are brittle as it is difficult to cover all
the cases that can occur in natural language. 
%
% (e.g. embedding the motion type as a noun instead of in a verb), and its underlying off-the-shelf dependency parser might place prepositional phrases at unpredictable parts of the parse tree when the sentences become long, causing it to fail to map phrasal content to \dslname{} predicates.
% ... \maneesh{say something about its problems. brittle for certain classes of prompts.}
% \maneesh{Would we be better off dropping the semantic parser completely from this paper?}
%
%% LLM unpredictable

Yet, even though our LLM-based approach generalizes well to a wide
variety of sentence structures and expressions, we find that it can
fail unexpectedly.  For example, it can randomly fail to include a
\texttt{mag()} predicate explicitly mentioned in the prompt (e.g., ``Rotate the square by 90 degrees'').
It might also output the wrong parameters for predicates,
such as flipping the sign of a directional parameter for \texttt{dir()}
or using \texttt{[1,0]} for an upward translation instead of
\texttt{[0,1]}.
%
There is no clear pattern to such failures and we also find that many 
similar prompts can produce correct results.
%
%In contrast, with the rule-based semantic parser, we can examine
%failures and possibly debug the rule-set to fix the parser.
%
%And because the baseline semantic parser performs relatively well
%overall, it may be possible to use both the semantic parser 
%and the LLM-based approach to 
%further reduce \dslname{} synthesis errors (e.g., to check one another). \maneesh{kill last sentence or move to future work?}
% \jiaju{0118: also use wrong [parameter format]}
%




\vspace{0.5em}
\noindent {\bf \em Effectiveness of LLM-based synthesis and verification pipeline.}
Table~\ref{tab:eval_pipeline} evaluates the effectiveness of
our fully automatic LLM-based motion graphics synthesis and verification
pipeline.
%(Figure~\ref{fig:pipeline}).
%
It shows that with 0 correction iterations the LLM-based animation
synthesizer produces a motion graphic that successfully passes all the
checks in the ground-truth \dslname{} program for 58.8\% of our test
prompts (pass@0 metric).  It requires one or more correction
iterations to successfully generate another 34.8\% of the test prompts
(pass@1+), and it fails to pass the ground-truth \dslname{} program
after 49 correction iterations for 6.4\% of the test prompts (fail).
We also see that for the pass@1+ test prompts, the average number of
iterations is 5.8 with a (min-max) range of 1-38.
%
Overall these numbers suggest that the automatic verification
and correction iterations in our pipeline (pass@0 + pass@1+)
significantly improves the number of correct animations produced,
compared to using only a single feed-forward pass of LLM-based
animation synthesis alone (pass@0). 


We also observe that correction iterations are especially effective
for the spatially and spatio-temporally relative prompts, as the
iterations fix 51.3\% and 53.4\% (pass@1+) of these cases
respectively.
%
Examining the test prompts in these categories, we find that
initially our LLM-based animation synthesizer often neglects the
height or width of objects when translating one to a position relative
to another, but such errors are often corrected within a couple of iterations.
%
%\jiaju{0120: Rewrote the previous sentences to: \\
Examining the test prompts in these categories, we find that
initially the LLM-based animation synthesizer
has trouble interpreting
certain relative spatial concepts. %the way \dslname{} defines them.
For example, for the test prompt ``Translate the black square to touch
the blue circle,'' the synthesizer might initially move the square to
fully overlap the circle, rather than moving it to the border of the circle.
%instead of treating it as
%\texttt{border()}.
Errors like these can often be corrected within a
couple of iteration as the feedback from the verification report allows
the LLM to fix the problem.
%we supply the \dslname{} documentation to the
%synthesizer.
%}
% \jiaju{0119: ``Translate the black square to touch the blue circle.''}
% ).
%%
%

%Test prompts that require more correction iterations often ask for a
%rotation or scale motion to position one object relative to another.
%In these cases, the LLM-based animation synthesizer sometimes defaults
%to use translation despite the explicit specification to do otherwise
%in the prompt.
% \jiaju{0119: ``The blue circle is rotated to be on the top side of the black square.''}
%\jiaju{0120: Rewrote the previous sentences to: \\
%
Test prompts that require more correction iterations often ask for a
rotation or scale motion to position one object relative to another.
For example, with the prompt ``The blue circle is rotated to lie
on the top side of the black square'',  the LLM-based
animation synthesizer sometimes defaults to using translation despite
the explicit specification otherwise in the prompt.
%}
%
It insists on using a translation for a few iterations, but after the
verification report marks the motion as failing, it eventually
corrects the motion to use a rotation.
%
%Though it can still struggle to generate the correct origin and
%magnitude of the transformation for a few more iterations.
% \maneesh{I think we should cut the remainder of the paragraph.}
% %
% % correct midpoint between the current and reference object to use as the origin of rotation 
% Sometimes, it might even start to explore using the remaining type of
% motion (e.g. when being asked to use rotation, it would switch to use
% scale).  From here, the LLM synthesizer would either gradually improve
% the animation based on the verification report within a few iterations
% (pass@>0: 51.3\% of the spatially relative prompts and 53.4\% of the
% spatio-temporally relative prompts), or continue to diverge until
% timed out (10.9\% and 11.0\% respectively).


While the numbers in Table~\ref{tab:eval_pipeline} are based on
assuming the pipeline has access to ground truth \dslname{} programs,
in practice our pipeline generates \dslname{} program
using an LLM.
%
As shown in Table~\ref{tab:eval_parsing} our pipeline generates
incorrect \dslname{} programs for 272 (4.9\%) test prompts. Running
these prompts through our full pipeline with incorrect \dslname{}
programs, we find 70 (25.7\%) pass@0, 20 (7.4\%) fail, 134 (49.3\%)
pass@1+ and these require an average of 6.7 (1-32) iterations, while
48 (17.6\%) generate \dslname{} programs that fail to execute.
%
In other words, running our full pipeline on these prompts would
falsely report that it produced correct animations for 204 (75\%) of
the prompts and that it produced incorrect animations for the
remaining 68 (25\%) prompts.


%In addition, we take the 272 test prompts for which the LLM \dslname{}
%generated incorrect verification programs in the previous experiment
%and feed them to our fully automatic pipeline.  Here we report the
%metrics: pass@0: 70 (25.7\%), time-outs: 20 (7.4\%), pass@>0: 134
%(49.3\%), avg. \# of iter.: 7.7 (2-33), error from inexecutable
%\dslname{} programs: 48 (17.6\%).  We observe a significantly low
%pass@0 at 25.7\%, which is expected as the input text prompt does not
%match completely with the \dslname{} program.



%% -----------------------------

%% Table~\ref{tab:eval_pipeline} reports the pass@0 metric and the
%% average number of iterations that our fully automatic pipeline (Figure~\ref{fig:pipeline})
%% % (with LLM-generated verification programs) \maneesh{maybe we should do this with ground truth -- or maybe with both?} 
%% takes to generate an animation program that passes all checks by the
%% ground-truth \dslname{} program.
%% % \maneesh{End of this section should say what the overall results are if we combine the unsuccessful verification programs with iteration results -- in terms of animations that pass an incorrect \dslname{} program and animations that fail an incorrect \dslname{} program. }
%% \maneesh{Next sentences have to go at very end of this section.}
%% In addition, we take the 272 test prompts for which the LLM \dslname{}
%% generated incorrect verification programs in the previous experiment
%% and feed them to our fully automatic pipeline.  Here we report the
%% metrics: pass@0: 70 (25.7\%), time-outs: 20 (7.4\%), pass@>0: 134
%% (49.3\%), avg. \# of iter.: 7.7 (2-33), error from inexecutable
%% \dslname{} programs: 48 (17.6\%).  We observe a significantly low
%% pass@0 at 25.7\%, which is expected as the input text prompt does not
%% match completely with the \dslname{} program.

%% % \maneesh{Present the overall results - -then breakdown by category. Also we should spend less space talking about pass@1. We basically need to say pass@1 is ?? overall, but fairly good for categories X and Y and not good for categories A and B}
%% We observe that our pipeline overall can produce animations without the need of correction for 58.8\% of the time.
%% It performs well on the single atomic and temporally relative prompts (88.3\% and 88.5\% respectively).
%% % animation directly without needing any correction iterations (pass@1) for 88.3\% of the atomic prompts and 88.5\% of the temporal prompts.
%% However, we observe significant drops in pass@0 (37.9\% and 35.6\%) for spatially and spatio-temporally relative prompts.
%% On average, our pipeline needed 7.2 and 7.7 correction iterations before producing an animation
%% that passes all the checks respectively.
%% %% reasons
%% % 1) relative positioning requires computing various magnitudes and determining the center of rotation/scaling, which the LLM is known to be struggling with (CITE).
%% %
%% %
%% % \maneesh{We don't want to examine the pass@1 results. Our focus needs to be on using the verification report and the correction iterations in this section. I would significantly reduce or better yet, cut the rest of this paragraph.}
%% % \maneesh{Should say something about the trends in number of iterations and whether the programs or getting worse iteration to iteration -- could possibly count the number of predicates/statements that are false.}
%% Examining the pass@>0 cases, we observe that the LLM synthesizer often neglect the height and width of objects when translating one to a position relative to another.
%% This can often be corrected within a couple of iterations.
%% %
%% Test prompts that require higher number of correction iterations often ask for a rotation or scale motion to position an object relative to another.
%% In these cases, the LLM synthesizer often defaults to use translation despite the explicit specification.
%% It would also insist on using this strategy for a few iterations.
%% After the verification report repeatedly states that translation cannot be used, the synthesizer eventually uses rotation or scale but then struggles to compute the correct origin of transformation or the appropriate magnitude.
%% % correct midpoint between the current and reference object to use as the origin of rotation 
%% Sometimes, it might even start to explore using the remaining type of motion (e.g. when being asked to use rotation, it would switch to use scale).
%% From here, the LLM synthesizer would either gradually improve the animation based on the verification report within a few iterations (pass@>0: 51.3\% of the spatially relative prompts and 53.4\% of the spatio-temporally relative prompts), or continue to diverge until timed out (10.9\% and 11.0\% respectively).

%% % \jiaju{01-18: maybe cite related work here.}

%% % \jiaju{TODO: can also present some interesting cases where the LLM did the unexpected e.g. the example of using both rotation and translation while the prompt only said rotate}
%% % With the help of the verification report, we observe that the LLM-based animation synthesizer is able to iteratively correct the animation by learning from report feedback, despite an increase in the number of iterations compared to other prompt types.  \maneesh{I don't follow the despite clause.}
%% %

\vspace{0.5em}
\noindent
{\bf \em Discussion.}
While we have focused on the automated correction iterations, one
benefit of our approach is that the verification reports are human
interpretable. Users can adapt their prompts
based on the report feedback. 
%
Figure~\ref{fig:discussion} shows an example from a user in our lab who gave the
pipeline the prompt ``Rotate the blue square to intersect with the
black square'', intending for the blue square to move in an arcing motion
about the black square.
%
Our pipeline produced an animation where the blue
square only rotated around its own center, but simultaneously
translated on a straight path to intersect the block square.
%
%This animation
It correctly passed the \dslname{} program and the user
realized that the prompt was ambiguous. Several different
animations could be consistent with it.
%
\dslname{} verification helped the user adapt their prompt to be more
precise and request ``rotation only'' motion and thereby debug the prompt.

%\maneesh{would need figure on figures page if we keep this
%  example. Should show inital result, iteration 8, and
%  then the update to the prompt to get the right result.}
    
%Upon examining some animations that pass all verification checks, we find some interesting cases where the final animation produced by the LLM synthesizer points out the ambiguity in the input text prompt.
%For example, for the prompt ``rotate the blue square to intersect with the black square,'' the LLM produces an animation after 8 iterations where the blue square only rotates around its own center, but added a translational motion over the same period of time as the rotation.
%Since the blue square did end up intersecting with the black one at the end of its rotation, this animation passes the check even though the displacement was brought entirely by the translation.
% Future work on the \dslname{} execution engine needs to be able to disentangle the effect of other motions to address cases like this. 
%This is a valid interpretation of the original ambiguous input prompt, although it might not match with the initial intent of the user.


%% \vspace{0.5em}
%% \noindent {\bf \em Limitations.}
%% \maneesh{Need to talk about real limitations in the scope of our work. What do we know won't work if we feed it to our pipeline -- e.g. complex motion paths. Maybe can also mention using the LLM verifier and the semantic parser together here. though that may be more like future work.}
%% % \maneesh{Need to end with a Limitations heading and Discuss the various limitations.}
%% % \jiaju{01-18: maybe say something like: there are some cases when a
%% %   more direct feedback from humans that offer some potential solutions
%% %   might be helpful, as the verification results only provide true or
%% %   false, hence the increase in number of timed-out prompts}
%% The verification report produced by a \dslname{} program is passive in
%% that it does not suggest potential solutions other than stating which
%% spatio-temporal concepts are satisfied or not.  Having a human in the
%% loop to propose solutions might be helpful in reducing the number of
%% iterations needed for correction.
%% \maneesh{I wouldn't call this prev one a limitation. It reads more like future work.}

%% \jiaju{
%% 0120: One limitation of \dslname{} is its inability to express the formation of shapes across space and time.
%% For example, one can say ``translate the squares to follow an infinity symbol-shaped path or ``arrange the squares to form a circle.''
%% These spatio-temporal properties formed by the coordination of multiple objects and motions are beyond the scope of \textit{atomic motions} and also first-order logic, which primarily operates on single entities.
%% Similarly, motions with different easing functions can be regarded as having complex temporal trajectories and cannot be directly handled by \dslname{}.
%% }


%% \subsection{Evaluation results}
%% \subsubsection{LLM-based verification program synthesizer}
%% \input{table_eval_parsing}
%% To evaluate how well our LLM-based synthesizer can parse an input motion prompt into the correct \dslname{} program, we built a rule-based semantic parser as the baseline.
%% The detailed implementation of the semantic parser can be found at Section~\ref{sec:semantic_parser}.
%% % \jiaju{TODO: Report the success rate of both parsers across categories}
%% We run both the LLM-based parser and the semantic parser on all XXXX prompts in our dataset and report the success rate in Table~\ref{tab:eval_parsing}.
%% We measure the string difference between the output of a parser and the ground truth verification program and count a complete match as success and otherwise failure.
%% Overall, we observe that the LLM-based synthesizer achieve high success rate across all four types of prompts.
%% This is validated by the comparison against the rule-based parser, which achieves a decent performance of XX\% across the board.


%% \subsubsection{Motion graphics generation pipeline}
%% \input{table_eval_pipeline}
%% We run our LLM-based motion graphics generation pipeline on all prompts in our dataset.
%% As described in Section~\ref{sec:llmsynth}, once an animation has been generated, we feedback it back along with the verification result for correction if the animation did not pass all verification checks.
%% We repeat the loop until either the LLM-synthesizer produces a fully-correct animation or until it reaches the maximum number of tries ($n=XXX$).
%% \jiaju{TODO: say that we manually inspect the final output animations to ensure that they indeed fulfills the requirements of the input motion prompt?}
%% % \jiaju{TODO: Report the number of tries across categories}
%% We report the results in Table~\ref{tab:eval_pipeline}.
%% We observe that the generation pipeline performances well in the categories without relative spatial positioning between objects, outputting the correct animation directly without correct in XX\% of the prompts.
%% When it comes to prompts in the object-relative and object-motion categories, we observe a drop in first attempt success rate and an increase in the number of timed-out attempts.
%% The LLM synthesizer on average needed X number of attempts before producing an animation that passes all the checks.
%% Upon examining the multi-attempt and timed-out cases, we make several observations on why the LLM struggled.
%% \jiaju{TODO: some reasons I have thought of so far: 1) relative positioning requires computing various magnitudes and determining the center of rotation/scaling, which the LLM is known to be struggling with (CITE). 2) there are some cases when a more direct feedback that offer some potential solutions might be helpful, as the verification results only provide true or false.}
%% Notes:
%% \jiaju{TOOD: also, can report by motion types, as the LLM struggles more when the motion type is not translation}
%% Notes:
%% % \jiaju{TODO: can also present some interesting cases where the LLM did the unexpected e.g. the example of using both rotation and translation while the prompt only said rotate}

% LocalWords:  verifier LLM iteratively spatio datatest XXXX AST dir
% LocalWords:  LLMs Prev prev translational
