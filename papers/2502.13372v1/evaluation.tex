\section{Results}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig_verification.pdf}
    \caption{Given an input animation, executing a verification program in \dslname{} outputs result messages indicating which parts of the animation satisfy the constraint predicates and which part fail. \maneesh{Let's talk about all these results figures.}}
    \label{fig:verification}
\end{figure*}

\maneesh{Should probably move this example to Section 3.1}
%
Figure~\ref{fig:verificationExample} shows an example animation and a 
\dslname{} verification program that checks for various
spatio-temporal properties we would like it to
include.
%
The verification program can be manually coded, or it can be generated from a text prompt using
LLM-based program synthesis as will describe in Section~\ref{sec:llmsynth}.

\maneesh{Should probably move this example to Section 3.2}
%
Executing a \dslname{} verfication program on an motion graphics animation yields a report specifying
the which logic statements and predicateâ€°s the animation successfully passes and which it fails (Figure~\ref{fig:verificationExample}).
\maneesh{Figure~\ref{fig:verification} visualizes this report 
for an example animation.}



Figures~\ref{fig:teaser} and Figures~\ref{fig:verification} show
examples of motion graphics animations generated with out LLM-based
synthesis and verification pipeline. \maneesh{More examples can be
  found in the video and supplemental materials.} In both cases the input
natural language prompt describes a variety of spatio-temporal properties
the animation should include.
%
%We can represent the constraints expressed by the prompt with
%predicates available in \dslname{} and compose a verification program
%as a collection of logical statements (top right of
%Figure~\ref{fig:verification}).
\maneesh{Next two sentences are essentially repeats of things we have already said. Probably need to
  focs on specifics of our example here.}
%
The LLM generated verification program
encodes these desired properties as logical statements
composed of \dslname{} predicates.
%
The LLM generated motion graphics animation program produces a
sequence of SVG frames and executing the verification program on the
animation produces a verification report (true/false values for each
logical statement and its constituent predicates) as visualized in the
Figures.
%
\maneesh{Probably need to just explain the example(s) teaser and here (or just here) with more specifics. Just jumping in on the verification result doesn't provide enough context. Need to talk about the input prompt (which feels a bit convoluted so we should talk about that) and animation as well as the verification.}
%
For example, the verification result in Figure~\ref{fig:verification}
indicates that, although the letter Y did border with E on its left
side at frame 80, it did not stay at that position until the end of
its translation (frame 100) as specified in the prompt.  \maneesh{Not sure I follow this specific example.}


%Here we demonstrate how \dslname{} can be applied to verify if an
%animation produced by our LLM animation program synthesizer fulfills
%all requirements expressed in an input motion prompt
%(Figure~\ref{fig:verification}), and subsequently how an LLM
%synthesizer can use the verification result to iteratively improve the
%animation (Figure~\ref{fig:iteration}).

%% \paragraph{Verifying a generated animation} As shown in Figure~\ref{fig:verification},
%% the input motion prompt describes a series of motions performed by three letters.
%% We can represent the constraints expressed by the motion prompt with
%% predicates available in \dslname{} and compose a verification program
%% as a collection of logical statements (top right of
%% Figure~\ref{fig:verification}).  Executing the verification program
%% against the animation program generated by our LLM synthesizer
%% produces result messages that show the boolean values of each
%% statement as well as its constituent predicates.  As shown in
%% Figure~\ref{fig:verification}, the verification result indicates that,
%% although the letter Y did border with E on its left side at frame 80,
%% it did not stay at that position until the end of its translation
%% (frame 100).
%% % that the letter Y did not end at a position where it only touch and do not overlap with the letter E, as required by the motion prompt.
%% Further, the translation of the letter S takes place over the same period of time as the translation of the letter Y. 
%% Our verifier indicates that this fails the requirement that these two translations should happen sequentially.

%\paragraph{Iteratively improve the generated animation}
% mention the human user can also make corrections too?

Figure~\ref{fig:iteration} shows the next two iterations of our synthesis and
verification pipeline for the example in Figure~\ref{}. In this case
we automatically fed the verification error report back to the LLM
animation synthesizer. After the first iteration the synthesizer is
verifier reports that the letter Y now properly border the letter E after being
translated, but that their respective translations still did not happen
sequentially.
%
In a second iteration the LLM synthesizer fixes this problem and every
statement and predicate in the verification program evaluates to true
confirming that the final animation correctly matches the input
prompt. 
%
\maneesh{Lace in more specificity perhaps or talk about specifics of both the teaser and this example. Not sure.}

%Once we have the verification result against an animation, we can
%directly feed the result messages, along with the \dslname{} DSL
%documentation and the verification program, back into the LLM
%animation synthesizer to ask it for correction.  As the LLM
%synthesizer has access to all the previous animations it has generated
%and the verification results for each of them, we are in essence
%supplying the LLM synthesizer with negative in-context examples to
%help it make corrections in an iterative manner~\cite{CITE}.  As shown
%in Figure~\ref{fig:iteration},in the LLM synthesizer's first attempt
%to improve the animation, running the verification program indicates
%that the letter Y now properly border the letter E after being
%translated, but their respective translations still did not happen
%sequentially.  We run the feedback loop again, and, in the second
%attempt, the remaining problem of non-sequential translations was
%fixed.  As all logical statements have evaluated to true, we exit the
%feedback loop and output the final animation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig_iteration.pdf}
    \caption{Using the output of a \dslname{} verification program, we automatically feed the verification result back into the LLM animation program synthesizer to iteratively refine the animation until all checks are passed. }
    \label{fig:iteration}
\end{figure*}


\section{Evaluation}

We evaluate our motion graphics synthesis and verification pipeline in
two ways.  (1) We first test the accuracy of our LLM-based
verification program synthesizer in generating \dslname{} code from
natural language descriptions of a motion graphics animation.  (2) We
then test how many iterations our fully automatic pipeline requires to
generate an animation that successfully passes the the corresponding
verification program.

\vspace{0.5em}
\noindent
{\bf \em Test dataset.}
For these evaluations we require a test datatest consisting of natural
language prompts describing a motion graphics animation and the
corresponding ground truth motion verification program. 
%
Because no such dataset is readily available we have synthetically
built one using a template-based approach in combination with a
generative LLM (Appendix~\ref{} details our approach for constructing
this test dataset).
Our test dataset is divided into four categories
%of prompts can corresponding
%\dslname{} verification programs, 
based on the the types of \dslname{}
predicates they exercise.

%We generate four categories of prompts, parameterized by the presence of types of predicates in our DSL.
%\jiaju{The category names are temporary.}
%We present the categories below, and more details about how we generate prompts can be found in Section~\ref{sec:prompt_generation}.
\begin{enumerate}[leftmargin=0.5cm]
    \item {\bf \em Single atomic motion.}  Each text prompt describes
      only one atomic motion with up to seven of its attributes
      (Table~\ref{tab:predicates}). (e.g. ``Expand the square
      upwards.'').
      %No reference to other objects or motions appears
      %in this category.
    
    \item {\bf \em Spatially relative motion.}  Each text prompt
      describes a single atomic motion that is relative to the spatial
      position of another object to exercise predicates from the 2D
      rectangle algebra
      (Figure~\ref{fig:allen_rectangle}). (e.g. ``Move the square next
      to the circle.'').

    
    \item {\bf \em Temporally relative motions.}  Each prompt describes at
      least two atomic motions with relative temporal sequencing relations to
      excercise predicates from the Allen temporal interval algebra
      (Figure~\ref{fig:allen_rectangle}). (e.g. ``Move the square,
      while scaling the circle'').

    
    \item {\bf \em  Spatio-temporally relative motions}: Each prompt
      includes multiple motions with relative temporal and spatial
      relationships (e.g. ``Move the square next to the circle, while
      scaling the circle'').
\end{enumerate}

\maneesh{Should say how many prompts all together and in each category}

%% We evaluate 1) the LLM-based program synthesizer's ability to parse a
%% natural language motion prompt into a \dslname{} verification program
%% and 2) the LLM-based motion graphics generation pipeline's ability to
%% output an animation that pass all checks in the \dslname{}
%% verification program from an input prompt.

%% \subsection{Motion prompt dataset}
%% We build a dataset consisting of motion prompts and their
%% corresponding ground truth \dslname{} motion verification
%% programs. \jiaju{should we explain why not ground truth animation?}
%% %
%% % \jiaju{TODO: dataset composition}
%% We generate motion prompts in four categories, parametrized by the presence of types of predicates in our DSL.
%% \jiaju{The category names are temporary.}
%% We present the categories below, and more details about how we generate prompts can be found in Section~\ref{sec:prompt_generation}.
%% \begin{enumerate}[leftmargin=0.5cm]
%%     \item \textit{Single atomic motions.}
%%     Each motion prompt within this category describes only one atomic motion with its seven attributes (Table~\ref{tab:predicates}).
%%     No reference to other objects or motions appears in this category of prompts.
    
%%     \item \textit{Object-relative motions.}
%%     Each prompt in this category describes a single atomic motion in reference to another object (e.g. ``Move the square next to the circle.''). 
%%     Each prompt involves concepts related to the Rectangle Algebra (Figure~\ref{fig:allen_rectangle})
    
%%     \item \textit{Motion-relative motions.}
%%     Each prompt of this category describes at least two atomic motions with temporal sequencing relations (e.g. ``Move the square, and then scale the circle'').
%%     Each prompt invokes concepts related to the Allen' Interval Algebra (Figure~\ref{fig:allen_rectangle}).
    
%%     \item \textit{Object-and-motion-relative motions}:
%%     Each prompt includes multiple motions with temporal relations and references to other objects in the scene (e.g. ``Move the square next to the circle, and then scale it'').
%% \end{enumerate}

\input{table_eval_parsing}

\vspace{0.5em}
\noindent {\bf \em Accuracy of LLM-based verification program synthesizer.}
Table~\ref{tab:eval_parsing} reports the success rate for the
LLM-based \dslname{} synthesizer across the prompts in our test dataset.
To compute this success rate we first compute the string difference between the
LLM synthesized program and the corresponding ground truth and count a success only
when there is a perfect string match.
%
This is a stringest test for success as the underlying semantics of
the two programs may match even without an exact string match. But we
leave a looser comparison (e.g. at the level of AST matching) to
future work.
%
Overall, we observe that our LLM-based synthesizer achieves a high
success rate across all four types of prompts.  \maneesh{Probably need
  something here about the categories and differences between them them.}


To better understand the generalization capabilities of our LLM-based
synthesis approach we have implemented a semantic
parser\,\cite{kamath2018survey} for converting a text prompt into a
corresponding \dslname{} program using a classical, rule-based
approach. Appendix~\ref{sec:semanticParser} describes the parser in
detail. As shown in Table~\ref{tab:eval_parsing} although the semantic
parser is fairly successful across our test set at \maneesh{XX}, it is
not quite as successful as the LLM-based approach across the four
prompt categories.
%
Examining the unsuccesful examplares we find that the LLM tends to
... \maneesh{say something about its problems -- that it can fail
  unexpectedly}.
%
In contrast the semantic parser has trouble ... \maneesh{say something
  about its problems. brittle for certain classes of prompts.}
\maneesh{Would we be better off dropping the semantic parser
  compoletely from this paper?}


\input{table_eval_pipeline}

\vspace{0.5em}
\noindent {\bf \em Number of automatic iterations required.}
Table~\ref{tab:eval_pipeline} reports the number of iterations our
fully automatic pipeline (with LLM-generated verification programs)
\maneesh{maybe we should do this with ground truth -- or maybe with
  both?} requires to produce a motion graphics animation that
successfully passes the corresponding verification program.
\maneesh{put stoppping limit in caption of table.}
%
%For each prompt in the test dataset we iteratively feed back the
%verification error report until all the verification tests pass, or we
%have performed \maneesh{XX iterations}.
We observe that our pipeline performs well in the 
prompt categories that do not include spatially relative motions.
outputting the correct animation directly without needing correction for XX\% of the prompts. \maneesh{Is this the best way to report this?}
%
For test prompts in the spatially relative and spatio-temporally
relative categories, we observe a drop in first attempt success rate
and an increase in the number of timed-out attempts.  Oour pipeline on
average needed \maneesh{X} number of attempts before producing an
animation that passes all the checks.  Examining the multi-attempt and
timed-out cases, we find that ...  \jiaju{TODO: some reasons I have
  thought of so far: 1) relative positioning requires computing
  various magnitudes and determining the center of rotation/scaling,
  which the LLM is known to be struggling with (CITE). 2) there are
  some cases when a more direct feedback that offer some potential
  solutions might be helpful, as the verification results only provide
  true or false.}  Notes: \jiaju{TOOD: also, can report by motion
  types, as the LLM struggles more when the motion type is not
  translation} Notes:
% \jiaju{TODO: can also present some interesting cases where the LLM did the unexpected e.g. the example of using both rotation and translation while the prompt only said rotate}




%% \subsection{Evaluation results}
%% \subsubsection{LLM-based verification program synthesizer}
%% \input{table_eval_parsing}
%% To evaluate how well our LLM-based synthesizer can parse an input motion prompt into the correct \dslname{} program, we built a rule-based semantic parser as the baseline.
%% The detailed implementation of the semantic parser can be found at Section~\ref{sec:semantic_parser}.
%% % \jiaju{TODO: Report the success rate of both parsers across categories}
%% We run both the LLM-based parser and the semantic parser on all XXXX prompts in our dataset and report the success rate in Table~\ref{tab:eval_parsing}.
%% We measure the string difference between the output of a parser and the ground truth verification program and count a complete match as success and otherwise failure.
%% Overall, we observe that the LLM-based synthesizer achieve high success rate across all four types of prompts.
%% This is validated by the comparison against the rule-based parser, which achieves a decent performance of XX\% across the board.


%% \subsubsection{Motion graphics generation pipeline}
%% \input{table_eval_pipeline}
%% We run our LLM-based motion graphics generation pipeline on all prompts in our dataset.
%% As described in Section~\ref{sec:llmsynth}, once an animation has been generated, we feedback it back along with the verification result for correction if the animation did not pass all verification checks.
%% We repeat the loop until either the LLM-synthesizer produces a fully-correct animation or until it reaches the maximum number of tries ($n=XXX$).
%% \jiaju{TODO: say that we manually inspect the final output animations to ensure that they indeed fulfills the requirements of the input motion prompt?}
%% % \jiaju{TODO: Report the number of tries across categories}
%% We report the results in Table~\ref{tab:eval_pipeline}.
%% We observe that the generation pipeline performances well in the categories without relative spatial positioning between objects, outputting the correct animation directly without correct in XX\% of the prompts.
%% When it comes to prompts in the object-relative and object-motion categories, we observe a drop in first attempt success rate and an increase in the number of timed-out attempts.
%% The LLM synthesizer on average needed X number of attempts before producing an animation that passes all the checks.
%% Upon examining the multi-attempt and timed-out cases, we make several observations on why the LLM struggled.
%% \jiaju{TODO: some reasons I have thought of so far: 1) relative positioning requires computing various magnitudes and determining the center of rotation/scaling, which the LLM is known to be struggling with (CITE). 2) there are some cases when a more direct feedback that offer some potential solutions might be helpful, as the verification results only provide true or false.}
%% Notes:
%% \jiaju{TOOD: also, can report by motion types, as the LLM struggles more when the motion type is not translation}
%% Notes:
%% % \jiaju{TODO: can also present some interesting cases where the LLM did the unexpected e.g. the example of using both rotation and translation while the prompt only said rotate}
