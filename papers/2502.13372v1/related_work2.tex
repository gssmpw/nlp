\input{table_predicates}

\section{Related Work}
\label{sec:related}

\noindent
{\bf \em Text-to-animation.}
Generating animations from a natural language prompt has been a long
standing challenge in computer graphics\,\cite{bouali2023review}.
%
Early text-to-animation methods often used rule-based natural language
processing techniques such as regular expression parsing, co-reference
resolution, and sentence simplification
%combined with a domain specific lexicon from a knowledge base,
to map the prompt to a semantic representation (e.g. a scene graph) that is rendered
%(i.e., a domain-specific scene graph)
into an
animation\,\cite{badler1999,aakerberg2003carsim,ma2006automatic,marti2018cardinal,zhang2019generating}.
%
%These methods then use a domain-specific renderer to convert the semantic representation into an animation.
%
While effective for specialized domains such as posing virtual
humans\,\cite{badler1999} or simulating car
accidents\,\cite{aakerberg2003carsim}, such methods typically
require many special case rules to handle the ambiguity of general
natural language. 

In the current era of large generative AI models, researchers have explored end-to-end methods for directly
generating animation from text prompts without passing
through a semantic
representation\,\cite{guo2023animatediff,guo2023sparsectrl,xing2024tooncrafter,chen2024videocrafter2,gal2024breathing}.
%
%While these end-to-end methods output pixel-level video, stroke-level
%vectors or joint-level trasjectories, they regularly fail to capture all of the
%desired spatio-temporal properties mentioned in the text prompt. Users
%have to resort to trial-and-error prompt adjustment to find prompt
%that can generate the desired animation.
%
While these end-to-end methods can generate impressive animations, they
regularly fail to capture all of the desired spatio-temporal
properties mentioned in the text prompt.
%
%
%Users have to resort to
%trial-and-error prompt adjustment to find a prompt that can generate the
%desired animation.
%
In contrast, we focus on automatically verifying such properties in synthesized
motion graphics animations.

%
%Notably, Gal et al.~\shortcite{gal2024breathing} generates animated sketches of single objects
%using a vector representation of strokes. 
%The method does not deal with the spatio-temporal properties of object trajectors.
%vector representation of each sketched strokes rather than pixel-level video.
%
%While this reprsentation enables editing of individual strokes, the method focuses on
%single object animations and does not deal with spatio-temporal
%properties of object trajectories.
%
%In contrast, we focus on verifying such properties in synthesized
%motion graphics animations.

%\maneesh{what about things like purvi's paper and things related to that. Need to put that in here I think.}

\vspace{0.5em}
\noindent
{\bf \em LLM-based program synthesis.} 
Our work builds on recent techniques that use LLMs for 
program synthesis via in-context learning\,\cite{gupta2023visprog,surismenon2023vipergpt}.
%
These methods typically provide language documentation with code examples
(pairs of text prompts with corresponding programs) as part of the
system prompt context.
%
The approach has been applied in a variety of visual synthesis domains
including 
%
images\,\cite{gupta2023visprog}, vector
graphics\,\cite{xing2024llm4svg}, 3D
scenes\,\cite{zhang2024scenelanguage}, CAD, and human
motion animation\,\cite{Goel_2024}.
%  animation\,\cite{tseng2024keyframer,liu2024logomotion}.
%
Closest to our work are Keyframer\,\cite{tseng2024keyframer} and
LogoMotion\,\cite{liu2024logomotion} which apply this LLM-based
program synthesis approach to the problem of generating 2D vector
animations.
%
%
While all these methods generate a human-editable program
representation for the visual content, none of them provide automated
verification capable of checking that the resulting visuals match
the prompt specification.
%
%This work usually presents a single-shot method that generates an
%output program and the corresponding visual content, but it does not
%provide automated verification that the resulting content matches the
%specification in the input prompt.
%If the result does not match the prompt,
It remains up to the user to manually identify mismatches and fix the
resulting program or to update their prompt and try again.
%
Our work aims to close this gap in the context of motion graphics 
by providing automated verification and thereby allowing
the LLM to iteratively correct the animation without user
intervention.


\vspace{0.5em}
\noindent
{\bf \em Visual verification.}
%Our work is inspired by recent efforts to provide automated
%verification for visual outputs (usually images) produced by large
%vision-language and diffusion models.
%
%One class of methods aims to simply score how how well the output
%image matches the input prompt using either crowdsourced human
%judgment\,\cite{,lee2023holisticevaluationtexttoimagemodels}, or a
%joint vision-langauge embedding such as
%CLIP\,\cite{Hessel2021CLIPScoreAR}.
%
%However such scores are coarse forms of verification in the sense that
%they cannot report which parts of the prompt are satisfied by the
%image and which are not.
Our work is inspired by recent efforts that use visual question
answering (VQA) models to check that properties mentioned in an input
prompt appear in diffusion generated
images\,\cite{Johnson_2017_ICCV,hu2023tifa,JaeminCho2024}.
%
For example, these methods convert the image generation prompt ``A
tree behind a motorcycle'' into into a series of natural language
questions like ``Does the image contain a tree?'' and ``Is the tree behind
the motorcycle?'' and use a VQA model to report the answer to each
one.
%
The answers can then be converted into a score or examined
individually by users.
%
%One challenge for these methods is that the VQA model may
%generate incorrect answers. 
%
%These methods rely on VQA models with strong spatial understanding of
%3D scenes
%
%In contrast our converts the prompt into a first-order logic based
%verification program that is then executed 
%
%Our work is especially inspired by
DreamSync\,\cite{sun2024dreamsync} incorporates this type of VQA-based
review of visual outputs into an iterative optimization loop where the output from
VQA model is automatically fed back to the image generating model.
%
SceneCraft\,\cite{hu2025scenecraft} applies this optimization approach
in the context of 3D scene layout generation, using LLM-based question
answering.
%
%similarly incorporates an
%LLM-based question answering VQA model into an iterative optimization loop
%but in the context of 3D scene layout generation.
%
Our work similarly provides an iterative optimization loop for motion
graphics animation, but, instead of a VQA model, it uses an LLM to convert the prompt into a first-order
logic based verification language.

%\maneesh{cut this paragraph.}
%Note that these VQA-based methods rely on VQA models with with strong
%spatial understanding of 3D scenes. These models have benefited from 
%
%from synthetically generated 3D scene datasets (e.g.,
%CLEVR\,\cite{johnson2017clevr}) which provides ground truth for
%checking spatial understanding and reasoning in such models.
%
%Our work similarly includes a motion graphics animation dataset to
%which can be used to check spatio-temporal understanding in such models.
%


\vspace{0.5em}
\noindent
{\bf \em Mapping language into formal logical specifications.}
One approach to representing the semantic meaning of natural language
expressions is to map the expression into formal logical
specifications.  For example, Artzi et
al.\,\shortcite{artzi2013mapping} show how to map English language
% navigational phrases such as ``chair in blue hallway'' to a
% corresponding logical form $\lambda x.chair(x)\land blue(x) \land
% hall(x)$.  
navigational phrases such as 
``move to the chair'' to a
corresponding logical form \texttt{$\lambda$a.move(a)$\land$to(a,$\iota$x.chair(x))}.
Researchers have explored this mapping approach in the
context of instructing robotic
manipulators\,\cite{wang2023programmatically}, 
% text-to-3D scene generation
VQA\,\cite{hsu2024left} and guidance for text-to-image
generation\,\cite{sueyoshi2024predicated}.
%
These methods often use specialized semantic parsing
techniques\,\cite{kamath2018survey} to convert natural language into
the formal logic.
%
While our \dslname{} verification language is also based on
formal, first-order logic, we use the program synthesis capabilities of
an LLM to map prompts to \dslname{} predicates. We show that our
LLM-based approach handles a wider variety of inputs than a rule-based
semantic parser.
%\maneesh{Might be best to cut this section.}




% LocalWords:  LLMs CarSim WordNet Zhang et al pre labeler ARF LLM
% LocalWords:  KeyFramer SVG LogoMotion VLM SVGs ToonCrafter VISPROG
% LocalWords:  ViperGPT neuro DSL VLMs TIFA Cho cho davidsonian Artzi
% LocalWords:  davisonian SceneCraft disjunctions Zettlemoyer CCG Hsu
% LocalWords:  shortcite Reddy binarization binarized zhang artzi VQA
% LocalWords:  ModelScope VideoCrafter Videofusion iteratively intra
% LocalWords:  verifiers spatio Keyframer DreamSync CLEVR
