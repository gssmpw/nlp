\section{Method}
\label{sec:method}

Our method takes a text prompt as input and generates a corresponding
motion graphics animation program in 3 main steps
(Figure~\ref{fig:pipeline}).
%
(1) A Motion Program Synthesizer uses in-context LLM-based program
synthesis to generate an SVG motion program.
%that represents each foreground object as an SVG group {\tt <g>}
%containing it's appearance and a sequence of per-frame motion
%transforms.
(2) A Prompt Parser converts $t$ into a FOL-based verification program
using our motion verification DSL.  \maneesh{We a explore two
  approaches to parsing in this work, a rule-based semantic parser and
  an in-context, LLM-based parser.}
%
(3) A Motion Verifier then applies the verification program to the SVG motion program
to check that space-time concepts mentioned in the prompt appear in the motion program.
%
If verification fails our system can automatically send the error
feedback to the synthesizer to update the program thereby supporting
iterative updates.
%
In addition a human user can examine the verification output and either
manually fix the motion program, or filter the feedback sent to the
synthesizer. \maneesh{Last sentence may not be necessary here.}

\maneesh{Might be able to kill the roadmap paragraph below.}
We first describe the Motion Program Synthesizer (Section~\ref{sec:synthesizer}) as
it generates the low-level SVG representation of motion our pipeline
operates on. We then describe our motion verification DSL
(Section~\ref{sec:dsl}). Next we explain how our Motion Verifier applies a
verification program to an SVG motion program
(Section~\ref{sec:verifier}). Finally we describe our Prompt Parsers
(Section~\ref{sec:parsers}).



\subsection{Motion Program Synthesizer}
\label{sec:synthesizer}

Our motion program synthesizer, follows prior work on in-context
LLM-based program synthesis\,\cite{}. \maneesh{see TIFA paper for references in
  addition to ViperGPT and VISPROG}. We provide a system prompt that
includes documentation for an SVG-based animation API and examples of
text prompts paired with corresponding output programs in this API
(see Appendix XX for the system prompt).
%
%Following prior work we use in-context LLM-based program
%synthesis\,\cite{see TIFA paper for references in addition to ViperGPT
%  and VISPROG}, with a system prompt that includes documentation
%for an SVG-based animation API and examples of prompts paired with
%corresponding output programs (Appendix XX shows the prompt.}
  %
While the animation API can be any high-level representation of the
SVG objects and their motions (e.g. a sparse keyframe based API such
as \maneesh{GSAP} \maneesh{Could we demonstrate a physics based
  API?}), our verifier operates on a low-level SVG representation
where each foreground object is represented as an SVG {\tt <shape>} ,
{\tt <g>}, or {\tt <image>} node with per-frame motion transforms,
similar to that used by Zhang et al.\,\shortcite{}.
%
Therefore our synthesizer also converts the high-level motion program
into the corresponding, generic low-level SVG motion program by
executing the high-level version and keeping track of the per object
transforms.

\subsection{Motion Verification DSL}
\label{sec:dsl}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{figs/predicate_table.png}
%     \caption{
%         Our first-order logic motion verification DSL. \maneesh{Needs to be made in Latex and also made to take up less room vertically.}
%     }
%     \label{fig:table_predicate}
% \end{figure*}

We have developed a domain specific language for verifying that
certain concepts related to space and time appear in an SVG motion
program.
%The language consists of functions and predicates 
%(i.e., functions that return boolean values)
The concepts are expressed as first-order logic predicates (functions
that return boolean values)
%in our language and are based on prior
%work in cognitive psychology\,\cite{} that has analyzed how people
%think about and describe animations
over variables representing objects $o_i$ and their
motions $m_j$.
%
For example, the predicate {\tt above($o_i$,$o_j$)} returns true for
each frame of the animation where object $o_j$ is above
$o_i$. \maneesh{above(o1,o2) is not directly in the table -- should give
  it as an example of a spatial concept.}
%
The predicate {\tt direction($m_i$, [0,1])} returns true for each frame where
motion $m_i$ is along the direction vector $[0,1]$ (i.e. rightwards).
%
And the expression $ \exists \texttt{m. type(m, "translate")} \land
\texttt{direction(m, [0, 1])} \land \texttt{magnitude(m, 100.0)} \land
\texttt{agent(m, iota(o, color(o,''black'')} \land
\texttt{shape(o,''square''))}$ returns true for each frame where a
square shaped object is translating upwards.  \maneesh{Might need to
  use $\iota$ symbol instead of iota(). Also maybe move long
  expression into a figure or table or equation block.}
%
Table~\ref{fig:table_predicate} shows the elements (predicates,
functions, boolean operators) of our language.


The predicates in our DSL are based on prior work from cognitive
psychology and linguistics that has analyzed how people think about
and describe meaningful spatio-temporal relationships in
animation\,\cite{Talmy,Tversky}. 
%
This work has shown that linguistic descriptions of animation
schematize the information to emphasize only the most salient
space-time configurations often without providing precise metric
properties in a absolute frame of reference (e.g., the square is at
(100,40) etc.), but rather in a frame of reference relative to other
objects (e.g., the square is next to the circle).
%
We build on Allen's\,\shortcite{} temporal algebra which provides a
formal representation for working with relative temporal
intervals as well as Balbiani et al.'s\,\shortcite{} rectangle algebra which
extends Allen's algebra to represent relative 2D spatial relationships.
%
Figure~\ref{} shows the various spatio-temporal relationships these
algebras can represent and our DSL provides predicates that can check
for each of them.


\input{table_predicates}
\subsection{Jiaju's Version: Motion Verification DSL}
We have developed \dslname{}, a domain specific language for verifying that concepts related to space and time expressed in natural language descriptions of motions are properly executed in a motion graphics animation.
%
We express these concepts as first-order logic predicates (functions that return boolean values) over variables representing objects $o_i$ and their motions $m_i$ ( Table~\ref{tab:predicates}).
%
The predicates in \dslname{} are based on prior work from cognitive psychology and linguistics that has analyzed how people think about and describe salient spatio-temporal relationships in animations~\cite{allen1983interval, talmy1975motion, Tversky}. 
%% concept of single motion
We organize these predicates into logical statements that describe \textit{atomic motions}, entities that have the seven attributes listed in Table~\ref{tab:predicates}.
For example, the expression $ \exists \texttt{m. type(m, "translate")} \land \texttt{direction(m, [0, 1])} 
\land \texttt{agent(m, iota(o, color(o,''black'')} 
\land \texttt{shape(o,''square''))}$ returns true for each frame where a square shaped object is translating upwards.
% \maneesh{Might need to use $\iota$ symbol instead of iota(). Also maybe move long expression into a figure or table or equation block.}
% the predicate {\tt direction($m_i$, [1,0])} returns true for each frame where motion $m_i$ is along the direction vector $[1,0]$ (i.e. rightwards).
%

%% allen and rectangle
To express various \textit{temporal} relations between atomic motions and different \textit{spatial} relations between their objects, we build on Allen's~\shortcite{allen1983interval} temporal algebra, which provides a formal representation for working with relative temporal intervals (Figure~\ref{fig:allen}), and Balbiani et al.'s~\shortcite{} rectangle algebra, which extends Allen's algebra to represent relative spatial relationships in the 2D space.
%
Figure~\ref{fig:allen} and \ref{fig:spatial} show the various spatio-temporal relationships these algebras can represent, and our DSL provides predicates that can check for each of them.
\jiaju{need to be clear here that we have all the low-level predicates as well as high-level ones. The examples below are the high level ones.}
% example temporal
For example, \texttt{while(m$_i$, m$_j$)} checks for whether the two motions have any amount of overlap across time.
% example spatial
The spatial predicate {\tt above($o_i$,$o_j$)} returns true for each frame of the animation where object $o_i$ is above $o_j$. 
% \maneesh{above(o1,o2) is not directly in the table -- should give it as an example of a spatial concept.}

%
% Table~\ref{fig:table_predicate} shows the elements (predicates,
% functions, boolean operators) of our language.

%
\jiaju{I like this sentence, but I am not sure if it should be here. I wonder if this can be moved to intro.}
This work has shown that linguistic descriptions of animation schematize the information to emphasize only the most salient space-time configurations often, without the need to provide precise metric properties in a absolute frame of reference (e.g., the square is at (100,40)), but rather in a frame of reference relative to other objects (e.g., the square is next to the circle).
%


\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figs/fig_allen.pdf}
  \caption{
      Illustration of the 13 relations in Allen's Interval Alegrab and mappings into 
      timing concepts in our DSL. 
  }
  \label{fig:allen}
\end{figure*}

\subsection{Verifying an SVG Motion Program}
\label{sec:verifier}

Our Motion Verifier is responsible for checking whether an SVG motion
program is consistent with a motion verification program written in our DSL.
%
%More specifically it executes the verification program on the SVG
%motion program and
%
It outputs the set of frames that evaluate to false with respect to
each logical expression in the verification program.
%
%Our Motion Verifier is responsible for executing a motion verification
%program on an SVG motion program.  It outputs the set of frames that
%evaluate to false with respect to each logical expression in the
%verification program.


To do this we first convert the SVG motion program into a 2D array
where each row represent an object and each column
represents a frame of the animation. In each cell $(i,j)$ of the array
we store a motion information for object $o_i$ at
frame $f_j$.
%
%% Specifically we store information about the motion type (translation,
%% rotation or scale), direction (``clockwise'' or ``counterclockwise''
%% for rotations, a normalized tangent vector for translations, and a 2D
%% vector of scalefactors (in $x$ and $y$) for scale), magnitude, and
%% center (of rotation or of scale).
%
Specifically for each object we obtain its per-frame transformation
matrix from the SVG motion program and factor it into motion parameters for
rotation (e.g. angle), translation (e.g. direction and magnitude),
etc. and store these in the corresponding cells of the array.
%
We separately store static information for each object such as its color,
shape, and bounding box (in object local coordinates).
%


With these data structure in place we implement our DSL as follows.
%
%{\tt exists(var, expr)}: we recursively execute {\tt expr} 
%
We execute each logical predicate in the verification program on each
cell of the 2D (object $\times$ frame) array and mark the
corresponding cell of an output boolean matrix as true if the
predicate is satisfied and false otherwise. Thus each predicate
produces a 2D boolean matrix of size $|O| \times |F|$.
%
For example, executing the predicate {\tt type(m,''rotation'')} sets
matrix cell $(i,j)$ as true if object $o_i$ has a non-zero rotation
angle in frame $f_j$. Similarly, executing the predicate {\tt
  shape(o,''square'')} sets the entire matrix row $(i,*)$ to true if
object $o_i$ is square shaped. 
%
A predicate expression (i.e., {\tt expr}) combines the results of
multiple predicates by applying logical boolean operators (e.g. and,
or, not) element-wise on the predicate matrices.
%
%and we can apply boolean operators (e.g. and, or, not) element-wise to
%each entry in the matrix to combine the results of different
%predicates.

While many predicates primarily involve looking up information in the
animation and object data structures, predicates that compare the
timing of two motions or the bounding boxes of two objects (from the
Allen or rectangle algebras) are more complex. These require
comparisons between the starting and ending frames of the motions or
between the relative positions of the bounding boxes of the objects
(Figure~\ref{}).




\maneesh{----------- OLD TEXT ---------}



\subsection{Semantic Parser}
\label{sec:parsers}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig_parser.pdf}
    \caption{
        The processing pipeline of our semantic parser. 
    }
    \label{fig:parser}
\end{figure*}
    
% \maneesh{Will be a lot easier to understand with a concrete example of a prompt and figures illustrating the main ideas. It will be easier to update this section as well once figures have been added -- rough, even handdrawn is best for now. Opening should explain the overall goals and a high-level walkthrough of the steps of the approach and assumptions made at each step.}

% \maneesh{The current version of the methods section reads like a list of things we do and mostly suggests that we string together lots of previous tools. I think it will be much stronger if it is written to emphasize the key ideas that we are bringing to make everything work. For example why the six categories of motion attributes. How did we choose those? These like this should be emphasized at the very beginning of this section if possible.}

%% motion attributes
% \maneesh{Why these six categories?}
\jiaju{Might be better to talk about these motion attributes when
  introducing the FOL DSL}
By analyzing parameters in existing animation software and
API~\cite{TODO} and summarizing relevant prompts found in the
VidProm~\cite{wang2024vidprom} dataset, we identify seven types of
motion attributes onto which constraints can be placed: \textit{type}
(type of motion, one of translation, rotation, and scaling),
\textit{agent} (object that performs the motion), \textit{direction},
\textit{magnitude}, \textit{origin} (for rotation and scaling),
\textit{timing} (duration and sequencing), and \textit{post state}
(e.g. end at a specific position).
\maneesh{I don't follow why we are explaining what is in this paragraph. It seems out of place. Goal needs to be explained first. I also don't understand how these things are constraints. We do we mean by the term ``constraint''?}

The goal of our semantic parser is to extract these types of
constraints placed on motions and their sequencing from an input
natural language description of a motion graphics animation and to
assemble these constraints into a verification program in the
FOL-based DSL described above.
\maneesh{I don't follow this goal because I don't know what we mean by ``constraint'' and I don't understand how the DSL is FOL-based, or what it even means to be FOL-based.}
%
% \maneesh{Should motivate each part of the pipeline. An example prompt can help with this. In general each part/step/stage of the work should be motivated.}
%
As shown in Figure~\ref{fig:parser}, our semantic parsing pipeline
consists of four stages: (1) we resolve object and motion
co-references (e.g. identifying that ``it'' refers to ``the black
square'', (2) we break the input prompt into atomic sentences
without complex clauses such that each sentence describes one
continuous motion (e.g. breaking down ``it rotates, while expanding''
into ``it rotates'' and ``it expands'', (3) we build a list of
constraint predicates for information contained within each atomic
sentence (e.g. \texttt{type(m, ''rotate'')} and \texttt{magnitude(m,
  90)} from ``rotates by 90 degrees'', and (4) we compose the final
verification program by adding timing predicates that describe
inter-sentence sequencing relationships.
\maneesh{Instead of intra- and inter- sentence processing stages we should give those two stages more descriptive names. Stage 4 seems to be about sequencing and stage 3 about single motions, or some such.}

In contrast to prior approaches \maneesh{citations here}, our parser can extract multiple
constraints added by a single expression (conflation) with a
multi-label text classification model in Stage 3 (e.g. \texttt{type}
and \texttt{direction} from the verb ``expand'' in
Figure~\ref{fig:parser}).  We describe each stage of our pipeline in
detail below. \maneesh{This paragraph seems out of place. I don't follow what a conflation is from the previous discussion and because we haven't explained how our stages work I can't follow the differences with prior work. Needs to move elsewhere.}

\subsubsection{Stage 1: Object and motion co-reference resolution}
The goal of this stage is to identify segments of text that refer to
the same object or motion.
%so that we do not add duplicate predicates
%later in the pipeline.
%
To resolve co-references to objects, we use the fastcoref
model~\cite{otmazgin2022fastcoref}.  As shown in
Figure~\ref{fig:parser}, this places``The black square'' and
``it'' in the same co-reference cluster.
%
For motion co-reference resolution, we first identify event triggers
(words that introduce a motion) based a set of heuristics.
Specifically, for each noun and verb in the input prompt, we check if
its lemmatized form matches words that describe a motion in our
vocabulary (e.g. ``spin'' is a word that introduces a rotation
motion). \maneesh{What does ``in our vocabulary'' mean. Where does
  that vocabulary come from. Need to explain this in more detail
  because this sounds very brittle -- it sounds as if we are giving
  the system a pre-specified vocabulary.}  If there is a match, then
we mark it as an event trigger.  We then match event triggers to each
other by comparing their modifiers and their positions in a
sentence~\cite{ahmed2023lemmamatching, kenyondean2018lemmaheuristic}.
\maneesh{Need  more explanation of how this matching occurs. What is the algorithm. Can come after the example.}
For example, in the sentence ``The square spins. While rotating, it
also scales'', ``spins'' and ``rotating'' are identified as event
triggers.  Moreover, because they both operate on the same object, and
``rotating'' sits within a subordinate clause, then we identify them
as co-references referring to the same motion of rotation.
% \maneesh{We should motivate each step. Why do we need three different prior techniques to do the co-reference resolution. Need to include detail about what the trigger words are. Paper needs to be re-implementable after reading. Right now the text is a bit too vague for that.}

\subsubsection{Stage 2: Text simplification}
At this stage, we aim to break down the original input prompt into
atomic sentences that each describes a single continuous motion.
This stage sets up the extractions of intra- and inter-sentence
constraints later. \maneesh{The motivation sentence is not that useful. Should be motivated more in terms why it is necessary to do the simplification. The current motivation begs the question of why intra- and inter- sentence stages are necessary.}

%
Prior works on text
simplification~\cite{zhang2019generating, ferres2016yats} have identified six types of syntactic structures for
simplification: passive constructions, appositive phrases, relative
clauses, coordinated clauses, correlated clauses, and adverbial
clauses. \maneesh{Simplification in what sense? Should explain why these six structures are relevant to breaking the prompt into simple sentences -- e.g. sentences that describe a single motion.}
%
We do not simplify appositive phrases as they modify nouns and do
not often introduce new motions.
\maneesh{I don't understand the previous sentence because I don't know what an ``appositive phrase is''. Also why should I as a reader believe you that they do not introduce new motions.}
We also do not process correlated
clauses (e.g. either...or...) as they are rare when describing
motions.  \maneesh{Again, why should I believe you?} For passive constructions, we process them directly in the
next stage. \maneesh{Why are are forcing passive constructions into the next stage? If the passive construction includes multiple motions it should be handled here -- no? Otherwise I'm not sure what simplification is about breaking sentences into single motions.}

Therefore, we simplify three syntactic constructions at this stage:
\textit{Coordinated Clauses}, \textit{Adverbial Clauses}, and
\text{Relative Clauses}.  We first identify if any of these
constructions exists in the input prompt based on dependency relations
obtained with a roberta-based transformer
model~\cite{liu2019roberta, honnibal2020spacy} and then recursively perform the
following set simplification operations based on clause type, until no
more complex structures are present.

\paragraph{Coordinate clauses} We identify this type of clause by the presence of the conjunctive ``and''. 
We split the sentence based on the number of conjuncts and copy relevant sentence parts. For example, given ``Rotate the blue and black square.'', we would edit it to become ``Rotate the blue square'' and ``Rotate the black square.''

\paragraph{Adverbial Clauses}
This type of clauses usually joins two motions together, such as the
second sentence in Figure~\ref{fig:parser}.  We identify this
construction if a verb has the ``advcl'' dependency tag and a ``mark''
word (such as ``while'' and ''before'') is present. \maneesh{Where are the dependency tags coming from? What do they mean?}
%
We first check if the motion contained in the adverbial clause (``while expanding by 2'' in Figure~\ref{fig:parser}) is a co-reference or not.  \maneesh{How is the check done?}
If not, then we split the adverbial clause into a standalone sentence from the main clause as it introduces a new motion (sentences with yellow and blue underlines in Figure~\ref{fig:parser}).
Otherwise, we leave it as-is since it is adding a timing constraint to the motion described in the main clause.

\paragraph{Relative Clauses}
This construction is less common but we process it similarly as
adverbial clauses.  We identify this type of clauses by the presence
of a noun labeled as ``relcl.''
\maneesh{I don't understand this description. Where are the dependency labels coming from and what do they mean.}
We similarly check if the motion
contained in the relative clause is a co-reference or not, and only
split the sentence if it is not. 


\subsubsection{Stage 3: Intra-sentence processing}
% \maneesh{What is a ``single motion''?} 
% \maneesh{Why is this assumption a safe assumption at this point? Should talk about these assumptions in opening of this section.}
% We assume that each input is an atomic sentence that describes and modifies a motion, and the motion is introduced by a verb (e.g. the black square rotates by 90 degrees).
At this stage, the original input prompts has been processed into
atomic sentences where each one introduces a single continuous motion
(Figure~\ref{fig:parser}).  The goal of intra-sentence processing is to extract
motion constraint predicates from each sentence, including from
\textit{conflated} expressions. \maneesh{Need to explain what a ``conflated expression'' is earlier and provide an example.}

For each sentence, we first use a roberta-based transformer model~\cite{liu2019roberta} \maneesh{what is a Roberta based transformer model. Should cite it} to
obtain its syntactic dependency tree via
spaCy~\cite{honnibal2020spacy}, whose root node is the motion verb
(see Stage 3 in Figure~\ref{fig:parser}).
The root node and each of
its subtrees can be seen as a modifier that adds constraints to a
motion (e.g. ``by 90 degrees'' and ``around its center'' attached to
``rotates'' in Figure~\ref{fig:parser}). 

For each subtree, we identify what types of motion attributes the phrase is adding constraints to by classifying it with a fine-tuned sentence transformer. \maneesh{prev sentence is awk.}
Specifically, we use SetFit~\cite{tunstall2022setfit} to fine-tune a sentence transformer on a multi-label vocabulary dataset of example attribute descriptions we curated manually. \maneesh{Where does the vocabulary come from. How brittle is this. Need more detail.}
% conflation!!
In this way, we can account for conflation cases where a single phrase
introduces more than one constraint by adding predicates for each
label. \maneesh{I don't understand the previous sentence as I don't follow how well SetFit does the classification or knows to treat words that expand into multiple constraints.} As shown in Figure~\ref{fig:parser}, the verb ``expanding'' is
parsed into two constraints: \texttt{type(m\_3, ``scale'')} and
\texttt{direction(m\_3, [1.0, 1.0])}.

% clausIE
% \maneesh{I;m a bit lost. Not sure where we introduced parse trees and root nodes. Should illustrate these things. More importantly should motivate why we are doing each step before we describe the step. Why do we need the finetuned SetFit transformer and how exactly do we finetune it.}
%
% describe object predicates
%

For each type of constraint that a phrase adds, we further process the content in the subtree to parse out arguments for the corresponding predicate.
For example, we identify ``by 90 degrees'' as adding a constraint on the \texttt{magnitude} attribute of the rotation motion.
Within the subtree of the phrase ``by 90 degrees'', we then look for a node with part of speech NUM (i.e. the node of the text ``90'') and use it to construct the predicate \texttt{magnitude(m\_2, 90)}.
\maneesh{What is the algorithm for this parsing procedure?}
\jiaju{Need to add more details on how we process direction/magnitude for scale}

% \maneesh{How do we know if a subtree falls in the categories. What is the algorithm for identifying the constraint. Examples need to be accompanied by a general algorithm.}
% \jiaju{TODO: write this in more detail}

\subsubsection{Stage 4: Inter-sentence processing}
An important part of describing motion graphics is the sequencing of
motions.  In the last stage of our parsing pipeline, we iterate
through every atomic sentence and look for expressions that add
sequencing constraints between motions.  We again use a fine-tuned
transformer model for classification.
\maneesh{For classification into what? What exactly do we do? What is the algorithm?}
As shown in
Figure~\ref{fig:parser}, the word ``then'' signals that the rotation
motion needs to happen after the translation motion, and ``while''
indicates that the scaling needs to take place at the same time as the
rotation.  We map each of these words to their corresponding timing
predicates as described in Allen's interval
algebra~\cite{allen1983interval}.
\maneesh{This interval algebra needs to be described in more detail and likely with a figure.}
The final output verification
program is shown at the bottom of Figure~\ref{fig:parser}.
% For example, if a sentence contains the word ``then'' as an adverb modifying the verb, we create a timing predicate that checks whether if the motion described in this sentence takes place after the previous sentence (e.g. \texttt{preceds(m\_1, m\_2)}).
% \jiaju{TODO: write this in more detail}

% LocalWords:  DSL str dir dur preceds motA motB fastcoref Intra NUM
% LocalWords:  spaCy subtree SetFit finetuned finetune TODO expr pre
% LocalWords:  VidProm intra lemmatized roberta advcl relcl subtrees
% LocalWords:  prev awk LLM SVG Verifier Parsers keyframe GSAP Zhang
% LocalWords:  verifier et al spatio Balbiani
