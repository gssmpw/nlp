\section{Method}
\label{sec:method}
The proposed method comprises three main components: perceptual hashing (Sec. \ref{sec:hashing}), similarity measurement and comparison (Sec. \ref{sec:similarity}), and pseudo-labeling (Sec. \ref{sec:pseudo}). Each component is designed to address specific challenges in distinguishing between human-generated and AI-generated content, as well as classifying content from different generative models. 

In Figure~\ref{fig:flow} we illustrate the overall framework, which consists of three stages: training, new label adaptation, and inference. In the training stage, we train the model using the ArcFace loss (Eq. \eqref{eq:arcface}) and apply pseudo-labeling to augment the dataset. Features are extracted from the input data and stored for later use, with pruning techniques for $k$-nearest neighbors employed to minimize storage requirements \cite{pedregosa2011scikit}. In the new label adaptation stage, features are extracted from the new data and seamlessly integrated into the existing feature storage. Finally, in the inference stage, features are extracted from incoming data and compared with the stored features to determine the most similar label. For text we adopted BART Large \cite{lewis2019bart}, and for image we used Swin Transformer V2 Base \cite{liu2022swin} as backbone model. This design ensures scalability and adaptability to new generative models while maintaining robust performance. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/flow.pdf}
    \caption{\small Overall framework of the proposed method, which consists of three stages, \ie, training, new label adaptation, and inference.}
    \label{fig:flow}
\end{figure*}


\subsection{Perceptual hashing}
\label{sec:hashing}
Perceptual hashing is key to our method, offering a compact and efficient data representation for comparison. We train a model with ArcFace loss \cite{deng2019arcface} to enhance feature discrimination by enforcing class margins. The trained model outputs high-dimensional feature vectors, which serve as perceptual hash digests for the input samples.

Once trained, the model extracts features from all samples, converting them into high-dimensional feature vectors, referred to as hashing digests. These vectors are stored in a database and used for similarity comparisons. This approach enables efficient comparison and retrieval, leveraging the discriminative capability of the learned representations to distinguish between human-generated and AI-generated content, as well as between different generation models.

\subsection{Similarity measurement and comparison}
\label{sec:similarity}
To compare features, we employ a $k$-nearest neighbor ($k$-NN) approach, inspired by solutions from prior competitions \cite{toofanee2023dfu, jeon20201st}. This non-parametric method allows us to measure the similarity between samples effectively, leveraging the local neighborhood structure within the feature space. Specifically, we calculate the similarity metric using the cosine similarity, which is well-suited for high-dimensional feature spaces and ensures scale-invariant comparisons. The simplicity and adaptability of $k$-NN make it particularly suited for our scenario, where the inclusion of new labels or generative models is anticipated.

When new labels appear, our method avoids retraining the entire model. Instead, features from a few representative samples generated by the new AI model extracted and then appended to the existing feature set. Seamlessly integrating the new class into the similarity-based comparison process. This approach ensures a scalable and efficient adaptation mechanism, maintaining the system's flexibility to accommodate evolving AI-generated content without significant computational overhead.


\subsection{Pseudo labeling}
\label{sec:pseudo}
Pseudo-labeling is employed as a crucial component in our approach to address the limitations of a small labeled training dataset while improving the model's adaptability and robustness. By leveraging unlabeled data, pseudo-labeling effectively expands the training set, where high-confidence predictions of the model are used as surrogate labels \cite{cascante2021curriculum}. This augmentation strategy not only increases the diversity of the training samples but also familiarizes the model with potential augmentations present in test data, thereby enhancing generalization performance.

In deployment scenarios, pseudo-labeling demonstrates its practicality by exploiting the abundance of unlabeled data derived from user inputs. These inputs, typically available in substantial volumes and without explicit labels, provide a valuable resource for iterative model training and fine-tuning. Furthermore, as AI-generated content evolves with updates to generative models over time, pseudo-labeling allows the model to adapt dynamically to these changes. This adaptability ensures robust performance in real-world applications, where continuous learning from new and diverse data is essential to maintaining high accuracy and relevance.

Similar to the approach proposed in \cite{toofanee2023dfu}, we utilize a dynamic pseudo-labeling mechanism to enhance our training process. Specifically, at each training epoch, the model is used to predict labels for the test set, and only the top $p_{pseudo}$ percent of predictions with the highest confidence probabilities are selected as pseudo-labeled data. These high-confidence samples are then added to the training process to further refine the model.

In subsequent epochs, the pseudo-labeling step is repeated, and a new set of top $p_{pseudo}$ percent high-confidence predictions is selected. This iterative process helps to progressively eliminate potentially incorrect pseudo-labels, ensuring that only reliable samples contribute to the training. Importantly, the original labeled training data remains the primary focus during training, as it provides a reliable foundation with ground-truth labels. The pseudo-labeled samples constitute only a small fraction of the training data, serving as a complementary augmentation to improve model adaptability. The training continues in this manner until the model converges, leveraging the evolving predictions to adapt effectively and maintain robustness against noisy pseudo-labels.
