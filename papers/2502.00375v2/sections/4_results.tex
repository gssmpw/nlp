\section{Experiments}
\label{sec:exp}
We conduct experiments to evaluate the effectiveness of our proposed method on both image and text datasets from the Defactify4 competition. The experiments are designed to test the modelâ€™s ability to distinguish between human-generated and AI-generated content, to classify content across different generative methods, and to adapt to new labels. Detailed descriptions of the datasets (Sec. \ref{sec:dataset}), implementation (Sec. \ref{sec:implementation}), and results (Sec. \ref{sec:results}) are provided in the following subsections.


\subsection{Dataset}
Our experiments utilize two benchmark datasets: \textbf{Defactify4-Image} and \textbf{Defactify4-Text}, each designed to evaluate AI vs. human classification and method-specific categorization. Both datasets comprise training and testing splits, with the testing sets further divided into unaltered data (\texttt{Test 1}) and augmented data (\texttt{Test 2}) to assess model robustness and generalization. \textbf{Defactify4-Image} consists of six classes, where one class represents real images derived from the COCO dataset \cite{lin2014microsoft}, and the remaining five classes correspond to outputs from different AI image-generation models. Meanwhile, \textbf{Defactify4-Text} comprises seven classes, including one class of human-written text and six classes generated by various text-generation models. Detailed descriptions of the datasets are provided in Appendix~\ref{sec:appendix_datasets}.


\subsection{Implementation}
\label{sec:implementation}
For the backbone model used to extract features from images, we adopt a Swin Transformer V2 Base model \cite{liu2022swin} with an image size of $256 \times 256$ and a window size of $8$, pre-trained on ImageNet \cite{deng2009imagenet}. For text data, we use a pre-trained BART Large model \cite{lewis2019bart} with a maximum token length of $512$. Training is done for $100$ epochs using Adam optimizer \cite{kingma2014adam} with a learning rate of $1 \times 10^{-4}$ and a batch size of $32$. In addition to the primary inputs, we incorporate auxiliary data that we found to be significant. For text data, we include the text length in terms of the number of characters and words as additional features. For image data, we leverage the image size as a supplementary input. After feature pooling from the backbone and concat with auxiliary data, we apply a fully connected layer with $512$ nodes to reduce the dimensionality of the features, using the Parametric ReLU (PReLU) function as activation layer.

For $k$-nearest neighbors ($k$-NN), we use the implementation provided by scikit-learn \cite{pedregosa2011scikit} with default parameters and adopt cosine as the metric, enabling faster computations and pruning to reduce the number of stored features. 
In the pseudo-labeling process, we select the top $p_{pseudo} = 5\%$ of predictions with the highest confidence probabilities for each predicted label $\hat{y}$, and assign $\hat{y}$ as the label for the corresponding data points.

Data augmentation is applied to both image and text data to enhance robustness. For text, we introduce random starting and ending positions in sequences and inject random meaningless strings at arbitrary points within the data. For image data, we employ augmentations including horizontal flip, Gaussian noise, image compression, and random brightness/contrast adjustments. To ensure compatibility with the input pipeline, resizing operations are performed while preserving the aspect ratio of the original image.


\subsection{Results}
\label{sec:results}
In this section, we present the results obtained from evaluating our proposed method on both image and text datasets. The competition for each data type is divided into two tasks: \texttt{Task A} focuses on distinguishing between AI-generated and human-produced content, while \texttt{Task B} classifies the content across different methods, with human-produced as one of the categories.

\begin{table}[h]
\begin{minipage}{.49\linewidth}
    \caption{\small Leaderboard of the \textbf{Defactify4-Image} task}
    \resizebox{0.7\linewidth}{!}{\begin{tabular}{l|ll}
    Team Name           & Task A           & Task B \\ \hline
    SeeTrails           & 0.8334           & 0.4986           \\
    NYCU                & 0.8329           & 0.491            \\
    random.py           & 0.8326           & 0.4936           \\
    Xiaoyu              & 0.8316           & 0.4888           \\
    TAHAKOM             & 0.8305           & 0.4816           \\
    SKDU                & 0.83             & 0.4864           \\
    Nitiz               & 0.8152           & 0.4193           \\
    OAR                 & 0.7996           & 0.2726           \\
    RoVIT               & 0.759            & 0.4222           \\ \hline
    dakiet (our method) & 0.833            & 0.4935          
    \end{tabular}}
    \label{tab:lb_image}
\end{minipage}
\begin{minipage}{.49\linewidth}
    \caption{\small Leaderboard of the \textbf{Defactify4-Text} task}
    \resizebox{0.7\linewidth}{!}{\begin{tabular}{l|ll}
    Team Name           & Task A           & Task B \\ \hline
    Sarang              & 1                & 0.9531           \\
    tesla               & 0.9962           & 0.9218           \\
    SKDU                & 0.9945           & 0.7615           \\
    Drocks              & 0.9941           & 0.627            \\
    Llama\_Mamba        & 0.988            & 0.4551           \\
    AI\_Blues           & 0.9547           & 0.3697           \\
    NLP\_great          & 0.9157           & 0.1874           \\
    Osint               & 0.8982           & 0.3072           \\
    Xiaoyu              & 0.803            & 0.5696           \\
    Rohan               & 0.7546           & 0.4053           \\ \hline
    dakiet (our method) & 0.9999           & 0.9082          
    \end{tabular}}
\label{tab:lb_text}

\end{minipage} 
\end{table}

In Table \ref{tab:lb_text} and Table \ref{tab:lb_image}, we present the leaderboards (evaluated on \texttt{Test 2}) for the text and image tasks, respectively, in the competition. Our proposed method demonstrates competitive performance compared to other teams. Specifically, we achieved the 2nd place in Task A and the 3rd place in Task B for both text and image datasets. This highlights the effectiveness of our method, which not only achieves high accuracy but also incorporates the ability for continual learning through pseudo-labeling. More importantly, our method has the potential to expand the number of labels, accommodating new classes in the future. This is crucial given the increasing diversity of AI-generated models and outputs. The capability to seamlessly add new labels ensures our approach remains adaptable and robust as the landscape of generative AI continues to evolve.
