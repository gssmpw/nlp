\section{Appenddix}
\label{sec:appendix}
\subsection{Related work}
\label{sec:appendix_related_work}
In this section, we review related work on three key areas. First, we discuss 
loss functions used for classification tasks (Sec. \ref{sec:lmcl}). 
Next, we explore advancements in text generation with large language models (LLMs), which have significantly advanced the field of natural language processing, and the detection of AI-generated text (Sec. \ref{sec:gen_text}). Finally, we present recent progress in image generation models and the detection of AI-generated images (Sec. \ref{sec:gen_image}).


\subsubsection{Loss functions for classification}
\label{sec:lmcl}
We begin by analyzing the conventional Softmax loss, a widely used loss function in classification tasks:

\begin{equation}
\label{eq:softmax}
    L_{\text{Softmax}}=-\frac{1}{N}\sum\limits_{i=1}^{N}{\log }\frac{{{e}^{W_{{{y}_{i}}}^{T}{{x}_{i}}+{{b}_{{{y}_{i}}}}}}}{\sum\limits_{j=1}^{n}{{{e}^{W_{j}^{T}{{x}_{i}}+{{b}_{j}}}}}}
    =
    -\frac{1}{N}\sum\limits_{i=1}^{N}{\log }\frac{{{e}^{\left\| {W_{{{y}_{i}}}} \right\|\left\| {{x}_{i}} \right\|\cos \left( {{\theta }_{y_i x_i}} \right)+{{b}_{{{y}_{i}}}}}}}{\sum\limits_{j=1}^{n}{{{e}^{\left\| {W_j} \right\|\left\| {{x}_{i}} \right\|\cos \left( {{\theta }_{j x_i}} \right)+{{b}_{j}}}}}}
\end{equation}
where ${{y}_{i}}$ is the class label of the $i^{\text{th}}$ sample, ${{\theta }_{j x_i}}$ is the angle between the weight ${{W}_{j}}^{T}$ and the feature ${{x}_{i}}$, $n$ is the total number of classes, and $N$ is the number of samples. Models trained with the Softmax loss are limited to a fixed set of classes and require retraining whenever new labels are added. This limitation comes from the fully connected layer, which has a fixed number of nodes corresponding to the predefined classes.


One workaround for models trained with Softmax loss is to remove the final fully connected layer and use the extracted feature embeddings. However, as shown in \cite{deng2019arcface}, this method still has limitations. While Softmax can generate separable feature embeddings, it often leads to unclear decision boundaries, making it ineffective for robust classification without further adjustments. In contrast, the Large Margin Cosine Loss (LMCL) explicitly introduces a margin between the closest classes, leading to better-defined decision boundaries and more robust separability.

By applying $l_{2}$ normalization to both the weights and features, ensuring $\left\| {{W}_{j}} \right\|=\left\| {{x}_{i}} \right\|=1$, introducing a scaling factor $s$, and setting the bias ${{b}_{j}}=0$, as described in \cite{wang2017normface}, the original Softmax loss function (\ref{eq:softmax}) is reformulated as:

\begin{equation}
\label{eq:normface}
    L_{NormFace}=-\frac{1}{N}\sum\limits_{i=1}^{N}{\log }\frac{{{e}^{s\cos {{\theta }_{{{y}_{i}}}}}}}{\sum\limits_{j=1}^{n}{{{e}^{s\cos {{\theta }_{ji}}}}}}=-\frac{1}{N}\sum\limits_{i=1}^{N}{\log }\frac{{{e}^{s\cos {{\theta }_{{{y}_{i}}}}}}}{{{e}^{s\cos {{\theta }_{{{y}_{i}}}}}}+\sum\limits_{j=1,j\ne {{y}_{i}}}^{n}{{{e}^{s\cos {{\theta }_{ji}}}}}}.
\end{equation}

As mentioned earlier, the Softmax loss function suffers from limitations when it comes to handling the addition of new classes and ensuring clear decision boundaries. Since the embedding features are distributed around each class center on the hypersphere, an additive angular margin penalty, $m$, between the feature vector $x_i$ and the corresponding class weight $W_{y_i}$ can be introduced. This margin enhances both the intra-class compactness and inter-class separability, encouraging better feature discrimination. In other words, the model learns to increase the angular distance between different classes while simultaneously reducing the distance within the same class.

\begin{equation}
\label{eq:arcface}
    L_{ArcFace}=-\frac{1}{N}\sum\limits_{i=1}^{N}{\log }\frac{{{e}^{s\left( \cos \left( {{\theta }_{{{y}_{i}}}}+m \right) \right)}}}{{{e}^{s\left( \cos \left( {{\theta }_{{{y}_{i}}}}+m \right) \right)}}+\sum\limits_{j=1,j\ne {{y}_{i}}}^{n}{{{e}^{s\cos {{\theta }_{ji}}}}}}
\end{equation}

This reformulation, known as ArcFace \cite{deng2019arcface}, incorporates the additive angular margin $m$ directly into the decision boundaries, as shown in Equation (\ref{eq:arcface}). ArcFace has established itself as a state-of-the-art approach for classification tasks, especially in scenarios where new labels may emerge outside the training set. While newer loss functions may surpass ArcFace in specific scenarios, its widespread use in competitions and ease of implementation have solidified its position as a benchmark choice for its combination of performance, versatility, and reliability \cite{jeon20201st}.


\subsubsection{Detection of AI-generated text}
\label{sec:gen_text}
Text generation has witnessed rapid advancements with the development of large language models (LLMs) such as GPT-3 \cite{brown2020language}, GPT-4 \cite{achiam2023gpt}, and Mistral \cite{jiang2023mistral}. These models are designed to generate coherent and contextually relevant text, leveraging vast pretraining datasets to produce outputs that closely resemble human-written content. Applications of these models include creative writing, summarization, and conversational agents, showcasing their ability to adapt across diverse linguistic tasks. 

One notable advantage of LLMs lies in their capacity to generalize across tasks with minimal fine-tuning, often performing well with zero-shot or few-shot learning. This flexibility has enabled their widespread adoption in various fields, but it has also introduced challenges in distinguishing between human-written and AI-generated text. As models become increasingly realistic, research has focused on methods to identify synthetic text, emphasizing the importance of robust detection frameworks to mitigate misuse and ensure trust in AI-generated content \cite{wu2024continual}. Recent studies have explored ensemble methods that aggregate output probabilities from multiple backbone models trained with softmax loss, achieving competitive results in text classification tasks \cite{mohamed2024proposed, abburi2023generative}. However, while effective, these approaches exhibit limited scalability when faced with a growing number of labels, underscoring the need for alternative methods capable of adapting to the dynamic nature of generative AI.

\subsubsection{Detection of AI-generated images}
\label{sec:gen_image}
Recent years have seen remarkable progress in image generation, driven by models such as Stable Diffusion \cite{rombach2022high}, DALL-E \cite{ramesh2021zero}, Midjourney \cite{midjourney}.
These generative models are capable of creating high-quality, photorealistic images from textual prompts, offering unprecedented control over the content and style of the generated visuals. Their applications range from digital art and design to content creation and entertainment, making them invaluable tools in creative industries.

These models operate by learning to generate images that align with textual descriptions, often leveraging latent space representations to ensure fine-grained control over image attributes. The rapid evolution of these models has enabled the creation of visually compelling and contextually accurate outputs, but it has also raised ethical and practical concerns. The ability to generate highly realistic images poses risks for misinformation and deceptive media. Consequently, the development of reliable classification systems to differentiate between real and AI-generated images has become a pressing need to ensure the ethical use of such technologies \cite{deandres2024frcsyn}.

Deep learning generated fake images are typically created by generative neural network (GAN) models, but can also be created using autoencoders \citep{wangCNNGeneratedImagesAre2020}. In most cases, the creation of fake images consists of replacing a person or a face in an existing image or video with another person or face.
As videos contain more information than images, most methods apply to the detection of fake videos \citep{camachoComprehensiveReviewDeepLearningBased2021}. 
However some methods have been proposed for images.
The first methods for fake image detection focus on the detection of images generated by a specific GAN \citep{marraDetectionGANGeneratedFake2018}. 
Common CNN models detect spatial cues such as artifacts on facial boundaries \citep{liFaceXRayMore2020}, or traces left by the GAN  \citep{liExposingDeepFakeVideos2018,xuanGeneralizationGANImage2019,wangCNNGeneratedImagesAre2020}.
\citet{khalidOCFakeDectClassifyingDeepfakes2020} propose a one-class variational autoencoder model to detect fake images as anomalies.
However, these methods cannot adapt to new AI image generation models.

\subsection{Additional experiments}
In this section, we present additional experiments to evaluate the adaptability of our method when integrating a new label without retraining the model. 
\begin{table}[h]
\caption{Performance results on the \textbf{Defactify4-Image} dataset for Task 1 (AI-generated vs. Human-produced) and Task 2 (Classification of different methods). \texttt{all} indicates using the entire dataset with the proposed method (using ArcFace loss), while \texttt{all-x} refers to the dataset excluding samples from the class labeled as \texttt{x}. And \texttt{swinv2's training data} refers to using a pre-trained Swin Transformer V2 \cite{liu2022swin} model to extract features from images without additional training.}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cccccccccc}
\multicolumn{2}{c|}{\multirow{3}{*}{Testing data}} & \multicolumn{10}{c}{Training data}                                                                                                                                                                                                                         \\ \cline{3-12} 
\multicolumn{2}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}swinv2's\\ training data\end{tabular}}} & \multicolumn{9}{c}{Defactify dataset}                                                                                                               \\ \cline{4-12} 
\multicolumn{2}{c|}{}                              & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{all-dalle-sdxl} & all-midjourney & all-dalle & all-real & all-sdxl & all-sd21 & \multicolumn{1}{c|}{all-sd3} & Softmax & all    \\ \hline
\multirow{2}{*}{Test 1}          & Task A          & \multicolumn{1}{c|}{0.98}                                                                            & \multicolumn{1}{c|}{0.98}           & 1.00           & 1.00      & 0.98     & 0.99     & 1.00     & \multicolumn{1}{c|}{0.99}    & 1.00    & 1.00   \\
                                 & Task B          & \multicolumn{1}{c|}{0.90}                                                                            & \multicolumn{1}{c|}{0.92}           & 1.00           & 0.99      & 0.94     & 0.98     & 0.99     & \multicolumn{1}{c|}{0.96}    & 1.00    & 1.00   \\ \hline
\multirow{2}{*}{Test 2}          & Task A          & \multicolumn{1}{c|}{0.7651}                                                                          & \multicolumn{1}{c|}{0.7817}         & 0.8295         & 0.8306    & 0.8034   & 0.8278   & 0.8293   & \multicolumn{1}{c|}{0.8250}  & 0.8303  & 0.8330 \\
                                 & Task B          & \multicolumn{1}{c|}{0.3170}                                                                          & \multicolumn{1}{c|}{0.3598}         & 0.4506         & 0.4766    & 0.3920   & 0.4672   & 0.4637   & \multicolumn{1}{c|}{0.4383}  & 0.4870  & 0.4935 \\ \hline
\end{tabular}}
\label{tab:result_image}
\end{table}

Table \ref{tab:result_image} shows the performance of our method on the \textbf{Defactify4-Image} dataset. In both tests, both the Softmax loss and our proposed framework (using ArcFace loss) perform well and have relatively similar results. Our proposed method performs slightly better in \texttt{Test 2}, and both methods achieve perfect accuracy in \texttt{Test 1}. Using the pre-trained model without fine-tuning results in relatively low performance. This is because the model was trained on a broad dataset and does not perform well on a fine-grained dataset for specific tasks. Additionally, AI-generated images may have a different distribution compared to the images the model was trained on, causing difficulties in performance. When removing two labels from the data, the model experiences a significant drop in performance, though it still outperforms the pre-trained Swin Transformer V2 Base model. When removing one label for training, the model still maintains acceptable performance, except when excluding the "real" label. The reason for this is that the gap between AI-generated images and human-produced images is relatively large, so training exclusively on AI-generated images causes the model to become unfamiliar with real image data. However, when removing one AI-generated label, the model performs slightly lower but still remains highly competitive compared to the Softmax method. This demonstrates the potential for future expansion of the number of labels in our proposed method.

\begin{table}[h]
\caption{Performance results on the \textbf{Defactify4-Text} dataset for Task 1 (AI-generated vs. Human-produced) and Task 2 (Classification of different methods). \texttt{all} indicates using the entire dataset with the proposed method (using ArcFace loss), while \texttt{all-x} refers to the dataset excluding samples from the class labeled as \texttt{x}. And \texttt{BART's training data} refers to using a pre-trained BART \cite{lewis2019bart} model to extract features from input text without additional training.}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|ccccccccccc}
\multicolumn{2}{c|}{\multirow{3}{*}{Testing data}} & \multicolumn{11}{c}{Training data}                                                                                                                                                                                                                                                     \\ \cline{3-13} 
\multicolumn{2}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BART's\\ training data\end{tabular}}} & \multicolumn{10}{c}{Defactify dataset}                                                                                                                                            \\ \cline{4-13} 
\multicolumn{2}{c|}{}                              & \multicolumn{1}{c|}{}                                                                              & \multicolumn{1}{c|}{all-yi-llama} & \multicolumn{1}{l}{all-Human} & all-gemma2 & all-mistral & all-qwen2 & all-llama & all-yi & \multicolumn{1}{c|}{all-gpt4o} & Softmax & all    \\ \hline
\multirow{2}{*}{Test 1}          & Task A          & \multicolumn{1}{c|}{0.98}                                                                          & \multicolumn{1}{c|}{0.99}         & 0.98                          & 1.00       & 1.00        & 0.99      & 1.00      & 1.00   & \multicolumn{1}{c|}{1.00}      & 1.00    & 1.00   \\
                                 & Task B          & \multicolumn{1}{c|}{0.94}                                                                          & \multicolumn{1}{c|}{0.95}         & 0.95                          & 0.96       & 0.96        & 0.95      & 0.95      & 0.96   & \multicolumn{1}{c|}{0.95}      & 0.96    & 0.96   \\ \hline
\multirow{2}{*}{Test 2}          & Task A          & \multicolumn{1}{c|}{0.9970}                                                                        & \multicolumn{1}{c|}{0.9978}       & 0.9950                        & 0.9980     & 0.9980      & 0.9970    & 0.9978    & 0.9971 & \multicolumn{1}{c|}{0.9947}    & 0.9963  & 0.9999 \\
                                 & Task B          & \multicolumn{1}{c|}{0.7946}                                                                        & \multicolumn{1}{c|}{0.8190}       & 0.8350                        & 0.8738     & 0.8692      & 0.8206    & 0.8462    & 0.8433 & \multicolumn{1}{c|}{0.8268}    & 0.9049  & 0.9082 \\ \hline
\end{tabular}}
\label{tab:result_text}
\end{table}

Table \ref{tab:result_text} shows the performance of our method on the \textbf{Defactify4-Text} dataset. In both tests, similar to the image task the Softmax method and our proposed method perform well and have relatively similar results. Our proposed method performs slightly better in \texttt{Test 2}. Using the pre-trained model without fine-tuning results in relatively low performance. When removing one label for training, the model still maintains acceptable performance. However, compared to the image dataset, the performance drops when using only the pre-trained BART Large model or excluding the "human" label is less noticeable here. This suggests that AI-generated text has a distribution that is not drastically different from the original wide dataset, and human-generated text does not deviate much from the other labels. This indicates that text generation models are performing better than image generation models, as the distribution of outputs between human and AI-generated text is less distinct. However, more data is needed to fully validate this observation. When removing two labels from the data, the model experiences a significant drop in performance, though it still outperforms the pre-trained model.

In both Table \ref{tab:result_image} and Table \ref{tab:result_text}, when removing one AI-generated label the performance of the model decreases in both tasks. However, the model performs slightly lower but still remains highly competitive compared to the Softmax method. This demonstrates the potential for future expansion of the number of labels in our proposed method.

\subsection{Dataset detail}
\label{sec:appendix_datasets}
The experiments are conducted on two benchmark datasets: \textbf{Defactify4-Image} (Sec. \ref{sec:data_img}) and \textbf{Defactify4-Text} (Sec. \ref{sec:data_text}). Each dataset includes training and testing splits tailored for two tasks: AI vs. human classification and method-specific categorization. The datasets are carefully designed with a mix of real and AI-generated data, incorporating augmentations to test the model’s robustness and generalization capabilities. Below, we provide detailed descriptions of each dataset.
\label{sec:dataset}
\subsubsection{Dataset of AI-generated images}
\label{sec:data_img}
The \textbf{Defactify4-Image} dataset is a benchmark designed to evaluate the ability to distinguish between real and AI-generated images. It comprises seven data categories (6 classes and captions),  including captions and various image sources. Among these, real images selected from the COCO dataset \cite{lin2014microsoft} are represented by the \texttt{coco\_image} class. While the other five categories (\texttt{sd3\_image}, \texttt{sd21\_image}, \texttt{sdxl\_image}, \texttt{dalle\_image}, \texttt{midjourney\_image}) are generated using specific AI models: Stable Diffusion (v3 \cite{esser2024scaling}, v2.1 \cite{rombach2022high}, XL \cite{podell2023sdxl}), DALL-E \cite{betker2023improving}, and MidJourney \cite{midjourney}, respectively. Captions act as the input prompts for these generative models and correspond to the caption of real images in the dataset.

\paragraph{Training Data}
The training set consists of $42,000$ images across six classes, with $7,000$ samples per class. Each class corresponds to one of the five generative models and the real image class (\texttt{coco\_image}). All images share the same caption within their index, for instance, \texttt{sd3\_image[i]}, \texttt{sd21\_image[i]} are generated from the same \texttt{caption[i]} of \texttt{coco\_image[i]}. Where \texttt{record[i]} refers to the $i^{th}$ sample of the $record$ in the dataset.

\paragraph{Testing Data} The dataset provides two distinct test sets to assess model performance. \texttt{Test 1} comprises $9,000$ images, each paired with its original caption, representing unaltered outputs from generative models or real images. While \texttt{Test 2} consists of $45,000$ images where augmentation techniques have been applied, enabling evaluation of the model's robustness and generalization across various transformations.

\subsubsection{Dataset of AI-generated text}
\label{sec:data_text}
The \textbf{Defactify4-Text} dataset is a benchmark designed to evaluate the ability to distinguish between human-written and AI-generated text. It comprises eight columns: \texttt{prompt}, \texttt{Human\_story}, \texttt{gemma-2-9b}, \texttt{mistral-7B}, \texttt{qwen-2-72B}, \texttt{llama-8B}, \texttt{yi-large}, and \texttt{GPT\_4-o}. The \texttt{prompt} column contains the instruction, while \texttt{Human\_story} is a human-written text corresponding to the \texttt{prompt}. The remaining columns represent outputs from various generative models, including \texttt{gemma-2-9b} \cite{gemma_2024}, \texttt{mistral-7B} \cite{jiang2023mistral}, \texttt{qwen-2-72B} \cite{qwen2}, \texttt{llama-8B} \cite{llama3modelcard}, \texttt{yi-large} \cite{young2024yi}, and \texttt{GPT\_4-o} \cite{achiam2023gpt}, based on the provided \texttt{prompt}.

\paragraph{Training Data}
The training set consists of $51,248$ text samples across seven classes, with $7,321$ samples per class. Each class corresponds to one of the generative models or the human-written text class (\texttt{Human\_story}). Similar to \textbf{Defactify4-Image} all samples share the same prompt within their index.

\paragraph{Testing Data}
For evaluation, the dataset provides two separate test sets. \texttt{Test 1} includes $10,983$ samples, each associated with its original prompt and the corresponding generated or human-written text. These samples represent the raw outputs from the generative models or human authors. On the other hand, \texttt{Test 2} contains $10,963$ samples where various augmentation techniques have been applied to the text data, allowing for the assessment of model robustness and generalization under various transformations.

\subsection{Discussion}
The proposed method demonstrates strong scalability, particularly in its ability to adapt to new labels and modalities. However, the approach has certain limitations, primarily the need to store feature representations. Specifically, the extracted features consist of high-dimensional floating-point vectors, which can impose significant memory requirements as the dataset size increases. For instance, in our implementation, each sample in the training set requires $32 \times 512 = 16,384$ bits to store, given that each floating-point number occupies $32$ bits, and $512$ is the length of the vector. This storage demand becomes challenging with large-scale datasets.

Recent studies have highlighted strategies to address this issue by converting high-dimensional floating-point vectors into shorter binary vectors \cite{cui2020exchnet, shen2022semicon}. These approaches significantly reduce storage requirements but often come with a trade-off in terms of performance, making them less suitable for competitive tasks such as those in this challenge. Exploring methods to balance memory efficiency and accuracy remains an important area for future research.
