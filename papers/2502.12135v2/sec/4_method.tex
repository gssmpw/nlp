\section{Methods}

\begin{figure*}
  \centering
\includegraphics[width=\textwidth]
  % \includegraphics[scale=0.29]
  {fig/fig4_ar_method.pdf}
  \caption{\textbf{Overview of our method for auto-regressive skeleton generation.} Given an input mesh, we begin by sampling point clouds from its surface. These sampled points are then encoded into fixed-length shape tokens, which are appended to the start of skeleton tokens to achieve auto-regressive skeleton generation conditioned on input shapes. The input mesh is generated by Rodin Gen-1 \cite{zhang2024clay}.}
  \label{fig_ar}
  % \vspace{-15pt}
\end{figure*}

We propose a two-stage pipeline to make 3D models articulation-ready. Given an input 3D mesh, our method first employs an auto-regressive transformer to generate a structurally coherent skeleton (\Cref{ar}). Subsequently, we predict skinning weights in a functional diffusion process, conditioning on both the input shape and its corresponding skeleton (\Cref{skin}). 

\subsection{Auto-regressive skeleton generation}
\label{ar}
In the initial stage of \ours{}, we generate skeletons for 3D models. Unlike previous approaches that rely on fixed templates, our method can handle the inherent structural diversity of 3D objects through an auto-regressive generation framework, as presented in \Cref{fig_ar}.

\subsubsection{Problem formulation}
Given an input 3D mesh $\mathcal{M}$, our goal is to generate a structurally valid skeleton $\mathcal{S}$ that captures the articulation structure of the object. A skeleton consists of two key components: a set of joints $\mathbf{J} \in \mathbb{R}^{j \times 3}$ defining spatial locations, and bone connections $\mathbf{B} \in \mathbb{N}^{b \times 2}$ specifying the topological structure through joint indices. Formally, we aim to learn the conditional distribution:

\begin{equation} \mathit{p}(\mathcal{S} | \mathcal{M}) = \mathit{p}(\mathbf{J}, \mathbf{B} | \mathcal{M}), \end{equation}
where $\mathcal{M}$ can be sourced from various inputs, including direct 3D models, text-to-3D generation, or image-based reconstruction.

A key challenge in skeleton generation lies in the variable complexity of articulation structures across different objects. Traditional approaches \cite{baran2007automatic, li2021learning} often adopt predefined skeleton templates, which work well for specific categories like human bodies but fail to generalize to objects with diverse structural patterns. 
This limitation becomes particularly apparent when dealing with our large-scale dataset that contains a wide range of object categories.


To address this challenge, we draw inspiration from recent advances in auto-regressive mesh generation \cite{siddiqui2024meshgpt, chen2024meshanythingv2} and reformulate skeleton generation as a sequence modeling task. This novel formulation allows us to:
1) handle varying numbers of bones or joints within skeletons across different 3D models;
2) capture the inherent dependencies between bones;
3) scale effectively to diverse object categories.

\subsubsection{Sequence-based generation framework}
Our framework transforms the skeleton generation task into a sequence modeling problem through four key components: skeleton tokenization, sequence ordering, shape conditioning, and auto-regressive generation. 

\boldstartspace{Skeleton tokenization.}
We represent each skeleton $\mathcal{S}$ as a sequence of bones, where each bone is defined by its two connecting joints ($6$ coordinates in total). To ensure consistent and discrete representation, we employ a carefully designed tokenization process. 
We first scale and translate the input mesh and corresponding skeleton to a unit cube $[-0.5, 0.5]^3$, ensuring their spatial alignment. 
Subsequently, we map the normalized joint coordinates to a discrete $128^3$ space, leading to a sequence length of $6b$ for $b$ bones. 
As such, the discretized coordinates are converted into tokens, which serve as input to the auto-regressive transformer.
Unlike MeshGPT \cite{siddiqui2024meshgpt}, we omit the VQ-VAE compression step based on our dataset analysis. Specifically, in \ourdata{}, most of the models have fewer than 100 bones (i.e., 600 tokens). 
Given these relatively short sequence lengths, using VQ-VAE compression would potentially introduce artifacts without significant benefits in computational efficiency.

\begin{figure}
  \centering
\includegraphics[scale=0.22]
{fig/fig_ordering.pdf}
  \caption{\textbf{Spatial sequence ordering versus hierarchical sequence ordering.} The numbers indicate the bone ordering indices.}
  \label{fig_ar}
  % \vspace{-15pt}
\end{figure}

\boldstartspace{Sequence ordering.} 
In this work, we investigate two distinct ordering strategies. Our first approach follows the sequence ordering strategy from recent 3D mesh generation methods \cite{nash2020polygen, siddiqui2024meshgpt}. In this approach, joints are initially sorted in ascending z-y-x order (with z representing the vertical axis), and the corresponding joint indices in the bones are updated accordingly. Bones are then ordered first by their lower joint index and subsequently by the higher one. Additionally, for each bone, the joint indices are cyclically permuted so that the lower index appears first. we refer to this ordering as \textbf{spatial sequence ordering} in this paper. However, this ordering strategy disrupts the parent-child relationships among bones and does not facilitate identifying the root joint. Consequently, additional processing is required to build the skeleton's hierarchy.

To overcome these limitations, we propose an alternative approach termed \textbf{hierarchical sequence ordering}\footnote{Hierarchical ordering is an extension of our under review version.}, which leverages the intrinsic hierarchical structure of the skeleton by processing bones layer by layer. After sorting joints in ascending z-y-x order and updating their indices in bones, we first order the bones directly connected to the root joint. When the root has several child joints, we begin with the bone linked to the child joint having the smallest index and then proceed in ascending order. For subsequent layers, bones are grouped by their immediate parent, and within each group, they are arranged in ascending order based on the child joint index. Additionally, among groups in the same layer, the group corresponding to the smallest parent joint index is processed first, followed by those with larger indices.

\boldstartspace{Shape-conditioned generation.}
Following the conventions in \cite{chen2024meshanythingv2, chen2024meshanything}, we utilize point clouds as the shape condition by sampling 8,192 points from the input mesh $\mathcal{M}$. 
We then process this point cloud through a pre-trained shape encoder \cite{zhao2024michelangelo}, which transforms the raw 3D geometry into a fixed-length feature sequence suitable for transformer processing. 
This encoded sequence is then appended to the start of the transformer's input skeleton sequence for auto-regressive generation. Additionally, for each sequence, we insert a $\textless \mathrm{bos} \textgreater$ token after the shape latent tokens to signify the beginning of the skeleton tokens. Similarly, a $\textless \mathrm{eos} \textgreater$ token is added following the skeleton tokens to denote the end of the skeleton sequence.


\boldstartspace{Auto-regressive learning.}
For skeleton generation, we employ a decoder-only transformer architecture, specifically the OPT-350M model \cite{zhang2022opt}, which has demonstrated strong capabilities in sequence modeling tasks.  During training, we provide the ground truth sequences and utilize cross-entropy loss for next-token prediction to supervise the model: \begin{equation} 
    \mathcal{L}_{pred} = \mathrm{CE}(\mathbf{T}, \mathbf{\hat{T}}) ,
\end{equation} 
where $\mathbf{T}$ represents the one-hot encoded ground truth token sequence, and $\mathbf{\hat{T}}$ denotes the predicted sequence.


At inference time, the generation process begins with only the shape tokens as input, and the model sequentially generates each skeleton token, ending when the $\textless \mathrm{eos} \textgreater$ token is produced. The resulting token sequence is then detokenized to recover the final skeleton coordinates and connectivity structure.

\subsection{Skinning weight prediction}
\label{skin}


The second stage focuses on predicting skinning weights, which controls how the mesh deforms with skeleton movements. In this work, we represent skinning weights as an $n$-dimensional function defined on mesh surfaces, which are continuous, high-dimensional, and exhibit significant variation across different skeletal structures. To address these complexities, we employ a functional diffusion framework for accurate skinning weight prediction.


\subsubsection{Preliminary: Functional diffusion}

Functional diffusion \cite{zhang2024functional} extends classical diffusion models to operate directly on functions, making it particularly suitable for our task. Consider a function $f_0$ mapping from domain $\mathcal{X}$ to range $\mathcal{Y}$:
\begin{equation}
f_0 : \mathcal{X} \rightarrow \mathcal{Y}.
\end{equation}

The diffusion process gradually adds functional noise $g$ (mapping the same domain to range) to the original function:
\begin{equation}
f_t(x) = \alpha_t \cdot f_0(x) + \sigma_t \cdot g(x), \quad t \in [0, 1]
\end{equation}
where $\alpha_t$ and $\sigma_t$ control the noise schedule. The goal is to train a denoiser $D$ that recovers the original function:
\begin{equation}
D_\theta[f_t, t](x) \approx f_0(x).
\end{equation}

This formulation naturally aligns with our task requirements. By treating skinning weights as continuous functions over the mesh surface, we can capture smoothly transitioning weights between vertices. Additionally, the framework's flexibility allows it to adapt to diverse mesh topologies and skeletal structures.

\subsubsection{Skinning weight prediction}
Building upon the functional diffusion framework, we formulate skinning weight prediction as learning a mapping \( f: \mathbb{R}^3 \rightarrow \mathbb{R}^n \) from 3D points to their corresponding weights. Specifically, the input to our model consists of 3D points \( \mathcal{P} \in \mathbb{R}^{v \times 3} \) sampled from the surface of the mesh. The output is an \( n \)-dimensional skinning weight matrix \( \mathcal{W} \in \mathbb{R}^{v \times n} \). Here, the ground truth skinning weights of sampled points for training are copied from their nearest vertices and will also be copied back when inference. $n$ denotes the maximum number of joints in the dataset.

To enhance prediction accuracy, we introduce two key components. 
First, we condition the generation on both joint coordinates and global shape features extracted by a pre-trained encoder \cite{zhao2024michelangelo}. 
Second, we leverage volumetric geodesic priors calculated from \cite{dionne2013geodesic}. Specifically, we compute the volumetric geodesic priors from each mesh vertex to each joint. We then assign these priors to sampled points based on their nearest vertices and normalize them to match the range of skinning weights, forming a volumetric geodesic matrix $\mathcal{G} \in \mathbb{R}^{v \times n}$. Our model learns to predict the residual between the actual skinning weights and this geometric prior, i.e., $f: \mathcal{P} \rightarrow (\mathcal{W}-\mathcal{G})$, enabling more stable predictions.

Following \cite{zhang2024functional}, we optimize our model using $x_0$-prediction with the objective:
\begin{equation}
   \mathcal{L}_{denoise} = \left\| D_{\theta} \left( \left\{ x, f_t(x) \right\}, t \right) - f_0(x) \right\|^2_2, \quad x \in \mathcal{P}.
\end{equation}
We employ the Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising} as our scheduler. In practice, we normalize the skinning weights and volumetric geodesic priors to the range \([-1, 1]\) before adding noise. We will conduct ablation studies on this design in \Cref{ablate_skin}.


