\section{Experiments}
\subsection{Implementation details}
\boldstartspace{Datasets.} We evaluate our method on two datasets: our proposed \ourdata{} and ModelsResource \cite{xu2020rignet, ModelsResource2019}.  \ourdata{} contains 33k samples, with 31.4k for training and 1.6k for testing. ModelsResource is a smaller dataset, containing 2,163 training and 270 testing samples. The number of joints for each object varies from 3 to 48, with an average of 25.0 joints. While the data in \res{} maintains a consistent upright and front-facing orientation, the 3D models in \ourdata{} exhibit varying orientations. We have verified that there are no duplications between \ourdata{} and ModelsResource.

\boldstartspace{Training details.} Our training process consists of two stages. For skeleton generation, we train the auto-regressive transformer on 8 NVIDIA A100 GPUs for approximately two days. For skinning weight prediction, models are trained on the same hardware configuration for about one day. To enhance model robustness, we apply data augmentation including scaling, shifting, and rotation transformations. For more details, please refer to the appendix.

\subsection{Skeleton generation results}
\boldstartspace{Metrics.}
We adopt three standard metrics following \cite{xu2020rignet} to evaluate skeleton quality: CD-J2J, CD-J2B, and CD-B2B. 
These Chamfer Distance-based metrics measure the spatial alignment between generated and ground truth skeletons by computing distances between joints-to-joints, joints-to-bones, and bones-to-bones respectively. Lower values indicate better skeleton quality.


\boldstartspace{Baselines.} 
We compare our method against two representative approaches: Pinocchio \cite{baran2007automatic}, a traditional template-fitting method, and RigNet \cite{xu2020rignet}, a learning-based method using graph convolutions. All methods are evaluated on the \ourdata{} and ModelsResource datasets.

\boldstartspace{Comparison results.}
Qualitative comparisons are presented in \Cref{compare_skel}, where we compare different methods across various object categories.  
Pinocchio struggles with objects that differ from its predefined templates, especially obvious in non-humanoid objects (as shown in the 2nd row and the 3rd row on the right).
RigNet demonstrates improved performance when tested on \res{}, where the data maintains a consistent upright and front-facing orientation. However, it still struggles with complex topologies (as illustrated in the 1st and 2nd rows on the left). Furthermore, RigNet performs worse on \ourdata{}, where the data exhibit varying orientations. 
In contrast, our method generates high-quality skeletons that closely match artist-created references across diverse object categories.


The quantitative results are shown in \Cref{comparison_skel}. Our method consistently outperforms baselines across all metrics on both datasets. Additionally, we compare our method using both spatial and hierarchical ordering strategies. The spatial ordering consistently achieves better performance, likely because the hierarchical ordering requires the model to allocate part of its capacity to learning the skeletonâ€™s hierarchy and identifying the root joint. Results obtained using spatial ordering are well-suited for applications such as skeleton-driven pose transfer \cite{zhang2024magicpose4d}, whereas those derived from hierarchical ordering are more readily integrated with 3D models for animation.


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{fig/fig5_compare_skel.pdf}
  \vspace{-10pt}
  \caption{\textbf{Comparison of skeleton creation results on \res{} (left) and \ourdata{} (right).} Our generated skeletons more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories. }
  \label{compare_skel}
  \vspace{-15pt}
\end{figure*}


\begin{table}
  \caption{\textbf{Quantitative comparison on skeleton generation.} We compare different methods using CD-J2J, CD-J2B, and CD-B2B as evaluation metrics on both \ourdata{} (Arti-XL) and ModelsResource (Modelres.). Lower values indicate better performance. The metrics are in units of $10^{-2}$. Here, * denotes models trained on \ourdata{} and tested on ModelsResource.}
  \vspace{-8pt}
  \label{comparison_skel}
  \centering
  \begin{tabular}{ccccc}
%   \hline
    \toprule
      & Dataset &  CD-J2J  & CD-J2B & CD-B2B \\
    \midrule
    RigNet*  & \multirow{7}{*}{\textit{ModelsRes.}} & 7.132 & 5.486 & 4.640 \\
    Pinocchio  &  & 6.852 & 4.824 & 4.089 \\
    Ours-hier*  &  & 4.451 & 3.454 & 2.998 \\
    RigNet  &  & 4.143 & 2.961  & 2.675 \\
    Ours-spatial*  &  & 4.103 & 3.101 & 2.672 \\
    Ours-hier     &   & 3.654 & 2.775 & 2.412     \\
    Ours-spatial     &   & \textbf{3.343} & \textbf{2.455} & \textbf{2.140}      \\
    
    \midrule
    Pinocchio  &\multirow{4}{*}{\textit{Arti-XL}} & 8.360 & 6.677  & 5.689 \\
    RigNet  & & 7.478 & 5.892 & 4.932   \\
    Ours-hier   &   & 3.025 & 2.408 & 2.083      \\
    Ours-spatial   &   & \textbf{2.586} & \textbf{1.959} & \textbf{1.661}      \\
    
    \bottomrule
  \end{tabular}
  \vspace{-15pt}
\end{table}

\boldstartspace{Generalization analysis.} 
To evaluate the generalization capability, we 
perform cross-dataset evaluation by training RigNet and our MagicArticulate on Articulation-XL and testing on ModelsResource.
As shown in \Cref{comparison_skel} (marked with \textbf{*}), our method maintains competitive performance compared to RigNet trained directly on ModelsResource, while RigNet's performance degrades significantly when tested on unseen data distributions, performing even worse than the template-based method Pinocchio.

To further assess real-world applicability, we evaluate all methods on AI-generated 3D meshes from Tripo 2.0 \cite{tripo3d} (\Cref{generalization}). 
Our method successfully generates plausible skeletons for diverse object categories, while RigNet fails to produce valid results despite being trained on our large-scale dataset. 
Notably, even Pinocchio's template-based approach struggles to generate accurate skeletons for basic categories like humans and quadrupeds, highlighting the advantage of our method in handling novel object structures.


\begin{figure}
  \centering
  \includegraphics[scale=0.28]{fig/fig7_generaliazation.pdf}
  % \vspace{-10pt}
  \caption{\textbf{Skeleton creation results on 3D generated meshes.} Our method has a better generalization performance than both RigNet \cite{xu2020rignet} and Pinocchio \cite{baran2007automatic} across difference object categories. The 3D models are generated by Tripo 2.0 \cite{tripo3d}.}
  \label{generalization}
  % \vspace{-20pt}
\end{figure}


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{fig/fig6_compare_skinning.pdf}
  % \vspace{-12pt}
  \caption{\textbf{Comparisons with previous methods for skinning weight prediction on \res{} (top) and \ourdata{} (bottom).} We visualize skinning weights and L1 error maps. For more results, please refer to the supplementary materials.}
  \label{compare_skin}
  % \vspace{-15pt}
\end{figure*}

\subsection{Skinning weight prediction results}
% \vspace{-10pt}
\boldstartspace{Metrics.} 
We evaluate skinning weight quality using three metrics: precision, recall, and L1-norm error.
Precision and recall measure the accuracy of identifying significant joint influences (defined as weights larger than $1e-4$ following \cite{xu2020rignet}, while the L1-norm error computes the average difference between predicted and ground truth skinning weights across all vertices. We will also report the deformation error in appendix.

\boldstartspace{Baselines.}
We compare our method against Geodesic Voxel Binding (GVB) \cite{dionne2013geodesic}, a geometric-based method available in Autodesk Maya \cite{AutodeskMaya2024} and RigNet \cite{xu2020rignet}. 
When trained on \ourdata{}, we filter out a subset containing 28k training and 1.2k testing samples, excluding data with more than 55 joints (which constitute a small fraction of both real-world cases and \ourdata{}).

\boldstartspace{Comparison results.}
Qualitative comparisons in \Cref{compare_skin} visualize the predicted skinning weights and their L1 error maps against artist-created references. 
Our method predicts more accurate skinning weights with significantly lower errors across diverse object categories.
In contrast, both GVB and RigNet show larger deviations, particularly in regions around joint boundaries. 

The quantitative results are shown in \Cref{comparison_skin}, which support qualitative observations,
demonstrating that our method consistently outperforms baselines across most metrics on both datasets. 


\begin{table}
  \caption{\textbf{Quantitative comparison on skinning weight prediction.} We compare our method with GVB and RigNet. For Precision and Recall, larger values indicate better performance. For average L1-norm error, smaller values are preferred. }
  \vspace{-10pt}
  \label{comparison_skin}
  \centering
  \begin{tabular}{ccccc}
    \toprule
      & Dataset &  Precision  & Recall & avg L1 \\
    \midrule
    GVB &  \multirow{3}{*}{\textit{ModelsResource}}  & 69.3\%  & 79.2\%  & 0.687    \\
    RigNet  &  & 77.1\% & \textbf{83.5\%}  & 0.464  \\
    Ours     &   & \textbf{82.1{\%}} & 81.6{\%} & \textbf{0.398}      \\
    \midrule
    GVB  &\multirow{3}{*}{\textit{\ourdata{}}} & 75.7\% & 68.3\% & 0.724  \\
    RigNet & & 72.4\% & 71.1\%& 0.698    \\
    Ours   &   & \textbf{80.7{\%}} & \textbf{77.2{\%}} & \textbf{0.337}      \\
    \bottomrule
  \end{tabular}
  \vspace{-12pt}
\end{table}

\vspace{-5pt}
\subsection{Ablation studies}

\subsubsection{Ablation studies on skeleton generation}
We conduct ablation studies to assess the impact of VLM-based data filtering and the number of sampled mesh points on skeleton generation. 
The results, presented in \Cref{ablation_skel}, show notable performance degradation without data filtering, highlighting the importance of high-quality training data. We also vary the number of sampled points as input to the pre-trained shape encoder \cite{zhao2024michelangelo}. As shown in \Cref{ablation_skel}, sampling 8,192 points yields superior performance.

\begin{table}
  \caption{\textbf{Ablation studies for skeleton generation.}}
  % \vspace{-6pt}
  \label{ablation_skel}
  \centering
  \begin{tabular}{cccc}
%   \hline
    \toprule
      &  CD-J2J  & CD-J2B & CD-B2B \\
    \midrule
    w/o data filtering   & 2.982 & 2.327  & 2.015 \\
    \midrule
    4,096 points & 2.635 & 2.024  & 1.727 \\
    12,288 points & 2.685 & 2.048  & 1.760 \\
    Ours (8,192)      & \textbf{2.586} & \textbf{1.959} & \textbf{1.661}   
    \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Ablation studies on skinning weight prediction}
\label{ablate_skin} 
We conduct ablation studies on three critical components of our skinning weight prediction framework. The quantitative results on \res{} are shown in \Cref{ab_skin}. First, removing the volumetric geodesic distance initialization reduces precision by 0.6\% and recall by 3.9\%, demonstrating its crucial role in guiding accurate weight distribution. Second, eliminating our normalization strategy, which scales both skinning weights and geodesic distances to \([-1, 1]\) before noise addition, leads to an 8.7\% increase in L1 error. Finally, excluding global shape features from the pre-trained encoder \cite{zhao2024michelangelo} results in less accurate predictions. All these results validate our design choices and show that each component contributes notably to the final performance.

\begin{table}[]
% \vspace{-10pt}
  \caption{\textbf{Ablation studies on skinning weight prediction.}}
  % \vspace{-8pt}
  \label{ab_skin}
  \centering
  \begin{tabular}{cccc}
%   \hline
    \toprule
      &  Precision  &Recall & avg L1  \\
    \midrule
    w/o geodesic dist.  & 81.5\% & 77.7\% & 0.444 \\
    w/o weights norm   & 82.0\% & 77.9\%  & 0.436 \\
     w/o shape features   & 81.4\% &
81.3\% &
0.412 \\
    Ours      & \textbf{82.1{\%}} & \textbf{81.6{\%}} & \textbf{0.398}  
    \\
    \bottomrule
  \end{tabular}
  \vspace{-15pt}
\end{table}
