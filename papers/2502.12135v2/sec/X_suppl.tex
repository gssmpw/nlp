\clearpage
% \setcounter{page}{1}
\maketitlesupplementary


\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}


\section*{Overview}
In this supplementary material, we provide additional details and experimental results for the main paper, including:

\begin{itemize}
    \item Further details of \ours{} (\Cref{method_detail}) and \ourdata{} (\Cref{detail_data});
    \item Additional experimental results on skeleton generation and skinning weight prediction (\Cref{additioanl_results});
    \item A discussion of the limitations of our work and future works (\Cref{limit}).
\end{itemize}


\section{More details of \ours{}}
\label{method_detail}

\subsection{Implementation details}
\label{implement_detail}
\boldstartspace{Skeleton generation.}
Our skeleton generation pipeline utilizes a pre-trained shape encoder \cite{zhao2024michelangelo} to process input meshes. For each mesh, we sample 8,192 points which are encoded into 257 shape tokens following MeshAnything \cite{chen2024meshanything}. To ensure consistent point cloud sampling across different data sources, we first extract the signed distance function from input mesh using \cite{wang2022dual}, followed by generating a coarse mesh via Marching Cubes \cite{lorensen1998marching}. We then sample point clouds and their corresponding normals from this coarse mesh.

For training on \ourdata{}, we use 8 NVIDIA A100 GPUs for approximately two days with a batch size of 64 per GPU, resulting in an effective batch size of 512. 
When training on \res{}, we utilize 4 NVIDIA A100 GPUs for about 9 hours with a batch size of 32 per GPU, which yields an effective batch size of 128.
During inference, the model generates skeleton tokens auto-regressively from shape tokens until reaching the $\textless \mathrm{eos} \textgreater$ token, followed by detokenization to recover the final skeleton coordinates in $[-0.5, 0.5]$ range.

\boldstartspace{Skinning weight prediction.} 
Our functional diffusion model employs the Denoising Diffusion Probabilistic Model (DDPM) with 1,000 timesteps and a linear beta schedule. 
During training, we condition the model on ground truth skeletons and supervise it with corresponding ground truth skinning weights. We add noise to the skinning weight function (the process is illustrated in \Cref{supp_func_process}) and then feed the noised skinning weights into our denoising network (\Cref{supp_func_network}). Following \cite{zhang2024functional}, our network architecture processes the noised set $\{(x, f_{t}(x)) \mid x \in \mathcal{P}\}$ by splitting it into smaller subsets and handling them through multiple cross-attention stages.
The time embedding at timestep $t$ is incorporated into each self-attention layer via adaptive layer normalization. For visual clarity, \Cref{supp_func_network} shows only one processing stage.

We train the model on \ourdata{}  using 8 NVIDIA A100 GPUs for approximately one day, with a batch size of 16 per GPU (effective batch size 128). Training on \res{} uses the same configuration for about 4 hours. During inference, we perform 25 denoising steps to generate predictions $\mathcal{W} \in \mathbb{R}^{v \times n}$ in the range $[-1, 1]$. These results are then normalized to $[0, 1]$, ensuring that each row of the skinning weight matrix sums to 1. To handle varying joint counts across different models, we employ a valid joint mask during both training and testing, with a maximum joint count of 55 as discussed in the main paper (Sections 4.2 and 5.3).



\begin{figure*}
    \centering
    \includegraphics[scale=0.5]
{fig/supp_fig1_func_diff.pdf}
\caption{\textbf{Overview of the function diffusion architecture for skinning weight prediction.} Given a set of noised skinning weight functions $\{(x, f_{t}(x)) \mid x \in \mathcal{P}\}$, conditioned on skeleton and shape features from \cite{zhao2024michelangelo}, we denoise the skinning weight functions to approximate the target weights.}
    \label{supp_func_network}
  \end{figure*}
  

\begin{figure}
    \centering
    \includegraphics[scale=0.3]
{fig/supp_fig2_func_diff_process.pdf}
\caption{\textbf{Process of adding noise to the skinning weight function.} Given $x \in \mathcal{P}$ and the original skinning weight function $f_{0}(x)$, we add the noise function $g(x)$ to obtain the noised function $f_{t}(x)$.}

    \label{supp_func_process}
  \end{figure}

  
\subsection{Experimental details}

For baseline comparisons, we use the implementations of RigNet \cite{xu2020rignet} and Pinocchio \cite{baran2007automatic} from the GitHub repositories\footnote{\url{https://github.com/zhan-xu/RigNet}, \url{https://github.com/haoz19/Automatic-Rigging}}. 
The Geodesic Voxel Binding (GVB) \cite{dionne2013geodesic} comparison is conducted using the implementation in Autodesk Maya \cite{AutodeskMaya2024}. When training RigNet on our \ourdata{}, we strictly follow the authors' data processing pipeline and six-stage training strategy as specified in their official implementation.


\subsection{Animation}
Many recent works have explored 3D animation, including skeleton-free pose transfer \cite{song20213d, song2023unsupervised, liao2022skeleton}, skeleton-driven pose transfer \cite{zhang2024magicpose4d}, and physics-driven animation \cite{fu2024sync4d}. In this paper, we propose a method that enables automatic articulation generation for any input 3D model, whether artist-created or AI-generated. The pipeline first generates a skeleton for the input model, then predicts skinning weights conditioned on both the model geometry and the generated skeleton. The resulting articulated model can be exported in standard formats (e.g., FBX, GLB), making it directly compatible with popular animation software such as Blender \cite{Blender} and Autodesk Maya \cite{AutodeskMaya2024}.

\section{Additional experimental results}
\label{additioanl_results}
\subsection{More results of skeleton generation}


\begin{figure*}
    \centering
    \includegraphics[scale=0.46]
{fig/supp_fig3_ood.pdf}
\caption{\textbf{Comparison of skeleton generation methods on out-of-domain data.} The input meshes are from 3D generation, 3D scan, and 3D reconstruction.}
    \label{supp_skel_ood}
  \end{figure*}
  
\begin{figure*}
    \centering
\includegraphics[width=\textwidth]
    {fig/supp_fig4_skeleton.pdf}

    \caption{\textbf{Comparison of skeleton generation methods on \res{} (left) and \ourdata{} (right).} Our results more closely resemble the artist-created references, while RigNet and Pinocchio struggle to handle various object categories.}
    \label{supp_skel}
  \end{figure*}

\begin{table*}
  \caption{\textbf{Quantitative comparison on skinning weight prediction.} We compare our method with GVB and RigNet. For Precision and Recall, larger values indicate better performance. For average L1-norm error and average distance error, smaller values are preferred. }
  \label{comparison_skin_supp}
  \centering
  \begin{tabular}{cccccc}
%   \hline
    \toprule
      & Dataset &  Precision  & Recall & avg L1 & avg Dist. \\
    \midrule
    GVB &  \multirow{3}{*}{\textit{ModelsResource}}  & 69.3\%  & 79.2\%  & 0.687  & 0.0067  \\
    RigNet  &  & 77.1\% & \textbf{83.5\%}  & 0.464 & 0.0054 \\
    Ours     &   & \textbf{82.1{\%}} & 81.6{\%} & \textbf{0.398}  & \textbf{0.0039}    \\
    \midrule
    GVB  &\multirow{3}{*}{\textit{\ourdata{}}} & 75.7\% & 68.3\% & 0.724 & 0.0095 \\
    RigNet & & 72.4\% & 71.1\%& 0.698  & 0.0091  \\
    Ours   &   & \textbf{80.7{\%}} & \textbf{77.2{\%}} & \textbf{0.337}  & \textbf{0.0050}    \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[]
% \vspace{-10pt}
  \caption{\textbf{Ablation studies on \res{} for skinning weight prediction.}}
  \label{ab_skin_supp}
  \centering
  \begin{tabular}{ccccc}
%   \hline
    \toprule
      &  Precision  &Recall & avg L1 & avg Dist. \\
    \midrule
    w/o geodesic dist.  & 81.5\% & 77.7\% & 0.444 & 0.0046 \\
    w/o weights norm   & 82.0\% & 77.9\%  & 0.436 & 0.0045 \\
     w/o shape features   & 81.4\% &
81.3\% &
0.412 & 0.0042 \\
    Ours      & \textbf{82.1{\%}} & \textbf{81.6{\%}} & \textbf{0.398}  & \textbf{0.0039}
    \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}[h]
    \centering
    \caption{\textbf{Object counts for each category in the \ourdata{} dataset.}}
    \label{num_obj}
    \begin{tabular}{cccccc}
    \toprule
    Category        & \# Objects & Category        & \# Objects & Category        & \# Objects   \\
    \midrule
    character       & 16020      & miscellaneous & 584  & architecture & 132 \\
    anthropomorphic & 13393      &  scanned data &  546 & planet & 49 \\
    animal          & 4760       & plant  & 382 & paper & 46
    \\
    mythical creature & 4734      &  accessories & 293  &  musical instrument & 25
    \\
    toy         & 1360      & vehicle & 283 &  sporting goods & 21
    \\
    weapon          & 1257       & sculpture & 276 & armor & 13
    \\
    anatomy          & 1227      & household items & 274 & robot & 4
    \\
    clothing          & 595       & food &  206 \\
    \bottomrule
    \end{tabular}
\end{table*}

We provide additional qualitative comparisons among \ours{}, RigNet \cite{xu2020rignet}, and Pinocchio \cite{baran2007automatic} for skeleton generation. 


\boldstartspace{More qualitative results on out-of-domain data.} 
We evaluate our method's generalization capability on diverse out-of-domain data sources: AI-generated meshes from Tripo2.0 \cite{tripo3d}, unregistered 3D scans from FAUST \cite{bogo2014faust}, and video-based 3D reconstructions \cite{song2024moda}.
As shown in \Cref{supp_skel_ood}, while existing methods struggle with generalization (RigNet fails across all cases, and Pinocchio shows misalignments even for human bodies, see skeleton results on the 3D scan), our method maintains robust performance across different data sources and categories. Notably, for human models, our method generates more detailed skeletal structures, including accurate hand skeletons, surpassing Pinocchio's template-based results.

\boldstartspace{More qualitative results on \ourdata{} and \res{}.}
We provide additional qualitative results on both \ourdata{} and \res{}  datasets. As illustrated in \Cref{supp_skel}, our method consistently generates high-quality skeletons that accurately match artist-created references across diverse object categories.

\boldstartspace{Robustness to various mesh orientations.}  To further validate our modelâ€™s robustness to various orientations, we include mesh rotations at multiple angles in \Cref{supp_rotation}. These examples show that our approach remains largely rotation-stable. While minor skeleton variations may occur, all generated results maintain anatomically valid and suitable for rigging purposes.

\begin{figure}
    \centering
\includegraphics[scale=0.35]{fig/supp_fig11_rotation.pdf}

    \caption{\textbf{Skeleton results on 3D models with different orientations.} Although minor differences may appear in the generated skeletons, all results maintain anatomically valid and suitable for rigging purposes.}
    \label{supp_rotation}
  \end{figure}
  
\subsection{More results of skinning weight prediction}
\boldstartspace{Quantitative results with deformation error.}
Beyond the precision, recall, and L1-norm metrics reported in the main paper, we evaluate the practical effectiveness of predicted skinning weights through deformation error analysis. This metric computes the average Euclidean distance between vertices deformed using predicted weights and ground truth weights across 10 random poses. The comprehensive results, shown in \Cref{comparison_skin_supp}, demonstrate our method's superior performance across most metrics on both datasets. We also include deformation error analysis in our ablation studies (\Cref{ab_skin_supp}), further validating the effectiveness of our design choices.


\boldstartspace{More qualitative results.}
We present additional qualitative comparisons between \ours{}, RigNet \cite{xu2020rignet}, and Geodesic Voxel Binding (GVB) \cite{dionne2013geodesic} for skinning weight prediction. \Cref{supp_skin} shows both the predicted skinning weights and their L1 error maps compared to artist-created references, demonstrating our method's superior accuracy across diverse object categories.

\begin{figure*}
    \centering
\includegraphics[width=\textwidth]
    % \includegraphics[scale=0.44]
    {fig/supp_fig5_compare_skinning.pdf}

    \caption{\textbf{Comparison of skinning weight prediction methods on \res{} (first three rows) and \ourdata{} (last three rows).} We visualize the predicted skinning weights alongside their corresponding L1 error maps.}
    \label{supp_skin}
  \end{figure*}

\section{More details of \ourdata{}}
\label{detail_data}

\subsection{Data Curation}

Our dataset curation process filters out duplicates, objects with extreme joint/bone counts, and multi-component objects. A detailed category-wise object distribution is provided in \Cref{num_obj}.

\subsection{Quality assessment}
We employ GPT-4o \cite{openai_gpt4o} for quality assessment of skeleton annotations. 
For each model, we generate four-view renders using Pyrender\footnote{\url{https://github.com/mmatl/pyrender}} showing both the 3D model and its skeleton (\Cref{rating_redner}). These renders are evaluated using specific quality criteria detailed in \Cref{rating_instruct}.



\begin{figure*}
    \centering
    \includegraphics[scale=0.44]
    {fig/supp_fig6_rating_instruction.pdf}

    \caption{\textbf{Input instructions to VLM for data filtering.} }
    \label{rating_instruct}
  \end{figure*}


\subsection{Category annotation}

For the Visual-Language Model (VLM)-based category labeling, we render each 3D model along with its normal maps from four viewpoints using Blender \cite{Blender} (see example in \Cref{cate_render}). We then utilize GPT-4o \cite{openai_gpt4o} to classify the categories of the 3D models based on specific instructions, as outlined in \Cref{cate_label_instruct}.

\begin{figure*}
    \centering

\includegraphics[scale=0.48]
{fig/supp_fig7_category_instruction.pdf}
    \caption{\textbf{Input instructions to VLM for category labeling.} }
    \label{cate_label_instruct}
  \end{figure*}

\begin{figure}
% \vspace{-30pt}
    \centering
    \includegraphics[scale=0.4]
{fig/supp_fig8_rating_render_exm.pdf}
    \caption{\textbf{Input rendered examples to VLM for data filtering.} }
    \label{rating_redner}
  \end{figure}

  
\begin{figure}
    \centering
   
    \includegraphics[scale=0.4]
{fig/supp_fig9_category_render_exm.pdf}
    \caption{\textbf{Input rendered examples to VLM for category labeling.} }
    \label{cate_render}
  \end{figure}
  

\section{Limitations and future work}
\label{limit}
Despite its strong performance, our method has several notable limitations. First, our approach struggles with coarse mesh inputs, often producing inaccurate skeletons as shown in \Cref{failure}. While we employ preprocessing techniques to handle inputs from different sources, the significant domain gap between training data and coarse meshes remains challenging. Potential solutions include incorporating mesh quality augmentation during training to enhance robustness.

A second limitation lies in our dataset composition. Although \ourdata{} is large in scale, it lacks sufficient coverage of common articulated objects like laptops, staplers, and scissors, which affects our model's generalization to these categories.

Future work will address these limitations by:
1) Developing more robust preprocessing and training strategies for handling varying mesh qualities;
2) Expanding dataset coverage to include a broader range of everyday articulated objects;
3) Exploring techniques to better bridge the domain gap between different data sources.


\begin{figure}
    \centering
    % \includegraphics[width=\textwidth]
    \includegraphics[scale=0.14]
{fig/supp_fig10_limitation.pdf}
    \caption{\textbf{Failure cases.} When input meshes possess very coarse surfaces (3D reconstruction results from \cite{song2023total}), our generated skeleton may exhibit inaccuracies, such as imperfect connections between the dog's trunk and legs.}
    \label{failure}
  \end{figure}
