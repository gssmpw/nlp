\section{Experiments}
\begin{figure}[t]
    \centering
	\begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/cifar10_uncond_nfe1_cm-crop.pdf}
		\small{(a) CM}
	\end{minipage}
	\begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/cifar10_uncond_nfe1_ctm-crop.pdf}
		\small{(b) CTM}
	\end{minipage}
        \begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/cifar10_coeff_2d-crop.pdf}
		\small{(c) Coefficients $f(t,\epsilon),g(t,\epsilon)$}
	\end{minipage}
	\caption{\label{fig:nfe-1}Training curves for single-step generation, and visualization of preconditionings for single-step jump on CIFAR-10 (conditional).}
	\vspace{-.1in}
\end{figure}
In this section, we demonstrate the impact of Analytic-Precond when applied to consistency distillation. Our experiments encompass various image datasets, including CIFAR-10~\citep{Krizhevsky09learningmultiple}, FFHQ~\citep{karras2019style} 64$\times$64, and ImageNet~\citep{deng2009imagenet} 64$\times$64, under both unconditional and class-conditional settings. We deploy Analytic-Precond across two paradigms: consistency models (CMs)~\citep{song2023consistency} and consistency trajectory models (CTMs)~\citep{kim2023consistency}, wherein we solely substitute the preconditioning while retaining other training procedures. For further experiment details, please refer to Appendix~\ref{appendix:exp-details}. 

Our investigation aims to address two primary questions:
\begin{itemize}
    \item Can Analytic-Precond yield improvements over the original preconditioning of CMs and CTMs, across both single-step and multi-step generation?
    \item How does Analytic-Precond differ from prior preconditioning across datasets, concerning the coefficients $f(t,s)$ and $g(t,s)$?
\end{itemize}
\subsection{Training Acceleration}
\paragraph{Effects on CMs and Single-Step CTMs} We first apply Analytic-Precond to CMs, where the consistency function $\fv_\theta(\x_t,t)$ is defined to map $\x_t$ on the teacher ODE trajectory to the starting point $\x_\epsilon$ at fixed time $\epsilon$. The models are trained with the consistency loss defined in Eqn.~\eqref{eq:loss-cm} and on the CIFAR-10 dataset, with class labels as conditions. As depicted in Figure~\ref{fig:nfe-1} (a), we observe that Analytic-Precond yields training curves similar to original CM, measured by FID. Since multi-step consistency sampling in CMs only involves evaluating $\fv_\theta(\x_t,t)$ multiple times, the results remain comparable even with an increase in sampling steps. Similar phenomena emerge in CTMs with single-step generation, as illustrated in Figure~\ref{fig:nfe-1} (b). The commonality between these two scenarios lies in the utilization of only the jumping destination at $\epsilon$. To investigate further, we plot the preconditioning coefficients $f(t,\epsilon)$ and $g(t,\epsilon)$ in CMs, CTMs and Analytic-Precond as a function of $\log t$, as illustrated in Figure~\ref{fig:nfe-1} (c). It is evident that across varying $t$, different preconditioning coefficients $f$ and $g$ exhibit negligible discrepancies when $s$ is fixed to $\epsilon$. This elucidates the rationale behind the comparable performance, suggesting that the original preconditionings for $t\rightarrow\epsilon$ are already quite optimal with minimal room for further optimization.
\begin{figure}[t]
    \centering
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/cifar10_uncond2-crop.pdf}
		\small{(a) CIFAR-10 (Unconditional)}
	\end{minipage}
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/cifar10_cond2-crop.pdf}
		\small{(b) CIFAR-10 (Conditional)}
	\end{minipage}
    \begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/ffhq_uncond2-crop.pdf}
		\small{(c) FFHQ 64$\times$64 (Unconditional)}
	\end{minipage}
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/imagenet_cond2-crop.pdf}
		\small{(d) ImageNet 64$\times$64 (Conditional)}
	\end{minipage}
	\caption{\label{fig:nfe-2} Training curves for two-step generation.}
	\vspace{-.1in}
\end{figure}

\textbf{Effects on Two-Step CTMs}$\mbox{ }$ We further track sample quality during the training process on CTMs, particularly focusing on two-step generation where an intermediate jump is involved ($T\rightarrow t_0\rightarrow\epsilon$). The models are training with both the consistency trajectory loss in Eqn.~\eqref{eq:loss-ctm} and the denoising score matching (DSM) loss $\E_t\E_{p_{\data}(\x_0)q(\x_t|\x_0)}[w(t)\|\Dv_\theta(\x_t,t,t)-\x_0\|_2^2]$, following CTMs\footnote{CTMs also propose to combine the GAN loss for further enhancing quality, which we will discuss later.}. As 
shown in Figure~\ref{fig:nfe-2}, across diverse datasets, Analytic-Precond enjoys superior initialization and up to 3$\times$ training acceleration compared to CTM's preconditioning. This observation indicates the suboptimality of the original intermediate trajectory jumps $t\rightarrow s>\epsilon$. We provided the generated samples in Appendix~\ref{appendix:additional-samples}.
\begin{figure}[t]
    \centering
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/vis_ctm-crop.pdf}
		\small{(a) CTM}
	\end{minipage}
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/vis_cifar10-crop.pdf}
		\small{(b) CIFAR-10}
	\end{minipage}
        \begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/vis_ffhq-crop.pdf}
		\small{(c) FFHQ 64$\times$64}
	\end{minipage}
	\begin{minipage}[t]{.24\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/vis_imagenet-crop.pdf}
		\small{(d) ImageNet 64$\times$64}
	\end{minipage}
	\caption{\label{fig:visualization}Visualizations of the preconditioning coefficient $g(t,s)$ for CTM, and for Analytic-Precond under different datasets.}
	\vspace{-.1in}
\end{figure}
\subsection{Generation with More Steps}
\begin{table}[t]
      \centering
    \caption{\label{tab:more-sampling-steps} FID results in multi-step generation with different number of function evaluations (NFEs).}
    \vskip 0.1in
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{c|ccccc|ccccc}
    \toprule
    & \multicolumn{10}{c}{NFE} \\
    \cmidrule{2-11}
    FID&2&3&5&8&10&2&3&5&8&10\\
    \midrule
    &\multicolumn{5}{c|}{CIFAR-10 (Unconditional)}&\multicolumn{5}{c}{CIFAR-10 (Conditional)}\\
    \arrayrulecolor{lightgray}\midrule
    \arrayrulecolor{black}
        CTM&3.83&3.58&3.43&3.33&3.22&3.00&2.82&2.59&2.67&2.56\\
        CTM + Ours&3.77&3.54&3.38&3.30&3.25&2.92&2.75&2.62&2.60&2.65\\
    \midrule
    &\multicolumn{5}{c|}{FFHQ $64\times64$ (Unconditional)}&\multicolumn{5}{c}{ImageNet $256\times256$ (Conditional)}\\
    \arrayrulecolor{lightgray}\midrule
    \arrayrulecolor{black}
        CTM&5.96&5.80&5.53&5.39&5.23&5.95&6.16&5.43&5.44&5.98\\
        CTM + Ours&5.71&5.56&5.47&5.31&5.12&5.73&5.67&5.34&5.43&5.70\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.1in}
\end{table}
Apart from the superiority of CTMs over CMs in single-step generation (Figure~\ref{fig:nfe-1}), another notable advantage of CTMs is the regularization effects of the DSM loss. This ensures that $\Dv_\theta(\x_t,t,t)$ functions as a valid denoiser in diffusion models, facilitating sample quality enhancement with additional sampling steps. To evaluate the effectiveness of Analytic-Precond with more steps, we employ the deterministic procedure in CTMs, which employs the consistency function to jump on consecutively decreasing timesteps from $T$ to $\epsilon$. As shown in Table~\ref{tab:more-sampling-steps}, Analytic-Precond brings consistent improvement over CTMs as the number of steps increases, indicating better alignment between the consistency function and the denoiser function.
\subsection{Analyses and Discussions}
\begin{wrapfigure}[9]{r}{0.3\textwidth}
\vspace{-0.6in}
\includegraphics[width=1.0\linewidth]{figures/cifar10_uncond_bcm-crop.pdf}
\caption{\label{fig:bcm}\small{Effects of BCM's preconditioning on CTMs.}}
\end{wrapfigure}

\textbf{Visualizations}$\mbox{ }$ To intuitively understand the distinctions between Analytic-Precond and the original preconditioning in CTMs, we investigate the variations in coefficients $f(t,s),g(t,s)$. We find that Analytic-Precond yields $f(t,s)$ close to that of CTMs, denoted as $f^{\text{CTM}}(t,s)$, with $|f^{\text{CTM}}(t,s)-f(t,s)|<0.03$ across various $t$ and $s$. However, $g(t,s)$ produced by Analytic-Precond tends to be smaller, with disparities of up to 0.25 compared to $g^{\text{CTM}}(t,s)$. This distinction is visually demonstrated in Figure~\ref{fig:visualization}, where we depict $g(t,s)$ as a binary function of $\log t$ and $\log s$. Notably, the distinction is more pronounced for short jumps $t\rightarrow s$ where $|t-s|/t$ is small.

\textbf{Comparison to BCMs}$\mbox{ }$  In a concurrent work called bidirectional consistency models (BCMs)~\citep{li2024bidirectional}, a novel preconditioning is derived from EDM's first principle (specified in Table~\ref{tab:comparison}). BCM's preconditioning also accommodates flexible transitions from $t$ to $s$ along the trajectory. However, as shown in Figure~\ref{fig:bcm}, replacing CTM's preconditioning with BCM's fails to bring improvements in both one-step and two-step generation.

\begin{wrapfigure}[10]{r}{0.3\textwidth}
\vspace{-0.1in}
\includegraphics[width=1.0\linewidth]{figures/cifar10_uncond_gan-crop.pdf}
\caption{\label{fig:gan}\small{Effects of Analytic-Precond with GAN loss.}}
\end{wrapfigure}
\textbf{Compatibility with GAN loss}$\mbox{ }$ CTMs introduce GAN loss to further enhance the one-step generation quality, employing a discriminator and adopting an alternative optimization approach akin to GANs. As shown in Figure~\ref{fig:gan}, when GAN loss is incorporated on CIFAR-10, Analytic-Precond demonstrates comparable performance. However, in this scenario, the consistency function no longer faithfully adheres to the teacher ODE trajectory, and one-step generation is even better than two-step, deviating from our theoretical foundations. Nevertheless, the utilization of Analytic-Precond does not lead to performance degradation.

\begin{figure}[ht]
    \centering
	\begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/toy/trajectories_with_densities.png}
		\small{(a) Teacher}
	\end{minipage}
	\begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/toy/trajectories_with_densities_edm.png}
		\small{(b) CTM's}
	\end{minipage}
        \begin{minipage}[t]{.32\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/toy/trajectories_with_densities_ours.png}
		\small{(c) Ours}
	\end{minipage}
	\caption{\label{fig:trajectory}Visualizations of the trajectory alignment, comparing \textcolor{red}{teacher} and 3-step \textcolor{blue}{student}.}
	\vspace{-.1in}
\end{figure}
\paragraph{Enhancement of the Trajectory Alignment} We observe that our method also leads to lower mean square error (MSE) in the multi-step generation of CTM, when compared to the teacher diffusion model under the same initial noise, indicating enhanced fidelity to the teacher's trajectory. To better illustrate the effect of Analytic-Precond in improving trajectory alignment, we adopt a toy example where the data distribution is a simple 1-D Gaussian mixture $\frac{1}{3}\Nc(-2,1)+\frac{2}{3}\Nc(1,0.25)$. In this case, we can analytically derive the optimal denoiser and visualize the ground-truth teacher trajectory. We initialize the consistency function with the optimal denoiser and apply different preconditionings. As shown in Figure~\ref{fig:trajectory}, our preconditioning produces few-step trajectories that better align with the teacher's and yields a more accurate final distribution.