\section{Proofs}
\label{appendix:proof}
\subsection{Proof of Proposition~\ref{proposition:gap}}
\label{appendix:proof1}
\begin{proof}
Denote $\{\x_\tau\}_{\tau=s}^t$ as data points on the same teacher ODE trajectory. The generalized ODE in Eqn.~\eqref{eq:ODE-with-l-s} can be reformulated as an integral:
\begin{equation}
    L_s\x_s-L_t\x_t=\int_{\eta_t}^{\eta_s}\hv_\phi(\x_{t_{\lambda_\eta}},t_{\lambda_\eta})\dm\eta
\end{equation}
where $\hv_\phi(\x_t,t)\coloneqq\frac{\gv_\phi(\x_t,t)}{S_t}$, and $\gv_\phi$ is defined by the teacher denoiser $\Dv_\phi$ in Eqn.~\eqref{eq:ODE-with-l}. On the other hand, by replacing the teacher denoiser $\Dv_\phi$ with the student denoiser $\Dv_\theta$ in the Euler discretization (Eqn.~\eqref{eq:generalize-discretization-not-rearranged}), the optimal student $\theta^*$ should satisfy
\begin{equation}
    L_s\x_s-L_t\x_t=(\eta_s-\eta_t)\hv_{\theta^*}(\x_t,t,s)
\end{equation}
where $\hv_\theta,\gv_\theta$ are defined similarly to $\hv_\phi,\gv_\phi$ as
\begin{equation}
    \hv_{\theta}(\x_t,t,s)=\frac{\gv_{\theta}(\x_t,t,s)}{S_t},\quad \gv_{\theta}(\x_t,t,s)=\Dv_{\theta}(\x_{t},t,s)-(1-l_{t})\x_{t}
\end{equation}
Combining the above equations, we have
\begin{equation}
\begin{aligned}
    \Dv_{\theta^*}(\x_{t},t,s)-\Dv_\phi(\x_t,t)&=S_t(\hv_{\theta^*}(\x_t,t,s)-\hv_\phi(\x_t,t))\\
    &=S_t\left(\frac{L_s\x_s-L_t\x_t}{\eta_s-\eta_t}-\hv_\phi(\x_t,t)\right)\\
    &=\frac{S_t}{\eta_s-\eta_t}\int_{\eta_t}^{\eta_s}\hv_\phi(\x_{t_{\lambda_\eta}},t_{\lambda_\eta})-\hv_\phi(\x_t,t)\dm\eta
\end{aligned}
\end{equation}
According to the mean value theorem, there exists some $\tau\in [t,t_{\lambda_\eta}]$ satisfying
\begin{equation}
\|\hv_\phi(\x_{t_{\lambda_\eta}},t_{\lambda_\eta})-\hv_\phi(\x_t,t)\|_2\leq (\eta-\eta_t)\left\|\frac{\dm \hv_\phi(\x_\tau,\tau)}{\dm\eta_\tau}\right\|_2
\end{equation}
Besides, the derivative $\frac{\dm\hv_\phi}{\dm\eta}$ can be calculated as
\begin{equation}
\begin{aligned}
    \frac{\dm \hv_\phi(\x_\tau,\tau)}{\dm\eta_\tau}&=\frac{\dm \hv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}\frac{1}{\dm\eta_\tau/\dm\lambda_\tau}\\
    &=\left(\frac{1}{S_\tau}\frac{\dm\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-\frac{\dm\log S_\tau}{\dm\lambda_\tau}\frac{\gv_\phi(\x_\tau,\tau)}{S_\tau}\right)\frac{1}{L_\tau S_\tau}\\
    &=\frac{\frac{\dm\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-s_\tau\gv_\phi(\x_\tau,\tau)}{L_\tau S_\tau^2}
\end{aligned}
\end{equation}
where we have used $\frac{\dm \log S_{\tau}}{\dm\lambda_\tau}=s_\tau$ and $\frac{\dm\eta_\tau}{\dm\lambda_\tau}=L_\tau S_\tau$. Therefore,
\begin{equation}
\label{eq:proof1-1}
\begin{aligned}
\|\Dv_{\theta^*}(\x_{t},t,s)-\Dv_\phi(\x_t,t)\|_2&\leq \frac{S_t}{\eta_s-\eta_t}\max_{s\leq \tau\leq t} \left\|\frac{\dm\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-s_\tau\gv_\phi(\x_\tau,\tau)\right\|\int _{\eta_t}^{\eta_s}\frac{\eta-\eta_t}{L_\tau S_\tau^2}\dm\eta\\
&\leq \max_{s\leq \tau\leq t} \left\|\frac{\dm\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-s_\tau\gv_\phi(\x_\tau,\tau)\right\|\int_{\lambda_t}^{\lambda_s}\frac{L_{t_\lambda}S_{t_\lambda}S_t}{L_\tau S_\tau^2}\dm\lambda
\end{aligned}
\end{equation}
Since we assumed $|l_t|,|s_t|\leq c$, according to $\tau\in[t,t_{\lambda}]$, we have
\begin{equation}
    \frac{L_{t_\lambda}}{L_\tau}=e^{\int_{\lambda_\tau}^{\lambda}l_{t_\lambda'}\dm\lambda'}\leq e^{c(\lambda-\lambda_t)},\quad\frac{S_{t_\lambda}}{S_\tau}\leq e^{c(\lambda-\lambda_t)},\quad\frac{S_t}{S_\tau}\leq e^{c(\lambda-\lambda_t)}
\end{equation}
Therefore,
\begin{equation}
\label{eq:proof1-2}
\int_{\lambda_t}^{\lambda_s}\frac{L_{t_\lambda}S_{t_\lambda}S_t}{L_\tau S_\tau^2}\dm\lambda\leq \int_{\lambda_t}^{\lambda_s}e^{3c(\lambda-\lambda_t)}\dm\lambda=\frac{e^{3c(\lambda_s-\lambda_t)}-1}{3c}=\frac{(t/s)^{3c}-1}{3c}
\end{equation}
Substituting Eqn.~\eqref{eq:proof1-2} in Eqn.~\eqref{eq:proof1-1} completes the proof.
\end{proof}
\section{Experiment Details}
\label{appendix:exp-details}
\begin{table}[t]
\vskip -0.2in
	\caption{Experimental configurations.}
	\label{tab:configurations}
	%\footnotesize
	\centering
 % \resizebox{1.0\linewidth}{!}{
	\begin{tabular}{lcccc}
		\toprule
		Configuration & \multicolumn{2}{c}{CIFAR-10}& FFHQ 64$\times$64 & ImageNet 64$\times$64 \\\cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}
        & Uncond & Cond & Uncond & Cond \\
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}
        Learning rate & 0.0004 & 0.0004 & 0.0004 & 0.0004 \\
        Student's stop-grad EMA parameter & 0.999 & 0.999 & 0.999 & 0.999 \\
        $N$ & 18 & 18 & 18 & 40 \\
        ODE solver & Heun & Heun & Heun & Heun \\
        Max. ODE steps & 17 & 17 & 17 & 20 \\
        EMA decay rate & 0.999 & 0.999 & 0.999 & 0.999 \\
        Training iterations & 200K & 150K & 150K & 60K \\
        Mixed-Precision (FP16) & True & True & True & True \\
        Batch size & 256 & 512 & 256 & 2048 \\
        Number of GPUs & 4 & 8 & 8 & 32 \\
        Training Time (A800 Hours) &490&735&900&6400\\
            \bottomrule
	\end{tabular}
 % }
 \vskip -0.1in
\end{table}
\subsection{Coefficients Computing}
\label{appendix:compute-ems}
At every time $t$, the parameters $l_t$ and $s_t$ can be directly computed according to Eqn.~\eqref{eq:l-solution} and Eqn.~\eqref{eq:s-solution}, relying solely on the teacher denoiser model $\Dv_\phi$. The computation of $l_t$ involves evaluating $\tr(\nabla_{\x_t}\Dv_\phi(\x_t,t))$, which is the trace of a Jacobian matrix. Utilizing Hutchinsonâ€™s trace estimator, it can be unbiasedly estimated as $\frac{1}{N}\sum_{n=1}^N \vv^\top\nabla_{\x_t}\Dv_\phi(\x_t,t)\vv$, where $\vv$ obeys a $d$-dimensional distribution with zero mean and unit covariance. Thus, only the Jacobian-vector product (JVP) $\Dv_\phi(\x_t,t)\vv$ is required, achievable in $\Oc(d)$ computational cost via automatic differentiation. Once $l_t$ is obtained, the function $\gv_\phi(\x_t,t)=\Dv_\phi(\x_t,t)-(1-l_t)\x_t$ is determined. The computation of $s_t$ involves evaluating $\frac{\dm\gv_\phi(\x_t,t)}{\dm\lambda_t}$, which expands as follows:
\begin{equation}
\begin{aligned}
    \frac{\dm\gv_\phi(\x_t,t)}{\dm\lambda_t}&=\frac{\dm\Dv_\phi(\x_t,t)}{\dm\lambda_t}+\frac{\dm l_t}{\dm\lambda_t}\x_t-(1-l_t)\frac{\dm\x_t}{\dm\lambda_t}\\
    &=\frac{\dm\Dv_\phi(\x_t,t)}{\dm\lambda_t}+\frac{\dm l_t}{\dm\lambda_t}\x_t-(1-l_t)(\Dv_\phi(\x_t,t)-\x_t)\\
\end{aligned}
\end{equation}
where $\frac{\dm\Dv_\phi(\x_t,t)}{\dm\lambda_t}$ can also be calculated in $\Oc(d)$ time by by automatic differentiation.

For the CIFAR-10 and FFHQ 64$\times$64 datasets, we compute $l_t$ and $s_t$ across 120 discrete timesteps uniformly distributed in log space, with 4096 samples used to estimate the expectation $\E_{q(\x_t)}$. For ImageNet 64$\times$64, computations are performed across 160 discretized timesteps following EDM's scheduling $(t_{\max}^{1/\rho}+\frac{i}{N}(t_{\min}^{1/\rho}-t_{\max}^{1/\rho}))^\rho$, using 1024 samples to estimate the expectation $\E_{q(\x_t)}$. The total computation times for CIFAR-10, FFHQ 64$\times$64, and ImageNet 64$\times$64 on 8 NVIDIA A800 GPU cards are approximately 38 minutes, 54 minutes, and 38 minutes, respectively.

% CIFAR-10 8.3s/10.45s, 512

% FFHQ 64$\times$64 13.01s/13.84s, 512

% ImageNet 64$\times$64 6.75s/7.35s, 128
\subsection{Training Details}
Throughout the experiments, we follow the training procedures of CTMs. The teacher models are the pretrained diffusion models on the corresponding dataset, provided by EDM. The network architecture of the student models mirrors that of their respective teachers, with the addition of a time-conditioning variable $s$ as input. Training of the student models involves minimizing the consistency loss outlined in Eqn.~\ref{eq:loss-ctm} and the denoising score matching loss $\E_t\E_{p_{\data}(\x_0)q(\x_t|\x_0)}[w(t)\|\Dv_\theta(\x_t,t,t)-\x_0\|_2^2]$. For the consistency loss, we use LPIPS~\citep{zhang2018unreasonable} as the distance metric $d(\cdot,\cdot)$, which is also the choice of CMs. $t$ and $s$ in the consistency loss are chosen from $N$ discretized timesteps determined by EDM's scheduling $(t_{\max}^{1/\rho}+\frac{i}{N}(t_{\min}^{1/\rho}-t_{\max}^{1/\rho}))^\rho$. The Heun sampler in EDM is employed as the solver in Eqn.~\ref{eq:loss-ctm}. The number of sampling steps, determined by the gap between $t$ and $s$, is restricted to avoid excessive training time. For CIFAR-10 and FFHQ 64$\times$64, we select $N=18$ and the maximum number of sampling steps as 17, i.e., not restricting the range of jumping from $t$ to $s$. For ImageNet 64$\times$64, we set $N=40$ and the maximum number of sampling steps to 20, so that the jumping range is at most half of the trajectory length. $\sg(\theta)$ in Eqn.~\ref{eq:loss-ctm} is an exponential moving average stop-gradient version of $\theta$, updated by
\begin{equation}
    \sg(\theta)=\texttt{stop-gradient}(\mu\sg(\theta)+(1-\mu)\theta)
\end{equation}
We follow the hyperparameters used in EDM, setting $\sigma_{\min}=\epsilon=0.002,\sigma_{\max}=T=80.0,\sigma_{\data}=0.5$ and $\rho=7$. The training configurations are summarized in Table~\ref{tab:configurations}.

We run the experiments on a cluster of NVIDIA A800 GPU cards. For CIFAR-10 (unconditional), we train the model with a batch size of 256 for 200K iterations, which takes 5 days on 4 GPU cards. For CIFAR-10 (conditional), we train the model with a batch size of 512 for 150K iterations, which takes 4 days on 8 GPU cards. For FFHQ 64$\times$64 (unconditional), we train the model with a batch size of 256 for 150K iterations, which takes 5 days on 8 GPU cards. For ImageNet 64$\times$64 (conditional), we train the model with a batch size of 2048 for 60K iterations, which takes 8 days on 32 GPU cards.
\subsection{Evaluation Details}
For single-step as well as multi-step sampling of CTMs, we utilize their deterministic sampling procedure by jumping along a set of discrete timesteps $T=t_0\rightarrow t_{1}\rightarrow\dots t_{N-1}\rightarrow t_N=\epsilon$ with the consistency function, formulated as the updating rule $\x_{t_{n}}=\fv_\theta(\x_{t_{n-1}},t_{n-1},t_{n})$. The timesteps $\{t_i\}_{i=0}^N$ are distributed according to EDM's scheduling $(t_{\max}^{1/\rho}+\frac{i}{N}(t_{\min}^{1/\rho}-t_{\max}^{1/\rho}))^\rho$, where $t_{\min}=\epsilon,t_{\max}=T$. We generate 50K random samples with the same seed and report the FID on them.
\subsection{License}
\label{appendix:license}
\begin{table}[ht]
    \centering
    \caption{\label{tab:license}The used datasets, codes and their licenses.}
    \vskip 0.1in
    \resizebox{\textwidth}{!}{% <------ Don't forget this %
    \begin{tabular}{llll}
    \toprule
    Name&URL&Citation&License\\
    \midrule
    CIFAR-10&\url{https://www.cs.toronto.edu/~kriz/cifar.html}&\citep{krizhevsky2009learning}&$\backslash$\\
    FFHQ&\url{https://github.com/NVlabs/ffhq-dataset}&\citep{karras2019style}&CC BY-NC-SA 4.0\\
    ImageNet&\url{https://www.image-net.org}&\citep{deng2009imagenet}&$\backslash$\\
    EDM&\url{https://github.com/NVlabs/edm}&\citep{karras2022elucidating}&CC BY-NC-SA 4.0\\
    CM&\url{https://github.com/openai/consistency_models_cifar10}&\citep{song2023consistency}&Apache-2.0\\
    CTM&\url{https://github.com/sony/ctm}&\citep{kim2023consistency}&MIT\\
    \bottomrule
    \end{tabular}% <------ Don't forget this %
    }
    \vspace{-0.1in}
\end{table}

We list the used datasets, codes and their licenses in Table~\ref{tab:license}.
\section{Additional Samples}
\label{appendix:additional-samples}
\begin{figure}[t]

\begin{minipage}{0.47\textwidth}
    \centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/cifar10_uncond_crop.jpg}}
\small{(a) CTM (CIFAR-10, Uncond)}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/cifar10_uncond_new_crop.jpg}
}
\small{(b) CTM + Ours (CIFAR-10, Uncond)}
\end{minipage}
\medskip

\begin{minipage}{0.47\textwidth}
    \centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/cifar10_cond_crop.jpg}}
\small{(c) CTM (CIFAR-10, Cond)}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/cifar10_cond_new_crop.jpg}
}
\small{(d) CTM + Ours (CIFAR-10, Cond)}
\end{minipage}
\medskip

\begin{minipage}{0.47\textwidth}
    \centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/ffhq_crop.jpg}}
\small{(e) CTM (FFHQ $64\times64$, Uncond)}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/ffhq_new_crop.jpg}
}
\small{(f) CTM + Ours (FFHQ $64\times64$, Uncond)}
\end{minipage}
\medskip

\begin{minipage}{0.47\textwidth}
    \centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/imagenet_crop.jpg}}
\small{(g) CTM (ImageNet $64\times64$, Cond)}
\end{minipage}
\hfill
\begin{minipage}{0.47\textwidth}
\centering
\resizebox{1\textwidth}{!}{
\includegraphics{figures/fig/imagenet_new_crop.jpg}
}
\small{(h) CTM + Ours (ImageNet $64\times64$, Cond)}
\end{minipage}
\medskip

\caption{\label{fig:samples-more}Random samples produced by CTM and CTM + Analytic-Precond (Ours) with NFE=2.}
\end{figure}