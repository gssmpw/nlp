\section{Introduction}
Diffusion models are a class of powerful deep generative models, showcasing cutting-edge performance in diverse domains including image synthesis~\citep{dhariwal2021diffusion,karras2022elucidating}, speech and video generation~\citep{chen2020wavegrad,ho2022imagen}, controllable image manipulation~\citep{nichol2021glide,ramesh2022hierarchical,rombach2022high,meng2021sdedit}, density estimation~\citep{song2021maximum,kingma2021variational,lu2022maximum,zheng2023improved} and inverse problem solving~\citep{chung2022diffusion,kawar2022denoising}. Compared to their 
generative counterparts like variational auto-encoders (VAEs)~\citep{kingma2013auto} and generative adversarial networks (GANs)~\citep{goodfellow2014generative}, diffusion models excel in high-quality generation while circumventing issues of mode collapse and training instability. Consequently, they serve as the cornerstone of next-generation generative systems like text-to-image~\citep{rombach2022high} and text-to-video~\citep{gupta2023photorealistic,bao2024vidu} synthesis.

The primary bottleneck for integrating diffusion models into downstream tasks lies in their slow inference processes, which gradually remove noise from data with hundreds of network evaluations. The sampling process typically involves simulating the probability flow (PF) ordinary ODE backward in time, starting from noise~\citep{song2020score}. To accelerate diffusion sampling, various training-free samplers have been proposed as specialized solvers of the PF-ODE~\citep{song2020denoising,zhang2022fast,lu2022dpm}, yet they still require over 10 steps to generate satisfactory samples due to the inherent discretization errors present in all numerical ODE solvers.

Recent advancements in few-step or even single-step generation of diffusion models are concentrated on distillation methods~\citep{luhman2021knowledge,salimans2022progressive,meng2022distillation,song2023consistency,kim2023consistency,sauer2023adversarial}. Particularly, consistency models (CMs)~\citep{song2023consistency} have emerged as a prominent method for diffusion distillation and successfully been applied to various data domains including latent space~\citep{luo2023latent}, audio~\citep{ye2023comospeech} and video~\citep{wang2023videolcm}. CMs consider training a student network to map arbitrary points on the PF-ODE trajectory to its starting point, thereby enabling one-step generation that directly maps noise to data. A follow-up work named consistency trajectory models (CTMs)~\citep{kim2023consistency} extends CMs by changing the mapping destination to encompass not only the starting point but also intermediate ones, facilitating unconstrained backward jumps on the PF-ODE trajectory. This design enhances training flexibility and permits the incorporation of auxiliary losses.

In both CMs and CTMs, the mapping function (referred to as the consistency function) must adhere to certain constraints. For instance, in CMs, there exists a boundary condition dictating that the starting point maps to itself. Consequently, the consistency functions are parameterized as a linear combination of the input data and the network output with pre-defined coefficients. This approach ensures that boundary conditions are naturally satisfied without constraining the form or expressiveness of the neural network. We term this parameterization technique \textit{preconditioning} in consistency distillation (Figure~\ref{fig:pipeline}), aligning with the terminology in EDM~\citep{karras2022elucidating}. The preconditionings in CMs and CTMs are intuitively crafted but may be suboptimal. Besides, despite efforts, CTMs have struggled to identify any distinct preconditionings that outperform the original one.

\begin{figure}[t]
    \centering
	\includegraphics[width=0.75\linewidth]{figures/fig-crop.pdf}
\vspace{-.1in}
\caption{\label{fig:pipeline}Consistency distillation with \textit{preconditioning} coefficients \textcolor{red}{$\alpha,\beta$}.}
\vspace{-.1in}
\end{figure}

In this work, we take the first step towards designing and enhancing preconditioning in consistency distillation to learn better "trajectory jumpers". We elucidate the design criteria of preconditioning by linking it to the discretization of the teacher ODE trajectory. We further convert the 
teacher PF-ODE into a generalized form involving free parameters, which induces a novel family of preconditionings. Through theoretical analyses, we unveil the significance of the consistency gap (referring to the gap between the teacher denoiser and the optimal student denoiser) in achieving good initialization and facilitating learning. By minimizing a derived bound of the consistency gap, we can optimize the preconditioning within our proposed family. We name the optimal preconditioning under our principle as \textit{Analytic-Precond}, as it can be analytically computed according to the teacher model without manual design or hyperparameter tuning. Moreover, the computation is efficient with less than 1\% time cost of the training process.

We demonstrate the effectiveness of Analytic-Precond by applying it to CMs and CTMs on standard benchmark datasets, including CIFAR-10, FFHQ 64$\times$64 and ImageNet 64$\times$64. While the vanilla preconditioning closely approximates Analytic-Precond and yields similar results in CMs, Analytic-Precond exhibits notable distinctions from its original counterpart in CTMs, particularly concerning intermediate jumps on the trajectory. Remarkably, Analytic-Precond achieves $2\times$ to $3\times$ training acceleration in CTMs in multi-step generation.