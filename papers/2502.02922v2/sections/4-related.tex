\section{Related Work}
\paragraph{Fast Diffusion Sampling} Fast sampling of diffusion models can be categorized into training-free and training-based methods. The former typically seek implicit sampling processes~\citep{song2020denoising,zheng2024masked,zheng2024diffusion} or dedicated numerical solvers to the differential equations corresponding to diffusion generation, including Heun's methods~\citep{karras2022elucidating}, splitting numerical methods~\citep{wizadwongsa2023accelerating}, pseudo numerical methods~\citep{liu2021pseudo} and exponential integrators~\citep{zhang2022fast,lu2022dpm,zheng2023dpm,gonzalez2023seeds}. They typically require around 10 steps for high-quality generation. In contrast, training-based methods, particularly adversarial distillation~\citep{sauer2023adversarial} and consistency distillation~\citep{song2023consistency,kim2023consistency}, have gained prominence for their ability to achieve high-quality generation with just one or two steps. While adversarial distillation proves its effectiveness in one-step generation of text-to-image diffusion models~\citep{sauer2024fast}, it is theoretically less transparent than consistency distillation due to its reliance on adversarial training. Diffusion models can also be accelerated using quantized or sparse attention~\citep{zhang2025sageattention,zhang2024sageattention2,zhang2025spargeattn}.
\paragraph{Parameterization in Diffusion Models} 
Parameterization is vital in efficient training and sampling of diffusion models. Initially, the practice involved parameterizing a noise prediction network \citep{ho2020denoising,song2020score}, which outperformed direct data prediction. A notable subsequent enhancement is the introduction of "v" prediction \citep{salimans2022progressive}, which predicts the velocity along the diffusion trajectory, and is proven effective in applications like text-to-image generation \citep{esser2024scaling} and density estimation \citep{zheng2023improved}. EDM~\citep{karras2022elucidating} further advances the field by proposing a preconditioning technique that expresses the denoiser function as a linear combination of data and network, yielding state-of-the-art sample quality alongside other techniques. However, the parameterization in consistency distillation remains unexplored.