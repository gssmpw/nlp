\section{Method}
Beyond the hand-crafted preconditionings outlined in Eqn.~\eqref{eq:cm-precond} and Eqn.~\eqref{eq:ctm-precond}, we seek a general paradigm of preconditioning design in consistency distillation. We first analyze their key ingredients and relate them to the discretization of the teacher ODE. Then we derive a generalized ODE form, which can induce a novel family of preconditionings. Finally, we propose a principled way to analytically obtain optimized preconditioning by minimizing the consistency gap.
\subsection{Analyzing the Preconditioning in Consistency Distillation}
\label{sec:method-1}
We examine the form of consistency function $\fv_\theta(\x,t,s)$ in CTMs, wherein it subsumes CMs as a special case by setting the jumping destination $s$ as the initial timestep $\epsilon$. Assume $\fv_\theta$ is parameterized as the following form of skip connection:
\begin{equation}
\label{eq:f-D-relation}
\fv_\theta(\x,t,s)=f(t,s)\x+g(t,s)\Dv_\theta(\x,t,s)
\end{equation}
where $\Dv_\theta(\x,t,s)=c_\skipp(t)\x+c_\out(t)\Fv_\theta(\x,t,s)$ represents the student denoiser function in alignment with EDM, and $f(t,s),g(t,s)$ are coefficients that linearly combine $\x$ and $\Dv_\theta$. We identify two essential constraints on the coefficients $f$ and $g$.
% \jianfei{it is too trickly to have $\fv$, $\Fv$, and $f$ for different meanings...}
\paragraph{Boundary Condition} For any free-form network $\Fv_\theta$ or $\Dv_\theta$, the consistency function $\fv_\theta$ must adhere to $\fv_\theta(\x,t,t)=\x$ (in-place jumping retains the original data point). Therefore, $f$ and $g$ should meet the conditions $f(t,t)=1$ and $g(t,t)=0$ for any time $t$.
\paragraph{Alignment with the Denoiser} Denote the optimal consistency function that precisely follows the teacher PF-ODE trajectory as $\fv_{\theta^*}(\x,t,s)$, and the optimal denoiser as $\Dv_{\theta^*}(\x,t,s)=\frac{\fv_{\theta^*}(\x,t,s)-f(t,s)\x}{g(t,s)}$ according to Eqn.~\eqref{eq:f-D-relation}. In CTMs, $f$ and $g$ are properly designed so that the limit $\Dv_{\theta^*}(\x,t,t)=\lim_{s\rightarrow t}\frac{\fv_{\theta^*}(\x,t,s)-f(t,s)\x}{g(t,s)}=\Dv_\phi(\x,t)$. Thus, the student denoiser at $s=t$, i.e. $\Dv_\theta(\x,t,t)$, ideally aligns with the teacher denoiser $\Dv_\phi$. This alignment offers two advantages: (1) $\Dv_{\theta}(\x_t,t,t)$ acts as a valid diffusion denoiser and is amenable to regularization with the DSM loss. (2) The teacher model $\Dv_\phi$ serves as an effective initializer of the student $\Dv_\theta$ at $s=t$, implying that $\Dv_\theta$ solely at $s<t$ is suboptimal and requires further optimization.

Precondionings satisfying these constraints can be derived by discretizing the teacher PF-ODE. Suppose the discretization from time $t$ to time $s$ is expressed as $\x_s=f(t,s)\x_t+g(t,s)\Dv_\phi(\x_t,t)$, then $f,g$ naturally satisfy the conditions: the discretization from $t$ to $t$ must be $\x_t=\x_t$; as $s\rightarrow t$, the discretization error tends to 0, and the optimal student for conducting infinitesimally small jumps is just $\Dv_\phi(\x_t,t)$. For instance, applying Euler method to the PF-ODE in Eqn.~\eqref{eq:teacher-ODE} yields:
\begin{equation}
\label{eq:raw-ODE-discretization}
    \x_s-\x_t=(s-t)\frac{\x_t-\Dv_\phi(\x_t,t)}{t}\Rightarrow \x_s=\frac{s}{t}\x_t+\left(1-\frac{s}{t}\right)\Dv_\phi(\x_t,t)
\end{equation}
which exactly matches the preconditioning used in CTMs by replacing $\Dv_\phi(\x_t,t)$ with $\Dv_\theta(\x_t,t,s)$. Elucidating preconditioning as ODE discretization also closely approximates CMs' choice in Eqn.~\eqref{eq:cm-precond}. For $t\gg\epsilon$, we have $t-\epsilon\approx t$, therefore $\fv_\theta$ in Eqn.~\eqref{eq:cm-precond} approximately equals the denoiser $\Dv_\theta$. On the other hand, as $\frac{\epsilon}{t}\approx 0$, CTMs' choice in Eqn.~\eqref{eq:ctm-precond} also indicates $\fv_\theta\approx \Dv_\theta$. Therefore, CMs' preconditioning is only distinct from ODE discretization when $t$ is close to $\epsilon$, which is not the case in one-step or few-step generation.
\subsection{Induced Preconditioning by Generalized ODE}
% \jianfei{not entirely clear what this part is about}
Based on the analyses above, the preconditioning can be induced from ODE discretization. Drawing inspirations from the dedicated ODE solvers in diffusion models~\citep{lu2022dpm,zheng2023dpm}, we consider a generalized representation of the teacher ODE in Eqn.~\eqref{eq:teacher-ODE}, which can give rise to alternative preconditionings that satisfy the restrictions.

Firstly, we modulate the ODE with a continuous function $L_t$ to transform it into an ODE with respect to $L_t\x_t$ rather than $\x_t$. Leveraging the chain rule of derivatives, we obtain $\frac{\dm(L_t\x_t)}{\dm t}=L_t\frac{\dm \x_t}{\dm t}+\frac{\dm L_t}{\dm t}\x_t$, where $\frac{\dm \x_t}{\dm t}$ can be substituted by the original teacher ODE, resulting in
\begin{equation}
    \frac{\dm(L_t\x_t)}{\dm t}=\frac{L_t}{t}\left[\left(1+\frac{\dm\log L_t}{\dm t}t\right)\x_t-\Dv_\phi(\x_t,t)\right]
\end{equation}
By changing the time variable from $t$ to $\lambda_t=-\log t$, the ODE can be further simplified to
\begin{equation}
    \label{eq:ODE-with-l}
    \frac{\dm(L_t\x_t)}{\dm \lambda_t}=L_t\gv_\phi(\x_t,t),\quad \gv_\phi(\x_t,t)\coloneqq \Dv_\phi(\x_{t},t)-(1-l_{t})\x_{t}
\end{equation}
where we denote $l_t\coloneqq\frac{\dm \log L_{t_\lambda}}{\dm\lambda}$, and $t_\lambda=e^{-\lambda}$ is the inverse function of $\lambda_t$. Moreover, $L_t$ can be represented by $l_t$ as $L_t=e^{\int_{\lambda_T}^{\lambda_t}l_{t_\lambda}\dm\lambda}$.
Secondly, instead of using $t$ or $\lambda_t$ as the time variable in the ODE (i.e., formulate the ODE as $\frac{\dm (\cdot)}{\dm t}$ or $\frac{\dm (\cdot)}{\dm \lambda_t}$), we can employ a generalized time representation $\eta_t=\int_{\lambda_T}^{\lambda_t} L_{t_{\lambda}}S_{t_{\lambda}}\dm\lambda$, where $S_t$ is any positive continuous function. This transformation ensures that $\eta$ monotonically increases with respect to $\lambda$, enabling one-to-one inverse mappings $t_\eta,\lambda_\eta$. To align with $L_t$, we express $S_t$ as $e^{\int_{\lambda_T}^{\lambda_t}s_{t_\lambda}\dm\lambda}$, where we denote $s_t\coloneqq\frac{\dm \log S_{t_\lambda}}{\dm\lambda}$. Using $\eta_t$ as the new time variable, we have $\dm\eta=L_{t_{\lambda}}S_{t_{\lambda}}\dm\lambda$, and the ODE in Eqn.~\eqref{eq:ODE-with-l} is further generalized to
\begin{equation}
\label{eq:ODE-with-l-s}
    \frac{\dm(L_t\x_t)}{\dm \eta_t}=\frac{\gv_\phi(\x_t,t)}{S_t}
\end{equation}
The final generalized ODE in Eqn.~\eqref{eq:ODE-with-l-s} is theoretically equivalent to the original teacher PF-ODE in Eqn.~\eqref{eq:teacher-ODE}, albeit with a set of introduced free parameters $\{l_t,s_t\}_{t=\epsilon}^T$. Applying the Euler method leads to different discretizations from Eqn.~\eqref{eq:raw-ODE-discretization}:
\begin{equation}
\label{eq:generalize-discretization-not-rearranged}
    L_s\x_s-L_t\x_t=(\eta_s-\eta_t)\frac{\gv_\phi(\x_t,t)}{S_t}
\end{equation}
which can be rearranged as
\begin{equation}
\label{eq:generalize-discretization-rearranged}
    \x_s=\frac{L_tS_t+(l_t-1)(\eta_s-\eta_t)}{L_sS_t}\x_t+\frac{\eta_s-\eta_t}{L_sS_t}\Dv_\phi(\x_t,t)
\end{equation}
Hence, the induced preconditioning can be expressed by Eqn.~\eqref{eq:f-D-relation} with a novel set of coefficients $f(t,s)=\frac{L_tS_t+(l_t-1)(\eta_s-\eta_t)}{L_sS_t}, g(t,s)=\frac{\eta_s-\eta_t}{L_sS_t}$. Originating from the Euler discretization of an equivalent teacher ODE, these coefficients adhere to the constraints outlined in Section~\ref{sec:method-1} under any parameters $\{l_t,s_t\}_{t=\epsilon}^T$, thus opening avenues for further optimization. The induced preconditioning can also degenerate to CTM's case $f(t,s)=\frac{s}{t},g(t,s)=1-\frac{s}{t}$ under specific selections $l_t=0,s_t=-1$ for $t\in[\epsilon,T]$.
\subsection{Principles for Optimizing the Preconditioning}
\begin{table}[t]
    \centering
    \caption{\label{tab:comparison}Comparison between different preconditionings used in consistency distillation.}
    \vskip 0.1in
    \resizebox{\textwidth}{!}{% <------ Don't forget this %
    \begin{tabular}{lcccc}
    \toprule
    Method&CM~\citep{song2023consistency}&BCM~\citep{li2024bidirectional}&CTM~\citep{kim2023consistency}&Analytic-Precond (\textbf{Ours})\\
    \midrule
    \makecell[l]{Free-form\\Network}&\multicolumn{1}{c}{$\Fv_\theta(\x_t,t)$}&\multicolumn{3}{|c}{$\Fv_\theta(\x_t,t,s)$}\\
    \midrule
    \makecell[tl]{Denoiser\\Function}&\multicolumn{1}{c}{\makecell[t]{$\Dv_\theta(\x,t)=c_{\skipp}(t)\x$\\$+\;c_{\out}(t)\Fv_\theta(\x,t)$}}&\multicolumn{3}{|c}{$\displaystyle\Dv_\theta(\x,t,s)=c_{\skipp}(t)\x+c_{\out}(t)\Fv_\theta(\x,t,s)$}\\
    \midrule
    \vspace{0.05in}
    % $c_{\skipp}(t)$&\multicolumn{4}{c}{$\displaystyle\frac{\sigma_\data^2}{\sigma_\data^2+t^2}$}\\
    % \vspace{0.05in}
    % $c_{\out}(t)$&\multicolumn{4}{c}{$\displaystyle\frac{\sigma_\data t}{\sqrt{\sigma_\data^2+t^2}}$}\\
    % \midrule
    % \vspace{0.05in}
    % \midrule
    \makecell[tl]{Consistency\\Function}&\multicolumn{1}{c}{\makecell[t]{$\fv_\theta(\x,t)=f(t,\epsilon)\x$\\$+\;g(t,\epsilon)\Fv_\theta(\x,t)$}}&\makecell[t]{$\fv_\theta(\x,t,s)=f(t,s)\x$\\$+\;g(t,s)\Fv_\theta(\x,t,s)$}&\multicolumn{2}{|c}{\makecell[t]{$\fv_\theta(\x,t,s)=f(t,s)\x$\\$+\;g(t,s)\Dv_\theta(\x,t,s)$}}\\
    \midrule
    \vspace{0.05in}
    $f(t,s)$&$\displaystyle\frac{\sigma_\data^2}{\sigma_\data^2+(t-s)^2}$&$\displaystyle\frac{\sigma_\data^2+ts}{\sigma_\data^2+t^2}$&$\displaystyle\frac{s}{t}$&$\displaystyle\frac{L_tS_s}{L_sS_s+(1-l_s)(\eta_s-\eta_t)}$\\
    \vspace{0.05in}
    $g(t,s)$&$\displaystyle\frac{\sigma_\data(t-s)}{\sqrt{\sigma_\data^2+t^2}}$&$\displaystyle\frac{\sigma_\data(t-s)}{\sqrt{\sigma_\data^2+t^2}}$&$\displaystyle1-\frac{s}{t}$&$\displaystyle\frac{\eta_s-\eta_t}{L_sS_s+(1-l_s)(\eta_s-\eta_t)}$\\
    \bottomrule
    \end{tabular}% <------ Don't forget this %
    }
    \vspace{-0.1in}
\end{table}
Derived from the generalized teacher ODE presented in Eqn.~\eqref{eq:ODE-with-l-s}, a range of preconditionings is now at our disposal with coefficients $f,g$ from Eqn.~\eqref{eq:generalize-discretization-rearranged}, governed by the free parameters $\{l_t,s_t\}_{t=\epsilon}^T$. Our aim is to establish guiding principles for discerning the optimal sets of $\{l_t,s_t\}_{t=\epsilon}^T$, thereby attaining superior preconditioning compared to the original one in Eqn.~\eqref{eq:ctm-precond}.

Firstly, drawing from the insights of Rosenbrock-type exponential integrators and their relevance in diffusion models~\citep{hochbruck2010exponential,hochbruck2009exponential,zheng2023dpm}, it is suggested that the parameter $l_t$ be chosen to restrict the gradient of Eqn.~\eqref{eq:ODE-with-l-s}'s right-hand side term with respect to $\x_t$. This choice ensures the robustness of the resulting ODE against errors in $\x_t$. An analytical solution for $l_t$ is derived as follows:
\begin{equation}
\label{eq:l-solution}
l_t=\argmin_{l}\E_{q(\x_t)}\left[\|\nabla_{\x_t}\gv_\phi(\x_t,t)\|_F\right]\Rightarrow l_t=1-\frac{\E_{q(\x_t)}\left[\tr(\nabla_{\x_t}\Dv_\phi(\x_t,t))\right]}{d}
\end{equation}
where $d$ is the data dimensionality, $\|\cdot\|_F$ denotes the Frobenius norm and $\tr(\cdot)$ represents the trace of a matrix. Secondly, to determine the optimal value of $s_t$, we dive deeper into the relationship between the teacher denoiser $\Dv_\phi(\x_t,t)$ and the student denoiser $\Dv_\theta(\x_t,t,s)$. As elucidated in Section~\ref{sec:method-1}, the preconditioning is properly crafted to ensure that the optimal student denoiser satisfies $\Dv_{\theta^*}(\x_t,t,t)=\Dv_\phi(\x_t,t)$. We further explore the scenario where $s<t$ by examining the gap $\|\Dv_{\theta^*}(\x_t,t,s)-\Dv_\phi(\x_t,t)\|_2$, which we refer to as the \textit{consistency gap}. Minimizing this gap extends the alignment of $\Dv_\phi$ and $\Dv_{\theta^*}$ to cases where $s<t$, ensuring that the teacher denoiser also serves as a good trajectory jumper. In the subsequent proposition, we derive a bound depicting the asymptotic behavior of the consistency gap:
\begin{proposition}[Bound for the Consistency Gap, proof in Appendix~\ref{appendix:proof1}]
\label{proposition:gap}
Suppose there exists some constant $C>0$ so that the parameters $\{l_t,s_t\}_{t=\epsilon}^T$ are bounded by $|l_t|,|s_t|\leq C$, then the optimal student denoiser function $\Dv_{\theta^*}$ under the preconditioning $f(t,s)=\frac{L_tS_t+(l_t-1)(\eta_s-\eta_t)}{L_sS_t}, g(t,s)=\frac{\eta_s-\eta_t}{L_sS_t}$ satisfies
\begin{equation}
    \|\Dv_{\theta^*}(\x_t,t,s)-\Dv_\phi(\x_t,t)\|_2\leq \frac{(t/s)^{3C}-1}{3C}\max_{s\leq \tau\leq t} \left\|\frac{\dm\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-s_\tau\gv_\phi(\x_\tau,\tau)\right\|_2
\end{equation}
\end{proposition}
The proposition conforms to the constraint $\Dv_{\theta^*}(\x_t,t,t)=\Dv_\phi(\x_t,t)$ when $s=t$. Moreover, considering $s$ in a local neighborhood of $t$, by Taylor expansion we have $\frac{(t/s)^{3C}-1}{3C}=\frac{e^{3C(\log t-\log s)}-1}{3C}=1+\Oc(\log t-\log s)$. Therefore, the consistency gap for $s\in (t-\delta,t)$, when $\delta$ is small, is roughly $\max_{s\leq \tau\leq t} \|\frac{\gv_\phi(\x_\tau,\tau)}{\dm\lambda_\tau}-s_\tau\gv_\phi(\x_\tau,\tau)\|_2$. Minimizing this yields an analytic solution for $s_t$:
\begin{equation}
\label{eq:s-solution}
s_t=\argmin_{s}\E_{q(\x_t)}\left[\left\|\frac{\dm\gv_\phi(\x_t,t)}{\dm\lambda_t}-s_t\gv_\phi(\x_t,t)\right\|_2\right]\Rightarrow s_t=\frac{\E_{q(\x_t)}\left[\gv_\phi(\x_t,t)^\top\frac{\dm\gv_\phi(\x_t,t)}{\dm\lambda_t}\right]}{\E_{q(\x_t)}\left[\|\gv_\phi(\x_t,t)\|_2^2\right]}
\end{equation}
We term the resulting preconditioning as \textit{Analytic-Precond}, as $l_t,s_t$ are analytically determined by the teacher $\phi$ using Eqn.~\eqref{eq:l-solution} and Eqn.~\eqref{eq:s-solution}. Though $l_t,s_t$ are defined over continuous timesteps, we can compute them on hundreds of discretized ones, while obtaining reasonable estimations of their related terms $L_t,S_t,\eta_t$. The computation is highly efficient utilizing automatic differentiation in modern deep learning frameworks, requiring less than 1\% of the total training time (Appendix~\ref{appendix:compute-ems}).
\paragraph{Backward Euler Method for Training Stability} Despite the approximation $\frac{(t/s)^{3C}-1}{3C}\approx 1$ holding true in local neighborhoods of $t$, the coefficient $\frac{(t/s)^{3C}-1}{3C}$ in the bound exhibits exponential behavior when $\frac{t}{s}\gg 1$. In practice, directly applying the preconditioning derived from Eqn.~\eqref{eq:generalize-discretization-rearranged} may cause training instability, especially on long jumps with large step sizes. Drawing inspiration from the stability of the backward Euler method, known for its efficacy in handling stiff equations without step size restrictions, we propose a backward rewriting of Eqn.~\eqref{eq:generalize-discretization-rearranged} from $s$ to $t$ as $\x_t=\hat f(s,t)\x_s+\hat g(s,t)\Dv_\phi$, where $\hat f,\hat g$ are the original coefficients from Eqn.~\eqref{eq:generalize-discretization-rearranged}. Rearranging this equation yields $\x_s=\frac{1}{\hat f(s,t)}\x_t-\frac{\hat g(s,t)}{\hat f(s,t)}\Dv_\phi$, giving rise to the \textit{backward coefficients} $f(t,s)=\frac{1}{\hat f(s,t)},g(t,s)=-\frac{\hat g(s,t)}{\hat f(s,t)}$. 

We summarize different preconditionings in Table~\ref{tab:comparison}, where we also included a concurrent work called bidirectional consistency models (BCMs)~\citep{li2024bidirectional} which proposed an alternative preconditioning to CTMs.