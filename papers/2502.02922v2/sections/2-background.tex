\section{Background}
\subsection{Diffusion Models}
Diffusion models~\citep{song2020score,sohl2015deep,ho2020denoising} transform a $d$-

dimensional data distribution $q_0(\x_0)$ into Gaussian noise distribution through a forward stochastic differential equation (SDE) starting from $\x_0\sim q_0$:
\begin{equation}
    \label{eq:forwardSDE}
    \dm \x_t = f(t)\x_t \dm t + g(t)\dm\wv_t
\end{equation}
where $t \in [0, T]$ for some finite horizon $T$, $f,g:[0,T] \to \R$ is the scalar-valued drift and diffusion term, and $\wv_t \in \R^d$ is a standard Wiener process. The forward SDE is accompanied by a series of marginal distributions $\{q_t\}_{t=0}^T$ of $\{\x_t\}_{t=0}^T$, and $f,g$ are properly designed so that the terminal distribution is approximately a pure Gaussian, i.e., $q_T(\boldsymbol{x}_T)\approx \Nc(\vect0,\sigma_T^2\Iv)$. An intriguing characteristic of this SDE lies in the presence of the probability flow (PF) ODE~\citep{song2020score}
$
    \dm\x_t = [f(t)\x_t - \frac{1}{2}g^2(t)\nabla_{\x_t} \log q_t(\x_t)] \dm t
$
whose solution trajectories at time $t$, when solved backward from time $T$ to time $0$, are distributed exactly as $q_t$. The only unknown term $\nabla_{\x_t}\log q_t(\x_t)$ is the \textit{score function} and can be learned by denoising score matching (DSM)~\citep{vincent2011connection}.

A prevalent noise schedule $f=0,g=\sqrt{2t}$ is proposed by EDM~\citep{karras2022elucidating} and followed in recent text-to-image generation~\citep{esser2024scaling}, video generation~\citep{blattmann2023stable}, as well as consistency distillation. In this case, the forward transition kernel of the forward SDE (Eqn.~\eqref{eq:forwardSDE}) owns a simple form $q(\x_t|\x_0)=\Nc(\x_0,t^2\Iv)$, and the terminal distribution $q_T\approx \Nc(\vect0,T^2\Iv)$. Besides, the PF-ODE can be represented by the \textit{denoiser function} $\Dv_\phi(\x_t,t)$:
\begin{equation}
\label{eq:teacher-ODE}
    \frac{\dm\x_t}{\dm t}=\frac{\x_t-\Dv_\phi(\x_t,t)}{t}
\end{equation}
where the denoiser function is trained to predict $\x_0$ given noisy data $\x_t=\x_0+t\epsilonv,\epsilonv\sim\Nc(\vect0,\Iv)$ at any time $t$, i.e., minimizing $\E_t\E_{p_{\data}(\x_0)q(\x_t|\x_0)}[w(t)\|\Dv_\phi(\x_t,t)-\x_0\|_2^2]$ for some weighting $w(t)$. This denoising loss is equivalent to the DSM loss~\citep{song2020score}. In EDM, another key insight is to employ preconditioning by parameterizing $\Dv_\phi$ as $\Dv_\phi(\x,t)=c_{\skipp}(t)\x+c_{\out}(t)\Fv_\phi(\x,t)$\footnote{More precisely, $\Dv_\phi(\x,t)=c_{\skipp}(t)\x+c_{\out}(t)\Fv_\phi(c_{\text{in}}(t)\x,c_{\text{noise}}(t))$. Since $c_{\text{in}}(t)$ and $c_{\text{noise}}(t)$ take effects inside the network, we absorb them into the definition of $\Fv_\phi$ for simplicity.}, where
\begin{equation}
    c_{\skipp}(t)=\frac{\sigma_\data^2}{\sigma_\data^2+t^2},\quad c_{\out}(t)=\frac{\sigma_\data t}{\sqrt{\sigma_\data^2+t^2}}, 
\end{equation}
$\Fv_\phi$ is a free-form neural network and $\sigma_\data^2$ is the variance of the data distribution.
\subsection{Consistency Distillation}
Denote $\phi$ as the parameters of the teacher diffusion model, and $\theta$ as the parameters of the student network. Given a trajectory $\{\xv_t\}_{t=\epsilon}^{T}$ with a fixed initial timestep $\epsilon$ of a teacher PF-ODE\footnote{In EDM, the range of timesteps is typically chosen as $\epsilon=0.002,T=80$.}, consistency models (CMs) ~\citep{song2023consistency} aim to learn a \textit{consistency function} $\fv_\theta: (\xv_t, t) \mapsto \xv_{\epsilon}$ which maps the point $\x_t$ at any time $t$ on the trajectory to the initial point $\x_\epsilon$. The consistency function is forced to satisfy the \textit{boundary condition} $\fv_{\theta}(\xv, \epsilon) = \xv$. To ensure unrestricted form and expressiveness of the neural network, $\fv_\theta$ is parameterized as
\begin{equation}
\label{eq:cm-precond}
    \fv_\theta(\x,t)=\frac{\sigma_\data^2}{\sigma_\data^2+(t-\epsilon)^2}\x+\frac{\sigma_\data (t-\epsilon)}{\sqrt{\sigma_\data^2+t^2}}\Fv_\theta(\x,t)
\end{equation}
which naturally satisfies the boundary condition for any free-form network $\Fv_\theta(\x_t,t)$. We refer to this technique as \textit{preconditioning} in consistency distillation, aligning with the terminology in EDM. 
The student $\theta$ can be distilled from the teacher by the training objective:
\begin{equation}
\label{eq:loss-cm}
\E_{t\in[\epsilon,T],s\in[\epsilon,t)}\E_{q_0(\xv_0)q(\xv_t|\xv_0)}\left[w(t)d\left(\fv_{\theta}(\xv_t, t),\fv_{\sg(\theta)}(\Solver_\phi(\x_t,t,s), s)\right)\right],
\end{equation}
where $w(\cdot)$ is a positive weighting function, $d(\cdot,\cdot)$ is a distance metric, $\sg$ is the (exponential moving average) stop-gradient and $\Solver_\phi$ is any numerical solver for the teacher PF-ODE.

Consistency trajectory models (CTMs)~\citep{kim2023consistency} extend CMs by changing the mapping destination to not only the initial point but also any intermediate ones, enabling unconstrained backward jumps on the PF-ODE. Specifically, the consistency function is instead defined as $\fv_\theta: (\xv_t, t,s) \mapsto \xv_{s}$, which maps the point $\x_t$ at time $t$ on the trajectory to the point $\x_s$ at any previous time $s<t$. The boundary condition is $\fv_\theta(\x,t,t)=\x$, which is forced by the following preconditioning:
\begin{equation}
\label{eq:ctm-precond}
    \fv_\theta(\x,t,s)=\frac{s}{t}\x+\left(1-\frac{s}{t}\right)\Dv_\theta(\x,t,s)
\end{equation}
where $\Dv_\theta(\x_t,t,s)=c_\skipp(t)\x+c_\out(t)\Fv_\theta(\x,t,s)$ is the student denoiser function, and $\Fv_\theta(\x,t,s)$ is a free-form network with an extra timestep input $s$. The student network is trained by minimizing
\begin{equation}
\begin{aligned}
    \label{eq:loss-ctm}\E_{t\in[\epsilon,T],s\in[\epsilon,t]u\in [s,t)}&\E_{q_0(\xv_0)q(\xv_t|\xv_0)}\\
    &\left[w(t)d\left(\fv_{\sg(\theta)}(\fv_{\theta}(\xv_t, t,s),s,\epsilon),\fv_{\sg(\theta)}(\fv_{\sg(\theta)}(\Solver_\phi(\x_t,t,u),u,s),s,\epsilon)\right)\right]
\end{aligned}
\end{equation}
An important property of CTM's precondtioning is that when $s\rightarrow t$, the optimal denoiser satisfies $\Dv_{\theta^*}(\x,t,s)\rightarrow \Dv_\phi(\x_t,t)$, i.e. the diffusion denoiser. Consequently, the DSM loss in diffusion models can be incorporated to regularize the training of $\theta$, which enhances the sample quality as the number of sampling steps increases, enabling speed-quality trade-off.