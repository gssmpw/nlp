
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Elucidating the Preconditioning in Consistency Distillation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Kaiwen Zheng$^{\dagger1}$\thanks{Work done during an internship at Shengshu; \quad $^\dagger$Equal contribution; \quad $^\ddagger$The corresponding author.}, \quad Guande He$^{\dagger1}$\footnotemark[1], \quad  Jianfei Chen$^1$, \quad  Fan Bao$^{12}$, \quad Jun Zhu$^{\ddagger123}$\\
  $^1$Dept. of Comp. Sci. \& Tech., Institute for AI, BNRist Center, THBI Lab\\
  $^1$Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China\\
  $^2$Shengshu Technology, Beijing \quad $^3$Pazhou Lab (Huangpu), Guangzhou, China \\
  \texttt{zkwthu@gmail.com; guande.he17@outlook.com;}\\
  \texttt{fan.bao@shengshu.ai; \{jianfeic, dcszj\}@tsinghua.edu.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.
\end{abstract}

\input{sections/1-intro}
\input{sections/2-background}
\input{sections/3-method}
\input{sections/4-related}
\input{sections/5-experiment}
\input{sections/6-conclusion}

\section*{Acknowledgments}
This work was supported by the National Natural Science Foundation of China (Nos. 62350080, 62106120, 92270001), Tsinghua Institute for Guo Qiang, and the High Performance
Computing Center, Tsinghua University; J. Zhu was also supported by the XPlorer Prize.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\newpage
\appendix
\input{sections/0-appendix}


\end{document}
