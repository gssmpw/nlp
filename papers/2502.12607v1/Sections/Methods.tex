\section{Methods}
\label{Methods}
In this section, we explain the proposed DGKIP method in details. First, we propose a theorem to calculate the DG bound and use it to derive the prediction upper bound and the test error upper bound. Then we explain the general pipeline of DGKIP. Finally, we showcase an example of logistic regression formulation.

\subsection{Duality Gap Bound}
\label{duality_gap_bound}

A key assumption of DGKIP is that the regularization term and potentially the entire objective is \(\lambda\)-strongly convex. 
\begin{definition}[\(\lambda\)-strong convexity]
  Let \(\lambda > 0\), and consider a function
  \(f: \mathbb{R}^d \to \mathbb{R} \cup \{\infty\}\) that is convex and has a nonempty domain.
  We say that \(f\) is \(\lambda\)-strongly convex if, for all
  \(\boldsymbol{\theta}, \boldsymbol{\theta}' \in \mathrm{dom}(f)\) and for every
  subgradient \(\boldsymbol{g} \in \partial f(\boldsymbol{\theta}')\), the following holds:
  \[
    f(\boldsymbol{\theta}) - f(\boldsymbol{\theta}') 
    \;\ge\;
    \boldsymbol{g}^\top \!\bigl(\boldsymbol{\theta} - \boldsymbol{\theta}'\bigr) 
    \;+\; \tfrac{\lambda}{2}\,\|\boldsymbol{\theta} - \boldsymbol{\theta}'\|_2^2.
  \]
  \end{definition}
  

% Let \(\lambda > 0\) and consider a function \(f: \mathbb{R}^d \to \mathbb{R} \cup \{\infty\}\) is assumed to be convex and nonempty. We say that \(f\) is \(\lambda\)-strongly convex if, for all \(\boldsymbol{\theta}, \boldsymbol{\theta}' \in \mathrm{dom}(f)\) and for every subgradient \(\boldsymbol{g} \in \partial f(\boldsymbol{\theta}')\), the following holds:
% \begin{equation*}
%   f(\boldsymbol{\theta}) \;-\; f(\boldsymbol{\theta}')  \;\ge\; 
%    \boldsymbol{g}^\top \!\bigl(\boldsymbol{\theta} - \boldsymbol{\theta}'\bigr) 
%   \;+\; \tfrac{\lambda}{2}\,\|\boldsymbol{\theta} - \boldsymbol{\theta}'\|_2^2.
%   \label{eq:lambda_strong_convex}
% \end{equation*}

The bound on machine learning model parameters based on the DG was first introduced in the context of safe screening. By adapting the idea of the DG bound for safe screening \cite{shibagaki2016simultaneous} to the dataset distillation problem, we obtain the following theorem.

\begin{theorem}[Bound on Parameter Deviation]
\label{thm:parameter_bound}
Suppose the objective function $P_{\mathcal{O}}(\boldsymbol{\theta})$ and $P_{\mathcal{S}}(\boldsymbol{\theta})$ are $\lambda$-strongly convex with respect to $\boldsymbol{\theta}$, and let $\bm \theta_{\cO}^*$ be its unique minimizer. Let $\bm \theta_{\cS}^*$ be the corresponding minimizer when the dataset is replaced by a smaller synthetic set $\mathcal{S}$. 
The duality gap is defined as
\[
  G_{\mathcal{S}}\bigl(\bm \theta_{\cO}^*, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)
  \;:=\;
  P_{\mathcal{S}}\bigl(\boldsymbol{\theta}_{\cO}^*\bigr)
  \;-\;
D_{\mathcal{S}}\bigl(\tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr),
\]
% \footnote{For $\ell$, the convex conjugate $\ell^*$ is taken for the second argument. The same for $\partial\ell$.}
where
\begin{equation*}
   \begin{aligned}
    P_\mathcal{S}(\bm \theta_{\cO}^*)&:= \sum_{i=1}^{n_\mathcal{S}} \ell(y_i^\mathcal{S}, f (\boldsymbol{x}_i^\mathcal{S};  \bm \theta_{\cO}^*))+\frac{\lambda}{2}\|\bm \theta_{\cO}^*\|^2  \\
D_{\mathcal{S}}\bigl(\tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)&:= \sum_{i=1}^{n_\mathcal{S}} \ell^*(y_i^\mathcal{S}, -\tilde{\alpha}_i^\mathcal{S})\\
&- \frac{1}{2\lambda}\sum_{i=1}^{n_\mathcal{S}} \sum_{i=j}^{n_\mathcal{S}}\tilde{\alpha}_i^\mathcal{S}\tilde{\alpha}_j^\mathcal{S} y_i^\mathcal{S} y_j^\mathcal{S} k (\boldsymbol{x}_i^\mathcal{S},\boldsymbol{x}_j^\mathcal{S}).
    \end{aligned}
\end{equation*}
Here, $\tilde{\boldsymbol{\alpha}}_{\mathcal{S}} \in \mathrm{dom}(D_{\mathcal{S}})$ is any feasible dual variable. Then, the deviation between the parameter solutions satisfies
\begin{equation}
      \bigl\|\bm \theta_{\cO}^* \;-\; \bm \theta_{\cS}^*\bigr\|_2^2
  \;\;\le\;\;
  \frac{2}{\lambda}\;
  G_{\mathcal{S}}(\bm \theta_{\cO}^*, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}).
\label{eq:dualitygap} 
\end{equation}

% , and the regularization term can also be expressed as $\frac{\lambda}{2}\|\boldsymbol{\theta}_\mathcal{O}\|^2 =\frac{1}{2\lambda}\sum_{i=1}^{n_\mathcal{O}} \sum_{i=j}^{n_\mathcal{O}}\alpha_i^\mathcal{O}\alpha_j^\mathcal{O} y_i^\mathcal{S} y_j^\mathcal{O} k (\boldsymbol{x}_i^\mathcal{O},\boldsymbol{x}_j^\mathcal{O})$
\end{theorem}
The proof of Theorem \ref{thm:parameter_bound} is present in Appendix \ref{proof_theorem}.



The feasible dual variable  $\tilde{\boldsymbol{\alpha}}_\mathcal{S}$ can be constructed arbitrarily as long as it satisfies the constraint conditions. In this work, to tighten the bound, $\tilde{\boldsymbol{\alpha}}_\mathcal{S}$ is set based on the KKT conditions \eqref{eq:ktt1} as follows:
\begin{equation}
  \tilde{\boldsymbol{\alpha}}_{\mathcal{S}} \in -\partial \ell\left(\boldsymbol{y_\mathcal{S}},  f (X_\mathcal{S};  \bm \theta_{\cO}^*)\right).
  \label{eq:alphaapproxi}
\end{equation}
Given the DGB in \eqref{eq:dualitygap}, we could say large DG will lead to loose bounds. In contrast, small DG can provide tight bounds. With a tight bound, we could derive the bound of prediction values. For any input from the training set, if
$\boldsymbol{\theta}_{\mathcal{S}}$ lies within an $L_2$-ball around
$\boldsymbol{\theta}_{\mathcal{O}}$, then for any vector
$\boldsymbol{x} \in \mathbb{R}^d$,
% \[
%   \bigl|\phi(\boldsymbol{x})^{\top}\boldsymbol{\theta}_{\mathcal{S}}
%   \;-\;
%   \phi(\boldsymbol{x})^{\top}\boldsymbol{\theta}_{\mathcal{O}}
%   \bigr|
%   \;\;\le\;\;
%   \|\phi(\boldsymbol{x})\|_2 \,\cdot\,
%   \|\boldsymbol{\theta}_{\mathcal{S}}
%   -
%   \boldsymbol{\theta}_{\mathcal{O}}\|_2.
% \]
% \[
%   \bigl|f(\boldsymbol{x};\boldsymbol{\theta}_{\mathcal{S}})
%   \;-\;
%   f(\boldsymbol{x};\boldsymbol{\theta}_{\mathcal{O}}
%   \bigr|
%   \;\;\le\;\;
%   \|\phi(\boldsymbol{x})\|_2 \,\cdot\,
%   \|\boldsymbol{\theta}_{\mathcal{S}}
%   -
%   \boldsymbol{\theta}_{\mathcal{O}}\|_2.
% \]
  \begin{equation*}
    \begin{aligned}
      \bigl|f(\boldsymbol{x};\bm \theta_{\cS}^*)\;-\;f(\boldsymbol{x};\bm \theta_{\cO}^*) \bigr| & \;\;\le\;\;\|\boldsymbol{\phi}(\boldsymbol{x})\|_2 \,\cdot\,\bigl\|\bm \theta_{\cO}^* \;-\; \bm \theta_{\cS}^*\bigr\|_2^2.\\                            & \;\;\le\sqrt{k(\boldsymbol{x},\boldsymbol{x}) \,\cdot\, \frac{2}{\lambda}G_{\mathcal{S}}(\bm \theta_{\cO}^*,\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}.
    \end{aligned}
  \end{equation*}
This immediately yields an upper bound on per-sample (and thus aggregate) prediction deviations after dataset distillation.

\begin{lemma}[Minimizing DG Also Minimizes the Prediction Upper Bound]
\label{lem:dg-minimizes-prediction-bound}
Let $G_{\mathcal{S}}$ be the duality gap defined in Theorem~\ref{thm:parameter_bound}. If $P_{\mathcal{O}}(\boldsymbol{\theta})$ is $\lambda$-strongly convex, then minimizing this duality gap with respect to $\mathcal{S}$ also minimizes the following quantity for any $\boldsymbol{x}$:
\begin{equation*}
  \min_{\boldsymbol{\theta} 
        \,\in\,
        \bigl\{
          \|\boldsymbol{\theta}
              -\bm \theta_{\cO}^*\|_2
          \,\le\,
          \sqrt{\tfrac{2}{\lambda}G_{\mathcal{S}}(\bm \theta_{\cO}^*, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}
        \bigr\}}
   \!\!
   \bigl|\boldsymbol{\phi}(\boldsymbol{x})^{\top}\boldsymbol{\theta}
          \;-\;
          \boldsymbol{\phi}(\boldsymbol{x})^{\top}\bm \theta_{\cO}^*\bigr|.
\end{equation*}
Equivalently, shrinking the duality gap, reduces the worst-case difference in predicted values 
$\boldsymbol{\phi}(\boldsymbol{x})^{\top}\boldsymbol{\theta}$ 
compared with 
$\boldsymbol{\phi}(\boldsymbol{x})^{\top}\bm \theta_{\cO}^*$.
\end{lemma}

The proof of Lemma \ref{lem:dg-minimizes-prediction-bound} is presented in Appendix \ref{proof_lemma3.2}

Now we have the prediction bound, the instances could be divided into three classes: certainly correct, certainly wrong, and unknown. When we apply it to the test set, test error bounds could be obtained.


\begin{lemma}
\label{lem:test-error-upper-bound}
The range of the test error (\textrm{TeEr}) on the test dataset $\left\{\left(\boldsymbol{x}_i^{\prime}, y_i^{\prime}\right)\right\}_{i=1}^{n^\prime}$ is derived using the bound of model parameters after retraining as follows:
    \begin{equation*}
\frac{n_{\text{mis}}}{n^\prime} \leq \textrm{TeEr} \leq \frac{n_{\text{mis}} + n_{\text{unk}}}{n^\prime} = \frac{n^\prime - n_{\text{cor}}}{n^\prime}
\end{equation*}
\[
n_{\text{cor}} = \sum_{i=1}^{n^\prime}  I\left[\min_{\boldsymbol{\theta}\in\Theta} y_i^\prime\boldsymbol{\theta}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime) > 0\right],
\]
\[
n_{\text{mis}} =  \sum_{i=1}^{n^\prime}  I\left[\max_{\boldsymbol{\theta}\in\Theta} y_i^\prime\boldsymbol{\theta}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime) < 0\right],
\]
\[
n_{\text{unk}} =\sum_{i=1}^{n^\prime}  I\left[\min_{\boldsymbol{\theta}\in\Theta} y_i^\prime{\bm \theta_{\cO}^*}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime) < 0,\quad \max_{\boldsymbol{\theta}\in\Theta} y_i^\prime{\bm \theta_{\cO}^*}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime) > 0\right],
\]
% \[
% n_{\text{all}} = n_{\text{cor}} + n_{\text{mis}} + n_{\text{unk}}
% \]
where
  \begin{equation*}
    \begin{aligned}
     & \min _{\boldsymbol{\theta} \in \Theta} y_i^\prime{\bm \theta_{\cO}^*}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime)\\ =&y_i^\prime{\bm \theta_{\cO}^*}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime)-  \|\boldsymbol{\phi}(\boldsymbol{x^\prime})\|_2 \,\cdot\, \sqrt{\frac{2}{\lambda}G_{\mathcal{S}}({\bm \theta_{\cO}^*},\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}\\= &y_i^\prime f(\boldsymbol{x}_i^\prime;  {\bm \theta_{\cO}^*})  -  \sqrt{k(\boldsymbol{x}^\prime,\boldsymbol{x}^\prime) \,\cdot\, \frac{2}{\lambda}G_{\mathcal{S}}({\bm \theta_{\cO}^*},\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}
    \end{aligned}
  \end{equation*}
  \begin{equation*}
    \begin{aligned}
     & \max _{\boldsymbol{\theta} \in \Theta} y_i^\prime{\boldsymbol{\theta}}^\top\boldsymbol{\phi}(\boldsymbol{x}_i^\prime)
     \\= &y_i^\prime f(\boldsymbol{x}_i^\prime;  {\bm \theta_{\cO}^*})  +  \sqrt{k(\boldsymbol{x}^\prime,\boldsymbol{x}^\prime) \,\cdot\, \frac{2}{\lambda}G_{\mathcal{S}}({\bm \theta_{\cO}^*},\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}
    \end{aligned}
  \end{equation*}

\end{lemma}
The proof related to the inequality shown in Lemma \ref{lem:test-error-upper-bound} is presented in Appendix \ref{proof_lemma3.3}.
We note that the bounds of TeEr can be computed even with the kernel model, that is, ${\bm \theta_{\cO}^*}$ does not need to be explicitly computed. 

In subsequent sections, we use the proposed theorem and lemmas to guide dataset distillation, ensuring that the distilled dataset $\mathcal{S}$ induces a model ${\bm \theta_{\cS}^*}$ that is not excessively distant from the original model ${\bm \theta_{\cS}^*}$.

\subsection{Overview of DGKIP}

DGKIP proceeds through the following key stages in practical dataset distillation. First, a baseline model is trained on the full dataset $\mathcal{O}$ to obtain parameters ${\bm \theta_{\cO}^*}$. This step is performed only once and serves as the reference solution. Next, a smaller synthetic set $\mathcal{S}$ with size $n_\mathcal{S}$ is initialized, either randomly or by a heuristic. DGKIP then uses ${\bm \theta_{\cO}^*}$ as a guide to approximate the dual variables $\tilde{\boldsymbol{\alpha}}$ for $\mathcal{S}$ by \eqref{eq:alphaapproxi}. 

With $\tilde{\boldsymbol{\alpha}}_\mathcal{S}$ and $\mathcal{S}$, we could calculate DG by \eqref{eq:dualitygap}. After each update of $\mathcal{S}$, we will approximate $\tilde{\boldsymbol{\alpha}}_\mathcal{S}$. Because the ${\bm \theta_{\cO}^*}$ is pre-compute, so the computation cost can be ignored. These two processes will iterate until convergence. A detailed description of each optimization step is shown in Algorithm \ref{alg:distillation}.

To efficiently incorporate neural-network-like kernels into the DGKIP, we adopt the NNGP random feature approximation \cite{loo2022efficient} to handle large-scale datasets, details is presented in Appendix \ref{nngp_intro}. In the following section, we will illustrate the DGKIP in a specific example of logistic regression. 



\subsection{Duality Gap for Logistic Regression (Binary Cross-entropy Loss)}
We consider a binary classification logistic regression with a $L_2$ regularization term. The corresponding dual objective involves variables $\alpha_i \in [0,1]$ for each training example, which captures how strongly each example influences the decision boundary.
With label set $y_i \in \{-1,+1\}$, we solve the following primal problem:
\begin{equation*}
\label{lr-theta-opt}
  P(\bm \theta) =  \sum_{i=1}^n \log[1+\exp(-y_i f( \boldsymbol{x}_i;\boldsymbol{\theta}))] + \frac{\lambda}{2} \|\boldsymbol{\theta}\|_2^2.
\end{equation*}
Through Fenchel duality, we obtain dual variables $\alpha_i \in [0,1]$ in the following dual problem:
\begin{equation*}
\begin{aligned}
D(\bm \alpha) =  &\frac{1}{2\lambda}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i\alpha_j y_iy_jk(\boldsymbol{x}_i,\boldsymbol{x}_j) \\ & + \sum_{i=1}^{N} \alpha_i \log \alpha_i + (1 - \alpha_i) \log(1 - \alpha_i)
\end{aligned}
\end{equation*}
% When shifting from the original dataset $\mathcal{O}$ to the synthetic set $\mathcal{S}$, one can form a duality gap that depends on $\boldsymbol{\theta}_{\mathcal{O}}$ as:
According to Theorem  \ref{thm:parameter_bound}, the duality gap is formulated as follows 
\begin{equation*}
  \begin{aligned}
G_{\mathcal{S}}(\bm \theta_{\cO}^*,\tilde{\boldsymbol{\alpha}}_\mathcal{S}) = & \sum_{i=1}^{n_{\mathcal{S}}} \log[1+\exp(-y_i^\mathcal{S} f(\boldsymbol{x}_i^\mathcal{S}; \boldsymbol{\theta}_\mathcal{O}))] + \frac{\lambda}{2}\|\boldsymbol{\theta}_\mathcal{O}\|\\
& +\frac{1}{2\lambda}\sum_{i=1}^{n_\mathcal{S}} \sum_{j=1}^{n_\mathcal{S}} \tilde{\alpha}_i^\mathcal{S} \tilde{\alpha}_j^\mathcal{S}  y_i^\mathcal{S} y_j^\mathcal{S} k(\boldsymbol{x}_i^\mathcal{S},\boldsymbol{x}_j^\mathcal{S})\\
& +\sum_{i=1}^{n_{\mathcal{S}}}(\tilde{\alpha}_i^\mathcal{S}\ln\tilde{\alpha}_i^\mathcal{S} + (1-\tilde{\alpha}_i^\mathcal{S})\ln(1-\tilde{\alpha}_i^\mathcal{S})),
  \end{aligned}
\end{equation*}
where
\begin{equation*}
\tilde{\boldsymbol{\alpha}}_{\mathcal{S}} \in -\partial \ell\left(\boldsymbol{y}_\mathcal{S}, f (X_\mathcal{S};  \bm \theta_{\cO}^*)\right)= \sigma(-\boldsymbol{y}_\mathcal{S} f(X_\mathcal{S}; \bm \theta_{\cO}^*)).
\end{equation*}
Here, $\sigma$ is the sigmoid function. Minimizing this gap places the logistic-regression solution on $\mathcal{S}$ close to the corresponding solution on $\mathcal{O}$, thereby preserving classification performance on the original dataset. 

The DGKIP approach avoids bi-level optimization with a single-level optimization on duality gap. By drawing on fundamental results in convex analysis and duality, the proposed framework ensures that shrinking the duality gap brings the distilled solution $\boldsymbol{\theta}_{\mathcal{S}}$ closer to $\boldsymbol{\theta}_{\mathcal{O}}$ in parameter space. This method is broadly applicable to various smooth convex losses. Complete details of the SVM variant are described in Appendix \ref{DGKIP-SVM}.

\begin{algorithm}[t]
\caption{Dataset Distillation Framework}
\label{alg:distillation}
\begin{algorithmic}
    \Require Original dataset $\mathcal{O}$
    \Ensure Distilled dataset $\mathcal{S}$
    \State \textbf{Step 1: Train on } $\mathcal{O}$
        \State \quad Solve primal for LR: 
               ${\bm \theta_{\cO}^*}$ by \eqref{eq:theta_o-opt}
    \State \textbf{Step 2: Initialize } $\mathcal{S}$ \text{ (size $n_\mathcal{S}$)}
    \Repeat
        \State \textbf{Step 3:} Obtain approximate dual $\tilde{\boldsymbol{\alpha}}_\mathcal{S}$ for $\mathcal{S}$
        \State \textbf{Step 4:} Minimize $G_{\mathcal{S}}({\bm \theta_{\cO}^*}, \tilde{\boldsymbol{\alpha}}_\mathcal{S})$ 
                 w.r.t. $\mathcal{S}$
    \Until{convergence}
    \State {\bfseries return} $\mathcal{S}$
\end{algorithmic}
\end{algorithm}