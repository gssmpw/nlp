\section{Conclusion}
\label{Conclusion}

We have presented Duality Gap KIP (DGKIP), a novel data distillation framework grounded in duality theory. By using the duality gap as an optimization objective, our approach not only avoids bi-level optimization but also generalizes the KIP method to a broader class of convex loss functions, such as hinge loss and cross-entropy. We provide a theoretical proof that minimizing the duality gap is equivalent to minimizing model parameters in solution space and further establishes upper bounds on prediction and test errors.

We construct two variants, DGKIP-SVM and DGKIP-LR, and compare them with existing dataset distillation methods on benchmark datasets. Experimental results show the effectiveness of DGKIP in terms of both classification accuracy and optimization speed. Additionally, we examine its transferability and discuss the impact of changes in the number of images generated per class and the choice of the NNGP kernel.


