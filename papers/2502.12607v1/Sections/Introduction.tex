\section{Introduction}
\label{Introduction}

Reducing the amount of training data while preserving model performance remains a fundamental challenge in machine learning. \emph{Dataset distillation} seeks to generate synthetic instances that encapsulate the essential information of the original data \cite{yu2023dataset}. This synthetic approach often proves more flexible and can potentially achieve greater data reduction than simply retaining subsets of actual instances. Such distilled datasets can also serve broader applications, for example by enabling efficient continual learning with reduced storage demands \cite{masarczyk2020reducing, rosasco2021distilled, enneng2023efficient}, and offering privacy safeguards through data corruption \cite{dong2022privacy,liu2023backdoor}.


\begin{figure}[t]
\vskip 0.2in
\centering
\includegraphics[width=\columnwidth]{img/fp.pdf}
\caption{Paradigm of dataset distillation with (a) Bi-level Optimization, (b) Kernel Inducing Points (KIP), and (c) proposed DGKIP. In each subfigure, $S$ means the synthetic data, $\theta_S$ means the model trained on $S$, and the arrow represents the optimization process. KIP method avoids bi-level optimization by simplified inner loop but restricted to square error loss, while DGKIP expands its paradigm to a large class of loss functions by evaluating the duality gap (DG) instead of $\bm\theta_\mathcal{S}$ itself.}
\label{solutionspace}
\vskip -0.2in
\end{figure}

Existing dataset distillation methods are essentially formulated as a bi-level optimization problem. This is because generating synthetic instances requires retraining the model with those instances as training data. Specifically, synthetic instances are created in the outer loop, and the model is trained in the inner loop, leading to high computational costs. A promising approach to avoid bi-level optimization is a method called \emph{Kernel Inducing Point (KIP)} \cite{nguyen2020dataset}. The KIP method avoids bi-level optimization by obtaining an analytical solution in its inner loop, effectively leveraging the fact that its loss function is a variant of squared loss. Although the KIP method is a type of kernel method, it can also be applied to dataset distillation in deep learning models by utilizing Neural Tangent Kernel (NTK) \cite{novak2019neural} or Neural-Network Gaussian Process (NNGP) ~\cite{lee2017deep}.

The main contribution of this paper is to extend the concept of the KIP method to a broader range of loss functions. Using the method proposed in this paper, dataset distillation can be performed without bi-level optimization, not only for squared loss but also for commonly used loss functions in classification problems, such as cross-entropy loss and hinge loss. The fundamental idea of the proposed method is to introduce the concept of \emph{Duality Gap (DG)} from the field of mathematical optimization. The DG is a measure that quantifies how close the current solution is to the optimal solution in an optimization problem. Our key idea is to quantify how close the solution based on synthetic instances is to the pre-distillation solution using the DG, enabling dataset distillation to be formulated as a single-level optimization problem. We call the proposed method \emph{Duality Gap KIP (DGKIP)}. Figure~\ref{solutionspace} illustrates the basic idea behind the proposed DGKIP method.

Our contributions are as follows:
\begin{enumerate} 
\item We propose a novel dataset distillation method that avoid bi-level optimization based on DG. Theoretical analysis shows that models trained on the distilled datasets maintain a bounded distance in parameter space compared to those trained on the full dataset. We could also use DG bound (DGB) to bound the prediction accuracy and test error.  
\item 
By using DG as the optimization objective, the DGKIP method extends its applicability to a class of convex loss functions beyond squared loss. Therefore, DGKIP can easily extend to hinge loss, cross entropy, and other classes of loss functions, thereby broadening KIPâ€™s applicability. 
\item Extensive experiments on three benchmarks including MNIST and CIFAR-10, show the effectiveness about test accuracy and time cost of DGKIP. The code for the proposed method and the scripts to reproduce the experiments are provided as supplementary material.
\end{enumerate}


\subsection{Related Works}
Dataset distillation methods aim to generate small synthetic versions of large datasets without sacrificing model accuracy. In this section, we first review dataset distillation methods based on bi-level optimization. Then we survey methods that simplify inner loop constraint. Finally, we discuss how duality gaps can be employed to guide data reduction.

\subsubsection{Dataset Distillation as bi-level optimization}
An early line of research introduced a bi-level optimization framework for dataset distillation \cite{wang2018dataset}. This framework consists of two main components: an inner loop that trains a model on synthetically generated data, and an outer loop that refines this synthetic data by evaluating performance on the original dataset.

This bi-level approach has demonstrated certain efficiency, achieving near-baseline performance with only a fraction of the original dataset. However, when applied to complex datasets, the benefits of the distilled synthetic data often diminish because of computational bottlenecks. In such a bi-level optimization framework, the outer loop depends on the parameters of the inner loop, so updating the synthetic data requires backpropagating through multiple unrolled steps of the inner optimization. This process is similar to backpropagation through time \cite{werbos1990backpropagation} in recurrent neural networks \cite{yu2019review}, where gradients must flow backward over potentially many iterations. As a result, the gradients reaching the outer loop can either explode or vanish, leading to training instabilities \cite{pascanu2013difficulty, vicol2021unbiased}.

\subsubsection{Inner Loop Simplification} 
While bi-level optimization has shown promise, it poses significant computational and stability challenges. Researchers focus on simplifying the inner loop in dataset distillation. An important line of research involves replacing the inner loop with kernel ridge regression (KRR) \citep{nguyen2020dataset, nguyen2021dataset, zhou2022dataset, loo2022efficient}, which often outperforms alternative methods. This is because the loss function for KRR is expressed in a quadratic form, allowing its solution to be derived analytically and thus avoiding bi-level optimization.

The Kernel Inducing Points (KIP) framework \cite{nguyen2020dataset} moved this line of research forward by harnessing expressive kernel functions, particularly neural tangent kernel (NTK) \cite{jacot2018neural}, to better approximate neural network behaviors. Building on this, Frepo \cite{zhou2022dataset} integrated neural feature extraction into ridge regression, achieving state-of-the-art results within the KRR paradigm.

However, KRR methods, especially with NTK, can be computationally expensive, often requiring 
high-cost computation for kernel matrix operations, even for simple kernels. When using NTK, the complexity further increases, making it challenging to scale. Loo et al. \cite{loo2022efficient} addressed this by introducing Random Feature Approximation (RFA), which replaces NTK with an NNGP kernel approximation \cite{lee2017deep}, reducing the computational cost.



\subsubsection{Duality Gap Bound}

The DG bound used in this study is derived from safe screening literature, a method developed for sparse modeling techniques such as Lasso or SVM \citep{ghaoui2010safe}.
%
Safe screening aims to identify the potential range of optimal model parameters, enabling the elimination of unnecessary features in Lasso \citep{wang2013lasso,ndiaye2017gap} or unnecessary instances in SVM \citep{ogawa2014safe,shibagaki2016simultaneous} before actually solving the optimization problem.
%
This concept has led to the development of DG-based techniques \citep{ndiaye2015gap}, which have been employed across various scenarios \citep{ndiaye2015gap,nakagawa2016safe,hanada2018efficiently,hanada2023generalized,zhai2020safe,dantas2021expanding}.
%
Particularly pertinent to this study is the application of DG bounds in distributionally robust coreset selection \citep{hanada2024distributionally,tanaka2025distributionally}.
%
Our technical contribution in this study lies in adapting the DG bound to enhance the dataset distillation method, KIP, broadening its compatibility with diverse loss functions.
