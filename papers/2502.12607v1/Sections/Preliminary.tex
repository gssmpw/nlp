\section{Preliminary}
\label{Preliminary}
%

In this section, we present the preliminary materials necessary for understanding the proposed DGKIP method.
%
First, in section \ref{subsec:primal_dual_formulation}, we formulate the primal and dual representations of the model considered in this study and define the duality gap (DG), which serves as the core concept of this research, based on these primal-dual representations.
%
Then, in section \ref{subsec:dataset_distillation}, we formulate the dataset distillation problem and outline the fundamental idea of the DGKIP method.


\subsection{Primal and Dual Formulation}
\label{subsec:primal_dual_formulation}
%

The proposed DGKIP method, like the KIP method, is formulated as a kernel function model, in which, by using NTK or NNGP kernels as the kernel function, data distillation for deep learning models becomes feasible.
%
In this section, to derive the kernel model, we first consider a linear model in a high-dimensional reproducing kernel Hilbert space (RKHS) as the primal problem and then derive the kernel function model as its dual problem.


Most machine learning methods can be viewed as regularized empirical risk minimization (ERM).
%
Let $\boldsymbol{x}_i \in \mathbb{R}^d, y_i \in \mathbb{R}(i=1,\ldots,n)$ represent the training inputs and their associated labels.
%
To derive a kernel-based model, let us first consider a linear model in a RKHS ${\cal H}$:
\begin{equation*}
 f(\bm x_i; \bm \theta) = \bm \phi(\bm x_i)^\top \bm \theta
\end{equation*}
where $\bm \phi: \mathbb{R}^d \rightarrow \mathcal{H}$ is a feature mapping function into the RKHS ${\cal H}$, and $\boldsymbol{\theta}$ represents the model parameters.
%
Given a loss function $\ell(y_i, f(\boldsymbol{x}_i; \boldsymbol{\theta}))$ and a regularization term $\psi(\boldsymbol{\theta})$, we define the optimal parameter $\boldsymbol{\theta}^*$ as
\begin{equation}
\boldsymbol{\theta}^* := \arg \min_{\boldsymbol{\theta} \in \mathbb{R}^d} P(\boldsymbol{\theta}),
\label{eq:primal}
\end{equation}
where
\begin{equation*}
 P(\boldsymbol{\theta}) := \sum_{i=1}^n \ell(y_i, f(\boldsymbol{x}_i;\boldsymbol{\theta})) + \psi(\bm \theta).
\end{equation*}
%
Here, both $\ell$ and $\psi$ are assumed to be convex and continuous.
%
In this paper, we focus on $L_2$ regularization term defined as $\psi(\boldsymbol{\theta})=\frac{\lambda}{2}\|\boldsymbol{\boldsymbol{\theta}}\|^2_2$ although the proposed method can go beyond the setting of $L_2$regularization, but it will be more complex to formulate.

We can invoke Fenchel duality to transform this primal problem into a corresponding dual problem. In the dual space, we introduce a dual variable $\alpha_i$ for each training instance.
%
Let $\ell^*$ and $\psi^*$ be the convex conjugates (Appendix \ref{app:Conjugate}) of $\ell$ and $\psi$, respectively. (For $\ell^*$, the convex conjugate is taken for the second argument of $\ell$).
%
Then, the dual problem can be written as
\begin{equation}
 \boldsymbol{\alpha}^* := \arg\max_{\boldsymbol{\alpha}\in\mathbb{R}^n} D(\boldsymbol{\alpha}),
  \label{eq:dual}
\end{equation}
where $D(\bm \alpha)$ is the dual objective defined as
\begin{equation*}
 D(\boldsymbol{\alpha}) := -\sum_{i=1}^n \ell^*(y_i, -\alpha_i) - \frac{1}{2\lambda}\sum_{i=1}^n \sum_{i=j}^n\alpha_i\alpha_j y_i y_j k (\boldsymbol{x}_i,\boldsymbol{x}_j).
\end{equation*}
%
Here, both $\ell$ and $\psi$ are assumed to be convex and continuous, and $k(\boldsymbol{x}_i,\boldsymbol{x}_j)= \bm \phi(\boldsymbol{x}_i)^\top \bm \phi(\boldsymbol{x}_j)$ is a kernel function.
%
% In this paper, we focus on $L_2$ regularization term defined as $\psi(\boldsymbol{\theta})=\frac{\lambda}{2}\|\boldsymbol{\theta}\|^2_2$ although the proposed method can go beyond the setting of $L_2$regularization, but it will be more complex to formulate.
%
%
Under certain regularity conditions (e.g., Slater's condition), strong duality holds, meaning the optimal values of the primal and dual objectives coincide:
\begin{equation*}
 P(\boldsymbol{\theta}^*)=D(\boldsymbol{\alpha^*}).
\end{equation*}

Furthermore, the Karush–Kuhn–Tucker (KKT) conditions ensure that, the primal and dual solutions satisfy certain subgradient relationships at the optimum, such as
\begin{equation}
 \boldsymbol{\theta}^* = \frac{1}{\lambda}\sum_{i=1}^n \alpha_i y_i \bm \phi(\boldsymbol{x}_i),\quad-\boldsymbol{\alpha}^* \in \partial\ell(\boldsymbol{y}, f(X ;  \boldsymbol{\theta}^*)),
 \label{eq:ktt1}
\end{equation}
where $\partial \ell(\cdot)$ is the subgradient for the second argument.
%
Using the dual variables, the classifier $f$ is written as a kernel model in the form of 
\begin{equation*}
 f(\boldsymbol{x};\boldsymbol{\theta})=\sum_{i=1}^n \alpha_i y_i k\left(\boldsymbol{x}_i, \boldsymbol{x}\right)
\end{equation*}
%
For any primal solution $\boldsymbol{\theta}$ and dual solution $\boldsymbol{\alpha}$, the DG is defined as
\begin{equation}
 {\rm DG}(\bm \theta, \bm \alpha) := P(\boldsymbol{\theta}) - D(\boldsymbol{\alpha}).
\label{eq:duality_gap}
\end{equation}
%
This gap indicates how close the current solution is to the optimal solution; it reaches zero if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}^*$ and $\boldsymbol{\alpha} = \boldsymbol{\alpha}^*$ are both optimal.

\subsection{Dataset Distillation}
\label{subsec:dataset_distillation}
%

In this study, we consider dataset distillation for a binary classification problem.
%
In order to differentiate the datasets before and after dataset distillation, let us denote the original dataset as $\cO = \left(X_{\cO}, \bm y_{\cO}\right) = \left\{(\bm x_i^{\cO}, y_i^{\cO}) \right\}_{i=1}^{n_{\cO}}$ where $n_{\cO}$ is the number of instances, $X_\cO \in \RR^{n_{\cO} \times d}$ is the input matrix, and $\bm y_{\cO} \in \{-1, +1\}^{n_{\cO}}$ is the corresponding label vector for the original dataset.
%
Given the model with parameter $\bm \theta$, the classification error on the training set is defined as 
\begin{equation*}
 {\rm TrEr}_\cO(\bm \theta)
 =
 \frac{1}{n_\cO}
 \sum_{i=1}^{n_\cO}
 I(y^\cO_i \neq {\rm sgn}\left( f(\bm x^\cO_i; \bm \theta)\right)),
\end{equation*}
where $I(\cdot)$ is the indicator function that returns 1 if the argument is true, while 0 otherwise, and ${\rm sgn}(\cdot)$ is the sign function that returns the sign of the argument.
%
For a test dataset, which follows the same distribution as the original training dataset, classification error on the test is similarly defined as ${\rm TeEr}_\cO(\bm \theta)$~\footnote{
%
In this paper, we assume that reducing the training error by appropriately controlling the model capacity during training will also reduce the test error. Therefore, in deriving the methodology, we do not explicitly distinguish between them. However, in the experiments, we use the test error \({\rm TeEr}_\cO(\bm \theta)\) as the evaluation metric.
%
}.
%
Therefore, the goal is to find the model parameter $\bm \theta$ that minimize ${\rm TrEr}_{\cO}(\bm \theta)$.%, which we denote



However, in the ordinary classification problem, since it is difficult to minimize the non-differentiable classification error directly, the model parameter $\bm{\theta}$ is trained to minimize the surrogate loss function, such as cross-entropy loss or hinge loss, on the original training dataset, i.e.,
\begin{equation}
 \bm \theta_{\cO}^*
 =
 \argmin_{\bm \theta}
 \sum_{i=1}^{n_{\cO}}
 \ell
 \left(
 y^{\cO}_i, f(\bm x^{\cO}_i; \bm \theta)
 \right)
 +
 \psi\left(\bm \theta\right).
\label{eq:theta_o-opt}
\end{equation}
%
This is justified because minimizing loss functions tailored to classification problem, such as cross-entropy loss or hinge loss, leads to the minimization of classification error ${\rm TrEr}_{\cO}(\bm \theta)$ and ${\rm TeEr}_{\cO}(\bm \theta)$.



In dataset distillation setting, our goal is to generate a much smaller synthetic dataset denoted as $\cS=\left(X_{\cS}, \bm y_{\cS}\right)=\left\{(\bm x^{\cS}_i, y^{\cS}_i)\right\}_{i=1}^{n_{\cS}}$ such that the number of reduced synthetic dataset size $n_{\cS} \ll n_{\cO}$.
%
With the reduced synthetic dataset, the model parameter is trained in the same way as
\begin{equation}
 \label{eq:theta_s-opt}
 \bm \theta_{\cS}^*
 =
 \argmin_{\bm \theta}
 \sum_{i=1}^{n_{\cS}}
 \ell
 \left(
 y^{\cS}_i, f(\bm x^{\cS}_i; \bm \theta)
 \right)
 +
 \psi\left(\bm \theta\right),
\end{equation}
%
Unfortunately, $\bm \theta_\cS^*$ is not guaranteed to lead to the minimization of classification error on the original dataset, i.e., ${\rm TrEr}_\cO(\bm \theta)$ or ${\rm TeEr}_\cO(\bm \theta)$.
%
Therefore, the goal of dataset distillation is to find $\cS$ such that the $\bm \theta^*_\cS$ minimizes the classification error on the original dataset, which is formally written as 
\begin{equation}
 \label{eq:bi-level-opt}
 \cS^* = \argmin_{\cS} {\rm TrEr}_\cO(\bm \theta^*_\cS).
\end{equation}
%
Note that the optimization problem in \eq{eq:bi-level-opt} is a bi-level optimization problem because the objective function includes \(\bm{\theta}_\cS^*\), which is the optimal solution of another optimization problem in \eq{eq:theta_s-opt}.
%
%Furthermore, it is worth noting that the inner-loop optimization problem in \eq{eq:theta_s-opt} is continuous optimization problem, while the outer-loop optimization problem in \eq{eq:bi-level-opt} is discrete optimization problem. 
%
As mentioned in \S\ref{Introduction}, since directly solving the bi-level optimization problem in \eq{eq:bi-level-opt} is challenging, various approaches have been proposed to approximate its solution.

The basic idea behind the proposed DGKIP method is to find the synthetic dataset $\cS$ that minimize \emph{an upper bound} of the difference between $\bm \theta_\cO^*$ and $\bm \theta_\cS^*$, which can be obtained without actually solving the inner-loop optimization problem in \eq{eq:theta_s-opt}.
%
This approach is justified in the sense that the minimizing \emph{an upper bound} of the difference between $\bm \theta_\cO^*$ and $\bm \theta_\cS^*$ leads to minimizing an upper bound of ${\rm TrEr}_\cO(\bm \theta)$ and ${\rm TeEr}_\cO(\bm \theta)$.


%We may directly minimize this with respect to $\mathcal{S}$, however, this minimization is difficult since it involves a bi-level optimization. To make this easier, since we may expect that $\boldsymbol{\theta}_\mathcal{O}$ produces low loss, we instead optimize $\boldsymbol{\theta}_\mathcal{S}$ so that it becomes similar to $\boldsymbol{\theta}_\mathcal{O}$, that is,
% which is similar to $\boldsymbol{\theta}_\mathcal{O}$ on the original dataset $\mathcal{O}$ (i.e., aiming for a high compression ratio while preserving model performance)."
% $\mathbb{E}_{\boldsymbol{x} \sim P_{\mathcal{D}}}\left[\ell(y, f(\phi(\boldsymbol{x}); \boldsymbol{\theta}_\mathcal{O})) \right] \simeq 
% \mathbb{E}_{\boldsymbol{x} \sim P_{\mathcal{D}}}\left[\ell(y, f(\phi(\boldsymbol{x}); \boldsymbol{\theta}_\mathcal{S})) \right] $
% Usually, to achieve this goal, one will need to optimize a model on $\mathcal{S}$ with a specific loss function $\mathcal{L}_\mathcal{S}$, and then use the classification loss on $\boldsymbol{\theta}_\mathcal{S}$  to optimize $\mathcal{S}$. This will form a bi-level optimization problem as:
% \begin{gather*}
%     \min_{\mathcal{S}} \mathcal{L}_\mathcal{O}(\boldsymbol{\theta}_\mathcal{S}) \\
%     \boldsymbol{\theta}_\mathcal{S}= \arg \min_{\boldsymbol{\theta}}  \mathcal{L}_\mathcal{S}(y_\mathcal{S}, f(\phi(X_\mathcal{S}); \boldsymbol{\theta}))
% \end{gather*}
% where $\mathcal{L}_\mathcal{O}(\boldsymbol{\theta}_\mathcal{S}) = \frac{1}{n_\mathcal{O}}\sum_{i=1}^{N_O} \ell(y_i, f(\phi(\boldsymbol{x}_i); \boldsymbol{\theta}_\mathcal{S}))$.
% Note that $\boldsymbol{\theta}_\mathcal{S}$ needs to be retrained every time the synthetic dataset $ \mathcal{S} $ is updated.
% % \mathcal{L}^{\mathcal{S}}(\boldsymbol{\theta})=\frac{1}{|\mathcal{S}|} \sum_{(s, y) \in \mathcal{S}} \ell\left(\phi_{\boldsymbol{\theta}}(\boldsymbol{s}), y\right)
%  % In this work, instead optimize the loss on the training set or training accuracy, 
% In this work, we aim for $\boldsymbol{\theta}_\mathcal{S}$ to converge to a solution similar to $ \boldsymbol{\theta}_\mathcal{O}$:
%\begin{equation}
%  \min_{\mathcal{S}} \bigl\| \boldsymbol{\theta}_\mathcal{O} - \boldsymbol{\theta}_\mathcal{S} \bigr\|_2^2,
%   \label{eq:parameter_dist}
%\end{equation}
% where 
% \begin{equation*}
%      \boldsymbol{\theta}_\mathcal{S}= \arg \min_{\boldsymbol{\theta}}  \mathcal{L}_\mathcal{S}(y_\mathcal{S}, f(X_\mathcal{S}; \boldsymbol{\theta})).
% \end{equation*}
%
%Therefore, it is expected that the generalization performance will also be similar. However, this optimization is still a bi-level optimization.  
%%
%The trick of the proposed method is to instead minimize the \emph{duality gap} (DG) between $\boldsymbol{\theta}_\mathcal{O}$ and $\boldsymbol{\theta}_\mathcal{S}$ (Section \ref{thm:parameter_bound}). DG can provide an upper bound of $\|\boldsymbol{\theta}_\mathcal{O} - \boldsymbol{\theta}_\mathcal{S}\|_2$ (equation \eqref{eq:parameter_dist}), so minimizing DG directly leads to minimize $\|\boldsymbol{\theta}_\mathcal{O} - \boldsymbol{\theta}_\mathcal{S}\|_2$. DG is easily computed by the difference between the primal and dual objective function values of a convex problem. So our aim is turned to compute  
%%
%\[
%\min_\mathcal{S} (\text{DG between}~\boldsymbol{\theta}_\mathcal{O}~\text{and}~\boldsymbol{\theta}_\mathcal{S}),
%\]
%%
%which can avoid the bi-level optimization.
%
%     Usually, to achieve this goal, one will need to optimize a model on $\mathcal{S}$ with a specific loss function $\mathcal{L}_\mathcal{S}$
% Therefore, it is expected that the generalization performance will also be similar.
% However, since this optimization is a bi-level optimization, the computational cost is high. Therefore, we seek its upper bound, the duality gap (DG). DG is obtained by calculating the distance between the primal and dual solutions of a convex problem. We then conduct dataset distillation by minimizing DG.
% \[
%     \min_{X_\mathcal{S}, y_\mathcal{S}}  \textrm{DG}
% \]
% which can avoid the bi-level optimization.
% When training data changes, the underlying optimization problem will change, causing the previously optimal dual parameters to no longer satisfy optimality. Consequently, the duality gap becomes non-zero. 
%
%We show that by minimizing DG, we can first constrain the model parameters and then further constrain both training and test accuracy.
%
%In DGKIP, we focus on regularized empirical risk minimization.

\subsection{How does KIP generalize to DGKIP?}
KIP considers the problem in \eqref{eq:theta_o-opt} with $L_2$ regularization (KRR) optimized by mean square error (MSE) loss. Under this setting, the optimization problem can be formulated as:
% \begin{equation}
%  \label{eq:theta_s-opt}
%  \bm \theta_{\cS}^*
%  =
%  \argmin_{\bm \theta}
%  \sum_{i=1}^{n_{\cS}}
%  \ell
%  \left(
%  y^{\cS}_i, f(\bm x^{\cS}_i; \bm \theta)
%  \right)
%  +
%  \psi\left(\bm \theta\right),
% \end{equation}
\begin{equation*}
    \min_{\bm \theta_{\cS}}
    \sum_{i=1}^{n_{\cO}}\bigl\|y^{\cO}_i - \bm \phi(x^{\cO}_i)^\top \bm \theta_{\cS} \bigr\|_2^2, 
\end{equation*}
where the optimal solution $\bm \theta_{\cS}^*$ is calculated in closed form as
\begin{equation*}
\bm \theta_{\cS}^* = \bm \phi(X_\mathcal{S}) (\bm \phi(X_\mathcal{S})^\top \bm \phi(X_\mathcal{S}) + \lambda I)^{-1} y_{\mathcal{S}}.
\end{equation*}
This approach then eliminates the need for bi-level optimization.  
By replacing $ y_\mathcal{O} $ with $ \bm \phi(X_\mathcal{O})^\top \boldsymbol{\theta}_\mathcal{O}^* $, we immediately obtain the following equivalence:
\[
\min_{\mathcal{S}} 
\sum_{i=1}^{n_{\cO}}
\bigl\|\boldsymbol{\theta}_\mathcal{O}^* - \boldsymbol{\theta}_\mathcal{S}^* \bigr\|_2^2 \| \bm \phi(x^{\cO}_i)\|^2 \propto \min_{\mathcal{S}} \bigl\|\boldsymbol{\theta}_\mathcal{O}^* - \boldsymbol{\theta}_\mathcal{S}^* \bigr\|_2^2.
\]
This result directly aligns with our formulation, which aims to minimize the discrepancy between model parameters. In section \ref{Methods}, we prove that the upper bound of 
$\bigl\|\bm \theta_{\cO}^* - \bm \theta_{\cS}^* \bigr\|_2^2$ can be optimized by DG in \eqref{eq:duality_gap} and apply to a boarder class of convex loss functions.










