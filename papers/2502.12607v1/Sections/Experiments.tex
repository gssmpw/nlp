\section{Experiments}
\label{Experiments}

\begin{table*}[p]
\caption{Distillation results on three datasets with varying synthetic dataset sizes. Convolutional neural network random features are extracted for DGKIP-SVM and DGKIP-LR. Bolded numbers indicate the best performance, and underlined numbers indicate the second best performance. All the methods used learned labels. Training times for each method are reported in seconds.}
\label{kernel-distillation-conv}
\begin{center}
\begin{sideways}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccccccccccc}
\toprule
Dataset & Img/Cls & Metric & DD & DC & DSA & DGKIP-SVM & DGKIP-LR \\
\midrule
\multirow{6}{*}{MNIST} 
& \multirow{2}{*}{1} & Acc. & 99.56 $\pm$ 0.16 & 98.96$\pm$0.81 & 98.79$\pm$0.99 & \textbf{99.97$\pm$0.02} & \underline{99.75$\pm$0.02} \\
& & Time & 0.237 & 0.186 & 0.192 & \textbf{0.007} & \underline{0.009} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & 99.63 $\pm$ 0.11 & 98.25$\pm$0.56 & \underline{99.77$\pm$0.08} & \textbf{99.99$\pm$0.02} & 99.76$\pm$0.00 \\
& & Time & 0.244 & 0.196 & 0.206 & \underline{0.012} & \textbf{0.011} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & 99.72 $\pm$ 0.11 & 98.96$\pm$0.91 & \underline{99.81$\pm$0.07} & \textbf{99.96$\pm$0.06} & 99.75$\pm$0.02 \\
& & Time & 0.602 & 0.217 & 0.434 & \underline{0.041} & \textbf{0.040} \\
\midrule
\multirow{6}{*}{CIFAR-10}
& \multirow{2}{*}{1} & Acc. & 78.04 $\pm$ 0.06 & 71.58$\pm$1.16 & 69.43$\pm$2.24 & \textbf{85.63$\pm$0.97} & \underline{83.18$\pm$2.68} \\
& & Time & 0.245 & 0.186 & 0.190 & \textbf{0.011} & \textbf{0.011} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & 77.58 $\pm$ 0.02 & 86.47$\pm$0.66 & 82.91$\pm$0.91 & \textbf{90.83$\pm$0.14} & \underline{89.72$\pm$0.10} \\
& & Time & 0.249 & 0.186 & 0.223 & \textbf{0.015} & \textbf{0.015} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & 76.91 $\pm$ 0.02 & \underline{90.45$\pm$0.47} & 88.10$\pm$0.61 & \textbf{91.17$\pm$0.72} & 90.01$\pm$0.02 \\
& & Time & 0.298 & 0.202 & 0.376 & \textbf{0.051} & \textbf{0.051} \\
\midrule
\multirow{6}{*}{Fashion-MNIST}
& \multirow{2}{*}{1} & Acc. & 95.96 $\pm$ 0.06 & 95.13$\pm$0.50 & 96.16$\pm$1.07 & \textbf{97.35$\pm$0.39} & \underline{96.70$\pm$0.71} \\
& & Time & 0.295 & 0.123 & 0.184 & \textbf{0.008} & \underline{0.009} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & 95.72 $\pm$ 0.18 & 98.01$\pm$0.41 & 97.30$\pm$0.36 & \textbf{98.70$\pm$0.15} & \underline{98.32$\pm$0.11} \\
& & Time & 0.353 & 0.139 & 0.213 & \textbf{0.011} & \textbf{0.011} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & 95.99 $\pm$ 0.04 & \textbf{98.82$\pm$0.15} & \underline{98.39$\pm$0.35} & 98.57$\pm$0.25 & 98.43$\pm$0.02 \\
& & Time & 0.597 & 0.203 & 0.415 & \textbf{0.041} & \textbf{0.041} \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{sideways}
\end{center}
\end{table*}

 \begin{table*}[p]
\caption{Distillation results on three datasets with varying synthetic dataset sizes. Fully connected neural network random features are extracted for DGKIP-SVM and DGKIP-LR. Bolded numbers indicate the best performance, and underlined numbers indicate the second best performance. All the methods used learned labels. Training times for each method are reported in seconds.}
\label{kernel-distillation-mlp}
\begin{center}
\begin{sideways}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccccccccccc}
\toprule
Dataset & Img/Cls & Metric & DD & DC & DSA & DGKIP-SVM & DGKIP-LR \\
\midrule
\multirow{6}{*}{MNIST} 
& \multirow{2}{*}{1} & Acc. & \textbf{99.91$\pm$0.00} & 97.11$\pm$1.55 & 99.60$\pm$0.09 & \underline{99.90$\pm$0.02} & 94.44$\pm$6.95 \\
& & Time & 0.128 & 0.065 & 0.069 & \underline{0.026} & \textbf{0.025} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & \underline{99.86$\pm$0.00} & 98.44$\pm$0.87 & 99.77$\pm$0.09 & \textbf{99.93$\pm$0.02} & 99.65$\pm$0.35 \\
& & Time & 0.132 & 0.067 & 0.151 & \textbf{0.025} & \underline{0.026} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & \underline{99.91$\pm$0.00} & 99.78$\pm$0.11 & 99.89$\pm$0.08 & \textbf{99.92$\pm$0.03} & 99.83$\pm$0.02 \\
& & Time & 0.133 & 0.072 & 0.326 & \textbf{0.026} & \underline{0.028} \\
\midrule
\multirow{6}{*}{CIFAR-10}
& \multirow{2}{*}{1} & Acc. & 75.90$\pm$0.00 & 71.45$\pm$0.83 & 71.73$\pm$0.73 & \underline{80.22$\pm$3.35} & \textbf{83.63$\pm$1.95} \\
& & Time & 0.137 & 0.061 & 0.069 & \underline{0.028} & \textbf{0.026} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & 77.00$\pm$0.00 & 80.05$\pm$0.92 & 80.58$\pm$0.87 & \textbf{88.23$\pm$0.38} & \underline{86.85$\pm$0.46} \\
& & Time & 0.131 & 0.063 & 0.137 & \textbf{0.025} & \underline{0.026} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & 76.40$\pm$0.00 & 77.37$\pm$1.15 & 76.32$\pm$0.98 & \textbf{89.49$\pm$0.27} & \underline{87.53$\pm$0.04} \\
& & Time & 0.137 & 0.065 & 0.328 & \textbf{0.026} & \underline{0.028} \\
\midrule
\multirow{6}{*}{Fashion-MNIST}
& \multirow{2}{*}{1} & Acc. & \textbf{98.25$\pm$0.00} & 88.27$\pm$1.03 & 88.38$\pm$1.07 & \underline{97.77$\pm$0.89} & 89.01$\pm$5.58 \\
& & Time & 0.126 & 0.063 & 0.069 & \underline{0.026} & \textbf{0.025} \\
\cmidrule{2-8}
& \multirow{2}{*}{10} & Acc. & \underline{98.40$\pm$0.00} & 97.96$\pm$0.32 & 97.77$\pm$0.34 & \textbf{98.95$\pm$0.08} & 98.14$\pm$0.08 \\
& & Time & 0.127 & 0.063 & 0.137 & \textbf{0.025} & \underline{0.026} \\
\cmidrule{2-8}
& \multirow{2}{*}{50} & Acc. & 97.95$\pm$0.00 & 98.54$\pm$0.13 & 98.47$\pm$0.16 & \textbf{98.89$\pm$0.12} & \underline{98.63$\pm$0.04} \\
& & Time & 0.131 & 0.064 & 0.348 & \textbf{0.026} & \underline{0.028} \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{sideways}
\end{center}
\end{table*}


% \begin{table*}[t]
%   \caption{Distillation results on four datasets with varying synthetic dataset sizes. Bolded numbers indicate the best performance, and underlined numbers indicate the second best performance. All the methods used learned labels. Training times for each method are reported in seconds.}
%   \label{kernel-distillation-mlp}
%   \vskip 0.15in
%   \begin{center}
%     \begin{scriptsize}
%       \begin{sc}
%         \begin{tabular}{lcccccccccccc}
%           \toprule
%            & Img/Cls & \multicolumn{2}{c}{DD}     & \multicolumn{2}{c}{DC} & \multicolumn{2}{c}{DSA} & \multicolumn{2}{c}{DGKIP-SVM} & \multicolumn{2}{c}{DGKIP-LR}                                                        \\
%           \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
%            &         & Acc.                       & Time                   & Acc.                    & Time                          & Acc.                         & Time & Acc.           & Time & Acc.           & Time \\
%           \midrule
%           \multirow{3}{*}{MNIST}
%            & 1       & \textbf{99.91$\pm$0.00} & 0.13                   & 97.11$\pm$1.55          & 0.07                          & 99.60$\pm$0.09               & 0.07 & \underline{99.90$\pm$0.02} & 0.03 & 94.44$\pm$6.95 & 0.03 \\
%            & 10      & \underline{99.86$\pm$0.00} & 0.13                   & 98.44$\pm$0.87          & 0.07                          & 99.77$\pm$0.09               & 0.15 & \textbf{99.93$\pm$0.04} & 0.03 & 99.65$\pm$0.35 & 0.03 \\
%            & 50      & \underline{99.91$\pm$0.00}             & 0.13                   & 99.78$\pm$0.11          & 0.07                          & 99.89$\pm$0.08               & 0.33 & \textbf{99.92$\pm$0.04} & 0.03 & 99.83$\pm$0.02 & 0.03 \\
%           \midrule
%           \multirow{3}{*}{CIFAR-10}
%            & 1       & 75.90$\pm$0.00    & 0.14                   & 71.45$\pm$0.83          & 0.06                          & 71.73$\pm$0.73               & 0.07 & \underline{80.22$\pm$3.35} & 0.03 & \textbf{83.63$\pm$1.95} & 0.03 \\
%            & 10      & 77.00$\pm$0.00             & 0.13                   & 80.05$\pm$0.92          & 0.06                          & 80.58$\pm$0.87               & 0.14 & \textbf{88.23$\pm$0.38} & 0.03 & \underline{86.85$\pm$0.46} & 0.03 \\
%            & 50      & 76.40$\pm$0.00             & 0.14                   & 77.37$\pm$1.15          & 0.07                          & 76.32$\pm$0.98               & 0.33 & \textbf{89.49$\pm$0.27} & 0.03 & \underline{87.53$\pm$0.04} & 0.03 \\
%           \midrule
%           \multirow{3}{*}{Fashion-MNIST}
%            & 1       & \textbf{98.25$\pm$0.00} & 0.13                   & 88.27$\pm$1.03          & 0.06                          & 88.38$\pm$1.07               & 0.07 & \underline{97.77$\pm$0.89} & 0.01 & 89.01$\pm$5.58 & 0.01 \\
%            & 10      & \underline{98.40}$\pm$0.00 & 0.13                   & 97.96$\pm$0.32          & 0.06                          & 97.77$\pm$0.34               & 0.14 & \textbf{98.95$\pm$0.08} & 0.01 & 98.14$\pm$0.08 & 0.01 \\
%            & 50      & 97.95$\pm$0.00             & 0.13                   & 98.54$\pm$0.13          & 0.06                          & 98.47$\pm$0.16               & 0.35 & \textbf{98.89$\pm$0.12} & 0.04 & \underline{98.63$\pm$0.04} & 0.04 \\
%           \bottomrule
%         \end{tabular}
%       \end{sc}
%     \end{scriptsize}
%   \end{center}
%   \vskip -0.1in
% \end{table*}


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
    \begin{minipage}{0.32\textwidth}
        \centerline{\includegraphics[width=\columnwidth]{img/plot_lr0.001_ipc=1.pdf}}
        \centerline{\raisebox{1pt}{(a) IPC=1}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centerline{\includegraphics[width=\columnwidth]{img/plot_lr0.001_ipc=10.pdf}}
        \centerline{\raisebox{1pt}{(b) IPC=10}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centerline{\includegraphics[width=\columnwidth]{img/plot_lr0.001_ipc=50.pdf}}
        \centerline{\raisebox{1pt}{(c) IPC=50}}
    \end{minipage}
\caption{Parameter deviation, duality gap, and test accuracy varying cross training process with 1, 10, 50 IPC on CIFAR10. The parameter deviation in the green line (left-hand side of Eq.~\eqref{eq:dualitygap}) and the duality gap in the blue line (right-hand side of Eq.~\eqref{eq:dualitygap}) show the same pattern. Minimizing the duality gap reduces the parameter deviation, leading to an increase in test accuracy.}
\label{learning_process}
\end{center}
\end{figure*}


In this section, we first demonstrate the performance of DGKIP on benchmark datasets and compare it with existing data distillation methods. We analyze the method's performance in terms of test accuracy and speed. We also conducted an ablation study on the model used for random feature approximation and the transfer ability when the model used in training is different from the model used in testing. Details setting is presented in Appendix \ref{exp_setting}.

\subsection{Quantitative Results}
To apply the DGKIP method to deep learning models,
% as is done in the KIP method,
we kernelize the method and employ an NNGP kernel equivalent to the target deep learning model. Specifically, We compared our method against established baseline approaches that employed cross-entropy, including DD \cite{wang2018dataset}, DC \cite{zhao2020dataset}, and DSA\cite{zhao2021dataset}. 

\paragraph{Benchmarks.} We applied our algorithm to three datasets: MNIST, Fashion-MNIST and CIFAR-10 \cite{krizhevsky2009learning, lecun2010mnist, xiao2017fashion}, distilling the datasets to synthetic datasets with 1, 10 or 50 images per class (IPC).

Table \ref{kernel-distillation-conv} and Table \ref{kernel-distillation-mlp} present comparative results on those benchmarks. We evaluate the DGKIP implemented in Support Vector Machine (SVM) and Logistic Regression (LR), named DGKIP-SVM and DGKIP-LR. The model used to extract random features are fully connected network (FCNet) and convolutional nerual network (ConvNet), details are also presented in Appendix \ref{exp_setting}. We conducted binary classification experiments based on classes 0 and 1. The test accuracy and training time of every iteration are reported. While cross entropy based approaches (DD, DSA, and DC) show substantial increases in training time as IPC grows, both DGKIP variants maintain remarkably low computational overhead throughout. 

Figure \ref{learning_process} illustrates the learning process of DGKIP based using ConvNet with different numbers of IPC, showing the relationship between parameter deviation $\bigl\|\boldsymbol{\theta}_{\mathcal{O}}-\boldsymbol{\theta}_{\mathcal{S}}\bigr\|_2/\bigl\|\boldsymbol{\theta}_{\mathcal{O}}\bigl\|_2$, its upper bound represented by the duality gap$\sqrt{(2 / \lambda) G_{\mathcal{S}}(\boldsymbol{\theta}_{\mathcal{O}}, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}})}/\|\boldsymbol{\theta}_{\mathcal{O}}\bigl\|_2$, and test accuracy. As training progresses, we observe that the duality gap (blue line) consistently decreases, leading to a reduction in the parameter deviation (green line), which ultimately results in improved test accuracy (red line). While test accuracy does not increase steadily when IPC is low (1 IPC), the optimization of the duality gap  remains efficient across all settings (1, 10, and 50 IPC). The results indicate that minimizing the duality gap effectively guides the distillation process toward better model performance, particularly with reasonable IPC values.

Although the basic SVM model restricts the DGKIP-SVM framework to binary classification tasks, we ensured fairness in experimental comparisons by adopting the same binary classification setup when evaluating other methods. Also, the base model for training are set to the same one, e.g. FCNet, ConvNet.

It is important to emphasize that the application scope of the DGKIP method itself is not limited to binary classification. When DGKIP is combined with models supporting multiclass classification, it can also achieve multiclass functionality. This is because DGKIP is fundamentally a generalized approach for Kernel Inducing Points, and its specific applicability depends on the chosen convex method. 



% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
%     \begin{minipage}{0.48\columnwidth}
%         \centerline{\includegraphics[width=\columnwidth]{img/time_mnist.pdf}}
%         \centerline{\raisebox{1pt}{(a) MNIST}}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.48\columnwidth}
%         \centerline{\includegraphics[width=\columnwidth]{img/time_cifar.pdf}}
%         \centerline{\raisebox{1pt}{(a) CIFAR10}}
%     \end{minipage}
% \caption{Comparison of per-iteration training time across different dataset distillation methods with varying images per class (Img/Cls) on MNIST (a) and CIFAR-10 (b).}
% \label{timecost}
% \end{center}
% \vskip -0.2in
% \end{figure}


\begin{table}[t]
\caption{Transfer experiment results (Algorithm before/after $\rightarrow$ indicates distillation/evaluation)}
\label{transfertest}
\begin{center}
\begin{tabular}{lccc}
\toprule
Dataset & Img/Cls & SVM$\to$LR & LR$\to$SVM \\
\midrule
MNIST & 1 & 99.97±0.02 & 99.76±0.00 \\
& 10 & 99.98±0.02 & 99.76±0.00 \\
& 50 & 99.93±0.02 & 99.78±0.04 \\
\midrule
F-MNIST & 1 & 97.30±0.39 & 96.72±0.69 \\
& 10 & 98.63±0.18 & 98.39±0.07 \\
& 50 & 98.36±0.11 & 98.58±0.08 \\
\midrule
CIFAR-10 & 1 & 85.59±1.04 & 83.23±2.60 \\
& 10 & 85.06±1.52 & 89.73±0.18 \\
& 50 & 88.21±1.09 & 90.41±0.11 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\subsection{Transfer Experiments}

To evaluate whether DGKIP is robust to model changes, we conduct transfer experiments where the model used for training is different from testing. The result are shown in Table \ref{transfertest}. The findings show that DGKIP-SVM-distilled data can be successfully utilized by DGKIP-LR, retaining nearly the same accuracy as when evaluated with SVM. Note that we used ConvNet in both models. A similar outcome is observed when DGKIP-LR-distilled data is used to train SVM, with no significant loss in performance. These results suggest that the distilled datasets preserve essential information across different model types, demonstrating consistent cross-model transferability.

\subsection{Discussion}


DGKIP optimizes the distilled dataset by fast compute the duality gap (DG), which measures the distance between the solution fitted to the synthetic data and the solution fitted to the original data. Intuitively, larger DG values correspond to greater changes in the new dataset's features and instance number, potentially introducing instability during convergence. This is obvious when the number of IPCs is set to differ greatly (e.g. 1 vs 50), as this can lead to variations in convergence speed, sometimes exceeding a factor of two. Furthermore, DGKIP is closely tied to the choice of convex loss function. If the loss functions used during training and testing are different, it can result in a performance drop.