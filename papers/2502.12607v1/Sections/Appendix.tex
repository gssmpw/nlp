\newpage
\appendix


\onecolumn
\section{Fenchel Duality Theory}
\label{App:StrongConvex}
\subsection{$\lambda$-Strong Convexity and $\nu$-Smoothness}
A convex function $f:R^n \to R\cup\{+\infty\}$ is called $\lambda$-strongly convex ($\lambda>0$) if, for any $\boldsymbol{x},\boldsymbol{y}$ in its domain and any subgradient $\boldsymbol{g}\in\partial f(\boldsymbol{y})$,
\begin{equation*}
f(\boldsymbol{x}) - f(\boldsymbol{y}) \geq \boldsymbol{g}^\top(\boldsymbol{x}-\boldsymbol{y}) + \frac{\lambda}{2} \|\boldsymbol{x}-\boldsymbol{y}\|_2^2.
\label{eq:lambdaconvex}
\end{equation*}
If $f$ is differentiable, this condition is equivalent to $\nabla^2f(\boldsymbol{x})\succeq\lambda I$, i.e., every eigenvalue of the Hessian is at least $\lambda$. Intuitively, $\lambda$-strong convexity enforces a lower bound on the curvature of $f$, guaranteeing that its minimizer is unique and that gradient-based methods can converge more quickly.

A differentiable function $f$ is called $\nu$-smooth ($\nu>0$) if its gradient is $\nu$-Lipschitz continuous:
\begin{equation*}
\|\nabla f(\boldsymbol{x})-\nabla f(\boldsymbol{y})\|_2 \leq \nu \|\boldsymbol{x}-\boldsymbol{y}\|_2, \quad \forall\,\boldsymbol{x},\boldsymbol{y}.
\label{eq:nusmooth}
\end{equation*}


Equivalently, if $f$ is twice differentiable, $\nabla^2f(\boldsymbol{x})\preceq\nu I$ means the Hessian's eigenvalues are all at most $\nu$. Geometrically, $\nu$-smoothness caps how sharply $f$ can bend, preventing its gradient from changing too abruptly.

\subsection{Dual (Conjugate) Relationship}
\label{app:Conjugate}
For a proper, closed, convex function $f$, the conjugate $f^*$ is defined by
\begin{equation*}
f^*(\boldsymbol{y}) = \sup_{\boldsymbol{x}\in\text{dom}(f)}(\langle \boldsymbol{x},\boldsymbol{y}\rangle - f(\boldsymbol{x})).
\label{eq:convexconjugate}
\end{equation*}

It is a well-known result in convex analysis that:
\begin{itemize}
\item If $f$ is $\nu$-smooth, then $f^*$ is $\frac{1}{\nu}$-strongly convex.
\item If $f$ is $\lambda$-strongly convex, then $f^*$ is $\frac{1}{\lambda}$-smooth.
\end{itemize}

Hence, strong convexity in the primal translates to smoothness in the dual, and vice versa. This duality underpins many optimization algorithms that exploit both primal and dual formulations (e.g., proximal methods, mirror descent).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof}
\subsection{Derivation of  Duality Gap Bound}
\label{proof_theorem}
Let $P_\mathcal{S}$ be a $\lambda$ strongly convex primal problem for $\mathcal{S}$, and let $D_\mathcal{S}$ be its corresponding dual problem.
Given the optimal solution $\boldsymbol{\theta}_\mathcal{O}^* \in P_\mathcal{S}$ for the original data $\mathcal{O}$ and an arbitrary feasible solution $\tilde{\boldsymbol{\alpha}}_{\mathcal{S}} \in \mathrm{dom}(D_{\mathcal{S}})$ , the following relationship holds.
 \begin{equation}
    \begin{aligned}
\left\|\bm \theta^*_\cO -\bm \theta^*_\cS \right\|_2 \leq & \sqrt{\frac{2\left(P_{\mathcal{S}}\left(\bm \theta^*_\cO \right)-D_{\mathcal{S}}(\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})\right)} {\lambda}}
    \end{aligned}
  \end{equation}
\begin{proof}
We follow the calculation in \cite{ndiaye2015gap,shibagaki2016simultaneous}. 
Given the relation in \eqref{eq:lambdaconvex} in $\lambda$-Strong Convexity, we can have
 \begin{equation}
    \begin{aligned}
      \frac{\lambda}{2}\left\|\bm \theta^*_\cO-\bm \theta^*_\cS\right\|_2^2  & \leq P_{\mathcal{S}}\left(\bm \theta^*_\cO \right)- P_{\mathcal{S}}\left(\bm \theta^*_\cS \right)
       +\partial P_{\mathcal{S}}\left(\bm \theta^*_\cS \right)^{\top}\left(\bm \theta^*_\cS -\bm \theta^*_\cO\right) \\
    &=P_{\mathcal{S}}\left(\bm \theta^*_\cO \right)-P_{\mathcal{S}}\left(\bm \theta^*_\cS\right)\\
    & =P_{\mathcal{S}}\left(\bm \theta^*_\cO \right)-D_{\mathcal{S}}\left(\boldsymbol{\alpha}^*_{\mathcal{S}}\right)\\
    & \leq P_{\mathcal{S}}\left(\bm \theta^*_\cO\right)-D_{\mathcal{S}}(\tilde{\boldsymbol{\alpha}}_{\mathcal{S}}) \\
\Leftrightarrow\left\|\bm \theta^*_\cO -\bm \theta^*_\cS \right\|_2 \leq & \sqrt{\frac{2\left(P_{\mathcal{S}}\left(\bm \theta^*_\cO \right)-D_{\mathcal{S}}(\tilde{\boldsymbol{\alpha}}_{\mathcal{S}})\right)} {\lambda}}
    \end{aligned}
  \end{equation}
\end{proof}
% In the practical kernel method, we usually define the kernelized objective with dual variables. We could introduce a new dual problem with dual variables. By leverage KKT conditions, we could define a ${DG}_\alpha$ in $\alpha$-space.  
% Minimizing this gap ensures $\alpha \approx \alpha^*$

% By DGB in \eqref{eq:dualitygap}, we could represent the bound in primal space 
% \begin{equation*}
% \begin{aligned}
% \|\theta_{\mathcal{O}} - \theta_{\mathcal{S}}\|_2^2  & = \|\theta_{\mathcal{O}}\|_2^2 - 2\theta_{\mathcal{O}}^{\top}\theta_{\mathcal{S}} + \|\theta_{\mathcal{S}}\|_2^2 \\
%  & = \frac{1}{\chi^2}[(\boldsymbol{\alpha}_{\mathcal{O}} \odot \mathbf{y}_{\mathcal{O}})^{\top}K(X_{\mathcal{O}}, X_{\mathcal{O}})(\boldsymbol{\alpha}_{\mathcal{O}} \odot \mathbf{y}_{\mathcal{O}}) \\
%  & -2(\boldsymbol{\alpha}_{\mathcal{O}} \odot \mathbf{y}_{\mathcal{O}})^{\top}K(X_{\mathcal{O}}, X_{\mathcal{S}})(\boldsymbol{\alpha}_{\mathcal{S}} \odot \mathbf{y}_{\mathcal{S}}) \\
%  & +(\boldsymbol{\alpha}_{\mathcal{S}} \odot \mathbf{y}_{\mathcal{S}})^{\top}K(X_{\mathcal{S}}, X_{\mathcal{S}})(\boldsymbol{\alpha}_{\mathcal{S}} \odot \mathbf{y}_{\mathcal{S}})]    
% \end{aligned}
% \end{equation*}

% Therefore, bounding $\|\tilde{\alpha} - \alpha^*\|^2$ also bounds $\|\theta - \theta^*\|^2$.

\subsection{Prediction Bound}
\label{proof_lemma3.2}
From Theorem~\ref{thm:parameter_bound}, 
$\|\bm \theta^*_\cS - \bm \theta^*_\cO\|_2^2 
 \,\le\,
 \tfrac{2}{\lambda} 
 G_{\mathcal{S}}\bigl(\bm \theta^*_\cO, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)$.
Thus, as $G_{\mathcal{S}}\bigl(\bm \theta^*_\cO, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)$ decreases, the radius of the permissible ball around $\bm \theta^*_\cO$ likewise decreases. Consequently, the term 
$\bigl|\phi(\boldsymbol{x})^{\top}\bm \theta^*_\cS -\phi(\boldsymbol{x})^{\top}\bm \theta^*_\cO\bigr|$
is bounded above by 
$\|\phi(\boldsymbol{x})\|_2 \sqrt{\tfrac{2}{\lambda}G_{\mathcal{S}}\bigl(\bm \theta^*_\cO, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)}$.
Minimizing $G_{\mathcal{S}}\bigl(\bm \theta^*_\cO, \tilde{\boldsymbol{\alpha}}_{\mathcal{S}}\bigr)$ 
directly minimizes this latter expression, thereby reducing the maximum possible deviation in $\phi(\boldsymbol{x})^{\top}\bm \theta^*_\cS$ for all $\boldsymbol{x}$.



\subsection{Inequality in Test Error Bound}
\label{proof_lemma3.3}
  For any vector $\boldsymbol{a}, \boldsymbol{c} \in \mathbb{R}^n$ and $S>0$,
  \begin{equation*}
    \min _{\boldsymbol{v} \in \mathbb{R}^n:\|\boldsymbol{v}-\boldsymbol{c}\|_2 \leq S} \boldsymbol{a}^{\top} \boldsymbol{v}=\boldsymbol{a}^{\top} \boldsymbol{c}-S\|\boldsymbol{a}\|_2, \quad \max _{\boldsymbol{v} \in \mathbb{R}^n:\|\boldsymbol{v}-\boldsymbol{c}\|_2 \leq S} \boldsymbol{a}^{\top} \boldsymbol{v}=\boldsymbol{a}^{\top} \boldsymbol{c}+S\|\boldsymbol{a}\|_2
  \end{equation*}
  \begin{proof}
      By Cauchy-Schwarz inequality,
  \begin{equation*}
    -\|\boldsymbol{a}\|_2\|\boldsymbol{v}-\boldsymbol{c}\|_2 \leq \boldsymbol{a}^{\top}(\boldsymbol{v}-\boldsymbol{c}) \leq\|\boldsymbol{a}\|_2\|\boldsymbol{v}-\boldsymbol{c}\|_2
  \end{equation*}
  The first inequality holds as equality when $\exists \omega > 0 : \boldsymbol{a} = -\omega(\boldsymbol{v} - \boldsymbol{c})$, and the second inequality holds as equality when $\exists \omega^{\prime} > 0 : \boldsymbol{a} = \omega^{\prime}(\boldsymbol{v} - \boldsymbol{c})$. Furthermore, since $\|\boldsymbol{v} - \boldsymbol{c}\|_2 \leq S$, the following inequality holds:
  \begin{equation*}
    -S\|\boldsymbol{a}\|_2 \leq \boldsymbol{a}^{\top}(\boldsymbol{v}-\boldsymbol{c}) \leq S\|\boldsymbol{a}\|_2
  \end{equation*}
  Equality holds when $\|\boldsymbol{v} - \boldsymbol{c}\|_2 = S$. Additionally, consider $\boldsymbol{v}$ that satisfies the equality conditions of the Cauchy-Schwarz inequality for both cases above:
  \begin{itemize}
    \item For the first inequality to hold as equality: $\boldsymbol{v} = \boldsymbol{c} - \left(\frac{S}{\|\boldsymbol{a}\|_2}\right) \boldsymbol{a}$,
    \item  For the second inequality to hold as equality: $\boldsymbol{v} = \boldsymbol{c} + \left(\frac{S}{\|\boldsymbol{a}\|_2}\right) \boldsymbol{a}$.
  \end{itemize}
\end{proof}
\section{DGKIP in Support Vector Machine formulation}
\label{DGKIP-SVM}
Support Vector Machine (SVM) seeks an optimal hyperplane that maximizes the margin between classes. Beyond linear separability, the kernel trick allows handling of non-linear classification by mapping samples into higher-dimensional feature spaces.

\paragraph{Primal Form}
For a training set $\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n$ with $y_i \in \{-1,+1\}$, the primal objective with $\lambda$-regularization is:
\begin{equation*}
P(\boldsymbol{\theta})=\sum_{i=1}^n \max\{0, 1-y_i f(\boldsymbol{x}_i; \boldsymbol{\theta})\} + \frac{\lambda}{2} \|\boldsymbol{\theta}\|_2^2.
\end{equation*}

\paragraph{Dual Form}
Introducing dual variables $\{\alpha_i\}_{i=1}^n$ with constraints $0 \leq\alpha_i \leq 1$, the dual problem becomes:
\begin{equation*}
  D (\boldsymbol{\alpha})=  \sum_{i=1}^n \alpha_i - \frac{1}{2\lambda}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\boldsymbol{x}_i,\boldsymbol{x}_j). 
\end{equation*}

\paragraph{Duality Gap for SVM}
\label{App:DGKIP-SVM}
% When switching to a new dataset (or subset) with size $n_{\mathcal{S}}$, let $\boldsymbol{\theta}$ be a candidate primal solution and $\boldsymbol{\alpha}$ be the corresponding dual variable. One can form the duality gap:

For the optimal solution $\bm \theta^*_\cO$ of $\mathcal{O}$ and any feasible dual variable $\tilde{\boldsymbol{\alpha}}_{\mathcal{S}}$, the duality gap is defined as follows:
% According to Theorem  \ref{thm:parameter_bound}, the duality gap is formulated as follows 
\begin{equation*}
  \begin{aligned}
    G_{\mathcal{S}}(\bm \theta^*_\cO,\tilde{\boldsymbol{\alpha}}_\mathcal{S}) & = \sum_{i=1}^{n_{\mathcal{S}}}\left[\max\{0, 1-y_i^\mathcal{S}f(\boldsymbol{x}_i^\mathcal{S}; \bm \theta^*_\cO)\} -\tilde{\alpha}_i^\mathcal{S}\right]+\frac{\lambda}{2}\|\bm \theta^*_\cO\|_2^2 \\
 &+  \frac{1}{2\lambda}\sum_{i=1}^{n_\mathcal{S}} \sum_{j=1}^{n_\mathcal{S}} \tilde{\alpha}_i^\mathcal{S} \tilde{\alpha}_j^\mathcal{S} y_i^\mathcal{S} y_j^\mathcal{S} k(\boldsymbol{x}_i^\mathcal{S},\boldsymbol{x}_j^\mathcal{S})
  \end{aligned}
\end{equation*}



% \begin{remark}[Remark on $\boldsymbol{\alpha}_\mathcal{S}$-Ranges]

% In logistic regression, each $\alpha_i$ naturally falls within $[0,1]$.
% In SVM (hinge loss), $\alpha_i \geq 0$. For better gradient-based updates, one might replace the non-differentiable hinge with a smooth approximation (e.g., soft-plus or sigmoid-based hinge).
% By iterating through these steps, the \textbf{duality gap} provides a direct metric for how closely the distilled dataset $\mathcal{S}$ can reproduce the original model's behavior—ensuring we keep $\bm \theta^*_\cS$ within a well-defined region around $\bm \theta^*_\cO$.

For SVM, different from LR, updating synthetic data $\mathcal{S}$ (Step 4 of Algorithm \ref{alg:distillation}) becomes difficult if $\tilde{\alpha}_i^{\mathcal{S}}$ is calculated naively. In fact, from equation \eqref{eq:alphaapproxi}, we have
\begin{equation*}
\tilde{\alpha}_i^{\mathcal{S}}\in 
- \partial\ell(y_i^\mathcal{S}, f(\boldsymbol{x}_i^\mathcal{S} ;  \boldsymbol{\theta}^*_\mathcal{O}))=\left\{\begin{array}{cc}
1 & \text{if }\quad y_i^\mathcal{S}f(\boldsymbol{x}_i^\mathcal{S} ;  \boldsymbol{\theta}^*_\mathcal{O})<1 \\
{[0,1]} & \text{if }\quad y_i^\mathcal{S}f(\boldsymbol{x}_i^\mathcal{S} ;  \boldsymbol{\theta}^*_\mathcal{O})=1 \\
0 & \text{if }\quad y_i^\mathcal{S}f(\boldsymbol{x}_i^\mathcal{S} ;  \boldsymbol{\theta}^*_\mathcal{O}) > 1
\end{array}\right.
\end{equation*}
As a result, the gradient with respect to $\boldsymbol{x}_i^\mathcal{S}$and $y_i^\mathcal{S}$ become zero.
Thus, in our implementation, we approximate it by using the smooth sigmoid function $\sigma$
\begin{equation*}
\tilde{\alpha}_i^{\mathcal{S}} \approx \sigma(1-y_i^\mathcal{S}f(\boldsymbol{x}_i^\mathcal{S} ;  \boldsymbol{\theta}^*))
\end{equation*}


% \end{remark}



\section{Details of Experiments}
\subsection{NNGP Random Feature Approximation}
\label{nngp_intro}
In brief, instead of directly computing the NNGP kernel
\[k^{\text{NNGP}}(\boldsymbol{x}, \boldsymbol{x}') = \mathbb{E}_{\boldsymbol{w}}[{f_{\boldsymbol{w}}(\boldsymbol{x})}^\top f_{\boldsymbol{w}}(\boldsymbol{x}')],\]
we draw a large number $M$ of random weight vectors $\{\boldsymbol{w}_m\}_{m=1}^M$ (e.g., i.i.d. from $\mathcal{N}(0, \sigma_{\boldsymbol{w}}^2I)$) and define the following explicit feature map into a finite-dimensional space:
\[\phi^{\text{NNGP}}({\boldsymbol{x}}) := \frac{1}{\sqrt{M}}[f_{\boldsymbol{w}_1}, f_{\boldsymbol{w}_2}, \ldots, f_{\boldsymbol{w}_M}]^\top.\]
Then, for any pair of samples $\boldsymbol{x}, \boldsymbol{x}' \in \mathbb{R}^d$, the inner product of these features approximates the NNGP kernel:
\[\phi^{\text{NNGP}}(\boldsymbol{x})^\top \phi^{\text{NNGP}}(\boldsymbol{x}') \approx k^{\text{NNGP}}(\boldsymbol{x}, \boldsymbol{x}'). \]
By substituting $\phi^{\text{NNGP}}(\cdot)$ for $\phi(\cdot)$, we obtain a random-feature-based formulation whose solution converges to that of the infinite-width NNGP model as $M \to \infty$.

\subsection{Settings}
\label{exp_setting}
We run all experiments on a single NVIDIA RTX A6000 GPU.
The regularization parameter \(\lambda\) was set according to the number of samples. Specifically, when obtaining the original data solution $\bm \theta^*_\cO$, we set $\lambda = n_\mathcal{O} \times 10^{-6}$, and dataset distillation process, we set $\lambda = n_\mathcal{S} \times 10^{-6}$.
We used Adabelief optimizer \cite{zhuang2020adabelief} with a learning rate of $1e-2$ and $\epsilon=1e-16$.

For random feature approximation, we chose the fully connected network and convolutional network. The fully connected network has three hidden layers with 1024 neurons, each layer initialized with Gaussian-distributed weights. Similarly, the convolutional network has three convolutional layers with 256 convolutional channels per layer initialized with Gaussian Gaussian-distributed weights. In practice, we construct 30 networks for a fully connected network and 8 networks for a convolutional network, concatenating their outputs to obtain a richer embedding.





  

% \begin{table*}[t]
%     \caption{Distillation results on four datasets with varying support set sizes. Bolded numbers indicate the best performance, and underlined numbers indicate the second best performance. All the methods used learned labels. Training times for each method are reported in seconds.}
%     \label{kernel-distillation}
%     \vskip 0.15in
%     \begin{center}
%         \begin{small}
%             \begin{sc}
%                 \begin{tabular}{lccccccc}
%                     \hline
%                      & Img/Cls                                & DD & DC & DSA & DGKIP-SVM & DGKIP-LR \\
%                     \hline
%                     \multirow{3}{*}{MNIST}
%                      & 1
%                      & \makecell{99.56 $\pm$ 0.16                                                    \\ 50s}
%                      & \makecell{98.96 $\pm$ 0.81                                                    \\ 60s}
%                      & \makecell{98.79 $\pm$ 0.99                                                    \\ 70s}
%                      & \makecell{\textbf{99.97 $\pm$ 0.02}                                           \\ 120s}
%                      & \makecell{\underline{99.75 $\pm$ 0.02}                                        \\ 100s} \\
%                     \cmidrule(lr){3-7}
%                      & 10
%                      & \makecell{99.63 $\pm$ 0.11                                                    \\ 150s}
%                      & \makecell{98.25 $\pm$ 0.56                                                    \\ 160s}
%                      & \makecell{\underline{99.77 $\pm$ 0.08}                                        \\ 170s}
%                      & \makecell{\textbf{99.99 $\pm$ 0.02}                                           \\ 300s}
%                      & \makecell{99.76 $\pm$ 0.00                                                    \\ 250s} \\
%                     \cmidrule(lr){3-7}
%                      & 50
%                      & \makecell{99.72 $\pm$ 0.11                                                    \\ 400s}
%                      & \makecell{98.96 $\pm$ 0.91                                                    \\ 450s}
%                      & \makecell{\underline{99.81 $\pm$ 0.07}                                        \\ 500s}
%                      & \makecell{\textbf{99.96 $\pm$ 0.06}                                           \\ 720s}
%                      & \makecell{99.75 $\pm$ 0.02                                                    \\ 650s} \\
%                     \hline
%                     \multirow{3}{*}{CIFAR-10}
%                      & 1
%                      & \makecell{78.04 $\pm$ 0.06                                                    \\ 80s}
%                      & \makecell{71.58 $\pm$ 1.16                                                    \\ 100s}
%                      & \makecell{69.43 $\pm$ 2.24                                                    \\ 120s}
%                      & \makecell{\textbf{85.63 $\pm$ 0.97}                                           \\ 150s}
%                      & \makecell{\underline{83.18 $\pm$ 2.68}                                        \\ 130s} \\
%                     \cmidrule(lr){3-7}
%                      & 10
%                      & \makecell{77.58 $\pm$ 0.02                                                    \\ 200s}
%                      & \makecell{86.47 $\pm$ 0.66                                                    \\ 240s}
%                      & \makecell{82.91 $\pm$ 0.91                                                    \\ 280s}
%                      & \makecell{\textbf{90.83 $\pm$ 0.14}                                           \\ 400s}
%                      & \makecell{\underline{89.72 $\pm$ 0.10}                                        \\ 360s} \\
%                     \cmidrule(lr){3-7}
%                      & 50
%                      & \makecell{76.91 $\pm$ 0.02                                                    \\ 450s}
%                      & \makecell{\underline{90.45 $\pm$ 0.47}                                        \\ 500s}
%                      & \makecell{88.10 $\pm$ 0.61                                                    \\ 550s}
%                      & \makecell{\textbf{91.17 $\pm$ 0.72}                                           \\ 900s}
%                      & \makecell{90.01 $\pm$ 0.02                                                    \\ 750s} \\
%                     \hline
%                     \multirow{3}{*}{Fashion-MNIST}
%                      & 1
%                      & \makecell{95.96 $\pm$ 0.06                                                    \\ 40s}
%                      & \makecell{95.13 $\pm$ 0.50                                                    \\ 50s}
%                      & \makecell{96.16 $\pm$ 1.07                                                    \\ 60s}
%                      & \makecell{\textbf{97.35 $\pm$ 0.39}                                           \\ 100s}
%                      & \makecell{\underline{96.70 $\pm$ 0.71}                                        \\ 80s} \\
%                     \cmidrule(lr){3-7}
%                      & 10
%                      & \makecell{95.72 $\pm$ 0.18                                                    \\ 100s}
%                      & \makecell{98.01 $\pm$ 0.41                                                    \\ 150s}
%                      & \makecell{97.30 $\pm$ 0.36                                                    \\ 200s}
%                      & \makecell{\textbf{98.70 $\pm$ 0.15}                                           \\ 250s}
%                      & \makecell{\underline{98.32 $\pm$ 0.11}                                        \\ 230s} \\
%                     \cmidrule(lr){3-7}
%                      & 50
%                      & \makecell{95.99 $\pm$ 0.04                                                    \\ 300s}
%                      & \makecell{\textbf{98.82 $\pm$ 0.15}                                           \\ 350s}
%                      & \makecell{\underline{98.39 $\pm$ 0.35}                                        \\ 400s}
%                      & \makecell{98.57 $\pm$ 0.25                                                    \\ 800s}
%                      & \makecell{98.43 $\pm$ 0.02                                                    \\ 700s} \\
%                     \hline
%                 \end{tabular}
%             \end{sc}
%         \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table*}
% \begin{table*}[t]
%    \caption{Distillation results on four datasets with varying support set sizes. Bolded numbers indicate the best performance, and underlined numbers indicate the second best performance. All the methods used learned labels. Training times for each method are reported in seconds.}
% \label{kernel-distillation}
%     \vskip 0.15in
%     \begin{center}
%       \begin{scriptsize}
%         \begin{sc}
%           \begin{tabular}{lcccccccccccc}
%             \toprule
%              & Img/Cls   & \multicolumn{2}{c}{DD} & \multicolumn{2}{c}{DC} & \multicolumn{2}{c}{DSA} & \multicolumn{2}{c}{DGKIP-SVM} & \multicolumn{2}{c}{DGKIP-LR} \\
%              \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
%              &          & Acc. & Time & Acc. & Time & Acc. & Time & Acc. & Time & Acc. & Time \\
%             \midrule
%             \multirow{3}{*}{MNIST}
%              & 1  & 99.56 $\pm$ 0.16 & 0.237 & 99.601$\pm$0.092 & 0.186 & 99.905$\pm$0.000 & 0.192 & \textbf{99.97$\pm$0.02} & 0.007 & \underline{99.75$\pm$0.02} & 0.009 \\
%              & 10 & 99.63 $\pm$ 0.11 & 0.244 & 99.771$\pm$0.089 & 0.196 & 99.858$\pm$0.000 & 0.206 & \textbf{99.99$\pm$0.02} & 0.012 & 99.76$\pm$0.00 & 0.011 \\
%              & 50 & 99.72 $\pm$ 0.11 & 0.602 & 99.887$\pm$0.079 & 0.217 & 99.905$\pm$0.000 & 0.434 & \textbf{99.96$\pm$0.06} & 0.041 & 99.75$\pm$0.02 & 0.040 \\
%             \midrule
%             \multirow{3}{*}{CIFAR-10}
%              & 1  & 78.04 $\pm$ 0.06 & 0.245 & 71.725$\pm$0.733 & 0.186 & 75.900$\pm$0.000 & 0.190 & \textbf{85.63$\pm$0.97} & 0.011 & \underline{83.18$\pm$2.68} & 0.011 \\
%              & 10 & 77.58 $\pm$ 0.02 & 0.249 & 80.581$\pm$0.866 & 0.186 & 77.000$\pm$0.000 & 0.223 & \textbf{90.83$\pm$0.14} & 0.015 & \underline{89.72$\pm$0.10} & 0.015 \\
%              & 50 & 76.91 $\pm$ 0.02 & 0.298 & 76.315$\pm$0.980 & 0.202 & 76.400$\pm$0.000 & 0.376 & \textbf{91.17$\pm$0.72} & 0.051 & 90.01$\pm$0.02 & 0.051 \\
%             \midrule
%             \multirow{3}{*}{Fashion-MNIST}
%              & 1  & 95.96 $\pm$ 0.06 & 0.295 & 88.381$\pm$1.073 & 0.123 & 98.250$\pm$0.000 & 0.184 & \textbf{97.35$\pm$0.39} & 0.008 & \underline{96.70$\pm$0.71} & 0.009 \\
%              & 10 & 95.72 $\pm$ 0.18 & 0.353 & 97.766$\pm$0.340 & 0.139 & 98.400$\pm$0.000 & 0.213 & \textbf{98.70$\pm$0.15} & 0.011 & \underline{98.32$\pm$0.11} & 0.011 \\
%              & 50 & 95.99 $\pm$ 0.04 & 0.597 & 98.467$\pm$0.163 & 0.203 & 97.950$\pm$0.000 & 0.415 & \textbf{98.57$\pm$0.25} & 0.041 & 98.43$\pm$0.02 & 0.041 \\
%             \bottomrule
%           \end{tabular}
%         \end{sc}
%       \end{scriptsize}
%     \end{center}
%     \vskip -0.1in
%   \end{table*}
  
