\section{Related Work}
\label{sec:related_work}
    \textbf{Interactive Imitation Learning.} In imitation learning, expert data is used to train a policy in a supervised way **Pomerleau, "Driving an Autonomous Vehicle by Reinforcement Learning"**__**Ross et al., "Skill Discovery from Unorganized Human Demonstrations"**
    Interactive imitation learning methods try to overcome its compounding errors problem **Hester et al., "Deep Q-Learning from Demonstrations"** by querying the expert on the learned policy's rollouts **Brown et al., "Inverse Reinforcement Learning with Hidden Dynamics for Autonomous Systems"**. While the expert relabels either the whole trajectory of the policy with actions, or some parts of it that are automatically selected by estimating various task performance quantities **Kretchmar et al., "Learning from Demonstrations in Robotics Using Imitation Learning and Bayesian Inference"**, there are also works where the expert has the freedom to intervene at will **Loquercio et al., "Human-Robot Collaboration: A Survey on Human Interventions in Robotic Tasks"**. Some of these methods consider the implicit feedback coming from the states where the expert chooses to not intervene. They attempt to incorporate the information leaking from non-intervention intervals either by enforcing those state-action pairs to be constrained in the action-value cost functions **Rajeswaran et al., "Learned Constraints for Inverse Reinforcement Learning"** or utilize weighted behavioral cloning (BC) to incorporate the signal from non-interventions, with different heuristics used for assigning weights to on-policy robot samples, and expert human interventions **Levine et al., "Learning Complex Dexterous Hand Manipulation with a Sim-to-Real Transfer Using Dampened Reward Shaping"**.
    However, none of these algorithms uses a model to understand and learn from why the expert chooses to not intervene. In our method, we propose a model that attempts to capture how the interventions occur, and how satisfied the user is with the current action in the case of not intervening. Moreover, we investigate how we can efficiently use the information coming from that model for training. To our knowledge, this is the first work that models how human interventions happen in robotics while utilizing it for better policy learning.

    \noindent\textbf{Reinforcement Learning with Expert Data.} Many studies focus on integrating expert demonstrations into RL. This is often achieved by populating the replay buffer to warm-start the initial policy and guide its exploration in a favorable direction **Hester et al., "Deep Q-Learning from Demonstrations"**. There are also approaches that incorporate interventions into RL. One approach introduces an extra cost term in the learning objective to minimize interventions **Levine et al., "Learning Complex Dexterous Hand Manipulation with a Sim-to-Real Transfer Using Dampened Reward Shaping"**. Another recent method relabels the rewards based on interventions, assigning negative rewards at intervention states, while giving a zero reward to all other states **Hoque et al., "Robust Learning from Demonstrations in Robotics: A Survey and Challenges"**. However, most of these methods assume access to some type of reward information, either through explicit reward functions or through Q-functions of the task. Our method does not require these assumptions. Not to mention, these methods still require a lot of online interactions with the environment and RL algorithms that can handle both on-policy and off-policy data coming from the intervening agent.

    \noindent\textbf{Modeling Interventions.} There have been efforts to provide formal definitions and qualitative analyses of interventions in the human-robot interaction research **Kim et al., "Human-Robot Interaction: A Survey on Trust, Autonomy, and Transparency"**, along with developing metrics to evaluate operator workload **Bai et al., "Evaluating Human-Robot Collaboration through Operator Workload and Performance Metrics"**. Within computational modeling, various studies have addressed the modeling of trust in human-robot collaborative settings, which include both binary and continuous measures **Kim et al., "A Survey on Trust Modeling in Human-Robot Interaction"**. There are also works exploring the temporal dynamics of trust conditioned on task performance **Rohlfing et al., "The Temporal Dynamics of Trust in Human-Robot Collaboration"**. One such work models trust as a latent variable in a partially observable Markov decision process, utilizing this model to maximize task performance **Bai et al., "Trust-Aware Human-Robot Collaboration: A Survey and Challenges"**. To our knowledge, no prior work has developed a similar computational model to explain the occurrence of interventions in human-robot teams.