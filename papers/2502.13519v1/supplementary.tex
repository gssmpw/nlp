\section*{Supplementary Material}

\subsection{Pseudocode of the Algorithm}
    \vspace{-5px}
    \begin{algorithm}
    \algrenewcommand\algorithmicindent{0.75em}%
    \small
    \caption{MILE: Model-based Intervention Learning in Iterative Setting}
    \label{algo:iterative_pseudocode}
    \noindent
    \begin{minipage}[c]{0.45\linewidth}
    \begin{algorithmic}
    \vspace{-32px}
    \State \textbf{Notations} \\
    $N$: maximum deployment rounds, $k$: number of rollout episodes in each deployment round, \\
    $l$: number of batches, $b$: batch size,\\
    % $m$: number of epochs in each learning round,\\
    $m$: number of epochs in each learning round, \\
    $\alpha$: learning rate, \\ 
    %$\alpha$: policy learning rate, $\lambda$: loss weight \\
    $\pi^{\theta}_{1}$: initial policy, $\hat{\pi}^{\xi}_{1}$: initial mental model
    
    \vspace{0.3cm}
    % \State \textbf{Iterative training loop}
    \For {$i \gets 1$ to $N$}
        % \State Run in parallel:
        \State $D^{i+1} \gets \text{DEPLOYMENT}(\pi^{\theta}_i, D^i)$
        \State $\pi^{\theta}_{i+1}, \hat{\pi}^{\xi}_{i+1} \gets \text{LEARNING}(\pi^{\theta}_i, \hat{\pi}^{\xi}_{i}, D^i)$
    \EndFor
    
    % \vspace{0.1cm}
    % \Function{DEPLOYMENT}{$\pi_\theta, D$}
    %     \State Collect rollouts w/ interventions $\tau_1, \ldots, \tau_k$
    %     \State $D' \gets D \cup \{\tau_1, \ldots, \tau_k\}$
    %     \State \Return $D'$
    % \EndFunction
    \end{algorithmic}

    \end{minipage}
    \begin{minipage}[c]{0.55\linewidth}
    \begin{algorithmic}
    \Function{DEPLOYMENT}{$\pi_\theta, D$}
        \State Collect rollouts w/ interventions $\tau_1, \ldots, \tau_k$
        \State $D' \gets D \cup \{\tau_1, \ldots, \tau_k\}$
        \State \Return $D'$
    \EndFunction
    \vspace{0.1cm}
    \Function{LEARNING}{$\pi_\theta, \hat{\pi}_{\xi}, D$}
        \For {$m$ epochs \textbf{and} $l$ batches each}
            %\For {$l$ batches}
                \State Get the next mini-batch $\left(s^i, a_r^i, a_h^i, \nu^i\right)_{i=1}^b \sim D$
                %\State Generate $\nu^i = 0$ if $a_h^i$ is undefined at $s^i$, $\nu^i = 1$ otherwise.
                \State Compute $\hat{\nu}(s^i; \theta, \xi) \!=\! p(\nu^i\!=\!1\!\mid\! s^i)$ based on Eq. (7)
                %\State $J_1(\theta, \xi) = -\frac{1}{b} \sum_{i=1}^b \left[ \nu^i \log(\hat{\nu}(s^i; \theta, \xi)) + (1 - \nu^i) \log(1 - \hat{\nu}(s^i; \theta, \xi)) \right]$
                %\State $J_2(\theta) = -\frac{1}{b} \sum_{i=1}^b \log(\pi_{\theta}(a_h^i \mid s^i))$
                %\State $J(\theta, \xi) = \lambda J_1(\theta,\xi) + (1-\lambda)J_2(\theta)$
                \State Compute $J(\theta, \xi)$ based on Eq. (12)
                \State Run in parallel:
                \State $\quad \theta \gets \theta - \alpha \nabla_\theta J(\theta, \xi)$
                \State $\quad \xi \gets \xi - \alpha \nabla_\xi J(\theta,\xi)$
            %\EndFor
        \EndFor
        \State \Return $\pi_\theta$, $\hat{\pi}_{\xi}$
    \EndFunction
    
    \end{algorithmic}
    \end{minipage}
    \end{algorithm}
    \vspace{-5px}

\subsection{Simulation Experiment Details}
Depending on the action space, we trained suboptimal initial policies and various levels of expert policies for simulated humans using SAC or DQN. We train a different policy for each task. For each task, the same expert is used to intervene across all methods. To generate the mental models of simulated humans, we collected 100 rollouts of the initial policy and trained a BC agent on them. In all result plots, we display the mean and standard error over 3 seeds for each method.

Regarding the observation space, we use true world states, i.e. low-level states of the robot and the task-related object. This includes robot joint positions and the positions and orientations of relevant objects. Our action space consists of the change in Cartesian coordinates of the robot end effector and the gripper state. Each episode has a maximum of 1000 timesteps, with early termination upon success.

To compare our method with RLIF, we used RLPD as its backbone algorithm in the domains with continuous action spaces as it was done in the original paper. For domains with discrete action spaces, we used DQN as the backbone of RLIF. We initialized the policy networks for all methods as the clones of the initial policy $\pi_\theta$. For the offline demonstration ablation, we also initialized RLIF's replay buffer with those demonstrations.

\subsection{Real Robot Experiment Details}
In this experiment, we use image observations (captured from a USB webcam) along with the robot's end effector position. We keep the same action space as in the simulation experiments for real-world settings. The robot begins with the octagonal block in its gripper. The initial BC policy is trained using 120 human-collected trajectories, gathered with a Meta Quest 2 headset. At the beginning of real-world experiments, we show 3 trajectories to the users to make them aware of the robot's capabilities. We also warm-start the initial mental model using these same demonstrations. This process requires no supervision from the human and minimal effort since the human only needs to observe the robot.

\subsection{Hyperparameters}
We used a Multi-Layer Perceptron (MLP) with hidden dimensions of 256 in both the MetaWorld and real-robot experiments for all methods. In order to get image embeddings in real-robot experiment, we used a pretrained R3M model with ResNet50 architecture. To retain temporal information, we concatenate the states of the previous three timesteps with the current timestep. For the LunarLander experiment, we used an MLP with hidden dimensions of 64 for all methods. The network outputs an action for the current step.

The log-probabilities of the actions vary in scale between the policies used in different tasks. This necessitates adjusting the standard deviation of the normal distribution whose cumulative distribution function (CDF) is used in Equation 2, along with task-specific cost terms (parameter $c$), to calculate smoother rather than skewed intervention probabilities.
\begin{table}[htb]
\centering
\caption{Hyperparameters across different simulation and real-world tasks.}
\resizebox{\linewidth}{!}{
\large
\begin{tabular}{lccccc@{}}
\toprule
          & \textbf{LunarLander} & \textbf{Button-Press}  & \textbf{Drawer-Open} & \textbf{Peg-Insert} & \textbf{WidowX Peg Insertion}  \\ \midrule
    \multicolumn{1}{@{}l}{\textbf{MILE (Ours)}}\\
    Learning Rate & 1e-5 & 1e-4 & 5e-4 & 1e-4 & 1e-5\\
    Mental Model Hidden Dims  & (64, 64) & (256, 256) & (256, 256) & (256, 256) & (256,256)\\
    Dataset size in Offline Experiment & 5 Trajectories & 15 Trajectories & 15 Trajectories & 15 Trajectories & - \\
    Number of Iterations & - & - & 20 & 20 & 6 \\
    Episodes Per Iteration & - & - & 1 & 1 & 3 \\
    Training Epochs per Iteration & - & - & 300 & 300 & 500 \\
    Intervention CDF c & 3 & 150 & 60 & 75 & 70\\
    Intervention CDF $\sigma$ & 1 & 200 & 75 & 175 & 100\\
  \midrule
    \multicolumn{1}{@{}l}{\textbf{HG-DAgger}}\\
    Learning Rate & 5e-6 & 5e-5 & 1e-4 & 5e-6 & 1e-5\\
    Number of Iterations & 5 & 5 & 5/20 & 5/20 & 6\\
    Episodes Per Iteration & 1 & 3 & 3/1 & 3/1 & 3\\
    Training Epochs per Iteration & 400 & 1000 & 1000/300 & 1000/300 & 500 \\
  \midrule
    \multicolumn{1}{@{}l}{\textbf{RLIF}}\\
    Batch Size & 64 & 256 & 256 & 256 & - \\
    Learning Rate & 5e-4 & 3e-4 & 3e-4 & 3e-4 & - \\
    Discount & 0.99 & 0.99 & 0.99 & 0.99 & - \\
     UTD Ratio & 4 & 4 & 4 & 4 & - \\  
  \midrule
    \multicolumn{1}{@{}l}{\textbf{IWR}}\\
    Learning Rate & 5e-6 & 5e-6 & 1e-4 & 1e-6 & 1e-5\\
    Number of Iterations & 5 & 5 & 5/20 & 5/20 & 6\\
    Episodes Per Iteration & 1 & 3 & 3/1 & 3/1 & 3\\
    Training Epochs per Iteration & 400 & 1000 & 1000/300 & 1000/300 & 500 \\
  \midrule
    \multicolumn{1}{@{}l}{\textbf{Sirius}}\\
    Learning Rate & 5e-6 & 5e-6 & 1e-4 & 1e-6 & 1e-5\\
    Number of Iterations & 5 & 5 & 5/20 & 5/20 & 6\\
    Episodes Per Iteration & 1 & 3 & 3/1 & 3/1 & 3\\
    Training Epochs per Iteration & 400 & 1000 & 1000/300 & 1000/300 & 500 \\
  \midrule
    \multicolumn{1}{@{}l}{\textbf{All Methods}}\\
    Policy Hidden Dims & (64, 64) &(256, 256) & (256, 256) & (256, 256) & (256, 256) \\
  \bottomrule                          
\end{tabular}
}
\end{table}



