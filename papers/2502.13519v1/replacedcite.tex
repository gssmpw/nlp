\section{Related Work}
\label{sec:related_work}
    \textbf{Interactive Imitation Learning.} In imitation learning, expert data is used to train a policy in a supervised way ____. Interactive imitation learning methods try to overcome its compounding errors problem ____ by querying the expert on the learned policy's rollouts ____. While the expert relabels either the whole trajectory of the policy with actions, or some parts of it that are automatically selected by estimating various task performance quantities ____, there are also works where the expert has the freedom to intervene at will ____. Some of these methods consider the implicit feedback coming from the states where the expert chooses to not intervene. They attempt to incorporate the information leaking from non-intervention intervals either by enforcing those state-action pairs to be constrained in the action-value cost functions ____ or utilize weighted behavioral cloning (BC) to incorporate the signal from non-interventions, with different heuristics used for assigning weights to on-policy robot samples, and expert human interventions ____.
    However, none of these algorithms uses a model to understand and learn from why the expert chooses to not intervene. In our method, we propose a model that attempts to capture how the interventions occur, and how satisfied the user is with the current action in the case of not intervening. Moreover, we investigate how we can efficiently use the information coming from that model for training. To our knowledge, this is the first work that models how human interventions happen in robotics while utilizing it for better policy learning.

    \noindent\textbf{Reinforcement Learning with Expert Data.} Many studies focus on integrating expert demonstrations into RL. This is often achieved by populating the replay buffer to warm-start the initial policy and guide its exploration in a favorable direction ____. There are also approaches that incorporate interventions into RL. One approach introduces an extra cost term in the learning objective to minimize interventions ____. Another recent method relabels the rewards based on interventions, assigning negative rewards at intervention states, while giving a zero reward to all other states ____. However, most of these methods assume access to some type of reward information, either through explicit reward functions or through Q-functions of the task. Our method does not require these assumptions. Not to mention, these methods still require a lot of online interactions with the environment and RL algorithms that can handle both on-policy and off-policy data coming from the intervening agent.

    \noindent\textbf{Modeling Interventions.} There have been efforts to provide formal definitions and qualitative analyses of interventions in the human-robot interaction research ____, along with developing metrics to evaluate operator workload ____. Within computational modeling, various studies have addressed the modeling of trust in human-robot collaborative settings, which include both binary and continuous measures ____. There are also works exploring the temporal dynamics of trust conditioned on task performance ____. One such work models trust as a latent variable in a partially observable Markov decision process, utilizing this model to maximize task performance ____. To our knowledge, no prior work has developed a similar computational model to explain the occurrence of interventions in human-robot teams.