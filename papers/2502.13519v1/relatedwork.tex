\section{Related Work}
\label{sec:related_work}
    \textbf{Interactive Imitation Learning.} In imitation learning, expert data is used to train a policy in a supervised way \cite{celemin2022interactive, argall2009survey, ho2016general, laskey2017dart, abbeel2010autononous, pomerleau1988alvinn}. Interactive imitation learning methods try to overcome its compounding errors problem \cite{ross2010efficient, ross2011reduction} by querying the expert on the learned policy's rollouts \cite{celemin2022interactive, kelly2019hgdagger, kunal2019ensembledagger, ross2011reduction, hoque2022thriftydagger,myers2023active}. While the expert relabels either the whole trajectory of the policy with actions, or some parts of it that are automatically selected by estimating various task performance quantities \cite{hoque2022thriftydagger, ross2011reduction}, there are also works where the expert has the freedom to intervene at will \cite{kelly2019hgdagger, liu2022robot, mandlekar2020humanintheloopimitationlearningusing, chisari2021correct}. Some of these methods consider the implicit feedback coming from the states where the expert chooses to not intervene. They attempt to incorporate the information leaking from non-intervention intervals either by enforcing those state-action pairs to be constrained in the action-value cost functions \cite{spencer2020learning} or utilize weighted behavioral cloning (BC) to incorporate the signal from non-interventions, with different heuristics used for assigning weights to on-policy robot samples, and expert human interventions \cite{liu2022robot, mandlekar2020humanintheloopimitationlearningusing, chisari2021correct}.
    However, none of these algorithms uses a model to understand and learn from why the expert chooses to not intervene. In our method, we propose a model that attempts to capture how the interventions occur, and how satisfied the user is with the current action in the case of not intervening. Moreover, we investigate how we can efficiently use the information coming from that model for training. To our knowledge, this is the first work that models how human interventions happen in robotics while utilizing it for better policy learning.

    \noindent\textbf{Reinforcement Learning with Expert Data.} Many studies focus on integrating expert demonstrations into RL. This is often achieved by populating the replay buffer to warm-start the initial policy and guide its exploration in a favorable direction \cite{hester2018deep, ball2023efficient, nair2018overcoming, xie2021policy}. There are also approaches that incorporate interventions into RL. One approach introduces an extra cost term in the learning objective to minimize interventions \cite{li2022efficient}. Another recent method relabels the rewards based on interventions, assigning negative rewards at intervention states, while giving a zero reward to all other states \cite{luo2023rlif}. However, most of these methods assume access to some type of reward information, either through explicit reward functions or through Q-functions of the task. Our method does not require these assumptions. Not to mention, these methods still require a lot of online interactions with the environment and RL algorithms that can handle both on-policy and off-policy data coming from the intervening agent.

    \noindent\textbf{Modeling Interventions.} There have been efforts to provide formal definitions and qualitative analyses of interventions in the human-robot interaction research \cite{wang2025effects}, along with developing metrics to evaluate operator workload \cite{scholtz2003theory, steinfeld2006common}. Within computational modeling, various studies have addressed the modeling of trust in human-robot collaborative settings, which include both binary and continuous measures \cite{rodriguez2023review}. There are also works exploring the temporal dynamics of trust conditioned on task performance \cite{guo2021modeling, chen2018planning}. One such work models trust as a latent variable in a partially observable Markov decision process, utilizing this model to maximize task performance \cite{chen2018planning}. To our knowledge, no prior work has developed a similar computational model to explain the occurrence of interventions in human-robot teams.