\section{Related Work}
\label{sec:relatedwork}
Traditionally, an experimental \ac{IR} collection includes three elements, a corpus, a set of topics, and the relevance judgments, defining which documents are relevant in response to the topics.
Over the last 30 years, since the first TREC campaign~\cite{DBLP:conf/trec/1992}, the most common strategy to obtain such relevance judgments has involved expert annotators, capable of providing the most accurate labels. 
The cost of this process can be partially reduced with pooling~\cite{croft2009search}, but the monetary and temporal costs of building an \ac{IR} experimental collection following this paradigm remain extremely high.

Automatic relevance judgment has recently received significant attention in the IR community. In earlier studies, ~\citet{faggioli2023perspectives} studied different levels of human and LLMs collaboration for automatic relevance judgment. They suggested the need for humans to support and collaborate with LLMs for a human-machine collaboration judgment. ~\citet{thomas2023large} leverage LLMs capabilities in judgment at scale, in Microsoft Bing. They used real searcher feedback to build an LLM and prompt in a way that matches the small sample of searcher preferences. Their experiments show that LLMs can be as good as human annotators in indicating the best systems. They also comprehensively investigated various prompts and prompt features for the task and revealed that LLM performance on judgments can vary with simple paraphrases of prompts. Recently, \citet{rahmani2024synthetic} have studied fully synthetic test collection using LLMs. In their study, they generated synthetic queries and synthetic judgment to build a full synthetic test collation for retrieval evaluation. They have shown that LLMs can generate a synthetic test collection that results in system ordering performance similar to evaluation results obtained using the real test collection.

On a different line, \citet{DBLP:conf/sigir/Dietz24} defines a LLM-based ``autograding'' approach. This evaluation strategy targets generated content that cannot be evaluated in a purely offline scenario and it consists of using a question bank as the evaluation test-bed. An \ac{LLM} measures the effectiveness of the generative model in answering the questions, possibly with the supervision of a human. The autograding approach proposed by \citet{DBLP:conf/sigir/Dietz24} includes an automatic passage evaluation whose task aligns with the one evaluated in \texttt{LLMJudge}.

\subsection{Criticisms and Open Challenges}
The use of \acp{LLM} as assessors comes with major bias risks and challenges that should not be neglected, especially considering the impact they might have in the development of \ac{IR} evaluation.

\partitle{Bias}
First and most importantly, \acp{LLM} are affected by bias~\cite{DBLP:conf/fat/BenderGMS21}. Their internal representation of the concepts is, by construction, conditioned on the context such concepts appear in~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}. Thus, depending on the underlying data, the \ac{LLM} might form a biased notion of relevance that might reflect upon the relevance judgments generated by it. Quantifying the bias, identifying its source, and mitigating its consequences are still open issues that need to be addressed. We hope that the release of this collection will help the research community with the needed data to study how to deal with the bias in \ac{LLM}-generated relevance judgments.

\partitle{Circularity}
A second source of concern when it comes to using \acp{LLM} as assessors relates to the risk of \textit{circular evaluation}~\cite{faggioli2023perspectives,DBLP:journals/corr/abs-2409-15133}. For example, the same \ac{LLM} might be used to generate relevance judgments and as a document ranker. This would induce a strong bias on the validity and generalizability of the relevance judgments.

\partitle{Environmental Impact}
An often hidden cost of the \acp{LLM} concerns their environmental impact in terms of energy utilization, carbon emissions~\cite{DBLP:journals/corr/abs-2408-09713,DBLP:conf/sigir/ScellsZZ22}, and water consumption~\cite{DBLP:conf/ictir/ZucconSZ23}.
While \acp{LLM} might allow building collections at a fraction of the monetary and temporal cost, we should account for the environmental impact of such a process, limiting our reliance on ``disposable'' relevance judgments.

\partitle{Vulnerability to Attacks and Adversarial Misuse}
\citet{DBLP:conf/ecir/ParryFMPH24} and \citet{DBLP:conf/sigir-ap/Alaofi0SS24} illustrate the vulnerability of the \acp{LLM} to mischievous manipulations of the corpus. For example,~\citet{DBLP:conf/ecir/ParryFMPH24} show that, by introducing keywords such as the term ``relevant'' in a document, it will more likely considered relevant by an \ac{LLM}. Similar behavior is observed also by \citet{DBLP:conf/sigir-ap/Alaofi0SS24}, who notice that by introducing the query on the document, more probably an \ac{LLM} will consider the document relevant to such a query --- even if the rest of the document is composed by random terms.
More recently, \citet{DBLP:journals/corr/abs-2412-17156} show how, by properly crafting an adversarial run, it is possible to cheat an \ac{LLM} used as an assessor. \citet{DBLP:journals/corr/abs-2412-17156} crafted a run following the same approach used by~\citet{upadhyay2024umbrela} to pool the documents and build the \ac{LLM}-generated relevance judgments used for TREC 2024 RAG. Such a run achieved consistently higher effectiveness under the fully automatic evaluation paradigm compared to its performance based on manual relevance judgments. 

By releasing this collection of \ac{LLM}-generated relevance judgments we want to foster the analysis and study of possible sources of biases and systematic errors, to mitigate them and allow for the development of more effective and robust future solutions that involve \acp{LLM} as tools to support the annotation process.