\section{LLMJudge Resource}
\label{sec:llmjudge_resource}
This section details how we designed the \texttt{LLMJudge} challenge task, the data construction process, and the evaluation metrics.

\begin{figure}
    \centering
    \subfloat[Dev set\label{fig:sample-dev}]
    {
        {
            \includegraphics[scale=0.26]{figs/data/Full-Human_Sample-Human_black_dev.pdf}
        }
    }%
    % \quad
    \subfloat[Test set\label{fig:sample-test}]
    {
        {
            \includegraphics[scale=0.26]{figs/data/Full-Human_Sample-Human_black_test.pdf}
        }
    }%
    \caption{Samples correlation with TREC 2023 DL full qrel}%
    \label{fig:data-samples}%
\end{figure}

\subsection{LLMJudge Task}
The task of the LLMJudge challenge is, given the query and passage as input, how they are relevant. Similar to TREC 2023 Deep Learning track \cite{craswell2024overview}, we use \textit{four-point scale} judgments to evaluate the relevance of the query to the passage as follows:

\begin{itemize}
    \item\textbf{[3] Perfectly relevant}: The passage is dedicated to the query and contains the exact answer. 
    \item\textbf{[2] Highly relevant}: The passage has some answers for the query, but the answer may be a bit unclear, or hidden amongst extraneous information. 
    \item\textbf{[1] Related}: The passage seems related to the query but does not answer it. 
    \item\textbf{[0] Irrelevant}: The passage has nothing to do with the query. 
\end{itemize}

More specifically, the LLMJudge challenge is, by providing the datasets that include queries, passages, and query-passage files to participants, to ask LLMs to generate a score [0, 1, 2, 3] indicating the relevance of the query to the passage.

\subsection{LLMJudge Data}
The \texttt{LLMJudge} challenge dataset is built upon the passage retrieval task dataset of the TREC 2023 Deep Learning track\footnote{\url{https://microsoft.github.io/msmarco/TREC-Deep-Learning.html}} \cite{craswell2024overview}. The TREC 2023 Deep Learning track qrel consists of 82 queries, including 51 real queries and 31 synthetic queries (13 generated by T5 and 18 generated by GPT-4). To create a dev and test set similar to the TREC 2023 Deep Learning track full qrel, we randomly sampled 15 queries from 51 real queries, 5 queries from T5 queries, and 5 queries from GPT-4 queries for each set. Figure \ref{fig:data-samples} shows Kendall's $\tau$ correlation of the TREC 2023 Deep Learning track run submission on LLMJudge sampled dev and test sets with the TREC 2023 Deep Learning track full qrel. Table \ref{tbl:llmjudge-dataset} shows the statistics of the \texttt{LLMJudge} challenge datasets. The test set is used for the generation of judgment by participants, while the development set could be used for few-shot or fine-tuning purposes.

\input{tables/tbl_dataset}

\subsection{Evaluation}
We evaluate submission results on two different levels, the correlation of the judgments and the ranking correlation of systems evaluated using judgment submissions:

\begin{itemize}
    \item \textbf{Label Correlation.} We use the automated evaluation metrics Cohen's Kappa ($\kappa$) and Krippendorff's Alpha ($\alpha$) on human judgments and the judgments submitted by participants;
    \item \textbf{System Ranking Correlation.} We use Kendall's Tau ($\tau$) and Spearman's rank ($\rho$) correlation to evaluate the system ordering of TREC 2023 Deep Learning Track \cite{craswell2024overview} submitted systems on human judgments and participants' LLM-based judgments.
\end{itemize}

We use \texttt{scikit-learn}\footnote{\url{https://scikit-learn.org/stable/index.html}} to compute Cohen's $\kappa$, Kendall's $\tau$, Spearman's $\rho$. Krippendorff's $\alpha$ is also calculated using the Fast Krippendorff\footnote{\url{https://github.com/pln-fing-udelar/fast-krippendorff}} Python package. 

\subsection{Publicly Available Resources}
To facilitate research in the area we have made the LLMJudge dataset, sample prompt, quick starter for automatic judgment, submitted runs, prompts, codes for quick starting the evaluation, and more detailed results publicly available on the \texttt{LLMJudge} webpage at: \url{https://llm4eval.github.io/LLMJudge-benchmark/}.