\section{Conclusion}
\label{sec:conclusion}
We introduce the LLMJudge resource, which builds upon the foundations established by the LLMJudge challenge \cite{rahmani2024llmjudge} at the LLM4Eval workshop \cite{rahmani2024llm4eval,rahmani2024report} co-located with SIGIR 2024. This resource provides a benchmark for evaluating the effectiveness of LLMs in an automatic relevance judgment task, helping comparisons across different models and prompting strategies.
In this paper, we detail the 42 sets of relevance judgments for the TREC 2023 Deep Learning track submitted to the LLMJudge challenge by 8 different international teams. 
We release this collection to serve multiple purposes. First, they can be used as a comparison for future LLM-based relevance assessments, allowing research teams that did not participate in the challenge to compare their approaches as well. Secondly, this resource can serve as a tool to determine or help empirically study the presence of systematic biases in LLM-generated relevance judgments, impacting a large body of research in the community.
Among the submitted sets of relevance judgments, 5 employ fine-tuning, and their results show that fine-tuning can lead to the highest correlation. Furthermore, 16 submissions are based on proprietary LLMs and 26 on open-source LLMs. Our analyses show that the latter tend to be more stable, while the former are affected by higher variability.
Methodologically, we provide in this paper a set of strategies to compare multiple automatically-generated relevance assessments that can serve future practitioners in determining the effectiveness of new LLMs as assessors.
In future work, we plan to investigate how LLM can be used to generate relevance judgment in a nugget-based evaluation scenario and extend the analysis to fully automatic collections, that include automatically generated queries and documents.