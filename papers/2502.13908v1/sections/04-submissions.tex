\section{Submitted Runs}
\label{sec:intro-methods}
We provide all submitted runs as a resource for future research and comparison. The submissions include 9 \emph{baseline approaches} developed by the organizers and 33 \emph{methods from participating teams}. Analysis of these submissions reveals several methodological directions in \ac{LLM}-based relevance assessment, focusing on \emph{prompting techniques, model adaptation, multi-phase evaluation, aggregation strategies, and classification-based refinement}.  

\input{tables/tbl_info}

Most submissions implement either \emph{direct prompting} or \emph{criteria decomposition pipelines}. \emph{Direct prompting methods} range from simple \emph{relevance scoring instructions} to \emph{chain-of-thought reasoning}, where \acp{LLM} justify their judgments before assigning a score. Some approaches explore \emph{zero-shot prompting}, while others incorporate \emph{semantic label assignments, linguistic alignment, or multi-prompt aggregation} to improve consistency and reduce overestimation biases.  
Beyond prompting, some teams \emph{fine-tune LLMs on relevance datasets}, including \emph{TREC Deep Learning track qrels} and the \emph{LLMJudge development set}, testing different \emph{model sizes (8B vs.~70B)} to assess the impact of adaptation on evaluation performance.  
A subset of submissions structures evaluation into \emph{multi-phase pipelines}, applying \emph{binary filtering before graded scoring}, \emph{question-based reasoning}, or \emph{decision trees} to refine assessments. Other approaches decompose relevance into \emph{specific dimensions} such as \emph{exactness, coverage, topicality, and contextual fit}, or employ \emph{nugget-based assessments} for more granular judgments.  
To enhance robustness, several methods \emph{combine outputs from multiple prompts or models} using \emph{multi-prompt averaging, binary-to-graded conversions, or conservative ensembling} to stabilize scores. Others treat \emph{relevance assessment as a classification task}, extracting features from LLM outputs and training \emph{Machine Learning classifiers} to refine final scores and improve alignment with human judgments.  

Below we detail the baselines and a summary of the submitted runs. We also summarize the submission details in Table \ref{tab:infos}.

\partitle{LLMJudge Baseline}
The baseline judges provided by the LLMJudge challenge organizers serve as reference methods for evaluation. Three distinct approaches are proposed as baselines: \texttt{llmjudge\\-simple}, \texttt{llmjudge-cot}, and \texttt{llmjudge-thomas}. The \texttt{llmjudge-\\simple} method employs a straightforward prompt, instructing the model to directly provide a relevance judgment based on the query and passage. In contrast, \texttt{llmjudge-cot} adopts a chain-of-thought (CoT) approach, prompting the model to articulate its reasoning process before delivering a judgment. Lastly, \texttt{llmjudge-thomas} incorporates the prompt design introduced by \cite{thomas2023large}, offering an alternative strategy for evaluation.

\partitle{NISTRetrieval-instruct}
This is a submission from NIST which has three different variants, namely, \texttt{NISTRetrieval-instruct0}, \texttt{NISTRetrieval-instruct1}, and \texttt{NISTRetrieval-instruct2} that aims to investigate the reproducibility of the method proposed by \citet{thomas2023large} and the reproducibility capabilities of LLMs when we used them for automatic relevance judgment.

\partitle{NISTRetrieval-reason}
Similar to \texttt{NISTRetrieval-instruct}, this NIST submission includes three related methods -- \texttt{NISTRetrieval-\\reason0}, \texttt{NISTRetrieval-reason1}, and \texttt{NISTRetrieval-reason2}. The team observed that prompting LLMs to provide reasoning across various tasks could improve response quality. To examine whether this approach could also enhance relevance judgment, they modified the prompt from \citet{thomas2023large} to allow the LLM to generate reasoning. These three runs were included to assess the reproducibility capabilities of LLMs when used for evaluation.

\partitle{Prophet-setting}
This method builds on the idea of fine-tuning an LLM with different available datasets for automatic relevance judgment, as described in \cite{meng2024query}\footnote{Code is available at \url{https://github.com/ChuanMeng/QPP-GenRE}}. Specifically, they fine-tuned \texttt{Llama-3-8B} under three different settings, training the model for five epochs in each. These settings include: \texttt{Prophet-setting1}, fine-tuned on the LLMJudge development set; \texttt{Prophet-setting2}, fine-tuned on the qrels of TREC-DL 2019, 2020, and 2021; and \texttt{Prophet-setting4}, which combines fine-tuning on the qrels of TREC-DL 2019, 2020, and 2021 with the LLMJudge development set.

\partitle{William-umbrela1}
This approach is zero-shot prompting the LLM to produce relevance assessments. They used UMBRELA \cite{upadhyay2024umbrela} to generate relevance judgments using the prompting technique suggested by \citet{thomas2023large}. The team mentioned that ``\textit{I tried many different approaches, but I did not manage to find anything that really seemed to consistently improve on zero-shot. It seemed like this dataset may have been harder and/or noisier than others referenced in the literature -- on my development set it was hard to get $> 0.3$ Cohen's $\kappa$, whereas the literature mentions values of $0.4$ up to $0.6$ even.}''.

\partitle{William-umbrela2}
The main idea of this method is to take the approach from the UMBRELA \cite{upadhyay2024umbrela} -- zero-shot prompting technique from \citet{thomas2023large}, but to see if the performance would be improved by asking the model to output semantic labels (i.e., \textit{Irrelevant}, \textit{Related}, \textit{Highly relevant}, \textit{Perfectly relevant}), rather than a numerical score (i.e., 0, 1, 2, 3).

\partitle{William-umbrela3}
This method is an ensemble of~ \texttt{William-\\umbrela1} and \texttt{William-umbrela2} approaches by taking the \textit{min}. The team mentioned that ``\textit{The logic behind using min as an aggregator is that in this dataset, it pays to be conservative in the rating. They also said that on a subset of the training data that they held out for testing, this ensembling approach outperformed either of the two other approaches (i.e., William-umbrela1 and William-umbrela2)''}.

\partitle{H2oloo-fewself}
This method uses the best prompt proposed by Thomas et al.~\cite{thomas2023large} to instruct GPT-4o. It incorporates few-shot examples to guide the model in distinguishing between relevant labels effectively.

\partitle{H2oloo-zeroshot1}
This method fined-tuned a Llama-8B using the TREC DL 2019 to 2022 qrels for relevance judgment prediction.

\partitle{H2oloo-zeroshot2}
This method fined-tuned a Llama-8B using the TREC DL 2019 to 2022 qrels and the LLMJudge dev set qrel for relevance judgment prediction.

\partitle{Olz-gpt4o}
This method uses a simple prompt where they just ask for the relevance judgment without any special techniques. The idea is to see how models can solve relevance judgment tasks without considering any particular prompting or fine-tuning techniques. The primary goal is to assess whether a low-effort prompt could reliably derive relevance labels from LLMs that are practically usable.

\partitle{Olz-exp}
This method is similar to \texttt{Olz-gpt4o} but they also asked LLM to reason its judgment as part of the evaluation.

\partitle{Olz-halfbin}
This method leverages Llama-3 models with $8B$ and $70B$ parameters to assess document relevance using nine distinct prompts. These prompts are divided into two categories: four \emph{graded relevance prompts}, which instruct the model to assign a score from 0 to 3 with slight instruction variations, and five \emph{binary relevance prompts}, which require binary judgments with different definitions of relevance.  
Both model variants generate outputs for all nine prompts. These outputs serve as features for training a logistic regression classifier, which produces the final graded labels. Training is conducted using labels generated by GPT-4o (via the \texttt{Olz-gpt4o} method) rather than the standard development set annotations, based on the assumption that the development and test set labels may have been derived using different methods. Analyzing these discrepancies, the team found GPT-4oâ€™s judgments more aligned with their expectations, leading to its adoption as the primary reference for training.

\partitle{Olz-somebin}
The procedure of this method is identical to the \texttt{Olz-halfbin} method, except the logistic regression classifier was trained on the provided development set labels instead of those generated by GPT-4o (using \texttt{Olz-gpt4o} method).

\partitle{Olz-multiprompt}
This method, instead of using a classifier like \texttt{Olz-halfbin} and \texttt{Olz-somebin}, directly aggregated the relevance judgments by averaging. The binary labels were first scaled by multiplying them by three (to convert them into $0$ or $3$). Then a simple average was calculated across the nine prompts and rounded on a scale of 0 to 3, and the resulting value served as the final graded label.

\partitle{RMIT-IR}
This submission introduces three relevance assessors, \texttt{RMITIR-GPT4o}, \texttt{RMITIR-llama38b}, and \texttt{RMITIR-llama70B}. The proposed approach begins by having the LLM provide a binary relevance judgment to filter out irrelevant queries and improve irrelevance filtering. Next, three scores are generated, and averaged, and the result is rounded to produce the final score. The method was tested using three different LLMs: GPT-4o (\texttt{RMITIR-GPT4o}), Llama3-8B (\texttt{RMITIR-llama38b}), and Llama3-70B (\texttt{RMITIR-llama70B}). The team noted that ``\textit{GPT-4o appears to be the best-performing model based on our experiences}.''

\partitle{TREMA-4prompt}   
This method evaluates passage relevance by decomposing it into four specific criteria: exactness (how precisely the passage answers the query), coverage (proportion of content discussing the query), topicality (subject alignment between passage and query), and contextual fit (presence of relevant background). The evaluation follows a two-phase process where each criterion is first assessed independently and then combined through a final prompt to determine the overall relevance label. Full details of the criteria and rationale are provided in \cite{farzi2024best}.

\partitle{TREMA-CoT}
This method implements a chain-of-thought evaluation process inspired by \citet{Sun2023IsCG}. The approach consists of three phases: First, the \ac{LLM} makes a binary relevance judgment (yes/no) of the passage. Based on this judgment, different relevance criteria are evaluated in the second phase - for relevant passages, exactness and coverage are assessed, while non-relevant passages are evaluated on contextual fit and topicality (all scored 0-3). In the final phase, these scores determine the overall relevance label: relevant passages receive labels 2-3 based on exactness and coverage scores, while non-relevant passages receive labels 0-1 based on contextual fit and topicality assessment.

\partitle{TREMA-other}
This approach investigates whether aligning the linguistic styles of queries and passages can enhance relevance judgments. In the first phase, the LLM generates a query-like representation for each passage, designed to match the query's linguistic style and length. This generated query serves as a summary of the passage's content, formatted in a way that aligns with typical query phrasing. In the second phase, the LLM evaluates the similarity between the original query and the generated query on a scale from 0 to 3, corresponding to the relevance labeling system. Higher similarity scores indicate a stronger alignment between the passage's content and the query's intent. This method integrates linguistic style alignment with content relevance to improve relevance labeling.

\partitle{TREMA-sumdecompose}
This method consists of two phases. Phase one is identical to the \texttt{TREMA-4prompt} method, where the ``relevance'' is decomposed into four criteria, leading to four criteria-specific grades.  In Phase Two, the individual grades from Phase One are summed to produce a total grade. Based on this total, a final relevance label between 0 and 3 is assigned to each query-passage pair: a total grade of 10-12 yields a relevance label of 3, 7-9 yields a relevance label of 2, 5-6 yields a relevance label of 1, and 0-4 yields a relevance label of 0.

\partitle{TREMA-naiveBdecompose}
This method consists of two phases. Phase one is identical to the \texttt{TREMA-4prompt} method, where the ``relevance'' is decomposed into four criteria, leading to four criteria-specific grades. In phase two, these decomposed grades are aggregated into a final relevance label using a Gaussian Naive Bayes model, implemented with Scikit-learn's GaussianNB() classifier. The model is trained on the decomposed feature grades and then predicts the relevance label for each passage.

\partitle{TREMA-rubric0}
This method is based on the RUBRIC Autograder Workbench \cite{dietz2024workbench}. This method defines the relevance of the query via 10 open-ended questions. The questions are generated using the ChatGPT 3.5 model. Each passage is scanned whether it is possible to answer each of the questions (and how well), which is captured as a grade. They use the FLAN-T5-large LLM from Huggingface to grade the answerability from 0 (worst) to 5 (best). Details and prompts are available in the Workbench benchmark \cite{dietz2024workbench}. The grades are mapped to relevance labels by a heuristic mapping on the second-highest grade achieved on any of the questions. Grade 5 is mapped to relevance label 3, grade 4 is mapped to label 1 and all other grades are mapped to label 0. This was the best manual mapping on the dev set \cite{farzi2024rubric}.

\partitle{TREMA-questions}
Same question and grading as in \texttt{TREMA\\-rubric0}, but uses a more elaborate calibration for converting grades to relevance labels, based on \texttt{scikit-learn}'s ExtraTrees classifier. The classifier is based on features that include ranked grades for each question (sorted in descending order), ranked question difficulty (based on average grades across the pool), and counts of correct answers at various grade thresholds (e.g., number of answers graded 5, 4 or better, etc.). Each of these features is encoded using both one-hot and numerical representations to capture detailed information about question-based relevance. The classifier is trained on the dev set.

\input{tables/tbl_results}

\partitle{TREMA-nuggets}
Same approach as \texttt{TREMA-questions}, but uses 10 open-ended key fact nuggets instead of questions, along with an adapted prompt that assesses whether key facts are mentioned in the passage. The same ExtraTrees classifier with the same features is used for converting grades into relevance labels.

\partitle{TREMA-direct}
This approach focuses exclusively on features of direct relevance
labeling methods, which instruct an LLM to judge whether a passage is relevant for a query, using a variety of prompts from \citet{Sun2023IsCG}, \citet{faggioli2023perspectives}, and HELM \cite{liang2022holistic}. The model excludes question-based and nugget-based features, simplifying its input to focus solely on the predictive power of direct labeling. The relevance labels are obtained with an ExtraTrees classifier trained on the dev set. Features include binary or multi-class predictions from labeling approaches. Each label is encoded using both one-hot and numerical encodings to capture both categorical and ordinal aspects of the predictions. This approach is computationally lighter than TREMA-all and serves as a baseline to evaluate how well direct relevance labels alone can predict passage relevance.

\partitle{TREMA-all}
This approach incorporates all features from \texttt{TREMA-\\questions}, \texttt{TREMA-nuggets}, and \texttt{TREMA-direct} approaches via a single ExtraTrees classifier that is trained on the dev set.