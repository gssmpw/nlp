\section{Introduction}
\label{sec:introduction}
The Cranfield paradigm has been the standard \ac{IR} evaluation methodology since the 1960s \cite{Cleverdon1960TheAC,10.5555/275537.275544}. This methodology requires three components to evaluate an \ac{IR} system: a document corpus, topics (queries), and \textit{corresponding relevance judgments} that indicate which documents are relevant to each topic.

In practice, an \ac{IR} system processes the topics to retrieve relevant documents from the corpus, while the relevance judgments serve as the ground truth for measuring system effectiveness. Of these three components, the first two are relatively straightforward to acquire. The document corpus can be constructed to mirror the target domain of the \ac{IR} system through various collection methods, such as web crawling, newspaper archives, or scientific literature databases. Similarly, topics can be manually handcrafted by non-experts or through the systematic analysis of query logs \cite{DBLP:conf/nips/NguyenRSGTMD16}, ensuring they represent an appropriate sample of the expected query space.

% Since its definition in the second half of the XX century, the Cranfield paradigm has been considered the standard evaluation procedure for \ac{IR} \cite{Cleverdon1960TheAC,10.5555/275537.275544}. Following its definition provided by~\citet{Cleverdon1960TheAC,10.5555/275537.275544}, to evaluate an \ac{IR} system, the Cranfield paradigm requires using an experimental collection composed of three elements, a corpus of documents, a set of topics, and a set of relevance judgments.
% In detail, the \ac{IR} system retrieves the documents from the corpus in response to the topics and the relevance judgments are used to quantify the effectiveness of the \ac{IR} system.
% The corpus and the set of topics are relatively easy to obtain. The corpus should mimic the documents on which the \ac{IR} system will be applied and can be obtained, for example, by crawling web pages, newspapers, or scientific articles. Similarly, the topics should be an independent and identically distributed representative subset of queries that the system under evaluation will address. Such topics can be manually handcrafted by non-experts or extracted from query logs~\cite{DBLP:conf/nips/NguyenRSGTMD16}.

% The major challenge is related to obtaining relevance judgments. Such relevance judgments put in relation topics and documents, describing the relevance degree of the document in satisfying the information need of the user represented by the topic.
The main challenge lies in obtaining relevance judgments. These judgments map topics to documents, specifically indicating how well each document satisfies the information need expressed in the topic. Creating complete relevance judgments requires significant human effort and careful quality control to ensure consistency and reliability~\citep{DBLP:journals/ftir/Sanderson10}.

Over time, three major approaches became the de facto standard to collect relevance judgments. The first approach, followed by the major evaluation campaigns such as TREC~\cite{DBLP:journals/ipm/Jones95}, NTCIR~\cite{kando1999overview} and CLEF~\cite{DBLP:conf/clef/2000}, is based on editorial assessments. In this case,  professional assessors judge whether a document is relevant in response to the topic. While these judgments are often of very high quality, they are also expensive to obtain in terms of time and cost~\cite{DBLP:journals/ftir/Sanderson10}.
A second strategy is based on employing crowd-assessors to annotate the documents. While crowd annotations are typically less expensive than editorial annotations, they are also qualitatively inferior. Crowd annotations often contain much more noise and errors.
Thirdly, annotations can be obtained as implicit feedback from user - \ac{IR} system interaction. These annotations are virtually free as they are based mostly on already available data (e.g., click logs) and can embed user-specific characteristics, such as their knowledge, tastes, and personal inclinations. Nevertheless, implicit feedback is also affected by noise, biases~\cite{JoachimsSwaminathanSchnabel2017}, and privacy problems \cite{DBLP:journals/tissec/AonghusaL16}.
In general, the types of annotations can be organized in a spectrum: on one side we have accurate and costly editorial judgments, in the middle we have labels produced by the crowd, on the other side, we find inexpensive but imprecise and biased implicit feedback.

\acp{LLM} have recently emerged as a promising fourth approach to gather relevance judgments~\cite{DBLP:conf/sigir/MacAvaneyS23,faggioli2023perspectives,thomas2023large,rahmani2024synthetic}. Initial experiments show that LLMs can achieve comparable performance to crowd workers on standard \ac{IR} tasks~\citep{DBLP:conf/sigir/BlancoHHMPTT11} and potentially reduce annotation costs. 
% investigated whether \acp{LLM} can help in further extending this spectrum. 
% In particular, \acp{LLM} appear to be effective predictors of the relevance judgments~\cite{thomas2023large} and might reduce the effort to collect annotations~\cite{faggioli2023perspectives}. 
The \texttt{LLMJudge} challenge \cite{rahmani2024llmjudge} was organized as part of the \texttt{LLM4Eval}\footnote{\url{https://llm4eval.github.io/}} workshop~\cite{rahmani2024llm4eval} at SIGIR 2024 as a shared task to study the effectiveness of using \acp{LLM} as annotation tools.
While \acp{LLM} have shown to be effective annotation tools, several aspects are yet to be understood. For example, it is not clear the impact of changing the prompt, which biases are present in the \ac{LLM}-generated relevance judgments, and if there is a risk of evaluation circularity~\cite{faggioli2023perspectives}. Beyond these challenges, there is also a need to explore the effectiveness of ensemble models, examine the trade-offs between different LLM-based and human assessments, and develop more advanced methodologies to enhance automated evaluation techniques. We release the relevance annotations produced by the teams participating in the \texttt{LLMJudge} challenge, to help the community investigate these aspects linked to using \ac{LLM} as automatic annotation tools.

 %Test collections are essential for evaluating \ac{IR} systems. The evaluation and tuning of a search system is largely based on relevance labels, which indicate whether a document is useful for a specific search and user. However, collecting relevance judgments on a large scale is costly and resource-intensive. Consequently, typical experiments rely on third-party labelers who may not always produce accurate annotations. 
 %The \texttt{LLMJudge} challenge aims to explore an alternative approach by using \acp{LLM} to generate relevance judgments. Recent studies have shown that LLMs can generate reliable relevance judgments for search systems. However, it remains unclear which LLMs can match the accuracy of human labelers, which prompts are most effective, how fine-tuned open-source LLMs compare to closed-source LLMs like GPT-4, whether there are biases in synthetically generated data, and if data leakage affects the quality of generated labels. This challenge will investigate these questions, and the collected data will be released as a package to support automatic relevance judgment research in information retrieval and search.

% The detailed description of the challenge can be found in the following document.

Our contributions are the following:
\begin{itemize}
    \item We release 42 pools of automatically generated relevance judgments produced by 8 different research teams that participated in the \texttt{LLMJudge} challenge.
%    \item We provide an analysis of such LLM-generated relevance assessments, observing \todo{...}.
    \item We confirm current observations about the state of the art, noticing that, while many approaches maintain ranking consistency, their absolute scoring tendencies differ, potentially introducing biases in evaluation.
    \item From the methodological perspective, we investigate several approaches that can be adopted to assess the effectiveness of an LLM-based relevance judgment process and provide a set of figures that will serve future researchers as baselines. 
\end{itemize}

The remainder of this paper is organized as follows: Section~\ref{sec:relatedwork} introduces the related works, including the challenges associated with automatically generated relevance judgments. Section~\ref{sec:llmjudge_resource} delineates the structure and describes the collection of the LLMJudge resource. Section \ref{sec:intro-methods} summaries submission runs of \texttt{LLMJudge} challenge. In Section~\ref{sec:results}, we analyze the dataset and provide some insights on the \ac{LLM}-generated judgments. Finally, in Section~\ref{sec:conclusion}, we draw our conclusion and outline our future work.