\section{Results}
\label{sec:results}

\input{tables/tbl_kappa}

\subsection{Descriptive Statistics}
For the LLMJudge challenge, we received a total of 42 submissions (i.e., the 42 labelers) from 8 teams. Of these, 9 runs were from the organizers, who contributed a range of baselines; these are included in the statistics. Among the submissions, 5 utilized fine-tuning for relevance judgment, while the rest relied on various prompting techniques. Moreover, 16 submissions are based on proprietary LLMs, whereas 26 used open-source LLMs.

\subsection{Methodological Comparison}
Here we provide an overview analysis of the submissions based on different methodological directions they applied for their LLM-based relevance assessments. Table \ref{tab:main-results} presents the results of submissions at the label and system ranking correlation. Comparing submissions concerning the models they used, we can see that while \texttt{willia-umbrela1} (GPT-4o) achieves the best Cohen's $\kappa$, \texttt{h2oloo-zeroshot1} (Llama3-8B) ranked second by only 1.61\% differences. More interestingly, when we compare the system ranking correlation (i.e., Kendall's $\tau$) of submissions, the best non-fine-tuned method is \texttt{TREMA-4prompts} which uses Llama3-8B. Previous studies \cite{thomas2023large,rahmani2024judgeblender} have shown the importance of the prompting techniques for automatic relevance judgments, and analyzing the LLMJudge submission results confirms the importance of the effect of prompt engineering. For instance, \texttt{TREMA-4prompts} uses the criteria decomposition technique by breaking down the concept of relevance into various criteria and generating the relevance label by asking the model about the specified criteria. Few submissions used fine-tuning, however, their results show that fine-tuning can lead to the highest correlation. For instance, \texttt{h2oloo-fewself} achieved the highest Krippendorff's $\alpha$ (0.4958, the best among all methods) and \texttt{prophet-setting2} is the best-performing submission considering Kendall's $\tau$. This confirms that fine-tuning can significantly enhance agreement with human judgments. Submissions that included both numerical and semantic labels tend to perform consistently across all evaluation metrics compared to those using only numerical labels. For example, submissions from the ``Olz'' team rank among the comparable submissions across all four evaluation metrics. In the following, we provide a more detailed discussion and analysis.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/results/llmjudge-kappa-vs-tau.pdf}
    \caption{Cohen's $\kappa$ vs.~Kendall's $\tau$}
    \label{fig:kappa-vs-tau}
\end{figure}

\subsection{Overall Results}
The results of the LLMJudge challenge, as presented in Table \ref{tab:main-results}, reveal significant variability in performance across the evaluated metrics, including Cohen's Kappa ($\kappa$), Krippendorff's Alpha ($\alpha$), Kendall's Tau ($\tau$), and Spearman’s Rho ($\rho$). Submissions such as \texttt{h2oloo-fewself}, \texttt{h2oloo-zeroshot1}, and \texttt{willia-umbrela1} emerge as top performers, achieving $\kappa$ values above $0.27$ and $\alpha$ values around $0.49$, indicative of strong agreement and reliability. These methods also demonstrate robust ranking capabilities, with $\tau$ and $\rho$ values exceeding $0.9$. This combination of high agreement and ranking correlation suggests that these approaches are well-suited for relevance judgment tasks. In contrast, submissions like \texttt{TREMA-nuggets} and \texttt{TREMA-rubric0} perform poorly, with $\kappa < 0.1$ and $\alpha < 0.2$, reflecting low agreement and reliability. Despite their low scores on agreement metrics, some of these models maintain moderate ranking correlations, indicating limited but specific utility in ranking-focused scenarios.

A comparison of group trends highlights the strengths and weaknesses of different methodological approaches. For instance, \texttt{NISTRetrieval} submissions consistently achieve high $\tau$ and $\rho$ values ($>0.9$), reflecting strong ranking performance, yet their lower $\kappa$ (\textasciitilde0.18) and $\alpha$ (\textasciitilde0.38) suggest limited alignment with human relevance judgments. In contrast, methods like \texttt{Olz-multiprompt} and \texttt{h2oloo-fewself} demonstrate comparable performance across all metrics. \texttt{Olz-multiprompt} leverages outputs from multiple prompts and \texttt{h2oloo-fewself} incorporates few-shot examples using a proprietary model, GPT4o, these approaches effectively mitigate individual model biases and enhance both reliability and agreement. On the other hand, single-model approaches, such as \texttt{TREMA-direct} and \texttt{llmjudge-simple1}, exhibit limited performance, underscoring the challenges faced by individual models in capturing the complexity of relevance judgment tasks.

\subsection{Label Correlation (Cohen's $\kappa$)}
Table \ref{tab:kappa} presents Cohen's $\kappa$ values for various submissions, providing insights into the agreement between relevance judgments under different granularity levels: 4-point, 0|123, 01|23, and 012|3. Across all groupings, there is noticeable variability in $\kappa$ scores, highlighting differences in consistency among submissions. Submissions like \texttt{h2oloo-fewsel} and \texttt{h2oloo-zeroshot1} demonstrate relatively high agreement across all groupings, particularly excelling in the 4-point and binary (0|123) categories, with $\kappa$ values exceeding 0.27 in most cases. In contrast, submissions such as \texttt{TREMA-nuggets} and \texttt{TREMA-rubric0} exhibit significantly lower agreement, with $\kappa$ values as low as 0.0604 and 0.0779 on the 4-point scale, reflecting limited reliability in their judgments.

In particular, coarse-grained groupings like 0|123 tend to produce higher $\kappa$ values than finer groupings like the 4-point scale, suggesting that systems or annotators achieve better consistency when relevance levels are aggregated. However, the 012|3 grouping, which isolates the highest relevance level, introduces greater variability, with some systems such as \texttt{willia-umbrela1} performing well, while others struggle to maintain consistency. These findings emphasize the importance of evaluation granularity in understanding system reliability and identifying approaches with stable performance across diverse grouping strategies.

\subsection{Cohen's $\kappa$ vs.~Kendall's $\tau$}
Figure \ref{fig:kappa-vs-tau} shows the performance of submitted runs on the \texttt{LLMJudge} test set. The x-axis represents Cohen's $\kappa$, and the y-axis shows the submission agreement on system order. Submissions exhibit low variability in Kendall's $\tau$ but greater variability in Cohen's $\kappa$. Most submissions cluster within a narrow range of $\tau$ values, indicating consistent system rankings but more variation in inter-rater reliability, as measured by Cohen's $\kappa$. This suggests that while submissions generally agree on rankings, their exact labels are less consistent, leading to the observed variability in $\kappa$.

\subsection{Average Label Comparison}

\begin{figure}
    \centering
    \subfloat[Avg.~Label vs.~Kendall $\tau$\label{fig:avg-labels-1}]
    {
        {
            \includegraphics[scale=0.2]{figs/results/avglabel-vs-kendall.pdf}
        }
    }%
    % \quad
    \subfloat[Avg.~Label vs.~Krippendorff's $\alpha$\label{fig:avg-labels-2}]
    {
        {
            \includegraphics[scale=0.2]{figs/results/avglabel-vs-kripp.pdf}
        }
    }%
    \caption{Average Labels}%
    \label{fig:avg-labels}%
\end{figure}

Figure \ref{fig:avg-labels} illustrates the relationship between ranking correlation metrics and the average label assigned by different evaluation methods for NDCG@10. The \textcolor{orange}{orange} dashed line represents the human average label (0.90), serving as a baseline for comparison. In Figure \ref{fig:avg-labels-1}, most methods exhibit strong agreement in ranking order, with values generally above 0.85. However, the assigned average labels vary significantly, with some methods, such as \texttt{TREMA-other} and \texttt{TREMA-4prompts}, assigning scores notably above the human baseline, while others, like \texttt{TREMA-rubric0} and \texttt{prophet-setting4}, produce lower scores. These variations indicate that while many approaches maintain ranking consistency, their absolute scoring tendencies differ, potentially introducing biases in evaluation.

Figure \ref{fig:avg-labels-2} provides further insight into inter-method agreement, capturing not just ranking order but also overall consistency in score distributions. Here, we observe a wider range of correlation values, with some methods achieving moderate agreement (e.g., \texttt{Olz-gpt4o} and \texttt{willia-umbrella2}) while others, such as \texttt{TREMA-rubric0} and \texttt{prophet-setting4}, show very low agreement and lower assigned scores. Notably, \texttt{TREMA-4prompts} and \texttt{TREMA-\\other} again stand out with higher average labels, but their variability suggests differences in how they align with human judgments. These findings emphasize the importance of calibration when aggregating synthetic judgments, as different methods may systematically overestimate or underestimate relevance scores despite high-ranking agreement.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/results/alpha-vs-tau.pdf}
    \caption{Krippendorff's $\alpha$ vs.~Kendall's $\tau$}
    \label{fig:alpha-vs-tau}
\end{figure}

\subsection{Krippendorff's \texorpdfstring{$\alpha$}{alpha} vs.~Kendall's \texorpdfstring{$\tau$}{tau}}
Figure \ref{fig:alpha-vs-tau} shows the relation between Krippendorff's $\alpha$ and Kendall's $\tau$, highlighting the agreement of different models with human judgments. Higher Krippendorff's $\alpha$ generally corresponds to better Kendall’s $\tau$, but with notable variance. Models like \texttt{prophet-setting2} and \texttt{TREMA-4prompts} achieve strong ranking consistency, while \texttt{TREMA-rubric0} and \texttt{TREMA-other} show weaker agreement. Proprietary models such as \texttt{h2oloo-fewself} and \texttt{Olz-gpt4o} perform well in both metrics, whereas some open-source models are more dispersed.

\subsection{Binarized Cohen's $\kappa$ vs.~Kendall's $\tau$}
Figure \ref{fig:binkappa-vs-tau} compares binary agreement (Cohen's $\kappa$ 01|23) with Kendall's $\tau$ across models. Higher kappa values tend to align with stronger ranking consistency, as seen with \texttt{prophet-setting2}, \texttt{TREMA-\\4prompts}, and \texttt{RMITIR-llama70B}. However, some models, like \texttt{TREMA-\\rubric0} and \texttt{TREMA-other}, show weak agreement despite moderate ranking correlations. Proprietary models, including \texttt{RMITIR-GPT4o} and \texttt{h2oloo-fewself}, perform competitively, suggesting that both fine-tuning and prompting strategies impact these relationships.

\subsection{LLMJudge Resource Use Cases}
The LLMJudge benchmark can be considered as a resource for evaluating the reliability of LLM-generated relevance judgments across different settings. For instance, JudgeBlender \cite{rahmani2024judgeblender} which is a framework that aggregates evaluations from multiple smaller models to enhance the robustness of relevance assessments recently leveraged LLMJudge both as a baseline for comparison and as a tool for analyzing the variability and bias of ensemble-based judgment aggregation methods.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/results/binkappa-vs-tau.pdf}
    \caption{Binarized Cohen's $\kappa$ vs.~Kendall's $\tau$}
    \label{fig:binkappa-vs-tau}
\end{figure}