\begin{table}[ht]
    \centering
    \caption{Krippendorff's $\alpha$ correlation in 4-point scale agreement and difference binarize the judgment scale. Judgment levels to the left of the pipe are considered irrelevant, while those to the right are considered relevant.}
    \adjustbox{max width=\columnwidth}{%
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Submission ID} & \textbf{4-point} & \textbf{0|123} & \textbf{01|23} & \textbf{012|3} \\
    \midrule
    NISTRetrieval-instruct0 & 0.3819 & 0.2811 & 0.3021 & -0.0444 \\
    NISTRetrieval-instruct1 & 0.3812 & 0.2801 & 0.3021 & -0.0444 \\
    NISTRetrieval-instruct2 & 0.3821 & 0.2823 & 0.3013 & -0.0444 \\
    NISTRetrieval-reason0 & 0.3874 & 0.263 & 0.3381 & -0.0336 \\
    NISTRetrieval-reason1 & 0.3872 & 0.2624 & 0.3385 & -0.0336 \\
    NISTRetrieval-reason2 & 0.3874 & 0.262 & 0.3388 & -0.0336 \\
    Olz-exp & 0.4701 & 0.3941 & 0.3499 & 0.2933 \\
    Olz-gpt4o & 0.502 & 0.421 & 0.3619 & 0.3067 \\
    Olz-halfbin & 0.4536 & 0.4005 & 0.2534 & 0.2405 \\
    Olz-multiprompt & 0.4551 & 0.3737 & 0.3829 & 0.2137 \\
    Olz-somebin & 0.4471 & 0.3851 & 0.378 & 0.1014 \\
    RMITIR-GPT4o & 0.4108 & 0.3125 & 0.395 & 0.257 \\
    RMITIR-llama38b & 0.3873 & 0.3169 & 0.3194 & 0.1268 \\
    RMITIR-llama70B & 0.4873 & 0.416 & 0.3679 & 0.2839 \\
    TREMA-4prompts & 0.2888 & 0.2644 & 0.1888 & 0.1661 \\
    TREMA-CoT & 0.3852 & 0.3172 & 0.3176 & 0.18 \\
    TREMA-all & 0.3855 & 0.3191 & 0.2957 & 0.0618 \\
    TREMA-direct & 0.3729 & 0.315 & 0.3259 & 0.0868 \\
    TREMA-naiveBdecompose & 0.3579 & 0.2949 & 0.2916 & -0.018 \\
    TREMA-nuggets & 0.1691 & 0.1499 & 0.0967 & -0.0076 \\
    TREMA-other & 0.2712 & 0.2547 & 0.1477 & 0.1399 \\
    TREMA-questions & 0.3148 & 0.2562 & 0.2758 & 0.0125 \\
    TREMA-rubric0 & 0.1036 & 0.1172 & -0.0895 & 0.0167 \\
    TREMA-sumdecompose & 0.3926 & 0.3138 & 0.343 & 0.1995 \\
    h2oloo-fewself & 0.4958 & 0.4108 & 0.428 & 0.2978 \\
    h2oloo-zeroshot1 & 0.4812 & 0.4058 & 0.385 & 0.3063 \\
    h2oloo-zeroshot2 & 0.3898 & 0.3418 & 0.3175 & 0.2769 \\
    llmjudge-cot1 & 0.3218 & 0.1764 & 0.2788 & 0.116 \\
    llmjudge-cot2 & 0.3263 & 0.2173 & 0.2429 & 0.2002 \\
    llmjudge-cot3 & 0.487 & 0.3853 & 0.3979 & 0.2233 \\
    llmjudge-simple1 & 0.2808 & 0.05 & 0.2857 & 0.1528 \\
    llmjudge-simple2 & 0.3672 & 0.2317 & 0.3097 & 0.2003 \\
    llmjudge-simple3 & 0.4642 & 0.3581 & 0.397 & 0.2012 \\
    llmjudge-thomas1 & 0.3207 & 0.1679 & 0.278 & 0.1725 \\
    llmjudge-thomas2 & 0.3853 & 0.294 & 0.2891 & 0.2229 \\
    llmjudge-thomas3 & 0.4877 & 0.3909 & 0.3942 & 0.2321 \\
    prophet-setting1 & 0.4069 & 0.3419 & 0.2892 & 0.1677 \\
    prophet-setting2 & 0.3144 & 0.2815 & 0.2225 & -0.0093 \\
    prophet-setting4 & 0.1623 & 0.1627 & 0.0797 & 0.0006 \\
    willia-umbrela1 & 0.4918 & 0.4129 & 0.3939 & 0.3124 \\
    willia-umbrela2 & 0.4556 & 0.3961 & 0.3298 & 0.3193 \\
    willia-umbrela3 & 0.4535 & 0.3965 & 0.3314 & 0.3185 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:alpha}
\end{table}