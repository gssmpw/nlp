\begin{table}[]
    \centering
    \caption{Judgment and system ranking correlation of LLMJudge submissions. $\kappa$: Cohen's Kappa, $\alpha$: Krippendorff's alpha, $\tau$: Kendall's Tau, $\rho$: Spearmanâ€™s rank correlation. The best results per column are denoted in bold and the second best results are denoted in \textit{italic}.}
    \adjustbox{max width=\columnwidth}{%
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Submission ID} & \textbf{$\kappa$} & \textbf{$\alpha$} & \textbf{$\tau$} & \textbf{$\rho$} \\
    \midrule
    NISTRetrieval-instruct0 & 0.1877 & 0.3819 & 0.9440 & 0.9907 \\
    NISTRetrieval-instruct1 & 0.1874 & 0.3812 & 0.9440 & 0.9907 \\
    NISTRetrieval-instruct2 & 0.1880 & 0.3821 & 0.9440 & 0.9907 \\
    NISTRetrieval-reason0   & 0.1844 & 0.3874 & 0.9052 & 0.9810 \\
    NISTRetrieval-reason1   & 0.1845 & 0.3872 & 0.9009 & 0.9802 \\
    NISTRetrieval-reason2   & 0.1838 & 0.3874 & 0.9052 & 0.9810 \\
    Olz-exp                 & 0.2519 & 0.4701 & 0.9009 & 0.9819 \\
    Olz-gpt4o               & 0.2625 & 0.5020 & 0.8793 & 0.9758 \\
    Olz-halfbin             & 0.2064 & 0.4536 & 0.9085 & 0.9830 \\
    Olz-multiprompt         & 0.2445 & 0.4551 & 0.9267 & 0.9867 \\
    Olz-somebin             & 0.2109 & 0.4471 & 0.9042 & 0.9822 \\
    RMITIR-GPT4o            & 0.2388 & 0.4108 & 0.8966 & 0.9798 \\
    RMITIR-llama38b         & 0.2006 & 0.3873 & 0.8879 & 0.9758 \\
    RMITIR-llama70B         & 0.2654 & 0.4873 & 0.9353 & 0.9883 \\
    TREMA-4prompts          & 0.1829 & 0.2888 & \textit{0.9483} & \textbf{0.9919} \\
    TREMA-CoT               & 0.1961 & 0.3852 & 0.8956 & 0.9799 \\
    TREMA-all               & 0.1471 & 0.3855 & 0.9138 & 0.9863 \\
    TREMA-direct            & 0.1742 & 0.3729 & 0.9009 & 0.9819 \\
    TREMA-naiveBdecompose   & 0.1741 & 0.3579 & 0.9128 & 0.9838 \\
    TREMA-nuggets           & 0.0604 & 0.1691 & 0.8664 & 0.9718 \\
    TREMA-other             & 0.1408 & 0.2712 & 0.8276 & 0.9447 \\
    TREMA-questions         & 0.1137 & 0.3148 & 0.9095 & 0.9839 \\
    TREMA-rubric0           & 0.0779 & 0.1036 & 0.8276 & 0.9544 \\
    TREMA-sumdecompose      & 0.2088 & 0.3926 & 0.9300 & 0.9870 \\
    h2oloo-fewself          & 0.2774 & \textbf{0.4958} & 0.9085 & 0.9822 \\
    h2oloo-zeroshot1        & \textit{0.2817} & 0.4812 & 0.9181 & 0.9827 \\
    h2oloo-zeroshot2        & 0.2589 & 0.3898 & 0.8353 & 0.9604 \\
    llmjudge-cot1           & 0.1284 & 0.3218 & 0.9267 & 0.9871 \\
    llmjudge-cot2           & 0.1560 & 0.3263 & 0.9267 & 0.9875 \\
    llmjudge-cot3           & 0.2271 & 0.4870 & 0.9267 & 0.9851 \\
    llmjudge-simple1        & 0.0754 & 0.2808 & 0.9181 & 0.9863 \\
    llmjudge-simple2        & 0.1327 & 0.3672 & 0.8966 & 0.9790 \\
    llmjudge-simple3        & 0.2110 & 0.4642 & 0.9052 & 0.9810 \\
    llmjudge-thomas1        & 0.1236 & 0.3207 & 0.8664 & 0.9689 \\
    llmjudge-thomas2        & 0.1723 & 0.3853 & 0.8793 & 0.9750 \\
    llmjudge-thomas3        & 0.2293 & 0.4877 & 0.9181 & 0.9867 \\
    prophet-setting1        & 0.1823 & 0.4069 & 0.9042 & 0.9826 \\
    prophet-setting2        & 0.1757 & 0.3144 & \textbf{0.9516} & \textit{0.9914} \\
    prophet-setting4        & 0.1471 & 0.1623 & 0.8568 & 0.9608 \\
    willia-umbrela1         & \textbf{0.2863} & \textit{0.4918} & 0.9009 & 0.9806 \\
    willia-umbrela2         & 0.2688 & 0.4556 & 0.8870 & 0.9769 \\
    willia-umbrela3         & 0.2741 & 0.4535 & 0.8707 & 0.9730 \\
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:main-results}
\end{table}