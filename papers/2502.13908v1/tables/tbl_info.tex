\begin{table}
    \centering
    \caption{LLMJudge challenge submissions details. Ensemble (Ens.) indicates if submissions combine multiple judges or use them as features to train a classifier for judgment. LR: Logistic Regression, ET: ExtraTrees, GaussianNB: Gaussian Naive Bayes are classifiers. If a submission used multiple prompts, we consider the more advanced one (CoT > Zero-Shot) in this table. FT: Fine-Tuning, N: Numerical, S: Semantic.}
    \adjustbox{max width=\columnwidth}{%
    \begin{tabular}{llccccc}
    \toprule
    \textbf{Submission ID} & \textbf{Model} & \textbf{Size} & \textbf{FT} & \textbf{Prompt} & \textbf{Label} & \textbf{Ens.} \\
    \midrule
    NISTRetrieval-instruct0 & Llama-3-Instruct & 8B & - & Zero-shot & N & - \\
    NISTRetrieval-instruct1 & Llama-3-Instruct & 8B & - & Zero-shot & N & - \\
    NISTRetrieval-instruct2 & Llama-3-Instruct & 8B & - & Zero-shot & N & - \\
    NISTRetrieval-reason0 & Llama-3-Instruct & 8B & - & CoT & N & - \\
    NISTRetrieval-reason1 & Llama-3-Instruct & 8B & - & CoT & N & - \\
    NISTRetrieval-reason2 & Llama-3-Instruct & 8B & - & CoT & N & - \\
    Olz-exp & GPT-4o & - & - & Zero-Shot & S & - \\
    Olz-gpt4o & GPT-4o & - & - & CoT & S & -  \\
    Olz-halfbin & Llama-3-Instruct & 8B & - & CoT & S + N & LR \\
    Olz-somebin & Llama-3-Instruct & 8B  & - & CoT & S + N & LR \\
    Olz-multiprompt & Llama-3-Instruct & 8B & - & CoT & S + N & \checkmark \\
    RMITIR-GPT4o & GPT-4o & - & - & Zero-Shot & N & \checkmark \\
    RMITIR-llama38b & Llama-3-Instruct & 8B & - & Zero-Shot & N & \checkmark \\
    RMITIR-llama70B & Llama-3-Instruct & 70B & - & Zero-Shot & N & \checkmark \\
    TREMA-4prompts & Llama-3-Instruct & 8B & - & Zero-Shot & N & - \\
    TREMA-CoT & Llama-3-Instruct & 8B & - & CoT & N & - \\
    TREMA-all & ChatGPT-3.5/FlanT5-Large & 783M & - & Few-Shot & N & ET \\
    TREMA-direct & ChatGPT-3.5/FlanT5-Large & 783M & - & Few-Shot & N & ET \\
    TREMA-naiveBdecompose & ChatGPT-3.5/FlanT5-Large & 783M & - & Zero-Shot & N & GNB \\
    TREMA-nuggets & ChatGPT-3.5/FlanT5-Large & 783M & - & Zero-Shot & N & ET \\
    TREMA-other & ChatGPT-3.5/FlanT5-Large & 783M & - & Zero-Shot & N & - \\
    TREMA-questions & ChatGPT-3.5/FlanT5-Large & 783M & - & Zero-Shot & N & ET \\
    TREMA-rubric0 & ChatGPT-3.5/FlanT5-Large & 783M & - & Zero-Shot & N & - \\
    TREMA-sumdecompose & Llama-3-Instruct & 8B & - & Zero-Shot & N & - \\
    h2oloo-fewself & GPT-4o & - & - & Few-Shot & N & - \\
    h2oloo-zeroshot1 & Llama-3-Instruct & 8B & \checkmark & Zero-Shot & N & - \\
    h2oloo-zeroshot2 & Llama-3-Instruct & 8B & \checkmark & Zero-Shot & N & - \\
    llmjudge-cot1 & GPT-3.5-turbo & - & - & CoT & N & - \\
    llmjudge-cot2 & GPT-3.5-turbo-16k & - & - & CoT & N & - \\
    llmjudge-cot3 & GPT-4-32k & - & - & CoT & N & - \\
    llmjudge-simple1 & GPT-3.5-turbo & - & - & Zero-Shot & N & - \\
    llmjudge-simple2 & GPT-3.5-turbo-16k & - & - & Zero-Shot & N & - \\
    llmjudge-simple3 & GPT-4-32k & - & - & Zero-shot & N & - \\
    llmjudge-thomas1 & GPT-3.5-turbo & - & - & Zero-Shot & N & - \\
    llmjudge-thomas2 & GPT-3.5-turbo-16k & - & - & Zero-Shot & N & - \\
    llmjudge-thomas3 & GPT-4-32k & - & - & Zero-Shot & N & - \\
    prophet-setting1 & Llama-3-Instruct & 8B & \checkmark & Zero-Shot & S & - \\
    prophet-setting2 & Llama-3-Instruct & 8B & \checkmark & Zero-Shot & S & - \\
    prophet-setting4 & Llama-3-Instruct & 8B & \checkmark & Zero-Shot & S & - \\
    willia-umbrela1 & GPT-4o & - & - & Zero-Shot & N & - \\
    willia-umbrela2 & GPT-4o & - & - & Zero-Shot & S & - \\
    willia-umbrela3 & GPT-4o & - & - & Zero-Shot & S + N & \checkmark \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:infos}
\end{table}