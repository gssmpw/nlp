\documentclass[11pt]{article}
\setlength{\parskip}{0.35 \baselineskip}

\input{imports}
\input{macros}

\newcommand{\email}[1]{\textsf{#1}}
\newtheorem{observation}[theorem]{\textbf{Observation}}

\title{Proper Learnability and the Role of Unlabeled Data}

\author{
% Hacky use of \and to get 3/2 author arrangement :) 
\and \and \and Julian Asilis \\ USC \\ \email{asilis@usc.edu} \and 
Siddartha Devic \\ USC\\ \email{devic@usc.edu} \and 
Shaddin Dughmi \\ USC  \\ \email{shaddin@usc.edu}  \and \and \and \and 
Vatsal Sharan \\ USC\\ \email{vsharan@usc.edu} \and 
Shang-Hua Teng \\ USC\\ \email{shanghua@usc.edu}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $\CH$, and often leads to learners with simple algorithmic forms (e.g., empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g.\ in multiclass classification. 
Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by \emph{distributional regularization}, a randomized generalization of regularization. We refer to this setting as the \emph{distribution-fixed} PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective). 

We complement this with impossibility results which obstruct any characterization of proper learnability in the classic (realizable) PAC model. First, we observe that there are problems whose proper learnability is logically \emph{undecidable}, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a \emph{local} property (in a precise sense). We also point out how the non-monotonicity of proper learning obstructs relaxations of the distribution-fixed model that preserve proper learnability, including natural notions of class-conditional learning of the unlabeled data distribution.  Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning \citep{ben2019learnability} to proper classification which may be of independent interest.
\end{abstract}

\section{Introduction}\label{Section:Introduction}

We are motivated by the following fundamental question in computational learning theory.

\begin{quote}
\begin{center}
\emph{When are supervised learning problems properly learnable? \linebreak
If so, by what kinds of proper learners?}
\end{center}
\end{quote}

Classification stands as perhaps the most fundamental setting in supervised learning. Namely, a learner receives a sequence of training points --- consisting of datapoints and their accompanying labels --- and must learn to correctly predict the label of an unseen datapoint. Notably, a predicted label is either correct and incurs a loss of zero or is incorrect and incurs a loss of one; there is no notion of a near-miss. Typical applications of this framework include image and document classification, sentiment analysis, and facial recognition, to name a few. Binary classification, the setting in which there are only two possible labels, is perhaps the single most-studied regime of learning. In this setting, the celebrated fundamental theorem of statistical learning theory establishes that a hypothesis class $\CH$ is learnable precisely when its VC dimension is finite, in which case it can be learned nearly-optimally by the learning rule of empirical risk minimization (ERM). Recall that ERM, upon receiving a training set $S$, simply selects one of the hypotheses in $\CH$ with best fit to $S$. Notably, ERM learners are an instance of \emph{proper learning}, that is, learning under the constraint that the emitted predictor always be an element of the underlying class $\CH$. 

\emph{Multiclass classification} proceeds identically as in binary classification, save for the fact that the collection of possible labels for the data --- denoted $\CY$ --- is permitted to be arbitrarily large, perhaps even infinite. To what extent do insights from binary classification extend to the multiclass case? Perhaps less than one may expect. \citet{DS14} showed that there exist multiclass classification problems which are learnable yet cannot be learned by \emph{any} proper learner. Notably, this demonstrates that ERM, perhaps the quintessential workhorse of machine learning (and particularly binary classification), does not enjoy the same success in multiclass classification. 

In fact, the task of characterizing multiclass learnability via some choice of dimension, analogous to the VC dimension for binary classification, remained a major open problem for decades. \citet{brukhim2022characterization} recently demonstrated in a breakthrough result that learnability is in fact characterized by the \emph{Daniely-Shalev-Shwartz} (DS) dimension. Further, they exhibited learners for all classes of finite DS dimension, based upon certain extensions of the \emph{one-inclusion graph} predictor of \citet{haussler1994predicting} and several novel ideas such as \emph{list} PAC learning. Interestingly, the learner of \citet{brukhim2022characterization} employs techniques which are strikingly different from --- and more intricate than --- ERM and the standard algorithmic approaches of binary classification. The complexity of existing (improper) learners for multiclass classification, and the necessity of some such complexity by the result of \citet{DS14}, raises a natural question: Can one succeed with simpler learning rules, under additional assumptions on $\CH$ or on the learning model? 

First, we observe that when a learner can infer a high degree of information about the marginal distribution over unlabeled datapoints, $\CD$, then improper learnability is equivalent to learnability by a proper learner. (As can be seen by a simple use of the triangle inequality to ``de-improperize" any improper learner by rounding its outputs to their nearest hypotheses in $\CH$.) More strikingly, we
establish that there always exists one such learner based upon a \emph{distributional regularization}, a form of regularization which assigns a score to each distribution over hypotheses in $\CH$ (i.e., to each randomized hypothesis). These results are formalized using the notion of \emph{distribution-fixed} PAC learning, in which the learner receives both a training sample $S$ and the marginal distribution $\CD$. We show an equivalence between learnability in the PAC and distribution-fixed PAC setting, for any bounded metric loss, along with an approximate equivalence between sample complexities. Perhaps surprisingly, therefore, knowing the marginal does not change learnability --- or even considerably alter sample complexities, in the worst case --- but rather greatly simplifies the form of the optimal algorithm. We ask whether proper learnability is equivalent to learnability by regularization (i.e., by \emph{structural risk minimization} (SRM)) in the classic PAC model as well, though leave that question open.

We complement this with several impossibility results demonstrating that the landscape of proper multiclass learning is considerably more complex than that of improper learning. First, we show that proper learnability can be logically \emph{undecidable}, i.e., independent of the standard ZFC axioms. This implies that it is not provable whether certain classes $\CH$ are properly learnable or not. Secondly, we show that proper learnability is not a local property: there exist classes $\CH, \CH'$ such that $\CH|_S = \CH'|_S$ for every finite set $S$ of unlabeled datapoints, yet $\CH$ is properly learnable and $\CH'$ is not. Lastly, we demonstrate that proper learnability is not a \emph{monotone property} --- it is not invariant under taking subsets or supersets. This poses several obstructions to characterizing proper multiclass learnability, and demonstrates that any such characterization must differ fundamentally from the usual dimensions enjoyed by learning theory (e.g., VC, DS, etc.). 

In light of our positive result, it is natural to ask whether one can draw a connection between proper learnability and unsupervised learning (i.e., the ability to infer distributional information about the unlabeled data distribution $\CD$) in the classic PAC model. Perhaps $\CH$ is properly learnable precisely when $\CD$ can be learned in some ``class-conditional" sense which depends upon $\CH$? Several such conditions have been proposed by \cite{hopkins2023pac} to study binary classification, including Weak TV-learning, Strong TV-learning, and Exact TV-learning. All such definitions are monotone, however, and thus --- by our previous impossibility result --- cannot characterize proper learnability. More generally, any notion of learning the marginal in a class-condition manner will likely take the form of a monotone property, and thus fail to characterize proper learnability. In short, a precise characterization of proper learnability may require a fundamentally different approach than the standard techniques of (improper) supervised learning. 

\subsection{Related Work}

\paragraph{Proper learnability.} We focus primarily --- but not exclusively --- on the setting of multiclass classification, i.e., learning under the 0-1 loss function. When $|\CY| = 2$, one recovers binary classification, for which learnability is characterized by the VC dimension and empirical risk minimization (ERM) is a nearly-optimal learner \citep{BEHW89,shalev2014understanding}.
As ERM is proper, learnability is thus equivalent to proper learnability in the binary case.\footnote{Attaining the optimal sample complexity, however, is known to require improper learning in general. Interestingly, recent work has demonstrated that the improperness requirement for optimal learning can be satisfied using simple aggregations of proper learners, such as a majority of only 3 ERM learners \citep{hanneke2016optimal,larsen2023bagging,aden2024majority}. In the multiclass setting, however, there are learnable classes which cannot be learned by any aggregation of a finite number of proper learners \citep{asilisunderstanding}.} In the multiclass case, $\CY$ is permitted to be of arbitrarily large size (even infinite), and the equivalence between proper learnability and improper learnability from the binary case was shown to fail by \citet{DS14}. They also proposed the \emph{Daniely-Shalev-Shwartz} (DS) \emph{dimension}, and conjectured that it characterizes improper multiclass learnability. This was recently confirmed in a breakthrough result of \citet{brukhim2022characterization}, resolving a long-standing open question. Regarding algorithmic templates for multiclass learning, relatively little is known: \citet{brukhim2022characterization} designed one learner for general DS classes, using an intricate sequence of arguments and algorithmic techniques (e.g., \emph{list} PAC learning, sample compression, one-inclusion graphs, etc.). It is natural to ask for simpler learners than that of \citet{brukhim2022characterization}, perhaps which bear a closer resemblance to algorithms enjoying practical success (e.g., structural risk minimization (SRM)). Recently, \citet{asilis2024regularization} made some progress by demonstrating that there always exist optimal learners taking the form of \emph{unsupervised local regularization}, a certain relaxation of classical regularization. The proof is non-constructive, however, saying little about the precise \emph{form} of the regularizer or the learner. Perhaps most relevant to our work is the line of research studying learnability via ERM. This includes work demonstrating that there can be arbitrarily large gaps between the sample complexities of different ERM learners, and that the sample complexity of ERM is closely related to the \emph{graph dimension} \citep{daniely2015multiclass}. Learnability by any ERM learner is \emph{not} equivalent to proper learnability, however, and thus this does not directly address our primary question. Regarding the issue of \emph{optimal} proper learning, \citet{bousquet2020proper} studied the conditions under which a binary hypothesis class $\CH$ can be learned with optimal sample complexity by a proper learner, and established a characterization via finiteness of the \emph{dual Helly number}, under general conditions on~$\CH$.  

\paragraph{The role of unlabeled data.} There is a long line of work studying the power of unlabeled data in learning, often formalized by the setting in which a learner receives both labeled and unlabeled datapoints, i.e., \emph{semi-supervised learning} (SSL) \citep{kaariainen2005generalization,zhu2005semi,chapelle2006semi,van2020survey}. One direction has studied SSL under the assumption that there is a relationship between the unlabeled data distribution $\CD$ and the true labeling function $h^*$, and demonstrated results supporting the power of unlabeled data in this setting \citep{castelli1995exponential,seeger2000learning,rigollet2007generalization,singh2008unlabeled,niyogi2013manifold}. Another line of work demonstrates that in that absence of any such assumptions, unlabeled data has little effect in binary classification from a worst-case perspective \citep{ben2008does,darnstadt2011smart,gopfert2019can}. Yet another direction of work studies the power of unlabeled training points from a fine-grained perspective, examining learners' sample complexities on particular data distributions rather than on a worst-case basis, and establishes the value of unlabeled data in learning binary classes of infinite VC dimension \citep{darnstadt2013unlabeled}.
We study a setting which makes no assumption on the unlabeled distribution $\CD$ or the true labeling function $h^* \in \CH$, but assumes that the learner receives complete information of $\CD$. The learner is then judged on a worst-case basis over all possible (realizable) distributions. This most closely aligns with the ``utopian" model of SSL studied by \citet{ben2008does} and \citet{lu2009fundamental}. Notably, \citet{gopfert2019can} demonstrated that this setting --- which they refer to as simply ``knowing the marginal" --- is of no additional help for binary classification. (I.e., the worst-case expected error rate of a learner does not improve by granting it knowledge of the marginal.) Our analogous result can be seen as extending this finding to a broader collection of bounded metric loss functions (Theorem~\ref{Theorem:distribution-fixed-sample-equivalence}).

\paragraph{Decidability in learning.} In Section~\ref{Section:obstructions-proper-learnability}, we establish several obstructions to characterizing proper learnability in multiclass classification, including by demonstrating that there exist classes~$\CH$ for which it is \emph{logically undecidable} whether $\CH$ can be properly learned. That is, within the ZFC axioms it can be neither proven nor disproven that $\CH$ is properly learnable. This result builds upon the breakthrough work of \citet{ben2019learnability}, which established that the learnability of certain EMX (Estimating the Maximum) learning problems can be undecidable. Notably, \citet{bandit-undecidable} established an equivalence between certain EMX learning problems and bandit problems in order to establish that bandit learnability can likewise be undecidable. 
A related line of work, also inspired by \citet{ben2019learnability}, investigates the \emph{algorithmic decidability} of learning, i.e., examining whether problems can be learned using learners which are computable, rather than merely abstract mathematical functions
\citep{agarwal2020learnability}. Recent developments in this area have established that there exist VC classes which cannot be learned by any computable learner \citep{sterkenburg2022characterizations}, and that learnability via (improper) computable learners is instead characterized by the \emph{effective VC dimension}, which roughly measures the smallest cardinality $k$ for which one can always compute a behavior on any $k + 1$ distinct unlabeled points which $\CH$ cannot express \citep{delle2023find}. Notably, this characterization holds for binary classification over the domain of the natural numbers; for binary classification over more general computable metric spaces, see \citet{ackerman2022computable}. 
Further work includes that of \citet{hasrati2023computable} on computable online learning, \citet{gourdeau2024computability} on the computability of robust PAC learning, and \citet{caro2023undecidability}, which studies the computability of learning when learners are equipped with a restricted form of black-box access to the underlying hypothesis class $\CH$.


\section{Preliminaries}\label{Section:Preliminaries}

\subsection{Notation}

For a set $Z$, we let $Z^*$ denote the collection of all finite sequences in $Z$, i.e., $Z^* = \bigcup_{i=1}^{\infty} Z^i$. When $P$ is a statement, we let $[P]$ denote the Iverson bracket of $P$, as in 
\begin{align*}
[P] = \begin{cases} 1 & P \text{ is true,} \\ 0 & P \text{ is false.} \end{cases}
\end{align*}
For a natural number $n \in \N$, $[n]$ denotes the set $\{1, 2, \ldots, n\}$. When $M$ is a measurable space, we let $\Delta(M)$ denote the collection of all probability measures over $M$. Finite sets $M_{\mathrm{fin}}$ are thought of as measurable spaces by endowing them with the discrete $\sigma$-algebra by default. $\Unif(M_{\mathrm{fin}})$ denotes the uniform distribution over $M_{\mathrm{fin}}$.

\subsection{Learning Theory}

Throughout, we use $\CX$ to denote the \defn{domain} in which unlabeled datapoints reside, and $\CY$ to denote the \defn{label set}. A labeled datapoint is a pair $(x, y) \in \CX \times \CY$. We may refer to both labeled and unlabeled datapoints merely as \emph{datapoints} when clear from context. 
A \defn{training set} is a sequence of labeled datapoints $S \in (\CX \times \CY)^*$. A function $f: \CX \to \CY$ is a \defn{predictor} or \defn{hypothesis}, and a \defn{hypothesis class} is a collection of such functions $\CH \subseteq \CY^\CX$.  
We will refer to a convex combination of hypotheses in $\CH$ as a \defn{randomized hypothesis} in $\CH$. 
Learning makes use of a \defn{loss function} $\ell : \CY \times \CY \to \R_{\geq 0}$ quantifying the quality of a predicted label $\hat{y}$ relative to the true label $y$. We will typically employ the 0-1 loss function $\ell_{0-1}(y, \hat{y}) = [y \neq \hat{y}]$ used in \defn{multiclass classification}, but we will occasionally permit $\ell$ to be any bounded metric. 

The underlying data-generating process is modeled using a probability distribution $\CD$ over the domain $\CX$, along with a choice of true labeling function $h^* \in \CH$ which assigns labels to datapoints drawn from $\CD$. For such a pair $(\CD, h^*)$, we let $\CD_{h^*}$ denote the distribution over $\CX \times \CY$ which draws unlabeled data from $\CD$ and labels it using $h^*$. That is, $\P_{\CD_h^*}(A) = \P_{x \sim \CD}\big((x, h^*(x)) \in A \big)$. We will often refer to such an $h^*$ as the ``ground truth'' hypothesis. Notably, we focus on the case of \defn{realizable} learning throughout the paper, in which the data is labeled by a hypothesis in $\CH$. For a given predictor $f \colon \CY \to \CX$, its \defn{true error} or simply \emph{error} incurred with respect to the previous data-generating process is defined as the average loss it incurs on a fresh datapoint drawn from $\CD$ and labeled by $h^*$, i.e., 
\[ L_{\CD_{h^*}}(f) = \E_{x \sim \CD} \Big[ \ell(f(x), h^*(x)) \Big] . \]
Similarly, the \defn{empirical risk} incurred by $f$ on a training set $S = \big((x_1, y_1), \ldots, (x_n, y_n) \big)$ is the average loss it experiences on the datapoints in $S$, 
\[ L_S(f) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i). \]
A \defn{learner} $A$ is a (possibly randomized) map from training sets to predictors, as in $A: (\CX \times \CY)^* \to \CY^\CX$. Note that $A$ is permitted to emit predictors which are not elements of the underlying hypothesis class $\CH$. A learner which happens to always output hypotheses in $\CH$ is referred to as \defn{proper}, while those which do not are \defn{improper}. 

A successful learner is one which attains vanishingly small error when trained on increasingly large datasets, as formalized by Valiant's celebrated Probably Approximately Correct (PAC) learning model \citep{valiant1984theory}. 

\begin{definition}\label{Definition:PAC-learning}
A hypothesis class $\CH \subseteq \CY^\CX$ is \defn{PAC learnable} if there exists a learner $A$ and \defn{sample function} $m \colon (0, 1)^2 \to \N$ with the following property: For any $\epsilon, \delta \in (0, 1)$, any distribution $\CD$ over $\CX$, and any true labeling function $h^* \in \CH$, when $A$ is trained on a dataset $S$ of points drawn i.i.d.\ from $\CD$ and labeled by $h^*$ with $|S| \geq m(\epsilon, \delta)$, then 
\[ L_{\CD_{h^*}} \big( A(S) \big) \leq \epsilon \]
with probability at least $1 - \delta$ over the random choice of $S$ and any internal randomness in $A$. 
\end{definition}

\begin{definition}
The \defn{sample complexity} of a PAC learner $A$ for a class $\CH$, denoted $m_A$, is its pointwise minimal sample function. That is, $m_{A}(\epsilon, \delta)$ is defined to be the smallest $n \in \N$ such that, for any distribution $\CD$ and true labeling function $h^*$, 
\[ L_{\CD_{h^*}}\big( A(S) \big) \leq \epsilon \]
with probability at least $1 - \delta$ over the choice of $|S| \geq n$ and any randomness internal to $A$. 
\end{definition}

\begin{definition}
The \defn{sample complexity} of a hypothesis class $\CH \subseteq \CY^\CX$, denoted $m_\CH$, is the pointwise minimal sample complexity enjoyed by any of its learners, i.e., 
\[ m_\CH(\epsilon, \delta) = \min_A m_A(\epsilon, \delta), \]
where $A$ ranges over all learners for $\CH$.
\end{definition}

In addition to the PAC model, which emphasizes high-probability guarantees, we will often judge learners' performance based upon their expected error guarantees. 

\begin{definition}\label{Definition:expected-error-model}
Let $A$ be a learner for a hypothesis class $\CH$. The sample complexity of $A$ in the expected error model, denoted $m_{\Exp, A}$, is defined by
\[ m_{\Exp, A}(\epsilon) = \min \left \{m \in \N : \E_{S \sim \CD_{h^*}^{m'}} L_{\CD_{h^*}}\big (A(S) \big) \leq \epsilon \; \t{ for all } m'\geq m, \CD \in \Delta(\CX), h^* \in \CH \right \}.  \]
The sample complexity of $\CH$ in the expected error model, denoted $m_{\Exp, \CH}$, is the minimal sample complexity attained by any of its learners, i.e., $m_{\Exp, \CH}(\epsilon) = \min_{A} m_{\Exp, A} (\epsilon)$.
\end{definition}

Our results in \cref{Section:proper-learning-through-dist-regularization} refer to randomized proper learners which are governed by a generalized form of regularization which we term \emph{distributional regularization}. For reference, we recall regularization in its classic form. 

\begin{definition}\label{Definition:SRM}
A \defn{regularizer} for a hypothesis class $\CH$ is a function $\psi \colon \CH \to \R_{\geq 0}$. A learner $A$ for $\CH$ is a \defn{structural risk minimizer} (SRM) if there exists a regularizer $\psi$ for $\CH$ such that for all training samples $S$, 
\[ A(S) \in \argmin_\CH L_S(h) + \psi(h). \]
\end{definition}

In the realizable setting, SRM learners are sometimes defined as those which minimize the regularization value $\psi(h)$ subject to a \emph{hard} constraint on attaining zero training error \citep{asilis2024open}. This perspective is essentially equivalent to Definition~\ref{Definition:SRM}, as one can normalize $\psi$ to have output strictly less than $\frac{1}{|S|}$ for the case of classification. (Note that this normalization depends upon $|S|$, however.)

\section{Proper Learning Through Distributional Regularization}\label{Section:proper-learning-through-dist-regularization}

We begin by establishing a sufficient condition for proper learnability, based upon knowledge of the marginal distribution $\CD$ over unlabeled datapoints. First, we observe that when the learner is granted full knowledge of $\CD$, then a hypothesis class $\CH$ can always be learned by a proper learner with optimal sample complexity, as measured in the expected error model. Next, we shed light on the particular algorithm form of such learners by demonstrating that one such learner always exists which is governed by \emph{distributional regularization} -- a form of regularization which assigns a complexity score to randomized hypotheses in $\CH$ (i.e., to convex combinations of hypotheses in $\CH$). Our results hold for domains $\CX$ and label sets $\CY$ of arbitrary finite size, with no dependence upon their (finite) cardinalities. We conjecture that our results hold for more general choices of $\CX$ and $\CY$, perhaps via topological arguments. Throughout the section, we remain in the setting of realizable learning. 

First, let us introduce \emph{distribution-fixed} learning. In short, it is a modification of PAC learning in which the learner is given complete information regarding the marginal distribution $\CD$ over unlabeled data. Notably, however, $\CD$ is permitted to be entirely arbitrary, and the learner will be judged on a worst-case basis over all possible choices of $\CD$, as we now describe.

\begin{definition}\label{Definition:distribution-fixed-learner}
A \defn{distribution-fixed learner} is a function $A \colon \Delta(\CX) \times (\CX \times \CY)^* \to \CY^\CX$, that is, a function which receives both a training sample and a probability distribution over $\CX$, and emits a predictor. 
\end{definition}

\begin{definition}\label{Definition:distribution-fixed-PAC-learning}
A hypothesis class $\CH \subseteq \CY^\CX$ is \defn{distribution-fixed PAC learnable} if there exists a distribution-fixed learner $A$ and function $m \colon (0, 1)^2 \to \N$ with the following property: For any $\epsilon, \delta \in (0, 1)$, any distribution $\CD$ over $\CX$, and any true labeling function $h^* \in \CH$, when $S$ is a training set of at least $m(\epsilon, \delta)$ many points drawn i.i.d.\ from $\CD$ and labeled by $h^*$, then 
\[ L_{\CD_{h^*}} \big( A(\CD, S) \big) \leq \epsilon \]
with probability at least $1 - \delta$ over the random choice of $S$ and any internal randomness in $A$. 
\end{definition}

As in classical PAC learning, the minimal function $m$ satisfying Definition~\ref{Definition:distribution-fixed-PAC-learning} is the \emph{sample complexity} of the learner $A$, and the minimal such function across all learners for $\CH$ is the sample complexity of $\CH$. (Note too that the expected error model of Definition~\ref{Definition:expected-error-model} can likewise be made distribution-fixed in the natural way.) 

\begin{remark}
The distribution-fixed model reflects the setting in which the marginal distribution over unlabeled data, $\CD$, is fully known to the learner at training time, yet the learner is judged on its worst-case performance across any choice of $\CD$ (and true labeling function $h^* \in \CH$). Other models, including the seminal work of \citet{benedek1991learnability}, focus on the case in which the marginal $\CD$ does \emph{not} vary (and only the true labeling function can vary). In this setting, the learner is endowed with complete information of $\CD$ ``by default", i.e., because its performance is only examined on distributions which share this marginal. In short, \citet{benedek1991learnability} adopt an instance-optimal perspective on learning under a particular marginal distribution. The version we study can be thought of as intermediate between the classical PAC model and that of Benedek and Itai. Informally, we maintain a worst-case perspective but equip learners with a complete understanding of the unlabeled data. 
\end{remark}

We now present a somewhat striking result: for any bounded loss function, the distribution-fixed and classical PAC models are equivalent at the level of learnability, and furthermore have sample complexities which differ by at most a logarithmic factor. 

\begin{theorem}[Equivalence between distribution-fixed and classical PAC models]\label{Theorem:distribution-fixed-sample-equivalence}
Let $\CX$ be an \linebreak arbitrary domain, $\CY$ an arbitrary label set, and $\CH \subseteq \CY^\CX$ a hypothesis class. Employ a loss function $\ell \colon \CY \times \CY \to \R_{\geq 0}$ which is bounded in $[0, 1]$. Let $m_{\CH}$ denote the sample complexity of  $\CH$ in the classic PAC model and $m_{\CH}^{\DF}$ its sample complexity in the distribution-fixed PAC model. Then,
\[ m_{\CH}^{\DF}(\epsilon, \delta) \leq m_{\CH}(\epsilon, \delta) \leq O\left( m_{\CH}^{\DF} \left(\frac{\epsilon}{11}, \frac{\epsilon}{11} \right) \cdot \log(1 / \delta) \right). \]
Furthermore, if $m_{\Exp, \CH}(\epsilon)$ and $m_{\Exp, \CH}^{\DF}(\epsilon)$ denote the sample complexities of learning $\CH$ to expected error $\leq \epsilon$ in the classic and distribution-fixed models, respectively, then 
\[ m_{\Exp, \CH}^{\DF}(\epsilon) \leq m_{\Exp, \CH}(\epsilon) \leq m_{\Exp, \CH}^{\DF}(\epsilon / e), \]
where $e \approx 2.718$ is Euler's number. 
\end{theorem}
\begin{proofsketch}
We defer the proof of the second set of inequalities to \cref{Appendix:proof-of-distribution-fixed-sample-equivalence}. For the first set of inequalities, begin by noting that $m_{\CH}^{\DF}(\epsilon, \delta) \leq m_{\CH}(\epsilon, \delta)$ is immediate; a distribution-fixed learner can elect to ignore the information of the marginal $\CD$. Then, assuming the second set of inequalities, we have: 
\begin{align*}
m_\CH(\epsilon, \delta) &\leq O\big( m_{\Exp, \CH}(\epsilon / 2) \log(1 / \delta) \big) \\
&\leq O\left( m_{\Exp, \CH}^{\DF} \left(\frac{\epsilon}{ 2e}\right) \log(1 / \delta) \right) \\
&\leq O\left( m_{\CH}^{\DF} \left(\frac{\epsilon}{4e}, \frac{\epsilon}{4e} \right) \log(1 / \delta) \right). 
\end{align*}
The first inequality makes use of a standard repetition argument; a learner incurring expected error at most $\epsilon / 2$ can be repeatedly trained on separate datasets and tested on a validation set in order to attain a high-probability guarantee. The second inequality invokes the claim whose proof we deferred to \cref{Appendix:proof-of-distribution-fixed-sample-equivalence}. The third inequality follows immediately from the fact that the loss function is bounded above by 1. Conclude by noting that $4e \approx 10.87 < 11$. 
\end{proofsketch}

We now observe a simple equivalence between proper and improper learnability in the distribution-fixed model, for learning with any metric loss function.\footnote{We thank Tosca Lechner for pointing us to this elegant observation.} In particular, an arbitrary (possibly improper) distribution-fixed learner $\CA$ can be ``properized" by replacing its output $\CA(\CD, S)$ with the nearest hypothesis in $\CH$, as measured by the distance function $\dist_{\CD}(f, g) = \E_{x \sim \CD} \ell\big( f(x), g(x) \big)$. Notice, however, that this settles the question of proper learnability in the distribution-fixed model, but says little about the \emph{algorithmic form} of (optimal) proper learners. In Theorem~\ref{Theorem:randomized-proper-for-finite-problems}, in contrast, we shed light upon such learners as following the principle of regularization, in a generalized form.

\begin{observation}\label{Observation:proper-dist-fixed}
Let $\CX$ be an arbitrary domain, $\CY$ an arbitrary label set, and $\CH \subseteq \CY^\CX$ a hypothesis class. Employ a metric loss function $\ell \colon \CY \times \CY \to \R_{\geq 0}$. Then $\CH$ has a proper distribution-fixed learner which is optimal, as measured by its expected error. 
\end{observation}
\begin{proof}
Let $\CA\colon \Delta(\CX) \times (\CX \times \CY)^* \to \CY^\CX$ be an optimal distribution-fixed learner for $\CH$, in the expected error regime. We will exhibit a proper distribution-fixed learner $\CB$ which attains equal performance to $\CA$, up to a factor of 2. To this end, let $\CD \in \Delta(\CX)$ be an arbitrary probability measure on $\CX$ and $S$ a training set. Let $\dist_{\CD}: \CY^\CX \times \CY^\CX \to \R_{\geq 0}$ be the distance measure defined as $\dist_{\CD}(f, g) = \E_{x \sim \CD} \ell\big( f(x), g(x) \big)$. 

Then we define $\CB$ to emit the following hypothesis on input pair $(\CD, S)$: 
\[ \CB(\CD, S) = \argmin_{\CH} \Big[ \dist_\CD \big(h, \CA(\CD, S) \big) \Big].  \]
To see that $\CB$ at most doubles the expected error of $\CA$ on any realizable distribution, fix one such distribution, as defined by a marginal $\CD \in \Delta(\CX)$ and ground truth hypothesis $h^* \in \CH$. Then we have,
\begin{align*}
L_{\CD_{h^*}}\big(\CB(\CD, S)\big) &= \E_{x \sim \CD} \ell \big( \CB(\CD, S), h^*(x) \big) \\
&\leq \E_{x \sim \CD} \ell \big( \CB(\CD, S), \CA(\CD, S) \big) + \E_{x \sim \CD} \ell \big( \CA(\CD, S), h^*(x) \big) \\ 
&\leq \E_{x \sim \CD} \ell \big( \CA(\CD, S), h^*(x) \big) + \E_{x \sim \CD} \ell \big( \CA(\CD, S), h^*(x) \big) \\
&= 2 \cdot L_{\CD_{h^*}}\big(\CA(\CD, S)\big). 
\end{align*}
The first inequality is an application of the triangle inequality for $\ell$, and the second inequality follows from the definition of $\CB$. 
\end{proof}

We now define \emph{distributional regularization}, a relaxation of classical regularization which assigns values to randomized hypotheses in $\CH$ (i.e., to probability distributions over $\CH$). 

\begin{definition}
A \defn{distributional regularizer} is a function $\psi: \Delta(\CH) \to \R_{\geq 0}$. A (randomized) learner $A$ for $\CH$ is a \defn{distributional structural risk minimizer (SRM)} if there exists a distributional regularizer $\psi$ such that for all training samples $S$, 
\[ A(S) \in \argmin_{\substack{P \in \Delta(\CH), \\ \E_{h \sim P} L_S(h) = 0 }}  \psi(P) . \]
\end{definition}

That is, a distributional SRM learner is one which outputs a randomized hypothesis minimizing the regularization value, subject to perfect performance on the training set. When there are ties in the regularization value $\psi(.)$, we evaluate the learner's performance with respect to worst-case tie-breaking among randomized hypotheses. An important lemma is that Bayesian learners can be witnessed as distributional SRMs. 

\begin{lemma}\label{Lemma:bayesian-is-distributional-SRM}
Let $\CX$ be a finite domain, $\CY$ a finite label space, and $\CH \subseteq \CY^\CX$ a hypothesis class. Let $Q \in \Delta(\CH)$ be a distribution over $\CH$, and $A$ be a Bayesian learner with respect to $Q$. That is, upon receiving a training sample $S$, $A$ emits $Q|S \in \Delta(\CH)$, the restriction of $D$ to those $h \in \CH$ with $L_S(h) = 0$. Then $A$ is a distributional SRM learner. 
\end{lemma} 
\begin{proof}
Given $Q$, let $\psi$ be the distributional regularizer which computes the relative entropy of a distribution $P \in \Delta(\CH)$ with respect to $Q$. That is, 
\begin{align*}
\psi(P) &= D_{KL}(P | Q) \\ 
&= \sum_{h \in \CH} P(h) \log \left( \frac{P(h)}{Q(h)} \right). 
\end{align*}
Then an SRM learner induced by $\psi$ is tasked with outputing a distribution $P$ which minimizes empirical error (i.e., is supported on $L_{S}^{-1}(0)$) while minimizing relative entropy to $Q$. By a standard result, the distribution supported on $L_{S}^{-1}(0)$ with minimal relative entropy to $Q$ is precisely $Q | S$, the restriction of $Q$ to $L_{S}^{-1}(0)$. (See, e.g., \citet[Lemma~55]{asilis2024regularization}.) This completes the argument.
\end{proof}

We will also make use of the fact that Bayesian learners, as described in Lemma~\ref{Lemma:bayesian-is-distributional-SRM}, are closed under convex combinations. 

\begin{lemma}\label{Lemma:convex-combo-of-Bayesian}
Let $\CX$ be a finite domain, $\CY$ a finite label space, and $\CH \subseteq \CY^\CX$ a hypothesis class. Let $A_1, \ldots, A_n$ be a collection of randomized learners for $\CH$ which are Bayesian with respect to priors $Q_1, \ldots, Q_n$. Then any convex combination $p_1 A_1 + \ldots + p_n A_n$ is itself a Bayesian learner with respect to the prior $p_1 Q_1 + \ldots + p_n Q_n$. 
\end{lemma}
\begin{proof}
Fix a probability distribution $(p_1, \ldots, p_n)$ and a training set $S$. Let $\CA = p_1 A_1 + \ldots + p_n A_n$, and let $\CA'$ be the Bayesian learner corresponding to the prior $p_1 Q_1 + \ldots + p_n Q_n$. For any $h \in \CH$, we have:
\begin{align*}
\CA(S)(h) &= p_1 A_1(S)(h) + \ldots + p_n A_n(S) (h) \\
&= p_1 \cdot Q_1(h \mid  L_S^{-1}(0)) + \ldots + p_n \cdot Q_n (h \mid L_S^{-1}(0)) \\
&= (p_1 Q_1 + \ldots + p_n Q_n) (h \mid L_{S}^{-1}(0)) \\
&= \CA'(S)(h). \qedhere
\end{align*}
\end{proof}

We now demonstrate the primary result of the section: all finite learning problems with bounded loss functions can be learned by an \emph{optimal} randomized proper learner, following the principle of distributional regularization. 

\vspace{0.3 cm}
\begin{theorem}[Distributional regularization]\label{Theorem:randomized-proper-for-finite-problems}
Let $\CX$ be a finite domain, $\CY$ a finite label set, and $\CH \subseteq \CY^\CX$ a hypothesis class. Let $\ell \colon \CY \times \CY \to \R_{\geq 0}$ be a metric loss function bounded in $[0, 1]$. Then in the distribution-fixed model, $\CH$ has a randomized proper learner which attains optimal expected error, up to a factor of 2. Furthermore, this learner can be witnessed as a distributional SRM.
\end{theorem}
\begin{proof}
Fix a sample size $n \in \N$. We will describe the action of a distributional SRM learner $\CA$ which attains optimal expected error on samples of size $n$, up to factor of 2. Further fix a distribution $\CD$ over $\CX$, of which $\CA$ is aware, as we are in the distribution-fixed model. 

Consider the zero-sum game $\CG$ in which the column player selects a function $h^* \in \CH$, thought of as the ground truth labeling function, and the row player responds with a learner $A: (\CX \times \CY)^n \to \CY^\CX$, defined only on samples of size $n$. For a given pair of actions $(h^*, A)$, the row player incurs a loss of 
\begin{align*}
\epsilon(h^*, A) &=  \E_{S \sim \CD_{h^*}^n} \left[ L_{\CD_{h^*}} A(S) \right] \\
&= \E_{S \sim \CD_{h^*}^n} \E_{x \sim \CD}  \left[  \ell\big(h^*(x), A(S)(x)\big) \right], 
\end{align*} 
i.e., the expected error incurred by $A$ when trained on samples of size $n$. As $\CG$ is zero-sum, the column player is rewarded with a value of $\epsilon(h^*, A)$. Note too that $\CG$ is a finite game; there are only finitely many  hypotheses in $\CH \subseteq \CY^\CX$ --- owing to finiteness of both $\CX$ and $\CY$ --- and likewise finitely many learners $(\CX \times \CY)^n \to \CY^\CX$. 
\vspace{-0.6 cm}
\begin{quote}
\begin{lemma}\label{Lemma:factor-2-game}
For any mixed strategy $\Lambda = \{\lambda_h\}_{h \in \CH}$ of the column player in game $\CG$, the row player can respond with a randomized proper learner $A_{\t{prop}}$ which attains optimal payoff, up to a factor of 2. The learner $A_{\t{prop}}$ is Bayesian and employs $\Lambda$ as its prior. 
\end{lemma}
\begin{proof}
Fix the mixed strategy $\Lambda = \{\lambda_h\}_{h \in \CH}$, denoting a prior probability distribution over the ground truth labeling function $h^* \in \CH$. For a training set $S$ and test point $x$, $A_{\t{prop}}$ predicts a label for $x$ by drawing from the posterior distribution over labels at $x$. (Equivalently, $A_{\t{prop}}$ emits an entire function $h \in \CH$ which is drawn from the posterior distribution of $\Lambda$ conditioned upon $S$.)

We now argue that the expected error incurred by $A_{\t{prop}}$ is optimal, up to a factor of 2.  To this end, fix a test point $x \in \CX$ and let $P = \{p_y\}_{y \in \CY}$ denote the posterior distribution over the true label at $x$ (upon conditioning $\Lambda$ by $S$). By linearity of expectation, the optimal prediction at $x$ can be assumed to be deterministic, i.e., predicting a fixed label $\hat{y} \in \CY$. Thus, the expected error incurred by the optimal learner at test point $x$ is $\E_{y \sim P} \ell(y, \hat{y})$. The error incurred by $A_{\t{prop}}$, in contrast, is $\E_{y, y' \sim P} \ell(y, y')$. With an application of the triangle inequality, we have: 
\begin{align*}
\E_{y, y' \sim P} \ell(y, y') &\leq \E_{y, y' \sim P} \ell(y, \hat{y}) + \ell(\hat{y}, y') \\
&= 2 \E_{y \sim P} \ell(y, \hat{y}). 
\end{align*}
Thus $A_{\t{prop}}$ indeed incurs expected error at most twice that of the optimum. 
\end{proof}
\end{quote}

Now consider the game $\CG_{\t{prop}}$ which is identical to $\CG$, save for the fact that the row player is obligated to select a Bayesian learner. Then $\CG_{\t{prop}}$ is a compact zero-sum game by the supposition that $\CX$ and $\CY$ are finite, meaning it enjoys the minimax theorem. There thus exists an optimal mixed strategy over Bayesian learners which attains the value $\epsilon^*$ of $\CG_{\t{prop}}$. By Lemma~\ref{Lemma:convex-combo-of-Bayesian}, the mixed strategy reduces to a pure strategy; that is, there exists a single Bayesian learner attaining the value~$\epsilon^*$. Furthermore, by Lemma~\ref{Lemma:factor-2-game}, $\epsilon^*$ is within a factor 2 of the value of $\CG$, i.e., of the best performance which can be attained by \emph{any} learner on samples of size $n$. Conclude by applying Lemma~\ref{Lemma:bayesian-is-distributional-SRM} to see that Bayesian learners can be witnessed as distributional SRM learners, as desired.
\end{proof}

\begin{remark}
Our proof of \cref{Theorem:randomized-proper-for-finite-problems} bears similarities to the proof of \citet[Lemma~5]{darnstadt2011smart}, which also models learning with respect to a fixed marginal distribution as a zero-sum game. Though the goals of their paper and this particular lemma are substantially different from ours, their analysis of the learner's best response problem is conceptually similar. One key difference is that their proof exploits structure particular to binary classification, which is their focus. Another difference is that they restrict attention to proper learning out of the gate, as this is without loss for binary classification, whereas we allow improper learning and conclude that properness is without much loss for more general problem classes. Finally, we aggregate the (Bayesian, and proper) best responses of the learner into a near-optimal learner of the same desired form, departing from the concerns and technical approach of their paper.
\end{remark}

Let us briefly remark upon two structural features of Theorem~\ref{Theorem:randomized-proper-for-finite-problems}'s proof. First, the fact that a learner $\CA$ has completely flexible control over its strategy in the zero-sum game $\CG$ relies crucially upon the fact that $\CA$ is a distribution-fixed learner. Otherwise, $\CA$'s actions would be coupled across all possible marginals $\CD$ used in the definition of $\CG$. Second, it is tempting to generalize Theorem~\ref{Theorem:randomized-proper-for-finite-problems}'s proof to more general settings, including spaces $\CX$ and $\CY$ which may be compact, convex, etc. This is a interesting direction which we leave open to future work. We mention only that in our attempts to do so, we were unable to find natural choices of structure on $\CX$, $\CY$, $\CH$, and $\ell$ which could simultaneously satisfy all properties required in defining the game and invoking the minimax theorem. Another interesting question is whether Theorem~\ref{Theorem:randomized-proper-for-finite-problems} can also be established for the high-probability regime of learning.
\newpage 

\paragraph{Weakening of the distribution-fixed assumption.} It is natural to ask whether the conclusion of Theorem~\ref{Theorem:randomized-proper-for-finite-problems} can be achieved by assuming a ``softer" form of distribution-fixed learning. For instance, perhaps one can remain in the classic PAC model yet assume that $\CH$ is sufficiently simple such that a learner $A$ can use the unlabeled data in $S$ in order to learn the marginal distribution $\CD$ over $\CX$ in some ``class-conditional" sense. (E.g., to learn $\CD$ sufficiently well so as to estimate $L_{\CD_{h}}(h')$ for all pairs $(h, h') \in \CH^2$.) This line of inquiry is studied by \citet{hopkins2023pac} for binary classification in the \emph{distribution-family} model, in which the marginal distribution $\CD$ over $\CX$ is restricted to a certain collection of distributions at the outset. They introduce precisely such ``class-conditional" notions of learning the marginal $\CD$, and provide distinct necessary and sufficient conditions for PAC learnability based upon learnability of $\CD$.  

Notably, however, the difficulty in \citet{hopkins2023pac} arises from studying the distribution-family model, and the complexity which it can endow binary classification.  Further, \citet{hopkins2023pac} do not emphasize the particular algorithmic or structural form of the learner, as we do. In PAC learning with respect to \emph{all} realizable distributions --- as we study --- binary classification problems are also well-known to be learnable by proper learners whenever learning is possible. 

It may be natural to ask, then, whether the techniques of \citet{hopkins2023pac} are applicable for classical PAC learning beyond the binary setting. To this, we present a negative result in Section~\ref{Section:obstructions-proper-learnability}, by demonstrating that proper learnability is not a \emph{monotone} property. That is, there exist hypothesis classes $\CH_0 \subsetneq \CH_1 \subsetneq \CH_2$ in multiclass classification such that only $\CH_0$ and $\CH_2$ are properly learnable. All notions of class-conditional learnability introduced by \citet{hopkins2023pac}, however, are monotone (e.g., Weak TV-learning, Strong TV-learning, Exact TV-learning). As such, any natural weakening of the distribution-fixed assumption --- reflecting the ability to learn $\CD$ in a ``class-conditional" way --- is unlikely to characterize proper learnability. 

\paragraph{Proper learnability and SRM in broader context.} A central point of Theorem~\ref{Theorem:randomized-proper-for-finite-problems} is that the proper learner is \emph{optimal} in terms of its expected error, up to a constant factor of 2. In particular, even for \emph{finite} problems in classical PAC learning (i.e., finite $\CX$ and $\CY$), there are known to be problems exhibiting arbitrarily large gaps in sample complexity between proper and improper learners. (See \citet[Theorem 1]{DS14}, along with the compactness result of \cite{asilis2024transductive} which equates the sample complexity of a learner to its worst-case over finite subproblems.) As such, Theorem~\ref{Theorem:randomized-proper-for-finite-problems} would not hold when stated in the classic PAC model. Furthermore, Theorem~\ref{Theorem:randomized-proper-for-finite-problems} establishes an equivalence between proper learnability and learnability by (distributional) SRM in the distribution-fixed model. It is natural to ask whether such an equivalence might hold more generally, such as in the classic PAC model. We conjecture that it may indeed be so, at least for the case of multiclass classification. 

\begin{conjecture}
Let $\CX$ be an arbitrary domain, $\CY$ an arbitrary label space, and employ the 0-1 loss function $\ell_{0-1}$. Then for any $\CH \subseteq \CY^\CX$, $\CH$ is properly learnable if and only if $\CH$ can be learned by an SRM learner. (Perhaps the same can be said if $\ell$ is any bounded metric loss function.)
\end{conjecture}

Let us present another conjecture, which --- along with the previous one --- would imply that \emph{all} classification problems $\CH$ can be learned by SRM, possibly on a superset of $\CH$. 

\begin{conjecture}
Let $\CH \subseteq \CY^\CX$ be a multiclass classification problem (i.e., employ $\ell_{0-1}$). Then $\CH$ is learnable if and only if there exists an $\CH' \supseteq \CH$ such that $\CH'$ is properly learnable. 
\end{conjecture}

\section{Obstructions to Characterizing Proper Learnability}\label{Section:obstructions-proper-learnability}

We now direct our attention from the distribution-fixed PAC to the classical PAC model, and ask: Under what conditions is a learning problem \emph{properly} learnable? Perhaps the most natural approach is to search for a combinatorial characterization of proper learnability, by analogy with existing characterizations of (improper) PAC learnability. We demonstrate several obstructions to any such approach. Together, they imply that proper learnability cannot be characterized by any \emph{property of finite character}, as described in \citet{ben2019learnability}. Our results also imply, more generally, that proper learnability cannot be characterized by any condition which is monotone, or which considers only ``finite projections" of the hypothesis class. Throughout the section, we remain in the setting of multiclass classification in the (realizable) PAC model. Along with binary classification, this forms perhaps the most fundamental setting of supervised learning. 

We now demonstrate a central technical result of the section: the pathological learning problem of \emph{EMX learning} --- which was not originally phrased as a supervised learning problem \citep{ben2019learnability} --- can in fact be witnessed as an instance of proper multiclass learning. We first recall the standard definition of EMX learning, and the result for which it was designed: EMX learnability can be logically undecidable. 

\begin{definition}
Let $\CF$ be a set. The EMX learning problem on $\CF$ is defined as follows: An adversary selects a probability distribution $P$ on $\CF$ which is supported on finitely many points, the learner receives a sample $S$ of points drawn i.i.d.\ from $P$, and it must emit a finite subset of $\CF$ with large $P$-measure. 
\end{definition}

The learning problem associated to $\CF$ is said to be EMX-learnable if there exists a learner which outputs sets of measure arbitrarily close to 1 as $|S| \to \infty$, with high probability over $S$ (and uniformly in the adversary's choice of $P$). We will often abbreviate this by stating that $\CF$ itself is, or is not, EMX-learnable. A breakthrough result of \citet{ben2019learnability} demonstrated that EMX learning can be undecidable, depending upon the cardinality of the underlying set $\CX$. 

\begin{theorem}[\cite{ben2019learnability}]\label{Theorem:EMX-undecidable}
The EMX-learnability of $\R$ is undecidable, i.e., logically independent of the ZFC axioms. More generally, a set $\CF$ is EMX-learnable if and only if $|\CF| < \aleph_{\omega}$.
\end{theorem}

We now demonstrate that EMX learning can be witnessed within multiclass classification. Our proof employs techniques developed by 
\cite{daniely2015multiclass} and \cite{DS14} in their design of the \emph{first Cantor class}. 

\begin{proposition}\label{Proposition:EMX-is-really-multiclass}
Let $\CF$ be any set. There exists a multiclass classification problem $\CH \subseteq \CY^\CX$ such that $\CH$ is properly learnable if and only if $\CF$ is EMX-learnable. 
\end{proposition}
\begin{proof}
Set $\CX = \CF$ and $\CY = \{\star\} \cup 2^{\CF}$, where $2^{\CF}$ denotes the power set of $\CF$. For each $A \subseteq \CF$, define $h_A: \CX \to \CY$ by 
\[ h_A(x) = \begin{cases} A & x \in A, \\ \star & x \notin A. \end{cases} \]
Then set $\CH = \{h_A : A \subseteq \CX, |\CX \setminus A| < \infty\}$. That is, each $h_A \in \CH$ outputs the label $\star$ on a finite set of points, and the label $A$ elsewhere. Note that the label $A$, in only being output by the hypothesis $h_A$, completely reveals the identity of $h_A$ as the true labeling function. In fact, if a learner  ever observes any label other than $\star$ in the training set, then it has fully identified the true labeling function. Then in order to learn $\CH$, one need only consider learnability with respect to pairs $(D, h)$ where $D$ places full measure on $h^{-1}(\star)$. In particular, upon observing a training set $S$ with $|S| = n$, either $S$ contains a non-$\star$ label, rendering learning trivial, or $S$ only contains the $\star$ label, from which one can conclude that there is $o_n(1)$ probability of ever observing a non-$\star$ label (and thus learning reduces to correctly predicting the $\star$ labels).\footnote{More precisely, for any distribution $D$ and sample size $n$, either $D$ places $O(\frac{1}{n})$ mass on non-$\star$ labels (thus learning reduces to correctly predicting the $\star$ label), or $S \sim D^n$ contains a non-$\star$ label with probability $1 - o(1)$.}

Now suppose that $\CH$ is properly learnable. Then there exists a learner $\CA$ for $\CH$ with the following property: for any marginal distribution $D$ over $\CX$ with finite support, when $\CA$ observes a training sample $S = (x_i, \star)_{i \in [n]}$ with $x_{i} \overset{\text{i.i.d.\ }}{\sim} D$, it emits a hypothesis $h_A \in \CH$ such that $\CX \setminus A$ is finite and has large $D$-measure. By modifying $\CA$ to receive only the data of $(x_i)_{i \in [n]}$ and to emit $\CX \setminus A$ rather than $h_A$, we produce an EMX learner for $\CF$. Conversely, any EMX learner $\CA$ for $\CF$ gives rise to a PAC learner for $\CH$ by nearly identical reasoning, i.e., by reformatting its input and output. 
\end{proof}

\vspace{0.04 cm}
\begin{remark}
In Proposition~\ref{Proposition:EMX-is-really-multiclass}, it suffices to endow $\CX$ with any $\sigma$-algebra such that points in $\CX$ are measurable. From this, one has that for each $h_A \in \CH$, $\CX \setminus A$ is measurable (as it is finite) and $A$ is measurable (as it is cofinite). Then, as previously described, any marginal distribution $D$ either places a negligible amount of mass on the event $A$, or otherwise reveals the label $A$ exponentially quickly in $|S|$. 
\end{remark}
\vspace{0.04 cm}

\begin{theorem}\label{Theorem:multiclass-undecidable}
There exists a multiclass classification problem $\CH$ such that it is undecidable whether $\CH$ is properly learnable. 
\end{theorem}
\begin{proof}
Invoke Theorem~\ref{Theorem:EMX-undecidable} with Proposition~\ref{Proposition:EMX-is-really-multiclass}. 
\end{proof}

An immediate consequence of \cref{Theorem:multiclass-undecidable} is that proper multiclass learnability is not a \emph{property of finite character}, in the sense of \citet{ben2019learnability}. This stands in stark contrast to the existing characterization of improper learnability by the DS dimension, and to similar dimension-based results across supervised learning. We now demonstrate that proper learnability is furthermore not a \emph{local} property. That is, there exist hypothesis classes sharing all local behaviors yet differing in their proper learnability. 

\begin{theorem}\label{Theorem:obstruction-local}
In multiclass classification, there exist a pair of hypothesis class $\CH, \CH' \subseteq \CY^\CX$ such that $\CH|_S = \CH'|_S$ for each finite $S \subseteq \CX$, yet $\CH$ is properly learnable and $\CH'$ is not. 
\end{theorem}
\begin{proof}
Let $\CF$ be a set of cardinality $|\CF| \geq \aleph_\omega$. Let $\CH \subseteq \CY^\CX$ be the multiclass problem associated to $\CF$, as per Proposition~\ref{Proposition:EMX-is-really-multiclass}. By Theorem~\ref{Theorem:EMX-undecidable}, $\CF$ is not EMX-learnable and thus $\CH$ is not properly learnable. Let $f^\star \colon \CX \to \CY$ be the constant function which only outputs the $\star$ label. Then $\CH' = \CH \cup \{f^\star\}$ is certainly properly learnable, by the learner which outputs $f^\star$ when it only sees the $\star$ label in the training set, and otherwise sees a label $A \neq \star \in \CY$ which fully identifies the true labeling function as $h_A$. Yet for any finite $S \subseteq \CX$, we have that $\CH|_S = \CH'|_S$, because the behavior $f^*|_S$ appears in $\CH|_S$ as the restriction of (for instance) $h_{\CX \setminus S}$ to $S$. This completes the argument. 
\end{proof}

\begin{theorem}
In multiclass classification, proper learnability is not a monotone property of the hypothesis class. That is, there exist hypothesis classes $\CH_0 \subsetneq \CH_1 \subsetneq \CH_2$ such that only $\CH_0$ and $\CH_2$ are properly learnable. 
\end{theorem}
\begin{proof}
Using the proof of Theorem~\ref{Theorem:obstruction-local}, we have a pair of hypothesis classes $\CH \subsetneq \CH'$ such that $\CH'$ is properly learnable yet $\CH$ is not. Conclude by setting $\CH_0$ to be any finite subset of $\CH$, which is properly learnable as it satisfies the uniform convergence property. 
\end{proof}

\section{Conclusion}

We study proper learnability in supervised learning, and begin by considering the distribution-fixed model of learning, in which the learner is given the full information of the marginal distribution $\CD$ over unlabeled data. We demonstrate an approximate equivalence between sample complexities in the distribution-fixed model and the classic PAC model, for any bounded metric loss function. This refutes the power of unlabeled data in PAC learning, i.e., for the worst-case distributions. We then establish that in the distribution-fixed model, all finite learning problems with metric losses can be learned to optimal expected error by a proper learner. We conjecture that this result can be extended to infinite domains $\CX$, perhaps via topological arguments. 

We then demonstrate impossibility results towards characterizing proper learnability in the classic PAC model. Our results are threefold: we show that proper learnability can be logically undecidable, that it is not a monotone property, and that it is not a local property. This strongly suggests that a characterization of proper learnability will require fundamentally different techniques from the usual dimensions in learning theory. Furthermore, the non-monotonicity of proper learnability rules out many natural characterizations in terms of unsupervised learning, such as class-conditional learning of the unlabeled data distribution. Interesting open questions include studying proper learning in the \emph{agnostic} case, establishing necessary or sufficient conditions for proper learnability in the classic PAC model, and understanding the algorithmic form of (optimal) proper learners.


\subsection*{Acknowledgments}

\noindent The authors thank Tosca Lechner for a helpful conversation and for suggesting the ideas behind Observation~\ref{Observation:proper-dist-fixed}.
Julian Asilis was supported by the Simons Foundation and by the National Science Foundation Graduate Research Fellowship Program under Grant No.\ DGE-1842487. This work was completed in part while Julian Asilis, Siddartha Devic, and Vatsal Sharan were visiting the Simons Institute for the Theory of Computing.
Shaddin Dughmi was supported by NSF Grant CCF-2432219. Vatsal Sharan
was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Shang-Hua
Teng was supported in part by NSF Grant CCF-2308744 and the Simons Investigator Award from
the Simons Foundation. Any opinions, findings, conclusions, or recommendations expressed in this
material are those of the authors and do not necessarily reflect the views of any of the sponsors such
as the NSF. 


\newpage 
\bibliographystyle{plainnat}
\bibliography{refs.bib}
\newpage 

\appendix


\section{Omitted proofs}

\subsection{Proof of \cref{Theorem:distribution-fixed-sample-equivalence}}\label{Appendix:proof-of-distribution-fixed-sample-equivalence}

\noindent Completing the proof of \cref{Theorem:distribution-fixed-sample-equivalence} amounts to establishing the following claim. 

\begin{lemma}\label{Lemma:appendix}
Let $\CX$ be an arbitrary domain, $\CY$ an arbitrary label space, and $\CH \subseteq \CY^\CX$ a hypothesis class. Employ a loss function $\ell : \CY \times \CY \to \R_{\geq 0}$ which is bounded in $[0, 1]$. Let $m_{\Exp, \CH}(\epsilon)$ and $m_{\Exp, \CH}^{\DF}(\epsilon)$ denote the sample complexity of learning $\CH$ to expected error $\leq \epsilon$ in the classic and distribution-fixed models, respectively. Then, 
\[ m_{\Exp, \CH}^{\DF}(\epsilon) \leq m_{\Exp, \CH}(\epsilon) \leq m_{\Exp, \CH}^{\DF}(\epsilon / e), \]
where $e \approx 2.718$ is Euler's number. 
\end{lemma}

The proof will make use of an equivalence between learners which attain low expected error and those which attain low error in the setting of \emph{transductive learning}. 

\begin{definition}
The (realizable) \defn{transductive learning} model is defined by the following sequence of steps: 
\begin{enumerate}
    \item An adversary selects a collection of $n$ unlabeled datapoints $S = (x_1, \ldots, x_n) \in \CX^n$, and a hypothesis $h^* \in \CH$. 
    \item The unlabeled datapoints $S$ are displayed to the learner. 
    \item One datapoint $x_i$ is selected uniformly at random from $S$. The remaining datapoints are labeled by $h^*$ and displayed to the learner. That is, the learner receives $(x_j, h(x_j))_{j \neq i}$. 
    \item The learner is prompted to predict the label of $x_i$, namely $h^*(x_i)$. 
\end{enumerate}
\end{definition}
The \defn{transductive error} incurred by a learner $A$ on a transductive learning instance $(S, h^*)$ is equal to its average prediction error over the uniformly random choice of $x_i$. That is, 
\[ L_{S, h^*}^{\Trans} (A) = \frac{1}{n} \sum_{i \in [n]} \ell \big( A(S_{-i}, h^*)(x_i), h^*(x_i) \big), \] 
where $A(S_{-i}, h)$ denotes the output of $A$ on the sample consisting of all unlabeled datapoints in $S$ other than $x_i$, which are labeled by $h^*$. One can then define the \defn{transductive error rate} of a learner $A$ as 
\[ \varepsilon_{A, \CH}(n) = \max_{S \in \CX^n, \; h \in \CH} L_{S, h}^{\Trans}(A), \] 
and similarly its \defn{transductive sample complexity} as 
\[ m_{\Trans, A}(\delta) = \min \{ m \in \N : \varepsilon_{A, \CH}(m') < \delta, \; \forall m' \geq m\}. \] 
Lastly, define the \emph{transductive sample complexity} of a hypothesis class $\CH$ as the pointwise minimal sample complexity attained by any of its learners, i.e., 
\[ m_{\Trans, \CH}(\epsilon) = \min_A m_{\Trans, A}(\epsilon). \]

\noindent We are now equipped to prove Lemma~\ref{Lemma:appendix}, and thus complete the proof of \cref{Theorem:distribution-fixed-sample-equivalence}. 

\begin{proofof}{Lemma~\ref{Lemma:appendix}}
Certainly $m_{\Exp, \CH}^{\DF}(\epsilon) \leq m_{\Exp, \CH}(\epsilon)$, as any learner in the distribution-fixed model can elect to ignore the information of the unlabeled data distribution. For the second inequality, we argue that: 
\begin{align}
m_{\Exp, \CH}(\epsilon) &\leq m_{\Trans, \CH}(\epsilon) \label{eq:1} \\
&\leq m_{\Exp, \CH}^{\DF}(\epsilon / e). \label{eq:2}
\end{align}
Inequality \eqref{eq:1} follows from the standard leave-one-out argument of transductive learning, which establishes that any learner $A$ incurring transductive error $\leq \epsilon$ on samples of size $n$ automatically incurs expected error $\leq \epsilon$ as well \citep{haussler1994predicting}. More explicitly, for any such learner $A$ and realizable distribution $D$ over $\CX \times \CY$, one has: 
\begin{align*}
\E_{S \sim D^m} L_D\big(A(S)\big) &= \E_{\substack{S \sim D^m \\ (x, y) \sim D}} \ell \big(A(S)(x), y \big) \\ 
&= \E_{S \sim D^{m+1}} \ell \big(A(S_{-(m+1)})(x_{m+1}), y_{m+1} \big) \\
&= \E_{S \sim D^{m+1}} \E_{i \in [m+1]} \ell \big( A(S_{-i})(x_{i}), y_{i} \big) \\ 
&\leq \sup_{S} \E_{i \in [m+1]} \ell \big( A(S_{-i})(x_{i}), y_{i} \big) \\
&\leq \epsilon. 
\end{align*}
In pursuit of inequality \eqref{eq:2}, let $A$ be a distribution-family learner attaining expected error at most $ \epsilon$ when trained on samples of size $\geq n$. We will design a (randomized) transductive learner $B$ attaining error at most $e \cdot \epsilon$ on samples of size $n$. For a training sample $S = (x_i, y_i)_{i \in [n]}$, let $S_{\CX} = (x_i)_{i \in [n]}$ denote its unlabeled datapoints.
Then $B$ acts as follows upon receiving a training set $S$ and test point $x$. If $x \in S$, $B$ simply returns the correct label for $x$, which was observed in $S$. Otherwise, $B$ generates a sample $T$ of $|S|$ many points drawn uniformly at random from $S$, and predicts $A(\Unif(S_{\CX} \cup \{x\}), T)(x)$. 

We now demonstrate that $B$ incurs error at most $e \cdot \epsilon$ on any transductive instance $S = (x_i, y_i)_{i \in [m]}$ with $m \geq n + 1$. Intuitively, this is due to the fact that $B$ mimics the performance of $A$ on i.i.d.\ data drawn from $\Unif(S)$, save for the fact that the test point $x$ must be excluded. Nevertheless, a sample of $|S|$ many points drawn i.i.d.\ from $S$ will happen to omit any given datapoint (such as $x$) with probability at least $\frac{1}{e}$. Thus, the lack of fidelity in $B$'s imitation of $A$ can at most inflate its error by a factor of $e$. More explicitly, 
\begingroup
\allowdisplaybreaks
\begin{align*}
L_{S}^{\Trans}(B) &= \E_{i \in [m]} \ell \big( B(S_{-i})(x_i), y_i \big) \\
&= \E_{i \in [m]} \E_{T \sim \Unif(S_{-i})^{m-1}} \ell \Big( A\big(\Unif(S_\CX), T\big)(x_i), y_i \Big) \\
&\leq e \cdot  \E_{i \in [m]} \E_{T \sim \Unif(S)^{m-1}} \ell \Big( A\big(\Unif(S_\CX), T\big)(x_i), y_i \Big) \\
&= e \cdot \E_{T \sim \Unif(S)^{m-1}} \E_{i \in [m]} \ell \Big( A\big(\Unif(S_\CX), T\big)(x_i), y_i \Big) \\ 
&= e \cdot \E_{T \sim \Unif(S)^{m-1}} L_{\Unif(S)} A(\Unif(S_\CX), T) \\
&\leq e \cdot \epsilon.
\end{align*}
\endgroup
The third line makes use of the fact that a sample $T \sim \Unif(S)^{m-1}$ avoids any given point $x_{i} \in S$ with probability $\geq \frac{1}{e}$. (Recall that a transductive learning instead on a labeled dataset $|S| = m$ employs training sets of size $m-1$.) In particular, let $f(m) = (1 - \frac{1}{m})^{m-1}$ denote the probability of a given $x_i \in S$ being avoided by $T \sim \Unif(S)^{m-1}$. First note that 
\[ \lim_{m\to \infty} f(m) = \lim_{m \to \infty} \left (1 - \frac 1m \right )^{-1} \cdot \lim_{n \to \infty} \left( 1 - \frac 1n \right) ^n =  1 \cdot \frac 1e  = \frac 1e.\]
Furthermore,
\[ \frac{\mathrm{d}}{\mathrm{d} x} f(x) = \frac{\left(\frac{x-1}{x}\right)^x \left( x \log \left( \frac{x-1}{x} \right) + 1 \right)}{x - 1} \leq 0 \qquad \text{for } x > 1, \]
as $\left( \frac{x-1}{x} \right)^x > 0$, $x - 1 > 0$, and 
\begin{align*}
x \log \left( \frac{x-1}{x} \right) + 1 &= x \log \left( 1 - \frac{1}{x} \right) + 1 \\
&\leq x \cdot - \frac 1x + 1 \\
&= 0. 
\end{align*}
Thus $f(m)$ is weakly decreasing on $(1, \infty)$ and $f(m) \geq \frac 1e$ for all $m \in \N$, as desired. 
\end{proofof}

\end{document}
