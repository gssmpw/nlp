\section{Related Work}
\paragraph{Proper learnability.} We focus primarily --- but not exclusively --- on the setting of multiclass classification, i.e., learning under the 0-1 loss function. When $|\CY| = 2$, one recovers binary classification, for which learnability is characterized by the VC dimension and empirical risk minimization (ERM) is a nearly-optimal learner ____.
As ERM is proper, learnability is thus equivalent to proper learnability in the binary case.\footnote{Attaining the optimal sample complexity, however, is known to require improper learning in general. Interestingly, recent work has demonstrated that the improperness requirement for optimal learning can be satisfied using simple aggregations of proper learners, such as a majority of only 3 ERM learners ____. In the multiclass setting, however, there are learnable classes which cannot be learned by any aggregation of a finite number of proper learners ____.} In the multiclass case, $\CY$ is permitted to be of arbitrarily large size (even infinite), and the equivalence between proper learnability and improper learnability from the binary case was shown to fail by ____. They also proposed the \emph{Daniely-Shalev-Shwartz} (DS) \emph{dimension}, and conjectured that it characterizes improper multiclass learnability. This was recently confirmed in a breakthrough result of ____, resolving a long-standing open question. Regarding algorithmic templates for multiclass learning, relatively little is known: ____ designed one learner for general DS classes, using an intricate sequence of arguments and algorithmic techniques (e.g., \emph{list} PAC learning, sample compression, one-inclusion graphs, etc.). It is natural to ask for simpler learners than that of ____, perhaps which bear a closer resemblance to algorithms enjoying practical success (e.g., structural risk minimization (SRM)). Recently, ____ made some progress by demonstrating that there always exist optimal learners taking the form of \emph{unsupervised local regularization}, a certain relaxation of classical regularization. The proof is non-constructive, however, saying little about the precise \emph{form} of the regularizer or the learner. Perhaps most relevant to our work is the line of research studying learnability via ERM. This includes work demonstrating that there can be arbitrarily large gaps between the sample complexities of different ERM learners, and that the sample complexity of ERM is closely related to the \emph{graph dimension} ____. Learnability by any ERM learner is \emph{not} equivalent to proper learnability, however, and thus this does not directly address our primary question. Regarding the issue of \emph{optimal} proper learning, ____ studied the conditions under which a binary hypothesis class $\CH$ can be learned with optimal sample complexity by a proper learner, and established a characterization via finiteness of the \emph{dual Helly number}, under general conditions on~$\CH$.  

\paragraph{The role of unlabeled data.} There is a long line of work studying the power of unlabeled data in learning, often formalized by the setting in which a learner receives both labeled and unlabeled datapoints, i.e., \emph{semi-supervised learning} (SSL) ____. One direction has studied SSL under the assumption that there is a relationship between the unlabeled data distribution $\CD$ and the true labeling function $h^*$, and demonstrated results supporting the power of unlabeled data in this setting ____. Another line of work demonstrates that in that absence of any such assumptions, unlabeled data has little effect in binary classification from a worst-case perspective ____. Yet another direction of work studies the power of unlabeled training points from a fine-grained perspective, examining learners' sample complexities on particular data distributions rather than on a worst-case basis, and establishes the value of unlabeled data in learning binary classes of infinite VC dimension ____.
We study a setting which makes no assumption on the unlabeled distribution $\CD$ or the true labeling function $h^* \in \CH$, but assumes that the learner receives complete information of $\CD$. The learner is then judged on a worst-case basis over all possible (realizable) distributions. This most closely aligns with the ``utopian" model of SSL studied by ____ and ____. Notably, ____ demonstrated that this setting --- which they refer to as simply ``knowing the marginal" --- is of no additional help for binary classification. (I.e., the worst-case expected error rate of a learner does not improve by granting it knowledge of the marginal.) Our analogous result can be seen as extending this finding to a broader collection of bounded metric loss functions (Theorem~\ref{Theorem:distribution-fixed-sample-equivalence}).

\paragraph{Decidability in learning.} In Section~\ref{Section:obstructions-proper-learnability}, we establish several obstructions to characterizing proper learnability in multiclass classification, including by demonstrating that there exist classes~$\CH$ for which it is \emph{logically undecidable} whether $\CH$ can be properly learned. That is, within the ZFC axioms it can be neither proven nor disproven that $\CH$ is properly learnable. This result builds upon the breakthrough work of ____, which established that the learnability of certain EMX (Estimating the Maximum) learning problems can be undecidable. Notably, ____ established an equivalence between certain EMX learning problems and bandit problems in order to establish that bandit learnability can likewise be undecidable. 
A related line of work, also inspired by ____, investigates the \emph{algorithmic decidability} of learning, i.e., examining whether problems can be learned using learners which are computable, rather than merely abstract mathematical functions
____. Recent developments in this area have established that there exist VC classes which cannot be learned by any computable learner ____, and that learnability via (improper) computable learners is instead characterized by the \emph{effective VC dimension}, which roughly measures the smallest cardinality $k$ for which one can always compute a behavior on any $k + 1$ distinct unlabeled points which $\CH$ cannot express ____. Notably, this characterization holds for binary classification over the domain of the natural numbers; for binary classification over more general computable metric spaces, see ____. 
Further work includes that of ____ on computable online learning, ____ on the computability of robust PAC learning, and ____, which studies the computability of learning when learners are equipped with a restricted form of black-box access to the underlying hypothesis class $\CH$.