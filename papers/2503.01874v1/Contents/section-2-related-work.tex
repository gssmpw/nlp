\section{Related Work}
\label{section-2}

%\textbf{Model merging} has become a vital strategy for combining multiple fine-tuned models into a single multitask model without requiring additional training. Fine-tuned models from the same pre-trained model often share part of the optimization trajectory, making them suitable for merging. This process can enhance performance on target tasks, improve out-of-domain generalization, and support applications such as federated learning, model compression, and continual learning.
\textbf{Model merging} has become a vital strategy for combining multiple fine-tuned models into a single multitask model without requiring additional training. The simplest merging method is directly averaging the model parameters~\citep{izmailov2018averaging,wortsman2022model}. However, this naive approach often fails to account for task-specific variations, leading to suboptimal performance. A more refined approach, \textbf{Task Arithmetic}~\citep{ilharco2022editing}, combines task vectors—differences between fine-tuned and pre-trained parameters—using weighted sums controlled by scaling coefficients
% ~\qbh{is ``merge factors'' a terminology used by prior works? It is a little weird as ``merge'' is verb.}~\yzz{I checked the terminology in task arithmetic, it should be ``scaling coefficients'', will repace them soon}
$\lambda$. These scaling coefficients allow precise control over the contribution of each task vector during merging, playing a critical role in balancing the influence of different tasks. However, it still struggles with parameter redundancy and sign conflicts.

%\textbf{Task Arithmetic}~\citep{ilharco2022editing} was introduced as a pioneering method in the realm of task vector-based merging. In Task Arithmetic, task vectors—computed as the difference between fine-tuned model parameters and their initial pre-trained values—are combined using weighted sums to create a multitask model. However, it struggle with issues such as parameter redundancy and sign conflicts.

%To address some of these issues, \textbf{TIES-Merging}~\citep{yadav2024ties} introduces a more sophisticated approach that operates in two key ways: first, by pruning parameters that are not significantly impactful, thereby reducing the influence of redundant parameters; and second, by resolving sign conflicts during the merging process. This dual approach minimizes interference between task vectors and ensures that the most critical parameters are preserved and properly aligned during the merge. 
To address these issues, \textbf{TIES-Merging}~\citep{yadav2024ties} prunes low-magnitude parameters and resolves sign conflicts, reducing interference and preserving critical parameters during merging. \textbf{DARE}~\citep{yu2024language}, a technique inspired by \textbf{Dropout}~\citep{srivastava2014dropout}, reveals the high redundancy in task vectors by randomly dropping 90\% of the parameters and rescaling the remaining ones. Using random pruning, DARE has been shown to outperform magnitude-based pruning methods in model merging. However, DARE does not fully explain the reasons for this improvement. Our analysis suggests that DARE helps mitigate some of the overlap and imbalance. However, the random nature of the approach can potentially sacrifice precision.

\textbf{Model pruning,} particularly \textbf{magnitude pruning}~\citep{zhu2017prune}, have been extensively studied for their role in optimizing model performance and reducing computational costs~\citep{liurethinking,frankle2018lottery,gale2019state,zhu2017prune}. Magnitude pruning retains parameters based on their magnitude, assuming that larger magnitudes correspond to more critical information~\citep{kovaleva2021bert,puccetti2022outliers,yin2023outlier}. However, when applied in the context of model merging, this approach can lead to an unbalanced distribution of retained weights, which exacerbates conflicts during the merging process and results in suboptimal performance.

To address this issue, while \textbf{n:m pruning}~\citep{zhou2021learning,xia2022structured} was originally designed for pruning and inference acceleration, we discovered that it can be repurposed to control the balance of sparsified task vectors in model merging. Although n:m pruning may not perform as well as unstructured pruning in traditional scenarios, our findings demonstrate that it effectively mitigates weight imbalance, leading to improved performance in merged models. %This insight forms a key contribution of our work, highlighting the potential of n:m pruning in enhancing model merging. 

Our proposed \textbf{CABS} method builds upon prior works by introducing CA, a novel approach designed to eliminate parameter overlap during model merging. Additionally, it repurposes the existing n:m pruning technique to mitigate unbalanced weight distribution. Together, CABS effectively enhances the stability and performance of model merging.