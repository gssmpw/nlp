\section{Methodology}
\label{section-4}

% \subsection{Overview of CABS Framework}

To address the aforementioned issues, we propose the CABS (Conflict-Aware and Balanced Sparsification) framework. As illustrated in Figure~\ref{CABS}, CABS resolves parameter conflicts and ensures balanced weight distribution, thus enhancing the performance of the merged model. The framework integrates two core strategies: Conflict-Aware Sparsification (CA) and Balanced Sparsification (BS), which will be detailed in the following sections. %Algorithm~\ref{algo:cabs_sparsity} demonstrates how these strategies are implemented in CABS.
The detailed implementation of CABS is provided in Appendix~\ref{Algorithm of CABS}.

\iffalse
\qbh{I agree to move the algo to appendix.}
\begin{algorithm}[htbp]
\caption{CABS}
\label{algo:cabs_sparsity}
\begin{algorithmic}[1]
    \REQUIRE Task vectors \( \tau_A , \tau_B \), base model \( W_{\text{base}} \), sparsity level \( n \) , \( m \), scaling coefficients \( \lambda_A \) , \(\lambda_B \)%replace ''rescale factors`` with ''merge factors``
    \ENSURE Parameters of the merged model \( W_{\text{final}} \)
    
    \STATE Apply n:m pruning to \( \tau_A \) and compute \( \text{mask}_A \) \\ \hfill \# include BS
    \STATE \( \tau_{\text{B remaining}} = \tau_B \odot (1 - \text{mask}_A) \) to eliminate overlap with \( \tau_A \) \hfill \# core step of CA
    \STATE Apply n:m pruning to \( \tau_{\text{B remaining}} \) to compute \( \text{mask}_B \) \\ \hfill \# include BS 
    \STATE Merge the pruned vectors with the base model: 
    \vspace{-0.7em}\[W_{\text{final}} = W_{\text{base}} + \lambda_A \times \text{mask}_A \odot \tau_A + \lambda_B \times \text{mask}_B \odot \tau_B\]\vspace{-1.9em}
    \STATE Return \( W_{\text{final}} \)
\end{algorithmic}
\end{algorithm}
\vspace{-0.15in}
\fi

\subsection{Conflict-Aware Sparsification (CA)}
\label{CA}

%\textbf{Motivation}. During model merging, overlapping task vectors degrade performance by introducing conflicting updates and complicating the control of task vector contributions through merge factors \(\lambda\).By minimizing these overlaps, it is expected to enhance the stability and performance of the merged model.
\textbf{Sequential Pruning and Mask Application}. CA aims to eliminate parameter overlap during model merging by employing a sequential pruning strategy. The process begins with the first vector $ \tau_A $ being pruned, producing a mask $mask_A$ that marks the positions of the retained weights. This mask is then used to guide the pruning of the second task vector $ \tau_B $, ensuring that there is no overlap between the parameters of $ \tau_A $ and $ \tau_B $. 

For the second task vector $ \tau_B $, the prior mask $mask_A$ is applied in an inverted form to determine the remaining weights that do not overlap with the first pruned task vector. Specifically, the remaining weights of $ \tau_B $ are calculated as: 
\begin{equation}
\tau_{\text{B remaining}} = \tau_B \odot (1 - \text{mask}_A).
\end{equation}
This ensures that only the non-overlapping weights in $ \tau_B $ are retained in the subsequent pruning process. Afterward, a second round of pruning is performed on $ \tau_{\text{B remaining}} $, generating a new sparse mask $ mask_B $, which can then be merged with the prior pruned task vector without overlap.

\textbf{Minimizing Overlap When Sparsity Limits are Exceeded}. When the sum of the sparsity levels across all task vectors exceeds 1 (e.g., when each vector retains 75\% of its parameters), it becomes impossible to achieve zero overlap. In such cases, the objective shifts from eliminating overlap to minimizing it as much as possible. Additional pruning steps are applied selectively to reduce the extent of overlap between task vectors. The detailed implementation is provided in Appendix \ref{Algorithm of CABS}.

\subsection{Balanced Sparsification (BS)}
\label{BS}

%\textbf{Motivation}. While CA can effectively reduce overlap, it does not address the imbalance in weight distribution that can arise within task vectors. These imbalances often lead to suboptimal performance in the merged model, affecting both its stability and efficiency. To mitigate this problem, we propose the Balanced Sparsification (BS) strategy, which enhances CA by addressing these imbalances and further improving the model's overall performance.
\textbf{Block-Based Pruning Strategy}. In BS, the weight matrix is divided into disjoint blocks of \( m \) consecutive weights, and within each block, the \( n \) weights with the largest absolute magnitude are retained, while the rest are pruned. This strategy is applied uniformly across all layers to ensure a more even weight distribution within each task vector. Minimizing imbalances prevents performance degradation of the merged models. A more detailed discussion about the differences between Balanced Sparsification (BS) and n:m pruning is presented in Appendix~\ref{Comparison of n:m Sparsity and BS}.

CABS can be integrated with other model merging techniques, where CA and BS can be applied independently or combined with other approaches to further enhance model merging. Additionally, Our analysis shows that CABS introduces minimal computational and memory overhead compared to standard merging methods, ensuring efficiency and scalability in various model merging scenarios. Detailed analyses are provided in Appendix~\ref{appendix:computational_overhead_analysis} and Appendix~\ref{appendix:memory_overhead}.

\subsection{Theoretical Analysis}
\label{sec:theoretical-analysis}

This section provides a theoretical analysis of how Conflict-Aware Sparsification (CA) reduces parameter overlap, ensures orthogonality of task vectors in parameter space, and mitigates interference during model merging.

\textbf{Sparse and Non-Overlapping Task Vectors.}  
CA employs a sequential pruning strategy to produce sparse task vectors \( \tau_A, \tau_B \in \mathbb{R}^{u \times v} \) with non-overlapping parameters. Their binary masks \( M_A, M_B \in \{0, 1\}^{u \times v} \) satisfy:
\begin{equation}
(M_A)_{ij} (M_B)_{ij} = 0, \quad \forall i, j.
\end{equation}
The task vectors are defined as:
\begin{equation}
\tau_A = \Delta \mathbf{W}_A \odot M_A, \quad \tau_B = \Delta \mathbf{W}_B \odot M_B.
\end{equation}
where~\(\Delta \mathbf{W}_A,\, \Delta \mathbf{W}_B\) are parameter updates from a base model, and \(\odot\) denotes elementwise multiplication. 
This ensures that \(\tau_A\) and \(\tau_B\) have disjoint non-zero entries. Prior studies~\citep{yu2024language,yadav2024ties} 
%~\qbh{add citations}
and our experimental results in~\ref{section-rescale_experiments} confirm that these sparse updates are nearly lossless in retaining task-specific information, as simple rescaling compensates for pruning-induced changes.

\textbf{Non-Overlap Implies Orthogonality.}  
The Frobenius inner product of the task vectors \(\tau_A\) and \(\tau_B\) is:
\begin{align}
\langle \tau_A, \tau_B \rangle_F 
&= \sum_{i=1}^{u} \sum_{j=1}^{v} (\tau_A)_{ij} (\tau_B)_{ij} \nonumber \\
&= \sum_{i=1}^{u} \sum_{j=1}^{v} (\Delta \mathbf{W}_A)_{ij} (\Delta \mathbf{W}_B)_{ij} (M_A)_{ij} (M_B)_{ij}.
\end{align}
Under the non-overlapping condition \((M_A)_{ij} (M_B)_{ij} = 0\), each term in the summation equals zero:
\begin{equation}
(\Delta \mathbf{W}_A)_{ij} (\Delta \mathbf{W}_B)_{ij} (M_A)_{ij} (M_B)_{ij} = 0, \quad \forall i, j.
\end{equation}
Thus, the inner product reduces to:
\begin{equation}
\langle \tau_A, \tau_B \rangle_F = 0.
\end{equation}
This guarantees that \(\tau_A\) and \(\tau_B\) are orthogonal.

\textbf{Orthogonality Reduces Interference.}  
%\qbh{In my understanding, Sec 4.4 aims to prove that CA can avoid interference introduced by scaling coefficients (i.e., merge factor). So it would be better to emphasize the importance of coefficients in model merging somewhere (e.g., in Secs 2 and 3). For now, it seems that only Sec 3 (i.e., Line 190) mentioned the merge factor but did not highlight its importance.}
Consider the combined weight update:
\begin{equation}
\Delta \mathbf{W} = \lambda_A \tau_A + \lambda_B \tau_B,
\end{equation}
where \(\lambda_A, \lambda_B \in \mathbb{R}\) are the scaling coefficients for the task vectors. The squared Frobenius norm of the update is:
{
\small
\begin{equation}
\|\Delta \mathbf{W}\|_F^2 = \|\lambda_A \tau_A\|_F^2 + \|\lambda_B \tau_B\|_F^2 + 2 \lambda_A \lambda_B \langle \tau_A, \tau_B \rangle_F.
\end{equation}
}
When \(\tau_A\) and \(\tau_B\) are orthogonal (i.e., \(\langle \tau_A, \tau_B \rangle_F = 0\)), the cross-term vanishes, and the norm simplifies to:
\begin{equation}
\|\Delta \mathbf{W}\|_F^2 = \|\lambda_A \tau_A\|_F^2 + \|\lambda_B \tau_B\|_F^2.
\end{equation}
This decoupling ensures that adjusting \(\lambda_A\) affects only the contribution of \(\tau_A\), with minimal direct interference to \(\tau_B\). As a result, task vector contributions can be independently scaled, avoiding interference %and enabling precise control 
during model merging. 

\textbf{On Overlap and Possible Synergy.}
While overlap often leads to conflicts, there may be cases where overlapping coordinates have aligned updates, providing synergistic effects. However, identifying exactly which overlap is ``helpful'' can be challenging, as it requires deep insights into each task's loss surface. Figure~\ref{fig:coupling_degree} shows that excessive overlap typically impairs performance, whereas minimized overlap yields stable and predictable gains. Hence, CA adopts a simpler strategy of systematically limiting overlap, ensuring robust improvements across various tasks.

%\textbf{Impact of Non-Orthogonality.}  
%In contrast, when \(\tau_A\) and \(\tau_B\) are not orthogonal, the cross-term \(2 \lambda_A \lambda_B \langle \tau_A, \tau_B \rangle_F\) becomes significant. High overlap between task vectors, as often observed in magnitude-based pruning methods, implies a larger value of \(\langle \tau_A, \tau_B \rangle_F\), directly increasing the cross-term. To better understand this, the Frobenius inner product can be expressed as:
%\begin{equation}
%\langle \tau_A, \tau_B \rangle_F = \|\tau_A\|_F \|\tau_B\|_F \cdot \mathrm{corr}(\tau_A, \tau_B),
%\end{equation}
%where \(\mathrm{corr}(\tau_A, \tau_B)\) represents the correlation between the two task vectors. High overlap may suggest that \(\mathrm{corr}(\tau_A, \tau_B)\) may be large, further contributing to the cross-term. 

%Consequently, adjusting \(\lambda_A\) to control \(\tau_A\)'s contribution inadvertently affects \(\tau_B\) due to the dependency introduced by the cross-term. This reduces the precision of task-specific control and destabilizes the optimization process. Moreover, overlapping updates often conflict in magnitude or direction, exacerbating interference and further degrading task performance.

\textbf{Conclusion.}  
CA eliminates parameter overlap by projecting task vectors onto nearly lossless orthogonal subspaces.
Although perfect functional separation cannot be guaranteed in a non-linear neural network, the resulting parameter-space orthogonality ensures that cross-terms vanish during model merging, allowing independent control of each taskâ€™s contribution through the scaling coefficients (\(\lambda\)). By minimizing interference and enabling precise scaling, CA improves both the stability of optimization and the overall efficiency and performance of the merged model. Thus, CA successfully tackles the central challenges of task-vector sparsification, forming a robust foundation for effective model merging.