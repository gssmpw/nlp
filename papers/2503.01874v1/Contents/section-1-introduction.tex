\section{Introduction}
\label{section-1}

\begin{figure*}[t]
    \vspace{-0em}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/cabsall.pdf}
    \vspace{-0.1in}
    \caption{Illustration of the \textbf{CABS} framework, which enhances model merging by addressing parameter overlap and weight imbalance. By integrating Conflict-Aware Sparsification (CA) and Balanced Sparsification (BS), CABS delivers more effective merging compared to standard merging with magnitude-based pruning (MP), leading to improved model performance.}
    \label{CABS} 
    \vspace{-1em}
\end{figure*}

Model merging has gained increasing attention in the deep learning community, particularly in the context of using task vectors for model merging in large language models (LLMs)~\citep{ilharco2022editing,li2023deep,wortsman2022model,jin2022dataless,matena2022merging,singh2020model,akiba2024evolutionary}. This technique has become especially popular for merging homologous models, those derived by fine-tuning the same base model on different tasks, to create a better-performing model. Many of the best-performing models on the LLM leaderboard~\citep{open-llm-leaderboard} are built by fine-tuning the base models and subsequently merging them to optimize task-specific performance. Additionally, major enterprises have employed model merging techniques in the development of pre-training models, such as Llama3~\citep{dubey2024llama} and Qwen2~\citep{yang2024qwen2,lu2024online}, to enhance generalization capabilities and improve performance across a range of tasks.

Recent studies have further shown that sparsifying task vectors before merging can mitigate parameter conflicts between different task vectors, leading to measurable improvements in merging performance~\citep{yu2024language,yadav2024ties,davari2023model,he2024localize}. These conflicts can be categorized into two types: (a) conflicts due to redundant parameters, where parameters that contribute little to performance are unnecessarily retained, and (b) conflicts due to overlapping parameters, where task vectors retain parameters that overlap, potentially with significantly different magnitudes or signs. Such overlaps hinder the effectiveness of the merging process. 

%Sparsifying can be achieved by selectively or randomly dropping part of a task vector. This process is similar to one-shot pruning, with the former aiming to reduce conflicts in model merging and the latter targeting model compression.
Sparsifying task vectors, whether selectively or randomly, aims to reduce conflicts in model merging. However, it shares methodological similarities with one-shot pruning, which primarily focuses on model compression.
Magnitude-based pruning~\citep{liang2021pruning} is one of the mainstream pruning techniques, which can estimate the importance of weights and selectively preserve the essential weights, thus being rightfully superior to random pruning. Inspired by pruning techniques, recent model merging studies~\citep{yadav2024ties} applied magnitude-based pruning to sparsify task vectors with the important weights retained. 
However, as pointed out by DARE~\citep{yu2024language}, the results are counterintuitive: magnitude-based pruning underperforms compared to random pruning methods in the context of model merging. 
%This unexpected outcome contradicts the observations from widely studied pruning techniques, which demonstrate that retaining important weights helps preserve model performance.
% \qbh{on second thought, such unexpected outcome does not contradict the observations from NN pruning. The task vectors obtained by magnitude-based pruning indeed outperforms those obtained by random pruning. It contradicts the intuition that merging optimal task vectors (obtained by magnitude-based pruning) should outperform merging suboptimal task vectors (obtained by random pruning.)}

Our research explores the reasons behind this discrepancy, especially in a setting where magnitude-based pruning is expected to perform well. 
Addressing these issues is key to developing high-performance merged models.
Specifically, by analyzing the weight distribution and overlap in task vectors produced by DARE and magnitude-based pruning, we identified two key factors contributing to the underperformance of magnitude-based pruning:

\textbf{High Parameter Overlap}: After magnitude-based pruning, the retained weights of different task vectors often exhibit significant overlap, particularly compared to random methods like DARE. The overlap increases conflicts between task vectors during model merging, ultimately degrading the performance of the merged model.

\textbf{Unbalanced Weight Distribution}: Magnitude-based pruning tends to distribute retained weights unevenly across the model's weight matrices, with some regions retaining significantly more weights than others. After pruning, the model merging process applies a uniform scaling coefficient globally across the model to restore performance. However, this process amplifies the existing imbalance, ultimately leading to suboptimal performance. %In contrast, random pruning methods like DARE can avoid this problem, which maintain better balance across the model by distributing weights more uniformly.

To address the issues above, we propose a novel framework: \textbf{Conflict-Aware and Balanced Sparsification (CABS)}. As illustrated in~\autoref{CABS}, CABS distinguishes itself from existing methods by introducing two key strategies:
 
\textbf{Conflict-Aware (CA) Sparsification}: CA addresses conflicts between task vectors by employing a sequential pruning approach, ensuring \textit{no overlap} between the retained weights of different task vectors. As shown in \autoref{CABS} (a), CA first applies pruning to task vector A (blue, \( \tau_A \)), and then masks the overlapping weights when pruning task vector B (yellow, \( \tau_B \)), resulting in Remaining \( \tau_B \). This masking technique minimizes conflicts during the merging process by removing shared weights, allowing for more effective task vector merging and improving the final model performance.

\textbf{Balanced Sparsification (BS)}: BS addresses the issue of unbalanced weight distribution by applying n:m pruning, which selectively retains \( n \) weights out of every \( m \) consecutive weights based on magnitude~\citep{zhou2021learning}. As demonstrated in \autoref{CABS} (a), BS is first applied to \( \tau_A \), followed by another application to Remaining \( \tau_B \) (derived by CA). This ensures a more uniform distribution of weights across layers, reducing the adverse effects of weight concentration in certain regions.
%These strategies are straightforward, yet highly effective. Our extensive experiments on both decoder-based Mistral-7B~\citep{Jiang2023mistral} and encoder-based RoBERTa-Base~\citep{liu2019roberta} models, spanning tasks from the LLM leaderboard and the GLUE~\citep{wang2018glue} dataset respectively, demonstrate that CABS effectively addresses the issues associated with magnitude-based pruning. In Mistral-7B experiments, CABS achieved an average performance of 76.50, outperforming the ``ideal'' virtual model (76.30), which is a hypothetical model that picks the highest score from each fine-tuned model for every task. with previous SOTA methods scoring 76.02 and fine-tuned models at 75.86. In RoBERTa-Base experiments, CABS improved task performance to 81.70, outperforming previous SOTA method (79.88) and the baseline task-arithmetic score (79.55). These results strongly confirm CABS's robustness and superiority across different architectures.
%While absolute improvements may appear small, they consistently confirm CABS's superiority across different architectures. Furthermore, an ablation study verifies the validity of each strategy.

These strategies are effective and easy to implement. We conducted extensive experiments on decoder-based Mistral-7B~\citep{Jiang2023mistral} and encoder-based RoBERTa-Base~\citep{liu2019roberta}, using tasks from the LLM leaderboard and the GLUE~\citep{wang2018glue} dataset. These experiments demonstrate that CABS effectively mitigates the issues caused by magnitude-based pruning. On Mistral-7B, CABS achieved an average performance score of 76.50, surpassing the ``ideal'' virtual model (76.30), which hypothetically selects the best performance score for each task. CABS also exceeds the state-of-the-art (76.02) and fine-tuned models (75.86). Similarly, on RoBERTa-Base, CABS achieved a score of 81.70, outperforming the SOTA (79.88) by 1.82 points and the vanilla baseline (79.55) by 2.15 points. These results strongly confirm CABS's superiority across diverse neural network architectures and various tasks. 

\textbf{Our contributions are as follows:}
\vspace{-0.1in}
\begin{itemize}
    \setlength{\itemsep}{0pt} 
    \setlength{\parsep}{0pt}  
    \setlength{\topsep}{0pt}  
    \item We identify two key issues encountered by magnitude-based pruning in the context of task vector sparsification, i.e., high parameter overlap and unbalanced weight distribution.
    \item We propose the CABS framework, consisting of conflict-aware sparsification and balanced sparsification strategies, which can effectively address the two identified issues.
    \item We conduct comprehensive experiments across a variety of tasks and model sizes, showing that CABS outperforms state-of-the-art methods.
    \item We are the first to introduce an ``ideal'' yet rigorous baseline for evaluation, where CABS outperforms this virtual baseline while all existing methods fall short.
\end{itemize}
\vspace{-0.1in}
Our code is available at \url{https://github.com/zongzhenyang/CABS}.

