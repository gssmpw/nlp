\section{Experiments}
\label{section-5}

We conducted extensive experiments to demonstrate the effectiveness of CABS in enhancing performance and stability in model merging across diverse tasks and model scales.

\subsection{Experimental Setup}
\label{section-5-experimental_setup}

\textbf{Datasets and Models for Large Language Model Experiments.}
For large-scale model evaluation, we utilized the LLM Leaderboard benchmark, encompassing six key tasks: AI2 Reasoning Challenge ~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag}, MMLU~\citep{hendrycks2020measuring}, TruthfulQA~\citep{lin2021truthfulqa}, Winogrande~\citep{sakaguchi2021winogrande}, and GSM8K~\citep{cobbe2021training}. These tasks were assessed using the Eleuther AI Language Model Evaluation Harness~\citep{eval-harness}, a standardized framework designed to test generative language models across various tasks. The models used in our experiments were based on the Mistral-7b-v0.1 backbone and included fine-tuned variants such as WildMarcoroni-Variant1-7B and WestSeverus-7B-DPO-v2. 

In addition, we conducted a new set of experiments using the Open LLM Leaderboard 2~\citep{open-llm-leaderboard-v2}, which includes six tasks: IFEval~\citep{zhou2023instructionfollowingevaluationlargelanguage}, BBH~\citep{suzgun2022challengingbigbenchtaskschainofthought}, MATH~\citep{hendrycks2021measuringmathematicalproblemsolving}, GPQA~\citep{rein2023gpqagraduatelevelgoogleproofqa}, MUSR~\citep{sprague2024musrtestinglimitschainofthought}, and MMLU-PRO~\citep{wang2024mmluprorobustchallengingmultitask}. For these experiments, we employed the qwen-2.5-7b-instruct~\citep{yang2024qwen2.5} model as the backbone and evaluated fine-tuned fq2.5-7b-it and Tsunami-0.5-7B-Instruct to assess performance across these additional benchmarks. 
More details about the datasets and models are provided in Appendix~\ref{datasets-backbones-decoder}.

\textbf{Datasets and Models for Small Language Model Experiments.}
For evaluating small-scale models, we utilized the GLUE benchmark, which includes four binary classification tasks: CoLA~\citep{warstadt2019neural}, SST-2~\citep{socher2013recursive}, MRPC~\citep{dolan2005automatically}, and RTE~\citep{dagan2005pascal,bar2006second,giampiccolo2007third,bentivogli2009fifth}. To increase task difficulty and diversity, we also included the multiple-choice reading comprehension task RACE~\citep{lai2017race} and the question-answering task SQuAD~\citep{rajpurkar2016squad}. We utilized RoBERTa~\citep{liu2019roberta} and GPT-2~\citep{radford2019language} as pre-trained backbones, with fine-tuned models sourced from HuggingFace. Due to the unavailability of test labels, the original validation sets were repurposed as test sets. Additional details are provided in Appendix~\ref{datasets-backbones-encoder}.

\textbf{Evaluation Metrics.}
Performance was evaluated primarily using accuracy for GLUE tasks. For tasks from the LLM Leaderboard, we used task-specific metrics, such as success rates and accuracy, depending on the default evaluation metric for each task. Detailed explanations of the evaluation metrics and the rationale behind these choices can be found in Appendix~\ref{appendix:evaluation_metrics}.

\textbf{Baselines.}
We compared CABS against several baseline methods in two main categories: conflict handling and sparsification strategies. For conflict handling, we evaluated Task Arithmetic %(averaging task vectors)
~\citep{ilharco2022editing} and TIES-Merging%(pruning low-magnitude deltas and resolving sign conflicts)
~\citep{yadav2024ties}. For sparsification, we compared CABS with DARE%(random weight dropping with rescaling)
~\citep{yu2024language}, Magnitude Pruning%(retaining highest-magnitude weights)
~\citep{zhu2017prune}, SparseGPT%(sparsification with weight importance computed via Hessian approximations)
~\citep{frantar2023sparsegpt}, and Wanda%(sparsification with weight importance computed via activation values)
~\citep{sun2023simple}.

It is worth mentioning that, to assess how far current model merging methods are from the ideal performance expected in this research field, we introduce an \textbf{``ideal model''} as a strict and meaningful baseline. The ideal model represents a hypothetical scenario where the merged model achieves optimal performance for each task. This baseline is constructed by selecting the best-performing individual task-specific model for each task, providing an upper bound for comparison.

\textbf{Other Implementation Details.}
Details on the grid search strategy and exact values of $\lambda$ are provided in Appendices \ref{appendix:grid_search_model_details} and \ref{appendix:lambda values}, respectively. Hardware setups, evaluation strategies, and hyperparameter configurations are detailed in Appendix \ref{appendix:implementation details}.

\subsection{Performance of CABS on Small LMs}
\label{section-performance_cabs_small_scale}
We conducted experiments on three task sets to evaluate the effectiveness of CABS in merging small-scale models (e.g., RoBERTa): 
1) 2-task set comprising RTE and MRPC, 
2) 4-task set comprising RTE, CoLA, MRPC, and SST-2, and 
3) 6-task set comprising RTE, CoLA, MRPC, SST-2, RACE, and SQuAD.

\textbf{Overall Performance.}
Table \ref{tab:multi_task_merging} presents the performance for merging four task vectors. 
Among the baselines, ``Task Arithmetic'' represents a vanilla approach without pruning, while other methods incorporate pruning techniques.
For our proposed CABS, the last four rows display results with different orders of sequential pruning (e.g., ``MRSC'' indicates pruning task vectors of MRPC, RTE, SST-2, and CoLA sequentially).
The last column displays the overall performance of the merged model (i.e., the average result across four tasks), with the results in brackets indicating the improvement over Task Arithmetic.
%While random-based pruning methods do offer performance gains over Task Arithmetic, these improvements are rather limited (e.g., ``TIES-Merging + DARE'' only yields a 0.33 increase). In contrast, magnitude-based approaches even degrade performance, consistent with previous analyses.

As we can see, random-based pruning methods offer limited performance improvements (e.g., ``TIES-Merging + DARE'' improves by only 0.33). Magnitude-based pruning even degrades performance, consistent with previous findings.
CABS achieves the highest average accuracy of 81.70, surpassing Task Arithmetic by 2.15 and delivering substantial improvements over all other methods.
%Additionally, the results demonstrate that different pruning orderings (e.g., CABS (SCMR), which represents the task order SST-2 $\rightarrow$ CoLA $\rightarrow$ MRPC $\rightarrow$ RTE) have minimal impact on the final average performance, with all variants achieving comparable average accuracy (e.g., 81.64 to 81.70). This consistency highlights the robustness of CABS to variations in pruning order, further reinforcing its effectiveness in model merging scenarios.
Additionally, the pruning order can affect the performance of the merged model on specific tasks. For instance, the best results for CoLA (78.52) and SST-2 (92.32) are achieved when these tasks are pruned first.
% , as shown in Table~\ref{tab:multi_task_merging}), 
However, the variation has minimal impact on overall performance. On average, all pruning orders achieve comparable results (81.64 to 81.70), highlighting the robustness of CABS in handling variations in pruning order despite task-specific differences.

\begin{table}[bt]
\vskip -0.05in
\centering
\caption{Performance of merging four task vectors (sparsity=0.90).}
\vskip 0.1in
\label{tab:multi_task_merging}
\resizebox{1.00\columnwidth}{!}{
\setlength{\tabcolsep}{0.6mm}
{
\begin{tabular}{l|ccccc}
\toprule
\textbf{Method}     &\textbf{CoLA} &\textbf{SST-2} &\textbf{RTE} &\textbf{MRPC} &\textbf{Avg} \\ \midrule
Ideal Model     &85.04   &94.04    &79.42    &91.18   &87.42    \\ \midrule
Task Arithmetic     &76.32   &90.83    &69.68    &81.37   &79.55    \\ 
~~ + Magnitude     &82.07   &87.04    &65.34    &79.66   &78.53 (-1.02) \\
~~ + DARE    &76.99   &90.14    &70.76    &81.13   &79.76 (+0.21) \\
TIES-Merging    &82.36   &86.93    &61.01    &79.41   &77.43 (-2.12) \\
~~ + DARE   &77.66   &90.94    &69.31    &81.62   &79.88 (+0.33) \\ \midrule
CABS (CSRM) (Ours)  &78.24 &\textbf{92.32} &74.37 &81.62 &81.64 (+2.09) \\
CABS (SCMR) (Ours)  &\textbf{78.52} &91.97 &73.65 &82.60 &81.69 (+2.14) \\
CABS (RCMS) (Ours)   &77.76 &92.09 &\textbf{75.09} &81.62 &81.64 (+2.09) \\
CABS (MRSC) (Ours)   &76.89 &92.09 &74.73 &\textbf{83.09} &\textbf{81.70 (+2.15)} \\ \bottomrule
\end{tabular}
}
}
\vskip -0.2in
\end{table}%因为空间放不下了,所以打算把不同顺序的结果放在这里，或者前面能删减出足够空间的话，还是单独放一个表里？

\textbf{Performance Impact of Number of Tasks.} Table \ref{tab:task_number} highlights the performance impact of task number on model merging. As the number of tasks increases, overall merging performance declines due to the increasing heterogeneity of tasks. This effect is particularly evident when transitioning from 4 to 6 tasks, as including QA and multiple-choice tasks (RACE and SQuAD) introduces additional complexity.

Despite these challenges, CABS consistently outperforms baseline methods across all scenarios. 
Compared to Task Arithmetic, CABS achieves improvements of 1.34, 2.15, and 3.06 for 2-task, 4-task, and 6-task sets, respectively. These results highlight the robustness and scalability of CABS in handling diverse and complex task sets, maintaining significant gains even as task heterogeneity increases.

\begin{table}[tb]
\centering
\caption{Impact of task number on model merging performance.}
\vskip 0.1in
\label{tab:task_number}
\resizebox{1\columnwidth}{!}{
\setlength{\tabcolsep}{3mm}
{
\begin{tabular}{l|ccc}
\toprule
\textbf{Method}    &\textbf{2 tasks} &\textbf{4 tasks} &\textbf{6 tasks} \\ \midrule
Ideal Model     &85.30   &87.42    &83.54 \\ \midrule
Task Arithmetic    &80.15    &79.55    &66.56     \\  
~~ + Magnitude    &80.38 (+0.23)    &78.53 (-1.02)    &68.28 (+1.72)   \\
~~ + DARE     &80.58 (+0.43)    &79.76 (+0.21)    &67.23 (+0.67)   \\
TIES-Merging     &80.20 (+0.05)    &77.43 (-2.12)    &65.46 (-1.10)   \\
~~ + DARE    &80.65 (+0.50)    &79.88 (+0.33)    &66.95 (+0.39) \\ \midrule
\textbf{CABS (Ours)}   &\textbf{81.49 (+1.34)} &\textbf{81.70 (+2.15)} &\textbf{69.62 (+3.06)} \\ \bottomrule
\end{tabular}
}
}
\vskip -0.15in
\end{table}

The detailed results for each configuration are presented in Table~\ref{tab:multi_task_merging}, Table~\ref{tab：rte-mrpc}, and Table~\ref{tab:six_task_merging}. Additional results for the CoLA and SST-2 tasks can be found in Table~\ref{tab:small_scale_cola_sst} (Appendix~\ref{appendix:cola_sst2_results}), and the results for the GPT-2 model are provided in Table~\ref{tab:gpt2_experiments} (Appendix~\ref{appendix:GPT2_experiments}).

\subsection{Performance of CABS on Large LMs}
\label{section-performance_cabs_large_scale}

\begin{table}[t]
\centering
\caption{Performance comparison on LLM Leaderboard using different methods (sparsity=0.75).}
\vskip 0.1in
\label{tab:large_scale_performance75}
\resizebox{1.00\columnwidth}{!}
{
\setlength{\tabcolsep}{0.6mm} 
{
\begin{tabular}{l|ccccccc}
\toprule
\textbf{Method} &\textbf{ARC} &\textbf{Hella.} &\textbf{MMLU} &\textbf{TQA} &\textbf{Wino.} &\textbf{GSM8K} &\textbf{AVG} \\ \midrule
WestSeverus   &71.30   &88.26   &63.92   &72.72   &83.69   &74.27   &75.69    \\
WildMarcoroni     &73.63   &88.67   &63.96   &70.07   &84.34   &74.48   &75.86    \\
Ideal Model    &73.63   &88.67   &63.96   &72.72   &84.34   &74.48   &76.30    \\ \midrule
Task Arithmetic  &72.52   &89.25   &63.39   &74.00   &83.46   &73.38   &76.02(-0.28) \\
~ + Magnitude    &71.93   &\textbf{89.32}   &63.18   &73.85   &84.12   &72.22   &75.77(-0.53) \\
~ + DARE     &72.64   &88.86   &63.54   &72.82   &84.03   &73.44   &75.89(-0.41) \\
TIES-Merging     &71.42   &89.17   &63.16   &73.82   &\textbf{84.74}   &73.01   &75.89(-0.41) \\
~ + DARE     &71.87   &88.95   &\textbf{63.56}   &72.87   &84.61   &73.21   &75.85(-0.46) \\ \midrule
\textbf{CABS (Ours)}    &\textbf{72.92}   &88.89   &63.50   &\textbf{74.41}   &84.63   &\textbf{74.65}   &\textbf{76.50(+0.20)} \\ \bottomrule
\end{tabular}
}
}
\vskip -0.2in
\end{table}

\textbf{Overall Performance.}
Table ~\ref{tab:large_scale_performance75} shows the results on large LMs. 
% The column ``AVG''  in Table~\ref{tab:large_scale_performance75} shows the average performance of various methods.
The last column, ``AVG'', represents the average performance of merged models across six tasks, with the numbers in parentheses indicating the gap from the ``ideal model''.
% The numbers in parentheses indicate the difference between each method's average accuracy and that of the ``ideal model''. 
Existing methods, whether based on magnitude pruning or random pruning, show similar performance and fail to outperform Task Arithmetic. These baselines remain notably below the ``ideal model'', highlighting the challenge of surpassing this strict baseline. In contrast, CABS achieves an average score of 76.50, surpassing all baselines and even exceeding the ``ideal model''. 

The result highlights the advantage of model merging in enhancing generalization. While the merged model may not surpass the ``ideal model'' on every individual task, it often achieves superior performance on specific tasks. For example, in the TruthfulQA task (see column ``TQA'' in Table~\ref{tab:large_scale_performance75}), the fine-tuned models scored 72.72 and 70.07, while the vanilla baseline reached 74.00, and CABS further increases the score to 74.41. Overall, CABS achieved an average performance of 76.50, exceeding the ``ideal model'' and significantly outperforming the best baseline score of 76.02. The result underscores the effectiveness of CABS in model merging for large-scale models.

\textbf{Notable Achievement on Open LLM Leaderboard 2.} As of February 24, 2025, our CABS framework enabled the creation of four merged models (qwen2.5-7b-cabs v0.1 through v0.4), which dominated the \textbf{top four} positions among models with 8B parameters or fewer on the Open LLM Leaderboard, As shown in Table~\ref{tab: Open LLM Leaderboard 2}. this achievement underscores CABS' effectiveness in improving model performance.

\begin{table}[t]
\centering
\caption{Results of 7B LLMs on the Open LLM Leaderboard 2(sparsity=0.75).}
\vskip 0.1in
\label{tab: Open LLM Leaderboard 2}
\resizebox{1.00\columnwidth}{!}
{
\setlength{\tabcolsep}{0.6mm} 
{
\begin{tabular}{l|ccccccc}
\toprule
\textbf{Models} &\textbf{IFEval} &\textbf{BBH} &\textbf{MATH} &\textbf{GPQA} &\textbf{MUSR} &\textbf{MMLU} &\textbf{AVG} \\ \midrule
Tsunami-0.5-7b     &74.00   &36.14   &50.45   &7.83   &12.21   &37.92   &36.43    \\
fq2.5-7b     &73.99   &\textbf{36.36}   &46.22   &6.94   &17.54   &37.92   &36.50    \\ \midrule
cabs-v0.1(Ours)     &75.06   &35.84   &47.96   &8.50   &14.17   &37.84   &36.56    \\
cabs-v0.2(Ours)     &74.18   &36.28   &49.02   &7.61   &14.86   &37.75   &36.61    \\
cabs-v0.3(Ours)     &75.70   &35.96   &49.32   &7.61   &15.24   &37.80   &36.94    \\
cabs-v0.4(Ours)     &\textbf{75.83}   &\textbf{36.36}   &48.49   &7.72   &15.17   &37.73   &36.88    \\ \bottomrule
\end{tabular}
}
}
\vskip -0.2in
\end{table}

\textbf{Performance Impact of Sparsity Rate.} Figure~\ref{fig:mistral_sparsity} illustrates the performance of different model merging methods across varying sparsity levels. 
The dashed lines represent the performance of the two pre-trained models, the merged model obtained via Task Arithmetic, and the ideal model.
% , which remain constant across all sparsity levels. 
The solid lines indicate the performance of merged models obtained using different methods at varying sparsity levels, highlighting their trends as sparsity increases. 
\begin{figure}[tb]
\vspace{0.1in}
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Figures/mistral_sparsity_comparison_bright_dare1.pdf}}
\vspace{-0.12in}
\caption{Performance comparison across sparsity.}
\label{fig:mistral_sparsity}
\end{center}
\vspace{-0.45in}
\end{figure}

As sparsity increases, all methods experience a performance decline, with the limitations of existing methods becoming particularly pronounced at 90\% sparsity. Random pruning-based methods (e.g., ``TA + DARE'') suffer the most significant degradation due to the loss of critical weights, while magnitude-based pruning approaches (e.g., ``TA + Magnitude'') also underperform due to imbalanced weight distribution.
In contrast, CABS consistently achieves superior performance across all sparsity levels, demonstrating its robustness and ability to preserve essential information even under high sparsity constraints.
More detailed results and discussions for each sparsity level are presented in Table~\ref{tab:large_scale_performance75}, Table~\ref{tab:large_scale_performance25}, and Table~\ref{tab:large_scale_performance90}. 

\subsection{Ablation Studies and Discussion}
\label{section-ablation_studies}

Within the CABS framework, we first analyze the independent contributions of CA and BS by examining the impact of parameter overlap and unbalanced weight distribution on model merging. Next, we perform ablation studies to isolate the contributions of CA and BS, demonstrating the importance of both strategies for achieving optimal results.

\textbf{Performance Impact of Overlap Rate (CA's Contribution).} We examined the impact of varying overlap rates on merged model performance to validate the importance of CA. The experiment was conducted on two task pairs (RTE-MRPC and CoLA-SST2) at a fixed sparsity level of 0.50, using random pruning for fair comparison. 
To achieve the target overlap rate ranging from 0\% (no overlap, i.e., CA) to 100\% (full overlap), we first pruned one task vector, then adjusted the pruning of the second vector by controlling the ratio of retained weights in the overlapping and non-overlapping regions. 

\begin{figure}[bt]
\vskip 0.1in
\centering
\includegraphics[width=0.9\linewidth]{Figures/performance_overlap_degree.pdf}
\vskip -0.1in
\caption{Merged model performance decreases as overlap rate increases, underscoring the importance of CA in reducing conflicts.}
\label{fig:coupling_degree}
\vskip -0.2in
\end{figure}

As shown in Figure~\ref{fig:coupling_degree}, a lower overlap rate generally leads to better performance.
Notably, the 50\% overlap rate, which corresponds to the expected overlap rate of DARE, performs worse than the non-overlapping condition achieved by CA. 
This result highlights the importance of minimizing parameter overlap, as achieved by CA. 
% This, along with the 0\% and 100\% overlap rates, has been specifically highlighted in the figure for clarity.

%CA becomes particularly critical at lower sparsity levels. For example, at 0.5 sparsity, the number and rate of overlapping parameters are much higher than at 0.9 sparsity. This makes CA especially valuable at lower sparsity levels, where task vectors retain more parameters and are thus more likely to result in significant overlap. 

\textbf{Comparisons with Magnitude-Based and Advanced Pruning Methods (BS's Contribution).} 
Table~\ref{tab:n_m_sparsity} compares BS to magnitude-based pruning approaches (including layer-wise and row-wise) and advanced pruning methods (i.e., SparseGPT and WANDA). The results show a clear progression in performance as balance improves: layer-wise pruning achieves 80.38, row-wise pruning improves to 80.61, and BS further increases to 81.30. This demonstrates that enhancing weight distribution balance can contribute to better model merging performance.

Advanced pruning methods, while effective in traditional pruning tasks, perform similarly to the worst-performing layer-wise magnitude pruning (e.g., 80.34 for SparseGPT). This indicates that such methods are less suitable for task vector sparsification in model merging scenarios. By effectively addressing weight distribution imbalances, BS demonstrates its robustness and effectiveness in improving model merging performance.

\begin{table}[tb]
\normalsize
\centering
\caption{Comparison of sparsity strategies (sparsity=0.9).}
\vskip 0.1in
\label{tab:n_m_sparsity}
\resizebox{1.00\columnwidth}{!}
{
\setlength{\tabcolsep}{2mm} % 调整列间距
{
\begin{tabular}{l|cccccc}
\toprule
\textbf{Method} &\textbf{RTE} &\textbf{MRPC} &\textbf{AVG}\\ \midrule
Fine-tuned on RTE     &79.42   &25.98   &52.70   \\
Fine-tuned on MRPC    &47.29   &91.18   &69.24   \\ \midrule
Task Arithmetic     &73.29   &87.01   &80.15   \\ 
~~ + Magnitude (layer-wise)    &\textbf{74.73}   &86.03   &80.38 (+0.23) \\
~~ + Magnitude (row-wise)    &74.06   &87.05   &80.61 (+0.46) \\ 
~~ + SparseGPT  &72.92 &87.75 &80.34 (+0.19) \\
~~ + WANDA    &73.29 &87.50 &80.40 (+0.25) \\ \midrule
\textbf{BS (Ours)}    &74.37   &\textbf{88.23}   &\textbf{81.30 (+1.08)} \\ \bottomrule
\end{tabular}
}
}
\vskip -0.15in
\end{table}

\textbf{Combined Effect of CA and BS.} To validate the effectiveness of CA and BS, we conducted an ablation study comparing configurations with only CA, only BS, and the full CABS framework. As shown in Table \ref{tab:Ablation study}, CABS not only benefits from CA and BS independently improving performance, 
% while both CA and BS independently improve performance, 
but their combination also minimizes overlap across all sparsity levels and achieves the highest accuracy.

\begin{table}[tb]
\centering
\caption{Ablation study of CABS across different sparsity levels.}
\label{tab:Ablation study}
\vskip 0.1in
\resizebox{1.00\columnwidth}{!}
{
\setlength{\tabcolsep}{1mm}
{
\begin{tabular}{l|c|c|c}
\toprule
\textbf{Sparsity Level} &\textbf{Method}    &\textbf{Overlap Rate} &\textbf{Avg Accuracy} \\ \midrule
0\%     &Task Arithmetic    &100.00     &76.02     \\ \midrule
     &TA+magnitude   &80.69    &76.03     \\
25\%     &CA Only    &66.67    &76.29    \\
     &BS Only     &80.97    &76.33    \\
     &CABS    &66.67    &76.48    \\ \midrule
     &TA+magnitude   &71.42    &75.77    \\
75\%     &CA Only &0.00     &76.21    \\
     &BS Only &58.63    &76.24    \\
     &CABS    &0.00     &76.50    \\ \bottomrule
\end{tabular}
}
}
\vskip -0.15in
\end{table}

%In conclusion, our ablation studies confirm the necessity of reducing overlap rates and maintaining balanced weight distribution for optimal model merging. They validate the crucial roles of CA and BS, showing that combining both strategies achieves the best performance across various tasks and sparsity settings.
Furthermore, we performed a series of analyses on varying $n:m$ ratios and provided additional results on the impact of different pruning orders in Appendix~\ref{appendix:Impact of nm ratios at fixed sparsity} and~\ref{appendix:Impact of sparse sequence}. These results further demonstrate the robustness of the CABS framework.
Additionally, we conducted rescaling experiments and found that applying rescaling to magnitude-pruned task vectors can restore performance to levels comparable to the original models, similar to what has been observed with DARE's random pruning method. Detailed results of these rescale experiments are included in Appendix~\ref{section-rescale_experiments}.