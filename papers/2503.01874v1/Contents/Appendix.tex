\appendix
\label{section-appendix}

\section{Additional Experiments Results}
\subsection{Impact of Lambda Search Grid on Performance}
\label{appendix:Impact of lambdas search}

In this section, we analyze the impact of different lambda search grids on the performance of various model merging methods. Our experiments demonstrate the importance of using fine-grained grid intervals to fairly compare the effectiveness of these methods. Table \ref{tab:lambda_grid} provides results across different grid intervals (0.01, 0.05, and 0.1) for several methods.

For most methods, performance declines as the grid interval increases, underscoring the importance of finer grids to accurately capture optimal lambda values. Coarser grids often miss these values, leading to noticeable drops in performance.

Interestingly, the DARE method maintains stable performance even with coarser grids (0.05 and 0.1). This is because the optimal lambda for DARE happens to coincide with a multiple of 0.1, resulting in no significant performance loss with coarser grids. However, when we exclude such coincidental ``sweet spot'' lambdas, as shown in Table \ref{tab:lambda_grid_cake}, DARE also exhibits a significant performance drop. This observation reinforces the idea that fine grid intervals are crucial for a fair and thorough evaluation of all methods. A finer grid ensures that all methods have an equal opportunity to find the best-performing lambda, though this must be balanced with computational cost

On the other hand, the CABS method demonstrates robust performance across all grid intervals. It consistently outperforms other methods, and its relative insensitivity to grid coarseness suggests that CABS is more robust and reliable under varying hyperparameter settings. This robustness, combined with its superior performance, makes CABS a strong choice for model merging.

\begin{table}[htb]
\caption{Performance comparison across different lambda grid intervals.``TA'' means ``Task Arithmetic''}
\label{tab:lambda_grid}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{\bf Grid} &\multicolumn{1}{c}{\bf Task} &\multicolumn{1}{c}{\bf DARE} &\multicolumn{1}{c}{\bf TA-} &\multicolumn{1}{c}{\bf TIES-} &\multicolumn{1}{c}{\bf TIES-} &\multicolumn{1}{c}{\bf CABS} \\
\multicolumn{1}{c}{\bf Interval} &\multicolumn{1}{c}{\bf Arithmetic} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{\bf Magnitude} &\multicolumn{1}{c}{\bf DARE} &\multicolumn{1}{c}{\bf Merging} &\\
\midrule
0.01 &80.15 &80.58(+0.43) &80.38(+0.23) &80.65(+0.40) &80.20(+0.05) &\textbf{81.49(+0.91)} \\
0.05 &79.85 &80.58(+0.73) &79.90(+0.05) &79.91(+0.06) &79.84(-0.01) &\textbf{81.19(+1.34)} \\
0.10 &79.43 &80.58(+1.15) &79.66(+0.23) &79.14(-0.29) &79.83(+0.40) &\textbf{80.82(+1.39)} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1em}
\end{table}

\begin{table}[htb]
\caption{Performance comparison across different lambda grid intervals excluding one pair sweet spot lambdas in DARE. }
\label{tab:lambda_grid_cake}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{\bf Grid} &\multicolumn{1}{c}{\bf Task} &\multicolumn{1}{c}{\bf DARE} &\multicolumn{1}{c}{\bf TA-} &\multicolumn{1}{c}{\bf TIES-} &\multicolumn{1}{c}{\bf TIES-} &\multicolumn{1}{c}{\bf CABS} \\
\multicolumn{1}{c}{\bf Interval} &\multicolumn{1}{c}{\bf Arithmetic} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{\bf Magnitude} &\multicolumn{1}{c}{\bf DARE} &\multicolumn{1}{c}{\bf Merging} &\\
\midrule
0.01 &80.15 &80.58(+0.43) &80.38(+0.23) &80.65(+0.40) &80.20(+0.05) &\textbf{81.49(+0.91)} \\
0.05 &79.85 &79.44(-0.41) &79.90(+0.05) &79.91(+0.06) &79.84(-0.01) &\textbf{81.19(+1.34)} \\
0.10 &79.43 &78.55(-0.88) &79.66(+0.23) &79.14(-0.29) &79.83(+0.40) &\textbf{80.82(+1.39)} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Detailed results of CABS on Small LMs Merging}
\label{Detailed results of CABS on Small LMs Merging}

This section provides Detailed results for the experiments on small LMs merging in Table~\ref{tab:task_number}. Table~\ref{tab：rte-mrpc} compares the performance on the RTE-MRPC task pair at 90\% sparsity, showing that CABS outperforms all baselines, achieving the highest average score of 81.49 (+1.34). Similarly, Table~\ref{tab:six_task_merging} presents the results of merging six task vectors at the same sparsity level, where CABS also demonstrates superior performance with an average score of 69.62 (+3.06), significantly surpassing other methods. These results highlight the effectiveness of CABS in achieving robust and consistent improvements across multiple tasks, even under high sparsity constraints.

\begin{table}[bt]
\vspace{-1em}
\centering
\caption{Performance comparison on RTE-MRPC task pair using different methods (sparsity=0.9).}
\label{tab：rte-mrpc}
\setlength{\tabcolsep}{0.6mm} 
{
\begin{tabular}{l|cccccc}
\hline
\textbf{Method}   &\textbf{RTE} &\textbf{MRPC} &\textbf{AVG} \\ \hline
Fine-tuned on RTE &79.42   &25.98   &52.70   \\
Fine-tuned on MRPC             &47.29   &91.18   &69.24   \\ 
Task Arithmetic   &73.29   &87.01   &80.15   \\ \hline
+ Magnitude       &\textbf{74.73}   &86.03   &80.38(+0.23) \\
+ DARE            &72.92   &88.24   &80.58(+0.43) \\
TIES-Merging      &74.37   &86.03   &80.20(+0.05)\\
+ DARE            &72.56   &88.73   &80.65(+0.50)\\ \hline
\textbf{CABS (Ours)}           &74.01   &\textbf{88.97}   &\textbf{81.49(+1.34)}\\ \hline
\end{tabular}
}
\end{table}

\begin{table}[bht]
\caption{Performance comparison of merging six task vectors(sparsity=0.9).}
\label{tab:six_task_merging}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c}
\toprule
\multicolumn{1}{c}{\bf METHOD} &\multicolumn{1}{c}{\bf RTE} &\multicolumn{1}{c}{\bf MRPC} &\multicolumn{1}{c}{\bf CoLA} &\multicolumn{1}{c}{\bf SST2} &\multicolumn{1}{c}{\bf RACE} &\multicolumn{1}{c}{\bf SQuAD} &\multicolumn{1}{c}{\bf AVG} 
\\ \midrule
Ideal Model                &79.42 &91.18 &85.04 &94.04 &71.71 &79.82 &83.54 \\ 
Task Arithmetic            &67.15 &79.41 &72.00 &85.78 &56.21 &38.82 &66.56 \\ \hline
+ Magnitude             &72.56 &81.13 &75.26 &87.50 &56.99 &36.23 &68.28 (+1.72) \\
+ DARE                  &71.12 &65.44 &72.48 &83.37 &59.57 &51.39 &67.23 (+0.67) \\
TIES-Merging           &68.94 &86.01 &66.43 &83.33 &40.11 &47.94 &65.46 (-1.10) \\
+ DARE                &\textbf{74.40} &83.83 &72.92 &56.37 &\textbf{60.38} &\textbf{53.80} &66.95 (+0.39) \\ \hline
CABS(Ours)     &68.95 &82.11 &73.92 &\textbf{90.83} &58.97 &42.96 &\textbf{69.62 (+3.06)} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1em}
\end{table}

\subsection{Additional Experiments on other Task Pairs for Small-Scale Experiments}
\label{appendix:cola_sst2_results}

In this section, we present additional results for the CoLA-SST2 task pair to complement the main text's findings on RTE and MRPC. These tasks were selected to further validate the robustness and effectiveness of the proposed \textbf{CABS} method across different types of natural language processing tasks, particularly focusing on tasks involving linguistic acceptability and sentiment analysis.

Table \ref{tab:small_scale_cola_sst} provides a detailed comparison of various model merging methods on the CoLA and SST2 tasks. The \textbf{CABS} method demonstrates superior performance, achieving the highest average scores across both tasks. The normalized accuracy scores (COLA-N and SST2-N) further emphasize the effectiveness of the \textbf{CABS} method, showing consistent improvements over the baseline methods.

The modest gains observed in the CoLA-SST2 experiments, similar to those in the RTE-MRPC pair, can be attributed to the fine-grained lambda grid search. This search process, which fine-tunes the sparsification parameters, improves the overall performance across all methods, thereby reducing the performance gaps. However, \textbf{CABS} still outperforms other methods, indicating its robustness in handling task-specific nuances during model merging.

\begin{table}[h]
\caption{Performance comparison on COLA-SST2 task pair using different methods.(sparsity=0.9)}
\label{tab:small_scale_cola_sst}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c}
\toprule
\multicolumn{1}{c}{\bf METHOD} &\multicolumn{1}{c}{\bf COLA} &\multicolumn{1}{c}{\bf SST2} &\multicolumn{1}{c}{\bf AVG} &\multicolumn{1}{c}{\bf COLA-N} &\multicolumn{1}{c}{\bf SST2-N} &\multicolumn{1}{c}{\bf AVG-N}
\\ \midrule
Fine-tuned model on COLA            &85.04 &50.92 &67.98 &100.00 &54.15 &77.08 \\
Fine-tuned model on SST2           &68.74 &94.04 &81.39 &80.83 &100.00 &90.32 \\
Task Arithmetic                   &81.59 &92.89 &87.24 &95.94 &98.78 &97.36 \\
\midrule
Task Arithmetic + Magnitude       &81.69 &93.46 &87.58(+0.34) &96.06 &99.38 &97.72(+0.36) \\
Task Arithmetic + DARE            &81.78 &93.46 &87.62(+0.38) &96.17 &99.38 &97.78(+0.42) \\
TIES-Merging                      &81.21 &93.58 &87.40(+0.16) &95.5 &99.51 &97.51(+0.19) \\
TIES-Merging + DARE               &81.78 &\textbf{93.69} &87.74(+0.50) &96.17 &\textbf{99.63} &97.90(+0.54) \\
\textbf{CABS (Ours)}              &\textbf{82.55} &93.35 &\textbf{87.95}(\textbf{+0.71}) &\textbf{97.07} &99.27 &\textbf{98.17(+0.81)} \\ \bottomrule
\end{tabular}
\end{center}
\end{table}
 
The results from these additional experiments support the conclusions drawn in the main paper, highlighting \textbf{CABS} as a robust and effective model merging technique across various tasks and evaluation metrics.

\subsection{Additional Experiments on GPT-2-Based Models}
\label{appendix:GPT2_experiments}

we have also extended our experiments to include other architectures, specifically GPT-2-based models~\citep{radford2019language}. The results, summarized in Table~\ref{tab:gpt2_experiments}, highlight the performance of CABS and other methods on tasks derived from \textbf{FusionBench}~\citep{tang2024fusionbench}. 

\begin{table}[h]
\caption{Performance comparison on GPT-2-based models.}
\label{tab:gpt2_experiments}
\begin{center}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c}
\toprule
\textbf{Method} &\textbf{CoLA} &\textbf{MRPC} &\textbf{AVG} \\
\midrule
Fine-tuned on CoLA          &76.80 &68.40 &72.60 \\
Fine-tuned on MRPC          &30.80 &80.39 &55.60 \\
Ideal Model                 &76.80 &80.39 &78.60 \\
Task Arithmetic (Dense)     &75.55 &77.45 &76.50 (-2.10) \\
TA + DARE                   &76.70 &77.21 &76.95 (-1.65) \\
TA + Magnitude              &76.61 &79.66 &78.13 (-0.47) \\
TIES + DARE                 &77.09 &76.72 &76.91 (-1.69) \\
TIES-Merging           &76.89 &77.94 &77.42 (-1.18) \\
\textbf{CABS (Ours)}        &\textbf{76.41} &\textbf{80.88} &\textbf{78.65 (+0.05)} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The results demonstrate that \textbf{CABS} outperforms all other methods and is the only method to surpass the Ideal Model. Although the improvement margin is relatively smaller due to the upper-bound constraint imposed by the Ideal Model, CABS consistently proves its effectiveness across tasks.

Interestingly, magnitude pruning shows unexpectedly strong results on GPT-2-based models, surpassing DARE by a significant margin. This contrasts with previous experiments on other architectures, suggesting a potential architecture-specific behavior in existing pruning methods. Nevertheless, CABS maintains its advantages across different architectures, showcasing its robustness and adaptability.These findings underscore the versatility of CABS and its potential for diverse architectures.

\subsection{Detailed results of CABS on Large LMs Merging}
\label{Detailed results of CABS on Large LMs Merging}

This section provides detailed results for the experiments on large LMs merging under different sparsity levels. Table~\ref{tab:large_scale_performance25} presents the results at 25\% sparsity. CABS achieves the highest average score of 76.48 (+0.18), outperforming all baselines and closely approaching the ideal model's performance. The results demonstrate the robustness of CABS in preserving task-relevant information and mitigating performance degradation, even under moderate sparsity constraints.

Table~\ref{tab:large_scale_performance90} shows the results at a much higher sparsity level of 90\%. Despite the challenging conditions, CABS maintains competitive performance with an average score of 76.10 (-0.20), surpassing other methods, including Task Arithmetic, TA-dare, and Ties-magnitude. These results highlight the effectiveness of CABS in achieving stable and high-quality model merging, even at extreme sparsity levels.

\begin{table}[tb]
\centering
\caption{Performance comparison on LLM Leaderboard using different methods. (sparsity=0.25)}
\label{tab:large_scale_performance25}
\setlength{\tabcolsep}{0.6mm} % 调整列间距
{
\begin{tabular}{l|ccccccc}
\hline
\textbf{Method} &\textbf{ARC} &\textbf{Hella.} &\textbf{MMLU} &\textbf{TQA} &\textbf{Wino.} &\textbf{GSM8K} &\textbf{AVG} \\ \hline
WestSeverus-7B-DPO-v2   &71.30   &88.26   &63.92   &72.72   &83.69   &74.27   &75.69    \\
WildMarcoroni-Variant1-7B     &73.63   &88.67   &63.96   &70.07   &84.34   &74.48   &75.86    \\
ideal model    &73.63   &88.67   &63.96   &72.72   &84.34   &74.48   &76.30    \\ \hline
Task Arithmetic  &72.52   &89.25   &63.39   &74.00   &83.46   &73.38   &76.02(-0.28) \\
+ Magnitude    &71.67   &89.15   &63.42   &74.05   &84.37   &73.53   &76.03(-0.27) \\
+ DARE     &72.30   &88.77   &\textbf{63.84}   &72.08   &84.40   &74.40   &75.96(-0.34) \\
TIES-Merging     &72.41   &\textbf{89.34}   &63.40   &74.03   &83.64   &73.69   &76.09(-0.21) \\
+ DARE     &72.30   &88.63   &63.76   &72.16   &85.06   &74.37   &76.05(-0.25) \\ \hline
TIES-Merging + CABS     &\textbf{72.97}   &89.20   &63.46   &74.00   &\textbf{85.16}   &74.50   &76.44(+0.14) \\ 
\textbf{CABS (Ours)}    &72.75   &89.17   &63.48   &\textbf{74.08}   &84.66   &\textbf{74.73}   &\textbf{76.48(+0.18)} \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[htb]
\caption{Performance comparison on LLM Leaderboard using different methods. (sparsity=0.90)}
\label{tab:large_scale_performance90}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c}
\toprule
\multicolumn{1}{c}{\bf METHOD} &\multicolumn{1}{c}{\bf ARC} &\multicolumn{1}{c}{\bf Hella.} &\multicolumn{1}{c}{\bf MMLU} &\multicolumn{1}{c}{\bf TQA} &\multicolumn{1}{c}{\bf Wino.} &\multicolumn{1}{c}{\bf GSM8K} &\multicolumn{1}{c}{\bf AVG} \\
\midrule
Mistral-7B-v0.1                &59.98 &83.31 &64.16 &42.15 &78.37 &37.83 &60.97 \\
WestSeverus-7B-DPO-v2          &71.30 &88.26 &63.92 &72.72 &83.69 &74.27 &75.69 \\
WildMarcoroni-Variant1-7B      &73.63 &88.67 &63.96 &70.07 &84.34 &74.48 &75.86 \\
Ideal Model                    &73.63 &88.67 &63.96 &72.72 &84.34 &74.48 &76.30 \\
\midrule
Task Arithmetic (Dense)        &72.52 &89.25 &63.39 &74.00 &83.46 &73.38 &76.02 \\
TA-dare                        &70.73 &87.18 &60.15 &70.69 &82.64 &67.93 &73.22 (-3.08) \\
TA-magnitude                   &71.47 &89.01 &62.74 &73.49 &83.48 &72.43 &75.44 (-0.86) \\
Ties-dare                      &70.31 &87.12 &60.38 &70.40 &83.66 &67.93 &73.30 (-3.00) \\
Ties-magnitude                 &71.57 &88.93 &62.71 &73.49 &84.08 &73.26 &75.67 (-0.63) \\
\textbf{CABS (Ours)}           &\textbf{71.87} &\textbf{89.01} &\textbf{62.95} &\textbf{74.04} &\textbf{84.65} &\textbf{74.06} &\textbf{76.10 (-0.20)} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Effect of Different n:m Ratios at Fixed Sparsity Levels}
\label{appendix:Impact of nm ratios at fixed sparsity}

This section examines how different n:m ratios impact the performance of the merged model while keeping the overall sparsity fixed at 75\%. The results in Table \ref{tab:Effect of Different nm Ratios} indicate that while higher n:m ratios (e.g., 64:256) tend to show slight improvements, the overall impact of varying n:m ratios remains relatively subtle, suggesting that model performance is not highly sensitive to these values.

\begin{table}[ht]
\caption{Impact of different n:m ratios on CABS.(sparsity=0.75) }
\label{tab:Effect of Different nm Ratios}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c}
\toprule
\multicolumn{1}{c}{\bf METHOD} &\multicolumn{1}{c}{\bf ARC} &\multicolumn{1}{c}{\bf Hella.} &\multicolumn{1}{c}{\bf MMLU} &\multicolumn{1}{c}{\bf TQA} &\multicolumn{1}{c}{\bf Wino.} &\multicolumn{1}{c}{\bf GSM8K} &\multicolumn{1}{c}{\bf AVG}
\\ \midrule
WestSeverus-7B-DPO-v2              &71.30 &88.26 &63.92 &72.72 &83.69 &74.27 &75.69 \\
WildMarcoroni-Variant1-7B          &73.63 &88.67 &63.96 &70.07 &84.34 &74.48 &75.86 \\
Ideal Model                        &73.63 &88.67 &63.96 &72.72 &84.34 &74.48 &76.30 \\
\midrule
Task Arithmetic(Dense)             &72.52 &89.25 &63.39 &74.00 &83.46 &73.38 &76.02(-0.28) \\
CABS(16:64)                        &72.44 &89.08 &63.11 &73.38 &84.79 &\textbf{75.11} &76.32(+0.02) \\
CABS(32:128)                       &\textbf{72.92} &88.89 &\textbf{63.50} &\textbf{74.41} &84.63 &74.65 &\textbf{76.50(+0.20)} \\
CABS(64:256)                       &72.38 &\textbf{89.29} &63.15 &73.47 &\textbf{85.40} &74.65 &76.39(+0.09) \\ \bottomrule
\end{tabular}
\end{center}
\end{table}



\subsection{Additional Experiments on Performance Impact of Sparsification Sequence}
\label{appendix:Impact of sparse sequence}
We analyze how different sparse sequences, referring to the order in which source models (e.g., ``wild'' and ``west'') undergo sparsification during the merging process, affect the merged model's performance. In this context, ``wild-first'' and ``west-first'' indicate which model is sparsified first. Our findings, summarized in Table \ref{tab:Impact of Sparse Sequence}, suggest that while the order of sparsification has some impact, the effect remains relatively small. 
\begin{table}[ht]
\caption{Performance comparison across different sparse sequences on LLM Leaderboard tasks.(sparsity=0.75)}
\label{tab:Impact of Sparse Sequence}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c@{\hskip 1pt}c}
\toprule
\multicolumn{1}{c}{\bf METHOD} &\multicolumn{1}{c}{\bf ARC} &\multicolumn{1}{c}{\bf Hella.} &\multicolumn{1}{c}{\bf MMLU} &\multicolumn{1}{c}{\bf TQA} &\multicolumn{1}{c}{\bf Wino.} &\multicolumn{1}{c}{\bf GSM8K} &\multicolumn{1}{c}{\bf AVG}
\\ \midrule
WestSeverus-7B-DPO-v2              &71.30 &88.26 &63.92 &72.72 &83.69 &74.27 &75.69 \\
WildMarcoroni-Variant1-7B          &73.63 &88.67 &63.96 &70.07 &84.34 &74.48 &75.86 \\
Ideal Model                        &73.63 &88.67 &63.96 &72.72 &84.34 &74.48 &76.30 \\
\midrule
Task Arithmetic(Dense)             &72.52 &89.25 &63.39 &74.00 &83.46 &73.38 &76.02(-0.28) \\
CABS(16:64)-wild-first             &72.30 &88.87 &63.47 &74.27 &84.77 &74.12 &76.3(+0.0) \\
CABS(16:64)-west-first             &72.44 &89.08 &63.11 &73.38 &84.79 &\textbf{75.11} &76.32(+0.02) \\
CABS(32:128)-wild-first            &72.92 &88.89 &\textbf{63.50} &74.41 &84.63 &74.65 &\textbf{76.50(+0.20)} \\
CABS(32:128)-west-first            &72.58 &89.19 &63.19 &74.22 &85.16 &74.15 &76.42(+0.12) \\
CABS(64:256)-wild-first            &\textbf{72.87} &89.02 &63.43 &\textbf{74.61} &84.37 &73.92 &76.37(+0.07) \\
CABS(64:256)-west-first            &72.38 &\textbf{89.29} &63.15 &73.47 &\textbf{85.40} &74.65 &76.39(+0.09) \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Rescale Experiments}
\label{section-rescale_experiments}

In previous research, TIES utilized magnitude pruning to reduce conflicts during task vector merging but did not include a rescale step. Subsequent work on DARE introduced a two-step process: random pruning followed by rescaling with a factor of \( \frac{1}{1-p} \), where \( p \) is the sparsity rate. DARE demonstrated that random pruning, when combined with rescaling, could restore performance to levels comparable to the original fine-tuned models. However, DARE did not explore the effect of rescaling on magnitude-pruned task vectors.

In our experiments, we evaluated the impact of rescaling on both magnitude-based and random pruning methods across different sparsity levels. As shown in Figure \ref{fig:rescale_performance}, rescaling allows magnitude-pruned task vectors to recover performance similar to that achieved by DARE, suggesting that rescaling is a crucial step for maintaining model performance post-pruning.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/rescale.pdf}
    \caption{Impact of rescaling on different pruning methods across various sparsity levels. Performance is evaluated on RTE and MRPC tasks using RoBERTa. The horizontal axis represents the sparsity ratio, while the vertical axis indicates the performance of the task vectors after rescaling.}
    \label{fig:rescale_performance}
    \vspace{-1em}
\end{figure}

These findings confirm that, with appropriate rescaling, both magnitude-based and random pruning methods can achieve near-original performance. This insight complements the primary contributions of our work by showing that magnitude pruning, which traditionally underperformed compared to random pruning in TIES, can be equally effective when combined with rescaling. Although this experiment supports the robustness of magnitude pruning under rescale conditions, it is not the main focus of our study and is therefore detailed here in the appendix.

\subsection{Impact of Lambda on Performance}
Figure \ref{fig:lambda} provides the average performance as a function of $\lambda$. It can be observed that within a certain range, the performance is relatively insensitive to variations in $\lambda$. This result corresponds to the performance of the CABS framework on the RTE-MRPC task. For visualization purposes, the same $\lambda$ values were used across the tasks rather than the task-specific $\lambda$ values reported in the paper. The $\lambda$ values range from 1 to 3, with a step size of 0.01, resulting in a total of 200 samples.

\begin{figure}[bht]
\vspace{-1em}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/lambda.pdf}
    \caption{Average performance vs.lambda}
    \label{fig:lambda}
\end{figure}

\subsection{Multilingual Applicability of CABS}
\label{appendix:multilingual_applicability}

While our primary experiments focused on English tasks to maintain comparability with prior work, we extended our evaluation to include two Korean language tasks, \textbf{kobest\_copa} and \textbf{kobest\_boolq}~\citep{jang2022kobest}, to investigate the multilingual applicability of our method. These additional experiments provide insight into the performance of CABS across diverse linguistic contexts. The results are summarized in Table~\ref{tab:multilingual_results}.

\begin{table}[bht]
\vspace{-1em}
\caption{Performance comparison on multilingual tasks, including Korean language benchmarks.}
\label{tab:multilingual_results}
\begin{center}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c}
\toprule
\textbf{Model} &\textbf{ARC} &\textbf{Hella.} &\textbf{MMLU} &\textbf{TQA} &\textbf{Wino.} &\textbf{GSM8K} &\textbf{Kcopa} &\textbf{Kboolq} &\textbf{Avg} \\
\midrule
Mistral-7B-v0.1                &59.98 &83.31 &64.16 &42.15 &78.37 &37.83 &59.00 &62.61 &60.93 \\
WestSeverus          &71.30 &88.26 &63.92 &72.72 &83.69 &74.27 &63.30 &81.91 &74.92 \\
WildMarcoroni      &73.63 &88.67 &63.96 &70.07 &84.34 &74.48 &64.80 &82.08 &75.25 \\
Ideal Model                    &73.63 &88.67 &63.96 &72.72 &84.34 &74.48 &64.80 &82.08 &75.59 \\
\midrule
TA (Dense)        &72.52 &89.25 &63.39 &74.00 &83.46 &73.38 &65.60 &72.58 &74.27 (-1.32) \\
TA + Magnitude    &71.93 &89.32 &63.18 &73.85 &84.12 &72.22 &64.70 &72.86 &74.02 (-1.57) \\
TA + DARE         &72.64 &88.86 &64.53 &72.82 &84.03 &73.44 &61.40 &79.34 &74.63 (-0.96) \\
TIES-Merging                   &71.42 &89.17 &63.16 &73.82 &84.74 &73.01 &64.80 &73.08 &74.15 (-1.44) \\
TIES + DARE            &71.87 &88.95 &63.56 &72.87 &84.61 &73.21 &61.40 &79.63 &74.51 (-1.08) \\
\textbf{CABS (Ours)}           &\textbf{72.92} &\textbf{88.89} &\textbf{63.50} &\textbf{74.41} &\textbf{84.63} &\textbf{74.65} &\textbf{65.10} &\textbf{79.20} &\textbf{75.41 (-0.18)} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1em}
\end{table}

For these experiments, we reused the merging configuration from our previous 7B experiments to ensure consistency across evaluations and to reduce computational overhead during this phase. CABS achieves an average score of \textbf{75.41}, closely matching the ideal model’s performance of \textbf{75.59} (a difference of -0.18). In comparison, the best alternative, Task Arithmetic + DARE, achieves \textbf{74.63} (-0.96), with other methods falling even further behind. These results confirm that CABS delivers competitive performance across both English and non-English tasks.

Additionally, these findings underscore the robustness of CABS in maintaining performance across multilingual benchmarks, highlighting its potential applicability to a wide range of languages and tasks. While the absolute improvement margins may vary due to upper-bound constraints imposed by the ideal model, CABS consistently demonstrates its effectiveness and adaptability across diverse settings.

\subsection{Model soups experimental results}
\label{appendix:soup}

\textbf{Merging Checkpoints of the Same Task for Better Robustness.} As shown in Table~\ref{tab:sst2_performance}, merging checkpoints fine-tuned on the same task improves performance, with CABS achieving the highest SST-2 accuracy of 0.9472, surpassing other methods by a notable margin (+1.49). These two checkpoints were fine-tuned for one epoch using Adam and AdamW optimizers, respectively, with a learning rate of $3 \times 10^{-5}$. The original training set was split 9:1 into a new training set and a validation set, with the validation set used as the test set. This result demonstrates the effectiveness of CABS in maintaining robustness and resolving conflicts during checkpoint merging.

\begin{table}[tb]
\caption{Model soups experimental setup. CABS improves performance when merging checkpoints on the same tasks.}
\label{tab:sst2_performance}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{l|c}
\hline
\textbf{Method} &\textbf{SST-2 Accuracy} \\ \hline
Fine-tuned model1 &0.9323 \\
Fine-tuned model2 &0.9289 \\ \hline
Task Arithmetic &0.9381 (+0.58) \\ 
+Magnitude &0.9381 (+0.58) \\
+DARE &0.9346 (+0.23) \\
TIES-Merging &0.9404 (+0.81) \\ 
+DARE &0.9358 (+0.35) \\
CABS(Ours) &\textbf{0.9472 (+1.49)} \\ \hline
\end{tabular}
\end{small}
\end{center}
\end{table}

\subsection{Effect of Learning Rate on Overlap Degree}

We conducted additional experiments to study the effect of learning rate on the parameter overlap degree under magnitude pruning with 90\% sparsity. Specifically, we fine-tuned the model using learning rates from the set $\{1\text{e-6}, 3\text{e-6}, 5\text{e-6}, 1\text{e-5}, 3\text{e-5}, 5\text{e-5}\}$ with both Adam and AdamW optimizers. After pruning, the parameter overlap degree was calculated to analyze the relationship between learning rate and parameter overlap.

Our observations, illustrated in Figure~\ref{fig:lr_overlap}, show that lower learning rates lead to a higher overlap degree among parameters. This indicates that fine-tuning at lower learning rates tends to preserve shared information across tasks, even under extreme sparsity conditions. Conversely, higher learning rates result in less overlap, likely due to more significant parameter updates during optimization. 

\begin{figure}[bt]
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/overlap_rate_vs_learning_rate_with_labels.pdf}
    \caption{The relationship between learning rate and parameter overlap degree under magnitude pruning with 90\% sparsity. Lower learning rates result in higher overlap.}
    \label{fig:lr_overlap}
\end{figure}

\section{Detailed Experimental Settings}
\subsection{Overlap Rate Calculation}
\label{appendix:overlap_rate_calculation}

The overlap rate between two task vectors is a metric used to quantify the extent to which the same parameters are retained after pruning. This metric is particularly useful in understanding how pruning strategies impact the sharing of model parameters across different tasks, which can lead to conflicts during model merging.

The overlap rate is calculated as follows: Given two task vectors $\tau_A$ and $\tau_B$, the overlap rate is defined as the ratio of the number of shared non-zero parameters to the total number of non-zero parameters in the first task vector $\tau_A$. Mathematically, this can be expressed as:
\[
\text{Overlap Rate} = \frac{|\tau_A \cap \tau_B|}{|\tau_A|}
\]
where $|\tau_A \cap \tau_B|$ represents the count of non-zero parameters that are common to both vectors $\tau_A$ and $\tau_B$, and $|\tau_A|$ denotes the total count of non-zero parameters in vector $\tau_A$. This calculation shows the extent of overlap between two task vectors. A higher overlap rate means more shared parameters, increasing the potential for conflicts during model merging.

\subsection{Weight Distribution Analysis Across Layers and Sparsity Ratios}
\label{appendix:weight_distribution_analysis}

This section provides a comprehensive analysis of the heatmaps illustrating weight distributions across different layers of the model and various sparsity ratios. Figures \ref{fig:kv_projection_weight_distribution_25}-\ref{fig:mlp_layer_weight_distribution_90} 
 show the weight distribution for four representative layers: \texttt{self\_attn.k\_proj.weight} (layer 6), \texttt{self\_attn.q\_proj.weight} (layer 12), \texttt{self\_attn.v\_proj.weight} (layer 24), and \texttt{mlp.up\_proj.weight} (layer 18) at sparsity ratios of 25\%, 50\%, 75\%, and 90\%.

These heatmaps demonstrate how increasing sparsity causes magnitude-based pruning to concentrate weights in localized regions of the parameter space. As the sparsity level increases, this clustering becomes more pronounced, especially at 75\% and 90\% sparsity levels, leading to potential imbalances that can degrade model performance.

The recurring pattern across all layers further highlights the significance of strategies like Balanced Sparsification (BS), which aim to distribute weights more evenly across the model. By ensuring a more uniform distribution of the retained weights, BS helps to maintain model stability and performance after sparsification.

\begin{figure}[bht]  
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/layers6.pdf}
    \caption{Heatmaps of weight distribution in model.layers.6.self\_attn.k\_proj.weight across different sparsity ratios (25\%, 50\%, 75\%, and 90\%).}
    \label{fig:kv_projection_weight_distribution_25}
\end{figure}

\begin{figure}[bhtp]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/layers12.pdf}
    \caption{Heatmaps of weight distribution in model.layers.12.self\_attn.q\_proj.weight across different sparsity ratios (25\%, 50\%, 75\%, and 90\%).}
    \label{fig:kv_projection_weight_distribution_50}
\end{figure}

\begin{figure}[bhtp]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/layers24.pdf}
    \caption{Heatmaps of weight distribution in model.layers.18.mlp.up\_proj.weight across different sparsity ratios (25\%, 50\%, 75\%, and 90\%).}
    \label{fig:mlp_layer_weight_distribution_90}
\end{figure}

\begin{figure}[bhtp]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/layers18.pdf}
    \caption{Heatmaps of weight distribution in model.layers.24.self\_attn.v\_proj.weight across different sparsity ratios (25\%, 50\%, 75\%, and 90\%).}
    \label{fig:kv_projection_weight_distribution_75}
\end{figure}

\subsection{Algorithm of CABS}
\label{Algorithm of CABS}

\begin{algorithm}[htbp]
\caption{CABS}
\label{algo:cabs_sparsity1}
\begin{algorithmic}[1]
    \REQUIRE Task vectors \( \tau_A , \tau_B \), base model \( W_{\text{base}} \), sparsity level \( n \) , \( m \), scaling coefficients \( \lambda_A \) , \(\lambda_B \)%replace ''rescale factors`` with ''merge factors``
    \ENSURE Parameters of the merged model \( W_{\text{final}} \)
    
    \STATE Apply n:m pruning to \( \tau_A \) and compute \( \text{mask}_A \) \\ \hfill \# include BS
    \STATE \( \tau_{\text{B remaining}} = \tau_B \odot (1 - \text{mask}_A) \) to eliminate overlap with \( \tau_A \) \hfill \# core step of CA
    \STATE Apply n:m pruning to \( \tau_{\text{B remaining}} \) to compute \( \text{mask}_B \) \\ \hfill \# include BS 
    \STATE Merge the pruned vectors with the base model: 
    \vspace{-0.7em}\[W_{\text{final}} = W_{\text{base}} + \lambda_A \times \text{mask}_A \odot \tau_A + \lambda_B \times \text{mask}_B \odot \tau_B\]\vspace{-1.9em}
    \STATE Return \( W_{\text{final}} \)
\end{algorithmic}
\end{algorithm}
\vspace{-0.15in}

\begin{algorithm}[tbbh]
\caption{CABS Implementation:minimize overlap rate}
\label{algo:low_overlap_sparsity}
\begin{algorithmic}[1]
    \REQUIRE Task vectors \( \tau_A , \tau_B \), base model \( W_{\text{base}} \), sparsity level \( n \) , \( m \), scaling coefficients \( \lambda_A \) , \(\lambda_B \)
    \ENSURE Merged model parameters \( W_{\text{final}} \)
    
    \STATE Apply n:m pruning to \( \tau_A \) and compute \( \text{mask}_A \) \hfill // include BS
    
    \STATE Compute \( \text{initial\_mask}_B = 1 - \text{mask}_A \), retaining non-overlapping regions of \( \tau_B \)
    
    \STATE If \( \text{initial\_mask}_B \) retains less than \( n\div m \) of weights, update \( \text{mask}_B \) by including additional weights from the overlapping region \( \text{mask}_A \odot \tau_B \) until the target sparsity \( n\div m \) is reached
    
    \STATE Merge the pruned vectors with the base model: 
    \[
    W_{\text{final}} = W_{\text{base}} + \lambda_A \times \text{mask}_A \odot \tau_A + \lambda_B \times \text{mask}_B \odot \tau_B
    \] 
    \STATE Return \( W_{\text{final}} \)
\end{algorithmic}
\end{algorithm}

In this section, we present the detailed steps for both the CABS sparsity algorithm and the Low-Overlap Sparsity approach. Algorithm \ref{algo:cabs_sparsity} outlines the process behind CABS, Algorithm~\ref{algo:low_overlap_sparsity} provide the detailed algorithm for Low-Overlap Sparsity designed to minimize direct conflicts during the model merging process. The algorithm sequentially applies sparsification to task vectors, ensuring that the non-overlapping portions of the task vectors are prioritized, thereby reducing overlap and conflict between different task vectors in the final merged model.

\subsection{Comparison of n:m pruning and BS}
\label{Comparison of n:m Sparsity and BS}
Although both n:m pruning and BS employ the same operation—selecting the top \( n \) values out of \( m \) consecutive weights based on magnitude—their goals and use cases differ:

- \textit{Goal}: The primary goal of n:m pruning is to achieve model compression and acceleration by reducing computational and memory costs. In contrast, BS is designed to maintain a balanced distribution of task vectors while minimizing conflicts between them during merging, not to merely discard unimportant weights.

- \textit{Result}: n:m pruning is typically used for structured pruning in models, aiming to reduce inference time and memory usage. On the other hand, BS is applied specifically to task vectors. After the task vectors are merged with a base model, the resulting model remains dense, meaning that the practical computation and memory savings are not realized, but the model gains improved capacity.

- \textit{Sparsity Ratios}: n:m pruning often uses configurations like 2:4 or 4:8, where the sparsity level is generally around 50\%. In contrast, the sparsification of task vectors under BS can involve much higher sparsity levels, as can be seen in Table \ref{tab:Effect of Different nm Ratios} (Appendix \ref{appendix:Impact of nm ratios at fixed sparsity}), with configurations such as 64:256 at 75\% sparsity.

- \textit{Effectiveness}: Typically, n:m pruning yields lower performance compared to magnitude pruning in compression tasks, as the more strict uniform distribution of sparsity across blocks (e.g., every 4 weights) tends to hurt performance. However, in model merging, n:m sparsity can outperform row-wise or layer-wise magnitude pruning due to its more balanced distribution.

\subsection{Computational Overhead Analysis}
\label{appendix:computational_overhead_analysis}

This section provides a detailed analysis of the computational complexity of the CABS framework, focusing on its core components: Balanced Sparsification (BS) and Conflict-Aware (CA) pruning strategies, as well as the scalability and parallelization potential.

\textbf{Balanced Sparsification (BS)} operates efficiently by dividing each layer's parameters into small, fixed-size blocks of \(m\) parameters. Within each block, the top \(n\) weights are selected based on magnitude, requiring a localized sorting operation with complexity \(O(m \log m)\) per block. For a layer with \(N/m\) blocks, the total complexity per task vector is \(O(N \log m)\), significantly more efficient than global magnitude pruning with a complexity of \(O(N \log N)\). When merging \(k\) task vectors, the total complexity becomes \(O(kN \log m)\), making BS highly scalable for large-scale model merging.

\textbf{Conflict-Aware Sparsification (CA)} introduces minimal computational overhead by sequentially applying a mask inversion and element-wise product to ensure non-overlapping pruned regions across task vectors. These operations align with standard sparsification frameworks and maintain the same order of complexity, adding negligible cost compared to traditional methods. Combined with BS, the CA strategy ensures robust conflict resolution while maintaining computational efficiency.

\textbf{Scalability and Parallelization.} The complexity of CABS scales linearly with the number of task vectors (\(k\)), ensuring \(O(kN \log m)\) efficiency for BS. Additionally, the block-based pruning operations in BS and the sequential processing in CA are inherently parallelizable, allowing task vector processing to occur independently across layers or blocks. This parallelization potential leverages modern hardware architectures, enabling efficient execution even for large-scale models. Without full parallelization, CABS still remains computationally efficient for real-world applications.

\textbf{Comparison and Conclusion.} Compared to traditional global magnitude pruning (\(O(N \log N)\)), the block-based sorting in BS (\(O(N \log m)\)) provides substantial computational savings. CA introduces negligible overhead, ensuring efficient and robust merging across multiple task vectors. Overall, with efficient scaling and inherent parallelization, CABS maintains a low computational overhead while effectively resolving task conflicts and ensuring balanced weight distribution, making it suitable for both small- and large-scale models.

\subsection{Memory Overhead Analysis}
\label{appendix:memory_overhead}

This section analyzes the memory overhead of CABS during the merging process and compares it to existing methods such as DARE and TIES-Merging.

\textbf{Memory Overhead of CABS.}  
During the merging process, CABS requires memory for storing the model parameters and two additional boolean-like masks: one to track weight usage and another to record pruning results. For a model with \(N\) parameters, the memory overhead of these masks is \(O(2 \cdot N \cdot 0.125\ \text{bytes})\), which is negligible compared to the memory required for storing the model parameters themselves (\(O(N \cdot 2\ \text{bytes})\)). As a result, the peak memory usage of CABS during the merging phase is comparable to other methods and remains efficient for large-scale models.

\textbf{Comparison with Other Methods.}  
DARE requires loading both source models into memory during the merging process. With lazy loading, the peak memory usage is \(O(2 \cdot N \cdot 2\ \text{bytes})\), where \(N\) is the number of parameters in a model. TIES-Merging, on the other hand, requires memory for all task vectors simultaneously during its election phase, resulting in \(O(k \cdot N \cdot 2\ \text{bytes})\), where \(k\) is the number of task vectors. However, with lazy loading, TIES-Merging can reduce its memory usage to \(O(2 \cdot N \cdot 2\ \text{bytes})\), matching that of DARE. CABS achieves a similar peak memory usage as DARE and TIES-Merging with lazy loading, as the additional memory required for the two boolean masks is negligible compared to the memory needed for model parameters. This makes CABS as memory-efficient as other existing methods while offering additional robustness and performance benefits.

\textbf{Conclusion.}  
CABS introduces minimal additional memory overhead, as the boolean masks required for Balanced Sparsification are lightweight compared to the model parameters. Furthermore, the merging process is typically performed on CPUs, where memory constraints are less critical than on GPUs. In practice, no memory bottlenecks have been observed during experiments, confirming that CABS is memory-efficient and scalable for merging large-scale models.

\subsection{Details of Datasets and Models for LLMs}
\label{datasets-backbones-decoder}

\textbf{Datasets:} Our evaluation framework comprises two benchmark suites that collectively assess a broad spectrum of language understanding, reasoning, and problem-solving capabilities.

\textbf{(1) Open LLM Leaderboard Benchmark:}
\begin{itemize}
    \item \textbf{AI2 Reasoning Challenge}: A set of grade-school science questions designed to test fundamental reasoning skills.
    \item \textbf{HellaSwag}: A commonsense inference task that poses challenges for state-of-the-art models while remaining straightforward for humans (with human accuracy around 95\%).
    \item \textbf{MMLU}: A multitask evaluation covering 57 subjects—including elementary mathematics, US history, computer science, and law—to gauge broad-domain knowledge.
    \item \textbf{TruthfulQA}: A benchmark that measures a model’s tendency to avoid reproducing widely circulated falsehoods.
    \item \textbf{Winogrande}: An adversarial task based on Winograd schemas, which tests nuanced commonsense reasoning.
    \item \textbf{GSM8K}: A collection of grade-school math word problems that require multi-step mathematical reasoning.
\end{itemize}

\textbf{(2) Open LLM Leaderboard 2 Benchmark:}
\begin{itemize}
    \item \textbf{IFEval}: Designed to evaluate inference capabilities across complex, varied scenarios.
    \item \textbf{BBH}: A subset of BIG-Bench hard tasks that challenges models with problems requiring deep reasoning.
    \item \textbf{MATH}: A dataset comprising challenging mathematical problems that demand multi-step, non-trivial problem solving.
    \item \textbf{GPQA}: A general-purpose question-answering benchmark that spans a diverse range of topics.
    \item \textbf{MUSR}: Focused on assessing multi-step reasoning in intricate contexts.
    \item \textbf{MMLU-PRO}: An advanced variant of MMLU that emphasizes professional and specialized domain knowledge.
\end{itemize}

\textbf{Models:} We evaluated two families of models corresponding to the two benchmark suites.

\textbf{(1) Open LLM Leaderboard Models:} These models are built on the \texttt{Mistral-7b-v0.1}\footnote{\url{https://huggingface.co/mistral-7b-v0.1}} backbone and include the following fine-tuned variants:
\begin{itemize}
    \item \texttt{WildMarcoroni-Variant1-7B}\footnote{\url{https://huggingface.co/WildMarcoroni-Variant1-7B}}
    \item \texttt{WestSeverus-7B-DPO-v2}\footnote{\url{https://huggingface.co/WestSeverus-7B-DPO-v2}}
\end{itemize}

\textbf{(2) Open Leaderboard 2 Models:} For the new benchmark suite, we use \texttt{Qwen/Qwen2.5-7B-Instruct}\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}} as the base model, and include the following fine-tuned variants:
\begin{itemize}
    \item \texttt{ehristoforu/fq2.5-7b-it-normalize\_false}\footnote{\url{https://huggingface.co/ehristoforu/fq2.5-7b-it-normalize_false}}
    \item \texttt{Tsunami-th/Tsunami-0.5-7B-Instruct}\footnote{\url{https://huggingface.co/Tsunami-th/Tsunami-0.5-7B-Instruct}}
\end{itemize}

These models were selected for their robust performance across the diverse tasks and their proven utility in prior research. 

\subsection{Details of Datasets and Models for Small LMs}
\label{datasets-backbones-encoder}

\textbf{Tasks}
The GLUE benchmark includes a variety of tasks designed to evaluate different aspects of natural language understanding. For our experiments, we selected the following four tasks:
\begin{itemize}
\item CoLA (Corpus of Linguistic Acceptability), which evaluates the grammatical acceptability of sentences with performance measured using the Matthews Correlation Coefficient (MCC); 
\item SST-2 (Stanford Sentiment Treebank), a binary sentiment classification task assessing whether a sentence expresses a positive or negative sentiment, evaluated using accuracy;
\item MRPC (Microsoft Research Paraphrase Corpus), a paraphrase identification task where models predict whether two sentences have the same meaning, evaluated using both accuracy and F1 score; 
\item RTE (Recognizing Textual Entailment), a natural language inference task where models determine whether a hypothesis is true based on a given premise, evaluated using accuracy.
\item SQuAD (Stanford Question Answering Dataset): A question-answering task that evaluates models on their ability to extract precise spans of text that answer questions from a given context, measured using F1 and exact match (EM) scores.
\item RACE (ReAding Comprehension from Examinations): A dataset for evaluating reading comprehension by requiring models to answer multiple-choice questions based on given passages. The dataset includes diverse linguistic phenomena, with performance measured using accuracy.
\end{itemize}

\textbf{Models} For each task, we utilized pre-trained and fine-tuned versions of RoBERTa, obtained from Hugging Face. Specifically, we used FacebookAI/roberta-base\footnote{\url{https://huggingface.co/FacebookAI/roberta-base}} as base model. 
textattack/roberta-base-CoLA\footnote{\url{https://huggingface.co/textattack/roberta-base-CoLA}}, textattack/roberta-base-SST-2\footnote{\url{https://huggingface.co/textattack/roberta-base-SST-2}}, textattack/roberta-base-MRPC\footnote{\url{https://huggingface.co/textattack/roberta-base-MRPC}}, textattack/roberta-base-RTE\footnote{\url{https://huggingface.co/textattack/roberta-base-RTE}}, Riiid/kda-roberta-base-race\footnote{\url{https://huggingface.co/Riiid/kda-roberta-base-race}} and deepset/roberta-base-squad2\footnote{\url{https://huggingface.co/deepset/roberta-base-squad2}}.
we also use pre-trained and fine-tuned versions of GPT-2, obtained from Hugging Face for additional experiments. Specifically, we used openai-community/gpt2\footnote{\url{https://huggingface.co/openai-community/gpt2}} as the base model, tanganke/gpt2-cola\footnote{\url{https://huggingface.co/tanganke/gpt2_cola}} and tanganke/gpt2-mrpc\footnote{\url{https://huggingface.co/tanganke/gpt2_mrpc}}.
 
\subsection{Evaluation Metrics}
\label{appendix:evaluation_metrics}

For GLUE tasks, accuracy was chosen as the uniform metric to facilitate fair comparison across tasks. While MCC is recommended for CoLA, we used accuracy to maintain consistency with other tasks. MCC typically reaches around 0.64 after fine-tuning for CoLA, whereas accuracy for other tasks often exceeds 0.9. This discrepancy makes it difficult to include MCC in an overall performance average.

For LLM Leaderboard tasks, the following metrics were used:
\begin{itemize}
    \item \textbf{ARC}: Success rate (25-shot)
    \item \textbf{HellaSwag}: Accuracy (10-shot)
    \item \textbf{MMLU and Winogrande}: Accuracy (5-shot)
    \item \textbf{TruthfulQA}: Factual accuracy (0-shot)
    \item \textbf{GSM8K}: Success rate (5-shot)
\end{itemize}
These metrics provide a consistent and comparable basis for evaluating model performance across various benchmarks.

\subsection{Grid Search Details}
\label{appendix:grid_search_model_details}

For small-scale tasks, we performed a fine-grained $\lambda$ parameter search with an interval of 0.01 (compared to 0.1 used in previous works) to ensure fair comparisons between methods. In contrast, because of the high computational cost of large-scale experiments (e.g., with 7B models), we followed prior work by adopting a coarser grid interval of 0.1, with equal $\lambda$ values for all vectors. The impact of lambda grid intervals is discussed in Appendix \ref{appendix:Impact of lambdas search}, showing how coarser intervals may lead to unfair comparisons by missing optimal values. 

In our small-scale experiments, we employed a two-step grid search strategy to determine the optimal scaling coefficients $\lambda$ that maximizes average performance across multiple tasks.

\textbf{Grid Search Strategy}
As the sparsity level increases, the range of potential optimal $\lambda$ values broadens, and performance typically follows a pattern of increasing and then decreasing with respect to $\lambda$. To address this, we adopted a two-step adaptive search strategy. First, a manual search with a 0.1 interval was performed to identify the broader region where the optimal $\lambda$ is likely to reside. Based on the results of this initial search, a more fine-grained search using a 0.01 interval was conducted, focusing on the identified region. 

To further evaluate the method's ability to merge multiple task vectors (\(k > 3\)), additional experiments were conducted by merging four models at 90\% sparsity. In these experiments, a unified $\lambda$ value was used across all task vectors, with a search interval of 0.01. This unified approach simplifies the process and mitigates the computational burden of searching for optimal $\lambda$ combinations, which would otherwise grow exponentially with the number of models \(k\).

Unlike a fixed-range search, this adaptive strategy allowed us to efficiently identify the most effective scaling coefficients for each sparsity level, ensuring precise performance optimization. The performance values presented in the main text correspond to the optimal $\lambda$ values found through this two-step process.

\subsection{Guidelines and Experimental $\lambda$ Values}
\label{appendix:lambda values}

This section describes the guidelines for setting $\lambda$ values and presents experimental results using a unified $\lambda$ across various sparsity levels for large-scale models and across different numbers of tasks for small-scale models.

\textbf{Guidelines for Setting $\lambda$:}
\begin{itemize}
    \item \textbf{Small-Scale Models}: A fine-grained grid search with an interval of 0.01 was used to ensure fair comparisons and avoid missing optimal values.
    \item \textbf{Large-Scale Models (e.g., 7B Models)}: A coarser grid search with an interval of 0.1 was adopted to reduce computational costs, consistent with prior work.
\end{itemize}

\begin{table}[h]
\centering
\caption{Unified $\lambda$ values for large-scale models at different sparsity levels.}
\label{tab:lambda_large}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|cccccc}
\hline
\multicolumn{1}{c|}{Sparsity Level} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Task-\\Arithmetic\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TA-\\Magnitude\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TA-\\DARE\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TIES-\\Merging\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TIES-\\DARE\end{tabular}} &
\multicolumn{1}{c}{CABS} \\ \hline
0    &0.6   &-     &-     &-     &-     &-     \\ 
0.25 &-     &0.6   &0.8*  &0.6   &0.8*  &0.6   \\ 
0.75 &-     &0.8   &2.2*  &0.8   &2.2*  &1.2   \\ 
0.90 &-     &1.2   &5.5*  &1.2   &5.5*  &1.8   \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Unified $\lambda$ values for small-scale models at different task numbers.}
\label{tab:lambda_small}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l|cccccc}
\hline
\multicolumn{1}{c|}{Task Number} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Task-\\Arithmetic\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TA-\\Magnitude\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TA-\\DARE\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TIES-\\Merging\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}TIES-\\DARE\end{tabular}} &
\multicolumn{1}{c}{CABS} \\ \hline
4 &0.48 &4.61* &1.07 &1.88 &5.72* &1.74 \\ 
6 &0.49 &5.61* &1.04 &1.88 &5.41* &1.64 \\ \hline
\end{tabular}
\end{table}

\textbf{Notes:}  
For DARE-relate method, the reported $\lambda$ values (e.g., $\lambda = 2.2$ for 0.75 sparsity and $\lambda = 5.61$ for 0.90 sparsity) correspond to task vectors that have already been rescaled by a sparsity-adjusted factor (e.g., $(1/(1-\text{sparsity}))$). However, directly using these rescaled task vectors for model merging without adjusting $\lambda$ effectively increases the step size of the $\lambda$ grid search. This results in a coarser optimization for DARE, making the comparison less fair. To address this, we ensured that the DARE method underwent a finer-grained $\lambda$ search to account for this implicit difference in grid interval and to enable a more equitable comparison with other methods.

\subsection{Hardware and Hyperparameter Configurations for Model Evaluation.}
\label{appendix:implementation details}

The model evaluations were performed on A100-40GB GPUs.
For small-scale and discriminative tasks in GLUE, we conducted a single evaluation per model, as minimal variance was observed across repeated runs. In contrast, for generative tasks involving large models, where results can be more variable, inference was implemented via the lm-evaluation-harness v0.4.0. To ensure consistency and robustness, we performed three evaluations and reported the average outcome. As for the hyperparameters of generative LMs, we set the maximum generation token limit to 256, the temperature to 1.0 for sampling, and the maximum context length to 2048 tokens. 

\subsection{Limitations and Future Work}
\label{Limitations and Future Works}

\textbf{General Limitations.} 
Like other task vector-based methods, our approach is limited to models with identical architectures due to the element-wise operations used in merging model weights. This constraint restricts the generalization of the framework to models with homogeneous structures. Furthermore, reliance on manual adjustment of the parameter \(\lambda\) remains a common challenge, especially for large-language models, which requires trial and error to optimize model performance.

\textbf{Limitations Specific to CABS.}
CABS introduces two new hyperparameters—the sparse sequence and the n:m ratios—unique to its design, as discussed in Appendix \ref{appendix:Impact of sparse sequence} and \ref{appendix:Impact of nm ratios at fixed sparsity}. While these hyperparameters were not particularly sensitive in our experiments, they add complexity and increase computational cost. 

\textbf{Future Work.}
Several directions could help overcome these limitations. Expanding model merging techniques to include heterogeneous architectures or models trained from scratch represents a key area for future research. Additionally, improving the performance of merged models in multi-task settings—where current approaches do not yet match the performance of original single-task models—remains a priority. Automating the search for optimal hyperparameters, particularly \(\lambda\), would reduce complexity and improve usability, especially in large-scale applications.