@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@article{wang2024towards_universality,
  title   = {Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures},
  author  = {Junxuan Wang and Xuyang Ge and Wentao Shu and Qiong Tang and Yunhua Zhou and Zhengfu He and Xipeng Qiu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.06672}
}

@inproceedings{balagansky2024mechanistic_permutability,
  title  = {Mechanistic Permutability: Match Features Across Layers},
  author = {Nikita Balagansky and Ian Maksimov and Daniil Gavrilov},
  year   = {2024},
  url    = {https://openreview.net/forum?id=MDvecs7EvO},
  booktitle = {openreview}
}

@article{balcells2024evolution,
  title   = {Evolution of SAE Features Across Layers in LLMs},
  author  = {Daniel Balcells and Benjamin Lerner and Michael Oesterle and Ediz Ucar and Stefan Heimersheim},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.08869}
}

@inproceedings{conmy2023automated,
      title={Towards Automated Circuit Discovery for Mechanistic Interpretability}, 
      author={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adri{\`a} Garriga-Alonso},
      booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
      year={2023},
      eprint={2304.14997},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{ge2024automatically,
  title   = {Automatically Identifying Local and Global Circuits with Linear Computation Graphs},
  author  = {Xuyang Ge and Fukang Zhu and Wentao Shu and Junxuan Wang and Zhengfu He and Xipeng Qiu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2405.13868}
}



@article{bussmann2024batchtopk,
  title   = {BatchTopK Sparse Autoencoders},
  author  = {Bart Bussmann and Patrick Leask and Neel Nanda},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.06410}
}

@article{he2024llamascope,
  title   = {Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders},
  author  = {Zhengfu He and Wentao Shu and Xuyang Ge and Lingjie Chen and Junxuan Wang and Yunhua Zhou and Frances Liu and Qipeng Guo and Xuanjing Huang and Zuxuan Wu and Yu-Gang Jiang and Xipeng Qiu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.20526}
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{jermyn2024op,
       title={Random Open Problems},
       author={Adam Jermyn and Joshua Batson and Chris Olah},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/jan-update/index.html#open-problems}
}

@article{lieberum2024gemmascope,
  title     = {Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},
  author    = {Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and J'anos Kram'ar and Anca Dragan and Rohin Shah and Neel Nanda},
  journal   = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year      = {2024},
  doi       = {10.48550/arXiv.2408.05147},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/890efc891e9b59e8cb5e8c244428f6b81ec0a4da}
}

@article{rajamanoharan2024jumping,
  title   = {Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders},
  author  = {Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and János Kramár and Neel Nanda},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.14435}
}

@article{dunefsky2024transcoders,
  title   = {Transcoders Find Interpretable LLM Feature Circuits},
  author  = {Jacob Dunefsky and Philippe Chlenski and Neel Nanda},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2406.11944}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@misc{balcells2024evolutionsaefeatureslayers,
      title={Evolution of SAE Features Across Layers in LLMs}, 
      author={Daniel Balcells and Benjamin Lerner and Michael Oesterle and Ediz Ucar and Stefan Heimersheim},
      year={2024},
      eprint={2410.08869},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08869}, 
}

@misc{eldan2023tinystoriessmalllanguagemodels,
      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
      author={Ronen Eldan and Yuanzhi Li},
      year={2023},
      eprint={2305.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.07759}, 
}

@misc{zhang2024autonomousdataselectionlanguage,
      title={Autonomous Data Selection with Language Models for Mathematical Texts}, 
      author={Yifan Zhang and Yifan Luo and Yang Yuan and Andrew Chi-Chih Yao},
      year={2024},
      eprint={2402.07625},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.07625}, 
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={{Gemma Team}},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{chalnev2024improvingsteeringvectorstargeting,
      title={Improving Steering Vectors by Targeting Sparse Autoencoder Features}, 
      author={Sviatoslav Chalnev and Matthew Siu and Arthur Conmy},
      year={2024},
      eprint={2411.02193},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.02193}, 
}

@misc{makhzani2014ksparseautoencoders,
      title={k-Sparse Autoencoders}, 
      author={Alireza Makhzani and Brendan Frey},
      year={2014},
      eprint={1312.5663},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5663}, 
}

@misc{lindsey2024crosscoders,
    title={Sparse Crosscoders for Cross-Layer Features and Model Diffing},
    author={Jack Lindsey and Adly Templeton and Jonathan Marcus and Thomas Conerly and Joshua Batson and Christopher Olah},
    year={2024},
    url={https://transformer-circuits.pub/2024/crosscoders/index.html},
}

@article{goh2021multimodal,
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  title = {Multimodal Neurons in Artificial Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030}
}eton2024scaling,
    title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year = {2024},
    journal = {Transformer Circuits Thread},
    url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{zhao2024steering,
  title   = {Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering},
  author  = {Yu Zhao and Alessio Devoto and Giwon Hong and Xiaotang Du and Aryo Pradipta Gema and Hongru Wang and Xuanli He and Kam-Fai Wong and Pasquale Minervini},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2410.15999}
}

@article{chalnev2024improving,
  title   = {Improving Steering Vectors by Targeting Sparse Autoencoder Features},
  author  = {Sviatoslav Chalnev and Matthew Siu and Arthur Conmy},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.02193}
}



@article{bricken2023monosemanticity,
    title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
    author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
    year={2023},
    journal={Transformer Circuits Thread},
    note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}
@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@article{makhzani2013ksparse,
  title     = {k-Sparse Autoencoders},
  author    = {Alireza Makhzani and Brendan J. Frey},
  journal   = {International Conference on Learning Representations},
  year      = {2013},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b94043a133e3d07ed0b1cfc036829e619ea0ba22}
}


@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}

@inproceedings{ghilardi-etal-2024-accelerating,
    title = "Accelerating Sparse Autoencoder Training via Layer-Wise Transfer Learning in Large Language Models",
    author = "Ghilardi, Davide  and
      Belotti, Federico  and
      Molinari, Marco  and
      Lim, Jaehyuk",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.32/",
    doi = "10.18653/v1/2024.blackboxnlp-1.32",
    pages = "530--550",
    abstract = "Sparse AutoEncoders (SAEs) have gained popularity as a tool for enhancing the interpretability of Large Language Models (LLMs). However, training SAEs can be computationally intensive, especially as model complexity grows. In this study, the potential of transfer learning to accelerate SAEs training is explored by capitalizing on the shared representations found across adjacent layers of LLMs. Our experimental results demonstrate that fine-tuning SAEs using pre-trained models from nearby layers not only maintains but often improves the quality of learned representations, while significantly accelerating convergence. These findings indicate that the strategic reuse of pretrained SAEs is a promising approach, particularly in settings where computational resources are constrained."
}

@article{
gurnee2023finding,
title={Finding Neurons in a Haystack: Case Studies with Sparse Probing},
author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=JYs1R9IMJr},
note={}
}

@misc{nanda2022atp,
title={Attribution Patching: Activation Patching At Industrial Scale},
author={Neel Nanda},
year={2022},
url={https://www.neelnanda.io/mechanistic-interpretability/attribution-patching}
}

@inproceedings{mikolov-etal-2013-linguistic,
    title = "Linguistic Regularities in Continuous Space Word Representations",
    author = "Mikolov, Tomas  and
      Yih, Wen-tau  and
      Zweig, Geoffrey",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N13-1090/",
    pages = "746--751"
}

@misc{marks2024geometrytruthemergentlinear,
      title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets}, 
      author={Samuel Marks and Max Tegmark},
      year={2024},
      eprint={2310.06824},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.06824}, 
}

@misc{gurnee2024languagemodelsrepresentspace,
      title={Language Models Represent Space and Time}, 
      author={Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2310.02207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.02207}, 
}

@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Eric J. Michaud and Isaac Liao and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}

@misc{csordás2024recurrentneuralnetworkslearn,
      title={Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations}, 
      author={Róbert Csordás and Christopher Potts and Christopher D. Manning and Atticus Geiger},
      year={2024},
      eprint={2408.10920},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.10920}, 
}

@inproceedings{
penedo2024thefineweb,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}

@misc{elhage2021circuits,
title={A Mathematical Framework for Transformer Circuits},
author={Nelson Elhage and Neel Nanda and Catherine Olsson and Tom Henighan and Nicholas Joseph† and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
year={2021},
url={https://transformer-circuits.pub/2021/framework/index.html}
}

@misc{marks2024sparsefeaturecircuitsdiscovering,
      title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models}, 
      author={Samuel Marks and Can Rager and Eric J. Michaud and Yonatan Belinkov and David Bau and Aaron Mueller},
      year={2024},
      eprint={2403.19647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.19647}, 
}

@misc{mcgrath2023hydraeffectemergentselfrepair,
      title={The Hydra Effect: Emergent Self-repair in Language Model Computations}, 
      author={Thomas McGrath and Matthew Rahtz and Janos Kramar and Vladimir Mikulik and Shane Legg},
      year={2023},
      eprint={2307.15771},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.15771}, 
}

@inproceedings{geva-etal-2021-transformer,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446/",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "Feed-forward layers constitute two-thirds of a transformer model`s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model`s layers via residual connections to produce the final output distribution."
}

@inproceedings{dai-etal-2022-knowledge,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581/",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers."
}