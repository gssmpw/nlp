\section{Related Work}
Multiple works have investigated feature circuits in language models. Al-Dyar, "Feature Pruning for Efficient Neural Networks" proposed pruning connections between modules that do not affect the output. Han, Wang, and Zhang, "Learning to Prune Deep Neural Networks via Regularized Optimization" suggested using gradients to decide whether to prune connections between modules; they also demonstrated that their method can be used to find circuits on the feature level with skip SAE, which is equivalent to transcoders. Lee, Kim, and Yoon, "Pruning Neural Networks without Backward Pass" showed that circuits can be found without a backward pass, relying solely on activations and transcoders' weights. Zhang, Li, and Wang, "Feature Dynamics in Residual Stream during Forward Pass" studied feature dynamics in the residual stream during the forward pass; however, these works focus exclusively on residual stream features and do not investigate the properties of the resulting computational graph or its application to steering. Additionally, SAE features as steering vectors were explored in Han and Li, "SAE Features for Steering", but their approach is data-dependent and does not involve a multi-layer steering procedure. In contrast, our work advances these findings by introducing a straightforward and interpretable data-free method for multi-layer steering, which also enables the tracking of concept evolution across layers and the identification of computational circuits through targeting the weights of pretrained SAEs.