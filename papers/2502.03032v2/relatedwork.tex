\section{Related Work}
Multiple works have investigated feature circuits in language models. \citet{conmy2023automated} proposed pruning connections between modules that do not affect the output. \citet{ge2024automatically} suggested using gradients to decide whether to prune connections between modules; they also demonstrated that their method can be used to find circuits on the feature level with skip SAE, which is equivalent to transcoders. \citet{dunefsky2024transcoders} showed that circuits can be found without a backward pass, relying solely on activations and transcoders' weights. \citet{balagansky2024mechanistic_permutability, balcells2024evolutionsaefeatureslayers} studied feature dynamics in the residual stream during the forward pass; however, these works focus exclusively on residual stream features and do not investigate the properties of the resulting computational graph or its application to steering. Additionally, SAE features as steering vectors were explored in \citet{chalnev2024improvingsteeringvectorstargeting}, but their approach is data-dependent and does not involve a multi-layer steering procedure. In contrast, our work advances these findings by introducing a straightforward and interpretable data-free method for multi-layer steering, which also enables the tracking of concept evolution across layers and the identification of computational circuits through targeting the weights of pretrained SAEs.