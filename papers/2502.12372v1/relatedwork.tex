\section{Related Work}
\label{sec:background}
\paragraph{Data-to-text generation (D2T) and factual inconsistency.}
Data-to-text generation (D2T)\citep{lin2024survey} aims to transform non-textual, semi-structured data---such as tables, graphs, or slot-value pairs (meaning representation, MR)---into human-readable text.
It can be categorized into three types based on source representation: graph-to-text\citep{gardent2017webnlg,nan2021dart}, table-to-text~\citep{bao2018table}, and meaning representation (MR)-to-text~\citep{novikova2017e2e,juraska2019viggo}.
Recently, LLMs have become foundational models for D2T due to their extensive pre-training on large text datasets~\citep{zhang2022opt} and their high model capacity~\citep{scao2022bloom}.
Moreover, with parameter-efficient fine-tuning techniques~\citep{dettmers2023qlora} and prompt-based learning~\citep{lester2021power}, LLMs have gained widespread popularity for D2T tasks~\citep{raffel2020exploring,lewis2020bart,scao2022bloom}, often outperforming earlier models in generation quality and overall performance~\citep{ge2023openagi}.
In D2T, LLMs are often prone to generating factually inconsistent text, presenting a key research challenge. 
Factual inconsistency, defined as the lack of factual entailment between generated text and input data, contributes to hallucinations and undermines model reliability.
Evaluation methods include human assessment (gold standard but costly) and automatic metrics (scalable but debated).
Recently, trained automatic metrics~\citep{fabbri2022qafacteval,zha2023alignscore} have shown strong correlations with human judgments, making them promising for factual inconsistency evaluation.

\paragraph{Scaling law for LLM.}
Scaling laws for LLMs describe how their performance scales with key factors such as model size (number of parameters) and training data size.  
\citet{hestness2017deep} demonstrated that deep language models follow a power law scaling, laying the foundation for scaling law research.
\citet{kaplan2020scaling} expanded this by systematically analyzing model size, data size, and computational efficiency, reinforcing the dominance of power law scaling in LLM performance. 
As research on scaling laws has expanded, various studies have explored their applications across different task domains, including close-ended text generation~\citep{bansal2022data} and open-ended text generation~\citep{kaplan2020scaling}.  
Recent investigations have further examined scaling in diverse paradigms, such as sparse modeling~\citep{frantar2024scaling} and parameter-efficient fine-tuning~\citep{zhang2024when}.  
Additionally, joint scaling laws---such as additive and multiplicative formulations---are gaining prominence in multi-factor scaling setups~\citep{hoffmann2022training, zhang2024when}.  
Scaling laws offer several key advantages, including optimizing hyperparameter tuning~\citep{hendrycksforthcomingintroduction}, estimating training costs~\citep{haegele2024scaling}, and setting realistic expectations for model performance~\citep{hoffmann2022training}. 
A recent study by \citet{bahri2024explaining} further reinforces the theoretical foundations of scaling laws.