\section{Related Work}
\label{sec:background}
\paragraph{Data-to-text generation (D2T) and factual inconsistency.}
Data-to-text generation (D2T)____ aims to transform non-textual, semi-structured data---such as tables, graphs, or slot-value pairs (meaning representation, MR)---into human-readable text.
It can be categorized into three types based on source representation: graph-to-text____, table-to-text____, and meaning representation (MR)-to-text____.
Recently, LLMs have become foundational models for D2T due to their extensive pre-training on large text datasets____ and their high model capacity____.
Moreover, with parameter-efficient fine-tuning techniques____ and prompt-based learning____, LLMs have gained widespread popularity for D2T tasks____, often outperforming earlier models in generation quality and overall performance____.
In D2T, LLMs are often prone to generating factually inconsistent text, presenting a key research challenge. 
Factual inconsistency, defined as the lack of factual entailment between generated text and input data, contributes to hallucinations and undermines model reliability.
Evaluation methods include human assessment (gold standard but costly) and automatic metrics (scalable but debated).
Recently, trained automatic metrics____ have shown strong correlations with human judgments, making them promising for factual inconsistency evaluation.

\paragraph{Scaling law for LLM.}
Scaling laws for LLMs describe how their performance scales with key factors such as model size (number of parameters) and training data size.  
____ demonstrated that deep language models follow a power law scaling, laying the foundation for scaling law research.
____ expanded this by systematically analyzing model size, data size, and computational efficiency, reinforcing the dominance of power law scaling in LLM performance. 
As research on scaling laws has expanded, various studies have explored their applications across different task domains, including close-ended text generation____ and open-ended text generation____.  
Recent investigations have further examined scaling in diverse paradigms, such as sparse modeling____ and parameter-efficient fine-tuning____.  
Additionally, joint scaling laws---such as additive and multiplicative formulations---are gaining prominence in multi-factor scaling setups____.  
Scaling laws offer several key advantages, including optimizing hyperparameter tuning____, estimating training costs____, and setting realistic expectations for model performance____. 
A recent study by ____ further reinforces the theoretical foundations of scaling laws.