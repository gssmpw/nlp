%%%% ijcai25.tex

\typeout{Empowering LLMs with Logical Reasoning: A Comprehensive Survey}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}
\usepackage[dvipsnames]{xcolor}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{float}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{forest}
\usepackage{amssymb} 
\usetikzlibrary{trees,positioning,shapes,shadows,arrows.meta}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Empowering LLMs with Logical Reasoning: A Comprehensive Survey}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Fengxiang Cheng$^1$
\and
Haoxuan Li$^{2,3}$\thanks{Haoxuan Li is the corresponding author.}\and
Fenrong Liu$^{1,4}$\and
Robert van Rooij$^1$\and\\
Kun Zhang$^{3,5}$\And
Zhouchen Lin$^{6,7,8}$
\\
\affiliations
$^1$Institute for Logic, Language and Computation, University of Amsterdam\\
$^2$Center for Data Science, Peking University\quad 
$^3$Machine Learning Department, MBZUAI\\
$^4$Department of Philosophy, Tsinghua University\quad $^5$Department of Philosophy, CMU\\
$^6$Institute
for Artificial Intelligence, Peking University\quad $^7$Peng Cheng Laboratory\\
$^8$National Key Lab of General AI, School of Intelligence Science and Technology, Peking University\\
\emails
\{f.cheng, r.a.m.vanrooij\}@uva.nl,
hxli@stu.pku.edu.cn,
fenrong@tsinghua.edu.cn,\\
kunz1@cmu.edu,
zlin@pku.edu.cn
}
% \fi
%high order reasoning
%非单调逻辑
\begin{document}

\maketitle

\begin{abstract}
%%问题背景：LLM文本生成能力很强，但是仍然缺乏逻辑推理能力，由于pre-train data缺少逻辑数据。本文将LLMs的逻辑推理能力归纳总结成在两个方面：（1）\emph{logical question answering}, which may 产生错误回答\emph{within} the each complex logical question（【给复杂逻辑问题一个定语】requiring 需要归纳能力、演绎能力、证明能力在first-order logic）的准确率很低 and（2）\emph{logical consistency}，which may出现逻辑矛盾\emph{across} different questions（讲问题），短例子。本文全面调研方法，并提出了novel分类法taxonomy。具体地，logical reasoning：solve-based，XX；logical consistency，XX。我们也调研了benchmarks和evaluation protocols，未来方向，包括XXX。``XXX''


Large language models (LLMs) have achieved remarkable successes on various natural language tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. This paper summarizes and categorizes the main challenges into two aspects: (1) \textbf{Logical question answering}, LLMs often fail to generate the correct answer \emph{within} complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) \textbf{Logical consistency}, LLMs are prone to producing responses contradicting themselves \emph{across} different questions. For example, a state-of-the-art Macaw question-answering LLM answers \emph{Yes} to both questions \emph{Is a magpie a bird?} and \emph{Does a bird have wings?} but answers \emph{No} to \emph{Does a magpie have wings?}.
To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, pretraining, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistency, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extensions to modal logic to account for uncertainty, and efficient algorithms satisfying multiple logical consistencies simultaneously.


%%% %For example, LLaMa-2 70B answers ``true" to both questions ``Is an albatross an organism?" and ``Is an albatross not an organism?".

%参考文献要简写
%要有汇总表

%negation
%nero AI no
%logic rules to training, error

%finetuning based和prompt based的区别
%最后要引用ijcai的论文

%idea1: 鸡蛋不能放进同一个篮子里：四种逻辑解法可以互相transform，这样能用四种形式化和solver来同时解一个问题(可以nl到SAT再到fol，也可以nl直接到fol，具体哪个好看实验)，四管齐下，最后再选出得票最高的。这能够提高结果的准确性
%%可能用到集成学习


%%%%benchmark：三段论；命题逻辑（自然推演系统）；谓词逻辑；（系统的）；看之前的缺陷是什么，我们能如何系统
%命题模态逻辑，之后再做

%%%%逻辑的复杂性：命题数量，连接词多少。嵌套数量多少个括号。


%后续可以做的工作2：基于Holiday的工作，提一个关于模态逻辑的prompt-based方法，基于Wesley的benchmark和SimboCoT；finetune的方法，extend to modal logic，或者extrapolation拓展到外部；二元分类器，让它输出不是0或1，而是概率值，就能把一阶逻辑推广到模态逻辑；试图把刘老师的偏好逻辑，引入推理方法

%idea3： logical reasoning of LLM need metric learning：data driven的方式，让模型自己learn出一个刻画推理能力的metric，beyond accuracy。
%idea4：LoT的解决确实和motivation对不上，我的质疑是对的。翻译的过程中会造成信息缺失，这是核心问题。但是不能只喷，还要自己想想解决办法。moti的问题有两种解决，一个是补充上缺失的信息，但很困难。另一个可以把条件里冗余的前件删掉，比如“person（x）”的条件，删掉条件后就不需要缺失的“Harry是人”的信息了。沿着这个思路的想法是，我们可以给premise2这样的条件里的每个前件赋权重，AI里的attention技术可以做到这一点，这样就能关注合取式条件的最重要的一支，而忽略那些翻译中的冗余条件。
%从语言模型到VLM和MLLM



%%写作学习（自己修改的方向）：
%每个新名词是不是有解释，读者小白能否读懂
%每句话有没有信息量，有没有废话
%上下两句话有没有关联，能否串起来
%检查语法
%用的动词等要确定什么意思



%%%idea：找工具，提升翻译的准确性，插件，理解句子的结构。句子的逻辑结构不同，对应的formulization也不同

%%让大模型学会句子之间的逻辑结构，抽象思维能力，学会逻辑规则

 \end{abstract}

\section{Introduction}

%%问题背景：LLM文本生成能力很强，但是still struggle with complex logical problems，由于pre-train data缺少逻辑数据。  
%%困难包括两个方面，一方面是logical reasoning cabilities to solve complex logical problems，举一个LLM推理失败的例子。另一方面是保证其生成答案的logical consistency。举一个LLM回答不一致的例子。
%%该问题受到了AI领域学者的广泛关注，主要。To address this problem, 前人有很多相关的研究。方法有很多，但是仍然缺少相关的方法汇总与分类。我们希望通过这个整理，能给未来的进一步研究【全面地】【最前沿的方法】

%%LLMs先夸再喷，缺乏逻辑推理能力。具体地，一方面，对于complex logical problems，答不对（用数据说话，例如LLaMa3 70B准确率只有多少【cite】），限制其在复杂真实世界场景的应用。另一方面，即使对于简单的问题，大模型的输出在不同问题间具有逻辑不一致性（logical inconsistency，解释一下），可信。被称为logical QA and logical consistency of the outputs.

Large language models (LLMs) have demonstrated remarkable performance in a broad range of natural language tasks including language generation, classification and translation. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. On one hand, learning syntax, semantics, and world knowledge through tasks such as next-word prediction or masked language modeling does not ensure the logical reasoning ability of LLMs~\cite{luo2023towards}. %%找文章原句：Language models learn syntax, semantics, and world knowledge through tasks like next-word prediction or masked language modeling, but this doesn’t ensure logical reasoning skills.
On the other hand, the pre-training corpus of LLMs primarily consists of human-written texts, which lack high-quality logical reasoning samples such as logical deduction and proofs~\cite{morishitaenhancing}. These challenges significantly limit the applicability of LLMs due to the following two summarized aspects.

% Thus, to overcome these challenges, it is essential to develop methods to improve the logical reasoning ability of LLMs.

LLMs often fail to generate the correct answer in \textbf{logical question answering}, which requires sophisticated deductive,
%应该只有演绎推理，后两个删去
inductive or abductive reasoning given a collection of premises and constraints. Specifically, these logical questions  can be broadly divided into two categories: (1) Determine whether a related statement can be deduced from the given information, namely, output the truth value of the statement: true, false or unknown. For example, the premise and constraints could be \emph{Metals conduct electricity. Insulators do not conduct electricity. If something is made of iron, then it is metal. Nails are made of iron.}, followed by the logical question \emph{Is the following statement true, false, or unknown? Nails cannot conduct electricity}. To answer this question correctly, LLMs need to conduct logical reasoning \emph{nails$\to$made of iron$\to$metal$\to$conduct electricity} before conclude that the statement is actually \emph{false}. (2) Find the correct option that can satisfy all the given premises and constraints from the multiple choices. Surprisingly, LLaMA-13B achieves 33.63\% accuracy under 8-shot prompting on the logical questions dataset FOLIO, which is only slightly better than random guess from true, false and unknown with an accuracy of 33.33\%~\cite{han-etal-2024-folio}. This significantly restricts the application of LLMs in complicated real-world situations, such as problem-solving and decision-making.

LLMs are also prone to producing responses contradicting themselves \emph{across} different questions, which is regarded as violation of \textbf{logical consistency}. Note that the form of logical consistency can be diverse. For example, LLaMa-2 70b answers \emph{true} to both questions \emph{Is an albatross an organism?} and \emph{Is an albatross not an organism?}~\cite{calanzone2024logically}, which violates the negation consistency (see Section \ref{sec:3} for formal definition). In addition, a state-of-the-art Macaw question-answering LLM answers \emph{Yes} to both questions \emph{Is a magpie a bird?} and \emph{Does a bird have wings?} but answers \emph{No} to \emph{Does a magpie have wings?}~\cite{mitchell-etal-2022-enhancing}, which violates the transitivity consistency. Unfortunately, many studies have shown that training on large question-answering datasets alone cannot ensure the logical consistency of LLMs~\cite{kassner-etal-2021-beliefbank,jung-etal-2022-maieutic}. As a result, these contradictory outputs concerns the reliability and trustworthiness of LLMs, which limits the practical deployment in especially high-stakes scenarios.

%%logical QA是什么？定义，分类，例子（详细，50%）。
%{逻辑问答}，主要指解决复杂的逻辑问题。 这些问题通常给出一系列前提和一些约束条件，要求模型通过演绎推理根据输入内容完成某些任务。 任务目标大致可分为两类：（1）判断某个相关问题或statement能否从给定的信息演绎地推理得到，即输出问题的真值：true, false or unknow。（2）从多个选择中，找出能够满足所有给定前提和约束的选项。例如：xx。要正确回答这个问题需要逻辑推理尤其是演绎推理能力，logical QA是我们可以测评和提高模型的逻辑推理能力的有效窗口。





%%logical consistency是什么？例子（详细，三段论，50%）。
%{逻辑一致性}，要求模型在对复杂问题进行推理的过程中，在回答不同问题时不自相矛盾。例如，如果一个模型回答“真”对一个问题 “郁金香是花吗？”，这隐含地表达了 “郁金香是花 ”的信念。但是，如果同一个模型回答“真”对一个问题 “郁金香不是花吗？”，这与该模型先前的回答就产生了逻辑矛盾，即模型的回复中有逻辑不一致性。具有逻辑一致性的LLM可以被人们无需复杂的基准而直接验证正确性，从而为最终用户在自己的工作对LLM的可靠性提升信心


%% In this paper，我们针对现有的logical QA和logical consistency方法提出了新的taxonomy（复数）。具体地，logical QA有xxx，可以半句话分别介绍。logical consistency有xxx。对于未来发展方向：（1）模态逻辑（参考holiday写作，易懂）（2）从语言模型到VLM和MLLM，举例（3）高效（2个甚至多个样本间，which raises efficiency concern）（4）developing unified method同时满足多个logical consistency。
%我们将准确地回答一个复杂的 logic question 和确保不同问题回答间的logic consistency ，看作提升大模型的逻辑推理能力的一体两面。且目前有众多研究工作涌入了这个话题。特别地，关于QA，我们总结归纳了四类，包括杠杆外部的逻辑求解器逻辑推理，改进prompt的方法让推理过程直接展示，补充包含逻辑推理过程的数据集进行预训练和微调.
   

In this paper, we consider accurately answering isolated complex logical problems and ensuring logical consistency across outputs to different questions as two sides of the coin in improving the logical reasoning capabilities of LLMs. To achieve this, many methods have been proposed to improve the logical question answering accuracy~\cite{xu-etal-2024-faithful,wan-etal-2024-logicasker} and logical consistency~\cite{daniel2025check,calanzone2024logically}, which have recently attracted widespread attention to the LLM community.  To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods, as shown in Figure \ref{fig:lit_surv}. Specifically, for logical question answering, these methods are classified into solver-based methods, prompt-based methods, pretrain and fine-tuning methods, according to whether they primarily rely on logic solvers, LLM prompts and in-context learning, or require further training. In general, solver-based approaches translate natural language problems to symbolic language expressions, and then solving them via external logical solvers~\cite{ye2023satlm,olausson-etal-2023-linc}. Prompt-based methods either explicitly model the logical chain when answering the questions~\cite{zhang2024diagram}, or translate natural language into symbolic language by well-defined prompt, and then utilize LLMs for better reasoning~\cite{xu-etal-2024-symbol}. Considering the lack of high-quality reasoning samples such as logical multi-step deduction or proofs in the pre-training corpus~\cite{morishitaenhancing}, pretrain and fine-tuning methods propose to training LLMs with augmented deductive proofs and natural language examples explicitly including the logical reasoning process. For logical consistency, we formulate most common logical consistency concepts, including implication, negation, transitivity, factuality consistency, and their composites, discuss the state-of-the-art solutions for enhancing each type of logical consistency, and review commonly used benchmark datasets and evaluation metrics. Lastly, we discuss promising research directions, such as extension to conditional and modal logic, and efficient algorithms satisfying multiple logical consistencies. 


%% To the best of our knowledge, this is the first work to XXXX。2.5（1）：focus on XX，as 一类specific的complex logical problem，but ignore。（2）缺少对most recent的XX方法的调研

To the best of our knowledge, this is the first work to comprehensively investigate the most cutting-edge research on improving LLM logical reasoning capabilities, covering complex logical question answering and logical consistency. As our most relevant work~\cite{zong-lin-2024-categorical}, our survey focuses only on categorical syllogisms, as a specific logical reasoning paradigm, but ignores other complex logic inference rules. In addition, \cite{lam2024closerlooklogicalreasoning} only discuss about the methods based on external tools such as logical solvers.
%and \cite{luo2023towards} lacks the discussions on the most recent methods.



\begin{figure*}[t]
% \vspace{-0.3cm}
\centering
\resizebox{\textwidth}{!}{
\tikzset{
    basic/.style  = {draw, text width=2cm, align=center, rectangle},
    root/.style   = {basic, rounded corners=2pt, thin, align=center, fill=purple!10, text width=0.5cm,},
    node1/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!07, text width=2cm,},
    node11/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!04, text width=2cm,},
    node12/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!04, text width=2cm,},
    node13/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!04, text width=2cm,},
    node14/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!04, text width=2cm,},
    node111/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!01, text width=13cm,},
    node121/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!01, text width=13cm,},
    node131/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!01, text width=13cm,},
    node141/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!01, text width=13cm,},
    node2/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!20, text width=2cm,},
    node21/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!15, text width=2cm,},
    node22/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!15, text width=2cm,},
    node23/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!15, text width=2cm,},
    node24/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!15, text width=2cm,},
    node25/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!15, text width=2cm,},
    node211/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!10, text width=2cm,},
    node212/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!10, text width=2cm,},
    node2111/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=10.05cm,},
    node2121/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=10.05cm,},
    node221/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=13cm,},
    node231/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=13cm,},
    node241/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=13cm,},
    node251/.style = {basic, thin, rounded corners=2pt, align=center, fill=orange!5, text width=13cm,},
    edge from parent/.style={draw=black, edge from parent fork right}
}
%
\begin{forest} for tree={
    grow=east,
    growth parent anchor=west,
    parent anchor=east,
    child anchor=west,
    edge path={\noexpand\path[\forestoption{edge}]  (!u.parent anchor) -- +(10pt,0pt) |-  (.child anchor) \forestoption{edge label};}
    % edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};},
    l sep=7mm,
    % s sep=5mm
    calign=center,
}
% l sep is used for arrow distance 
[\rotatebox{90}{Logical Reasoning}, root, l sep=7mm, yshift = -11mm
    [Logical Consistency, node2, l sep=7mm, 
        % [Benchmark, node01,l sep=7mm
        %     [Benchmark,node1]
        % ]
        [Compositional Consistency, node25,l sep=5mm, yshift = 0.1mm
            [{REFLEX~\cite{kassner-etal-2023-language}, Aligning with Logic~\cite{liu2024aligning}, LOCO-LMs~\cite{calanzone2024logically}, LLM-based fact-checking framework~\cite{ghosh2025logical}},node251, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Factuality Consistency, node24,l sep=5mm, yshift = 0.2mm
            [{BeliefBank~\cite{kassner-etal-2021-beliefbank}, LOCO-LMs~\cite{calanzone2024logically}, LLM-based fact-checking framework~\cite{ghosh2025logical}},node241, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Transitivity Consistency, node23,l sep=5mm, yshift = 0.1mm
            [{Logic-Guided~\cite{asai-hajishirzi-2020-logic}, ConCoRD~\cite{mitchell-etal-2022-enhancing}, Aligning with Logic~\cite{liu2024aligning}},node231, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Implication Consistency, node22,l sep=5mm, yshift = -5.7mm
            [{BeliefBank~\cite{kassner-etal-2021-beliefbank}, ConCoRD~\cite{mitchell-etal-2022-enhancing}, Maieutic Prompting~\cite{jung-etal-2022-maieutic}, 
            \\REFLEX~\cite{kassner-etal-2023-language}, Aligning with Logic~\cite{liu2024aligning}, 
            \\LOCO-LMs~\cite{calanzone2024logically}, LLM-based fact-checking framework~\cite{ghosh2025logical}, },node221, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Negation Consistency, node21,l sep=7mm
            [Antonyms, node212,l sep=5mm, yshift = -1.82mm
                [{Logic-Guided~\cite{asai-hajishirzi-2020-logic}, Aligning with Logic~\cite{liu2024aligning}}, node2121, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
            ]
            [Negation, node211,l sep=5mm,  yshift = -7.65mm
                [{BeliefBank~\cite{kassner-etal-2021-beliefbank}, ConCoRD~\cite{mitchell-etal-2022-enhancing}, \\Maieutic Prompting~\cite{jung-etal-2022-maieutic}, REFLEX~\cite{kassner-etal-2023-language}, Aligning with Logic~\cite{liu2024aligning}, \\LOCO-LMs~\cite{calanzone2024logically}, LLM-based fact-checking framework~\cite{ghosh2025logical}},node2111, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
            ]
        ]
    ]
    [Logical Question Answering, node1,   l sep=7mm
        [Benchmark, node11, l sep=5mm,  yshift = -2.2mm
            [{LogicNLI~\cite{tian2021diagnosing}, LOGIGLUE~\cite{luo2023towards}, LogicBench~\cite{parmar-etal-2024-logicbench}, LogicAsker~\cite{wan-etal-2024-logicasker}}, node111, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Pretrain and fine-tuning, node12,   l sep=5mm, yshift = 0.1mm
            [{LReasoner~\cite{wang-etal-2022-logic}, LOGIPT~\cite{feng-2024-languagecanbe}, ALT~\cite{morishitaenhancing}, LogicAsker~\cite{wan-etal-2024-logicasker}, LogicLLM~\cite{jiao2024exploring}}, node121, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]        
        [Prompt-based, node13,   l sep=5mm, yshift = -1.8mm
            [{CR~\cite{zhang2023cumulative}, DoT~\cite{zhang2024diagram}, \\SymbCoT~\cite{xu-etal-2024-faithful}, Logic-of-thought~\cite{liu2024logicofthoughtinjectinglogiccontexts}, LINA~\cite{li2024leveraging}}, node131, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
        [Solver-based, node14,  l sep=5mm,  yshift = -2.05mm
            [{Faithful~\cite{lyu2023faithfulCoT}, Satlm~\cite{ye2023satlm}, LOGIC-LM~\cite{pan-etal-2023-logic}, LINC~\cite{olausson-etal-2023-linc}}, node141, edge path={\noexpand\path[\forestoption{edge}] (!u.parent anchor) --  (.child anchor) \forestoption{edge label};}]
        ]
    ]
]
\end{forest}
}
\caption{Our taxonomy tree of Logical Reasoning, which is primarily divided into Logical Question Answering and Logical Consistency.}
\label{fig:lit_surv}
% \vspace{-0.5cm}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{framework.pdf}
    \caption{The overview of workflow in solver-aided approaches.}
    \label{fig:solver_framework}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figure3.pdf}
    \caption{Directed Acyclic Graph (DAG) of iterative reasoning constructed in the Diagram-of-Thought (DoT) method.}
    \label{fig:enter-label}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure4.pdf}
    \caption{A general framework to enhance the logical consistency of LLM outputs across different questions.}
    \label{fig:figure4}
\end{figure*}

\section{Logical Question Answering}
%%问题是什么，任务是什么。
%%总述如何分类。
%%每一类，总述方法，分述每篇怎么做。每一类最后讲一下优势和limitation
%prompt那一节，讲prompt研究传统
%一小节 Evaluation和database
%%最后，discussion

Leveraging large language models (LLMs) for complex reasoning problems has been a key focus of recent researches~\cite{luo2023towards,sun-etal-2024-determlr,zhang2024diagram}. Numerous studies have explored methods to enhance the logical reasoning capabilities of LLMs, which can be broadly categorized into several branches, including reliance on external solvers, prompting techniques without further training LLMs, and pretraining and fine-tuning. In addition, many benchmarks are developed to evaluate the logical reasoning performance of LLMs.

% \subsubsection{An Overview for Methods}

%提升大模型解决复杂问题的准确率的方法讨论，自大模型诞生以来已经有了很长和很丰富的发展脉络。从最早的CoT方法包括其相关的系列扩展和变种，利用prompt方法mainly enhance reasoning by eliciting intermediate steps，有效地提升了大模型的推理能力。但是在解决复杂的逻辑问题时，仍然展现了其不足。一方面，由于 lack of explicit logical grounding and the inherent ambiguous and nuanced nature of natural language。FOLIO 新颖地展示了incorporating first-order logic (FOL) translations into the context can notably enhance LLM’s performance.,并建立了相关的benchmark来评估大模型的一阶逻辑推理能力。
%而在符号化处理自然语言之后，如何进行推理的过程，现有学者们的方法大致呈现出两个stream。一方面仍然让大模型自己进行推理过程，通过改进prompt的方法，或者微调、预训练的方式提高其回答复杂问题的准确率；一方面，只让大模型发挥其处理自然语言的特长进行翻译工作，对于逻辑推理尤其是演绎推理过程利用外部求解器进行，包括编程求解器、一阶逻辑求解器、SAT求解器等。这两种方法各自有其优劣，我们将在下文详细讨论。
%具体地，我们下文将从五个方面来分类讨论现有的方法，包括 reliance on external solvers, prompting techniques, pretraining, fine-tuning, and other methodologies.

% The development of methodology to improve the reasoning capabilities of LLM has been a long journey since its inception. Chain-of-Thought (CoT)~\cite{wei2022chain}, with its various extensions and modifications, represents one of the most influential approaches. These prompt-based methods enhance reasoning primarily by guiding the model to generate the results steps by steps. However, when it comes to tackling complex reasoning problems, these approaches falls short. One of the main reasons is the missing explicit logical foundation, and the inherently ambiguous and nuanced nature of natural language. Relevant research~\cite{han-etal-2024-folio,wang-etal-2022-logic} demonstrates that the performance of LLM can be significantly improved by integrating symbolic translations such as first-order language(FOL) into the contexts, and establishes a relevant benchmark to evaluate the first-order logic reasoning ability of large models.

% After the symbolic processing of natural language, as for how to carry out the complex logical reasoning process, the existing approaches roughly show two streams. On the one hand, LLMs are allowed to independently perform reasoning by refining prompting methods, leveraging pretraining, or employing fine-tuning techniques, enhancing the reasoning ability of LLMs via invoking logical symbols and expressions. On the other hand, confining the model’s role to translation work by taking advantage of its natural language processing expertise, the logical reasoning—particularly deductive inference—is delegated to external solvers, such as programmatic solvers, first-order logic solvers, and SAT solvers. Each of these two paradigms has its own strengths and weaknesses, which will be discussed in detail in the following parts.

% Specifically, we will categorize and analyze existing approaches from five key perspectives: reliance on external solvers, prompting techniques, pretraining, fine-tuning, and other methodologies.




\subsection{Solver-Aided Approach: Translation Via LLM and Proof Via Solver}

% solver-aided方法指to transform textualized problems into logical expressions via LLMs, then solve them with symbolic logic solvers。

% 总体上看，solver-aided的方法大体上可以分为三部分，首先，让LLM将自然语言翻译为求解器可识别的符号语言（比如Logic Programming (LP) Language，first-order language and Boolean Satisfiability (SAT) Formulation），细致和正确的翻译结果是这下一步的基础。然后，用对应语言的求解器进行演绎推理得到需要的答案。最后，用多种方式验证结果的正确性，并让LLM将符号语言翻译回自然语言，输出答案。


% Faithful CoT ~\cite{lyu2023faithfulCoT} 最早提出引入外部工具的思路来提高LLM回答的faithfulness。他将Natural Language query翻译为  symbolic reasoning chain，然后用一个deterministic solver( such as the Python/Datalog interpreter, or a PDDL planner)获得答案。 However, programs can't solve logic problems due to the challenge of representing their highly “non-linear” reasoning procedure with functional programming. Unlike Faithful CoT that uses task-specific formulations and task-specific solvers for different problem types, Samlt~\cite{ye2023satlm} prompt an LLM to formulate all the tasks as general SAT instances that can be solved by a single SAT solver. While Samlt cast a natural language (NL) reasoning problem as a satisfiability (SAT for short) problem, LogicLM~\cite{pan-etal-2023-logic} utilizes LLMs to translate NL problem into four types of symbolic formulations separately, including Logic Programming (LP) Language, FOL, Constraint Satisfaction (CSP) and Boolean Satisfiability (SAT) Formulation. Then the corresponding symbolic solver (Pyke, Prover9, python-constranit, Z3, respectively) performs inference on the formulated problem. LogicLM utilises the solver’s incorrect information to revise iteratively correction of LLM translation results， while LINC~\cite{olausson-etal-2023-linc} 采用了另一种方式来确保翻译的正确性. LINC prompt LLMs to the NL problem into candidate samples in FOL and after the filtering and inference by the solver, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best answer. 

    

%%在solver-aided方法出现之前，主要有两方面的方法来让LLM解决推理问题~\cite{pan-etal-2023-logic}：{ fine-tuning approaches} that optimize LLMs’ reasoning ability through fine-tuning or training specialized modules ,和 {in-context learning approaches }that design special prompts to elicit LLMs’ step-by-step reasoning capabilities. 但是正如前文所说，前人方法的局限性一方面在于the intrinsic complexity and ambiguity of NL bringing unfaithful reasoning and hallucinations，另一方面因为之前的方法 solely relies upon LLM’s logical reasoning prowess and is susceptible to issues such as hallucinations and taking shortcuts。
%优点和缺点

The solver-aided approach refers to parsing natural language (NL) problems to symbolic language (SL) expressions, and then solving them via external solvers. In general, the working flow of solver-aided methods can be summarized into three steps, as shown in Figure \ref{fig:solver_framework}. First, these methods use LLM to translate natural language into a symbolic language (such as logic programming (LP) language, first-order logic (FOL), constraint satisfaction (CSP), and boolean satisfiability (SAT) formulation) that can be recognizable by corresponding solvers. Next, they adopt an external solver for logical reasoning to output the desired answer. Finally, they use LLMs to translate the symbolic answer provided by the external solver into natural language to generate the final answer based on some ensemble algorithms such as majority vote.

Faithful CoT~\cite{lyu2023faithfulCoT} based on the Chain-of-Thought strategy translates a natural language query into a symbolic reasoning chain with a deterministic solver (e.g., Python/Datalog interpreter or a PDDL planner) to improve the faithfulness of LLM-generated answers. However, because of the limitation of functional programming to represent the highly "non-linear" process of reasoning, programs may fail to solve complex logic problems. Unlike Faithful CoT, which applies task-specific formulations and solvers for different problem types, Samlt~\cite{ye2023satlm} and LINC~\cite{olausson-etal-2023-linc} prompt the NL problems as general SAT and FOL formulation, respectively, and derive the answer by the corresponding solvers. In addition, LogicLM~\cite{pan-etal-2023-logic} utilizes LLMs to translate NL problems into four types of symbolic formulations separately, including LP language, FOL, CSP, and SAT formulation, for a more comprehensive understanding of NL problems. However, these methods ignore the complex logical semantics hidden in the NL problems when performing translation. To bridge this gap, CLOVER \cite{ryu2025divide} prompts LLM to parse a natural language sentence into logical dependency structures, then translates the subsentences sequentially. 

However, the solver-aided approach has many drawbacks. First, transforming logical problems into formal expressions will result in information loss~\cite{liu2024logicofthoughtinjectinglogiccontexts}, leading to the problem being unsolvable. For example, in the symbolic translation of ``When a person reads a book, that person gains knowledge. \textit{Harry} read \textit{Walden}. Whether this inference is correct: Harry gains knowledge." The solver will output uncertain, since the symbolic translation process loses the vital hidden information ``Harry is a person" and ``Walden is a book", which actually will be easily inferred by humans~\cite{li2024leveraging,liu2024logicofthoughtinjectinglogiccontexts}. Second, a small mistake in translation from NL problems to SL formulation will severely affect the results derived from SL solvers. Meanwhile, the translation capacities are limited in some models, for example, GPT3.5 is not being able to add a complete bracket~\cite{feng-2024-languagecanbe,lam2024closerlooklogicalreasoning}. In addition, as the complexity of the problem increases, it will require the exponential expansion of the search space and weeks or even months to resolve using current solvers~\cite{zhang2024dilaenhancingllmtool}.

\subsection{Prompt-Based Approach: Translate, Reason and Verify Via LLM}

%prompt的历史
Prompting is a direct and effective technique for stimulating the logical reasoning capabilities of LLMs. To achieve a more accurate answer of logical questions, the prompt-based approaches can be broadly divided into two categories, the first is to explicitly model the logical chain when answering the questions. For example, Chain-of-Thought (CoT) \cite{wei2022chain} prompt strategy that enables LLMs to output the reasoning process step-by-step. Based on this, Tree-of-Thought (ToT)~\cite{yao2024tree} was proposed to let LLMs self-evaluate and select among multiple inference paths. Meanwhile, Graph-of-Thoughts (GoT)~\cite{besta2024GoT} represents the generated information as an arbitrary graph and allows to extract the core of whole networks of thoughts based on the induced graph. To ensure that the reasoning process can more realistically reflect rational logical thoughts, Diagram of Thought (DoT)~\cite{zhang2024diagram} was proposed to model iterative logical reasoning in LLM as constructing a directed acyclic graph (DAG) within the model, as shown in Figure \ref{fig:enter-label}, which consists of nodes representing propositions, criticisms, refinements, and verifications. The edges in the DAG are all oriented without any cyclic paths, which represent logical relationships or dependencies between each node. In addition, many previous studies focus on the decomposition of logical questions. For example, cumulative reasoning~\cite{zhang2023cumulative} was proposed to leverage three specialized types of LLMs (proposer, verifier, and reporter) in a collaborative reasoning process.

The second category is to translate natural language into symbolic language by well-defined prompts, and then utilize LLMs for better reasoning. For example, 
Symbolic Chain-of-Thought (SymbCoT)~\cite{xu-etal-2024-symbol} is a LLM-based framework that combines symbolic representation and logical rules with CoT prompting. Specifically, after translating NL problems to SL formulation, it generates a step-by-step solution to address the task with logic inference rules such as modus ponens (MP) rule, and then provides a verifier for checking the translation and inference chain. Another work named Logic-of-Thought~\cite{liu2024logicofthoughtinjectinglogiccontexts} first uses Python algorithms to do deductive reasoning and output the implicated logical expressions based on the logic rules and the expanded logical information is then translated back into natural language by LLM and incorporated into the input prompt to derive the final answers. In addition, LINA~\cite{li2024leveraging} employs a hypothetical-deductive approach, breaking the limitation of closed-choice questions based on FOL rules, and a majority voting mechanism is then used to validate the final answer and ensure reliability.

%prompt方法的优点很明确，方法相对直接和容易实现。真正实现了让LLM进行“推理”，并且推理过程是透明公开的。但相应的，其对应的缺点也很大。
Although there are many advantages of the prompt method such as transparency and interpretability, there are still many limitations of such methods. On the one hand, challenges in ensuring causality and reliability remain for prompt-based methods. This limitation arises from the reliance on LLMs for conscious inference, which are susceptible to hallucinations and may confound causality. On the other hand, While LLM inference takes place in every searching step, solving a problem usually needs numerous ones, resulting in a huge computational cost
\cite{yang2023neuro}.


\subsection{Pre-training and Fine-tuning Approach}
The limited reasoning abilities of LLMs can be attributed to the lack of high-quality reasoning samples (especially logical multi-step deduction or proofs) in the pre-training corpus, which is composed mainly of human-written texts~\cite{morishitaenhancing}. Human-written texts usually show reflexive thinking rather than rigid reasoning, let alone logical deduction or inference. Intuitively, training LLMs with deductive proofs and natural language examples that demonstrate explicitly the logical reasoning process is an effective way to enhance their intrinsic logical reasoning capabilities.

%数据反馈的两个大的方向
Implementation of pre-training and fine-tuning on logical questions with the reasoning process requires additional data. A major distinction between them is based on the source of the feedback, which can be internal to the LLM or external. Internal feedback uses the intrinsic knowledge and parameters of the model to reassess its outputs. On the contrary, external feedback integrates inputs from human, other models, or other sources of knowledge base and tools. DiLA~\cite{zhang2024dilaenhancingllmtool}
utilizes the LLM to generate an original solution, and then iteratively refines this solution by forward and backward passes through a network logic layer involving first-order logic constraints. The constraint satisfiability is then checked by the SAT solver, and the solution is updated until all constraints are satisfied. A logic-driven contrastive learning approach~\cite{wang-etal-2022-logic} was proposed to encourage the pre-trained model to better capture logical information. In addition, a logic-driven data augmentation approach~\cite{bao-etal-2024-abstract} named AMR-LDA was proposed to transform the initial text into a structured semantic representation. Next, operations within them are applied to generate logically modified AMR graphs and then translated back into natural language to produce augmented data. 

To further enhance the transparency and interpretability, LOGIPT~\cite{feng-2024-languagecanbe} was proposed to directly internalize and emulate the reasoning processes of the solver Pyke and output the process directly. In addition, it is fine-tuned on a constructed instruction-tuning dataset containing both natural language logical questions and the solver’s symbolic reasoning process. In addition, ALT~\cite{morishitaenhancing} builds a synthetic logic corpus based on logical principles, containing natural language examples translated from a logical deductive reasoning process generated by a logical-based algorithm. Then ALT adopts this process to implement supervised fine-tuning, requiring LLMs to generate logical steps to infer a given hypothesis from provided facts. Similar to ALT, LogicAsker~\cite{wan-etal-2024-logicasker} established a set of fundamental reasoning skills grounded in propositional and predicate logic and generated natural language examples for each skill based on LLMs. Based on the skill set, LogicAsker generates reasoning questions by translating standard logic expressions into natural language, assesses the accuracy of LLMs per skill, pinpoints weaknesses, and creates in-context-learning examples and fine-tuning data to bolster reasoning abilities. Additionally, to reduce manual annotation costs, LogicLLM~\cite{jiao2024exploring} introduces a fully self-supervised framework for integrating logical reasoning capabilities into LLMs.

%优势劣势






\subsection{Evaluation: Tasks and Benchmark Datasets}

In general, a logical reasoning question contains a natural language description that outlines a set of propositions or constraints related to certain objects, along with a corresponding question about these objects. The objective is to infer the correct answer to the question based on the given information~\cite{ye2023satlm}. These question-answering pairs are typically categorized into two primary types~\cite{luo2023towards}:

(1) Free-Form Qustion-Answering: Assess whether a given question or statement can be logically inferred from the provided information, producing an output of true, false, or unknown. The formulation includes logic programming language, propositional logic language, and first-order language. There are many typical datasets such as Proofwriter~\cite{tafjord2021proofwriter}, FOLIO~\cite{han-etal-2024-folio}, Ruletaker~\cite{clark2021transformers} .

% The dataset includes:
%  \begin{itemize}
%     \item  The RuleTaker\cite{clark2021transformers} dataset is automatically generated via programming, utilizing connectives such as conjunction, negation, and implication. Each question comprises a context and a conclusion.
%     \item The ProofWriter dataset~\cite{tafjord2021proofwriter} comprises numerous small rulebases composed of facts and rules, and contains RuleTaker-style datasets with 500k questions, answers and proofs over natural language rulebases.[with proof]
%     \item  The FOLIO dataset\cite{han-etal-2024-folio} is a comprehensive and diverse dataset designed for reasoning in natural language, which is characterized by its human annotations, open-domain nature, and logical complexity. It boasts first-order logic (FOL) annotations, comprising 1,435 unique examples of conclusions.
%     \item Abductive Rules\cite{young-etal-2022-abductionrules}  is a dataset that evaluates the abductive reasoning capabilities of language models. It is generated similarly to ParaRule Plus, but in this task, the model has to generate an answer to explain an observation.
%     \item PrOntoQA \cite{PrOntoQAsaparovlanguage} is a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis.
%     \end{itemize}

(2) Multiple Choice Question-Answering: Identify the correct option from multiple choices satisfying all given premises and constraints. The formulation includes CSP and boolean SAT formulation. There are also many typical datasets such as ReClor~\cite{WeihaoReClor2020}, LogiQA~\cite{liu2021logiqa} and AR-LSAT~\cite{zhong2022analytical}.

Besides, there are some other efforts to refine the benchmark and evaluate the LLMs's logical reasoning, such as~\cite{parmar-etal-2024-logicbench,tian2021diagnosing,luo2023towards}. 

%有很多经典的benchmark dataset，比如FOLIO，Proofwriter，并且学者在不断探索新的，比如LogicBench增加了non-monotonic logic的内容，LogicNLI



%他们区分了四种逻辑推理的形式化和对应的solver，根据四类逻辑问题（问题的形式其实可以互相转换，本质是两大类，1、2是判断命题对错，3、4是能否找一个满足所有条件的可能赋值）



% LLMs的逻辑推理能力capabilities可以使用其回答复杂逻辑问题的准确性来评估。包含几类问题：（1）多选题，possible answer；（2）判断题，true，false或者unknown；（3）XXX。

% Besides, there are some other efforts to refine the benchmark and evaluate the logical reasoning capabilities of LLMs. ~\cite{parmar-etal-2024-logicbench} incorporates non-monotonic logical reasoning patterns in the evaluation process and constructed LogicBench dataset. LogicNLI~\cite{tian2021diagnosing} introduced logical relations-paradoxes, requiring models to explore at least two distinct reasoning paths leading to contradictory conclusions, which helps mitigate spurious correlations from the biased training dataset. LOGIGLUE~\cite{luo2023towards} dataset provided a brief survey on the logical reasoning capabilities of language models, which is a benchmark composed of tasks that necessitate deriving conclusions through deductive, inductive, or abductive reasoning from given information. LogicAsker~\cite{wan-etal-2024-logicasker} established a set of fundamental reasoning skills grounded in propositional and predicate logic and generated natural language examples for each skill to assess LLMs’ reasoning abilities. 

% \subsection{Other methods and variants}
% %除了上面的三种主流方法，还有一些其他的方法和相关研究，我们在本节集中讨论
% In addition to the three mainstream approaches above, there are a couple of other approaches and relevant research that we focus on in this section.

% In order to enhance the accuracy of the final answer, many studies have iteratively corrected the initial output by feed them to diverse particular  configurations or layers.   
%  Without the SAT solver, Determlm~\cite{sun-etal-2024-determlr} leads LLMs in transforming indeterminate data into increasingly determinate insights. It automates the storage and extraction of available premises and inference paths with memory.  Graph of logic~\cite{GraphofLogic} introducing a graph layer, translates NL to four types of SL as shown in ~\cite{pan-etal-2023-logic}. Then the intricate relationships and dependencies among the facts are represented by the graph structure. Besides, Some studies also incorporate graph methods to solve complex logic problems.
% LARK~\cite{GraphofLogic} formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and LLM, respectively. 



% \newpage
\section{Logical Consistency}\label{sec:3}
%%一致性是什么，讲其重要性，
%benchmark是什么：有哪些task, databaset:Constraints Source 
%%metrics和 Evaluation(metrics) 
%%具体分类哪些种类的一致性。区分事实性和逻辑性
%%Inconsistency Identification & Improvement Methods
The development of reliable large language models (LLMs) and their secure deployment is increasingly important, especially when they are used as knowledge sources. In trustworthy systems, logically consistent LLMs are essential: LLMs with logical consistency can be directly verified for correctness without requiring complicated benchmarks, thus increasing confidence in the reliability of LLM for end users in their work~\cite{calanzone2024logically,ghosh2025logical}.


Logical consistency requires LLMs not to contradict themselves, the knowledge base, or logical rules when answering different questions in the process of reasoning the complex problems~\cite{calanzone2024logically}. Sometimes, ensuring that LLMs can reason deductively without contradicting themselves is referred to as self-consistency~\cite{mitchell-etal-2022-enhancing,tafjord-etal-2022-entailer}. However, it is found that although LLMs seem to show improved ability of
comprehension and reasoning, they still fail often to generate logically consistent answers~\cite{jang-lukasiewicz-2023-consistency}. Moreover, training on large datasets for question answering alone cannot satisfy the consistency demands, as has been amply demonstrated~\cite{calanzone2024logically}. Addressing these limitations, recently there has been an influx of research to improve the logical consistency of LLMs. 

%%区分belief和fact，memory和KB。这对应着self-consistency和factual consistency。我们讨论的逻辑一致性，包括这三个方面。三个方面相辅相成，很难严格区分。并且给出两个话题分别是哪些文章在讨论，有些文章两个都讨论。不只是不能违反answers of themselves（self-consistency），还需要不能违反给定的constraint（多种逻辑规则的consistency）或者是knowledge base（factual consistency）

In this section, we will take a closer look at various logical consistencies, showing the logical relationships of one, between two up to among three entities or statements, along with different methods to enhance the logical consistency LLM, and its gold evaluation.





%分别从1、2、3人之间的关系，以及其他分类
\subsection{Negation Consistency} 
%所有文章都讨论到的是矛盾，negation问题。这是最基础的不一致性
%%%分为两方面，找contradiction，和一个命题或是词语的相反
%fact checking，simple fact
%LOGIC，例子For example, in our experiments LLaMa-2 70b (Touvron et al., 2023) answers “true” to both questions “Is an albatross an organism?” and “Is an albatross not an organism?”.
%prompt和康莱德
%rationality，异或

%被拒的，偏好序的相反
%augment，用反义词扩充数据
Logical consistency is fundamentally all about avoiding contradictions. Negation consistency, as the basics of logical consistency, can be interpreted in two ways. On the one hand, it requires that $p$ and $\neg p$ cannot hold simultaneously while one of them is true $p\oplus \neg p$, \textit{i.e.}, $(p\vee\neg p)\wedge\neg(\neg p\wedge p)$~\cite{kassner-etal-2023-language,ghosh2025logical}. On the other hand, it can be seen as an antonym in data augmentation~\cite{asai-hajishirzi-2020-logic}. LLMs are also prone to producing responses contradicting themselves across different questions. For example, in the experiments LLaMa-2 70b answers true to both questions \textit{Is an albatross an organism?} and \textit{Is an albatross not an organism?}~\cite{ghosh2025logical}.

To address this challenge,~\cite{kassner-etal-2021-beliefbank} added a memory layer, called Beliefbank, originally composed of LLM's answers to a collection of questions. Then it leverages a weighted MaxSAT solver to flip the beliefs that obviously clash with others and use a feedback mechanism to ask further questions along with relevant beliefs as the query context. Differed from the Beliefbank  constraints are hand-provided implications, ConCoRD~\cite{mitchell-etal-2022-enhancing} first generates several candidates answers to each question from the initial model. Then the pair-wise candidates in ConCoRD are constrained by both the base model and the entailment derived by natural language inference. Finally, ConCoRD utilizes the weighted MaxSaT solver to find out the most satisfactory answer candidate with the highest score. Following this line, research efforts can be summarized in this similar framework, as shown in Figure \ref{fig:figure4}.



\subsection{Implication Consistency} 
%beliefbank给了前件和约束作为context，让模型自己推理能够得到答案。比如“a swallow has gills?”. 给了 fact “a swallow is not a fish”, 和约束“鱼没有鱼鳍”
%jung 找abuduction的原因之间的关系，entailment
%康莱德 找不同问题的多个回答候选人之间的关系，entailment
%factchecking没有放implication，因为可以用DNF表示
%LOGIC包括了implications和Reverse implication consistency.但都是关于KB的
%rational，entailment规则
The implication consistency is based on the logical rule $p\to q, p \vDash q$. It means that given a constraint ``$p\to q$" and the premise $p$, it can be implicated that ``$q$ is True". If the model outputs that ``$q$ is False", then we say the answer violates implication consistency. For example, given the physical fact that \emph{All Iron (p) is metal (q)}, LLMs are not to be expected to answer \emph{True} to \emph{This material is iron (p)} and \emph{False} to \emph{This material is metal (q)} at the same time.

% given the constraint $p\to \neg q$, where $p$ and $q$ respectively represent ``Puppies are mammals" and ``Puppies have feathers", as well as the true premise ``Puppies are mammals", the question is 
%原文例子应该是否定？要和fact align

Intuitively, implication consistency is closely related to the CoT method, because each chain can be regarded as a logical implication. Failure to satisfy implication consistency will lead to inaccurate outputs obtained from these explanation-based prompting methods. To address this challenge, Maieutic Prompting~\cite{jung-etal-2022-maieutic} infers the correct answer from unreliable explanations. Unlike previous methods that generate a single explanation, Maieutic Prompting produces abductive implications supporting both "true" and "false" answers. These implications are recursively converted into logical constraints and the optimal answer is inferred using a MAX-SAT solver to avoid implication inconsistency.

 % Maieutic Prompting~\cite{jung-etal-2022-maieutic} similarly adopts the SAT solver to revise the inconsistencies among the generated explanation chains for the answers, while it propose a different way to collect the responses of LLM. This methods prompts the LLM to generate the abductive and recursive explanations according to its previous answers, which consist a reasoning tree. 

% Furthermore, the Reflex method proposes adding a rational, self-reflecting layer on top of LLMs to address the inconsistency problem~\cite{kassner-etal-2023-language}. Specifically, Reflex recursively generates model beliefs and their inferential relationships relevant to the query, forming a belief graph. Next, Reflex identifies contradictions within the belief graph and resolves them by using a formal constraint reasoner to flip the values of certain beliefs. After eliminating the contradictions, Reflex generates a consistent belief graph and derives the final answer based on it. 



 % Similarly, REFLEX~\cite{kassner-etal-2023-language} also establishes a LLM's belief graph via the backward-chaining process, which helps to extract the inferential relationships among LLM's candidate answers. In this way, it is intuitively to identify and minimize inconsistency in the graph leveraging a constraint solver.



\subsection{Transitivity Consistency} 
%康莱德，三段论式的，可以被看作传递性
%被拒的，偏好序的传递性
Transitivity represents the logical relationships among three predicates or propositions. Given two premises $p\to q$ and  $q\to r $ and it can be inferred that $p\to r$, which is regarded as the transitivity consistency. It has been shown that LLMs lack transitivity consistency. For example, a state-of-the-art Macaw question-answering (QA) model answers both \textit{Yes} to \textit{Is a sparrow a bird?} and \textit{Does a bird have feet?}, but answers \textit{No} to \textit{Does a sparrow have feet?}~\cite{mitchell-etal-2022-enhancing}. The previous two affirmative answers can implicate that \textit{A sparrow has the feet } according to the transitivity rule, which is incompatible with the answer \textit{No}.

To mitigate the transitivity inconsistency of LLMs,~\cite{asai-hajishirzi-2020-logic} leverage the logical inference rules of transitivity and symmetric, to prompt the models to generate complemental statements. For example, based on the questions \textit{If a tsunami happens, will wood be more moist?} and \textit{If wood is more moist, is more weathering occurring?} along with answers \textit{More} and \textit{More}, the implicated information \textit{If a tsunami happens, is more weathering occurring?} together the answer \textit{More} will then be augmented by the transitivity. Similarly, for the symmetric rules, the antonyms and the negation of the statements will be also added to the dataset, In this way, this method augments the training data and then a consistency-based regularizer is adopted to train the models to enhance their logical consistency containing the transitivity and symmetric.


\subsection{Factuality Consistency} 
%beliefbank，建了一个KB，第一步就是fact
%factchecking，区分了simple fact，和complex fact，第一个用DNF
%LOGIC，和KB一致

Factuality consistency refers to the degree of alignment between a model's generated responses or reasoning outcomes and real-world knowledge. In fact-checking tasks, the system evaluates the factual accuracy of model outputs by comparing them with reliable knowledge sources, while detecting potential logical errors or factual inaccuracies. When it comes to factuality consistency, there are some subtle distinctions between it and logical consistency. While logical consistency covers contradicting themselves, constraints or a knowledge base (KB), factual consistency agrees on single facts contained in a KB~\cite{calanzone2024logically}. 


To address the issue of insufficient logical consistency in existing language models under complex query scenarios, a logic consistency evaluation and improvement framework for LLMs based on Retrieval-Augmented Generation (RAG) was proposed~\cite{ghosh2025logical}. Specifically, a propositional logic-based evaluation metric is firstly designed to quantify the model's consistency in logical operations such as implication, conjunction, and disjunction. Second, by constructing three benchmark datasets based on knowledge graph (KG), it is demonstrated that the mainstream LLMs exhibit poor logical consistency in complex queries. To address this issue, this study introduces a KG-contextualized supervised fine-tuning approach to enhance the logical consistency of LLMs in complex fact-checking tasks, leveraging parameter-efficient fine-tuning methods such as QLoRA to improve efficiency.

 
% We propose a methodology to assess and improve the logical consistency of LLMs in fact-checking, ICLR 25
% %fact checking,把prompt改成了有监督微调
% %用了图KG
% We demonstrate how existing LLMs can be fine-tuned to improve their consistency via supervised fine-tuning (Radford et al., 2019; Devlin et al., 2018) (§4). We showcase that pre-train, prompt, and predict paradigm is often insufficient to improve the consistency of LLMs for complex fact-checking with KG contexts, thus we resort to pre-train, fine-tune, and predict approach. Although supervised fine-tuning is shown to be effective in other downstream tasks (Wei et al., 2021; Li  Liang, 2021), it has not yet been conducted for logical fact-checking queries over KGs.

\subsection{Compositional Consistency} 
%factchecking，logical rules（三个律），还有引入合取的DNF的真值及例子%However, upon prompting Llama2-7B with the constituent simple facts individually, we receive the response LLMppq “ 0 and LLMpqq “ 1. In this case, the LLM is inconsistent because LLMpp ^ qq “ 1 ‰ 0 “ LLMppq ^ LLMpqq.

%偏好序的三个，交换性
%LOGIC，complex constraints，合取等复杂公式，不只是DNF

%rationality： entail rule，异或规则；MC规则（至少选一个，不得多于一个）
Compositional consistency refers to the ability to maintain overall logical consistency when combining multiple facts or logical constraints. Specifically, when a model needs to combine independent facts into a complex chain of reasoning by logical operators (e.g., implication, conjunction, etc.), it should ensure that each step of the derivation conforms to the logical rules, and makes final conclusions self-consistent and are logically correct under contradictions. This ability requires the model to not only understand the meaning of individual fact, but also to correctly capture the logical relationships when they are combined, avoiding inference errors. LOCO-LMS~\cite{calanzone2024logically} is proposed to ensure the compositional consistency via neuro-symbolic integration, which makes factual and logically (self-)consistent by fine-tuning a base LLM according to a knowledge base of facts and rules. The constraints that can be arbitrary propositional logic formulas are compiled into a circuit and then used to encourage the model to allocate non-zero probability only to factual and consistent facts. 
In addition,~\cite{liu2024aligning} proposed a universal framework to quantify the compositional logical consistency via three fundamental proxies: transitivity, commutativity, and negation invariance.




% \subsection{Benchmark and Evaluation}
% Evaluating superhuman models with consistency checks~\cite{fluri2024evaluating}: even when we cannot evaluate the correctness of LLM decisions, we can evaluate their logical consistency; demonstrated that GPT-4 and GPT-3.5-turbo, when asked one-sentence forecasting questions, were inconsistent on simple logical checks such as negation.

% Consistency Checks for Language Model Forecasters~\cite{daniel2025check}

\subsection{Evaluation}
%%%[举例子]
%% A collection of “facts”：true or false（实体和谓词）
% F1（为什么？As in Kassner et al. (2021); we use F1 for evaluation because gold answers are highly biased (这个词换掉) towards true No. answers.）
%% 谓词间的一个有向图，表示first-order logical formulae
% consistency of questions using the complement of Li et al.(2019)’s conditional constraint violation metric $\tau$
%%beliefbank: belief 是（带上置信度的）一个句子的真假，constraint是（带上置信度的）两个句子的蕴涵关系，用前后件的真值来表示整个蕴涵式的真假
\cite{kassner-etal-2021-beliefbank} introduce a gold standard approach to evaluate the accuracy and consistency of a set of beliefs. Following work including ~\cite{mitchell-etal-2022-enhancing}, ~\cite{kassner-etal-2023-language} adopt this typical evaluation approach. We give a simple introduction and borrow the symbolic language in ~\cite{mitchell-etal-2022-enhancing}. It is defined as:
\begin{itemize}
    \item A set of entities $s_m \in S$;
    \item A set of unary predicates $P_n \in P$;
    \item A collection of "facts" $\left(P_n\left(s_m\right)\right)_i$, whose binary truth value is known;
    \item A directed graph of gold-standard constraints $G(P, E)$, whose edges $\left(P_n, P_{n^{\prime}}\right) \in$ $E$ represent first-order logical formulae $\forall x\left(P_n(x) \rightarrow P_{n^{\prime}}(x)\right)$.
\end{itemize}

%This  formulation can be well applied to represent natural language sentences. For example, for fact $P_n\left(s_m\right)$, if entity $s_m$ represents a student and predicate $P_n$ represents an ability to play piano, then the corresponding question answer pair ( $q_i, a_i$ ) means the question \textit{ $Q$: Is it true that a student is able to play piano?} together with the answer \textit{$A$: Yes.}

To assess accuracy by using binary F1, comparing elements \( z_i \) of the configuration \( Z \), which maximizes \( \phi(Z) \) and the truth values of facts \( P_n(s_m) \)~\cite{mitchell-etal-2022-enhancing}. In this way, F1 for evaluation since the gold answers tend to be heavily biased towards the true "No" answers.

Regarding measuring the self-consistency, Beliefbank~\cite{kassner-etal-2021-beliefbank}, ConCoRD~\cite{mitchell-etal-2022-enhancing} and \cite{kassner-etal-2023-language} all apply the complement of conditional constraint violation $\tau$ metric by ~\cite{li2019logic}, which is defined as the ratio of relevant gold constraints in \( G \) that are violated.  A constraint \( \forall x(P_n(x) \rightarrow P_{n'}(x)) \) is considered relevant or applicable means, for certain entity \( s_m \), there exists a belief \( b_i \in B_{s_m} \) from the fact \( P_n(s_m) \) satisfying \( z_i = 1 \), and there is a corresponding belief \( b_j \in B_{s_m} \) from the fact \( P_n(s_m) \) such that when \( z_j = 0 \), the constraint is violated. 

\section{Future Research Directions}
\subsection{Extension to Conditional and Modal Reasoning}
Despite state-of-the-art approaches greatly enhance the predicate logic reasoning ability of LLMs, there is still a lack of exploration into the more complicated and challenging conditional and modal reasoning abilities. Specifically, sentences
 involving conditional reasoning can be written in the form of \emph{\textbf{If} $\ldots$, \textbf{then} $\ldots$}. Conditional reasoning also has strict logical rules that should be followed. For example, \emph{\textbf{If} the match is struck (p), \textbf{then} it will light (q)}
implies
\emph{\textbf{If} the match is struck (p) and has been soaked in water (r), \textbf{then} it will light (q)}, which can be formally formulated as $p \rightarrow q \vDash(p \wedge r) \rightarrow q$. In addition, modal logic extends predicate logic by further incorporating \emph{\textbf{must}} and \emph{\textbf{may}} to account for the certainty and possibility, respectively. For example, \emph{Mary \textbf{might} ($\Diamond$) not ($\neg$) have been at the wedding (p)} implies \emph{It’s not ($\neg$) the case that Mary \textbf{must} ($\square$) have been at the wedding (p)}, which can be formally formulated as $\Diamond \neg p \vDash \neg \square p$. Despite expanding to conditional and modal logic reasoning can significantly increase the applicability of LLMs, recent work shows almost all LLMs make some basic mistakes with conditionals or modals, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals~\cite{holliday-etal-2024-conditional}. Therefore, it is still worthwhile to explore new methods to make LLM have the ability to perform reasoning on conditional and uncertain events.


% Some work evaluates and discusses more logical inference patterns or interpretability. Holiday et al. \cite{holliday-etal-2024-conditional} focus on inference patterns involving conditionals and epistemic modals (\textit{must }and \textit{might}). The assessment shows that the best-performing LLMs also suffer from basic errors and logically inconsistent choices across logical inference patterns containing modal operators and conditionals.  To enhance the interpretability of the solver-based approach, Yang et al.~\cite{yang2023neuro} prompt LLM translates the NL problem into Prolog language, and then presents the proof process of solvers with the Prolog interpreter. Moreover, Symbol-LLM~\cite{xu-etal-2024-symbol} and LLM-TRes~\cite{toroghi-etal-2024-verifiable} discussed approaches to solving more reasoning challenges, including the logical reasoning problem.

\subsection{Higher-Order Logical Reasoning of LLMs}
Compared to first-order logic, higher-order logic allows for reasoning about properties and functions, enabling more complex statements and proofs. First-order logic only quantifies over individual variables, thus can only perform reasoning about properties of objects, \emph{not} properties of properties. In contrast, higher-order logic allows quantification over sets, functions, and predicates, enabling statements about properties of properties and relationships between functions. For example, a simple first-order logic statement is \emph{All cats are mammals}, which can be formulated as $\forall x (\operatorname{Cat}(x) \to \operatorname{Mammal}(x))$. A related higher-order logical reasoning example would be \emph{There is a property that all cats have, such that any animal with that property is a mammal}, which can be formulated as $\exists P \forall x(\operatorname{Cat}(x) \to P(x)) \wedge \forall y(P(y) \to \operatorname{Mammal}(y))$. This demonstrates the ability to quantify over properties (like $P$, which can be considered a set of objects) instead of just individual objects, allowing for more complex expressions and reasoning on LLMs.

\subsection{Efficient Algorithms Satisfying Multiple Logical Consistencies}
Despite many methods having been proposed to enhance various types of logical consistency of LLMs, there are still two critical challenges. On one hand, most methods only apply to enhancing a specific type of logical consistency, rather than satisfying multiple logical consistencies simultaneously. For example, as a data augmentation approach, logic-guided data augmentation~\cite{asai-hajishirzi-2020-logic} only simulates reverse samples and transitive logical rules to enhance negation consistency and transitivity consistency of LLMs, respectively. Nonetheless, improving a specific type of logical consistency does not necessarily improve other types of logical consistency. On the other hand, enumerating all possible combinations of answers to all questions to verify logical consistencies of LLMs would cost exponential space storage and unexpectedly large computational overhead. Taking the transitivity consistency checking as an example, which states that $p\to q$ and  $q\to r $ and infer $p\to r$, one need to determine the answer to each of the three associated questions, which requires time complexity $\mathcal{O}(n^3)$. 
Therefore, it is essential to develop more efficient methods that can simultaneously satisfy the various logical consistencies of LLM.



\section{Conclusion}
In summary, this survey provides a comprehensive overview of the current state of the art in logical reasoning abilities of LLMs. Despite impressive advances in most natural language tasks, LLMs continue to face significant challenges in their logical reasoning abilities, particularly in the areas of logical question answering and logical consistency. Through a comprehensive taxonomy, we classify cutting-edge approaches to tackling these challenges, highlighting logical question answering based on external solvers or implemented through prompting, pretraining, and fine-tuning, as well as a variety of logical consistency concepts and solutions, including negation, implication, transitivity, factuality consistencies, and their composites.
In addition, we review benchmark datasets and evaluation metrics used in this area that are critical for assessing LLM performance in logical reasoning tasks. 
Looking ahead, promising research directions include extending the logical reasoning abilities of LLMs to modal logic to tackle questions with uncertainty and developing efficient algorithms that can satisfy multiple logical consistencies simultaneously. These advances are critical to improving the reliability and robustness of LLMs in applications that require precise logical reasoning, and will ultimately close the gap between the current abilities of LLMs and the demands of complex reasoning in real-world scenarios.

% \subsection{Logical Question Answering}
% \subsubsection{Solver-based}
% \begin{itemize}
%     \item Faithful~\cite{lyu2023faithfulCoT}  
%     \item Satlm~\cite{ye2023satlm}
%     \item LOGIC-LM~\cite{pan-etal-2023-logic}
%     \item LINC~\cite{olausson-etal-2023-linc}
% \end{itemize}

% \subsubsection{Prompt-based}
% \begin{itemize}
%     \item CR~\cite{zhang2023cumulative}
%     \item DoT~\cite{zhang2024diagram}

%      \item SymbCoT~\cite{xu-etal-2024-faithful}  
%     \item Logic-of-thought~\cite{liu2024logicofthoughtinjectinglogiccontexts}
%      \item LINA~\cite{li2024leveraging}  

% \end{itemize}


% \subsubsection{Pretrain and fine-tuning}
% \begin{itemize}
%  \item LReasoner~\cite{wang-etal-2022-logic}, 
% \item  LOGIPT~\cite{feng-2024-languagecanbe}
%   \item ALT~\cite{morishitaenhancing}
%    \item  LogicAsker~\cite{wan-etal-2024-logicasker}
%  \item LogicLLM~\cite{jiao2024exploring}
% \end{itemize}

% \subsubsection{Benchmark}

%     \begin{itemize}
%      \item LogicBench~\cite{parmar-etal-2024-logicbench}
%     \item LogicNLI ~\cite{tian2021diagnosing}
% \item LOGIGLUE \cite{luo2023towards} 
% \item  LogicAsker~\cite{wan-etal-2024-logicasker}
%     \end{itemize}

















\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

