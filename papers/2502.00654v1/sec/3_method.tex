\section{Method}
\label{sec:method}



% Our goal is to control the overall facial expression of a 3D talking head, including emotions, eye blinking, eyebrow movements and lip movements, by conditioning on the continuous valence/arousal values, action units and speech signals. Towards this goal, w

We propose EmoTalkingGaussian, which renders emotional talking heads using 3D Gaussian splatting method, conditioned on valence/arousal values, action units, and audio input: We first employ TalkingGaussian~\cite{li2024talkinggaussian} pipeline to synthesize 3D Gaussian splatting-based talking heads conditioned on audio and action units. Then, we generate lip-aligned emotional face images to effectively train the emotion manipulator of EmoTalkingGaussian with diverse emotional facial images. This simple method leads us to synthesize talking heads reflecting diverse valence/arousal conditions, though the rendering quality slightly diminishes. Furthermore, the lip becomes unsynchronized specifically when conditioned on unseen audio. To relieve the challenges, we improve rendering quality by involving a normal map loss and enhance lip synchronization by employing a sync loss which enforces consistency between the image rendered by EmoTalkingGaussian and the input audio generated by a text-to-speech network~\cite{gTTS}. Our EmoTalkingGaussian framework is shown in Fig.~\ref{fig:pipeline}.

% In this loss, we additionally render normal maps from the 3D Gaussian and enforce its consistency to the normal maps estimated from lip-synchronized emotional images that we generated. Second, we further augment the data to further synchronize lips to unseen speech signals. 

% We involve the text-to-speech (TTS) method~\cite{gTTS} to generate diverse speech signals and input them to our system and enforce the consistency between the input speech signal and the generated images.

%, we decompose the task into two branches: the inside mouth branch and the face branch. The inside mouth branch of the network is conditioned solely on the speech input, ensuring that the inside of the mouth are synchronized with the speech. In contrast, the face branch of the network is conditioned on the emotion values, the speech, and the action units, allowing for precise control of the facial expressions based on the desired emotional state and the speech.


\subsection{3D Gaussian Splatting}
%\noindent \textbf{3D Gaussian Splatting.} 
3D Gaussian splatting~\cite{kerbl20233d} utilizes a set of 3D Gaussians, which are represented by a position $\mu$, a scaling factor $s$, a rotation quaternion $q$, an opacity value $\alpha$, and a color $c$, to describe a 3D scene. In the point-based rendering, at pixel $\mbx_p$, the color $C(\mbx_p)$ and the opacity $\calA(\mbx_p)$ are calculated based on the contributions of a set of 
$N$ Gaussians as follows:
\begin{eqnarray}
    C(\mbx_p) &=& \sum_{i \in N} c_i \tilde{\alpha}_i \prod^{i-1}_{j=1} (1-\tilde{\alpha}_j),\label{eq:gaussian rendering1}\\
    \calA(\mbx_p) &=& \sum_{i \in N} \tilde{\alpha}_i \prod^{i-1}_{j=1} (1-\tilde{\alpha}_j),
    \label{eq:gaussian rendering2}
\end{eqnarray}
where $ \tilde{\alpha}_i = \alpha_i \calG^{proj}_i(\mbx_p), \quad
\calG_i(\mbx) = e^{-\frac{1}{2}(\mbx-\mu_i)^T\Sigma^{-1}_i(\mbx-\mu_i)}$.
% \begin{eqnarray}
%     \Tilde{\alpha}_i = \alpha_i \calG^{proj}_i(\mbx_p), \quad
%     \calG_i(\mbx) = e^{-\frac{1}{2}(\mbx-\mu_i)^T\Sigma^{-1}_i(\mbx-\mu_i)},
%     \label{eq:gaussian rendering3}
% \end{eqnarray}
A covariance matrix $\Sigma_i$ is derived from $s_i$ and $q_i$, and a 2D Gaussian $\calG^{proj}_i$ is the projection of a 3D Gaussian $\calG_i$ onto the image plane. During optimization, 3DGS updates parameters $\theta = \{ \mu, s, q, \alpha, c \}$, and applies both densification and pruning of the 3D Gaussians to find the appropriate number of Gaussians for accurately representing the scene.

Following GaussianShader~\cite{jiang2024gaussianshader}, which proposed a method for rendering normal maps directly from 3D Gaussians, we add a normal residual $\Delta \mbn$ to the 3D Gaussian parameters for the face region, defined as $\theta = \{ \mu, s, q, \alpha, c, \Delta \mbn \}$. This residual refines the normal direction to improve the quality of the rendered normal maps. We then use the rendered normal map to apply the normal map loss.


% Additionally, we render the normal map from the 3D Gaussian for use in the normal consistency loss of EmoTalkingGaussian. GaussianShader~\cite{jiang2024gaussianshader} proposed a method for rendering normal maps directly from 3D Gaussians. In this process, a normal residual $\Delta \mbn$ is required to refine the normal direction, thus it is added to the 3D Gaussian parameters for the face region, defined as $\theta = \{ \mu, s, q, \alpha, c, \Delta \mbn \}$.

\subsection{EmoTalkingGaussian} 
We propose the EmoTalkingGaussian that synthesizes talking heads conditioned on the input audio as well as the continuous emotion and expression values, \ie, valence/arousal and action units. We employ TalkingGaussian~\cite{li2024talkinggaussian} pipeline as our baseline architecture for synthesizing the talking head. It separately models an inside-mouth region and a face region using two distinct persistent Gaussian fields. These fields remain stable and preserve the geometry of the face while allowing dynamic deformations based on input audio features $\mba$ extracted by DeepSpeech~\cite{hannun2014deep} and upper-face action units $\mbu$ extracted by OpenFace~\cite{Baltrusaitis2018openface}. To enable precise control over the deformation of the Gaussians, offsets are calculated using a tri-plane hash encoder $H$~\cite{li2023efficient}, which allows accurate adjustments to the Gaussians' parameters.

For the inside-mouth region, the offset $\delta^m_i=\{ \Delta \mu_i \}$ is estimated via the inside-mouth region manipulation network $f^\text{M}$ conditioned only on $\mba$ as follows:
\begin{eqnarray}
    \delta^m_i = f^\text{M}(H^\text{M}(\mu_i) \oplus \mba), 
    \label{eq:mouth mlp}
\end{eqnarray}
where $\mu_i$ denotes the position of the canonical Gaussian $\theta^\text{M}_C$, and $H^\text{M}(\cdot)$ is the tri-plane hash encoder for the inside-mouth. The inside-mouth deformed Guassians are represented as: $\theta^\text{M}=\theta^\text{M}_C+\delta^m=\{\mu+\Delta \mu, s, q, \alpha, c\}$

For the face region, the offset $\delta^{\mbu}_i=\{\Delta \mu^{\mbu}_i,\Delta s^{\mbu}_i, \Delta q^{\mbu}_i \}$ for each Gaussian is estimated using both $\mba$ and $\mbu$ through the face region manipulation network $f^\text{F}$ as follows:
\begin{eqnarray}
    \delta^{\mbu}_i = f^\text{F}(H^\text{F}(\mu_i) \oplus \mba_{r,i} \oplus \mbu_{r,i}),
    \label{eq:face mlp}
\end{eqnarray}
where $\mba_{r,i}=A_{\mba,i} \odot \mba$ and $\mbu_{r,i}=A_{\mbu,i} \odot \mbu$ represent the region-aware features at position $\mu_i$, and the attention maps $A_{\mba, i}$ and $A_{\mbu, i}$ are derived from $\mba$ and $\mbu$, respectively. $\odot$ and $\oplus$ denote Hadamard product and concatenation, respectively. $\mu_i$ is the position of the canonical Gaussian $\theta^\textbf{F}_C$, and $H^\text{F}(\cdot)$ is the tri-plane hash encoder for the face. The face deformed Gaussians are represented as: $\theta^\text{F}=\theta^\text{F}_C + \delta^{\mbu}=\{\mu+\Delta \mu^{\mbu}, s+\Delta s^{\mbu}, q+\Delta q^{\mbu}, \alpha, c, \Delta \mbn \}$.

% Inspired by Li~\etal~\cite{li2024talkinggaussian}, we decompose the generation branch into the inside mouth branch and the face branch. 
We introduce the emotion branch to manipulate the facial emotion based on continuous valence and arousal values $\mbe$. This allows the emotion manipulation network $f^\text{E}$ to estimate the offset $\delta^{\mbe}_i = \{ \Delta \mu^{\mbe}_i, \Delta s^{\mbe}_i, \Delta q^{\mbe}_i \}$ that aligns with the desired emotion.
% This allows the emotion manipulation network $f^\text{E}$ to generate the appropriate position offset $\Delta \mu^{\mbe}_i$, scaling factor offset $\Delta s^{\mbe}_i$, and rotation quaternion offset $\Delta q^{\mbe}_i$ that align with the desired emotion.
% However, to manipulate the facial emotion, we modify the MLP in face branch so that it can receive not only the audio $\mba$ and action units $\mbe$ but also the valance and arousal values $\mbm$. This allows the network to generate the appropriate position offset $\Delta \mu_i$, scaling factor offset $\Delta s_i$, and rotation quaternion offset $\Delta q_i$ that align with the desired emotion.
\begin{eqnarray}
    \delta^{\mbe}_i = f^\text{E}(H^\text{E}(\mu_i+\Delta \mu^{\mbu}_i) \oplus \mbe_{r,i}),
    \label{eq:emotion mlp}
\end{eqnarray}
where $\mbe_{r,i}=A_{\mbe,i} \odot \mbe$ represents the region-aware features at position $\mu_i+\Delta \mu^{\mbu}_i$ of the deformed Gaussian $\theta^\text{F}$, and $H^\text{E}(\cdot)$ is the tri-plane hash encoder for the emotion. The attention map $A_{\mbe, i}$ is derived from $\mbe$, and $\odot$ and $\oplus$ denote Hadamard product and concatenation, respectively. The emotional deformed Gaussians are expressed as: $\theta^\text{E}=\theta^\text{F} + \delta^{\mbe}=\{\mu+\Delta \mu^{\mbu}+\Delta \mu^{\mbe}, s+\Delta s^{\mbu}+\Delta s^{\mbe}, q+\Delta q^{\mbu}+\Delta q^{\mbe}, \alpha, c, \Delta \mbn \}$.

% The final rendering color combines outputs from both face and in-mouth regions, using Eqs.~\ref{eq:gaussian rendering1}, \ref{eq:gaussian rendering2}, \ref{eq:gaussian rendering3}, and \ref{eq:gaussian rendering4}, as follows:
% \begin{eqnarray}
%     C_{\text{head}}(\mbx_p) &=& C_{\text{face}}(\mbx_p) \times \calA_{\text{face}}(\mbx_p) \nonumber\\
%     &+& C_{\text{mouth}}(\mbx_p) \times (1 - \calA_{\text{face}}(\mbx_p))
%     \label{eq:fused image}
% \end{eqnarray}
% where the face and mouth contributions are blended based on their opacities.

\subsection{Synthetic Image and Audio Augmentation}
When training the EmoTalkingGaussian using the provided personal speech video, the emotion manipulation network $f^\text{E}$ is not able to properly model the emotions for given subjects. Furthermore, the speech audio data is also limited. To relieve the challenges of properly training $f^\text{E}$, the face region manipulation network $f^\text{F}$ and the inside-mouth region manipulation $f^\text{M}$ with rich emotional and speech variations, we involve the synthetic images and audio. Especially, we augment the subject-specific emotional face image by involving a lip-aligned emotional face generator. Also, we use the text-to-speech network~\cite{gTTS} to synthesize new speech audio.
% When training the EmoTalkingGaussian, the face region manipulation network $f^\text{F}$ and the inside-mouth region manipulation network $f^\text{M}$ are mainly trained using the given videos with speech signals. However, the original video signals do not contain much emotional variations and thus, $f^\text{F}$ is not able to properly model the emotions for given subjects. Furthermore, the speech signals are also limited. To relieve the challenges to properly train $f^\text{F}$ and $f^\text{M}$ with rich emotional and speech variations, we involved the synthetic images and audios. Especially, we augment the subject-specific emotional face image by involving lip-algined emotional face generator, as shown in Fig.~\ref{fig:synthetic data}. Also, we use the text-to-speech (TTS) generator~\cite{gTTS} to synthesize new speech signals.

\subsubsection{Lip-aligned Emotional Face Generator} 
\label{sec:lip-aligned emotional face generator}
% Our lip-aligned emotional face generation network $g^\text{LEF}$ needs to generate facial images that reflect both the valence/arousal $\mbe$ as well as lip synchronization to speech signals $\mba$, so that we can further augment the training of $f^\text{E}$ with images displaying diverse emotional expressions and precise lip alignment.
% \textcolor{blue}{which is conditioned on both $\mbe$ and $\mba$.}

To obtain the lip-aligned emotional face generation network $g^\text{LEF}$, we initially adopt the framework of EmoStyle~\cite{azari2024emostyle} that is able to adjust the facial emotions in an input image $I$ conditioned on the valence and arousal $\mbe$, while preserving background, identity and head pose. However, during the adjustment, EmoStyle~\cite{azari2024emostyle} also transforms lips to excessively represent the emotions. 
% This hinders its usage in the talking head synthesis task, since it is essential to ensure the alignment between the lip and the speech. 

To prevent this, we extend EmoStyle~\cite{azari2024emostyle} to be additionally conditioned on the lip landmarks of $I$, ensuring that the lip shape on the generated image closely matches that of $I$. We extract lip heatmaps $H$ from $I$ using the off-the-shelf landmark detector $D_l$~\cite{bulat2017far}. The heatmaps $H$ are then processed by a 2D convolutional encoder $E$, which outputs a lip embedding vector $z_l$. We concatenate the lip embedding vector $z_l$ with an emotional latent code $\calW'$ generated by the original EmoStyle. We then introduce the LipExtract module $M_\text{lip}$ to further process the combined embedding. The LipExtract module outputs a lip modification vector $d_l$, which is added to $\calW'$, resulting in a lip-aligned emotional latent code $\calW''$. This process is expressed as follows:
\begin{eqnarray}
    \calW'' = \calW' + d_l, \ \ d_l = M_\text{lip}(z_l \oplus \calW'), \nonumber \\
    \calW' = EmoStyle(I), \ \ z_l = E(H), \ \ H = D_l(I), 
\end{eqnarray}
where $\oplus$ means concatenation. StyleGAN2~\cite{karras2020analyzing} uses $\calW''$ to generate the synthetic emotional image $I^\text{E}$, aligning the lips with those in $I$ while expressing the desired emotion.

% To prevent this, we extended the framework of EmoStyle~\cite{azari2024emostyle} to be additionally conditioned on the lip landmarks. We extracted heatmaps of the lip $H \in \R^{20 \times 64 \times 64}$ from $I$ using the off-the-shelf landmark detector~\cite{bulat2017far}. These heatmaps are then processed by a 2D convolutional encoder $E$, which outputs a lip embedding vector $e_l \in \R^{64}$. We concatenate $e_l$ with an emotional latent code $\calW' \in \R^{512}$ generated by EmoStyle and we proposed the LipExtract module to further process it. The LipExtract module outputs a lip modification vector $d_l \in \R^{512}$ and it is added to $\calW'$, resulting in a lip-aligned latent code $\calW''$. Then, StyleGANv2~\cite{karras2020analyzing} uses $\calW''$ to generate the final image $\hat{I}$, ensuring that the lip is properly aligned with $I$ while expressing the desired emotion.

We utilize the following loss functions to train the encoder $E$ and the LipExtract module $M_\text{lip}$, and fine-tune the StyleGAN2~\cite{karras2020analyzing}, while freezing other components of Emostyle~\cite{azari2024emostyle}:
\begin{eqnarray}
    \calL = \lambda_1 \cdot \calL_{ll} + \lambda_2 \cdot \calL_{lp} + \lambda_3 \cdot \calL_{reg} + \lambda_4 \cdot \calL_{emo} + \lambda_5 \cdot \calL_{id},
\end{eqnarray}
where lip landmark loss $\calL_{ll}$, lip pixel loss $\calL_{lp}$, and regularization loss $\calL_{reg}$ are defined as follows:
\begin{eqnarray}
    \calL_{ll} = ||\hat{L}_l-L_l||^2_2, \ \ \calL_{lp} = ||\mathbb{M}_l \odot (I^\text{E}-I)||^2_2, \ \ \calL_{reg} = || d_l ||^2_2,
\end{eqnarray}
where $L_l$ and $\hat{L}_l$ represent the lip landmarks estimated from the input image $I$ and the output image $I^\text{E}$ using the landmark detector~\cite{bulat2017far}, respectively. $\mathbb{M}_l$ denotes a rectangle mask created from the lip landmarks $L_l$. The losses $\calL_{ll}$ and $\calL_{lp}$ ensure that the lips in $I$ and $I^\text{E}$ are aligned. The regularization loss $\calL_{reg}$ prevents $d_l$ from diverging. Additionally, the emotion loss $\calL_{emo}$~\cite{azari2024emostyle} guarantees that $I^\text{E}$ reflects the desired emotion, while the identity loss $\calL_{id}$~\cite{azari2024emostyle} ensures that $I^\text{E}$ preserves the identity of $I$. $\lambda_i$ is the weight of each loss term.

\subsubsection{TTS-based Speech Audio Generator}
\label{sec:audio generator}
To enhance the generalizability of lip sync, we employ ChatGPT~\cite{ChatGPT} and a text-to-speech algorithm~\cite{gTTS} to generate curated speech audio data. Specifically, we prompt ChatGPT to create 10 text samples that cover a broad range of English phonetic variations, including essential phonemes and various pronunciation phenomena. For instance, the sentence ``The quick brown fox jumps over the lazy dog" includes most English consonants and vowels, providing comprehensive phoneme coverage. 
% Another example, ``She's going to buy some new clothes at the mall," showcases diphthongs and weakened forms, with "going to" often contracted to "gonna" in casual speech. 
% Additionally, the phrase ``Better late than never, they say" illustrates flapping in American English, where "better" sounds like \textipa{/b\textepsilon \textfishhookr \textschwa r/}.

By converting these texts into speech audio using the text-to-speech network~\cite{gTTS}, we perform self-supervised learning to train EmoTalkingGaussian on a diverse set of speech variations. To do this, we apply a sync loss 
$\calL_\text{sync}$, which calculates the L2 loss between audio features and image features using SyncNet~\cite{chung2017out} as follows:
\begin{eqnarray}
    \calL_\text{sync} = || S_\text{I} (\hat{I}) - S_\text{A} (A) ||^2_2, 
    \label{eq:sync_loss}
\end{eqnarray}
where $S_\text{I}$ and $S_\text{A}$ denote the image encoder and audio encoder of SyncNet. $A$ represents the audio input, and $\hat{I}$ denotes the image rendered by EmoTalkingGaussian, conditioned on $A$. This loss enhances synchronization accuracy, enabling our model to be trained on additional speech audio data without paired RGB video, thus allowing for greater flexibility in data use.

\begin{table*}[t]
    \centering
    \begin{tabular}{l|ccccccc}
    \hline
    Method & PSNR ($\uparrow$) & SSIM ($\uparrow$) & LPIPS ($\downarrow$) & LMD ($\downarrow$) & Sync-E($\downarrow$)/C($\uparrow$) & AUE-U($\downarrow$)/L($\downarrow$) 
    & FPS\\
    \hline
    Ground truth & - & 1 & 0 & 0 & 6.546/7.827 & 0/0 & - \\
    \hline
    % EAT~\cite{}     &  &  &  &  &  &  \\
    % PD-FGC~\cite{}     &  & & &  &  & \\
    % \hline
    % GeneFace~\cite{ye2023geneface} & 31.13 & 0.926 & 0.0561 & 3.120 & 8.732/5.196 & 1.422/1.030 & 23 \\
    ER-NeRF~\cite{li2023efficient} & 33.06 & 0.935 & 0.0274 & 3.110 & 8.443/5.554 & 0.779/0.565 & 34 \\
    GaussianTalker~\cite{cho2024gaussiantalker} & 33.02 & 0.939 & 0.0333 & 3.206 &  8.554/5.741 & 0.766/0.523 & \textbf{121} \\
    TalkingGaussian~\cite{li2024talkinggaussian} & 33.64 & 0.940 & \underline{0.0256} & \underline{2.610} & 8.129/5.919 & 0.279/0.550 & \underline{108} \\
    \hline
    Ours w/o Emo. branch & \textbf{33.87} & \textbf{0.944} & \textbf{0.0255} & \textbf{2.557} & \underline{7.750}/\underline{6.270} & \textbf{0.207}/\textbf{0.515} & 107 \\
    Ours & \underline{33.78} & \underline{0.943} & 0.0267 & 2.638 & \textbf{7.702}/\textbf{6.279} & \underline{0.278}/\underline{0.520} & 101 \\
    \hline
    \end{tabular}
    \caption{We compare quantitative results for self-reconstruction scenario. We highlight the best results in \textbf{bold} and the second-best in \underline{underline}. ``Ours w/o Emo. branch" denotes our method without the emotion branch.}
    \label{tab:scenario 1}
    \vspace{-3mm}
\end{table*}

\subsection{Training}
We independently train each branch of EmoTalkingGaussian (inside-mouth, face, and emotion).
\subsubsection{Optimizing Canonical Gaussians}
We optimize the inside-mouth canonical Gaussians $\theta^\text{M}_C$ and the face canonical Gaussians $\theta^\text{F}_C$ through the L1 loss $L_1$ and D-SSIM loss $L_\text{D-SSIM}$:
\begin{eqnarray}
    \calL_\text{rgb} = \calL_1 (\hat{I}_C, I_\text{mask}) + \gamma_1 \calL_\text{D-SSIM} (\hat{I}_C, I_\text{mask}),
    \label{eq:rgb_loss}
\end{eqnarray}
% \begin{eqnarray}
%     L_\text{rgb} = L_1 (\hat{I}_C, I_\text{mask}) &+& \gamma_1 L_\text{D-SSIM} (\hat{I}_C, I_\text{mask}) \nonumber \\
%     &+& \gamma_2 L_\text{LPIPS}(\hat{I}_C, I_\text{mask}),
%     \label{eq:rgb_loss}
% \end{eqnarray}
where $\hat{I}_C$ represents the image rendered from either $\theta^\text{M}_C$ or $\theta^\text{F}_C$, and $I_\text{mask}$ denotes the masked ground truth for either the inside-mouth region or the face region, where the mask is extracted from the ground truth $I$ following \cite{li2024talkinggaussian}. For optimizing $\theta^\text{F}_C$, the normal map loss is additionally applied to update the positions $\mu$ and the normal residuals $\Delta \mbn$ as follows: 
\begin{eqnarray}
    \calL_\text{normal} = \gamma_2 \calL_1 (\hat{N}^\text{F}_C, N^\text{F}_\text{mask}) + \gamma_3 \calL_\text{tv}(\hat{N}^\text{F}_C) 
    + \gamma_4 || \Delta \mbn ||^2_2,
    \label{eq:normal_loss}
\end{eqnarray}
where $\hat{N}^\text{F}_C$ represents the normal map rendered from 3D Gaussians $\theta^\text{F}_C$, while $N^\text{F}_\text{mask}$ is the masked normal map extracted by the predictor~\cite{Abrevaya_2020_CVPR}. $\calL_\text{tv}$ denotes the total variation loss used to enforce the spatial smoothness in the rendered normal map, and the regularization loss ensures that $\Delta \mbn$ does not diverge. $\gamma_i$ is the weight of each loss term.

\subsubsection{Training Networks for Inside-Mouth and Face Regions}
\label{sec:training network}
We train the tri-plane hash encoder $H$ (inside-mouth $H^\text{M}$ and face $H^\text{F}$) and manipulation network $f$ (inside-mouth $f^\text{M}$ and face $f^\text{F}$) with the following loss function:
\begin{eqnarray}
    \calL = \calL_\text{rgb} + \calL_\text{normal} + \calL_\text{sync},
\end{eqnarray}
where
\begin{eqnarray}
    \calL_\text{rgb} &=& \calL_1 (\hat{I}, I_\text{mask}) + \beta_1 \calL_\text{D-SSIM} (\hat{I}, I_\text{mask}) \nonumber \\
    &+& \beta_2 \calL_\text{LPIPS}(\hat{I}, I_\text{mask}), \\
    \calL_\text{normal} &=& \beta_3 \calL_1 (\hat{N}^\text{F}, N^\text{F}_\text{mask}) + \beta_4 \calL_\text{tv}(\hat{N}^\text{F}) \nonumber\\ 
    &+& \beta_5 || \Delta \mbn ||^2_2,
\end{eqnarray}
where $\hat{I}$ is rendered from either $\theta^\text{M}$ or $\theta^\text{F}$, and $\hat{N}^\text{F}$ is rendered from $\theta^\text{F}$. $\calL_\text{LPIPS}$ denotes the LPIPS loss. Furthermore, we apply a sync loss $\calL_\text{sync}$, defined in Eq.~\ref{eq:sync_loss}, using both original and synthesized audio data to improve lip synchronization accuracy. $\beta_i$ is the weight of each loss term.

% losses $L_\text{rgb}$ and $L_\text{normal}$, defined in Eqs.~\ref{eq:rgb_loss} and \ref{eq:normal_loss}, respectively. This involves replacing $\hat{I}_C$ with $\hat{I}$, which is rendered from $\theta^\text{M}$ or $\theta^\text{F}$, and substituting $\hat{N}^\text{F}_C$ with $\hat{N}^\text{F}$, rendered from $\theta^\text{F}$. Furthermore, we apply a sync loss $L_\text{sync}$, defined in Eq.~\ref{eq:sync_loss}, using both original and synthesized speech data to improve synchronization accuracy.

\subsubsection{Training Network for Emotion}
We train the emotion tri-plane hash encoder $H^\text{E}$ and the emotion manipulation network $f^\text{E}$ with the following loss function:
\begin{eqnarray}
    \calL = \calL_\text{rgb} + \calL_\text{normal} + \calL_\text{sync},
\end{eqnarray}
where
\begin{eqnarray}
    \calL_\text{rgb} &=& \calL_1 (\hat{I}^\text{E}, I_\text{mask}) + \kappa_1 \calL_\text{D-SSIM} (\hat{I}^\text{E}, I_\text{mask}) \nonumber \\
    &+& \kappa_2 \calL_\text{LPIPS}(\hat{I}^\text{E}, I_\text{mask}), \\
    \calL_\text{normal} &=& \kappa_3 \calL_1 (\hat{N}^\text{E}, N^\text{E}_\text{mask}) + \kappa_4 \calL_\text{tv}(\hat{N}^\text{E}) \nonumber\\ 
    &+& \kappa_5 || \Delta \mbn ||^2_2,
\end{eqnarray}
where both $\hat{I}^\text{E}$ and $\hat{N}^\text{E}$ are rendered from $\theta^\text{E}$. Additionally, we apply a sync loss $\calL_\text{sync}$, defined in Eq.~\ref{eq:sync_loss}, using both original and synthesized audio data to enhance the lip synchronization. $\kappa_i$ is the weight of each loss term. 

% We further jointly fine-tune the 3D Gaussians $\theta_C$. Please refer to the supplementary material for further details. 


% losses $L_\text{rgb}$ and $L_\text{normal}$, defined in Eqs.~\ref{eq:rgb_loss} and \ref{eq:normal_loss}, respectively. In this process, $\hat{I}_C$ is replaced with $\hat{I}^\text{E}$, and $\hat{N}_C$ is replaced with $\hat{N}^\text{E}$, both of which are rendered from $\theta^\text{E}$. These losses use the synthesized images, described in Sec.~\ref{sec:lip-aligned emotional face augmenter}, as ground truth. Additionally, the sync loss $L_\text{sync}$, as defined in Eq.~\ref{eq:sync_loss}, is applied to train the networks.


% For inside-mouth branch, we initialize the inside-mouth Gaussian parameters $\theta^M$ to construct the persistent Gaussian fields through the following loss:
% \begin{eqnarray}
%     L_\text{init} = L_1 (\hat{I}^M_\text{init}, I^M) + \gamma_1 L_\text{D-SSIM} (\hat{I}^M_\text{init}, I^M),
% \end{eqnarray}
% where $\hat{I}^M_\text{init}$ is the image rendered using the parameters $\theta^M$ and $I^M$ denotes the mouth region of the ground-truth image. We take the L1 loss $L_1$ and D-SSIM loss $L_\text{D-SSIM}$ between the rendered image and the ground-truth image. After the initialization, the MLP in Eq.~\ref{eq:mouth mlp} and $\theta^M$ are updated using the following loss:
% \begin{eqnarray}
%     L_\text{main} = L_1 (\hat{I}^M, I^M) + \gamma_1 L_\text{D-SSIM} (\hat{I}^M, I^M),
% \end{eqnarray}
% where $\hat{I}^M$ is the image rendered using the parameters $\theta^M+\delta^M$, and $\delta^M$ is estimated from Eq.~\ref{eq:mouth mlp}.

% For face branch, we also initialize the face Gaussian parameters $\theta^F$ through the following loss:
% \begin{eqnarray}
%     L_\text{init} &=& L_1 (\hat{I}^F_\text{init}, I^F) + \gamma_1 L_\text{D-SSIM} (\hat{I}^F_\text{init}, I^F) \nonumber\\ 
%     &+& \gamma_2 L_1 (\hat{\mbN}^F_\text{init}, \mbN^F)
%     + \gamma_3 L_{tv} (\hat{\mbN}^F_\text{init}, \mbN^F) \nonumber\\ 
%     &+& \gamma_4 || \Delta \mbn ||^2_2,
% \end{eqnarray}
% where $\hat{I}^F_\text{init}$ and $\hat{\mbN}^F_\text{init}$ represent the image and normal map rendered using the parameters $\theta^F$, respectively. Meanwhile, $I^F$ and $\mbN^F$ denote the face region of the ground-truth image and the face region of the normal map estimated by \cite{Abrevaya_2020_CVPR}. We apply the total variation loss $L_{tv}$ and the regularization loss $|| \Delta \mbn ||^2_2$. After the initialization, the MLP in Eq.~\ref{eq:face mlp} and $\theta^F$ are updated using the following loss:
% \begin{eqnarray}
%     L_\text{main} &=& L_1 (\hat{I}^F, I^F) + \gamma_1 L_\text{D-SSIM} (\hat{I}^F, I^F) \nonumber\\ 
%     &+& \gamma_2 L_1 (\hat{\mbN}^F, \mbN^F)
%     + \gamma_3 L_{tv} (\hat{\mbN}^F, \mbN^F) \nonumber\\ 
%     &+& \gamma_4 || \Delta \mbn ||^2_2,
% \end{eqnarray}
% where $\hat{I}^F$ and $\hat{\mbN}^F$ represent the image and normal map rendered using the parameters $\theta^F+\delta^F$, and $\delta^F$ is estimated from Eq.~\ref{eq:face mlp}.

% To jointly fine-tune the inside mouth and face branches, we add an LPIPS loss between the fused image $\hat{I}$ and the ground-truth image $I$. Here, $\hat{I}$ is the fused result of $\hat{I}^M$ and $\hat{I}^F$, as defined in Eq.~\ref{eq:fused image}. In addition, we include a lip-sync loss that evaluates the similarity between visual and audio features using SyncNet~\cite{chung2017out} and incorporate additional optimal speech data.
% % \subsection{Self-Supervised Learning}
% % Text-to-speech and visual-audio synchronization-network

% \noindent\textbf{Normal rendering.} To mitigate the domain gap between the real and synthetic images and enhance the fidelity of 3D Gaussians, we utilize normal rendering. Following GaussianShader~\cite{jiang2024gaussianshader}, we take the shortest axis of each Gaussian as the coarse normal direction $\mbv$ and employ the trainable normal residual $\Delta \mbn$ for further refinement. Consequently, the face Gaussian parameters $\theta^F$ include $\Delta \mbn$, defined as $\theta^F = \{ \mu^F, s^F, q^F, \alpha^F, c^F, \Delta \mbn \}$. Considering the potential ambiguity in the direction of $\mbv$, the refined normal vector $\mbn$ is expressed as follows:
% \begin{eqnarray}
%     \mbn = 
%     \begin{cases} 
%     \mbv + \Delta \mbn_1 & \text{if } \omega_0 \cdot \mbv > 0, \\
%     -(\mbv + \Delta \mbn_2) & \text{otherwise}
%     \end{cases}
% \end{eqnarray}
% where $\omega_0$ denotes the viewing direction. To create the ground truth, we employ a face normal estimator~\cite{Abrevaya_2020_CVPR} that infers the normal map directly from the image, unlike GaussianShader, which uses normals derived from the rendered depth map.
