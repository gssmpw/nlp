\section{Experiments}
\label{sec:experiments}

\begin{table}[t]
    \centering
    \begin{tabular}{l|c|c}
    \hline
     & Testset A & Testset B \\
    Method & Sync-E($\downarrow$)/C($\uparrow$) & Sync-E($\downarrow$)/C($\uparrow$) \\
    \hline
    % EAT~\cite{}     &  &  &  &   \\
    % PD-FGC~\cite{}     &  & & &   \\
    % \hline
    Ground truth & 7.589/7.158 & 7.398/7.112 \\
    \hline
    % GeneFace~\cite{ye2023geneface} & 9.801/4.410 & 9.274/4.712 \\
    ER-NeRF~\cite{li2023efficient} & 9.960/4.305 & 9.397/4.938 \\
    GaussianTalk.~\cite{cho2024gaussiantalker} & 10.208/4.375 & 9.419/5.001 \\
    TalkingGau.~\cite{li2024talkinggaussian} & 9.369/4.835 & 9.009/5.261 \\
    \hline
    % Ours w/o Emo. & \textbf{9.323}/\textbf{4.964} & \textbf{8.828}/\textbf{5.457}  \\
    Ours & \textbf{9.262}/\textbf{4.930} & \textbf{8.746}/\textbf{5.426}  \\
    \hline
    \end{tabular}
    \caption{We evaluate cross-domain audio scenario, highlighting the best results in \textbf{bold}.}
    \label{tab:scenario 2}
    \vspace{-3mm}
\end{table}

\begin{table*}[t]
    \centering
    \begin{tabular}{l|cccccc}
    \hline
    Method & Sync-E($\downarrow$)/C($\uparrow$) & V-RMSE($\downarrow$) & A-RMSE($\downarrow$) & V-SA($\uparrow$) & A-SA($\uparrow$) & E-Acc($\uparrow$) \\
    \hline
    Ground truth & 7.830/7.042 & - & - & - & - & -\\
    \hline
    % EAT~\cite{}     &  &  &   \\
    % PD-FGC~\cite{}     &  & &   \\
    % \hline
    % GeneFace~\cite{ye2023geneface}  & 10.263/4.208  & 0.494 & 0.489 & 0.500 & 0.500 & 24.2  \\
    ER-NeRF~\cite{li2023efficient}  & 10.109/4.225 & 0.479 & 0.502 & 0.500 & 0.500 & 27.3 \\
    GaussianTalker~\cite{cho2024gaussiantalker}  & 10.243/4.539 & 0.491 & 0.503 & 0.500 & 0.500 & 22.9 \\
    TalkingGaussian~\cite{li2024talkinggaussian}  & 9.580/4.779 & 0.467 & 0.474 & 0.515 & 0.500 & 29.1 \\
    \hline
    % Ours w/o Emo. branch  & \textbf{9.006}/\textbf{5.320} & 0.481 & 0.490 & 0.500 & 0.500 & 25.3 \\
    Ours & \textbf{9.082}/\textbf{5.152} & \textbf{0.352} & \textbf{0.383} & \textbf{0.766} & \textbf{0.637} & \textbf{46.6}  \\
    \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{We compare the models' ability to reflect the desired emotion on the face. The best score is highlighted in \textbf{bold}.}
    \label{tab:scenario 3}
    \vspace{-3mm}
\end{table*}

\subsection{Setup}
\noindent\textbf{Dataset.} We evaluate our method on publicly available videos~\cite{guo2021ad,li2023efficient,ye2023geneface}, following the setting of Li~\etal~\cite{li2024talkinggaussian}. The dataset has $4$ subjects: ``Macron", ``Obama", ``Lieu", and ``May." Each video is cropped and resized to ensure that faces are centered. The average video length is $6,500$ at $25$ FPS, with a resolution of $512\times512$ pixels, except for the ``Obama" video which has $450\times450$ resolution. Each video is divided into train and test sets with a 10:1 ratio.

\noindent\textbf{Baselines.} We compare our method with NeRF-based approaches, ER-NeRF~\cite{li2023efficient}, and 3DGS-based approaches, TalkingGaussian~\cite{li2024talkinggaussian} and GaussianTalker~\cite{cho2024gaussiantalker}. These baseline methods are limited to basic facial expressions, such as eye blinking, and do not enable manipulation of facial emotions. Some 2D-based talking head generation methods~\cite{gan2023efficient,wang2023progressive} require large datasets for training, therefore we do not include them in our comparisons.

\noindent\textbf{Scenarios.} We evaluate methods across three scenarios: \emph{self-reconstruction}, \emph{cross-domain audio}, and \emph{emotion-conditioned} scenarios. In the \emph{self-reconstruction} scenario, we evaluate methods using the test set's audio, action units, and valence/arousal values. In the \emph{cross-domain audio} scenario, we train models on train set and test them on two cross-domain audio samples extracted from \cite{suwajanakorn2017synthesizing}, to evaluate their performance on in-the-wild audio. In the \emph{emotion-conditioned} scenario, we evaluate the models' ability to reflect the desired emotion. For our model, we manipulate emotional expressions by using 12 points selected on a 2D circle as inputs for valence and arousal. For other baseline models, action units are used. Specifically, we generate emotional facial images using EmoStyle~\cite{azari2024emostyle} with the 12 valence and arousal points, then extract action units from these images to use as input. Additionally, we use the cross-domain audio sample extracted from \cite{suwajanakorn2017synthesizing}.
% , which were employed by previous methods~\cite{cho2024gaussiantalker}. 

\noindent\textbf{Metrics.}
We utilize PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable} to evaluate the quality of the rendered images. To evaluate lip synchronization, we use the mouth landmark distance (LMD)~\cite{chen2018lip}, and the synchronization error (Sync-E) and the synchronization confidence score (Sync-C) of SyncNet~\cite{chung2017out}. We measure the upper-face action unit error (AUE-U) and lower-face action unit error (AUE-L) using OpenFace~\cite{baltruvsaitis2015cross,Baltrusaitis2018openface}. These metrics ensure that the rendered images accurately capture and reflect the target facial action units. To evaluate the emotion consistency, we employ the valance and arousal root mean square error (V-RMSE and A-RMSE)~\cite{toisoul2021estimation}, and valance and arousal sign agreement (V-SA and A-SA)~\cite{toisoul2021estimation}. Additionally, we utilize the top-3 emotion classification accuracy (E-Acc). We employ EmoNet~\cite{toisoul2021estimation} to extract valence, arousal, and emotion label from the rendered images.

% The RSME is defined as: $RMSE = \sqrt{\frac{1}{N}\sum^N_i(E^{pred}_i-E^{true}_i)^2}$ where $E^{pred}_i$ and $E^{true}_i$ are the predicted and true emotion values (valence or arousal) for the $i$-th sample. The SA is defined as: $SA=\frac{1}{N}\sum^N_i\mathbb{I}(\text{sign}(E^{pred}_i)==\text{sign}(E^{true}_i))$ where $\mathbb{I}(\cdot)$ is the indicator function that returns 1 if the condition inside is true and 0 otherwise.

\subsection{Quantitative Results}
We evaluate various metrics across three different scenarios, as reported in Tabs~\ref{tab:scenario 1}, \ref{tab:scenario 2}, and \ref{tab:scenario 3}. In \emph{self-reconstruction} scenario, our method without the emotion branch achieves the best scores in pixel-based metrics, such as PSNR, SSIM, and LPIPS, as well as the landmark distance (LMD) and action unit error for the lower face (AUE-L) and upper face (AUE-U). Our method, which incorporates emotion and is trained on both emotional synthetic and original facial data, slightly compromises detail preservation, but it remains comparable to other baselines in pixel-based metrics, and outperforms them with higher lip synchronization confidence and lower action unit error for expressions. In \emph{cross-domain audio} scenario, where we cannot use ground-truth images, we measure synchronization error (Sync-E) and confidence (Sync-C). Our method outperforms other methods in both Sync-E and Sync-C, demonstrating its ability to handle the cross-domain audio effectively. In \emph{emotion-conditioned} scenario, we use Sync-E and Sync-C metrics to evaluate the lip-synchronization, and V-RMSE, A-RMSE, V-SA, and A-SA to evaluate the models' ability to convey the desired emotion to the talking head. Other methods show V-SA and A-SA scores around 0.5, indicating that the valence and arousal values estimated from the rendered images are concentrated in only one quadrant of the valence-arousal circle. Additionally, they exhibit high V-RMSE and A-RMSE errors, along with low emotion classification accuracy. These suggest that these methods do not effectively reflect the desired emotions. In contrast, our model demonstrates superior performance across all metrics, confirming its ability to effectively convey the intended emotions on the face.

\subsection{Qualitative Results}
We present the qualitative results in Fig.~\ref{fig:qualitative_results}. The word pronounced by the subject is highlighted in red. To manipulate facial emotion with our method, we use valence and arousal, shown below the pronounced word as V and A. To better understand which specific emotion the valence and arousal represent, the emotion label is displayed below the V and A values. For other methods, such as ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker} and TalkingGaussian~\cite{li2024talkinggaussian}, the action units are used to manipulate facial expressions. As shown in the blue dashed boxes in Fig.~\ref{fig:qualitative_results}, action units have a limitation in expressing natural emotions. In contrast, our method expresses emotions around the eyes and mouth based on valence and arousal values. For lip synchronization, mismatches are highlighted with brown dashed boxes. Our method effectively synchronizes lip movements with speech audio while conveying the desired emotions.

\begin{table}[t]
    \centering
    \begin{tabular}{l|cc|c}
    \hline
    Method & LPIPS ($\downarrow$) & Sync-C($\uparrow$) & E-Acc ($\uparrow$) \\
    \hline
    Ours w/o $L_\text{normal}$ & 0.0274 & 6.090 & 45.4 \\
    Ours w/o $L_\text{sync}$ & 0.0269 & 5.925 & 46.1 \\
    \hline
    Combined face b.  & \textbf{0.0253} & 4.489 & 42.1 \\
    Ours w/o emo. b. & 0.0255 & 6.270 & 25.3 \\
    \hline
    Ours & 0.0267 & \textbf{6.279} & \textbf{46.6} \\
    \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Ablation study on loss functions and model architectures across different model configurations: `Ours w/o $L_\text{normal}$' (without normal map loss), `Ours w/o $L_\text{sync}$' (without sync loss), `Combined face b.' (model combining face branch and emotion branch), `Ours w/o emo. b.' (our model without the emotion branch), and `Ours' (our full model). The best performance for each metric is highlighted in \textbf{bold}.}
    \label{tab:ablation study}
    \vspace{-3mm}
\end{table}

\subsection{Ablation Study}
We conduct ablation study on loss functions and model architectures, as reported in Tab.~\ref{tab:ablation study}. We use the self-reconstruction scenario to measure LPIPS and Sync-C, and the emotion-conditioned scenario to measure E-Acc. For \emph{loss functions}, including the normal map loss $L_\text{normal}$ and the sync loss $L_\text{sync}$, we compare the performance of our full model with `Ours w/o $L_\text{normal}$' and `Ours w/o $L_\text{normal}$'. `Ours w/o $L_\text{normal}$' do not use $L_\text{normal}$ and `Ours w/o $L_\text{sync}$' do not use $L_\text{sync}$, during training of EmoTalkingGaussian. $L_\text{normal}$ impacts the quality of the rendered image, as evidenced by an increase in LPIPS. Similarly, $L_\text{sync}$ influences lip sync accuracy, as reflected in changes in Sync-E/C. Additionally, for \emph{model architectures}, we compare our full model with `Combined face b.' and `Ours w/o Emo. b.'. `Combined face b.' model renders emotional facial images using only the face manipulation network $f^{F}$ in the face branch, where the network $f^{F}$ takes valence, arousal, action units, and audio as inputs. `Ours w/o Emo. b.' represents the our model without the emotion branch. `Combined face b.' achieves the best LPIPS score but struggles with accurate lip synchronization. `Ours w/o Emo. b.' fails to reflect emotion in the rendered image. While our full model shows slightly lower performance in image quality, it outperforms other baseline in both lip synchronization and emotion control.

% \subsubsection{Architectures.}
% We conduct an ablation study on model architectures, including the emotion branch and a new face branch that also incorporates emotion values. 



\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{images/main/qualitative_results.pdf}
    \caption{We present qualitative comparisons with other baselines, including ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker}, and TalkingGaussian~\cite{li2024talkinggaussian}. The word is displayed with the spoken word highlighted in \textcolor{red}{red}. The last sample shows the phonetic transcription. `V' and `A' stand for valence and arousal, and emotion labels indicate the emotion that `V' and `A' values represent. Emotional inconsistencies and lip mismatches are highlighted with \textcolor{skyblue}{blue} and \textcolor{brown}{brown} dashed boxes, respectively.}
    \label{fig:qualitative_results}
    \vspace{-3mm}
\end{figure*}