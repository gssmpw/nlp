\clearpage
\setcounter{page}{1}
\maketitlesupplementary


In this supplementary material, we provide implementation details; details of lip-aligned emotional face generator; rendering; details of training EmoTalkingGaussian; curated audio data; evaluation in emotion-conditioned scenario; attention visualization; inside-mouth normal map; limitations of simple fused approach; limitations of diffusion model; user study; and qualitative results. Additionally, for animatable results, please refer to the accompanying supplementary video, which includes the emotion-conditioned scenario comparison, valence-arousal interpolation, 360Â° valence-arousal interpolation (radius: 0.8), and dynamic emotion transitions during speech.

\section{Implementation Details}
Our method is implemented using PyTorch. All experiments are conducted using RTX 4090 GPUs. We train the lip-aligned emotional face generator for 10 epochs using the Adam optimizer with a learning rate of $1\times10^{-4}$. The mouth branch, face branch, and emotion branch are each trained for 50,000 iterations, and the face canonical Gaussians are fine-tuned with an additional 20,000 iterations. We use the AdamW optimizer with a learning rate of $5\times10^{-3}$ for the hash encoder and $5\times10^{-4}$ for the other parts. The learning rates are adjusted using an exponential scheduler. The total training time is 2 hours. The loss weights are described in Secs.~\ref{sec:lef losses} and \ref{sec:ground truth preparation and trainig losses}.
% \begin{eqnarray}
% \{\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5\} &=& \{ 1, 5, 0.03, 0.2, 1.5 \}, \nonumber \\
% \{\gamma_1, \gamma_2, \gamma_3, \gamma_4\} &=& \{ 0.2, 0.05, 0.005, 0.001 \}, \nonumber \\
% \{\beta_1, \beta_2, \beta_3, \beta_4, \beta_5 \} &=& \{ 0.2, 0.2, 0.05, 0.005, 0.001 \}, \nonumber \\
% \{\kappa_1, \kappa_2, \kappa_3, \kappa_4, \kappa_5 \} &=& \{ 0.2, 0.2, 0.05, 0.005, 0.001 \}. \nonumber
% \end{eqnarray}


\section{Details of Lip-aligned Emotional Face Generator}

\subsection{Pipeline}
The overall framework of our lip-aligned emotional face generator $g^\text{LEF}$ is illustrated in Fig.~\ref{fig:pipeline of face generator}. EmoStyle~\cite{azari2024emostyle} first creates a latent code $\calW$ from a source image $I$ using an inversion module ~\cite{azari2024emostyle,tov2021designing}. It then produces an emotional latent code $\calW'$ by adding $\calW$ to an emotion modification vector $d$. The vector $d$ is generated by EmoExtract $M$, which takes a concatenated vector $(f_\text{emo} \oplus \calW)$ as input. Here, $f_\text{emo}$ represents the valence/arousal features derived from the valence/arousal input $\mbe$ through an up-sampling module. EmoStyle generates a facial image with lips that do not match those of the source image $I$.

To achieve lip alignment, a lip encoder $E$ generates a lip embedding vector $z_l$ from lip heatmaps $H$, which are extracted from the source image $I$ using a LipDetector $D_l$~\cite{bulat2017far}. A LipExtract module $M_\text{lip}$ then produces a lip modification vector $d_l$ by taking a concatenated vector ($z_l \oplus \calW'$) as input. By adding $d_l$ to the emotional latent code $\calW'$, a lip-aligned emotional latent code $\calW''$ is obtained. Finally, our generator employs StyleGAN2~\cite{karras2020analyzing} to generate a lip-aligned emotional facial image $I^\text{E}$ from the latent code $\calW''$.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/lip_aligned_emotional_face_generator.pdf}
    \caption{Overview of the lip-aligned emotional face generator. While EmoStyle~\cite{azari2024emostyle} cannot produce lip-aligned emotional facial images, our generator creates such images by aligning lips based on lip heatmaps. $\boxplus$ denotes vector summation.}
    \label{fig:pipeline of face generator}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/lip_aligned_emotional_face_gen_loss.pdf}
    \caption{Lip landmark loss $\calL_{ll}$, lip pixel loss $\calL_{lp}$, emotion loss $\calL_{emo}$, identity loss $\calL_{id}$ are utilized to train the lip-aligned emotional face generator $g^\text{LEF}$. We use LipDetector~\cite{bulat2017far}, EmoNet~\cite{toisoul2021estimation}, and VggFace2~\cite{cao2018vggface2}.}
    \label{fig:pipeline of face gen loss}
\end{figure}

\subsection{Losses}
\label{sec:lef losses}
We train the encoder $E$ and the LipExtract module $M_\text{lip}$, and fine-tune StyleGAN2~\cite{karras2020analyzing} using the following loss functions: lip landmark loss, lip pixel loss, regularization loss, emotion loss, and identity loss. These losses are illustrated in Fig.~\ref{fig:pipeline of face gen loss}.

The lip landmark loss $\calL_{ll}$ is defined as follows:
\begin{eqnarray}
    \calL_{ll} = ||\hat{L}_l-L_l||^2_2,
\end{eqnarray}
where $L_l$ and $\hat{L}_l$ denote the lip landmarks extracted from the source image $I$ and the output image $I^\text{E}$, respectively, using the landmark detector~\cite{bulat2017far}. This loss encourages the model to produce an output image $I^\text{E}$ that closely aligns with the original lip structure of $I$, ensuring the positional accuracy of the generated lip region.

The lip pixel loss $\calL_{lp}$ is defined as follows:
\begin{eqnarray}
    \calL_{lp} = ||\mathbb{M}_l \odot (I^\text{E}-I)||^2_2,
\end{eqnarray}
where the lip region mask $\mathbb{M}_l$ is applied to the pixel-wise difference between $I^\text{E}$ and $I$. $\mathbb{M}_l$ is a rectangular mask created based on the lip landmarks $L_l$, and $\odot$ denotes element-wise multiplication. This loss penalizes differences in pixel values within the lip area, encouraging $I^\text{E}$ to closely resemble $I$ specifically in the lip region.

The regularization loss $\calL_{reg}$ is defined as follows:
\begin{eqnarray}
    \calL_{reg} = || d_l ||^2_2.
\end{eqnarray}
This loss prevents $d_l$ from diverging.

The emotion loss $\calL_{emo}$ is defined as follows:
\begin{eqnarray}
    \calL_{emo} = || EmoNet(I^\text{E}) - \mbe ||^2_2,
\end{eqnarray}
where $EmoNet$~\cite{toisoul2021estimation} outputs valence and arousal values $\hat{\mbe}$ from the output image $I^\text{E}$. This loss encourages the output image $I^\text{E}$ to reflect the emotion specified by the input valence and arousal values $\mbe$.

The identity loss $\calL_{id}$ is defined as follows:
\begin{eqnarray}
    \calL_{id} = ||VF(I^\text{E})-VF(I)||_1
\end{eqnarray}
where $VF$ represents VggFace2~\cite{cao2018vggface2}, which extracts identity embeddings corresponding to the face's identity. This loss ensures that the identity of the output image $I^\text{E}$ is preserved, matching that of the source image $I$.

Total training loss are expressed as follows:
\begin{eqnarray}
    \calL = \lambda_1 \cdot \calL_{ll} + \lambda_2 \cdot \calL_{lp} + \lambda_3 \cdot \calL_{reg} \nonumber \\ 
    + \lambda_4 \cdot \calL_{emo} + \lambda_5 \cdot \calL_{id},
\end{eqnarray}
where $\lambda_1$, $\lambda_2$, $\lambda_3$, $\lambda_4$, and $\lambda_5$ are 1, 5, 0.03, 0.2, and 1.5, respectively.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/rendering.pdf}
    \caption{Examples of rendering are displayed, and their relationships are described in Eqs.~\ref{eq:C mouth bg} and \ref{eq:rendering final image}.}
    \label{fig:rendering examples}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{images/supplementary/synthetic_data_sm_ind.pdf}
    \caption{Columns (a), (b), (c), and (d) display source images, images generated by the lip-aligned emotional face generator $g^\text{LEF}$, simple cut-and-paste composite images, and seamless cloning results, respectively. The first row uses valence and arousal values of 0.6 and 0.2 (happy), while the second row applies values of -0.8 and 0.4 (angry). \textcolor{OliveGreen}{Green} and \textcolor{Goldenrod}{yellow} boxes indicate domain gaps, while \textcolor{brown}{brown} boxes highlight boundary artifacts. \textcolor{red}{Red} and \textcolor{blue}{blue} boxes demonstrate that seamless cloning effectively addresses both domain gaps and boundary artifacts, respectively.}
    \label{fig:synthetic data}
\end{figure*}

\section{Rendering}
The $i$-th 3D Gaussian contains a position $\mu_i$, a scaling factor $s_i$, a rotation quaternion $q_i$, an opacity value $\alpha_i$, and a color $c_i$. The rendering process for 3D Gaussians is expressed as follows:
\begin{eqnarray}
    C(\mbx_p) &=& \sum_{i \in N} c_i \tilde{\alpha}_i \prod^{i-1}_{j=1} (1-\tilde{\alpha}_j),\\
    \calA(\mbx_p) &=& \sum_{i \in N} \tilde{\alpha}_i \prod^{i-1}_{j=1} (1-\tilde{\alpha}_j),
\end{eqnarray}
where $C(\mbx_P)$ and $\calA(\mbx_P)$ represent color and opacity at pixel $\mbx_p$, respectively, as described in Eqs. 1 and 2 of the main paper. To generate a final image $\hat{I}$ by combining the face color $C_\text{face}$, inside-mouth color $C_\text{inside-mouth}$, and background color $C_\text{background}$, we use the respective opacities $\calA_\text{face}$ and $\calA_\text{inside-mouth}$ as follows:
\begin{eqnarray}
    C_\text{mouth-bg} &=& C_\text{inside-mouth} \times \calA_\text{inside-mouth} \nonumber \\
    &+& C_\text{background} \times (1-\calA_\text{inside-mouth}),
    \label{eq:C mouth bg}
\end{eqnarray}
and 
\begin{eqnarray}
    \hat{I} &=& C_\text{face} \times \calA_\text{face} + C_\text{mouth-bg} \times (1-\calA_\text{face}),
    \label{eq:rendering final image}
\end{eqnarray}
where we blend the inside-mouth color $C_\text{inside-mouth}$ with the background color $C_\text{background}$ using the opacity $\calA_\text{inside-mouth}$ to produce the combined inside-mouth and background color $C_\text{mouth-bg}$, and then blend this result with the face color $C_\text{face}$, using the opacity $\calA_\text{face}$ to produce the final image $\hat{I}$. The background color $C_\text{background}$ is provided, while $C_\text{inside-mouth}$ and $\calA_\text{inside-mouth}$ are rendered from the inside-mouth deformed Gaussians $\theta^\text{M}$, and $C_\text{face}$ and $\calA_\text{face}$ are rendered from the emotionally deformed Gaussians $\theta^\text{E}$. Examples of rendering are shown in Fig.~\ref{fig:rendering examples}.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/masking.pdf}
    \caption{BiSeNet~\cite{yu2018bisenet} estimates the coarse mask to parse the face and mouth regions, while the Teeth parser~\cite{li2024talkinggaussian} estimates the teeth mask. These two masks are combined to create the refined mask, which more accurately separates the face and mouth compared to the coarse mask. The areas of discrepancy between the coarse and refined masks are highlighted in red, denoted as ``Difference", representing pixel differences between the face regions masked by each individual mask.}
    \label{fig:processing mask}
\end{figure*}

\section{Details of Training EmoTalkingGaussian}
\subsection{Improve Synthetic Emotional Facial Image}
\label{sec:seamless cloning}
The initial generated image is produced by our lip-aligned emotional face generation network $g^\text{LEF}$, manipulates a source image by taking any valence/arousal $\mbe$ as input to generate an emotional face image that align with $\mbe$. However, the generated images exhibit domain gaps compared to real images, particularly noticeable in artifacts such as blurry hair or irregularities in skin texture, as seen in the green boxes in column (a) and the yellow boxes in column (b) of Fig.~\ref{fig:synthetic data}. To reduce this gap, we use a cut-and-paste composite method. Rectangular regions around the eyes and mouth, which are expressive of emotion in the generated image, are cut and pasted onto the source image, as shown in column (c) of Fig.~\ref{fig:synthetic data}. These rectangular regions are determined using eye and mouth landmarks extracted from the generated image. However, while this approach reduces the domain gap by preserving realism in the source image and transferring emotion-expressive regions, it introduces boundary artifacts around the pasted areas, as shown in the brown boxes in column (c) of Fig.~\ref{fig:synthetic data}. To resolve this, we apply seamless cloning~\cite{perez2023poisson} that allows for smooth blending between the two images. Unlike the simple cut-and-paste composite, which results in visible boundary artifacts, seamless cloning integrates textures and colors more naturally, effectively eliminating hard edges and creating a cohesive, realistic appearance, as seen in the red and blue boxes in column (d) of Fig~\ref{fig:synthetic data}. This realistic synthesis is leveraged for the training of our EmoTalkingGaussian.

\subsection{Processing Mask}
\label{sec:processing mask}
Following TalkingGaussian~\cite{li2024talkinggaussian}, we utilize BiSeNet~\cite{yu2018bisenet} and Teeth parser~\cite{li2024talkinggaussian} to extract face and inside-mouth masks from a source image, as shown in Fig.~\ref{fig:processing mask}. BiSeNet, pre-trained on CelebAMask-HQ dataset~\cite{lee2020maskgan}, generates a coarse mask which distinguishes face and mouth regions. By masking with the blue region of the coarse mask on the source image, as indicated by the above blue arrow, we extract the coarse masked face. However, as shown, this coarse mask cannot accurately separate the face and mouth regions, especially the teeth. Thus, to address this issue, the Teeth parser, trained on the EasyPortrait dataset~\cite{kvanchiani2023easyportrait}, is employed to generate a teeth mask. By combining the coarse mask and the teeth mask, we create a refined mask. When the source image is masked using the blue region of the refined mask, as indicated by the below blue arrow, the teeth are no longer visible. The mask shown in red highlights the differences between the coarse masked face and the refined masked face. The green region of the refined mask is used to extract the inside-mouth region in Sec.~\ref{sec:ground truth preparation and trainig losses}.


\subsection{Ground Truth Preparation and Details of Training Losses}
\label{sec:ground truth preparation and trainig losses}
We prepare the ground truth data for each branch: the inside-mouth branch, face branch, and emotion branch, as shown in Fig.~\ref{fig:processing data}. 

For the inside-mouth branch, we extract an inside-mouth region $I_\text{mask} (I^\text{M}_\text{mask})$ from a source image $I$ using a green region of the mask estimated in Sec.~\ref{sec:processing mask}. The inside-mouth RGB image $I^\text{M}_\text{mask}$ is used as $I_\text{mask}$ in Eqs.~11 and 14 of the main paper. Accordingly, the inside-mouth canonical Gaussians $\theta^\text{M}_C$ are optimized using the following loss function:
\begin{eqnarray}
    \calL = \calL_1 (\hat{I}^\text{M}_C, I^\text{M}_\text{mask}) + \gamma_1 \calL_\text{D-SSIM} (\hat{I}^\text{M}_C, I^\text{M}_\text{mask}),
\end{eqnarray}
where $\hat{I}^\text{M}_C$ denotes the RGB image rendered from $\theta^\text{M}_C$, and $\gamma_1$ is 0.2. The inside-mouth region tri-plane hash encoder $H^\text{M}$ and the inside-mouth region manipulate network $f^\text{M}$ are trained using the following loss function:
\begin{eqnarray}
    \calL &=& \calL_1 (\hat{I}^\text{M}, I^\text{M}_\text{mask}) + \beta_1 \calL_\text{D-SSIM} (\hat{I}^\text{M}, I^\text{M}_\text{mask}) \nonumber \\
    &+& \beta_2 \calL_\text{LPIPS} (\hat{I}^\text{M}, I^\text{M}_\text{mask}) + \beta_6 \calL_\text{sync},
\end{eqnarray}
where $\hat{I}^\text{M}$ denotes the RGB image rendered from inside-mouth deformed Gaussians $\theta^\text{M}$, and $\beta_1$, $\beta_2$, and $\beta_6$ are 0.2, 0.2, and 0.05, respectively.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/processing_data.pdf}
    \caption{Preparation of ground truth data for training EmoTalkingGaussian involves the lip-aligned emotional face generator $g^\text{LEF}$ combined with seamless cloning (described in Sec.~\ref{sec:seamless cloning} and `S.C.' stands for seamless cloning), the normal estimator~\cite{Abrevaya_2020_CVPR}, and the estimated mask (described in Sec.~\ref{sec:processing mask}).}
    \label{fig:processing data}
\end{figure*}

For the face branch, we extract a face region $I_\text{mask} (I^\text{F}_\text{mask})$ from the source image $I$ using the blue region of the estimated mask. Additionally, we create a masked face normal map $N^\text{F}_\text{mask}$ using a normal estimator~\cite{Abrevaya_2020_CVPR} along with the estimated mask. The face RGB image $I^\text{F}_\text{mask}$ is used as $I_\text{mask}$ in Eqs.~11 and 14 of the main paper, while the normal map $N^\text{F}_\text{mask}$ is utilized in Eqs.~12 and 15 of the main paper. Accordingly, the face canonical Gaussians $\theta^\text{F}_C$ are optimized using the following loss function:
\begin{eqnarray}
    \calL &=& \calL_1 (\hat{I}^\text{F}_C, I^\text{F}_\text{mask}) + \gamma_1 \calL_\text{D-SSIM} (\hat{I}^\text{F}_C, I^\text{F}_\text{mask}) \nonumber\\
    &+& \gamma_2 \calL_1 (\hat{N}^\text{F}_C, N^\text{F}_\text{mask}) + \gamma_3 \calL_\text{tv} (\hat{N}^\text{F}_C) \nonumber \\
    &+& \gamma_4 || \Delta n ||,
\end{eqnarray}
where $\hat{I}^\text{F}_C$ and $\hat{N}^\text{F}_C$ denote the RGB image and normal map rendered from $\theta^\text{F}_C$, and $\gamma_1$, $\gamma_2$, $\gamma_3$, and $\gamma_4$ are 0.2, 0.05, 0.005, and 0.001, respectively. The face region tri-plane hash encoder $H^\text{F}$ and the face region manipulate network $f^\text{F}$ are trained using the following loss function:
\begin{eqnarray}
    \calL &=& \calL_1 (\hat{I}^\text{F}, I^\text{F}_\text{mask}) + \beta_1 \calL_\text{D-SSIM} (\hat{I}^\text{F}, I^\text{F}_\text{mask}) \nonumber \\
    &+& \beta_2 \calL_\text{LPIPS} (\hat{I}^\text{F}, I^\text{F}_\text{mask}) + \beta_3 \calL_1 (\hat{N}^\text{F}, N^\text{F}_\text{mask}) \nonumber \\
    &+& \beta_4 \calL_\text{tv} (\hat{N}^\text{F}) + \beta_5 || \Delta n || + \beta_6 \calL_\text{sync},
\end{eqnarray}
where $\hat{I}^\text{F}$ and $\hat{N}^\text{F}$ denote the RGB image and normal map rendered from face deformed Gaussians $\theta^\text{F}$, and $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, $\beta_5$, and $\beta_6$ are 0.2, 0.2, 0.05, 0.005, 0.001, and 0.05, respectively.

For the emotion branch, we generate the emotional facial image $I^\text{E}$ using our lip-aligned emotional face generator $g^\text{LEF}$ and seamless cloning algorithm described in Sec.~\ref{sec:seamless cloning}. Similarly to the previous branches, we extract the RGB $I_\text{mask} (I^\text{E}_\text{mask})$ and normal map $N^\text{E}_\text{mask}$ for the face region using the estimated mask and the normal estimator~\cite{Abrevaya_2020_CVPR}. The emotional face RGB image $I^\text{E}_\text{mask}$ is used as $I_\text{mask}$ in Eq.~17 of the main paper, while the emotional face normal map $N^\text{E}_\text{mask}$ is utilized in Eq.~18 of the main paper. Accordingly, the emotion tri-plane hash encoder $H^\text{E}$ and the emotion manipulation network $f^\text{E}$ are trained using the following loss function:
\begin{eqnarray}
    \calL &=& \calL_1 (\hat{I}^\text{E}, I^\text{E}_\text{mask}) + \beta_1 \calL_\text{D-SSIM} (\hat{I}^\text{E}, I^\text{E}_\text{mask}) \nonumber \\
    &+& \beta_2 \calL_\text{LPIPS} (\hat{I}^\text{E}, I^\text{E}_\text{mask}) + \beta_3 \calL_1 (\hat{N}^\text{E}, N^\text{E}_\text{mask}) \nonumber \\
    &+& \beta_4 \calL_\text{tv} (\hat{N}^\text{E}) + \beta_5 || \Delta n || + \beta_6 \calL_\text{sync},
\end{eqnarray}
where $\hat{I}^\text{E}$ and $\hat{N}^\text{E}$ denote the RGB image and normal map rendered from emotionally deformed Gaussians $\theta^\text{E}$, and $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, $\beta_5$, and $\beta_6$ are 0.2, 0.2, 0.05, 0.005, 0.001, and 0.001, respectively.

After finishing training, the face canonical Gaussians $\theta^\text{F}_C$ are further optimized to erase artifacts around face border using the following loss function, focusing only on optimizing the Gaussian's opacity $\alpha$ and color $c$:
\begin{eqnarray}
    \calL = \calL_1 (\hat{I}, I) &+& \eta_1 \calL_\text{D-SSIM} (\hat{I}, I) \nonumber \\
    &+& \eta_2 \calL_\text{LPIPS} (\hat{I}, I),
\end{eqnarray}
where $\hat{I}$ represents the image rendered by Eq.~\ref{eq:rendering final image}, $\eta_1$ and $\eta_2$ are 0.2 and 0.5, respectively.

\section{Curated Audio Data}
We synthesize curated audio data to improve lip synchronization by using ChatGPT~\cite{ChatGPT} and text-to-speech network~\cite{gTTS}. The 10 text descriptions of the audio are listed as follows:
% \begin{enumerate}
%     \item \textbf{Sentence 1}: ``The quick brown fox jumps over the lazy dog."
%     \item \textbf{Sentence 2}: ``She's going to buy some new clothes at the mall."
%     \item \textbf{Sentence 3}: ``I can't believe it's already half past eight!"
%     \item \textbf{Sentence 4}: ``Do you want to grab some water?"
%     \item \textbf{Sentence 5}: ``Better late than never, they say."
%     \item \textbf{Sentence 6}: ``She needs to see the doctor immediately."
%     \item \textbf{Sentence 7}: ``Did you eat yet?"
%     \item \textbf{Sentence 8}: ``Next stop is Central Park."
%     \item \textbf{Sentence 9}: ``An unknown number called me yesterday."
%     \item \textbf{Sentence 10}: ``We can meet at the cafÃ© if you'd like."
% \end{enumerate}

\begin{description}
    \item \textbf{Sentence 1.} The quick brown fox jumps over the lazy dog.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/Di:/, /kwIk/, /braUn/, /fA:ks/, /dZ2mps/, /"oUv\textrhookschwa/, /Di:/, /"leIzi/, /dAg/}.
        \item \textbf{Sound Coverage:} Includes nearly all English consonants and vowels.
        \item \textbf{Liaison:} /r/ liaison occurs in ``over the" (\textipa{/oUv@ D@/}).
        \item \textbf{Stress:} Content words such as ``quick", ``brown", ``fox", ``jumps", ``over", ``lazy", and ``dog" are stressed.
        \item \textbf{Weak Form:} ``The" (\textipa{/D@/}).
    \end{itemize}

    \item \textbf{Sentence 2.} She's going to buy some new clothes at the mall.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/Si:z/, /"goUIN/, /tu:/, /baI/, /s2m/, /nu:/, /kloUDz/, /\ae t/, /Di:/, /mA:l/}.
        \item \textbf{Diphthongs:} \textipa{/aI/} (buy), \textipa{/oU/} (clothes).
        \item \textbf{Weak Forms:} 
            \begin{itemize}
                \item ``to": \textipa{/t@/}.
                \item ``some": \textipa{/s@m/}.
                \item ``at": \textipa{/@t/}.
                \item ``the": \textipa{/D@/}.
            \end{itemize}
    \end{itemize}

    \item \textbf{Sentence 3.} I can't believe it's already half past eight!
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/aI/, /k\ae nt/, /bI"li:v/, /Its/, /A:l"redi/, /h\ae f/, /p\ae st/, /eIt/}.
        \item \textbf{Contractions:} ``can't" as \textipa{/k\ae nt/}, ``it's" as \textipa{/Its/}.
        \item \textbf{Stress:} ``can't", ``believe", ``already", ``half", and ``eight" are stressed.
        \item \textbf{Silent Letters:} The ``l" in ``half" is silent.
    \end{itemize}

    \item \textbf{Sentence 4.} Do you want to grab some water?
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/du:/, /ju:/, /wA:nt/, /tu:/, /gr\ae b/, /s2m/, /wA:\v*t\textrhookschwa/}.
        \item \textbf{R-pronunciation:} ``water" is pronounced as \textipa{/"wA:\v*t\textrhookschwa/}.
        \item \textbf{Weak Forms:}
            \begin{itemize}
                \item ``to": \textipa{/t@/}.
                \item ``some": \textipa{/s@m/}.
            \end{itemize}
    \end{itemize}

    \item \textbf{Sentence 5.} Better late than never, they say.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/be\v*t\textrhookschwa/, /leIt/, /D \ae n/, /"nev\textrhookschwa/, /DeI/, /seI/}
        \item \textbf{Flapping:} ``better" sounds like \textipa{/"be\v*t\textrhookschwa/} with a flapped t.
        \item \textbf{Liaison:} Connection in ``than never" with n.
        \item \textbf{Weak Form:} ``than" (\textipa{D@n}).
    \end{itemize}

    \item \textbf{Sentence 6.} She needs to see the doctor immediately.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/Si:/, /ni:ds/, /tu:/, /si:/, /Di:/, /"dA:kt\textrhookschwa/, /I"mi:di@tli/}
        \item \textbf{Long Vowel:} Long \textipa{/i:/} sounds in ``needs" and ``see".
        \item \textbf{Stress:} ``needs", ``see", ``doctor", and ``immediately" are stressed.
        \item \textbf{Weak Forms:}
            \begin{itemize}
                \item ``to": \textipa{/t@/}.
                \item ``the": \textipa{/D@/}.
            \end{itemize}
    \end{itemize}

    \item \textbf{Sentence 7.} Did you eat yet?
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/dId/, /ju:/, /i:t/, /jet/}
        \item \textbf{Liaison /j/:} ``Did you" sounds like \textipa{/dIdZu:/}.
        \item \textbf{Stress:} ``eat" is stressed.
    \end{itemize}

    \item \textbf{Sentence 8.} Next stop is Central Park.
    \begin{itemize}
        \item \textbf{Phonetic Notation:}  \textipa{/nekst/, /stA:p/, /Iz/, /"sentr@l/, /pA:rk/}
        \item \textbf{Consonant Cluster:} \textipa{/"sentr@l/} in ``central".
        \item \textbf{Elision:} The ``t" is dropped in ``next stop", sounding like \textipa{/nEks stAp/}.
        \item \textbf{Stress:} ``next", ``stop", ``Central", and ``Park" are stressed.
    \end{itemize}

    \item \textbf{Sentence 9.} An unknown number called me yesterday.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/\ae n/, /2n"n@Un/, /"n2mb\textrhookschwa/, /kA:ld/, /mi:/, /"jest\textrhookschwa deI/}
        \item \textbf{Double Consonant:} The n is lengthened in ``unknown" (\textipa{/n"n/}).
        \item \textbf{R-pronunciation:} The word ``number" has an r-colored schwa vowel (\textipa{\textrhookschwa r}).
        % \item \textbf{Syllable Insertion:} A schwa \textrhookschwa is inserted between \textipa{/b/} and \textipa{/r/} in ``number".
        \item \textbf{Stress:} ``unknown", ``number", ``called", and ``yesterday" are stressed.
        \item \textbf{Weak Form:} ``An" (\textipa{@n}).
    \end{itemize}

    \item \textbf{Sentence 10.} We can meet at the cafÃ© if you'd like.
    \begin{itemize}
        \item \textbf{Phonetic Notation:} \textipa{/wi:/, /k \ae n/, /mi:t/, /\ae t/, /Di:/, /k\ae f"eI/, /If/, /ju:d/, /laIk/}
        \item \textbf{Contraction:} ``you'd" pronounced as \textipa{/ju:d/}.
        \item \textbf{Stress:} ``meet", ``cafÃ©", and ``like" are stressed.
        \item \textbf{Weak Forms:}
            \begin{itemize}
            \item ``can": \textipa{/k@n/}.
            \item ``at": \textipa{/@t/}.
            \item ``the": \textipa{/D@/}.
            \end{itemize}
    \end{itemize}
\end{description}

\section{Evaluation in Emotion-conditioned Scenario}
We evaluate our method against state-of-the-art approaches across various scenarios. While prior studies~\cite{li2023efficient,cho2024gaussiantalker,li2024talkinggaussian} primarily focused on self-reconstruction and cross-domain audio scenarios, this paper introduces the emotion-conditioned scenario for the first time. This new scenario allows us to evaluate each method's ability to accurately reflect the desired emotion in the rendered face. To evaluate them, we first select 12 valence and arousal values:

\noindent\([0.74, 0.31], [0.31, 0.74], [-0.31, 0.74], [-0.74, 0.31],\)
\\
\([-0.74, -0.31], [-0.31, -0.74], [0.31, -0.74],\) \\
\([0.74, -0.31], [0.35, 0.35], [-0.35, 0.35],\) \\
\([-0.35, -0.35], [0.35, -0.35]\).
Specifically, the valence-arousal points with a radius of 0.8 are selected by dividing 360Â° into 8, with each point separated by a 45Â° interval. Similarly, the valence-arousal points with a radius of 0.5 are chosen by dividing 360Â° into 4, placing each point at a 90Â° interval. This approach ensures that the valence-arousal points are evenly distributed around the circle.

Using these valence-arousal points, our model conveys emotion directly. In contrast, other methods~\cite{li2023efficient,cho2024gaussiantalker,li2024talkinggaussian} cannot utilize valence-arousal values directly. Instead, they rely on EmoStyle~\cite{azari2024emostyle} to generate each individual's emotional face based on valence-arousal points. Action units are then extracted from this generated image using OpenFace~\cite{Baltrusaitis2018openface}, which are subsequently used to adjust facial expressions when rendering the talking heads in these methods~\cite{li2023efficient,cho2024gaussiantalker,li2024talkinggaussian}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/attn_vis.pdf}
    \caption{Attention visualization. $A_\mba$, $A_\mbu$, and $A_\mbe$ represent attention maps for audio, action units, and valence/arousal, respectively.}
    \label{fig:attn vis}
\end{figure}

We measure V-RMSE, A-RMSE, V-SA, and A-SA. The root mean square error RMSE is defined as: 
\begin{eqnarray}
    \text{RMSE} = \sqrt{\frac{1}{N}\sum^N_{i=1}(\calE^{pred}_i-\calE^{true}_i)^2},
\end{eqnarray}
where $\calE^{true}_i$ denotes the selected valence or arousal point used as the condition, $\calE^{pred}_i$ represents the valence or arousal value estimated by EmoNet~\cite{toisoul2021estimation} for the $i$-th frame sample, and $N$ denotes the number of frames.

The sign agreement SA is defined as: 
\begin{eqnarray}
    \text{SA} = \frac{1}{N}\sum^N_{i=1}\mathbb{I}(\text{sign}(\calE^{pred}_i)==\text{sign}(\calE^{true}_i)),
\end{eqnarray}
where $\mathbb{I}(\cdot)$ is the indicator function that returns 1 if the condition inside is true and 0 if false. The $\text{sign}(\cdot)$ function outputs 1 if the input is positive, -1 if it is negative, and 0 if it is zero.

Additionally, we utilize frame-wise emotion classification accuracy. Since assigning a single precise emotion class label to each valence-arousal pair is challenging, we evaluate the performance using top-3 accuracy. EmoNet~\cite{toisoul2021estimation} predicts emotion class labels from images rendered by each talking head synthesis model, and the accuracy is measured by comparing the predicted class labels with the predefined emotion class labels. Predefined emotion class labels are assigned to each valence-arousal point as follows:

\noindent\([0.74, 0.31]: \ \text{Happy}, \quad [0.31, 0.74]: \ \text{Surprise},\) \\
\([-0.31, 0.74]: \ \text{Angry}, \quad [-0.74, 0.31]: \ \text{Disgust},\) \\
\([-0.74, -0.31]: \ \text{Sad}, \quad [-0.31, -0.74]: \ \text{Sad}, \) \\
\([0.31, -0.74]: \ \text{Contempt}, \quad [0.74, -0.31]: \ \text{Contempt}, \) \\
\([0.35, 0.35]: \ \text{Happy}, \quad [-0.35, 0.35]: \ \text{Angry}, \) \\
\([-0.35, -0.35]: \ \text{Sad}, \quad [0.35, -0.35]: \ \text{Contempt}\).

\section{Attention Visualization}
We apply an attention mechanism before feeding the audio features $\mba$ and action units $\mbu$ into the manipulation network $f^\text{F}$, as described in Eq.~5 of the main paper, and the valence/arousal $\mbe$ into the emotion manipulation network $f^\text{E}$, as described in Eq.~6 of the main paper. The attention maps, denoted as $A_\mba$, $A_\mbu$, and $A_\mbe$, are visualized in Fig.~\ref{fig:attn vis}. The audio attention map $A_\mba$ focuses primarily on the regions around the lips and jaw, capturing movements conditioned on input speech audio. The action units attention map $A_\mbu$ emphasizes areas around the eyelids and eyebrows, highlighting expressive changes based on action units, such as eye blinking and eyebrow movements. Meanwhile, the emotion attention map $A_\mbe$ concentrates on the mouth and eyes, emphasizing features crucial for reflecting emotional expressions.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/mouth_normal.pdf}
    \caption{The normal map is estimated by the method in \cite{Abrevaya_2020_CVPR}. The row below shows zoomed-in RGB and normal map images of the inside-mouth region, masked with the refined mask described in Sec.~\ref{sec:processing mask}.}
    \label{fig:inside-mouth normal map}
\end{figure}

\section{Inside-mouth Normal Map}
Fig.~\ref{fig:inside-mouth normal map} presents an RGB image and a normal map of the inside-mouth region. The normal map is estimated by \cite{Abrevaya_2020_CVPR}. The normal map of the inside-mouth region is not well estimated, as shown by the inconsistency between the upper and lower teeth. It leads to unstable optimization of the 3D canonical Gaussians $\theta^\text{M}_C$, along with unstable training of the tri-plane hash encoder $H^\text{M}$ and the manipulation network $f^\text{M}$ for inside-mouth region.

\section{Limitations of Simple Fused Approach}
We fuse TalkingGaussian~\cite{li2024talkinggaussian} with our lip-aligned emotional face generator $g^\text{LEF}$ and visualize the results in Fig.~\ref{fig:direct emostyle}. Although our face generator $g^\text{LEF}$ adjusts emotions based on a valence/arousal setting of (0.5, 0.1), which corresponds to a slight smile, it does not align the teeth shape with the output of TalkingGaussian.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/direct_emostyle.pdf}
    \caption{The results of directly applying our lip-aligned emotional face generator $g^\text{LEF}$ to the results of TalkingGaussian (TG)~\cite{li2024talkinggaussian}.}
    \label{fig:direct emostyle}
\end{figure}

\section{Limitations of Diffusion-based Model}
There are two primary approaches to generate emotional facial images: GAN-based models and diffusion-based models. In our task, there are numerous possible combinations of valence, arousal, and video frames, making it impractical to generate and store emotional facial images in advance. Therefore, we generate the images during the training of EmoTalkingGaussian. However, diffusion-based face generation models are inherently slow, which inevitably increases training time. Additionally, these models tend to significantly alter the pose and style of the source face image, often resulting in a lack of synchronization with the source image and producing unrealistic appearances. To modify the emotion in facial images using diffusion-based models, we use Arc2Face~\cite{paraperas2024arc2face} and ControlNet~\cite{zhang2023adding}, which require conditions rendered from the FLAME model~\cite{li2017learning}. We utilize the normal map rendered from the FLAME mesh as the condition. The FLAME mesh incorporates pose and shape parameters obtained from the source image, along with expression parameters that represent smiling-related features. Although these methods enable changes in emotion, as shown in Fig.~\ref{fig:diffusion}, the resulting face undergoes significant alterations and appears unnatural. Therefore, we utilize GAN-based models for more efficient and effective generation of emotional facial images.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/diffusion.pdf}
    \caption{Result from diffusion-based models, Arc2Face~\cite{paraperas2024arc2face}, and ControlNet~\cite{zhang2023adding}.}
    \label{fig:diffusion}
\end{figure}

\section{User Study}
In Table~\ref{tab:user study}, we report the results of a user study conducted to evaluate the reflection of emotions, lip synchronization to the audio, and overall quality. While lip synchronization performance is comparable across models, our method demonstrates the highest capability in reflecting the desired emotions and receives the most selections for overall quality.

\begin{table}[h]
    \centering
    \begin{tabular}{l|ccc}
        \hline
          & SD1 (\%) & SD2 (\%) & SD3 (\%) \\
         \hline
         ER-NeRF~\cite{li2023efficient} & 4.17 & 15.83 & 16.25 \\
         GaussianTalker~\cite{cho2024gaussiantalker} & 10.00 & 31.67 & 25.84 \\
         TalkingGaussian~\cite{li2024talkinggaussian} & 5.42 & 14.58 & 11.25 \\
         Ours & \textbf{69.58} & \textbf{32.92} & \textbf{43.33} \\
         none & 10.83 & 5.00 & 3.33 \\
         \hline
    \end{tabular}
    \caption{The proportion of videos selected by users for each evaluation criterion is presented in this table. `SD1' denotes Standard 1, representing the proportion of videos chosen by users as best reflecting emotions. `SD2' represents Standard 2, indicating the proportion of videos selected as best in lip synchronization to the audio, and `SD3' denotes Standard 3, reflecting the proportion of videos rated as best in overall quality.}
    \label{tab:user study}
\end{table}

\section{Qualitative Results}
We present the qualitative results across three scenarios: self-reconstruction, cross-domain audio, and emotion-conditioned generation, as shown in Figs.~\ref{fig:scenario1}, \ref{fig:scenario2}, and \ref{fig:scenario3}. For the each scenario, we compare our results with those of other models, including ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker}, and TalkingGaussian~\cite{li2024talkinggaussian}. In the emotion-conditioned scenario, we demonstrate the results using valence and arousal values of 0.31 and 0.74, respectively. Additionally, we illustrate our method's ability to reflect the desired emotion in the rendered face, as shown in Fig~\ref{fig:change emotion}. Furthermore, we showcase the transition of emotions by changing the valence and arousal values. For a detailed view of the continuous transitions in facial emotions and additional results, please refer to the supplementary video, including emotion-conditioned scenario comparison, valence-arousal interpolation, 360Â° valence-arousal interpolation (radius: 0.8), and dynamic emotion transitions during speech.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/scenario1.pdf}
    \caption{We present the qualitative comparisons in the self-reconstruction scenario against other methods, including ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker}, and TalkingGaussian~\cite{li2024talkinggaussian}. Misalignment with the ground truth is highlighted using \textcolor{gray}{gray} arrows for discrepancies in the eyebrows and forehead wrinkles, \textcolor{blue}{blue} arrows for blinking errors, and \textcolor{red}{red} boxes for lip misalignment.}
    \label{fig:scenario1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/scenario2.pdf}
    \caption{We present qualitative comparisons in the cross-domain audio scenario against other methods, including ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker}, and TalkingGaussian~\cite{li2024talkinggaussian}. Lip misalignment with the ground truth is highlighted using \textcolor{red}{red} boxes.}
    \label{fig:scenario2}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/scenario3.pdf}
    \caption{We demonstrate qualitative comparisons in the emotion-conditioned scenario against other methods, including ER-NeRF~\cite{li2023efficient}, GaussianTalker~\cite{cho2024gaussiantalker}, and TalkingGaussian~\cite{li2024talkinggaussian}. The valence and arousal values are set to 0.31 and 0.74, respectively, to represent the emotion of surprise. We use \textcolor{blue}{blue} boxes to highlight the area around the eyes, emphasizing the expression of emotion. The words being pronounced by the speaker in each column are highlighted in \textcolor{red}{red}.}
    \label{fig:scenario3}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/supplementary/change_emotion.pdf}
    \caption{We demonstrate our proposed method's ability to reflect the desire emotion in the rendered face. Each row utilizes specific valence-arousal values: (0.3, 0.7), (0.8, -0.5), (-0.6, 0.5), and (-0.8, -0.2). The first row changes the valence/arousal from (0.3, 0.7) to (-0.8, 0.5). The second row changes the valence/arousal from (0.8, -0.5) to (0.6, 0.5). The third row changes the valence/arousal from (-0.6, 0.4) to (0.8, 0.2). The fourth row changes the valence/arousal from (-0.8, -0.2) to (0.3, 0.6). Additionally, the words being pronounced by the speaker in each column are highlighted in \textcolor{red}{red}.}
    \label{fig:change emotion}
\end{figure*}