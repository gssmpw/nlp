\section{Related Work}
\label{sec:related_work}
\subsection{3D Gaussian Splatting}
3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} was proposed to address the high computational cost and slow rendering speed issues faced by NeRF~\cite{mildenhall2021nerf}. 3DGS enables fast and efficient rendering, making it highly suitable for real-time applications. 3DGS represents scenes using point-based 3D Gaussians, where each Gaussian contains attributes such as position, scale, rotation, color, and opacity. These Gaussians are aggregated and rendered efficiently using fast differentiable rasterization. In addition to its rendering speed, 3DGS maintains high visual quality, producing detailed and accurate representations of complex scenes, making it suitable for high-fidelity rendering tasks. Thanks to its efficiency and high fidelity, recent research has extended to complex 3D representations of humans~\cite{pang2024ash,jiang2024hifi4g,moon2024exavatar,zhou2024hugs,hu2024gauhuman,pokhariya2024manus,li2024talkinggaussian}, demonstrating potential for diverse applications.

\subsection{Emotional Face Synthesis}
% Emotional face synthesis methods utilize various networks such as Conditional GANs~\cite{mirza2014conditional}, StarGAN~\cite{choi2018stargan}, StyleGAN~\cite{karras2019style}, StyleGAN2~\cite{karras2020analyzing}, and diffusion models~\cite{ho2020denoising,song2020denoising}. 
%
Some emotional face synthesis approaches use Conditional GANs~\cite{mirza2014conditional}, which are conditioned on either a one-hot encoding vector~\cite{ding2018exprgan} or a continuous emotion vector~\cite{lindt2019facial,d2021ganmut}. Lindt~\etal~\cite{lindt2019facial} take valence and arousal values as input to generate corresponding face images. However, they mentioned that preserving the identity becomes difficult if the input image expresses the extreme emotion. Ding~\etal~\cite{d2021ganmut} update valence and arousal vectors so that the generated images are classified as desired emotions.
%
Some methods~\cite{pumarola2018ganimation,kollias2020va} employ StarGAN~\cite{choi2018stargan} to manipulate the emotion in the input image. Pumarola~\etal~\cite{pumarola2018ganimation} use action units (AUs) to control face muscles, while Kollias~\etal~\cite{kollias2020va} use valence and arousal values to control the emotions of the generated images. StyleGAN~\cite{choi2018stargan} and StyleGAN2~\cite{karras2020analyzing} are utilized in several approaches~\cite{abdal2021styleflow,harkonen2020ganspace,khodadadeh2022latent,patashnik2021styleclip,shen2020interpreting,azari2024emostyle}. 
% Abdal~\etal~\cite{abdal2021styleflow} explored the entangled nature of the GAN latent space to prevent the unwanted distortion by other attributes. 
% Harkonen~\etal~\cite{harkonen2020ganspace} highlighted the importance of the direction in the GAN latent space by applying principal component analysis (PCA). Khodadadeh~\etal~\cite{khodadadeh2022latent} introduced a latent-to-latent mapper and a combination of losses to preserve the identity while editing facial attributes. StyleCLIP~\cite{patashnik2021styleclip} enables facial image editing with text prompts, leveraging CLIP~\cite{radford2021learning}. 
EmoStyle~\cite{azari2024emostyle} uses valence and arousal values to control facial emotions and it proposed a combination of multiple losses related to pixel, landmark, identity, and emotion.
%
Diffusion~\cite{ho2020denoising,song2020denoising} mechanism-based methods utilize the 3D face mesh model, FLAME~\cite{li2017learning}, to create surface normals, albedo, and Lambertian renderings for conditioning~\cite{ding2023diffusionrig}. These methods~\cite{paskaleva2024unified, paraperas2024arc2face} extend the valence and arousal space into 3D to express more diverse emotions or employ ID vectors to generate desired facial identities.

% Although some existing methods can control emotion using continuous valence and arousal, the mouth region in the generated images often does not align properly with that of the input images. When using these generated images to train 3D Gaussian-based talking head models, this mouth misalignment leads to poor performance in emotional talking head synthesis.

\subsection{Audio-driven Talking Head Synthesis}
2D-based talking head synthesis approaches~\cite{prajwal2020lip,zhang2023dinet,chen2018lip,chung2017out,zhou2019talking,song2018talking,gan2023efficient,wang2023progressive} have advanced by utilizing intermediate representations such as motion~\cite{wang2021audio2head,chen2019hierarchical,wang2022one} and landmarks~\cite{zakharov2019few,zhou2020makelttalk,zhong2023identity,zhong2023identity}. However, these 2D-based methods struggle to maintain naturalness and consistency, especially when there are large changes in head pose.

3D-based talking head synthesis methods~\cite{guo2021ad,li2023efficient,ye2023geneface,shen2022learning,tang2022real,li2024s,cho2024gaussiantalker} utilize neural radiance fields~\cite{mildenhall2021nerf} (NeRF) or 3D Gaussians splatting~\cite{kerbl20233d} (3DGS) to generate a photo-realistic and personalized head models. Recently, TalkingGaussian~\cite{li2024talkinggaussian} proposed the 3DGS-based method for audio-driven talking head synthesis. It employs audio features to synchronize the lips with the input audio and utilizes the action units (AUs) to manipulate facial expressions. However, because it is trained on only a 3-5 minute video, it struggles to represent a continuous and wide range of emotions. This limitation is not exclusive to TalkingGaussian but is a common issue among 3D-based approaches. To address this issue, He~\etal~\cite{he2024emotalk3d} collected EmoTalk3D dataset and proposed \textit{Speech-to-Geometry-to-Appearance} framework. However, because this method is data-driven and person-specific, it has the drawback of requiring data collection for new individuals.