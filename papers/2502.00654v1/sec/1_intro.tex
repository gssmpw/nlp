\section{Introduction}
\label{sec:intro}
3D Gaussian splatting (3DGS)~\cite{kerbl20233d} has been recently established as the alternative to neural radiance fields (NeRF)~\cite{mildenhall2021nerf}, offering substantial improvements in both rendering speed and quality. Talking head synthesis domain also reflects the trend: NeRF-based approaches~\cite{peng2024synctalk,ye2023geneface,tang2022radnerf,shen2022learning,li2023efficient,guo2021ad,chatziagapi2023lipnerf} have been prevailed; while they are being rapidly replaced by the 3D Gaussian splatting-based approaches~\cite{li2024talkinggaussian,cho2024gaussiantalker,yu2024gaussiantalker,he2024emotalk3d}, thanks to its real-time speed and high-fidelity rendering quality.

Despite the advancements, we argue that existing pipelines lack the important aspect of human \emph{emotions}. We observed that existing models~\cite{li2024talkinggaussian,cho2024gaussiantalker} are able to handle basic facial expressions such as \emph{eye blinking} and \emph{eyebrow movement}, as seen in the 3-5 minute training video; however they struggle to represent continuous and diverse emotions such as \emph{happy}, \emph{sad}, \emph{angry}, etc. When talking, humans convey diverse emotions. To achieve the truly life-like talking heads by filling the gap, we insist that synthesized talking heads need to represent such diverse human emotions while talking. Although He~\etal~\cite{he2024emotalk3d} proposed a method that relies on collecting new data, a significant drawback is the high cost involved, as additional data must be captured to train a 3D Gaussian model for each new person.

% Furthermore, in our preliminary experiment, we observed that it is especially hard to simultaneously achieve  synchronizing lips with speeches and representing diversity in emotions. If we try to manipulate facial images to reflect emotions, the lip is transformed and becomes unsynchronized with the given speech as in Fig.~\ref{fig:teaser}. 

%Given unseen speeches, many approaches~\cite{} fail to achieve the lip synchronization. Also, though some existing 3DGS-based methods~\cite{li2024talkinggaussian,he2024emotalk3d,yu2024gaussiantalker} are able to handle basic facial expressions such as \emph{happy}, \emph{sad}, \emph{disgust}, etc., they struggle to represent continuous and diverse emotions. These two characteristics are essential to accomplish the realistic human avatars.

%Especially, emotional expression is critical since users will interact with these virtual avatars, and human communication heavily relies on emotional exchange. Additionally, %These limitations significantly hinder the creation of truly lifelike and emotionally engaging talking heads.


%, driven by the need for both rapid rendering and high visual fidelity.
%Real-time inference is crucial in synthesizing talking heads, since the synthesized faces need to respond to users immediately in applications such as virtual assistants, entertainment, and education. 
 
% In this paper, we propose EmoTalkingGaussian that integrates continuous emotional expression into 3D Gaussian splatting-based talking head synthesis. Specifically, (1) we propose to manipulate 3D Gaussians conditioned on continuous values, \ie, valence and arousal~\cite{russell1980circumplex}, to reflect diverse facial emotions; (2) we propose an lip-aligned emotional face generator to guide the training of our talking head synthesis pipeline with lip-aligned emotional faces;
% %augment the data and to enhance the emotional diversity of 3D synthesized talking heads; 
% (3) we propose to leverage normal maps to mitigate the domain gap between real and synthetic images, and improve the quality of rendered images; and (4) we propose a self-supervised learning method that leverages a text-to-speech network and a visual-audio synchronization network to further ensure accurate lip synchronization with in-the-wild audio signals.

% To manipulate facial emotions, we utilize valence and arousal as conditions of EmoTalkingGaussain, which are continuous variables representing the degree of positiveness or negativeness, and the level of excitability or calmness, respectively. These values range from -1 to 1, enabling continuous adjustments to facial emotions, as shown in Fig.~\ref{fig:teaser}. To train EmoTalkingGaussian, we augment emotional facial images to have diverse emotions which are not seen in the original train video. %using EmoStyle~\cite{azari2024emostyle}. 
% %\seung{In our preliminary experiment, diffusion-based models~\cite{paraperas2024arc2face,ding2023diffusionrig,paskaleva2024unified} were considered; however  we observed that it frequently suffers from preserving the pose and identity of the source images~\cite{huang2024diffusion}, when conditioning on continous emotions.} %we use the StyleGAN2~\cite{karras2020analyzing}-based EmoStyle because %modifying emotions while %diffusion-based models cannot modify emotions with continuous values such as valence and arousal while 
% However, when training EmoTalkingGaussian with data obtained from the pre-trained EmoStyle, a mismatch arises between lip movements and speeches, making the result look unnatural, as EmoStyle does not handle their alignment, as shown in Fig.~\ref{fig:motivate finetune emotstyle}. To resolve this, we modify EmoStyle to better align the lip movements with the input image. Furthermore, to mitigate the domain gap between real images and synthetic images generated by modified EmoStyle, we apply a loss function that leverages normal maps generated by \cite{Abrevaya_2020_CVPR}. To improve the synchronization of lip movements with cross-domain speech, we use a text-to-speech algorithm~\cite{gTTS} to generate an optimal speech dataset that is small but diverse in English pronunciation. By incorporating SyncNet~\cite{chung2017out}, we apply a loss function that measures the alignment between the rendered image and input speech, improving synchronization.

In this paper, we propose EmoTalkingGaussian that integrates continuous emotional expression into 3D Gaussian splatting-based talking head synthesis. To manipulate facial emotions, we utilize valence and arousal~\cite{russell1980circumplex} as conditions of EmoTalkingGaussain. Valence represents the degree of positiveness or negativeness, and arousal indicates the level of excitability or calmness, both ranging from -1 to 1. Unlike action units~\cite{ekman1978facial} that capture basic facial expressions such as \emph{eye blinking}, valence and arousal enable continuous adjustments to facial emotions, (\eg, \emph{happy}, \emph{surprise}, \emph{sad}, etc), as shown in Fig.~\ref{fig:teaser}. To train EmoTalkingGaussian on diverse emotional facial images not seen in the original train video, one solution is to augment the data using EmoStyle~\cite{azari2024emostyle}, which modifies the emotion of a source image based on valence/arousal inputs.
%using EmoStyle~\cite{azari2024emostyle}. 
%\seung{In our preliminary experiment, diffusion-based models~\cite{paraperas2024arc2face,ding2023diffusionrig,paskaleva2024unified} were considered; however  we observed that it frequently suffers from preserving the pose and identity of the source images~\cite{huang2024diffusion}, when conditioning on continous emotions.} %we use the StyleGAN2~\cite{karras2020analyzing}-based EmoStyle because %modifying emotions while %diffusion-based models cannot modify emotions with continuous values such as valence and arousal while 
However, when training EmoTalkingGaussian with data obtained from the pre-trained EmoStyle, a mismatch arises between lip movements and speech audio, as EmoStyle does not handle their alignment, as shown in Fig.~\ref{fig:motivate finetune emotstyle}. To resolve this, we propose a lip-aligned emotional face generator to better align the lip movements with the source image while effectively reflecting the intended emotions based on valence/arousal. Furthermore, to mitigate the domain gap between real images and synthetic images generated by our lip-aligned emotional face generator, we apply a loss function that leverages normal maps generated by \cite{Abrevaya_2020_CVPR}. To improve the synchronization of lip movements with in-the-wild audio samples, we use a text-to-speech network~\cite{gTTS} to generate curated speech audio data that is small but diverse in English pronunciation. By incorporating SyncNet~\cite{chung2017out}, we apply a loss function that encourages the alignment between input audio and the image rendered by EmoTalkingGaussian, improving synchronization.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/main/motivate_finetune_emostyle.pdf}
    \vspace{-3mm}
    \caption{(a) shows the source image, (b) and (c) represent images for the `\emph{happy\&surprise}' emotion (valence of 0.8, arousal of 0.6), which are generated by EmoStyle~\cite{azari2024emostyle} and our lip-aligned emotional face generator, respectively. 
    % EmoStyle~\cite{azari2024emostyle} significantly changes lip regions (in \textcolor{red}{red} boxes); since it only reflects valence/arousal values, without any constraints on lips. Our lip-aligned emotional face generator is able to synthesize faces reflecting valence/arousal values, while retaining lips.
    %Since EmoStyle does not consider the lip alignment, training our talking head synthesis model using the data it generates can cause lip movements to be out of sync with the input audio.
    }
    \label{fig:motivate finetune emotstyle}
    \vspace{-3mm}
\end{figure}

The main contributions are summarized as follows:
\begin{itemize}
    \item We propose EmoTalkingGaussian, an audio-driven talking head generation model that leverages valence and arousal to render continuous emotional expressions without requiring additional data capturing.

    \item We introduce self-supervised learning with a sync loss to improve lip synchronization, utilizing a curated speech audio dataset generated via a text-to-speech network.

    \item Extensive experiments demonstrate that EmoTalkingGaussian effectively renders diverse emotional talking head with lip movements synchronized to the input audio, surpassing the limitations in emotional expressiveness of existing state-of-the-art methods.
\end{itemize}

% Contribution

% Figure 1: Baseline does not manipulate the emotion using the valence and arousal. but ours can do it. Simple EmoStyle do not take account into the lip.

% Small area of valence and arousal in the training video.

% However, training the 3DGS network on a single monocular video is not sufficient to expose the model to a wide range of emotions. This is because the training video typically contains specific emotions tied to particular situations (e.g., speeches, news broadcasts). To address this issue, data containing rich emotional facial expressions while preserving identity should be generated.