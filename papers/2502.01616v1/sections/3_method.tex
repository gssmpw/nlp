\section{Preliminaries} \label{sec:background}

We consider the standard RL setup~\cite{sutton2018reinforcement} where an agent interacts with an environment in discrete time. Formally, at each timestep $t$, the agent observes a state $s_t$ from the environment and selects an action $a_t$ according to its policy $\pi$. In response, the environment provides a reward $r_t$ and transitions the agent to the next state $s_{t+1}$. The return $R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$ represents the discounted sum of future rewards starting at timestep $t$, where $\gamma \in [0, 1)$ is the discount factor. The goal of RL is to maximize the expected return from each state $s_t$ under the agent's policy.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/vlm.pdf}
    \caption{VLM reward (Eqn.~\ref{eq:cosine_sim}) for an optimal trajectory given the task description \textit{``Open a door with a revolving joint.''} Although the reward reflects partial task progression, it is noisy and poorly aligned with the actual task progress, as evident from the image observations. 
    % \textcolor{red}{Better to use PDF format and shorten the reward figure.}
    }
    \label{fig:noisy_reward}
\end{figure}
\subsection{Learning rewards from preferences} \label{sec:pref}
We adopt the standard preference-based learning framework, where a teacher provides preferences over pairs of trajectory segments, and a reward function $r_\theta$ is learned to align with these preferences. Formally, a trajectory segment $\sigma$ is defined as a sequence of observations and actions: $\sigma = \{(s_1, a_1), (s_2, a_2),...,(s_T, a_T)\}$. Given a pair of segments $(\sigma_0, \sigma_1)$, preferences are expressed as $y \in \{(0,1), (1,0)\}$, where $(1,0)$ indicates $\sigma_0 \succ \sigma_1$ while $(0,1)$ indicates $\sigma_1 \succ \sigma_0$. Here $\sigma_i \succ \sigma_j$ implies segment $i$ is preferred to segment $j$. The probability of one segment being preferred over another is modeled via the Bradley-Terry model~\cite{bradley1952rank}:
\begin{equation}
\label{eq:btmodel}
    P_{\theta}[\sigma_1 \succ \sigma_0] = \frac{\exp{\sum_{t}} r_{\theta}(s_t^1, a_t^1)}{\sum_{i\in\{0,1\}}\exp{\sum_{t}} r_{\theta}(s_t^i, a_t^i)} \ .
\end{equation}
To train the reward function $r_\theta$, we minimize the cross-entropy loss between the predicted preferences $P_{\theta}$ and the observed preferences $y$,
\begin{equation}\label{eq:btloss}
\begin{split}
    \mathcal{L}_{\text{pref}} = - \mathbb{E}_{(\sigma_0,\sigma_1,y)\sim D} \big[&y(0)P_\theta[\sigma_0 \succ\sigma_1] + \\ &y(1)P_\theta[\sigma_1\succ\sigma_0]\big] \ .
\end{split}
\end{equation}
In preference-based RL, a policy $\pi_\phi$ and the reward
function $r_\theta$ are updated in an alternating fashion. First, the reward function is updated using the sampled preferences as described above. Next, the policy is optimized to maximize the expected cumulative reward under the learned reward function using standard RL algorithms. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/prefvlm.pdf}
    \caption{Overview of our approach. Given a task description, \algo~iteratively updates the policy $\pi_\phi$ via reinforcement learning using the reward model $r_\theta$. Trajectory segments from the replay buffer are sampled and labeled with VLM-generated preferences. These samples are then classified as clean or noisy using thresholds $\tau_{upper}$ and $\tau_{lower}$. A budgeted subset of noisy samples is sent for human annotation. The reward model is trained on both VLM and human-labeled preferences, while the VLM is fine-tuned using human annotations and replay buffer samples. 
    }
    \label{fig:overview}
\end{figure*}
\subsection{VLM as a reward model} \label{sec:vlm}
The use of VLMs in RL has primarily centered around CLIP-style models~\cite{radford2021learning}. These models comprise a language encoder $\mathcal{F}_L$ and an image encoder $\mathcal{F}_I$, which map text and image inputs into a shared latent space. Through contrastive learning on image-caption pairs, often augmented with task-specific or dynamics-aware objectives~\cite{ma2023liv}, VLMs align textual and visual representations effectively.

Given the image observation $o_t$ corresponding to a state $s_t$, and the language description of the task $l$, most works~\cite{mahmoudieh2022zero,adeniji2023language,fu2024furl} define the reward as:
\begin{equation}
\label{eq:cosine_sim}
    r^{vlm}_t = \frac{\langle \mathcal{F}_{L}(l), \mathcal{F}_{I}(o_t) \rangle}{\Vert \mathcal{F}_{L}(l) \Vert \cdot \Vert \mathcal{F}_{I}(o_t) \Vert} \ .
\end{equation}
However, this reward is often too coarse for fine-grained tasks like manipulation. Figure~\ref{fig:noisy_reward} shows the VLM reward curve for an expert trajectory, computed using~\eqref{eq:cosine_sim}. Ideally, the curve should align with the expert’s progress, assigning higher rewards as the state nears task completion. While the curve captures some aspects of task progress, it fails to fully reflect the expert’s trajectory. Given these limitations, fine-tuning VLMs with human feedback becomes essential. 
% Human preferences are particularly well-suited for this purpose, as they are simpler to provide - requiring only a comparison between trajectory segments rather than detailed numerical evaluations. To harness the strengths of both, we propose a hybrid framework: VLMs are used to generate coarse feedback over trajectory segments, which is then complemented and refined through targeted human feedback.



\section{Method} \label{sec:method}
In this section, we present \algo, a framework designed to minimize the reliance on time-intensive human supervision in preference-based RL by leveraging VLMs. 
We first outline how VLMs are utilized to provide preference feedback for training a reward model (Sec.~\ref{sec:method_vlm_as_pref}). Then, we address the limitations of directly applying VLMs to new environments and propose a data-efficient adaptation approach that combines self-supervised learning with minimal human feedback to align the VLM with the environment's dynamics (Sec.~\ref{sec:method_vlm_adapt}). 
Finally, to ensure robust training in the presence of noisy machine-generated feedback, we introduce a noise-mitigation mechanism that selectively identifies unreliable VLM outputs and refines them with minimal human input, achieving more stable and accurate reward model learning (Sec.~\ref{sec:method_sample_select}). 
Figure~\ref{fig:overview} provides an overview of our approach.



\subsection{VLM as a preference feedback model} \label{sec:method_vlm_as_pref}
To leverage VLMs for generating preference feedback, we begin by extracting image sequences corresponding to each trajectory segment. 
Specifically, for a given pair of segments $(\sigma_0, \sigma_1)$, we extract the image sequences $(O_0, O_1)$, where each sequence is defined as $O_i = \{o^i_0, o^i_1, o^i_2, \dots, o^i_T\}$ for $i \in \{0, 1\}$. 
Here, $o_t$ represents the image observation of the state $s_t$ at the $t$-th time step.

Using the language description of the task $l$, we compute the return for each segment as $R_i = \sum_{t=0}^T r^{vlm}_t$, with $r^{vlm}_t$ the reward at time step $t$ derived from the VLM via~\eqref{eq:cosine_sim}. 
Based on these returns, the preference label $\tilde{y}$ is assigned as: 
\begin{equation} \label{eq:vlm_pref} 
    \tilde{y} = \begin{cases} 
        (0, 1), & \text{if } R_0 > R_1, \\ 
        (1, 0), & \text{otherwise}. 
    \end{cases} 
\end{equation}
These preferences are then used to train a reward model $r_\theta(s_t, a_t)$ by minimizing the preference loss in~\eqref{eq:btloss}. 
The trained reward model can be integrated into a preference-based RL algorithm for policy optimization. 
In this work, we specifically leverage PEBBLE~\cite{lee2021pebble}, a preference-based RL framework that combines unsupervised pre-training with off-policy learning using Soft Actor-Critic (SAC)~\cite{haarnoja2018soft}.


\subsection{VLM adaptation} \label{sec:method_vlm_adapt}
A key challenge in directly applying VLMs to downstream RL tasks is the domain gap between their pretraining data and the target environment~\cite{raychaudhuri2021cross,fu2024furl}. 
This mismatch often results in noisy or misaligned feedback. 
To address this, we propose a data-efficient adaptation strategy that combines minimal human feedback with self-supervised learning to align the VLM with the target environment's dynamics.

We freeze the VLM and introduce two learnable layers, $\mathcal{G}_L$ and $\mathcal{G}_I$, on top of the language and image embedding layers, respectively. 
These layers are fine-tuned to adapt the VLM. 
The layer $\mathcal{G}_L$ processes the language embedding $\mathcal{F}_L(l)$ of the task description $l$ and produces an adapted text embedding, $\mathcal{G}_L\circ\mathcal{F}_L(l)$. 
Similarly, for each image observation $o_t$, the adapted image embedding is generated by $\mathcal{G}_I\circ\mathcal{F}_I(o_t)$. 
Using these adapted embeddings, the reward function for preference feedback is updated as: 
\begin{equation} \label{eq:cosine_sim_aligned} 
    r^{vlm}_t = \frac{\langle \mathcal{G}_L\circ\mathcal{F}_L(l), \mathcal{G}_I\circ\mathcal{F}_I(o_t) \rangle}{\Vert \mathcal{G}_L\circ\mathcal{F}_L(l) \Vert \cdot \Vert \mathcal{G}_I\circ\mathcal{F}_I(o_t) \Vert}. 
\end{equation}
A small number of human-provided preferences are sampled to fine-tune the VLM using the preference loss~\eqref{eq:btloss}. 
The dense rewards for training are derived from the updated similarity measure~\eqref{eq:cosine_sim_aligned}. 
The methodology for selecting human feedback samples is detailed in Section~\ref{sec:method_sample_select}.

We also fine-tune the VLM using an unsupervised objective designed to align the VLM embeddings with environment dynamics. 
Given the current observation $o_t$, action $a_t$, and the next observation $o_{t+1}$, we train the VLM to predict the action which leads to the transition between observations: 
\begin{equation} \label{eq:inv_loss} 
    \Vert f\left(\mathcal{G}_I\circ\mathcal{F}_I(o_t), \mathcal{G}_I\circ\mathcal{F}_I(o_{t+1})\right) - a_t \Vert^2, 
\end{equation} 
where $f$ is a linear layer. 
This encourages the adapted embeddings to capture task-relevant dynamics, improving the precision of preference feedback.

\subsection{Noise mitigation and human feedback} \label{sec:method_sample_select}
Machine-generated preferences, while scalable, are prone to noise and lack the reliability afforded by human-provided annotations. 
To ensure robust training, it is thus crucial to distinguish between accurate and noisy samples. 
This categorization not only improves the stability of the reward model training but also optimizes the use of human feedback by focusing on refining the noisy samples. 

\noindent \textbf{Identifying noisy samples.} Our approach is grounded in insights from research on noisy training~\cite{li2020gradient,cheng2024rime}, which shows that deep neural networks tend to learn generalizable patterns in the early stages of training before overfitting to noise in the data. Leveraging this observation, we prioritize samples with lower training losses as clean, while treating high-loss samples as potential candidates for human review. 

To formalize this, we use the preference loss defined in~\eqref{eq:btloss} to train the reward model $r_\theta$. 
Assuming the loss for clean samples is bounded by $\rho$, \citet{cheng2024rime} have shown that the KL divergence between the predicted preference distribution $P_{\theta}$ and the true preference label $\tilde{y}$ for a sample $(\sigma_0, \sigma_1)$ is lower-bounded, that is:
\begin{equation}\label{eq:KL_div} 
    D_{KL}(\tilde{y}\Vert P_\theta) \geq -\ln{\rho} + \frac{\rho}{2} + \mathcal{O}(\rho^2) \ . \end{equation}
%
To filter out unreliable samples, we take a lower bound on the KL divergence, $\tau_{base} = -\ln{\rho} + \alpha \rho$, where $\rho$ is the maximum loss calculated on the filtered samples from the latest update, and $\alpha$ is a tunable hyperparameter, with a theoretical range of $(0, 0.5]$. 
However, because of shifts in the state distribution during RL training, we employ another auxiliary term $\tau_{unc} = \beta_{t} \cdot s_{KL}$, to account for uncertainty. 
Here, $\beta_{t}$ is a time-dependent parameter, and $s_{KL}$ is the standard deviation of KL divergence. 
The corrected lower bound is given by $\tau_{lower} = \tau_{base} + \tau_{unc}$. 
To control the uncertainty over time, $\beta_t$ follows a linear decay schedule that allows greater tolerance for noisy samples in the early stages of training while becoming more conservative as training progresses. 
Specifically, $\beta_t = \max(\beta_{min}, \beta_{max} - kt)$, where $\beta_{min}$ and $\beta_{max}$ are fixed values, and $k$ is a constant that controls the rate of decay. 

\noindent \textbf{Handling noisy samples.} Samples with a KL divergence below $\tau_{lower}$ are considered clean and are incorporated into the reward model training: 
\begin{equation} 
\label{D_lower}
    \mathcal{D}_{\tau_{l}} = \{(\sigma^0, \sigma^1, \tilde{y}) : D_{KL}(\tilde{y} \Vert P_\theta(\sigma_0, \sigma_1)) < \tau_{lower}\} \ . 
\end{equation}
%
Conversely, samples with a KL divergence exceeding a higher threshold $\tau_{upper}$ are presumed to contain noisy labels. To preserve their utility, we relabel these samples by flipping their predicted labels and include them in a separate dataset:
\begin{equation}
\label{D_upper}
    \mathcal{D}_{\tau_u} = \{(\sigma_0, \sigma_1, \mathbf{1}-\tilde{y}) : D_{KL}(\tilde{y} \Vert P_\theta(\sigma_0, \sigma_1)) > \tau_{upper}\} \ .
\end{equation}
%
The remaining samples, with KL divergence between $\tau_{lower}$ and $\tau_{upper}$, are deemed uncertain, and we randomly sample from them based on the available human annotation budget. 
These samples are particularly valuable, as both the VLM and reward model struggle to assign reliable labels. 
By focusing human effort on this subset, $\mathcal{D}_{h}$, we ensure that annotations address the most challenging cases. 
% \amit{How are all the thresholds determined? Do you have an analysis of the sensitivity to the thresholds? These thresholds seem critical to overall performance.}

\noindent \textbf{Training with selective feedback.} The reward model is trained on $N_{\text{vlm}} = |\mathcal{D}_{\tau_l}| + |\mathcal{D}_{\tau_u}|$ machine-labeled samples, supplemented by $N_{\text{human}}=|\mathcal{D}_h|$ human-labeled samples from uncertain cases. 
Only $N_{\text{human}}$ samples are used to update the VLM. 
This targeted feedback mechanism accelerates training convergence and improves accuracy while significantly reducing the overall annotation burden. 

\subsection{Overall algorithm} \label{sec:method_overall}
\algo~proceeds by initializing the policy $\pi_\phi$, reward function $r_\theta$, and additional layers $\mathcal{G}$ on top of the VLM randomly. 
Given a task description $l$, the method iteratively follows a structured cycle. 
First, the policy $\pi_\phi$ is updated using %reinforcement learning with 
the reward function $r_\theta$, interacting with the environment and storing observed trajectories in a buffer. From this buffer, trajectory segment pairs are randomly sampled and assigned preferences using the VLM. 
These labeled pairs are then categorized into clean and noisy samples based on the filtering strategy outlined in Section~\ref{sec:method_sample_select}. 
A subset of the noisy samples is sent for human annotation, subject to a predefined budget. 
The reward model is updated using the preference-labeled pairs (both VLM and human annotated) through~\eqref{eq:btloss}, while the VLM is fine-tuned using the human-annotated samples, following the adaptation strategy outlined in Section~\ref{sec:method_vlm_adapt}. 
The pseudo-code of our approach is shown in Algorithm~\ref{alg:prefvlm} in the appendix.