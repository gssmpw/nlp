\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exp_main.pdf}
    \caption{Learning curves for all methods on the 5 Meta-World tasks. \algo~consistently outperforms all baselines with minimal human feedback and matches or exceeds PEBBLEâ€™s performance while using $2\times$ fewer annotations. Results are averaged over 5 seeds, with shaded regions indicating the standard error. 
    % \textcolor{red}{Too much white space is wasted, better to adjust the figure design to make it more compact. Consider a single column figure with 3 rows with 2 figures in each row, the last one being the legend box.}
    }
    \label{fig:exp_main}
\end{figure}
\section{Experiments} \label{sec:experiments}

To assess the effectiveness of \algo~in leveraging the synergy between VLMs and human feedback to improve the efficiency of preference-based RL we seek to answer:

$\bullet$ Can \algo~achieve comparable task success with significantly less human feedback than standard preference-based RL methods?

$\bullet$ Given the same amount of human input, does \algo~yield higher task success than existing methods?

$\bullet$ Can \algo~transfer its knowledge from one task to similar tasks, further reducing the need for human feedback?

\subsection{Experimental setup} \label{sec:exp_setup}
We evaluate \algo~on five manipulation tasks from Meta-World~\cite{yu2020meta}: \emph{door open}, \emph{drawer close}, \emph{drawer open}, \emph{window open}, and \emph{window close}. 
SAC~\cite{haarnoja2018soft} is used as the RL policy learning algorithm, while the VLM is initialized with the pre-trained LIV~\cite{ma2023liv} model. 
We optimize using Adam~\cite{kingma2014adam} with a learning rate of 0.0001. 
reference feedback is provided for trajectory segments of length 50. 
The policy is learned with state observations. 
Additional details on implementation and environments are provided in Appendix~\ref{app:env},~\ref{app:impl}.

\subsection{Baselines} \label{sec:exp_baseline}
We compare \algo~against the following baselines:
\begin{enumerate}
    \item \textbf{VLM-as-reward}~\cite{rocamonde2023vision}: This approach directly uses a VLM to assign rewards to each state following~\eqref{eq:cosine_sim}.
    \item \textbf{PEBBLE}~\cite{lee2021pebble}: A preference-based RL method which uses human feedback to learn policies using SAC.
    \item \textbf{VLM-pref-reward}: A modification of the above where segment pairs are ranked based on~\eqref{eq:vlm_pref} instead of a human. 
    These annotated segment pairs are used to train the reward function $r_{\theta}$ in PEBBLE.
    \item \textbf{\algo~w/o selection}: A variant of our method that does not exploit the noise mitigation strategies crucial to human-machine interaction. 
    Here, human feedback is obtained for randomly selecting pairs of segments from the replay buffer. 
\end{enumerate}


\subsection{Main results} \label{sec:exp_results}

% \textcolor{red}{The subsubsection title style seems not to fit in well. Can we not use all capital letters? It also takes up a lot of space.}
\paragraph{Does \algo~improve feedback efficiency?}
Figure~\ref{fig:exp_main} shows the learning curves for all methods, comparing task success rates of the learned policies. 
We evaluate \algo~with 1000 human feedback samples, while PEBBLE, the baseline preference-based method, is assessed with both 1000 and 2000 feedback samples to highlight the impact of feedback efficiency. 

Across all tasks, \algo~matches PEBBLE's performance while requiring only half the human feedback, demonstrating the effectiveness of VLMs in reducing human annotation costs without compromising learning performance. 
However, directly using VLMs as standalone reward models is ineffective, as both \textit{VLM-as-reward} and \textit{VLM-as-pref} fail to achieve meaningful success. 
This is due to the inherently noisy VLM outputs when applied without supervision (see Section~\ref{sec:vlm}), underscoring the necessity of integrating human feedback rather than relying on VLMs in a zero-shot manner. 

Additionally, while \textit{\algo~w/o selection} achieves reasonable performance and even surpasses PEBBLE with the same amount of feedback, it remains inferior to the full method. This is because it allows some noisy labels to influence reward model training and introduces redundancy in human annotations, as humans may not always label the most uncertain samples for the VLM. This further highlights the importance of selective feedback in optimizing human effort and maintaining robust learning.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exp_feedback.pdf}
    \caption{Success rate as a function of human feedback. \algo~leverages human feedback more efficiently by complementing it with VLM-based feedback, resulting in higher success rates with fewer human annotations. Results are averaged over 5 seeds, with shaded regions representing the standard error.}
    \label{fig:human_feedback}
\end{figure}

\paragraph{How does success rate improve with human feedback?} 
Since \algo~significantly reduces the need for human feedback in preference-based RL, we further analyze how task success rates scale with varying amounts of feedback. As shown in Figure~\ref{fig:human_feedback}, our approach consistently achieves higher success rates as the amount of human feedback increases. In contrast, PEBBLE shows slower growth in success rate with additional feedback, as it lacks the ability to leverage VLM-based feedback, unlike our method.

\paragraph{Can \algo~transfer knowledge across tasks?}
A key objective of \algo~is to refine the VLM with human feedback, mitigating its inherent noise. An important question is whether an already adapted VLM can generalize to related tasks with minimal additional supervision.

To evaluate this, we consider two types of task transfer: (1) \emph{same task, different object}: adapting from ``door close'' to ``drawer close'', and (2) \emph{same object, different task}: adapting from ``window close'' to ``window open''. In both cases, we initialize the VLM for the target task with the weights obtained after adaptation on the source task, while keeping the rest of the algorithm unchanged. As shown in Figure~\ref{fig:transfer_results}, \algo~achieves the success rate of PEBBLE trained with 2000 human feedback samples using only 500 samples - a $4\times$ reduction in annotation effort. This demonstrates that adapting the VLM on one task enables efficient generalization to related tasks, significantly improving feedback efficiency in preference-based RL.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exp_domain_adapt.pdf}
    \caption{Learning curves for knowledge transfer experiments. \algo~achieves comparable or superior performance to PEBBLE while requiring $4\times$ fewer annotations, demonstrating its ability to transfer knowledge across both \emph{same task, different object} (left) and \emph{same object, different task} (right) settings. This highlights an additional pathway for reducing human feedback in preference-based RL. Results are averaged over 5 seeds, with shaded regions representing standard error.}
    \label{fig:transfer_results}
\end{figure}


\subsection{Analysis} \label{sec:exp_analysis}

\paragraph{Impact of VLM adaptation} 
Figure~\ref{fig:adapted_vlm} compares VLM rewards before and after adaptation, averaged over five expert trajectories. Ideally, rewards should increase along the trajectory, reflecting true task progress. As shown, the adapted VLM better aligns with ground-truth task progress, yielding higher reward values toward the end of trajectories when the task is successfully completed, without abrupt drops. This adaptation significantly reduces reliance on human feedback, allowing VLMs to serve as a more effective prior, mitigating the need to learn a reward model from scratch.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exp_adapt.pdf}
    \caption{VLM reward on \emph{Door Open} and \emph{Drawer Close} before (top) and after (bottom) adaptation, averaged across the same five expert trajectories. The adapted VLM better aligns with ground-truth task progress.}
    \vspace{-0.2cm}
    \label{fig:adapted_vlm}
\end{figure}

\paragraph{Impact of inverse dynamics loss.}  
To evaluate the efficacy of the self-supervised loss, we trained the VLM using only contrastive loss in~\eqref{eq:cosine_sim_aligned}. 
As shown in Figure~\ref{fig:ablation}, the success rate initially increases but then degrades after a certain number of steps. 
In the early stages, when human feedback is available, the VLM performs well by leveraging reliable supervision. However, without the inverse dynamics loss, its performance declines as the policy evolves. This degradation occurs because while the VLM learns from human preferences, these preferences are based on samples from a suboptimal policy. As the policy improves and the data distribution shifts, the VLM struggles to generalize due to its limited understanding of environment dynamics. In contrast, incorporating the inverse dynamics loss allows the VLM to adapt to distribution shifts, maintaining stable performance throughout training.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exp_ablation.pdf}
    \caption{Success rates for (i) VLM adaptation without inverse dynamics loss and (ii) VLM-generated preferences with noise filtering. Results show that inverse dynamics loss stabilizes learning while filtering alone is ineffective without initial human feedback. Results are averaged over 5 seeds, with shaded regions representing standard error.}
    \vspace{-0.2cm}
    \label{fig:ablation}
\end{figure}

\paragraph{Preference feedback from VLM with noise mitigation.} \label{sec:exp_vlm_selection} 
We investigate whether preference feedback from VLMs (denoted as \emph{VLM-pref-reward} in Section~\ref{sec:exp_baseline}) can be effectively combined with the filtering strategy described in Section~\ref{sec:method_sample_select} to achieve success rates comparable to human feedback. As shown in Figure~\ref{fig:ablation}, this approach alone fails to yield significant improvements. The primary limitation arises from the inherent noise in VLM-generated preferences, leading to unreliable feedback. When the reward model is trained on these noisy labels, it learns a suboptimal reward function, and filtering samples based on KL divergence in cross-entropy loss further amplifies these errors, resulting in poor overall performance. This underscores the need for at least some human feedback to adapt the VLM. 