\section{Conclusion}
We introduce a novel approach, \algo, to reduce human feedback requirements in preference-based reinforcement learning by leveraging vision-language models. While VLMs encode rich world knowledge, their direct application as reward models is hindered by alignment issues and noisy predictions. To address this, we develop a synergistic framework where limited human feedback is used to adapt VLMs, improving their reliability in preference labeling. Further, we incorporate a selective sampling strategy to mitigate noise and prioritize informative human annotations.

Our experiments demonstrate that this method significantly improves feedback efficiency, achieving comparable or superior task performance with up to 50\% fewer human annotations. Moreover, we show that an adapted VLM can generalize across similar tasks, further reducing the need for new human feedback by 75\%. These results highlight the potential of integrating VLMs into preference-based RL, offering a scalable solution to reducing human supervision while maintaining high task success rates. 

\section*{Impact Statement}
This work advances embodied AI by significantly reducing the human feedback required for training agents. This reduction is particularly valuable in robotic applications where obtaining human demonstrations and feedback is challenging or impractical, such as assistive robotic arms for individuals with mobility impairments. By minimizing the feedback requirements, our approach enables users to more efficiently customize and teach new skills to robotic agents based on their specific needs and preferences. The broader impact of this work extends to healthcare, assistive technology, and human-robot interaction. One possible risk is that the bias from human feedback can propagate to the VLM and subsequently to the policy. This can be mitigated by personalization of agents in case of household application or standardization of feedback for industrial applications. 