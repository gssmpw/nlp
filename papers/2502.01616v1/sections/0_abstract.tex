\begin{abstract}
% \textcolor{blue}{Add one or two sentences about background, motivation, and challenges}
% We introduce a new method for generating reward functions in preference-based reinforcement learning, enabling agents to learn new tasks using only a textual description of the task goal and the agent's visual observations. By synergistically integrating feedback from vision-language models with minimal human input, the proposed approach significantly reduces the reliance on extensive human feedback traditionally required for preference-based RL. \amit{I assume this is incomplete.}
Preference-based reinforcement learning (RL) offers a promising approach for aligning policies with human intent but is often constrained by the high cost of human feedback. In this work, we introduce \algo, a framework that integrates Vision-Language Models (VLMs) with selective human feedback to significantly reduce annotation requirements while maintaining performance. Our method leverages VLMs to generate initial preference labels, which are then filtered to identify uncertain cases for targeted human annotation. Additionally, we adapt VLMs using a self-supervised inverse dynamics loss to improve alignment with evolving policies. Experiments on Meta-World manipulation tasks demonstrate that \algo~achieves comparable or superior success rates to state-of-the-art methods while using up to $2\times$ fewer human annotations. Furthermore, we show that adapted VLMs enable efficient knowledge transfer across tasks, further minimizing feedback needs. Our results highlight the potential of combining VLMs with selective human supervision to make preference-based RL more scalable and practical.

\end{abstract}