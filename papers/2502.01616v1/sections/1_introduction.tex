\section{Introduction} \label{sec:intro}

Deep reinforcement learning (RL) has achieved remarkable performance across a broad range of sequential decision-making tasks, from mastering games~\cite{mnih2015human,silver2017mastering,silver2018general} to solving complex robotic challenges~\cite{duan2016benchmarking,levine2016end,kaufmann2023champion}. Despite these successes, a significant obstacle to deploying RL in real-world settings lies in designing reward functions that consistently guide agents toward desired behaviors.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/teaser.png}
    \caption{\algo~leverages a VLM to obtain preference labels over pairs of the agent's trajectory segments. These preference labels are then used to train a reward function. In scenarios where the VLM exhibits high uncertainty, \algo~can seamlessly incorporate human feedback to refine its understanding and adapt the VLM to the specific environment. By combining machine-generated and expert-guided feedback, \algo~learns high-quality reward function while significantly reducing the amount of human supervision required compared to existing preference-based RL methods.}
    \label{fig:teaser}
\end{figure}
%
Manually crafting reward functions is particularly challenging for complex, long-horizon tasks, as it often demands substantial human expertise and ad hoc engineering. Preference-based RL~\cite{christiano2017deep,lee2021pebble} has emerged as a promising alternative, bypassing the need for hand-crafted rewards by relying on human-provided preferences over pairs of agent behaviors. These preferences are then used to infer a reward function for training the agent. While preference-based RL simplifies reward design, it remains limited by its heavy reliance on human feedback, which can be resource-intensive and thus impede scalability. 

To address this limitation, recent efforts have explored the use of vision-language models (VLMs)~\cite{radford2021learning} as a scalable and intuitive means to specify rewards through natural language descriptions (\textit{e.g.}, ``open the door''). VLMs align visual observations with textual descriptions in a shared latent space, enabling dense, zero-shot reward generation across diverse tasks~\cite{mahmoudieh2022zero,adeniji2023language,ma2023liv,rocamonde2023vision,sontakke2024roboclip}. However, employing VLMs alone as reward functions introduces challenges: their outputs are often noisy and lack the granularity required for fine-grained tasks like robotic manipulation~\cite{fu2024furl}.

In this paper, we propose \algo, a novel framework that enhances the efficiency of preference-based RL by synergistically integrating VLMs with minimal human feedback. Unlike existing methods that rely exclusively on VLMs or extensive human supervision for reward generation, \algo~uses VLMs to generate coarse, trajectory-level preference signals, which are subsequently refined with targeted human input in cases of high uncertainty. This approach reduces the annotation burden ($\sim 2\times$) while ensuring the precision required for complex tasks. 
% \amit{give a ballpark number for the reduction}. 
Figure~\ref{fig:teaser} illustrates our approach.

Our approach has two core components. 
\textit{First}, we introduce a parameter-efficient fine-tuning strategy for VLMs that combines an unsupervised dynamics-aware objective with sparse human feedback, improving the quality of preference labels. 
While prior efforts have sought to reduce noise in VLM rewards by incorporating expert trajectories and additional environment-based rewards~\cite{fu2024furl}, obtaining high-quality demonstrations remains a major challenge because of the high cost of data collection~\cite{akgun2012keyframe} and substantial domain gaps~\cite{smith2019avid}. 
In contrast, providing preference feedback is a more accessible and resource-efficient alternative~\cite{hejna2023few}. 
\textit{Second}, we leverage robust training techniques~\cite{song2022learning,cheng2024rime} to improve the learning process. 
By dynamically constraining the KL divergence between the reward model and VLM predictions, we identify and prioritize reliable samples for training while isolating uncertain ones for targeted human annotation. 
This two-pronged approach ensures both efficiency and precision in reward learning.

We evaluate \algo~on five robotic manipulation tasks and demonstrate that it produces high-quality reward functions for training optimal policies. 
Our results show that \algo~reduces human feedback requirements by \emph{half} compared to existing preference-based RL methods while maintaining comparable or superior performance. 
These findings highlight how VLMs can be leveraged to make preference-based RL significantly more efficient without compromising precision or scalability.

\begin{mdframed}[innertopmargin=0pt,leftmargin=0pt, rightmargin=0pt, innerleftmargin=10pt, innerrightmargin=10pt, skipbelow=0pt]
\paragraph{Our work has three main contributions:}
\begin{enumerate}[leftmargin=*]
    \item Conceptually, we demonstrate that VLMs are effective at providing coarse, trajectory-level preferences, making them a scalable and efficient alternative for reward model learning in preference-based RL. By leveraging these models, we show that the dependence on extensive human feedback can be significantly reduced.
    \item Methodologically, we propose a new approach, \algo, that leverages VLMs to generate preference labels over trajectory pairs and integrates them into a preference learning framework. Our method combines parameter-efficient fine-tuning of VLMs using minimal human feedback, and robust training techniques to handle noise in machine-generated preferences.
    \item Empirically, we evaluate \algo~on robotic manipulation tasks and show that it achieves comparable or superior performance to existing preference-based RL methods while reducing the need for human supervision by half, demonstrating its scalability and practicality.
\end{enumerate}
\end{mdframed}


% While recent work has attempted to reduce the noise in VLM-generated rewards by leveraging expert trajectories and additional environment-based rewards~\cite{fu2024furl}, we note that obtaining high-quality demonstrations remains a significant challenge due to the high cost of data collection~\cite{akgun2012keyframe} and large domain gaps~\cite{smith2019avid}. Preference feedback, by contrast, is more accessible, as comparing outcomes is simpler than generating expert behaviors~\cite{hejna2023few}.

% Second, we incorporate robust training techniques~\cite{song2022learning, cheng2024rime} to further improve the learning process. This involves dynamically setting lower and upper bounds on the KL divergence between the reward model and VLM predictions. These bounds serve two key purposes: (1) filtering out noisy or unreliable samples, and (2) actively identifying uncertain samples that are sent for targeted human annotation. Together, these innovations ensure that \algo~effectively balances the strengths of VLMs and human feedback, enabling efficient and accurate reward learning with minimal supervision.


% We evaluate \algo~on five manipulation tasks and demonstrate its ability to produce reward functions that successfully train optimal policies. Our experimental results highlight that \algo~not only significantly outperforms prior VLM-based reward generation methods but also achieves performance comparable to preference-based RL approaches while requiring $2\times$ less human feedback. 


% In this work, we introduce a key insight: while VLMs may lack the precision to generate accurate step-by-step reward signals, their strong semantic understanding makes them ideal for labeling trajectory-level preferences. Importantly, collecting preference feedback from humans is significantly easier and less time-intensive than generating expert behaviors~\cite{hejna2023few}. By leveraging VLMs to compare and rank task executions, we not only capitalize on their semantic capabilities but also enable seamless integration of human preferences to fine-tune and adapt the VLM for specific environments.

% % Building on this insight, we introduce \algo, a novel approach for generating reward functions using VLMs. Given a language description of a task, \algo~queries the VLM to provide preferences over pairs of the agent's trajectories. These machine generated preferences can then be seamlessly integrated into a preference learning framework~\cite{lee2021pebble,ouyang2022training,rafailov2024direct}, facilitating the automatic generation of reward functions. To address potential noise in the preference labels - since they are machine generated rather than provided by humans - \algo~employs two key strategies.

% Building on this insight, we present \algo, a novel framework for reward generation using VLMs. Given a natural language task description, \algo~queries the VLM to provide preferences over pairs of agent trajectories. These machine generated preferences are incorporated into a preference-learning framework~\cite{lee2021pebble,ouyang2022training,rafailov2024direct} to automatically generate reward functions. To address potential noise in VLM-generated preferences, \algo~employs two complementary strategies.

% % First, we propose a parameter-efficient fine-tuning approach for the VLM, which uses an unsupervised, dynamics-aware objective alongside minimal human preference feedback. This fine-tuning process calibrates the VLM to the specific task and environment, improving the quality of the preference labels. While recent work has attempted to reduce the noise in VLM-generated rewards by leveraging expert trajectories and additional environment-based rewards~\cite{fu2024furl}, we note that obtaining high-quality demonstrations remains a significant challenge due to the high cost of data collection~\cite{akgun2012keyframe} and large domain gaps~\cite{smith2019avid}. In contrast, preference feedback is easier to collect since comparing outcomes is simpler than generating expert behaviors~\cite{hejna2023few}. 

% First, we introduce a parameter-efficient fine-tuning approach for the VLM. This combines an unsupervised, dynamics-aware objective with minimal human preference feedback to adapt the VLM to the task and environment. This calibration improves the quality of preference labels. While recent work has attempted to reduce the noise in VLM-generated rewards by leveraging expert trajectories and additional environment-based rewards~\cite{fu2024furl}, we note that obtaining high-quality demonstrations remains a significant challenge due to the high cost of data collection~\cite{akgun2012keyframe} and large domain gaps~\cite{smith2019avid}. Preference feedback, by contrast, is more accessible, as comparing outcomes is simpler than generating expert behaviors~\cite{hejna2023few}.

% Second, we incorporate robust training techniques~\cite{song2022learning, cheng2024rime} to further improve the learning process. This involves dynamically setting lower and upper bounds on the KL divergence between the reward model and VLM predictions. These bounds serve two key purposes: (1) filtering out noisy or unreliable samples, and (2) actively identifying uncertain samples that are sent for targeted human annotation. This combination of fine-tuning and robust training ensures that \algo~synergistically leverages the strengths of the VLM while minimizing reliance on human input, enabling the efficient learning of a high-quality reward model. An overview of our method is shown in Figure~\ref{fig:teaser}.

% We evaluate \algo~on five manipulation tasks and demonstrate its ability to produce reward functions that successfully train optimal policies. Our experimental results highlight that \algo~not only significantly outperforms prior VLM-based reward generation methods but also achieves performance comparable to RLHF approaches while requiring $2\times$ less human feedback. This showcases the powerful synergy between VLMs and human inputs, where the semantic capabilities of VLMs are effectively complemented by minimal human annotations, drastically reducing the overall annotation effort.
% \begin{mdframed}[innertopmargin=0pt,leftmargin=0pt, rightmargin=0pt, innerleftmargin=10pt, innerrightmargin=10pt, skipbelow=0pt]
% \paragraph{In summary, our work has three main contributions:}
% \begin{enumerate}[leftmargin=*]
%     \item Conceptually, we demonstrate that VLMs excel at providing trajectory-level preferences, making them valuable tools for learning reward models.
%     \item Methodologically, we propose a new approach, \algo, that leverages VLMs to generate preference labels over trajectory pairs and integrates them into a preference learning framework. Our method combines parameter-efficient fine-tuning of VLMs using minimal human feedback, and robust training techniques to handle noise in machine-generated preferences.
%     \item Empirically, we demonstrate that VLMs can effectively serve as a substitute for human feedback, achieving comparable performance to RLHF approaches while significantly reducing the need for human supervision.
% \end{enumerate}
% \end{mdframed}

%udita: Right now, although the foundation models are powerful, being nascent, they cannot be deployed in real world without human in the loop. Hence, human interference is indispensible. In this work, although required, we aim to minimize the human interference with the help of these foundation models and make them capable with some hand-holding from the humans.



% \dripta{1/ Rewards are hard to design, but language description is easy to obtain. 2/ VLMs have shown potential in this aspect, but are very noisy. Also, code-based reward generation is another line but requires too many assumptions. 3/ Recently, some works have tried to inject external knowledge in terms of goal images/task rewards to improve foundation models, but we argue that getting these are hard and also, RL algorithms often discover ways to achieve high returns by unexpected, unintended means. In general, there is nuance in how we might want agents to behave, such as obeying social norms, that are difficult to account for and communicate effectively through goal images or success indicators, instead a minimal amount of preferences are easy to get and also useful. This synergy of VLMs and preferences allow us to leverage the benefits of RLHF but also without the need for excess feedback.}

% In this work, we leverage such pairwise preferences to adapt the VLM. Crucially, we show that this adaptation can be achieved with \emph{minimal human input}, by using the VLM itself to provide preference data.



% Our approach combines the strengths of VLMs with targeted human feedback through a five-stage process. First, we utilize the pretrained VLM to generate initial preference labels over trajectory pairs, establishing a base dataset of comparative judgments. Second, we develop a novel uncertainty-based filtering mechanism to identify potentially noisy or ambiguous samples where the VLM's confidence is low. Third, these identified samples are strategically routed for human annotation, ensuring that human input is focused on the most challenging and informative cases. Fourth, we update the VLM through a combination of human-provided labels and a self-supervised loss term that maintains consistency with the model's original visual-language alignments. Finally, we leverage both the refined VLM and the collected human feedback to update our reward model, resulting in more precise and reliable reward signals for task-specific behaviors. This carefully orchestrated pipeline allows us to maximize the benefit of limited human feedback while maintaining the scalability advantages of foundation models.

% Our approach involves querying the VLM to provide preferences between pairs of the agent’s image observations based on the textual description of the task goal. We then learn a separate reward function from these preference labels, rather than directly prompting the model to output a raw reward score, which can be noisy~\cite{sontakke2024roboclip}. Since directly using the VLM can still lead to suboptimal performance, we further improve the model by aligning it with the task through minimal human preferences, alongside an unsupervised, dynamics-aware objective.

% We test \dots

% In summary, \dots

