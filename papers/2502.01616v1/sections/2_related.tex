\section{Related Works} \label{sec:related_works}

\noindent \textbf{Designing reward functions.} 
Designing effective reward functions remains a fundamental challenge in RL~\cite{singh2009rewards,sutton2018reinforcement}. Traditional methods often rely on manual trial-and-error processes, which require substantial domain expertise and struggle to scale to complex, long-horizon tasks~\cite{booth2023perils,knox2023reward}. Inverse reinforcement learning offers an alternative by attempting to infer reward functions from expert demonstrations~\cite{abbeel2004apprenticeship,ziebart2008maximum,ho2016generative}. Evolutionary algorithms have also been explored for automated reward function discovery~\cite{niekum2010genetic,chiang2019learning}. 

Recently, foundation models have emerged as a promising avenue for reward function generation. Multimodal Large language models (MLLMs) have been employed to derive reward functions directly from natural language task descriptions~\cite{yu2023language,ma2023eureka,wang2024}, although these approaches either often assume access to the environment's source code or are too expensive for repeated queries. VLMs present another promising direction~\cite{mahmoudieh2022zero,adeniji2023language,ma2023liv,rocamonde2023vision,sontakke2024roboclip}, leveraging task descriptions to generate rewards by aligning the agent's visual observations with the task description in the joint language-image embedding space. However, rewards generated by VLMs often exhibit noise and lack the granularity needed for precise tasks~\cite{fu2024furl}. To address these challenges, our work introduces a hybrid approach that synergistically combines the coarse preference signals provided by VLMs with targeted human feedback. Instead of relying on VLMs as zero-shot reward generators, we leverage their ability to infer trajectory-level preferences as a foundation, while strategically incorporating human annotations to refine these preferences in uncertain cases.

\noindent \textbf{Learning from human feedback.} 
Incorporating human feedback has been widely explored across domains, including natural language processing~\cite{ouyang2022training,rafailov2024direct} and robotics~\cite{lee2021pebble}. In RL, preference-based frameworks have proven particularly effective, with \citet{christiano2017deep} pioneering the use of human-provided trajectory comparisons to guide learning. Subsequent works, such as PEBBLE~\cite{lee2021pebble}, enhanced feedback efficiency by incorporating unsupervised exploration, while SURF~\cite{park2022surf} augmented preference datasets using semi-supervised learning.

These approaches are rooted in the observation that humans often find it easier to provide relative judgments, \emph{i.e.}, comparing behaviors as better or worse, rather than defining absolute metrics. 
In this work, we extend this idea by integrating VLMs to generate preferences over pairs of an agent's trajectories. 
This approach reduces the annotation cost of human feedback while maintaining performance.

\noindent \textbf{Learning from noisy labels.} 
The challenge of learning from noisy or imprecise labels has been extensively studied in supervised learning, particularly with the rise of machine-generated annotations~\cite{wang2022self}. Robust training methods address this issue through a variety of strategies, including architectural modifications~\cite{goldberger2017training}, regularization techniques~\cite{lukasik2020does}, specialized loss functions~\cite{zhang2018generalized}, and sample selection mechanisms~\cite{wang2021denoising}.

In our framework, the preference labels generated by VLMs may contain noise due to the inherent limitations of these models. To handle this, we adopt a sample selection strategy inspired by RIME~\cite{cheng2024rime}, which identifies confident labels for training while flagging uncertain samples for human refinement. This combination of robust training and targeted human feedback ensures that our reward model remains both accurate and efficient.


