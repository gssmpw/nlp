\newpage
\appendix
\onecolumn
\section{Pseudo-code}
\label{app:pseudocode}
\begin{algorithm}[ht]
%   \small
\caption{Language Guided RL through preference feedback from human and VLM (\algo)}\label{alg:guiderl}
\begin{algorithmic}[1]
\REQUIRE Text description of the task \textit{l}, maximum human feedback $M_{human}$, pre-trained VLM $\mathcal{F}$
\STATE Initialize policy $\pi_\phi$, reward model $r_\theta$, and VLM trainable layers $\mathcal{G}$
\STATE Initialize replay buffer $\mathcal{R}$, preference buffer $\mathcal{B}$, human preference buffer $\mathcal{H}$, feedback batch size $N$, and feedback collected counter $m_h$
\FOR{each iteration $i$}
\STATE Collect sample state $s_{i+1}$, image $o_{i}$ by taking $a_i \sim \pi_\theta(a_i|s_i)$
\STATE Add sample $\mathcal{R} \longleftarrow \mathcal{R} \cup \{s_i, a_i, s_{i+1}, r_\theta(s_i, a_i), o_i\}$
\FOR{each policy update step}
\STATE Sample  minibatch $\{(s_i, a_i, s_{i+1}, r_\theta(s_i, a_i))\} \sim \mathcal{R}$
\STATE Optimize policy $\pi_\phi$ using the sampled batch with any off-policy RL algorithm 
\ENDFOR
\IF{$i$ mod $K == 0$}
\FOR{$n = 1$ to $N$}
\STATE Sample preference pair $(\sigma_0, \sigma_1)\sim\mathcal{R}$
\STATE Obtain preference feedback $\tilde{y}$ from the VLM
\STATE Update $\mathcal{B} \longleftarrow \mathcal{B} \cup \{\sigma_0, \sigma_1, \tilde{y}\}$
\ENDFOR
\IF{$m_{h} < M_{human}$}
\STATE Obtain $\mathcal{D}_{\tau_l}$ and $\mathcal{D}_{\tau_u}$ from $\mathcal{B}$ using Eqn.~(\ref{D_lower}) and Eqn.~(\ref{D_upper}).
\STATE Sample $\mathcal{D}_h$ from the $\mathcal{B} - \mathcal{H}$, such that $|\mathcal{D}_h| = \min(|\mathcal{B}| - |\mathcal{D}_{\tau_l}| - |\mathcal{D}_{\tau_u}|, 0.05*N)$ 
\STATE Obtain human feedback $y$ for $\mathcal{D}_h$.
\STATE Update $\mathcal{H} \longleftarrow \mathcal{H} \cup \{\mathcal{D}_h, y\}$.
\STATE Update $\mathcal{B} \longleftarrow \mathcal{B} \cup \{\mathcal{D}_h, y\}$
\STATE $m_h = m_h + |\mathcal{D}_h|$
\FOR{each VLM update step}
\STATE Update the VLM layers $\mathcal{G}$ with $\mathcal{H}$ using Eqn~(\ref{eq:btloss}) and Eqn.~(\ref{eq:inv_loss})
\ENDFOR
\ENDIF
\FOR{each reward model update step}
\STATE Obtain $\mathcal{D}_{\tau_l}$ and $\mathcal{D}_{\tau_u}$ from $\mathcal{B}$ using Eqn.~(\ref{D_lower}) and Eqn.~(\ref{D_upper})
\STATE Optimize $r_\theta$ with $\mathcal{D}_{\tau_l}$, $\mathcal{D}_{\tau_u}$ and $\mathcal{H}$, with Eqn.~(\ref{eq:btloss}) 
\ENDFOR
\STATE Relabel $\mathcal{R}$ with $r_\theta$.
\ENDIF

\ENDFOR
\end{algorithmic}  
\label{alg:prefvlm}
\end{algorithm}

\section{Environmental details and task descriptions}
\label{app:env}
We show results on 5 different environments on Meta World as shown in Figure~\ref{fig:env}. For each of the environment, the state $s\in\mathbb{R}^{39}$, and the action $a\in\mathbb{R}^4$. Observations are captured from Camera 2 and rendered as 300 Ã— 300 images. Task descriptions are sourced directly from the Meta-World paper~\cite{yu2020meta}. The corresponding prompts for each environment are provided below:
\begin{itemize}
    \item \textbf{door-open-v2} : \textit{Open a door with a revolving joint}
    \item \textbf{door-close-v2} : \textit{Push and close a door with a revolving joint}
    \item \textbf{window-open-v2} : \textit{Push and open a window}
    \item \textbf{window-close-v2} : \textit{Push and close a window}
    \item \textbf{drawer-close-v2} : \textit{Push and close a drawer}
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/app_env.pdf}
    \caption{Example expert trajectories demonstrating the tasks we test on.}
    \label{fig:env}
\end{figure}


\section{Implementation details}
\label{app:impl}
In our experiments, we begin by collecting initial data using a random policy for the first 1000 interactions with the environment. Following this, we conduct unsupervised exploration for the next 5000 interactions, during which we update state entropy for Soft Actor-Critic (SAC). Once 6000 interactions are completed, we initiate the training process for the policy, reward model, and VLM.

During the early training phase, we allocate 250 human feedback samples from the total budget ($M_{human}$) to train both the VLM and reward model, setting the initial feedback allocation as $m_h = 250$. After this initialization, training proceeds as outlined in Appendix~\ref{app:pseudocode}, following the full algorithmic procedure. We set $K=3000$. 

For the reward model, we use an ensemble of 3 MLPs, each with 3 hidden layers of 256 nodes and Leaky ReLU activation, while the final layer applies a tanh activation. The reward model is trained with a learning rate of 0.0003, a batch size of 128, and 200 update steps per iteration. The maximum feedback budget is set to 30000 samples (this includes the human feedback $M_{human}$ and the VLM preference feedback).

We use LIV~\cite{ma2023liv} as the VLM in our algorithm. The trainable layers $\mathcal{G}_L$ and $\mathcal{G}_I$ are both 2-layer MLPs with 256 and 64 hidden units respectively, with ReLU activation. The final layer does not have any activation. The inverse dynamics prediction layer $f$ is implemented as another MLP with 128 length input layer, hidden layer of size 64 with ReLU activation, and output layer of 4 units which is the dimension of the action vector. The learning rate is set to 0.0003. 

For the SAC policy, both the actor and critic are MLPs with three hidden layers of 256 units each, trained with a learning rate of 0.0001, with the critic $\tau$ set to the default value of 0.005.

Lastly, for label selection, we adopt the same hyperparameter values as reported in \cite{cheng2024rime}. Specifically, we set $\alpha = 0.5$, $\beta_{min} = 1$,  $\beta_{max} = 3$, $k = 1/300$ and $\tau_{upper} = 3\ln(10)$. 