

We first ask if it is possible to avoid error amplification \emph{information-theoretically}, irrespective of computational practicality. We find that the \emph{$\rho$-estimator}, a recent agnostic estimation technique from the statistics literature \citep{baraud2018rho}, yields an imitation learning algorithm that achieves near-optimal misspecification tolerance (i.e., achieves $\Capx=\bigoh(1)$), while matching the performance guarantee for \loglossbc in \cref{prop:mle_finite} in the well-specified setting.
For a policy class $\Pi$, we define \emph{$\rho$-estimator behavior cloning} ($\rhobc$) to be the algorithm that, given trajectories $\cD=\crl*{o\ind{i}}_{i=1}^{n}$ with $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$, returns\footnote{The $\rho$-\emph{estimator} is named thus in \citet{baraud2018rho} because it is presented in terms of the function $\rho(x) \ldef \tau(1/x^2)$.  We find the current parameterization more convenient for our purposes.}  \loose
\begin{align}\label{eq:rhobc}
    \pihat := \argmin_{\pi \in \Pi} \sup_{\pi' \in \Pi} 
\sum_{i=1}^n \tau\left(
\prod_{h=1}^{H}\frac{\pihi}{\piphi}\right), \qquad \text{where } \qquad \tau(x) \ldef \frac{\sqrt{1/x} - 1}{\sqrt{1/x} + 1}.
\end{align}
We have the following guarantee, which shows that $\rhobc$ achieves $\Capx = \bigoh(1)$. %

\begin{theorem}\label{thm:rho-il}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. Let $n \in \NN$ and $\delta > 0$. \arxiv{Let $\cD=\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$ from $\bbP^{\pistar}$. Then the} policy $\pihat$ produced by $\rhobc$
satisfies, with probability at least $1-\delta$,{\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{equation} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|\delta^{-1})}{n} + \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\label{eq:rho-il-thm}\end{equation}}
\end{theorem}
We defer the proof of \Cref{thm:rho-il} to \Cref{app:rho}; briefly, it follows by applying a guarantee for $\rho$-estimators \citep{baraud2018rho} to the family of distributions $\cP=\{\BP^\pi: \pi \in\Pi\}$. Since%
\arxiv{\loose\[
  \frac{\BP^\pi(o)}{\BP^{\pi'}(o)} = \prod_{h=1}^H \frac{\pi_h(a_h\mid{} s_h)}{\pi'_h(a_h\mid{} s_h)}
\]}
 for any policies $\pi,\pi'$ and trajectory $o = (s_1,a_1,\dots,s_H,a_H)$, \cref{eq:rhobc} implicitly applies the $\rho$-estimator to $\cP$, in spite of the fact that the transition probabilities are unknown. The function $\tau(x)$ can be viewed as a better-behaved replacement for the negative log likelihood, $-\log(x)$, that (a) is uniformly bounded (allowing tight concentration under misspecification), yet (b) enjoys similar statistical properties,
\ahreplace{; in particular, the expectation of $\tau$ can be related to the Hellinger distance (\cref{lemma:rho-estimator-bounds}).}{because $\tau$, in expectation, can be related to the Hellinger distance (\cref{lemma:rho-estimator-bounds}).}\footnote{Note if we replace $\tau(x)$ with $-\log(x)$ in \cref{eq:rhobc}, \arxiv{the inner maximization problem becomes irrelevant and the algorithm} coincides with \loglossbc.}\loose

The $\rhobc$ algorithm has some similarity to recent work in imitation learning based on \emph{inverse reinforcement learning} (IRL) \citep{ho2016generative,ke2021imitation,swamy2021moments}\arxiv{; while the precise setting for these IRL-based algorithms is different, they also solve a minimax problem in order to minimize some $f$-divergence between the expert and learned policy. Compared to these works, which require online interaction with the MDP or knowledge of the dynamics, $\rhobc$ remains fully offline in the sense that \emph{no interaction with the MDP or expert is required}. Further, the derivation of the algorithm is somewhat different: IRL-style algorithms are typically derived from a variational representation for the $f$-divergence under consideration, while $\rhobc$---per the discussion above---is better understood as a smoothed or better-behaved generalization of maximum likelihood.}%


While the statistical performance of $\rhobc$ is essentially optimal, it is substantially less attractive when viewed through a computational lens: 
the product over $H$ ratios and the min-max optimization make it impractical compared to $\loglossbc$.\footnote{Interestingly, we show in \cref{sec:density} that the additional difficulty of a min-max objective (as opposed to a single minimization problem as in \cref{eq:bc}) can be overcome if we assume the learner has access to \emph{\densobs}\arxiv{, in which case the maximal $\pi'$ in \eqref{eq:rhobc} can be replaced by the true density $\pistar$}. %
}
Thus, while $\rhobc$ is our gold standard for statistical performance, we will need to look further for practical algorithms.\loose





