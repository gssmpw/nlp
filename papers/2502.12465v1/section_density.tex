
In \cref{sec:improvements}, we showed that given access to
\densobs $\pistar_h(a_h\ind{i}\mid{}s_h\ind{i})$ for the examples
$(s_h\ind{i},a_h\ind{i})$, one can smooth $\loglossbc$ to achieve improved
misspecification tolerance $\Capx=\bigoht(H)$; the resulting method $\smoothedloglossbc$ could be of practical interest, even though we do not know how to implement it efficiently in our testbed of autoregressive linear models. In this section, we
present two algorithms with \emph{optimal} misspecification tolerance
$\Capx=\bigoh(1)$ enabled by \densobs; both algorithms seem likely somewhat impractical (and both are
computationally inefficient for autoregressive linear models), but they do slightly simplify the $\rho$-estimator,
and may be of independent interest.

\paragraph{Logarithmic loss with trajectory-level smoothing}
Given expert dataset
$\cD=\crl*{(s_h\ind{i},a_h\ind{i},\pistar_h(a_h\ind{i}\mid{}s_h\ind{i}))}_{i\in\brk{n}}$,
consider the following estimator, which we refer to as
\emph{log-loss behavior cloning with trajectory-level smoothing}:
\begin{align}
  \label{eq:traj_smoothing}
  \pihat=\argmax_{\pi\in\Pi}\sum_{i=1}^{n}\log\prn*{\prod_{h=1}^{H}\pi_h(a_h\ind{i}\mid{}s_h\ind{i})
  + \prod_{h=1}^{H}\pistar_h(a_h\ind{i}\mid{}s_h\ind{i})}.
\end{align}
To see why this algorithm is natural, we observe that it can be viewed
as an instance of maximum likelihood over the class
$\crl*{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}_{\pi\in\Pi}$ of
smoothed trajectory distributions; indeed, for any policy $\pi$ and trajectory $o = (s_1,a_1,\dots,s_H,a_H)$, we have
%
\begin{align}
  \log\prn*{\frac{1}{2}\prn*{\bbP^{\pi}(o) + \bbP^{\pistar}(o)}}%
  &=\log\prn*{\frac{1}{2}\prn*{\prod_{h=1}^{H}\bbP_h(s_{h+1}\mid{}s_h,a_h)\pi_h(a_h\mid{}s_h)
  +
    \prod_{h=1}^{H}\bbP_h(s_{h+1}\mid{}s_h,a_h)\pistar_h(a_h\mid{}s_h)}}\\
  &=\log\prn*{\prod_{h=1}^{H}\pi_h(a_h\mid{}s_h)
    + \prod_{h=1}^{H}\pistar_h(a_h\mid{}s_h)}
    + \log\prn*{\frac{1}{2}\prod_{h=1}^{H}\bbP_h(s_{h+1}\mid{}s_h,a_h)}.\label{eq:traj_equiv}
\end{align}%
%
Note that the second term above is independent of the policy being
optimized over, and hence does not affect the maximizer. By applying the results of \citet{foster2024behavior} and performing
some elementary manipulations, we can deduce the following result.
\begin{proposition}
  \label{prop:traj_smoothing}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. For i.i.d. trajectories $o\ind{1},\dots,o\ind{n}$ from $\bbP^{\pistar}$, the policy $\pihat$ in \cref{eq:traj_smoothing} satisfies, with
probability at least $1-\delta$, 
\begin{align}
    \Dhels{\bbP^{\pihat}}{\bbP^{\pistar}} 
  \approxleq{} \frac{\log(\abs{\Pi}\delta^{-1})}{n}
      +   \min_{\pi\in\Pi}\Dhels{\bbP^{\pistar}}{\bbP^{\pi}}.
\end{align}
\end{proposition}
That is, trajectory-level smoothing substantially improves over
layer-wise smoothing (cf. \cref{thm:layerwise_smoothing}), achieving $\Capx=\bigoh(1)$ and matching the
result for the $\rho$-estimator in \cref{thm:rho}. Our results in \cref{sec:computational} show that in a worst-case
sense, one should not hope to implement the objective in
\cref{prop:traj_smoothing}, but it is certainly simpler than the
$\rho$-estimator itself, and may be interesting to explore further.


\begin{proof}[\pfref{prop:traj_smoothing}]
By Proposition B.1 in \citet{foster2024behavior} and \cref{eq:traj_equiv}, we have that with
probability at least $1-\delta$,
\begin{align}
  \label{eq:4}
\Dhels{\frac{1}{2}(\bbP^{\pihat}+\bbP^{\pistar})}{\bbP^{\pistar}}
    &\approxleq{} \frac{\log(\abs{\Pi}\delta^{-1})}{n}
      +
      \min_{\pi\in\Pi}\Dchis{\bbP^{\pistar}}{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}.
\end{align}
For any policy $\pi\in\Pi$, we have
\begin{align}
  \Dchis{\bbP^{\pistar}}{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}
  &\leq  \Dkl{\bbP^{\pistar}}{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})} \\ &\lesssim \Dhels{\bbP^{\pistar}}{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}
\end{align}
where the second inequality uses the fact that $\Dkl{\BP}{\BQ} \leq (2+\log(W))\Dhels{\BP}{\BQ}$ whenever $\BP(z)/\BQ(z) \leq W$ for all $z$ \citep[Lemma 4]{yang1998asymptotic}. Finally, by \cref{lemma:avg-hels} we have for any $\pi\in\Pi$ that
\[
  \Dhels{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}{\bbP^{\pistar}} \approxleq{} \Dhels{\bbP^{\pi}}{\bbP^{\pistar}} \approxleq{} \Dhels{\frac{1}{2}(\bbP^{\pi}+\bbP^{\pistar})}{\bbP^{\pistar}}.
\]
Combining the above bounds completes the proof.  
\end{proof}


\paragraph{Reducing the $\rho$-estimator to a single maximization problem}
Recall that the $\rho$-estimator solves a min-max problem of the form
\[
\pihat = \argmin_{\pi \in \Pi} \sup_{\pi' \in \Pi} 
\sum_{i=1}^n \tau\left(
  \prod_{h=1}^{H}\frac{\pi_h(a_h\ind{i}\mid{}s_h\ind{i})}{\pi'_h(a_h\ind{i}\mid{}s_h\ind{i})}
\right)
\]
for $\tau(x) \ldef \frac{\sqrt{1/x} - 1}{\sqrt{1/x} + 1}$. Given
access to \densobs
$\cD=\crl*{(x_h\ind{i},a_h\ind{i},\pistar_h(a_h\ind{i}\mid{}x_h\ind{i}))}_{i\in\brk{n}}$,
we consider the following variant of the $\rho$-estimator, which
simplifies to a single minimization problem:
\begin{align}
  \label{eq:rho_simple}
  \pihat = \argmin_{\pi \in \Pi}
\sum_{i=1}^n \tau\left(
  \prod_{h=1}^{H}\frac{\pi_h(a_h\ind{i}\mid{}s_h\ind{i})}{\pistar_h(a_h\ind{i}\mid{}s_h\ind{i})}
\right).
\end{align}
Like the $\rho$-estimator itself, this algorithm achieves
$\Capx=\bigoh(1)$, as shown below.
\begin{proposition}
  \label{prop:rho_simple}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. For i.i.d. trajectories $o\ind{1},\dots,o\ind{n}$ from $\bbP^{\pistar}$, the policy $\pihat$ in \cref{eq:rho_simple} satisfies, with
probability at least $1-\delta$, 
\begin{align}
    \Dhels{\bbP^{\pihat}}{\bbP^{\pistar}} 
  \approxleq{} \frac{\log(\abs{\Pi}\delta^{-1})}{n}
      +   \min_{\pi\in\Pi}\Dhels{\bbP^{\pistar}}{\bbP^{\pi}}.
\end{align}
\end{proposition}
As above, we do not expect to be able to implement
the simplified objective efficiently in general, but it may be of
further interest.

\begin{proof}[\pfref{prop:rho_simple}]
  We first observe that $\abs{\tau(a)} \leq 1$ for all $a \geq 0$.
  Hence, by Freedman's inequality, for any fixed $\pi$ and any $\lambda \geq{}2$, with probability at least $1-\delta$,
  \begin{align}
    \abs{\frac 1n \cdot \sum_{i = 1}^n \tau\left( \frac{\pp^{\pi}(\obs^i)}{\pp^{\pistar}(\obs^i)} \right) - \ee_{\obs \sim \pp^{\pistar}}\left[ \tau\left(\frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)}\right) \right]} \leq \frac{1}{\lambda} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] + \frac{2(1 + \lambda) \log\left( 2 /\delta \right)}{n}.
  \end{align}
  Taking a union bound over all $\pi \in \Pi$ and setting $\lambda = 16 \sqrt{2}$, we then have that there is an event $\cE$ occurring with probability at least $1 - \delta$ such that for all $\pi \in \Pi$, it holds that
  \begin{align}
    \abs{\frac 1n \cdot \sum_{i = 1}^n \tau\left( \frac{\pp^{\pi}(\obs^i)}{\pp^{\pistar}(\obs^i)} \right) - \ee_{\obs \sim \pp^{\pistar}}\left[ \tau\left(\frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)}\right) \right]} \leq \frac{1}{16 \sqrt{2}} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] + \frac{33 \sqrt{2} \log\left( 2 \abs{\Pi}/\delta \right)}{n};
  \end{align}
  we condition on this event moving forward.  We now note that
  \begin{align}
    \frac{\pp^\pi(\obs)}{\pp^{\pistar}(\obs)} = \prod_{h = 1}^H \frac{\bbP_h(s_{h+1} \mid{} a_h, s_h) \pi_h(a_h | s_h)}{\bbP_h(s_{h+1} \mid{} a_h, s_h) \pistar_h(a_h | s_h)} = \prod_{h = 1}^H \frac{\pi_h(a_h \mid{} s_h)}{\pistar_h(a_h \mid{} s_h)},
  \end{align}
  and thus
  \begin{align}
    \pihat = \argmin_{\pi \in \Pi} \sum_{i = 1}^n \tau\left( \frac{\pp^\pi(\obs)}{\pp^{\pistar}(\obs)} \right).
  \end{align}
  Now, let $\pibar = \argmin_{\pi \in \Pi} \Dhels{\pp^{\pi}}{\pp^{\pistar}}$ and observe that by \Cref{lemma:rho-estimator-bounds}, it holds for all $\pi \in \Pi$ that
  \begin{align}
    \frac{3}{8} \cdot \Dhels{\pp^{\pistar}}{\pp^{\pi}} &\leq \ee_{\obs \sim \pp^{\pistar}}\left[ \tau\left( \frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)} \right) \right] \leq 4 \cdot \Dhels{\pp^{\pistar}}{\pp^{\pi}}
    \end{align} 
    and
    \begin{align}
    \ee\left[ \tau^2\left( \frac{\pp^{\pi}(\obs)}{\pp^{\pistar}(\obs)} \right) \right] &\leq 3 \sqrt{2} \cdot \Dhels{\pp^{\pistar}}{\pp^{\pi}}.
  \end{align}
  Thus in the event $\cE$, we compute
  \begin{align}
    \frac{3}{8} \cdot \Dhels{\pp^{\pistar}}{\pp^{\pihat}} &\leq \ee_{\obs \sim \pp^{\pistar}}\left[ \tau\left( \frac{\pp^{\pihat}(\obs)}{\pp^{\pistar}(\obs)} \right) \right] \\
    &\leq \frac 1n \cdot \sum_{i = 1}^n \tau\left( \frac{\pp^{\pihat}(\obs^i)}{\pp^{\pistar}(\obs^i)} \right) +  \frac{1}{16 \sqrt{2}} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pihat}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] + \frac{33 \sqrt{2} \log\left( 2 \abs{\Pi}/\delta \right)}{n} \\
    &\leq \frac 1n \cdot \sum_{i = 1}^n \tau\left( \frac{\pp^{\pibar}(\obs^i)}{\pp^{\pistar}(\obs^i)} \right) +  \frac{1}{16 \sqrt{2}} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pihat}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] + \frac{33 \sqrt{2} \log\left( 2 \abs{\Pi}/\delta \right)}{n} \\
    &\leq \ee_{\obs \sim \pp^{\pistar}}\left[ \tau\left( \frac{\pp^{\pibar}(\obs)}{\pp^{\pistar}(\obs)} \right) \right]+  \frac{1}{16 \sqrt{2}} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pihat}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] \\ 
    &\quad+ \frac{1}{16 \sqrt{2}} \cdot \ee_{\obs \sim \pp^{\pistar}}\left[ \tau^2\left(\frac{\pp^{\pibar}(\obs)}{\pp^{\pistar}(\obs)}\right) \right] +  \frac{66 \sqrt{2} \log\left( 2 \abs{\Pi}/\delta \right)}{n} \\
    &\leq 4 \cdot \Dhels{\pp^{\pistar}}{\pp^{\pibar}} + \frac{3}{16} \cdot \Dhels{\pp^{\pistar}}{\pp^{\pihat}}  + \frac{3}{16} \cdot \Dhels{\pp^{\pistar}}{\pp^{\pibar}} +  \frac{66 \sqrt{2} \log\left( 2 \abs{\Pi}/\delta \right)}{n},
  \end{align}
  where the first and penultimate inequalities follow from the preceding display, the second and fourth inequalities follow from the Bernstein calculation above, and the third inequality follows from the definition of $\pihat$.  Rearranging the above and plugging in the definition of $\pibar$ concludes the proof.
\end{proof}

\subsection{Supporting Technical Lemmas}

\begin{lemma}\label{lemma:avg-hels}
Let $\BP,\BQ \in \Delta(\MX)$ be distributions. Then
\[\Dhels{\BP}{\frac{\BP+\BQ}{2}} \lesssim \Dhels{\BP}{\BQ} \lesssim \Dhels{\BP}{\frac{\BP+\BQ}{2}}.\]
\end{lemma}

\begin{proof}[\pfref{lemma:avg-hels}]
For any real numbers $0 \leq x,y$, we have $x-y = (\sqrt{x} - \sqrt{y})(\sqrt{x} + \sqrt{y})$, so on the one hand,
\[\frac{|x-y|}{2\max(\sqrt{x},\sqrt{y})} \leq |\sqrt{x}-\sqrt{y}| \leq \frac{|x-y|}{\max(\sqrt{x},\sqrt{y})}.\]
On the other hand, $\frac{x-y}{2} = (\sqrt{x} - \sqrt{\frac{x+y}{2}})(\sqrt{x} + \sqrt{\frac{x+y}{2}})$, so
\begin{align}\frac{|x-y|}{4\max(\sqrt{x},\sqrt{y})} \leq \frac{|x-y|}{4\max(\sqrt{x},\sqrt{\frac{x+y}{2}})} \leq \left|\sqrt{x} - \sqrt{\frac{x+y}{2}}\right| &\leq \frac{|x-y|}{2\max(\sqrt{x},\sqrt{\frac{x+y}{2}})} \\&\leq \frac{|x-y|}{\sqrt{2} \max(\sqrt{x},\sqrt{y})}.\end{align}
It follows that
\[\left|\sqrt{x} - \sqrt{\frac{x+y}{2}}\right| \lesssim |\sqrt{x}-\sqrt{y}| \lesssim \left|\sqrt{x} - \sqrt{\frac{x+y}{2}}\right|.\]
The claim now follows from the definition of Hellinger distance, i.e. $\Dhels{\BP}{\BQ} = \int (\sqrt{\BP} - \sqrt{\BQ})^2$.
\end{proof}




