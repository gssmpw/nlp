Our results highlight the computational-statistical tradeoffs inherent to autoregressive
sequence modeling and imitation learning under misspecification and
show that while some further improvement to the next-token prediction
objective may be possible, there is little hope of developing
efficient algorithmic interventions that offer substantial
improvement beyond the next-token prediction barrier at $\Capx=\bigom(H)$---at least in a worst-case sense. More broadly, we view
our results as a first step toward a
computational theory of autoregressive sequence modeling and imitation
learning. Natural questions for future research include:
\begin{itemize}
\item \emph{Beyond offline imitation learning.} To what extent can our hardness
  results for learning under misspecification be bypassed through additional side
  information or access
  to the expert? 
  For example, can online/interactive algorithms that do not correspond to iterative learners (\cref{def:iterative}) 
  bypass the
  $\Capx=\bigom(H)$ barrier for next-token prediction?
\item \emph{Computational-statistical tradeoffs for general policy
    classes.} Our computational-statistical tradeoff for
  autoregressive linear models in \cref{sec:computational} is achieved
  through rather specialized algorithmic techniques---particularly
  the use of kernel-based approximation. Is there any hope of
  efficiently achieving similar tradeoffs for general policy
  parameterizations (assuming, e.g., access to an oracle for maximum likelihood)?\loose
\item \emph{Beyond additive misspecification.} While additive
  misspecification is a simple and well-studied solution concept, it
  is not clear whether this notion is meaningful for 
  autoregressive sequence modeling applications like language model
  pre-training. Are there more natural notions of
  misspecification---possibly with different algorithm design
  principles---that allow for non-trivial guarantees even when
  additive misspecification is large or constant? 
  \end{itemize}

  \subsection*{Acknowledgements}
  We thank Sivaraman Balakrishnan and Cyril Zhang for helpful discussions.

%





