
This section gives proofs for the main results from
\cref{sec:next_token}. In \cref{sec:log-loss-app} we prove \cref{thm:log-loss-bounded}, a sharp statistical analysis of $\loglossbc$ under a density bound assumption. In \cref{sec:improvements-app}, we prove \cref{prop:log-loss-prob-lb}, which proves statistical lower bounds for $\loglossbc$; \cref{prop:boosted-bc}, which shows that $\loglossbc$ can be boosted to high probability success via cross validation; and \cref{thm:layerwise_smoothing}, which provides a statistical analysis of $\smoothedloglossbc$. In \cref{sec:limits-app}, we prove \cref{thm:ntp-lb}, which shows that no next-token prediction algorithm can achieve $\Capx = o(H)$.


\subsection{Proofs from Section \ref*{sec:logloss} (Log-Loss Next-Token Prediction)}\label{sec:log-loss-app}


In this section, we prove a generalization of \cref{thm:log-loss-bounded} which allows for (a) infinite policy classes and (b) optimization error, since both will be useful for the setting of autoregressive linear models.

\begin{definition}\label{def:cover}
Fix a policy class $\Pi$ on state space $\MS$ and action space $\MA$. For $\eps>0$, we say that $\Pi' \subset \Pi$ is an \emph{$\eps$-cover} of $\Pi$ if for each $\pi\in\Pi$, there is some $\pi'\in\Pi'$ with $\log(\pi(a\mid{}s)/\pi'(a\mid{}s)) \leq \eps$ for all $a\in\MA$ and $s\in\MS$. We write $\Nlog(\Pi,\eps)$ to denote the cardinality of the smallest $\eps$-cover of $\Pi$.
\end{definition}

For a policy class $\Pi$ and a parameter $\epopt>0$, $\epopt$-approximate $\loglossbc$ takes as input trajectories $o\ind{1},\dots,o\ind{n}$ where $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$, and outputs some policy $\pihat$ satisfying
\[\wh L(\pihat) \geq \max_{\pi \in \Pi} \wh L(\pi) - \epopt \quad\text{where}\quad \wh L(\pi) := \sum_{i=1}^n \sum_{h=1}^H \log (\pihi).\]

\begin{theorem}[Full version of \cref{thm:log-loss-bounded}]\label{thm:log-loss-bounded-app}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. Suppose that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}) for some $\denbound \geq 1$. Let $n \in \NN$ and $\epsilon,\epopt,\delta > 0$. Let $\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$ from $\BP^{\pistar}$. Then any policy $\pihat$ produced by $\epopt$-approximate $\loglossbc$ satisfies, with probability at least $1-\delta$,
\begin{align} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} &\lesssim \frac{\epopt}{n} + H\epsilon+\frac{\log(\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{H\log(eW)\log(1/\delta)}{n} \\
&+ H\log(eW) \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\label{eq:log-loss-thm-1}\end{align}
Additionally, $\pihat$ satisfies, with probability at least $1-\delta$,
\begin{align} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} &\lesssim \frac{\epopt}{n} + H\epsilon+\frac{\log(\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{\log(eW)\log(1/\delta)}{n} \\&+ \frac{H\log(eW)}{\delta} \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\label{eq:log-loss-thm-2}\end{align}
\end{theorem}

In particular, \cref{thm:log-loss-bounded} follows from
\cref{eq:log-loss-thm-2} by taking $\epsilon = \epopt = 0$. Notice that \cref{eq:log-loss-thm-1} avoids dependence on $1/\delta$ in the approximation ratio, but incurs an extra factor of $H$ in the statistical rate. 

\paragraph{Proof overview} The proofs of the two bounds \cref{eq:log-loss-thm-1,eq:log-loss-thm-2} are largely similar; the difference is that \cref{eq:log-loss-thm-1} is derived by applying Bernstein's inequality in the final step, whereas \cref{eq:log-loss-thm-2} uses Markov's inequality. In both cases, the first observation is that by a standard argument (\cref{lemma:hels-to-lhat}), it suffices to bound the empirical excess risk of the best-in-class model $\pibar := \argmin_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}$:
\begin{equation}\wh L(\pistar) - \wh L(\pibar) = \sum_{i=1}^n\sum_{h=1}^H \log\frac{\pistarhi}{\pibarhi}.\label{eq:lhat-overview}\end{equation}
\cref{eq:lhat-overview} can be interpreted as an empirical analogue of $\Dkl{\bbP^{\pistar}}{\bbP^{\pibar}}$, and in prior work it is upper bounded in terms of the (population-level) $\chi$-squared divergence $\Dchis{\bbP^{\pistar}}{\bbP^{\pibar}}$. However, even under $W$-boundedness, this divergence cannot be bounded by Hellinger distance without paying a factor of $W^H$. Instead, our goal is to upper bound \cref{eq:lhat-overview} in terms of the \emph{sum of conditional squared Hellinger distances}, i.e.
\[\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right],\]
which can be upper bounded by $O(H) \cdot \Dhels{\bbP^{\pistar}}{\bbP^{\pibar}}$ by a standard information-theoretic argument (\cref{cor:hell-reverse-chain-bound}). To achieve this, we use $W$-boundedness together with a more layer-wise concentration argument. The main technical subtlety is that $W$-boundedness only gives an upper bound on the terms in \cref{eq:lhat-overview} (they could still be arbitrarily negative), which is problematic for naive concentration arguments; however, since an upper bound is ultimately what we care about, this can be fixed by appropriately ``truncating'' the logarithm prior to concentration. We now proceed to the formal proof.

%

\begin{proof}[\pfref{thm:log-loss-bounded-app}]
Define $\pibar := \argmin_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}$, and define $f: [0,\infty) \to \RR$ by 
\[f(t) := \begin{cases} 
\log(t) & \text{ if } t \geq 1 \\ t - 1 & \text{ if } t < 1 
\end{cases}.\] Then we have
\begin{align} 
\wh L(\pistar) - \wh L(\pihat) - \epopt
&\leq \wh L(\pistar) - \wh L(\pibar) \\ 
&= \sum_{i=1}^n \sum_{h=1}^H \log \frac{\pistarhi}{\pibarhi} \\ 
&\leq \sum_{i=1}^n \sum_{h=1}^H f\left(\frac{\pistarhi}{\pibarhi}\right)\label{eq:lhat-bound}
\end{align}
where the first inequality is by definition of $\pihat$, and the second inequality uses that $f(t) \geq \log(t)$ for all $t \geq 0$. Define $Z_{i,h} = f\left(\frac{\pistarhi}{\pibarhi}\right)$. By \cref{ass:density-bound}, we have $\frac{\pistarhi}{\pibarhi} \in [0,\denbound]$ and hence $|Z_{i,h}| \leq 1+\log(\denbound)$ almost surely. Consider the filtration $(\cF_{i,h})_{i,h}$ where $\cF_{i,h}$ is induced by $o\ind{1},\dots,o\ind{i-1}$ and $s_1\ind{i},a_1\ind{i},\dots,s_h\ind{i},a_h\ind{i},s_{h+1}\ind{i}$. Then the sequence of random variables $(Z_{i,h})_{i,h}$ is adapted to this filtration. By Freedman's inequality, there is an event $\cE_1$ that occurs with probability at least $1-\delta/3$, under which we have 
\begin{align}
\sum_{i=1}^n \sum_{h=1}^H Z_{i,h} \leq \sum_{i=1}^n \sum_{h=1}^H \EE[Z_{i,h}\mid{} \cF_{i,h-1}] + \frac{1}{1+\log(\denbound)} \sum_{i=1}^n \sum_{h=1}^H \EE[Z_{i,h}^2\mid{} \cF_{i,h-1}] + (1+\log(\denbound))\log(3/\delta)
\end{align}
where for notational convenience we write $\cF_{i,0}$ to denote $\cF_{i-1,H}$. Now observe that for any $i,h$,
\begin{align}
\EE[\exp(-Z_{i,h}) \mid{} \cF_{i,h-1}]
&= \EE\left[\exp\left(-f\left(\frac{\pistarhi}{\pibarhi}\right)\right) \middle|\, s_h\ind{i}\right] \\ 
&\leq \EE\left[\exp\left(-\log\left(\frac{\pistarhi}{\pibarhi}\right)\right) \middle|\, s_h\ind{i}\right] \\ 
&= \EE_{a_h \sim \pistar_h(\cdot\mid{} s_h\ind{i})}\left[\frac{\pibar_h(a_h\mid{}s_h\ind{i})}{\pistar_h(a_h\mid{}s_h\ind{i})}\right] \\ 
&= 1
\end{align}
where the inequality again uses that $f(t) \geq \log(t)$ for all $t \geq 0$. Therefore \cref{lemma:central-to-bernstein} with $\eta := 1$ and $V := 1+\log(\denbound)$ gives that
\[\EE[Z_{i,h}^2\mid{} \cF_{i,h-1}] \leq 4(2+\log(\denbound))\EE[Z_{i,h}\mid{} \cF_{i,h-1}].\]
We conclude that in event $\cE_1$,
\begin{align} 
\sum_{i=1}^n \sum_{h=1}^H Z_{i,h} &\leq 9\sum_{i=1}^n \sum_{h=1}^H \EE[Z_{i,h}\mid{} \cF_{i,h-1}] + (1+\log(\denbound))\log(3/\delta) \\ 
&= 9\sum_{i=1}^n \sum_{h=1}^H \EE_{a_h \sim \pistar_h(\cdot\mid{} s_h\ind{i})}\left[f\left(\frac{\pistar_h(a_h\mid{}s_h\ind{i})}{\pibar_h(a_h\mid{}s_h\ind{i})}\right)\right] + (1+\log(\denbound))\log(3/\delta) \\ 
&\leq 9(4+\log(\denbound))\sum_{i=1}^n \sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h\ind{i})}{\pibar_h(\cdot\mid{}s_h\ind{i})} + (1+\log(\denbound))\log(3/\delta)\label{eq:e1-bound}
\end{align}
where the final inequality is by \cref{lemma:f-to-hels} and again uses \cref{ass:density-bound}. Next, by Bernstein's inequality applied to the i.i.d. random variables $\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h\ind{i})}{\pibar_h(\cdot\mid{}s_h\ind{i})}$ for $i = 1,\dots,n$, there is an event $\cE_2$ that occurs with probability at least $1-\delta/3$ under which 
\begin{align} 
\sum_{i=1}^n \sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h\ind{i})}{\pibar_h(\cdot\mid{}s_h\ind{i})} &\leq n\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right] \\
&\qquad+ \frac{n}{H}\En^{\pistar}\left[\left(\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right)^2\right] \\ 
&\qquad+ H\log(3/\delta) \\ 
&\leq 2n\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right] + H\log(3/\delta).\label{eq:e2-bound}
\end{align}
Additionally, by Markov's inequality, there is an event $\cE_2'$ that occurs with probability at least $1-\delta/3$ under which 
\begin{align} 
\sum_{i=1}^n \sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h\ind{i})}{\pibar_h(\cdot\mid{}s_h\ind{i})} 
&\leq \frac{3n}{\delta}\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right].\label{eq:e2p-bound}
\end{align}
Finally, by \cref{lemma:hels-to-lhat}, there is an event $\cE_3$ that occurs with probability at least $1-\delta/3$ under which
\begin{align}\Dhels{\BP^{\pihat}}{\bbP^{\pistar}}\leq 4H\epsilon + \frac{4\log(3\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{2}{n}\left(\wh L(\pistar) - \wh L(\pihat)\right).\label{eq:e3-bound}\end{align}
Combining \eqref{eq:e3-bound} with \cref{eq:lhat-bound,eq:e1-bound,eq:e2-bound} we get that in the event $\cE_1 \cap \cE_2 \cap \cE_3$, which occurs with probability at least $1-\delta$,
\begin{align}
\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} &\lesssim \frac{2\epopt}{n} + 4H\epsilon + \frac{\log(3\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{(1+\log(\denbound))H\log(3/\delta)}{n} \\&+ (1+\log(\denbound))\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right].
\end{align}
The result \eqref{eq:log-loss-thm-1} now follows from the above bound and \cref{cor:hell-reverse-chain-bound}: in particular, applying \cref{cor:hell-reverse-chain-bound} to the distributions $\bbP := \bbP^{\pistar}$ and $\bbQ := \bbP^{\pibar}$ gives
\[H \cdot \Dhels{\bbP^{\pistar}}{\bbP^{\pibar}} \gtrsim \En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)} + \Dhels{\bbP_h(\cdot\mid{}s_h,a_h)}{\bbP_h(\cdot\mid{}s_h,a_h)}\right],\]
and the terms involving the transition probabilities all vanish.

Similarly, combining \eqref{eq:e3-bound} with \cref{eq:lhat-bound,eq:e1-bound,eq:e2p-bound} we get that in the event $\cE_1 \cap \cE'_2 \cap \cE_3$, which occurs with probability at least $1-\delta$,
\begin{align}
\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} &\lesssim \frac{2\epopt}{n} + 4H\epsilon + \frac{\log(3\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{(1+\log(\denbound))\log(3/\delta)}{n} \\
&+ \frac{(1+\log(\denbound))}{\delta}\En^{\pistar}\left[\sum_{h=1}^H \Dhels{\pistar_h(\cdot\mid{}s_h)}{\pibar_h(\cdot\mid{}s_h)}\right].
\end{align}
The result \eqref{eq:log-loss-thm-2} follows from this bound and \cref{cor:hell-reverse-chain-bound}.
\end{proof}

\subsubsection{Supporting Lemmas}

The following result is implicit in the proof of \citet[Proposition B.1]{foster2024behavior}. We include the proof for completeness.

\begin{lemma}[\cite{foster2024behavior}]\label{lemma:hels-to-lhat}
In the setting of \cref{thm:log-loss-bounded}, it holds with probability at least $1-\delta$ that 
\[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}}\leq 4H\epsilon + \frac{4\log(\Nlog(\Pi,\epsilon)/\delta)}{n} + \frac{2}{n}\left(\wh L(\pistar) - \wh L(\pihat)\right).\]
\end{lemma}

\begin{proof}[\pfref{lemma:hels-to-lhat}]
Let $\Pi'$ be an $\eps$-cover for $\Pi$ (\cref{def:cover}) and fix $\pihat' \in \Pi'$ with $\log(\pihat(a\mid{}s)/\pihat'(a\mid{}s)) \leq \eps$ for all $a\in\MA$ and $s\in\MS$. Note that $\log(\BP^{\pihat}(o)/\BP^{\pihat'}(o)) \leq H\epsilon$ for any trajectory $o$, and hence $\Dkl{\BP^{\pihat}}{\BP^{\pihat'}} \leq H\epsilon$. For each $\pi'\in\Pi'$ and $i \in [n]$, define the random variable
\[X_i(\pi') := \sum_{h=1}^H \log \frac{\pistarhi}{\piphi} = \log \frac{\BP^{\pistar}(o\ind{i})}{\BP^{\pi'}(o\ind{i})}.\]
Note that the random variables $X_1(\pi'),\dots,X_n(\pi')$ are independent and identically distributed. Thus, by an exponential Markov bound and the union bound, it holds with probability at least $1-\delta$ that for all $\pi'\in\Pi'$,
\begin{align}
\frac{1}{2}\left(\wh L(\pi') - \wh L(\pistar)\right)
&= \log\left(\prod_{i=1}^n e^{-\frac{1}{2}X_i(\pi')}\right) \\ 
&\leq \log(|\Pi'|/\delta) + n\cdot \log\left(\EE[e^{-\frac{1}{2}X_1(\pi')}]\right).\label{eq:exp-markov}
\end{align}
Condition on this event henceforth. For any $\pi'\in\Pi'$,
\begin{align}
\log\left(\EE[e^{-\frac{1}{2}X_1(\pi')}]\right) 
&= \log \EE_{o\sim\pistar}\left[\exp\left(-\frac{1}{2} \log \frac{\BP^{\pistar}(o)}{\BP^{\pi'}(o)}\right)\right]\\ 
&= \log\left(1 - \frac{1}{2}\Dhels{\BP^{\pistar}}{\BP^{\pi'}}\right) \\ 
&\leq -\frac{1}{2} \Dhels{\BP^{\pistar}}{\BP^{\pi'}}.
\end{align}
Setting $\pi' := \pihat'$ and substituting into \cref{eq:exp-markov}, we get that
\begin{equation} \wh L(\pihat') - \wh L(\pistar) \leq 2\log(|\Pi'|/\delta) - n\Dhels{\BP^{\pistar}}{\BP^{\pihat'}}.\label{eq:piphat-bound}\end{equation}
Therefore
\begin{align}
\Dhels{\BP^{\pihat}}{\BP^{\pistar}}
&\leq 2\Dhels{\BP^{\pihat}}{\BP^{\pihat'}} + 2\Dhels{\BP^{\pihat'}}{\BP^{\pistar}} \\ 
&\leq 2H\epsilon + \frac{4\log(|\Pi'|/\delta)}{n} + \frac{2}{n}\left(\wh L(\pistar) - \wh L(\pihat')\right) \\ 
&\leq 4H\epsilon + \frac{4\log(|\Pi'|/\delta)}{n} + \frac{2}{n}\left(\wh L(\pistar) - \wh L(\pihat)\right)
\end{align}
where the first inequality uses that $\Dhel{\cdot}{\cdot}$ is a metric; the second inequality uses the fact that $\Dhels{\BP^{\pihat}}{\BP^{\pihat'}} \leq \Dkl{\BP^{\pihat}}{\BP^{\pihat'}} \leq \epsilon$ as well as \cref{eq:piphat-bound}; and the third inequality uses that
\[\wh L(\pihat) - \wh L(\pihat') = \sum_{i=1}^n \log\frac{\BP^{\pihat}(o\ind{i})}{\BP^{\pihat'}(o\ind{i})} \leq nH\epsilon.\]
This completes the proof.
\end{proof}

We also use the following supporting lemmas in the proof of \cref{thm:log-loss-bounded-app}. \cref{lemma:f-to-hels} shows that even though we ``truncated'' the logarithm, we can still upper bound the corresponding $f$-divergence in terms of Hellinger distance, under a density ratio bound; it is a modification of e.g. \cite[Lemma 4]{yang1998asymptotic}.

\begin{lemma}[Central-to-Bernstein \citep{mehta2017fast}]\label{lemma:central-to-bernstein}
  Let $X\in\brk{-V,V}$ be a random variable with
  $\En\brk*{\exp(-\eta{}X)}\leq{}1$. Then $\En\brk*{X^2}\leq{} 4(\eta^{-1}+V)\En\brk*{X}$.
\end{lemma}

\begin{lemma}\label{lemma:f-to-hels}
Define $f: [0,\infty) \to \RR$ by 
\[f(t) := \begin{cases} 
\log(t) & \text{ if } t \geq 1 \\ t - 1 & \text{ if } t < 1 
\end{cases}.\]
For any set $\cX$ and densities $p,q \in \Delta(\cX)$ with $V := \sup_{x \in \cX} \frac{p(x)}{q(x)}$, it holds that
\[\EE_{x \sim p} f\left(\frac{p(x)}{q(x)}\right) \leq (4+\log(V)) \Dhels{p}{q}.\]
\end{lemma}

\begin{proof}[\pfref{lemma:f-to-hels}]
We have
\begin{align}
\EE_{x \sim p} \left[f\left(\frac{p(x)}{q(x)}\right)\right]
&= \EE_{x \sim q} \left[\frac{p(x)}{q(x)} f\left(\frac{p(x)}{q(x)}\right)\right] \\ 
&= \EE_{x \sim q} \left[\frac{p(x)}{q(x)} f\left(\frac{p(x)}{q(x)}\right) - \frac{p(x)}{q(x)} + 1\right] \\ 
&= \EE_{x \sim q} \left[h\left(\frac{p(x)}{q(x)}\right)\right]
\end{align}
where $h(t) := tf(t) - t + 1$. For any $t \in [0,1]$, we have
\begin{align}
h(t)
&= (1 - t)^2 \\ 
&= (1 + \sqrt{t})^2 (1 - \sqrt{t})^2 \\ 
&\leq 4 (1 - \sqrt{t})^2.
\end{align}
Next, observe that for any $t \geq 1$, since $\log(t) = 2\log\sqrt{t} \leq 2\sqrt{t}-2$, we have
\begin{align}
h(t)
&= t\log(t) + 1 - t \\ 
&= t\log(t) + (1-\sqrt{t})(1+\sqrt{t}) \\ 
&\leq t\log(t) + (1-\sqrt{t})(1+\sqrt{t}) + (2\sqrt{t}-1)(2\sqrt{t}-2-\log(t)) \\ 
&= (\sqrt{t} - 1)^2(3 + \log(t)).
\end{align}
Since $p(x)/q(x) \in [0,V]$ for all $x \in \cX$, we get that
\begin{align}
\EE_{x \sim p} \left[f\left(\frac{p(x)}{q(x)}\right)\right] 
&= \EE_{x \sim q} \left[h\left(\frac{p(x)}{q(x)}\right)\right] \\
&\leq (4 + \log(V))\EE_{x \sim q} \left[\left(\sqrt{\frac{p(x)}{q(x)}} - 1\right)^2\right] \\ 
&= (4 + \log(V)) \Dhels{p}{q}
\end{align}
as claimed.
\end{proof}



  \subsection{Proofs from Section \ref*{sec:improvements} (Improvements
    to Next-Token Prediction)}\label{sec:improvements-app}



Here we prove \cref{prop:log-loss-prob-lb}, \cref{prop:boosted-bc}, and \cref{thm:layerwise_smoothing}. The following result shows that $\loglossbc$ necessarily incurs either a factor of $H\log(\denbound)$ in the rate (where $H$ is the horizon and $\denbound$ is the norm bound in \cref{ass:density-bound}), or has approximation ratio scaling with $1/\delta$.

\begin{proposition}[Restatement of \cref{prop:log-loss-prob-lb}]\label{prop:log-loss-prob-lb-app}
  Fix any $H \in \NN$ and $\denbound \geq 2$ and $\delta \in (0,1/2)$, and set $n_0 := H\log(\denbound)$. There is an $H$-step autoregressive MDP $M$, a policy class $\Pi$ of size $|\Pi| = 2$, and an expert policy $\pistar$ such that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}), with the following property. Given $n_0$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$ from $\BP^{\pistar}$, the estimator $\pihat$ produced by $\loglossbc$ satisfies, with probability at least $\delta$,
  \[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \gtrsim \frac{H\log(\denbound)}{\delta} \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.\]
  \end{proposition}

  
  \begin{proof}[\pfref{prop:log-loss-prob-lb-app}]
  Let $M$ be the $H$-step autoregressive MDP with context space $\cX = \{\xfrak,\yfrak\}$, action space $\cA = \{\afrak,\bfrak\}$, and context distribution $\rho \ \in\Delta(\cX)$ with $\rho(\yfrak) = 2\delta/(H\log \denbound)$. Define $\pistar$ so that 
  \[ 
  \Pr^{\pistar}[a_{1:H} \mid{} x] = \begin{cases} 
  \mathbbm{1}[a_{1:H}=(\afrak,\dots,\afrak)] &\text{ if } x =\xfrak \\ 
  \mathbbm{1}[a_{1:H} = (\bfrak,\dots,\bfrak)] & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\pia$ so that
\[ 
  \Pr^{\pia}[a_{1:H} \mid{} x] = \begin{cases} 
  \mathbbm{1}[a_{1:H}=(\afrak,\dots,\afrak)] &\text{ if } x=\xfrak \\ 
  \prod_{h=1}^H \left(1-\frac{1}{\denbound}\right)^{\mathbbm{1}[a_h=\afrak]}\left(\frac{1}{\denbound}\right)^{\mathbbm{1}[a_h=\bfrak]} & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\pib$ so that
  \[ 
  \Pr^{\pib}[a_{1:H} \mid{} x] = \begin{cases}
  \left(\frac{4}{5}\right)^{\mathbbm{1}[a_1=\afrak]} \left(\frac{1}{5}\right)^{\mathbbm{1}[a_1=\bfrak]}\mathbbm{1}[a_{2:H}=(\afrak,\dots,\afrak)] &\text{ if } x=\xfrak \\ 
  \mathbbm{1}[a_{1:H} = (\bfrak,\dots,\bfrak)] & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\Pi := \{\pia,\pib\}$. Observe that $\Pr^{\pia}[a_h=\afrak \mid{} x=\xfrak, a_{1:h-1}=a'_{1:h-1}] = 1$ and $\Pr^{\pia}[a_h=\bfrak \mid{} x=\yfrak, a_{1:h-1}=a'_{1:h-1}] \geq 1/\denbound$ for any $h \in [H]$ and $a'_{1:h-1} \in \cA^{h-1}$. Moreover $\Pr^{\pib}[a_h=\afrak \mid{} x=\xfrak, a_{1:h-1}=a'_{1:h-1}] \geq 4/5 \geq 1/\denbound$ and $\Pr^{\pib}[a_h=\bfrak \mid{} x=\yfrak, a_{1:h-1}=a'_{1:h-1}] = 1$ for any $h \in [H]$ and $a'_{1:h-1} \in \cA^{h-1}$. Thus, $\pistar$ is $\denbound$-bounded with respect to $\Pi$.
  
  
  Consider $n$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a\ind{i}_{1:H})$ from $\BP^{\pistar}$. By choice of the context distribution $\rho$, we have $\Pr[x\ind{i}=\yfrak] = 2\delta/(H\log \denbound)$ for each $i \in [n]$. Let $\cE$ be the event that $n_\yfrak := \#\{i \in [n]: x\ind{i} = \yfrak\} \geq 1$. Then 
  \[\Pr[\cE] \geq 1 - \left(1 - \frac{2\delta}{H\log(\denbound)}\right)^n \geq 1 - e^{-2\delta} \geq \delta\]
  by choice of $n$. Condition on the event $\cE$. Again by choice of $n$, we have $n_\yfrak \geq \frac{n}{2H\log(\denbound)}$. Thus, we have
  \begin{align} 
  \sum_{i=1}^n \sum_{h=1}^H \log \pia_h(a_h\ind{i} \mid{} x\ind{i}, a_{1:h-1}\ind{i})
  &= n_\yfrak \sum_{h=1}^H \log \pia_h(\bfrak \mid{} \yfrak, \bfrak,\dots,\bfrak) \\ 
  &= n_\yfrak H \log (1/\denbound) \\ 
  &\leq -\frac{n}{2}
  \end{align} 
  since $n_\yfrak \geq \frac{n}{2H\log(\denbound)}$ and $\denbound \geq 1$. On the other hand,
  \begin{align}
  \sum_{i=1}^n \sum_{h=1}^H \log \pib_h(a_h\ind{i} \mid{} x\ind{i}, a_{1:h-1}\ind{i})
  &= (n-n_\yfrak) \sum_{h=1}^H \log \pib_h(\afrak \mid{} \xfrak, \afrak,\dots,\afrak) \\
  &= (n-n_\yfrak) \log(4/5) \\
  &\geq -n\log(5/4).
  \end{align}
  Since $\log(5/4) < 1/2$, it follows from the definition of $\loglossbc$ that $\pihat = \pib$. Moreover, $\Dhels{\BP^{\pistar}}{\BP^{\pib}} \geq \Omega(1)$ whereas $\Dhels{\BP^{\pistar}}{\BP^{\pia}} \leq \delta/(H\log \denbound)$. The claim follows.
  \end{proof}

  \begin{proof}[Proof of \cref{prop:boosted-bc}]%
  
For each $1 \leq i \leq K/2 = \log(2/\delta)$, applying the second guarantee of \cref{thm:log-loss-bounded-app} with dataset size $n/(2\log(2/\delta))$ and failure probability $1/2$ gives that with probability at least $1/2$,
\[\Dhels{\BP^{\pihat^i}}{\bbP^{\pistar}} \lesssim \frac{\log(2|\Pi|)\log(2/\delta)}{n} + \frac{(1+\log(\denbound))\log(2/\delta)}{n} + H(1+\log(\denbound)) \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.\]
Thus, with probability at least $1-(1/2)^{K/2} = 1-\delta/2$, there exists at least one $i \in [K/2]$ that satisfies the above bound. Condition on this event. By the guarantee of \cref{thm:rho-il} with dataset size $n/2$, failure probability $\delta/2$, and policy class $\{\pihat^1,\dots,\pihat^{K/2}\}$, with probability at least $1-\delta/2$ the output of the algorithm $\pihat$ satisfies
\begin{align} 
\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} &\lesssim \frac{\log(K/\delta)}{n} + \Dhels{\BP^{\pihat^i}}{\bbP^{\pistar}}  \\ 
&\lesssim \frac{\log(2|\Pi|\denbound)\log(1/\delta) + \log\log(1/\delta)}{n} + H(1+\log(\denbound)) \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}} \\ 
&\lesssim \frac{\log(2|\Pi|\denbound)\log(1/\delta)}{n} + H(1+\log(\denbound)) \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}
\end{align}
as claimed.
\end{proof}

\begin{proof}[Proof of \cref{thm:layerwise_smoothing}]
For each $\pi \in \Pi$ let $\pi^\lambda$ denote the ``smoothed'' policy defined by
\[\pi^\lambda_h(a_h\mid{}s_h) := (1-\lambda)\pi(a_h\mid{}s_h) + \lambda\pistar(a_h\mid{}s_h),\]
and define $\Pi^\lambda := \{\pi^\lambda: \pi \in \Pi\}$. Then applying $\smoothedloglossbc$ with policy class $\Pi$ is the same as applying $\loglossbc$ with policy class $\Pi^\lambda$ (save for outputting $\pihat$ rather than $\pihat^\lambda$). Moreover, for any $\pi^\lambda \in \Pi^\lambda$ and any $h \in [H]$, $s_h \in \cS$, and $a_h \in \cA$, it holds that
\[\frac{\pistar_h(a_h\mid{}s_h)}{\pi^\lambda_h(a_h\mid{}s_h)} = \frac{\pistar_h(a_h\mid{}s_h)}{(1-\lambda)\pi_h(a_h\mid{}s_h) + \lambda \pistar_h(a_h\mid{}s_h)} \leq \frac{1}{\lambda}.\]
Thus, $\pistar$ is $1/\lambda$-bounded with respect to $\Pi^\lambda$. By the first guarantee of \cref{thm:log-loss-bounded} with $\denbound := 1/\lambda = H^2 n$, we have with probability at least $1-\delta$ that 
\begin{equation} \Dhels{\BP^{\pihat^\lambda}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|/\delta)}{n} + \frac{H\log(Hn)\log(1/\delta)}{n} + H\log(Hn) \cdot \min_{\pi\in\Pi} \Dhels{\BP^{\pi^\lambda}}{\bbP^{\pistar}}.\label{eq:smoothed-intermediate}
\end{equation}
Since $\Dhel{\cdot}{\cdot}$ is a metric, we have for any $\pi \in \Pi$ that
\begin{align} 
\Dhels{\BP^{\pi^\lambda}}{\bbP^{\pistar}} 
&\leq 2\left(\Dhels{\BP^{\pi^\lambda}}{\BP^\pi} + \Dhels{\BP^\pi}{\bbP^{\pistar}}\right) \\
&\leq 4\Dtv{\BP^{\pi^\lambda}}{\BP^\pi} + 2\Dhels{\BP^\pi}{\bbP^{\pistar}} \\
&\leq 4\lambda H + 2\Dhels{\BP^\pi}{\bbP^{\pistar}}
\end{align}
where the final inequality can be derived by coupling a trajectory drawn from $\BP^\pi$ with a trajectory drawn from $\BP^{\pi^\lambda}$: at each step, the trajectories deviate with probability at most $\lambda$. By a symmetric argument, we also have
\[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \leq 4\lambda H + 2\Dhels{\BP^{\pihat^\lambda}}{\bbP^{\pistar}}.\]
Substituting the preceding bounds into \cref{eq:smoothed-intermediate} and using that $\lambda = 1/(H^2 n)$, we get that
\[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \preceq \frac{\log(|\Pi|/\delta)}{n} + \frac{H\log(Hn)\log(1/\delta)}{n} + H\log(Hn) \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}\]
as claimed.
\end{proof}
