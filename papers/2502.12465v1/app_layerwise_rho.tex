In this section, we design a next-token prediction algorithm (i.e. iterative learner---see \cref{def:iterative}) that achieves $\Capx = O(H)$, which by \cref{thm:ntp-lb} is the best possible approximation ratio for any next-token prediction algorithm. In comparison, $\loglossbc$ requires assuming either a bound on density ratios, or query access to the density of $\pistar$, in order to achieve this guarantee. We emphasize that this result is mainly of interest statistically, and from the perspective of understanding the limits of next-token prediction---the algorithm is likely not efficiently implementable for autoregressive linear models.

For simplicity, we assume that the policy class $\Pi$ has no parameter sharing, as defined below. Note that \cref{thm:ntp-lb} also applies in this setting. Moreover, the assumption is nearly without loss of generality from a statistical perspective, since if $\Pi$ does have parameter sharing then one can define a new policy class $\overline{\Pi} := \Pi_1 \times \dots \times \Pi_H$ where $\Pi_h$ is the class of possible conditional distributions at layer $h$. Since $|\overline{\Pi}| \leq |\Pi|^H$, this will worsen the statistical rate by a factor of at most $H$, but $\overline{\Pi}$ has no parameter sharing so the below result then applies.

\begin{definition}
A policy class $\Pi$ \emph{has no parameter sharing} if there are sets $\Pi_1,\dots,\Pi_h$ so that $\pi = (\pi_h)_h \in \Pi$ if and only if $\pi_h \in \Pi_h$ for all $h \in [H]$.
\end{definition}

For a policy class $\Pi$ with no parameter sharing, \layerrhobc takes as input trajectories $o\ind{1},\dots,o\ind{n}$ where $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$, and outputs the policy $\pihat = (\pihat_h)_{h=1}^H$ defined by
\[\pihat_h := \argmin_{\pi_h \in \Pi_h} \sup_{\pi'_h \in \Pi_h} \sum_{i=1}^n\tau\left( \frac{\pihi}{\piphi}\right).\]


\begin{proposition}\label{prop:layerwise-rho}
Fix an MDP $M$, a policy class $\Pi$ with no parameter sharing, and an expert policy $\pistar$. Let $n \in \NN$ and $\delta \in (0,1/2)$. Let $\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$ from $\BP^{\pistar}$. Then the policy $\pihat$ produced by \layerrhobc satisfies, with probability at least $1-\delta$,
\begin{equation} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|) + H\log(H/\delta)}{n} + H \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\end{equation}
\end{proposition}

\begin{proof}[\pfref{prop:layerwise-rho}]
For each $h \in [H]$ and $\pi \in \Pi\cup\{\pistar\}$, let $\BP^{\pi}_{1:h}$ denote the distribution of the prefix $(s_1,a_1,\dots,s_h,a_h)$ of a trajectory $(s_1,a_1,\dots,s_H,a_H)$ drawn from $\BP^\pi$. Let $\BP^{\pistar \circ_h \pi_h}_{1:h}$ denote the distribution of $(s_1,a_1,\dots,s_h,a_h)$ when $(s_1,a_1,\dots,s_h)$ is drawn from $\BP^{\pistar}$ and $a_h \sim \pi_h(\cdot\mid{}s_h)$. Define the family of distributions $\cP := \{\BP^{\pistar\circ_h\pi_h}_{1:h}: \pi_h \in \Pi_h\}$. Observe that for any $\pi_h, \pi'_h \in\Pi_h$ and trajectory prefix $(s_1,a_1,\dots,s_h,a_h)$, we have
\[\frac{\BP^{\pistar\circ_h\pi_h}_{1:h}(s_1,a_1,\dots,s_h)}{\BP^{\pistar\circ_h\pi'_h}_{1:h}(s_1,a_1,\dots,s_h)} = \frac{\pi_h(a_h\mid{}s_h)}{\pi'_h(a_h\mid{}s_h)}.\]
Thus, for each $h \in [H]$, by applying \cref{thm:rho} with family $\cP$, we have with probability at least $1-\delta/H$ that 
\begin{align} 
\Dhels{\BP^{\pistar\circ_h\pihat_h}_{1:h}}{\BP^{\pistar}_{1:h}} &\lesssim \frac{\log(H|\Pi_h|/\delta)}{n} + \min_{\pi_h\in\Pi_h} \Dhels{\BP^{\pistar\circ_h\pi_h}_{1:h}}{\BP^{\pistar}_{1:h}} \\ 
&= \frac{\log(H|\Pi_h|/\delta)}{n} + \min_{\pi=(\pi_k)_k\in\Pi} \Dhels{\BP^{\pistar\circ_h\pi_h}_{1:h}}{\BP^{\pistar}_{1:h}}.
\end{align}
Condition on the event that this bound holds for all $h \in [H]$, which occurs with probability at least $1-\delta$. Let $\pibar := \argmin_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}$. We have
\begin{align}
\Dhels{\BP^{\pihat}}{\BP^{\pistar}}
&\lesssim \sum_{h=1}^H \En^{\pistar}\left[\Dhels{\pihat_h(\cdot\mid{}s_h)}{\pistar_h(\cdot\mid{}s_h)}\right] \\ 
&= \sum_{h=1}^H \Dhels{\BP^{\pistar\circ_h\pihat_h}_{1:h}}{\BP^{\pistar}_{1:h}} \\ 
&\lesssim \frac{\log(|\Pi|)+H\log(H/\delta)}{n} + \sum_{h=1}^H \min_{\pi \in \Pi} \Dhels{\BP^{\pistar\circ_h\pi_h}_{1:h}}{\BP^{\pistar}_{1:h}} \\ 
&\lesssim \frac{\log(|\Pi|)+H\log(H/\delta)}{n} + \sum_{h=1}^H \min_{\pi \in \Pi} \Dhels{\BP^\pi_{1:h}}{\BP^{\pistar}_{1:h}} \\ 
&\leq \frac{\log(|\Pi|)+H\log(H/\delta)}{n} + H\cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}
\end{align}
where the first inequality is by \cref{lemma:hell-chain-bound}, the third inequality is by \cref{lemma:hell-reverse-chain-bound}, and the fourth inequality is by the data processing inequality. 
\end{proof}
