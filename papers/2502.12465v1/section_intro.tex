

Next-token prediction with the logarithmic loss \citep{shannon1951prediction} is a cornerstone of autoregressive sequence modeling---particularly language
model pre-training \citep{vaswani2017attention,radford2019language}. It estimates a distribution over sequences $(a_1,\ldots,a_H)$ by jointly fitting a sequence
  of conditional models
  $\pihat(a_h\mid{}a_{1:h-1})$ to
  maximize log-likelihood.
  This method is appealing in its simplicity and scalability, but seemingly ignores the \emph{feedback loop}
  inherent to autoregressive generation, whereby outputs sampled from
  the learned model depend on tokens previously generated by
    the same (possibly imperfect) model. This can
  lead to the widely-observed phenomenon known as \emph{error
    amplification} (or \emph{exposure
  bias}), where small inaccuracies in the conditional model
$\pihat(a_h\mid{}a_{1:h-1})$ compound, leading to out-of-distribution
sequences with poor performance on downstream tasks of interest
\citep{holtzman2019curious,braverman2020calibration,arora2022exposure,block2023butterfly};
some have speculated this to be a fundamental limitation \citep{lecun2023large,bachmann2024pitfalls}.

\ah{I think Akshay makes a good point that perhaps we should give ourselves more credit for 
(now in my words) "showing for the first time that log-loss with misspecification causes error compounding, 
and that offline algorithms are sufficient to achieve good guarantees" }
\ah{My other comment is that it could help our delivery to combine, rather than differentiate (as it currently reads to me), 
the causes for misspecification in robotics vs language modeling.}
Next-token prediction can be seen as a special case of
    \emph{behavior cloning}, a fundamental approach to the more general
    problem of \emph{imitation learning} (IL) \citep{pomerleau1988alvinn}, for which similar compounding errors
    (e.g., a learned policy
    for a self-driving car
    slowly drifting off of the road) have been observed    \citep{ross2010efficient,laskey2017dart,block2023butterfly}. Here,
    a growing body of empirical work
\citep{bansal2018chauffeurnet,de2019causal,spencer2021feedback}
    suggests that error amplification may
    arise from \emph{misspecification}, where the learned policy is not
    sufficiently powerful to represent the target policy. For example, in applications of IL to robotics, there may be issues of partial
    observability or privileged information \citep{de2019causal}\arxiv{---e.g., if the conditional
    distribution $\pistar_h$ depends on the full history, but the
    model $\pihat_h$ is Markovian---} and in language modeling,
    misspecification may arise when using a
    model of limited capacity to represent a
    complex distribution (e.g., the distribution over all
    text on the internet) \citep{braverman2020calibration}, or when trying to distill a powerful
    teacher into a weaker student \citep{touvron2023llama,team2024gemini}. However, there is little theoretical understanding of the impact of misspecification in IL.\loose




    In this work, we draw inspiration from the \IL{}
    literature \citep{ross2010efficient,rajaraman2020toward,rajaraman2021provably,block2024provable,foster2024behavior}, and
    quantify error amplification through the effect of
    \emph{horizon} (sequence length) on model performance. Through
    this lens, recent work of
\citet{foster2024behavior} shows that in the absence of
    misspecification, next-token prediction with the log-loss can avoid error
    amplification entirely. Yet, under misspecification, there are simple problem instances (cf. \cref{cor:unbounded}) where it %
    fails to learn a non-trivial model,
    even when a good model exists and optimization
    error is not a concern. %
    This motivates us to investigate \emph{whether error amplification is fundamental in autoregressive sequence modeling and \IL{} under misspecification}. Concretely, we ask whether next-token prediction with the log-loss suffers from:\loose
\begin{itemize}[noitemsep, topsep=0.25pt]%
      \item[\textbf{(a)}] An \emph{algorithmic} limitation, which we can hope to 
        mitigate through (efficient)
        algorithmic interventions alone (e.g., by modifying the
        next-token prediction objective)?%
      \item[\textbf{(b)}] A \emph{computational} limitation, in the sense that
        there is enough information in the training data to avoid error amplification, but extracting it is
        computationally intractable?%
      \item[\textbf{(c)}] An \emph{information-theoretic/statistical} limitation, in the
      sense that there is simply not enough information in the
      training data to
      avoid error amplification?%
      \loose
    \end{itemize}
We show that error amplification is information-theoretically
avoidable; moreover, non-trivial algorithmic interventions to
next-token prediction \emph{are} possible, but there is a fundamental
limit to the improvement that can be achieved by efficient algorithms, at what we call the \emph{next-token prediction barrier}.







    









%
\arxiv{\subsection{Error Amplification in Next-Token Prediction under
  Misspecification}}


For the exposition, we focus on autoregressive sequence modeling, and defer discussion of the more general \IL{} setting to \cref{sec:setting}. %
The goal is to learn a
conditional distribution/model $\pistar: \cX \to \Delta(\cA^H)$, where $\cX$
is the context space, $\cA$ is a token space, and $H$ is the horizon. By Bayes' rule, any model $\pi:\cX\to\Delta(\cA^H)$ can be represented autoregressively \arxiv{in terms of $H$ token-level conditional distributions $\pi_h: \cX\times\cA^{h-1} \to \Delta(\cA)$:\loose
\begin{align}
  \pi(a_{1:H}\mid{} x) = \prod_{h=1}^H \pi_h(a_h\mid{}
  x,a_{1:h-1}).
  \label{eq:bayes}
\end{align}}%
For a fixed context distribution $\mu \in \Delta(\cX)$ and any model
$\pi$, we write $\BP^\pi$ to denote the distribution of sequences $(x,
a_{1:H})$ induced by sampling $x \sim \mu$ and $a_{1:H} \sim
\pi(\cdot\mid{}x)$. 

Given a model class $\Pi$ (represented by, e.g., transformers or
other deep networks) and a dataset
$\crl*{(x\ind{i},a_{1:H}\ind{i})}_{i=1}^n$ assumed to be sampled i.i.d. from $\BP^{\pistar}$, \emph{next-token prediction with the logarithmic loss}
(e.g., \citet{radford2019language}) solves the following optimization problem: 
\begin{align}
  \label{eq:bc}
  \pihat \in \argmax_{\pi\in\Pi}
  \sum_{i=1}^{n}\sum_{h=1}^{H}\log\prn*{\pi_h(a_h\ind{i}\mid{}x\ind{i},a_{1:h-1}\ind{i})}.
\end{align}
For the more general imitation learning setting, this
  coincides with \emph{behavior cloning} (\cref{sec:basic}).
As noted by
\citet{foster2024behavior}, the objective in \cref{eq:bc} is equivalent
to maximum likelihood estimation (MLE) over the distribution family
$\crl*{\bbP^{\pi}}_{\pi\in\Pi}$, so that standard MLE guarantees
\arxiv{\citep{wong1995probability,Sara00,zhang2006from}} imply convergence in
Hellinger distance---a standard metric for distribution estimation
defined via $\Dhels{\bbP}{\bbQ}=\int(\sqrt{\mathrm{d}\bbP}-\sqrt{\mathrm{d}\bbQ})^2$---when the problem is \emph{realizable/well-specified} in the sense
that $\pistar\in\Pi$:\arxiv{\footnote{This result follows from a well-known
  connection between the moment generating function for the
  logarithmic loss and Hellinger distance\arxiv{ (and other Renyi
  divergences)}. Importantly, this holds with no assumption on
  boundedness of the densities.\loose}}
\begin{proposition}[\citet{foster2024behavior}]
      \label{prop:mle_finite}
      Whenever $\pistar\in\Pi$, the estimator $\pihat$ in \cref{eq:bc} satisfies that $\DhelsX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}} \leq
  2\log(\abs{\Pi}\delta^{-1})/n$ with
      probability at least $1-\delta$.\footnote{For simplicity, we work with finite classes
        $\Pi$, following a
        common convention in reinforcement learning theory
        \citep{agarwal2019reinforcement}.  \cref{prop:mle_finite} (and later results) extends to infinite classes via standard covering arguments.
      }\loose
\end{proposition}

This result yields \emph{horizon-independent} guarantees on generation
performance (as long as the expressivity of $\Pi$ is controlled,
e.g. via parameter sharing). Namely, for any function
$r(x,a_{1:H})\in\brk*{0,1}$ measuring quality of generated sequences (e.g.,\arxiv{ text coherence,
chatbot quality, or} correctness of generated proofs\arxiv{ or code}), we have\loose
\begin{align}
  \label{eq:rollout}
  \En_{\pihat}\brk*{r(x,a_{1:H})} \geq
  \En_{\pistar}\brk*{r(x,a_{1:H})}
  - \DhelX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}},
\end{align}
so by \cref{prop:mle_finite}, the quality improves as $n\to\infty$, with no dependence on the horizon $H$. %
\loose



\paragraph{Error amplification  under misspecification}
Unfortunately, if the model class is misspecified, i.e. $\pistar \not
\in \Pi$, the guarantees above break down. A trivial failure occurs when densities for models in $\Pi$ are not bounded away from $0$, allowing
the loss in \cref{eq:bc} to take value $-\infty$, and leading to arbitrarily bad performance.\footnote{If $\BP^{\pistar}$ is $\veps$-close to $\Pi$ in
  \emph{\chis-divergence}, then next-token prediction can 
  avoid error amplification. Concretely,
    \citet{foster2024behavior} show that \cref{eq:bc} achieves $\DhelsX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}}
  \approxleq{} \frac{\log(\abs{\Pi}\delta^{-1})}{n} +
  \inf_{\pi\in\Pi}\DchisX{\big}{\bbP^{\pi}}{\bbP^{\pistar}}$.
  However, 
  \chis-divergence can be infinite even when Hellinger distance is small.} A more troubling issue is that it can be the case (cf. \cref{prop:log-loss-lb}) that all $\pi\in\Pi$ have well-behaved densities, yet the estimator $\pihat$ in \cref{eq:bc} incurs explicit horizon dependence: 
    \begin{equation}
      \Dhels{\BP^{\pihat}}{\BP^{\pistar}}\geq \Omega(\min(1, \veps^2 H)) \quad\text{while}\quad \min_{\pi\in\Pi}\Dhels{\BP^{\pi}}{\BP^{\pistar}} \leq \veps^2.
      \label{eq:failure-h}
    \end{equation}
  That is,
  even though the best model in $\Pi$ is
    $\veps$-suboptimal with respect to generation performance (via
    \cref{eq:rollout}), next-token prediction with the log-loss yields a model whose
    generation performance degrades with $H$---a marked departure from the well-specified setting. One of our initial contributions is a sharp characterization of this phenomenon.

    
    \arxiv{
      \begin{remark}[Connection to imitation learning]
      In imitation learning (IL), the goal is to learn a \emph{policy}
      $\pihat$ that matches the distribution of an \emph{expert
        policy} in a Markov decision process. Autoregressive
      sequence modeling can be viewed as a special case of this problem, associating sequence models with policies in a \emph{token-level MDP}, and the next-token prediction objective in \cref{eq:bc} is a special
      case of \emph{behavior cloning}, the most basic and widely used
      algorithm in IL.
       Understanding the impact of horizon/sequence length on
performance is a central theme in \IL{}
\citep{ross2010efficient,rajaraman2020toward,rajaraman2021provably,foster2024behavior}.
 Further, as discussed in \cref{sec:setting}, the estimation in Hellinger distance is directly connected to \IL{}
performance. While
      we focus on autoregressive modeling in this section for the
      purpose of exposition, we present our main results in sections
      that follow in the general \IL{} framework; see
      \cref{sec:setting} for a formal overview. \loose\end{remark} %
        }
    

\begin{remark}[Terminology for next-token prediction]
      \label{rem:next_token}
\arxiv{  Throughout the paper, we} use the term \emph{next-token prediction}
  to refer to the broader paradigm of minimizing any sum of token-wise or per-timestep loss
  functions. Next-token prediction with the logarithmic loss \ahreplace{, defined in \cref{eq:bc},}{(\cref{eq:bc})} represents the most widely
  used instantiation.\arxiv{ \citet{foster2024behavior} show that
  the logarithmic loss enjoys benefits in horizon
  dependence over other standard losses (e.g., square or indicator)
  even in the well-specified setting, motivating our focus on it in
  this exposition.}\loose
\end{remark}



    %

  %
  

  

  












  
\subsection{Our Question: Agnostic Guarantees for Hellinger Distance}

With the goal of mitigating error amplification (i.e., avoiding the failures discussed above), we ask whether it is possible to %
achieve \emph{agnostic estimation} guarantees with respect to
sequence-level Hellinger distance. Concretely, consider any
model class $\Pi$, and let $\pistar$ be an unknown model
which may or may not lie in $\Pi$. We would like a learning algorithm
that---given $n$ \iid trajectories drawn from $\bbP^{\pistar}$---produces $\pihat$ satisfying the following agnostic estimation guarantee with high probability:\loose
\begin{equation} \Dhels{\BP^{\pihat}}{\BP^{\pistar}} \leq \Capx \cdot \min_{\pi\in\Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}} + \vepsstatsn.
\label{eq:hels-goal-intro}
\end{equation}
Here, $\vepsstatsn$ represents statistical error with
$\vepsstatsn\to{}0$ as $n\to\infty$, and should ideally be not much
larger than in the well-specified setting (i.e.,
$\vepsstatsn\approxleq\frac{\log(\abs{\Pi}\delta^{-1})}{n}$ for a
finite class). Meanwhile, $\vepsmis^2\ldef{}\min_{\pi\in\Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}}$ represents \emph{irreducible error} for
estimation, since any proper learning algorithm selecting
$\pihat\in\Pi$ must (trivially) have
$\Dhels{\BP^{\pihat}}{\BP^{\pistar}} \geq  \vepsmis^2.$
The parameter $\Capx\geq{}1$ is an \emph{approximation
ratio}; if $\Capx=1$, then $\pihat$ is no worse at approximating
$\pistar$ than the best model in $\Pi$
asymptotically, but this may be too much to ask (for either statistical
or computational reasons).

By \cref{eq:failure-h}, next-token prediction with the log-loss incurs $\Capx \geq \Omega(H)$ even for well-behaved $\Pi$; it incurs $\Capx=\infty$ in the worst case (cf. \cref{cor:unbounded}). %
Restating our central question, we ask: \emph{what is the tightest
  approximation ratio $\Capx$ that can be achieved} \textbf{(a)} via practical interventions to the next-token prediction
    objective; \textbf{(b)}  via any computationally efficient algorithm; and \textbf{(c)} via \emph{any} algorithm, irrespective of
    computational efficiency? 












\paragraph{Computational testbed: Autoregressive linear models}
To formalize questions of computational efficiency, our testbed 
will be the class $\Pi$ of \emph{autoregressive linear models}, defined by a known feature map $\phi: \MX \times \MA^\star \to \RR^d$. For each parameter $\theta \in\Theta\subset \RR^d$, the model $\pi_\theta=(\pi_{\theta,h})_{h=1}^H$ is defined by 
\begin{equation}
  \label{eq:linear}
  \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) \propto \exp(\langle \theta,
    \phi(x,a_{1:h})\rangle).
\end{equation}
Recall that in practice \citep{radford2019language}, autoregressive
sequence models (e.g., based on transformers) typically generate each token by sampling from a
softmax distribution determined by a linear combination of learned
features. \cref{eq:linear} is a simplification
where we freeze the features, but it can still capture rich
non-Markovian structure (depending on the choice of feature map).
In this setting, the log-loss objective (\cref{eq:bc}) is concave in parameter space with efficiently computable
gradients, so it can be efficiently optimized. In conjunction with \cref{prop:mle_finite} (generalized to infinite model classes), it follows that learning \emph{well-specified} autoregressive linear models is end-to-end computationally tractable, under appropriate norm bounds.

\begin{proposition}[informal; see \cref{cor:linear-wellspec-logloss}]\label{prop:gdlogloss-intro}
Let $\Pi := \{\pi_\theta: \theta\in\Theta\}$ for a convex set $\Theta\subseteq \RR^d$. Given $n$ i.i.d. samples from $\BP^{\pistar}$ for some $\pistar \in\Pi$, projected gradient ascent on \cref{eq:bc} can be implemented in time $\poly(n,d,H,|\MA|)$ and yields $\wh\theta \in \Theta$ such that, \arxiv{with high probability}, %
\arxiv{
\[\Dhels{\BP^{\pi_{\wh\theta}}}{\BP^{\pistar}}
\leq \widetilde{O}\left(\frac{d}{n}\right).\]
}
\end{proposition}

This algorithm can still be \arxiv{efficiently implemented} when $\pistar \not
\in \Pi$, but may suffer from the statistical issues in the
prequel a priori; even in this concrete setting, the computational-statistical tradeoffs are unclear.





\subsection{Contributions}

We illuminate
the computational-statistical tradeoffs inherent to autoregressive
sequence modeling and imitation learning under misspecification. While error
amplification can be avoided information-theoretically
($\Capx\ll{}H$), the regime $\Capx=\bigom(H)$ represents a fundamental barrier that no computationally efficient algorithm
can substantially surpass.
Our results apply to both next-token prediction and imitation learning, which we formally introduce and relate in \cref{sec:setting}. %
\loose


\vspace{0.3em}
\noindent\textbf{The statistical gold standard avoids error amplification (\cref{sec:optimal}).}
As a starting point that motivates our main
results, we show that the $\rho$-estimator of \cite{baraud2017new,baraud2018rho} can be applied in the general imitation
learning setting, which addresses question \textbf{(c)} above: information-theoretically,
\cref{eq:hels-goal-intro} is achievable with
$\Capx=O(1)$. %
Unfortunately,
the $\rho$-estimator is computationally impractical compared to traditional methods, as it requires min-max optimization.\loose




\vspace{0.3em}
\noindent\textbf{Robustifying the log-loss, and a barrier to further improvement (\cref{sec:next_token}).}
Toward practical algorithms that mitigate error amplification,
we explore whether better bounds on $\Capx$ can be achieved by
modifying the log-loss in imitation learning and next-token prediction (i.e., question \textbf{(a)} above).
First, we give sharp upper and lower bounds on the performance of
the log-loss, revealing that $\Capx$ depends not just on the horizon $H$, but also on (i) the failure
probability $\delta$, and (ii) a lower bound on the densities of $\pi\in\Pi$. 
We alleviate dependence on (i) via a new cross validation procedure, and dependence on (ii) by smoothing the objective, 
given access to per-timestep \emph{\densobs} from the expert model $\pistar$.
These results constitute a practical method that achieves $\Capx=\bigoht(H)$, and we uncover a fundamental barrier to further
improvement: \emph{any} next-token
prediction objective (cf. \cref{rem:next_token}), including those used in online imitation learning algorithms, 
must suffer $\Capx = \Omega(H)$.\loose







\vspace{0.3em}
\noindent\textbf{Computational-statistical tradeoffs at the next-token prediction barrier (\cref{sec:computational}).}
Can clever algorithm design circumvent the
$\Capx=\bigom(H)$ barrier, without sacrificing computational
efficiency (cf. question \textbf{(b)} above)? To make the question concrete, we focus on autoregressive
linear models, where our preceding improvements to next-token
prediction achieve $\Capx=\bigoht(H)$
in polynomial time. On the negative side, we show that achieving $\Capx =2^{\log^{1-\Omega(1)}(H)}$ is computationally hard under a standard
cryptographic assumption.
On the positive side, we show that it \emph{is}
possible to smoothly trade computation for statistical power, at least when $\abs{\cA}=2$: for
any constant $K$, there is a polynomial-time algorithm 
with $\Capx \leq \lceil H/K\rceil$; this is achieved through an
\emph{improper} relaxation to the $\rho$-estimator based on kernel
approximation \citep{shalev2011learning}.\loose




  \arxiv{
Taken together, we view our results as a promising first step toward a
computational theory of autoregressive sequence modeling and imitation
learning; we highlight open problems and future
directions in \cref{sec:conclusion}.\loose
}




\section{Problem Setting: Autoregression and Imitation Learning}\label{sec:setting}



As mentioned in the prequel, we present our main results in a general 
\emph{imitation learning} (IL) setting which encompasses autoregressive sequence modeling as a special case. %
This allows us to present
  our results---which we expect to find broader use in \IL{}---in the most general form possible.\loose

\vspace{0.3em}
\noindent\textbf{Basic notation.}
  For an integer $n\in\bbN$, we let $[n]$ denote the set
  $\{1,\dots,n\}$. For a set $\cX$, we let $\Delta(\cX)$ denote the
  set of all probability distributions over $\cX$. We adopt
    standard big-oh notation and write $f=\bigoht(g)$ to denote that
    $f = \bigoh(g\cdot{}\max\crl*{1,\mathrm{polylog}(g)})$ and
    $a\approxleq{}b$ as shorthand for $a=\bigoh(b)$. \loose
  


\vspace{0.3em}
\noindent\textbf{Markov decision processes.} We consider \IL{}{} 
in a (reward-free) Markov decision process (MDP) given by a tuple $M = (H, \MS,\MA,
(\BP_h)_{h \in \crl{0,\ldots,H-1}})$ where $\MS$ is
the\arxiv{ (potentially large)} \ahdelete{\emph}{state space}; $\MA$ is the
\ahdelete{\emph}{action space}; $\BP_0 \in \Delta(\MS)$ is the \ahdelete{\emph}{initial
  state distribution}; and for each $h \in [H-1]$, $\BP_h: \MS \times \MA
\to \Delta(\MS)$ is the \ahdelete{\emph}{transition distribution} at step $h$.
A (randomized) \ahdelete{\emph}{policy} $\pi$ is a
collection of mappings $\pi_h: \MS \to \Delta(\MA)$ for $h \in [H]$,
with $\pi_h(a_h \mid{} s_h)$ denoting the density of $\pi_h(s_h)$ at
$a_h$. Each policy $\pi$ in the MDP $M$ induces a distribution
$\BP^{\pi}$ over \ahdelete{\emph}{trajectories} $(s_1,a_1,\dots,s_H,a_H)$ defined
as follows. First, sample $s_1 \sim \BP_0$. Then, for each $1 \leq h <
H$, sample $a_h \sim \pi_h(\cdot\mid{}s_h)$ and $s_{h+1} \sim
\BP_h(s_h,a_h)$. For any \arxiv{real-valued }function $f$ on trajectories, we write
$\EE^{\pi}[f]$ to denote the expectation of $f(s_{1:H},a_{1:H})$
under $(s_{1:H},a_{1:H}) \sim \BP^{\pi}$.\loose

Our running example will be the \emph{autoregressive MDP}. For a context space $\MX$ (with context distribution $\mu\in\Delta(\MX)$), token space $\MA$, and $H \in \NN$, the $H$-step autoregressive MDP has state space $\MX\times\MA^\st$ and action space $\MA$, where $\MA^\st$ is the set of all finite-length strings formed by concatenation of elements of $\MA$. The initial distribution is $\mu$, and the transition dynamics are defined by deterministic concatenation: $s_{h+1} \gets (s_h,a_h) = ((x,a_{1:h-1}),a_h)$. The autoregressive MDP is \emph{accretive}, in the sense that $(s_1,a_1),\dots,(s_{h-1},a_{h-1})$ is a measurable function of $s_h$.

\vspace{0.3em}
\noindent\textbf{Imitation learning (IL).} In (offline) imitation learning \citep{pomerleau1988alvinn,ross2010efficient,foster2024behavior}, we are given a dataset $\cD=\crl*{o\ind{i}}_{i=1}^{n}$ of $n$
trajectories
$o\ind{i}=(s_1\ind{i},a_1\ind{i}, \ldots, s_H\ind{i},a_H\ind{i})$
sampled \iid by executing an \emph{expert policy}
$\pistar=\crl*{\pistar_h:\cS\to\Delta(\cA)}_{h=1}^{H}$ in the
underlying MDP. For an (unknown) reward function $r_h: \MS\times\MA
\to \RR$ measuring quality at some task of interest, the goal of
\IL{} is typically formulated as \emph{regret
  minimization}: Given a policy class $\Pi$,  we aim to learn a policy
$\pihat$ such that the \emph{regret} $J(\pistar;r) - J(\pihat;r)$ is
minimized; here $J(\pi;r) :=
\EE^{\pi}\brk[\big]{\sum_{h=1}^H r_h(s_h,a_h)}$ denotes the \emph{value}
of the policy $\pi$ in MDP $M$. %
\ahreplace{ 
We emphasize that the MDP $M$ itself (i.e., the transition
distribution) is not known to the learner in this framework.
}{Neither the MDP nor its transitions are known to the learner.
}






  
  



%
\subsection{Equivalence of Regret Minimization with Distribution Learning}\label{sec:regret-vs-learning}


A priori, regret minimization seems unrelated to the task of minimizing Hellinger distance. %
However, since the
rewards are never observed by the learner in the \IL{}{} protocol, it turns out that there is a close connection.
Concretely, suppose the rewards $r$ are normalized so that
$\sum_{h=1}^{H}r_h\in\brk*{0,R}$ for a parameter $R>0$. We refer to
such a reward function as \emph{$R$-bounded}, and for simplicity take $R=1$. Then as observed by \cite{foster2024behavior}, for any accretive MDP, it holds that %
\begin{align}
  \label{eq:tv_equivalence}
  \sup_{r: \text{$1$-bounded}}
  \crl*{J(\pistar;r) - J(\pihat;r)}
  = \DtvX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}},
\end{align}
where
$\Dtv{\bbP}{\bbQ}=\frac{1}{2}\int\abs{\mathrm{d}\bbP-\mathrm{d}\bbQ}$
is the\arxiv{ total variation }distance. Thus, \IL{}{} is a form of structured distribution learning where we aim
to learn the law of the trajectory induced by $\pistar$.


\vspace{0.3em}
\noindent\textbf{Hellinger vs total variation.} \cref{eq:tv_equivalence} suggests minimizing TV-distance. However, 
Hellinger distance is
equivalent up to a quadratic factor
($\Dtvs{\bbP}{\bbQ}\approxleq\Dhels{\bbP}{\bbQ}\approxleq\Dtv{\bbP}{\bbQ}$),
so the guarantee from \cref{eq:hels-goal-intro} does
approximately minimize TV-distance when $\vepsmis^2 \geq
\Omega(1)$. We focus on Hellinger distance because it leads to a tighter statistical theory---see \cref{sec:comparing} for additional motivation---but we do not see this as a critical conceptual distinction. The key point is that via \cref{eq:tv_equivalence}, any agnostic estimation error bound
as in \cref{eq:hels-goal-intro} leads to a bound on regret of order
$\vepsstat(n) + \Capx^{1/2}\vepsmis$.\footnote{As discussed in \cref{sec:comparing}, tighter variance-dependent bounds are also possible.} Notably, such a bound depends on
$\vepsstat(n)$ and $\vepsmis$ in a \emph{horizon-independent} fashion
whenever $\Capx=\bigoh(1)$, motivating our goal of mitigating error amplification. %


  















\vspace{0.3em}
\noindent\textbf{Autoregressive sequence modeling as \IL{}.} With the
perspective above, the autoregressive sequence modeling formulation in
\cref{sec:intro} is simply
\IL{}{} in the autoregressive MDP. %
Each model $\pi:\cX\to\Delta(\cA^H)$ in the model class is a policy $(\pi_h)_{h=1}^H$ in the policy class, where $\pi_h: \cX\times\cA^{h-1}\to\Delta(\cA)$ is the conditional distribution $\pi(a_h\mid{}x,a_{1:h-1})$. In the same way, the true model $\pistar$ is the expert policy. The learned policy $\pihat$ yields a model via autoregressive generation on any initial context. %
\loose
%
\subsection{Basic Algorithms: Next-Token Prediction and Behavior
  Cloning}
\label{sec:basic}

The next-token prediction objective in \cref{eq:bc}
specializes a canonical IL algorithm, \emph{behavior cloning with the
  logarithmic loss} ($\loglossbc$) to the autoregressive setting. For
general \IL{} with a policy class $\Pi$, $\loglossbc$ takes as input trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$, and outputs the policy\loose
%
  \begin{equation}
    \label{eq:bc_general}
    \pihat := \argmax_{\pi \in \Pi} \wh L(\pi) \quad\text{where}\quad
  \wh L(\pi) := \sum_{i=1}^n \sum_{h=1}^H \log 
  (\pihi).\end{equation}
 %
$\loglossbc$
  enjoys the guarantee in \cref{prop:mle_finite} for arbitrary MDPs \citep{foster2024behavior}.




