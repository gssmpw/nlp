%

Our emphasis on Hellinger distance
(versus total variation distance) is additionally motivated by recent results
of \citet{foster2024behavior}, which show that Hellinger distance
leads to tighter problem-dependent regret bounds that improve over \cref{eq:tv_equivalence}:\loose
\begin{proposition}[\cite{foster2024behavior}]\label{prop:hellinger-to-il}
For any $R$-bounded reward function $r$ and policies
  $\pistar$ and $\pihat$,
  \begin{align}
    \label{eq:bc_stochastic}
    J(\pistar;r)-J(\pihat;r)
    \approxleq{} \sqrt{\sigmastar^2\cdot\DhelsX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}}}
    + \bigoht\prn*{R}\cdot\DhelsX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}},
  \end{align}
  where
  $\sigmastar^2=\sum_{h=1}^{H}\En^{\pistar}\brk*{(\Vstar_h(x_h)-\Qstar_h(x_h,a_h))^2}\leq{}R^2$
  is the \emph{expert variance}.\footnote{We
    define $V_h^{\pi}(s)\coloneqq\En^{\pi}\brk[\big]{\sum_{h'=h}^{H}r_{h'}\mid{}x_h=s}$ and
    $Q_h^{\pi}(s,a)\coloneqq\En^{\pi}\brk[\big]{\sum_{h'=h}^{H}r_{h'}\mid{}s_h=s,
      a_h=a}$ as the\arxiv{ state- and state-action value functions for a
    policy} $\pi$.}
  Further, if $\pistar$ is deterministic, then
  for all $\pihat$, %
  \arxiv{\begin{align}
        J(\pistar;r)-J(\pihat;r)
    \leq
    4R\cdot{}\DhelsX{\big}{\bbP^{\pihat}}{\bbP^{\pistar}}.
  \end{align}}
  Finally, for any accretive MDP, i.e. with the property that
  $(x_1,a_1),\ldots,(x_{H-1},a_{H-1})\subset x_H$, there exists a
  reward function for which each inequality is tight up to logarithmic factors.
\end{proposition}

In particular, consider the extreme case where the expert is deterministic, and hence $\sigmastar = 0$. Then for any estimator $\pihat$ that satisfies 
\[\Dhels{\BP^{\pihat}}{\BP^{\pistar}} \leq \Capx \cdot \vepsmis^2 + \vepsstat^2(n),\]
where $\vepsmis^2 = \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}$ is the irreducible misspecification error, the regret of $\pihat$ can be bounded as
\begin{align}
J(\pistar;r)-J(\pihat;r) &\lesssim R \cdot (\Capx \vepsmis^2 + \vepsstat^2(n)).
\end{align}
Moreover, whenever the underlying MDP is accretive (as is the case for the autoregressive MDP) and $\Capx = O(1)$, this bound is asymptotically optimal up to logarithmic factors, since by \cref{prop:hellinger-to-il} it holds that
\[\min_{\pi\in\Pi} J(\pistar;r)-J(\pi;r) \geq \bigomt\left(R \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}\right) = \bigomt(R \cdot \vepsmis^2).\]
More generally, similar guarantees hold whenever the variance of the
expert policy is sufficiently small. As final motivation, we observe
that fast statistical rates are achievable in Hellinger distance (as
in \cref{prop:mle_finite}), but generically unachievable for
TV-distance \citep{han2015minimax}. We remark that measuring
  misspecification through information-theoretic divergences as we do
  here may be overly pessimistic if the reward function belongs to a
  class with known structure (e.g., linear rewards); understanding
  the role of misspecification in this setting is an interesting
  direction for future work.\loose
  


\begin{remark}[KL-Divergence]
  Another natural divergence to use for distribution estimation is
  KL-divergence; for example, one might aim to minimize
  $\Dkl{\bbP^{\pistar}}{\bbP^{\pihat}}$ and measure misspecification
  via $\min_{\pi\in\Pi}\Dkl{\bbP^{\pistar}}{\bbP^{\pi}}$. However,
  even in the well-specified case, it is not possible to perform
  distribution estimation in KL-divergence (for general classes $\Pi$)
  without making assumptions on boundedness of the densities under
  consideration (e.g., \citet{bilodeau2021minimax}), which is not required for Hellinger distance
  (\cref{prop:mle_finite}). Moreover, $\min_{\pi\in\Pi}\Dkl{\bbP^{\pistar}}{\bbP^{\pi}}$ can easily be infinite even when the Hellinger misspecification is arbitrarily small.
\end{remark}


