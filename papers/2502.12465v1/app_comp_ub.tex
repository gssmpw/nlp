
%

%

In this section we prove \cref{thm:chunk-kr-informal}, stated formally below as \cref{thm:chunk-kr-main}. We start by formally introducing the problem setting, expanding upon the discussion in \cref{sec:computational}.

\paragraph{Problem setting} Let $\MX$ be the context space and let $\MA = \{0,1\}$ be the action space. Let $d,H \in \NN$. Let $M$ be the $H$-step autoregressive MDP with context space $\MX$, action space $\MA$, and some initial context distribution $\MD \in \Delta(\MX)$. Let $\phi:\MX\times\MA^\st \to \RR^d$ be a $d$-dimensional feature mapping, and let $\Theta \subset \RR^d$ be a convex parameter set. Define the set of autoregressive linear policies as $\Pi := \{\pi_\theta:\theta\in\Theta\}$ where $\pi_\theta=  (\pi_{\theta,h})_{h=1}^H$ is as defined in \cref{eq:linear}. We assume that for any $(x,a_{1:h}) \in \MX\times\MA^h$ (where $0 \leq h \leq H$), we may query $\phi(x,a_{1:h})$ in time $\poly(d,H)$. Additionally, we assume that the features and parameter space satisfy the following assumption for a known parameter $L$.\footnote{We do not require a projection oracle for $\Theta$, since the algorithm will relax to a larger policy class depending only on $L$.}

\begin{assumption}[Norm bounds]\label{ass:linear-norm-bounds-ub}
  Let $L\geq 1$ be a parameter. It holds that $\norm{\phi(x,a_{1:h})}_2 \leq L$ for all $(x,a_{1:h}) \in \MX\times\MA^\st$ and $\norm{\theta}_2 \leq L$ for all $\theta\in\Theta$.
\end{assumption}

\input{alg_chunkkr}

We can now formally restate the desired result, which shows that the $\ChunkKR$ algorithm (\cref{alg:chunkkr}) achieves the approximation guarantee in \cref{thm:chunk-kr-informal}.

\begin{theorem}[Formal statement of \cref{thm:chunk-kr-informal}]\label{thm:chunk-kr-main}
There is a constant $C_{\ref{thm:chunk-kr-main}}>0$ such that the following guarantee for $\ChunkKR$ (\cref{alg:chunkkr}) holds. Let $\delta \in (0,1/2)$, $\epsilon \in (0,1)$, $K \in [H]$, $L \geq 1$, and $n \in \NN$. Suppose that \cref{ass:linear-norm-bounds-ub} holds with parameter $L$, and that 
\[n \geq (2^{L^2+K}H/\epsilon)^{C_{\ref{thm:kern-rho-main}}L^2K} \log(H/\delta).\] 
Let $(x\ind{i},a\ind{i})_{i=1}^n$ be i.i.d. samples from $\bbP^\st\ldef{}\bbP^{\pistar}\in \Delta(\MX\times\MA^H)$. Then with probability at least $1-\delta$, the output $\pihat\gets \ChunkKR((x\ind{i},a\ind{i})_{i=1}^n,K,L, \epsilon)$ satisfies
\[\Dhels{\BP^{\pihat}}{\BP^\star}
\lesssim \epsilon + \frac{H}{K} \min_{\pi\in\Pi} \Dhels{\BP^{\pi}}{\BP^\star}.\]
The time complexity of the algorithm is $\poly(n,H)$, and sampling from $\pihat$ can be done in time $\poly(n, H)$.
\end{theorem}

Henceforth we suppose that \cref{ass:linear-norm-bounds-ub} holds with parameter $L$; we omit restating this assumption in subsequent theorem and lemma statements.

\paragraph{Organization of this appendix} In \cref{subsec:comp-ub-overview}, we outline the proof of \cref{thm:chunk-kr-main}. In this proof, the main ingredient is \cref{thm:kern-rho-main}, an analysis of the subroutine $\KernRho$ (\cref{alg:kernrho}). In \cref{subsec:kernel-apx,subsec:kernrho-statistical,subsec:kernrho-computational} we assemble the key lemmas for the proof of \cref{thm:kern-rho-main}, and in \cref{subsec:kernrho-proof} we complete the proof of \cref{thm:kern-rho-main}. Finally, in \cref{subsec:chunkkr-proof} we complete the proof of \cref{thm:chunk-kr-main} and hence \cref{thm:chunk-kr-informal}.

\subsection{Algorithm and Proof Overview}\label{subsec:comp-ub-overview}

In this section we provide an overview of the algorithm $\ChunkKR$ (\cref{alg:chunkkr}) and outline the proof of \cref{thm:chunk-kr-main}. As discussed in \cref{sec:comp-lb}, the key subroutine of $\ChunkKR$ is an algorithm $\KernRho$ (\cref{alg:kernrho}), which learns misspecified autoregressive linear models with optimal approximation ratio, and with time complexity scaling polynomially in $d$ but exponentially in the horizon $H$.

As shown in \cref{alg:chunkkr}, the full algorithm $\ChunkKR$ divides the horizon into $H/K$ chunks of length $K$. For each chunk $\{h+1-K,\dots,h\}$, it applies $\KernRho$ to learn the distribution of $a_{h+1-K:h}$ under $\BP^\st$, conditioned on the initial context $x$ and the first $h-K$ actions $a_{1:h-K}$. In particular, the entire tuple $(x, a_{1:h-K})$ is interpreted as a ``context'' in a new autoregressive MDP with horizon $K$. We show that if $\BP^\st$ is close to some autoregressive linear model, then this new distribution is close to an autoregressive linear model in the new MDP; moreover, if we learn each chunk $\{h+1-K,\dots,h\}$ up to squared Hellinger distance $\epsilon_h$, then we learn the overall model up to squared Hellinger distance $\sum_{i=0}^{H/K-1} \epsilon_{(i+1)K}$. It follows that if $\KernRho$ has optimal approximation ratio, then $\ChunkKR$ has approximation ratio $O(H/K)$. We defer the formal analysis of $\ChunkKR$ to \cref{subsec:chunkkr-proof}; the interim is devoted to the analysis of $\KernRho$. We now outline the proof of the following guarantee.

\input{alg_kernrho}

\begin{theorem}[Main guarantee for $\KernRho$]\label{thm:kern-rho-main}
There is a constant $C_{\ref{thm:kern-rho-main}}>0$ so that the following guarantee for $\KernRho$ (\cref{alg:kernrho}) holds. Let $\delta \in (0,1/2)$ and $\epsilon \in (0,1)$. Suppose that $n \geq (2^{L^2+H}/\epsilon)^{C_{\ref{thm:kern-rho-main}}L^2H} \log(1/\delta)$. Fix an arbitrary policy $\pistar$, and let $(x\ind{i},a\ind{i})_{i=1}^n$ be i.i.d. samples from $\BP^{\star}\ldef\BP^{\pistar}$. Then with probability at least $1-\delta$, the output $\pihat\gets \KernRho((x\ind{i},a\ind{i})_{i=1}^n, B,\gamma,\epapx,\epopt)$, with $B := (L^2H2^{3H+2}e^{2L^2 H+1}/\epsilon)^{C_{\ref{thm:kernel-apx}}L^2H}$, $\gamma := e^{-2L^2 H-1}2^{-H}$, $\epapx := e^{-3L^2 H-2}2^{-3H/2}\epsilon$, and $\epopt := \epsilon$, satisfies
\[\Dhels{\BP^{\pihat}}{\BP^\star}
\leq \frac{88}{3}\min_{\pi\in\Pi} \Dhels{\BP^{\pi}}{\BP^\star} + O\left(\epsilon\right).\]
The time complexity of the algorithm is $\poly(n)$, and sampling from $\pihat$ can be done in time $\poly(n)$.
\end{theorem}

See \cref{alg:kernrho} for pseudocode for $\KernRho$. The main idea is to implement an improper relaxation of the $\rho$-estimator $\rhobc$ from \cref{sec:optimal}. This is motivated by the fact that the min-max objective solved by $\rhobc$ is convex-concave in \emph{policy} space, yet even for autoregressive linear models, the objective is not convex-concave in \emph{parameter} space\footnote{Nor is the set of autoregressive linear models convex in policy space.}---at least, not with the natural parametrization $\theta \mapsto \pi_\theta$. However, any autoregressive linear model $\pi_\theta$ \emph{can} be approximated by a function in an infinite-dimensional reproducing kernel Hilbert space (RKHS) with efficiently computable kernel $K$. This motivates our basic approach: relax the program to the RKHS, and use the ``kernel trick'' to reduce back to a finite-dimensional program. This approach was pioneered by \cite{shalev2011learning} for agnostic learning of halfspaces; to compare, our relaxation requires additional care to ensure that the statistical properties of $\rhobc$ are preserved, and the resulting program is a convex-concave min-max program rather than a convex minimization program, since it is based on $\rhobc$ rather than Empirical Risk Minimization.

\paragraph{Kernel approximation (\cref{subsec:kernel-apx})}
We now describe the kernel function, the approximation result, and how it suggests a relaxation of $\rhobc$. We begin by defining a convenient reparameterization of the feature map $\phi$.
\begin{definition}[Joint feature map]\label{def:uvec}
For each $x \in \MX$ and $a_{1:H} \in \MA^H$, define $\uf(x,a_{1:H}) \in (\RR^d)^H$ by
\[\uf(x,a_{1:H})_h := \frac{1}{2L}\left(\phi(x,a_{1:h-1},1-a_h) - \phi(x,a_{1:h-1},a_h)\right)\]
for each $h \in [H]$. %
\end{definition}

Let $\ball$ denote the Euclidean unit ball in $\RR^d$. Note that $\uf(x,a_{1:H})_h \in \ball$ for each $h$, as a consequence of \cref{ass:linear-norm-bounds-ub}.

\begin{definition}[Kernel function] \label{def:kernel} Define $K: (\ball)^H \times (\ball)^H \to \RR$ by  
\[K(u_{1:H},u'_{1:H}) := \prod_{h=1}^H \frac{1}{1 - \frac{1}{2} \langle u_h, u'_h\rangle}.\]
\end{definition}

This kernel function coincides with that of \cite{shalev2011learning} when we set $H = 1$. In \cref{subsec:kernel-apx}, we describe an infinite-dimensional mapping $\psi: (\ball)^H \to \RR^\NN$ (\cref{def:psi}) with the following properties. First, $\psi$ induces the kernel $K$ (i.e., satisfies $\langle \psi(\cdot),\psi(\cdot)\rangle = K(\cdot,\cdot)$). Second, any autoregressive linear policy has sequence-level density $\pi_\theta(a_{1:H}\mid{} x)$ approximated by a bounded linear function of $\psi(\uf(x,a_{1:H}))$:

\begin{theorem}[Kernel approximation of autoregressive linear policies]\label{thm:kernel-apx}
There is a constant $C_{\ref{thm:kernel-apx}}>0$ so that the following holds. Let $\theta \in \Theta$ and $\epsilon>0$. There is some $v_\theta \in \RR^\NN$ such that $\norm{v_\theta}_2^2 \leq (L^2H2^H/\epsilon)^{C_{\ref{thm:kernel-apx}}L^2H}$ and, for all $x \in \MX$,
\[\sum_{a_{1:H} \in \MA^H} \left|\pi_\theta(a_{1:H}\mid{}x) - \langle v_\theta, \psi(\uf(x,a_{1:H}))\rangle\right| \leq \epsilon.\]
\end{theorem}

Notice that the norm bound in \cref{thm:kernel-apx} scales exponentially with the horizon $H$ and the parameter $L$ from \cref{ass:linear-norm-bounds-ub}, but not the dimension $d$ of the original features. This is crucial since the norm bound will be directly reflected in the sample complexity of $\KernRho$ (via Rademacher bounds for generalization), and hence in the time complexity.

\paragraph{Relaxing $\rhobc$ to the RKHS} Recall from \cref{sec:optimal} that $\rhobc$ is defined in terms of $\tau: (0,\infty) \to \RR$ defined as $\tau(z) = \frac{\sqrt{1/z} - 1}{\sqrt{1/z} + 1}.$
\cref{thm:kernel-apx} suggests relaxing $\rhobc$ to the following program:
\begin{equation} \wh v \gets \argmin_{v \in \RR^\NN:\norm{v}_2^2\leq B} \max_{w \in \RR^\NN:\norm{w}_2^2\leq B} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u_{1:H}\ind{i})\rangle}{\langle w, \psi(u_{1:H}\ind{i})\rangle}\right)\label{eq:kernel-rho-first-attempt}\end{equation}
where $B := (L^2H2^H/\epsilon)^{C_{\ref{thm:kernel-apx}}L^2H}$, and $u_{1:H}\ind{i} = \uf(x\ind{i},a_{1:H}\ind{i})$. Since $(x,y) \mapsto \tau(x/y)$ is convex-concave (\cref{lemma:tau-convex-concave}), this program is convex-concave, albeit infinite-dimensional. Unfortunately, analyzing the program as written leads to statistical issues: in the standard analysis of $\rhobc$, the key properties relating the population-level loss $\EE_{x\sim p^\st}[\tau(p(x)/q(x))]$ to the Hellinger distances $\Dhels{p}{p^\st}$ and $\Dhels{q}{p^\st}$ (\cref{lemma:rho-estimator-bounds}) crucially use that $p,q$ are distributions. In \cref{eq:kernel-rho-first-attempt}, not all $v,w \in \RR^\NN$ correspond to distributions, so it unclear whether the corresponding losses relate to any useful error metric. Even worse, $\tau$ is only well-defined on $(0,\infty)$, but the argument of $\tau$ in \cref{eq:kernel-rho-first-attempt} could be negative. Finally, even if the argument were always non-negative, $\tau$ is non-Lipschitz near $0$, which poses issues for generalization arguments based on Rademacher complexity. We fix all of these issues by adding additional constraints to ensure that $v$ and $w$ approximately correspond to conditional distributions with densities bounded above zero, at least when conditioning on the observed contexts:
\begin{equation} \wh v \gets \argmin_{v \in \wh V} \sup_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u_{1:H}\ind{i})\rangle}{\langle w, \psi(u_{1:H}\ind{i})\rangle}\right)
\label{eq:kern-rho-second-attempt}
\end{equation}
where
\begin{align} 
\wh V := \wh W := \Big\{v \in \RR^\NN: \left(\norm{v}_2^2 \leq B\right) &\land \left(\forall i \in [n], a \in \MA^H: \langle v, \psi(\uf(x\ind{i},a_{1:H}))\rangle \geq \gamma\right) \\ 
&\land \left(\forall i \in [n]: 1-\epapx \leq \langle v, s\ind{i}\rangle \leq 1+\epapx\right)\Big\}.
\end{align}
where $s\ind{i} = \sum_{a \in \MA^H} \psi(\uf(x\ind{i},a_{1:H}))$, and $\gamma,\epapx>0$ are parameters defined in \cref{thm:kern-rho-main}. Proving \cref{thm:kern-rho-main} now requires (1) showing that the program \cref{eq:kern-rho-second-attempt} is statistically efficient, and (2) it can be reduced to a finite-dimensional program and efficiently solved.

\paragraph{Statistical analysis (\cref{subsec:kernrho-statistical})} The main results of \cref{subsec:kernrho-statistical} are (1) \cref{lemma:minmax-ub}, which shows that the min-max value of \cref{eq:kern-rho-second-attempt} can be bounded by the best-in-class Hellinger distance of $\pistar$ with respect to $\Pi$, and (2) \cref{lemma:hellinger-by-loss}, which shows that for any potential solution $v$ to the program \cref{eq:kern-rho-second-attempt}, if we convert it to a conditional distribution $\pibar^v$, the Hellinger distance from $\pibar^v$ to $\pistar$ can be bounded in terms of the best-in-class Hellinger distance and the min-max loss. Together, \cref{lemma:minmax-ub,lemma:hellinger-by-loss} imply that if we can approximately solve \cref{eq:kern-rho-second-attempt} (and compute the corresponding policy $\pibar^{\wh v}$), then we achieve the statistical guarantee required for \cref{thm:kern-rho-main}.

To prove these lemmas, we use the constraints on $\wh V = \wh W$ to show that with high probability any $v \in \wh V$ approximately corresponds to some real conditional distribution $\pibar^v$ for most contexts (\cref{lemma:tv-to-simplex}). We then 
 use standard Rademacher bounds, applied to an everywhere-Lipschitz mollification of the loss in \cref{eq:kern-rho-second-attempt}, to show that the empirical loss concentrates for all $v,w \in \wh V=\wh W$. Finally, we use \cref{thm:kernel-apx} together with the choice of $B$ to show that there is some $v \in \wh V$ for which $\pibar^v$ has near-optimal Hellinger distance to $\pistar$ (\cref{lemma:best-to-pibar}). With these tools, the desired statistical guarantees then follow from \cref{lemma:rho-estimator-bounds}.

\paragraph{Computational analysis (\cref{subsec:kernrho-computational})} While the program defined in \cref{eq:kern-rho-second-attempt} is a convex-concave min-max program with convex constraint sets, but it is infinite-dimensional. To reduce to finite dimensions, we essentially use a generalization of the Representer Theorem to min-max losses. In particular, it suffices to optimize over $v \in \wh V$ and $w \in \wh W$ that are linear combinations of the vectors $\{\psi(\uf(x\ind{i},a_{1:H})): i \in [n], a_{1:H}\in \MA^H\}$. For such vectors, the loss in \cref{eq:kern-rho-second-attempt} and the linear constraints can be written explicitly in terms of the kernel function and the coefficients of the linear combination: for any $v := \sum_{i,a} \alpha_{i,a_{1:H}} \psi(\uf(x\ind{i},a_{1:H}))$ and any $(x',a'_{1:H}) \in \MX\times\MA^H$, we can write
\[\langle v, \psi(\uf(x,a_{1:H}))\rangle = \sum_{i,a_{1:H}} \alpha_{i,a_{1:H}} K(\uf(x\ind{i},a_{1:H}),\uf(x',a'_{1:H})).\]
Additionally, since $K$ is positive-semidefinite, the Euclidean norm constraint translates to an ellipsoid constraint. Ultimately, we get the following program:
\begin{equation}
\wh\alpha \gets \argmin_{\alpha \in \wh J} \max_{\beta \in \wh K} \emploss(\alpha,\beta)
\label{eq:kern-rho-finite}
\end{equation}
where
\[\emploss(\alpha,\beta) := \frac{1}{n}\sum_{i=1}^n \tau \left( \frac{\sum_{j=1}^n \sum_{a \in \MA^H} \alpha_{j,a} K(\uop{x\ind{j}}{a}, \uop{x\ind{i}}{a\ind{i}})}{\sum_{j=1}^n \sum_{a \in \MA^H} \beta_{j,a} K(\uop{x\ind{j}}{a}, \uop{x\ind{i}}{a\ind{i}})}\right)\]
and
\begin{align}
\wh J := \wh K := \Big\{ \alpha \in \RR^{n2^H}: &\left(\sum_{j,j'=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a}\alpha_{j',a'}K(\uop{x\ind{j}}{a},\uop{x\ind{j'}}{a'}) \leq B \right) \\
&\land \left(\forall i \in [n],a\in\MA^H: \sum_{j=1}^n \sum_{a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) \geq \gamma\right) \\ 
&\land \left(\forall i \in [n]: 1-\epapx \leq \sum_{j=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) \leq 1+\epapx\right)\Big\}.
\end{align}
Now, \cref{eq:kern-rho-finite} is a convex-concave min-max program with convex constraints, and the constraint sets lie in $n2^H$ dimensions. We would like to solve it using projected gradient descent-ascent---see e.g. \cite[Theorem 5.1]{bubeck2015convex}. There is one remaining technical detail: there is no evident Euclidean norm bound on the constraint sets $\wh J, \wh K$, since the kernel matrix $\Sigma \in \RR^{n2^H \times n2^H}$ that is implicit in the ellipsoid constraint (see \cref{eq:sigma-def} for the explicit definition) could be arbitrarily ill-conditioned. To fix this, we apply a change-of-basis by $\Sigma^{-1/2}$ and observe that the loss function is still Lipschitz in the new basis. This results in the program solved by $\KernRho$ in \cref{alg:kernrho}.\footnote{We omit the norm bound in the definition of the constraint set (\cref{eq:y-def}), since projected gradient descent-ascent provides implicit regularization. However, adding in the norm bound would somewhat improve the rate, at the cost of a more complex projection oracle.}




\subsection{Kernel Approximation of Autoregressive Policies}\label{subsec:kernel-apx}

In this section we define the mapping $\psi: (\ball)^H \to \RR^\NN$ that induces the kernel function $K$ (\cref{def:kernel}), and prove \cref{thm:kernel-apx}. This material is a straightforward generalization of analogous results in \cite{shalev2011learning} to our autoregressive linear setting.



\begin{definition}\label{def:psi}
Identify $\NN$ with $I^H$ where $I := \emptyset\sqcup[d]\sqcup[d]^2 \sqcup\dots$. For any tuple $\bfi \in I$, write $|\bfi|$ to denote the length of $\bfi$ (for example, if $\bfi\in [d]^{j}$, then $\abs{\bfi}=j$). 

Define mapping $\psi: (\ball)^H \to \RR^\NN$ so that for any $u_{1:H} \in (\ball)^H$, the value of $\psi(u_{1:H})$ at index $\bfi_{1:H} \in I^H$ is 
\[\psi(u_{1:H})_{\bfi_{1:H}} := \prod_{h=1}^H 2^{-|\bfi_h|/2} \prod_{k=1}^{|\bfi_h|} u_{h,\bfi_{h,k}}.\]
\end{definition}

The following lemma shows that $\psi$ induces the kernel function $K$ (and as a byproduct, that $K$ is positive semi-definite).

\begin{lemma}\label{lemma:psi-k}
For any $u_{1:H},u'_{1:H} \in (\ball)^H$, we have
\[K(u_{1:H},u'_{1:H}) = \langle \psi(u_{1:H}), \psi(u'_{1:H})\rangle.\]
\end{lemma}

\begin{proof}[\pfref{lemma:psi-k}]
We have
\begin{align}
\langle \psi(u_{1:H}),\psi(u'_{1:H})\rangle 
&= \sum_{\bfi_{1:H} \in I^H} \prod_{h=1}^H 2^{-|\bfi_h|} \prod_{k=1}^{|\bfi_h|} u_{h,\bfi_{h,k}}u'_{h,\bfi_{h,k}} \\ 
&= \prod_{h=1}^H \sum_{\bfi_h \in I}
2^{-|\bfi_h|} \prod_{k=1}^{|\bfi_h|} u_{h,\bfi_{h,k}}u'_{h,\bfi_{h,k}} \\
&= \prod_{h=1}^H \sum_{j=0}^\infty 2^{-j} \sum_{\bfi_h \in [d]^j} \prod_{k=1}^j u_{h,\bfi_{h,k}}u'_{h,\bfi_{h,k}} \\ 
&= \prod_{h=1}^H \sum_{j=0}^\infty 2^{-j} \langle u_h,u'_h\rangle^j \\ 
&= K(u_{1:H},u'_{1:H})
\end{align}
as claimed.
\end{proof}

To prove \cref{thm:kernel-apx}, we first show that for any $\theta \in \Theta$, the density $\pi_\theta(a_{1:H}\mid{}x)$ can be approximated by a product of Taylor series in the variables $\langle \uf(x,a_{1:H})_h, \theta\rangle$ (\cref{lemma:policy-poly-apx}), where the coefficients of the Taylor series satisfy a certain decay condition dependent on the norm bound $L$ from \cref{ass:linear-norm-bounds-ub}. We then show that any such product of Taylor series is a bounded linear function of $\psi(\uop{x}{a_{1:H}})$ (\cref{lemma:poly-to-rkhs}).

\begin{lemma}\label{lemma:policy-poly-apx}
Let $\epsilon \in (0,1)$ and suppose $L \geq 2$. There is a Taylor series $p(t) = \sum_{j=0}^\infty \beta_j t^j$ with $\sum_{j=0}^\infty \beta_j^2 2^j \leq (L^2H/\epsilon)^{O(L^2)}$, such that for all $(x,a_{1:H}) \in \MX\times\MA^H$ and $\theta \in \Theta$, it holds that
\[\left|\pi_\theta(a_{1:H}\mid{}x) - \prod_{h=1}^H p(\langle \uop{x}{a_{1:H}}_h,\theta/L\rangle)\right| \leq \epsilon.\]
\end{lemma}

\begin{proof}[\pfref{lemma:policy-poly-apx}]
Recall that $\uop{x}{a_{1:H}}_h = \frac{1}{2L}\left(\phi(x,a_{1:h-1},1-a_h) - \phi(x,a_{1:h})\right)$ (\cref{def:uvec}), so that
\begin{align}
\pi_\theta(a_{1:H}\mid{}x)
&= \prod_{h=1}^H \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) \\ 
&= \prod_{h=1}^H \frac{\exp(\langle \phi(x,a_{1:h}),\theta\rangle)}{\exp(\langle \phi(x,a_{1:h-1},1-a_h),\theta\rangle) + \exp(\langle \phi(x,a_{1:h}),\theta\rangle)} \\
&= \prod_{h=1}^H \sigma\left(\left\langle \uop{x}{a_{1:H}}_h, \theta/L\right\rangle\right),
\end{align}
where $\sigma: \RR \to (0,\infty)$ is defined by $\sigma(z) = 1/(1 + e^{2L^2 z})$. Notice that the argument of $\sigma$ above lies in $[-1,1]$, by \cref{ass:linear-norm-bounds-ub}.

By \cite[Lemma 2.5]{shalev2011learning}, there is a Taylor series $p$ satisfying the stated coefficient bound, with 
\[|\sigma(z) - p(z)| \leq \epsilon/(2H)\]
for all $z \in [-1,1]$. Since $\sigma(z) \in (0,1)$ for all $z \in \RR$, it follows that
\begin{align} 
&\left|\prod_{h=1}^H \sigma(\langle \uop{x}{a_{1:H}}_h, \theta'\rangle) - \prod_{h=1}^H p(\langle \uop{x}{a_{1:H}}_h, \theta'\rangle)\right| \\
&\leq 
\sum_{h=1}^H \left|\prod_{k=1}^{h-1} \sigma(\langle \uop{x}{a_{1:H}}_k, \theta'\rangle)\right| |\sigma(\langle \uop{x}{a_{1:H}}_h, \theta'\rangle) - p(\langle \uop{x}{a_{1:H}}_h, \theta'\rangle)| \left|\prod_{k=h+1}^H p(\langle \uop{x}{a_{1:H}}_k, \theta'\rangle)\right| \\ 
&\leq \sum_{h=1}^H \frac{\epsilon}{2H} (1 + \epsilon/(2H))^H \\ 
&\leq \epsilon,
\end{align}
where we have written $\theta' := \theta/L$.
\end{proof}

\begin{lemma}\label{lemma:poly-to-rkhs}
Let $B>0$ and let $p: \RR \to \RR$ be a Taylor series $p(t) = \sum_{j=0}^\infty \beta_j t^j$ with $\sum_{j=0}^\infty \beta_j^2 2^j \leq B$. For any $\theta \in \Theta$ there is some $v_\theta \in \RR^\NN$ such that for all $u_{1:H} \in (\ball)^H$,
\[\langle v_\theta,\psi(u_{1:H})\rangle = \prod_{h=1}^H p(\langle u_h, \theta/L\rangle).\]
Moreover, $\norm{v_\theta}_2^2 \leq B^H.$
\end{lemma}

\begin{proof}[\pfref{lemma:poly-to-rkhs}]Write $\theta' := \theta/L \in \ball$. Recall that we identified $\NN$ with $I^H$ where $I = \emptyset\sqcup[d]\sqcup[d]^2 \sqcup\dots$, and that for any $\bfi\in I$ we write $|\bfi|$ to denote the length of $\bfi$. Define $v_\theta$ at index $\bfi_{1:H} \in I^H$ to have value 
\[(v_\theta)_{\bfi_{1:H}} := \prod_{h=1}^H 2^{|\bfi_h|/2} \beta_{|\bfi_h|} \prod_{k=1}^{|\bfi_h|} \theta'_{ \bfi_{h,k}}.\]
Then for any $u_{1:H} \in (\ball)^H$,
\begin{align}
\langle v_\theta,\psi(u_{1:H})\rangle
&= \sum_{\bfi_{1:H}\in I^H} \prod_{h=1}^H \beta_{|\bfi_h|} \prod_{k=1}^{|\bfi_h|} \theta'_{\bfi_{h,k}} u_{h,\bfi_{h,k}} \\ 
&= \prod_{h=1}^H \sum_{j=0}^\infty \beta_j \sum_{\bfi_h \in [d]^j} \prod_{k=1}^j \theta'_{\bfi_{h,k}} u_{h,\bfi_{h,k}} \\ 
&= \prod_{h=1}^H \sum_{j=0}^\infty \beta_j \langle \theta',u_h\rangle^j \\ 
&= \prod_{h=1}^H p(\langle \theta',u_h\rangle).
\end{align}
Similarly,
\begin{align}
\norm{v_\theta}_2^2
&= \sum_{\bfi_{1:H}\in I^H} \prod_{h=1}^H 2^{|\bfi_h|} \beta_{|\bfi_h|}^2 \prod_{k=1}^{|\bfi_h|} (\theta'_{\bfi_{h,k}})^2 \\ 
&= \prod_{h=1}^H \sum_{j=0}^\infty 2^j \beta_j^2 \norm{\theta'}_2^{2j} \\ 
&\leq B^H
\end{align}
where the final inequality uses the fact that $\norm{\theta'}_2 \leq 1$.
\end{proof}

The proof of \cref{thm:kernel-apx} is now straightforward from the above lemmas.

\begin{proof}[Proof of \cref{thm:kernel-apx}]
Let $\epsilon > 0$. By \cref{lemma:policy-poly-apx}, there is a Taylor series $p(t) = \sum_{j=0}^\infty \beta_j t^j$ with $\sum_{j=0}^\infty \beta_j^{2j} 2^j \leq (L^2 H2^H/\epsilon)^{O(L^2)}$, such that for all $a_{1:H} \in \MA^H$ and $\theta \in \Theta$, it holds that
\[\left|\pi_\theta(a_{1:H}\mid{}x) - \prod_{h=1}^H p(\langle \uop{x}{a_{1:H}}_h,\theta/L\rangle)\right| \leq \epsilon/2^H.\]
By \cref{lemma:poly-to-rkhs} applied to $p$, for every $\theta\in\Theta$ there is some $v_\theta\in\RR^\NN$ such that $\norm{v_\theta}_2^2 \leq (L^2 H/\epsilon)^{O(L^2 H)}$ and, for all $u_{1:H} \in (\ball)^H$,
\[\langle v_\theta,\psi(u_{1:H})\rangle = \prod_{h=1}^H p(\langle u_h, \theta/L\rangle).\]
It follows that for any $\theta \in \Theta$ and $(x,a_{1:H}) \in \MX\times\MA^H$, since $\uop{x}{a_{1:H}} \in (\ball)^H$ (by \cref{ass:linear-norm-bounds-ub}),
\[\left|\pi_\theta(a_{1:H}\mid{}x) - \langle v_\theta, \psi(\uop{x}{a_{1:H}})\rangle)\right| \leq \epsilon/2^H.\]
The result now follows from summing over $a_{1:H} \in \MA^H$.
\end{proof}

\subsection{Statistical Analysis for $\KernRho$}\label{subsec:kernrho-statistical}

In this section we prove \cref{lemma:minmax-ub,lemma:hellinger-by-loss}, which together show that if we can approximately solve \cref{eq:kern-rho-second-attempt}, then we achieve the statistical guarantee required for \cref{thm:kern-rho-main}. For purposes of the analysis (particularly since $\KernRho$ effectively solves a relaxation of \cref{eq:kern-rho-second-attempt} and its solution may not lie in $\wh V$), it is convenient to define analogues of $\wh V$ (the infinite-dimensional constraint set) and $\wh J$ (the finite-dimensional constraint set before the change-of-basis) with different parameter choices:
\begin{align} 
\wh V(B',\gamma',\epapx') := \Big\{v \in \RR^\NN: \left(\norm{v}_2^2 \leq B'\right) &\land \left(\forall i \in [n], a \in \MA^H: \langle v, \psi(\uop{x\ind{i}}{a})\rangle \geq \gamma'\right) \\ 
&\land \left(\forall i \in [n]: 1-\epapx' \leq \langle v, s\ind{i}\rangle \leq 1+\epapx'\right)\Big\}
\end{align}
where as before, $s\ind{i} = \sum_{a\in\MA^H} \psi(\uop{x\ind{i}}{a})$, so that $\wh V(B,\gamma,\epapx) = \wh V$, and
\begin{align}
\wh J(B',\gamma',\epapx') := \Big\{ \alpha \in \RR^{n2^H}: &\left(\sum_{j,j'=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a}\alpha_{j',a'}K(\uop{x\ind{j}}{a},\uop{x\ind{j'}}{a'}) \leq B' \right) \\
&\land \left(\forall i \in [n],a\in\MA^H: \sum_{j=1}^n \sum_{a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) \geq \gamma'\right) \\ 
&\land \left(\forall i \in [n]: 1-\epapx' \leq \sum_{j=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) \leq 1+\epapx'\right)\Big\},
\end{align}
so that $\wh J(B,\gamma,\epapx) = \wh J$.

\paragraph{Rounding to a true conditional distribution} For any $v \in \wh V$, we may consider the function
\begin{equation} f^v(a_{1:H}\mid{}x) := \langle v, \psi(\uop{x}{a_{1:H}})\rangle.\label{eq:fv}\end{equation}
Under the constraints of $\wh V$, it holds for each observed context $x\ind{i}$ that $f^v(\cdot\mid{}x\ind{i})$ is $\epapx$-close to a valid distribution, and in particular to some distribution with densities lower bounded by $\gamma$. However, it may not be close for all $x\in\MX$. Moreover, $\KernRho$ ultimately needs to output (a sampler for) a valid conditional distribution (policy). Below, we define $\pibar^v$ as the closest conditional distribution to $f^v$ with all densities lower bounded by $\gamma$. 
It is most convenient to work with this object throughout the analysis. 

\begin{definition}\label{def:pibarv}
For each $v \in \RR^\NN$, define 
\[\pibar^v \in \argmin_{\pi: \MX \to \Delta_\gamma(\MA^H)} \EE_x\brk*{ \sum_{a \in \MA^H} \left| \langle v, \psi(u(x,a_{1:h-1})_{h=1}^H)\rangle - \pi(a_{1:H}\mid{}x)\right|}\]
where $\Delta_\gamma(\MA^H)$ denotes the set of distributions $p \in \Delta(\MA^H)$ such that $p(a) \geq \gamma$ for all $a \in \MA^H$.
\end{definition}

\begin{remark}
As we will see later, constructing $\pibar^v$ (for appropriately represented $v$) is not as computationally intractable as it looks; essentially, $\pibar^v$ can be computed on a context-by-context basis.
\end{remark}

\subsubsection{Feasibility}

The following lemma shows that for the optimal choice of $v \in \wh V$, the Hellinger distance of $\pibar^v$ from $\pistar$ is not much larger than the best-in-class Hellinger distance. The proof uses \cref{thm:kernel-apx}, the definition of the constraint set $\wh V$ from \cref{eq:kern-rho-second-attempt}, and the fact that any policy $\pi \in \Pi$ has conditional densities bounded away from $0$.\loose

\begin{lemma}\label{lemma:best-to-pibar}
Suppose that $B \geq (L^2H 2^{2H+2}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H}$ and $\gamma \leq (e^{-2L^2 H} - \epapx)2^{-H-1}$. Then
\[\min_{v \in \wh V} \Dhels{\BP^{\pistar}}{\BP^{\pibar^v}} \leq 2\min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + 4\epapx.\]
Moreover, the set $\wh V(B/2, 2\gamma, \epapx/2)$ is non-empty.
\end{lemma}

\begin{proof}[\pfref{lemma:best-to-pibar}]
Pick $\theta^\star \in \argmin_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}}$. By \cref{thm:kernel-apx}, there is some $v^\star \in \RR^\NN$ such that $\norm{v^\star}_2^2 \leq (L^2H2^{2H+1}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H}$ and, for all $x \in \MX$ and $a \in \MA^H$,
\begin{equation} \left|\pi_{\theta^\star}(a_{1:H}\mid{}x) - \langle v^\star, \psi(\uop{x}{a_{1:H}})\rangle\right| \leq \epapx 2^{-H-1}.\label{eq:pithetastar-to-v}
\end{equation}
By the lemma assumption, $(L^2H2^{2H+1}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H} \leq B/2$. By \cref{lemma:softmax-density-lb}, for all $x \in \MX$ and $a \in \MA^H$, we have $\pi_{\theta^\star}(a_{1:H} \mid{} x) \geq e^{-2L^2 H} 2^{-H}$, so $\langle v^\star, \psi(\uop{x}{a_{1:H}})\rangle \geq (e^{-2L^2 H} - \epapx) 2^{-H} \geq 2\gamma$ by the lemma assumption. Moreover for each $x \in \MX$, since $\pi_{\theta^\star}(\cdot\mid{}x)$ is a distribution, \cref{eq:pithetastar-to-v} implies that
\[1-\frac{\epapx}{2} \leq \sum_{a_{1:H} \in \MA^H} \langle v^\star, \psi(\uop{x}{a_{1:H}})\rangle \leq 1 + \frac{\epapx}{2}.\]
Thus, $v^\star \in \wh V(B/2,2\gamma,\epapx/2) \subseteq \wh V$, and $\pi_{\theta^\star}(\cdot\mid{}x) \in \Delta_\gamma(\MA^H)$ (\cref{def:pibarv}) for all $x \in \MX$. The latter means that by definition of $\pibar^{v^\star}$,
\begin{align} 
&\EE_x \sum_{a_{1:H} \in \MA^H} \left| \langle v^\star, \psi(\uop{x}{a_{1:H}})\rangle - \pibar^{v^\star}(a_{1:H}\mid{}x)\right|\\ &\leq \EE_x \sum_{a_{1:H} \in \MA^H} \left| \langle v^\star, \psi(\uop{x}{a_{1:H}})\rangle - \pi_{\theta^\star}(a_{1:H}\mid{}x)\right| \\
&\leq \epapx.\label{eq:vstar-nearness}\end{align}
Finally, we compute that
\begin{align}
\Dhels{\BP^{\pistar}}{\BP^{\pibar^{v^\star}}}
&\leq 2\Dhels{\BP^{\pistar}}{\BP^{\pi_{\theta^\star}}} + 2\Dhels{\BP^{\pi_{\theta^\star}}}{\BP^{\pibar^{v^\star}}} \\
&= 2\Dhels{\BP^{\pistar}}{\BP^{\pi_{\theta^\star}}} + 2\EE_x \brk*{\sum_{a_{1:H} \in \MA^H} \left(\sqrt{\pi_{\theta^\star}(a_{1:H}\mid{}x)} - \sqrt{\pibar^{v^\star}(a_{1:H}\mid{}x)}\right)^2} \\ 
&\leq 2\Dhels{\BP^{\pistar}}{\BP^{\pi_{\theta^\star}}} + 2\EE_x\brk*{\sum_{a_{1:H} \in \MA^H} \left|\pi_{\theta^\star}(a_{1:H}\mid{}x) - \pibar^{v^\star}(a_{1:H}\mid{}x)\right|} \\
&\leq 2\Dhels{\BP^{\pistar}}{\BP^{\pi_{\theta^\star}}} + 4\epapx
\end{align}
where the final inequality is by \cref{eq:vstar-nearness} and the triangle inequality. 
\end{proof}

\begin{lemma}\label{lemma:softmax-density-lb}
For any $\theta \in \Theta$, for all $x \in \MX$ and $a_{1:H} \in \MA^H$, it holds that
\[\pi_\theta(a_{1:H}\mid{}x) \geq e^{-2L^2 H} 2^{-H}.\]
\end{lemma}

\begin{proof}[\pfref{lemma:softmax-density-lb}]
Note that $\pi_\theta(a_{1:H}\mid{}x) = \prod_{h=1}^H \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1})$. The lemma is therefore a consequence of \cref{lemma:auto-linear-density-bound} with $\Bdot := L^2$ and $|\MA| = 2$.
\end{proof}

\subsubsection{Generalization}

Next, we prove that the empirical loss from \cref{eq:kern-rho-second-attempt} concentrates near the population loss of the \emph{rounded} policies $\pibar^v,\pibar^w$, uniformly over $v,w \in \wh V(\Blarge,\gamma,\epapx)$ (which is a quantitative relaxation of the constraint sets $\wh V = \wh W$). In this section, we assume $(x\ind{i},a_{1:H}\ind{i})_{i=1}^n$ are i.i.d. trajectories from $\BP^{\pistar}$, and we write $u\ind{i} := \uop{x\ind{i}}{a_{1:H}\ind{i}}$. We also fix a parameter $\Blarge \geq 1$. Note that $\wh V$ and the relaxation $\wh V(\Blarge,\gamma,\epapx)$ are random sets, since they are defined in terms of the data $(x\ind{i},a_{1:H}\ind{i})_{i=1}^n$.

\begin{lemma}\label{lemma:tau-conc}
There is a constant $C_{\ref{lemma:tau-conc}}>0$ so that the following holds. Let $\delta \in (0,1/2)$. If $n \geq C_{\ref{lemma:tau-conc}} 2^H \Blarge \log(1/\delta)/(\gamma^3 \epstat^2)$, then with probability at least $1-\delta$, it holds for any $v,w \in \wh V(\Blarge,\gamma,\epapx)$ that
\[\left|\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right) - \En^{\pistar}\left[ \tau\left(\frac{\pibar^{v}(a_{1:H}\mid{}x)}{\pibar^w(a_{1:H}\mid{}x)}\right)\right]\right| \lesssim \frac{\epapx+\epstat}{\gamma^{3/2}}.\]
\end{lemma}


To prove \cref{lemma:tau-conc}, we start by showing that with high probability, every policy $\pibar^v$ is close to the approximate policy $f^v$ from \cref{eq:fv}, on average over contexts $x$. Since the constraints enforce closeness on observed contexts, this follows from standard Rademacher bounds applied to the distance function ``context $x$ maps to the distance of $f^v(\cdot\mid{}x)$ from $\Delta_\gamma(\MA^H)$''.

\begin{lemma}\label{lemma:tv-to-simplex}
There is a constant $C_{\ref{lemma:tv-to-simplex}}>0$ so that the following holds. Let $\delta \in (0,1/2)$. If $n \geq C_{\ref{lemma:tv-to-simplex}} 2^{4H} \Blarge \log(1/\delta) / \epstat^2$, then with probability at least $1-\delta$, it holds for all $v \in \wh V(\Blarge,\gamma,\epapx)$ that
\[\EE_x\brk*{ \sum_{a_{1:H} \in \MA^H} \left| \langle v, \psi(\uop{x}{a_{1:H}})\rangle - \pibar^v(a_{1:H}\mid{}x)\right|} \leq \epapx + \epstat.\]
\end{lemma}

\begin{proof}[\pfref{lemma:tv-to-simplex}]
For each $a \in \MA^H$, define a function class $\MF_a := \{f_{v,a}: \MX \to \RR \mid v \in \RR^\NN, \norm{v}_2^2 \leq \Blarge\}$
where
\[f_{v,a}(x) := \langle v, \psi(\uop{x}{a})\rangle.\]
For any $u = \uop{x}{a}$, we have $\norm{\psi(u)}_2^2 = K(u,u) = \prod_{h=1}^H \frac{1}{1-\frac{1}{2}\norm{\uop{x}{a}_h}_2^2} \leq 2^H$ since $\norm{\uop{x}{a}_h}_2 \leq 1$ for each $h \in [H]$. Thus, the Gaussian complexity (cf. \cref{def:gaussian_complexity}) of $\MF_a$ is bounded as $\MG_n(\MF_a) \leq \sqrt{2^{H+1}\Blarge/n}$. Now define $\iota: \RR^{\MA^H} \to \RR$ by \[\iota(z) := \min_{\mu \in \Delta_\gamma(\MA^H)} \sum_{a \in \MA^H} |z - \mu(a)|.\] Let $\MF$ be the class of functions $\{f_v: \MX \to \RR: v \in \RR^\NN, \norm{v}_2^2 \leq \Blarge\}$ where
\[f_v(x) := \iota(f_{v,a}(x): a \in \MA^H) = \min_{\mu \in \Delta_\gamma(\MA^H)} \sum_{a \in \MA^H} \left| f_{v,a}(x) - \mu(a)\right|.\]
Since $\iota$ is $\sqrt{|\MA^H|} = 2^{H/2}$-Lipschitz with respect to the Euclidean norm, it follows from \cref{lem:rc-composition} that $\MG_n(\MF) \leq 2^{2H+1} \sqrt{\Blarge/n}$. Moreover, for each $v \in \RR^\NN$ with $\norm{v}_2^2 \leq \Blarge$ and each $x \in \MX$, we know that $|f_v(x)| \leq 1 + \sum_{a \in \MA^H} |f_{v,a}(x)| \leq 1+\sqrt{2^H \Blarge}$. By \cref{lem:unif-conv} and assumption on $n$, it holds with probability at least $1-\delta$ that for all $v \in \RR^\NN$ with $\norm{v}_2^2 \leq \Blarge$,
\begin{equation} \left|\frac{1}{n}\sum_{i=1}^n f_{v}(x\ind{i}) - \EE_x [f_{v}(x)]\right| \leq \epstat.
\label{eq:dist-to-simplex-conc}
\end{equation}
Condition on this event and fix $v \in \wh V(\Blarge,\gamma,\epapx)$. We know $\norm{v}_2^2 \leq \Blarge$ so the bound \eqref{eq:dist-to-simplex-conc} holds. Moreover, for each $i \in [n]$, we know from the definition of $\wh V(\Blarge,\gamma,\epapx)$ that $f_{v,a}(x\ind{i}) \geq \gamma$ for all $a \in \MA^H$, and similarly $\sum_{a \in \MA^H} f_{v,a}(x\ind{i}) \in [1-\epapx,1+\epapx]$. Thus, there is $\mu \in \Delta_\gamma(\MA^H)$ with $\sum_{a \in \MA^H} |f_{v,a}(x\ind{i}) - \mu(a)| \leq \epapx$. Hence, $f_v(x\ind{i}) \leq \epapx$. Since this holds for all $i \in [n]$, invoking \eqref{eq:dist-to-simplex-conc} gives that $\EE_x[f_{v}(x)] \leq \epapx + \epstat$. Since for each $x \in \MX$, $\pibar^v(\cdot\mid{}x)$ minimizes $\sum_{a \in \MA^H} |f_{v,a}(x) - \mu(a)|$ over all $\mu \in \Delta_\gamma(\MA^H)$, and this minimum value is exactly $f_v(x)$, it follows that
\[\EE_x\brk*{\sum_{a \in \MA^H} |f_{v,a}(x) - \pibar^v(a\mid{}x)|} \leq \epapx + \epstat\]
as claimed.
\end{proof}

Next, we would like to prove a statement of the form ``for all $v, w \in \wh V$ with $\norm{v}_2^2,\norm{w}_2^2 \leq \Blarge$, the empirical loss at $v,w$ concentrates near the population loss''. Unfortunately, since $\tau$ is only defined on $(0,\infty)$, the empirical loss and the naively-defined population loss are not well-defined on this entire parameter space, and moreover the subspace where they are well-defined is data-dependent. Instead, we mollify the loss so that it is well-defined and Lipschitz on the entire parameter space (and equals the original loss for all $v,w \in \wh V(\Blarge,\gamma,\epapx)$). We then invoke standard generalization bounds for Rademacher complexity to show that the mollified empirical loss concentrates (\cref{lemma:mol-tau-conc}).

\begin{definition}
Define $F: \RR^2 \to \RR$ by
\[F(x,y) := \tau\left(\frac{x}{y}\right)\mol{\gamma}{x}\mol{\gamma}{y}\]
where 
\[\mol{\gamma}{x} = \begin{cases} 1 & \text{ if } x \geq \gamma \\ 0 & \text{ if } x \leq \gamma/2 \\ \frac{2x}{\gamma}-1 & \text{ if } \gamma/2 < x < \gamma \end{cases}.\]
\end{definition}

\begin{lemma}\label{lemma:mol-tau-lipschitz}
The function $F$ is $6\gamma^{-3/2}$-Lipschitz with respect to the $\ell_1$ norm.
\end{lemma}

\begin{proof}[\pfref{lemma:mol-tau-lipschitz}]
Note that $\tau(z) \in [-1,1]$ and $|\tau'(z)| \leq -1/\sqrt{z}$ for all $z>0$. Thus, for any $x,y \in\bbR$ %
at which $F$ is differentiable, we have
\begin{align}
\left|\frac{\partial}{\partial x} F(x,y)\right|
&= \left|\frac{1}{y} \tau'\left(\frac{x}{y}\right) \mol{\gamma}{x}\mol{\gamma}{y} + \tau\left(\frac{x}{y}\right) \frac{2 \cdot \mathbbm{1}[\gamma/2 \leq x \leq \gamma]}{\gamma} \mol{\gamma}{y} \right | \\ 
&\leq \frac{1}{\gamma^{3/2}} + \frac{2}{\gamma}.
\end{align}
Since $F(x,y) = -F(y,x)$, the same bound holds on $\left|\frac{\partial}{\partial y} F(x,y)\right|$. The lemma follows.
\end{proof}

\begin{lemma}\label{lemma:mol-tau-conc}
There is a constant $C_{\ref{lemma:mol-tau-conc}}>0$ so that the following holds. Let $\delta \in (0,1/2)$. If $n \geq C_{\ref{lemma:mol-tau-conc}} 2^H \Blarge \log(1/\delta)/(\gamma^3 \epstat^2)$, then with probability at least $1-\delta$, it holds for all $v,w \in \RR^\NN$ with $\norm{v}_2^2 , \norm{w}_2^2 \leq \Blarge$ that
\begin{align}
\left|\frac{1}{n} \sum_{i=1}^n F(\langle v,\psi(u\ind{i})\rangle,\langle w, \psi(u\ind{i})\rangle - \En^{\pistar} \left[F(\langle v,\psi(\uop{x}{a_{1:H}})\rangle,\langle w, \psi(\uop{x}{a_{1:H}})\rangle) \right]\right| \leq \epstat
\label{eq:F-conc}
\end{align}
\end{lemma}

\begin{proof}[\pfref{lemma:mol-tau-conc}]
Define a function class $\MF := \{f_v: \MX \times \MA^H \to \RR \mid v \in \RR^\NN, \norm{v}_2^2 \leq \Blarge\}$
where
\[f_v(x,a) := \langle v, \psi(\uop{x}{a})\rangle.\]
For any $u = \uop{x}{a}$, we have $\norm{\psi(u)}_2^2 = K(u,u) = \prod_{h=1}^H \frac{1}{1-\frac{1}{2}\norm{\uop{x}{a}_h}_2^2} \leq 2^H$, since $\norm{\uop{x}{a}_h}_2 \leq 1$ for each $h \in [H]$. Thus, the Gaussian complexity of $\MF$ is bounded as $\MG_n(\MF) \leq \sqrt{2^{H+1} \Blarge/n}$. Now define the function class \[\mathcal{\widetilde F} := \{\widetilde f_{v,w}: \MX \times \MA^H \to \RR \mid v,w \in \RR^\NN, \norm{v}_2^2 \leq \Blarge, \norm{w}_2^2 \leq \Blarge\}\]
where
\[\widetilde f_{v,w}(x,a) := F(f_v(x,a),f_w(x,a)).\]
By \cref{lemma:mol-tau-lipschitz,lem:rc-composition} we have $\MG_n(\mathcal{\widetilde{F}}) \leq 12\sqrt{2^{H+2} \Blarge/(n\gamma^3)}$. Moreover, by definition of $F$, we know that all functions in $\mathcal{\widetilde F}$ have range in $[-1,1]$. Thus, by \cref{lem:unif-conv} and choice of $n$, the bound \eqref{eq:F-conc} holds for all $v,w \in \RR^\NN$ with $\norm{v}_2^2,\norm{w}_2^2 \leq \Blarge$ with probability at least $1-\delta$.
\end{proof}





We can now prove \cref{lemma:tau-conc} by combining \cref{lemma:tv-to-simplex,lemma:mol-tau-conc}.

\begin{proof}[Proof of \cref{lemma:tau-conc}]
By \cref{lemma:tv-to-simplex}, \cref{lemma:mol-tau-conc}, and the lemma assumption that \[n \geq C_{\ref{lemma:tau-conc}} 2^H\Blarge \log(1/\delta)/(\gamma^3\epstat^2),\] so long as $C_{\ref{lemma:tau-conc}}>0$ is a sufficiently large constant, we have with probability at least $1-\delta$ that the events of both \cref{lemma:tv-to-simplex} and \cref{lemma:mol-tau-conc} hold. Condition henceforth on the intersection of these events. For any $v,w \in \wh V(\Blarge,\gamma,\epapx)$, we have
\begin{align}
\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right)
&= \frac{1}{n}\sum_{i=1}^n F\left(\langle v,\psi(u\ind{i})\rangle,  \langle w, \psi(u\ind{i})\rangle\right) \\ 
&\leq \En^{\pi^\star}\left[ F\left(\left\langle v,\psi\left(\uop{x}{a_{1:H}}\right)\right\rangle,  \left\langle w, \psi\left(\uop{x}{a_{1:H}}\right)\right\rangle\right)\right] + \epstat \\ 
&\leq \En^{\pi^\star}\left[ F\left(\pibar^{v}(a_{1:H}\mid{}x), \pibar^w(a_{1:H}\mid{}x)\right)\right] + \epstat\\ 
&+ \frac{6}{\gamma^{3/2}}\Big(\En^{\pistar} \left|\left\langle v,\psi\left(\uop{x}{a_{1:H}}\right)\right\rangle - \pibar^{v}(a_{1:H}\mid{}x)\right| \\
&\qquad+ \left|\left\langle w,\psi\left(\uop{x}{a_{1:H}}\right)\right\rangle - \pibar^w(a_{1:H}\mid{}x)\right|\Big) \\ 
&\leq \En^{\pi^\star}\left[ F\left(\pibar^{v}(a_{1:H}\mid{}x), \pibar^w(a_{1:H}\mid{}x)\right)\right] + \epstat + \frac{12}{\gamma^{3/2}}(\epapx+\epstat)
\end{align}
where the first equality is because $v,w \in \wh V(\Blarge,\gamma,\epapx)$, so $\langle v, \psi(u\ind{i})\rangle \geq \gamma$ and $\langle w, \psi(u\ind{i})\rangle \geq \gamma$; the first inequality is by the event of \cref{lemma:mol-tau-conc}; the second inequality is by triangle inequality and \cref{lemma:mol-tau-lipschitz}; and the third inequality is by the event of \cref{lemma:tv-to-simplex}. Now
\begin{align}
\En^{\pistar}\left[ F\left(\pibar^{v}(a_{1:H}\mid{}x), \pibar^w(a_{1:H}\mid{}x)\right)\right]
&= \En^{\pistar} \left[\tau\left(\frac{\pibar^{v}(a_{1:H}\mid{}x)}{\pibar^w(a_{1:H}\mid{}x)}\right)\right]
\end{align}
since $\pibar^v(\cdot\mid{}x),\pibar^w(\cdot\mid{}x) \in \Delta_\gamma(\MA^H)$ for all $x \in \MX$. This proves one direction of the claimed inequality, and the other direction follows by a symmetric argument
\end{proof}

\subsubsection{Completing the Statistical Analysis}

We can now complete the statistical analysis of \cref{eq:kern-rho-second-attempt} using \cref{lemma:best-to-pibar}, \cref{lemma:tau-conc}, and the classical inequality for the $\rho$-estimator (\cref{lemma:rho-estimator-bounds}). The following lemmas together show that for any $v \in \wh V(\Blarge,\gamma,\epapx)$ with near-optimal min-max loss, the policy $\pibar^v$ achieves near-optimal error (as measured by trajectory-level Hellinger distance).

\begin{lemma}\label{lemma:minmax-ub}
Suppose that $B \geq (L^2H 2^{2H+2}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H}$ and $\gamma \leq (e^{-2L^2 H} - \epapx)2^{-H-1}$. In the event of \cref{lemma:tau-conc}, it holds that
\[\min_{v \in \wh V} \max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right) \leq 8\min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + O\left(\frac{\epapx + \epstat}{\gamma^{3/2}}\right)\]
\end{lemma}

\begin{proof}[\pfref{lemma:minmax-ub}]
Condition on the event of \cref{lemma:tau-conc}, so that for any $v,w \in \wh V(\Blarge,\gamma,\epapx)$ we have
\begin{align}
\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right)
&\leq \En^{\pistar}\left[ \tau\left(\frac{\pibar^{v}(a_{1:H}\mid{}x)}{\pibar^w(a_{1:H}\mid{}x)}\right)\right]+ O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right) \\ 
&\leq 4\Dhels{\BP^{\pistar}}{\BP^{\pibar^v}} - \frac{3}{8} \Dhels{\BP^{\pistar}}{\BP^{\pibar^w}} + O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right)\\ 
&\leq 4\Dhels{\BP^{\pistar}}{\BP^{\pibar^v}}+ O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right)
\end{align}
where the second inequality is by \cref{lemma:rho-estimator-bounds} and the basic equality $\frac{\BP^{\pibar^v}(x,a_{1:H})}{\BP^{\pibar^w}(x,a_{1:H})} = \frac{\pibar^v(a_{1:H}\mid{}x)}{\pibar^w(a_{1:H}\mid{}x)}$. Thus, for any $v \in \wh V$, we have
\begin{equation}
\max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right) \leq 4\Dhels{\BP^{\pistar}}{\BP^{\pibar^v}} + O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right).\label{eq:sup-loss-bound}
\end{equation}

Minimizing over $v \in \wh V$ and applying \cref{lemma:best-to-pibar} (via the assumed bounds on $B$, $\gamma$, and $\epapx$), we get
\begin{equation}
\min_{v \in \wh V} \max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right) \leq 8\min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right)
\end{equation}
which completes the proof.
\end{proof}

\begin{lemma}\label{lemma:hellinger-by-loss}
Suppose that $B \geq (L^2H 2^{2H+2}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H}$ and $\gamma \leq (e^{-2L^2H} - \epapx)2^{-H-1}$. In the event of \cref{lemma:tau-conc}, for all $v \in \wh V(\Blarge,\gamma,\epapx)$,
\[\Dhels{\BP^{\pistar}}{\BP^{\pibar^v}} \leq \frac{64}{3}\min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + \frac{8}{3}\max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right) + O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right).\]
\end{lemma}

\begin{proof}[\pfref{lemma:hellinger-by-loss}]
Condition on the event of \cref{lemma:tau-conc}, so that for any $v,w \in \wh V(\Blarge,\gamma,\epapx)$ we have
\begin{align}
\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right)
&\geq \En^{\pistar}\left[ \tau\left(\frac{\pibar^{v}(a_{1:H}\mid{}x)}{\pibar^w(a_{1:H}\mid{}x)}\right)\right]- O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right) \\ 
&\geq \frac{3}{8}\Dhels{\BP^{\pistar}}{\BP^{\pibar^v}} - 4 \Dhels{\BP^{\pistar}}{\BP^{\pibar^w}} - O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right)
\end{align}
where the second inequality is by \cref{lemma:rho-estimator-bounds}. Thus, for any $v \in \wh V(\Blarge,\gamma,\epapx)$, we have
\begin{align}
\max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(u\ind{i})\rangle}{\langle w, \psi(u\ind{i})\rangle}\right)
&\geq \frac{3}{8}\Dhels{\pistar}{\pibar^v} - 4\min_{w \in \wh W} \Dhels{\BP^{\pistar}}{\BP^{\pibar^w}} - O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right) \\ 
&\geq \frac{3}{8}\Dhels{\pistar}{\pibar^v} - 8\min_{\theta \in \Theta} \Dhels{\pistar}{\pi_\theta} - O\left(\frac{\epapx+\epstat}{\gamma^{3/2}}\right)
\end{align}
where the second inequality is by \cref{lemma:best-to-pibar}. Rearranging completes the proof.
\end{proof}

\subsection{Computational Analysis for $\KernRho$}\label{subsec:kernrho-computational}

We now analyze $\KernRho$ (\cref{alg:kernrho}) itself. In particular, we show that it computes a succinct representation $\wh \alpha$ of an approximately optimal solution $\wh v$ to the infinite-dimensional program defined in \cref{eq:kern-rho-second-attempt}:

\begin{lemma}\label{lemma:kernrho-minmax}
Let $\epopt>0$ and suppose that $\Blarge \geq 2C_{\ref{thm:sp-pgd}}^2 2^{H+1}B/(\gamma^{3/2}\epopt)$. Define \[\wh v := \sum_{j=1}^n \sum_{a\in\MA^H} \wh \alpha_{j,a} \psi(\uop{x\ind{j}}{a})\] where $\wh\alpha$ is the parameter computed in \cref{line:wh-alpha} of \[\KernRho((x\ind{i},a\ind{i}_{1:H})_{i=1}^n, B,\gamma,\epapx,\epopt).\]
Then $\wh v \in \wh V(\Blarge,\gamma,\epapx)$ and
\[\max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle \wh v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) \leq \min_{v\in\wh V} \max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) + \epopt.\]
Moreover, the time complexity of $\KernRho$ with these parameters is $\poly(n,2^H,B,1/\gamma,1/\epopt)$, and for any $x\in\MX$, $\pihat(\cdot\mid{}x)$ can be explicitly computed in time $\poly(n,2^H)$.
\end{lemma}

To prove \cref{lemma:kernrho-minmax}, we combine two representational facts---\cref{lemma:representer,lemma:reverse-representer}, which together allow translating back and forth between the infinite-dimensional space and finite-dimensional space---with \cref{lemma:optimization-loss-guarantee}, which uses convexity-concavity of $\tau$ (\cref{lemma:tau-convex-concave}) and standard guarantees for projected gradient descent-ascent to show that $\wh \alpha$ is an approximately optimal solution to the finite-dimensional program defined in \cref{eq:kern-rho-finite}.

\subsubsection{Representational Results}

The following lemma shows that any vector $\alpha$ in the finite-dimensional constraint set $J(\Blarge,\gamma,\epapx)$ corresponds a vector $v$ in the infinite-dimensional constraint set $\wh V(\Blarge,\gamma,\epapx)$.

\begin{lemma}\label{lemma:reverse-representer}
For each $\alpha \in \wh J(\Blarge,\gamma,\epapx)$, the vector $v = \sum_{j=1}^n \sum_{a \in \MA^H} \alpha_{j,a} \psi(\uop{x\ind{j}}{a})$ satisfies 
\[\langle v, \psi(\uop{x\ind{i}}{a'})\rangle = \sum_{j=1}^n \sum_{a \in \MA^H} \alpha_{j,a} K(\uop{x\ind{j}}{a},\uop{x\ind{i}}{a'})\]
for all $i \in [n]$ and $a' \in \MA^H$, and moreover $v \in \wh V(\Blarge,\gamma,\epapx)$.
\end{lemma}

\begin{proof}[\pfref{lemma:reverse-representer}]
The display equation is immediate from the fact that $\langle \psi(\cdot),\psi(\cdot)\rangle = K(\cdot,\cdot)$ (\cref{lemma:psi-k}). The fact that $v \in \wh V(\Blarge,\gamma,\epapx)$ then follows from the display equation and the definitions of $\wh J(\Blarge,\gamma,\epapx)$ and $\wh V(\Blarge,\gamma,\epapx)$. %
\end{proof}

The converse of \cref{lemma:reverse-representer} is not true; not every vector in $\wh V(\cdot,\cdot,\cdot)$ can be expressed as a linear combination of $\{\psi(\uop{x\ind{j}}{a}): j\in[n], a \in \MA^H\}$. However, for every $v$ there does exist some $v'$ that (1) can be expressed as a linear combination, and (2) is equivalent to $v$ for all intents and purposes, i.e. $\norm{v'}_2 \leq \norm{v}_2$ and $\langle v,\psi(\uop{x\ind{i}}{a})\rangle = \langle v',\psi(\uop{x\ind{i}}{a})\rangle$ for all $i\in[n]$ and $a \in \MA^H$; note that the loss function only depends on such inner products. This fact is also the basis for the Representer Theorem for ERM in an RKHS. Formally, we need the following result.

\begin{lemma}\label{lemma:representer}
For any $v \in \wh V$, there is $\alpha \in \wh J$ with 
\[\langle v, \psi(\uop{x\ind{i}}{a'})\rangle = \sum_{j=1}^n \sum_{a \in \MA^H} \alpha_{j,a} K(\uop{x\ind{j}}{a},\uop{x\ind{i}}{a'})\]
for all $i \in [n]$ and $a' \in \MA^H$.
\end{lemma}


\begin{proof}[\pfref{lemma:representer}]
This is a consequence of standard facts about Hilbert spaces. Let $Y$ be the span of the vectors $\{\psi(\uop{x\ind{j}}{a}): j \in [n], a \in \MA^H\}$ in $\ell^2$. Since $\ell^2$ is a Hilbert space and $Y$ is a closed subspace of $\ell^2$, for any $v \in \wh V$ there are $y \in Y$ and $z \in Y^\perp$ such that $v=y+z$. By definition of $Y$, there is $\alpha \in \RR^{n \times 2^H}$ such that 
\[y = \sum_{j=1}^n \sum_{a \in \MA^H} \alpha_{j,a} \psi(\uop{x\ind{j}}{a}).\]
By definition of $z$, for each $j \in [n]$ and $a \in \MA^H$ we have $\langle z, \psi(\uop{x\ind{j}}{a})\rangle = 0$, and hence
\begin{align}
\langle v, \psi(\uop{x\ind{j}}{a})\rangle
&= \langle y, \psi(\uop{x\ind{j}}{a})\rangle \\ 
&= \sum_{j'=1}^n \sum_{a'\in\MA^H} \alpha_{j',a'} \langle \psi(\uop{x^{j'}}{a'}), \psi(\uop{x\ind{j}}{a})\rangle \\  
&= \sum_{j'=1}^n \sum_{a'\in\MA^H} \alpha_{j',a'} K(\uop{x^{j'}}{a'},\uop{x\ind{j}}{a}).
\end{align}
This proves the lemma's stated equality, and it remains to show $\alpha \in \wh J$. Using the above fact and the definition of $\wh V$, we get for any $i \in [n]$ and $a \in \MA^H$ that 
\[\sum_{j=1}^n \sum_{a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) = \langle v, \psi(\uop{x\ind{i}}{a})\rangle \geq \gamma\]
and similarly
\[\sum_{j=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a'} K(\uop{x\ind{j}}{a'},\uop{x\ind{i}}{a}) = \sum_{a \in \MA^H} \langle v, \psi(\uop{x\ind{i}}{a})\rangle \in [1-\epapx,1+\epapx].\]
Finally,
\[\sum_{j,j'=1}^n \sum_{a,a' \in \MA^H} \alpha_{j,a}\alpha_{j',a'}K(\uop{x\ind{j}}{a},\uop{x^{(j')}}{a'}) = \langle y,y\rangle \leq \langle v,v\rangle \leq B.\]
where the first inequality is since $\langle y,z\rangle = 0$ and hence $\langle v,v\rangle = \langle y,y\rangle + \langle z,z\rangle$. We conclude that $\alpha \in \wh J$.
\end{proof}




\subsubsection{Optimization Guarantee for Finite-Dimensional Program}




\begin{lemma}\label{lemma:optimization-loss-guarantee}
Let $\epopt>0$ and suppose that $\Blarge \geq 2C_{\ref{thm:sp-pgd}}^2 2^{H+1}B/(\gamma^{3/2}\epopt)$. Then the parameter $\wh\alpha$ computed in \cref{line:wh-alpha} of \[\KernRho((x\ind{i},a\ind{i}_{1:H})_{i=1}^n, B,\gamma,\epapx,\epopt)\] satisfies $\wh\alpha \in \wh J(\Blarge,\gamma,\epapx)$ and
\[\max_{\beta \in \wh K} \emploss(\wh \alpha, \beta) \leq \min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss( \alpha, \beta) + \epopt.\]
Moreover, the time complexity of $\KernRho$ with these parameters is $\poly(n,2^H,B,1/\gamma,1/\epopt)$, and for any $x\in\MX$, $\pihat(\cdot\mid{}x)$ can be explicitly computed in time $\poly(n,2^H)$.
\end{lemma}

\begin{proof}[\pfref{lemma:optimization-loss-guarantee}]
Define $f: \MY \times \MY \to \RR$ by 
\[f(\alphatil,\betatil) := \frac{1}{n}\sum_{i=1}^n \tau\left(\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}\right),\]
where $\Sigma$ is defined in \cref{eq:sigma-def} and $\MY$ is defined in \cref{eq:y-def}. Notice that by definition of $\Sigma$, we have $\emploss(\alpha,\beta) = f(\Sigma^{1/2}\alpha,\Sigma^{1/2}\beta)$ for all $\alpha,\beta$, where $\emploss$ is the empirical loss function defined in \cref{eq:kern-rho-finite}.

We check the conditions of \cref{thm:sp-pgd}. Observe that $\MY$ is defined by intersection of linear constraints; hence, $\MY$ is convex. By \cref{lemma:tau-convex-concave} and the fact that $e_{i,a\ind{i}} \Sigma^{1/2} \alphatil$ is a linear function of $\alphatil$, we get that $f$ is convex in $\alphatil$, and similarly that $f$ is concave in $\betatil$. Next, since $|\tau'(z)| \leq 1/\sqrt{z}$ for all $z \in (0,\infty)$, we have for all $\alphatil,\alphatil',\betatil \in \MY$ that 
\begin{align}
|f(\alphatil,\betatil) - f(\alphatil',\betatil)|
&\leq \max_{i \in [n]} \left|\tau\left(\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}\right) - \tau\left(\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil'}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}\right)\right| \\ 
&\leq \max_{i \in [n]} \frac{ \left|\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil} - \frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil'}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}\right|}{\min\left(\sqrt{\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}}, \sqrt{\frac{e_{i,a\ind{i}}\Sigma^{1/2}\alphatil'}{e_{i,a\ind{i}}\Sigma^{1/2}\betatil}}\right)} \\ 
&\leq \frac{1}{\gamma} \sqrt{\frac{1+\epapx}{\gamma}} \cdot \max_{i \in [n]} \left|e_{i,a\ind{i}}\Sigma^{1/2}\alphatil-e_{i,a\ind{i}}\Sigma^{1/2}\alphatil'\right| \\ 
&\leq \frac{2}{\gamma} \sqrt{\frac{4}{\gamma}} \norm{\alphatil - \alphatil'}_2 \cdot \max_{i \in [n]} \sqrt{\Sigma_{(i,a\ind{i}),(i,a\ind{i})}} \\ 
&\leq \frac{2^{H+2}}{\gamma^{3/2}} \norm{\alphatil-\alphatil'}_2
\end{align}
where the third inequality uses the fact that \[\gamma \leq e_{i,a}\Sigma^{1/2} y \leq 1+\epapx\] for all $y \in \MY$, the fourth inequality uses Cauchy-Schwarz, and the final inequality uses \cref{eq:sigma-def}. Hence, $f$ is $2^{H+2}\gamma^{-3/2}$-Lipschitz in $\alphatil$, with respect to the Euclidean norm. A symmetric argument, using the fact that $\tau(1/z)=-\tau(z)$, shows that $f$ is also $2^{H+2}\gamma^{-3/2}$-Lipschitz in $\betatil$. Finally, by definition $\Oproj$ is an $\epproj$-approximate projection oracle for $\MY$ with $\epproj = 1/(16BT^4)$, and $\Ovec$ implements queries to the vector field $g(\alphatil,\betatil) = (\grad_{\alphatil} f(\alphatil,\betatil), -\grad_{\betatil} f(\alphatil,\betatil))$. Thus, applying \cref{thm:sp-pgd} with $R := B$, we get
\[\max_{\betatil\in\MY: \norm{\betatil}_2 \leq \sqrt{B}} f\left(\frac{1}{T}\sum_{t=1}^T \alphatil_t, \betatil\right) - \min_{\alphatil\in\MY: \norm{\alphatil}_2 \leq \sqrt{B}} f\left(\alphatil,\frac{1}{T}\sum_{t=1}^T \betatil_t\right) \leq C_{\ref{thm:sp-pgd}} \left(\frac{2^{H+1}\sqrt{B}}{\gamma^{3/2}\sqrt{T}}\right) \leq \epopt\] 
by choice of $T := 4C_{\ref{thm:sp-pgd}}^2 2^{2H+2} B/(\gamma^3 \epopt^2)$. Moreover, 
$\frac{1}{T}\sum_{t=1}^T \alphatil_t, \frac{1}{T}\sum_{t=1}^T \betatil_t \in \MY$ with \[\norm{\frac{1}{T}\sum_{t=1}^T \alphatil_t}_2, \norm{\frac{1}{T}\sum_{t=1}^T \betatil_t}_2 \leq C_{\ref{thm:sp-pgd}} \sqrt{BT} \leq \Blarge\] by lemma assumption. By definition of $\wh\alpha$, observe that $\frac{1}{T}\sum_{t=1}^T \alphatil_t = \Sigma^{1/2} \wh\alpha + y$ for some $y \in \ker(\Sigma)$. The fact that $\frac{1}{T}\sum_{t=1}^T \alphatil_t \in \MY$ and $\norm{\frac{1}{T}\sum_{t=1}^T \alphatil_t}_2 \leq \Blarge$ implies that $\Sigma^{1/2} \wh\alpha \in \MY$ and $\norm{\Sigma^{1/2}\wh\alpha}_2 \leq \Blarge$, and so $\wh\alpha \in \wh J(\Blarge,\gamma,\epapx)$. Similarly, there is some $\wh\beta \in \wh K(\Blarge,\gamma,\epapx)$ and $y' \in \ker(\Sigma)$ such that $\frac{1}{T}\sum_{t=1}^T\betatil_t = \Sigma^{1/2} \wh \beta + y'$.

For any $\beta \in \wh K$, we have $\Sigma^{1/2} \beta \in \MY$ and $\norm{\Sigma^{1/2}\beta}_2 \leq \sqrt{B}$, so 
\[\max_{\beta \in \wh K} \emploss(\wh\alpha,\beta) = \max_{\beta \in \wh K} f(\Sigma^{1/2} \wh\alpha, \Sigma^{1/2}\beta) = \max_{\beta \in \wh K} f\left(\frac{1}{T}\sum_{t=1}^T \alphatil_t, \Sigma^{1/2}\beta\right) \leq \max_{\betatil \in \MY:\norm{\betatil}_2\leq \sqrt{B}}  f\left(\frac{1}{T}\sum_{t=1}^T \alphatil_t, \betatil\right).\]
Next, for any $\alpha \in \wh J$, we have $\Sigma^{1/2}\alpha \in \MY$ and $\norm{\Sigma^{1/2}\alpha}_2 \leq \sqrt{B}$, so
\begin{align}
\min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\alpha,\beta) 
&\geq \min_{\alpha \in \wh J} \emploss(\alpha,\wh\beta) \\
&\geq \min_{\alpha \in \wh J} f(\Sigma^{1/2} \alpha, \Sigma^{1/2} \wh\beta)\\
&\geq \min_{\alphatil \in \MY:\norm{\alphatil}_2 \leq \sqrt{B}} f(\alphatil, \Sigma^{1/2}\wh\beta)\\ 
&= \min_{\alphatil \in \MY:\norm{\alphatil}_2 \leq \sqrt{B}} f\left(\alphatil,\frac{1}{T}\sum_{t=1}^T \betatil_t\right).
\end{align}
We conclude that 
\[\max_{\beta \in \wh K} \emploss(\wh \alpha, \beta) \leq \min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\wh \alpha, \beta) + \epopt\]
as claimed. It remains to analyze the time complexity. Excluding the final step of the algorithm (computing $\pihat$), the claimed bound is immediate from the description of $\PGD$ (\cref{alg:pgd}) together with the choice of parameter $T$ and the fact that both oracles in $\KernRho$ can be implemented in polynomial time. In particular, \cref{lemma:projection-time} shows that $\Oproj$ can be implemented in polynomial time (since it is straightforward to check that the queries to the projection oracle will have polynomially-bounded norm), and it is evident from direct differentation that $\Ovec$ can be implemented in polynomial time. 

Now we argue that for any given $x\in\MX$, $\pihat(\cdot\mid{}x)$ can be explicitly computed in time $\poly(n,2^H)$. Indeed, this only requires $\poly(n,2^H)$ evaluations of the kernel function, followed by $\ell_1$ projection of a $n2^H$-dimensional vector onto $\Delta_\gamma(\MA^H)$. Evaluations of the kernel function are efficient by \cref{def:kernel}, and the projection step can be implemented efficiently by greedily increasing all coordinates which are less than $\gamma$, and then either greedily increasing or decreasing the largest coordinate(s) until the sum is exactly $1$.
\end{proof}

In the preceding proof, we used the following technical lemmas:

\begin{lemma}\label{lemma:tau-convex-concave}
The function $(x,y) \mapsto \tau(x/y)$ with domain $(0,\infty)^2$ is convex in $x$ and concave in $y$.
\end{lemma}

\begin{proof}[\pfref{lemma:tau-convex-concave}]
We can check that $\tau(x) = \frac{2}{1+\sqrt{x}} - 1$, so $\tau'(x) = -\frac{1}{\sqrt{x}(1+\sqrt{x})^2}$, which is non-decreasing in $x$. This establishes convexity of $(x,y) \mapsto \tau(x/y)$ in $x$. Similarly, $\tau(1/y) = 1 - \frac{2}{1+\sqrt{y}} = -\tau(y)$, which establishes concavity in $y$.
\end{proof}

\begin{lemma}\label{lemma:projection-time}
For any $\epproj > 0$ and query point $q$ with norm at most $N$, the $\epproj$-approximate projection oracle $\Oproj$ can be implemented in time $\poly(n,2^H, \log(N/(\epproj\epapx\gamma)))$. 
\end{lemma}

\begin{proof}[\pfref{lemma:projection-time}]
We apply the ellipsoid method with function $z \mapsto \norm{z - q}_2^2$ and constraint set $\MY \cap \{z: \norm{z}_2 \leq 2N\}$ (\cref{eq:y-def}), which admits an efficient separating hyperplane oracle. By definition, the set lies in $\RR^{n2^H}$ and is contained in a Euclidean ball of radius $2N$. Moreover, by \cref{lemma:best-to-pibar}, there is some $v^\star \in \wh V(B/2,2\gamma,\epapx/2)$. By \cref{lemma:representer}, there is $\alphatil \in \MY$ with $\langle v,\psi(\uop{x\ind{i}}{a})\rangle = e_{i,a}^\top \Sigma^{1/2} \alphatil$ for all $i,a$, and thus (by definition of $\wh V(B/2,2\gamma,\epapx/2)$), $e_{i,a}^\top \Sigma^{1/2} \alphatil \geq 2\gamma$ for all $i,a$ and $1-\epapx/2 \leq \sum_{a \in \MA^H} e_{i,a}^\top \Sigma^{1/2} \alphatil \leq 1+\epapx/2$ for all $i$. Moreover $\norm{\alphatil}_2^2 = \norm{v^\star}_2^2 \leq B/2$. Since $\norm{e_{i,a}}_\Sigma \leq 2^H$ for all $i,a$, it follows that for any $y \in \RR^{n2^H}$ with $\norm{y-\alphatil}_2 \leq \min(\epapx 2^{-3H/2-1}, \gamma 2^{-H/2-1})$, we have $\norm{y}_2 \leq N$ and $e_{i,a}^\top \Sigma^{1/2} y \geq \gamma$ and $1-\epapx \leq \sum_{a \in \MA^H} e_{i,a}^\top \Sigma^{1/2} y \leq 1+\epapx$ for all $i$, so that $y$ lies in the constraint set. Thus, the constraint set contains a Euclidean ball of radius $\min(\epapx 2^{-3H/2-1}, \gamma 2^{-H/2-1})$. Finally, note that the function $z \mapsto \norm{z-q}_2^2$ has range bounded in $[0,9N^2]$. Thus, we can conclude from \cite[Theorem 2.4]{bubeck2015convex} that the ellipsoid method finds, in time $\poly(n,2^H, \log(N/(\epproj\epapx\gamma)))$, a point $\hat z \in \MY$ satisfying 
\[\norm{\hat z - q}_2^2 \leq \min\{\norm{z-q}_2^2: z \in \MY, \norm{z}_2 \leq 2N\} + \epproj.\]
But we know that
\[\min\{\norm{z-q}_2^2: z \in \MY, \norm{z}_2 \leq 2N\} = \min\{\norm{z-q}_2^2: z \in \MY\}\]
since $\norm{q}_2 \leq N$ and $\norm{\alphatil}_2 \leq N$. Thus, the ellipsoid method implements an $\epproj$-approximate projection oracle.
\end{proof}

\subsubsection{Optimization Guarantee for Infinite-Dimensional Program}

We now prove \cref{lemma:kernrho-minmax} by appealing to \cref{lemma:optimization-loss-guarantee} as well as \cref{lemma:reverse-representer,lemma:representer}.\vspace{0.5em}

\begin{proof}[Proof of \cref{lemma:kernrho-minmax}]
The time complexity bound is immediate from \cref{lemma:optimization-loss-guarantee}; it remains to prove the inequality. By \cref{lemma:optimization-loss-guarantee} and assumption on $\Blarge$, we have $\wh\alpha \in \wh J(\Blarge,\gamma,\epapx)$. By \cref{lemma:reverse-representer}, we have \[\wh v := \sum_{j=1}^n \sum_{a \in \MA^H} \wh\alpha_{j,a} \psi(\uop{x\ind{j}}{a}) \in  \wh V(\Blarge,\gamma,\epapx).\] 
By \cref{lemma:representer}, for each $w \in \wh W$ there is some $\beta^w \in \wh K$ such that 
\[\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle = \sum_{j=1}^n \sum_{a \in \MA^H} \beta^w_{j,a} K(\uop{x\ind{j}}{a},\uop{x\ind{i}}{a\ind{i}})\]
for all $i \in [n]$. The analogous relation also holds for $\wh v$ and $\wh \alpha$, by \cref{lemma:reverse-representer}. Hence, for each $w \in \wh W$,
\[\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle \wh v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) = \emploss(\wh \alpha, \beta^w),\]
so supremizing over $w \in \wh W$ gives
\begin{align} 
\max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle \wh v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) 
&\leq \max_{\beta \in \wh K} \emploss(\wh \alpha, \beta) \\ 
&\leq \min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\alpha,\beta) + \epopt\label{eq:loss-bound-minmax}
\end{align}
where the second inequality is by \cref{lemma:optimization-loss-guarantee}. Now fix any $v^\star \in \wh V$. By \cref{lemma:representer}, there is some $\alpha^\star \in \wh J$ such that
\[\langle v^\star, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle = \sum_{j=1}^n \sum_{a \in \MA^H} \alpha^\star_{j,a} K(\uop{x\ind{j}}{a},\uop{x\ind{i}}{a\ind{i}})\]
for all $i \in [n]$. For each $\beta \in \wh K(\Blarge,\gamma,\epapx)$, defining $w^\beta = \sum_{j=1}^n \sum_{a \in \MA^H} \beta_{j,a} \psi(\uop{x\ind{j}}{a})$ we have $w^\beta \in \wh W(\Blarge,\gamma,\epapx)$ by \cref{lemma:reverse-representer}, and the analogous relation to the above holds for $w^\beta$ and $\beta$. Thus,
\[\frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v^\star, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w^\beta, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) = \emploss(\alpha^\star, \beta).\]
We conclude that
\begin{align}
\min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\alpha,\beta) 
&\leq \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\alpha^\star, \beta) \\ 
&\leq \max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v^\star, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right).
\end{align}
Since $v^\st\in\wh V$ was arbitrary, it follows that
\[\min_{\alpha \in \wh J} \max_{\beta \in \wh K(\Blarge,\gamma,\epapx)} \emploss(\alpha,\beta)  \leq \min_{v\in\wh V}\max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right).\]
Substituting into \cref{eq:loss-bound-minmax} completes the proof.
\end{proof}

\subsection{Proof of Theorem \ref*{thm:kern-rho-main}}\label{subsec:kernrho-proof}



\begin{proof}[Proof of \cref{thm:kern-rho-main}]
For purposes of the analysis, set $\Blarge := 2C_{\ref{thm:sp-pgd}}^2 2^{H+1}B/(\gamma^{3/2}\epopt)$ and $\epstat := \gamma^{3/2}\epsilon$. Condition on the event that the bound from \cref{lemma:tau-conc} holds, which occurs with probability at least $1-\delta$ over the data $(x\ind{i},a_{1:H}\ind{i})_{i=1}^n$ from $\BP^{\pistar}$, since $n \geq C_{\ref{lemma:tau-conc}} 2^H \Blarge \log(1/\delta)/(\gamma^3 \epstat^2)$ by theorem assumption, so long as $C_{\ref{thm:kern-rho-main}}$ is a sufficiently large constant. 

Recall the definition of $\wh\alpha$ from \cref{line:wh-alpha}. By \cref{lemma:kernrho-minmax} and choice of $\Blarge$, we have \[\wh v := \sum_{j=1}^n \sum_{a \in \MA^H} \wh\alpha_{j,a} \psi(\uop{x\ind{j}}{a}) \in  \wh V(\Blarge,\gamma,\epapx).\] Thus, applying \cref{lemma:hellinger-by-loss} to $\wh v$ (note that $B \geq (L^2H 2^{2H+2}/\epapx)^{C_{\ref{thm:kernel-apx}}L^2H}$ and $\gamma \leq (e^{-2L^2H} - \epapx)2^{-H-1}$, and we have conditioned on the event of \cref{lemma:tau-conc}, so the conditions of the lemma are satisfied),
\begin{align}
\Dhels{\BP^{\pistar}}{\BP^{\pibar^{\wh v}}}
&\leq \frac{64}{3} \min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + \frac{8}{3} \max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle \wh v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) + O\left( \frac{\epapx + \epstat}{\gamma^{3/2}}\right).
\end{align}
By \cref{lemma:kernrho-minmax} and choice of $\Blarge$, we have
\begin{align} 
\max_{w \in \wh W} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle \wh v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right)
\leq \min_{v\in\wh V}\max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle  v, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) + \epopt.
\end{align}
By \cref{lemma:minmax-ub}, we have
\begin{align}
\min_{v \in \wh V} \max_{w \in \wh W(\Blarge,\gamma,\epapx)} \frac{1}{n} \sum_{i=1}^n \tau\left(\frac{\langle v,\psi(\uop{x\ind{i}}{a\ind{i}})\rangle}{\langle w, \psi(\uop{x\ind{i}}{a\ind{i}})\rangle}\right) \leq 8\min_{\theta \in \Theta} \Dhels{\BP^{\pi^\star}}{\BP^{\pi_\theta}} + O\left(\frac{\epapx + \epstat}{\gamma^{3/2}}\right)
\end{align}
Putting everything together, we get
\begin{align}
\Dhels{\BP^{\pistar}}{\BP^{\pibar^{\wh v}}}
&\leq \frac{88}{3}\min_{\theta \in \Theta} \Dhels{\BP^{\pistar}}{\BP^{\pi_\theta}} + O\left(\epopt + \frac{\epapx+\epstat}{\gamma^{3/2}}\right).
\end{align}
Substituting in the chosen values of $\epopt,\epapx,\epstat,\gamma$, and observing that $\pibar^{\wh v}$ is exactly the policy $\pihat$ produced by $\KernRho$, gives the claimed result. The time complexity bound is immediate from \cref{lemma:minmax-ub}.
\end{proof}



\subsection{Proof of Theorem \ref*{thm:chunk-kr-main}/Theorem \ref*{thm:chunk-kr-informal}}\label{subsec:chunkkr-proof}

We now complete the proof of \cref{thm:chunk-kr-main}, which formally proves \cref{thm:chunk-kr-informal}.

\begin{proof}[Proof of \cref{thm:chunk-kr-main}]
First, we remark that the distribution $\BP^{\pihat}$ where $\pihat = (\pihat_j)_{j=1}^H$ is the output of $\ChunkKR$ is identical to the distribution autoregressively induced by the ``chunked'' policies $\pihat_{h+1-K:h}$ for $h \in \{K,2K,\dots,H\}$; indeed, $\pihat_j(a_j\mid{} x,a_{1:j-1})$ is defined precisely to be the conditional distribution of $a_j$ given $a_{h+1-K:j-1}$ under $a_{h+1-K:h} \sim \pihat_{h+1-K:h}(\cdot\mid{}x,a_{1:h-K})$. Thus, sampling from $\BP^{\pihat}$ is equivalent to sampling $x$, then successively sampling $a_{1:K} \sim \pihat_{1:K}(\cdot\mid{}x)$, followed by $a_{K+1:2K} \sim \pihat_{K+1:2K}(\cdot\mid{}x,a_{1:K})$ and so forth.

For each $h \in \{K,2K,\dots,H\}$ let $\BP^\star_{h+1-K:h}(\cdot\mid{}x,a_{1:h-K})$ denote the marginal distribution of $a_{h+1-K:h}$ under $(x,a_{1:H}) \sim \BP^\star$ conditioned on $(x,a_{1:h-K})$. Observe that $\pihat_{h+1-K:h}(\cdot\mid{}x,a_{1:h-K})$ is precisely the analogous conditional distribution under $(x,a_{1:H}) \sim \BP^{\pihat}$. Also let $\BP^\star_{:h}$ denote the marginal distribution of $(x, a_{1:h})$ under $(x,a_{1:H}) \sim \BP^\star$, and let $\BP^\star_{:h-K} \circ \pihat_{h+1-K:h}$ denote the distribution of $(x,a_{1:h})$ obtained by sampling $(x,a_{1:h-K}) \sim \BP^\star_{:h-K}$ and then $a_{h+1-K:h} \sim \pihat_{h+1-K:h}(\cdot\mid{}x,a_{1:h-K})$. By \cref{lemma:hell-chain-bound}, we have
\begin{align} 
\Dhels{\BP^\star}{\BP^{\pihat}} 
&\leq 7 \cdot \EE_{(x,a_{1:H}) \sim \BP^\star}\left[\sum_{i=0}^{H/K-1} \Dhels{\BP^\star_{iK+1:(i+1)K}(\cdot\mid{}x,a_{1:iK})}{\pihat_{iK+1:(i+1)K}(\cdot\mid{}x,a_{1:iK})}\right] \\ 
&= 7 \cdot \sum_{i=0}^{H/K-1} \Dhels{\BP^\star_{:(i+1)K}}{\BP^\star_{:iK} \circ \pihat_{iK+1:(i+1)K}}\label{eq:chunk-1}
\end{align}
where the equality is by \cref{lemma:hell-cond}. Now consider the execution of $\ChunkKR$ and fix some particular $h \in \{K,2K,\dots,H\}$. Observe that each $(x\ind{i,h}, a\ind{i}_{h+1-K:h})$ has joint distribution $\BP^\star_{:h}$. We now apply \cref{thm:kern-rho-main} to this data, taking the parameter $H$ in \cref{thm:kern-rho-main} to be $K$. By the theorem assumption on $n$ and the parameter choices in $\ChunkKR$, we get that with probability at least $1-\delta/H$, 
\begin{equation}
\Dhels{\BP^\star_{:h}}{\BP^\star_{:h-K} \circ \pihat_{h+1-K:h}} \leq \frac{88}{3} \min_{\theta \in \Theta} \Dhels{\BP^\star_{:h}}{\BP^\star_{:h-K} \circ \pi_{\theta,h+1-K:h}} + \frac{\epsilon}{H}\label{eq:chunk-2}
\end{equation}
where here $\BP^\star_{:h-K} \circ \pi_{\theta,h+1-K:h}$ is the distribution over $(\MX \times \MA^{h-K}) \times \MA^K$ induced by sampling $(x,a_{1:h-K}) \sim \BP^\star_{:h-K}$ and then autoregressively sampling $a_{h+1-K:h} \sim \pi_\theta(\cdot\mid{}x,a_{1:h-K})$. Condition henceforth on the event that \cref{eq:chunk-2} holds for all $h \in \{K,2K,\dots,H\}$, which occurs with probability at least $1-\delta$. Combining \cref{eq:chunk-1,eq:chunk-2}, we get
\begin{align}
\Dhels{\BP^\st}{\BP^{\pihat}}
&\lesssim \epsilon + \sum_{i=0}^{H/K-1} \min_{\theta\in\Theta} \Dhels{\BP^\st_{:(i+1)K}}{\BP^\st_{:iK} \circ \pi_{\theta,iK+1:(i+1)K}} \\ 
&\leq \epsilon + \min_{\theta\in\Theta} \sum_{i=0}^{H/K-1} \Dhels{\BP^\st_{:(i+1)K}}{\BP^\st_{:iK} \circ \pi_{\theta,iK+1:(i+1)K}} \\ 
&= \epsilon + \min_{\theta\in\Theta} \EE_{(x,a_{1:H})\sim\BP^\st}\left[\sum_{i=0}^{H/K-1} \Dhels{\BP^\st_{iK+1:(i+1)K}(\cdot\mid{}x,a_{1:iK})}{\pi_{\theta,iK+1:(i+1)K}(\cdot\mid{}x,a_{1:iK})}\right] \\ 
&\lesssim \epsilon + \frac{H}{K}\min_{\theta\in\Theta} \Dhels{\BP^\st}{\BP^{\pi_\theta}}
\end{align}
where the equality is by \cref{lemma:hell-cond} and the final inequality is by \cref{lemma:hell-reverse-chain-bound}. Finally, the time complexity bound is immediate from \cref{thm:kern-rho-main}.
\end{proof}

\subsection{Technical Lemmas}

\subsubsection{Information Theory}

\begin{lemma}[{\citet[Lemma D.2]{foster2024online}}]\label{lemma:hell-chain-bound}
Let $n \in \NN$ and let $\MX$ be a set. Let $\BP, \BQ \in \Delta(\MX^n)$. Then 
\[\Dhels{\BP}{\BQ} \leq 7 \cdot \EE_{x \sim \BP}\left[ \sum_{i=1}^n \Dhels{\BP_i(\cdot\mid{}x_{1:i-1})}{\BQ_i(\cdot\mid{}x_{1:i-1})},\right]\]
where $\BP_i(\cdot\mid{}x_{1:i-1})$ is the marginal of $x_i$ under $x \sim \BP$ conditioned on $x_{1:i-1}$, and $\BQ_i(\cdot\mid{}x_{1:i-1})$ is the marginal of $x_i$ under $x \sim \BQ$ conditioned on $x_{1:i-1}$.
\end{lemma}

\begin{lemma}[e.g. {\citet[Proposition 7.5(4)]{polyanskiy2024information}}]\label{lemma:hell-cond}
For any two joint distributions $\BP,\BQ$ over random variables $(X,Y)$,
\[\Dhels{\BP_{X,Y}}{\BP_X\BQ_{Y\mid{}X}} = \EE_{x\sim \BP_X}[\Dhels{\BP_{Y\mid{}X=x}}{\BQ_{Y\mid{}X=x}}].\]
\end{lemma}

\begin{lemma}[{\citet[Lemma A.9]{foster2021statistical}}]\label{lemma:hell-reverse-chain-bound}
For any two joint distributions $\BP,\BQ$ over random variables $(X,Y)$,\loose
\[\Dhels{\BP_{X,Y}}{\BP_X\BQ_{Y\mid{}X}} \leq 4\Dhels{\BP_{X,Y}}{\BQ_{X,Y}}.\]
\end{lemma}

The following bound provides a converse to \cref{lemma:hell-chain-bound}, though it loses a factor of $n$; it follows from applying \cref{lemma:hell-reverse-chain-bound} (in conjunction with \cref{lemma:hell-cond}) and the data processing inequality to individually upper bound each term of the summation by $\Dhels{\BP}{\BQ}$.

\begin{corollary}\label{cor:hell-reverse-chain-bound}
Let $n \in \NN$ and let $\MX$ be a set. Let $\BP, \BQ \in \Delta(\MX^n)$. Then 
\[\EE_{x \sim \BP}\left[ \sum_{i=1}^n \Dhels{\BP_i(\cdot\mid{}x_{1:i-1})}{\BQ_i(\cdot\mid{}x_{1:i-1})}\right] \leq 4n \cdot \Dhels{\BP}{\BQ}\]
where $\BP_i(\cdot\mid{}x_{1:i-1})$ is the marginal of $x_i$ under $x \sim \BP$ conditioned on $x_{1:i-1}$, and $\BQ_i(\cdot\mid{}x_{1:i-1})$ is the marginal of $x_i$ under $x \sim \BQ$ conditioned on $x_{1:i-1}$.
\end{corollary}

\subsubsection{Generalization Theory}

\begin{definition}
\label{def:gaussian_complexity}
For a set $\MX$ and a class $\MF$ of functions $f : \MX \to \RR$, and $n \in \NN$, the \emph{Gaussian complexity} of $\MF$ with respect to samples $x_1, \ldots, x_n \in \MX$ is
\[\MG_n(\MF; x_{1:n}) := \frac{1}{n}\EE_{\xi_{1:n} \sim N(0,1)}\left[\sup_{f\in\MF} \sum_{i=1}^n \xi_i f(x_i) \right].\] 
We write $\MG_n(\MF) = \sup_{x_{1:n}} \MG_n(\MF;x_{1:n})$.
\end{definition}



\begin{lemma}[Composition of Gaussian complexities e.g. {\cite[Lemma B.6]{golowich2024exploring}}]
  \label{lem:rc-composition}
  Let $\MX$ be a set. Fix $A,L \in \NN$ and let $\MF_1, \ldots, \MF_A$ be classes of functions mapping $\MX$ to $\RR$. Let $\phi : \RR^A \to \RR$ be $L$-Lipschitz with respect to the Euclidean distance on $\RR^A$. Let $\MF$ be the class of real-valued functions on $\MX$ defined as follows:\loose
  \begin{align}
\MF := \left\{ x \mapsto \phi(f_1(x), \ldots, f_A(x)) \ : \ f_1 \in \MF_1, \ldots, f_A \in \MF_A \right\}\nonumber.
  \end{align}
  Then for all $n \in \NN$, 
  \begin{align}
\MG_n(\MF) \leq L \sum_{a=1}^A \MG_n(\MF_a)\nonumber.
  \end{align}
\end{lemma}

\begin{lemma}[{\citet[Theorem 26.5]{shalev2014understanding} + \citet[Exercise 5.5]{wainwright2019high}}] \label{lem:unif-conv}
  Suppose $\MX$ is a set and $\MF$ is a class of functions $f : \MX \to [-B, B]$ for some $B > 0$. Suppose $P$ is a distribution on $\MX$. Then for any $n \in \NN$ and $\delta \in (0,1)$, with probability at least $1-\delta$ over an i.i.d.~sample $X_1, \ldots, X_n \sim P$, it holds that\loose
  \begin{align}
\sup_{f \in \MF} \left| \EE_{X \sim P}[f(X)] - \frac 1n \sum_{i=1}^n f(X_i) \right| \leq \sqrt{2\pi}\MG_n(\MF) + 4B\sqrt{\frac{2 \log(4/\delta)}{n}}.\nonumber
  \end{align}
\end{lemma}

\subsubsection{Optimization}

\begin{algorithm}
\caption{$\PGD(\Ovec,\Oproj,T,\eta)$: approximate projected gradient descent}\label{alg:pgd}
\begin{algorithmic}
\Require Vector field oracle $\Ovec: \RR^n \to \RR^n$; projection oracle $\Oproj: \RR^n \to \RR^n$; iteration complexity $T \in \NN$; step size $\eta>0$
\State $x_1 \gets \Oproj(0)$
\For{$1 \leq t \leq T-1$}
    \State $y_{t+1} \gets x_t - \eta \Ovec(x_t)$
    \State $x_{t+1} \gets \Oproj(y_{t+1})$
\EndFor
\State \textbf{return} $(x_t)_{t=1}^T$
\end{algorithmic}
\end{algorithm}

\begin{definition}
Let $\MX \subset \RR^n$ be a compact set. An $\epproj$-approximate projection oracle $\Oproj$ for $\MX$ takes input $y \in \RR^n$ and returns $x \in \MX$ such that
\[\norm{y-x}_2^2 \leq \min_{x' \in \MX} \norm{y-x'}_2^2 + \epproj.\]
\end{definition}

\begin{lemma}\label{lemma:apx-proj-pythag}
Let $\MX \subset \RR^n$ be a convex set, and let $\Oproj$ be an $\epproj$-approximate projection oracle for $\MX$. For any $x \in \MX$ and $y \in \RR^n$, it holds that 
\[\norm{y - x}_2^2 \geq (1-\sqrt{\epproj})\norm{\Oproj(y)-x}_2^2 - \sqrt{\epproj}\]
\end{lemma}

\begin{proof}[\pfref{lemma:apx-proj-pythag}]
For any $\alpha \in (0,1)$, we have $\alpha x + (1-\alpha)\Oproj(y) \in \MX$, so
\[\norm{\Oproj(y) - y}_2^2 \leq \norm{\Oproj(y) - y + \alpha ( x - \Oproj(y))}_2^2 + \epproj.\]
Therefore
\[2\alpha \langle y - \Oproj(y), \Oproj(y) - x\rangle \geq -\alpha^2 \norm{x - \Oproj(y)}_2^2 - \epproj.\]
Setting $\alpha = \sqrt{\epproj}$, we use the above bound to get
\begin{align}
\norm{y-x}_2^2
&= \norm{y-\Oproj(y)}_2^2 + \norm{\Oproj(y)-x}_2^2 + 2\langle y-\Oproj(y), \Oproj(y)-x\rangle \\ 
&\geq (1-\sqrt{\epproj})\norm{\Oproj(y)-x}_2^2 - \sqrt{\epproj}
\end{align}
as claimed.
\end{proof}

\begin{lemma}[Modification of {\cite[Theorem 4.2]{bubeck2015convex}}]\label{lemma:pgd}
Let $L,R \geq 1$ and $T \in \NN$. Let $\MX \subset \RR^n$ be a convex set and let $g: \RR^n \to \RR^n$ be a vector field. Suppose that $\norm{g(x)}_2 \leq L$ for all $x \in \MX$. Let $\Oproj$ be an $\epproj$-approximate projection oracle for $\MX$. If $\epproj \leq 1/(R^2 T^4)$, the iterates $(x_t)_{t=1}^T \gets \PGD(g,\Oproj,T, \frac{R}{L}\sqrt{\frac{2}{T}})$ satisfies, for any $x \in \MX$ with $\norm{x}_2 \leq R$,
\[\frac{1}{T}\sum_{t=1}^T \langle g(x_t), x_t - x\rangle \leq O\left(\frac{RL}{\sqrt{T}}\right).\]
Moreover, $x_t \in \MX$ and $\norm{x_t}_2 \leq O(R\sqrt{T})$ for all $t \in [T]$.
\end{lemma}

\begin{proof}[\pfref{lemma:pgd}]
For notational convenience, write $g_t := g(x_t)$ for each $t \in [T]$. For any $t \in [T]$, we have
\begin{align}
\langle g_t, x_t - x\rangle
&= \frac{1}{\eta} \langle x_t - y_{t+1}, x_t - x \rangle \\ 
&= \frac{1}{2\eta} \left(\norm{x_t-y_{t+1}}_2^2 + \norm{x_t - x}_2^2 - \norm{y_{t+1}-x}_2^2 \right) \\ 
&\leq \frac{1}{2\eta} \left(\norm{x_t-y_{t+1}}_2^2 + \norm{x_t - x}_2^2 - \norm{x_{t+1}-x}_2^2 \right) + \frac{\sqrt{\epproj}(1+\norm{x_{t+1}-x}_2^2)}{2\eta} \\ 
&\leq \frac{1}{2\eta} \left(\norm{x_t - x}_2^2 - \norm{x_{t+1}-x}_2^2 \right) + \frac{\sqrt{\epproj}(1+\norm{x_{t+1}-x}_2^2)}{2\eta} + \frac{\eta L^2}{2}
\end{align}
where the first inequality uses \cref{lemma:apx-proj-pythag} and the second inequality uses that $\norm{x_t - y_{t+1}}_2 = \eta\norm{g_t}_2 \leq \eta L$. Averaging the above bound and telescoping,
\[\frac{1}{T}\sum_{t=1}^T \langle g_t,x_t - x\rangle \leq \frac{\norm{x_1 - x}_2^2}{2\eta T} + \frac{\eta L^2 }{2} + \frac{\sqrt{\epproj}}{2\eta} \max_{t \in [T]} (1 + \norm{x_{t+1} - x}_2^2).\]
For each $t \in [T]$, we have
\begin{align}
\norm{x_{t+1} - x}_2
&\leq (1+2\sqrt{\epproj})\norm{y_{t+1}-x}_2 + \epproj^{1/4} \\ 
&\leq (1+2\sqrt{\epproj})\left(\norm{x_t - x}_2 + \eta L + \epproj^{1/4}\right) \\ 
&\leq e^{2t\sqrt{\epproj}} \norm{x_1-x}_2 + \sum_{s=1}^t e^{2s\sqrt{\epproj}} (\eta L + \epproj^{1/4}) \\ 
&\leq e^2 \norm{x_1 - x}_2 + e^2 T (\eta L + \epproj^{1/4})
\end{align}
where the first inequality is by \cref{lemma:apx-proj-pythag}, and the last inequality is by assumption that $\epproj \leq 1/T^2$. Moreover, again by \cref{lemma:apx-proj-pythag},
\[\norm{x_1 - x}_2 \leq e^{2\sqrt{\epproj}} \norm{x}_2 + \epproj^{1/4}.\]
Since $\norm{x}_2 \leq R$, we conclude that
\[\frac{1}{T}\sum_{t=1}^T \langle g_t,x_t - x\rangle \leq O\left(\frac{R^2+\sqrt{\epproj}}{2\eta T} + \frac{\eta L^2 }{2} + \frac{\sqrt{\epproj}}{2\eta}(1 + R^2 + T^2\eta^2 L^2 + T^2\sqrt{\epproj})\right).\]
Substituting in $\eta = (R/L)\sqrt{2/T}$ and using the assumption that $\epproj \leq 1/(R^2 T^4)$ gives
\[\frac{1}{T}\sum_{t=1}^T \langle g_t,x_t - x\rangle \leq O\left(\frac{LR}{\sqrt{T}}\right).\]
Moreover, $\norm{x_t - x}_2 \leq O(R \sqrt{T})$ as claimed. The fact that $x_t \in \MX$ is by definition of $\Oproj$.
\end{proof}

\begin{theorem}[Modification of {\cite[Theorem 5.1]{bubeck2015convex}}]\label{thm:sp-pgd}
There is a universal constant $C_{\ref{thm:sp-pgd}}>0$ so that the following holds. Let $L,R\geq 1$ and $T \in \NN$. Let $\MX \subset \RR^n$ be a convex set and let $f: \RR^n \times \RR^n \to \RR$ be a function. Suppose that for each $y \in \MX$, $f(\cdot,y)$ is convex and $L$-Lipschitz w.r.t. $\norm{\cdot}_2$ on $\MX$, and that for each $x \in \MX$, $f(x,\cdot)$ is concave and $L$-Lipschitz w.r.t. $\norm{\cdot}_2$ on $\MX$. Define $g(x,y) = (\grad_x f(x,y), -\grad_y f(x,y))$. Let $\Oproj$ be an $(\epproj/2)$-approximate projection oracle for $\MX$ with $\epproj \leq 1/(8R^2 T^4)$. Then $(x_t,y_t)_{t=1}^T \gets \PGD(g, \Oproj \oplus \Oproj, T, \frac{R}{L}\sqrt{\frac{2}{T}})$ satisfies
\begin{equation} \max_{y \in \MX: \norm{y}_2 \leq R} f\left(\frac{1}{T}\sum_{t=1}^T x_t, y\right) - \min_{x \in \MX: \norm{x}_2 \leq R} f\left(x, \frac{1}{T}\sum_{t=1}^T y_t\right) \leq C_{\ref{thm:sp-pgd}} \cdot \left(\frac{LR}{\sqrt{T}}\right)
\label{eq:sp-guarantee}
\end{equation}
and $\frac{1}{T}\sum_{t=1}^T x_t \in \MX$, $\frac{1}{T}\sum_{t=1}^T y_t \in \MX$ with $\norm{\frac{1}{T}\sum_{t=1}^T x_t}_2,\norm{\frac{1}{T}\sum_{t=1}^T y_t}_2 \leq C_{\ref{thm:sp-pgd}} \cdot R\sqrt{T}$.
\end{theorem}

\begin{proof}[\pfref{thm:sp-pgd}]
We apply \cref{lemma:pgd} with set $\MX \times \MX \subset \RR^{2n}$ and vector field $g$. For any $(x,y) \in \MX \times \MX$, we have
\[\norm{g(x,y)}_2^2 = \norm{\grad_x f(x,y)}_2^2 + \norm{\grad_y f(x,y)}_2^2 \leq 2L^2\]
by $L$-Lipschitzness of $f(\cdot,y)$ and $f(x,\cdot)$. Next, observe that the projection oracle $\Oproj \oplus \Oproj$ defined by $(\Oproj \oplus \Oproj)(x,y) := (\Oproj(x),\Oproj(y))$ is a $2\epproj$-approximate projection oracle for $\MX\times\MX$. Thus, \cref{lemma:pgd} gives for any $(x,y) \in \MX\times\MX$ with $\norm{x}_2,\norm{y}_2 \leq R$ that
\[\frac{1}{T}\sum_{t=1}^T \langle \grad_x f(x_t,y_t), x_t - x\rangle - \langle \grad_y f(x_t,y_t), y_t - y\rangle \leq O\left(\frac{RL}{\sqrt{T}}\right).\]
Now for each $t \in [T]$, by convexity of $f(\cdot,y_t)$, we have
\[f(x,y_t) - f(x_t,y_t) \geq \langle \grad_x f(x_t,y_t), x - x_t\rangle.\]
Similarly, by concavity of $f(x_t,\cdot)$,
\[f(x_t, y) - f(x_t,y_t) \leq \langle \grad_y f(x_t,y_t), y - y_t\rangle.\]
Summing, we get
\[f(x_t,y) - f(x,y_t) \leq \langle \grad_x f(x_t,y_t), x_t - x\rangle - \langle \grad_y f(x_t,y_t), y_t - y\rangle.\]
Finally, convexity of $f(\cdot,y)$ and concavity of $f(x,\cdot)$ gives
\begin{align}
f\left(\frac{1}{T}\sum_{t=1}^T x_t,y\right) - f\left(x,\frac{1}{T}\sum_{t=1}^T y_t\right) 
&\leq \frac{1}{T}\sum_{t=1}^T f(x_t,y) - f(x,y_t) \\ 
&\leq \frac{1}{T}\sum_{t=1}^T \langle \grad_x f(x_t,y_t), x_t - x\rangle - \langle \grad_y f(x_t,y_t), y_t - y\rangle \\
&\leq O\left(\frac{RL}{\sqrt{T}}\right).
\end{align}
Since this bound holds for all $x,y \in \MX$ with $\norm{x}_2,\norm{y}_2 \leq R$, we have proven \cref{eq:sp-guarantee}. The containments $\frac{1}{T}\sum_{t=1}^T x_t \in \MX$, $\frac{1}{T}\sum_{t=1}^T y_t \in \MX$ and norm bounds $\norm{\frac{1}{T}\sum_{t=1}^T x_t}_2,\norm{\frac{1}{T}\sum_{t=1}^T y_t}_2 \leq C_{\ref{thm:sp-pgd}} \cdot R\sqrt{T}$ are immediate from convexity of $\MX$ and the guarantee of \cref{lemma:pgd} that $(x_t,y_t) \in \MX\times\MX$ with $\norm{(x_t,y_t)}_2^2 \leq O(R\sqrt{T})$ for all $t \in [T]$.\loose
\end{proof}
