


In this section we prove \cref{thm:comp-lb-main}, which asserts that learning misspecified autoregressive linear models with optimal approximation ratio requires super-polynomial time under sub-exponential hardness of the \emph{Learning Parities with Noise (LPN)} problem. In \cref{subsec:comp-lb-setting}, we formally describe the problem setting and restate the theorem. In \cref{subsec:comp-lb-overview}, we give a proof overview, expanding on the overview given in \cref{sec:comp-lb}, and introduce relevant notation. In \cref{subsec:generating-samples,subsec:bounding-misspec,subsec:policy-to-parity} we put together the key lemmas for the proof, and in \cref{subsec:comp-lb-proof} we complete the proof.

\subsection{Formal Problem Setting and Theorem Statement}\label{subsec:comp-lb-setting}

\paragraph{Problem setting} A learning algorithm $\Alg$ for (misspecified) autoregressive linear models operates in the following computational framework. Let $\MX,\MA$ be sets with $|\MA|<\infty$, and let $d,H \in \NN$. Let $M$ be the $H$-step autoregressive MDP with context space $\MX$, action space $\MA$, and some initial context distribution $\MD \in \Delta(\MX)$. Let $\phi: \MX\times\MA^\st \to \RR^d$ be a $d$-dimensional feature mapping, and let $\Theta \subset \RR^d$ be a convex parameter set. Let $(o\ind{i})_{i=1}^T$ be trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$. The algorithm $\Alg$ receives input $((o\ind{i})_{i=1}^T, \MA, \epsilon)$, and it has access to the following computational oracles:
\begin{enumerate}
\item Given $h \in \{0,\dots,H\}$ and $(x,a_{1:h}) \in \MX\times\MA^h$, query $\phi(x,a_{1:h})$.
\item Given $\theta \in \RR^d$, query $\Proj_\Theta(\theta) = \argmin_{\theta'\in\Theta} \norm{\theta-\theta'}_2$.
\end{enumerate}
The algorithm $\Alg$ is required to output a policy $\pihat = (\pihat_h)_{h=1}^H$ where each $\pihat_h$ is represented as a circuit $\MC_{\pihat_h}$: that is, given $(x,a_{1:h-1}) \in \MX\times\MA^{h-1}$, the distribution of $\MC_{\pihat_h}(x,a_{1:h-1},r)$ for independent randomness $r$ is $\pihat_h(\cdot\mid{}x,a_{1:h-1})$. Note that the feature map $\phi$ is not explicitly specified to the learner in this framework, and must be accessed through querying the first oracle above.

\paragraph{Hardness assumption: Learning Parities with Noise (LPN)}
We define the noisy parity distribution with noise level $\eta$ as follows.
\begin{definition}[Noisy parity distribution]
  \label{def:parity}
Fix $n \in \NN$, $S\subseteq [n]$, and $\eta \in (0, 1/2)$. We let $\BP^\st_{n,S,\eta}$ denote the distribution of $(x,y)$ where $x \sim \Unif(\{-1,1\}^n)$ and $y = (-1)^\xi \prod_{i\in S} x_i$ for an independent random variable $\xi \sim \Ber(1/2-\eta)$. We further let $\BQ^\st_n$ denote the distribution of $(x,y)$ where $x\sim \Unif(\{-1,1\}^n)$ and $y \sim \Unif(\{-1,1\})$ are independent.
\end{definition}

The following assumption asserts that it requires near-exponential time to distinguish between samples from $\BP^\st_{n,S,\eta}$ and $\BQ^\st_{n}$ for an unknown set $S \subseteq [n]$.

\begin{assumption}[Sub-exponential hardness of decisional LPN]\label{assumption:lpn}
Fix any constant $\eta>0$. Suppose that $\Alg^\MD$ is an algorithm that takes as input a sampling oracle for a distribution $\MD \in \Delta(\{-1,1\}^n \times \{-1,1\})$ and produces an output in $\{0,1\}$. Suppose that the following guarantees hold:
\begin{itemize}
\item For any $S \subseteq [n]$, $\EE[\Alg^{\BP^\st_{n,S,1,\eta}}] \geq 5/8$.
\item $\EE[\Alg^{\BQ_{n,1}}] \leq 1/2$.
\end{itemize}
Then $\Alg^\MD$ has time complexity $2^{n^{1-o(1)}}$.
\end{assumption}

While \cref{assumption:lpn} is phrased in terms of a decision task, this task is polynomial-time equivalent to the task of \emph{learning} noisy parities, via standard boosting and self-reducibility arguments. The conjectural computational hardness of LPN has seen extensive use in cryptography \citep{alekhnovich2003more,applebaum2009fast,pietrzak2012cryptography} and learning theory \citep{kearns1994learnability,mossel2005learning,golowich2024exploration}.  While $2^{n^{1-o(1)}}$-hardness (as opposed to, say, hardness for \emph{some} sub-exponential function) is a stronger assumption than what is used in many of these works, the fastest known algorithm for LPN has time complexity $2^{O(n/\log n)}$ \citep{blum2003noise}.  For further discussion, see e.g. \cite{yu2021smoothing} and references therein.\loose

Under \cref{assumption:lpn}, we show that efficiently learning misspecified autoregressive linear models inherently leads to error amplification.

\begin{theorem}[Restatement of \cref{thm:comp-lb-main}]\label{thm:comp-lb-app}
Fix any $c,C>0$ and let $\Alg$ be a learning algorithm for autoregressive linear models with the following guarantee. Suppose $|\MA|=2$ and \cref{ass:linear-norm-bounds-main} holds with parameters $B=\sqrt{d}$ and $\Bdot=1$; then for any policy $\pistar$, if $(o\ind{i})_{i=1}^n$ are $T = (dH/\epsilon)^C$ i.i.d. trajectories from $\BP^{\pistar}$, the time complexity of $\Alg((o\ind{i})_{i=1}^T,\MA,\epsilon)$ is $O(T)$\footnote{Note that $\Alg$ is not required to read the entire input. It would be equivalent to allow for time complexity $\poly(T)$, or to give $\Alg$ a sampling oracle for $\BP^{\pistar}$.} and the output $\pihat$ satisfies, with probability at least $9/10$,
\[\Dhels{\BP^{\pihat}}{\BP^{\pistar}} \lesssim \epsilon + e^{(\log \max(d,H))^{1-c}} \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}.\]
Then \cref{assumption:lpn} is false.
\end{theorem}

It is straightforward to check from the proof that, under the weaker assumption of $2^{\sqrt{n}+\epsilon}$-hardness of LPN \citep{yu2019collision,yu2021smoothing} for a given $\eps>)$, there exists some constant $c = c(\epsilon)>0$ such that $\Capx \leq e^{(\log \max(d,H))^c}$ is impossible for any computationally efficient learner. We remark that \cref{thm:comp-lb-app} does not apply if the learner has access to \densobs; resolving this (either with an efficient algorithm or an improved lower bound) is an interesting open problem.

\subsection{Proof Overview and Definitions}\label{subsec:comp-lb-overview}

The proof of \cref{thm:comp-lb-main} is inspired by the main result of \cite{diakonikolas2022hardness}. When translated to our setting, their results essentially show that for $H=1$ and large $B$, any computationally efficient agnostic learner must pay super-constant misspecification factor. The main idea of \cite{diakonikolas2022hardness} is to consider the problem of learning a noisy parity function over the uniform distribution. In the notation of autoregressive sequence modelling (with horizon $1$), the context space is $\MX := \{-1,1\}^n$ and the action space $\MA := \{-1,1\}$. The features $\phi$ are defined by the degree-$t$ Veronese map, i.e. all $d = O(n^t)$ degree-$\leq t$ monomials on the context $x \in \MX$. While polynomial approximation of an $n$-variable parity function uniformly on its domain would require degree nearly $n$, a concentration argument shows that there is a degree-$t=\tilde{O}(\sqrt{n})$ polynomial that approximates the parity function on \emph{most} of the domain. Hence, with a feature mapping of dimension $d=n^{\tilde{O}(\sqrt{n})}$, the policy class has small misspecification. Since learning noisy parities is believed to require $\exp(n^{1-o(1)})$ time, this rules out a polynomial-time algorithm for the agnostic learning problem.

Unfortunately, the polynomial approximation argument requires the policy class to have very large norm bound (concretely, $B \sim \poly(d)$). In our setting, we are interested in how the misspecification factor scales with $H$ when $B = O(1)$. Since $\loglossbc$ achieves $\Capx = O(B)$ in a computationally efficient manner when $H, |\MA| = O(1)$ (\cref{cor:linear-misspec-logloss}), deriving a computational lower bound in our setting fundamentally requires exploiting the long horizon.

The main new technical ingredient in our proof is the observation that decreasing the signal strength in the noisy parity distribution (i.e. sending the noise level $\eta$ towards $1/2$) correspondingly decreases the norm of the polynomial approximator. In particular, if $\eta \geq 1/2- 1/\poly(d)$, then we can set $B = O(1)$. Of course, when the noise is so close to uniform, an agnostic learner could achieve small Hellinger distance without learning the parity function, by simply outputting the uniform distribution. This is where we use the long horizon to ``boost'' the signal: instead of trying to learn the distribution of $(x, a)$ where $x \sim \Unif(\{-1,1\}^n)$ and $a = (-1)^\xi \prod_{i \in S} x_i$ for noise $\xi \sim \Ber(\eta)$ and parity set $S \subseteq [n]$, we try to learn the distribution of $(x,a_1,\dots,a_H)$ where $a_h = (-1)^{\xi_h} \prod_{i \in S} x_i$ for independent random variables $\xi_1,\dots,\xi_H \sim \Ber(\eta)$. For $H \sim 1/\eta^2$, the effective signal strength is constant, and since $\eta$ is small for each $h$, each conditional distribution $a_h\mid{}x$ admits a low-norm polynomial approximation.

\paragraph{Construction of hard instance} Formally, for parameters $n,H \in \NN$
, we let $M = M_{n,H}$ be the $H$-step autoregressive MDP with context space $\{-1,1\}^n$, initial context distribution $\Unif(\{-1,1\}^n)$, action space $\{-1,1\}$, and horizon $H$. We define a policy class $\Pi$ consisting of autoregressive linear models, where the features are monomials in the initial context $x$.



\begin{definition}[Policy class]\label{def:comp-lb-policy-class}
Let $n,t \in \NN$ and set $d := \sum_{i=0}^t \binom{n}{i}$. Identify $[d]$ with the collection of all subsets of $[n]$ of size at most $t$. We define a feature map $\phiver_{n,t}: \{-1,1\}^n \times \{-1,1\}^\st \to \RR^d$ by 
\[\phiver_{n,t}(x,a_{1:h})_T := a_h \prod_{i \in T} x_i\]
for each $T \subseteq [n]$ with $|T| \leq t$. Let $\Theta = \{\theta \in \RR^d:\norm{\theta}_1 \leq 1\}$. We then define $\Pi_{n,t,H} := \{\pi_\theta:\theta\in\Theta\}$ where $\pi_\theta = (\pi_{\theta,h})_{h=1}^H$ is defined by
\begin{equation}
  \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) := \frac{\exp(\langle \theta,
    \phiver_{n,t}(x,a_{1:h})\rangle)}{\sum_{a'_h \in \MA}
    \exp(\langle \theta,\phiver_{n,t}(x,a_{1:h-1},a'_h)\rangle)}.
    \label{eq:comp-lb-density}
\end{equation}
\end{definition}

Next, we define the family of possible expert policies that our data may be generated by, which is parametrized by an unknown subset $S \subseteq [n]$.\footnote{Note that rather than defining the conditional distributions $\pistar_h(a_h\mid{}x,a_{1:h-1})$ for each $h$, we are directly defining the conditional distribution $a_{1:H}\mid{} x$ that would be generated autoregressively in $M$ under $\pistar$; however, this is equivalent.}

\begin{definition}[Noisy parity policies]
  \label{def:parity_policy}
Let $n,H \in \NN$. Let $\eta \in [0,1/2)$ and $S \subseteq [n]$. For $x \in \{-1,1\}^n$, we define $\pi^\st_{n,S,\eta,H}: \{-1,1\}^n \to \Delta(\{-1,1\}^H)$ so that $\pi^\st_{n,S,\eta,H}(\cdot\mid{}x)$ is the distribution of $\left((-1)^{\xi_h}\prod_{j \in S} x_j\right)_{h=1}^H$, where $\xi_1,\dots,\xi_H \sim \Ber(1/2-\eta)$ are independent.
\end{definition}

We also introduce the following notation for the trajectory distribution induced in $M$ by a noisy parity policy $\pistar_{n,S,\eta,H}$ (note that it corresponds to drawing $x \sim \Unif(\MX)$ and then $y \sim \pistar_{n,S,\eta,H}(\cdot\mid{}x)$).


\begin{definition}
Let $n,H \in \NN$ and $\eta \in [0,1/2)$. Let $S \subseteq [n]$. Then we define $\BP^\st_{n,S,H,\eta} := \BP^{\pi^\st_{n,S,\eta,H}}$. We also define $\BQ_{n,H} := \BP^\st_{n,S,H,0}$ (notice that the latter distribution does not depend on $S$).
\end{definition}

For example, $\BP^\st_{n,S,1,\eta}$ is the same as the noisy parity distribution $\BP^\st_{n,S,\eta}$ in \cref{def:parity}, and similarly $\BQ_{n,1} = \BQ_n$. \loose

With this notation, our goal is to show that an autoregressive learning algorithm with the guarantees specified in \cref{thm:comp-lb-app} enables learning the set $S$ from samples, and that this violates \cref{assumption:lpn}. To this end, there are three pieces to the proof. First, we show (\cref{subsec:generating-samples}) that given \emph{standard} LPN samples, i.e. samples from $\BP^\st_{n,S,\gamma}$ for some constant $\gamma \in (0,1/2)$ and unknown set $S \subseteq [n]$, one can efficiently generate samples from $\BP^\star_{n,S,H,\eta}$, so long as $\eta \ll 1/\sqrt{H}$. Second, we show (\cref{subsec:bounding-misspec}) that for any $S \subseteq [n]$, the joint distribution $\BP^\star_{n,S,H,\eta}$ has small misspecification (in Hellinger distance) with respect to $\Pi$. Third, we show
(\cref{subsec:policy-to-parity}) that learning a policy with small Hellinger distance to $\BP^\star_{n,S,H,\eta}$ enables recovering $S$.

\subsection{Step 1: Generating Samples}\label{subsec:generating-samples}


We start by showing that, given a sample from $\BP^\star_{n,S,1,1/4}$ for some unknown set $S \subseteq [n]$, we can efficiently generate a sample from a distribution close to $\BP^\star_{n,S,H,\eta}$, where $\eta = \gamma/(C\sqrt{H})$ for some constant $C$ and parameter $\gamma \in (0,1)$ that we will choose later. Essentially, the signal can be efficiently ``spread out'' across the $H$ steps (\cref{lemma:spread-signal}).

The following lemma is crucial to this reduction: it shows that given a noisy measurement of some bit $b \in \{-1,1\}$, and two distributions $p(-1)$ and $p(1)$, if $p(-1)$ and $p(1)$ have bounded density ratios, then it is possible to generate a sample from $p(b)$ (despite not observing $b$ directly).

\begin{lemma}\label{lemma:factor-bsc}
There is a polynomial-time algorithm $\Alg_{\ref{lemma:factor-bsc}}$ with the following property. Let $H \in \NN$, $\eta \in (0,1/2)$, and $p: \{-1,1\} \to \Delta([H])$. Suppose that \[\max\left\{\norm{\frac{p(1)}{p(-1)}}_\infty, \norm{\frac{p(-1)}{p(1)}}_\infty\right\} \leq \frac{1-\eta}{\eta}\]
where we define $0/0 = 1$. Then for any fixed $b \in \{-1,1\}$, for $\xi \sim \Ber(\eta)$, the output of $\Alg_{\ref{lemma:factor-bsc}}(p,(-1)^\xi b,\eta)$ has marginal distribution $p(b)$. %
\end{lemma}

\begin{proof}[\pfref{lemma:factor-bsc}]
Let $P \in \RR^{H \times 2}$ be the matrix with columns $p(-1), p(1) \in \RR^H$. Define the matrix
\[Q := P \begin{bmatrix} \frac{1-\eta}{1-2\eta} & -\frac{\eta}{1-2\eta} \\ -\frac{\eta}{1-2\eta} & \frac{1-\eta}{1-2\eta} \end{bmatrix}.\]
Observe that \[\sum_{i=1}^n Q_{i1} = \sum_{i=1}^H \frac{1-\eta}{1-2\eta} P_{i1} - \frac{\eta}{1-2\eta} P_{i2} = 1\] where the final equality uses that $\sum_{i=1}^H P_{i1} = \sum_{i=1}^H P_{i2} = 1$. Moreover, for each $i \in [H]$,
$Q_{i1} \geq 0$ by assumption that $\norm{p(1)/p(-1)}_\infty \leq (1-\eta)/\eta$. Thus, the first column of $Q$ represents a distribution $q(-1)$ over $[H]$. Similarly, the second column represents a distribution $q(1)$ over $[H]$. On input $(p,b')$, we define the algorithm $\Alg_{\ref{lemma:factor-bsc}}(p,b',\eta)$ to sample and output $x \sim q(b')$.

Now observe that the marginal distribution of $x =\Alg_{\ref{lemma:factor-bsc}}(p,(-1)^\xi b,\eta)$ when $b=-1$ and $\xi\sim\Ber(\eta)$ is 
\[Q \begin{bmatrix} 1-\eta \\ \eta \end{bmatrix} = P \begin{bmatrix} \frac{1-\eta}{1-2\eta} & -\frac{\eta}{1-2\eta} \\ -\frac{\eta}{1-2\eta} & \frac{1-\eta}{1-2\eta} \end{bmatrix} \begin{bmatrix} 1-\eta & \eta \\ \eta & 1-\eta \end{bmatrix} e_1 = p(-1).\]
Similarly, the marginal distribution of $x$ when $b=1$ is exactly $p(1)$.
\end{proof}

We now construct the desired reduction. Given a sample $(x,y)$ from $\BP^\star_{n,S,1,1/4}$, note that $y$ is a noisy measurement of $\prod_{i \in S} x_i$; we would like to produce $H$ independent samples from the distribution on $\{-1,1\}$ with bias $1/2 + \eta\prod_{i \in S} x_i$. If we could produce a sample $k$ from the binomial distribution $\Bin(H, 1/2 + \eta\prod_{i \in S} x_i)$, then we would be done since we could output $(x, a_{1:H})$ where $(a_1,\dots,a_H)$ is a uniformly random string in $\{-1,1\}^H$ subject to the constraint of containing $k$ ones. Unfortunately, the density ratio between $\Bin(H, 1/2 + \eta)$ and $\Bin(H, 1/2 - \eta)$ is not bounded unless $\eta = O(1/H)$, so we cannot directly apply \cref{lemma:factor-bsc}. Instead, we truncate the binomial distributions to the range $[H/2 - \tilde{O}(\sqrt{H}), H/2 + \tilde{O}(\sqrt{H})]$. The resulting distributions have bounded density ratios, and the truncation introduces negligible error, so long as $\eta = \tilde{O}(1/\sqrt{H})$.

\begin{lemma}\label{lemma:spread-signal}
There are universal constants $c_{\ref{lemma:spread-signal}}, C_{\ref{lemma:spread-signal}} > 0$ and a polynomial-time algorithm $\Alg_{\ref{lemma:spread-signal}}$ with the following property. Let $n,H \in \NN$ and $\gamma \in (0,1/2)$. For any $S \subseteq [n]$, for $(x,y) \sim \BP^\star_{n,S,1,1/4}$, the output of $\Alg_{\ref{lemma:spread-signal}}(x,y,\gamma,H)$ has marginal distribution $\mu$ satisfying \[\Dtv{\BP^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}}{\mu} \leq 2\exp(-c_{\ref{lemma:spread-signal}}/\gamma^2).\]
\end{lemma}

\begin{proof}[\pfref{lemma:spread-signal}]
For notational convenience, let $f(n,k,\delta)$ denote the mass of the distribution $\Bin(n,\delta)$ at $k \in \{0,\dots,n\}$.  On input $(x,y,\gamma)$, the algorithm $\Alg_{\ref{lemma:spread-signal}}(x,y,\gamma,H)$ computes the function $p: \{-1,1\} \to \Delta(\{0,\dots,H\})$ where 
\begin{align}
p(-1)_k &\propto f\left(H, k, \frac{1}{2}-\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}}\right) \mathbbm{1}\left[\frac{H}{2} - \frac{\sqrt{H}}{\gamma} \leq k \leq \frac{H}{2} + \frac{\sqrt{H}}{\gamma}\right], \\ 
p(1)_k &\propto f\left(H, k, \frac{1}{2}+\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}}\right) \mathbbm{1}\left[\frac{H}{2} - \frac{\sqrt{H}}{\gamma} \leq k \leq \frac{H}{2} + \frac{\sqrt{H}}{\gamma}\right].
\end{align}
It then computes $K := \Alg_{\ref{lemma:factor-bsc}}(p, y, 1/4)$ (cf. \cref{lemma:factor-bsc}) and outputs $(x,y')$ where $y' \in \{-1,1\}^H$ is uniformly random subject to the constraint $|\{h \in [H]: y'_h = 1\}| = K$.

We now analyze the algorithm. Observe that 
\begin{align}
&\Prr_{X \sim \Bin(H, \frac{1}{2}-\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}})}\left[\frac{H}{2} - \frac{\sqrt{H}}{\gamma} \leq X \leq \frac{H}{2} + \frac{\sqrt{H}}{\gamma}\right] \\
&= \Prr_{Y \sim \Bin(H, \frac{1}{2}+\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}})}\left[\frac{H}{2} - \frac{\sqrt{H}}{\gamma} \leq Y \leq \frac{H}{2} + \frac{\sqrt{H}}{\gamma}\right]
\end{align}
because $H-X$ and $Y$ are identically distributed. Thus, for any $k \in \{0,\dots,n\}$, we have either $p(-1)_k = p(1)_k = 0$ or else $|H/2 - k| \leq \sqrt{H}/\gamma$ and hence
\[\frac{p(-1)_k}{p(1)_k} = \frac{f\left(H, k, \frac{1}{2}-\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}}\right)}{f\left(H, k, \frac{1}{2}+\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}}\right)} = \left(\frac{1-2\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}{1+2\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}\right)^{2k-H} \leq 2\]
so long as $C_{\ref{lemma:spread-signal}}>0$ is a sufficiently large constant. Thus, $\norm{p(-1)/p(1)}_\infty \leq 2$. Similarly, $\norm{p(1)/p(-1)}_\infty \leq 2$. 

Condition on $x$. We have $y = (-1)^\xi \prod_{i \in S} x_i$ where $\xi \sim \Ber(1/4)$. It follows from \cref{lemma:factor-bsc} and the preceding bounds that $K$ has distribution $p(\prod_{i \in S} x_i)$. If $K$ had distribution $\Bin(H, 1/2+\gamma\prod_{i \in S} x_i/(C_{\ref{lemma:spread-signal}}\sqrt{H}))$ (i.e., if we did not truncate), then $y'$ would have distribution exactly $\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}(\cdot\mid{}x)$ (cf. \cref{def:parity_policy}), and thus $(x,y')$ would have distribution exactly $\BP^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}$. We bound the error induced by truncation as follows. By the data processing inequality, we have that
\begin{align}
&\Dtv{\Law(x,y')}{\BP^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}} \\
&= \EE_{x\sim \Unif(\{-1,1\}^n)} \Dtv{\Law(y'\mid{}x)}{\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}(\cdot\mid{}x)} \\ 
&\leq \EE_{x\sim \Unif(\{-1,1\}^n)} \Dtv{p\left(\prod_{i \in S} x_i\right)}{\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}(\cdot\mid{}x)} \\ 
&\leq 1 - \Prr_{Z \sim \Bin(H, \frac{1}{2}-\frac{\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}})}\left[\frac{H}{2} - \frac{\sqrt{H}}{\gamma} \leq Z \leq \frac{H}{2} + \frac{\sqrt{H}}{\gamma}\right] \\ 
&\leq 2\exp(-c_{\ref{lemma:spread-signal}}/\gamma^2)
\end{align}
where the final inequality is by a Chernoff bound, and holds so long as $c_{\ref{lemma:spread-signal}}>0$ is sufficiently small.
\end{proof}

\subsection{Step 2: Bounding the Misspecification}\label{subsec:bounding-misspec}

Next, we argue that $\BP^\star_{n,S,H,\eta}$ is close in Hellinger distance to some policy in the class $\Pi_{n,t,H}$ (\cref{def:comp-lb-policy-class}), so long as $\eta \leq n^{-\omega(\sqrt{n})}$ (i.e. the noise is sufficiently close to uniform) and $t = \omega(\sqrt{n})$ (i.e. the policy class is sufficiently rich). See \cref{lemma:bic-hell} for the formal statement. Except for the 
choice of $\eta$ to be small, the proof closely follows the analogous arguments in \cite{diakonikolas2022hardness}. %

\begin{lemma}\label{lemma:polynomial-interpolation}
Let $t,K \in \NN$. There is a degree-$t$ polynomial $f: \RR \to \RR$ such that:
\begin{itemize}
\item $f(k) = (-1)^{\frac{K-k}{2}}$ for all integers $k \in [-t,t]$ with $k \equiv K \pmod{2}$
\item $\sum_{i=0}^{t} |a_i| \leq O(t^3)$ where $a_0,\dots,a_{t+1}$ are the coefficients of $f$.
\end{itemize}
\end{lemma}

\begin{proof}[\pfref{lemma:polynomial-interpolation}]
For notational convenience, let $T$ denote the set of integers $k \in [-t,t]$ with $k \equiv K \pmod{2}$. Define $f:\RR \to \RR$ by
\[f(x) = \sum_{k \in T} (-1)^k \frac{\prod_{j \in T \setminus \{k\}} (x - j)}{\prod_{j \in T \setminus \{k\}} (k-j)}.\]
Since $|T| \leq t+1$, it is clear that $f$ has degree at most $t$. It is also clear that $f(k) = (-1)^{\frac{K-k}{2}}$ for all $k \in T$. 

Suppose $K$ is even, so without loss of generality $t$ is even. For any $k \in [T]$, the polynomial $\prod_{j \in T \setminus \{k\}} (x-j)$ has $\ell_1$ coefficient norm at most $\prod_{j \in T \setminus \{k\}} (1 + |j|) \leq ((t+1)!!)^2$. Moreover, $\prod_{j \in T \setminus \{k\}} |k-j| \geq \prod_{j \in T \setminus \{0\}} |j| = (t!!)^2$. It follows that the coefficient norm of $f$ is at most $|T| \cdot (t+1)^2 = (t+1)^3$. Now suppose $K$ is odd, so without loss of generality $t$ is odd. For any $k \in [T]$, the polynomial $\prod_{j \in T \setminus \{k\}} (x-j)$ has coefficient norm at most $\prod_{j \in T \setminus \{k\}} (1 + |j|) \leq ((t+1)!!)^2$. Moreover, $\prod_{j \in T \setminus \{k\}} |k-j| \geq (t-1)!!(t+1)!!$. Thus the coefficient norm of $f$ is at most $|T| \cdot (t+1) = (t+1)^2$.
\end{proof}



\begin{lemma}\label{lemma:bic-hell}
There is a universal constant $c_{\ref{lemma:bic-hell}}$ so that the following holds. Let $n,t,H \in \NN$ and $\eta>0$. Let $S \subseteq [n]$. If $\eta < c_{\ref{lemma:bic-hell}} t^{-4} n^{-t}$, then
\[\min_{\pi \in \Pi_{n,t,H}} \Dhels{\BP^\st_{n,S,H,\eta}}{\BP^\pi} \leq 4\exp(-t^2/(2n)).\]
\end{lemma}

\begin{proof}[\pfref{lemma:bic-hell}]
By \cref{lemma:polynomial-interpolation}, there is a polynomial $f: \RR \to \RR$ with $f(k) = (-1)^{\frac{|S|-k}{2}}$ for all integers $k \in [-t,t]$ with $k \equiv |S| \pmod{2}$, and with $\ell_1$ coefficient norm $O(t^3)$. Define $g: \RR^n \to \RR$ by $g(x) = f(\sum_{i \in S} x_i)$. For any $x \in \{-1,1\}^n$ with $|\sum_{i \in S} x_i| \leq t$, we have $\sum_{i \in S} x_i \equiv |S| \pmod{2}$, so 
\[g(x) = (-1)^{\sum_{i \in S} \frac{1-x_i}{2}} = (-1)^{\#\{i \in S: x_i = -1\}} = \prod_{i \in S} x_i.\]
Moreover, $g$ can be represented as a degree-$t$ polynomial in $x_1,\dots,x_n$, with $\ell_1$ coefficient norm at most $O(t^4 n^t)$. Let $\tilde g$ be the square-free reduction of $g$ on $\{-1,1\}^n$, and let $(c_T)_T$ be the coefficient vector of $\tilde g$. Define $\theta \in \RR^d$ by $\theta_T := \alpha c_T$ where $\alpha := \frac{1}{2}\log(\frac{1+2\eta}{1-2\eta})$. Observe that $\norm{\theta}_1 \leq \alpha \cdot O(t^4 K^t) \leq O(\eta t^4 n^t) \leq 1$ by assumption that $\eta \leq c_{\ref{lemma:bic-hell}} t^{-4} n^{-t}$, so long as $c_{\ref{lemma:bic-hell}} > 0$ is a sufficiently small universal constant. Thus, $\theta \in \Theta$, and so the policy $\pi_\theta$ defined in \cref{eq:comp-lb-density} lies in $\Pi_{n,t,H}$. Moreover, for any $h \in [H]$ and $(x,a_{1:h}) \in \{-1,1\}^n \times \{-1,1\}^h$, we have by \eqref{eq:comp-lb-density} that
\begin{align}
\pi_\theta(a_h\mid{}x,a_{1:h-1})
&= \frac{\exp\left(\sum_{|T| \leq t} \alpha c_T \phiver_{n,t}(x,a_{1:h})_T\right)}{\sum_{a'_h \in \{-1,1\}}\exp\left(\sum_{|T| \leq t} \alpha c_T \phiver_{n,t}(x,a_{1:h-1},a'_h)_T\right)} \\
&= \frac{\exp\left(\alpha a_h g(x)\right)}{\sum_{a'_h \in \{-1,1\}}\exp\left(\alpha a'_h g(x)\right)} \\ 
&= \frac{1}{1 + \exp(-2\alpha a_h g(x))}.
\end{align}
If $|\sum_{i \in S} x_i| \leq t$, then in particular we have
\begin{align}
\pi_\theta\left(\prod_{i \in S} x_i \,\middle|\,x,a_{1:h-1}\right)
= \frac{1}{1 + \exp(-2\alpha)}
= \frac{1}{2} + \eta
\end{align}
by choice of $\alpha$. Thus, $\BP^{\pi_\theta}(\cdot\mid{}x)$ and $\BP^\st_{n,S,H,\eta}(\cdot\mid{}x)$ are identical (as distributions over $\{-1,1\}^H$) for any $x \in \{-1,1\}^n$ such that $|\sum_{i \in S} x_i| \leq t$. We conclude that
\begin{align}
\Dhels{\BP^{\pi_\theta}}{\BP^\st_{n,S,H,\eta}}
&= \EE_{x \sim \Unif(\{-1,1\}^n)} [\Dhels{\BP^{\pi_\theta}(\cdot|x)}{\BP^\st_{n,S,H,\eta}(\cdot|x)} ]\\ 
&\leq 2 \Prr_{x \sim \Unif(\{-1,1\}^n)}\left[\left|\sum_{i \in S} x_i\right| > t\right] \\ 
&\leq 4\exp\left(-\frac{t^2}{2n}\right)
\end{align}
by Hoeffding's inequality. This completes the proof.
\end{proof}

\subsection{Step 3: From Policies to Parity Functions}\label{subsec:policy-to-parity}

Next, we show that for any policy $\pihat$ such that $\BP^{\pihat}$ is close in Hellinger distance to the true distribution over trajectories, if we can sample from the conditional distribution $\pihat(\cdot\mid{}x) \in \Delta(\{-1,1\}^H)$ for any given $x \in \{-1,1\}^n$, then we can predict the parity function $\prod_{i \in S} x_i$. The idea is to sample multiple times from $\pihat(\cdot\mid{}x)$ and take majority; each individual trajectory gives a fairly weak signal since $\eta \sqrt{H}$ is sub-constant, but after boosting, the predictor has low error.


\begin{lemma}\label{lemma:hellinger-to-secret}
There is an algorithm $\Alg_{\ref{lemma:hellinger-to-secret}}$ with the following property. Let $n, H \in \NN$ and $S \subseteq [n]$. Given access to a conditional sampling oracle $\wh\MO$ for a policy $\pihat: \{-1,1\}^n \to \{-1,1\}^H$ and inputs $x \in \{-1,1\}^n$ and $\delta,\gamma>0$, it holds that
\[\Prr\left[\Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}(X,\delta,\gamma) \neq \prod_{i \in S} X_i\right] \leq \delta + \frac{6C_{\ref{lemma:spread-signal}}\log(1/\delta)}{\gamma^2} \sqrt{\Dhels{\BP^{\pihat}}{\BP^{\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}}}}\]
where the probability is over $X \sim \Unif(\{-1,1\}^n)$. Moreover, the time complexity of $\Alg^{\wh{\cO}}_{\ref{lemma:hellinger-to-secret}}$ is $\poly(n,H,\gamma^{-1},\log(1/\delta))$.
\end{lemma}

\begin{proof}[\pfref{lemma:hellinger-to-secret}]
Fix a realization $X = x \in \{-1,1\}^n$. The algorithm $\Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}$ does the following on input $(x,\delta,\gamma)$. Set $N := 6C_{\ref{lemma:spread-signal}}^2\gamma^{-2}\log(1/\delta).$ The algorithm draws $N$ independent samples $y^{(1)},\dots,y^{(N)} \sim \pihat(\cdot\mid{}x)$, and outputs 
\[\Maj \{y^{(i)}_j: i \in [N], j \in [H]\}.\]
Let $\MO$ be the conditional sampling oracle for $\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}$. Then in the execution of $\Alg_{\ref{lemma:hellinger-to-secret}}^{\MO}(x,\delta,\gamma)$, we have that $(y^{(i)}_j: i \in [N], j \in [H])$ are $NH$ independent and identically distributed random variables with $\Pr[y^{(i)}_j \neq \prod_{i \in S} x_i] = 1/2 - \gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})$. It follows that
\begin{align}
\Prr\left[\Alg_{\ref{lemma:hellinger-to-secret}}^{\MO}(x,\delta,\gamma) \neq \prod_{i \in S} x_i\right]
&= \Prr\left[\sum_{i,j} \mathbbm{1}\left[y^{(i)}_j \neq \prod_{i \in S} x_i\right] \geq \frac{NH}{2}\right] \\ 
&\leq \exp\left(-\left(\frac{2\gamma}{C_{\ref{lemma:spread-signal}}\sqrt{H}}\right)^2 \cdot \frac{NH}{6}\right) \\ 
&\leq \delta.
\end{align}
Therefore by the data processing inequality,
\begin{align}
\Prr\left[\Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}(x,\delta,\gamma) \neq \prod_{i \in S} x_i\right]
&\leq \delta + \Prr\left[\Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}(x,\delta,\gamma) \neq \Alg_{\ref{lemma:hellinger-to-secret}}^{\MO}(x,\delta,\gamma)\right] \\ 
&\leq \delta + N \cdot \Dtv{\pihat(\cdot\mid{}x)}{\pi^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}(\cdot\mid{}x)}.
\end{align}
Taking expectation over $X \sim \Unif(\{-1,1\}^n)$, we get that
\begin{align}
\Prr\left[\Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}(X,\delta,\gamma) \neq \prod_{i \in S} X_i\right]
&\leq \delta + N \cdot \Dtv{\BP^{\pihat}}{\BP^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}} \\ 
&\leq \delta + N \cdot \sqrt{\Dhels{\BP^{\pihat}}{\BP^\st_{n,S,H,\gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})}}}
\end{align}
as claimed.
\end{proof}


%

\subsection{Putting Everything Together: Proof of Theorem \ref*{thm:comp-lb-main}}\label{subsec:comp-lb-proof}

We now restate and prove \cref{thm:comp-lb-app}, and hence \cref{thm:comp-lb-main}. The proof is a straightforward consequence of \cref{lemma:spread-signal,lemma:bic-hell,lemma:hellinger-to-secret} together with appropriate parameter choices.

\begin{proof}[\pfref{thm:comp-lb-main}]
Suppose that $\Alg$ is a learning algorithm that satisfies the guarantees specified in the theorem statement, with parameters $C,c>0$. We design a algorithm $\Adv$ (short for ``Adversary'') %
that, for any $n$, distinguishes between $\BP^\st_{n,S,1,1/4}$ and $\MD = \BQ_{n,1}$. In particular, $\Adv$ takes as input $1^n$ and a sampling oracle $\MO$ for some distribution $\MD$ over $\{-1,1\}^n \times \{-1,1\}$. $\Adv$ then has the following behavior.

Set $t := n^{4/(4+c)}$, $d := \sum_{i=0}^t \binom{n}{i}$, $H := n^{3t}$, $\epsilon := 1/H^2$, $\gamma := \sqrt{\frac{c_{\ref{lemma:spread-signal}}}{\log(2H)}}$, $\eta := \gamma/(C_{\ref{lemma:spread-signal}}\sqrt{H})$. Also set $T = (dH/\epsilon)^C$. First, $\Adv$ draws $T$ independent samples $(x\ind{i},y\ind{i})_{i=1}^T$ from $\MO$, and for each $i \in [T]$ computes $o\ind{i} \sim \Alg_{\ref{lemma:spread-signal}}(x\ind{i},y\ind{i},\gamma)$. Next, $\Adv$ simulates $\Alg$ with inputs $(o\ind{i})_{i=1}^T$, $\MA := \{-1,1\}$, and $\epsilon$. Recall that $\Alg$ requires access to two computational oracles, which $\Adv$ simulates efficiently as follows:
\begin{enumerate}
\item Feature oracle: when $\Alg$ queries $(x,a_{1:h}) \in \MX\times\MA^h$ for some $h \in [H]$, $\Adv$ returns $\phiver_{n,t}(x,a_{1:h})$.
\item Projection oracle: when $\Alg$ queries $\theta \in \RR^d$, $\Adv$ returns the projection of $\theta$ onto $\Theta$, which is the unit $\ell_1$ ball, using the method of \cite{duchi2008efficient}.
\end{enumerate}
The output of $\Alg$ is a collection of circuits $\MC_{\pihat_h}$ that sample from the conditional distributions of a policy $\pihat=(\pihat_h)_{h=1}^H$; chaining these together gives a conditional sampler $\wh\MO$ for the distribution of $a_{1:H}$ under $\pihat$ for any given $x$. $\Adv$ draws a fresh sample $(\xtest,\ytest) \sim \MO$ and computes $\wh y := \Alg_{\ref{lemma:hellinger-to-secret}}^{\wh\MO}(\xtest,1/100,\gamma)$ (cf. \cref{lemma:hellinger-to-secret}). Finally, $\Adv$ outputs $\mathbbm{1}[\ytest = \wh{y}]$.


\paragraph{Analysis} First suppose that $\MO$ is a sampling oracle for $\BP^\st_{n,S,1,1/4}$ for some unknown $S \subseteq [n]$. Let $\mu$ be the distribution of each $o\ind{i}$. By \cref{lemma:spread-signal} and choice of $\gamma$, we have 
\begin{equation} \Dhel{\BP^\st_{n,S,H,\eta}}{\mu} \leq \sqrt{2\Dtv{\BP^\st_{n,S,H,\eta}}{\mu}} \leq 2\exp(-c_{\ref{lemma:spread-signal}}/\gamma^2) \leq \frac{1}{H}.
\label{eq:sample-to-truth}
\end{equation}
Moreover, by \cref{lemma:bic-hell} with $K := n$ and the fact that %
$\eta < 1/\sqrt{H} = n^{-1.5t} \leq c_{\ref{lemma:bic-hell}}t^{-4}n^{-t}$ for sufficiently large $n$, we have 
\begin{equation}
\min_{\pi\in\Pi_{n,t,H}} \Dhels{\BP^\st_{n,S,H,\eta}}{\BP^\pi} \leq 4e^{-t^2/(2n)} \leq e^{-(3t\log n)^{1-\frac{c}{2}}} = e^{-(\log H)^{1-\frac{c}{2}}}
\label{eq:bic-applied}
\end{equation}
where the second inequality holds by choice of $t$, for sufficiently large $n$, and the final equality holds by choice of $H$. We now invoke the guarantee of $\Alg$. Since the parameter space $\Theta$ consists of vectors $\theta$ with $\norm{\theta}_1 \leq 1$, and the range of the feature map $\phiver_{n,t}$ is contained in $[-1,1]^d$, \cref{ass:linear-norm-bounds-main} is satisfied with parameters $B = \sqrt{d}$ and $\Bdot = 1$. Moreover, by construction the action space has size $2$. Thus, we get that the time complexity of $\Alg$ (modulo oracle calls) is $\poly(d,H,1/\epsilon)$, and with probability at least $9/10$ it holds that 
\begin{equation}\Dhels{\BP^{\pihat}}{\mu} \lesssim \epsilon + e^{(\log \max(d,H))^{1-c}} \cdot \min_{\pi\in\Pi_{n,t,H}} \Dhels{\BP^\pi}{\mu}.
\label{eq:cmis-bound}\end{equation}
Combining \cref{eq:sample-to-truth,eq:bic-applied,eq:cmis-bound}, we get that in an event $\ME$ that occurs with probability at least $9/10$,
\begin{align}
\Dhel{\BP^{\pihat}}{\BP^\st_{n,S,H,\eta}}
&\leq \Dhel{\BP^{\pihat}}{\mu} + \frac{1}{H} \\
&\leq \sqrt{\epsilon + e^{(\log \max(d,H))^{1-c}} \cdot \min_{\pi\in\Pi_{n,t,H}} \Dhels{\BP^\pi}{\mu}} + \frac{1}{H} \\ 
&\leq \sqrt{\epsilon + e^{(\log \max(d,H))^{1-c}} \cdot \left(\frac{1}{H}+e^{-(\log H)^{1-\frac{c}{2}}}\right)} + \frac{1}{H} \\ 
&\leq O(e^{-\frac{1}{3}(\log H)^{1-\frac{c}{2}}})
\end{align}
where the final inequality also uses the choice of $\epsilon := 1/H^2$ and the fact that $d \leq H$. In event $\ME$, we have by \cref{lemma:hellinger-to-secret}, choice of $\gamma$, and the above bound, that
\[\Prr\left[\wh y \neq \prod_{i \in S} \xtest_i\right] \leq \frac{1}{100} + O(\log^2 H) \cdot O(e^{-\frac{1}{3}(\log H)^{1-\frac{c}{2}}}) \leq \frac{1}{99}\]
so long as $n$ is sufficiently large. Hence, the total probability of the event $\{\wh y \neq \prod_{i \in S} \xtest_i\}$ is at most $\Pr[\overline \ME] + 1/99 \leq 1/8$. Now $\ytest = (-1)^{\xi} \prod_{i \in S} \xtest$ where $\xi \sim \Ber(1/4)$, so $\Pr[\wh y \neq \ytest] \leq 3/8$. Thus, the distinguisher outputs $1$ with probability at least $5/8$. %

On the other hand, suppose that $\MO$ is a sampling oracle for $\BQ_{n,H}$. Since $\ytest$ is uniformly random conditioned on $\xtest$, we have that $\ytest$ is independent of $\wh y$ and hence $\Adv$ outputs $1$ with probability exactly $1/2$.

Finally, note that the time complexity of $\Adv$ is $\poly(d,H,1/\epsilon) = n^{O(n^{4/(4+c)})} = 2^{n^{1-\Omega(1)}}$, since the time complexity is dominated by the simulation of $\Alg$, and both computational oracles can be implemented in time $\poly(d,H)$. This contradicts \cref{assumption:lpn}.
\end{proof}

\begin{remark}[On computational hardness of regret minimization]\label{remark:il-hardness}
By \cref{eq:tv_equivalence} and the quadratic equivalence between Hellinger distance and TV-distance, one can convert \cref{thm:comp-lb-app}
into a statement about the computational hardness of approximate regret minimization with unknown, worst-case bounded reward function, though this requires examining misspecification level in the construction. In fact, in the proof of \cref{thm:comp-lb-app} it is not necessary to go through Hellinger distance at all (except for the fact that the theorem concerns agnostic estimation in Hellinger): \cref{lemma:bic-hell} achieves the bound on squared Hellinger misspecification through bounding TV-misspecification, and similarly \cref{lemma:hellinger-to-secret} achieves the error bound in terms of Hellinger distance via an error bound in terms of TV-distance. Thus, a slightly more direct argument proves that there is no computationally efficient algorithm achieving  \[\Dtv{\BP^{\pihat}}{\BP^{\pistar}} \lesssim \epsilon + e^{(\log \max(d,H))^{1-c}} \cdot \min_{\pi \in \Pi} \Dtv{\BP^\pi}{\BP^{\pistar}}.\]
Via \cref{eq:tv_equivalence}, this precisely shows hardness of approximate regret minimization with worst-case bounded reward.
\end{remark}
