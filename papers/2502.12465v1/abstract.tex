
  Next-token prediction with the logarithmic loss is a cornerstone of autoregressive sequence modeling, but, in practice, suffers from \emph{error amplification}, where errors in the model compound and generation quality degrades as sequence length $H$ increases. From a theoretical perspective, this phenomenon should not appear in \emph{well-specified} settings, and, indeed, a growing body of empirical work hypothesizes that \emph{misspecification}, where the learner is not sufficiently expressive to represent the target distribution, may be the root cause. Under misspecification---where the goal is to learn as well as the best-in-class model up to a multiplicative approximation factor $\Capx\geq{}1$---we confirm that $\Capx$ indeed grows with $H$ for next-token prediction, lending theoretical support to this empirical hypothesis.
  We then ask whether this mode of error amplification is avoidable algorithmically, computationally, or information-theoretically, and uncover inherent computational-statistical tradeoffs.
  
We show: \textbf{(1)} Information-theoretically, one can avoid error amplification and achieve $\Capx=O(1)$. \textbf{(2)} Next-token prediction can be made robust\arxiv{ so as} to achieve $\Capx=\bigoht(H)$, representing moderate error amplification, but this is an inherent barrier: \emph{any} next-token prediction-style objective must suffer $\Capx=\Omega(H)$. \textbf{(3)} For the natural testbed of autoregressive \emph{linear} models, \emph{no computationally efficient algorithm} can achieve sub-polynomial approximation factor $\Capx=e^{(\log H)^{1-\Omega(1)}}$; however, at least for binary token spaces, one can smoothly trade compute for statistical power and improve on $\Capx=\Omega(H)$ in sub-exponential time.
  Our results have consequences in the more general setting of imitation learning, where the widely-used behavior cloning\arxiv{ algorithm} generalizes next-token prediction.\loose






  









