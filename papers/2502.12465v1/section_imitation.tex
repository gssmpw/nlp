As a simple special case, if $\pistar$ is deterministic, then
combining \cref{prop:mle_finite}, leads to a guarantee on rollout
performance of the form $J(\pistar) - J(\pihat) \approxleq{}
R\cdot\frac{\log(\abs{\Pi}\delta^{-1})}{n}$. Under the ``dense-reward'' setting where $r_h\in\brk{0,1}$ for all $h\in\brk{H}$
so that $R=H$, this shows that behavior cloning can achieve constant
suboptimal with $\bigoh(H)$ examples, thereby evading the ``curse of
horizon'' known to plague other imitation learning algorithms (e.g.,
behavior cloning with other loss functions), under which $\bigom(H^2)$
sample complexity is observed \citep{ross2010efficient,ross2011reduction,ross2014reinforcement,
  rajaraman2020toward,rajaraman2021value,rajaraman2021provably,swamy2022minimax}. This
finding is salient because the curse of horizon has classically
been thought to blame for instabilities of imitation learning observed
in practice. %

per ,
also $\vepsmis$ represents irreducible error for rollout performance,
in the sense that any proper algorithm must have
$J(\pistar)-J(\pihat)\geq{}R\cdot\vepsmis^2$ for a worst-case reward
function. More importantly, the agnostic guarantee in
\cref{eq:hels-goal-intro} is sufficient to \emph{avoid the curse of
  horizon} for rollout performance: By , any algorithm that
satisfies \cref{eq:hels-goal-intro} achieves %
\[
  J(\pistar)-J(\pihat) \leq R\cdot{}(\Capx\cdot\vepsmis^2 + \vepsstatsn)
\]
when $\pistar$ is deterministic, meaning that even in the case of
dense rewards where $R=H$, the misspecification level is only scaled
by a \emph{linear} (as opposed to quadratic) factor in the horizon---so long as $\Capx$ itself is
independent of $H$.
