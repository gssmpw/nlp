


In this section we study the autoregressive linear setting as formally introduced in \cref{sec:computational}, and prove \cref{cor:linear-misspec-logloss} (restated below as \cref{cor:linear-misspec-logloss-app}) by analyzing $\BALM$ (\cref{alg:balm}), which simply implements $\boostedloglossbc$, using projected gradient ascent in parameter space (\cref{alg:gdlogloss}) to approximately implement the invocations of $\loglossbc$. 

To restate the setting, let $\MX$ and $\MA$ be sets where $|\MA|<\infty$. Fix $H \in \NN$, and let $M$ be the $H$-step autoregressive MDP with context space $\MX$, action space $\MA$, and some initial context distribution $\MD \in \Delta(\MX)$. We define an autoregressive policy class $\Pi := \{\pi_\theta: \theta \in \Theta\}$ where $\Theta \subseteq \RR^d$ is a convex parameter set, and each policy $\pi_\theta = (\pi_{\theta,h)})_{h=1}^H$ is defined by
\begin{equation} \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) := \frac{\exp(\langle \phi(x,a_{1:h}), \theta\rangle)}{\sum_{a'_h\in\MA} \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle)}.\label{eq:auto-linear-app}
\end{equation}
We assume that in $\poly(d,H)$ time we can (a) query $\phi(x,a_{1:h})$ for any given $(x,a_{1:h}) \in \MX\times\MA^\st$ (with $h \leq H$), and (b) compute the Euclidean projection $\Proj_\Theta[\theta] = \argmin_{\theta'\in\Theta}\norm{\theta-\theta'}_2$ of any point $\theta \in \RR^d$ onto $\Theta$. We also make the following norm bound assumption.

\begin{algorithm}[t]
\caption{\BALM: Boosted Log-Loss Optimization for Autoregressive Linear Models}\label{alg:balm}
\begin{algorithmic}[1]
\State\textbf{input:} Samples $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$; iteration complexity $T$, desired failure probability $\delta$.
\State Partition $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$ into $K := 2\log(2/\delta)$ disjoint equal-sized folds $\cD^1,\dots,\cD^K$.
\For{$1 \leq k \leq K/2$}
    \State Compute $\theta^k \gets \GDALM(\MD^k, T)$.\hfill\algcommentlight{Approximates $\loglossbc$; see \cref{alg:gdlogloss}.}
\EndFor
\For{$1 \leq k,k' \leq K/2$}
    \State Compute 
    \[f_{k,k'} \gets \sum_{\ell=K/2+1}^K \sum_{(x,a_{1:H})\in\MD^\ell} \tau\left(\prod_{h=1}^H \frac{\pi_{\theta^k,h}(a_h\mid{}x,a_{1:h-1})}{\pi_{\theta^{k'},h}(a_h\mid{}x,a_{1:h-1})}\right).\]
\EndFor
\State \textbf{return} $\theta^{\widehat k}$ where $\widehat k := \argmin_{k\in[K/2]} \max_{k'\in[K/2]} f_{k,k'}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{\GDALM: Gradient Ascent on Log Likelihood for Autoregressive Linear Models}\label{alg:gdlogloss}
\begin{algorithmic}[1]
\State\textbf{input:} Samples $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$; iteration complexity $T$.
\State Set $\theta^{(1)} := 0 \in \RR^d$ and $\eta := \frac{1}{2nH\sqrt{T}}$.
\For{$1 \leq t < T$}
    \State Compute
\[g^{(t)} := \sum_{i=1}^n \sum_{h=1}^H \left(\phi(x\ind{i},a\ind{i}_{1:h}) - \frac{\sum_{a_h'\in\MA} \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h) \exp(\langle \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h),\theta^{(t)}\rangle)}{\sum_{a_h'\in\MA} \exp(\langle \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h),\theta^{(t)}\rangle)}\right).\]
\State Set $\theta^{(t+1)} := \Proj_\Theta[\theta^{(t)} + \eta g^{(t)}]$. \hfill\algcommentlight{Euclidean projection onto $\Theta$.}
\EndFor
\State \textbf{return} $\thetahat := \frac{1}{T}\sum_{t=1}^t \theta^{(t)}$.
\end{algorithmic}
\end{algorithm}

\begin{assumption}[Norm bounds]\label{ass:linear-norm-bounds}
Let $B>0$ be a parameter. It holds that $\norm{\phi(x,a_{1:h})}_2 \leq B$ for all $(x,a_{1:h}) \in \MX\times\MA^\st$ and $\norm{\theta}_2 \leq B$ for all $\theta\in\Theta$. Moreover, $|\langle \phi(x,a_{1:h}),\theta\rangle| \leq \Bdot$ for all $(x,a_{1:h})\in\MX\times\MA^\st$ and $\theta \in \Theta$. 
\end{assumption}

Obviously, we can always take $\Bdot := B^2$; however, we separate these parameters because the time complexity will scale with $\poly(B)$, whereas the approximation ratio will only scale with $\Bdot$, and the latter can be much smaller in natural settings (e.g. if $\phi(x,a_{1:h}) \in [-1,1]^d$ and $\Theta$ is the $\ell_1$ ball). The following proposition states that \cref{alg:gdlogloss} (which is simply projected gradient ascent on the next-token prediction log-loss in parameter space $\Theta$) is both computationally efficient and achieves a non-trivial statistical guarantee even in the presence of misspecification:

\begin{proposition}[Restatement of \cref{cor:linear-misspec-logloss}]\label{cor:linear-misspec-logloss-app}
Suppose that \cref{ass:linear-norm-bounds} holds with parameters $B,\Bdot\geq 1$. Let $(x^i,a^i_{1:H})_{i=1}^n$ be i.i.d samples from $\BP^{\pistar}$ for any unknown policy $\pistar$. Then for any $\delta \in (0,1/2)$, the output $\thetahat$ of $\BALM((x^i,a^i_{1:H})_{i=1}^n, 2B^4 H^2 n^2,\delta)$ satisfies $\thetahat \in \Theta$ and, with probability at least $1-\delta$,
\begin{align}
  \Dhels{\BP^{\pi_{\thetahat}}}{\BP^{\pistar}} &\lesssim \frac{(d\log(BHn) + \Bdot + \log|\MA|)\log(1/\delta)}{n} + (\Bdot+\log|\MA|)H \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}.\end{align}
Moreover, the time complexity of the algorithm is $\poly(n,d,H,|\MA|,B,\log(1/\delta))$.
\end{proposition}

To prove \cref{cor:linear-misspec-logloss-app}, we start by analyzing the subroutine $\GDALM$ (\cref{alg:gdlogloss}), which approximately implements $\loglossbc$. In particular, we show that the log-loss is concave in parameter space and invoke a standard guarantee for projected gradient ascent (\cref{lemma:gd}) to prove that the output of $\GDALM$ is an approximate maximizer of the log-loss: %

\begin{lemma}\label{lemma:gd}
Suppose that \cref{ass:linear-norm-bounds} holds with parameters $B,\Bdot>0$. Fix $n \in \NN$ and let $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$ be arbitrary elements of $\MX\times\MA^H$. Then the output $\thetahat$ of $\GDALM$ (\cref{alg:gdlogloss}) with samples $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$ and iteration complexity $T$ satisfies $\thetahat \in \Theta$ and
\[\sum_{i=1}^n\sum_{h=1}^H \log \pi_{\thetahat,h}(a\ind{i}_h\mid{}x\ind{i},a\ind{i}_{1:h-1}) \geq \max_{\theta\in\Theta}\sum_{i=1}^n\sum_{h=1}^H \log \pi_{\theta,h}(a\ind{i}_h\mid{}x\ind{i},a\ind{i}_{1:h-1}) - \frac{2B^2 Hn}{\sqrt{T}}.\]
\end{lemma}

\begin{proof}[\pfref{lemma:gd}]
The guarantee $\thetahat \in \Theta$ is immediate from the projection step. Next, observe that \cref{alg:gdlogloss} is performing projected gradient ascent with projection set $\Theta$ and loss function
\begin{align}
\wh L(\theta) 
&:= \sum_{i=1}^n\sum_{h=1}^H \log \pi_{\theta,h}(a\ind{i}_h\mid{}x\ind{i},a\ind{i}_{1:h-1}) \\ 
&= \sum_{i=1}^n\sum_{h=1}^H \langle \phi(x\ind{i},a\ind{i}_{1:h}),\theta\rangle - \log \sum_{a_h'\in\MA} \exp(\langle \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h),\theta\rangle).
\end{align}
Indeed, for any $\theta \in \Theta$ we can write
\begin{align}
\grad_\theta \wh L(\theta) 
&= \sum_{i=1}^n \sum_{h=1}^H \left(\phi(x\ind{i},a\ind{i}_{1:h}) - \frac{\sum_{a_h'\in\MA} \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h) \exp(\langle \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h),\theta\rangle)}{\sum_{a_h'\in\MA} \exp(\langle \phi(x\ind{i},a\ind{i}_{1:h-1},a'_h),\theta\rangle)}\right) \\ 
&= \sum_{i=1}^n \sum_{h=1}^H \left(\phi(x\ind{i},a\ind{i}_{1:h}) - \EE_{a'_h \sim \pi_\theta(\cdot\mid{}x\ind{i},a\ind{i}_{1:h-1})}[\phi(x\ind{i},a\ind{i}_{1:h-1},a'_h)]\right), \label{eq:grad-linear-exp}
\end{align}
and furthermore
\begin{align}
\grad^2_\theta \wh L(\theta) 
&= \sum_{i=1}^n\sum_{h=1}^H\Bigg( \EE_{a'_h \sim \pi_\theta(\cdot\mid{}x\ind{i},a\ind{i}_{1:h-1})}[\phi(x\ind{i},a\ind{i}_{1:h-1},a'_h)]\EE_{a'_h \sim \pi_\theta(\cdot\mid{}x\ind{i},a\ind{i}_{1:h-1})}[\phi(x\ind{i},a\ind{i}_{1:h-1},a'_h)]^\top \\ 
&\qquad\qquad\qquad- \EE_{a'_h \sim \pi_\theta(\cdot\mid{}x\ind{i},a\ind{i}_{1:h-1})}\left[\phi(x\ind{i},a\ind{i}_{1:h-1},a'_h)\phi(x\ind{i},a\ind{i}_{1:h-1},a'_h)^\top\right]\Bigg).\label{eq:hessian-linear-exp}
\end{align}
From \cref{eq:hessian-linear-exp} and the fact that any covariance matrix is positive semi-definite, we see that $\grad^2\theta \wh L(\theta) \preceq 0$ for all $\theta$, and hence $L$ is concave. By \cref{ass:linear-norm-bounds}, we know that $\Theta$ is contained in a Euclidean ball of norm $B$ centered at $\theta^{(1)} = 0$. Moreover, by \cref{eq:grad-linear-exp} it is clear that $\wh L$ is $2nHB$-Lipschitz. The lemma statement now follows from standard analyses of projected gradient ascent (e.g. \cite[Theorem 3.2]{bubeck2015convex}). 
\end{proof}

We can now prove \cref{cor:linear-misspec-logloss-app} by essentially repeating the original analysis of $\boostedloglossbc$ (\cref{prop:boosted-bc}), and using with a lower bound on the densities of autoregressive linear models. One minor differnce is that we cannot directly use \cref{thm:log-loss-bounded}, since the policy class is infinite and there is non-zero optimization error, but we actually prove a more general version (\cref{thm:log-loss-bounded-app}) that handles both of these complications---it suffices to bound the covering number of the policy class, which we do in \cref{lemma:auto-linear-cover}.
\vspace{1em}

\begin{proof}[Proof of \cref{cor:linear-misspec-logloss-app}]
Fix any $1 \leq k \leq K/2$. Consider the invocation of \GDALM (\cref{alg:gdlogloss}) on the dataset $\cD^k$ with iteration complexity $T = 2B^4 H^2 n^2$. Moreover, by \cref{lemma:gd}, we have that
\[\sum_{(x,a_{1:H})\in\cD^k} \sum_{h=1}^H \log \pi_{\thetahat,h}(a_h\mid{}x,a_{1:h-1}) \geq \max_{\theta\in\Theta}\sum_{(x,a_{1:H})\in\cD^k}\sum_{h=1}^H \log \pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) - 1.\]
Thus, $\thetahat$ is a solution to $1$-approximate $\loglossbc$ with dataset $\cD^k$, as defined in \cref{sec:log-loss-app}. By \cref{lemma:auto-linear-density-bound}, the expert policy $\pistar$ is $|\MA|\exp(2\Bdot)$-bounded with respect to $\Pi$ (\cref{ass:density-bound}). We now apply the second guarantee of \cref{thm:log-loss-bounded-app} with dataset size $n' := n/(2\log(2/\delta))$, cover discretization $\epsilon := 1/Hn$, optimization error $\epopt := 1$, and density bound $\denbound := |\MA|\exp(2\Bdot)$. We get that with probability at least $1/2$,
\begin{align}
\Dhels{\BP^{\pi_{\theta^k}}}{\BP^{\pistar}} &\lesssim \frac{2}{n'} + \frac{\log(2\Nlog(\Pi,1/(Hn)))}{n'} + \frac{\log(e\denbound)\log(2)}{n'} \\
&\qquad+ 2H\log(e\denbound) \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}} \\ 
&\lesssim \frac{(d\log(BHn) + \Bdot + \log|\MA|)\log(1/\delta)}{n} + (\Bdot+\log|\MA|)H \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}
\end{align}
where the second inequality uses \cref{lemma:auto-linear-cover}. Now by independence of $\cD^1,\dots,\cD^{K/2}$, it holds with probability at least $1 - (1/2)^{K/2} = 1-\delta/2$ that there is at least one $k \in [K/2]$ satisfying the above bound. Condition on this event. Observing that the final steps of \cref{alg:balm} precisely implement $\rhobc$ with dataset $\cD^{K/2+1}\sqcup\dots\sqcup\cD^K$ and policy class $\{\pi_{\theta^1},\dots,\pi_{\theta^{K/2}}\}$, applying \cref{thm:rho-il} gives that with probability at least $1-\delta/2$,
\begin{align}
\Dhels{\BP^{\pi_{\theta^{\widehat k}}}}{\BP^{\pistar}} &\lesssim \frac{\log(K/\delta)}{n} + \min_{k\in[K/2]} \Dhels{\BP^{\pi_{\theta^k}}}{\BP^{\pistar}} \\ 
&\lesssim \frac{\log(1/\delta)}{n} + \min_{k\in[K/2]} \Dhels{\BP^{\pi_{\theta^k}}}{\BP^{\pistar}}\end{align}
where the second inequality is because $K \lesssim 1/\delta$. By the union bound, we have with probability at least $1-\delta$ that
\begin{align}
\Dhels{\BP^{\pi_{\theta^{\widehat k}}}}{\BP^{\pistar}} &\lesssim \frac{(d\log(BHn) + \Bdot + \log|\MA|)\log(1/\delta)}{n} + (\Bdot+\log|\MA|)H \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}\end{align}
as needed. We know that $\theta_{\widehat k} \in \Theta$ by \cref{lemma:gd}. Finally, we analyze the time complexity of the algorithm. Each iteration of $\GDALM$ has time complexity $\poly(n,H,d,|\MA|)$, so the overall time complexity of $\GDALM$ is $\poly(n,H,d,|\MA|,B)$. It follows that the $K$ invocations of $\GDALM$ require time $\poly(n,H,d,|\MA|,B,\log(1/\delta))$. For each $k,k' \in [K/2]$, $f_{k,k'}$ can be computed in time $\poly(n,d,H,|\MA|)$, since each conditional density can be computed using $|\MA|+1$ queries to the feature map $\phi$. It follows that the overall time complexity is $\poly(n,H,d,|\MA|,B,\log(1/\delta))$.
\end{proof}

\cref{cor:linear-misspec-logloss-app} specializes to the well-specified setting as follows:

\begin{proposition}\label{cor:linear-wellspec-logloss}
Suppose that $\norm{\phi(x,a_{1:h})}_2 \leq \sqrt{d}$ for all $(x,a_{1:h}) \in \MX\times\MA^\star$ and $\norm{\theta}_2\leq \sqrt{d}$ for all $\theta \in \Theta$. There is a $\poly(n,d,H,|\MA|)$-time algorithm that takes $n$ i.i.d. samples $(x^i,a^i_{1:H})_{i=1}^n$ from $\BP^{\pistar}$ for any unknown policy $\pistar \in \Pi$, and outputs $\pihat\in\Pi$ so that with probability at least $1-\delta$,
\begin{align}
\Dhels{\BP^{\pihat}}{\BP^{\pistar}} &\lesssim \frac{(d+\log|\MA|)\log(dHn/\delta)}{n}.\end{align}
\end{proposition}

\begin{proof}[\pfref{cor:linear-wellspec-logloss}]
Immediate by setting $\min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}} = 0$ in \cref{cor:linear-misspec-logloss}.
\end{proof}

\subsubsection{Supporting lemmas}


The following lemma shows that the policy class $\Pi$ has a small cover, in the sense of \cref{def:cover}:

\begin{lemma}\label{lemma:auto-linear-cover}
Suppose that \cref{ass:linear-norm-bounds} holds with parameters $B,\Bdot>0$. For any $\epsilon>0$, it holds that $\Nlog(\Pi,\epsilon) \leq (6B/\epsilon)^d$.
\end{lemma}

\begin{proof}[\pfref{lemma:auto-linear-cover}]
Since $\Theta$ is contained in the $d$-dimensional Euclidean ball, there is a set $\Theta' \subset \Theta$ be a of size at most $(6B^2/\epsilon)^d$, such that for every $\theta\in\Theta$ there is some $\theta' \in \Theta'$ with $\norm{\theta-\theta'}_2 \leq \epsilon/(2B)$. Define $\Pi' = \{\pi_{\theta'}: \theta'\in\Theta'\}$. For any $\theta,\theta' \in \Theta$ with $\norm{\theta-\theta'}_2 \leq \epsilon/(2B)$, and any $(x,a_{1:h})\in\MX\times\MA^\star$, observe that
\[\log\frac{\pi_\theta(a_h\mid{}x,a_{1:h-1})}{\pi_{\theta'}(a_h\mid{}x,a_{1:h-1})} = \langle \phi(x,a_{1:h}),\theta-\theta') + \log \frac{\sum_{a'_h\in\MA} \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta'\rangle)}{\sum_{a'_h\in\MA} \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle)}.\]
The first term has magnitude at most $B\norm{\theta-\theta'}_2 \leq \epsilon/2$. For the second term, note that for any $a_h'\in\MA$,
\begin{align}
\exp(\langle \phi(x,a_{1:h-1},a'_h),\theta'\rangle) 
&= \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle)\exp(\langle \phi(x,a_{1:h-1},a'_h),\theta'-\theta\rangle) \\ 
&\leq \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle) \cdot \exp(\epsilon/2)
\end{align}
and
\[\exp(\langle \phi(x,a_{1:h-1},a'_h),\theta'\rangle) \geq \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle) \cdot \exp(-\epsilon/2).\]
It follows that 
\[\frac{\sum_{a'_h\in\MA} \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta'\rangle)}{\sum_{a'_h\in\MA} \exp(\langle \phi(x,a_{1:h-1},a'_h),\theta\rangle)} \in [\exp(-\epsilon/2),\exp(\epsilon/2)]\]
and hence the second term is bounded  in magnitude by $\epsilon/2$ as well. We conclude that
\[\log\frac{\pi_\theta(a_h\mid{}x,a_{1:h-1})}{\pi_{\theta'}(a_h\mid{}x,a_{1:h-1})}.\]
This shows that $\Pi'$ is an $\epsilon$-cover for $\Pi$.
\end{proof}

\begin{lemma}\label{lemma:auto-linear-density-bound}
Suppose that \cref{ass:linear-norm-bounds} holds with parameters $B,\Bdot>0$. Let $(x,a_{1:h}) \in \MX\times\MA^\star$ and let $\theta\in\Theta$. Then $\pi_{\theta,h}(a_h\mid{}x,a_{1:h-1}) \geq \frac{1}{|\MA|\exp(2\Bdot)}$.
\end{lemma}

\begin{proof}[\pfref{lemma:auto-linear-density-bound}]
We have $\exp(\langle \phi(x,a_{1:h}),\theta\rangle) \geq \exp(-\Bdot)$ whereas $\sum_{a_h'\in\MA} \exp(\langle \phi(x,a_{1:h-1},a_h'),\theta\rangle) \leq |\MA|\exp(\Bdot)$. The result follows from \cref{eq:auto-linear-app}.
\end{proof}

