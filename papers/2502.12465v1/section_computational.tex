Our results in \cref{sec:next_token} show that to beat the $\Capx=\Omega(H)$ barrier, we need to move beyond next-token prediction entirely. However, they leave open the possibility of a completely different algorithm that gets a better guarantee without sacrificing computational efficiency. To investigate this possibility, we restrict our focus to \arxiv{autoregressive sequence modeling, specifically to }the \emph{autoregressive linear models} defined in \cref{eq:linear}; working in this simple, concrete setting allows us to formalize questions of computational complexity.

\paragraph{Notation and computational framework} Fix sets $\MX,\MA$ with $|\MA|<\infty$, and parameters $d,H\in\NN$. Let $M$ be an $H$-step autoregressive MDP with context space $\MX$ and action space $\MA$. Let $\phi:\MX\times\MA^\st \to \RR^d$ be a given $d$-dimensional feature map, and let $\Theta\subset\RR^d$ be a convex parameter set. We consider the policy class $\Pi := \{\pi_\theta: \theta \in \Theta\}$ where $\pi_\theta = (\pi_{\theta,h})_{h=1}^H$ is the autoregressive linear policy defined as in \cref{eq:linear}. We assume that in $\poly(d,H)$ time, a learning algorithm can (i) query $\phi(x,a_{1:h})$ for any given $(x,a_{1:h}) \in \MX\times\MA^\st$ (with $h \leq H$), and (ii) compute the Euclidean projection of any point $\theta \in \RR^d$ onto $\Theta$. In addition, we assume the
following norm bounds.

\begin{assumption}[Norm bounds]\label{ass:linear-norm-bounds-main}
  Let $B, \Bdot \geq{}1$ be parameters. It holds that $\norm{\phi(x,a_{1:h})}_2, \norm{\theta}_2 \leq B$ and $|\langle \phi(x,a_{1:h}),\theta\rangle| \leq \Bdot$ for all $(x,a_{1:h}) \in \MX\times\MA^\st$ and $\theta\in\Theta$.\footnote{
    While $\Bdot\leq{}B$, the upper bounds we present for next-token prediction scale polynomially in $\Bdot$, yet logarithmically in $B$, so we separate these parameters to accommodate situations where $B\gg\Bdot$.
    }
\end{assumption}
$\boostedloglossbc$ is end-to-end computationally efficient in this setting. Moreover, any density of any policy in $\Pi$ can be lower bounded by $\frac{1}{|\MA|}\exp(-2\Bdot)$. Thus, a (straightforward) generalization of \cref{prop:boosted-bc} implies a guarantee for efficient learning in the presence of misspecification, where the approximation ratio scales with the horizon $H$ and the inner product bound $\Bdot$.\loose

\begin{proposition}\label{cor:linear-misspec-logloss}
  Suppose that \cref{ass:linear-norm-bounds-main} holds with parameters $B,\Bdot\geq 1$. There is a $\poly(n,d,H,|\MA|,B)$-time algorithm that takes $n$ i.i.d. samples $(x^i,a^i_{1:H})_{i=1}^n$ from $\BP^{\pistar}$ for any unknown policy $\pistar$, and outputs $\pihat\in\Pi$ so that with probability at least $1-\delta$, %
\arxiv{\begin{align}
         \Dhels{\BP^{\pihat}}{\BP^{\pistar}} &\lesssim \frac{(d\log(BHn) + \Bdot + \log|\MA|)\log(\delta^{-1})}{n} + (\Bdot+\log|\MA|)H \cdot \min_{\pi\in\Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}.\end{align}
}
\end{proposition}

See \cref{sec:alm-gd} for the proof. Unfortunately, even for $\Bdot,|\MA| = O(1)$, the approximation ratio scales with $\Omega(H)$. In \cref{sec:comp-lb}, we show that this dependence cannot be improved substantially for polynomial-time algorithms, but in \cref{sec:comp-ub} we show that---at least when $|\MA| = 2$---there is a non-trivial \emph{trade-off} achievable between time complexity and approximation ratio.



\subsection{Computational Lower Bounds for Optimal Misspecification Tolerance}\label{sec:comp-lb}

Our main result for this section is a computational lower bound for learning autoregressive linear models based on hardness of \emph{Learning Parities with Noise (LPN)} (\cref{assumption:lpn}; see \cref{subsec:comp-lb-setting}).

\begin{theorem}\label{thm:comp-lb-main}
Suppose the sub-exponential decisional LPN hypothesis (\cref{assumption:lpn}) holds. Fix any $c,C>0$. Then \textbf{{no learning algorithm $\Alg$ has the following guarantee}}. Suppose $|\MA|=2$ and \cref{ass:linear-norm-bounds-main} holds with parameters $B=\sqrt{d}$ and $\Bdot=1$; then when given $T = (dH/\epsilon)^C$ i.i.d. samples from $\BP^{\pistar}$ for some unknown policy $\pistar$, the time complexity of $\Alg$ is $O(T)$ and the output is an $O(T)$-time conditional sampler for a policy $\pihat$ such that, with probability at least $9/10$,
\[\Dhels{\BP^{\pihat}}{\BP^{\pistar}} \lesssim \epsilon + e^{\log^{1-c} (\max(d,H))} \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\BP^{\pistar}}.\]
\end{theorem}

\paragraph{Implications} \cref{thm:comp-lb-main} shows that, under a plausible cryptographic assumption (\citet{yu2021smoothing}; see \cref{subsec:comp-lb-setting} for details), it is impossible to dramatically bypass the next-token prediction barrier in polynomial time (concretely, the result rules out $\Capx\leq{}e^{\log^{1-c}(H)}$ when $d\geq{}H$). It also implies computational hardness of regret minimization for worst-case unknown reward (\cref{remark:il-hardness}). We emphasize that the result applies to \emph{improper} learners, i.e., $\pihat$ does not itself need to be autoregressive linear, but does leave open the possibility of achieving $\Capx=H^{c}$ for some $c<1$, or $\Capx=\poly(d)$.\footnote{However, our ultimate interest is in broader policy classes, and a learner with strong dependence on the dimension in the autoregressive linear setting seems unlikely to be more broadly applicable.} \arxiv{It also does not apply if the learner is given access to the conditional densities of $\pistar$ (the setting of $\smoothedloglossbc$).} We emphasize that since \loglossbc is provably efficient for the class $\Pi$ (\Cref{prop:gdlogloss-intro}), this result implies that, even if we assume access to an \emph{oracle} for maximum likelihood (a common approach when working with general function classes \citep{foster2021statistical,foster2023foundations}), there is no hope for an \emph{oracle-efficient} algorithm achieving a better approximation factor.
  
\loose




\paragraph{Proof overview}
To prove \cref{thm:comp-lb-main}, we adapt an argument of \cite{diakonikolas2022hardness} that gives LPN-based hardness of agnostic PAC learning for a neuron with softmax activation function (concretely, their result implies that for $H = 1$, the approximation ratio of any polynomial-time autoregressive learner must scale with $\Bdot$ when the dimension $d$ is large). Our construction is similar, but ``spreads'' the signal in the noisy parity distribution across the $H$ steps of the autoregressive sequence model. For each individual step, the conditional distribution is much closer to uniform, so we can take $\Bdot=O(1)$, thereby isolating the impact of $H$ on the approximation ratio from the impact of $\Bdot$. We defer a more detailed overview and the formal proof to \cref{app:comp-lb}.

\subsection{A Computational-Statistical Tradeoff for Autoregressive Linear Models}\label{sec:comp-ub}

An interesting question left open by \cref{thm:comp-lb-main} is whether there exist polynomial time algorithms that achieve approximation guarantees of the form $\Capx=H^{c}$ for $c\in(0,1)$, i.e., in the regime between the $\Capx=\bigom(H)$ barrier for next-token prediction and the sub-polynomial $\Capx\ll\poly(H)$ region ruled out by the theorem. For our final result, we give some positive evidence in this direction, showing that for the special case of autoregressive linear models with $\abs{\cA}=2$, there exists an efficient algorithm based on an improper relaxation of the $\rho$-estimator (the \emph{chunked, kernelized $\rho$-estimator}, or $\ChunkKR$) that achieves $\Capx=\bigoh(\nicefrac{H}{K})$ for any constant $K$.\loose

\begin{theorem}[Informal; see \cref{thm:chunk-kr-main}]\label{thm:chunk-kr-informal}
Fix $\MA = \{0,1\}$ and suppose that \cref{ass:linear-norm-bounds-main} holds with parameters $B,\Bdot$. There is an algorithm $\ChunkKR$ (\cref{alg:chunkkr}) with the following property. For any $\delta \in (0,1/2)$, $\epsilon \in (0,1)$, and $K \in [H]$, there is some $n = (2^{B^2+K}H/\epsilon)^{O(B^2 K)} \log(H/\delta)$ so that if $(x\ind{i},a\ind{i}_{1:H})_{i=1}^n$ are i.i.d. samples from $\BP^{\pistar}$ for some unknown $\pistar$, then with probability at least $1-\delta$, the output $\pihat\gets \ChunkKR((x^{(i)},a^{(i)})_{i=1}^n, \epsilon)$ is computed in time $\poly(n,H)$ and satisfies
\[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}
\lesssim \epsilon + \frac{H}{K} \min_{\pi\in\Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}}.\]
\end{theorem}
For example, when $B=\bigoh(1)$, $\ChunkKR$ (with $K=H^{1/3}$) achieves a sublinear approximation factor $\Capx=\bigoh(H^{2/3})$ (beating next-token prediction) in subexponential time $e^{\bigoht(H^{2/3})}$, for $\eps\geq1/\poly(H)$.
\loose
\arxiv{We leave (i) a sharper understanding of computational-statistical tradeoffs, and (ii) developing similar tradeoffs for general classes $\Pi$ as directions for future work.}

\paragraph{Overview of algorithm design and proof techniques}
The $\ChunkKR$ algorithm in \cref{thm:chunk-kr-informal} uses two key algorithmic ideas: ``chunking'' the sequence into blocks \citep{chi2023diffusion,zhao2023learning,block2024provable}, and applying an improper, kernel-based relaxation to each chunk. The first idea\arxiv{, which may be of practical value and is reminiscent of tokenization,} is to learn the autoregressive model in chunks of size $K$: i.e., learn $\BP^{\pistar}(a_{iK+1:(i+1)K}\mid{}x,a_{1:iK})$ separately for each $i$. If, for each chunk, we can efficiently learn with approximation ratio $O(1)$, then by standard information-theoretic bounds, the combined model has $\Capx = O(H/K)$.\loose

With this insight, it remains to design an algorithm for learning misspecified autoregressive linear models with horizon $K \ll H$ that is efficient, yet achieves $\Capx=\bigoh(1)$---note that for this subproblem, we are allowed time complexity exponential in the horizon $K$ (but not in the dimension $d$). To achieve this, we implement $\rhobc$ via a generalization of the kernel-relaxation technique of \citet{shalev2011learning}, showing that we can approximately represent $\pi_{\theta}(a_{1:K}\mid{}x)$ as a function in an infinite-dimensional RKHS of bounded norm. %
After applying this relaxation, the $\rho$-estimator for each chunk becomes convex-concave in parameter space, and the resulting min-max program can be solved in polynomial time via projected gradient-descent-ascent (using the so-called ``kernel trick''). See \cref{app:comp-ub} for the full algorithm description and formal proof.\loose

