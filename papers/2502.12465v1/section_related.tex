
In this section we discuss additional related work not already covered
in detail.

\paragraph{Imitation learning} In the empirical literature on
imitation learning,
error amplification in next-token prediction and behavior
  cloning can be mitigated empirically to some extent through interactive access
  to the target distribution (demonstrating expert)
  \citep{ross2013learning,kim2013learning,gupta2017cognitive,bansal2018chauffeurnet,laskey2017dart,
    choudhury2018data,kelly2019hg,barnes2023world,zhuang2023robot,lum2024dextrah}
  or additional side information
  \citep{pfrommer2022tasil,block2024provable}. However, such access
  may not always be realistic or practical. Given the ubiquity of next-token
  prediction, our work focuses on the purely offline setting, seeking
  to understand whether error amplification can be
  mitigated without collecting additional data.

On the theoretical side, various improved imitation learning
procedures have been proposed with or without additional interactive
access or side information  
\citep{ross2010efficient,ross2011reduction,ross2014reinforcement,sun2017deeply,cheng2018convergence,cheng2020policy,cheng2019accelerating,yan2021explaining,spencer2021feedback}. Comparing
these results under misspecification is somewhat subtle, as many use
different, incomparable notions of supervised learning error, and
passing between these different notions often incurs additional
dependence on the horizon $H$. To our knowledge, the only work that
provides tight guarantees for general policy class $\Pi$, even in the
realizable/well-specified case, is \citet{foster2024behavior}, though
various works provide tight guarantees for specific (e.g., tabular or linear)
policy classes
\citep{rajaraman2020toward,rajaraman2021value,rajaraman2021provably}.

An important conceptual distinction is that---following
\citet{foster2024behavior}---we focus on estimating the
trajectory-level distribution $\bbP^{\pistar}$, which readily
translates to guarantees on generation performance in a horizon-free
fashion. A complementary approach used in many theoretical works
\citep{rajaraman2020toward,swamy2021moments} is to estimate
\emph{occupancy measures} given by
\[
  d_h^{\pi}(s,a)
  = \bbP^{\pi}\brk*{s_h=s, a_h=a}.
\]
Note that in the autoregressive setting, we have
$d_{H}^{\pi}\equiv\bbP^{\pi}$, since the final state fully determines the entire trajectory. For general MDPs, we are not aware of any
techniques based on occupancy measure estimation that give tight
dependence on horizon for general policy classes $\Pi$ even in the well-specified setting, irrespective of
computation.\loose

  \paragraph{Agnostic estimation in theoretical computer science}
  Agnostic estimation in $f$-divergences (particularly total
  variation distance) has been investigated in the theoretical
  computer science literature, and efficient algorithms have been
  identified for many specific distribution families of
  interest---particularly over low-dimensional or discrete domains
  \citep{acharya2015fast,diakonikolas2016learning,acharya2017sample,bousquet2019optimal}. Our
  results for autoregressive linear models are most closely related to a line of work on agnostically
  learning generalized linear models
  \citep{shalev2011learning,diakonikolas2022hardness,diakonikolas2022learning,gollakota2024agnostically},
  which corresponds to a special case when $H=1$ and $\abs{\cA}=2$
  (though the loss function in these works are different from the
  Hellinger distance objective we consider); notably our hardness results build on
  \citet{diakonikolas2022hardness} and our algorithms build on
  \citet{shalev2011learning}. On the hardness side, an important distinction is that
  our lower bounds aim to isolate the effect of the horizon $H$
  while controlling other problem-dependent parameters such as the
  norm of the weights.

  \paragraph{Misspecified estimation in statistics}
  Motivated by the insufficiency of maximum likelihood estimation under
  misspecification \citep{le1990maximum,birge2006model}, guarantees
  for misspecified distribution estimation in $f$-divergences
  like total variation distance and Hellinger distance have received
  some investigation in statistics \citep{devroye2001combinatorial,baraud2017new,baraud2018rho}, with the \emph{Scheff\'e
    tournament} \citep{devroye2001combinatorial} as perhaps the most
  well-known technique for general distribution classes. This line of
  work is not concerned with computational efficiency. In addition,
  while some techniques can be used essentially as-is for the general imitation
  learning setting we consider \citep{baraud2018rho}, not all
  techniques (including the Scheff\'e
    tournament itself) can be applied without knowledge of the
  underlying MDP dynamics.\loose

  
  
