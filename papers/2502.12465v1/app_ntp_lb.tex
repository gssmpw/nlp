Recall from \cref{def:iterative} that an \emph{iterative learner} for a policy class $\Pi$ and expert policy $\pistar$ is any algorithm that produces $\pihat \in \Pi$ by iteratively defining each token-level conditional distribution $\pihat_h$ in terms of $\pihat_{1:h-1}$ and $\pistar_{1:h}$. In this section we explain why this is a natural definition and then prove \cref{thm:ntp-lb}, which is a lower bound on the approximation ratio of any iterative learner.

\cref{def:iterative} is most natural for policy classes with no parameter sharing, as formally defined below.

\begin{definition}
A policy class $\Pi$ \emph{has no parameter sharing} if there are sets $\Pi_1,\dots,\Pi_h$ so that $\pi = (\pi_h)_h \in \Pi$ if and only if $\pi_h \in \Pi_h$ for all $h \in [H]$.
\end{definition}

For such policy classes, we show that \emph{any} estimator defined by minimizing a layer-wise loss---like $\loglossbc$ and $\smoothedloglossbc$---can be simulated by an iterative learner. Thus, \cref{thm:ntp-lb} applies to all such algorithms.

\begin{proposition}\label{prop:iterative-simulation}
For any MDP $M$ and policy class $\Pi$ with no parameter sharing, there is an iterative learner that, for any expert policy $\pistar$, simulates the execution of $\loglossbc$ on i.i.d. trajectories from $\BP^{\pistar}$. Moreover, the same holds for any estimator of the form
\[\pihat := \argmin_{\pi\in\Pi} \sum_{i=1}^n \sum_{h=1}^H L_h(\pi_h(a_h\ind{i}\mid{}s_h\ind{i}), \pistar_h(a_h\ind{i}\mid{}s_h\ind{i})),\]
where $L_{1:H}$ are arbitrary real-valued loss functions.
\end{proposition}

\begin{proof}[\pfref{prop:iterative-simulation}]
We give a proof for the general case, which clearly contains $\loglossbc$ via the loss function $L_h(p,q) = -\log p$. Since $\Pi$ has no parameter sharing, it suffices to draw $n$ i.i.d. trajectories from $\BP^{\pistar}$ and, for each $h \in [H]$, compute the following estimator, all within the computational framework of \cref{def:iterative}:
\[\pihat_h := \argmin_{\pi_h\in\Pi_h} \sum_{i=1}^n L_h(\pi_h(a_h\ind{i}\mid{}s_h\ind{i}),\pistar_h(a_h\ind{i}\mid{}s_h\ind{i})).\]
We compute $\pihat_1,\dots,\pihat_H$ in order. Since we know $M$, we can draw i.i.d. initial states $s_1\ind{1},\dots,s_1\ind{n}$. For each $h \in [H]$, we use knowledge of $\pistar_h$ to draw $a_h\ind{i} \sim \pistar_h(\cdot\mid{}s_h\ind{i})$ for each $i \in [n]$. Since we know $\Pi_h$, we can now compute $\pihat_h$ as above. We then use knowledge of the MDP to draw the next states $s_{h+1}\ind{i} \sim \BP(\cdot\mid{}s_h\ind{i},a_h\ind{i})$. By construction, $\pihat_h$ is a (random) function of $\pistar_{1:h}$, as needed.
\end{proof}

We now prove \cref{thm:ntp-lb}, restated below.

\begin{theorem}[Restatement of \cref{thm:ntp-lb}]\label{thm:ntp-lb-app}
Fix $H \in \NN$ and sets $\MX := \{0,1\}^H$ and $\MA := \{0,1,\perp\}$. Let $\MD := \Unif(\MX)$, and let $M$ be the $H$-step autoregressive MDP with initial context space $\MD$ and action space $\MA$. There is a policy class $\Pi$ with no parameter sharing, so that for any iterative learner, there exists a policy $\pistar$ such that
\[\EE\left[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}\right] \geq \Omega(H) \cdot \min_{\pi \in \Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}}\]
where $\pihat$ is the (potentially random) output of the iterative learner.
\end{theorem}


The basic idea is to embed a ``consistency game'' in the distribution learning problem. The expert policy is a fixed (but a priori unknown) action sequence $z$, and each conditional distribution class $\Pi_h$ contains a policy $\pi_h$ for each possible action sequence, so each $\pihat_h$ computed by the learner can be thought of as a ``guess'' for $z$. Minimizing Hellinger distance requires the guess at step $h$ to match $z$ on the first $h$ actions, but also being \emph{consistent}, i.e. minimizing the number of different guesses made (across the $H$ steps). Since the iterative learner must determine $\pihat_{1:h}$ without knowledge of $z_{h+1:H}$, this is provably impossible. We now make this idea formal.

\begin{proof}[Proof of \cref{thm:ntp-lb-app}]
Define $\Gamma := \{0,1\}^H$. Define a policy class $\Pi := \{\pi^{\gamma_{1:H}}: \gamma_{1:H} \in \Gamma^H\}$ where for each $\gamma_{1:H} \in \Gamma^H$, the policy $\pi^{\gamma_{1:H}} = (\pi^{\gamma_{1:H}}_h)_{h=1}^H$ is defined by
\[\pi^{\gamma_{1:H}}_h(a_h\mid{}x,a_{1:h-1}) := f_{h,\gamma_h}(a_h\mid{}x,a_{1:h-1}) := \begin{cases} 
\mathbbm{1}[a_h = (\gamma_h)_h] & \text{ if } (x \neq \gamma_h) \land (a_{1:h-1} = (\gamma_h)_{1:h-1}) \\ 
\mathbbm{1}[a_h = \perp] & \text{ otherwise}
\end{cases}.\]
Note that $\Pi$ indeed has no parameter sharing since $\pi^{\gamma_{1:H}}_h$ is solely a function of $\gamma_h$. 

Fix any iterative learner. Consider selecting $\pistar$ randomly via the following procedure. Draw $z \sim \Unif(\{0,1\}^H)$, and set $\pistar := \pi^z$ where $\pi^z(a\mid{}x) := \mathbbm{1}[a=z]$ for all $a \in \MA^H$ and $x\in\MX$ (recall that a policy in an autoregressive MDP can be equivalently identified by a sequence-level conditional distribution $\MX \to \MA^H$). With this random choice of $\pistar$, let $\pihat$ be the random output of the iterative learner.

By definition of an iterative learner, we always have $\pihat\in\Pi$, so there are some (random) $\wh \gamma^{1:H} \in \Gamma^H$ with $\pihat = \pi^{\wh\gamma_{1:H}}$. We can characterize $\Dhels{\BP^{\pihat}}{\BP^{\pistar}}$ in terms of $z$ and $\wh\gamma_{1:H}$: %
\begin{itemize}
    \item If there is any $h \in [H]$ with $(\wh \gamma_h)_{1:h} \neq z_{1:h}$, then $\Dhels{\BP^{\pihat}}{\BP^{\pistar}} = 1$. Indeed, pick the first such $h$. We have
    \[\BP^{\pihat}[a_{1:H}=z_{1:H}] \leq \EE_{x\sim\MD} \pihat_h(z_h\mid{}x,z_{1:h-1}).\]
    But for any $x\in\MX$, for the partial trajectory $a_{1:h-1} = z_{1:h-1}$, if $(\wh\gamma_h)_{1:h-1} \neq z_{1:h-1}$ then $\pihat$ plays action $\perp$ at step $h$. Otherwise $(\wh\gamma_h)_h \neq z_h$, so again $\pihat$ does not play $z_h$ at step $h$. Together with the above inequality, this shows that $\BP^{\pihat}[a_{1:H}=z_{1:H}] = 0$, i.e. $\BP^{\pihat}$ and $\BP^{\pistar}$ have disjoint supports.
    \item If $(\wh\gamma_h)_{1:h} = z_{1:h}$ for all $h \in [H]$, then \begin{equation} \Dhels{\BP^{\pihat}}{\BP^{\pistar}} = \EE_{x\sim\MD}[\Dhels{\pihat(\cdot\mid{}x)}{\pistar(\cdot\mid{}x)}] = 2^{-H}|\{\wh\gamma_h: h \in [H]\}|.\label{eq:consensus-bound}\end{equation}
    Indeed, condition on any $x \not \in \{\wh\gamma_h:h\in[H]\}$. For each $h \in [H]$, we have
    \[\pihat_h(z_h\mid{}x,z_{1:h-1}) = \pihat_h((\wh\gamma_h)_h\mid{}x,(\wh\gamma_h)_{1:h-1}) = 1,\]
    so inductively we have $\BP^{\pihat}[a_{1:H}=z_{1:H}\mid{}x] = 1$. Conversely, if $x \in \{\wh\gamma_h:h\in[H]\}$ then it is clear that $\BP^{\pihat}[a_{1:H}=z_{1:H}\mid{}x] = 0$.
\end{itemize}
For any $h \in [H]$, by definition of an iterative learner (and the fact that $z_1,\dots,z_H$ are independent) we have that $\wh\gamma_h$ is independent of $z_{h+1:H}$. Hence, for any $h < k$,
\begin{align}
\Pr[(\wh\gamma_h = \wh\gamma_k) \land (\Dhels{\BP^{\pihat}}{\BP^{\pistar}} < 1)]
&\leq \Pr[(\wh\gamma_h = \wh\gamma_k) \land ((\wh\gamma_h)_{1:h} = z_{1:h}) \land ((\wh\gamma_k)_{1:k} = z_{1:k})] \\ 
&\leq \Pr[(\wh\gamma_h)_{h+1:k} = z_{h+1:k}] \\ 
&= \frac{1}{2^{k-h}}.\label{eq:agreement-ub}
\end{align}
Next, observe that the following inequality holds with probability $1$, since either $2^H \Dhels{\BP^{\pihat}}{\BP^{\pistar}} = \{\wh\gamma_h: h \in [H]\}$ or else $\Dhels{\BP^{\pihat}}{\BP^{\pistar}} = 1$:
\[2^H \Dhels{\BP^{\pihat}}{\BP^{\pistar}} \geq \frac{1}{2}\left(|\{\wh\gamma_h: h \in [H]\}| + H \cdot \mathbbm{1}[\Dhels{\pistar}{\pistar} = 1]\right)\]
Using this bound, we get
\allowdisplaybreaks
\begin{align}
&2^H\EE[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}] \\
&\geq \frac{1}{2}\EE\left[|\{\wh\gamma_h: h \in [H]\}| + H \cdot \mathbbm{1}[\Dhels{\pistar}{\pistar} = 1]\right] \\
&\geq \frac{1}{2}\sum_{h \in [H], \text{$h$ odd}} \Pr[\wh\gamma_h \not \in \{\wh\gamma_1,\dots,\wh\gamma_{h-2}\}] + \Pr[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}=1] \\ 
&\geq \frac{1}{2}\sum_{h \in [H], \text{$h$ odd}} \Pr[(\wh\gamma_h \not \in \{\wh\gamma_1,\dots,\wh\gamma_{h-2}\}) \lor (\Dhels{\BP^{\pihat}}{\BP^{\pistar}}=1)] \\ 
&= \frac{1}{2} \sum_{h \in [H], \text{$h$ odd}}\left(1 - \Pr[(\wh\gamma_h  \in \{\wh\gamma_1,\dots,\wh\gamma_{h-2}\}) \land (\Dhels{\BP^{\pihat}}{\BP^{\pistar}}<1)]\right) \\
&\geq \frac{1}{2}\sum_{h \in [H], \text{$h$ odd}}\left(1 - \sum_{k=1}^{h-2} \Pr[(\wh \gamma_h = \wh \gamma_{k-2}) \land (\Dhels{\BP^{\pihat}}{\BP^{\pistar}}<1)]\right) \\ 
&\geq \frac{1}{2}\sum_{h \in [H], \text{$h$ odd}}\left(1 - \sum_{k=1}^{h-2} \frac{1}{2^{k-h}}\right) \\ 
&\geq \frac{H}{8}
\end{align}
where the penultimate inequality is by \cref{eq:agreement-ub}. However, if we define $\pibar := \pi^{\overline{\gamma}_{1:H}} \in \Pi$ where $\overline{\gamma}_h := z$ for all $h \in [H]$, then $\Dhels{\pibar}{\pistar} = 1/2^H$ by \cref{eq:consensus-bound}. Thus,
\[\EE\left[\Dhels{\BP^{\pihat}}{\BP^{\pistar}} - \frac{H}{8} \min_{\pi\in\Pi}\Dhels{\pi}{\pistar}\right] \geq 0.\]
Note that the expectation is over the randomness of $z$ and the interactive learner. It follows that there is some fixed choice of $\pistar$ for which
\[\EE\left[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}\right] \geq  \frac{H}{8} \min_{\pi\in\Pi}\Dhels{\pi}{\pistar}\]
where the expectation is over the randomness of the interactive learner.
\end{proof}
