\documentclass{article}
\usepackage{titletoc}

\usepackage{etoolbox}
\usepackage{comment}
\newtoggle{colt}
\newcommand{\colt}[1]{\iftoggle{colt}{#1}{}}
\newcommand{\arxiv}[1]{\iftoggle{colt}{}{#1}}
\newcommand{\loose}{\looseness=-1}

%

\input{arxiv_style}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %

\usepackage{tocloft}            %

\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{upgreek}


\usepackage{enumitem}

\usepackage{breakcites}


\usepackage{mathrsfs}

\usepackage{algorithm}
\usepackage{verbatim}
\usepackage[noend]{algpseudocode}
\newcommand{\multiline}[1]{\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1}}

\usepackage{multicol}

\usepackage{colortbl}
\usepackage{setspace}

\usepackage{transparent}

\usepackage{inconsolata}
\usepackage[scaled=.90]{helvet}
\usepackage{xspace}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{bm}
\newcommand{\x}{\bm{x}}

\usepackage{tablefootnote}

\hbadness = 10000





\DeclareFontFamily{U}{jkpmia}{}
\DeclareFontShape{U}{jkpmia}{m}{it}{<->s*jkpmia}{}
\DeclareFontShape{U}{jkpmia}{bx}{it}{<->s*jkpbmia}{}
\DeclareMathAlphabet{\mathfrak}{U}{jkpmia}{m}{it}
\SetMathAlphabet{\mathfrak}{bold}{U}{jkpmia}{bx}{it}


\input{dylan}
\input{macros}
\let\underbar\undefined
\input{widebar}




\usepackage[suppress]{color-edits}
 \addauthor{df}{ForestGreen}
 \addauthor{ab}{red}
 \addauthor{dr}{purple}
 \addauthor{ak}{BurntOrange}
 \addauthor{ah}{olive}
 \newcommand{\dfc}[1]{}
 \newcommand{\dhruv}[1]{\drcomment{#1}}
 \newcommand{\ah}[1]{\ahcomment{#1}}




 


\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

 \usepackage{accents}
 \newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\let\oldparagraph\paragraph
\newcommand{\paragraphi}[1]{\oldparagraph{\emph{#1}.}}
\renewcommand{\paragraph}[1]{\oldparagraph{#1.}}


\newcommand{\meet}{\wedge} 
\newcommand{\nn}{\nonumber}


\arxiv{



\title{\Large Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: \\
Autoregressive and Imitation Learning under Misspecification
}

}


%


\date{}


\begin{document}
\maketitle\vspace{-5em}
\begin{center}
\large
\setlength{\tabcolsep}{20pt}
\begin{tabular}{ccc}
\makecell{Dhruv Rohatgi \\ \small{\texttt{drohatgi@mit.edu}}}
&
\makecell{Adam Block \\ \small{\texttt{blockadam@microsoft.com}}}
&
\makecell{Audrey Huang \\ \small{\texttt{audreyh5@illinois.edu}}}
\end{tabular}\vspace{1em}
\begin{tabular}{cc}
\makecell{Akshay Krishnamurthy \\ \small{\texttt{akshaykr@microsoft.com}}}
&
\makecell{Dylan J. Foster \\ \small{\texttt{dylanfoster@microsoft.com}}}
\end{tabular}
\end{center}
\vspace{1em}


\begin{abstract}
\input{abstract}
\end{abstract}
%
\arxiv{
\setcounter{tocdepth}{0}
}

\section{Introduction}
\label{sec:intro}
\input{section_intro}

\section{An Inefficient Algorithm with Optimal Misspecification Tolerance}
\label{sec:optimal}
\input{section_optimal}

\section{Next-Token Prediction under Misspecification: Improvements
  and Limitations}
\label{sec:next_token}
\input{section_next_token}

\section{Computational-Statistical Tradeoffs for Misspecification
  Tolerance}
\label{sec:computational}
\input{section_computational}

\arxiv{
\section{Conclusion}
\label{sec:conclusion}
\input{section_conclusion}
}

\bibliography{refs}

\clearpage

\appendix

\renewcommand{\contentsname}{Contents of Appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
{
 \hypersetup{hidelinks}
 \tableofcontents
}


\newpage
\part{Additional Discussion and Results}

\section{Additional Related Work}
\label{sec:related}
\input{section_related}



\section{Comparing Hellinger Distance to Other Misspecification Notions}
\label{sec:comparing}
\input{section_comparing}


\section{Further Benefits of \DensObs}
\label{sec:density}
\input{section_density}

\newpage

\part{Proofs}

\section{Supporting Results}

This section of the appendix contains proofs for various supporting
and secondary results. In \cref{sec:alm-gd}, we show $\boostedloglossbc$ can be implemented computationally efficiently for autoregressive linear models, and achieves approximation ratio $\Capx = \bigoht(H)$ with high probability (\cref{cor:linear-misspec-logloss-app}). In \cref{sec:ll-failure-large-n} we prove a lower bound on the approximation ratio of $\loglossbc$ in the large-sample regime (\cref{prop:log-loss-lb}), complementing \cref{prop:log-loss-prob-lb}. In \cref{sec:layerwise-rho} we show that a layerwise version of $\rhobc$ achieves the optimal approximation ratio among next-token prediction algorithms (\cref{prop:layerwise-rho}), matching our lower bound from \cref{thm:ntp-lb}.

\subsection{Next-Token Prediction for Autoregressive Linear Models}\label{sec:alm-gd}

\input{app_gdlogloss}

\subsection{Failure of $\loglossbc$ in Large-Sample Regime}\label{sec:ll-failure-large-n}



The following result (\cref{prop:log-loss-lb}) shows that the approximation ratio of
$\loglossbc$ necessarily scales with $H\log(\denbound)$, where $H$ is the
horizon and $\denbound$ is the density bound parameter from
\cref{ass:density-bound}----even as $n\to\infty$. This result is incomparable to \cref{prop:log-loss-prob-lb-app}, where the lower bound scales with $1/\delta$ (where $\delta$ is the failure probability) but the number of samples $n$ is not allowed to grow. We use \cref{prop:log-loss-lb} to show that the approximation ratio of $\loglossbc$ can be arbitrarily bad without a density bound (\cref{cor:unbounded}).
    \begin{proposition}\label{prop:log-loss-lb}
  Fix any $n,H \in \NN$ and $\denbound \geq 2$. Let $\veps \in (0, 1/(1+H\log(\denbound)))$. Suppose that $n \geq 8/\veps$. There is an $H$-step autoregressive MDP $M$, a policy class $\Pi$ of size $|\Pi| = 2$, and an expert policy $\pistar$ such that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}), with the following property. Given $n$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$ from $\BP^{\pistar}$, the estimator $\pihat$ produced by $\loglossbc$ satisfies, with probability at least $1-1/e$,
  \[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \gtrsim H\log(\denbound) \cdot \veps\]
  while $\min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}} \leq \veps$.
  \end{proposition}

  \begin{proof}[\pfref{prop:log-loss-lb}]
Let $M$ be the $H$-step autoregressive MDP with context space $\cX = \{\perp,\xfrak,\yfrak\}$, action space $\cA = \{\afrak,\bfrak\}$, and context distribution $\rho \ \in\Delta(\cX)$ with $\rho(\yfrak) = \veps$ and $\rho(\xfrak) = H\log(\denbound) \cdot \veps$. Recall that any policy in an autoregressive MDP is (uniquely) identified by a conditional distribution $\cX \to \Delta(\cA^H)$. Define $\pistar$ so that 
  \[ 
  \Pr^{\pistar}[a_{1:H} \mid{} x] = \begin{cases} 
  \mathbbm{1}[a_{1:H}=(\afrak,\dots,\afrak)] &\text{ if } x \in \{\perp,\xfrak\} \\ 
  \mathbbm{1}[a_{1:H} = (\bfrak,\dots,\bfrak)] & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\pia$ so that
\[ 
  \Pr^{\pia}[a_{1:H} \mid{} x] = \begin{cases} 
  \mathbbm{1}[a_{1:H}=(\afrak,\dots,\afrak)] &\text{ if } x\in\{\perp,\xfrak\} \\ 
  \prod_{h=1}^H \left(1-\frac{1}{\denbound}\right)^{\mathbbm{1}[a_h=\afrak]}\left(\frac{1}{\denbound}\right)^{\mathbbm{1}[a_h=\bfrak]} & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\pib$ so that
  \[ 
  \Pr^{\pib}[a_{1:H} \mid{} x] = \begin{cases} 
  \mathbbm{1}[a_{1:H}=(\afrak,\dots,\afrak)] &\text{ if } x=\perp \\ 
  \left(\frac{4}{5}\right)^{\mathbbm{1}[a_1=\afrak]} \left(\frac{1}{5}\right)^{\mathbbm{1}[a_1=\bfrak]}\mathbbm{1}[a_{2:H}=(\afrak,\dots,\afrak)] &\text{ if } x=\xfrak \\ 
  \mathbbm{1}[a_{1:H} = (\bfrak,\dots,\bfrak)] & \text{ if } x = \yfrak 
  \end{cases}.
  \]
  Define $\Pi := \{\pia,\pib\}$. Observe that $\Pr^{\pia}[a_h=\afrak \mid{} x=\xfrak, a_{1:h-1}=a'_{1:h-1}] = 1$ and $\Pr^{\pia}[a_h=\bfrak \mid{} x=\yfrak, a_{1:h-1}=a'_{1:h-1}] \geq 1/\denbound$ for any $h \in [H]$ and $a'_{1:h-1} \in \cA^{h-1}$. Moreover $\Pr^{\pib}[a_h=\afrak \mid{} x=\xfrak, a_{1:h-1}=a'_{1:h-1}] \geq 4/5 \geq 1/\denbound$ and $\Pr^{\pib}[a_h=\bfrak \mid{} x=\yfrak, a_{1:h-1}=a'_{1:h-1}] = 1$ for any $h \in [H]$ and $a'_{1:h-1} \in \cA^{h-1}$. Moreover, $\pia(\cdot\mid{}\perp) = \pib(\cdot\mid{}\perp) = \pistar(\cdot\mid{}\perp)$. Thus, $\pistar$ is $\denbound$-bounded with respect to $\Pi$.

  Now consider $n$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$ from $\BP^{\pistar}$. By choice of the context distribution $\rho$, we have $\Pr[x\ind{i}=\yfrak] = \veps$ for each $i \in [n]$. Let $\cE$ be the event that $n_\yfrak := \#\{i \in [n]: x\ind{i} = \yfrak\} \geq \frac{n\veps}{2}$ and $n_\xfrak := \#\{i \in [n]: x\ind{i} = \xfrak\} \leq 2nH\log(\denbound)\cdot\veps$. By Chernoff bounds and the assumption that $n \geq 8/\veps$, we have 
  \[\Pr[\cE] \geq 1 - 2e^{-\frac{n\veps}{8}} \geq 1 - 2/e.\]
  Condition on the event $\cE$ henceforth. By definition of $\pistar$, we know that $a_{1:H}\ind{i} = (\afrak,\dots,\afrak)$ whenever $x\ind{i} \in \{\perp,\xfrak\}$, and conversely $a_{1:H}\ind{i} = (\bfrak,\dots,\bfrak)$ whenever $x\ind{i} = \yfrak$. Thus, we have
  \begin{align} 
  \sum_{i=1}^n \sum_{h=1}^H \log \pia_h(a_h\ind{i} \mid{} x\ind{i}, a_{1:h-1}\ind{i})
  &= n_\yfrak \sum_{h=1}^H \log \pia_h(\bfrak \mid{} \yfrak, \bfrak,\dots,\bfrak) \\ 
  &= n_\yfrak H \log (1/\denbound) \\ 
  &\leq -\frac{nH\log(\denbound) \cdot \veps}{2}
  \end{align} 
  since $n_\yfrak \geq \frac{n\veps}{2}$ and $\denbound \geq 1$. On the other hand,
  \begin{align}
  \sum_{i=1}^n \sum_{h=1}^H \log \pib_h(a_h\ind{i} \mid{} x\ind{i}, a_{1:h-1}\ind{i})
  &= n_\xfrak \sum_{h=1}^H \log \pib_h(\afrak \mid{} \xfrak, \afrak,\dots,\afrak) \\
  &= n_\xfrak \log(4/5) \\
  &\geq -2nH\log(\denbound)\log(5/4) \cdot \veps.
  \end{align}
 Since $2\log(5/4) < 1/2$, it follows from the definition of $\loglossbc$ that $\pihat = \pib$. However, 
  \[\Dhels{\BP^{\pistar}}{\BP^{\pib}} \geq \rho(\xfrak) \Dtv{\pistar(\cdot\mid{}\xfrak)}{\pib(\cdot\mid{}\xfrak)}^2 \geq \Omega(H\log(\denbound) \cdot \veps)\]
  whereas
  \[\Dhels{\bbP^{\pistar}}{\BP^{\pia}} = \EE_{x \sim \rho} \Dhels{\bbP^{\pistar}(\cdot\mid{}x)}{\BP^{\pia}(\cdot\mid{}x)} \leq \rho(\yfrak) = \veps.\]
  The claim follows.
  \end{proof}

The following result asserts that without a density bound, the approximation ratio of $\loglossbc$ can be arbitrarily poor. The proof is immediate from \cref{prop:log-loss-lb} by taking $\denbound := e^{\frac{1}{H}(\frac{1}{\veps} - 1)}$. Notice that since the result applies for arbitrarily large sample complexity $n$, it is fundamentally a statement about the approximation ratio (and not the statistical rate).

\begin{proposition}\label{cor:unbounded}
  Fix any $n,H \in \NN$ and $\veps \in (0,1/2)$. Suppose that $n \geq 8/\veps$. There is an $H$-step autoregressive MDP $M$, a policy class $\Pi$ of size $|\Pi| = 2$, and an expert policy $\pistar$, with the following property. Given $n$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$ from $\BP^{\pistar}$, the estimator $\pihat$ produced by $\loglossbc$ satisfies, with probability at least $1-1/e$,
  \[\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \gtrsim \Omega(1)\]
  while $\min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}} \leq \veps$.
  \end{proposition}

\subsection{A Statistically Optimal Next-Token Prediction Algorithm}\label{sec:layerwise-rho}

\input{app_layerwise_rho}

\newpage
\section{Proof of Theorem \ref*{thm:rho-il} ($\rho$-Estimator)}\label{app:rho}

\input{app_rho}

\newpage
\section{Proofs from Section \ref*{sec:next_token} (Next-Token Prediction)}\label{app:ntp}

\input{app_ntp_ubs}

\subsection{Proofs from Section \ref*{sec:limits} (Limits of Next-Token Prediction)}\label{sec:limits-app}

\input{app_ntp_lb}

\newpage
\section{Proof of Theorem \ref*{thm:comp-lb-main} (Computational Lower Bound)}\label{app:comp-lb}

\input{app_comp_lb}

\newpage
\section{Proof of Theorem \ref*{thm:chunk-kr-informal} (Computational-Statistical Tradeoff)}\label{app:comp-ub}

\input{app_comp_ub}


\end{document}
