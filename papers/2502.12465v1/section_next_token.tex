


With $\rhobc$ as a statistical skyline, we return to the most widely-used \IL{} algorithm: behavior cloning with the logarithmic loss (\loglossbc; \cref{eq:bc_general}), an instance of next-token prediction for the general \IL{} setting.
We show that simple algorithmic tweaks can improve its performance substantially, but the performance of $\rhobc$ cannot be matched: $\Capx = \Omega(H)$ is a barrier for \emph{any} next-token prediction algorithm (cf. \cref{rem:next_token}). Proofs are deferred to \cref{app:ntp}.

\subsection{Sharp Analysis of Log-Loss Behavior Cloning}
\label{sec:logloss}


We start by giving a tight analysis for log-loss behavior cloning\arxiv{, with no modifications,} in the general IL setting.
While $\loglossbc$ can fail to achieve any finite approximation ratio in pathological examples (\cref{cor:unbounded}), we show that it achieves bounded \arxiv{approximation ratio} whenever density ratios of the form $\nicefrac{\pistar_h(a\mid{}s)}{\pi_h(a\mid{}s)}$ are bounded, an assumption satisfied in many settings including autoregressive linear models.\arxiv{ Formally, we consider the following assumption.}\loose %

\begin{definition}[Density ratio bound]\label{ass:density-bound}
For $\denbound \geq 2$, we say that a policy $\pistar$ is $\denbound$-bounded with respect to policy class $\Pi$ if \arxiv{
\[\max_{\pi \in \Pi} \max_{(s,a) \in \cS \times \cA} \max_{h \in [H]} \frac{\pistar_h(a\mid{}s)}{\pi_h(a\mid{}s)} \leq \denbound.\]}
\end{definition}

For example, if $\min_{s,a,h}\pi_h(a\mid{}s) \geq 1/\denbound$ for all $\pi\in\Pi$, then any policy $\pistar$ is $\denbound$-bounded with respect to $\Pi$. We show that $\loglossbc$ has approximation ratio roughly $\Capx \approx H\log \denbound$.\loose

\begin{theorem}\label{thm:log-loss-bounded}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. Suppose that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}) for some $\denbound \geq 2$. Let $n \in \NN$ and $\delta > 0$.\arxiv{ Let $\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$ from $\BP^{\pistar}$.} Then the policy $\pihat$ produced by $\loglossbc$ (\cref{eq:bc_general}) satisfies, with probability at least $1-\delta$,
\begin{equation} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|\delta^{-1})}{n} + \frac{\log{}\denbound\log(\delta^{-1})}{n} + \frac{H\log{}\denbound}{\delta} \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\end{equation}
\end{theorem}
Concretely, the approximation ratio scales as $\Capx=\frac{H\log \denbound}{\delta}$ for failure probability $\delta$; note the polynomial rather than logarithmic scaling in $\delta^{-1}$. It is possible to avoid the dependence of $\Capx$ on $\delta^{-1}$ at the cost of an additional factor of $H$ in the statistical rate (cf. \cref{thm:log-loss-bounded-app}), but this horizon dependence may be undesirable. We remark that while $\loglossbc$ can be interpreted as maximum likelihood on trajectories, \arxiv{and hence analyzed directly at the sequence level, }\cref{thm:log-loss-bounded} is \emph{not} a corollary of existing analyses for maximum likelihood in terms of e.g., $\chi^2$-misspecification \citep[Proposition B.1]{foster2024behavior}: naively converting $\min_{\pi\in\Pi} \Dchis{\bbP^{\pistar}}{\bbP^{\pi}}$ to Hellinger misspecification via the density ratio bound would incur a factor of $\Capx \approx \denbound^H$. The proof of \cref{thm:log-loss-bounded} fundamentally uses the sequential structure of the IL setting.\loose



  \subsection{Robustifying Next-Token Prediction via Cross-Validation and Smoothing}
  \label{sec:improvements}

There are two shortcomings of \cref{thm:log-loss-bounded}, even ignoring the fact that the approximation ratio scales with $H$ (which, as we will show later, is essentially necessary). First, to get the optimal rate, we pay a factor of $1/\delta$ in the approximation ratio. Second, the theorem only holds under $\denbound$-boundedness. The following result
shows that both of these shortcomings are inherent, and not artifacts of the analysis.\loose

\begin{proposition}[Tightness of \creftitle{thm:log-loss-bounded}]\label{prop:log-loss-prob-lb}
  Fix any $H \in \NN$ and $\denbound \geq 2$ and $\delta \in (0,1/2)$, and set $n_0 := H\log{}\denbound$. There is an $H$-step autoregressive MDP $M$, a policy class $\Pi$ of size $|\Pi| = 2$, and an expert policy $\pistar$ such that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}), with the following property. Given $n_0$ i.i.d. trajectories $o\ind{i} = (x\ind{i},a_{1:H}\ind{i})$ from $\BP^{\pistar}$, the estimator $\pihat$ produced by $\loglossbc$ satisfies $\Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \gtrsim 1$ with probability at least $\delta$, even though $\min_{\pi\in\Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}} \lesssim \frac{\delta}{H\log R}.$
  \end{proposition}

This result shows that the tradeoff discussed after \cref{thm:log-loss-bounded} is tight: either $\Capx = \Omega(1/\delta)$, or the statistical rate must scale as $\Omega(\nicefrac{(H\log{} \denbound)}{n})$. Additionally, one can show that  the dependence of $\Capx$ on $H\log{}\denbound$ is necessary even as $n \to \infty$ (\cref{prop:log-loss-lb}). Next, we present two algorithmic modifications to $\loglossbc$ that avoid these shortcomings: (1) boosting the success probability via cross-validation, and (2) addressing unbounded density ratios through access to \emph{\densobs}.\loose
  
  \subsubsection{Boosting to High Probability via $\rho$-Estimator Cross-Validation}
We can boost \loglossbc to achieve a high probability guarantee 
  (without $\poly(1/\delta)$ dependence in the approximation factor, and without worsening the statistical rate)
  by first running \loglossbc on $K$ independent partitions of the data 
  to obtain an intermediate policy class $\Pi' = \crl*{\pihat_{1},\ldots,\pihat_K}$,
  then running \rhobc with $\Pi'$ to output the final policy $\pihat$.
  We call the resulting algorithm \boostedloglossbc.
  
\arxiv{Formally, given a parameter $\delta>0$, a policy class $\Pi$, and $n$ trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$, consider the algorithm $\boostedloglossbc$ defined by the following procedure:
\begin{enumerate}\arxiv{\setlength\itemsep{0.3em}} 
\item Divide the dataset $\cD=(o\ind{i})_{i=1}^n$ into $K := 2\log(2/\delta)$ disjoint equal-sized folds $\cD^1,\dots,\cD^K$.
\item For each $1 \leq i \leq K/2$, compute policy $\pihat^i$ by applying $\loglossbc$ with dataset $\cD^i$ and policy class $\Pi$.
\item Output the policy $\pihat$ obtained by applying $\rhobc$ with dataset $\cD^{K/2+1} \cup \dots \cup \cD^{K}$ and policy class $\Pi'=\{\pihat^1,\dots,\pihat^{K/2}\}$.
\end{enumerate}
The main guarantee for this algorithm is as follows.
}%

\begin{corollary}\label{prop:boosted-bc}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. Suppose that $\pistar$ is $\denbound$-bounded with respect to $\Pi$ (\cref{ass:density-bound}) for some $\denbound \geq 2$. \arxiv{Let $n \in \NN$ and $\delta \in (0,1/2)$. Let $\cD=\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\dots,s_H\ind{i},a_H\ind{i})$ from $\BP^{\pistar}$. Then the} policy $\pihat$ produced by $\boostedloglossbc$ satisfies, with probability at least $1-\delta$,
\begin{equation} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|\denbound)\log(\delta^{-1})}{n} + H\log{}\denbound \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\end{equation}
\end{corollary}

\arxiv{We emphasize that }$\boostedloglossbc$ has minimal computational overhead over $\loglossbc$: while $\rhobc$ is computationally unattractive for general policy classes, for a \emph{finite} policy class of size $K$ it can be computed in time $O(K^2 nH)$ through enumeration; we take $K=\bigoh(\log(\delta^{-1}))$. As a result, $\boostedloglossbc$ can be implemented \emph{provably efficiently} for autoregressive linear models,  giving a baseline for the computational-statistical tradeoffs that we explore in \cref{sec:computational}---see \cref{cor:linear-misspec-logloss}.\loose


\subsubsection{Addressing Unbounded Densities via Smoothing}

Next, suppose that in addition to the usual expert trajectories, we have access to \emph{\densobs} of the form $\pistar_h(a\ind{i}_h\mid{}s\ind{i}_h)$. Such access is a natural assumption for the task of \emph{expert distillation}, where we aim to distill a large model $\pistar$ into a smaller model $\pihat$ \citep{hinton2015distilling}. Given access to such observations, we can remove the dependence on the density ratio $\denbound$ through the following algorithm, which we refer to as \smoothedloglossbc: For a parameter $\lambda\in(0,1)$, output the policy:{\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
  \label{eq:smoothed_bc}
  \pihat := \argmax_{\pi\in\Pi} \sum_{i=1}^n \sum_{h=1}^H \log ((1-\lambda)\pihi +\lambda \pistarhi).
  \end{align}}%
  $\smoothedloglossbc$ can be viewed as applying $\loglossbc$ to an augmented policy class $\Pi^\lambda$ where each policy is mixed with $\pistar$; this has some similarity to knowledge distillation objectives in the literature \citep{hinton2015distilling,lopez2015unifying}, but mixes the teacher's logits with the student's instead of mixing them with the labels. Since $\pistar$ is $(1/\lambda)$-bounded with respect to $\Pi^\lambda$---with no assumptions on the original policy class---\cref{thm:log-loss-bounded} implies the following improved guarantee.\loose
  \begin{corollary}
    \label{thm:layerwise_smoothing}
Fix an MDP $M$, a policy class $\Pi$, and an expert policy $\pistar$. Let $n \in \NN$ and $\delta \in (0,1/2)$.\arxiv{ Let $\cD=\{o\ind{i}\}_{i=1}^n$ be i.i.d. trajectories $o\ind{i} = (s_1\ind{i},a_1\ind{i},\pistar(a_1\ind{i}\mid{}s_1\ind{i}),\dots,s_H\ind{i},a_H\ind{i},\pistar(a_H\ind{i}\mid{}s_H\ind{i}))$ from $\BP^{\pistar}$.} The policy $\pihat$ produced by $\smoothedloglossbc$ with smoothing parameter $\lambda = 1/(H^2n)$ satisfies, with probability at least $1-\delta$,\loose
\begin{equation} \Dhels{\BP^{\pihat}}{\bbP^{\pistar}} \lesssim \frac{\log(|\Pi|\delta^{-1})}{n} + \frac{\log(Hn)\log(\delta^{-1})}{n} + \frac{H\log(Hn)}{\delta} \cdot \min_{\pi \in \Pi} \Dhels{\BP^\pi}{\bbP^{\pistar}}.
\end{equation}
\end{corollary}
We emphasize that the loss function in \cref{eq:smoothed_bc}---like the vanilla next-token prediction loss itself---is concave in policy space, though it may not be concave in parameter space in general. This estimator can also be boosted to succeed with high probability via cross-validation; we omit the details.

\subsection{A Barrier for Next-Token Prediction}
\label{sec:limits}

While cross-validation and smoothing mitigate certain shortcomings of $\loglossbc$, the main weakness remains: the approximation ratio $\Capx$ scales linearly in the horizon $H$. We now show that $\Capx = \Omega(H)$
cannot be surpassed by \emph{any} next-token prediction algorithm, i.e., any method that minimizes a sequence of token-level or per-timestep losses.
  Formally, we introduce the abstract notion of an \emph{iterative learner} that is given $\pistar$ directly, but is limited in how it can be used.
\begin{definition}\label{def:iterative}
  For a given MDP $M$ and policy class $\Pi$, an \emph{iterative learner} is an algorithm that, for any expert policy $\pistar$, produces an estimate $\pihat \in \Pi$ ``autoregressively'' as follows: for $h = 1,\dots,H$, it defines $\pihat_h$ as some (potentially randomized) function of $\pistar_{1:h}$ and $\pihat_{1:h-1}$.\arxiv{\footnote{Notably, the iterative learner can draw samples from $\pistar_{1:h}$ (it has full knowledge of the underlying MDP $M$) and compute any function thereof. We require the learner to be proper as otherwise, it could output $\pihat_h := \pistar_h$, since there is no statistical error.}}%
\end{definition}
%

This definition is most meaningful if the policy class $\Pi$ has no \emph{parameter sharing} across layers, i.e., there are families $\Pi_{1:H}$ so that $\pi\in\Pi$ if and only if $\pi_h \in \Pi_h$. In this case, any estimator defined by a loss function that decomposes additively across layers---including $\loglossbc$ and $\smoothedloglossbc$, but not $\rhobc$---is an iterative learner (\cref{prop:iterative-simulation}), though an iterative learner has additional flexibility (e.g., $\pihat_h$ may depend on $\pihat_{1:h-1}$ in some clever way). This flexibility
notwithstanding, we show that any iterative learner incurs linear dependence on $H$.

\begin{theorem}%
  \label{thm:ntp-lb}
  Fix $H \in \NN$. There is an $H$-step autoregressive MDP $M$ and a policy class
  $\Pi$ with no parameter sharing, so that for any iterative learner, there exists a policy $\pistar$ such that\loose
  {
%
\begin{equation} \EE\left[\Dhels{\BP^{\pihat}}{\BP^{\pistar}}\right] \geq \Omega(H) \cdot \min_{\pi \in \Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}}\label{eq:ntp-lb}\end{equation}
}where $\pihat$ is the (potentially random) output of the iterative learner, and $\min_{\pi \in \Pi} \Dhels{\BP^{\pi}}{\BP^{\pistar}} = 2^{-H}$. %
\end{theorem}

\cref{thm:ntp-lb} implies that $\Capx = \Omega(H)$ is a barrier for estimators defined by layer-wise loss functions, regardless of how many samples they are given.\footnote{Since $\log|\Pi|=\Omega(H)$ for any class with no parameter sharing, the rate term $\vepsstat(n)$ will scale with $H$ for any estimator, but \cref{thm:ntp-lb} holds in an infinite-data limit $n \to \infty$, so it is fundamentally a statement about $\Capx$.} In fact, since $\pihat_h$ may depend on $\pihat_{1:h-1}$, \cref{thm:ntp-lb} even applies to some \emph{online/interactive} imitation learning algorithms, e.g., \texttt{Forward} \citep{ross2011reduction}. The caveat of \cref{thm:ntp-lb} is that the misspecification in the construction is exponentially small; finding a stronger construction is an interesting technical question. We prove the result by embedding a ``consistency game'' in the learning task---see \cref{sec:limits-app}.%
\arxiv{ 

}
$\loglossbc$ (nearly) matches \cref{eq:ntp-lb} under either bounded densities or with smoothing. We remark that a \emph{layerwise} version of $\rhobc$ matches \cref{eq:ntp-lb} with no assumptions  (\cref{prop:layerwise-rho}), but unlike $\loglossbc$, it requires optimizing an objective that is non-convex even for autoregressive linear models.\loose

