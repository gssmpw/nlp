\section{Related Work}
% Our work is closely aligned with text-to-image and text-to-diagram generation, for which we offer a thorough review of the most relevant prior research.

\noindent 
\textbf{Text-to-Image Generation.}
Text-to-image generation~\cite{zhang2023text, bie2024renaissance, jia2024human} has become a rapidly growing field in computer vision and machine learning.
% , driven by advancements in deep generative models. 
This progress traced back to the emergence of Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative}, which paved the way for research focused on generating images from textual prompts~\cite{reed2016generative, tao2022df, xu2018attngan,zhang2021cross,zhang2017stackgan,zhang2018stackgan++}.
Transformer-based autoregressive models~\cite{ding2021cogview, gafni2022make, ramesh2021zero, yu2022scaling} have attracted significant attention due to their strong capabilities in modeling text-image alignment, as demonstrated by typical models such as DALL-E~\cite{ramesh2021zero} and STAR~\cite{ma2024star}. 
% Recent work Fluid~\cite{fan2024fluid} trains a random-order autoregressive model on continuous tokens, leveraging multi-level attention to fine-tune the alignment of object layouts and sizes. 
In parallel, diffusion models~\cite{gu2022vector, nichol2021glide,ramesh2022hierarchical, rombach2022high,saharia2022photorealistic} have emerged as a prominent type of generative model for image generation, achieved through the gradual introduction of noise in iterative steps. Notable examples include Imagen~\cite{saharia2022photorealistic}
% which produces high-resolution images with remarkable detail
and others focus on improving compositionality,
e.g., attribute binding~\cite{chefer2023attend,feng2022training}. 
% PreciseControl~\cite{parihar2024precisecontrol}, which allows fine-grained control over specific attributes.

Although these approaches have advanced the generation of realistic scene imagery and propelled text-to-image generation into the spotlight of machine learning research, they struggle with tasks that demand precise control over complex structures and intricate relationships. This includes the generation of diagrams in fields like geometry, architecture, or other technical domains.

\noindent 
\textbf{Text-to-Diagram Generation.}
Generating diagrams from text has long been an intriguing area of research and has recently garnered considerable attention, driven by the success of text-to-image generation. Early efforts~\cite{ghosh2018automated,shahbaz2011automatic, btoush2015generating} primarily focused on generating entity-relationship diagrams, utilizing semantic heuristics to identify entities, attributes, and relationships from natural language specifications. With the rise of LLMs in various language generation tasks~\cite{touvron2023llama, touvron2023llama1, openai2024gpt,chung2024scaling, mann2020language, chowdhery2023palm}, recent work has also leveraged LLMs to facilitate spatial control in diagram generation. 
% Researchers have explored various ways to enable precise, fine-grained spatial control, which is crucial for generating accurate diagrams. 
These methods can be generally classified into two categories: layout-guided models and code-guided methods. Layout-guided approaches, exemplified by DiagrammerGPT~\cite{zala2023diagrammergpt}, employ a two-stage framework that first leverages LLMs to plan layout, then applies layout-guided diffusion models.
% to refine diagram accuracy. 
Code-guided methods, such as AutomaTikZ~\cite{belouadi2023automatikz}, fine-tune LLMs on large TikZ datasets to generate code for scientific vector graphics, while DiagramAgent~\cite{wei2024words} introduces a four-agent framework leveraging code for text-to-diagram generation and editing.

In geometric diagram generation, both existing approaches face significant limitations. First, image generators suffer from limited spatial fidelity~\cite{gokhale2022benchmarking, chatterjee2024revision, chatterjee2024getting}, despite extensive research in the layout-to-image field~\cite{li2023gligen, yang2023reco,balaji2022ediff, singh2023high, couairon2023zero, xie2023boxdiff}. This limitation prevents these methods from fulfilling precise geometric constraints.
Second, code-guided models for diagram generation are restricted by the capabilities of text-to-code models~\cite{roziere2023code,fried2022incoder,li2022competition,hui2024qwen2,guo2024deepseek}, which rely on large, data-intensive datasets for effective performance.

In contrast, we propose a training-free method that avoids the need for supervised data, leveraging precise point coordinates to enforce stringent geometric constraints. Our approach shares similarities with~\citet{zhengyu2023precise}, which also utilizes point coordinates, but diverges in three key aspects. 1) We leverage the zero-shot capabilities of LLMs to extract points and constraints, bypassing the labor-intensive process of building entity relationship extractors. 2) We introduce a self-verification module to correct LLM-extracted information when the optimization problem is unsolvable. 3) We leverage text-to-code LLMs for TikZ code generation, enabling richer textual insights such as point connections, a capability not fully explored in~\citet{zhengyu2023precise}. Finally, empirical results demonstrate our system's ability of generating complex geometric diagrams.





% \noindent 
% \textbf{LLM with External Planner \& Solver.}



%Scientific Vector graphics generation