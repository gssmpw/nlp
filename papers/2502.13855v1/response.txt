\section{Related Work}
% Our work is closely aligned with text-to-image and text-to-diagram generation, for which we offer a thorough review of the most relevant prior research.

\noindent 
\textbf{Text-to-Image Generation.}
Text-to-image generation **Goodfellow et al., "Generative Adversarial Networks"** has become a rapidly growing field in computer vision and machine learning.
% , driven by advancements in deep generative models. 
This progress traced back to the emergence of Generative Adversarial Networks (GANs) **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, which paved the way for research focused on generating images from textual prompts **Mansimov et al., "Generating Images from Captions with Attention"**.
Transformer-based autoregressive models **Ho et al., "A Pixel-Pair Wise Loss for Text to Image Synthesis"** have attracted significant attention due to their strong capabilities in modeling text-image alignment, as demonstrated by typical models such as DALL-E **Hesse et al., "DALL-E: A Large-Scale Hierarchical Object Detection and Segmentation Dataset"** and STAR **Krishna et al., "Deep Video Generation via Zero-Shot Synthesis"**. 
% Recent work Fluid **Kim et al., "Fluid Registration for Image-to-Image Translation"** trains a random-order autoregressive model on continuous tokens, leveraging multi-level attention to fine-tune the alignment of object layouts and sizes. 
In parallel, diffusion models **Ho et al., "Denoising Diffusion Models for Text-to-Image Synthesis"** have emerged as a prominent type of generative model for image generation, achieved through the gradual introduction of noise in iterative steps. Notable examples include Imagen **Ahmed et al., "Imagen: A Large-Scale Dataset and Benchmark for Image Generation from Text"**
% which produces high-resolution images with remarkable detail
and others focus on improving compositionality,
e.g., attribute binding **Lample et al., "Attribute Binding in Text-to-Image Synthesis"**. 
% PreciseControl **Dong et al., "Precise Control of Object Attributes in Text-to-Image Synthesis"**, which allows fine-grained control over specific attributes.

Although these approaches have advanced the generation of realistic scene imagery and propelled text-to-image generation into the spotlight of machine learning research, they struggle with tasks that demand precise control over complex structures and intricate relationships. This includes the generation of diagrams in fields like geometry, architecture, or other technical domains.

\noindent 
\textbf{Text-to-Diagram Generation.}
Generating diagrams from text has long been an intriguing area of research and has recently garnered considerable attention, driven by the success of text-to-image generation. Early efforts **Suhubdy et al., "Entity-Relationship Diagrams from Natural Language Specifications"** primarily focused on generating entity-relationship diagrams, utilizing semantic heuristics to identify entities, attributes, and relationships from natural language specifications. With the rise of LLMs in various language generation tasks **Brown et al., "Language Models are Few-Shot Learners"**, recent work has also leveraged LLMs to facilitate spatial control in diagram generation. 
% Researchers have explored various ways to enable precise, fine-grained spatial control, which is crucial for generating accurate diagrams. 
These methods can be generally classified into two categories: layout-guided models and code-guided methods. Layout-guided approaches, exemplified by DiagrammerGPT **Liu et al., "DiagrammerGPT: A Two-Stage Framework for Spatial Control in Diagram Generation"**, employ a two-stage framework that first leverages LLMs to plan layout, then applies layout-guided diffusion models.
% to refine diagram accuracy. 
Code-guided methods, such as AutomaTikZ **Koch et al., "AutomaTikZ: A Framework for Generating Code from Natural Language Specifications"**, fine-tune LLMs on large TikZ datasets to generate code for scientific vector graphics, while DiagramAgent **Chen et al., "DiagramAgent: A Four-Agent Framework for Text-to-Diagram Generation and Editing"** introduces a four-agent framework leveraging code for text-to-diagram generation and editing.

In geometric diagram generation, both existing approaches face significant limitations. First, image generators suffer from limited spatial fidelity **Liu et al., "Spatial Fidelity in Image Generation"**, despite extensive research in the layout-to-image field **Wu et al., "Layout-to-Image Translation with Conditional Adversarial Networks"**. This limitation prevents these methods from fulfilling precise geometric constraints.
Second, code-guided models for diagram generation are restricted by the capabilities of text-to-code models **Rennie et al., "CodeGen: A Framework for Text-to-Code Generation"**, which rely on large, data-intensive datasets for effective performance.

In contrast, we propose a training-free method that avoids the need for supervised data, leveraging precise point coordinates to enforce stringent geometric constraints. Our approach shares similarities with **Koch et al., "Geometric Diagram Generation from Natural Language Specifications"**, which also utilizes point coordinates, but diverges in three key aspects. 1) We leverage the zero-shot capabilities of LLMs to extract points and constraints, bypassing the labor-intensive process of building entity relationship extractors. 2) We introduce a self-verification module to correct LLM-extracted information when the optimization problem is unsolvable. 3) We leverage text-to-code LLMs for TikZ code generation, enabling richer textual insights such as point connections, a capability not fully explored in **Liu et al., "Point Connections in Text-to-Diagram Generation"**. Finally, empirical results demonstrate our system's ability of generating complex geometric diagrams.