\section{Method}
\label{sec:method}
In this section, we propose a posterior score averaging methods for point cloud denoising. We first summarize the proposed method, then elaborate the details of each proposed step and the network structure. Finally, we give our loss function and further analyze our method.
%-------------------------------------------------------------------------
    \subsection{Overview}
We propose Noise2Score3D, which learns a score function from only noise-contaminated point cloud data, and then estimates a clean point cloud using Tweedie's formula. Specifically, Noise2Score3D consists of two steps: estimating the score function using the noisy point cloud and then estimating the clean point cloud using Tweedie's formula.
This decoupling allows Noise2Score3D to have unique advantages over other unsupervised deep learning based methods, the most important of which is that the loss function is generalized during the training phase, and the same loss function can be used regardless of the noise model and parameters. Secondly, the output of the network is only the score, and the denoising process is done in one step, which distinguishing it from iterative denoising methods such as TotalDn \cite{hermosilla2019TotalDenoising} and ScoreDenoise \cite{luo_score-based_2021}. An additional advantage is that the property can be utilized to estimate unknown noise parameters without retraining the neural network.  As a result, our approach is simple to train and use, and does not expect the user to provide parameters to characterize the surface or noise model.
%-------------------------------------------------------------------------
    \subsection{Tweedie's denoising formula for exponential family distributions}
    \label{secTweedie'sformula}
We employ Tweedie’s denoising formula to handle noise data following exponential family distributions, a concept first introduced by Noise2Score \cite{noise2score2021}. In their work, they demonstrated that it is possible to learn a mapping from noisy images to clean images without supervision of clean data. This idea serves as a key inspiration for our method, as the formula provides an explicit solution for the posterior expectation. The core idea of Tweedie’s formula is to estimate the mode of the posterior distribution through the gradient of the log-likelihood function, i.e., the score function. In our approach, we extend this concept to the task of 3D point cloud denoising.

First, assume that the noisy point cloud data $Y$ is generated by adding Gaussian noise to the clean point cloud data $X$, i.e.,
\begin{eqnarray}
Y = X + w, \quad w \sim N(0, \sigma^2I).
\end{eqnarray}
where $w$ represents independent and identically distributed Gaussian noise with zero mean and variance $\sigma^2$. According to Tweedie’s formula, the posterior expectation of the clean point cloud $X$ given $Y$ can be computed as:
\begin{eqnarray}
E[X \mid Y] = Y + \sigma^2 \nabla_Y \log p(Y).
\end{eqnarray}
where $\nabla_Y \log p(Y)$ is the score function of $Y$ data distribution. This formula provides the optimal posterior estimation of $X$ in the sense of Minimum Mean Squared Error (MMSE)\cite{Serfling1980ApproximationTO}.

More generally, Tweedie’s formula has been generalized to arbitrary exponential family distribution \cite{efron2011tweedie}.  Combined with the score function estimation, Tweedie’s formula provide a unified approach to the denoising problem under different noise models. As noted in \cite{noise2score2021}, efficient denoising can be achieved by utilizing sufficient statistics of the noise distribution even if the noise parameters are unknown. It is note-worthy that once trained, there is no need to retrain the neural network for different noise level, making our denoising algorithm generalizing better compared to traditional methods.

%-------------------------------------------------------------------------
    \subsection{Denoising score estimation}
We build upon the KPConv architecture introduced by Thomas \etal \cite{KPconv} to estimate the score function $\nabla_Y \log p(Y)$ . 
KPConv network is a point cloud feature encoder which accurately captures local geometric structures of point clouds and adapts to varying point densities. We modified it to accept only the XYZ coordinates of the noisy point cloud $Y$ as input, and output the estimated score for each point. The network architecture is illustrated in \cref{fig:kpconv}. 
The output score values are interpreted as the values statistically closest to the gradient of the log-probability density function at each point.
\begin{eqnarray}
S(Y) = \nabla_Y \log p(Y).
\end{eqnarray}
The estimated score function is essential for reconstructing the clean point cloud using Tweedie's formula as detailed in Section~\ref{secTweedie'sformula}.

%-------------------------------------------------------------------------
    \subsection{Loss function}
The loss function is crucial for training the network to accurately estimate the score function $\nabla_Y \log p(Y)$ from noisy point cloud data.
In the unsupervised setting, we assume that clean point cloud $X$ is not accessible during training. Therefore, any loss function that requires knowledge of $X$ is not feasible. Instead, we need a loss function that relies solely on the noisy observations $Y$.

To achieve this goal, we employ an unsupervised learning method inspired by the Denoising Score Matching proposed by Lim \cite{vincent2011connection} and the Amortized Residual Denoising Autoencoder (AR-DAE) \cite{lim2020ar-dae}. These are further developed in the Noise2Score framework \cite{noise2score2021}, and their neural network model trained with AR-DAE was able to stably estimate the score function only from noisy images. This inspires us to employ AR-DAE to construct our loss function from noisy data. The loss function is defined as follows:
\begin{eqnarray} 
\mathcal{L}_{\text{AR-DAE}} = \frac{1}{N} \sum_{i=1}^{N} \left\| \sigma_t \cdot S(Y'_i) + u \right\|^2.
\end{eqnarray}
where
$N$ is the number of points in the point cloud,
$Y'_i$ is a perturbed version of a point $Y_i$ in the noisy point cloud $Y$, generated by adding noise: $Y'_i = Y_i + u \cdot \sigma_t$, where $u \sim \mathcal{N}(0, I)$ represents random Gaussian noise.
$\sigma_t$ is the noise standard deviation during training. 
$S(Y'_i)$ is the network's estimated score at the perturbed point $Y'_i$.
$u$ is an additional random noise vector sampled from a Gaussian distribution with mean 0 and covariance matrix $I$.

The loss function aims to minimize the difference between the scaled estimated score $\sigma_t \cdot S(Y'_i)$ and the additional noise $u$. By doing so, the network learns to approximate the negative residual noise scaled by the noise level, which corresponds to the score function of the noisy data distribution.

%-------------------------------------------------------------------------
\begin{table*}[htbp]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{cc|cccccc|cccccc}
\hline
\multicolumn{2}{c|}{Number of points}         & \multicolumn{6}{c|}{Gaussian 10k} & \multicolumn{6}{c}{Gaussian 50k}  \\
\multicolumn{2}{c|}{Noise level}     & \multicolumn{2}{c}{1\%} & \multicolumn{2}{c}{2\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{1\%} & \multicolumn{2}{c}{2\%} & \multicolumn{2}{c}{3\%} \\ \cline{3-14} 
\multicolumn{1}{c|}{Dataset}   & Model  & CD & P2M & CD & P2M & CD & P2M & CD & P2M & CD & P2M & CD & P2M   \\ \hline
\multicolumn{1}{c|}{\multirow{5}{*}{Modelnet-40}} 
& TotalDn\cite{hermosilla2019TotalDenoising} &8.079 &4.778  &18.031 &12.277  &29.617 &21.673     & 5.044& 4.442  & 13.130  & 11.165      & 22.627      & 19.334            \\
\multicolumn{1}{c|}{}  & DMR \cite{luo2020DMR}  &\underline{5.531} &\bf2.918  &\bf7.174 &\bf4.054  &\bf12.382 &\bf8.103  &\bf1.672 & \bf1.794   & \underline{4.430} & \bf3.963  &\underline{12.868} &\underline{11.106}      \\
\multicolumn{1}{c|}{}  & DMR-u\cite{luo2020DMR} &8.210 &5.044  &12.770 &8.201  &22.086 &15.602  & 3.250 & 2.946  & 10.430 & 8.975  & 23.596 & 20.426    \\
\multicolumn{1}{c|}{}  & Score-u\cite{luo_score-based_2021} &5.514 &2.975  &11.072 &6.412  &18.239 &11.335  & 2.696 & 2.317   & 10.153  &  8.269   & 26.169  & 21.664  \\ \cline{2-14} 
\multicolumn{1}{c|}{} & \bf Ours  &\bf5.283 &\underline{3.212}  &\underline{8.624} &\underline{5.332}  &\underline{12.791} &\underline{8.568}      & \underline{1.881} &\underline{1.988}  &\bf4.393  &\underline{4.002}   &\bf8.917  &\bf8.181  \\ \hline
\end{tabular}}
\end{center}
\caption{Comparison of denoising performance of different algorithms on ModelNet-40 with Gaussian noise. The values for CD and P2M are multiplied by $10^4$. The best and second-best results are shown in \textbf{bold} and \underline{underlined}.}
\label{table:quantitativemd}
\end{table*}

%-------------------------------------------------------------------------
\subsection{Denoising with unknown noise parameters}
\label{sec:TVPC}
For real world scenarios, the noise level of point cloud is often unknown. While our method does not depend on clean point cloud data, a good estimate of the noise parameters is crucial for denoising outcomes. To deal with this case, We proposed a in-loop denoising procedure to estimate the unknown noise parameters with proposed Total Variation for Point Cloud. 

Inspired by Total Variation (TV) regularization commonly used in image denoising \cite{RUDIN1992}, we adapted this concept to point cloud processing and introduced Total Variation for Point Cloud ($TV_{PC}$). The detailed explanation of $TV_{PC}$ is provided in the Supplementary Material. We estimate the noise parameter \( \sigma^* \) by minimizing the $TV_{PC}$ of the resulting point cloud:
\begin{equation}\label{eq:sigma}
\sigma^* = \arg\min\limits_\sigma TV_{PC}\left(\hat{X}(\sigma)\right).
\end{equation}
where $\hat X(\sigma) = Y + \sigma^2 S(Y)$.
This allows us to automatically find the best noise parameter from the data, ensuring that the processed point cloud achieves optimal smoothness while minimizing geometric distortion. It is worth mentioning that $TV_{PC}$ can also be applied to iterative algorithms like ScoreDenoise to determine the number of iteration steps.
By leveraging $TV_{PC}$ as assessment metrics, we provide a way to evaluate denoising output and the unknown parameters in the denoising process, ensuring that the output maintains structural integrity while reducing noise.


