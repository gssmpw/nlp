\section{Related Work}
In recent years, Transformer has shown superior performance in the area of Natural Language Processing~(NLP), particularly machine translation and sequence-to-sequence tasks. A CO problem is essentially a sequence optimization problem, so their solutions can be described as sequential decisions. Therefore, TSP and EVRP, as typical CO problems, are also being studied using DRL methods. For example, Vaswani et al., "Attention Is All You Need" proposed a Graph Attention Network~(GAT) based on encoder that can provide high-dimensional node embedding and graph embedding for downstream tasks of EVRP. Guo et al., "Transformer-Based Deep Reinforcement Learning Method for Electric Vehicle Routing Problem" formulated an energy consumption model rather than a traditional distance model using the transformer-based DRL method. To address the low efficiency for large-scale EVRP, Wang et al., "Efficient Two-Layer Model for Solving Large-Scale Electric Vehicle Routing Problem" designed a two-layer model that finds near-optimal solutions based on predefined feasibility conditions and rewards.

In addition, based on the self-play training strategy, AlphaGo Zero Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" achieved superhuman performance on the game of Go. Tesauro et al., "Improving Automated Auction Design and Analysis through Game-Theoretic Multiple Candidate Agents" used MCTS with warm-start enhancements to enhance the quality of the plays produced by self-play. Although AlphaGo Zero was designed for two-player games, many researchers attempted to apply the AlphaZero algorithm to single-player tasks by creating competitive environments. Most of them reconstruct reward mechanisms based on self-competition for different problems Zhang et al., "Multi-Agent Reinforcement Learning for Single-Player Games" . However, these methods do not use MCTS during training the policy network or value network, they only apply MCTS guided network after training. To address this issue, Lee et al., "Learning to Plan with Complete Information from Search Trees in Multi-Agent Reinforcement Learning" improved the policy by employing complete information from the MCTS search tree and learning the trajectory produced by MCTS. To save simulation costs, GAZ was proposed and in order to improve the GAZ-based policy network Li et al., "Efficient Game-Tree Search for Large-Scale Games with Multi-Agent Reinforcement Learning" , Liu et al., "Multi-Agent Reinforcement Learning for Single-Player Games" further developed a new framework.

Through self-competition, the agent learns to find strong trajectories by planning against potential strategies of its previous self and has shown higher performance on classical CO problems such as TSP and JSSP. But GAZ-based methods have not been investigated yet in multi-constrained EVRP with variable planning steps, which still remains a challenge.