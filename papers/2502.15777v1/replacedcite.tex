\section{Related Work}
In recent years, Transformer has shown superior performance in the area of Natural Language Processing~(NLP), particularly machine translation and sequence-to-sequence tasks. A CO problem is essentially a sequence optimization problem, so their solutions can be described as sequential decisions. Therefore, TSP and EVRP, as typical CO problems, are also being studied using DRL methods. For example,____ proposed a Graph Attention Network~(GAT) based on encoder that can provide high-dimensional node embedding and graph embedding for downstream tasks of EVRP.____ formulated an energy consumption model rather than a traditional distance model using the transformer-based DRL method. To address the low efficiency for large-scale EVRP,____ designed a two-layer model that finds near-optimal solutions based on predefined feasibility conditions and rewards. 

In addition, based on the self-play training strategy, AlphaGo Zero____ achieved superhuman performance on the game of Go.____ used MCTS with warm-start enhancements to enhance the quality of the plays produced by self-play. Although AlphaGo Zero was designed for two-player games, many researchers attempted to apply the AlphaZero algorithm to single-player tasks by creating competitive environments. Most of them reconstruct reward mechanisms based on self-competition for different problems____. However, these methods do not use MCTS during training the policy network or value network, they only apply MCTS guided network after training. To address this issue,____ improved the policy by employing complete information from the MCTS search tree and learning the trajectory produced by MCTS. To save simulation costs, GAZ was proposed and in order to improve the GAZ-based policy network____,____ further developed a new framework. 

Through self-competition, the agent learns to find strong trajectories by planning against potential strategies of its previous self and has shown higher performance on classical CO problems such as TSP and JSSP. But GAZ-based methods have not been investigated yet in multi-constrained EVRP with variable planning steps, which still remains a challenge.