@inproceedings{0547ebb970b949cf8ec4326c525f408a,
title = "Policy-Based Self-Competition for Planning Problems",
abstract = "AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent{\textquoteright}s historical performances and to reshape an episode{\textquoteright}s reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ {\textquoteleft}Play-to-Plan{\textquoteright} (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our approach in two well-known combinatorial optimization problems, the Traveling Salesman Problem and the Job-Shop Scheduling Problem. With only half of the simulation budget for search, GAZ PTP consistently outperforms all selected single-player variants of GAZ.",
keywords = "Reinforcement Learning, Machine Learning",
author = "Jonathan Pirnay and Quirin G{\"o}ttl and Jakob Burger and Grimm, {Dominik Gerhard}",
year = "2023",
language = "English",
booktitle = "The Eleventh International Conference on Learning Representations (ICLR)",
}

@ARTICLE{10608117,
  author={Wang, Mengqin and Wei, Yanling and Huang, Xueliang and Gao, Shan},
  journal={IEEE Internet of Things Journal}, 
  title={An End-to-End Deep Reinforcement Learning Framework for Electric Vehicle Routing Problem}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Decoding;Routing;Heuristic algorithms;Approximation algorithms;Optimization;Internet of Things;Batteries;Deep reinforcement learning;electric vehicle routing;attention mechanism},
  doi={10.1109/JIOT.2024.3432911}}

@INPROCEEDINGS{9356942,
  author={Wang, Hui and Preuss, Mike and Emmerich, Michael and Plaat, Aske},
  booktitle={2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={Tackling Morpion Solitaire with AlphaZero-like Ranked Reward Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={149-152},
  keywords={Scientific computing;Reinforcement learning;Games;Morpion Solitaire;Ranked Reward;Reinforcement Learning;AlphaZero;Self-play},
  doi={10.1109/SYNASC51798.2020.00033}}

@ARTICLE{9924526,
  author={Zhang, Ying and Li, Muyang and Chen, Yuanchang and Chiang, Yao-Yi and Hua, Yunpeng},
  journal={IEEE Transactions on Smart Grid}, 
  title={A Constraint-Based Routing and Charging Methodology for Battery Electric Vehicles With Deep Reinforcement Learning}, 
  year={2023},
  volume={14},
  number={3},
  pages={2446-2459},
  keywords={Roads;Batteries;Routing;Planning;Electric vehicles;Reinforcement learning;Costs;Constraint-based route planning;two-layer model;battery electric vehicles;deep reinforcement learning},
  doi={10.1109/TSG.2022.3214680}}

@article{Bansal2017EmergentCV,
  title={Emergent Complexity via Multi-Agent Competition},
  author={Trapit Bansal and Jakub W. Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.03748},
  url={https://api.semanticscholar.org/CorpusID:3921978}
}

@inproceedings{Danihelka2022PolicyIB,
  title={Policy improvement by planning with Gumbel},
  author={Ivo Danihelka and Arthur Guez and Julian Schrittwieser and David Silver},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:251648078}
}

@article{Silver2018AGR,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and L. Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
  journal={Science},
  year={2018},
  volume={362},
  pages={1140 - 1144},
  url={https://api.semanticscholar.org/CorpusID:54457125}
}

@article{TANG2023121711,
title = {Energy-optimal routing for electric vehicles using deep reinforcement learning with transformer},
journal = {Applied Energy},
volume = {350},
pages = {121711},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121711},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923010759},
author = {Mengcheng Tang and Weichao Zhuang and Bingbing Li and Haoji Liu and Ziyou Song and Guodong Yin},
keywords = {Electric vehicle routing problem, Minimize energy consumption, Deep reinforcement learning, Transformer},
abstract = {This paper presents an end-to-end deep reinforcement learning (DRL) approach aimed at efficiently determining energy-optimal routes for a group of electric logistic vehicles, with the objective of minimizing operating costs. First, an Energy-Minimization Electric Vehicle Routing Problem (EM-EVRP) is formulated with an energy consumption model for electric vehicles, rather than Distance Minimization EVRP commonly favored in the literature. The energy consumption model incorporates several factors such as vehicle dynamics, road information, and charging losses. Then, the problem is reformulated based on the Markov decision process and solved using the transformer-based DRL method. The policy network is designed following the Transformer structure, including an encoder, a feature embedding module, and a decoder, where the feature embedding module is added to provide contextual information. Finally, extensive experiments demonstrate the superior of the proposed DRL method over existing learning-based methods and conventional methods, in solving both EM-EVRP and DM-EVRP. Notably, the formulated EM-EVRP achieves greater cost reduction than the traditional DM-EVRP.}
}

@article{WANG2021104422,
title = {Learning to traverse over graphs with a Monte Carlo tree search-based self-play framework},
journal = {Engineering Applications of Artificial Intelligence},
volume = {105},
pages = {104422},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104422},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621002700},
author = {Qi Wang and Yongsheng Hao and Jie Cao},
keywords = {Combinatorial optimization, Deep learning, Reinforcement learning, Monte Carlo tree search},
abstract = {The combinatorial optimization (CO) problems on the graph are the core and classic problems in artificial intelligence (AI) and operations research (OR). For example, the Vehicle Routing Problem (VRP) and Traveling Salesman Problem (TSP) are fascinating NP-hard problems and have important significance for the existing transportation system. Traditional methods such as heuristics methods, exact algorithms, and solution solvers can already find approximate solutions on small-scale graphs. However, they are helpless for large-scale graphs and other problems with similar structures. Moreover, traditional methods often require artificially designed heuristic functions to aid decision-making. In recent years, more and more work has focused on applying deep learning and reinforcement learning (RL) to learn heuristics, which allows us to learn the internal structure of the graph end-to-end and find the optimal path under the guidance of heuristic rules. However, most of these still need manual assistance, and the RL method used has the problems of low sampling efficiency and small searchable space. This paper proposes a novel framework (called OmegaZero) based on Alphago Zero, which does not prescribe expert experience or label data but is trained through self-play. We divide the learning into two stages: in the first stage, we employ graph attention network (GAT) and GRU to learn node representations and memory history trajectories. In the second stage, we employ Monte Carlo tree search (MCTS) and deep RL to search for the solution space and train the model.}
}

@inproceedings{Wang2021AdaptiveWM,
  title={Adaptive Warm-Start MCTS in AlphaZero-like Deep Reinforcement Learning},
  author={Hui Wang and Mike Preuss and Aske Plaat},
  booktitle={Pacific Rim International Conference on Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:234482994}
}

@inproceedings{laterre2018ranked,
      title={Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization}, 
      author={Alexandre Laterre and Yunguan Fu and Mohamed Khalil Jabri and Alain-Sam Cohen and David Kas and Karl Hajjar and Torbjorn S. Dahl and Amine Kerkeni and Karim Beguir},
      year={2018},
      booktitle={AAAI Conference on Artificial Intelligence},
}

