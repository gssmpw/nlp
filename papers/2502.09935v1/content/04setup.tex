\section{Experimental setup}
\label{sec:setup}


\paragraph{Benchmark.} For evaluation, we adapt two benchmarks from~\cite{yang2024glyphcontrol} for the text editing. \textbf{SimpleBench} consists of 400 prompts following the template \textit{'A sign that says "\textless keyword\textgreater".'}, while \textbf{CreativeBench} includes 400 more complex prompts adapted from GlyphDraw~\cite{ma2023glyphdraw}, such as \textit{'Flowers in a beautiful garden with the word "\textless keyword\textgreater" written.'}.
The keywords used in the benchmarks are from a pool of single-word candidates from Wikipedia and categorized into four buckets based on their frequency: \textbf{$\text{Bucket}^{\text{1k}}_{\text{top}}$}, \textbf{$\text{Bucket}^{\text{10k}}_{\text{1k}}$}, \textbf{$\text{Bucket}^{\text{100k}}_{\text{10k}}$}, and \textbf{$\text{Bucket}^{\text{plus}}_{\text{100k}}$}. Both benchmarks contain the same set of keywords, which serve as text that should be generated in the images.
In this work, we use 100 prompts from each benchmark, with words from \textbf{$\text{Bucket}^{\text{1k}}_{\text{top}}$}, as a \textit{validation set}, and the remaining 300 prompts as a \textit{test set}. The prompts from these benchmarks serve as the source prompts $p_{S}$. To create the target prompt $p_{T}$ for each $p_{S}$, we use the same prompt template as in $p_{S}$, but select the keyword from a different source prompt, ensuring that the corresponding $p_{S}$ and $p_{T}$ differ only in the keywords.

\paragraph{Metrics.} We measure two main aspects of the generations. As text alignment, we refer to the correspondence to the keyword provided in the prompt. As image alignment, we calculate the quality of the image outside of the modified text (\eg background).
To measure the text alignment, we use the \textbf{OCR F1 Score}, which is calculated as follows:
$
    \text{F1 Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}},
$
where \textit{Precision} measures the ratio of predicted characters in the keyword, and \textit{Recall} measures the ratio of characters in the keyword that are covered by the prediction.
Additionally, we compute the \textbf{Levenshtein distance (LD)} between the keyword and the text predicted by the OCR model and
\textbf{CLIP-T Score}~\cite{clip} measuring the similarity of the target text (contained in the target prompt $p_{T}$) and the text in the edited image.
To measure the alignment between original and edited images, we calculate \textbf{Mean Squared Error (MSE)}, which is the average squared difference between the reference and generated images, indicating how close the generated image is to the reference; lower values indicate higher similarity. We also compute a \textbf{Structural Similarity Index Measure (SSIM)}~\cite{wang2004image} that evaluates the perceived quality of a generated image by comparing its luminance, contrast, and structure to a reference image, with higher values indicating greater similarity. Finally, we use the \textbf{Peak Signal-to-Noise Ratio (PSNR)}, which measures the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation, where the signal, in our case, is the reference image and the noise is the error introduced by editing the image; higher PSNR values indicated greater fidelity.

\textbf{Models.} We identify the layers responsible for text generation in the three recent DMs, namely SDXL~\citep{podell2023sdxl}, DeepFloyd IF~\citep{DeepFloydIF}, and SD3~\citep{esser2024scalingSD3}, that differ significantly in their architecture, especially in the text encoder parts and the implementations of attention layers.
To detect text in generated images, we use the EasyOCR model. We choose a non-multi-modal method for this task to ensure that OCR-based metrics are computed purely based on the text present in images. We observe that multi-modal OCR models tend to guess the text based on the visual context, even when not present in the image.
As a text detection model, we use the DBNet~\citep{Liao_Wan_Yao_Chen_Bai_2020}.
