\section{Background and Related Work}
\label{sec:related}






\paragraph{Text-to-Image diffusion models.}
Diffusion models~\citep{song2020,ho2020} approximate data distribution by training a noise estimator $\epsilon_\theta(x_t, t, y)$ to reverse the diffusion process. The synthetic images are then generated by sampling an initial Gaussian noise, denoted as $x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, and progressively removing the predicted noise at each time step $t = T, \ldots, 1$ up until obtaining clean data sample $x_0$.
The noise predictor $\epsilon_\theta(x_t, t, y)$ is usually implemented as a U-Net~\citep{ronneberger2015unet} or, recently, (as in SD3~\cite{esser2024scalingSD3}) a transformer-based model~\citep{vaswani2017attention,peebles2023scalable}.
In common text-to-image DMs~\citep{dalle_2,rombach2022high,saharia2022imagen,DeepFloydIF}, the conditioning input $y$ is a text embedding derived from a textual prompt $p$ using pre-trained text encoders, such as the text encoder from CLIP~\citep{clip} or the large language models like T5~\citep{raffel2020exploringT5}
as used in DeepFloyd IF~\citep{DeepFloydIF} or SD3~\citep{esser2024scalingSD3}).

\paragraph{Cross and Joint Attention layers.} The integration of text conditioning into the denoising process is achieved through cross-attention layers~\citep{vaswani2017attention}. The most standard cross-attention (used, \eg, in Stable Diffusion or SDXL~\citep{rombach2022high}) operates by computing three components: the query \( Q = h W^Q \), the key \( K = e W^K \), and the value \( V = e W^V \), where \( h \) and \( e \) represent the hidden image and text representations, respectively, and \( W^Q \), \( W^K \), and \( W^V \) are learnable weight matrices. The attention probabilities are then calculated using the following equation: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) \cdot V,$
where \( d \) is a scaling factor equal to the dimension of the queries and keys.
More recent diffusion models extend this mechanism further. Specifically, the DeepFloyd IF~\citep{DeepFloydIF} model implements cross-attention layers where the keys and values are formed by concatenating the projections of both \( h \) and \( e \). ~\citet{esser2024scalingSD3} further advance this mechanism by introducing a so-called \textit{joint attention}, where each attention component (\( Q \), \( K \), and \( V \)) is a concatenation of projections from both \( h \) and \( e \). Crucially, in this setup, both image and text projections are propagated throughout the diffusion model, in contrast to standard cross-attention layers where each attention block received the same static text-encoder embedding \( e \) as input. In our work, we demonstrate that our patching technique is invariant to these implementation changes and can be applied effectively across all of them.





\paragraph{Interpretability of diffusion models.}
Recent works have explored the inner workings of diffusion models by analyzing cross-attention layers~\citep{tang2022daaminterpretingstablediffusion,hertz2023prompttoprompt}. On the other hand, \citet{park2024explaining} explains the predictions of diffusion models at each denoising step using saliency maps.
Other research efforts have focused on localizing where specific concepts are stored within diffusion models. For instance, \citet{hintersdorf2024finding} pinpoint the memorization of individual training data samples within DMs at the neuron level in cross-attention layers, using the \textit{z-score}. \citet{basu2024-localizing-knowledge} develop a framework utilizing causal tracing~\citep{pearl2001causalTracing} to identify where knowledge of various styles, objects, or facts is stored within the Stable Diffusion model~\citep{rombach2022high}. In follow-up work, \citet{basu2024mechanistic} extend this framework by introducing a mechanistic approach to knowledge localization across different text-to-image DMs. Despite being effective across models with standard cross-attention implementations, such as Stable Diffusion XL (SDXL)~\citep{podell2023sdxl} and DeepFloyd IF~\cite{DeepFloydIF}, it lacks analysis on the most recent attention variants, such as \textit{joint attention}~\citep{esser2024scalingSD3}. In contrast, our approach localizes small fractions of components responsible for generating textual content and is applicable across different cross-attention variants.



\paragraph{Text rendering in diffusion models.}
Recent diffusion models, such as Stable Diffusion~\citep{rombach2022high}, generate high-quality images conditioned on text prompts but often struggle with rendering coherent visual text. To address this limitation, more advanced DM architectures (e.g., SDXL, Deep Floyd IF, SD3~\citep{esser2024scalingSD3}, and FLUX~\citep{flux}) incorporate multiple text encoders, often based on models like CLIP~\citep{clip} or large language models like T5~\citep{raffel2020exploringT5}, to enhance the quality of generated text within images.

In parallel with the above efforts, several other approaches have emerged to improve the fidelity of generated text by adding components to the generation pipeline. For example, TextDiffuser~\cite{textdiffuser} employs a two-stage process where a layout transformer~\citep{gupta2021layouttransformer} first identifies text coordinates as segmentation masks, which are later used to fine-tune a latent diffusion model to accurately inpaint or modify text based on prompts. Similarly, AnyText~\citep{tuo2024anytext} integrates an auxiliary latent module to process inputs like text glyphs or masked images and a text embedding module using OCR to blend stroke data with image caption embeddings. Additionally, other works incorporate extra conditioning during generation, such as ~\citet{zhang2024brush} with sketch images or ~\citet{yang2024glyphcontrol}, which leverages glyph instructions.















\paragraph{Fine-tuning diffusion models with LoRA.}
Low-Rank Adaptation (LoRA)~\citep{hu2022lora} is a fine-tuning approach known for its capacity to deliver high-quality results with both spatial and temporal efficiency. LoRA achieves this by introducing external low-rank weight matrices, which are optimized for the attention layers of the base model while keeping the pre-trained model weights unchanged. After the training process, these low-rank matrices define the adapted model, which can then be applied to the target task. Recently, \citep{frenkel2024implicit} introduced B-LoRAs that leverage LoRA to explicitly disentangle the style and components of an image. In our work, we tune the localized layers using LoRA to further improve the generated text within images.

\paragraph{Controlling diffusion models with cross-attention.} In~\Cref{app:related_cross}, we further describe related work on text-to-image models fine-tuning and image editing by leveraging cross-attention layers and manipulating the denoising steps through keys and values.











