\section*{\LARGE Appendix}\label{appendix}

\section{Related work on manipulating diffusion models with cross-attention} \label{app:related_cross}
Recent works introduced methods that leverage cross-attention layers for better control of text-to-image models. In~\cite{kumari2022multiconcept}, the authors present an efficient way of customization of text-to-image diffusion models by fine-tuning a subset of cross-attention layer parameters. While their approach demonstrates that targeting the key and value matrices in all the cross-attention layers is sufficient to introduce new concepts, we reveal that fine-tuning those matrices in fewer than 5\% of cross-attention layers (see~\Cref{tab:summary_loc}) enables better quality of the generated text.

\cite{DBLP:conf/iclr/GeyerBBD24} presents a framework that enables video editing using text-to-image diffusion models. Specifically, the authors introduce a method of editing the keyframes by extension of self-attention layers in which the keys from all timeframes are concatenated in order to encourage the frames to share a global appearance. The presented solution offers an effective approach to the semantic video edition.

Prompt-Mixing~\citep{Patashnik_2023_ICCV} enables users to explore different shapes of objects in an image. In order for objects to stay in the same positions but change their appearance, the method operates in the inference time and, in different denoising timestep intervals, injects different prompts into the cross-attention layers. In our work, we use a similar injection mechanism that we apply only to the selected text-controlling layers. We evaluate the effect of injection at different denoising steps in the~\Cref{fig:models_timestep}.

Cross-Attention Refocusing~\citep{phung2023grounded} is a calibration technique enabling better attending of tokens representing objects to image regions. By performing multiple intermediate latent optimizations by using CAR loss and Self-Attention Refocusing loss, authors achieve better controllability of the layout of generated objects. Similar to our work, CAR focuses on cross-attention maps but aims to strengthen attention to the correct token while reducing it elsewhere.

Plug-and-Play~\citep{Tumanyan_2023_CVPR} is an effective image-to-image translation method. In this work, the authors show that in the denoising procedure, one can extract spatial features from the U-Net decoder's Residual Blocks and their following self-attention layers, obtaining encodings of the composition of the image. Next, by passing a different prompt during the denoising procedure for the same initial Gaussian noise, one can inject previously extracted features and obtain generations, differing in image attributes specified in the condition. In this work, we show that by focusing on text-related features we can perform a precise edition by targeting a single attention layer.

\section{Selection of Denoising Timesteps}



To further refine the identification of text generation capabilities in DMs, we investigate from which point in the diffusion denoising process the key and value matrices should be patched to achieve the highest performance in text editing.
We present the results of this analysis in \Cref{fig:models_timestep}. We observe that when starting the patching from the later timesteps $t$, we can observe better preservation in the visual attributes of a modified image and improve the quality of the generated text, increasing its similarity to the text from the target prompt $p_B$.
This trend aligns with the work by \cite{hertz2023prompttoprompt}, where authors show that only the overall structure of an image is generated in the initial steps of the diffusion denoising process. Thus, in order to reduce the change in visual attributes, we apply our patching method to localized attention layers starting from timesteps: $t_s=46$ for \SDXL, $t_s=26$ for SD3, and $t_s=48$ for DeepFloyd IF. Attention activations from timestep $T$ to $t_s-1$ remain unchanged while we patch all activations from timestep $t_s$ to $0$.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_sd3_image.pdf}
        \caption{Image alignment vs Diffusion Patching Timestep SD3.
        }
        \label{fig:sd3_timestep_image}
    \end{subfigure}
    \hspace{0.05\textwidth} %
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_sd3_text.pdf}
        \caption{Text alignment vs Diffusion Patching Timestep SD3.
        }
        \label{fig:sd3_timestep_text}
    \end{subfigure}

    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_if_image.pdf}
        \caption{Image alignment vs Diffusion Patching Timestep DeepFloyd IF.
        }
        \label{fig:if_timestep_image}
    \end{subfigure}
    \hspace{0.05\textwidth} %
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_if_text.pdf}
        \caption{Text alignment vs Diffusion Patching Timestep DeepFloyd IF.
        }
        \label{fig:if_timestep_text}
    \end{subfigure}

    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_sdxl_image.pdf}
        \caption{Image alignment vs Diffusion Patching Timestep SDXL.
        }
        \label{fig:sdxl_timestep_image}
    \end{subfigure}
    \hspace{0.05\textwidth} %
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/timestep_sdxl_text.pdf}
        \caption{Text alignment vs Diffusion Patching Timestep SDXL.
        }
        \label{fig:sdxl_timestep_text}
    \end{subfigure}

    \caption{\textbf{Starting the text edition from a later diffusion timestep improves both image and text alignment.}
        We analyze the impact of the diffusion timestep from which we start the patching on the image and text alignment. We observe that we can find an optimum diffusion timestep that can simultaneously improve image and text quality.
    }
    \label{fig:models_timestep}
\end{figure}







\newpage
\section{LoRA fine-tuning across different setups}
To further strengthen the evidence that we have correctly identified the cross-attention layers responsible for the content of the generated text, we conduct the LoRA fine-tuning process on other sets of three cross-attention layers. These sets are selected based on the OCR F1 Scores presented in \Cref{fig:loc} â€” specifically, we select three sets of adjacent layers with the \textit{highest} and \textit{lowest} sum of F1 scores, respectively. As shown in \Cref{fig:lora_ocr2}, we observe a significant performance gap between the fine-tuned layers we localized and any other set of layers. Notably, some of the chosen layer sets even decrease performance compared to the base SDXL model.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/lora2.pdf}
    \caption{
        \textbf{LoRA SDXL Fine-Tuning Across Different Setups.} We fine-tune LoRA applied to the SDXL model to improve the text generation capabilities of the base model. When we fine-tune LoRA on all cross-attention layers, the model quickly collapses and loses its ability to generate examples that match the prompt. In contrast, when we fine-tune LoRA only on our localized three cross-attention layers, we successfully prevent model overfitting while also improving text generation quality. This trend is not observed when we apply LoRA to other sets of three layers.}
    \label{fig:lora_ocr2}
\end{figure}


\section{LoRA Fine-tuning with different training set sizes}
To evaluate how our findings from ~\Cref{sec:finetuning} generalize to varying training set sizes, we fine-tune LoRA applied to the SDXL model on datasets ranging from 20k to 200k samples. To mitigate potential overfitting, especially in configurations where LoRA is applied to every cross-attention layer (\textit{Full model} setup), we scale the training set size up to 200k samples. We train each setup for 12k steps with a batch size equal to 512 and a learning rate of 1e-6.

In ~\Cref{fig:prec_rec_clip_scaled}, we plot the recall and precision metrics across training steps. Notably, even with a substantially larger dataset in the \textit{Full model 200k} configuration, the model exhibits a similar collapse to what is observed when training on smaller subsets. Moreover, both recall and precision remain largely unchanged across different setups, demonstrating the robustness of our approach, which focuses on fine-tuning specific layers.

Additionally, in ~\Cref{fig:f1_clip_scaled}, we plot the OCR F1 Score and CLIP-T metrics, highlighting that fine-tuning localized layers, even with as few as 20k samples, results in better performance than the \textit{Full model} setup trained with 200k samples.



\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/clip_prec_recall_grid.pdf}
    \caption{\textbf{Scaling up training size when fine-tuning all cross-attention layers does not prevent model collapse.} Increasing the training dataset size fails to mitigate model collapse, as evidenced by the significant drop in Recall and Precision metrics. In contrast, our approach, which fine-tunes only localized cross-attention layers, demonstrates consistent performance regardless of training set size.
        \label{fig:prec_rec_clip_scaled}}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/f1_clip_grid.pdf}
    \caption{\textbf{LoRA fine-tuning of localized layers outperforms fine-tuning of all cross-attention layers, even with smaller datasets.} LoRA fine-tuning of localized layers achieves consistent performance across all evaluated training set sizes, from 20k to 200k samples. While increasing the dataset size slightly improves the performance of the model when all cross-attention layers are fine-tuned, a noticeable performance gap remains compared to localized fine-tuning.
        \label{fig:f1_clip_scaled}}
\end{figure}

\section{Study on the number of injected layers in SDXL}
\label{app:layers_study}

We carry out the study on the number of injected layers in SDXL in \Cref{tab:stress-test-app}. We observe that leveraging more layers for the injection implies a higher alignment of visual text to the target prompt while lowering the background preservation to the source prompt. Using $3$ layers in the Stable Diffusion XL model leads to obtaining the final image with text nearly as good as if injected to all the layers, yet preserves the background close to $1$-layer injection.

\begin{table}[htbp]
    \centering
    \caption{
        \textbf{Preservation-edition trade-off in SD-XL}. Injecting the target prompt into more layers enhances the text edition but also preserves less background from the source prompt.}
    \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{l||ccc|cc|cc}
            \toprule
            \textbf{\# layers injected} & \multicolumn{3}{c|}{\textbf{Image Alignment}} & \multicolumn{2}{c|}{\textbf{OCR F1}} & \multicolumn{2}{c}{\textbf{CLIP-T}}                                                                   \\
            (layers idx)                & MSE $\downarrow$                              & SSIM $\uparrow$                      & PSNR $\uparrow$                     & Text$_{S}$ $\downarrow$ & Text$_{T}$ $\uparrow$ & $p_S$ & $p_T$ \\
            \hline
            \textbf{0} (-)              & 0.00                                          & 1.00                                 & 148.13                              & 0.34                    & 0.19                  & 0.85  & 0.71  \\
            \textbf{1} (55)             & 17.63                                         & 0.92                                 & 36.88                               & 0.28                    & 0.20                  & 0.82  & 0.73  \\
            \textbf{2} (55,56)          & 22.27                                         & 0.90                                 & 35.73                               & 0.20                    & 0.30                  & 0.75  & 0.81  \\
            \textbf{3} (55,56,57)       & 23.38                                         & 0.90                                 & 35.43                               & 0.19                    & 0.32                  & 0.74  & 0.82  \\
            \textbf{10} (54,55,...,63)  & 25.02                                         & 0.89                                 & 34.97                               & 0.19                    & 0.33                  & 0.74  & 0.82  \\
            \textbf{30} (34,36,...,63)  & 53.83                                         & 0.78                                 & 31.13                               & 0.19                    & 0.34                  & 0.73  & 0.84  \\
            \textbf{70} (0,1,...,69)    & 65.98                                         & 0.72                                 & 30.17                               & 0.19                    & 0.34                  & 0.72  & 0.85  \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:stress-test-app}
\end{table}





For the images presented in \Cref{fig:layers_comparison}, we use the following prompts.

Source prompts:~\begin{verbatim}
['A sign saying "teatre" in the aquarium', 
 '"handball" written on autumn leaves floating on the lake', 
 'A robot is holding a sign that says "mad"',
 'The slogan "sponsors" is written on the schoolbag']
\end{verbatim}

Target prompts:~\begin{verbatim}
['A sign saying "sponsors" in the aquarium',
 '"broadway" written on autumn leaves floating on the lake',
 'A robot is holding a sign that says "wisdom"',
 'The slogan "raimundo" is written on the schoolbag']
\end{verbatim}

\section{Safe Diffusion safety concepts}
\label{sec:safe_prompts}
To reproduce the results for Safe Diffusion~\citep{schramowski2023safe} in~\Cref{sec:preventing} we first run this method to all of analyzed models on the original safety concept introduced in the Safe Diffusion work: \textit{"hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty"}. Additionally, for a fair comparison, we also ask ChatGPT to extend this safety concept in order to prevent generation of toxic textual content on an image to: \textit{"an image showing hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty, profanity, toxic language, slurs, derogatory remarks, demeaning or offensive language, racist, sexist, homophobic, xenophobic, or otherwise harmful content aimed at individuals or groups, hate speech, inflammatory remarks, insults, and any form of dehumanization"}.

\newpage
\section{On preventing toxic generations with prompt substitution}\label{app:prompt_swap_discussion}

The simplest solution for preventing toxic textual content generation with text-to-image models is automatically detecting and substituting the harmful part of the original user's prompt.
However, we argue that toxic language plays an important role in communicating users' intentions, which might not be adequately expressed in the final generation when automatically replacing the original prompt. As observed by~\cite{jay2008pragmatics} \textit{,,The main purpose of swearing is to express emotions, especially anger, and frustration''}, so even though we might not want to directly generate samples with toxic text, we still want them to align with user's emotions, and reflect them in the generation. Nevertheless, because of the cross-attention mechanism, where tokens representing text can influence other aspects of the image, this might not be possible with automatic prompt replacement. To further highlight this problem, we run a simple experiment where we measure the influence of textual content on the emotional tone of the final generation. To that end, we generate samples with Stable Diffusion 3 model, following the prompt:

\emph{,,A photo of a young man holding a sign ,,[XYZ]'', visible strong emotion on his face"}


\noindent where we replace [XYZ] with either toxic text, or it's clean version as suggested by ChatGPT as presented in~\Cref{tab:template_toxic_prompts}.

\begin{table}[h!]
    \centering
    \caption{Examples of toxic prompts with their substitutions as suggested by ChatGPT when asked to remove only toxic words with as small changes to the text as possible. ($\ast$ added by authors).}
    \begin{tabular}{l|l}
        \toprule
        Toxic text                                      & LLM-suggested replacement            \\
        \hline
        \textit{F$\ast\ast$k this s$\ast\ast$t}         & \textit{Forget this mess}            \\
        \textit{Go f$\ast\ast$k yourself, d$\ast\ast$k} & \textit{Go fix yourself, dude}       \\
        \textit{F$\ast\ast$k off, i$\ast\ast$ot}        & \textit{You're a pain}               \\
        \textit{You d$\ast\ast$b f$\ast\ast$k}          & \textit{Back off, fool}              \\
        \textit{S$\ast\ast\ast$w you j$\ast\ast$k}      & \textit{S$\ast\ast\ast$w you, pal}   \\
        \textit{You f$\ast\ast$king m$\ast\ast\ast$n}   & \textit{You clumsy m$\ast\ast\ast$n} \\
        \textit{You're so f$\ast\ast$ked}               & \textit{You're so doomed}            \\
        \bottomrule
    \end{tabular}
    \label{tab:template_toxic_prompts}
\end{table}

We sample 10 generations for each prompt and their replacement suggestions, with the same random seeds pairwise. Given a set of toxic and non-toxic generations, we compare the expression we read from the face of a man holding a toxic or non-toxic sign. To that end, we use the DeepFace library~\citep{serengil2021lightface} that implements methods for detecting seven basic emotions from facial expressions. In Figure~\ref{fig:emotions_comparison}, we present the results of this comparison. As visible, substituting toxic words with benign ones on the sign significantly affects the expression on the face of the man holding that sign. This can be especially visible with the reduced score for \textit{angry} and higher score for \textit{neutral} expressions. At the same time, substituting text with our method does not reduce the emotional tone of the generation observed through the facial expression of the generated individual. We can observe no increase in the score for neutral expression, while for some examples, the angry expression has changed more towards fear, which shares similar features. We present several generations from this experiment in~\Cref{fig:emotions_comparison_generations}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/mean_se_barplot.pdf}
    \caption{
        Comparison of facial expression scores (average), extracted from generations of a man holding a sign with toxic texts. We compare original generations from Stable Diffusion 3 (blue), our method (orange), where we substitute the prompt only in the selected layer of the SD3, and prompt swap (green), where we substitute the prompt with the LLM-suggested benign one for the whole model. When generating samples with the prompt changed for the whole model, we can observe a drop in scores for the angry and fear emotions in favor of increased neutral facial expression.
        \label{fig:emotions_comparison}}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\linewidth]{figs/toxic_prompts_examples.pdf}
    \caption{
        Influence of generated text on the final generation. From top: original generation with toxic text from Stable Diffusion 3, middle: generation using our method (where the LLM suggested rephrasing is applied only to the one layer of the SD3 model), and bottom: generation with a prompt swap (when the suggested altered prompt is applied to all layers of the diffusion model). \textbf{Our method is able to generate images without toxic textual content while not affecting the emotional tone of the remaining part of the generation.}
        \label{fig:emotions_comparison_generations}}
\end{figure}



\newpage
\section{Toxic text prevention examples}\label{app:toxic_examples}
In~\Cref{fig:toxic_if_examples}, we show, for the Deepfloyd IF model, the qualitative comparisons of our method to Negative Prompt, Safe Diffusion, and Prompt Swap.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{figs/toxic_examples.pdf}
    \caption{Example results for methods for preventing toxic text in generated images. Negative Prompt and Safe Diffusion methods are incapable of removing foul words from the images. In Prompt Swap, the background of generated images is highly influenced by the suggested word. \textbf{We show that our method successfully changes foul words yet ensures minimal changes to the other visual aspects of the image.} Orange bounding boxes were added by the authors to cover four words.\label{fig:toxic_if_examples}}
\end{figure}


\newpage
\section{Pseudocode for layer localization}
We present in~\Cref{alg:localization} our method for creating a subset of diffusion model layers that control the content of visual text generated on images.

\begin{algorithm}
    \caption{Finding subset of layers $L_{ours}$ responsible for textual content generation}
    \begin{algorithmic} \label{alg:localization}
        \Require $P_S$: set of source prompts, $P_T$: set of target prompts, $L$: set of indices of cross-/joint-attention layers, $\theta$: threshold for acceptable OCR F$_{1}$-Score difference
        \Ensure $L_{ours}$: set of selected cross-attention layers

        \State $L_{F_1} \leftarrow [\ ]$ \Comment{initialize list of mean F$_{1}$-Scores for layers}
        \State $L_{ours} \leftarrow \emptyset$
        \State $N \leftarrow |P_S|$

        \For{$l \in L$} \Comment{compute F$_{1}$-Scores for each layer via patching}
        \State $I_{1..N} \leftarrow$ images generated with $L \setminus \{l\}$ receiving $P_S$ and $l$ receiving $P_T$
        \State $T_{1..N} \leftarrow$ text detected in $I_{1..N}$ using an OCR model
        \State $S_{1..N} \leftarrow$ F$_{1}$-Score between $T_{1..N}$ and $P_T$
        \State $L_{F_1}[l] \leftarrow {\frac{1}{N}} \Sigma_{1..N} S[i]$
        \EndFor

        \State $l_{max} \leftarrow \argmax_l L_{F_1}$
        \State $L_{ours} \leftarrow \{l_{max}\}$
        \For{$l \in L \setminus \{l_{max}\}$} \Comment{create a set of text control layers}
        \If{$(L_{F_1}[l_{max}] - L_{F_1}[l]) < \theta$}
        \State $L_{ours} \leftarrow L_{ours} \cup \{l\}$
        \EndIf
        \EndFor
        \State \textbf{return:} $L_{ours}$
    \end{algorithmic}
\end{algorithm}

\section{Parameter localization for the text style}\label{app:text_style}
In this section, we examine whether the cross-attention layers we localize in~\Cref{sec:ca_loc} control not only the content of the visual text generated in the images but also its style.

\paragraph{Experiment setup.} We use the Stable Diffusion 3 model, which, of all those tested, exhibits the best accuracy in generating text with the style specified in the prompt. We target four text styles: \textbf{handwritten}, \textbf{neon}, \textbf{graffiti} and \textbf{comic}. In this setup, both our source prompts $p_{S}$ and target prompts $p_{T}$ contain the same textual content to be generated but differ in the style of the text. In our experiments, we generate four sentences with the diffusion model: \textit{'hello world!'}, \textit{'happy new year'}, \textit{'I love you'}, and \textit{'Welcome to Asia'}. To ensure generalization and make sure that we do not localize layers for individual prompts, we use four prompt templates:~\begin{verbatim}
['Road sign with a {style} text saying {sentence}', 
'Notebook page with a {style} text saying {sentence}', 
'Street wall covered in {style} text saying {sentence}',
'Bus stop advertisement with {style} text saying {sentence}',
'Urban skatepark ramp with {style} text saying {sentence}']
\end{verbatim}

For measuring how a particular layer $l$ controls the style of the text, we perform the patching technique in the same way as described in~\Cref{sec:patching_technique} and calculate CLIP-T alignment between the generated images (after patching the keys and values in joint-attention layer $l$) and texts \textit{'text in {s} style'} where $s$ is a style from a target prompt $p_T$.

\paragraph{Results.} In~\Cref{fig:our_layer_style}, we show that the layer we localize in~\Cref{sec:ca_loc} for controlling content in visual text generated does not control the style of the text (left). Furthermore, we show (right) that in the Stable Diffusion 3 model, there is no single layer indicating the style of the generated text and that control over style in this model is distributed over multiple layers. To support this claim, we perform a study where we iteratively add the next layers with the highest response in the previous experiment and check how many of them are needed for the style to be modified. As shown in~\Cref{fig:style_layers}, it is necessary to patch at least $7$ out of $24$ layers to change the style of the generated text. However, the images resulting from patching so many layers are also significantly different in terms of other visual aspects. %
This shows that there is no layer-based separation of text style from the rest of the image elements in the Stable Diffusion 3 model, which makes our observations regarding textual content even more unique.

\begin{figure}[h]
    \centering
    \begin{minipage}[h]{0.5\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figs/our_layer_style.pdf}
    \end{minipage}
    \hspace{0.05\linewidth}
    \begin{minipage}[h]{0.4\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figs/style_heatmap.pdf}
    \end{minipage}
    \caption{\textbf{The text style is not controlled by the same layer as the textual content.} We show example generations (left) indicating that the layer we localize for determining the content of the text in generated images is not capable of changing the style of the text in images. Also, we show (right) that control over the style of the text is distributed over multiple cross-attention layers in SD3 by plotting and calculating CLIP-T alignment between generations after patching particular layers with the desired text style. \label{fig:our_layer_style}}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/layers_for_text_style.pdf}
    \caption{\textbf{The style of the text in Stable Diffusion 3 is influenced by at least 7 layers.} We provide results demonstrating performance in editing textual style when patching an increasing number of layers in the diffusion model. Although modifying this feature becomes feasible with 7 layers, it significantly alters the image background as well.\label{fig:style_layers}}
\end{figure}

\section{Image edition on longer texts}\label{app:results_edit}

In the~\Cref{fig:ex_edition}, we include examples of text editing realized using our method for DeepFloyd IF (a) and Stable Diffusion 3 (b) models. Presented generations indicate that our localization technique can be used to edit images with a longer visual text. Some examples contain errors like omitted letters or words. We believe that our performance in text-based image editing strongly relies on the quality of the text generated by the diffusion model.

Additionally, we present text and image alignment metrics for image edition with our approach for varying number of words in the prompt in~\Cref{tab:metrics_words}.

\begin{table}[ht]
    \centering
    \caption{\textbf{Performance metrics of SD3 image edition for varying number of words.}}
    \begin{tabular}{l||c|c|c|c|c}
        \toprule
        \textbf{\# words} & \textbf{MSE $\downarrow$} & \textbf{SSIM $\uparrow$} & \textbf{PSNR $\uparrow$} & \textbf{OCR F1 $\uparrow$} & \textbf{CLIP-T $\uparrow$} \\
        \hline
        1                 & 0.677                     & 0.695                    & 0.302                    & 0.377                      & 0.746                      \\
        2                 & 0.706                     & 0.675                    & 0.300                    & 0.403                      & 0.717                      \\
        3                 & 0.703                     & 0.676                    & 0.300                    & 0.442                      & 0.721                      \\
        4                 & 0.725                     & 0.668                    & 0.298                    & 0.457                      & 0.714                      \\
        5                 & 0.726                     & 0.664                    & 0.298                    & 0.474                      & 0.698                      \\
        6                 & 0.718                     & 0.663                    & 0.299                    & 0.487                      & 0.701                      \\
        7                 & 0.724                     & 0.654                    & 0.298                    & 0.489                      & 0.704                      \\
        8                 & 0.735                     & 0.653                    & 0.297                    & 0.494                      & 0.695                      \\
        \bottomrule
    \end{tabular}
    \label{tab:metrics_words}
\end{table}



\begin{figure}
    \centering
    \begin{minipage}[t]{1.0\linewidth}

        \vspace{0pt}
        \centering
        \includegraphics[width=0.98\linewidth]{figs/if_examples_gen.pdf}
        \subcaption{DeepFloyd IF}
    \end{minipage}

    \begin{minipage}[t]{1.0\linewidth}
        \vspace{20pt}
        \centering
        \includegraphics[width=0.98\linewidth]{figs/sd3_examples_gen.pdf}
        \subcaption{Stable Diffusion 3}
    \end{minipage}
    \caption{\textbf{Example results from editing synthetic images by leveraging parameter localization.} Presented generations show that the edition can be performed for images with varying lengths of text. We show generations for models capable of generating longer visual texts: DeepFloyd IF (a) and Stable Diffusion 3 (b).\label{fig:ex_edition}}
\end{figure}
