\section{Applications of Our Method}




Focusing on the localization of cross and joint attention layers for text generation offers several key advantages.
In this section we highlight specific use cases where it plays an instrumental role. We first show that we can precisely fine-tune selected layers to improve the quality of the generated text of a base model without affecting its remaining generative capabilities. Then, we
present that with our patching technique, we can efficiently edit text from the model generations. We then extend the latter application to the cost-free technique for mitigating harmful or inappropriate text generation.


\subsection{Improving text generation through fine-tuning}
\label{sec:finetuning}
We leverage our localization insights to fine-tune pre-trained DMs on the task of visual text generation. In particular, we show that by applying Low-Rank Adaptation (LoRA) only to the localized text-specific layers, we can significantly improve the quality of the generated text without affecting the model's performance on other tasks.


\subsubsection{Training Setup}
For training, we utilize a randomly chosen subset of 74,285 images from the MARIO-LAION 10M dataset~\cite{textdiffuser}. In order for the training text captions to contain text that is directly presented on the corresponding training image, we construct them according to
the template \textit{'An image with text saying "\textless text\textgreater"'}, where "\textless text\textgreater" constitutes of OCR labels corresponding to the image.
We compare the performance of applying LoRA to the localized layers with the baseline adaptation approach, for which we directly follow~\cite{hu2022lora} and apply LoRA to all cross-attention layers. We optimize both models until convergence and evaluate the quality of model generations after the next epochs on our test set introduced in~\Cref{sec:setup}.

To assess the quality of the generated text, we report OCR F1-Score and CLIP-T. Additionally, to quantify the effect of fine-tuning on the general generative capabilities of the model, we use the distribution precision and recall metrics~\citep{kynkaanniemi2019improved} that measure the quality of individual samples (precision) and their diversity (recall) against the generations before fine-tuning. We adapt the original method to high-resolution generations from large diffusion models by substituting the original inception embeddings with the CLIP ones.



\subsubsection{Fine-tuning results}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.54\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figs/lora3.pdf}
        \label{fig:lora_ocr}

        \centering
        \includegraphics[width=0.8\textwidth]{figs/clip_embs.pdf}
        \label{fig:clip_embs}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth} %
        \centering
        \includegraphics[width=\textwidth]{figs/lora_generations5.jpg}
        \label{fig:third_plot}
    \end{subfigure}

    \caption{\textbf{Fine-tuning LoRA on localized layers improves text generation quality without compromising overall generation capabilities.}
        We apply LoRA fine-tuning to the SDXL model to enhance its text generation capabilities.
        \textbf{(top left)} The LoRA fine-tuning on the localized layers converges to a higher quality of the generated text (as measured by OCR F1 and CLIP-T metrics).
        \textbf{(bottom left)} When fine-tuning LoRA on all cross-attention layers (denoted as C-A), the model quickly collapses, losing its ability to generate examples that match the prompt. The diversity is significantly reduced, as indicated by a recall. In contrast, fine-tuning LoRA only on our localized cross-attention layers prevents model overfitting while improving text generation quality. It preserves diversity while achieving higher fidelity measured by precision.
        \textbf{(right)} We also present this effect on sample generations. Longer LoRA fine-tuning (measured in epochs) on localized layers improves text quality while preserving visual content, however, applying LoRA to all layers results in significant degradation of the image quality and diversity.
    }
    \label{fig:lora_plots}
    \vspace{-1em}
\end{figure}




Our results demonstrate that by fine-tuning only the three cross-attention layers, identified as instrumental for the generation of textual content, one can obtain a model yielding higher-quality visual text compared to the model with all of the cross-attention layers fine-tuned while preserving the models' generation capabilities. As presented in ~\Cref{fig:lora_plots} (top left), even though fine-tuning of the whole model initially converges faster towards the higher performance, after 20 epochs of training, the model starts to overfit, what can be observed as a significant drop in the recall of generated samples presented in ~\Cref{fig:lora_plots} (bottom left). On the other hand, when fine-tuning selected layers, we can observe steady improvement in the quality of the generated text, with little effect on the model's generation performance and no visible mode collapse. Additionally, ~\Cref{fig:lora_plots} (right) presents sample generations from different training epochs, illustrating the changes to the base model induced by fine-tuning.
We focus on LoRA for SDXL since this model has a significantly lower text generation quality than other studied DMs. We also present a comparison between LoRA, the basic version of our method, and another editing method in \Cref{tab:p2p}. The results indicate that our LoRA approach outperforms the other methods on all but two metrics. Overall, it achieves superior image and text alignment while preserving the fast execution time (from the basic version of our method).













\subsection{Edition of Generated Text in Images}
In this section, we evaluate our patching method leveraging the localized cross-attention layers in the task of text edition on images, where the goal is to preserve most of the source prompt-driven output while selectively modifying only the regions of the image where the source and target prompts disagree.
Our work can be directly compared to the prompt-to-prompt editing framework ~\citep{hertz2023prompttoprompt} (denoted as P2P), where the image edition is controlled only by the text provided by the user. P2P also utilizes cross-attention layers in its design to modify visual concepts and defines a target prompt, which is derived from the source prompt. We evaluate both methods on \SDXL, DeepFloyd IF, and SD3 models and present the results in~\Cref{tab:p2p} on our test set. Our standard patching method (denoted as "Ours") consistently outperforms P2P in terms of image alignment to the source and text alignment to the target prompt. Additionally, our approach is significantly faster in editing a single image, as reflected in the Execution Time column of the table.

\renewcommand{\mycolspace}{4.6pt}
\renewcommand{\arraystretch}{1.3}
\addtolength{\tabcolsep}{-\mycolspace}
\begin{table}[htbp]
    \tiny
    \centering
    \caption{
        \textbf{Our method outperforms P2P in text editing by generating higher-quality text while preserving the other visual components.} We bold the best result for a given DM in each metric.
    }
    \begin{tabular}{lc||ccc|ccc||ccc|ccc||c}
        \toprule
        \textbf{Setup}                       & \textbf{Diffusion}
                                             & \multicolumn{6}{c||}{\textbf{SimpleBench}} & \multicolumn{6}{c||}{\textbf{CreativeBench}} & Execution                                                                                                                                                                                                                                                                                                        \\
                                             & \textbf{Model}                             & \multicolumn{3}{c|}{Image alignment}         & \multicolumn{3}{c||}{Text alignment} & \multicolumn{3}{c|}{Image alignment} & \multicolumn{3}{c||}{Text alignment} & Time [s] $\downarrow$
        \\
                                             &                                            & MSE $\downarrow$                             & SSIM $\uparrow$                      & PSNR $\uparrow$                      & OCR F1 $\uparrow$                    & CLIP-T $\uparrow$     & LD $\downarrow$ & MSE $\downarrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & OCR F1 $\uparrow$ & CLIP-T $\uparrow$ & LD $\downarrow$ &                                  \\
        \hline
        \rowcolor{blue!10} Ours ($t_s=50$)   & \SDXL                                      & 44.78                                        & 0.80                                 & 32.09                                & 0.34                                 & \textbf{0.78}         & 75.95           & 25.34            & 0.89            & 35.06           & 0.32              & \textbf{0.82}     & 102.88          & \textbf{10.37}$_{\pm\text{.25}}$ \\
        \rowcolor{blue!10} Ours ($t_s=46$)   & \SDXL                                      & 43.24                                        & 0.81                                 & 32.25                                & 0.34                                 & \textbf{0.78}         & 75.45           & 23.49            & 0.90            & 35.42           & 0.32              & \textbf{0.82}     & 102.79          & \textbf{10.37}$_{\pm\text{.25}}$ \\
        \rowcolor{blue!10} Ours LoRA         & \SDXL                                      & \textbf{27.63}                               & \textbf{0.90}                        & \textbf{36.38}                       & \textbf{0.43}                        & 0.77                  & \textbf{26.24}  & \textbf{22.83}   & \textbf{0.91}   & \textbf{37.47}  & \textbf{0.33}     & 0.77              & \textbf{38.31}  & \textbf{10.37}$_{\pm\text{.25}}$ \\
        \rowcolor{blue!10} P2P               & \sdxl                                      & 57.26                                        & 0.82                                 & 30.77                                & 0.29                                 & 0.69                  & 75.72           & 57.26            & 0.83            & 30.93           & 0.26              & 0.78              & 99.50           & 31.17$_{\pm\text{.19}}$          \\
        \hline
        \rowcolor{orange!10} Ours ($t_s=50$) & DeepFloyd IF                               & 73.15                                        & 0.63                                 & 29.70                                & \textbf{0.70}                        & 0.80                  & 10.65           & 57.92            & 0.71            & 31.05           & 0.47              & \textbf{0.84}     & 22.55           & \textbf{13.87}$_{\pm\text{.04}}$ \\
        \rowcolor{orange!10} Ours ($t_s=48$) & DeepFloyd IF                               & \textbf{70.27}                               & \textbf{0.64}                        & \textbf{29.90}                       & \textbf{0.70}                        & \textbf{0.81}         & 10.85           & 53.50            & \textbf{0.74}   & 31.46           & \textbf{0.48}     & \textbf{0.84}     & 21.40           & \textbf{13.87}$_{\pm\text{.04}}$ \\
        \rowcolor{orange!10} P2P             & DeepFloyd IF                               & 105.60                                       & 0.41                                 & 27.90                                & 0.27                                 & 0.61                  & \textbf{10.23}  & \textbf{44.89}   & \textbf{0.74}   & 96.84           & 0.08              & 0.61              & \textbf{9.39}   & 28.04$_{\pm\text{.28}}$          \\
        \rowcolor{orange!10} P2P*            & DeepFloyd IF                               & 105.29                                       & 0.21                                 & 27.91                                & 0.41                                 & 0.67                  & 13.48           & 44.64            & 0.67            & \textbf{96.85}  & 0.11              & 0.62              & 13.80           & 28.04$_{\pm\text{.28}}$          \\
        \hline
        \rowcolor{green!10} Ours ($t_s=28$)  & SD3                                        & 73.98                                        & 0.74                                 & 29.59                                & 0.68                                 & 0.76                  & 4.96            & 69.21            & 0.69            & 30.09           & 0.39              & 0.74              & 60.79           & \textbf{15.23}$_{\pm\text{.19}}$ \\
        \rowcolor{green!10} Ours ($t_s=26$)  & SD3                                        & \textbf{ 70.89}                              & 0.72                                 & \textbf{29.84}                       & 0.53                                 & 0.70                  & 5.79            & \textbf{63.13}   & 0.73            & \textbf{30.61}  & 0.41              & 0.75              & \textbf{42.52}  & \textbf{15.23}$_{\pm\text{.19}}$ \\
        \rowcolor{green!10} P2P              & SD3                                        & 90.79                                        & \textbf{0.82}                        & 28.65                                & 0.31                                 & 0.57                  & 9.31            & 82.53            & \textbf{0.82}   & 29.13           & 0.29              & 0.71              & 60.55           & 118.30$_{\pm\text{.55}}$         \\
        \rowcolor{green!10} P2P*             & SD3                                        & 98.22                                        & 0.58                                 & 28.24                                & \textbf{0.90}                        & \textbf{0.88}         & \textbf{2.06}   & 85.77            & 0.64            & 28.90           & \textbf{0.66}     & \textbf{0.90}     & 62.59           & 118.30$_{\pm\text{.55}}$         \\
        \bottomrule
    \end{tabular}
    \label{tab:p2p}
\end{table}
\addtolength{\tabcolsep}{\mycolspace}

While P2P is effective for DMs where the cross-attention layers’ keys and values consist solely of text representations from the text encoder (such as SDXL), it struggles with models like DeepFloyd IF and SD3, where both text and image representations contribute to the keys and values. To address this, we introduce a modified version of P2P, denoted as P2P*, for these models. Instead of overwriting the entire keys and values during image generation, as in the standard approach, P2P* overwrites only the textual components of the keys, allowing image elements to change. This modification enables effective text editing according to the target prompt, albeit with more noticeable alterations to the source image.
Furthermore, in our visual text modification approach, the target prompt $p_T$ can differ from the source prompt $p_S$ in the prompt length and positions of tokens representing the text to change, as opposed to the P2P approach. In the~\Cref{app:results_edit}, we present example edition results for our localization-based text edition method. In particular, we show that we can modify texts of varying lengths with our method.

\subsection{Preventing generation of toxic text}
\label{sec:preventing}
We observe that diffusion models, even the ones equipped with safeguards against generating NSFW (Not Safe For Work) content, tend to simply copy-paste the text from the prompt to the image. As a result, while the visual content may be safe thanks to careful filtering of the fine-tuning dataset, the text in the generated images can still be harmful. We carry out experiments on known methods, such as Safe Diffusion~\citep{schramowski2023safe} and Negative Prompts~\citep{negative-promtps}, to evaluate their effectiveness in preventing the generation of toxic content and find out that those methods underperform. To address this issue, we propose a new approach -- the application of our edition technique to prevent the generation of toxic text within images.

Our goal is to address scenarios where a model provider exposes a diffusion model for generating images from textual prompts. In this setting, a user may submit a source prompt $p_S$ containing toxic textual content intended to appear in the generated image. Detecting toxicity in the images is crucial for online platforms to enforce community guidelines and remove inappropriate material. With advancements in large language models, toxic text can be reliably identified~\citep{zhang2024efficient} and rephrased to ensure that the generated image suppresses harmful content. To achieve this, the toxic portion of the source prompt is replaced with a non-harmful text or a placeholder sequence, such as a series of stars (*).

We harness our precise localization of the cross-attention layers responsible for generating textual content in images to prevent the model from outputting harmful text. In particular, the prompts identified as toxic are substituted with a non-harmful text \emph{on the fly} using our patching technique. This allows us to remove the toxic content from the final generation without altering the remaining visual content. We achieve this result with a single pass through the diffusion denoising process without imposing any additional computational cost.

\begin{table}
    \centering
    \tiny
    \caption{\textbf{Our method can be used to prevent the generation of toxic text in images.} We bold the best result for a given DM in each metric and the runner-up is underlined.
    }
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lc||ccc||cc}
            \toprule
            \textbf{Method}                      & \textbf{Diffusion Model} & MSE $\downarrow$  & SSIM $\uparrow$  & PSNR $\uparrow$   & OCR F1 $\downarrow$ & Toxicity score $\downarrow$ \\
            \hline
            \rowcolor{blue!10} Ours              & SDXL                     & \textbf{48.20 }   & \underline{0.79} & \underline{31.68} & \underline{0.20}    & \underline{0.003}           \\
            \rowcolor{blue!10} Negative prompt   & SDXL                     & 77.95             & 0.71             & \textbf{31.76}    & 0.23                & 0.052                       \\
            \rowcolor{blue!10} Safe Diffusion    & SDXL                     & 49.46             & \textbf{0.81}    & 31.33             & 0.34                & 0.222                       \\
            \rowcolor{blue!10} Safe Diffusion*   & SDXL                     & \underline{49.41} & \textbf{0.81}    & 31.33             & 0.33                & 0.209                       \\
            \rowcolor{blue!10} Prompt Swap       & SDXL                     & 79.41             & 0.66             & 31.65             & \textbf{0.19}       & \textbf{0.000}              \\
            \hline
            \rowcolor{orange!10} Ours            & DeepFloyd IF             & 74.96             & 0.61             & 29.60             & \underline{0.32}    & \underline{0.018}           \\
            \rowcolor{orange!10} Negative prompt & DeepFloyd IF             & 100.50            & 0.37             & 28.12             & 0.59                & 0.250                       \\
            \rowcolor{orange!10} Safe Diffusion  & DeepFloyd IF             & \underline{64.30} & \underline{0.73} & \underline{30.19} & 0.79                & 0.555                       \\
            \rowcolor{orange!10} Safe Diffusion* & DeepFloyd IF             & \textbf{63.65}    & \textbf{0.74}    & \textbf{30.25}    & 0.79                & 0.540                       \\
            \rowcolor{orange!10} Prompt Swap     & DeepFloyd IF             & 100.99            & 0.35             & 28.10             & \textbf{0.30}       & \textbf{0.015}              \\
            \hline
            \rowcolor{green!10} Ours             & SD3                      & 72.61             & 0.70             & 29.72             & \underline{0.32}    & \underline{0.018}           \\
            \rowcolor{green!10} Negative prompt  & SD3                      & 101.63            & 0.53             & 28.08             & 0.77                & 0.407                       \\
            \rowcolor{green!10} Safe Diffusion   & SD3                      & \underline{34.99} & \underline{0.86} & \underline{34.25} & 0.73                & 0.571                       \\
            \rowcolor{green!10} Safe Diffusion*  & SD3                      & \textbf{33.67}    & \textbf{0.87}    & \textbf{34.56}    & 0.73                & 0.568                       \\
            \rowcolor{green!10} Prompt Swap      & SD3                      & 98.58             & 0.51             & 28.22             & \textbf{0.30}       & \textbf{0.015}              \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:toxic}
\end{table}


In \Cref{tab:toxic}, we compare our method with three baseline techniques. First, we leverage negative prompting. It was suggested~\citep{negative-promtps} that the generative process can be more effectively guided by using \textit{negative} text prompts that instruct a diffusion model to exclude specific elements from its generated images. In that approach, we set the negative prompt to \textit{'text "$<$word$>$"'}, where $<$word$>$ is a harmful word from $p_S$. We also run Safe Diffusion~\citep{schramowski2023safe} on safe prompts, which works by intervening directly in the latent space of diffusion models to remove and suppress inappropriate content during image generation. Additionally, we introduce Safe Diffusion*, where we adapt the method (its safe prompts) to the task of toxic language removal. We present the details of adaptation in ~\Cref{sec:safe_prompts}.

In our approach, we replace the toxic word in the source prompt $p_S$ with a non-harmful suggestion and form the target prompt $p_T$. We also include a potential method, that, similarly to us, is based on prompt edition, which we refer to as Prompt Swap. In this method, we apply the LLM-rephrased non-toxic prompt to the entire diffusion model instead of doing it only for our localized layers.

In the experiments, each of the source prompts $p_S$ (we use 400 in total) contains a harmful word from~\cite{harmful-words}. We obtain the edited generations from each approach, run the OCR on the output images, and for the text returned from OCR, we calculate the toxicity score using the RoBERTa-based classifier~\citep{liu2022robustly}. We show that Negative Prompt and Safe Diffusion (in both versions) methods are incapable of removing toxic textual content from generated images. For Prompt Swap, we observe that this method marginally outperforms our approach in toxic text prevention. However, the introduced change in the modified prompt strongly impacts other visual aspects of an image, which is not the case for our solution.

In the~\Cref{app:prompt_swap_discussion}, we argue that preventing the change of visual attributes, even when the end user did not see the original image, is important in order to, i.e., preserve the emotions expressed in the original prompt to the model. We demonstrate that our approach successfully substitutes toxic text from the generated images without significantly altering the remaining part of the generation, making it the most reliable solution. We include example generations and detailed evaluation supporting this claim in~\Cref{fig:toxic_if_examples}.
