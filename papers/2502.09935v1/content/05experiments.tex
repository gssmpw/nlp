\section{Localization of Attention Layers Responsible for Textual Content Generation}
We begin by presenting details of our patching technique for cross and joint attention layers, which we employ to localize the components of diffusion models responsible for the content of the generated text. We demonstrate that our method generalizes across diverse model architectures despite differences in the implementations of attention layers and with different configurations as well as types of text encoders.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/loc_schema_rev2.pdf}
    \caption{\textbf{Overview of the localization process.} Our goal is to edit the image generated from the source prompt $p_S$ using the target prompt $p_T$. To find which cross and joint attention layers should be modified, we pass the target prompt $p_T$ through the DM, caching the keys and values. Then, while generating the image from $p_S$ we substitute the keys and values with the cached ones. We select the layers which yield the highest image and text alignment.
        (A) Localizing by Patching is applied to SD3, and (B) Localizing by Injection is used for SDXL and DeepFloyd IF.
    }
    \label{fig:teaser}
\end{figure}


\subsection{Patching Technique}\label{sec:patching_technique}
\label{sec:patching}
Recent works \citep{basu2024mechanistic, orgad2023editing} demonstrate that altering the key and value matrices of cross-attention layers can effectively influence the concepts generated by diffusion models. Specifically, \citet{basu2024mechanistic} show that only certain attention layers within DMs are responsible for generating specific visual concepts, such as objects or styles. This approach that we call \textit{injection} is effective in U-Net-based DMs such as Stable Diffusion or DeepFloyd IF, as shown in \Cref{fig:teaser} B. These models implement cross-attention layers that directly input the prompt embedding $e$ and multiply it by the key $W^K$ and value $W^V$ matrices. However, it is unsuitable for the most recent DMs that leverage the joint attention mechanism~\citep{esser2024scalingSD3}, such as SD3 and FLUX. In these models, the subsequent attention layers process and modify both image and conditioning text, allowing each following layer to receive text embeddings modified by its preceding layers.

In our work, we leverage the \textit{activation patching} technique~\citep{meng2022locatingPatching} to identify the cross and joint attention layers responsible for generating text content in images across different DM's architectures.
We present the overview of the patching process in \Cref{fig:teaser} \textbf{A}.
Suppose we want to edit the text in the image $i_S$ generated from the source prompt $p_S= \text{'A sign that says "} t_S \text{".'}$ to match the text in the target prompt $p_T = \text{'A sign that says "} t_T \text{".'}$.
To measure the impact of each individual cross-attention layer $l$ on the content of text generated in the output image, we first generate an image $i_T$ from $p_T$, caching the keys $K_T = e_T W_l^{K}$ and values $V_T = e_T W_l^V$ \textbf{(A.I)}, where $e_T$ denotes the textual input part to the cross-attention layer. Then, while generating $i_S$ from $p_S$, we overwrite $K_S$ with $K_T$ and $V_S$ with $V_T$ \textbf{(A.II)}. We then calculate image and text alignment metrics for the generations produced by the diffusion model with modified attention activations.
To ensure consistency in our method, we always cache and overwrite only the \textit{text} keys and values, which result from multiplying the textual parts of the residual stream by the key and value matrices. It allows us to apply our technique across different DM architectures despite their differences in attention implementations.`








\subsection{Cross-attention layer localization}\label{sec:ca_loc}

\renewcommand{\mycolspace}{4.5pt}
\addtolength{\tabcolsep}{-\mycolspace}
\begin{table}
    \centering
    \scriptsize
    \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{l|c|c|c|c}
            \multirow{2}{*}{\textbf{Model}}           & \# localized & total \# of cross- & \# localized & fraction of model \\
                                                      & layers       & -attention layers  & parameters   & parameters [\%]   \\
            \hline
            \textbf{SDXL}~\citep{podell2023sdxl}      & 3            & 70                 & 15.7M        & 0.61\%            \\
            \textbf{DeepFloyd IF}~\citep{DeepFloydIF} & 1            & 22                 & 8.9M         & 0.21\%            \\
            \textbf{SD3}~\citep{esser2024scalingSD3}  & 1            & 24                 & 4.7M         & 0.23\%            \\
        \end{tabular}
    }
    \caption{\textbf{Less than $1$\% of DMs' parameters influence text generation within the images.}}
    \label{tab:summary_loc}
\end{table}
\addtolength{\tabcolsep}{\mycolspace}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/loc_heatmap.pdf}
    \caption{\textbf{Localized attention layers responsible for the content of the generated text.} We selectively patch individual cross and joint attention layers with computations for the target prompt and measure the responses with OCR F1 Score. We identify three layers with the highest responses in \SDXL (55, 56, and 57), one layer in DeepFloyd IF (17), and one layer in SD3 (10).}
    \label{fig:loc}
\end{figure}



We localize the layers responsible for text generation in three DMs with different architectures and text encoders: \SDXL, \DeepFloyd, and SD3. To this end, we run our patching approach for each cross-attention layer in each model on our validation set.
As presented in the overview of the results in \Cref{tab:summary_loc} and \Cref{fig:loc}, we are able to successfully identify cross-attention layers that, when patched, cause the DMs to produce the text that closely matches the text in the target prompt $p_T$. In both DeepFloyd IF and SD3 models, there is only a single layer that strongly responds when patched with the other prompt. On the other hand, in the SDXL model, we identify three such layers. The fact that in SDXL, the responses measured in the F1 Score are much more distributed than in other analyzed models may be attributed to the fact that SDXL has significantly more cross-attention layers than the other models and exhibits the lowest text generation capabilities. Overall, our findings suggest that a very small fraction of the DM's parameters is primarily responsible for the text content in the generated images. Additionally, the successful localization of DM components across models demonstrates the applicability of our localization method across different DM architectures. In \Cref{fig:layers_comparison}, we additionally visualize how patching a different number of layers affect the final generation in Stable Diffusion XL.


\begin{figure}
    \centering
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \vspace{0pt}
        \includegraphics[width=0.92\linewidth]{figs/layers_comparison.pdf}
    \end{minipage}
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \hspace{14pt}
        \includegraphics[width=0.87\linewidth]{figs/layers_samples.pdf}
    \end{minipage}

    \caption{\textbf{The localized layers effectively balance the text alignment with the target prompt $p_T$ and the image alignment with the source prompt $p_S$.} For ease of exposition, we measure the text alignment with OCR F1 and the image alignment with SSIM. We observe that injecting the target prompt $p_T$ to too many layers decreases the image alignment and introduces undesirable artifacts, \eg the Japanese text on the robot's chest in 2nd image from the right and the lack of fish in the 1st image from the right. Conversely, injecting $p_T$ to too few layers does not edit the generated text.
        We present more details about the experiment in \Cref{app:layers_study}.
    }
    \label{fig:layers_comparison}
    \vspace{-1em}
\end{figure}

















\subsection{Specialization of the Localized Layers}



In the previous section, we localized layers that are responsible for the generation of the textual content. Here, we delve deeper into this analysis and evaluate their specialization. In particular, we study what is the information extracted from the prompt by the selected layers and how it affects the generation. To measure this effect, we conduct a series of experiments with artificial prompts created as a combination of a \emph{template} that describes the background of the image and \emph{text}, usually in the form of a simple word. We present examples of such prompts in Table~\ref{tab:template_text}.

\begin{wraptable}{r}{0.35\textwidth}
    \vspace{-0.0em}
    \centering
    \small
    \caption{Examples of prompts.}
    \begin{tabular}{r|l}
        \toprule
        Template                          & Text            \\
        \hline
        \textit{A book cover with text}   & \textit{'Love'} \\
        \textit{A sign that says}         & \textit{'STOP'} \\
        \textit{A paper letter with note} & \textit{'Lies'} \\
        \bottomrule
    \end{tabular}
    \label{tab:template_text}
    \vspace{-1em}
\end{wraptable}

\renewcommand{\mycolspace}{2pt}
\addtolength{\tabcolsep}{-\mycolspace}
\begin{figure}[t]
    \tiny
    \centering
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{tabular}{lc||cc||cc}
            \toprule
            \multirow{2}{*}{\textbf{Target prompt}}        & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c||}{\textbf{CLIP-T}} & \multicolumn{2}{c}{\textbf{OCR F1}}                                   \\
                                                           &                                 & Template$_{S}$                        & Template$_{T}$                      & Text$_{S}$     & Text$_{T}$     \\
            \hline
            \rowcolor{blue!10} Template$_{S}$:Text$_{S}$   & SDXL                            & \textbf{0.727}                        & 0.436                               & \textbf{0.354} & 0.206          \\
            \rowcolor{blue!10} Template$_{S}$:Text$_{T}$   & SDXL                            & \textbf{0.732}                        & 0.436                               & 0.194          & \textbf{0.324} \\
            \rowcolor{blue!10} Template$_{T}$:Text$_{T}$   & SDXL                            & \textbf{0.724}                        & 0.440                               & 0.203          & \textbf{0.331} \\
            \hline
            \rowcolor{orange!10} Template$_{S}$:Text$_{S}$ & DeepFloyd IF                    & \textbf{0.721}                        & 0.453                               & \textbf{0.554} & 0.244          \\
            \rowcolor{orange!10} Template$_{S}$:Text$_{T}$ & DeepFloyd IF                    & \textbf{0.729 }                       & 0.453                               & 0.260          & \textbf{0.475} \\
            \rowcolor{orange!10} Template$_{T}$:Text$_{T}$ & DeepFloyd IF                    & \textbf{0.721 }                       & 0.465                               & 0.275          & \textbf{0.452} \\
            \hline
            \rowcolor{green!10} Template$_{S}$:Text$_{T}$  & SD3                             & \textbf{0.675}                        & 0.443                               & \textbf{0.544} & 0.231          \\
            \rowcolor{green!10} Template$_{S}$:Text$_{T}$  & SD3                             & \textbf{0.599}                        & 0.443                               & 0.266          & \textbf{0.333} \\
            \rowcolor{green!10} Template$_{T}$:Text$_{T}$  & SD3                             & \textbf{0.684}                        & 0.446                               & 0.276          & \textbf{0.304} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.23\linewidth,trim={0 0 8.5cm 0},clip]{figs/templ_only_source.pdf}
        \includegraphics[width=0.23\linewidth,trim={0 0 8.5cm 0},clip]{figs/templ_same_templates.pdf}
        \includegraphics[width=0.23\linewidth,trim={0 0 8.5cm 0},clip]{figs/templ_diff_templates.pdf}
    \end{minipage}
    \caption{\textbf{Patching preserves visual components from the source prompt, taking only the textual information from the injected target prompt.} In all the combinations of templates and texts that we inject to localized layers of diffusion models (with other layers receiving both source template and source text), the final visual components of the image are always closer to the original template, while the textual content is always aligned with the one from an injected prompt. The source prompt is always defined as $p_S$=Template$_{S}$:Text$_{S}$, while we change the target prompts to Template$_{S}$:Text$_{S}$, Template$_{S}$:Text$_{T}$, and Template$_{T}$:Text$_{T}$ (from left to right for the images).
    }
    \label{fig:stress-test}
    \vspace{-2em}
\end{figure}
\addtolength{\tabcolsep}{\mycolspace}

We show that selected layers are only affected by the part of the target prompt that mentions the textual content. To that end, we sample images with a  prompt $p_S=\text{Template}_{S}:\text{Text}_{S}$ used as conditioning for almost all the layers while patching the localized layers with one of three target prompt options: (1) the same prompt ($p_T=p_S$), (2) a prompt that shares the same template but different text ($p_T=\text{Template}_S:\text{Text}_T$) or (3) a prompt with different template and text ($p_T=\text{Template}_T:\text{Text}_T$). We present the result of this experiment in \Cref{fig:stress-test}. We observe that the final generation follows the text provided by the prompt $p_T$ used for patching. However, at the same time, changing the template in the target prompt does not affect the final generation, as the background image is always significantly more aligned with the template from the source prompt. This observation means that the layers localized by our method are not only used for generating the textual content in the final sample but are also highly specialized, focusing solely on the textual content of the input prompt.










