\section{Introduction}


Recent advancements in generative models for the vision domain have demonstrated remarkable efficacy in image synthesis tasks and significant improvements in the quality and diversity of the generated outputs (DDPM~\citep{ho2020}, LDM~\citep{rombach2022high}). The next generation of models, including DeepFloyd IF~\citep{DeepFloydIF}, Imagen~\citep{saharia2022imagen}, Stable Diffusion~3 (SD3)~\citep{esser2024scalingSD3}, and FLUX~\citep{flux}, extend this progress to photo-realistic generations with \textit{high-quality visual text}. While introducing impressive capabilities, such models usually operate as black-boxes with complex architectures entangling various skills.

In this work, we propose to shed some light on the inner workings of recent diffusion models and introduce the first method to localize parts of the model responsible for the generation of textual content, based on activation patching technique~\citep{meng2022locatingPatching}.
We determine that only $0.61\%$ of Stable Diffusion XL~\citep{podell2023sdxl}, $0.21\%$ of Deepfloyd IF~\citep{DeepFloydIF}, and $0.23\%$ of Stable Diffusion 3~\citep{esser2024scalingSD3} parameters are responsible solely for this task. Our observations hold across various DMs' architectures, both U-Net and Transformer-based, for DMs utilizing diverse text encoders, such as CLIP~\citep{clip} and T5~\citep{raffel2020exploringT5,roberts2022t5x}.
Additionally, we present several applications that benefit from our %
localization method.

We first show that by selectively fine-tuning only the identified subset of layers responsible for textual content, we can significantly enhance the model’s performance in generating text within images without reducing the quality and diversity of generated samples.
Then, we present that by selectively applying \patching, we are able to substitute the generated text without affecting other visual attributes of an image. Our method does not require any additional extra data (potentially with human annotations), DM training~\citep{brooks2023instructPix2Pix}, semantic maps which indicate which part of images should be preserved during the diffusion process~\citep{andonian2021paintByWord,tuo2024anytext}, or optimization.
Finally, we extend our edition technique to prevent the generation of toxic text, \emph{on the fly} without imposing additional computational cost.




\textbf{Our contributions can be summarized as follows:}

\begin{enumerate}
    \item We localize a small subset of cross and joint attention layers in diffusion models that determine text generated within images. Our observations are architecture-agnostic.
    \item We introduce a new fine-tuning strategy that targets only the localized subset of layers responsible for textual content, improving text generation performance while maintaining the model’s overall generation diversity and efficiency.
    \item We incorporate our findings into the new image-to-image method for the text edition within synthetic images, outperforming previous techniques on standard benchmarks for image text editing, achieving superior accuracy and visual consistency.
    \item We show that our method can also be effectively used to prevent the generation of harmful or toxic text within images in one generation pass.
\end{enumerate}



