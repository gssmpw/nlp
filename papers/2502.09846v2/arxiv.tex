\documentclass[10pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{cite}
\usepackage{flushend}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{amsfonts,amsthm}
\renewcommand{\qedsymbol}{$\blacksquare$}
\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{float}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{float}  %设置图片浮动位置的宏包
%加跳转
%\usepackage{hyperref}
%\hypersetup{hypertex=true,
%colorlinks=true,
%linkcolor=blue,
% anchorcolor=blue,
% citecolor=blue}
%
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\allowdisplaybreaks
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\newtheorem{Assumption}{Assumption}


\begin{document}

\title{Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization}

\author{Ziqiong Wang, Xiaoxue Yu, Rongpeng Li, and Zhifeng Zhao
    \thanks{This work was supported in part by the National Key Research and Development Program of China under Grant 2024YFE0200600, in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LR23F010005.}
    
    \thanks{Z. Wang, X. Yu and R. Li are with the College of Information Science and Electronic Engineering, Zhejiang University (email: \{wangziqiong, sdwhyxx, lirongpeng\}@zju.edu.cn).}
    \thanks{Z. Zhao is with Zhejiang Lab as well as the College of Information Science and Electronic Engineering, Zhejiang University (email: zhaozf@zhejianglab.com).}
    %\thanks{H. Zhang is with the City University of Macau, China (email:  hgzhang@cityu.edu.mo).}
}

\maketitle
   
\begin{abstract}
Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered.
% which reduces the communication volume determining the necessity of triggering an event based on the significance of information. We simultaneously employ a GIB regularizer to concurrently optimize both the communication graph and the information flow. 
Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.
\end{abstract}

\begin{IEEEkeywords}
Communication and control co-design, event trigger, graph information bottleneck optimization, consensus-oriented, multi-agent reinforcement learning. 
\end{IEEEkeywords}

\section{Introduction}\label{sec1_Introduction}

Nowadays, the thriving development of Multi-Agent Systems (MAS) \cite{MAS} has propelled the integration of communication and control into a pivotal research direction, showcasing significant system-wide advancements. %Communication-Control Co-Design not only emphasizes the efficient transmission of information but also focuses on enhancing the overall coordination capabilities of the system, with its principles increasingly applied in fields such as Unmanned Aerial Vehicles (UAV) and Internet of Vehicles (IoV), showcasing how its co-design philosophy unlocks new possibilities for seamless integration and system-wide advancements. 
Typically, such a co-design is contingent on Multi-Agent Reinforcement Learning (MARL) \cite{MARLsurvey}, but suffers from
%which faces significant challenges due to 
partial observability, as agents can only access limited or incomplete information about the environment and other agents' states. 
%To address this issue, Graph Neural Networks (GNNs) can be employed to enhance 
Therefore, in order to make informed collaborative decisions, it necessitates the information sharing and aggregation among agents \cite{Tarmac2019}. 
%By modeling agents and their interactions as a graph, GNNs benefit agents to exchange and integrate information from their neighbors, even when they lack direct visibility of the entire system.This approach enables each agent to better infer the hidden states of others and make more informed decisions, thereby improving coordination and mitigating the challenges posed by partial observability in MARL environments. 
%Typically, such a co-design is contingent on XX 
%While these hidden states can help steer control, the inference of these latent variables potentially encounters the inconsistency issue. 
Although full-mesh, raw data transmission \cite{masia2022} could theoretically resolve this problem, the overwhelming information exchange makes it often practically infeasible, especially under bandwidth-limited scenarios.
Until recently,
%In this context, 
there starts to emerge algorithms \cite{xiang2023decentralized} that establish consensus over well-calibrated latent variables from neighbors only. %offer a more efficient solution.
Compared to direct data transmission, the consensus-based algorithm boosts scalable decentralized execution.
%Therefore, we require an algorithmic framework of multi-agent reinforcement learning based on establishing consensus. Meanwhile, to tackle 
Nevertheless, the insufficiency of communication and control co-design, such as the blunt transmission of all less informatively transmitted messages \cite{niu2021}, leaves significant redundancy in exchanged messages. Correspondingly, the inevitable difficulties posed by limited bandwidth and noisy channels still call for a more efficient algorithmic framework. %, one strategy is to use event-triggered communication to reduce interactions with other agents, while the other is to compress the information content to eliminate redundant information.

As a remedy, theoretically guided refinement and compression contribute to squeezing the amount of exchanged information for consensus inference. In that regard, the integration of Information Bottleneck (IB) emerges as a highly promising direction to improve overall communication efficiency. In the context of general representation learning, the IB principle \cite{IB2020} emphasizes that the optimal representation should contain sufficient and minimal information that is beneficial for ultimate tasks. Concurrently, TOCF \cite{TOCF} introduces IB theory into the multi-agent communication reinforcement learning (MACRL) scenarios, enabling efficient message compression in communication. However, such IB-based representation learning methods typically require input data to meet independent and identically distributed (i.i.d.) conditions, which do not always hold in the context of multi-agent communication. Meanwhile, Ref. \cite{TOCF} overlooks that both graph structure and agent features carry important information in MACRL, whereas Graph Information Bottleneck (GIB) theory \cite{GIB} introduces a local-dependence assumption and provides a paradigm that regularizes the topological as well as the feature attributes, offering significant advantages, but has not yet been applied to Reinforcement Learning (RL). In this regard, MAGI \cite{MAGI2024} extends the GIB principle to MACRL methods to derive more effective and concise message representations. However, the classical MAGI framework optimizes the communication graph and information flow separately. Furthermore, MAGI requires agents to communicate with all agents within a certain range before assessing the importance of information. Therefore, it inevitably adds significantly to the communication volume, necessitating the design of a more efficient information compression approach.
%we introduce an event-triggered algorithm to overcome this limitation. 

Conventional control algorithms have adopted event-triggered mechanisms \cite{kim2019learning,ATOC2018}, which reduce redundant exchanged information by appropriately tuning the frequency of communications. However, these methods often determine the triggering moment solely based on current observations, but neglect the influence of historical information. In contrast, ETCNet \cite{ETCNET2023} enables agents to make more informed decisions by leveraging both immediate and historical data. Nevertheless, it fails to offer a detailed explanation regarding the criteria for setting the threshold, while many articles \cite{He2022} rely on fixed thresholds, potentially encountering accumulative errors due to stale updates. Furthermore, the effective incorporation of event-triggered communications into consensus inference still awaits for comprehensive investigation. 
%In principle, higher thresholds are required during the initial stages of collaboration when the discrepancies in the exchanged information are significant, whereas lower thresholds become more efficient as the collaboration progresses.
% Existing event-triggered methods \cite{kim2019learning,ATOC2018} often determine the triggering moment solely based on current observations, neglecting the influence of historical information. Additionally, some works \cite{Mao2020} limit an agent's sending behavior to binary decisions (broadcasting to all agents or none), overlooking the fact that information value varies among receiving agents. Thus, selective triggering mechanism tailored to individual agents is necessary. Furthermore, most of the articles \cite{He2022} rely on fixed thresholds, which are neither adaptive nor flexible. For instance, higher thresholds are needed in early collaboration stages when agent errors are significant, whereas lower thresholds are more efficient in later stages.
% Although the approach proposed in \cite{xia2023} addresses these issues and entirely prevent the Zeno phenomenon, its complexity in control performance analysis raises concerns about practicality.

% The GNN-based graph communication paradigm is vulnerable to adversarial attacks and noise disturbances, which can leave many practical applications based on MACRL models exposed to severe risk. In such scenarios, hostile attacks or noise perturbations targeting communication links between UAVs can severely disrupt coordination, potentially causing drones to deviate from their intended formation, collide with obstacles, or misinterpret environmental signals, ultimately affecting the completion of missions.
% To further enhance the robustness of UAV swarm communication, integrating Information Bottleneck (IB) theory into the framework is a promising direction. In the context of general representation learning, the IB principle \cite{IB2020} emphasizes that the optimal representation should contain sufficient and minimal information that is beneficial for ultimate tasks.
% In addition to the event-triggered communication, theoretically guided refinement and compression contribute to further squeezing the amount of exchanged information for consensus inference.
% %The previously mentioned control algorithms aim to minimize the frequency of communication between agents through the event-triggered mechanisms. To further refine communication messages, a promising approach involves compressing the exchanged information, thereby effectively eliminating redundant messages. 
% In that regard, the integration of Information Bottleneck (IB) emerges as a highly promising direction to improve overall communication efficiency. In the context of general representation learning, the IB principle \cite{IB2020} emphasizes that the optimal representation should contain sufficient and minimal information that is beneficial for ultimate tasks.
% However, previous IB-based representation learning methods typically require input data to meet independent and identically distributed (i.i.d.) conditions, which do not always hold in the context of MARL.
% In this regard, MAGI \cite{MAGI2024} introduces a local-dependence assumption \cite{wu2020graph} and extends the IB principle to graph-based MARL methods to derive more effective and concise message representations. However, the classical MAGI framework remains restricted to discrete action scenarios, revealing inherent limitations in its applicability.

In this paper, we propose the Consensus-Driven Event-based GIB (CDE-GIB) algorithm. Specifically, 
% to overcome the limitations of poor flexibility in event-based mechanisms, we introduce a variable-threshold event-triggering mechanism that takes account of both historical data and current observations and determines whether to trigger an event from the perspective of information importance.Additionally, we extend the discrete MAGI \cite{MAGI2024} to continuous information distribution scenarios and develop a graph IB-based message representation method. 
in contrast to the separate compression of the communication graph and data flow \cite{MAGI2024,GIB}, we propose a more efficient GIB method for joint optimization, thus avoiding high computational complexity due to inner loops. Additionally, to further reduce the communication volume required for agents to reach consensus during interaction, we introduce a variable-threshold event-triggering mechanism that takes account of both historical data and current observations and determines whether to trigger an event from the perspective of information importance.
In comparison to existing works in the literature, the contribution of this paper can be summarized as follows.

\begin{itemize}
    \item We propose the CDE-GIB framework, which novelly combines event-triggering and GIB, to ameliorate the message inefficiency of consensus inference in decentralized MARL.
    % We propose the CDE-IB framework, which not only facilitates selective message transmission via a variable-threshold event-triggering mechanism but also leverages Information Bottleneck theory to further compress the message representations based on establishing consensus, thereby enhancing the efficiency of decision-making.
    % IB 后面
    \item We develop a variable-threshold event-triggering mechanism (VT-ETM) that dynamically evaluates the information importance towards inferring the consensus across multiple agents. Such an information importance-driven event-triggering mechanism also significantly distinguishes with existing MARL-empowered solutions \cite{kim2019learning,ATOC2018,ETCNET2023}.
    %The fused message is broadcast to relevant agents only when its similarity with consensus information surpasses a dynamic threshold. 
    \item Additionally, we devise a GIB regularizer that fuses the communication graph and information flow to obtain more concise message representations, which are applicable in consensus inference in decentralized MARL, thereby improving the efficiency of downstream control tasks.
    \item We validate the universal effectiveness of our framework through extensive simulations in the multi-agent particle environment \cite{MPE2017}.
\end{itemize}

The remainder of the paper is organized as follows. Sec. \ref{sec2_System Model} briefly introduces the system model and formulates the problem. Sec. \ref{sec3_Framework} presents the overview of our proposed CDE-GIB framework. In Sec. \ref{sec4_Experiment}, we elaborate on the experimental results and discussions. Finally, Sec. \ref{sec5_Conclusions} concludes the paper.

\section{System Model and Problem Formulation}\label{sec2_System Model}

\begin{figure}[tp]
    \centering
    \includegraphics[scale=0.4]{environment.pdf}
    \vspace{-0.2cm}
\caption{Illustration of MARL information control.} 
\vspace{-1em}
\label{environment}
\end{figure} 

Beforehand, we summarize the main notations in Table \ref{nota}.

\begin{table}[tbp]
    \renewcommand\arraystretch{1.3}
    \centering
    \caption{Major notations used in the paper.}
    \vspace{-0.3em}
    \begin{tabular}{lm{6.5cm}}
    \hline
    \textbf{Notation} & \textbf{Definition} \\ \hline
    $\textbf{s}^{(t)},\textbf{o}_g^{(t)}$ & Global state and observation.\\
    $\textbf{z}_i^{(t)},\textbf{u}_i^{(t)}$ & Local state, individual action of agent $i$ at time step $t$.\\
    $\textbf{o}_i^{(t)},\textbf{m}_i^{(t)}$ & Local observation, exchanged message of agent $i$ at time step $t$. \\
    $\textbf{p}_i^{(t)},\textbf{v}_i^{(t)}$ & Velocity and position that constitute the local observation of agent $i$ at time step $t$.\\
    $\xi _{i}^{\left ( t \right )}$ & Neighbors within the observation range of agent $i$ at time step $t$. \\
    $\beta_i^{(t)}$ & Importance ratio of MAPPO of agent $i$ at time step $t$. \\
    $\alpha$ & Coefficient of policy entropy.\\
    $\pi_{\theta_i}$, $\theta_i$ & Target policy and its parameter of agent $i$.\\
    $\pi_{\theta_{i,old}}$, $\theta_{i,old}$ & Behavior policy and its parameter of agent $i$. \\
    $\mathcal{T}_i$ & Sequence of event-triggering times of agent $i$. \\
    $\mathcal{G}$ & Graph in the Graph Neural Network.\\
    
    \hline
    \end{tabular}
    \label{nota}
    \vspace{-0.5cm}
\end{table}
\subsection{System Model}

The MARL problems can be typically modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP), which is characterized by a tuple $\left \langle \mathcal{N, S, U, P, R}, \Omega, \mathcal{O, \gamma} \right \rangle $. 
$\mathcal{N}$ represents the set of $N$ active nodes.
%, each corresponding to $F$-length feature attributes. 
Due to the practical communication limitation, the heterogeneous connectivity across nodes can be characterized by an adjacency matrix $\mathbf{A} \in \mathbb{R}^{N \times N}$ \texttt{--} $A(i,j) = 1$ if and only if the Euclidean distance between $i \in \mathcal{N}$ and $j \in \mathcal{N}$ is less than the maximum observation range $\delta _\text{com}$; and it nulls otherwise. Naturally, the MAS can be denoted as a graph $\mathcal{G} = (\mathcal{N}, \mathcal{E}, \mathcal{H})$, where $\mathcal{E} \subseteq \mathcal{N} \times \mathcal{N}$ represents the edge set characterized by $\mathbf{A}$, and $\mathcal{H} = \left\{ \mathbf{h}^{(t)} \mid t = 1, 2, \dots, T \right\}$ contains the feature attributes of the nodes.
%$\mathcal{I}$ represents there exist $N$ active UAVs. 
%Meanwhile, the multi-agent system is represented as a graph $G = (\mathcal{I}, E, U)$ with $N$ nodes, where $\mathcal{I} = \{1, 2, \ldots, N\}$ denotes the set of agents, $E \subseteq \mathcal{I} \times \mathcal{I}$ represents the edge set, and $U \in \mathbb{R}^{n \times f}$ contains the feature attributes of the nodes. We define $A \in \mathbb{R}^{n \times n}$ as the adjacency matrix of $G$, where $A_{ij} = 1$ if $ (i, j) \in E $, and $ A_{ij} = 0 $ otherwise.
$\mathcal{S}$ denotes the global state space of the problem and $\mathcal{U}$ is the homogeneous action space for the multi-agent system. The joint action $\textbf{u}^{(t)}=\left\{\textbf{u}_i^{(t)}\mid\forall i \in \mathcal{N}\right\}$ executed at the current state $\textbf{s}^{(t)}$ makes the environment transit to the next state $\textbf{{s}}^{(t+1)}$ according to the transition probability function $ \mathcal{P}(\textbf{s}^{(t+1)} \mid \textbf{s}^{(t)}, \textbf{u}^{(t)}) : \mathcal{S} \times \mathcal{U} \times \mathcal{S} \to [0, 1]$. Due to the limited capacity for perception of the complex environment, each agent $i$ acquires a local observation $\textbf{o}^{(t)}_{i} \in \Omega$ via the observation function $\mathcal{O}(\textbf{o}_{i}^{(t)} \mid \textbf{s}_i^{(t)}, i) : \mathcal{S} \times \mathcal{N} \times \Omega \rightarrow [0, 1]$. All agents share a global reward function $ \mathcal{R}(\textbf{s}^{(t)}, \textbf{u}^{(t)}) : \mathcal{S} \times \mathcal{U} \ \to \mathbb{R} $ and the overall objective is to maximize the total discounted reward $\mathbb{E}\left [  {\textstyle \sum_{t}\gamma ^{t}\mathcal{R}^{\left ( t \right ) }  }  \right ]  $, where $\gamma \in \left [ 0, 1 \right ] $ means a discount factor. In alignment with the Dec-POMDP framework, we specify the elements as follows.

1) State: The state $\mathbf{s}^{(t)} \in \mathcal{S}$ can be task-dependent. For example, in an Unarmed Aerial Vehicle (UAV) scenario, an agent $i$ can obtain direct observation $\textbf{o}_{i} ^{\left ( t \right ) }  = \left \{ \left ( \textbf{p}_{j}^{\left ( t \right ) }, \textbf{v}_{j}^{\left ( t \right ) }     \right )\mid \forall j \in \xi _{i}^{\left ( t \right ) }   \right \} $ composed of the positions $\textbf{p}_{j}^{\left ( t \right )}$ and velocities $\textbf{v}_{j}^{\left ( t \right )}$ of itself and its neighbors and receives exchanged messages $\left \{ \textbf{m}_{j}^{\left ( t \right ) }\mid \forall j \in \xi _{i}^{\left ( t \right ) }   \right \} $, where $\textbf{m}_{j}^{\left ( t \right ) }$ represents a learnable vector intended for communication and $\xi _{i}^{\left ( t \right )}$ comprises agent $i$ and its neighbors. Moreover, the global observation $\textbf{o}_g^{(t)}$ can be expressed as $\textbf{o}_g^{(t)} = \left\{ \left(\textbf{p}_i^{(t)}, \textbf{v}_i^{(t)}\right) \mid \forall i \in \mathcal{N} \right\}$. Therefore, the global state $\textbf{s}^{(t)} = \left ( \textbf{o}_g^{(t)}, \left\{ \textbf{m}_i^{(t)} \mid \forall i \in \mathcal{N} \right\}  \right ) \in \mathcal{S}$ encompasses local states $\textbf{z}_i^{(t)} = \left(\textbf{o}_i^{(t)}, \left\{\textbf{m}_j^{(t)} \mid j \in \xi_i^{(t)}\right\}\right)$ of all agents.

2) Action: Based on the local state $\textbf{z}_i^{(t)}$, each agent determines its acceleration $\textbf{u}_i^{(t)} = \left( u_{x_i}^{(t)}, u_{y_i}^{(t)} \right) \in \mathcal{U}$ according to its policy $\pi_{\theta_i}\left(\cdot \mid \textbf{z}_i^{(t)}\right)$ individually to accomplish assigned relative tasks.

3) Reward: We define the reward function as a weighted sum of multiple task-oriented and/or communication-related components, which are detailed in Section \ref{sec4_Experiment}.
% formulated as follows,
% \begin{equation}
% \vspace{-0.3em}
% \mathcal{R}^{(t)} = \omega_k \mathcal{R}_k^{(t)} + \omega_m \mathcal{R}_m^{(t)},
% \end{equation}
% where the task-oriented reward $\mathcal{R}_k^{(t)}$ evaluates the agents' efficacy in executing specified tasks, which are detailed in Section \ref{sec4_Experiment}. To optimize the utilization of resources efficiently, the event-triggered reward $\mathcal{R}_m^{(t)}$ imposes a penalty on agents for transmitting information when triggered by specific events. Additionally, the coefficients $ \omega_k, \omega_m $ represent the corresponding weights.



\subsection{Multi-Agent Proximal Policy Optimization}\label{mappo}

To steer all agents toward maximizing the discounted accumulated reward $\mathbb{E}\left [ \sum_{t}\gamma ^{t}\mathcal{R}^{(t)}\right ]$, MAPPO \cite{MAPPO2022} is employed as the base RL method, which combines single-agent PPO \cite{PPO2017} with the centralized training and decentralized execution (CTDE) paradigm, aiming to learn both the individual policy $\pi_{\theta_i}\left(\cdot \mid \textbf{z}_i^{(t)}\right)$ for each agent $i$ and the value function $V_{\phi}(\textbf{s}^{(t)}) = \mathbb{E}_\pi \left [ \sum_{t}\gamma ^{t}\mathcal{R}^{(t)}|  \textbf{s}^{(t)}\right ]: \mathcal{S} \rightarrow \mathbb{R}$, parameterized by $\theta _{i} $ and $\phi $, respectively. Following the design of PPO, MAPPO retains the old versions of $\theta_{i,\text{old}} $ and $\phi_\text{old}$, while $\theta_{i,\text{old}} $ is used to interact with the environment and accumulate the samples. Additionally, the parameters $\theta _{i} $ and $\phi $ are periodically updated to maximize the objective function,
\begin{align}
J_{\pi_i}^{(t)}(\theta_i) &= \min\left(\beta_i^{(t)} \hat{A}^{(t)}, \ \text{clip}\left(\beta_i^{(t)}, 1 - \varepsilon, 1 + \varepsilon\right) \hat{A}^{(t)}\right), \notag \\
J_V^{(t)}(\phi) &= -\left(V_{\phi}(\textbf{s}^{(t)}) - \left(\hat{A}^{(t)} + V_{\phi_{\text{old}}}(\textbf{s}^{(t)})\right)\right)^2 \label{MAPPO},
\end{align}
where $\beta_i^{(t)} = \frac{\pi_{\theta_i}(\textbf{a}_i^{(t)} \mid \textbf{z}_i^{(t)})}{\pi_{\theta_{i,\text{old}}}(\textbf{a}_i^{(t)} \mid \textbf{z}_i^{(t)})}$ represents the importance ratio, $\varepsilon $ denotes a hyperparameter, while the Generalized Advantage Estimation (GAE) $\hat{A}^{(t)} = \sum_{l=0}^{T-t-1} (\gamma \lambda)^l \delta^{(t+l)}$ with the advantage estimate $\delta^{(t)}  = \mathcal{R}^{(t)} + \gamma V_{\phi_{\text{old}}}(\textbf{s}^{(t+1)}) - V_{\phi_{\text{old}}}(\textbf{s}^{(t)})$. Consequently, the final optimization objective of MAPPO is given by
\begin{equation}
\vspace{-0.3em}
J_{\text{MAPPO}} = \mathbb{E}_{i, t} \left[ J_{\pi_i}^{(t)}(\theta_i) + J_{V}^{(t)}(\phi) + \alpha H\left(\pi_{\theta_i}(\cdot \mid \textbf{z}_i^{(t)})\right) \right],
\end{equation}
where $\alpha$ is a coefficient and $H$ represents the entropy function.

\subsection{Problem Formulation}

As illustrated in Fig. \ref{environment}, our objective is to enable MARL-driven agents to maximize the global reward. Nevertheless, due to the partial observability of the global state $\textbf{s}^{(t)}$ and the distinction over local states $\textbf{z}_i^{(t)}$, it is essential to infer some global consensus across nodes beforehand, thus making consistent actions in a decentralized manner. Indeed, the consensus inference lies in how to leverage limited available information $\textbf{z}_i^{(t)}$ to make the inferred state as close to $\textbf{s}^{(t)}$ as possible. Many solutions \cite{xiang2023decentralized, Tarmac2019, masia2022} have been proposed in this area. Considering its performance superiority, ConsMAC \cite{xiang2023decentralized} is taken into account in this manuscript, while the proposed solution is applicable to other works such as TarMAC \cite{Tarmac2019} and MASIA \cite{masia2022}.

Generally, ConsMAC first leverages the combination of a GRU-like memory module $\mathcal{F}_{\psi_M}$, parameterized by $\psi_M$, and positional encoding-based concatenation \cite{attention2017,gnn2020,gnn2024} to efficiently embed local observation and exchanged messages $\left\{\textbf{m}_j^{(t)} \mid j \in \xi_i^{(t)}\right\}$ in $\textbf{o}_i^{(t)}$.  Mathematically,
\begin{align}
&\mathbf{E}_{m_{i}}^{(t)} \label{Emi}
\\
=& \left[\mathcal{F}_{\psi_M} \left([\textbf{m}_{i_0}^{(t)} \, \| \, \textbf{o}_i^{(t)}]\right) \, \| \, \mathbf{\Phi}_{d_0}^{(t)}, \cdots, \left(\mathcal{A}_j \, \| \, \mathbf{\Phi}_{d_j}^{(t)} \right), \cdots\right]^\top,\nonumber
\end{align}
where $\left |  \right | $ denotes the concatenation operation,  
$
\mathbf{\Phi}_{d_j}^{(t)} = \sqrt{\frac{1}{D}} $$\left[\cos(w_1 L_2(\textbf{p}_{i_j}^{(t)})), \ldots, \cos(w_D L_2( \textbf{p}_{i_j}^{(t)} )) \right]^\top    \label{d}
$ with $D$ learnable weights $\psi_D = [w_1, \cdots, w_D]$, and $\mathcal{A}(\cdot)$ denotes the availability of information. Notably, for each agent $i \in \mathcal{N}$, $i_j$ represents the $j$-th nearest neighbor, while $i_0$ refers to the agent itself. Therefore, assuming the existence of a lossless channel from $i$ to $j$, $\mathcal{A}(\cdot)$ becomes valid only if the occurrence of sending messages $\textbf{m}_{i_j}^{(t)}$ from $j$ to $i$.

Afterward, each agent then aggregates a latent vector $\textbf{h}^{(t)}$ as
\begin{equation}
\begin{aligned}
\textbf{h}^{(t)} = \text{MHA}_{\psi_A} \left(\mathbf{E}_{m_i}^{(t)}, \mathbf{E}_{m_{i}}^{(t)}, \mathbf{E}_{m_{i}}^{(t)}\right),
\end{aligned}
    \label{H}
\end{equation}
where $\text{MHA}$ refers to a multi-head attention layer parameterized by $\psi_A$. On the basis, ConsMAC utilizes a global estimator $\mathcal{F}_{\psi_E}$, parameterized by $\psi_E$, to estimate the state embedding $\hat{\textbf{e}}^{(t)} = \mathcal{F}_{\psi_E}\left(\textbf{h}^{(t)}\right)$ in a supervised learning manner. Specifically, the Consensus Establishment (CE) loss function is computed as
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{CE}}(\Psi) = \mathbb{E}_{{t}}[\parallel\textbf{o}_g^{(t)}-\hat{\textbf{e}}^{(t)}\parallel^2],
\label{CE}
\end{aligned}
\end{equation}
% where $\textbf{e}_{g}^{(t)}=\operatorname{softmax}\left(\textbf{o}_{g}^{(t)}\right)$ and $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E]$. 
where $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E]$. 
Through this process, the intermediate outputs of ConsMAC implicitly encode the global information, effectively establishing a consensus that captures the states of all agents. Meanwhile, the information flow also serves as an optimal message representation that encompasses all necessary details for the GIB. Besides, the message $\textbf{m}_{i}^{(t+1)}$ for time-step $t+1$ will be computed as
\begin{equation}
    \textbf{m}_{i}^{(t+1)} = \mathbf{E}_{o_i}^{(t)} + \varpi _{i}^{(t)} \textbf{h}^{(t)},
    \label{mt+1}
\end{equation}
where the embedding vector $\mathbf{E}_{o_i}^{(t)}$ and the communication information weight \( \varpi _{i}^{(t)} \) can be obtained by Multi-Layer Percepton (MLP)-based encoders $\mathcal{F}_{\theta_O}$ and $\mathcal{F}_{\theta_W}$ as
$\mathbf{E}_{o_i}^{(t)} = \mathcal{F}_{\theta_O} \left( \textbf{o}_i^{(t)} \right)$ and $\varpi _i^{(t)} = \mathcal{F}_{\theta_W} \left( \textbf{o}_i^{(t)} \right)$.

Additionally, an executor $\mathcal{F}_{\theta_E}$ is employed to sample the final action output by calculating the mean of the Gaussian distribution as,
\begin{equation}
\begin{aligned}
\mu_i^{(t)} = \mathcal{F}_{\theta_E}\left(\textbf{m}_i^{(t+1)}\right), \quad \textbf{u}_i^{(t)} \sim \text{Normal}\left(\mu_i^{(t)}, \sigma^2\right),
\end{aligned}
    \label{action}
\end{equation}
where $\sigma^2$ represents the variance constant that introduces randomness to the agent's actions during exploration and gradually diminishes throughout the training process. As mentioned in Sec. \ref{mappo}, in alignment with MAPPO, we consider $[ \Theta, \Psi ]$ as the parameters of the final policy $\pi_{\theta_i}$ in Eq. \eqref{MAPPO}, where $\Theta = [\theta_O, \theta_W, \theta_E]$.


Based on the aforementioned consensus inference mechanism, the problem can be reformulated as the calibration of $\mathcal{A}$ by 
%adaptively form an ideal topology and move towards the destination $\Delta_p$ while avoiding collisions, reduce communication frequency 
a variable-threshold event-triggering mechanism and global consensus inference $\mathbf{h}^{(t)}$. In other words,
%the representation optimization of consensus information through the Information Bottleneck theory. Meanwhile, 
if all agents share the same set of parameters during training to enhance learning efficiency, expressed as $\pi_{\theta_i} = \pi_{\theta}\ (\forall i \in \mathcal{N})$, it can be written as
% Accordingly, we define a unified system utility function $\Gamma $ as the objective for policy optimization, which can be formulated as,
\begin{equation}
\begin{aligned}
& \max_{\pi_{\theta}} \mathbb{E}_{t} \left[\sum_{t}\gamma ^{t}\mathcal{R}^{(t)} \mid \pi_{\theta}  \right], \\
\text{s.t.}\quad
&\textbf{u}_i(t) = \pi_\theta \left( g \left(\mathcal{A}^{(t)}, \textbf{h}^{(t)} \right) \right).
\end{aligned}
\end{equation}
where $g$ function denotes the specific calculations, which will be detailed in Sec. \ref{sec3_Framework}.

\section{The Framework of CDE-GIB}\label{sec3_Framework} 

The overall framework of the proposed CDE-GIB is shown in Fig. \ref{framework}. In addition to the policy execution module, it also encompasses the VT-ETM module and GIB-based consensus establishment module, which evaluate the information importance based on current and historic observations and encode concise message representation for consensus inference, respectively.
% 要围绕introduction做一些简单的重复
% The agent $i$ integrates the local observation information with the historical information from previous trigger moment to conduct semantic similarity analysis with the global information, and triggers if the resemblance exceeds the time-varying threshold. Subsequently, other agents combine their local observations with the trigger messages received from the agent $i$, and input fused information into the attention mechanism module to derive the global consensus estimation that incorporates distance information among agents. The application of Information Bottleneck theory further optimizes the global consensus estimation into an optimal message representation, which will be elaborated in the following subsections.

\begin{figure}[tp]
    \centering
    \includegraphics[scale=0.45]{framework.pdf}
    \vspace{-0cm}
\caption{The overall framework of CDE-GIB.} 
\vspace{-1.2em}
\label{framework}
\end{figure} 

% \subsection{ConsMAC-based Action Selection}
% %这一节需要与problem formulation整合
% % 这一篇工作只在如何trigger 和 IB；前置工作不是framework的一部分
% For the sake of clarity in notation, for each agent $i \in \mathcal{N}$, $i_j$ represents the $j$-th nearest neighbor, while $i_0$ refers to the agent itself.

% To effectively aggregate observations $\textbf{o}_{i}$ and messages $\textbf{m}_{j}$ received from neighbors, ConsMAC \cite{xiang2023decentralized} is meticulously designed with the following components. Firstly, to integrate historical information, each agent $i$ employs a GRU-like memory module $\mathcal{F}_{\psi_M}$ parameterized by $\psi_M$ to concatenate the agent's own message $\mathbf{m}_{i_{0}}^{(t)}$ with its local observation $\textbf{o}_{i}^{(t)}$, forming the subsequent input below,
% \begin{equation}
% \vspace{-0.3em}
% \textbf{m}'_{i_0} = \mathcal{F}_{\psi_M} \left([\textbf{m}_{i_0}^{(t)} \, \| \, \textbf{o}_i^{(t)}]\right),
%     \label{m}
% \end{equation}
% where $\left |  \right | $ denotes the concatenation operation, and both $\mathbf{m}'_{i_0}$ and $\textbf{m}_{i_0}^{(t)}$ share the same dimensions. Subsequently, inspired by the word encoding mechanism in Transformers \cite{attention2017} and the temporal encoding approach in temporal graph networks \cite{gnn2020,gnn2024}, we introduce a learnable distance encoder, which is designed to differentiate messages from agents at varying distances, thereby assigning different attention weights accordingly. Formally, this can be expressed as,
% \begin{equation}
% \begin{aligned}
% \mathbf{E}_{m_{i}}^{(t)} &= \left[\textbf{m}'_{i_0} \, \| \, \mathbf{\Phi}_{d_0}^{(t)}, \ldots, \textbf{m}_{i_k}^{(t)} \, \| \, \mathbf{\Phi}_{d_k}^{(t)}\right]^\top.
% \end{aligned}
%     \label{E}
% \end{equation}
% where
% \begin{equation}
% \begin{aligned}
% \mathbf{\Phi}_{d_j}^{(t)} = \sqrt{\frac{1}{D}} \left[\cos(w_1 \| \textbf{p}_{i}^{(t)} \|), \ldots, \cos(w_D \| \textbf{p}_{i}^{(t)} \|) \right]^\top,
% \end{aligned}
%     \label{d}
% \end{equation}

% The parameter vector $\boldsymbol{\psi}_D = [w_1, \ldots, w_D]$ contains learnable weights, where $D$ denotes the latent space dimension. Each agent then aggregates a latent vector $\textbf{h}_i^{(t)}$ as,
% \begin{equation}
% \begin{aligned}
% \textbf{h}_i^{(t)} = \text{MHA}_{\psi_A} \left(\mathbf{E}_{m_i}^{(t)}, \mathbf{E}_{m_{i}}^{(t)}, \mathbf{E}_{m_{i}}^{(t)}\right),
% \end{aligned}
%     \label{H}
% \end{equation}
% where $\text{MHA}$ refers to a multi-head attention layer parameterized by $\psi_A$. Unlike previous works \cite{niu2021,Tarmac2019}, we specifically infer global information using a global estimator $\mathcal{F}_{\psi_E}$, which is parameterized by $\psi_E$. The resulting estimated state embedding is expressed as,
% \begin{equation}
% \begin{aligned}
% \hat{\textbf{e}}_i^{(t)} = \mathcal{F}_{\psi_E}\left(\textbf{h}_i^{(t)}\right).
% \end{aligned}
% \end{equation}

% We utilize the global observation \(\textbf{o}_g^{(t)} \in \textbf{s}^{(t)}\) as the label for supervised learning, adopting pointwise Kullback–Leibler (KL) divergence as the loss function. Thus, the loss function of ConsMAC is defined as,
% \begin{equation}
% \begin{aligned}
% \mathcal{L}_{\text{CE}}(\Psi) = \mathbb{E}_{i \in \mathcal{N}} \left[ \sum_{x} \textbf{e}_g^{(t)}(x) \ln \frac{\textbf{e}_g^{(t)}(x)}{\hat{\textbf{e}}_i^{(t)}(x)} \right].
% \end{aligned}
% \end{equation}

% where $\textbf{e}_{g}^{(t)}=\operatorname{softmax}\left(\textbf{o}_{g}^{(t)}\right)$ and $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E]$. Through this process, the intermediate outputs of ConsMAC implicitly encode the global information, effectively establishing a consensus that captures the states of all agents.

% Accordingly, the RL-based policy generates appropriate actions in response to the observations and established consensus. In particular, an embedding vector $\mathbf{E}_{o_i}^{(t)}$ and the communication information weight \( w_{i}^{(t)} \) can be obtained by MLP-based encoders $\mathcal{F}_{\theta_O}$ and $\mathcal{F}_{\theta_W}$ as,
% \begin{equation}
% \mathbf{E}_{o_i}^{(t)} = \mathcal{F}_{\theta_O} \left( \textbf{o}_i^{(t)} \right), \quad w_i^{(t)} = \mathcal{F}_{\theta_W} \left( \textbf{o}_i^{(t)} \right).
% \end{equation}

% Afterward, the message $m_{i}^{(t+1)}$ for time-step $t+1$ will be computed as,
% \begin{equation}
%     \textbf{m}_{i}^{(t+1)} = \mathbf{E}_{o_i}^{(t)} + w_{i}^{(t)} \textbf{h}_{i}^{(t)} .
%     \label{mt+1}
% \end{equation}

% Additionally, an executor $\mathcal{F}_{\theta_E}$ is employed to sample the final action output by calculating the mean of the Gaussian distribution as,
% \begin{equation}
% \begin{aligned}
% \mu_i^{(t)} = \mathcal{F}_{\theta_E}\left(\textbf{m}_i^{(t+1)}\right), \quad \textbf{a}_i^{(t)} \sim \text{Normal}\left(\mu_i^{(t)}, \sigma\right),
% \end{aligned}
%     \label{action}
% \end{equation}

% where $\sigma$ represents the variance constant that introduces randomness to the agent's actions during exploration and gradually diminishes throughout the training process. As mentioned in Sec. \ref{mappo}, in alignment with MAPPO, we consider $[ \Theta, \Psi ]$ as the parameters of the final policy $\pi$ in Eq. \eqref{MAPPO}, where $\Theta = [\theta_O, \theta_W, \theta_E]$.

\subsection{Variable Threshold-Event Triggered Mechanism}\label{VT-ETMtitle}
The practical design of VT-ETM involves two parts. Firstly, to effectively reduce the frequency of event triggers, we use a zero vector $\textbf{o}_k^{(t)}$ as the label and apply Mean-Squared-Error (MSE) as the loss function to minimize the output of an event trigger function $\kappa(\cdot)$. Consequently, it ensures $\kappa(\cdot)$ to become as small as possible (i.e., the minimization of unnecessary triggering). Mathematically, the formula can be shown as
% \begin{align}
% &\mathcal{L}_{\text{VT-ETM}}(\psi_T) \label{eq:VT-ETM_loss} \\
% = & \mathbb{E}_{{i, t}} \left[D_{\text{KL}} \left( \kappa\left\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)} \right\}\parallel \textbf{o}_k^{(t)} \right) \right],\nonumber
% \end{align}
\begin{align}
&\mathcal{L}_{\text{VT-ETM}}(\psi_T) \label{eq:VT-ETM_loss} \\
= & \mathbb{E}_{{i, t}} \left[\parallel\kappa\left\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)} \right\}- \textbf{o}_k^{(t)}\parallel^2 \right],\nonumber
\end{align}
where $\mathcal{F}_{\psi_T}$ denotes an MLP parameterized by $\psi_T$ to process the agent $i$'s observation $\textbf{o}_{i}^{\left ( t \right )}$ and its historical local observation $\textbf{o}_{i}^{\left( \tilde{t_i}  \right) }$ at the last trigger moment $\tilde{t_i}$ that is memorized by Zero-Order Hold (ZOH) \cite{ZOH}. 
% 参考文献
Note that each agent $i$ maintains a sequence of event-triggering times $\mathcal{T}_i$ and when the agent determines that its information is semantically useful to other agents, it will add the corresponding time $t$ to the set $\mathcal{T}_i$. Correspondingly, $\tilde{t_i} = \displaystyle\arg \min_{\tau\in \mathcal{T}_i}{\left \{ t-\tau \right \}  }  \in \mathcal{T}_i$. Besides, the function $\kappa(\cdot)$ computes the similarity between the predictive information induced by $\mathcal{F}_{\psi_T}$ and the inferred consensus $\hat{\textbf{e}}^{(t-1)}$. Many similarity metrics can be adopted, such as cosine similarity, Manhattan distance, or Euclidean distance, depending on the characteristics of the data and the specific requirements of the task.
%where $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E, \psi_T]$\footnote{For simplicity of representation, we slightly abuse the notations here.} and $\textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) }$ represents the local observation of the agent $i$ that was memorized by Zero-Order Hold (ZOH) at the last trigger moment. Note that each agent possesses a sequence of event-triggering times following $ \tilde{t_i}\in \mathcal{T}_i $, $ \tilde{t_i} = \displaystyle\arg \min_{\tau\in \mathcal{T}_i}{\left \{ t-\tau \right \}  } $. Only when the agent determines that its information is semantically useful to other agents, it decides to trigger and adds time $\tilde{t_i}$ to the set $\mathcal{T}_i$. 
During the centralized training, we add $\psi_T$ to $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E, \psi_T]$\footnote{For simplicity of representation, we slightly abuse the notations here.} for parameter updating.

On the other hand, during decentralized execution, we introduce an exponential function that decreases over time, namely $G_\text{threshold} = c\zeta  ^{t}$, where $c> 0$ and $0<\zeta  < 1$, as the variable threshold. In other words, for agent $i$
\begin{equation}
\mathcal{A}_i \text{ is }
\begin{cases}
\texttt{VALID}, &\!\!\!\! \kappa\left\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)}\right\} > G_{\text{threshold}}; \\
\texttt{VOID}, &\!\!\!\! \kappa\left\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)}\right\} \leq G_{\text{threshold}}.
\end{cases}
    \label{VT-ETM}
\end{equation}
Exploiting a time-decreasing threshold $G_\text{threshold}$ aligns with the branch-out approaches used to mitigate accumulative errors during model rollouts \cite{rollout}, %补充参考文献，可以问一下念念
progressively compromising the accuracy of predictions. Therefore, an adaptive threshold that encourages increasingly frequent updates can naturally counteract the gradually enlarged errors. 
%To counteract this issue, we have devised an adaptive threshold  that helps the system maintain efficient communication, encouraging increasingly frequent updates to minimize the impact of accumulating errors. Finally, event trigger function $\kappa(\cdot)$ is used to evaluate the level of semantic similarity between the comprehensive information represented by the function $\mathcal{F}_{\psi_T}$ and the consensus $\hat{\textbf{e}}^{(t-1)}$. To quantify this alignment, the similarity can be calculated using various methods, such as cosine similarity, Manhattan distance, or Euclidean distance, depending on the characteristics of the data and the specific requirements of the task.
% The reason for adjusting the event trigger threshold to decrease over time aligns with the approach used to mitigate modeling errors during extended rollouts. In such rollouts, errors tend to accumulate, progressively compromising the accuracy of predictions. To counteract this issue, we have devised an adaptive thresholding mechanism that helps the system maintain efficient communication, encouraging more frequent updates to minimize the impact of accumulating errors. Finally, event trigger function $\kappa(\cdot)$ is used to evaluate the level of semantic similarity between the comprehensive information represented by the function $\mathcal{F}_{\psi_T}$ and the consensus $\hat{\textbf{e}}^{(t-1)}$. To quantify this alignment, the similarity can be calculated using various methods, such as cosine similarity, Manhattan distance, or Euclidean distance, depending on the characteristics of the data and the specific requirements of the task.
% Thus, we specify $g_i(t)$ to denote the agent's triggering behavior as follows,
% \begin{equation}
% g_i(t) =
% \begin{cases}
% b_i^{\text{send}}, &\!\!\!\! \kappa\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)}\} - G_{\text{threshold}} > 0 \\
% b_i^{\text{none}}, &\!\!\!\! \kappa\{\mathcal{F}_{\psi_T}\left ( \textbf{o}_{i}^{\left ( t \right ) } , \textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) } \right ) , \hat{\textbf{e}}^{(t-1)} \} - G_{\text{threshold}} \leq 0
% \end{cases}
%     \label{VT-ETM}
% \end{equation}
Finally, if $\mathcal{A}_i$ is \texttt{VOID}, neighboring agents will be unable to receive any exchanged messages from agent $i$. In this case, instead of using the memorized message in \cite{ETCNET2023}, the corresponding elements in Eq. \eqref{Emi} will be replaced by an all-zero vector. Such a setting contributes to allow agents receiving messages infrequently to avoid over-reliance on outdated memorized messages that may hinder their ability to make timely decisions. 

%If the similarity exceeds the threshold value $G_{\text{threshold}}$, agent $i$ will process its original information $\textbf{m}_{i}^{(t)}$ into $\tilde{\boldsymbol{\textbf{m}}}_{i}^{(t)}$ and dispatch it to the corresponding agent cluster as required, while the agent that does not receive the message will represent the absence of the message with a zero vector and wait for updated information from other agents instead of using the memorized message in \cite{ETCNET2023}. Such a setting benefits agents that receive messages infrequently during certain triggering moments, relying on outdated memorized messages may hinder their ability to make timely decisions. 
% where $\textbf{o}_{i}^{\left ( \tilde{t_i}  \right ) }$ represents the local observation of the agent $i$ that was memorized by Zero-Order Hold (ZOH) at the last trigger moment. Note that each agent possesses a sequence of event-triggering times following $ \tilde{t_i}\in \mathcal{T}_i $, $ \tilde{t_i} = \displaystyle\arg \min_{\tau\in \mathcal{T}_i}{\left \{ t-\tau \right \}  } $. Only when the agent determines that its information is semantically useful to other agents, it decides to trigger and adds time $\tilde{t_i}$ to the set $\mathcal{T}_i$. To model the variable threshold, we employ an exponential function that decreases over time: $G_\text{threshold} = c\alpha ^{t}$, where $c> 0$ and $0<\alpha < 1$. Finally, we assess the level of semantic similarity between the comprehensive information represented by the function $\mathcal{F}_{\psi_T}$ and the consensus $\textbf{e}_i^{(t-1)}$. If the similarity exceeds the threshold value $G_{\text{threshold}}$, agent $i$ will process its original information $\textbf{m}_{i}^{(t)}$ into $\tilde{\boldsymbol{\textbf{m}}}_{i}^{(t)}$ and dispatch it to the corresponding agent cluster as required.

% The essence of event-triggered communication lies in selecting the appropriate target agents for message transmission by agent $i$. In the ETCNet paper \cite{ETCNET2023}, the strategy is proposed where "if no message is received, the agent uses the memorized message" to prevent information loss. However, for agents that receive messages infrequently during certain triggering moments, relying on outdated memorized messages may hinder their ability to make timely decisions. Therefore, it is more beneficial to reset the message to zero and wait for updated information from other agents.

% In summary, the loss function of VT-ETM can be formulated as,
% \begin{equation}
% \mathcal{L}_{\text{VT-ETM}}(\psi_T) = \mathbb{E}_{i \in \mathcal{N}}[D_{\text{KL}} \left( \kappa\parallel \textbf{o}_k^{(t)} \right)],
% \end{equation}
% where $\textbf{o}_k^{(t)}$ is a zero vector and $\Psi = [\psi_D, \psi_M, \psi_A, \psi_E, \psi_T]$.

% In contrast to methods like ATOC \cite{ATOC2018}, our approach not only considers the current information but also incorporates data from the previous triggering moment, thereby ensuring a more comprehensive representation. Compared to ETCNet \cite{ETCNET2023}, which also takes into account both current and historical information, we further introduce a variable-threshold mechanism that allows for adaptive adjustments tailored to specific requirements of formation control. （挪到了introduction）

% Meanwhile, the validity of the designed conditions often hinges on the mechanism’s ability to effectively prevent Zeno behavior, which is characterized by an infinite number of triggers within a finite time period. However, excluding Zeno behavior is not a concern for periodic event-triggered mechanisms, as they inherently have a lower bound for sampling frequency. Likewise, in the context of reinforcement learning (RL), Zeno behavior is naturally avoided due to its intrinsic periodic nature.

% \textbf{Lemma 1} \cite{lemma1}: The Zeno behavior for edge $(i, j)$ occurs if $\exists T < +\infty$ such that $\lim_{k \to \infty} t^{ij}_k = T$, where $t^{ij}_k$ represents the $k$-th triggering instant of $(i, j)$.

\subsection{Graph Information Bottleneck for the Communication Graph and Information Flow Optimization}
%总体来看这一段要忍心做大手术，仔细琢磨一下怎么起承转合，而不是微调一下。
%起的点感觉是放在MAGI上（想想怎么在第一段过渡到这个上面）。
%再之前的内容，感觉太老了，会显得啰嗦。要站在SOTA上，说他的不足。
As mentioned above, MAGI \cite{MAGI2024} generates excessive communication volume due to the cumbersome interaction with all agents within a specified range. Moreover, the algorithm suffers from high computational complexity caused by the separate compression of the communication graph and data flow. To address this challenge, we propose the GIB-based joint optimization. Specifically, different from MAGI \cite{MAGI2024}, which considers the triplet $\langle$\texttt{Feature}, \texttt{Communication/Graph Information}, \texttt{Explicit Action}$\rangle$, our method incorporates a novel triplet $\langle$\texttt{Feature}, \texttt{Implicit Consensus}, \texttt{Global Observation}$\rangle$ to infer the consensus from data flow and the communication graph simultaneously.% MAGI, on the other hand, considers $\langle$\texttt{Feature}, \texttt{Communication/Topology Information}, \texttt{Explicit Action}$\rangle$ as its primary research objective.

Beforehand, for graph-structured agent features, we introduce a local-dependence assumption to avoid explicitly requiring input data to be i.i.d.



% To achieve the objective of information compression in agent swarms, we enhance the conventional Information Bottleneck theory \cite{IB2020} by integrating a consensus-based communication model. Specifically, we further compress the consensus $\textbf{h}^{(t)}$ established by integrating the exchanged information $\textbf{m}^{(t)}$ in the communication process. Our proposed method introduces a novel triplet format that integrates the fused information of observations and communication graph-based data, as well as global representations and labels. This also differs from the previous IB-RL algorithmic framework \cite{MAGI2024}, which utilizes a combinatorial optimization model integrating observations with the communication graph, along with information representations and actions.

% According to the traditional IB theory, our algorithm can be expressed as,
% \begin{equation}
% \min  -I(\textbf{h}; \textbf{o}_g) + \eta  I(\textbf{h}; \textbf{E}_{m_i}).
% \label{IB}
% \end{equation}

% The input feature data for the communication learning mechanism based on GNNs can be universally expressed as $\mathcal{D} = (\mathcal{E}, \mathcal{M})$, where $\mathcal{D}$ denotes the aggregated messages and topology representation $\textbf{E}_{m_i}$ as shown in \eqref{Emi}. Our primary objective is to compress the consensus $\textbf{h}$ from $\textbf{E}_{m_i}$, while promoting consensus $\textbf{E}_{m_i}$ to closely approximate the target global observation labels $\textbf{o}_g$. 

% However, previous IB-based representation learning methods typically require input data to meet independent and identically distributed (i.i.d.) conditions. Consequently, applying the IB principle in MARL becomes challenging, as such conditions do not hold for agent-specific features. To solve this problem, we introduce a local-dependence assumption for graph-structured agent features stated in Assumption \ref{assumption}. This assumption allows us to effectively constrain the optimal space for communication message representations, thereby making the optimization of our objective more tractable.
\begin{Assumption}
    For each agent $i$, given the neighbor-related agents within a certain number of hops, the features of the remaining agents are considered independent of the feature of agent $i$. 
\label{assumption}
\end{Assumption}

Contingent on Assumption \ref{assumption}, we discuss the implementations of the GIB-based joint compression of the communication graph and information flow towards a more compact consensus representation. Without loss of generality, the input feature data for the communication learning mechanism based on GNNs can be universally expressed as $\mathcal{D} = (\mathcal{E}, \mathcal{H})$, where $\mathcal{D}$ denotes the aggregated message and the embedding representation $\textbf{E}_{m_i}^{(t)}$ as shown in \eqref{Emi}. Therefore, our primary objective is to compress the consensus $\textbf{h}^{(t)}$ from $\textbf{E}_{m_i}^{(t)}$, while promoting consensus $\textbf{h}^{(t)}$ to closely approximate the target global observation labels $\textbf{o}_g^{(t)}$. Mathematically, we want to minimize $\mathcal{L}_{\text{IB}} = -I(\textbf{h}^{(t)}; \textbf{o}_g^{(t)}) + \eta I(\textbf{h}^{(t)}; \textbf{E}_{m_i}^{(t)})$. Due to the difficulty to know the joint distribution $p(\textbf{h}^{(t)}, \textbf{o}_g^{(t)})$ and $p(\textbf{h}^{(t)}, \textbf{E}_{m_i}^{(t)})$, by Lemma \ref{I_NWJ1}, we have the following theorem.

\begin{lemma}[Nguyen, Wainright \& Jordan’s bound \cite{GIB}]
\label{I_NWJ1}
For two random variables $X$ and $Y$, 
\begin{align}
I\left(Y ; X\right) &\geq 1+\mathbb{E}_{p(Y)}\left[\log \frac{\prod_{i \in \mathcal{N}} p_{1}\left(Y_i \mid X_{i}\right)}{p_{2}(Y)}\right]\\
&-\mathbb{E}_{p(Y) p\left(X\right)}\left[\frac{\prod_{i \in \mathcal{N}} p_{1}\left(Y_{i} \mid X_i\right)}{p_{2}(Y)}\right].\nonumber
\end{align} %where $\mathcal{N}$ denotes the pool of nodes.
\end{lemma}

\begin{theorem}
\label{theorem:gib}
 The GIB can be bounded by
    \begin{align}
    \mathcal{L}_{\text{IB}} \leq& \mathbb{E}_{{i, t}} \left[ D_{\text{KL}} \left( p(\textbf{o}_g^{(t)}) \parallel p(\textbf{h}^{(t)}) \right) \right] \label{IB}\\
     &\underbrace{+ \eta\mathbb{E}_{p(\textbf{E}_{m_i}^{(t)})} \left[ D_{\text{KL}} \left( p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \parallel q_{\textbf{h}}(\textbf{h}^{(t)}) \right) \right]}_{\mathcal{L}_\text{KL}}, \nonumber
    % I(\textbf{h}^{(t)}; \textbf{o}_g^{(t)}) &\geq - \mathbb{E}_{i \in \mathcal{N}} \left[ D_{\text{KL}} \left( \textbf{h}^{(t)} \parallel \textbf{o}_g^{(t)} \right) \right], \label{lower} \\
    % I(\textbf{h}^{(t)}; \textbf{E}_{m_i}^{(t)}) &\leq \mathbb{E}_{p(\textbf{E}_{m_i}^{(t)})} \left[ D_{\text{KL}} \left( p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \parallel q_{\textbf{h}}(\textbf{h}^{(t)}) \right) \right]. \label{upper}
    \end{align}
    where $q_{\textbf{h}}(\textbf{h}^{(t)}) $ denotes a probability function sharing the same variable space as $p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)})$.
\end{theorem}
\begin{proof}
    % \begin{align}
    % I(\textbf{h}^{(t)}; \textbf{o}_g^{(t)}) &= \int p(\textbf{h}^{(t)}, \textbf{o}_g^{(t)}) \log \frac{p(\textbf{h}^{(t)} , \textbf{o}_g^{(t)})}{p(\textbf{h}^{(t)})p(\textbf{o}_g^{(t)})} \, d\textbf{h}^{(t)}\, d\textbf{o}_g^{(t)} \notag \\
    % &= \int p(\textbf{h}^{(t)}, \textbf{o}_g^{(t)}) \log \frac{p(\textbf{o}_g^{(t)} | \textbf{h}^{(t)})}{p(\textbf{o}_g^{(t)})} \, d\textbf{h}^{(t)} \, d\textbf{o}_g^{(t)} \notag \\
    % &= \int p(\textbf{h}^{(t)}, \textbf{o}_g^{(t)}) \log p(\textbf{o}_g^{(t)} | \textbf{h}^{(t)}) \, d\textbf{h}^{(t)} \, d\textbf{o}_g^{(t)} + H(\textbf{o}_g^{(t)}) \notag \\
    % &\overset{(a)}{\geq} \int p(\textbf{h}^{(t)}, \textbf{o}_g^{(t)}) \log q(\textbf{o}_g^{(t)} | \textbf{h}^{(t)}) \, d\textbf{h}^{(t)} \, d\textbf{o}_g^{(t)} \notag \\
    % &= \mathbb{E} \left[ \log q(\textbf{o}_g^{(t)} | \textbf{h}^{(t)}) \right].
    % \label{lower_IB}
    % \end{align}
    \begin{align}
    &I\left(\textbf{h}^{(t)}; \textbf{o}_g^{(t)}\right) \nonumber                   \\
    \overset{(a)}{\geq}&1+\mathbb{E}_{{p}(\textbf{o}_g^{(t)})}\left[\log \frac{ p(\mathcal{F}_{\psi_E}\left(\textbf{h}^{(t)}\right))}{p\left(\textbf{o}_g^{(t)}\right)}\right]      \nonumber \\
    &\quad -\mathbb{E}_{p{(\textbf{o}_g^{(t)})}p{(\textbf{h}^{(t)})}}\left[\frac{ p(\mathcal{F}_{\psi_E}\left(\textbf{h}^{(t)}\right))}{p\left(\textbf{o}_g^{(t)}\right)}\right]    \label{lower_IB}     \\
    \overset{(b)}{\geq} &- \mathbb{E}_{{p}(\textbf{o}_g^{(t)})}\left[\log \frac{p\left(\textbf{o}_g^{(t)}\right)}{p(\mathcal{F}_{\psi_E}\left(\textbf{h}^{(t)}\right))}\right]    \nonumber   \\
    =&- \mathbb{E}_{{t}} \left[ D_{\text{KL}} \left( p(\textbf{o}_g^{(t)}) \parallel p(\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})) \right) \right], \nonumber
    \end{align}
    where the inequality (a) uses the Nguyen, Wainright \& Jordan’s bound $I_\text{NWJ}$ \cite{GIB} and the inequality (b) is derived from the condition $1-\mathbb{E}_{p{(\textbf{o}_g^{(t)})}p{(\textbf{h}^{(t)})}}\left[\frac{ p(\mathcal{F}_{\psi_E}\left(\textbf{h}^{(t)}\right))}{p\left(\textbf{o}_g^{(t)}\right)}\right] > 0$. 
    % In other words, for two random variables $X$ and $Y$, 
    % \begin{align}
    % I\left(Y ; X\right) &\geq 1+\mathbb{E}_{p(Y)}\left[\log \frac{\prod_{i \in \mathcal{N}} p_{1}\left(Y_i \mid X_{i}\right)}{p_{2}(Y)}\right]\\
    % &-\mathbb{E}_{p(Y) p\left(X\right)}\left[\frac{\prod_{i \in \mathcal{N}} p_{1}\left(Y_{i} \mid X_i\right)}{p_{2}(Y)}\right],\nonumber
    % \end{align} where $\mathcal{N}$ denotes the pool of nodes.
    On the other hand,
    \begin{align}
    &I(\textbf{h}^{(t)}; \textbf{E}_{m_i}^{(t)}) \nonumber \\
    =&\iint p(\textbf{h}^{(t)},\textbf{E}_{m_i}^{(t)})\log \frac{p(\textbf{h}^{(t)},\textbf{E}_{m_i}^{(t)})}{p(\textbf{h}^{(t)})p(\textbf{E}_{m_i}^{(t)})} \, d\textbf{h}^{(t)} \, d\textbf{E}_{m_i}^{(t)}  \label{upper_IB}  \\
    =& \iint p(\textbf{E}_{m_i}^{(t)}) p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \log \frac{p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)})}{p(\textbf{h}^{(t)})} \, d\textbf{h}^{(t)} \, d\textbf{E}_{m_i}^{(t)} \nonumber \\
    \overset{(c)}{\leq}& \iint p(\textbf{E}_{m_i}^{(t)}) p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \log \frac{p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)})}{q_{\textbf{h}}(\textbf{h}^{(t)})} \, d\textbf{h}^{(t)} \, d\textbf{E}_{m_i}^{(t)} \nonumber \\
    =& \mathbb{E}_{p(\textbf{E}_{m_i}^{(t)})} \left[ D_{\text{KL}} \left( p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \parallel q_{\textbf{h}}(\textbf{h}^{(t)}) \right) \right],\nonumber 
    \end{align} 
where the inequality (c) originates from Gibbs' inequality, where $p \log p \geq p \log q$, with equality if and only if $p$ and $q$ are the same distribution. To further estimate $p(\textbf{h}^{(t)})$, we treat $q_{\textbf{h}}(\textbf{h}^{(t)})$ as the variational approximation.% to $p(\textbf{h}^{(t)})$ and model $q_{\textbf{h}}(\textbf{h}^{(t)})$ as a mixture of Gaussian distributions. Specifically, we define $q_{\textbf{h}}(\textbf{h}^{(t)}) \sim \sum_{c=1}^N w_c \, \mathcal{N}(\mu_{c}, \sigma_{c}^2)$, where $w_c$, $\mu_{c}$, and $\sigma_{c}$ are learnable parameters shared across all agents. 

% Combining Eq. \eqref{lower_IB} and Eq. \eqref{upper_IB}, we have the lemma.
\end{proof}
Compared to Eq. \eqref{CE}, Theorem \ref{theorem:gib} unveils the impact of GIB on the consensus-building module. In a nutshell, the loss function of CDE-GIB can be expressed as,
\begin{equation}
\mathcal{L}_{\text{CDE-IB}}(\Theta, \phi, \Psi) = -J_{\text{MAPPO}} + \mathcal{L}_{\text{CE}} + \varrho\mathcal{L}_{\text{VT-ETM}} + \rho\mathcal{L}_{\text{GIB}}.
    \label{loss}
\end{equation}

However, it remains difficult to compute the KL divergence directly during training. Fortunately, we have the following lemma. 

\begin{lemma}
[Ref. \cite{GMM}]
    \label{lem:guassian_kl}
    % Guassian GMM KL
    % Considering $f(\mathbf{x})$ and $g(\mathbf{x})$ ($\mathbf{x} \in \mathbb{R}^K$) as a Gaussian distribution and a mixture (parameterized by $w_l,\ l\in\{1,\cdots,L\}$) of Gaussian distributions, respectively, that is,
    % \begin{equation}
    %     f(\mathbf{x}) \sim \mathcal{N}(\bm{\mu}_{m}, \bm{\Sigma}_{m}),\ 
    %     g(\mathbf{x}) \sim \sum\nolimits_{l=1}^L w_l \, \mathcal{N}(\bm{\mu}_{l}, \bm{\Sigma}_{l}),
    % \end{equation}
    Considering $f(\mathbf{x})$ and $g(\mathbf{x})$ ($\mathbf{x} \in \mathbb{R}^K$) as Gaussian distributions, that is,
    \begin{equation}
        f(\mathbf{x}) = \mathcal{N}(\bm{\mu}_{m}, \bm{\Sigma}_{m}),\ 
        g(\mathbf{x}) =  \mathcal{N}(\bm{\mu}_{l}, \bm{\Sigma}_{l}),
    \end{equation}
    where $\bm{\mu} = [\mu^1,\cdots,\mu^K]$\footnote{For simplicity of representation, we omit the subscript $m$ and $l$ for $\bm{\mu}$ and $\bm{\Sigma}$ here.} and $\bm{\Sigma} = \text{diag}[(\sigma^1)^2,\cdots,(\sigma^K)^2]$, 
    the KL divergence between these two distributions can be computed as
    % \begin{align}
    %     &\mathbb{E} \left[ D_{\text{KL}} \left( f(\mathbf{x}) \parallel g(\mathbf{x}) \right) \right] \label{eq:kl_guassian_gmm}\\ \leq &\mathbb{E}\left[\sum_{l=1}^{L} w_l\left( \sum_{k=1}^{K} \left(\log \frac{\sigma_{m}^k}{\sigma_{l}^k}+\frac{\left(\sigma_{m}^k\right)^{2}+\left(\mu_{m}^k-\mu_{l}^k\right)^{2}}{2\left(\sigma_{l}^k\right)^{2}}-\frac{1}{2}\right)\right)\right].\nonumber
    % \end{align}
    % Specially, when $L=1$, Eq. \eqref{eq:kl_guassian_gmm} degenerates to a closed form
    \begin{align}
        &\mathbb{E} \left[ D_{\text{KL}} \left( f(\mathbf{x}) \parallel g(\mathbf{x}) \right) \right] \label{eq:kl_guassian_guassian}\\ =&\mathbb{E}\left[ \sum_{k=1}^{K} \left(\log \frac{\sigma_{m}^k}{\sigma_{l}^k}+\frac{\left(\sigma_{m}^k\right)^{2}+\left(\mu_{m}^k-\mu_{l}^k\right)^{2}}{2\left(\sigma_{l}^k\right)^{2}}-\frac{1}{2}\right)\right].\nonumber
    \end{align}
    % Subsequently, we adopted the degenerative approach to simplify $g_1(x)$ into $g_2(x)$ by selecting representative values for the mean and variance, and the formula is as follow,
    % \begin{equation}
    % \label{SR}
    %     \mathcal{L}_{\text{SR}} = \mathbb{E} \left[ D_{\text{KL}} \left( f(x) \parallel g_2(x) \right) \right],
    % \end{equation}
    % \begin{equation}
    %     f(x) \sim  w_m \, \mathcal{N}(\mu_{m}, \sigma_{m}^2),\quad
    %     g_2(x) \sim w_{l_1} \, \mathcal{N}(\mu_{l_1}, \sigma_{l_1}^2) , l_1\in [1,L],
    % \end{equation}
    % so we can transfer Eq. \eqref{SR} as the following SR loss according to the analytical solution for calculating the KL divergence between two Gaussian distributions,
    % \begin{equation}
    %     \mathcal{L}_{\text{SR}}=\mathbb{E}\left[\sum_{d=1}^{D}\left(\log \frac{\sigma_{m}^{d}}{\sigma_{l_1}^{d}}+\frac{\left(\sigma_{m}^{d}\right)^{2}+\left(\mu_{m}^{d}-\mu_{l_1}^{d}\right)^{2}}{2\left(\sigma_{l_1}^{d}\right)^{2}}\right)\right].
    % \end{equation}
\end{lemma}
Contingent on the following Assumption \ref{assump:guassian}, we can have a corollary to faciliate the computations of GIB, which lays the very foundation for computing GIB across batches. 
\begin{Assumption}[Consistent with Ref. \cite{TOCF}]
Taking a batch of collected data, we assume $\textbf{o}_g^{(t)}$, $\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})$, and $\textbf{h}^{(t)} $ satisfy the following Gaussian distributions,
\begin{align}
\label{eq:guassian}
    p(\textbf{o}_g^{(t)}) =& \mathcal{N}(\bm{\mu}_{\textbf{o}_g^{(t)}},\bm{\Sigma}_{\textbf{o}_g^{(t)}}), \nonumber\\ 
   % p(\textbf{h}^{(t)}) =  \mathcal{N}(\bm{\mu}_{\textbf{h}^{(t)}},\bm{\Sigma}_{\textbf{h}^{(t)}})\nonumber,\\
   p(\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})) =& \mathcal{N}(\bm{\mu}_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})},\bm{\Sigma}_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})}),\\
   p(\textbf{h}^{(t)} ) =& \mathcal{N}(\bm{\mu}_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}},\bm{\Sigma}_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}}),\nonumber
\end{align}
where 
$\bm{\mu}_{\textbf{o}_g^{(t)}},\ \bm{\mu}_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})},\ \bm{\sigma}_{\textbf{o}_g^{(t)}},\ \bm{\sigma}_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})} \in \mathbb{R}^{K_1}$, and 
$\bm{\mu}_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}},\ \bm{\sigma}_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}} \in \mathbb{R}^{K_2}$.

\label{assump:guassian}
\end{Assumption}

\begin{corollary}
\label{coro:gib}
    Contingent on Assumption \ref{assump:guassian}, if the variational approximation $q_{\textbf{h}}(\textbf{h}^{(t)}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$, the GIB bound can be computed as Eq. \eqref{eq:gib_opt}.
\end{corollary}
The corollary can be easily obtained by applying Assumption \ref{assump:guassian} and Lemma \ref{lem:guassian_kl} in Theorem \ref{theorem:gib}. In summary, the training procedure for CDE-GIB is presented in Algorithm \ref{algorithm}.

\begin{figure*}
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{GIB}}
    \leq&\mathbb{E}\left[ \sum_{k=1}^{K_1} \left(\log \frac{\sigma_{\textbf{o}_g^{(t)}}^k}{\sigma_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})}^k}+ \frac{\left(\sigma_{\textbf{o}_g^{(t)}}^k\right)^{2}+\left(\mu_{\textbf{o}_g^{(t)}}^k-\mu_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})}^k\right)^{2}}{2\left(\sigma_{\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})}^k\right)^{2}}\right)\right] \\
    &+\mathbb{E}\left[ \sum_{k=1}^{K_2} \left(\log \sigma_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}}^k
    +\frac{\left(\sigma_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}}^k\right)^{2}+\left(\mu_{\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}}^k\right)^{2}}{2}\right)\right]-1.
\end{aligned}\label{eq:gib_opt}
\end{equation}
\hrulefill
\vspace{-1.5em}
\end{figure*}



% If $p(\textbf{o}_g^{(t)})$, $p(\textbf{h}^{(t)})$ and their inductive variants $p(\mathcal{F}_{\psi_E}(\textbf{h}^{(t)}))$ and  $p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)})$ can be approximated by Gaussian distribution $\mathcal{N}(\bm{\mu}_{\textbf{o}_g^{(t)}},\bm{\Sigma})$ and $\mathcal{N}(\bm{\mu}_{\textbf{h}^{(t)}},\bm{\Sigma})$, respectively; while $q_{\textbf{h}}(\textbf{h}^{(t)})$ is a mixture of Gaussian distributions, i.e., $q_{\textbf{h}}(\textbf{h}^{(t)}) \sim \sum_{l=1}^L w_l \, \mathcal{N}(\bm{\mu}_{l}, \bm{\Sigma})$. In other words, all distributions share the same variance. Then, we have the following theorem.
% \begin{corollary}
% \label{theorem:appro}
% Optimizing the KL divergence in Theorem \ref{theorem:gib} can be approximated by minimizing the mean square error (MSE). In other words, we have Eq. \eqref{eq:kl_mse_appro}.
% Under XXX approximated by guassian,
% theorem 1 --> MSE
%Under the assumption that the global distribution $\textbf{o}_g^{(t)}$ and the consensus distribution $\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})$ are approximated by Gaussian distributions, we proceed with the following approach,
% \begin{figure*}[t]
% \begin{equation}
%     \label{eq:kl_mse_appro}
%     \arg \max_{\Theta, \Psi} \mathcal{L}_{\text{KL}} \approx \arg \max_{\Theta, \Psi} \mathbb{E}\left[\sum_{k=1}^{K} \left(\mu_{\textbf{o}_g^{(t)}}^k-\mathcal{F}_{\psi_E}(\mu_{\textbf{h}^{(t)}}^k)\right)^2  +  \sum_{l=1}^{L} w_l\left( \sum_{k=1}^{K} \left(\mu_{l}^k-\mu_{\textbf{h}^{(t)}}^k\right)^{2} \right)\right]. 
%     %\mathcal{L}_{\text{SR}}=-\mathbb{E}_{i \in \mathcal{N}}[\operatorname{MSE}(\textbf{o}_g^{(t)},\mathcal{F}_{\psi_E}(\textbf{h}^{(t)}))].
% \end{equation}
% \hrule
% \end{figure*}
% \end{corollary}
% \begin{proof}
% By Lemma \ref{lem:guassian_kl}, Eq. \eqref{lower_IB} can be written as
%     \begin{align}
%     &I\left(\textbf{h}^{(t)} ; \textbf{o}_g^{(t)}\right) \nonumber                   \\
%     &\geq- \mathbb{E}_{i \in \mathcal{N}} \left[ D_{\text{KL}} \left( p(\textbf{o}_g^{(t)}) \parallel p(\mathcal{F}_{\psi_E}(\textbf{h}^{(t)})) \right) \right],    \\
%     &=-\mathbb{E}_{i \in \mathcal{N}}\left[\sum_{d=1}^{D}\left(\log \frac{\sigma_{m}^{d}}{\sigma_{l_1}^{d}}+\frac{\left(\sigma_{m}^{d}\right)^{2}+\left(\mu_{m}^{d}-\mu_{l_1}^{d}\right)^{2}}{2\left(\sigma_{l_1}^{d}\right)^{2}}\right)\right] \nonumber  \\
%     &\overset{(c)}{=}-\mathbb{E}_{i \in \mathcal{N}}[\operatorname{MSE}(\textbf{o}_g^{(t)},\mathcal{F}_{\psi_E}(\textbf{h}^{(t)}))],\nonumber  
%     \end{align}
% where the equality at (c) indicates that the variance computation does not affect the minimization of the target loss function and can be ignored.
% Similarly, Eq. \eqref{upper_IB} can be written as XXX.
% $p(\textbf{h}^{(t)}\mid\textbf{E}_{m_i}^{(t)})=\operatorname{softmax}\left(\textbf{h}^{(t)}\right)$ according to Eq. \eqref{H}.  
% The inequality at (b) originates from Gibbs' inequality, where $p \log p \geq p \log q$, with equality if and only if $p$ and $q$ are the same distribution. 
% By substituting Eq. \eqref{lower_IB} and Eq. \eqref{upper_IB} into Eq. \eqref{IB}, we derive the optimization objective as follows,
% \begin{align}
%     &\mathcal{L}_{\text{IB}} \overset{(c)}{=} \mathbb{E}_{i \in \mathcal{N}} \left[ D_{\text{KL}} \left( \textbf{h}^{(t)} \parallel \textbf{o}_g^{(t)} \right) \right]\notag\\ &\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!+\mathbb{E}_{p(\textbf{E}_{m_i}^{(t)})} \left[ D_{\text{KL}} \left( p(\textbf{h}^{(t)} \mid \textbf{E}_{m_i}^{(t)}) \parallel q_{\textbf{h}}(\textbf{h}^{(t)}) \right) \right].
% \end{align}
% To ensure the stability of the optimization process, the equality at (c) is used for calculation.
% \end{proof}
% The proof naturally comes from Lemma \ref{lem:guassian_kl} and is omitted here due to the space limitation.
% In each round $l$ of message exchange, we leverage the local dependency assumption. Each agent's communication message can be refined by aggregating the features of its neighbors and extracting information through multiple layers of attention mechanisms. Ultimately, we utilize the high-level latent vector representations to establish a global consensus. Therefore, the optimization objective can be simplified to the following representation,
% \begin{equation}
% \min -I(Y; H_M^L) + \eta  I(\mathcal{D}; H_M^L),
%     \label{IB}
% \end{equation}
% According to \cite{MAGI2024}, MAGI establishes a lower bound for the term $I(Y; H^L)$ and an upper bound for $I(\mathcal{D}; H_U^L)$, as shown in Eq. \eqref{upper_IB} and Eq. \eqref{lower_IB}, respectively. For any distributions $\mathbb{V}_1(Y_i \mid H_{U,i}^L)$ for agent $i \in \mathcal{N}$  and $\mathbb{V}_2(Y)$, the lower bound for $I(Y; H_U^L)$ is given in Eq. \eqref{upper_IB}.
% \begin{align}
% I(Y; H_U^L) &\geq 1 + \mathbb{E} \left[ \log \frac{\prod_{i \in \mathcal{N}} \mathbb{V}_1(Y_i \mid H_{U,i}^L)}{\mathbb{V}_2(Y)} \right] \notag \\
% &\hspace{-2em} + \mathbb{E}_{\mathbb{P}(Y)\mathbb{P}(H_U^L)} \left[ \frac{\prod_{i \in \mathcal{N}} \mathbb{V}_1(Y_i \mid H_{U,i}^L)}{\mathbb{V}_2(Y)} \right]. 
%     \label{upper_IB}
% \end{align}
% MAGI selects two index sets $S_U, S_A \subset [L]$ such that 
% $\mathcal{D} \perp H_U^L \mid \left( H_U^l \right)_{l \in S_U} \cup \left( H_A^l \right)_{l \in S_A}$, depending on the Markovian dependence, where $H_A^l$ represents the graph structure and $M_1 \perp M_2 \mid M_3$ denotes that $M_1$ and $M_2$ are conditionally independent given $M_3$. For any probability distributions $\mathbb{V}(M_U^l)$, $l \in S_U$ and $\mathbb{V}(H_A^l)$, $l \in S_A$,
% \begin{align}
% I(\mathcal{D}; H_U^L) &\leq I\left(\mathcal{D}; (H_U^l)_{l \in S_U} \cup (H_A^l)_{l \in S_A}\right) \notag \\
% &\leq \sum_{l \in S_A} \text{AIB}^l + \sum_{l \in S_U} \text{UIB}^l.
%     \label{lower_IB}
% \end{align}
% \begin{equation}
% \text{AIB}^l = \mathbb{E} \left[ \log \frac{\mathbb{P}(H_A^l \mid A, H_U^{l-1})}{\mathbb{V}(H_A^l)} \right],
%     \label{AIB}
% \end{equation}
% \begin{equation}
% \text{UIB}^l = \mathbb{E} \left[ \log \frac{\mathbb{P}(H_U^l \mid H_U^{l-1}, H_A^l)}{\mathbb{V}(H_U^l)} \right].
%     \label{UIB}
% \end{equation}
% According to \cite{MAGI2024}, the MAGI algorithm, which separately optimizes the communication graph and information flow, is relatively complex and cumbersome. Therefore, we adopt a more efficient joint optimization approach, where the refinement of the second term in Eq. \eqref{IB} is shown as follows,
% \begin{equation}
% \begin{aligned}
% I(\textbf{h}; \textbf{E}_{m_i}) &=\int p(\textbf{h},\textbf{E}_{m_i})\log \frac{p(\textbf{h},\textbf{E}_{m_i})}{p(\textbf{h})p(\textbf{E}_{m_i})} \, d\textbf{h} \, d\textbf{E}_{m_i}      \\
% &= \int p(\textbf{E}_{m_i}) p(\textbf{h} \mid \textbf{E}_{m_i}) \log \frac{p(\textbf{h} \mid \textbf{E}_{m_i})}{p(\textbf{h})} \, d\textbf{h} \, d\textbf{E}_{m_i} \\
% &\leq \int p(\textbf{E}_{m_i}) p(\textbf{h} \mid \textbf{E}_{m_i}) \log \frac{p(\textbf{h} \mid \textbf{E}_{m_i})}{q_{\textbf{h}}(\textbf{h})} \, d\textbf{h} \, d\textbf{E}_{m_i} \\
% &= \mathbb{E}_{p(\textbf{E}_{m_i})} \left[ D_{\text{KL}} \left( p(\textbf{h} \mid \textbf{E}_{m_i}) \parallel q_{\textbf{h}}(\textbf{h}) \right) \right].
% \end{aligned}
% \end{equation}
%Hence, the following inequality can be derived,
% \begin{equation}
% \begin{aligned}
% I(\textbf{h}; \textbf{E}_{m_i}) &\leq \mathbb{E}_{p(\textbf{E}_{m_i})} \left[ D_{\text{KL}} \left( p(\textbf{h} \mid \textbf{E}_{m_i}) \parallel q_{\textbf{h}}(\textbf{h}) \right) \right],
% \end{aligned}
%     \label{upper}
% \end{equation}
% where $\textbf{h}$ represents the specific form of $H_M^l$ in the framework as well as $\textbf{E}_{m_i}$, which is the instantiation of $\mathcal{D}$ and $q_{\textbf{h}}(\textbf{h})$ is the variational approximation to $p(\textbf{h})$. To further estimate $\textbf{h}$, we model $q_{\textbf{h}}(\textbf{h})$ as a mixture of Gaussian distributions. Specifically, for any agent $i$ where $H_M \sim \mathbb{V}(H_M)$, we define $H_{M} \sim \sum_{u=1}^N w_u \, \mathcal{N}(\mu_{u}, \sigma_{u}^2)$, where $w_u$, $\mu_{u}$, and $\sigma_{u}$ are learnable parameters shared across all agents.
% To characterize Eq. \eqref{IB}, We can directly set $\mathbb{V}_2(Y) = \mathbb{P}(Y)$ and $\mathbb{V}_1(Y_i | H_{U,i}^L) = \text{Softmax}(H_{U.i}^LW_{\text{out}}) $. Notably, we have addressed the limitation of discrete actions in MAGI. Thus, Eq. \eqref{upper_IB} simplifies to the Kullback–Leibler (KL) divergence loss function as shown below,
% \begin{equation}
% I(Y; H_U^L) \rightarrow - \mathbb{E}_{i \in \mathcal{N}}[D_{\text{KL}} \left( H_{U,i}^LW_{\text{out}}\parallel Y_i \right)].
%     \label{lower}
% \end{equation}
%%%%%%%%%另一种含最大化互信息推导的写法%%%%%%%%%%
% To characterize Eq. \eqref{IB}, the derived formula is shown as follows,
% \begin{align}
% I(\textbf{h}; \textbf{o}_g) &= \int p(\textbf{h}, \textbf{o}_g) \log \frac{p(\textbf{h} , \textbf{o}_g)}{p(\textbf{h})p(\textbf{o}_g)} \, d\textbf{h}\, d\textbf{o}_g \notag \\
% &= \int p(\textbf{h}, \textbf{o}_g) \log \frac{p(\textbf{o}_g | \textbf{h})}{p(\textbf{o}_g)} \, d\textbf{h} \, d\textbf{o}_g \notag \\
% &= \int p(\textbf{h}, \textbf{o}_g) \log p(\textbf{o}_g | \textbf{h}) \, d\textbf{h} \, d\textbf{o}_g + H(\textbf{o}_g) \notag \\
% &\geq \int p(\textbf{h}, \textbf{o}_g) \log q(\textbf{o}_g | \textbf{h}) \, d\textbf{h} \, d\textbf{o}_g \notag \\
% &= \mathbb{E} \left[ \log q(\textbf{o}_g | \textbf{h}) \right],
% \label{lower_IB}
% \end{align}
% where $\textbf{o}_{g}$ represents the specific form of $Y$ in the framework.
% Based on the first term in Eq. \eqref{IB}, our goal is to maximize $I(Y; H_M^L)$, which is equivalent to maximizing the log-likelihood expression $\mathbb{E} \left[ \log q(\textbf{o}_g | \textbf{h}) \right]$. According to Lemma 1, the maximization can be transformed into minimizing the KL divergence, ultimately achieving the same objective in a different form. Thus, Eq. \eqref{lower_IB} simplifies to the KL divergence loss function as shown below,
% \begin{equation}
% I(Y; H_M^L) = - \mathbb{E}_{i \in \mathcal{N}}[D_{\text{KL}} \left( \textbf{h}\parallel o_g \right)].
% \end{equation}
% Corollary \ref{theorem:appro} implies that during training, we can approximate the KL divergence by minimizing MSE in sample batches. Furthermore, our training experience indicates that computing the MSE in a softmax kernel yields superior performance \cite{xiang2023decentralized}. In summary, the training procedure for CDE-IB is presented in Algorithm \ref{algorithm}.

% \begin{theorem}
%     Maximum Likelihood Estimation (MLE) seeks to find the model parameters that best approximate the true data distribution by maximizing the logarithm of the probability of the observed data under the model distribution.
% \end{theorem}
% \begin{proof}
%     \begin{align}
% \theta ^{*} &= \arg \max_{\theta }{\log{\prod_{i=1}^{m}}P_\theta (x^i) } \notag\\
% &= \arg \max_{\theta }\sum_{i=1}^{m} {\log P_\theta (x^i) } \notag \\
% &\approx \arg \max_{\theta }\mathbb{E}_{x\sim P_{data}}[ {\log P_\theta (x) } ] \notag\\
% &=\arg \max_{\theta }\int\limits_{x} P_{data}(x){\log P_\theta (x) }dx \notag\\
% &-\int\limits_{x}P_{data}(x){\log P_{data} (x) }dx \notag\\
% &= \arg \max_{\theta }\int\limits_{x} P_{data}(x){\frac{\log P_\theta (x)}{\log P_{data} (x)}  }dx \notag\\
% &=\arg \min_{\theta }KL(P_{data}||P_\theta).
% \end{align}
% \end{proof}
% Therefore, maximizing the Maximum Likelihood Estimate is equivalent to minimizing the KL divergence.

%\textbf{Lemma 1}: Maximum Likelihood Estimation (MLE) seeks to find the model parameters that best approximate the true data distribution by maximizing the logarithm of the probability of the observed data under the model distribution, and the corresponding formula is shown below,


%%%%%%%%%另一种含最大化互信息推导的写法%%%%%%%%%%

\newcommand{\INITIALIZE}{\item[\textbf{Initialize:}]}
\addtolength{\topmargin}{0.05in}
\begin{algorithm}[tbp]\small
  \caption{The Training of CDE-GIB}
  \label{algorithm}
  \begin{algorithmic}[1]
    \INITIALIZE {The length of episodes $T$, variance constant $\sigma$, the actor and critic network with random parameters $\Theta, \Psi, \phi$ and the replay memory $\mathcal{B} \leftarrow \varnothing$;}
    \FOR {each train epoch}
        \STATE $\text{Clone}$ $\Theta_{\text{old}} \leftarrow \Theta$, $\Psi_{\text{old}} \leftarrow \Psi$, $\phi_{\text{old}} \leftarrow \phi$;
        \STATE Initialize the environment with $N$ agents;
        \FOR {$t=\{1, \cdots, T\}$}
            \FOR {\textbf{each agent $i$}}
            \STATE $\textbf{z}_i^{(t)} \leftarrow $ obtains a local state $\textbf{z}_i^{(t)} =\left(\textbf{o}_i^{(t)}, \left\{\textbf{m}_j^{(t)} \mid j \in \xi_i^{(t)}\right\}\right)$;
            \STATE $\mathcal{A}_i,\textbf{E}_{m_{i}}^{(t)} \leftarrow $ calculates triggering behavior $\mathcal{A}_i$ by Eq. \eqref{VT-ETM} and generates encoded information $\mathbf{E}_{m_{i}}^{(t)}$ by Eq. \eqref{Emi};
            \STATE $\textbf{h}^{(t)}, \textbf{m}_i^{(t+1)}, \textbf{u}_i^{(t)} \leftarrow $ establishes the consensus $\textbf{h}^{(t)}$ by Eq. \eqref{H} and computes the message $\textbf{m}_i^{(t+1)}$ by Eq. \eqref{mt+1} to sample an action $\textbf{u}_i^{(t)} \sim \text{Normal}(\mu^{(t)}_{\text{i, old}}, \sigma^2)$ by Eq. \eqref{action};
            % \STATE $\textbf{s}_i^{(t+1)} \leftarrow $ executes the action $\textbf{u}_i^{(t)}$, and observes the next state $\textbf{s}_i^{(t+1)}$;
            \ENDFOR
        \STATE $r^{(t)} , V_{\phi_{old}}(\textbf{s}^{(t)}) ,\textbf{s}^{(t+1)}\leftarrow $ obtain the reward $r^{(t)}$, state value $V_{\phi_{old}}(\textbf{s}^{(t)})$ and $\textbf{s}^{(t+1)}$;
        \ENDFOR
        \STATE For each time-step $t$, each agent calculates $\mu^{(t)}_i$, $\hat{\textbf{e}}^{(t)}$ based on $\textbf{z}^{(t)}_i$ by Eq. \eqref{Emi}-\eqref{IB}, and obtains $V_{\phi}(\textbf{s}^{(t)})$;
        \STATE Update $\Theta, \Psi, \phi$ according to Eq. \eqref{loss} via Adam optimizer;
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
\section{Simulation Settings and Results}\label{sec4_Experiment}
\subsection{Simulation Settings}
In this section, we evaluate the performance of CDE-GIB in terms of executing the decentralized formation control task \cite{xiang2023decentralized} in the multi-agent particle environment \cite{MPE2017}. Notably, we use cosine similarity during computing $\kappa$ in Eq. \eqref{eq:VT-ETM_loss}. 
Additionally, the reward function $\mathcal{R}^{(t)}$ is defined as a weighted combination of task-oriented reward $\mathcal{R}_t^{(t)}$ and the event-triggered reward $\mathcal{R}_m^{(t)}$. Specifically, consistent with \cite{xiang2023decentralized}, the task-oriented reward $\mathcal{R}_t^{(t)}$, which contains the formation completeness, individual navigation distance, and penalty on collision arising from decentralized control, evaluates the agents' efficacy in executing specified tasks. Meanwhile, the event-triggered reward $\mathcal{R}_m^{(t)}$ imposes a penalty on agents for transmitting information. 
Mathematically, 
\begin{equation}
    \mathcal{R}^{(t)} = \omega_k \mathcal{R}_t^{(t)} + \omega_m \mathcal{R}_m^{(t)},
\end{equation}
where the coefficients $ \omega_t, \omega_m $ represent the corresponding weights. We also evaluate the distributed consensus establishment method ConsMAC\cite{xiang2023decentralized}, the attention-based message aggregation approach TarMAC \cite{Tarmac2019}, and the state-of-the-art supervised learning-based information extraction algorithm MASIA\cite{masia2022} as baselines. The key parameters are summarized in Table \ref{para}.

%In this section, we evaluate the performance of CDE-IB in the multi-agent particle environment \cite{MPE2017} for the formation control task, focusing on verifying its effectiveness, universality, and ability to reduce communication overheads, while also assessing the contribution of each individual component. Additionally, we use cosine similarity to evaluate the level of semantic similarity between the merged information calculated by the function $\mathcal{F}_{\psi_T}$ and the consensus $\hat{\textbf{e}}^{(t-1)}$ in the experiments. The key parameters are summarized in Table \ref{para}.
\begin{table}[t!]
    \centering
    \caption{The Key Parameter Settings of The Environment.}
    \vspace{-.5em}
    \label{para}
    \begin{tabular}{@{}c|c|c@{}}
        \toprule
        \textbf{Environment Parameters} & \textbf{Symbol}& \textbf{Value} \\ 
        \midrule
        Number of UAVs & $N$ & $7$ \\
        Maximum observation distance & $\delta _{obs}$ & $3$ m \\
        Destination & $\Delta_p$ & $(0, 10)$ m\\
        Discount factor & $\gamma $ & $0.8$ \\
        GAE factor & $\lambda $ & $0.95$\\
        The range of acceleration & $\textbf{u}_i^{(t)}$ & $[-0.5, 0.5] $ m/s\textsuperscript{2}\\
        The range of position & $\textbf{p}_i^{(0)},\textbf{p}_j^{(0)}$ & $[-2, 2]$ m\\
        Reward function coefficients & $\omega_k,\omega_m$ &  $1$, $0.1$ \\
        % Reward function coefficients & \makecell{$(\omega_1,\omega_2,\omega_f,$\\$\omega_r,\omega_c,\omega_k,\omega_m)$} & \makecell{$(0.9, 0.7, 10,$\\$ 5, 6, 1, 0.1)$} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5em}
\end{table}
\subsection{Simulation Results}
%\subsection{Effectiveness of IB Experiments}
%To demonstrate the efficiency of the IB metric, we compare our approach with three high-performing communication algorithms (i.e., the distributed consensus establishment method ConsMAC\cite{xiang2023decentralized}, the attention-based message aggregation approach TarMAC \cite{Tarmac2019}, and the state-of-the-art supervised learning-based information extraction algorithm MASIA\cite{masia2022}). As shown in Fig. \ref{experiment2}, it is evident that our proposed IB mechanism significantly compresses the consensus information, effectively removing redundant communication messages and leading to notable performance improvements over the baseline algorithms.
\begin{figure}[tp]
    \centering
    \includegraphics[scale=0.45]{experiment2.pdf}
    \vspace{-0.2cm}
\caption{Learning curves of consensus algorithms with and without GIB optimization.} 
\vspace{-1em}
\label{experiment2}
\end{figure} 
%\subsection{Ablation Experiments}
%The CDE-IB framework contains three critical components: the Consensus-oriented Multi-Agent Communication (ConsMAC) component, the Variable Threshold Event Triggering Mechanism (VT-ETM) component and the Information Bottleneck (IB) theory optimization component. To evaluate their individual contributions, we design two variants of CDE-IB: 1) CDE-IB w/o IB is ConsMAC with the VT-ETM component, and 2) CDE-IB w/o ETM is ConsMAC with the IB component.
We first conduct ablation studies to show the contribution of individual modules. As shown in Table \ref{communication_volume_m} and \ref{communication_volume_h}, the comparison between ConsMAC with and without ETM indicates the removal of the VT-ETM component causes a slight decline in information processing performance and a significant increase in redundant communication volume $\tilde{\textbf{m}}_{i}^{(t)}$. Moreover, removing the GIB module also results in a compression performance degradation and a greater accumulation of unnecessary inferred consensus $\textbf{h}^{(t)}$. Afterward, we compare ConsMAC with GIB on top of ConsMAC with other baselines  \cite{xiang2023decentralized,Tarmac2019,masia2022}. Fig. \ref{experiment2} presents the corresponding result. It can be observed that due to the incorporation of GIB, it significantly outperforms other baselines, including the state-of-the-art ConsMAC algorithm.
\begin{table}[t!]
    \centering
    \caption{Comparison of the communication volume with and without VT-ETM, under different maximum observation ranges $\delta _\text{com}$.}
    \vspace{-.5em}
    \begin{tabular}{ccccccccc}
        \toprule
        $\delta _\text{com}$ (m) &$\mathbf{2.1}$ & $\mathbf{2.4}$ & $\mathbf{2.7}$ & $\mathbf{2.8}$ \\ 
        \midrule
        w. ETM  & $\mathbf{846.32}$ & $\mathbf{880.47}$ & $\mathbf{994.17}$ & $\mathbf{931.34}$ \\
        w.o. ETM & $874.95$ & $950.84$ & $1086.77$ & $955.89$ \\
        \bottomrule
    \end{tabular}
    \label{communication_volume_m}
    \vspace{-2em}
\end{table}

\begin{table}[t!]
    \centering
    \caption{Comparison of inferred consensus with and without GIB, under different maximum observation ranges $\delta _\text{com}$.}
    \vspace{-.5em}
    \begin{tabular}{ccccccccc}
        \toprule
        $\delta _\text{com}$ (m) &$\mathbf{2.1}$ & $\mathbf{2.4}$ & $\mathbf{2.7}$ & $\mathbf{2.8}$ \\ 
        \midrule
        w. GIB  & $\mathbf{208.35}$ & $\mathbf{199.77}$ & $\mathbf{216.48}$ & $\mathbf{202.54}$ \\
        w.o. GIB & $450.38$ & $464.04$ & $506.77$ & $511.13$ \\
        \bottomrule
    \end{tabular}
    \label{communication_volume_h}
    \vspace{-1em}
\end{table}

%%%%%%  图和表格两者选其一  %%%%%%%%
% \begin{figure}[tbp]
% \subfigcapskip = -1pt
% \begin{center}
%     \subfigure[The event-triggered mechanism]
%     {\includegraphics[scale =0.4]{Latex/h1.pdf}}
%     \subfigure[The Information Bottleneck theory]
%     {\includegraphics[scale =0.4]{Latex/h.pdf}}
% \vspace{-0.3em}
% \caption{Communication Volume comparison for the event-triggered mechanism and Information Bottleneck theory.}  
% \vspace{-0.6cm}
% \label{bar}
% \end{center}
% \end{figure} 

%\subsection{Generality of VT-ETM Experiments}

Finally, we focus on the universal applicability of VT-ETM plugin in other baselines, and provide the related results in Fig. \ref{experiment3}. A clear trend emerges that the adoption of VT-ETM leads to notable performance improvement for all methods. In particular, 
%under the task-oriented reward $\mathcal{R}_k^{(t)}$, which will be described in detail below. 
% For simplicity of representation, we denote the center point of the agents at each time-step $t$ as $\bar{\textbf{p}}^{(t)}$ and their relative positions as,
% \begin{equation}
% \textbf{P}^{(t)} = (\textbf{p}_1^{(t)} - \bar{\textbf{p}}^{(t)}, \dots, \textbf{p}_n^{(t)} - \bar{\textbf{p}}^{(t)}).
% \end{equation}
% % Additionally, the reward function $\mathcal{R}$ we defined is as follows,
% % % formulated as follows,
% % \begin{equation}
% % \vspace{-0.3em}
% % \mathcal{R}^{(t)} = \omega_k \mathcal{R}_k^{(t)} + \omega_m \mathcal{R}_m^{(t)},
% % \end{equation}
% % where the task-oriented reward $\mathcal{R}_k^{(t)}$ evaluates the agents' efficacy in executing specified tasks. To optimize the utilization of resources efficiently, the event-triggered reward $\mathcal{R}_m^{(t)}$ imposes a penalty on agents for transmitting information when triggered by specific events. Additionally, the coefficients $ \omega_k, \omega_m $ represent the corresponding weights.
% % \begin{equation}
% % \mathcal{R}_k^{(t)} = \omega_f \mathcal{R}_f^{(t)} + \omega_v \mathcal{R}_v^{(t)} - \omega_c \mathcal{R}_c^{(t)},
% % \end{equation}
% % where $\omega_f$, $\omega_v$, and $\omega_c$ represent the weights corresponding to each component.
% % \subsubsection{Formation reward}\
% % \addtolength{\topmargin}{0.05in}
% % In order to compute the maximum individual movement distance required for agents to achieve an ideal topology, the Hausdorff Distance (HD) is utilized, resulting in a formation reward $\mathcal{R}_f^{(t)}$, which is defined as,
% % \begin{equation}
% % \mathcal{R}_f^{(t)} = -d_{\text{HD}}(\textbf{P}^{(t)}, \Delta_f) - \omega_1 \mathcal{R}_f^{(t-1)},
% % \end{equation}
% % where the HD between two formations $\Upsilon _1$ and $\Upsilon _2$ is given by,
% % \begin{equation}
% % d_{\text{HD}}(\Upsilon_1, \Upsilon_2) = \max\{h(\Upsilon_1, \Upsilon_2), h(\Upsilon_2, \Upsilon_1)\},
% % \end{equation}
% % with
% % \begin{equation}
% % h(\Upsilon_1, \Upsilon_2) = \max_{x \in \Upsilon_1} \min_{y \in \Upsilon_2} d(x, y).
% % \end{equation}
% % Additionally, the initial reward is set as $r_f^{(0)} = 0$, $\Delta_f$ denotes the formation and $\omega_1$ represents the formation lag coefficient.
% \subsubsection{Navigation reward}\
% \addtolength{\topmargin}{0.05in}
% To guide the agents towards the target point, the navigation reward is defined based on the Euclidean distance between the agents' center point and the destination, which is expressed as,
% \begin{equation}
% \mathcal{R}_v^{(t)} = -d(\bar{\textbf{p}}^{(t)}, \Delta p) - \omega_2 \mathcal{R}_v^{(t-1)},
% \end{equation}
% where $\mathcal{R}_v^{(0)} = 0$, and $\omega_2$ denotes the navigation lag coefficient.
% \subsubsection{Collision penalty}\
% \addtolength{\topmargin}{0.05in}
% The collision penalty is defined as the total number of collisions at time-step $t$, and can be formulated as,
% \begin{equation}
% \mathcal{R}_c^{(t)} = \sum_{i \neq j}\left(d(\textbf{p}_i^{(t)}, \textbf{p}_j^{(t)}) < \delta_\text{safe}\right),
% \end{equation}
% where $\delta_\text{safe}$ represents the minimum safety distance between agents.
%Using a seven-agent formation as an example, we compare three representative MARL communication methods discussed previously. As illustrated in Fig. \ref{experiment3}, the performance of MASIA and TarMAC without the ETM plugin is relatively similar, underscoring the importance of the plugin in reducing redundant communication message. Notably, TarMAC+ETM achieves a more significant performance improvement compared to MASIA+ETM, demonstrating the versatility of the ETM plugin and its superior compatibility with the TarMAC algorithm. Furthermore, 
integrating the VT-ETM mechanism into the ConsMAC algorithm yields an even more remarkable performance enhancement, further reinforcing the effectiveness and reliability of the ETM plugin across diverse algorithms.

\begin{figure}[tp]
    \vspace{-0cm}
    \centering
    \includegraphics[scale=0.45]{experiment3.pdf}
    \vspace{-0.2cm}
\caption{Performance Comparison of consensus algorithms with and without ETM.} 
\label{fig:R} 
\vspace{-1.6em}
\label{experiment3}
\end{figure} 

\section{Conclusions}\label{sec5_Conclusions}
In this work, we have proposed and validated CDE-GIB, a robust, event-triggered integrated communication and control framework with GIB optimization. To be specific, 
% Using the Consensus-oriented Multi-Agent Communication (ConsMAC) methodology, we developed the VT-ETM module, enabling agents to infer global information from their local states, thereby implicitly achieving consensus while minimizing message transmissions among agents. Additionally, we extended Information Bottleneck theory to continuous spaces, facilitating the compression of consensus content into an efficient and compact representation. 
we have implemented a GIB module that jointly optimizes the communication graph and data flow in ConsMAC methodology, which effectively compresses the consensus into a sufficient and compact representation. Additionally, a VT-ETM algorithm has been employed to assess the information importance based on the fusion of historical data and current observations, while an opportunistic transmission mechanism has been leveraged to reduce the dissemination of redundant communication messages during the interactive process of reaching consensus. We have conducted extensive experiments to demonstrate the effectiveness and adaptability of our proposed method in communication-limited environments. In future work, we will further explore larger-scale formations under stricter communication constraints and deploy the approach on a more practical hardware platform.
\bibliographystyle{IEEEtran}
\bibliography{arxiv}
\end{document}