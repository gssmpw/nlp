\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{sections/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{wrapfig}
\makeatletter
\newcommand*{\resetMathstrut}{%
  \setbox\z@\hbox{(}%
  \ht\Mathstrutbox@\ht\z@
  \dp\Mathstrutbox@\dp\z@
}
\makeatother
\let\mathdefault\relax

\newcommand{\wenlong}[1]{{\color{blue} #1}}

\title{Recurrent Memory for Online Interdomain Gaussian Processes}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Wenlong Chen\thanks{Equal Contribution}\\
Imperial College London\\
\texttt{wenlong.chen21@imperial.ac.uk} \\
\And
Naoki Kiyohara$^*$\\
Imperial College London, Canon Inc.\\
\texttt{n.kiyohara23@imperial.ac.uk} \\
\And
Harrison Bo Hua Zhu$^*$\\
University of Copenhagen, Imperial College London\\
\texttt{harrison.zhu@sund.ku.dk} \\
\And
Yingzhen Li\\
Imperial College London\\
\texttt{yingzhen.li@imperial.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bK}{\mathbf{K}}
% \newcommand{\bm}{\mathbf{m}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bbX}{\mathbf{X}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
We propose a novel online Gaussian process (GP) model that is capable of capturing long-term memory in sequential data in an online regression setting. Our model, Online HiPPO Sparse Variational Gaussian Process Regression (OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators) framework, which is popularized in the RNN domain due to its long-range memory modeling capabilities. We interpret the HiPPO time-varying orthogonal projections as inducing variables with time-dependent orthogonal polynomial basis functions, which allows the SGPR inducing points to memorize the process history. We show that the HiPPO framework fits naturally into the interdomain GP framework and demonstrate that the kernel matrices can also be updated online in a recurrence form based on the ODE evolution of HiPPO. We evaluate our method on time series regression tasks, showing that it outperforms the existing online GP method in terms of predictive performance and computational efficiency.
\end{abstract}

\section{Introduction}
Gaussian processes (GPs) are a popular choice for modeling time series due to their functional expressiveness and uncertainty quantification abilities \citep{roberts_gaussian_2013, fortuin_gpvae_2020}. However, GPs are computationally expensive and memory intensive, with cubic and quadratic complexities, respectively. In online regression settings, such as weather modeling, the number of time steps can be very large, quickly making GPs infeasible. Although variational approximations, such as utilizing sparse inducing points (SGPR \citep{titsias_variational_2009}; SVGP \citep{hensman_gaussian_2013,hensman_scalable_2015}) and Markovian GPs \citep{wilkinson_sparse_2021}, have been proposed to address the computational complexity, it would still be prohibitive to re-fit the GP model from scratch every time new data arrives.

\citet{bui_streaming_2017} proposes an online sparse GP (OSGPR) learning method that sequentially updates the GP posterior distribution only based on the newly arrived data. However, as indicated in their paper, their models may not maintain the memory of the previous data, as the inducing points will inevitably shift as new data arrive. This is a major drawback, as their models may not model long-term memory unless using a growing number of inducing points. 

In deep learning, as an alternative to Transformers \citep{vaswani2017attention}, significant works on state space models (SSMs) have been proposed to model long-term memory in sequential data. Originally proposed to instill long-term memory in recurrent neural networks, the HiPPO (High-order Polynomial Projection Operators) framework \citep{gu_hippo_2020} provides mathematical foundations for compressing continuous-time signals into memory states through orthogonal polynomial projections. HiPPO is the basis for the state-of-the-art SSMs, such as the structured state space sequential (S4) model \citep{gu_s4_2022} and Mamba \citep{gu_mamba_2023,dao_mamba2_2024}. HiPPO is computationally efficient and exhibits strong performance in long-range memory tasks.


Inspired by HiPPO, we propose Online HiPPO SGPR (OHSGPR), by applying the HiPPO framework to SGPR in order to leverage the long-range memory modeling capabilities. Our method interprets the HiPPO time-varying orthogonal projections as inducing variables of an interdomain SGPR \citep{lazaro-gredilla_inter-domain_2009,leibfried_tutorial_2020,van_der_wilk_framework_2020}, where the basis functions are time-dependent orthogonal polynomials. We show that we are able to significantly resolve the memory-loss issue in OSGPR, thereby opening up the possibility of applying GPs to long-term time series regression tasks. In more detail, we make the following contributions:
\begin{itemize}
  \item We demonstrate that HiPPO integrates into the interdomain GPs by interpreting the HiPPO time-varying orthogonal projections as inducing variables with time-dependent orthogonal polynomial basis functions. This allows the inducing variables to memorize the process history, capturing long-term memory in the data.
  \item We show that the kernel matrices can leverage the efficient ODE evolution of the HiPPO framework, bringing an extra layer of computational efficiency to OHSGPR.
  \item We demonstrate OHSGPR on time series regression tasks, showing that it outperforms vanilla OSGPR in terms of predictive performance and computational efficiency.
\end{itemize}


\section{Background}
In this section, we provide a brief overview of GPs, inducing point methods, and online learning with GPs. In addition, we review the HiPPO method, which is the basis of our proposed method. 
\subsection{Gaussian processes}
Let $\mathcal{X}$ be the input space. For time series data, $\mathcal{X} = [0,\infty)$, the set of non-negative real numbers. A Gaussian process (GP) $f\sim\mathcal{GP}(0,k)$ is defined with a covariance function $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$. It has the property that for any finite set of input points $\bbX = [x_1,\ldots,x_n]^\intercal$, the random vector $\bbf\equiv f(\bX)=[f(x_1),\ldots,f(x_n)]^\intercal\sim \mathcal{N}(0, \bK_{\bbf\bbf})$, where $\bK_{\bbf\bbf}$ is the kernel matrix with entries $[k(\bX, \bX)]_{ij}\equiv[\bK_{\bbf\bbf}]_{ij} = k(x_i,x_j)$. For notational convenience and different sets of input points $\bX_1$ and $\bX_2$, we denote the kernel matrix as $\bK_{\bbf_1\bbf_2}$ or $k(\bX_1, \bX_2)$. The computational and memory complexities of obtaining the GP posterior on $\bX_1$ scale cubically and quadratically respectively, according to $n_1=|\bbX_1|$. Given responses $\by$ and inputs $\bX$, a probabilistic model can be defined as $y_i \sim p(y_i \mid f(x_i))$ with a GP prior $f \sim \mathcal{GP}(0,k)$, where $p(y_i \mid f(x_i))$ is the likelihood distribution. However, for non-conjugate likelihoods, the posterior distribution is intractable, and approximate inference methods are required, such as, but not limited to, variational inference \citep{titsias_variational_2009,hensman_gaussian_2013,hensman_scalable_2015} and Markov chain Monte Carlo (MCMC) \citep{hensman_mcmc_2015}.

\subsection{Variational inference and interdomain Gaussian processes}\label{sec:vi_and_interdomain_GP}
To address the intractability and cubic complexity of GPs, Sparse Variational Gaussian Processes (SGPR \citep{titsias_variational_2009}; SVGP \citep{hensman_gaussian_2013,hensman_mcmc_2015}) cast the problem as an optimization problem. By introducing $M$ inducing points $\bZ\in\mathcal{X}^M$ that correspond to $M$ inducing variables $\bu = [f(\bz_1), \ldots, f(\bz_M)]^\intercal$, the variational distribution $q(\bbf, \bu)$ is defined as $q(\bbf, \bu) := p(\bbf \mid \bu)q_\theta(\bu)$, where $q_\theta(\bu)$ is the variational distribution of the inducing variables with parameters $\theta$. Then, the evidence lower bound (ELBO) is defined as
\begin{align}
    \log p(\by) \geq  \sum_{i=1}^n \mathbb{E}_{q(f_i)}[\log p(y_i \mid f_i)] - \text{KL}\left[q_\theta(\bu) \middle\| p(\bu)\right]=: \mathcal{L}_\theta,
\end{align}
where $q(f_i) = \int p(f_i \mid \bu)q_\theta(\bu) \mathrm{d}\bu$ is the posterior distribution of $f_i\equiv f(x_i)$. Typical choices for the variational distribution are $q_\theta(\bu) = \mathcal{N}(\bu; \mathbf{m}_{\bu}, \bS_{\bu})$, where $\mathbf{m}_{\bu}$ and $\bS_{\bu}$ are the free-form mean and covariance of the inducing variables, and yields the posterior distribution:
\begin{align}
    q(f_i) = \mathcal{N}(f_i; \bK_{f_i\bu}\bK_{\bu\bu}^{-1}\mathbf{m}_{\bu}, \bK_{f_i f_i} - \bK_{f_i\bu}\bK_{\bu\bu}^{-1}[\bK_{\bu\bu} - \bS_{\bu}]\bK_{\bu\bu}^{-1}\bK_{\bu f_i}).
\end{align}
When the likelihood is conjugate Gaussian, the ELBO can be optimized in closed form and $\mathbf{m}_\bu$ and $\bS_\bu$ can be obtained in closed form (SGPR; \citet{titsias_variational_2009}). In addition to setting the inducing variables as the function values, interdomain GPs \citep{lazaro-gredilla_inter-domain_2009} propose to generalize the inducing variables to $u_m := \int f(x)\phi_m(x) \mathrm{d}x$, where $\phi_m(x)$ are basis functions, to allow for further flexibility. This yields $[\bK_{f \bu}]_m = \int k(x, x^\prime)\phi_m(x^\prime)\mathrm{d}x^\prime$ and $[\bK_{\bu\bu}]_{nm} = \iint k(x, x^\prime)\phi_n(x)\phi_m(x^\prime)\mathrm{d}x\mathrm{d}x^\prime$. 

We see that the interdomain SGPR bypasses the selection of the inducing points $\bZ\in\mathbb{R}^{M}$, and reformulates it with the selection of the basis functions $\phi_i$. The basis functions dictate the structure of the kernel matrices, which in turn modulate the function space of the GP approximation. In contrast, SGPR relies on the inducing points $\bZ$, which can shift locations according to the training data. Some examples of basis functions include Fourier basis functions \citep{hensman2018variational} and the Dirac delta function $\delta_{\bz_m}$, the latter recovering the standard SGPR inducing variables.


\subsection{Online Gaussian processes}\label{sec:online_gp}
In this paper, we focus on online learning with GPs, where data arrives sequentially in batches $(\bX_{t_1},\by_{t_1}), (\bX_{t_2},\by_{t_2}),\ldots$ etc. In the time series regression setting, the data arrives in intervals of $(0, t_1), (t_1, t_2), \ldots$ etc. The online GP learning problem is to sequentially update the GP posterior distribution as data arrives. Suppose that we have already obtained $p_{t_1}(y|f)p_{t_1}(f|\bu_{t_1})q_{t_1}(\bu_{t_1})$ of the likelihood and variational approximation (with inducing points $\bZ_{t_1}$), from the first data batch $(\bX_{t_1},\by_{t_1})$. Online SGPR (OSGPR; \citet{bui_streaming_2017}) utilizes the online learning ELBO
\begin{equation}
\begin{aligned}
    &\sum_{i=1}^{n_{t_2}} \mathop{\mathbb{E}}_{q_{t_2}(f_i)}
    \left[ \log p_{t_2}(y_i \mid f_i) \right] 
    + \text{KL} \left( \tilde{q}_{t_2}(\bu_{t_1}) \,\middle\|\, p_1(\bu_{t_1}) \right) \\
    &\quad - \text{KL} \left( \tilde{q}_{t_2}(\bu_{t_2}) \,\middle\|\, q_{t_1}(\bu_{t_1}) \right) 
    - \text{KL} \left( q_{t_2}(\bu_{t_2}) \,\middle\|\, p_{t_2}(\bu_{t_2}) \right),
\end{aligned}
\end{equation}
where $y_i\in\by_{t_2}$ for $i=1,\ldots,n_{t_2}$ and $\tilde{q}_{t_2}(\bu_{t_1}) := \int p_{t_2}(\bu_{t_1}|\bu_{t_2})q_{t_2}(\bu_{t_2}) \mathrm{d}\bu_{t_2}$. Note that subscripts $()_{t_1}$ are utilizing hyperparameters from the first batch, and $()_{t_2}$ are hyperparameters from the second batch that are to be optimized with the ELBO. Theoretically, \citet{bui_streaming_2017} show that the OSGPR method achieves the same posterior distribution as the batch SGPR method when the inducing points are fixed. Unfortunately, for online time series regression, OSGPR may not capture the long-term memory in the time series data since as new data arrives, it is not guaranteed that the inducing points after optimization can sufficiently cover all the previous tasks' data domains.

\subsection{HiPPO: recurrent memory with optimal polynomial projections}
The HiPPO framework \citep{gu_hippo_2020} provides mathematical foundations for compressing continuous-time signals into finite-dimensional memory states through optimal polynomial projections. Given a time series \( y(t) \), HiPPO maintains a memory state \( c(t) \in \mathbb{R}^M \) that optimally approximates the historical signal \( \{y(s)\}_{s \leq t} \). The framework consists of a time-dependent measure \( \omega^{(t)}(s) \) over \( (-\infty, t] \) that defines input importance, along with normalized polynomial basis functions \( \{g_n^{(t)}(s)\}_{n=0}^{M-1} \) that are orthonormal under \( \omega^{(t)}(s) \), satisfying \( \int_{-\infty}^t g_m^{(t)}(s)g_n^{(t)}(s)\omega^{(t)}(s)\mathrm{d}s = \delta_{mn} \). The historical signal is encoded through projection coefficients given by
\begin{align}
c_n(t) = \int_{-\infty}^t y(s)g_n^{(t)}(s)\omega^{(t)}(s)\mathrm{d}s.
\end{align}
This yields the approximation $y(s) \approx \sum_{n=0}^{M-1}c_n(t)g_n(s)$ for $s \in (-\infty,t]$, minimizing the $L^2$-error $\int_{-\infty}^t \|y(s)-\sum_n c_n(t)g_n(s)\|^2 \omega^{(t)}(s)\mathrm{d}s$.

Differentiating \( \mathbf{c}(t) := [c_0(t), \ldots, c_{M-1}(t)]^\intercal \) induces a linear ordinary differential equation
\(
\frac{\mathrm{d}}{\mathrm{d}t}\mathbf{c}(t) = \mathbf{A}(t)\mathbf{c}(t) + \mathbf{B}(t)y(t)
\)
with matrices \( \mathbf{A}(t), \mathbf{B}(t) \) encoding measure-basis dynamics. Discretization yields the recurrence
\(
\mathbf{c}_t = \mathbf{A}_t \mathbf{c}_{t-1} + \mathbf{B}_t y_t
\)
enabling online updates. The structured state space sequential (S4) model \citep{gu_s4_2022} extends HiPPO with trainable parameters and convolutional kernels, while Mamba \citep{gu_mamba_2023,dao_mamba2_2024} introduces hardware-aware selective state mechanisms, both leveraging HiPPO for efficient long-range memory modeling.

HiPPO supports various measure-basis configurations \citep{gu_hippo_2020,gu_httyh_2023}. A canonical instantiation, HiPPO-LegS, uses a uniform measure \( \omega^{(t)}(s) = \frac{1}{t}\mathbf{1}_{[0,t]}(s) \) with scaled Legendre polynomials adapted to \([0,t]\), \(g_n^{(t)}(s) = (2 m+1)^{1 / 2} P_m\left(\frac{2 s}{t}-1\right)\). This uniform measure encourages HiPPO-LegS to keep the whole past in memory.


\section{Interdomain inducing point Gaussian processes with HiPPO}
\label{sec:method}
We bridge the HiPPO framework with interdomain Gaussian processes by interpreting HiPPO's state vector defined by time-varying orthogonal projections as interdomain inducing points. This enables adaptive compression of the history of a GP while preserving long-term memory.

\subsection{HiPPO as interdomain inducing variables}

Recall that in an interdomain setting in Section~\ref{sec:vi_and_interdomain_GP}, inducing variables are defined through an integral transform against a set of basis functions. Let $f\sim\mathcal{GP}(0,k)$, and consider time-dependent basis functions \(\phi_{m}^{(t)}(x) = g_{m}^{(t)}(x)\omega^{(t)}(x)\),
where \(g_{m}^{(t)}\) are the orthogonal functions of HiPPO and \(\omega^{(t)}\) is the associated measure. We define the corresponding interdomain inducing variables as $u_{m}^{(t)} = \int f(x)\phi_{m}^{(t)}(x)\mathrm{d}x$. These inducing variables adapt in time, capturing long-range historical information in a compact form via HiPPO's principled polynomial projections. 


\subsection{Adapting the kernel matrices over time}
When new observations arrive at later times in a streaming scenario, we must adapt both the prior cross-covariance \(\mathbf{K}_{\mathbf{fu}}\) and the prior covariance of the inducing variables \(\mathbf{K}_{\mathbf{uu}}\). In particular, the basis functions in our HiPPO construction evolve with time, so the corresponding kernel quantities also require updates. Below, we describe how to compute and update these matrices at a new time \(t_2\) given their values at time \(t_1\). For clarity, we first discuss \(\mathbf{K}_{\mathbf{fu}}\), then \(\mathbf{K}_{\mathbf{uu}}\).

\subsubsection{Prior cross-covariance \(\mathbf{K}_{\mathbf{fu}}^{(t)}\)}
Recall that for a single input \(x_{n}\), the prior cross-covariance with the \(m\)-th inducing variable is $\left[\mathbf{K}_{\mathbf{fu}}^{(t)}\right]_{nm} = \int k\left(x_{n}, x\right)\phi_{m}^{(t)}(x)\mathrm{d}x$. We can compute the temporal evolution of \(\mathbf{K}_{\mathbf{fu}}^{(t)}\) in a manner consistent with the HiPPO approach, leveraging the same parameters \(\mathbf{A(t)}\) and \(\mathbf{B(t)}\). Specifically,  
\begin{align}
\frac{\mathrm{d}}{\mathrm{d}t}\left[\mathbf{K}_{\mathbf{fu}}^{(t)}\right]_{n,:}
=
\mathbf{A(t)}\left[\mathbf{K}_{\mathbf{fu}}^{(t)}\right]_{n,:}
+
\mathbf{B(t)}k\left(x_{n},t\right),
\end{align}
where $\left[\mathbf{K}_{\mathbf{fu}}^{(t)}\right]_{n,:}$ is the $n$-th row of $\mathbf{K}_{\mathbf{fu}}^{(t)}$. The matrices $\mathbf{A(t)}$ and $\mathbf{B(t)}$ depend on the specific choice of the HiPPO measure and basis functions. In our experiments, we employ the HiPPO-LegS variant, whose explicit matrix forms are provided in Appendix~\ref{appendix:hippo-legs-matrices}. One then discretizes in \(t\) (e.g. using an Euler method or a bilinear transform) to obtain a recurrence update rule.

\subsubsection{Prior covariance of the inducing variables \(\mathbf{K}_{\mathbf{uu}}^{(t)}\)}
\label{sec:kuu_ode_rff} 
The $lm$-th element of the prior covariance matrix for the inducing variables is given by $\left[\mathbf{K}_{\mathbf{uu}}^{(t)}\right]_{\ell m}
=
\iint
k\left(x,x^\prime\right)\phi_{\ell}^{(t)}(x)\phi_{m}^{(t)}(x^\prime)\mathrm{d}x\mathrm{d}x^\prime$. 
Since $k(x, x^\prime)$ depends on both $x$ and
$x^\prime$, a recurrence update rule based on the original HiPPO formulation, which is designed for single integral, can not be obtained directly for \(\mathbf{K}_{\mathbf{uu}}^{(t)}\). Fortunately, for stationary kernels, Bochner Theorem \citep{rudin_fourier_1994} can be applied to factorize the double integrals into two separate single integrals, which gives rise to Random Fourier Features (RFF) approximation \citep{rahimi_random_2007}: for a stationary kernel \(k(x,x^\prime) = k(|x-x^\prime|)\), RFF approximates it as follows:
\begin{equation}
k(x,x^\prime) \approx \frac{1}{N}\sum_{n=1}^N\left[
\cos\left(w_nx\right)\cos\left(w_nx^\prime\right) + \sin\left(w_nx\right)\sin\left(w_nx^\prime\right)\right],
\end{equation}
where $w_n\sim p(w)$ is the spectral density of the kernel. Substituting this into the double integral factorizes the dependency on \(x\) and \(x^\prime\), reducing \([\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}\) to addition of products of one-dimensional integrals. Each integral, with the form of either \(\int \cos(w_dx)\phi_{\ell}^{(t)}(x)\mathrm{d}x\) or \(\int \sin(w_dx)\phi_{\ell}^{(t)}(x)\mathrm{d}x\), again corresponds to a HiPPO-ODE in time. By sampling sufficiently many random features, updating them recurrently to time $t$, and averaging, we obtain an accurate approximation of \(\mathbf{K}_{\mathbf{uu}}^{(t)}\). The details of the ODE for recurrent updates of the RFF samples appear in Appendix~\ref{appendix:rff}.

Alternatively, one may differentiate \(\mathbf{K}_{\mathbf{uu}}^{(t)}\) directly with respect to \(t\). This yields a matrix ODE of the form different from the original HiPPO formulation. Empirically, we note that a naive implementation of this approach is numerically unstable. Therefore, we conduct our experiments based on RFF approximation for now. For details, see Appendix~\ref{appendix:direct-ode-hippo-legs}. 


\subsubsection{Sequential variational updates}
Having obtained \(\mathbf{K}_{\mathbf{fu}}^{(t_2)}, \mathbf{K}_{\mathbf{uu}}^{(t_2)}\) at a new time \(t_2>t_1\), we perform variational updates following the online GP framework described in Section~\ref{sec:online_gp}. This ensures the posterior at time \(t_2\) remains consistent with both the new data and the previous posterior at time \(t_1\), based on \(\mathbf{K}_{\mathbf{fu}}^{(t_1)}, \mathbf{K}_{\mathbf{uu}}^{(t_1)}\). Overall, this procedure endows interdomain HiPPO-based GPs with the ability to capture long-term memory online. By viewing the induced kernel transforms as ODEs in time, we efficiently preserve the memory of past observations while adapting our variational posterior in an online fashion.

\section{Experiments}
\label{sec:train_kernel_model}
\begin{figure}[b]
    \vspace{-6mm}
  \centering
  \begin{subfigure}[b]{0.245\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_rmse.pgf}
      }
      \vspace{-7mm}
      \caption{Test RMSE (Solar)}
      \label{fig:solar_rmse}
  \end{subfigure}
  \begin{subfigure}[b]{0.245\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_nlpd.pgf}
      }
      \vspace{-7mm}
      \caption{Test NLPD (Solar)}
      \label{fig:solar_nlpd}
  \end{subfigure}
  \begin{subfigure}[b]{0.245\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_rmse.pgf}
      }
      \vspace{-7mm}
      \caption{Test RMSE (Audio)}
      \label{fig:timit_rmse}
  \end{subfigure}
  \begin{subfigure}[b]{0.245\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_nlpd.pgf}
      }
      \vspace{-7mm}
      \caption{Test NLPD (Audio)}
      \label{fig:timit_nlpd}
  \end{subfigure}
  \vspace{-6mm}
  \caption{Comparison of OSGPR, OVFF and OHSGPR (ours) on the Solar Irradiance and Audio signal prediction datasets across 10 tasks. Log-scale is used in the y-axes of the plots for NLPD}
  \vspace{-3mm}
  \label{fig:comparison_trained_kernel}
\end{figure}
%

We evaluate OHSGPR on online time series regression tasks. For all experiments, we construct inducing variables based on HiPPO-LegS \citep{gu_hippo_2020}. We consider the following experimental setup:

\vspace{-3mm}
\paragraph{Datasets.} We consider two benchmarks:
\begin{itemize}[leftmargin=15pt, topsep=-1pt]
\item \textbf{Solar Irradiance} \citep{lean2004solar}. Following \citet{gal_improving_2015}: we scale the original outputs with its standard deviation and removed 5 segments of length 20 to create the test set.
\item \textbf{Audio signal prediction dataset} \citep{bui_tree_2014}. It was produced from the TIMIT database \citep{Garofolo1993timit} and we shifted the signal down to the baseband and select a segment of length 18,000 to construct interleaved training and testing sets consisting of 9,000 time stamps. Moreover, we linearly scale the input time stamps to the range [0, 10].
\end{itemize}
We construct online learning tasks by splitting each dataset into 10 sequential partitions with an equal number of training instances, and the model is updated incrementally as new task/split becomes available without re-visiting the data from past tasks/splits.

\paragraph{Baseline.} We compare OHSGPR with OSGPR (based on standard inducing points) \citep{bui_streaming_2017}, and OVFF (OSGPR based on variational Fourier feature (VFF), an interdomain inducing point approach from \citet{hensman2018variational}) across different numbers of inducing variables \( M \). For all experiments, we use RBF kernel with a tunable length-scale for OSGPR and OHSGPR, and Mat\'ern-$\frac{5}{2}$ kernel with a tunable length-scale for OVFF, since VFF is tailored specifically to Mat\'ern kernels. It is also worth noting that, in contrast with OHSGPR where after each intermediate task, the kernel matrices only need to be evolved up to the maximum times tamp observed so far, OVFF requires computing kernel matrices as integrals over a predefined interval that covers the whole range of the time stamps from all tasks (including the unobserved ones), which is impractical in real-world online learning tasks. Here, for OVFF, we set the two edges of this interval to be the minimum and maximum time stamp among the data points from all the 10 tasks, respectively. All the models are trained using ADAM \citep{kingma2015adam} with a learning rate of 0.01 and 5000 (by default) iterations per task. We set the default RFF sample size during training and testing to be 500 and 5000, respectively. The total number of discretization steps (for all 10 tasks) is 160 by default.

\paragraph{Evaluations \& metrics.} After observing task $i$, we report Root Mean Squared Error (RMSE) and Negative Log Predictive Density (NLPD) over all the past test points from task $1$ to task $i-1$. Additionally, we report wall-clock accumulated running time for learning all the tasks for OHSGPR and the baseline methods to demonstrate the computational efficiency of OHSGPR, compared with OSGPR, in the case where the kernel hyperparameters and the likelihood variance are fixed. 

\subsection{Results based on trainable kernel hyperparameters and likelihood variance}
\label{sec: exp_trainable_kernel}
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/svgp1_se_150z.pdf}
      \vspace{-5mm}
      \caption{OSGPR (after task 1)}
      \label{fig:solar_OSGPR_task1}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/svgp6_se_150z.pdf}
      \vspace{-5mm}
      \caption{OSGPR (after task 6)}
      \label{fig:solar_OSGPR_task6}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/svgp10_se_150z.pdf}
      \vspace{-5mm}
      \caption{OSGPR (after task 10)}
      \label{fig:solar_OSGPR_task10}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/hsgp1_se_150z.pdf}
      \vspace{-5mm}
      \caption{OHSGPR (after task 1)}
      \label{fig:solar_OHSGPR_task1}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/hsgp6_se_150z.pdf}
      \vspace{-5mm}
      \caption{OHSGPR (after task 6)}
      \label{fig:solar_OHSGPR_task6}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/hsgp10_se_150z.pdf}
      \vspace{-5mm}
      \caption{OHSGPR (after task 10)}
      \label{fig:solar_OHSGPR_task10}
  \end{subfigure}
  \vspace{-3mm}
  \caption{Predictive mean $\pm2$ standard deviation of OSGPR and OHSGPR after tasks 1, 6, and 10 of the Solar dataset. $M=150$ inducing variables are used.}
  \label{fig:solar_predictive_distributions}
  % \vspace{-7mm}
\end{figure}
%
Figure~\ref{fig:comparison_trained_kernel} shows test RMSE and NLPD (over the past data) of OHSGPR and the baseline methods during online learning over the 10 tasks constructed from the Solar Irradiance and Audio dataset. Overall, OHSGPR outperforms OSGPR and OVFF, and whilst OHSGPR does not outperform OSGPR after seeing the first few tasks, OHSGPR demonstrates superior performance as more and more tasks are revealed. In particular, OHSGPR consistently achieves better overall performance than OSGPR and OVFF after seeing the final task, suggesting OHSGPR is able to maintain long-term memory, while OSGPR and OVFF may suffer from catastrophic forgetting. In Figure \ref{fig:solar_predictive_distributions}, we illustrate the catastrophic forgetting pathology of OSGPR by comparing the predictive distributions of OHSGPR and OSGPR (with $M=150$) trained on Solar Irradiance. As OSGPR seeing more and more tasks, it starts to exhibit a noticeable degradation in performance over the initial few data regions. In particular, the inducing points (in red) tend to move to the regions where the later tasks live after training, and the prediction of OSGPR in the initial regions without sufficient inducing points becomes more erratic. In contrast, OHSGPR maintains consistent performance across both early and recent time periods, suggesting effective preservation of long-term memory through its HiPPO-based memory mechanism.


\subsection{Wall-clock accumulated running time of OHSGPR with fixed kernel}
\label{sec:fixed_kernel_model}

\begin{table}[t]
    \vspace{-3mm}
  \caption{Wall-clock accumulated runtime for learning all the 10 tasks on a single NVIDIA RTX3090 GPU in seconds), of OSGPR, OVFF and OHSGPR models with fixed kernel on the Solar dataset. In addition to different number $M$ of inducing points, we also compare OHSGPRs with 160, 320, and 480 total number of discretization steps and 500, and 5000 RFF samples.}
  \centering
  %\begin{adjustbox}{width=\textwidth}
    \vspace{-3mm}
  \begin{tabular}{lccccc}
  \hline
   & \multicolumn{3}{c}{\textbf{Solar Irradiance}} &  \multicolumn{2}{c}{\textbf{Audio Data}}\\
      \hline
      \multirow{2}{*}{\textbf{Method}} 
      & \multicolumn{3}{c}{ \( M \)} & \multicolumn{2}{c}{\( M \)} \\
      & \( 50 \) & \( 100 \) & \( 150 \) 
      & \( 100 \) & \( 200 \) \\
      \hline
      OSGPR (1000 iterations) & 134 & 134 & 140 & 144 & 199\\
      OSGPR (5000 iterations) & 672 & 675 & 698 & 720 & 997\\
      \hline
      OVFF & 0.288 & 0.313 & 0.349 & 0.295 & 0.356 \\
      \hline
      OHSGPR (160 disc, 500 RFF) & 0.262 & 0.289 & 0.333 & 0.282 & 0.402\\
      OHSGPR  (320 disc, 500 RFF)  & 0.289 & 0.334 & 0.401 & 0.312 & 0.485\\
      OHSGPR  (480 disc, 500 RFF) ) & 0.301 & 0.346 & 0.410 & 0.353 & 0.576\\
      \hline
      OHSGPR (160 disc, 5000 RFF & 0.310 & 0.447 & 0.650 & 0.739 & 1.271\\
      OHSGPR (320 disc, 5000 RFF ) & 0.388 & 0.629 & 0.938 & 0.902 & 1.822\\
      OHSGPR (480 disc, 5000 RFF) & 0.450 & 0.787 & 1.211 & 1.044 & 2.369\\
      \hline
  \end{tabular}
%\end{adjustbox}
\vspace{-7mm}
  \label{table:wall_time}
\end{table}

Table~\ref{table:wall_time} shows the accumulated wall-clock time for OSGPR (varying iterations per task), OVFF and OHSGPR (varying discretization steps and RFF samples) with fixed kernel hyperparameters and likelihood variance (from a full GP trained on the first two tasks). Unlike OSGPR, which must iteratively optimize inducing points, OHSGPR and OVFF, based on interdomain inducing points bypass this cumbersome optimization. In particular, OHSGPR recurrently evolves $\mathbf{K}_{\mathbf{fu}}$ and $\mathbf{K}_{\mathbf{uu}}$ for each new task with no training required. As a result, OHSGPR and OVFF run significantly faster, adapting to all tasks within a couple of seconds for Solar Irradiance and Audio data. Appendix \ref{sec:additional_result} compares fixed-kernel models to trainable-kernel ones and finds that they are as competitive, if not superior, in performance. In addition, OHSGPR still outperforms OSGPR in preserving long-term memory in this comparison. Interestingly, while OVFF underperforms with trainable kernel (see Section \ref{sec: exp_trainable_kernel}), it is competitive with OHSGPR when kernel hyperparameters and likelihood variance are fixed, which suggests hyperparameter tuning for VFF with the online learning ELBO may be challenging.



\section{Conclusion}

We introduce OHSGPR, a novel online Gaussian process model that leverages the HiPPO framework for robust long-range memory in online regression. By interpreting HiPPO's time-varying orthogonal projections as interdomain GP basis functions, we establish a link between SSMs and GPs. This connection allows OHSGPR to harness HiPPO's efficient ODE-based recurrent updates while preserving GP-based uncertainty quantification. Empirical results on time series regression tasks show that OHSGPR significantly outperforms existing online GP methods, especially in scenarios requiring long-term memory. Moreover, with fixed kernel hyperparameters, its recurrence-based kernel updates yield far lower computational overhead than OSGPR's sequential inducing point optimization. This efficient streaming capability and preservation of historical information make OHSGPR well-suited for real-world applications demanding both speed and accuracy. Future work includes exploring alternative basis and measure selections and integrating OHSGPR with deep learning (e.g., enabling online learning in GPVAE models \citep{fortuin_gpvae_2020, jazbec_scalable_2021}).

\clearpage
\bibliography{reference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\input{sections/iclr_workshop_appendix}


\end{document}
