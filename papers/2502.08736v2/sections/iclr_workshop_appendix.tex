\section{HiPPO-LegS matrices}
\label{appendix:hippo-legs-matrices}

Here we provide the explicit form of matrices used in our implementation of HiPPO-LegS \citep{gu_hippo_2020}. For a given time $t$, the measure $\omega^{(t)}(x) = \frac{1}{t}\mathbf{1}_{[0,t]}(x)$ and basis functions $\phi_m^{(t)}(x) = g_m^{(t)}(x)\omega^{(t)}(x) = \frac{\sqrt{2m+1}}{t}P_m\left(\frac{2x}{t}-1\right)\mathbf{1}_{[0,t]}(x)$ are used, where $P_m(\cdot)$ is the $m$-th Legendre polynomial and $\mathbf{1}_{[0,t]}(x)$ is the indicator function on the interval $[0,t]$. These basis functions are orthonormal, i.e.,
\begin{equation}
\int_0^t \frac{g_m^{(t)}(x)g_n^{(t)}(x)}{t}\mathrm{d}x = \delta_{mn}
\end{equation}

Following \citet{gu_hippo_2020}, the HiPPO-LegS framework maintains a coefficient vector $\mathbf{c}(t) \in \mathbb{R}^{M \times 1}$ that evolves according to the ODE:
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}\mathbf{c}(t) = \mathbf{A}(t)\mathbf{c}(t) + \mathbf{B}(t)f(t)
\end{equation}
where $f(t)$ is the input signal at time $t$. The matrices $\mathbf{A}(t) \in \mathbb{R}^{M \times M}$ and $\mathbf{B}(t) \in \mathbb{R}^{M \times 1}$ are given by:

\begin{equation}
\label{eq:hippo_matrix_a}
[\mathbf{A(t)}]_{nk} = \begin{cases}
    -\frac{1}{t}\sqrt{(2n+1)(2k+1)} & \text{if } n > k \\
    -\frac{n+1}{t} & \text{if } n = k \\
    0 & \text{if } n < k
\end{cases}
\end{equation}

and

\begin{equation}
[\mathbf{B(t)}]_n = \frac{\sqrt{2n+1}}{t}
\end{equation}

These matrices govern the evolution of the basis function coefficients over time, where the factor $1/t$ reflects the time-dependent scaling of the basis functions to the adaptive interval $[0,t]$. When discretized, this ODE yields the recurrence update used in our implementation.

\section{Computing prior covariance of the inducing variables \(\mathbf{K}_{\mathbf{uu}}^{(t)}\)}
We provide the detailed derivation for the following two approaches when the inducing functions are defined via HiPPO-LegS. Recall that
\begin{equation}
[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}
=
\iint
k(x, x^{\prime}) \phi_{\ell}^{(t)}(x)\phi_{m}^{(t)}(x^{\prime}) \mathrm{d}x \mathrm{d}x^{\prime},
\label{eq:kuu_definition}
\end{equation}
where $\phi_{\ell}^{(t)}(x) = g_{\ell}^{(t)}(x)\,\omega^{(t)}(x)$ are the time-varying basis functions under the HiPPO-LegS framework.

\subsection{RFF approximation}
\label{appendix:rff}
For stationary kernels (e.g. an RBF kernel), Bochner's theorem permits a representation of the kernel as an expectation with respect to its spectral measure:
\begin{equation}
   k(x,x^{\prime}) = \mathbb{E}_{p(w)}\Bigl[\cos(wx)\,\cos(wx^{\prime}) +\sin(wx)\,\sin(wx^{\prime})\Bigr],
\end{equation}
where \( p(w) \) is the spectral density of the kernel.

Substituting this into \eqref{eq:kuu_definition}, for a given Monte Carlo sample of \( w \) we define
\begin{equation}
    Z_{w,\ell}^{(t)} = \int \cos(wx)\phi_{\ell}^{(t)}(x)\,\mathrm{d}x, \quad Z_{w,\ell}^{\prime(t)} = \int \sin(wx)\phi_{\ell}^{(t)}(x)\,\mathrm{d}x, 
\end{equation}
and
\begin{equation}
    \mathbf{Z}_{w}^{(t)} = \left[Z_{w,1}^{(t)},  \cdots, Z_{w,M}^{(t)}\right]^\intercal, \quad \mathbf{Z}_{w}^{\prime(t)} = \left[Z_{w,1}^{\prime(t)}, \cdots, Z_{w,M}^{\prime(t)}\right]^\intercal. 
\end{equation}
Collecting \(N\) Monte Carlo samples, we form the feature matrix
\begin{equation}
    \mathbf{Z}^{(t)} = \begin{bmatrix} \mathbf{Z}_{w_1}^{(t)} & 
    \mathbf{Z}_{w_2}^{(t)} & \cdots & 
    \mathbf{Z}_{w_N}^{(t)} & 
    \mathbf{Z}_{w_1}^{\prime(t)} & 
    \mathbf{Z}_{w_2}^{\prime(t)} & \cdots & 
    \mathbf{Z}_{w_N}^{\prime(t)} 
    \end{bmatrix},
\end{equation}
and the RFF approximation of the covariance is
\begin{equation}
    \mathbf{K}_{\mathbf{uu}}^{(t)} \approx \frac{1}{N}\,\mathbf{Z}^{(t)}\left(\mathbf{Z}^{(t)}\right)^\intercal.
\end{equation}


Moreover, as before, the evolution of \(\mathbf{Z}_{w}^{(t)}\) and \(\mathbf{Z}_{w}^{\prime(t)}\) is governed by the ODE
\begin{equation}
    \frac{\mathrm{d}}{\mathrm{d}t}\mathbf{Z}_{w}^{(t)} = \mathbf{A}(t)\,\mathbf{Z}_{w}^{(t)} + \mathbf{B}(t)\,h(t), \quad \frac{\mathrm{d}}{\mathrm{d}t}\mathbf{Z}_{w}^{\prime(t)} = \mathbf{A}(t)\,\mathbf{Z}_{w}^{\prime(t)} + \mathbf{B}(t)\,h^{\prime}(t),
\end{equation}
with \( h(t) = \cos(wt) \) and \( {h}^{\prime}(t) = \sin(wt) \) and these ODEs can be solved in parallel for different Monte Carlo samples, thereby fully exploiting the computational advantages of GPU architectures.

\subsection{Direct ODE evolution}
\label{appendix:direct-ode-hippo-legs}

Differentiating $[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}$ with respect to $t$ gives
\begin{equation}
\frac{d}{dt}\,[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}
=
\iint
k(x,x^{\prime})
\frac{\mathrm{d}}{\mathrm{d}t}
\left[\phi_{\ell}^{(t)}(x)\phi_{m}^{(t)}(x^{\prime})\right]
\mathrm{d}x\mathrm{d}x^{\prime}.
\end{equation}
Applying the product rule:
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}
=
\iint
k(x,x^{\prime})\frac{\mathrm{d}}{\mathrm{d}t}\phi_{\ell}^{(t)}(x)\phi_{m}^{(t)}(x^{\prime})\mathrm{d}x\mathrm{d}x^{\prime}
+
\iint
k(x,x^{\prime})\phi_{\ell}^{(t)}(x)\frac{\mathrm{d}}{\mathrm{d}t}\phi_{m}^{(t)}(x^{\prime})\mathrm{d}x\mathrm{d}x^{\prime}.
\end{equation}
In HiPPO-LegS, each $\phi_{\ell}^{(t)}(x)$ obeys an ODE governed by scaled Legendre polynomials on $[0,t]$ and a Dirac boundary term at $x=t$. Concretely,
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}\phi_{\ell}^{(t)}(x)
=
-\frac{\sqrt{2\ell+1}}{t}
\left[
\frac{\ell+1}{\sqrt{2\ell+1}}\phi_{\ell}^{(t)}(x)
+
\sqrt{2\ell-1}\phi_{\ell-1}^{(t)}(x)
\right]
+
\frac{1}{t}\,\delta_{t}(x),
\end{equation}
where $\delta_{t}(x)$ is the Dirac delta at $x=t$.

Substituting this expression into the integrals yields the boundary terms $\int k(t, x^{\prime})\,\phi_{m}^{(t)}(x^{\prime})\,\mathrm{d}x^{\prime}$, along with lower-order terms involving $[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell-1,m}$, etc. Summarizing in matrix form leads to
\begin{equation}
\label{eq:kuu_ode}
\frac{\mathrm{d}}{\mathrm{d}t}\,\mathbf{K}_{\mathbf{uu}}^{(t)}
=
-\frac{1}{t}
\left[
\mathbf{A} \,\mathbf{K}_{\mathbf{uu}}^{(t)}
+
\mathbf{K}_{\mathbf{uu}}^{(t)}\mathbf{A}^\intercal
\right]
+
\frac{1}{t}
\left[
\tilde{\mathbf{B}}(t)+\tilde{\mathbf{B}}(t)^\intercal
\right],
\end{equation}
where $\mathbf{K}_{\mathbf{uu}}^{(t)} \in \mathbb{R}^{M \times M}$ has entries $[\mathbf{K}_{\mathbf{uu}}^{(t)}]_{\ell m}$, $\mathbf{A} \in \mathbb{R}^{M\times M}$ is the same lower-triangular matrix from the HiPPO-LegS framework defined in equation \eqref{eq:hippo_matrix_a}, and $\tilde{\mathbf{B}}(t) \in \mathbb{R}^{M\times M}$ is built from the boundary contributions as
\begin{equation}
\label{eq:boundary_matrix}
\tilde{\mathbf{B}}(t) = \mathbf{c}(t) \mathbf{1}_M,
\end{equation}
where $\mathbf{c}(t) \in \mathbb{R}^{M \times 1}$ is the coefficient vector and $\mathbf{1}_M \in \mathbb{R}^{1\times M}$ is a row vector of ones of size $M$ with
\begin{equation}
    c_{\ell}(t)
    =
    \int\!
    k\left(t, x\right)\,\phi_{\ell}^{(t)}(x)\,\mathrm{d}x.
\end{equation}
After discretizing in $t$ (e.g.\ an Euler scheme), one repeatedly updates $\mathbf{K}_{\mathbf{uu}}^{(t)}$ and the boundary vector $\mathbf{c}(t)$ over time.

\subsubsection{Efficient computation of \( \tilde{\mathbf{B}}(t) \)}

Computing \( \tilde{\mathbf{B}}(t) \) directly at each time step requires evaluating \( M \) integrals, which can be computationally intensive, especially when \( t \) changes incrementally and we need to update the matrix \( \tilde{\mathbf{B}}(t) \) repeatedly.

To overcome this inefficiency, we propose an approach that leverages the HiPPO framework to compute \(\tilde{\mathbf{B}}(t) \) recursively as \( s \) evolves. This method utilizes the properties of stationary kernels and the structure of the Legendre polynomials to enable efficient updates.

\paragraph{Leveraging Stationary Kernels}

Assuming that the kernel \( k(x, t) \) is stationary, it depends only on the difference \( d = |x - t| \), so \( k(x, t) = k(d) \). In our context, since we integrate over \( x \in [t_{\text{start}}, t] \) with \( x \leq t \), we have \( d = t - x \geq 0 \). Therefore, we can express \( k(x, t) \) as a function of \( d \) over the interval \( [0, t - t_{\text{start}}] \):
\begin{equation}
    k(x, t) = k(t - x) = k(d), \quad \text{with} \quad d \in [0, t - t_{\text{start}}].
\end{equation}

Our goal is to approximate \( k(d) \) over the interval \( [0, t - t_{\text{start}}] \) using the orthonormal Legendre basis functions scaled to this interval. Specifically, we can represent \( k(d) \) as
\begin{equation}
    k(d) \approx \sum_{m=0}^{M-1} \tilde{c}_m(t) \, g_m^{(t)}(d),
\end{equation}
where \( g_m^{(t)}(d) \) are the Legendre polynomials rescaled to the interval \( [0, t - t_{\text{start}}] \).

\paragraph{Recursive Computation via HiPPO-LegS}

To efficiently compute the coefficients \( \tilde{c}_m(t) \), we utilize the HiPPO-LegS framework, which provides a method for recursively updating the coefficients of a function projected onto an orthogonal basis as the interval expands. In our case, as \( t \) increases, the interval \( [t_{\text{start}}, t] \) over which \( k(d) \) is defined also expands, and we can update \( \tilde{c}_m(t) \) recursively.

Discretizing time with step size \( \Delta t \) and indexing \( t_k = t_{\text{start}} + k \Delta t \), the update rule using the Euler method is:
\begin{equation}
    \tilde{\mathbf{c}}_{k+1} = \left( \mathbf{I} - \frac{1}{k} \mathbf{A} \right) \tilde{\mathbf{c}}_k + \frac{1}{k} \mathbf{B} \, k(t_k),
    \label{eq:hippo_coeff_update}
\end{equation}
where \( \tilde{\mathbf{c}}_k = \left[\tilde{c}_0(t_k), \tilde{c}_1(t_k), \ldots, \tilde{c}_{M-1}(t_k)\right]^\intercal \), and \( \mathbf{A} \in \mathbb{R}^{M \times M} \) and \( \mathbf{B} \in \mathbb{R}^M \) are again matrices defined by the HiPPO-LegS operator \citep{gu_hippo_2020}.

\paragraph{Accounting for Variable Transformation and Parity}

The change of variables from \( x \) to \( d = t - x \) introduces a reflection in the function domain. Since the Legendre polynomials have definite parity, specifically,
\begin{equation}
    P_m(-x) = (-1)^m P_m(x),
\end{equation}
we need to adjust the coefficients accordingly when considering the reflected function.

As a result of this reflection, when projecting \( k(d) \) onto the Legendre basis, the coefficients \( \tilde{c}_m(t) \) computed via the HiPPO-LegS updates will correspond to a reflected version of the function. To account for this, we apply a parity correction to the coefficients. Specifically, the corrected coefficients \( c_m(t) \) are related to \( \tilde{c}_m(t) \) by a sign change determined by the degree \( m \):
\begin{equation}
    c_m(t) = (-1)^m \tilde{c}_m(t).
\end{equation}

This parity correction ensures that the computed coefficients properly represent the function over the interval \( [t_{\text{start}}, t] \) without the effect of the reflection.


By computing \( \mathbf{c}(t) \) recursively as \( t \) evolves, we can efficiently update \( \tilde{\mathbf{B}}(t) = \mathbf{c}(t) [1, \dots, 1] \) at each time step without the need to evaluate the integrals directly. This approach significantly reduces the computational burden associated with updating \( \tilde{\mathbf{B}}(t) \) and allows for efficient computation of \( \mathbf{K}_\mathbf{uu}^{(t)} \) via the ODE.

\section{Performance of OHSGPR and baselines with fixed kernel}
\label{sec:additional_result}
In Figure \ref{fig:solar_comparison_disc_fixed_kernel} and \ref{fig:timit_comparison_disc_fixed_kernel}, we present the performance, measured by RMSE and NLPD, of OSGPR, OVFF and OHSGPR models with fixed kernel shown in Section \ref{sec:fixed_kernel_model} for Solar Irradiance and Audio dataset, respectively. OHSGPR again outperforms OSGPR in terms of long-term memory preservation and achieves better RMSE and NLPD at the later stage where most of the tasks have been revealed. Interestingly, the overall performance of these fixed-kernel models is competitive or even better with the models with trainable kernel in Section \ref{sec:train_kernel_model}. The performance of OVFF with fixed kernel is significantly better than OVFF with trainable kernel, suggesting it may be hard to find good hyperparamters with the online ELBO objective for OVFF. Furthermore, compared to OHSGPR with a trainable kernel, OHSGPR with a fixed kernel achieves better performance on the audio dataset when we increase the total number of discretizations from 160 to 320 or 480. The best RMSE and NLPD for OHSGPR after the final task for audio data in this work (0.2738 and 0.5510, respectively) are achieved by a fixed-kernel OHSGPR with 480 total discretization steps (Figure \ref{fig:timit_rmse_disc_5000} and \ref{fig:timit_nlpd_disc_5000}), suggesting in practice, for some applications, if the kernel hyperparameters are already in a good range, further optimization over it might show diminishing returns and in this case the effect of fine-grained discretization might play a more critical role in the performance. 


\begin{figure}[htbp]
\captionsetup[subfigure]{font=scriptsize,labelfont=scriptsize}
  \centering
   \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_rmse_iter.pgf}
      }
      \caption{RMSE (OSGPR)}
      \label{fig:solar_rmse_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_rmse_vff.pgf}
      }
      \caption{RMSE (OVFF)}
      \label{fig:solar_rmse_vff}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_rmse_disc_rff_500.pgf}
      }
      \caption{RMSE (OHSGPR500)}
      \label{fig:solar_rmse_disc_500}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_rmse_disc_rff_5000.pgf}
      }
      \caption{RMSE (OHSGPR5000)}
      \label{fig:solar_rmse_disc_5000}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_nlpd_iter.pgf}
      }
      \caption{NLPD (OSGPR)}
      \label{fig:solar_nlpd_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_nlpd_vff.pgf}
      }
      \caption{NLPD (OVFF)}
      \label{fig:solar_nlpd_vff}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_nlpd_disc_rff_500.pgf}
      }
      \caption{NLPD (OHSGPR500)}
      \label{fig:solar_nlpd_disc_500}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/solar_nlpd_disc_rff_5000.pgf}
      }
      \caption{NLPD (OHSGPR5000)}
      \label{fig:solar_nlpd_disc_5000}
  \end{subfigure}
  \caption{Test RMSE and NLPD results for OSGPR, OVFF and OHSGPR (with RFF sample size of 500 or 5000) on the Solar Irradiance dataset (10 splits). Results are shown for three values of $M$ ($M = 50, 100, 150$), and different numbers of total discretization steps for OHSGPR. The kernel hyperparameters are fixed to those obtained from a full GP trained on splits 1-2.}
  \label{fig:solar_comparison_disc_fixed_kernel}
\end{figure}

\begin{figure}[htbp]
\captionsetup[subfigure]{font=scriptsize,labelfont=scriptsize}
  \centering
   \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_rmse_iter.pgf}
      }
      \caption{RMSE (OSGPR)}
      \label{fig:timit_rmse_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_rmse_vff.pgf}
      }
      \caption{RMSE (OVFF)}
      \label{fig:timit_rmse_vff}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_rmse_disc_rff_500.pgf}
      }
      \caption{RMSE (OHSGPR500)}
      \label{fig:timit_rmse_disc_500}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_rmse_disc_rff_5000.pgf}
      }
      \caption{RMSE (OHSGPR5000)}
      \label{fig:timit_rmse_disc_5000}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_nlpd_iter.pgf}
      }
      \caption{NLPD (OSGPR)}
      \label{fig:timit_nlpd_iter}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_nlpd_vff.pgf}
      }
      \caption{NLPD (OVFF)}
      \label{fig:timit_nlpd_vff}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_nlpd_disc_rff_500.pgf}
      }
      \caption{NLPD (OHSGPR500)}
      \label{fig:timit_nlpd_disc_500}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\textwidth}
      \centering
      \scalebox{0.35}{
      \input{figures/timit_nlpd_disc_rff_5000.pgf}
      }
      \caption{NLPD (OHSGPR5000)}
      \label{fig:timit_nlpd_disc_5000}
  \end{subfigure}
  \caption{Test RMSE and NLPD results for OSGPR, OVFF and OHSGPR (with RFF sample size of 500 or 5000) on the Audio signal prediction dataset (10 splits). Results are shown for two values of $M$ ($M = 100, 200$), and different numbers of total discretization steps for OHSGPR. The kernel hyperparameters are fixed to those obtained from a full GP trained on splits 1-2.}
  \label{fig:timit_comparison_disc_fixed_kernel}
\end{figure}

