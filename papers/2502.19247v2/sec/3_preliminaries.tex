\section{Preliminaries}
\label{sec:pre}
% In this section, we first review the pros and cons of softmax and linear attention. Then we analyze the basic transformation for modifying point cloud manifold in ego-centric 3D visual grounding.

% \subsection{Self Attention}
% The self-attention mechanism~\cite{vaswani2017attention} is fundamental to many Transformer-based models, enabling them to capture global dependencies in input sequences. Given an input sequence of $N$ tokens represented as $x \in \mathbb{R}^{N \times C}$, initially, they are projected to feature embedding space for subsequent alignment:
% \begin{equation} \label{eq:project}
%     \begin{split}
%         Q = xW_Q, \quad K = xW_K, \quad V = xW_V, 
%     \end{split}
% \end{equation}
% then the general form of self-attention is defined as:
% \begin{equation} \label{eq:attn}
%     o_i = \sum_{j=1}^{N} \frac{{\rm sim}(q_i, k_j)}{\sum_{k=1}^{N} {\rm sim}(q_i, k_{k})} v_j,
% \end{equation}
% where $W_Q, W_K, W_V \in \mathbb{R}^{C \times d}$ are learnable projection matrices, and ${\rm sim}(\cdot,\cdot)$ is the attention affinity function, which measures the affinity between the query $q_i$ and key $k_j$. To incorporate the positional information, a positional embedding $p \in \mathbb{R}^{N \times C}$ is added to the input.

% The most common attention form is Softmax attention, where the affinity between query $q_i$ and key $k_j$ is computed as:
% \begin{equation}
%     {\rm sim}(q_i, k_j) = \exp\left(\frac{q_i k_j^T}{\sqrt{d}}\right).
% \end{equation}
% This approach has been widely adopted in Point Transformers~\cite{zhao2021point,wu2022point,wu2024pointv3,pan20213d,guo2021pct}, but it suffers a high computational cost of $\mathcal{O}(N^2)$ due to the need to calculate attention for every query-key pair, especially when encounting a large number of points.

% To reduce computational complexity, linear attention approximates the affinity function using kernel-based feature maps:
% \begin{equation}
%     {\rm sim}(q_i, k_j) = \psi(q_i) \cdot \psi(k_j)^T,
% \end{equation}
% where $\psi(\cdot)$ is a feature transformation. This reformulation reduces complexity to $\mathcal{O}(N)$ by changing the computation order. However, the design of effective $\psi(\cdot)$ functions remains challenging, and linear attention often underperforms compared to Softmax attention~\cite{performer}.


% 这里其实可以基于ego-centric来考虑，因为其实也涉及最终检测的bbox在空间点云中的定位问题，顺便就可以和transformation一起讲讲，其实只需要把几种基本的线性变换和平移拿出来讲讲就可以了
% ego-centric的意义
% In real-world scenarios, intelligent agents perceive their environment without any prior scene knowledge. 
% They often rely on ego-centric observations, such as multi-view RGB-D images, rather than pre-established scene-level priors like pre-reconstructed 3D point clouds of the entire scene, as commonly used in previous studies~\citep{wu2024pointv3, huang2023segment3d}. 

% % 形式化研究问题
% Following \cite{wang2023embodiedscan}, we formalize the ego-centric 3D visual grounding task as follows: 
% Given \(V\) views of RGB-D images \(\{(I_v, D_v)\}_{v=1}^V\), where \(I_v \in \mathbb{R}^{H \times W \times 3}\) represents the RGB image and \(D_v \in \mathbb{R}^{H \times W}\) denotes the depth image of the \(v\)-th view, along with a language description \(L \in \mathbb{R}^T\), the objective is to output a 9-degree-of-freedom (9DoF) bounding box \(B = (x, y, z, l, w, h, \theta, \phi, \psi)\). 
% Here, \((x, y, z)\) are the 3D coordinates of the object's center, \((l, w, h)\) are its dimensions, and \((\theta, \phi, \psi)\) are its orientation angles. 
% The task is to determine \(B\) such that it accurately corresponds to the object described by \(L\) within the scene represented by \(\{(I_v, D_v)\}_{v=1}^V\). 


% In real-world applications, intelligent agents rely on ego-centric observations to understand their surroundings without prior knowledge of the entire scene. Unlike traditional approaches that assume pre-reconstructed 3D point clouds of the scene~\citep{wu2024pointv3, huang2023segment3d}, ego-centric 3D visual grounding tasks utilize multi-view RGB-D images to detect and localize objects in real time within an unstructured environment. In this context, an essential aspect of 3D perception involves transformations applied directly to point clouds, enabling robust and adaptive scene understanding across varying perspectives.

% To facilitate accurate localization in the point cloud space, we employ a set of fundamental geometric transformations: linear transformations, which include rotation, scaling, and shear, as well as translation. These transformations enable agents to align, scale, or position point cloud data with respect to the ego-centric coordinate system, making them foundational for tasks such as bounding box regression and scene alignment.
% \subsection{Manifold Transformation}
% In real-world applications, intelligent agents rely on ego-centric observations, specifically multi-view RGB-D images, to understand their surroundings online without prior knowledge of the entire scene. Unlike traditional approaches that assume a pre-reconstructed 3D point cloud of the scene~\citep{wu2024pointv3, huang2023segment3d}, ego-centric 3D visual grounding tasks dynamically reconstruct a point cloud using RGB images \( I_v \in \mathbb{R}^{H \times W \times 3} \) and depth images \( D_v \in \mathbb{R}^{H \times W} \) captured from \( V \) viewpoints. This enables agents to detect and localize objects in real time within unstructured environments based on these online observations.

% To facilitate accurate localization within the reconstructed point cloud space, we preshape point clouds with a set of fundamental geometric transformations, including rotation, scaling, shear, and translation.
\noindent \textbf{Manifold Transformation.}
Given a set of points \( P = \{p_i\}_{i=1}^N \) in 3D space where \( p_i = (x_i, y_i, z_i) \in \mathbb{R}^3 \), fundamental geometric transformations, including rotation, scaling, shear, and translation, are applied to adjust point cloud's position and orientation based on the agent's viewpoint, which provides a better point cloud manifold for following tasks.

\noindent\textbf{Rotation.}
Rotation adjusts the orientation of points around an axis using a rotation matrix \( R \in \mathbb{R}^{3 \times 3} \). For example, a rotation around the \(z\)-axis by angle \(\theta\) is given by
\begin{equation} \label{eq:rot_z}
    R_z(\theta) = 
    \left(
    \begin{matrix}
        \cos \theta & -\sin \theta & 0 \\
        \sin \theta & \cos \theta & 0 \\
        0 & 0 & 1
    \end{matrix}.
    \right)
\end{equation}
Applying \( R_z(\theta) \) to a point \( p_i \) yields a rotated position \( p_i' = R_z(\theta) p_i \), realigning the point in 3D space.

\noindent\textbf{Scaling and Shear.}
Scaling modifies the size of a point cloud by a diagonal matrix \( S = \text{diag}(s_x, s_y, s_z) \), while shear skews the point cloud using a shear matrix \( H \). The matrices are represented as:

\begin{equation}
    S = 
    \left(
    \begin{matrix}
        s_x & 0 & 0 \\
        0 & s_y & 0 \\
        0 & 0 & s_z
    \end{matrix}
    \right), \quad
    H_{xy} = 
    \left(
    \begin{matrix}
        1 & k & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{matrix}
    \right),
\end{equation}
where \( s_x \), \( s_y \), \( s_z \) are scaling factors along each axis, and \( k \) is the shear factor. These transformations adjust the cloud's dimensions and structure, enabling perspective adjustments.

\noindent\textbf{Translation.}
Translation shifts the point cloud by a constant vector \( t = (t_x, t_y, t_z) \), repositioning each point as \( p_i' = p_i + t \), aligning the observed data with the ego-centric reference frame.

Together, these transformations provide the foundation for modifying point cloud manifold in ego-centric 3D visual grounding, enabling accurate localization and orientation of objects in the observed scene.


% This transformation repositions the point cloud in space, aligning observed data with the ego-centric reference frame.

% Together, these transformations form the basis for point cloud manipulation in ego-centric 3D visual grounding tasks, enabling agents to accurately locate and orient objects within their observed scene.

% In real-world applications, intelligent agents rely on ego-centric observations to understand their surroundings without prior knowledge of the entire scene. Unlike traditional approaches that assume pre-reconstructed 3D point clouds of the scene~\citep{wu2024pointv3, huang2023segment3d}, ego-centric 3D visual grounding tasks utilize multi-view RGB-D images to detect and localize objects in real time within an unstructured environment. In this context, an essential aspect of 3D perception involves transformations applied directly to point clouds, enabling robust and adaptive scene understanding across varying perspectives.

% To facilitate accurate localization in the point cloud space, we employ a set of fundamental geometric transformations: linear transformations, which include rotation, scaling, and shear, as well as translation. These transformations enable agents to align, scale, or position point cloud data with respect to the ego-centric coordinate system, making them foundational for tasks such as bounding box regression and scene alignment.

% \subsection{Linear Transformations on Point Clouds}
% Point clouds represent a set of discrete spatial points in a 3D coordinate space. Applying linear transformations to point clouds modifies their shape, orientation, or scale, while preserving structural relationships between points. The primary transformations include:

% \paragraph{Rotation}
% Rotation involves rotating the point cloud around one or more of the coordinate axes. A rotation matrix \( R \in \mathbb{R}^{3 \times 3} \) can be constructed based on rotation angles \(\theta_x\), \(\theta_y\), and \(\theta_z\) around the \(x\)-, \(y\)-, and \(z\)-axes, respectively. For instance, a rotation around the \(z\)-axis by an angle \(\theta\) is represented as:
% \[
% R_z(\theta) = 
% \begin{bmatrix}
% \cos \theta & -\sin \theta & 0 \\
% \sin \theta & \cos \theta & 0 \\
% 0 & 0 & 1
% \end{bmatrix}.
% \]
% Applying \(R\) to a point \(p = (x, y, z)\) yields a new position for \(p\) that is rotated within the 3D space.

% \paragraph{Scaling}
% Scaling adjusts the size of the point cloud while maintaining the relative distances between points. A scaling transformation is defined by a diagonal matrix \( S = \text{diag}(s_x, s_y, s_z) \), where \(s_x\), \(s_y\), and \(s_z\) represent scaling factors along each axis. Applying \(S\) to each point \(p\) results in a uniformly or non-uniformly scaled point cloud.

% \paragraph{Shear}
% Shear transformation skews the point cloud along an axis, creating a non-orthogonal, tilted shape. This transformation is represented by a shear matrix \( H \in \mathbb{R}^{3 \times 3} \), where off-diagonal elements control the degree of shear along each axis. Shearing is particularly useful for approximating perspective distortions in 3D data.

% \subsection{Translation}
% Translation involves shifting all points within the point cloud by a constant vector \( t = (t_x, t_y, t_z) \). This is a non-linear transformation that alters the position of the entire point cloud without affecting its internal structure. The translated position of a point \( p \) is given by:
% \[
% p' = p + t.
% \]
% In the context of ego-centric 3D visual grounding, translation is essential for positioning detected bounding boxes accurately within the 3D space defined by the agent's viewpoint.

% These transformations, when applied effectively, enable intelligent agents to interpret and manipulate point clouds within ego-centric perspectives, aligning 3D object positions with the agent’s current observational context. By combining these operations, the agent can dynamically adjust point cloud data, facilitating accurate and adaptive object localization.