\section{Introduction}
\label{sec:intro}


\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/illustrate.drawio.png}
    \caption{Illustration of our main idea and SOTA results. Ground truth and reference boxes are shown in \textcolor{green}{green} and \textcolor{purple}{purple} respectively. Circular regions represent 3D areas where point cloud enhancement is applied. \textcolor{black}{Black} areas indicate regions that do not contribute to grounding performance and would increase unnecessary computation overhead. Traditional single-modality point cloud guidance would enhance these redundant areas. \textcolor{red}{Red} regions highlight areas where multimodal-guided point cloud enhancement is efficiently applied. Text modality, containing global relative position information among different critical objects, guides translation vectors for these region, while image modality, with local fine-grained semantic details, guides transformation matrices within each target regions. Our model achieves better results with reduced computation, about which details are in~\cref{table:val,tab:attn}.}
    \label{fig:illu}
\end{figure*}


%% 1. Research Task
In recent years, the field of embodied AI has gained increasing attention, spurred by 3D visual grounding benchmarks~\citep{achlioptas2020referit3d, chen2020scanrefer, wang2023embodiedscan} that have led to a surge of research~\citep{guo2023viewrefer, wu2023eda, zhao20213dvg, jain2022bottom, yang2024exploiting, huang2022multi, wang2023embodiedscan}. The 3D visual grounding task, which involves locating target objects in real-world 3D environments based on natural language descriptions, is a core perception capability for embodied agents. This ability is crucial in enabling agents to interpret and interact with their surroundings via language, which supports applications in robotics and human-computer interaction.
%For example, accurate 3D visual grounding allows robots to perform tasks like object retrieval and manipulation based on verbal commands, expanding their effectiveness in service and assistive roles.

Despite these advancements, several significant challenges continue to hinder the performance of 3D visual grounding systems. One key challenge is the limited perception of embodied agents, who rely on ego-centric observations from multiple viewpoints, often lacking a holistic, scene-level understanding. While some methods attempt to improve scene-level perception using preconstructed 3D point clouds~\citep{wu2023eda, zhao20213dvg, jain2022bottom, yang2024exploiting, guo2023viewrefer, huang2022multi}, following previous 3D perception approaches~\citep{wu2024pointv3, jiang2020pointgroup}, they are impractical in real-world applications where comprehensive scene-level information is not readily available.


% 需要介绍一下虽然之前的工作提升了语言特征以及图像特征，但是egocentric的点云 inherently has more noise due to it's dense nature.
In response, EmbodiedScan~\citep{wang2023embodiedscan} was introduced, utilizing multi-view ego-centric RGB-D scans to enable models to process scenes directly from sparse, partial viewpoints. Previous methods\cite{wang2023embodiedscan, zhu2024scanreasonempowering3dvisual, zhengdenseg,zhengdensegrounding} decouple the encoding of RGB images and depth-reconstructed point clouds from ego-centric perspectives to extract both semantic and geometric information, which are then projected into 3D space using intrinsic and extrinsic matrices to form a semantically enriched point cloud for bounding box regression. While effective, these methods overlook the noise, such as errors in depth on non-lambertian surfaces, captured by the depth sensors \cite{ikemura2024robust, tykkala2011direct, tariqul2017robust} that causes suboptimal performance. Consequently, critical geometric details of corresponding target objects are lost, potentially distorting their original manifold and further compromising the model’s grounding performance. Moreover, a substantial portion of the sampled points represents background regions, leading to a diminished density of foreground object points.

In light of such critical challenges, previous works have focused on improving point cloud structure such as point cloud denoising \cite{wang2023transformer,de2023iterativepfn,de2024straightpcf} or completion methods~\cite{boulch2022poco,leng2024point,zhang2024walkformer, yu2021pointr}. However, these methods often require extensive preprocessing the point cloud data, making them unsuitable for real-time ego-centric 3D visual grounding. Moreover, these methods rely on traditional statistical~\cite{vizzo2022make} methods or learning-based methods~\cite{luo2021score,rakotosaona2020pointcleannet} to enhance the point cloud structure in single modality, which cannot fully utilize the multimodal information available in our task.

% \TODO{Add methods that use multimodal features to imrpove PC}

Given the limitations of the above methods in ego-centric 3D visual grounding, we propose \textbf{Proxy Transformation}, which enhances point cloud manifolds before feature learning.
This approach effectively reduces redundant computation in background point cloud regions and fully leverages the available multimodal information in current context to optimize the submanifolds of target regions. Notably, our method does not require pre-trained offline reconstruction of scene-level point clouds, making it better suited for real-time ego-centric 3D visual grounding task.

Specifically, to generate transformations for point cloud submanifolds, reference points are first initialized as a uniform 3D grid for different scenes. Motivated by the success of deformable offsets in 2D domain~\cite{xia2022vision,dai2017deformable}, a 3D offset network then takes the initial point cloud clusters centered on these reference points as input, producing offsets for each reference cluster center. In this way, these deformable cluster centers shift toward critical regions. Subsequently, they serve as cluster centers in a succeeding stage to select candidate submanifolds on the original point cloud for further transformations. For each submanifold, we employ a novel generalized attention mechanism, \textbf{Proxy Attention}, to learn the corresponding transformation matrixes and translation vectors. Specific proxy tokens can be selected based on task requirements (\eg downsampling points after pooling, multi-view image features or textual features).


% These proxy tokens can be selected according to the model's representational learning needs (\eg downsampling points after pooling, multi-view image features or textual features).

In the context of online point cloud submanifold enhancement, we utilize text and image features as proxy tokens to guide submanifold transformations. In this approach, text information provides global positional relationships among different submanifolds, while image information offers local semantic details within each submanifold, as illustrated in~\cref{fig:illu}. Leveraging these designs, our method supplies the subsequent 3D backbone with a higher-quality point cloud manifold, thereby enhancing the model's effectiveness and robustness.

In summary, our contributions are as follows. (1) We propose Proxy Transformation, enabling multimodal point cloud augmentation in the context of 3D visual grounding. (2) To obtain more desirable submanifolds for target regions, we design deformable point clustering, utilizing a 3D offset network to generate flexible and adaptive deformable clusters suited to diverse scenes. (3) We introduce a generalized proxy attention mechanism, allowing the selection of different proxy tokens based on task requirements, achieving linear computational complexity. (4) Our model significantly outperforms all existing methods, achieving an impressive improvement of \textbf{7.49\%} on easy targets and \textbf{4.60\%} on hard targets, while reducing the computational overhead of attention blocks by \textbf{40.6\%}, establishing a new SOTA in ego-centric 3D visual grounding.



