\clearpage
% \setcounter{page}{1}
% \maketitlesupplementary


\section*{{\large Appendix}}



% \begin{equation}
%     B^{\rm P} = B_b + B_c + B_r
% \end{equation}

\section*{A. Additional Experiments}

\noindent \textbf{Ablation on the number of Proxy Layers.}
As shown in~\cref{tab:layers}, we conduct several experiments to determine the optimal layers of our proxy attention block for efficient learning. Based on these findings, we selected the optimal hyperparameters for our experiments. But we can see that even with just a single layer our model can achieve a SOTA performance. This demonstrates that the gain stems from leveraging multi-modal information rather than merely increasing model parameters. Thus, we can use a single-layer as final setup in practice, achieving SOTA results with minimal computational cost.



\noindent \textbf{Ablation on $s$.}
As discussed when introducing deformable offsets, \(s\) represents the maximum value of the offsets. Before generating the grid prior, we scale down the cuboid defined by the maximum and minimum coordinates of the point cloud based on \(s\), ensuring that reference points, when adjusted by deformable offsets, do not move outside the boundaries of the point cloud. As shown in~\cref{tab:max value}, we selected an optimal maximum value for \(s\). An excessively large \(s\) results in the reduced preset grid losing essential prior information, while an overly small \(s\) prevents reference points from shifting toward more critical target regions, thereby reducing the flexibility of the model.

\section*{B. Ego-Centric 3D Visual Grounding}

In real-world applications, intelligent agents interact with their surroundings without prior knowledge of the entire scene. Instead of relying on pre-reconstructed 3D point clouds or other scene-level priors commonly used in previous studies~\citep{wu2024pointv3, huang2023segment3d}, they primarily depend on ego-centric observations, such as multi-view RGB-D images.

Following the definition in \cite{wang2023embodiedscan}, we formalize the ego-centric 3D visual grounding task as follows: Given a natural language query \(L \in \mathbb{R}^T\), along with \(V\) RGB-D image views \(\{(I_v, D_v)\}_{v=1}^V\), where \(I_v \in \mathbb{R}^{H \times W \times 3}\) denotes the RGB image and \(D_v \in \mathbb{R}^{H \times W}\) represents the depth map for the \(v\)-th view, and their corresponding sensor intrinsics \(\{(K^I_v, K^D_v)\}_{v=1}^V\) and extrinsics \(\{(T^I_v, T^D_v)\}_{v=1}^V\), the goal is to predict a 9-degree-of-freedom (9DoF) bounding box \(B = (x, y, z, l, w, h, \theta, \phi, \psi)\). 

In this context, \((x, y, z)\) specify the 3D center coordinates of the target object, \((l, w, h)\) define its dimensions, and \((\theta, \phi, \psi)\) represent its orientation angles. The task is to determine \(B\) such that it accurately localizes the object described by \(L\) within the scene captured by \(\{(I_v, D_v)\}_{v=1}^V\).


\section*{C. Details about Proxy Bias}
% 正如方法中提到的，为了补偿位置信息的缺失和特征的多样性，我们提出了全新的proxy bias:
As mentioned in the methodology, to compensate for the lack of positional information and the diversity of features, we propose a novel \textbf{Proxy Bias}:
\begin{equation}
    F = F_0 + B^{\rm P},
\end{equation}
where $F \in \mathbb{R}^{N \times C}$ is the input of Proxy Block and $F_0 \in \mathbb{R}^{N \times C}$ is our deformable cluster features. $B^{\rm P} \in \mathbb{R}^{N \times C}$ is our novel proxy bias.

Initially, we set three learnable parameters $B_d \in \mathbb{R}^{N \times D \times D}$, $B_c \in \mathbb{R}^{N \times 1 \times S}$ and $B_r \in \mathbb{R}^{N \times S \times 1}$. Here, $C = S^2 = D^4$. Therefore, our parameters are way less than directly setting $B^{\rm P}$ as a learnable parameter, thus improving our parameter efficiency.

We first interpolate \( B_d \) into \( B_1 \in \mathbb{R}^{N \times S \times S} \), mapping the low-dimensional subspace into a higher-dimensional feature space to enhance feature diversity. Subsequently, we add \( B_c \) and \( B_r \) to obtain the final \( B_2 \in \mathbb{R}^{N \times S \times S} \), representing the linear union of two low-dimensional subspaces to form the final high-dimensional space, expressed as \( V = V_1 \cup V_2 \). Finally, we get $B^{\rm P} = (B_1 + B_2).{\rm reshape}(N,C)$, which can enrich the feature space with positional information and guide ProxyAttention to focus on diverse regions.

\begin{table}
    \centering
    \begin{tabular}{c |c c |c}
    \toprule
    Max & Easy &  Hard& Overall\\
    \midrule
     3 & 38.14 & 29.65 & 37.46 \\
     \textbf{4} & \textbf{41.66} & \textbf{34.38} & \textbf{41.08} \\
     5 & 36.67 & 26.71 & 35.87 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation on the Max Value of Offsets. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:max value}
\end{table}



\begin{table}
    \centering
    \begin{tabular}{c |c c |c}
    \toprule
    Layers & Easy &  Hard& Overall\\
    \midrule
     1 & 41.60 & 34.07 & 40.99 \\
     2 & 41.57 & 31.97 & 40.79 \\
     3 & 41.66 & \textbf{34.38} & \textbf{41.08} \\
     4 & \textbf{41.72} & 31.55 & 40.90 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation on the number of Proxy Layers. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:layers}
    \vspace{-0.2cm}
\end{table}