\section{Related Works}
\label{sec:related}
% 3d visual grounding may use my first writing in iclr paper
% 但是manifold reshape的文章似乎不多或者不是很相关

% 1. 3D Visual Grounding 2. vision-language interaction based on LLMs
% dataset
\noindent\textbf{3D Visual Grounding.} 3D visual grounding integrates multimodal data to localize target objects in 3D point clouds and it has gained significant attention in recent years~\citep{dionisio20133d,wang2019reinforced, feng2021cityflow}. 3D visual grounding methods are divided into one-stage and two-stage architectures. One-stage approaches~\citep{liao2020real,luo20223d,geng2024viewinfer3d,he2024refmask3d} fuse textual and visual features to generate predictions in a single step, enabling end-to-end optimization and faster inference. However, they may struggle with complex scene layouts due to the absence of explicit object proposal refinement.In contrast, two-stage approaches~\citep{yang2019dynamic,achlioptas2020referit3d,huang2022multi,guo2023viewrefer,wu2024dora,chang2024mikasa} follow a sequential process: they first utilize pre-trained object detectors~\citep{jiang2020pointgroup,wu2024dora} to generate object proposals, which are subsequently matched with the linguistic input to identify the most likely target. This separation of detection and grounding allows for more precise alignment of visual and textual features, enhancing accuracy, particularly in complex scenes, but at the cost of increased computational overhead and inference time. 

Recent studies~\citep{jain2022bottom,roh2022languagerefer,huang2022multi,Shi_2024_CVPR,chang2024mikasa} have explored transformer-based structures for 3D visual grounding. BUTD-DETR~\citep{jain2022bottom} incorporates outputs from pre-trained detection networks, including predicted bounding boxes and their corresponding class labels, as additional inputs. Multi-View Transformer~\citep{huang2022multi} projects the 3D scene into a multiview space to learn robust feature representations. MiKASA Transformer~\citep{chang2024mikasa} integrates a scene-aware object encoder and an multi-key-anchor technique, enhancing object recognition accuracy and understanding of spatial relationships. In our paper, we focus on the more challenging one-stage methods with transformer modules.  

\noindent\textbf{Point Cloud Enhancement.}
3D point clouds provide rich geometric information and dense semantic details, playing a critical role in various domains~\cite{yang2022ransacs,xiang2023multi,wang2023embodiedscan,yu2021pointr}. However, their inherent sparsity and irregularity limit model performance, and the deficiencies of existing sensors exacerbate issues such as high noise levels, sparsity, and incomplete structures in point clouds~\cite{muzahid2020curvenet,zhu2019vision}. These challenges make 3D point cloud augmentation a critical and challenging problem~\cite{quan2024deep}. Traditional point cloud enhancement methods are often based on interpolation and optimization techniques~\cite{vizzo2022make}, but their computational and memory overhead limits their applicability to large-scale datasets. Current deep learning-based point cloud augmentation methods can be broadly categorized into three main approaches: point cloud denoising, completion, and upsampling.

Point cloud denoising~\cite{zhao2022noise,chen2022repcd,wang2023transformer,de2023iterativepfn,de2024straightpcf} can eliminate noise in the point cloud while approximating and preserving the underlying surface geometry information and semantic details. Point cloud completion focuses mainly on completion for objects~\cite{chen2024learning, leng2024point, yu2021pointr,zhang2024walkformer} and scenes~\cite{xia2023scpnet,xu2023casfusionnet,wang2024semantic,zhang2023point}. And point cloud upsampling~\cite{yu2018pu,liu2022pufa,kumbar2023tp,rong2024repkpu,du2022point} aims to improve the resolution of point clouds while maintaining the integrity of the original geometric structure. 


However, these methods are still confined to a single point cloud modality and are not well-suited for tasks with high real-time requirements. In the context of ego-centric 3D visual grounding, multimodal information is available. We leverage both textual and visual modalities as guidance to transform point cloud sub-manifolds. This approach enables simultaneous point cloud denoising and densification, providing higher-quality data for subsequent processing. Additionally, our method is designed with efficiency in mind, ensuring it meets the task's real-time requirements.

% Deep learning methods for point cloud learning can be classified into three categories: projection-based, voxel-based and point-based networks. Projection-based methods~\cite{su2015multi,chen2017multi, li2016vehicle, lang2019pointpillars} first project 3D point cloud into 2D planes and then use successful 2D backbones to learn representations. However, the geometric information within point clouds is often lost during the projection phase, and the inherent sparsity of point clouds may be underutilized when dense pixel grids are formed on projection planes. Additionally, the selection of projection planes can significantly affect recognition performance, while occlusions in 3D space may further hinder accuracy. Voxel-based methods~\cite{maturana2015voxnet, song2017semantic,xu2021voxel} transform irregular and sparse point cloud to dense voxel grids, which can be processed by naive 3D convolution. But notable computation and memory costs are unacceptable when voxel resolution grows. Although 3D sparse convolution~\cite{graham20183d,choy20194d,wang2017cnn} alleviates this issue to some extent, the loss of geometric information from compressing point clouds into voxels continues to impact model accuracy. Rather than projecting or quantizing irregular point clouds onto regular grids in 2D or 3D, previous point-based networks~\cite{qi2017pointnet, qi2017pointnet++} extract features directly from the point cloud sets and get high quality representations for downstream tasks.

% Recently, pure Transformer architectures have been employed to process point clouds. PCT~\cite{guo2021pct} computes self-attention globally across the entire point cloud, resulting in poor scalability due to its quadratic computational complexity as the number of points increases. PointFormer~\cite{pan20213d} and PointTransformer~\cite{wu2022point,zhao2021point} use transformer-based architectures on each point's local neighborhood, but their efficiency is constrained by neighborhood querying and feature restructuring. And PointTransformerV3~\cite{wu2024pointv3} replaces the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns, which significantly improves processing speed and memory efficiency. However, these methods remain fundamentally constrained by the inherent sparsity and irregularity of point clouds. Additionally, in certain tasks, \eg 3D object detection and 3D visual grounding, a substantial amount of background points contributes minimally to detection performance, while incurring increased computational and memory overhead. To address this, we propose a manifold transformation on point clouds prior to feature extraction, which reduces computation on background points and optimizes the data structure of target regions.



