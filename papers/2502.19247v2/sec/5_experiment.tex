\section{Experiments}
\label{sec:experiments}
%\TODO{可以参考denseg的结构，首先讲讲强化的baseline，我们这里其实可以选取包含llm augmented language的embodiedscan作为baseline，然后再和冠军算法进行比较，具体设计可以搬运。然后是后续的消融实验，一个是模块消融，一个是gs对比，一个是block数量对比，一个是ddr对比。}
\begin{table*}
    \centering
    \begin{tabular}{c|c@{\hskip 4pt}c|c@{\hskip 2pt}c|c|c@{\hskip 4pt}c|c@{\hskip 2pt}c|c}
    % \setlength{\tabcolsep}{5pt}
    \toprule
    \multirow{2}{*}{Method}  & Easy & Hard & Indep & Dep & Overall & Easy & Hard & Indep & Dep & Overall\\
    & AP$_{25}$ & AP$_{25}$ & AP$_{25}$ & AP$_{25}$ & AP$_{25}$& AP$_{50}$ & AP$_{50}$ & AP$_{50}$ & AP$_{50}$ & AP$_{50}$ \\
    \midrule
    ScanRefer$^\dagger$~\cite{chen2020scanrefer}  & 13.78& 9.12& 13.44& 10.77&12.85 &- & -& -&- & -\\
    BUTD-DETR$^\dagger$~\cite{jain2022bottom}  & 23.12& 18.23& 22.47& 20.98&22.14 & -& - & - & - & -\\
    L3Det$^\dagger$~\cite{zhu2023object2sceneputtingobjectscontext}  & 24.01& 18.34& 23.59& 21.22&23.07 & - & - & - & - &- \\
    % EmbodiedScan$^\text{paper}$ & Full & 27.11& 20.12& 26.37& 23.42&25.72\\
    EmbodiedScan$^\dagger$  & 39.82 & 31.02 & 40.30 & 38.48 & 39.10 & 18.79  & 14.93  &  18.03 &  18.71 & 18.48\\

    EmbodiedScan~\cite{wang2023embodiedscan}  & 33.87& 30.49& 33.55& 33.61&33.59 & 14.58& 12.41& 13.92& 14.65&14.40\\
     % & Mini & 35.77 & 30.51 & 35.89 & 34.30 & 35.34\\

    DenseG~\cite{zhengdenseg} &40.17 &34.38 &38.79 &40.18 &39.70 &18.52 &\textbf{15.88} &17.47 &18.75 &18.31 \\

    \textbf{ProxyTransformation} &
    \textbf{41.66}   &
    \textbf{34.38}  &
    \textbf{41.57} &
    \textbf{40.81} & 
    \textbf{41.08}
    &
    \textbf{19.43}  &
    14.09  &
    \textbf{18.65} &
    \textbf{19.18} & 
    \textbf{19.00}\\

    \bottomrule
    \end{tabular}
    \caption{Main Results on the official validation set. The table displays accuracy performance considering instances where IoU exceeds 25\% and 50\% under different circumstance as mentioned in~\cref{sec:experiments}. 'Indep' and 'Dep' mean the targets are view-independent and view-dependent. And $^\dagger$ denotes that models are trained on full train dataset. Although our model is trained on mini train dataset (approximately 20\% of the full data), but it still surpasses previous methods trained on the full train dataset.}
    \label{table:val}
\end{table*}

\noindent\textbf{Dataset and Benchmark.}
The EmbodiedScan dataset~\citep{wang2023embodiedscan} used in our experiments is a large-scale, multi-modal, ego-centric resource tailored for comprehensive 3D scene understanding. It comprises \textbf{5,185} scene scans from widely-used datasets like ScanNet~\citep{dai2017scannet}, 3RScan~\citep{wald2019rio}, and Matterport3D~\citep{chang2017matterport3d}. This diverse dataset offers a rich foundation for 3D visual grounding, covering a broader range of scenes than previous datasets. The training set includes \textbf{801,711} language prompts, while the validation set contains \textbf{168,322} prompts, making EmbodiedScan notably larger and more challenging, thus providing a rigorous benchmark for ego-centric 3D visual grounding tasks.

\noindent\textbf{Experimental Settings.}
Due to limited resources, we train on the official mini dataset. Despite this, our model outperforms the official baseline trained on the full training set, as shown in~\cref{table:val}. For analysis experiments in~\cref{sub:analysis}, we also use the mini training and validation sets available through the official EmbodiedScan release~\citep{wang2023embodiedscan}.

Our reported accuracy uses the official IoU metric, focusing on cases where IoU exceeds 25\% and 50\%. We also assess model performance on both "Easy" and "Hard" scenes, where a "Hard" scene contains three or more target objects of the same category. The "View-Dependent" and "View-Independent" metrics further test the model’s spatial reasoning by evaluating performance with and without perspective-specific descriptions.

\noindent\textbf{Implementation Details.}
This work builds upon the strong baseline from EmbodiedScan for ego-centric 3D visual grounding. We applied several techniques to enhance the original baseline. First, we replace the RoBERTa language encoder~\citep{liu2019robertarobustlyoptimizedbert} with the CLIP encoder~\citep{radford2021learning} to achieve superior alignment between language and vision. Additionally, we incorporate Class-Balanced Grouping and Sampling (CBGS)~\cite{zhu2019class} during pretraining to address data imbalance and enhance detection accuracy across rare and common object categories. We follow EmbodiedScan’s approach by using pretrained visual encoders. Finally, we also adopt text augmented by LLM to provide richer semantic context when training, proposed by~\cite{zhengdenseg}. Moreover, our Proxy Transformation is trained using the AdamW optimizer with a learning rate of 5e-4, weight decay of 5e-4, and a batch size of 48. Training spans 12 epochs, with the learning rate reduced by 0.1 at epochs 8 and 11.


\subsection{Main Results}\label{sub:main results}
We evaluate the 3D visual grounding performance of our proposed method, Proxy Transformation, with results presented in~\cref{table:val}, comparing it against established SOTA methods from the dataset benchmark. In addition, the upper methods with $^\dagger$ are trained on the full dataset, while the lower methods are trained on a mini dataset, approximately 20\% of the full dataset.

As shown in~\cref{table:val}, Proxy Transformation achieves a notable 7.49\% and 4.60\% improvement over the previous strongest baseline, EmbodiedScan, on the Overall@0.25 and Overall@0.50. Although only trained on the mini dataset, our method even surpasses the baseline trained on the full dataset. Through deformable point clustering, our model focuses on the most crucial target regions in the scene, reducing the extra computation overhead caused by redundant point clouds and improving efficiency. Additionally, a grid prior preserves essential original spatial information, mitigating early training instability. Recognizing that text information provides global positional relationships among different submanifolds and image information offers local semantic details within each submanifold, we designed generalized proxy attention to guide local transformations of the selected point cloud submanifolds. These transformations optimize local point cloud structures for each submanifold, ultimately providing higher-quality data for subsequent feature extraction and fusion.


In summary, \textbf{Proxy Transformation} consistently outperforms the previous state-of-the-art across multiple evaluation metrics, including easy and hard samples as well as view-independent and view-dependent tasks. These consistent improvements across diverse metrics highlight not only the robustness and generalizability of our approach for 3D visual grounding tasks but also its adaptability to complex scenes and dynamic environments, making it an effective solution for advancing multimodal grounding capabilities in real-world embodied AI applications.

 

\subsection{Analysis Experiments}
\label{sub:analysis}


\begin{table}
    \centering
    \begin{tabular}{c c c|c c |c}
    \toprule
    Grid Prior & Offsets & PT & Easy& Hard& Overall\\
    \midrule
    &&&37.05 & 30.60 & 36.53 \\
     & &\checkmark & 40.39 & 32.60 & 39.76 \\
    &\checkmark&\checkmark & 40.59 & 32.18 & 39.91 \\
    \checkmark &\checkmark & \checkmark & \textbf{41.66} & \textbf{34.38} & \textbf{41.08}\\
    \bottomrule
    \end{tabular}
    \caption{Ablation of Proposed Modules. 'PT' represents 'Proxy Transformation'. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:module ablation}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{c |c @{\hskip 4pt}c |c @{\hskip 4pt}c |c}
    \toprule
    Attn & FLOPs & \#Params & Easy &  Hard& Overall\\
    \midrule
     Self & 8.36G & 2.52M & 40.71 & 33.65 & 40.14 \\
     Cross & 4.53G & 2.42M & 36.87 & 28.81 & 36.22 \\
     Proxy   & \textbf{4.97G}&2.71M & \textbf{41.66} & \textbf{34.38} & \textbf{41.08} \\
    \bottomrule
    \end{tabular}    
    \caption{Ablation on Proxy Attention. Here, we compare the vanilla self-attention block and cross-attention block with our Proxy Block. The FLOPs and parameters are computed over 3 transformer blocks with cluster features as input. We use a grid size of $12$ and a drop radio of $0.6$. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:attn}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{c |c c |c}
    \toprule
    Grid Size & Easy &  Hard& Overall\\
    \midrule
     w/o & 40.59 & 32.18 & 39.91 \\
     10 & 40.94 & 32.81 & 40.29 \\
     \textbf{12} & \textbf{41.66} & \textbf{34.38} & \textbf{41.08} \\
     14 & 41.11 & 34.28 & 40.56 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation on the size of Grid Prior. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:grid}
\end{table}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/visualization.drawio.png}
    \caption{Visualization of ground truth and predictions. Ground truth boxes are shown in \textcolor{green}{green}, baseline in \textcolor{red}{red}, and ProxyTransformation's predictions in \textcolor{violet}{violet}.}
    \label{fig:predict_anchors}
    \vspace{-0.2cm}
\end{figure*}

% \begin{table}
%     \centering
%     \caption{Ablation on Cluster Drop Radio $\beta$. A smaller \( \beta \) indicates that a greater number of submanifolds are used for subsequent transformations. The reported values are mAP for predictions greater than 25\% IoU.}
%     \begin{tabular}{c |c c |c}
%     \toprule
%     Drop Radio & Easy &  Hard& Overall\\
%     \midrule
%      0.5 & 41.30 & 34.60 & 40.76 \\
%      \textbf{0.6} & \textbf{41.66} & \textbf{34.38} & \textbf{41.08} \\
%      0.7 & 40.67 & 32.70 & 40.02 \\
%      0.8 & 39.65 & 31.02 & 38.95 \\
%      0.9 & 36.40 & 27.02 & 35.64 \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:drop radio}
% \end{table}

\noindent \textbf{Ablation on Proposed Modules.}
%我们逐个模块进行消融实验来验证他们的有效性。为了保证实验的公平性，这里的baseline是借助了implementation details中提到的方法进行增强。首先引入了不含deformable point clustering的proxy transformation， 可以看到该模型已经有了x%的巨大提升，这显示了我们的多模态点云增强的有效性和必要性。随后，我们又引入了deformable offsets，进一步获得了y%的提升，这显示出我们学习到的offsets可以帮助聚类中心向目标区域移动。最后我们引入了grid prior，该prior可以提升训练过程的稳定性，同时在hard samples上获取z%的提升，这显示出网格先验可以缓解训练初期的offsets的随机性，为模型保存基本的场景信息。
We conducted ablation studies on each module to verify their effectiveness as shown in~\cref{tab:module ablation}. To ensure fair comparisons, the baseline is enhanced with several modules described in the implementation details. First, we introduce Proxy Transformation without deformable point clustering, resulting in a substantial improvement of 3.23\%, which demonstrates the effectiveness and necessity of our multimodal point cloud enhancement. Then, we add deformable offsets, yielding an additional improvement on easy samples, indicating that offsets effectively guide cluster centers toward target regions. Finally, we incorporate grid prior, which improves training stability and achieves an increase of 2.20\% on hard samples. This result shows that the grid prior mitigates the randomness of offsets in the early stages of training, preserving essential scene information for the model.

\noindent \textbf{Ablation on Proxy Attention.}
% 我们首先验证我们提出的proxy attention的有效性与高效性，我们将vanilla block与proxy block进行对比，输入的子流形的特征是对齐的，此外正如sec中提到的proxytokens的数量是远远小于输入序列的数量。代理媒介数量的减少可以压缩注意力模块中的冗余，提高计算效率，这与实验结果结果是相符的。从表格中可以看出我们以更少的参数量与计算代价达到了相比于vanilla attention更高的表现，有xx的提升。
We validate the effectiveness and efficiency of proxy attention by comparing the vanilla attention block with the proxy block. The input cluster features are aligned across both blocks. As discussed in~\cref{sub:proxy}, the number of proxy tokens is significantly smaller than the input sequence length. This reduction in proxy tokens compresses redundancy within the attention module, enhancing computational efficiency, which aligns with our experiment results. As shown in~\cref{tab:attn}, our approach achieves higher performance than vanilla attention, while reducing the computation overhead by \textbf{40.6\%}. And we have added an additional baseline that applies cross-attention as shown in \cref{tab:attn}, which demonstrate that our proposed Proxy Attention achieves a better trade-off between accuracy performance and computational efficiency.


\begin{table}
    \centering
    \begin{tabular}{c |c c |c}
    \toprule
    Drop Radio & Easy &  Hard& Overall\\
    \midrule
     0.5 & 40.55 & 34.17 & 40.03 \\
     \textbf{0.6} & \textbf{41.66} & \textbf{34.38} & \textbf{41.08} \\
     0.7 & 40.67 & 32.70 & 40.02 \\
     0.8 & 39.65 & 31.02 & 38.95 \\
     0.9 & 36.40 & 27.02 & 35.64 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation on Cluster Drop Radio $\beta$. A smaller \( \beta \) indicates that a greater number of submanifolds are used for subsequent transformations. The reported values are mAP for predictions greater than 25\% IoU.}
    \label{tab:drop radio}
\end{table}

\noindent \textbf{Ablation on Main Hyperparameters.} 
Then, we ablate several hyperparameters in deformable point clustering and proxy transformation. As shown in~\cref{tab:grid}, the size of the grid prior also influences performance by reducing instability during the early training stages, which can result from the randomness of the initial transformations.
%This grid prior also somewhat compensates for the model's initial unfamiliarity with indoor environments, thus enhancing training stability without compromising model performance. 
%As shown in~\cref{tab:drop radio}, an excessively high drop ratio reduces model performance, highlighting the effectiveness of our submanifold transformations. A lower drop ratio retains more submanifolds for subsequent Proxy Transformation, enhancing results. However, experiments show that an overly low drop ratio also degrades performance, as point cloud enhancement should complement rather than overly alter the original structure. 
As shown in~\cref{tab:drop radio}, an excessively high drop ratio reduces model performance, highlighting the effectiveness of our submanifold transformations. A lower drop ratio retains more submanifolds for subsequent Proxy Transformation, enhancing results. However, experiments show that an overly low drop ratio also degrades performance, as point cloud enhancement should complement rather than overly alter the original structure. Moreover, further ablation studies are given in the supplementary material.
% \noindent \textbf{Ablation on different $s$}
% 最后再跑，前提是其他的实验都已经跑完了


\subsection{Qualitative Results and Discussion}
\label{sub:qualitative}
With our deformable point clustering and Proxy Transformation, the point cloud structure in target regions is optimized, providing higher-quality data for subsequent feature learning and fusion. As shown in~\cref{fig:predict_anchors}, reference objects in these regions are small and difficult to distinguish, but with the enhanced manifold structure, our model effectively captures the relationships between target and reference objects, achieving improved grounding performance. 