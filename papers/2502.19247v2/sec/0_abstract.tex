\begin{abstract}
% The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
% Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
% The abstract is to be in 10-point, single-spaced type.
% Leave two blank lines after the Abstract, then begin the main text.
% Look at previous \confName abstracts to get a feel for style and length.

Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions. A foundational task in this domain is ego-centric 3D visual grounding. However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions. Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks. We propose \textbf{Proxy Transformation} suitable for multimodal task to efficiently improve the point cloud manifold. Our method first leverages \textbf{Deformable Point Clustering} to identify the point cloud sub-manifolds in target regions. Then, we propose a \textbf{Proxy Attention} module that utilizes multimodal proxies to guide point cloud transformation. Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions. Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions. Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of \textbf{7.49\%} on easy targets and \textbf{4.60\%} on hard targets, while reducing the computational overhead of attention blocks by \textbf{40.6\%}. These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.
\end{abstract}

% 具身智能要求智能体能够根据语言指令与环境进行实时交互，这对于人机交互有着重要意义。该领域的一个基础性工作就是ego-centric 3d visual grounding。然而该任务也面临着严峻的挑战：（1）由RGB-D图像的实时渲染的点云结构经过随机采样后不仅破坏了原始的点云流形，还保存了大量的背景点云 (2)现有工作在增加参数量的同时忽视了计算量的增加，计算效率的问题有待优化。基于这些问题，我们提出了ProxyTransformation来解决这些问题，通过基于ProxyAttention的子流形变换生成模块来优化点云流形。ProxyAttention融合了自注意力与交叉注意力的优势，借助多模态proxy来aggregate点云全局信息，同时将计算复杂度降低为线性，提高了模型训练推理的效率；基于ProxyAttention我们设计了子流形变换生成模块，文本信息可以从全局角度指引不同子流形之间的平移变换，优化不同目标区域的相对位置关系，图像信息可以从局部角度指引子流形内部的线性变换，优化特定目标区域的内部点云流形。额外的实验证明了ProxyTransformation显著超越了现有的所有方法，分别在easy和hard targets上达到了x%和y%的巨大提升，超越了现有的所有方法达到了SOTA。同时我们也超越了Championship，体现了我们方法的有效性与鲁棒性。