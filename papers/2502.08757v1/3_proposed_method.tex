In this work, we propose a \gls{DL}-based precoder that takes \gls{CSI} as input and outputs a precoding matrix, designed to be adaptable to various deployment scenarios while prioritizing energy efficiency. 
To address computational challenges, we replace the costly matrix inversion in eq.~(\ref{eq:construct_beamforming_matrix}) with a \gls{DNN} trained to estimate the parameters of the precoding matrix. 
However, our experiments have shown that simple training procedures are unable to generate DNN models with the same level of performance and generalization as \gls{WMMSE}.
We address this limitation using a teacher-student training approach combined with \gls{MLDG} training to enhance generalization and reduce complexity.
%, which results in enhanced generalization and superior zero-shot performance.
%By using \gls{MLDG} and leveraging the generalization capabilities of the features provided by the \gls{WMMSE} approach, we enhance generalization and achieve superior zero-shot performance. 
This method enables efficient adaptation to new channel environments with minimal data requirements, making \gls{DL}-based \gls{mMIMO} systems more practical for real-world applications.



\subsection{Ray-Tracing Dataset}
In this work, we introduce a custom dataset designed to model channel characteristics in a \gls{mMIMO} system and evaluate the performance of the PaPP, focusing on its adaptability and efficiency in simulated real-world wireless environments. The dataset is generated using the MATLAB Ray-Tracing toolbox, simulating realistic propagation conditions with both \gls{LOS} and \gls{NLOS} components. 
Base stations are deployed in several locations in the greater Montreal area, utilizing OpenStreetMap (OSM) \cite{OpenStreetMap} data to incorporate real-world structures and materials. The propagation environment is configured to account for up to 10 reflections with no diffraction. This setup ensures the dataset accurately captures realistic multipath characteristics influenced by building structures, terrain, and other environmental factors. The training dataset encompasses diverse locations, including areas such as ``Université de Montréal'', ``Parc'', ``Rachel'', ``Cathcart'', ``Old Port'', ``Sherbrooke'', and ``Okapark'', ensuring the model generalizes effectively across different environments. In this paper, we exclude three deployment datasets from the training and use them to test the generalization of the methods: ``Ericsson'' features an industrial environment with 75\% \gls{LOS} users, ``Decarie'' is a residential area with a balanced \gls{LOS}/\gls{NLOS} mix, and ``Sainte-Catherine'', a downtown area, has 75\% \gls{NLOS} users, providing a rigorous test for model adaptability.
% For dataset construction, we selected $N_{\sf{U}}=4$ locations for each sample, where each sample is composed of $N_{\sf{U}}$ CSI vectors representing diverse user positions relative to the base station.

\subsection{Neural Network Architecture}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/ICC_2025.png}
    \caption{Proposed \gls{DNN} architecture.}
    \label{fig:arch}
\end{figure}
Our \gls{DNN} architecture for \gls{FDP} is illustrated in Figure \ref{fig:arch}. We employ a \gls{CNN} layer to extract features from the CSI, due to the larger number of antennas. The architecture is divided into three main components: \textit{Shared Feature Extraction}, \textit{Teacher Model}, and \textit{Student Model}. The teacher model contains three output layers to estimate $\mathbf{v}$, $\mathbf{u}$, and $\mu$, which are used to construct the precoding matrix using eq. (\ref{eq:construct_beamforming_matrix}).
The student model directly learns to estimate the precoding matrix under supervised training, guided by the constructed precoding matrix from the teacher model, while simultaneously maximizing the sum rate through \gls{SSL}. This architecture employs ReLU activation functions for non-linearity, along with 15\% dropout layers during backbone training to promote generalization. Batch normalization is also incorporated to improve convergence and stability, ensuring the model effectively adapts to \gls{mMIMO} environments while maintaining robust generalization performance.
% This direct estimation approach is designed to learn a mapping from the shared extracted features to the precoding matrix, enabling an energy-efficient solution for the precoding method without requiring computationally expensive matrix inverse. 



\input{files/algorithm}
\subsection{Backbone Training}
The backbone model represents the generalized precoding model, trained for efficient deployment in diverse wireless environments. The backbone training process consists of three components: a teacher-student approach for knowledge transfer, \gls{SSL} to optimize the model, and \gls{MLDG} to enhance generalization.
We summarize the PaPP algorithm in Alg.~\ref{Alg1}. 

To enhance the adaptability of the precoding solution, we incorporated \gls{MLDG} as described in \cite{10624768}. The \gls{MLDG} framework is utilized during the backbone training phase, where the model is trained on channel data from different sites to improve both in-domain performance and out-of-domain adaptability. During this meta-training process, the model is exposed to a set of training domains and a separate set of generalization domains, ensuring it can effectively generalize to previously unseen distributions during deployment. At each iteration, we randomly divide the source domain $\bs{\mathcal{D}}$ which is a set of different site datasets to construct $\bs{\mathcal{D}}^{\text{train}}$ and $\bs{\mathcal{D}}^{\text{gen}}$. This two-phase training process (meta-training and meta-testing) enables the model to generalize better in various wireless channel environments. 

By employing a teacher-student training approach \cite{hu2023teacher}, we leverage a well-performing, complex model (the teacher with parameters $\Theta$) to transfer knowledge to a simpler, more efficient model (the student with parameters $\Phi$). In this approach, the student model learns to approximate the teacher's predictions, enabling it to perform effectively with significantly reduced computational complexity. 
The teacher precoder $W_T$ is obtained by performing a single iteration of the WMMSE, but using intermediate variables generated by the teacher DNN. Re-using (\ref{eq:construct_beamforming_matrix}), we set $\mathbf{W}_T = [\mathbf{w}_1^{(1)}, \dots, \mathbf{w}_k^{(1)}, \dots, \mathbf{w}_{N_{\sf{U}}}^{(1)}]$, while $u_k^{(0)}$, $v_k^{(0)}$, $\mu$ are obtained from the teacher DNN output. 
The student model uses shared features to directly predict the precoding matrix. Lines \ref{line:8}--\ref{line:10} in Alg.~\ref{Alg1} outline the parameter update process, where \(\Theta\), \(\Pi\), and \(\Phi\) are updated for meta-testing via gradient descent on \(\bs{\mathcal{D}}^{\text{train}}\). The feature extractor is updated only during teacher training. Lines \ref{line:16}--\ref{line:18} describe the meta-update process, updating \(\Theta\), \(\Pi\), and \(\Phi\) to optimize both domains’ losses and ensure harmonious convergence.


The teacher model is trained in a self-supervised manner, using sum-rate maximization as the objective function. This training regime ensures that the teacher model is optimized to directly predict effective precoding vectors without needing extensive labeled data, thereby enhancing generalization ability while reducing training complexity. The loss function of the teacher model is given by
\begin{align}
    \mathcal{L}_{T} = - R(\mb{W}_{T}) \,.
\end{align}
For training the student model, we design a loss function that ensures the student not only mimics the teacher but also optimizes itself to achieve a higher sum-rate. The following is the loss function for the student model: 
\begin{align}
    \mathcal{L}_{S} = 
\begin{cases}
    \mathcal{L}_{\text{MSE}}                             &  \text{if $R(\mb{W}_{T}) < 0.8\times R_{\text{WMMSE}}$}, \\
    \mathcal{L}_{\text{MSE}} - \lambda  R(\mb{W}) &  \text{otherwise},
\end{cases}
\end{align}
where $R_{\text{WMMSE}}$ represents the sum rate obtained using \gls{WMMSE} \cite{shi2011iteratively}, and
\begin{align}
    \mathcal{L}_{\text{MSE}} = \frac{1}{N_{\sf{T}} N_{\sf{U}}} \sum_{i=1}^{N_{\sf{T}}} \sum_{j=1}^{N_{\sf{U}}} \left ( \mb{W}_{T,ij} - \mb{W}_{ij} \right)^2 \,.
\end{align}
% 
Parameter $\lambda$ is a regularization parameter that balances the trade-off between imitating the teacher model’s predictions and maximizing the student model's sum-rate. When the student model’s performance $R(\mb{W}_{T})$ is below 80\% of $R_{\text{WMMSE}}$, the loss focuses purely on reducing MSE. Otherwise, the term $\lambda R(\mb{W})$ encourages the student model to improve the sum-rate further. 

Once the training is complete, the model uses only the student component along with the trained feature extractor, enabling rapid precoding design with significantly reduced computational complexity.

\subsection{Deployment}
The fine-tuning process enables our model to adapt and perform effectively in real-world deployment scenarios online. During the deployment phase, the teacher portion of the original architecture is discarded, and the model operates in a zero-shot (i.e., without fine-tuning) mode or can be fine-tuned using \gls{SSL} on the entire model with varying amounts of local data. Furthermore, this local data is augmented by generating permutations of the CSI samples \cite{10624768}. This flexibility ensures that the model can adapt to new, unseen environments with minimal data, while still improving its performance with additional fine-tuning.

