\subsection{Simulation Setup}
The detailed configuration parameters of the dataset and system setup are as follows. All base stations employ a uniform planar array with $N_{\sf{T}} = 8 \times 8$ elements, each spaced at half the wavelength $(\frac{\lambda}{2})$, operating at a carrier frequency of 2\,GHz. The transmitter antenna is positioned at a height of 20\,m, and the transmitter power is set to 20\,W. System losses are modeled with a 10\,dB margin to account for practical constraints in wireless communication. To generate the dataset, a circular arrangement of user locations is created around the BS, covering distances from 50\,m to 350\,m, and users spread evenly at 10 degree intervals. Each sample in the dataset includes $N_{\sf{U}}=4$ such user locations.

Table \ref{tab:hyper} summarizes the key hyperparameters used for training the PaPP model. These hyperparameters were carefully selected based on preliminary experimentation to achieve a balance between computational efficiency and model performance, ensuring robust operation across different deployment environments.
\input{files/table_hyperparameters}

\subsection{Results}
Our experimental results show that \gls{DL}-based algorithms trained on a single site are sensitive to changes in the \gls{BS} location, resulting in performance degradation when deployed in new environments. For example, using the \gls{DL}-based method introduced in \cite{hojatian2021unsupervised}, if a precoder design model is trained in a downtown Montreal site (Old Port) and deployed to another downtown site (Sainte-Catherine), the zero-shot performance degrades by 53\% compared to when the model is trained on the same site. 

We analyze the performance of the PaPP model in comparison with \gls{ZF} \cite{nayebi2017precoding}, \gls{WMMSE} \cite{shi2011iteratively} (with stopping criteria of $10^{-3}$ and maximum number of iteration of 100), the MAML-CNN, and the single-site CNN, which is trained from scratch on the deployment site. The results are shown in Fig.~\ref{fig:sidebyside}.

At a high SNR (40\,dB), the PaPP zero-shot method consistently delivers competitive sum rates across all three locations, outperforming the WMMSE method by a margin of approximately 3--7\% and the MAML-CNN method by around 2--5\%. This suggests that while MAML-CNN leverages meta-learning principles, limitations prevent it from fully optimizing at high SNR conditions. Additionally, the PaPP trails the Single-site CNN approach by a negligible margin (<1\%). This demonstrates the outstanding adaptability of PaPP zero-shot to new sites without fine-tuning.
Meanwhile, the Z approach is consistently outperformed by the other methods, delivering rates that are approximately 15--53\% lower than the PaPP zero-shot.

 At a low SNR (10\,dB), the PaPP zero-shot method outperforms MAML-CNN by around 10--60\%, but trails WMMSE by 15--21\%, depending on the location. This suggests that while PaPP zero-shot leverages its design effectively, it requires fine-tuning on the deployment site to unlock its full potential under low-SNR conditions. The WMMSE iterative optimization outperforms all other zero-shot techniques across all sites. This highlights WMMSE's robustness in managing challenging noise-dominated scenarios.
 The PaPP zero-shot falls just 2--4\% behind the single-site CNN approach, which is a significant achievement considering it does so without any local data. This demonstrates the model's ability to effectively perform in diverse environments with zero-shot learning, showcasing its robustness even without fine-tuning.
 As expected, the ZF approach performs the worst also under low SNR conditions, delivering sum rates that are approximately 51--416\% lower than the PaPP zero-shot. This sharp decline is mainly because ZFâ€™s simplistic strategy, although useful for interference cancellation, tends to amplify noise, resulting in significantly reduced effectiveness in poor channel conditions.
le performance improvements, particularly at low SNR. 
Notably, at an SNR of 10 dB, fine-tuning PaPP with 4M samples outperforms WMMSE by approximately 5-10\%. This is a ignificant achievement since PaPP also has XY$\times$ lower computatonal complexity than WMMSE., highlighting how the utilization of more diverse training data enhances the model's generalization ability compared to single-site training. Additionally, leveraging a backbone model with limited local fine-tuning reduces the overall training complexity by enabling efficient adaptation to new sites without requiring extensive retraining. This makes the approach computationally efficient while maintaining high performance.\,1873--86, depending on the site