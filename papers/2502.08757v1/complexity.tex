\input{files/table_complexity}
Table \ref{tab:complexity} shows an example of the computational complexity of different precoding methods, measured by the number of real multiplications required for processing when the deployment site is ``Ericsson''. This analysis offers insights into the trade-offs between computational demands and sum-rate performance.

The \gls{WMMSE} algorithm is renowned for achieving good weighted sum rate in mMIMO systems. However, this performance comes at the cost of high computational complexity. Specifically, with stopping criteria of $10^{-3}$ and an average of 12.5 iterations for this setup, the WMMSE method requires approximately 36 million multiplications. This complexity can limit real-time applications or systems constrained by computational resources. The total number of real multiplications for the WMMSE method is given by
\[
 4I \bigg(\frac{2}{3} N_{\sf{T}}^3N_{\sf{U}} + N_{\sf{T}}^2N_{\sf{U}} + 2N_{\sf{T}}(2N_{\sf{U}}^2+N_{\sf{U}})+ N_{\sf{U}}^2+\frac{14}{3}N_{\sf{U}}\bigg) \, ,
\]
where $I$ is the total number of iterations. 

As another baseline, we consider the method proposed in \cite{lyu2023downlink}, which combines a multilayer perceptron (MLP) model with the WMMSE algorithm. However, we replace the original MLP model with a CNN model similar to the approach in \cite{hojatian2021unsupervised}.
This modification was made to better capture spatial features in the \gls{CSI}, enabling the model to handle the increased complexity introduced by a larger number of antennas and users. By leveraging the representational power of CNNs, MAML-CNN achieves more competitive results in our experimental settings compared to the original MAML-MLP. Since it combines a \gls{DNN} with an additional matrix inversion step, it exhibits significantly higher complexity than \gls{ZF} but remains less demanding than WMMSE. 
The number of real multiplications required by MAML-CNN is given by
\begin{align*}
 & \
C_{\text{out}}N_{\sf{T}}N_{\sf{U}}C_{\text{in}}k^2 + C_{\text{out}}N_{\sf{T}}N_{\sf{U}}(3N_{\sf{U}}+1) \\
  & + 8\bigg(\frac{4}{3} N_{\sf{T}}^3 + N_{\sf{T}}^2 (3N_{\sf{U}}+2) + N_{\sf{T}}(2N_{\sf{U}}+3)\bigg) \, ,
\end{align*}
where $C_{\text{in}}$ and  $C_{\text{out}}$ are the input and output channels of the CNN layer, and $k$  is the kernel size. 

The proposed PaPP method leverages a DNN to provide a zero-shot precoding solution. While it exhibits greater computational complexity compared to traditional methods like ZF due to its convolutional and fully connected layers, this approach offers significant performance benefits, including improved interference mitigation and adaptability to unseen sites. The total number of real multiplications required for the PaPP method can be expressed as
\begin{align*}
 & \ C_{\text{out}}N_{\sf{T}}N_{\sf{U}}C_{\text{in}}k^2 + C_{\text{out}}N_{\sf{T}}N_{\sf{U}}D_{\text{FC1}} \\
& +D_{\text{FC1}}D_{\text{FC2}} + D_{\text{FC2}}D_{\text{FC3}} + D_{\text{FC3}}D_{\text{FC4r}} + D_{\text{FC3}}D_{\text{FC4i}}\, ,
\end{align*}
where $D_{\text{FCi}}$ ($i=1,2,3,4$) represent the sizes of the \glspl{FCL}. 

The \gls{ZF} \cite{nayebi2017precoding} precoding method offers significantly lower computational complexity, requiring approximately 8.4 thousand multiplications. This efficiency stems from its reliance on simpler linear algebra operations, specifically the inversion of smaller matrices when $N_{\sf{T}}>N_{\sf{U}}$. Despite its computational efficiency, ZF is susceptible to performance degradation in high-interference scenarios or under adverse channel conditions. The total number of real multiplications required by ZF is
\[
8 N_{\sf{U}}^2 N_{\sf{T}} + \frac{8}{3} N_{\sf{U}}^3 \, .
\]