\section{Methodology}
\label{sec:dataset}

Creating a high-quality dataset for inpainting detection requires addressing three key challenges: ensuring semantic coherence in manipulations, maintaining diversity in inpainting approaches, and assessing the realism of generated images. To address these challenges, we propose a systematic methodology that carefully considers each aspect of the dataset creation process. Our approach combines language models for contextual understanding, multiple inpainting models for diverse manipulations, and vision-language models for quality assessment. This methodology consists of three main components: (1) Semantically Aligned Object Replacement (SAOR) that ensures contextually appropriate manipulations (\Cref{sec:SAOR}), (2) Multi-Model Image Inpainting (MMII) that uses various state-of-the-art models for diverse modifications (\Cref{sec:mmii}), and (3) Uncertainty-Guided Deceptiveness Assessment (UGDA) that evaluates the realism of generated images (\Cref{sec:ugda}). \Cref{fig:SAOR_mmpi} shows the workflows of SAOR and MMII,  while \Cref{fig:ugda_fig} illustrates the workflow of UGDA.


\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/saor_mmii_v2.png}  % Adjust width as needed
    \caption{Overview of SAOR and MMII. The pipeline begins with an input image. The segmentation model $\Phi_{\text{seg}}$ performs instance segmentation, generating a list of objects. The captioning model $\Psi_{\text{cap}}$ produces a caption for the image. The language model $\Theta_{\text{llm}}$ then uses the caption and object list to select an object and generate a prompt. Finally, the inpainting model $\Gamma_j$ takes the image, the prompt, and a mask corresponding to the selected object to produce the inpainted result.}
    \label{fig:SAOR_mmpi} 
\end{figure*}
\setlength{\textfloatsep}{12pt}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/ugre_v2.png}  % Adjust width as needed
    \caption{Overview of UGDA. Given an inpainted image $I_i'$, UGDA first evaluates its realism using $\Omega_{vlm}$. If deemed realistic, UGDA performs a two-way comparison with the original image $I_i$ to assess potential deceptiveness.}
    \label{fig:ugda_fig} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Semantically Aligned Object Replacement (SAOR)}
\label{sec:SAOR}

Generating realistic inpainting examples requires selecting appropriate objects for replacement and creating contextually relevant prompts that maintain semantic consistency with the image. To address this challenge, we propose Semantically Aligned Object Replacement (SAOR), a method that automates object selection and prompt generation.

Given an initial set of authentic images $\mathcal{I}$, SAOR processes each image through three stages. First, given $I_i \in \mathcal{I}$, object masks $M_i$ and their respective labels $O_i$ are obtained directly from labeled data if available; otherwise, a segmentation model $\Phi_\text{seg}: I_i \rightarrow (O_i, M_i)$ is used. Second, descriptive captions $C_i$ are taken directly from labeled data if available; otherwise, they are generated by a captioning model $\Psi_\text{cap}: I_i \rightarrow C_i$. Finally, a language model $\Theta_\text{llm}: (C_i, O_i) \rightarrow (p_i, o_i)$ selects an object $o_j \in \mathcal{O}_i$ (in text form) and generates a text prompt $p_i$ to replace it. The language model, leveraging the caption to understand the image content, is instructed to produce complex, contextually appropriate prompts, which enhance results \cite{rosenman2024neuropromptsadaptiveframeworkoptimize, manas2024improvingtexttoimageconsistencyautomatic}, maintaining semantic consistency with the surrounding content.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-5pt}
\subsection{Multiple Model Image Inpainting (MMII)}
\label{sec:mmii}

The quality and diversity of inpainted images are crucial for creating a robust dataset for forgery detection. To achieve this, we employ multiple state-of-the-art inpainting models and various post-processing techniques, ensuring a wide range of realistic modifications.

Once the object $o_i \in O_i$ has been selected, with its corresponding mask $m_j \in M_i$, and the generated prompt $p_i$, we proceed to generate the inpainted image $\hat{I}_i$. We employ a set of state-of-the-art inpainting models $\Gamma = {\Gamma_1, ..., \Gamma_N}$. Each inpainting model $\Gamma_j \in \Gamma$ receives an equal number of images for processing, with uniformly distributed settings such as diffusion models and post-processing techniques. Each model takes the original image $I_i$, the mask $m_i$, and the prompt $p_i$ as inputs, producing the inpainted image as $\hat{I}_i = \Gamma_j(I_i, m_i, p_i)$. For diversity, a subset of images undergoes $K$ additional rounds of inpainting using different models from $\Gamma$. The original, unprocessed images are retained for model training.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Uncertainty-Guided Deceptiveness Assessment (UGDA)}
\label{sec:ugda}

The quality of inpainted images must be assessed to create a meaningful dataset for forgery detection. To this end, we propose Uncertainty-guided Deceptiveness Assessment (UGDA), a process leveraging a Vision-Language Model (VLM) that compares inpainted images with their originals. Let $I_i$ denote the original image and $\hat{I}_i$ the inpainted version of $I_i$. Using a vision-language model $\Omega_\text{vlm}$, we perform the assessment in two stages.

Based on empirical observations, unrealistic manipulations were easily identifiable, while realistic ones required more thorough evaluation. $\hat{I}_i$ undergoes the first UGDA stage where we ask the VLM if the image is realistic. For images passing the initial realism check, we introduce a comparative assessment leveraging two key insights: (1) when presented with both images, a VLM should more easily identify authentic content, so images where it fails to do so are likely more deceiving, and (2) a confident assessment should remain stable under input perturbations. A perturbation introducing variance to the assessment was input order. Thus, we perform two evaluations with reversed image order: $s_1 = \Omega_\text{vlm}(I_i, \hat{I}_i)$ and $s_2 = \Omega_\text{vlm}(\hat{I}_i, I_i)$, where $s_1, s_2 \in \{I_i, \hat{I}_i, \text{both}\}$. Any variation in responses indicates model uncertainty about $\hat{I}_i$'s realism.

The final classification follows these rules: $\hat{I}_i \text{ is labeled as } \textit{deceiving} \text{ if } (s_1 = \hat{I}_i \vee s_2 = \hat{I}_i) \vee (s_1 = s_2 = \textit{both})$. Otherwise, if both responses indicate $I_i$ or one indicates $I_i$ and the other \textit{both}, $\hat{I}_i$ is labeled as \textit{undeceiving}. This approach enhances reliability in distinguishing between deceiving and easily identifiable synthetic images through uncertainty-aware, order-based evaluations. \Cref{fig:deceptive_dataset_examples} shows representative examples from UGDA across different sources. The first row demonstrates high-quality inpainting examples classified as deceiving by UGDA, where the manipulations are seamlessly integrated with the original content. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}
\label{sec:datasource}

\paragraph{Source of authentic data.}
To ensure diversity and robustness, we leverage datasets spanning multiple domains: general object detection, high-resolution photography, and large-scale segmentation. Specifically, we utilize three publicly available datasets for authentic image sources:
(1) MS-COCO \cite{lin2014coco}, which provides images with captions and object masks across 80 categories
(2) RAISE \cite{dang2015raise}, a high-resolution dataset of 8,156 uncompressed RAW images designed for forgery detection evaluation
(3) OpenImages \cite{benenson2022openimages}, which offers extensive object segmentation data with over 2.7 million segmentations across 350 categories.
\vspace{-14pt} 
\paragraph{SAOR configuration.}
For SAOR, we use dataset-provided masks and captions for COCO and OpenImages, while for RAISE, we use OneFormer \cite{jain2022oneformertransformerruleuniversal} as the segmentation model $\Phi_\text{seg}$ and BLIP-2 \cite{li2023blip2bootstrappinglanguageimagepretraining} as the captioning model $\Psi_\text{cap}$. For prompt generation, we use ChatGPT 3.5 \cite{openai2023chatgpt3.5} (for COCO/RAISE) and Claude Sonnet 3.5 \cite{claude2024sonnet} (for OpenImages) as the language model $\Theta_\text{llm}$. The prompt engineering methodology is detailed in the supplementary material.
\vspace{-14pt} 
\paragraph{MMII configuration.}
For MMII, we utilize five inpainting pipelines ($\Gamma$): HD-Painter \cite{manukyan2024hdpainter}, BrushNet \cite{ju2024brushnetplugandplayimageinpainting}, PowerPaint \cite{zhuang2024powerpaint}, ControlNet \cite{zhang2023addingconditionalcontroltexttoimage}, and Inpaint-Anything \cite{yu2023inpaintanythingsegmentmeets}, along with its Remove-Anything variant for object removal. These pipelines collectively support eight inpainting models, primarily based on Stable Diffusion \cite{rombach2022stablediffusion}, except for Remove-Anything which employs LaMa architecture \cite{suvorov2021lama} based on CNNs and Fourier convolutions. Due to memory constraints, images were resized to a maximum dimension of 2048 pixels, except for the Inpaint Anything pipeline, which preserved original dimensions using cropping and resizing techniques. We perform $K=2$ additional inpainting rounds on one-sixth of the images. 
\vspace{-14pt} 
\paragraph{Preservation of unmasked area.}
We categorize inpainted images based on how models handle unmasked regions: if the unmasked region is preserved, we refer to them as Spliced (SP) images; if regenerated, as Fully Regenerated (FR) images. Inpaint-Anything preserves unmasked regions through copy-paste (SP), while ControlNet regenerates the full image (FR). BrushNet, PowerPaint, and HD-Painter can produce both SP and FR images depending on post-processing settings (e.g., blending or upscaling). Remove-Anything, based on LaMa architecture, inherently preserves unmasked regions, thus producing SP images. This diversity in processing approaches contributes to a more comprehensive dataset, as FR images are typically more challenging to detect than SP images \cite{tailanian2024diffusionmodelsmeetimage}.
\vspace{-14pt} 
\paragraph{UGDA configuration.}
For UGDA, we use GPT-4o \cite{openai2023chatgpt4} as the vision-language model $\Omega_\text{vlm}$, chosen for its effectiveness in synthetic image detection \cite{ye2024lokicomprehensivesyntheticdata}. The complete prompt engineering methodology is detailed in the supplementary material. Based on empirical observations that higher QAlign scores correlate with better inpainting quality and realism, we applied UGDA to approximately half of the test inpainted images, selecting those with the highest QAlign scores. The complete prompt engineering methodology is detailed in the supplementary material.
\vspace{-14pt} 
\paragraph{Dataset splits.}
As shown in Table \ref{tab:dataset_splits}, we structure our dataset to evaluate both in-domain performance and generalization to new data. For in-domain evaluation, we use COCO (60,000 randomly selected training images and nearly all 5,000 validation images for validation and testing) and RAISE (7,735 images processed with $\Phi_\text{seg}$, yielding 25,674 image-mask-model combinations through 1-7 masks or prompts per image, with derived images kept in the same split, as each image was inpainted up to 4 times only in this dataset). To test generalization, we create an out-of-domain testing split using OpenImages—a dataset not used during training—comprising 6,000 randomly selected test images. This split uses a different language model $\Theta_\text{llm}$ (Claude) than COCO and RAISE (ChatGPT), providing a way to evaluate how well models perform on both new data and different prompting approaches. Throughout our experiments, we refer to the COCO and RAISE test splits as in-domain and the OpenImages test split as out-of-domain.

\begin{table}[!htbp]
\centering
\begin{tabular}{lccc}
\hline
 & Training & Validation & Testing \\
\hline
\multirow{2}{*}{COCO \cite{lin2014coco}} & 59,708 & 1,950 & 2,922 \\
 & (75\%) & (31\%) & (29\%) \\
\multirow{2}{*}{RAISE \cite{dang2015raise}} & 19,741 & 4,262 & 1,671 \\
 & (25\%) & (69\%) & (16\%) \\
\multirow{2}{*}{OpenImages \cite{benenson2022openimages}} & \multirow{2}{*}{\textit{N/A}} & \multirow{2}{*}{\textit{N/A}} & 5,585 \\
 & & & (55\%) \\
 \hline
Inpainted & 79,449 & 6,212 & 10,178 \\
Authentic & 79,449 & 6,212 & 9,071 \\
\hline
\end{tabular}
\caption{Overview of dataset splits across COCO, RAISE, and OpenImages. The table shows the number of images in each split. The total number of images, including authentic and inpainted versions, is provided. Percentages represent the distribution of each dataset within the total split for inpainted images.}
\label{tab:dataset_splits}
\end{table}

\vspace{-10pt}