\section{Experimental Evaluation}
\label{sec:experiments}
%\vspace{-5pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental Setup}

To establish a comprehensive benchmark for the presented \emph{\datasetname} dataset, we evaluate the performance of several state-of-the-art image inpainting detection models. 
\vspace{-14pt} 
\paragraph{Problem definition.} Given an RGB image \( x^{rgb} \in \mathbb{R}^{(H \times W \times 3)} \), the inpainting detection model aims to predict either a pixel-level inpainting localization mask \( \hat{y}^{loc} \in (0,1)^{(H \times W \times 1)} \) and/or an image-level inpainting detection probability \( \hat{y}^{det} \in (0,1) \). The former will be referred to as a localization problem, and the latter as a detection problem. All evaluations are conducted on the test sets.
\vspace{-14pt}
\paragraph{Forensics models.} For inpainting detection, we used four state-of-the-art models: PSCC \cite{luy2022pscc}, CAT-Net \cite{kwon2022catnet}, TruFor \cite{guillaro2023trufor}, and MMFusion \cite{triaridis2023mmfusion}. CAT-Net provides only a pixel-level localization mask, while the other models also output an image-level detection probability. For CAT-Net, image-level detection was evaluated by taking the maximum probability from the predicted localization mask.
\vspace{-14pt} 
\paragraph{Training protocol.} We evaluated both pretrained models and versions retrained on the \emph{\datasetname} dataset. This allowed us to compare the performance of the original models with their retrained counterparts. %Unless stated otherwise, model performance refers to the retrained models.
\vspace{-14pt} 
\paragraph{Implementation Details.} We retrained all models from scratch, following the training protocol outlined in their original papers. CAT-Net was trained on an NVIDIA A100 GPU, while PSCC, TruFor, and MMFusion were trained on an NVIDIA RTX 4090. Models were trained on the training set of \emph{\datasetname} and the best checkpoints were selected based on validation set performance. 
\vspace{-14pt} 
\paragraph{Evaluation metrics.} We used distinct metrics to assess model performance at the image and pixel levels. For image-level evaluation (detection), we use accuracy to classify images as inpainted or not, with the positive class referring to inpainted regions. For pixel-level evaluation (localization), Intersection over Union (IoU) was used to measure the accuracy of localizing inpainted regions. In both cases, the threshold was arbitrarily set at $0.5$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Localization and Detection Results}

We evaluate \emph{\datasetname} by comparing the performance of four state-of-the-art inpainting detection models: PSCC, CAT-Net, TruFor, and MMFusion. We assess both pre-trained models and versions retrained on our dataset. Table \ref{tab:performance_comparison} presents both the localization (IoU) and detection (accuracy) results for in-domain and out-of-domain testing sets and spliced (SP) and fully regenerated (FR) images. 

\begin{table*}[!htb]
\centering
\setlength{\tabcolsep}{1.2mm}
\scalebox{1}{
\begin{tabular}{lcccccccccccccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{8}{c}{Mean IoU} & \multicolumn{8}{c}{Accuracy} \\
& ID &  & OOD &  & SP &  & FR &  & ID &  & OOD &  & SP &  & FR &  \\
\cmidrule(lr){1-1} \cmidrule(lr){2-9} \cmidrule(lr){10-17}
CAT-Net & 0.39 &  & 0.21 &  & 0.41 &  & 0.06 &  & 0.60 &  & 0.42 &  & 0.85 &  & 0.40 &  \\
CAT-Net† & \textbf{0.69} & {\footnotesize +77\%} & 0.23 & {\footnotesize +10\%} & 0.47 & {\footnotesize +15\%} & 0.36 & {\footnotesize +500\%} & \textbf{0.99} & {\footnotesize +65\%} & 0.53 & {\footnotesize +26\%} & \textbf{1.00} & {\footnotesize +18\%} & \textbf{1.00} & {\footnotesize +150\%} \\
PSCC & 0.35 &  & 0.24 &  & 0.20 &  & 0.09 &  & 0.60 &  & 0.53 &  & 0.40 &  & 0.33 &  \\
PSCC† & 0.58 & {\footnotesize +66\%} & 0.58 & {\footnotesize +142\%} & 0.43 & {\footnotesize +115\%} & 0.20 & {\footnotesize +122\%} & 0.63 & {\footnotesize +5\%} & 0.72 & {\footnotesize +36\%} & 0.50 & {\footnotesize +25\%} & 0.36 & {\footnotesize +9\%} \\
MMFusion & 0.20 &  & 0.19 &  & 0.46 &  & 0.17 &  & 0.62 &  & 0.63 &  & 0.63 &  & 0.26 &  \\
MMFusion† & 0.64 & {\footnotesize +220\%} & 0.41 & {\footnotesize +116\%} & 0.73 & {\footnotesize +59\%} & 0.50 & {\footnotesize +194\%} & 0.93 & {\footnotesize +50\%} & 0.81 & {\footnotesize +29\%} & 0.86 & {\footnotesize +37\%} & 0.82 & {\footnotesize +215\%} \\
TruFor & 0.12 &  & 0.19 &  & 0.40 &  & 0.20 &  & 0.58 &  & 0.60 &  & 0.41 &  & 0.13 &  \\
TruFor† & 0.65 & {\footnotesize +442\%} & \textbf{0.66} & {\footnotesize +247\%} & \textbf{0.87} & {\footnotesize +117\%} & \textbf{0.72} & {\footnotesize +260\%} & 0.96 & {\footnotesize +66\%} & \textbf{0.93} & {\footnotesize +55\%} & 0.95 & {\footnotesize +132\%} & 0.94 & {\footnotesize +623\%} \\
\hline
\end{tabular}
}
\caption{Performance comparison of image forensics methods across domains. Evaluation metrics include Mean IoU and Accuracy for in-domain (ID), out-of-domain (OOD), SP, and FR images. Methods marked with † are retrained models, while unmarked ones are original. Bold values indicate column maxima, and percentages show retraining improvements.}
\label{tab:performance_comparison}
\end{table*}

Retraining on our dataset improves performance significantly. TruFor's IoU increases from 0.12 to 0.65 for in-domain and 0.19 to 0.66 for out-of-domain testing. CAT-Net shows similar in-domain gains (0.39 to 0.69) but limited out-of-domain improvement (0.21 to 0.23). Domain generalization varies across models. While retrained CAT-Net achieves the highest in-domain IoU (0.69), it drops to 0.23 for out-of-domain. In contrast, retrained TruFor and PSCC maintain consistent performance across domains (0.65/0.66 and 0.58/0.58 respectively). The disconnect between IoU and accuracy metrics, particularly evident in the retrained CAT-Net's results, suggests that high accuracy does not necessarily translate to precise localization of manipulated regions. SP detection is generally easier than FR, with retrained TruFor reaching 0.87 IoU on SP tasks. For FR images, most original models struggle (IoUs 0.06-0.20) but show clear improvements after retraining, with TruFor reaching 0.72 IoU. This suggests that detecting fully regenerated regions remains challenging, even for retrained models. TruFor’s strong performance likely stems from its pretrained NoisePrint++ network, which enhances its ability to detect diverse manipulations. In contrast, CAT-Net and MMFusion may struggle with generalization due to their initial design for specific artifacts (e.g., JPEG compressions) and added input complexities, respectively, which increase susceptibility to overfitting.

\subsection{Model Compression Robustness Analysis}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/compression_metrics.png}
    \label{fig:compression_metrics}
    \caption{Robustness of model detection performance under compression. Top row shows model detection performance when subjected to JPEG compression at varying quality levels, while bottom row shows detection performance under WEBP compression.} 
    \label{fig:compression_det}
\end{figure}


We evaluate model robustness against JPEG and WEBP image compression at quality levels $0.85$, $0.7$, and $0.5$. Figure \ref{fig:compression_det} presents detection and localization results. Retrained TruFor shows the strongest resilience, maintaining stable performance across quality levels for both compression types. WEBP compression affects performance more than JPEG, particularly for localization tasks. All models show higher degradation in IoU scores compared to accuracy metrics, indicating that manipulation localization is more sensitive to compression artifacts than detection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Studies}

We conduct two ablations to validate our design choices. First, we evaluate SAOR's use of language models by comparing LLM-generated prompts against simple object labels. Second, we assess UGDA's effectiveness through a human study with 42 participants on 1,000 images, comparing human perception against model performance on images classified as \textit{deceiving} or \textit{undeceiving}.
\vspace{-14pt} 
\paragraph{Towards using language models.} 

\begin{table*}[!htb]
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{lcccccccccccccc}
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{CS $\uparrow$} & \multirow{2}{*}{QA Qlt $\uparrow$} & \multirow{2}{*}{QA AE $\uparrow$} & \multirow{2}{*}{AS $\uparrow$} & \multicolumn{2}{c}{PSNR $\uparrow$} & \multicolumn{2}{c}{LPIPS $\times 10^3$ $\downarrow$} & \multicolumn{2}{c}{MSE $\times 10^3$ $\downarrow$} & \multicolumn{2}{c}{MAE $\times 10^3$ $\downarrow$} & \multicolumn{2}{c}{SSIM $\uparrow$} \\
& & & & & FR & SP & FR & SP & FR & SP & FR & SP & FR & SP \\
\hline
Object Labels & 0.33 & 4.16 & 2.66 & 5.56 & \textbf{27.37} & 104.89 & \textbf{37.82} & 0.56 & 5.92 & 0.01 & \textbf{37.29} & \textbf{0.85} & 0.85 & 1.00 \\
LLM prompts & \textbf{0.62} & \textbf{4.17} & \textbf{2.73} & \textbf{5.74} & 27.02 & \textbf{105.83} & 40.28 & \textbf{0.55} & \textbf{5.69} & 0.01 & 38.41 & 0.86 & 0.85 & 1.00 \\
\hline
CocoGlide & -1.27 & 2.88 & 1.79 & 5.40 & - & \textbf{61.27} & - & \textbf{0.07} & - & \textbf{0.00} & - & 1.06 & - & \textbf{1.00} \\
TGIF & -0.69 & 3.94 & 2.53 & 5.65 & 14.41 & 61.09 & 289.55 & 0.32 & 60.43 & 0.05 & 173.97 & \textbf{0.15} & 0.53 & 1.00 \\
\emph{\datasetname} (ours) & \textbf{0.27} & \textbf{4.06} & \textbf{2.84} & \textbf{5.69} & \textbf{25.79} & 48.66 & \textbf{44.24} & 2.65 & \textbf{5.08} & 0.05 & \textbf{41.16} & 2.36 & \textbf{0.81} & 1.00 \\
\hline
\end{tabular}
\caption{Comparison of datasets based on quality, aesthetics, and fidelity metrics. The first two rows compare inpainting results when using prompts from LLMs versus directly using object labels under the same base settings. The last three rows compare our full dataset against existing datasets TGIF and Cocoglide. Metrics include quality assessments (QAlign Quality, QAlign Aesthetic, Aesthetic Score) and fidelity measures (PSNR, LPIPS, MSE, MAE, SSIM), with separate columns for FR and SP images.}
\label{tab:metrics_v2}
\vspace{-10pt}
\end{table*}


\begin{table}[!htb]
\centering
\setlength{\tabcolsep}{0.9mm}
\begin{tabular}{lcccccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Accuracy} & \multicolumn{4}{c}{Mean IoU} \\
& All & Dec. & Int. & Und. & All & Dec. & Int. & Und. \\
\hline
Human & 0.67 & 0.35 & 0.60 & 0.74 & 0.15 & 0.13 & 0.28 & 0.40 \\
PSCC & 0.52 & 0.30 & 0.38 & 0.29 & 0.29 & 0.15 & 0.14 & 0.14 \\
CAT-Net & 0.50 & 0.70 & 0.59 & 0.58 & 0.29 & 0.29 & 0.14 & 0.15 \\
PSCC† & 0.74 & 0.50& 0.50 & 0.54 & 0.63 & 0.35 & 0.30 & 0.43 \\
TruFor & 0.60 & 0.35 & 0.25 & 0.22 & 0.17 & 0.35 & 0.25 & 0.27 \\
MMFus & 0.62 & 0.49 & 0.34 & 0.36 & 0.19 & 0.38 & 0.31 & 0.27 \\
CAT-Net† & 0.69 & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & 0.44 & 0.46 & 0.45 & 0.53 \\
MMFus† & 0.88 & 0.89 & 0.91 & 0.92 & 0.51 & 0.66 & 0.70 & 0.72 \\
TruFor† & \textbf{0.95} & 0.99 & \textbf{1.00} & \textbf{1.00} & \textbf{0.68} & \textbf{0.87} & \textbf{0.87 }& \textbf{0.89} \\
\hline
\end{tabular}
\caption{Human vs. model performance comparison on inpainting detection. Results show accuracy and IoU for full test set (All) and images classified by UGDA as Deceiving (Dec.) or Undeceiving (Und.). † indicates models retrained on our dataset. Bold values indicate best performance per column.}
\label{tab:human_comparison}
\end{table}

\begin{table*}[!htb]
\centering
\setlength{\tabcolsep}{0.9mm}
\begin{tabular}{lccccccccc r}
\hline
\multirow{2}{*}{Model} & Orig. & Original & Inp. & Inp. & Inp. & Double & Human & Out-of-Domain & \multirow{2}{*}{Type} & \multirow{2}{*}{Resolution}\\
 & Datasets & Images & Images & Models & Pipes & Inp. & Benchmark & Test Set &  &  \\
\hline
CocoGlide & 1 & 512 & 512 & 1 & \ding{55} & \ding{55} & \ding{55} & \ding{55} & AIGC & $256\times256$\\
TGIF & 1 & 3,124 & 74,976 & 3 & \ding{55} & \ding{55} & \ding{55} & \ding{55} & AIGC & up to $1024p$ \\
\datasetname (ours) & 3 & 77,900 & 95,839 & 8 & 5 & \checkmark & \checkmark & \checkmark & AIGC/OR & up to $2048p$* \\
\hline
\end{tabular}
\vspace{-4pt}
\caption{Comparison of Inpainting Dataset Characteristics. Our dataset surpasses existing ones in scale (number of images), diversity (source datasets, models, pipelines). Resolution varies based on source dataset. The "Human Benchmark" column indicates whether a dataset includes a subset for human evaluation, while the "Out-of-Domain Test Set" column specifies whether the dataset contains images from distribution shifts for robustness assessment.}
\label{tab:cocoglide_vs_tgif_vs_ours}
\vspace{-8pt}
\end{table*}

%Are LLM's prompts necessary? 
To justify the use of LLM-generated prompts in SAOR over simpler object label-based prompts, we processed a total of 900 original images (300 from each dataset), generating 4,500 images (900 from each of the five inpainting models) using LLM-generated prompts, and another 4,500 images using object class labels as prompts. We evaluated the aesthetics and quality of the generated images using metrics designed to align with human perception, including CLIP Similarity for Image Aesthetics \cite{hentschel2022clipimageaeshetics} (referred to as CS, with "outstanding" and "atrocious" as positive and negative prompts), QAlign for Quality Assessment (QA qtl) and Aesthetics Assessment (QA AE) \cite{wu2023qalignteachinglmmsvisual}, and Aesthetic Score \cite{schuhmann2022laion5bopenlargescaledataset}. In contrast, fidelity metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonableeffectivenessdeepfeatures} assess the preservation of the non-inpainted area. Fidelity metrics are most meaningful for FR images, whereas for SP images, where the compared areas are nearly identical, they provide limited insight. The results, presented in the first two rows of \Cref{tab:metrics_v2}, show that, on average, the images generated using LLM prompts yielded consistently higher aesthetic scores compared to those generated using object class labels, with benefits that solely only from prompt variations. Since all other settings remain unchanged (masks, inpainting models) technical metrics nearly identical between the two cases. This demonstrates the advantage of using LLMs for generating semantically rich and context-aware prompts.
% \vspace{-6pt} 
\paragraph{UGDA human evaluation.}
To validate UGDA's effectiveness and establish a human baseline, we conducted a comprehensive user study comparing human perception with model detection capabilities. We created a balanced evaluation set of 1,000 images: 250 classified as deceiving by UGDA, 250 as undeceiving, and 500 unaltered images as control. Images were randomly selected with constraints to avoid redundancy: no overlap between authentic images and originals of inpainted images, and no multiple inpainted versions from the same source. The study involved 42 participants evaluating batches of 20 images, with each image receiving 3-5 independent assessments. Participants included 26 males, 6 females, and 10 undisclosed. Ages ranged from 18 to 65+, with the largest group being 18-24 (19), followed by 25-34 (8) and 35-44 (6). Users were asked to detect inpainting and draw bounding boxes around suspected manipulated regions. For IoU computation, ground truth masks were converted to bounding boxes.

Table \ref{tab:human_comparison} presents the comparison between human evaluators and automated models. Human performance reached 0.69 accuracy and 0.15 IoU, significantly lower than retrained models like TruFor (0.95 accuracy, 0.68 IoU) and MMFusion (0.88 accuracy, 0.51 IoU). Results are broken down into four categories: \textit{All} represents performance on the complete test set, while \textit{Deceiving}, \textit{Undeceiving} and \textit{Intermediate} correspond to UGDA's classification of images based on their potential to fool human perception. The \textit{Intermediate} category includes images that passed the initial realism check but not the second. Users particularly struggled with deceiving images (0.35 accuracy, 0.13 IoU) compared to undeceiving ones (0.74 accuracy, 0.40 IoU), validating UGDA's effectiveness in identifying manipulations that are challenging for human perception. Also, the performance of humans on the intermediate category (0.60 accuracy, 0.28 IoU) confirms that the second stage is indeed efficient in discarding images that are not truly deceiving. In contrast, retrained models maintain high performance even on these challenging cases, with TruFor achieving 0.99 accuracy and 0.87 IoU on deceiving images. The performance gap between humans and models emphasizes the importance of automated detection methods, particularly for high-quality inpainting that can bypass human perception.
\vspace{-14pt} 
\paragraph{Quantitative comparison with state-of-the-art.} 
To the best of our knowledge, \emph{\datasetname} is the largest collection of AI-generated inpainted images. Table \ref{tab:cocoglide_vs_tgif_vs_ours} presents a thorough comparison between our dataset and existing inpainting datasets in the field of AI-generated image detection.   With 95,839 inpainted images derived from 77,900 originals, our dataset surpasses TGIF (74,976 from 3,124) and CocoGlide (512 from 512) in scale. Unlike existing datasets relying on a single source, ours integrates COCO, RAISE, and OpenImages, enhancing diversity. It also employs eight inpainting models across five pipelines, enabling complex manipulations, including double inpainting. Supporting resolutions up to 2048p, it exceeds TGIF’s 1024p and CocoGlide’s $256 \times 256$. Additionally, it uniquely includes a human benchmark subset and an out-of-domain test set, reinforcing its value for evaluating inpainting detection under diverse and challenging conditions. As shown in Table \ref{tab:metrics_v2}, our dataset demonstrates superior performance in aesthetic, quality, and fidelity metrics compared to existing datasets. This indicates that our dataset achieves higher perceptual alignment with human judgment (CS, QA Alt, QA AE, AS) and better preservation of the non-inpainted areas (PSNR, LPIPS, MSE, MAE, SSIM) in FR cases, where it is most relevant. By combining AI-generated image content (AIGC) and object removal (OR) techniques, our dataset offers a comprehensive benchmark that supports inpainting detection across a wider range of manipulation scenarios. This approach positions our dataset as a superior resource for advancing inpainting detection models and sets a new standard for benchmarking in the field.
% \vspace{-4pt}




