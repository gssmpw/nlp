\section{Related Work}
\label{sec:related}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Image Inpainting}

Early image inpainting methods used diffusion to simulate the restoration of an image region \cite{bertalmio2000pdeinpainting, bertalmio2001pde2, bertalmio2005pde3, chan2001pde}. While later methods leveraged exemplar-based approaches for removing large objects from digital images \cite{criminisi2004patch, jin2015patch2, kawai2016patch3, guo2018patch4}. Recent image inpainting methods are deep learning-based, leveraging Convolutional Neural Networks (CNNs) \cite{lecun1997cnn}, autoencoders \cite{kramer1991autoencoder}, Generative Adversarial Networks (GANs) \cite{goodfellow2014generativeadversarialnetworks}, Transformers \cite{vaswani2017attentionneed}, Diffusion Models \cite{chang2023designfundamentalsdiffusionmodels}, or some combination of these, and have significantly improved the quality and realism of inpainted regions. Early approaches demonstrated the potential of CNNs for inpainting tasks, sometimes incorporating frequency domain information through techniques like Fourier convolutions \cite{suvorov2021lama}. GAN-based methods have also been widely adopted, combining autoencoders and GANs to generate coherent images, with techniques such as Context Encoders \cite{pathak2016context} and dilated convolutions improving results \cite{yu2018generativeimageinpaintingcontextual}. Diffusion models have emerged as a powerful approach, using noise removal to reconstruct missing regions of images \cite{lugmayr2022repaint}, with models like GLIDE \cite{nichol2022glidephotorealisticimagegeneration} and Stable Diffusion \cite{rombach2022stablediffusion} incorporating additional guidance, such as text, for improved control. Other models, such as WavePaint \cite{jeevan2023wavepaintresourceefficienttokenmixerselfsupervised}, introduce wavelet transforms for efficient processing, while hybrid methods combining transformers and autoencoders have also shown promise in generating detailed reconstructions \cite{esser2021taming}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Inpainting Detection}

The detection of inpainted regions in images has become increasingly important with the advances in inpainting methods. Early approaches focused on patch comparison and connectivity analysis to identify inconsistencies introduced by inpainting algorithms \cite{wu2008detection, chang2013forgerydetection, liang2015efficientforgerydetection, zhang2018robustforgery}. These methods often draw inspiration from copy-move forgery detection frameworks, incorporating techniques such as Gabor magnitude analysis and color correlation \cite{christlein2012evaluationcopymove, lee2015copymovegabor, jin2018sparcitybased, mahfoudi2020forgerydetectionreflection}. Machine learning-based approaches, such as SVM classifiers, rely on hand-crafted features \cite{shen2017svm}. Deep learning models, including CNN and ResNet \cite{he2015deep} architectures, sometimes combined with Long Short-Term Memory (LSTM) Networks, showed better performance in distinguishing between modified and unmodified areas \cite{zhu2018deeplearningapproachpatchbasedcnn, li2019highpasssfullyconvulutionalresnetcnn, lu2020detectionlstmcnn, kumar2021semanticsegmentioninpainting, simonyan2015deepconvolutionalnetworkslargescalevgg}. Recent developments introduced more complex architectures, such as hybrid transformer-CNN models and U-Net \cite{ronneberger2015unet} variants, focusing on detecting noise inconsistencies and enhancing inpainting traces \cite{zhu2023transformercnn, zhang2023localizationinpaintingfeatureenhancemnet, ronneberger2015unet, wu2022iid}. Notable recent methods include SPAN \cite{hu2020span}, which uses a pyramidal self-attention structure, CFL-Net \cite{niloy2022cflnetimageforgerylocalization} using contrastive learning, and PSCC-Net \cite{luy2022pscc} with its two-path model for feature extraction and mask enhancement. CatNet \cite{kwon2022catnet} uses Discrete Cosine Transform (DCT) coefficients and JPEG compression artifacts, while TruFor \cite{guillaro2023trufor} combines RGB data with a noise-sensitive fingerprint (Noiseprint++ \cite{cozzoline2020noiseprint}) for robust alteration detection. MMFusion \cite{triaridis2023mmfusion} extends TruFor by adding more filter convolutions, while others propose fusion architectures for combining semantic and low-level artifacts~\cite{karageorgiou2024fusion}. The FOCAL method \cite{wu2023rethinkingimageforgerydetectionfocal} uses contrastive learning and unsupervised clustering to address the differentiation between forged and authentic regions. These approaches represent the current state-of-the-art in inpainting detection, offering both localization of altered areas and overall image tampering scores, improving the field's ability to identify complex image manipulations \cite{criminisi2004patch, bayar2016bayarfilter, fridrich2012srm}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Datasets for Image Inpainting Detection}

The development and evaluation of inpainting detection models rely on diverse datasets that capture various inpainting techniques and scenarios. Early datasets such as MICC \cite{amerini2011micc}, CMFD \cite{christlein2012cmfd}, and CoFoMoD \cite{tralic2013cofomod} focused primarily on copy-move forgeries, which can be considered a form of inpainting \cite{barglazan2024image}. The Realistic Tampering Dataset \cite{Korus2016TIFS, Korus2016WIFS} introduced larger images and object removal forgeries, and was followed by the comprehensive MFC dataset \cite{guan2019mfc} with its widely used NIST16 subset \cite{kwon2022catnet, wu2023rethinkingimageforgerydetectionfocal, guillaro2023trufor, triaridis2023mmfusion}. Specialized datasets emerged to address specific inpainting detection challenges, including DEFACTO \cite{mahfoudi2019defacto}, which covers multiple forgery types using MS COCO images \cite{lin2014coco, daisy2014asmarterexemplarbased}, IMD2020 \cite{novozamsky2020wacv} combining classic and learning-based inpainting methods \cite{yu2018generativeimageinpaintingcontextual}, and DID/IID \cite{wu2022iid} incorporating various inpainting techniques. The rise of AI-generated content has led to datasets like CocoGlide \cite{guillaro2023trufor} and TGIF \cite{mareen2024tgiftextguidedinpaintingforgery}, which utilize advanced models such as GLIDE, Stable Diffusion, and Adobe Firefly for inpainting. 