\section{Introduction}
\label{sec:intro}

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}{\textwidth}
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/000_final_paper_good_000000002549_val.jpg}
            \includegraphics[width=0.49\linewidth]{examples/000_final_paper_good_coco_000000446117_powerpaint_realisticvision_1_None.jpg}
            \caption{"a delicious apple to add a pop of color to the scene"}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/001_final_paper_good_rc54a3103t_Q71.jpg}
            \includegraphics[width=0.49\linewidth]{examples/001_final_paper_good_raise_rc54a3103t_Q71_brushnet_sd1-5_1_Blended.jpg}
            \caption{"lush green vines and foliage, intertwining around ancient stone pillars"}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/002_final_paper_good_c232ae91a5cffe3c.jpg}
            \includegraphics[width=0.49\linewidth]{examples/002_final_paper_good_openimages_c232ae91a5cffe3c_hdpainter_sd2_1_None.jpg}
            \caption{"a fluffy golden retriever"}
        \end{subfigure}
    \end{subfigure}
    
    \vspace{1em}
    
    \begin{subfigure}{\textwidth}
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/000_final_paper_bad_000000001485_val.jpg}
            \includegraphics[width=0.49\linewidth]{examples/000_final_paper_bad_coco_000000001675_hdpainter_dreamshaper_1_None.jpg}
            \caption{"a fluffy white bunny sitting inside a laptop."}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/001_final_paper_bad_r7c8c98b1t_Q100.jpg}
            \includegraphics[width=0.49\linewidth]{examples/001_final_paper_bad_raise_r7c8c98b1t_Q100_removeanything_lama_1_None.jpg}
            \caption{No prompt}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[width=0.49\linewidth]{examples/002_final_paper_bad_0bcc1d9f92e418b4.jpg}
            \includegraphics[width=0.49\linewidth]{examples/002_final_paper_bad_openimages_0bcc1d9f92e418b4_powerpaint_realisticvision_1_None.jpg}
            \caption{"a curious otter exploring the grassy"}
        \end{subfigure}
    \end{subfigure}
    
    \caption{Original (with semi-transparent red inpainting mask) and inpainted images from three datasets, with prompts shown below each pair for text-guided models. The first row shows images classified as deceiving by UGDA \ref{sec:ugda}, and the second row shows undeceiving images. Each column corresponds to a different dataset: COCO (first), RAISE (second), and OpenImages (third).}
    \label{fig:deceptive_dataset_examples}
\end{figure*}

Image inpainting—the process of reconstructing missing or corrupted regions in images— has become increasingly accessible with the emergence of powerful generative AI tools, enabling even non-experts to create highly photorealistic edits \cite{zhang2024texttoimagediffusionmodelsgenerative}. In particular, text-guided image inpainting—the process of adding, removing, or altering specific regions in images using textual prompts—has seen remarkable advancement through models like Stable Diffusion \cite{rombach2022high}, DALL-E \cite{ramesh2021zero}, and Imagen \cite{saharia2022photorealistic}. These models allow users to add, remove, or alter content in images with high fidelity, using a textual description and a mask specifying the region to be edited.

% Challenges
While these developments offer new creative possibilities, they also present challenges regarding their potential misuse for malicious purposes, including the creation of deepfakes, and other deceptive media \cite{verdoliva2020media, wu2022defakehop}. Consequently, there is an increasing need for robust methods to detect and localize manipulated images to maintain trust and integrity in visual media \cite{zhou2018learning, wang2020cnn}.

% Image forgery detection
Image forgery detection techniques often rely on identifying inconsistencies in noise patterns, compression artifacts, or statistical irregularities in pixel distributions \cite{bayar2016bayarfilter, zhou2018learning}. However, modern generative models produce images that closely resemble natural images in terms of statistical properties, rendering many conventional detection approaches less effective \cite{frank2020leveraging}. Moreover, existing publicly available datasets for training and evaluating inpainting detection algorithms are limited in scale, diversity, or do not encompass the latest generative techniques \cite{novozamsky2020imd2020, guillaro2023trufor, mareen2024tgif}. For instance, datasets like MICC-F220 \cite{amerini2011micc}, DEFACTO \cite{mahfoudi2019defacto}, and CocoGlide \cite{guillaro2023trufor} focus on outdated inpainting methods or comprise a limited number of low-resolution images.

% In this work

Creating a high-quality dataset for inpainting detection involves two key challenges in automated generation. First, while state-of-the-art inpainting models are effective at image manipulation, they need detailed prompts for realistic results \cite{mahajan2023promptinghardhardlyprompting} - basic object labels often do not provide enough context for seamless integration. Second, evaluating image realism is difficult to automate, as standard quality metrics may not reflect how convincing the manipulations appear.

To address these challenges, we propose two key improvements to the dataset creation process. First, we use Large Language Models (LLMs) to create detailed, context-aware prompts that guide the inpainting process, producing results of higher aesthetic quality compared to simple object labels. Second, we develop a realism assessment method that uses vision-language models to compare inpainted images with their originals, helping identify convincing manipulations. Human studies validate this approach, showing it aligns with human perception of image realism.

Using this methodology, we create the \emph{\datasetname} (\textbf{Di}versity and \textbf{Qu}ality-aware \textbf{I}npainting \textbf{D}ataset) dataset for detecting AI-generated inpainting. The dataset contains 95,839 inpainted images generated from 78,684 original images from three datasets: MS-COCO \cite{lin2014coco}, RAISE \cite{dang2015raise}, and OpenImages \cite{kuznetsova2020open}. We provide each inpainted image with its original version, inpainting mask, and text prompt. To ensure variety in manipulation types, we use multiple state-of-the-art diffusion-based inpainting models.

For thorough model evaluation, we create both in-domain and out-of-domain testing splits. The in-domain split uses the same LLM and source images as the training set, allowing assessment on familiar data. The out-of-domain split uses different source images and a different LLM, testing how well models handle new types of data.

Our main \textbf{contributions} are summarized as follows:

\begin{itemize} 
\item We present the \emph{\datasetname} dataset, the largest and most diverse dataset for AI-generated image inpainting detection to date, addressing the need for large-scale, high-quality data in this area. 
\item We propose a framework for generating contextually appropriate inpainted content by leveraging language models for prompt generation and multiple state-of-the-art inpainting pipelines mainly using diffusion models for image manipulation, supporting both spliced and fully regenerated manipulations
\item We conduct extensive experiments comparing LLM-generated prompts to object class labels, demonstrating superior aesthetic quality scores across multiple datasets, validating the effectiveness of our approach.
\item We introduce an uncertainty-guided realism assessment methodology using vision-language models, validated through human studies showing strong correlation with perceived realism in manipulated images.
\item We provide comprehensive benchmarking results across multiple state-of-the-art detection models, revealing significant performance gaps between in-domain (up to 0.69 IoU) and out-of-domain (0.23-0.66 IoU) scenarios, while demonstrating substantial improvements through retraining on our dataset.
\end{itemize}
