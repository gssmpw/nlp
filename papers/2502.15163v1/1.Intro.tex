\IEEEPARstart{H}{yperspectral} image (HSI) can record the spectral characteristics of ground objects~\cite{6555921}. As a key technology of HSI processing, HSI classification is aimed at assigning a unique category label to each pixel based on the spectral and spatial characteristics of this pixel~\cite{FPGA,10078841,10696913,10167502}, which is widely used in agriculture~\cite{WHU-Hi}, forest~\cite{ITreeDet}, city~\cite{WANG2022113058}, ocean~\cite{WHU-Hi} studies and so on.

The existing HSI classifiers~\cite{FPGA,10325566,10047983,9573256} typically assume the closed-set setting, where all HSI pixels are presumed to belong to one of the \textit{known} classes. However, due to the practical limitations of field investigations across wide geographical areas and the high annotation costs associated with the limited availability of domain experts, it is inevitable to have outliers in the vast study area~\cite{MDL4OW,Fang_OpenSet,Kang_OpenSet}. These outliers do not belong to any known classes and will be referred as \textit{unknown} classes hereafter. A classifier based on closed-set assumption will misclassify the unknown class as one of the known classes. For example, in the University of Pavia HSI dataset (Fig.~\ref{fig:open_set_example}), objects such as vehicles, buildings with red roofs, carports, and swimming pools are ignored from the original annotations~\cite{MDL4OW}. These objects are misclassified as one of predefined known classes.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\columnwidth]{example_paviau.png}
    \caption{Comparison of classification results between closed-set based classifier and open-set based classifier for the University of Pavia dataset. The dataset originally contains nine \textit{known} land cover classes, however, significant misclassifications occur in the \textit{unknown} classes in closed-set based results. For instance, these unknown buildings with red roofs are misclassified as Bare S., Meadows, and other known materials by closed-set based classifier~\cite{FPGA}. Note that there is a significant overlap in the distribution of spectral curves between known and unknown classes in HSI datasets, which poses a major problem to open-set HSI classification.}
    \label{fig:open_set_example}
\end{figure}

Open-set classification (Fig.~\ref{fig:open_set_example}), as a critical task for safely deploying models in real-world scenarios, addresses the above problem by accurately classifying known class samples and rejecting unknown class outliers~\cite{OpenMax,MDL4OW,Fang_OpenSet}. Moreover, the recent advanced researches have explored training with an auxiliary unknown classes dataset to regularize the classifiers to produce lower confidence~\cite{Entropy,WOODS} or higher energies~\cite{Energy} on these unknown classes samples.

Despite its promise, there are some limitations when open-set classification meets HSI. First, the limited number of training samples, combined with significant spectral overlap between known and unknown classes (see Fig.~\ref{fig:open_set_example}), causes the classifier to overfit on the training samples. Second, the distribution of the auxiliary unknown classes dataset may not align well with the distribution of real-world unknown classes, potentially leading to the misclassification of the test-time data. Finally, it is labor-intensive to ensure the collected extra unknown classes dataset does not overlap with the known classes.

To mitigate these limitations, this paper leverages unlabeled ``in-the-wild'' hyperspectral data (referred to as ``wild data''), which can be collected \textit{freely} during deploying HSI classifiers in the open real-world environments, and has been largely neglected for open-set HSI classification purposes. Such data is abundant, has a better match to the test-time distribution than the collected auxiliary unknown classes dataset, and does not require any annotation workloads. Moreover, the information about unknown classes stored in the wild data can be leveraged to promote the rejection of unknown classes in the case of spectral overlap. While leveraging wild data naturally suits open-set HSI classification, it also poses a unique challenge: wild data is not pure and consists of both known and unknown classes. This challenge originates from the marginal distribution of wild data, which can be modeled by the Huber contamination model~\cite{Huber}:
\begin{equation}
    \mathbb{P}_{wild}=\pi\mathbb{P}_{k}+(1-\pi)\mathbb{P}_{u},
    \label{eq:huber_contamination_model}
\end{equation}
where $\mathbb{P}_{k}$ and $\mathbb{P}_{u}$ represent the distributions of known and unknown classes, respectively. Here, $\pi=\pi_{1}+\dots+\pi_{C}$, and $\pi_{c}$ refers to the probability (or class prior~\cite{DistPU}) of the known class $c \in [1,C]$ in $\mathbb{P}_{wild}$.
The known component of wild data acts as noise, potentially disrupting the training process (further analysis can be found in Section~\ref{sec:Methodology}). 

\begin{center}
    \fbox{\begin{minipage}{23em}
        This paper aims to propose a novel framework---\textit{HOpenCls}---to effectively leverage wild data for open-set HSI classification. Wild data is easily available as it's naturally generated during classifier deployment in real-world environments. This framework can be regarded as training open-set HSI classifiers in their \textit{living environments}.
    \end{minipage}}
\end{center}

To handle the lack of ``clean'' unknown classes datasets, the insight of this paper is to formulate a positive-unlabeled (PU) learning problem~\cite{DistPU,T-HOneCls} in the rejection of unknown classes: learning a binary classifier to classify positive (known) and negative (unknown) classes only from positive and unlabeled (wild) data. What's more, the high intra-class variance of positive class and the high class prior of positive class are potential factors that limit the ability of PU learning methods to address unknown class rejection task. To overcome these limitations, the multi-label strategy is introduced to the \textit{HOpenCls} to decouple the original unknown classes rejection task into multiple sub-PU learning tasks, where the $c$-th sub-PU learning task is responsible for classifying the known class $c$ against all other classes. Compared to the original unknown classes rejection task, each sub-PU learning task exhibits reduced intra-class variance and class prior in the positive class.

Beyond the mathematical reformulation, a key contribution of this paper is a novel PU learning method inspired by the abnormal gradient weights found in wild data. First of all, when the auxiliary unknown classes dataset is replaced by the wild data, this paper demonstrates that the adverse effects impeding the rejection of unknown classes originate from the larger gradient weights associated with the component of known classes in the wild data. Therefore, a gradient contraction (Grad-C) module is designed to reduce the gradient weights associated with all training wild data, and then, the gradient weights of wild unknown samples are recovered by the gradient expansion (Grad-E) module to enhance the fitting capability of the classifier. Compared to other PU learning methods~\cite{nnPU,DistPU,PUET,HOneCls}, the combination of Grad-C and Grad-E modules provides the capability to reject unknown classes in a class prior-free manner. Given the spectral overlap characteristics in HSI, estimating class priors for each known class is highly challenging~\cite{T-HOneCls}, and the class prior-free PU learning method is more suitable for open-set HSI classification.

Extensive experiments have been conducted to evaluate the proposed \textit{HOpenCls}. For thorough comparison, two groups of methods are compared: (1) trained with only $\mathbb{P}_{k}$ data, and (2) trained with both $\mathbb{P}_{k}$ data and an additional dataset. The experimental results demonstrate that the proposed framework substantially enhances the classifier's ability to reject unknown classes, leading to a marked improvement in open-set HSI classification performance. Taking the challenging WHU-Hi-HongHu dataset as an example, \textit{HOpenCls} boosts the overall accuracy in open-set classification (Open OA) by 8.20\% compared to the strongest baseline, with significantly improving the metric of unknown classes rejection (F1\textsuperscript{U}) by 38.91\%. The key contributions of this paper can be summarized as follows:
\begin{itemize}
    \item[1)] This paper proposes a novel framework, \textit{HOpenCls}, for open-set HSI classification, designed to effectively leverage wild data. To the best of our knowledge, this paper pioneers the exploration of PU learning for open-set HSI classification.
    \item[2)] The multi-PU head is designed to incorporate the multi-label strategy into \textit{HOpenCls}, decoupling the original unknown classes rejection task into multiple sub-PU learning tasks. As demonstrated in the experimental section, the multi-PU strategy is crucial for bridging PU learning with open-set HSI classification.
    \item[3)] The Grad-C and Grad-E modules, derived from the theoretical analysis of abnormal gradient weights, are proposed for the rejection of unknown classes. The combination of these modules forms a novel class prior-free PU learning method.
    \item[4)] Extensive comparisons and ablations are conducted across: (1) a diverse range of datasets, and (2) varying assumptions about the relationship between the auxiliary dataset distribution and the test-time distribution. The proposed \textit{HOpenCls} achieves state-of-the-art performance, demonstrating significant improvements over existing methods.
\end{itemize}