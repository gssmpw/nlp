@article{brown2016mathematics,
  title={Mathematics: Identifying and addressing student errors},
  author={Brown, Janice and Skow, Kim and IRIS, Center},
  journal={The Iris Center},
  volume={31},
  year={2016},
  publisher={The Iris Center United States of America}
}
@article{nie2022time,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={The Eleventh International Conference on Learning Representations (ICLR'23)},
  year={2023}
}




@article{zhang2024logora,
  title={LogoRA: Local-Global Representation Alignment for Robust Time Series Classification},
  author={Zhang, Huanyu and Zhang, Yi-Fan and Zhang, Zhang and Wen, Qingsong and Wang, Liang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}


@article{wen2024onenet,
  title={Onenet: Enhancing time series forecasting models under concept drift by online ensembling},
  author={Wen, Qingsong and Chen, Weiqi and Sun, Liang and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{zhou2023solving,
  title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
  author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
  booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{kdd24eduworkshop,
author = {Wen, Qingsong and Liang, Jing and Sierra, Carles and Luckin, Rose and Tong, Richard and Liu, Zitao and Cui, Peng and Tang, Jiliang},
title = {AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning},
year = {2024},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'24)},
pages = {6743–6744}
}

@article{li2024bringing,
  title={Bringing generative AI to adaptive learning in education},
  author={Li, Hang and Xu, Tianlong and Zhang, Chaoli and Chen, Eason and Liang, Jing and Fan, Xing and Li, Haoyang and Tang, Jiliang and Wen, Qingsong},
  journal={arXiv preprint arXiv:2402.14601},
  year={2024}
}

@inproceedings{wang2021educational,
  title={Educational question mining at scale: Prediction, analysis and personalization},
  author={Wang, Zichao and Tschiatschek, Sebastian and Woodhead, Simon and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Jones, Simon Peyton and Baraniuk, Richard G and Zhang, Cheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'21)},
  volume={35},
  number={17},
  pages={15669--15677},
  year={2021}
}

@inproceedings{touretzky2019envisioning,
  title={Envisioning AI for K-12: What should every child know about AI?},
  author={Touretzky, David and Gardner-McCune, Christina and Martin, Fred and Seehorn, Deborah},
  booktitle={Proceedings of the AAAI conference on artificial intelligence (AAAI)},
  volume={33},
  number={01},
  pages={9795--9799},
  year={2019}
}


@article{roy2018mapping,
  title={Mapping to declarative knowledge for word problem solving},
  author={Roy, Subhro and Roth, Dan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={159--172},
  year={2018}
}


@inproceedings{priyani2018error,
  title={Error analysis of mathematical problems on TIMSS: A case of Indonesian secondary students},
  author={Priyani, HA and Ekawati, R},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={296},
  number={1},
  pages={012010},
  year={2018},
  organization={IOP Publishing}
}

@inproceedings{haryanti2019analysis,
  title={Analysis of students’ error in solving mathematical word problems in geometry},
  author={Haryanti, MD and Herman, T and Prabawanto, S},
  booktitle={Journal of Physics: Conference Series},
  volume={1157},
  number={4},
  pages={042084},
  year={2019},
  organization={IOP Publishing}
}


@inproceedings{feldman2018automatic,
  title={Automatic diagnosis of students' misconceptions in k-8 mathematics},
  author={Feldman, Molly Q and Cho, Ji Yong and Ong, Monica and Gulwani, Sumit and Popovi{\'c}, Zoran and Andersen, Erik},
  booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2018}
}

@InProceedings{Agustsson_2017_CVPR_Workshops,
	author = {Agustsson, Eirikur and Timofte, Radu},
	title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
	booktitle = {CVPR},
	year = {2017}
} 

@article{yang2023hq,
  title={HQ-50K: A Large-scale, High-quality Dataset for Image Restoration},
  author={Yang, Qinhong and Chen, Dongdong and Tan, Zhentao and Liu, Qiankun and Chu, Qi and Bao, Jianmin and Yuan, Lu and Hua, Gang and Yu, Nenghai},
  journal={arXiv preprint arXiv:2306.05390},
  year={2023}
}

@inproceedings{zhang2021benchmarking,
  title={Benchmarking ultra-high-definition image super-resolution},
  author={Zhang, Kaihao and Li, Dongxu and Luo, Wenhan and Ren, Wenqi and Stenger, Bj{\"o}rn and Liu, Wei and Li, Hongdong and Yang, Ming-Hsuan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={WACV},
  year={2021}
}
@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={CVPR},
  year={2019}
}

@misc{li2024multimodal,
      title={Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models}, 
      author={Lei Li and Yuqi Wang and Runxin Xu and Peiyi Wang and Xiachong Feng and Lingpeng Kong and Qi Liu},
      year={2024},
      eprint={2403.00231},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}


@article{li2024seed,
  title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}


@inproceedings{Liu4K, 
   author={J. Liu and D. Liu and W. Yang and S. Xia and X. Zhang and Y. Dai}, 
   booktitle={arXiv}, 
   title={A Comprehensive Benchmark for Single Image Compression Artifacts Reduction}, 
   year={2019}, 
   }

@article{sun2021fair1m,
    title     = {FAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery},
    author    = {Xian Sun and Peijin Wang and Zhiyuan Yan and F. Xu and Ruiping Wang and W. Diao and Jin Chen and Jihao Li and Yingchao Feng and Tao Xu and M. Weinmann and S. Hinz and Cheng Wang and K. Fu},
    journal   = {ISPRS},
    year      = {2021},
}


@article{hou2019v,
  title={V-RSIR: An open access web-based image annotation tool for remote sensing image retrieval},
  author={Hou, Dongyang and Miao, Zelang and Xing, Huaqiao and Wu, Hao},
  journal={IEEE Access},
  year={2019},
}

@article{tang2024mtvqa,
  title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering},
  author={Tang, Jingqun and Liu, Qi and Ye, Yongjie and Lu, Jinghui and Wei, Shu and Lin, Chunhui and Li, Wanqing and Mahmood, Mohamad Fitri Faiz Bin and Feng, Hao and Zhao, Zhen and others},
  journal={arXiv preprint arXiv:2405.11985},
  year={2024}
}


@article{yuan2019ctw,
  author  = {Tai{-}Ling Yuan and Zhe Zhu and Kun Xu and Cheng{-}Jun Li and Tai{-}Jiang Mu and Shi{-}Min Hu},
  title   = {A Large Chinese Text Dataset in the Wild},
  journal = {JCST},
  year    = {2019},
}

@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{chen2024rh20t,
  title={RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents},
  author={Chen, Zeren and Shi, Zhelun and Lu, Xiaoya and He, Lehan and Qian, Sucheng and Fang, Hao Shu and Yin, Zhenfei and Ouyang, Wanli and Shao, Jing and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2403.19622},
  year={2024}
}

@misc{DriveVLM,
title={DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models},
author={Xiaoyu Tian and Junru Gu and Bailin Li and Yicheng Liu and Zhiyong Zhao and Yang Wang and Kun Zhan and Peng Jia and Xianpeng Lang and Hang Zhao},
year={2024},
eprint={2402.12289},
archivePrefix={arXiv},
primaryClass={cs.CV}
}


@article{chen2024we,
title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
journal={arXiv preprint arXiv:2403.20330},
year={2024}
}


@inproceedings{yu2024mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  booktitle={ICML},
  year={2024}
}

@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={ICLR},
  year      = {2024}
}

@article{zhang2024mathverse,
  title={MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Gao, Peng and others},
  journal={arXiv preprint arXiv:2403.14624},
  year={2024}
}

@article{li2024seed2plus,
  title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}

@article{li2023seed2,
  title={SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2311.17092},
  year={2023}
  }


@misc{mmtbench,
    title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI}, 
    author={Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao},
    year={2024},
    eprint={2404.16006},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@article{yin2024lamm,
  title={Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark},
  author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and Bai, Lei and others},
  journal={NeurIPS},
  year={2024}
}

@article{lu2024wildvision,
  title={WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences},
  author={Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.11069},
  year={2024}
}

@article{bitton2023visit,
  title={Visit-bench: A benchmark for vision-language instruction following inspired by real-world use},
  author={Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schimdt, Ludwig},
  journal={arXiv preprint arXiv:2308.06595},
  year={2023}
}

@article{tong2024cambrian,
  title={Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@misc{liu2024hidden,
      title={On the Hidden Mystery of OCR in Large Multimodal Models}, 
      author={Yuliang Liu and Zhang Li and Biao Yang and Chunyuan Li and Xucheng Yin and Cheng-lin Liu and Lianwen Jin and Xiang Bai},
      year={2024},
      eprint={2305.07895},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{mishra2019ocr,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR},
  year={2019},
}

@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2022}
}


@article{chen2021websrc,
  title={Websrc: A dataset for web-based structural reading comprehension},
  author={Chen, Xingyu and Zhao, Zihan and Chen, Lu and Zhang, Danyang and Ji, Jiabao and Luo, Ao and Xiong, Yuxuan and Yu, Kai},
  journal={arXiv preprint arXiv:2101.09465},
  year={2021}
}

@article{zhang2024vcr,
  title={VCR: Visual Caption Restoration},
  author={Zhang, Tianyu and Wang, Suyuchen and Li, Lu and Zhang, Ge and Taslakian, Perouz and Rajeswar, Sai and Fu, Jie and Liu, Bang and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2406.06462},
  year={2024}
}

@inproceedings{wichers2018resolving,
  title={Resolving referring expressions in images with labeled elements},
  author={Wichers, Nevan and Hakkani-T{\"u}r, Dilek and Chen, Jindong},
  booktitle={SLT},

  year={2018},

}

@article{li2020widget,
  title={Widget captioning: Generating natural language description for mobile user interface elements},
  author={Li, Yang and Li, Gang and He, Luheng and Zheng, Jingjie and Li, Hong and Guan, Zhiwei},
  journal={arXiv preprint arXiv:2010.04295},
  year={2020}
}

@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile UI summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={UIST},
  year={2021}
}

@misc{hsiao2024screenqa,
      title={ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots},
      author={Yu-Chung Hsiao and Fedir Zubach and Maria Wang and Jindong Chen},
      year={2024},
      eprint={2209.08199},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{sunkara2022towards,
  title={Towards better semantic understanding of mobile interfaces},
  author={Sunkara, Srinivas and Wang, Maria and Liu, Lijuan and Baechler, Gilles and Hsiao, Yu-Chung and Sharma, Abhanshu and Stout, James and others},
  journal={arXiv preprint arXiv:2210.02663},
  year={2022}
}

@article{baechler2024screenai,
  title={Screenai: A vision-language model for ui and infographics understanding},
  author={Baechler, Gilles and Sunkara, Srinivas and Wang, Maria and Zubach, Fedir and Mansoor, Hassan and Etter, Vincent and C{\u{a}}rbune, Victor and Lin, Jason and Chen, Jindong and Sharma, Abhanshu},
  journal={arXiv preprint arXiv:2402.04615},
  year={2024}
}

@article{you2024ferret,
  title={Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs},
  author={You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2404.05719},
  year={2024}
}

@inproceedings{das2018embodied,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  year={2018}
}

   @ARTICLE{Damen2021PAMI,
   title={The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   journal={T-PAMI},
   year={2021},
} 

@inproceedings{burns2022dataset,
  title={A dataset for interactive vision-language navigation with unknown command feasibility},
  author={Burns, Andrea and Arsan, Deniz and Agrawal, Sanjna and Kumar, Ranjitha and Saenko, Kate and Plummer, Bryan A},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{ma2022sqa3d,
  title={SQA3D: Situated Question Answering in 3D Scenes},
  author={Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan},
  booktitle={ICLR},
  year={2023},
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@article{jia2022egotaskqa,
  title={Egotaskqa: Understanding human tasks in egocentric videos},
  author={Jia, Baoxiong and Lei, Ting and Zhu, Song-Chun and Huang, Siyuan},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{datta2022episodic,
  title={Episodic memory question answering},
  author={Datta, Samyak and Dharur, Sameer and Cartillier, Vincent and Desai, Ruta and Khanna, Mukul and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2024embodiedscan,
  title={Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai},
  author={Wang, Tai and Mao, Xiaohan and Zhu, Chenming and Xu, Runsen and Lyu, Ruiyuan and Li, Peisen and Chen, Xiao and Zhang, Wenwei and Chen, Kai and Xue, Tianfan and others},
  booktitle={CVPR},
  year={2024}
}

@article{kim2018textual,
  title={Textual Explanations for Self-Driving Vehicles},
  author={Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
  journal={ECCV},
  year={2018}
}

@InProceedings{kim2019CVPR,
        author = {Jinkyu Kim and Teruhisa Misu and Yi-Ting Chen and Ashish Tawari and John Canny},
        title = {Grounding Human-To-Vehicle Advice for Self-Driving Vehicles},
        booktitle = {CVPR},
        year = {2019}
}

@article{deruyttere2019talk2car,
  title={Talk2car: Taking control of your self-driving car},
  author={Deruyttere, Thierry and Vandenhende, Simon and Grujicic, Dusan and Van Gool, Luc and Moens, Marie-Francine},
  journal={arXiv preprint arXiv:1909.10838},
  year={2019}
}

@article{sima2023drivelm,
  title={DriveLM: Driving with Graph Visual Question Answering},
  author={Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Luo, Ping and Geiger, Andreas and Li, Hongyang},
  journal={arXiv preprint arXiv:2312.14150},
  year={2023}
}

@inproceedings{malla2023drama,
  title={Drama: Joint risk localization and captioning in driving},
  author={Malla, Srikanth and Choi, Chiho and Dwivedi, Isht and Choi, Joon Hee and Li, Jiachen},
  booktitle={WACV},
  year={2023}
}

@inproceedings{sachdeva2024rank2tell,
  title={Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning},
  author={Sachdeva, Enna and Agarwal, Nakul and Chundi, Suhas and Roelofs, Sean and Li, Jiachen and Kochenderfer, Mykel and Choi, Chiho and Dariush, Behzad},
  booktitle={WACV},
  year={2024}
}

@article{wu2023language,
  title={Language prompt for autonomous driving},
  author={Wu, Dongming and Han, Wencheng and Wang, Tiancai and Liu, Yingfei and Zhang, Xiangyu and Shen, Jianbing},
  journal={arXiv preprint arXiv:2309.04379},
  year={2023}
}

@inproceedings{qian2024nuscenes,
  title={Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario},
  author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},
  booktitle={AAAI},
  year={2024}
}

@article{nie2023reason2drive,
  title={Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving},
  author={Nie, Ming and Peng, Renyuan and Wang, Chunwei and Cai, Xinyue and Han, Jianhua and Xu, Hang and Zhang, Li},
  journal={arXiv preprint arXiv:2312.03661},
  year={2023}
}

@article{marcu2023lingoqa,
  title={LingoQA: Video Question Answering for Autonomous Driving}, 
  author={Ana-Maria Marcu and Long Chen and Jan Hünermann and Alice Karnsund and Benoit Hanotte and Prajwal Chidananda and Saurabh Nair and Vijay Badrinarayanan and Alex Kendall and Jamie Shotton and Oleg Sinavski},
  journal={arXiv preprint arXiv:2312.14115},
  year={2023},
}

@inproceedings{huang2024smartedit,
  title={Smartedit: Exploring complex instruction-based image editing with multimodal large language models},
  author={Huang, Yuzhou and Xie, Liangbin and Wang, Xintao and Yuan, Ziyang and Cun, Xiaodong and Ge, Yixiao and Zhou, Jiantao and Dong, Chao and Huang, Rui and Zhang, Ruimao and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{cheng-etal-2023-edit,
    title = "Can We Edit Multimodal Large Language Models?",
    author = "Cheng, Siyuan  and
      Tian, Bozhong  and
      Liu, Qingbin  and
      Chen, Xi  and
      Wang, Yongheng  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "EMNLP",
    year = "2023",
}

@misc{huang2024vlkeb,
      title={VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark}, 
      author={Han Huang and Haitian Zhong and Tao Yu and Qiang Liu and Shu Wu and Liang Wang and Tieniu Tan},
      year={2024},
      eprint={2403.07350},
      archivePrefix={arXiv}
}

@article{cai2023benchlmm,
  title={Benchlmm: Benchmarking cross-style visual capability of large multimodal models},
  author={Cai, Rizhao and Song, Zirui and Guan, Dayan and Chen, Zhenhao and Luo, Xing and Yi, Chenyu and Kot, Alex},
  journal={arXiv preprint arXiv:2312.02896},
  year={2023}
}

@article{tu2023how,
  title={How Many Unicorns Are In This Image? A Safety Evaluation Benchmark For Vision LLMs},
  author={Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang},
  journal={arXiv preprint arXiv:2311.16101},
  year={2023}
}

@article{zhang2024benchmarking,
  title={Benchmarking large multimodal models against common corruptions},
  author={Zhang, Jiawei and Pang, Tianyu and Du, Chao and Ren, Yi and Li, Bo and Lin, Min},
  journal={arXiv preprint arXiv:2401.11943},
  year={2024}
}

@article{AesBench,
    title={AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception},
    author={Huang, Yipo and Yuan, Quan and Sheng, Xiangfei and Yang, Zhichao and Wu, Haoning and Chen, Pengfei and Yang, Yuzhe and Li, Leida and Lin, Weisi},
   journal={arXiv preprint arXiv:2401.08276},
    year={2024},
}

@article{li2024fakebench,
  title={FakeBench: Uncover the Achilles' Heels of Fake Images with Large Multimodal Models},
  author={Li, Yixuan and Liu, Xuelin and Wang, Xiaoyang and Wang, Shiqi and Lin, Weisi},
  journal={arXiv preprint arXiv:2404.13306},
  year={2024}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={JMLR},
  year={2023}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}
@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={https://vicuna.lmsys.org},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}
@article{laurenccon2024obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={NeurIPS},
  year={2024}
}
@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}
@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}
@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}
@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}


@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}
@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@misc{fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title = {Introducing our Multimodal Models},
  url = {https://www.adept.ai/blog/fuyu-8b},
  year = {2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={OpenAI.},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}
@article{li2023monkey,
  title={Monkey: Image resolution and text label are important things for large multi-modal models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}
@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    year={2024}
}

@article{xu2024llava,
  title={LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}
@article{li2024mini,
  title={Mini-gemini: Mining the potential of multi-modality vision language models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv preprint arXiv:2403.18814},
  year={2024}
}

@article{ge2024convllava,
  title={ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models},
  author={Ge, Chunjiang and Cheng, Sijie and Wang, Ziming and Yuan, Jiale and Gao, Yuan and Song, Jun and Song, Shiji and Huang, Gao and Zheng, Bo},
  journal={arXiv preprint arXiv:2405.15738},
  year={2024}
}


@article{xu2024llava-uhd,
  title={{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}

@article{zhu2021detection,
  title={Detection and tracking meet drones challenge},
  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},
  journal={T-PAMI},
  year={2021},
}

@inproceedings{jia2021llvip,
  title={LLVIP: A visible-infrared paired dataset for low-light vision},
  author={Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},
  booktitle={ICCV},
  year={2021}
}

@article{liu2024textmonkey,
  title={Textmonkey: An ocr-free large multimodal model for understanding document},
  author={Liu, Yuliang and Yang, Biao and Liu, Qiang and Li, Zhang and Ma, Zhiyin and Zhang, Shuo and Bai, Xiang},
  journal={arXiv preprint arXiv:2403.04473},
  year={2024}
}

@article{hu2024mplug,
  title={mplug-docowl 1.5: Unified structure learning for ocr-free document understanding},
  author={Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
  journal={arXiv preprint arXiv:2403.12895},
  year={2024}
}

@article{chen2023sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@article{chen2023minigpt,
  title={Minigpt-v2: large language model as a unified interface for vision-language multi-task learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{lu2024deepseek,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{zhang2023internlm,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
  journal={arXiv preprint arXiv:2309.15112},
  year={2023}
}

@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@inproceedings{caesar2020nuscenes,
  title={nuscenes: A multimodal dataset for autonomous driving},
  author={Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  booktitle={CVPR},
  year={2020}
}


@inproceedings{li2022coda,
  title={Coda: A real-world road corner case dataset for object detection in autonomous driving},
  author={Li, Kaican and Chen, Kai and Wang, Haoyu and Hong, Lanqing and Ye, Chaoqiang and Han, Jianhua and Chen, Yukuai and Zhang, Wei and Xu, Chunjing and Yeung, Dit-Yan and others},
  booktitle={ECCV},
  year={2022},
}

@article{zhou2024embodied,
  title={Embodied understanding of driving scenarios},
  author={Zhou, Yunsong and Huang, Linyan and Bu, Qingwen and Zeng, Jia and Li, Tianyu and Qiu, Hang and Zhu, Hongzi and Guo, Minyi and Qiao, Yu and Li, Hongyang},
  journal={arXiv preprint arXiv:2403.04593},
  year={2024}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}


@article{gao2024survey,
  title={A Survey for Foundation Models in Autonomous Driving},
  author={Gao, Haoxiang and Li, Yaqian and Long, Kaiwen and Yang, Ming and Shen, Yiqing},
  journal={arXiv preprint arXiv:2402.01105},
  year={2024}
}

@article{yang2023llm4drive,
  title={Llm4drive: A survey of large language models for autonomous driving},
  author={Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
  journal={arXiv e-prints},
  pages={arXiv--2311},
  year={2023}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}
@article{li2023open,
  title={Open-sourced data ecosystem in autonomous driving: the present and future},
  author={Li, Hongyang and Li, Yang and Wang, Huijie and Zeng, Jia and Cai, Pinlong and Xu, Huilin and Lin, Dahua and Yan, Junchi and Xu, Feng and Xiong, Lu and others},
  journal={arXiv preprint arXiv:2312.03408},
  year={2023}
}

@article{li2024evaluating,
  title={Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction},
  author={Li, Xiaoyuan and Wang, Wenjie and Li, Moxin and Guo, Junrong and Zhang, Yang and Feng, Fuli},
  journal={arXiv preprint arXiv:2406.00755},
  year={2024}
}


@article{brown1978diagnostic,
  title={Diagnostic models for procedural bugs in basic mathematical skills},
  author={Brown, John Seely and Burton, Richard R},
  journal={Cognitive science},
  volume={2},
  number={2},
  pages={155--192},
  year={1978},
  publisher={Elsevier}
}

@article{li2024knowledge,
  title={Knowledge Tagging System on Math Questions via LLMs with Flexible Demonstration Retriever},
  author={Li, Hang and Xu, Tianlong and Tang, Jiliang and Wen, Qingsong},
  journal={arXiv preprint arXiv:2406.13885},
  year={2024}
}

@article{xu2016personalized,
  title={Personalized course sequence recommendations},
  author={Xu, Jie and Xing, Tianwei and Van Der Schaar, Mihaela},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={20},
  pages={5340--5352},
  year={2016},
  publisher={IEEE}
}

@article{meier2015predicting,
  title={Predicting grades},
  author={Meier, Yannick and Xu, Jie and Atan, Onur and Van der Schaar, Mihaela},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={4},
  pages={959--972},
  year={2015},
  publisher={IEEE}
}


@article{xu2017machine,
  title={A machine learning approach for tracking and predicting student performance in degree programs},
  author={Xu, Jie and Moon, Kyeong Ho and Van Der Schaar, Mihaela},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={11},
  number={5},
  pages={742--753},
  year={2017},
  publisher={IEEE}
}


@article{wang2024large,
  title={Large language models for education: A survey and outlook},
  author={Wang, Shen and Xu, Tianlong and Li, Hang and Zhang, Chaoli and Liang, Joleen and Tang, Jiliang and Yu, Philip S and Wen, Qingsong},
  journal={arXiv preprint arXiv:2403.18105},
  year={2024}
}

@article{gannot2023data,
  title={Data science education: The signal processing perspective [SP education]},
  author={Gannot, Sharon and Tan, Zheng-Hua and Haardt, Martin and Chen, Nancy F and Wai, Hoi-To and Tashev, Ivan and Kellermann, Walter and Dauwels, Justin},
  journal={IEEE Signal Processing Magazine},
  volume={40},
  number={7},
  pages={89--93},
  year={2023},
  publisher={IEEE}
}


@article{prochazka2021integrating,
  title={Integrating the role of computational intelligence and digital signal processing in education: Emerging technologies and mathematical tools},
  author={Prochazka, Ales and Vysata, Oldrich and Marik, Vladimir},
  journal={IEEE Signal Processing Magazine},
  volume={38},
  number={3},
  pages={154--162},
  year={2021},
  publisher={IEEE}
}


@article{maghsudi2021personalized,
  title={Personalized education in the artificial intelligence era: what to expect next},
  author={Maghsudi, Setareh and Lan, Andrew and Xu, Jie and van Der Schaar, Mihaela},
  journal={IEEE Signal Processing Magazine},
  volume={38},
  number={3},
  pages={37--50},
  year={2021},
  publisher={IEEE}
}


@inproceedings{jarvis2004applying,
  title={Applying machine learning techniques to rule generation in intelligent tutoring systems},
  author={Jarvis, Matthew P and Nuzzo-Jones, Goss and Heffernan, Neil T},
  booktitle={International Conference on Intelligent Tutoring Systems},
  pages={541--553},
  year={2004},
  organization={Springer}
}


@article{xu2024foundation,
  author={Xu, Tianlong and Tong, Richard and Liang, Jing and Fan, Xing and Li, Haoyang and Wen, Qingsong},
  journal={IEEE Intelligent Systems}, 
  title={Foundation Models for Education: Promises and Prospects}, 
  year={2024},
  volume={39},
  number={3},
  pages={20-24}
}

@inproceedings{yue2023mmmu,
        title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
        author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
        booktitle={Proceedings of CVPR},
        year={2024},
      }
@misc{huang2024mmevalprocalibratingmultimodalbenchmarks,
      title={MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation}, 
      author={Jinsheng Huang and Liang Chen and Taian Guo and Fu Zeng and Yusheng Zhao and Bohan Wu and Ye Yuan and Haozhe Zhao and Zhihui Guo and Yichi Zhang and Jingyang Yuan and Wei Ju and Luchen Liu and Tianyu Liu and Baobao Chang and Ming Zhang},
      year={2024},
      eprint={2407.00468},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.00468}, 
}

@article{fu2024vita,
  title={Vita: Towards open-source interactive omni multimodal llm},
  author={Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others},
  journal={arXiv preprint arXiv:2408.05211},
  year={2024}
}


@article{zhang2024debiasing,
  title={Debiasing large visual language models},
  author={Zhang, Yi-Fan and Yu, Weichen and Wen, Qingsong and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu},
  journal={arXiv preprint arXiv:2403.05262},
  year={2024}
}

@article{zhang2024beyond,
  title={Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models},
  author={Zhang, Yi-Fan and Wen, Qingsong and Fu, Chaoyou and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong},
  journal={arXiv preprint arXiv:2406.08487},
  year={2024}
}


@article{zhang2024mme,
  title={MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
  author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
  journal={arXiv preprint arXiv:2408.13257},
  year={2024}
}

@article{fu2024mme,
  title={MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs},
  author={Fu, Chaoyou and Zhang, Yi-Fan and Yin, Shukang and Li, Bo and Fang, Xinyu and Zhao, Sirui and Duan, Haodong and Sun, Xing and Liu, Ziwei and Wang, Liang and others},
  journal={arXiv preprint arXiv:2411.15296},
  year={2024}
}


@article{yan2024errorradar,
  title={Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection},
  author={Yan, Yibo and Wang, Shen and Huo, Jiahao and Li, Hang and Li, Boyan and Su, Jiamin and Gao, Xiong and Zhang, Yi-Fan and Xu, Tianlong and Chu, Zhendong and others},
  journal={arXiv preprint arXiv:2410.04509},
  year={2024}
}

@article{xu2024ai,
  title={Ai-driven virtual teacher for enhanced educational efficiency: Leveraging large pretrain models for autonomous error analysis and correction},
  author={Xu, Tianlong and Zhang, Yi-Fan and Chu, Zhendong and Wang, Shen and Wen, Qingsong},
  journal={arXiv preprint arXiv:2409.09403},
  year={2024}
}

@InProceedings{Agustsson_2017_CVPR_Workshops,
	author = {Agustsson, Eirikur and Timofte, Radu},
	title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
	booktitle = {CVPR},
	year = {2017}
} 

@article{yang2023hq,
  title={HQ-50K: A Large-scale, High-quality Dataset for Image Restoration},
  author={Yang, Qinhong and Chen, Dongdong and Tan, Zhentao and Liu, Qiankun and Chu, Qi and Bao, Jianmin and Yuan, Lu and Hua, Gang and Yu, Nenghai},
  journal={arXiv preprint arXiv:2306.05390},
  year={2023}
}

@inproceedings{zhang2021benchmarking,
  title={Benchmarking ultra-high-definition image super-resolution},
  author={Zhang, Kaihao and Li, Dongxu and Luo, Wenhan and Ren, Wenqi and Stenger, Bj{\"o}rn and Liu, Wei and Li, Hongdong and Yang, Ming-Hsuan},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={WACV},
  year={2021}
}

@misc{li2024multimodal,
      title={Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models}, 
      author={Lei Li and Yuqi Wang and Runxin Xu and Peiyi Wang and Xiachong Feng and Lingpeng Kong and Qi Liu},
      year={2024},
      eprint={2403.00231},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}
@article{fu2024limsim++,
  title={LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving},
  author={Fu, Daocheng and Lei, Wenjie and Wen, Licheng and Cai, Pinlong and Mao, Song and Dou, Min and Shi, Botian and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.01246},
  year={2024}
}


@article{li2024seed,
  title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}
@article{nan2024beyond,
  title={Beyond the Hype: A dispassionate look at vision-language models in medical scenario},
  author={Nan, Yang and Zhou, Huichi and Xing, Xiaodan and Yang, Guang},
  journal={arXiv preprint arXiv:2408.08704},
  year={2024}
}


@article{tuo2022construction,
  title={Construction and Application of a Human-Computer Collaborative Multimodal Practice Teaching Model for Preschool Education},
  author={Tuo, Meimei and Long, Baoxin},
  journal={Computational Intelligence and Neuroscience},
  volume={2022},
  number={1},
  pages={2973954},
  year={2022},
  publisher={Wiley Online Library}
}


@inproceedings{Liu4K, 
   author={J. Liu and D. Liu and W. Yang and S. Xia and X. Zhang and Y. Dai}, 
   booktitle={arXiv}, 
   title={A Comprehensive Benchmark for Single Image Compression Artifacts Reduction}, 
   year={2019}, 
   }

@article{sun2021fair1m,
    title     = {FAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery},
    author    = {Xian Sun and Peijin Wang and Zhiyuan Yan and F. Xu and Ruiping Wang and W. Diao and Jin Chen and Jihao Li and Yingchao Feng and Tao Xu and M. Weinmann and S. Hinz and Cheng Wang and K. Fu},
    journal   = {ISPRS},
    year      = {2021},
}


@article{hou2019v,
  title={V-RSIR: An open access web-based image annotation tool for remote sensing image retrieval},
  author={Hou, Dongyang and Miao, Zelang and Xing, Huaqiao and Wu, Hao},
  journal={IEEE Access},
  year={2019},
}

@article{tang2024mtvqa,
  title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering},
  author={Tang, Jingqun and Liu, Qi and Ye, Yongjie and Lu, Jinghui and Wei, Shu and Lin, Chunhui and Li, Wanqing and Mahmood, Mohamad Fitri Faiz Bin and Feng, Hao and Zhao, Zhen and others},
  journal={arXiv preprint arXiv:2405.11985},
  year={2024}
}


@article{yuan2019ctw,
  author  = {Tai{-}Ling Yuan and Zhe Zhu and Kun Xu and Cheng{-}Jun Li and Tai{-}Jiang Mu and Shi{-}Min Hu},
  title   = {A Large Chinese Text Dataset in the Wild},
  journal = {JCST},
  year    = {2019},
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{fu2023challenger,
  title={A challenger to gpt-4v? early explorations of gemini in visual expertise},
  author={Fu, Chaoyou and Zhang, Renrui and Lin, Haojia and Wang, Zihan and Gao, Timin and Luo, Yongdong and Huang, Yubo and Zhang, Zhengye and Qiu, Longtian and Ye, Gaoxiang and others},
  journal={arXiv preprint arXiv:2312.12436},
  year={2023}
}


@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{chen2024rh20t,
  title={RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents},
  author={Chen, Zeren and Shi, Zhelun and Lu, Xiaoya and He, Lehan and Qian, Sucheng and Fang, Hao Shu and Yin, Zhenfei and Ouyang, Wanli and Shao, Jing and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2403.19622},
  year={2024}
}

@misc{DriveVLM,
title={DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models},
author={Xiaoyu Tian and Junru Gu and Bailin Li and Yicheng Liu and Zhiyong Zhao and Yang Wang and Kun Zhan and Peng Jia and Xianpeng Lang and Hang Zhao},
year={2024},
eprint={2402.12289},
archivePrefix={arXiv},
primaryClass={cs.CV}
}


@article{chen2024we,
title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
journal={arXiv preprint arXiv:2403.20330},
year={2024}
}


@article{zhang2024mathverse,
  title={MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Gao, Peng and others},
  journal={arXiv preprint arXiv:2403.14624},
  year={2024}
}

@article{li2024seed2plus,
  title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}

@article{li2023seed2,
  title={SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2311.17092},
  year={2023}
  }

@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}

@misc{han2023coremm,
  title={InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models}, 
  author={Xiaotian Han and Quanzeng You and Yongfei Liu and Wentao Chen and Huangjie Zheng and Khalil Mrini and Xudong Lin and Yiqi Wang and Bohan Zhai and Jianbo Yuan and Heng Wang and Hongxia Yang},
  year={2023},
  eprint={2311.11567},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@article{yin2024lamm,
  title={Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark},
  author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and Bai, Lei and others},
  journal={NeurIPS},
  year={2024}
}

@article{bai2023touchstone,
  title={Touchstone: Evaluating vision-language models by language models},
  author={Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.16890},
  year={2023}
}

@article{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2404.12390},
  year={2024}
}

@article{tong2024cambrian,
  title={Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@misc{liu2024hidden,
      title={On the Hidden Mystery of OCR in Large Multimodal Models}, 
      author={Yuliang Liu and Zhang Li and Biao Yang and Chunyuan Li and Xucheng Yin and Cheng-lin Liu and Lianwen Jin and Xiang Bai},
      year={2024},
      eprint={2305.07895},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{mishra2019ocr,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR},
  year={2019},
}

@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2022}
}


@article{chen2021websrc,
  title={Websrc: A dataset for web-based structural reading comprehension},
  author={Chen, Xingyu and Zhao, Zihan and Chen, Lu and Zhang, Danyang and Ji, Jiabao and Luo, Ao and Xiong, Yuxuan and Yu, Kai},
  journal={arXiv preprint arXiv:2101.09465},
  year={2021}
}

@article{zhang2024vcr,
  title={VCR: Visual Caption Restoration},
  author={Zhang, Tianyu and Wang, Suyuchen and Li, Lu and Zhang, Ge and Taslakian, Perouz and Rajeswar, Sai and Fu, Jie and Liu, Bang and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2406.06462},
  year={2024}
}

@inproceedings{wichers2018resolving,
  title={Resolving referring expressions in images with labeled elements},
  author={Wichers, Nevan and Hakkani-T{\"u}r, Dilek and Chen, Jindong},
  booktitle={SLT},

  year={2018},

}

@article{li2020widget,
  title={Widget captioning: Generating natural language description for mobile user interface elements},
  author={Li, Yang and Li, Gang and He, Luheng and Zheng, Jingjie and Li, Hong and Guan, Zhiwei},
  journal={arXiv preprint arXiv:2010.04295},
  year={2020}
}

@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile UI summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={UIST},
  year={2021}
}

@misc{hsiao2024screenqa,
      title={ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots},
      author={Yu-Chung Hsiao and Fedir Zubach and Maria Wang and Jindong Chen},
      year={2024},
      eprint={2209.08199},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{sunkara2022towards,
  title={Towards better semantic understanding of mobile interfaces},
  author={Sunkara, Srinivas and Wang, Maria and Liu, Lijuan and Baechler, Gilles and Hsiao, Yu-Chung and Sharma, Abhanshu and Stout, James and others},
  journal={arXiv preprint arXiv:2210.02663},
  year={2022}
}

@article{baechler2024screenai,
  title={Screenai: A vision-language model for ui and infographics understanding},
  author={Baechler, Gilles and Sunkara, Srinivas and Wang, Maria and Zubach, Fedir and Mansoor, Hassan and Etter, Vincent and C{\u{a}}rbune, Victor and Lin, Jason and Chen, Jindong and Sharma, Abhanshu},
  journal={arXiv preprint arXiv:2402.04615},
  year={2024}
}

@article{you2024ferret,
  title={Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs},
  author={You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2404.05719},
  year={2024}
}

@inproceedings{das2018embodied,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  year={2018}
}

   @ARTICLE{Damen2021PAMI,
   title={The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   journal={T-PAMI},
   year={2021},
} 

@inproceedings{burns2022dataset,
  title={A dataset for interactive vision-language navigation with unknown command feasibility},
  author={Burns, Andrea and Arsan, Deniz and Agrawal, Sanjna and Kumar, Ranjitha and Saenko, Kate and Plummer, Bryan A},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{ma2022sqa3d,
  title={SQA3D: Situated Question Answering in 3D Scenes},
  author={Ma, Xiaojian and Yong, Silong and Zheng, Zilong and Li, Qing and Liang, Yitao and Zhu, Song-Chun and Huang, Siyuan},
  booktitle={ICLR},
  year={2023},
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@article{jia2022egotaskqa,
  title={Egotaskqa: Understanding human tasks in egocentric videos},
  author={Jia, Baoxiong and Lei, Ting and Zhu, Song-Chun and Huang, Siyuan},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{datta2022episodic,
  title={Episodic memory question answering},
  author={Datta, Samyak and Dharur, Sameer and Cartillier, Vincent and Desai, Ruta and Khanna, Mukul and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2024embodiedscan,
  title={Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai},
  author={Wang, Tai and Mao, Xiaohan and Zhu, Chenming and Xu, Runsen and Lyu, Ruiyuan and Li, Peisen and Chen, Xiao and Zhang, Wenwei and Chen, Kai and Xue, Tianfan and others},
  booktitle={CVPR},
  year={2024}
}

@article{kim2018textual,
  title={Textual Explanations for Self-Driving Vehicles},
  author={Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
  journal={ECCV},
  year={2018}
}

@InProceedings{kim2019CVPR,
        author = {Jinkyu Kim and Teruhisa Misu and Yi-Ting Chen and Ashish Tawari and John Canny},
        title = {Grounding Human-To-Vehicle Advice for Self-Driving Vehicles},
        booktitle = {CVPR},
        year = {2019}
}

@article{deruyttere2019talk2car,
  title={Talk2car: Taking control of your self-driving car},
  author={Deruyttere, Thierry and Vandenhende, Simon and Grujicic, Dusan and Van Gool, Luc and Moens, Marie-Francine},
  journal={arXiv preprint arXiv:1909.10838},
  year={2019}
}

@article{sima2023drivelm,
  title={DriveLM: Driving with Graph Visual Question Answering},
  author={Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Luo, Ping and Geiger, Andreas and Li, Hongyang},
  journal={arXiv preprint arXiv:2312.14150},
  year={2023}
}

@inproceedings{malla2023drama,
  title={Drama: Joint risk localization and captioning in driving},
  author={Malla, Srikanth and Choi, Chiho and Dwivedi, Isht and Choi, Joon Hee and Li, Jiachen},
  booktitle={WACV},
  year={2023}
}

@inproceedings{sachdeva2024rank2tell,
  title={Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning},
  author={Sachdeva, Enna and Agarwal, Nakul and Chundi, Suhas and Roelofs, Sean and Li, Jiachen and Kochenderfer, Mykel and Choi, Chiho and Dariush, Behzad},
  booktitle={WACV},
  year={2024}
}

@article{wu2023language,
  title={Language prompt for autonomous driving},
  author={Wu, Dongming and Han, Wencheng and Wang, Tiancai and Liu, Yingfei and Zhang, Xiangyu and Shen, Jianbing},
  journal={arXiv preprint arXiv:2309.04379},
  year={2023}
}

@inproceedings{qian2024nuscenes,
  title={Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario},
  author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},
  booktitle={AAAI},
  year={2024}
}

@article{nie2023reason2drive,
  title={Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving},
  author={Nie, Ming and Peng, Renyuan and Wang, Chunwei and Cai, Xinyue and Han, Jianhua and Xu, Hang and Zhang, Li},
  journal={arXiv preprint arXiv:2312.03661},
  year={2023}
}

@article{marcu2023lingoqa,
  title={LingoQA: Video Question Answering for Autonomous Driving}, 
  author={Ana-Maria Marcu and Long Chen and Jan Hünermann and Alice Karnsund and Benoit Hanotte and Prajwal Chidananda and Saurabh Nair and Vijay Badrinarayanan and Alex Kendall and Jamie Shotton and Oleg Sinavski},
  journal={arXiv preprint arXiv:2312.14115},
  year={2023},
}

@inproceedings{huang2024smartedit,
  title={Smartedit: Exploring complex instruction-based image editing with multimodal large language models},
  author={Huang, Yuzhou and Xie, Liangbin and Wang, Xintao and Yuan, Ziyang and Cun, Xiaodong and Ge, Yixiao and Zhou, Jiantao and Dong, Chao and Huang, Rui and Zhang, Ruimao and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{cheng-etal-2023-edit,
    title = "Can We Edit Multimodal Large Language Models?",
    author = "Cheng, Siyuan  and
      Tian, Bozhong  and
      Liu, Qingbin  and
      Chen, Xi  and
      Wang, Yongheng  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "EMNLP",
    year = "2023",
}

@misc{huang2024vlkeb,
      title={VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark}, 
      author={Han Huang and Haitian Zhong and Tao Yu and Qiang Liu and Shu Wu and Liang Wang and Tieniu Tan},
      year={2024},
      eprint={2403.07350},
      archivePrefix={arXiv}
}

@article{cai2023benchlmm,
  title={Benchlmm: Benchmarking cross-style visual capability of large multimodal models},
  author={Cai, Rizhao and Song, Zirui and Guan, Dayan and Chen, Zhenhao and Luo, Xing and Yi, Chenyu and Kot, Alex},
  journal={arXiv preprint arXiv:2312.02896},
  year={2023}
}

@article{tu2023how,
  title={How Many Unicorns Are In This Image? A Safety Evaluation Benchmark For Vision LLMs},
  author={Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang},
  journal={arXiv preprint arXiv:2311.16101},
  year={2023}
}

@article{zhang2024benchmarking,
  title={Benchmarking large multimodal models against common corruptions},
  author={Zhang, Jiawei and Pang, Tianyu and Du, Chao and Ren, Yi and Li, Bo and Lin, Min},
  journal={arXiv preprint arXiv:2401.11943},
  year={2024}
}

@article{AesBench,
    title={AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception},
    author={Huang, Yipo and Yuan, Quan and Sheng, Xiangfei and Yang, Zhichao and Wu, Haoning and Chen, Pengfei and Yang, Yuzhe and Li, Leida and Lin, Weisi},
   journal={arXiv preprint arXiv:2401.08276},
    year={2024},
}

@article{li2024fakebench,
  title={FakeBench: Uncover the Achilles' Heels of Fake Images with Large Multimodal Models},
  author={Li, Yixuan and Liu, Xuelin and Wang, Xiaoyang and Wang, Shiqi and Lin, Weisi},
  journal={arXiv preprint arXiv:2404.13306},
  year={2024}
}

@misc{fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title = {Introducing our Multimodal Models},
  url = {https://www.adept.ai/blog/fuyu-8b},
  year = {2023}
}

@article{dai2024instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={NeurIPS},
  year={2024}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}
@article{li2023monkey,
  title={Monkey: Image resolution and text label are important things for large multi-modal models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}
@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    year={2024}
}

@article{xu2024llava,
  title={LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}
@article{li2024mini,
  title={Mini-gemini: Mining the potential of multi-modality vision language models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv preprint arXiv:2403.18814},
  year={2024}
}


@article{ge2024convllava,
  title={ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models},
  author={Ge, Chunjiang and Cheng, Sijie and Wang, Ziming and Yuan, Jiale and Gao, Yuan and Song, Jun and Song, Shiji and Huang, Gao and Zheng, Bo},
  journal={arXiv preprint arXiv:2405.15738},
  year={2024}
}


@article{xu2024llava-uhd,
  title={{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}

@article{zhu2021detection,
  title={Detection and tracking meet drones challenge},
  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},
  journal={T-PAMI},
  year={2021},
}

@inproceedings{jia2021llvip,
  title={LLVIP: A visible-infrared paired dataset for low-light vision},
  author={Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},
  booktitle={ICCV},
  year={2021}
}

@article{liu2024textmonkey,
  title={Textmonkey: An ocr-free large multimodal model for understanding document},
  author={Liu, Yuliang and Yang, Biao and Liu, Qiang and Li, Zhang and Ma, Zhiyin and Zhang, Shuo and Bai, Xiang},
  journal={arXiv preprint arXiv:2403.04473},
  year={2024}
}

@article{hu2024mplug,
  title={mplug-docowl 1.5: Unified structure learning for ocr-free document understanding},
  author={Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
  journal={arXiv preprint arXiv:2403.12895},
  year={2024}
}

@article{chen2023sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@article{chen2023minigpt,
  title={Minigpt-v2: large language model as a unified interface for vision-language multi-task learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{lu2024deepseek,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{zhang2023internlm,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
  journal={arXiv preprint arXiv:2309.15112},
  year={2023}
}

@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@inproceedings{caesar2020nuscenes,
  title={nuscenes: A multimodal dataset for autonomous driving},
  author={Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  booktitle={CVPR},
  year={2020}
}


@inproceedings{li2022coda,
  title={Coda: A real-world road corner case dataset for object detection in autonomous driving},
  author={Li, Kaican and Chen, Kai and Wang, Haoyu and Hong, Lanqing and Ye, Chaoqiang and Han, Jianhua and Chen, Yukuai and Zhang, Wei and Xu, Chunjing and Yeung, Dit-Yan and others},
  booktitle={ECCV},
  year={2022},
}

@article{zhou2024embodied,
  title={Embodied understanding of driving scenarios},
  author={Zhou, Yunsong and Huang, Linyan and Bu, Qingwen and Zeng, Jia and Li, Tianyu and Qiu, Hang and Zhu, Hongzi and Guo, Minyi and Qiao, Yu and Li, Hongyang},
  journal={arXiv preprint arXiv:2403.04593},
  year={2024}
}


@article{gao2024survey,
  title={A Survey for Foundation Models in Autonomous Driving},
  author={Gao, Haoxiang and Li, Yaqian and Long, Kaiwen and Yang, Ming and Shen, Yiqing},
  journal={arXiv preprint arXiv:2402.01105},
  year={2024}
}

@article{yang2023llm4drive,
  title={Llm4drive: A survey of large language models for autonomous driving},
  author={Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
  journal={arXiv e-prints},
  pages={arXiv--2311},
  year={2023}
}

@misc{li2024llavanext-strong,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}
@article{li2023open,
  title={Open-sourced data ecosystem in autonomous driving: the present and future},
  author={Li, Hongyang and Li, Yang and Wang, Huijie and Zeng, Jia and Cai, Pinlong and Xu, Huilin and Lin, Dahua and Yan, Junchi and Xu, Feng and Xiong, Lu and others},
  journal={arXiv preprint arXiv:2312.03408},
  year={2023}
}