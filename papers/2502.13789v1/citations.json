[
  {
    "index": 0,
    "papers": [
      {
        "key": "yin2023survey",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong",
        "title": "A survey on multimodal large language models"
      },
      {
        "key": "fu2023challenger",
        "author": "Fu, Chaoyou and Zhang, Renrui and Lin, Haojia and Wang, Zihan and Gao, Timin and Luo, Yongdong and Huang, Yubo and Zhang, Zhengye and Qiu, Longtian and Ye, Gaoxiang and others",
        "title": "A challenger to gpt-4v? early explorations of gemini in visual expertise"
      },
      {
        "key": "zhang2024debiasing",
        "author": "Zhang, Yi-Fan and Yu, Weichen and Wen, Qingsong and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong and Tan, Tieniu",
        "title": "Debiasing large visual language models"
      },
      {
        "key": "fu2024mme",
        "author": "Fu, Chaoyou and Zhang, Yi-Fan and Yin, Shukang and Li, Bo and Fang, Xinyu and Zhao, Sirui and Duan, Haodong and Sun, Xing and Liu, Ziwei and Wang, Liang and others",
        "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gpt4",
        "author": "OpenAI.",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chiang2023vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others",
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90\\%* chatgpt quality"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "awadalla2023openflamingo",
        "author": "Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",
        "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dai2024instructblip",
        "author": "Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2023visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2024beyond",
        "author": "Zhang, Yi-Fan and Wen, Qingsong and Fu, Chaoyou and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong",
        "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "fu2024vita",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others",
        "title": "Vita: Towards open-source interactive omni multimodal llm"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "fu2023mme",
        "author": "Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others",
        "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "mmtbench",
        "author": "Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao",
        "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2024mme",
        "author": "Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others",
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "fu2024blink",
        "author": "Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay",
        "title": "Blink: Multimodal large language models can see but not perceive"
      },
      {
        "key": "bitton2023visit",
        "author": "Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schimdt, Ludwig",
        "title": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yu2024mm",
        "author": "Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities"
      },
      {
        "key": "han2023coremm",
        "author": "Xiaotian Han and Quanzeng You and Yongfei Liu and Wentao Chen and Huangjie Zheng and Khalil Mrini and Xudong Lin and Yiqi Wang and Bohan Zhai and Jianbo Yuan and Heng Wang and Hongxia Yang",
        "title": "InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "lu2024mathvista",
        "author": "Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yan2024errorradar",
        "author": "Yan, Yibo and Wang, Shen and Huo, Jiahao and Li, Hang and Li, Boyan and Su, Jiamin and Gao, Xiong and Zhang, Yi-Fan and Xu, Tianlong and Chu, Zhendong and others",
        "title": "Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection"
      }
    ]
  }
]