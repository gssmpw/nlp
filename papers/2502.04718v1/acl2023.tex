% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{CJKutf8}
\usepackage{array}
\usepackage{amsmath}
\usepackage{arydshln}
\usepackage{pifont}
\usepackage{amssymb}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{lipsum,xcolor}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\renewcommand{\paragraph}[1]{\noindent\textbf{#1}\quad}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Text Style Transfer Evaluation}
\title{Which Text Style Transfer Metrics Can You Rely On?}
\title{A Systematic Evaluation of Text Style Transfer Metrics}
\title{Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Sourabrata Mukherjee\textsuperscript{1}, \textbf{Atul Kr. Ojha\textsuperscript{2},} \textbf{John P. McCrae\textsuperscript{2},} \textbf{Ondřej Dušek\textsuperscript{1}}\\
\textsuperscript{1}Charles University, Faculty of Mathematics and Physics, Prague, Czechia \\
%Institute of Formal and Applied Linguistics, Prague, Czech Republic\\
\textsuperscript{2}Insight Research Ireland Centre for Data Analytics, DSI, University of Galway, Ireland\\
{\tt \{mukherjee,odusek\}@ufal.mff.cuni.cz} \\
{\tt \{atulkumar.ojha,john.mccrae\}@insight-centre.org}}

\begin{document}
\maketitle
\begin{abstract}
Text Style Transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. %Although human evaluation provides nuanced insights, it is costly, time-intensive, and lacks reproducibility, making it impractical for frequent or large-scale use. Although automated metrics offer a practical alternative, current TST evaluation approaches have yet to fully leverage sophisticated metrics that have proven successful in other NLP domains, such as machine translation and text summarization.
Using human evaluation is ideal but costly, same as in other natural language processing (NLP) tasks; however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization.
In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks—sentiment transfer and detoxification—in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles.
% - that experimentally combine multiple metrics to achieve stronger alignment with human evaluations. 
Additionally, we investigate the potential of Large Language Models (LLMs) as tools for TST evaluation.
Our findings highlight that certain advanced NLP metrics and and experimental-hybrid-techniques, provide better insights than existing TST metrics for delivering more accurate, consistent, and reproducible TST evaluations.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Text Style Transfer (\textsc{TST}) refers to the task of modifying a given text to reflect a specific style while preserving its original content \cite{hu2022text}. Previous work in this domain has explored altering various stylistic dimensions, such as sentiment \cite{prabhumoye2018style}, romantic tone \cite{li2018delete}, politeness \cite{madaan-etal-2020-politeness}, or political slant \cite{prabhumoye2018style}. Different modeling approaches have been proposed for \textsc{TST}, including methods that manipulate latent representations of text \cite{zhao2018adversarially,prabhumoye2018style} and techniques that identify and replace style-related lexicons directly \cite{DBLP:conf/naacl/LiJHL18, fu_rethinking_2019}. 
Despite the growing interest in \textsc{TST}, reliably assessing the performance of \textsc{TST} models continues to be a bottleneck \cite{hu2022text}. 
%Several efforts have explored more sophisticated metrics and human evaluation protocols to capture the multi-faceted nature of \textsc{TST}~\cite{yamshchikov2021style, briakou2021review, tikhonov2019style}. 
While human evaluation is often regarded as the standard for capturing subtle cues in style, it is expensive, time-intensive, and difficult to reproduce at scale \cite{briakou2021review}. Consequently, automated metrics have become a proxy for human judgment, but there is a notable lack of standardization and consensus on which metrics best capture style transfer accuracy, content preservation, and overall naturalness \cite{mir2019evaluating, briakou2021evaluating}. In addition, large language models (LLMs) could serve as alternatives to traditional human evaluation and automated metrics for TST evaluation \cite{ostheimer2023text}. However, the rapid evolution of LLMs, particularly for closed-source models, raises concerns about reproducibility %and consistency over time 
\cite{gao2024llm, chen2023chatgpt}.

% The question of whether existing automated metrics and LLM-based evaluations align with human judgments in TST tasks remains largely open.
We address this gap by experimenting with existing and novel metrics for two popular \textsc{TST} subtasks: \emph{sentiment transfer} \cite{prabhumoye2018style} and \emph{detoxification} \cite{Dementieva2022114}. Our experiments span a multilingual setting, covering English, Hindi, and Bengali, to investigate the utility of these metrics across diverse linguistic contexts. We then conduct a meta-evaluation of the proposed metrics by measuring their correlation with human judgments. %, showcasing their strengths and weaknesses for \textsc{TST} evaluation.
To further explore the potential of automated metrics, we %not only use them individually but 
also combine them in ensembles, experimentally creating hybrid scores. 
Additionally, we investigate the applicability of large language models (\textsc{LLMs}) as an alternative evaluation tool. %, highlighting their ability to provide nuanced judgments without the cost and scalability issues of human annotation.
Our results show that both these hybrid approaches and LLMs can improve correlation with human evaluations, offering a more robust and comprehensive assessment of \textsc{TST} outputs. 
%Overall, our findings underscore the effectiveness of advanced automated methods and hybrid techniques in improving the accuracy, consistency, and reproducibility of \textsc{TST} evaluation. 
We will release our code and resources with the final version of this paper.

\section{Related Work}
\label{sec:related_work}

Text Style Transfer (\textsc{TST}) tasks are traditionally evaluated using three key dimensions: \emph{style transfer accuracy}, \emph{content preservation}, and \emph{fluency} \cite{mukherjee2024textstyletransferintroductory, hu2022text, jin2022deep}. Prior work underscores the challenge of jointly capturing subtle stylistic nuances and preserving semantic content \cite{briakou2021review, tikhonov2019style}.

\paragraph{Style Transfer Accuracy}
A common approach is to train a dedicated classifier to check whether the transformed text reflects the intended style \cite{prabhumoye2018style, shen2017style}. Alternatively, unsupervised methods rely on distributional shifts in style-related features \cite{yang2018unsupervised, tikhonov2019style}.

\paragraph{Content Preservation}
Metrics such as \emph{BLEU} \cite{papineni2002bleu} and embedding-based similarity \cite{rahutomo2012semantic, reimers2019sentence} often serve as proxies for semantic fidelity. However, they may overlook nuances introduced by stylistic transformations in both single-language and multilingual contexts \cite{yamshchikov2021style, briakou2021evaluating}, and recent studies highlight the shortcomings of traditional similarity measures when evaluating paraphrase-like modifications \cite{yamshchikov2021style, briakou2021review}.

\paragraph{Fluency}
\emph{Fluency} is typically estimated using perplexity from a pre-trained language model such as \emph{GPT-2} \cite{radford2019language}. Nonetheless, perplexity may fail to capture context-specific grammatical coherence, especially if the style domain diverges from the model’s training data \cite{tikhonov2019style, briakou2021review}, and can yield inconsistent performance across languages \cite{briakou2021evaluating}.

\section{Metrics Compared}

We conduct evaluations in two scenarios: (1) \emph{reference-based}, where metrics are computed against a reference text (when available), and (2) \emph{reference-free}, where metrics are computed directly using source and generated text, without requiring a reference.

% \vspace{0.15cm}
\paragraph{Previously Used TST Metrics}
Traditional \textsc{TST} metrics can be grouped under style transfer accuracy, content preservation, and fluency. For style transfer accuracy, we include \emph{Sentence Accuracy} based on a fine-tuned \textit{XLM-RoBERTa-base} \cite{conneau-etal-2020-unsupervised} classifier from \citet{mukherjee-etal-2024-large-language}, and \emph{WMD}~\cite{10.5555/3045118.3045221,WEI202348}. 
For content preservation: \emph{BLEU}~\cite{papineni-etal-2002-bleu,tikhonov2019style}, \emph{Cosine Similarity} \cite{rahutomo2012semantic}, \emph{Masked BLEU and Masked Cosine Similarity} \cite{mukherjee2022balancing}, 
\emph{ROUGE-2} and \emph{ROUGE-L}~\cite{lin-hovy-2003-automatic,lin-2004-rouge,lin-och-2004-automatic}.
For fluency, we use \emph{Perplexity} of \emph{GPT-2} \cite{radford2019language} and \emph{MGPT} \cite{shliazhko-etal-2024-mgpt}.

%\vspace{0.15cm}
\paragraph{Newly Applied Text Metrics}
We expand the TST evaluation by introducing additional metrics from other NLP tasks. 
For style transfer accuracy, we include \emph{EMD}~\cite{rubner2000earth}, \emph{KL Divergence}~\cite{kullback1997information}, \emph{Cosine Similarity}~\cite{rahutomo2012semantic}, \emph{Jensen-Shannon Divergence}~\cite{lin1991divergence}, and  \emph{Classifier Confidence} for the \emph{Sentence Accuracy} classifier described above. 
To assess content preservation, we adopt \emph{PINC}~\cite{chen2011collecting}, \emph{METEOR}~\cite{banerjee-lavie-2005-meteor}, \emph{WMD}~\cite{10.5555/3045118.3045221,WEI202348}, \emph{BERTScore}~\cite{bert-score}, \emph{S\textsuperscript{3}BERT}~\cite{opitz-frank-2022-sbert}, \emph{BLEURT}~\cite{sellam-etal-2020-bleurt}, \emph{TER}~\cite{snover-etal-2006-study}, \emph{TED}~\cite{doi:10.1137/0218082}, \emph{Hybrid\_Simulation}, and \emph{Hybrid\_Learned}. 
For fluency evaluation, we introduce \emph{Perplexity} of \emph{Finetuned GPT-2} and \emph{Finetuned MGPT} on the target styles (see details in Appendix~\ref{appendix:finetune}).

%\vspace{0.15cm}
\paragraph{Novel Metrics}
We parse both source/reference and the system outputs into \emph{AMR} \cite{banarescu-etal-2013-abstract} and \emph{Syntactic Dependencies} \cite{straka-strakova-2017-tokenizing} and measure \emph{Smatch} similarity \cite{cai-knight-2013-smatch} of the resulting graphs/trees.
%\emph{UDPIPE\_SMATCH}~\cite{straka-strakova-2017-tokenizing}, \emph{AMR\_SMATCH}~\cite{shou-lin-2023-evaluate}
%<explain, todo: souro>

%\vspace{0.15cm}
\paragraph{LLM Prompting}
We used GPT-4 \cite{achiam2023gpt} and Llama-3.1 8B \cite{dubey2024llama}, to assess the TST tasks. We employed a Likert-scale-based approach \cite{allen2007likert,mukherjee-etal-2024-large-language} to evaluate style transfer accuracy, content preservation, and fluency. To facilitate direct comparison with \emph{Sentence Accuracy}, we also conducted a binary evaluation for style transfer accuracy (\emph{GPT4-bin-acc, LLAMA-bin-acc}). Detailed prompt instructions are provided in Appendix~\ref{appendix:prompt_details}.

%\vspace{0.15cm}
\paragraph{Hybrid} 
We propose two ensemble-based approaches %-- \emph{Hybrid-Simulation} and \emph{Hybrid-Learned}  -- 
to integrate multiple metrics. % for comprehensive evaluation.
In \emph{Hybrid-Simulation}, we first select the top three metrics (based on correlation with human judgments) for each task and language from Tables~\ref{tab:automatic_evaluation_reference_free_with_style_transfer} and \ref{tab:automatic_evaluation_reference_free_with_content_preservation}. We then conduct a simulation to determine the relative weights of these metrics and compute their geometric average to form the final ensemble score.
In \emph{Hybrid-Learned}, we use a \emph{RandomForestRegressor}~\cite{liaw2002classification} to calculate feature importance for all metrics with respect to human annotations. We select the three metrics with the highest normalized importance scores and again derive their geometric average, weighted by these importance scores, to generate the ensemble result.
For details, see  %on the selected metrics and their respective weights, see 
Tables~\ref{tab:metrics_weight_simulation} and \ref{tab:Important_features-2} in Appendix~\ref{appendix:hybrid}.

%\vspace{0.15cm}
\paragraph{Overall Score}
Following \citet{loakman-etal-2023-twistlist} and \citet{yang-jin-2023-attractive}, we adopt the geometric mean of style transfer accuracy, content preservation, and fluency as a single aggregated score for comparison. Based on the Pearson correlation results from our experiments (Tables \ref{tab:automatic_evaluation_reference_free_with_style_transfer}, \ref{tab:automatic_evaluation_reference_free_with_content_preservation} and, \ref{tab:fluency-results}), we select the best-performing metrics for these three dimensions from existing, \emph{LLM}-based, and newly proposed methods (excluding hybrid approaches), creating \emph{Ours1} score. We then extend \emph{Ours1} by incorporating the top-performing metrics from our proposed approaches, including hybrids, to construct \emph{Ours2}.

\section{Experiment Setup}

%\paragraph{Tasks and Languages}
\paragraph{Evaluation Data: Tasks, Languages and Model Outputs}
We evaluate our methods on the outputs of TST models and human annotations provided by \citet{mukherjee-etal-2024-large-language}.
This comprises two TST tasks -- sentiment transfer (positive to negative statements and vice versa), where data is available for English, Hindi and Bengali, and detoxification (toxic to clean text), with English and Hindi data.
%We evaluate our methods on two popular TST subtasks where multilingual data is available. For sentiment transfer, we perform experiments in English, Hindi, and Bengali, considering both positive-to-negative and negative-to-positive tasks \cite{mukherjee-etal-2024-multilingual-text, mukherjee-etal-2023-low}. For text detoxification, we focus on English and Hindi with the single task of transferring toxic text into clean text \cite{mukherjee2024text, mukherjee-etal-2024-large-language}.
%\paragraph{Models and Their Outputs}
Model outputs for all tasks were produced by 
%We utilize the best-selected model setups and their outputs for our experiments provided by \citet{mukherjee-etal-2024-large-language}, categorized as follows:
%\emph{Closed LLM:} 
GPT-3.5 \cite{chatgpt}, %GPT-4 \cite{openai2023gpt4},
%\emph{Open Models:} 
LLaMA-2-7B-Chat \cite{touvronLlamaOpenFoundation2023,llama3modelcard} and Mistral-7B-Instruct \cite{jiang2023mistral}, %\emph{Previous SOTA:} 
as well as previous finetuned BART models by \citet{mukherjee2024multilingual,mukherjee2024text}.
%\citet{mukherjee2024multilingual}'s models for sentiment transfer (\emph{Joint} and \emph{Parallel}) and \citet{mukherjee2024text}'s models for text detoxification (\emph{Seq2seq + CLS\_OP} and \emph{KT}).

\paragraph{Meta-Evaluation Approach}
We follow common practice for meta-evaluation \cite{kilickaya-etal-2017-evaluating,bert-score,liu-etal-2023-g} and compute all metrics' correlation with human judgment using Pearson (PC), Spearman (SC), and Kendall's Tau (KC) Correlations \cite{schober2018correlation,Puka2011}.

\section{Results Analysis}

Since reference-based metrics generally underperform their reference-free variants, we generally focus on the reference-free setting in the analysis. 
% We include reference-based results in Appendix~\ref{appendix:additional_results}.
We include reference-based results in Appendix~\ref{appendix:additional_results}, along with supplementary statistics in Appendix~\ref{appendix:statistics}, providing both detailed correlation patterns among metrics and insights into their value distributions.

\subsection{Style Transfer Accuracy}

%The meta-evaluation results for Style Transfer Accuracy metrics are presented across two tables based on their reference requirements. Table \ref{tab:automatic_evaluation_reference_free_with_style_transfer} presents results for reference-free metrics, which evaluate style transfer without requiring reference outputs. For completeness, the results of reference-based metrics, which utilize reference outputs in their evaluation, can be found in Table \ref{tab:automatic_evaluation_reference_free_with_style_transfer_with_reference} in Appendix \ref{sec:appendix}.
The results for style transfer accuracy in the reference-free setting are shown in Table~\ref{tab:automatic_evaluation_reference_free_with_style_transfer}. 

\paragraph{Previously Used} \emph{Sentence Accuracy} generally achieves moderate to good correlation with human judgments, suggesting that direct style classification accuracy can be a reliable indicator of style transfer quality. Meanwhile, \emph{EMD} demonstrates a moderate degree of alignment, implying that capturing distributional shifts of stylistic cues correlates respectably with human perceptions.

\paragraph{Newly Applied:} 
% \emph{Classifier Confidence} tends to yield decent alignment with human judgment, indicating that a model’s confidence in its style prediction can be a solid proxy for style shift. \emph{KL Divergence} typically attains relatively strong correlation by focusing on distributional differences in style probabilities. By contrast, \emph{Cosine Similarity} sometimes displays negative correlations, suggesting that simple embedding-based similarity does not always capture the nuances of style change. \emph{Jensen-Shannon Divergence} provides robust performance, underscoring the utility of symmetric distributional measures for style intensity comparisons.
\emph{Classifier Confidence} and \emph{KL Divergence} generally yield strong alignment with human judgments by capturing style-related probabilities, while \emph{Cosine Similarity} can struggle with more nuanced shifts. \emph{Jensen-Shannon Divergence} further proves effective by using a symmetric measure of style intensity.

\paragraph{LLMs:} 
% \emph{GPT-4} consistently achieves very high correlations, indicating that it captures stylistic nuances closely aligned with human assessments. Its variant, \emph{GPT-4-bin-acc}, remains strong but generally lags slightly behind the full GPT-4 output. \emph{LLaMA}, in contrast, often exhibits weaker or even negative correlations, whereas \emph{LLaMA-bin-acc} shows some improvement but still does not match GPT-4-based metrics.
 \emph{GPT-4} exhibits consistently high correlations, whereas \emph{LLaMA} performs notably worse, although a binarized version (\emph{LLaMA-bin-acc}) shows some moderate improvements.

\paragraph{Hybrid:} \emph{Hybrid-Simulation} demonstrates strong alignment with human ratings by combining multiple signals into a single score, while \emph{Hybrid-Learned} performs comparably, though it may fall marginally below its simulation-based counterpart in certain cases.

% Overall, these observations highlight that direct classification metrics can be dependable for gauging style accuracy, while distribution-based measures and LLM-derived evaluations can further enhance alignment with human perceptions. Hybrid methods that integrate multiple metrics prove especially robust, underscoring the value of diverse, advanced approaches for capturing style, content, and fluency in \textsc{TST} tasks.

% Additionally across the tasks, for English, metrics like GPT-4 and Hybrid methods reach very high correlations, indicating that the available signals align closely with human judgments. In Hindi and Bengali, while top metrics still perform strongly (e.g., KL \& Jensen-Shannon Divergence, Hybrid methods), the performance gap among metrics is more evident, reflecting potential linguistic complexities.
Direct classification metrics reliably capture style accuracy, while distribution-based and LLM-based evaluations enhance overall alignment with human judgments, especially when integrated in hybrid frameworks. In English tasks, approaches like GPT-4 and hybrid methods achieve particularly high correlations, whereas in Hindi and Bengali, top metrics (e.g., KL, JS Divergence, and hybrid approaches) remain strong but show more pronounced performance gaps, potentially due to greater linguistic complexity.

%style_transfer table begins from here
\begin{table*}[!t]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lrrr rrr rrr rrr rrr} 
    \toprule
    & \multicolumn{9}{c}{\bf Sentiment Transfer (reference free)} &  \multicolumn{6}{c}{\bf Detoxification (reference free)} \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
    %\toprule
           \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
\midrule
 \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
 \midrule
Sentence Accuracy &  \bf0.51 &  0.49 &  0.48 & \bf0.61 &  0.61 &  0.59 &  \bf0.57 &  0.57 &  0.54 &  \bf0.36 &  0.36 &  0.35 &  0.38 &  0.37 &  0.36  \\
EMD &  0.27 &  0.24 &  0.20 &  0.36 &  0.43 &  0.34 &  0.50 &  0.52 &  0.40 &  0.29 &  0.21 &  0.17 & \bf 0.47 &  0.53 &  0.43  \\
% \midrule
% \multicolumn{16}{c}{\textit{Evaluation using LLMS}}\\
% \midrule
\hdashline
GPT4 & \bf0.92 & 0.81 & 0.79 & \bf0.87 & 0.84 & 0.79 & \bf0.82 & 0.83 & 0.77 & \bf0.74 & 0.72 & 0.65 & \bf0.74 & 0.74 & 0.68  \\
GPT4-bin-acc &  0.89 &  0.78 &  0.77 &  0.84 &  0.83 & 0.80 &  0.77 &  0.78 &  0.74 &  0.61 &  0.61 &  0.59 &  0.60 &  0.61 &  0.59  \\
LLAMA &  0.16 &  0.17 &  0.15 &  -0.11 &  -0.10 &  -0.09 & -0.17 &  -0.15 &  -0.13 &  0.20 &  0.18 &  0.17 &  0.20 &  0.16 &  0.15  \\
LLAMA-bin-acc &  0.49 &  0.44 &  0.43 &  0.50 &  0.51 &  0.49 &  0.31 &  0.31 &  0.30 &  0.24 &  0.24 &  0.23 &  0.27 &  0.27 &  0.27  \\
\midrule
\multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
\midrule
Classifier Confidence &  0.51 &  0.43 &  0.35 &  0.66 &  0.57 &  0.46 &  0.59 &  0.52 &  0.40 &  0.39 &  0.32 &  0.25 &  0.41 &  0.38 &  0.30  \\
KL Divergence &  0.59 &  0.31 &  0.24 &  0.66 &  0.66 &  0.54 &  \textbf{0.62} &  0.62 &  0.50 &  \textbf{0.46} &  0.46 &  0.36 &  0.51 &  0.60 &  0.49  \\
Cosine Similarity &  -0.55 &  -0.44 & -0.36 &  -0.66 &  -0.67 &  -0.54 &  -0.53 &  -0.59 &  -0.46 &  -0.43 &  -0.40 &  -0.32 &  -0.48 &  -0.58 &  -0.47  \\
Jensen-Shannon Divergence &  \bf 0.67 &  0.40 &  0.32 & \bf 0.69 &  0.67 & 0.55 &  \textbf{0.62} & 0.64 & 0.51 &  0.41 & 0.50 & 0.39 & \bf 0.53 &  0.60 & 0.50  \\
\hdashline
Hybrid-Simulation  & \textbf{0.69} &  0.40 &  0.32 & \textbf{0.71} &  0.67 &  0.54 &  \textbf{0.62} & 0.64 & 0.51 &  \bf 0.44 &  0.47 &  0.37 &  0.53 &  0.61 &  0.49 \\
Hybrid-Learned  & 0.67 &  0.37 &  0.30 &  0.70 &  0.63 &  0.50 &  0.61 &  0.62 &  0.49 &  0.43 &  0.47 &  0.37 &  \textbf{0.55} & 0.62 &  0.50 \\
%\bottomrule
 \bottomrule
 %\multicolumn{16}{c}{\textit{Style Transfer Accuracy}}\\
 %\midrule
 %& & & & & & & & & & & & & & &\\
\end{tabular}
% \bigskip
%     \begin{tabular}{lrrr rrr rrr rrr rrr} 
%     \toprule
%     & \multicolumn{9}{c}{\bf Sentiment Transfer (reference-based)} &  \multicolumn{6}{c}{\bf Detoxification (reference-based)} \\
%                & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
%             \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
% \midrule
% %COSINE\_SIM &  \textbf{0.32} &  0.32 &  0.26 &  0.59 &  0.60 &  0.49 &  0.34 &  0.45 &  0.35 &  0.28 &  0.32 &  0.25 &  0.30 &  0.32 &  0.26 \\
% EMD &  -0.22 &  -0.27 &  -0.22 &  -0.28 &  -0.33 &  -0.26 &  -0.33 &  -0.37 &  -0.29 &  -0.28 &  -0.23 &  -0.18 &  -0.31 &  -0.28 &  -0.22 \\
% KL\_DIS &  -0.30 & -0.36 & -0.29 & -0.62 &  -0.58 & -0.46 & -0.46 & -0.48 & -0.38 & -0.34 & -0.30 & -0.24 & -0.36 &  -0.28 &  -0.23 \\
% COSINE\_SIM & \bf0.32 & 0.32 & 0.26 & \bf 0.59 & 0.60 & 0.49 &  \bf 0.34 &  0.45 &  0.35 &  \bf 0.28 & 0.32 & 0.25 &  \bf 0.30 & 0.32 & 0.26 \\
% JS\_SIM &  -0.29 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.47 & -0.37 & -0.28 &  -0.29 &  -0.23 &  -0.35 &  -0.28 &  -0.23 \\
% % EMD &  -0.20 &  -0.25 &  -0.20 &  -0.31 &  -0.36 &  -0.29 &  -0.32 &  -0.36 &  -0.28 &  -0.35 &  -0.37 &  -0.29 &  -0.31 &  -0.28 &  -0.22 \\
% % KL &  -0.32 &  -0.33 &  -0.26 &  -0.63 &  -0.57 &  -0.46 &  -0.43 &  -0.45 &  -0.36 &  -0.34 &  -0.40 &  -0.33 &  -0.36 &  -0.28 &  -0.23 \\
% % COSINE &  0.33 &  0.31 &  0.25 &  0.60 &  0.60 &  0.48 &  0.34 &  0.44 &  0.36 &  0.31 &  0.38 &  0.30 &  0.30 &  0.32 &  0.26 \\
% % JS &  -0.30 &  -0.32 &  -0.26 &  -0.63 &  -0.57 &  -0.46 &  -0.45 &  -0.48 &  -0.39 &  -0.28 &  -0.39 &  -0.32 &  -0.35 &  -0.28 &  -0.23 \\
% % Sentence Accuracy & 0.11 & 0.17 & 0.17 & -0.08 & -0.07 & -0.07 & -0.04 & -0.03 & -0.03 & 0.36 & 0.36 & 0.35 & 0.38 & 0.37 & 0.36\\
% % EMD & -0.22 & -0.27 & -0.22 & -0.28 & -0.33 & -0.26 & -0.33 & -0.37 & -0.29 & -0.28 & -0.23 & -0.18 & -0.31 & -0.28 & -0.22\\
% % KL & -0.30 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.48 & -0.38 & -0.34 & -0.30 & -0.24 & -0.36 & -0.28 & -0.23\\
% %COSINE & 0.32 & 0.32 & 0.26 & 0.59 & 0.60 & 0.49 & 0.34 & 0.45 & 0.35 & 0.28 & 0.32 & 0.25 & 0.30 & 0.32 & 0.26\\
% % JS & -0.29 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.47 & -0.37 & -0.28 & -0.29 & -0.23 & -0.35 & -0.28 & -0.23\\
% % %Style\_Classifier\_
% % Confidence & -0.09 & -0.09 & -0.07 & 0.10 & 0.12 & 0.09 & 0.04 & 0.08 & 0.07 & 0.39 & 0.32 & 0.25 & 0.41 & 0.38 & 0.30\\
% % BLEU &  &   &   &   &   &   &   &   &  &   &   &      &    &   &   \\
% \bottomrule
%    \end{tabular}            
\caption{Style transfer quality (reference-free). Pearson (PC), Spearman (SC) and Kendall's Tau (KC) correlations.}
\label{tab:automatic_evaluation_reference_free_with_style_transfer}
\end{table*}

\begin{table*}[!t]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lrrr rrr rrr rrr rrr} 
    \toprule
    & \multicolumn{9}{c}{\bf Sentiment Transfer (reference free)} &  \multicolumn{6}{c}{\bf Detoxification (reference free)} \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
    %\toprule
           \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
\midrule
 %\midrule
 %\multicolumn{16}{c}{\textit{Content preservation}}\\
 %\midrule
 \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
 \midrule
BLEU & 0.24 & 0.22 & 0.18 & 0.24 & 0.19 & 0.15 & 0.32 & 0.31 & 0.25 & 0.14 & 0.13 & 0.11 & 0.45 & 0.37 & 0.31\\
Cosine Similarity & \bf0.54 & 0.27 & 0.22 & \bf0.33 & 0.24 & 0.20 & \bf0.43 & 0.40 & 0.32 & \bf0.28 & 0.19 & 0.15 & \bf0.59 & 0.45 & 0.38\\
Masked BLEU & 0.21 & 0.21 & 0.17 & 0.15 & 0.12 & 0.10 & 0.23 & 0.24 & 0.19 & 0.15 & 0.15 & 0.12 & 0.45 & 0.39 & 0.32\\
Masked Cosine Similarity & 0.36 & 0.17 & 0.14 & 0.19 & 0.13 & 0.11 & 0.28 & 0.29 & 0.23 & 0.23 & 0.15 & 0.12 & 0.56 & 0.45 & 0.37\\
METEOR & 0.38 & 0.25 & 0.21 & 0.20 & 0.18 & 0.14 & 0.33 & 0.27 & 0.22 & 0.16 & 0.10 & 0.08 & 0.54 & 0.34 & 0.28\\
ROUGE-2 & 0.24 & 0.19 & 0.16 & 0.19 & 0.20 & 0.16 & 0.28 & 0.30 & 0.24 & 0.17 & 0.11 & 0.09 & 0.41 & 0.37 & 0.31\\
ROUGE-L & 0.39 & 0.25 & 0.21 & 0.26 & 0.23 & 0.19 & 0.28 & 0.32 & 0.25 & 0.22 & 0.12 & 0.10 & 0.46 & 0.39 & 0.33\\
%\midrule
%\multicolumn{16}{c}{\textit{Evaluation using LLMS}}\\
%\midrule
\hdashline
GPT4 & \bf0.42 & 0.36 & 0.35 & \bf0.39 & 0.41 & 0.39 & \bf0.51 & 0.54 & 0.48 & \bf0.46 & 0.31 & 0.30 & \bf0.46 & 0.42 & 0.40\\
LLAMA & 0.24 & 0.26 & 0.24 & 0.32 & 0.28 & 0.26 & 0.32 & 0.38 & 0.35 & 0.25 & 0.11 & 0.10 & 0.28 & 0.16 & 0.16\\
\midrule
\multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
\midrule
PINC & -0.18 & -0.17 & -0.15 & -0.16 & -0.12 & -0.10 & -0.27 & -0.28 & -0.23 & -0.12 & -0.12 & -0.10 & -0.41 & -0.36 & -0.30\\
WMD & 0.35 & 0.28 & 0.23 & 0.27 & 0.24 & 0.20 & 0.34 & 0.35 & 0.28 & 0.15 & 0.14 & 0.11 & 0.41 & 0.38 & 0.32\\
BERTScore & \bf 0.50 & 0.31 & 0.26 & \bf 0.45 & 0.33 & 0.27 & 0.49 & 0.44 & 0.36 & 0.21 & 0.19 & 0.15 & \bf 0.62 & 0.38 & 0.31\\
Smatch (Dependency Trees) & 0.25 & 0.24 & 0.20 & 0.18 & 0.20 & 0.17 & 0.26 & 0.30 & 0.25 & 0.16 & 0.15 & 0.12 & 0.34 & 0.31 & 0.26\\
Smatch (AMR) & 0.38 & 0.25 & 0.20 & 0.22 & 0.20 & 0.17 & 0.32 & 0.32 & 0.26 & 0.19 & 0.13 & 0.11 & 0.37 & 0.34 & 0.28\\
S3BERT & 0.46 & 0.23 & 0.19 & 0.30 & 0.18 & 0.14 & 0.30 & 0.30 & 0.24 & 0.22 & 0.20 & 0.16 & 0.49 & 0.38 & 0.31\\
BLEURT & 0.47 & 0.30 & 0.25 & 0.41 & 0.35 & 0.29 & \bf 0.56 & 0.53 & 0.42 & 0.18 & 0.17 & 0.14 & \bf 0.62 & 0.43 & 0.35\\
TER & 0.42 & 0.26 & 0.22 & \bf 0.45 & 0.28 & 0.24 & 0.34 & 0.33 & 0.27 & 0.21 & 0.17 & 0.14 & 0.58 & 0.37 & 0.31 \\%&  41.2 &  52.2 &  79.1 &  39.8 &  57.0  \\
TED &  0.43 &  0.24 &  0.22 &  0.42 &  0.29 &  0.25 &  0.20 &  0.28 &  0.24 &  \bf0.48 &  0.21 &  0.18 &  0.48 &  0.36 &  0.30 \\
\hdashline
% Hybrid\_Simulation  &  0.55 &  0.31 &  0.26 &  \bf0.48 &  0.33 &  0.27 &  \bf0.57 &  0.53 &  0.43 & \textbf{0.48} & 0.21 & 0.17 &  0.63 &  0.41 &  0.34 \\
% Hybrid\_Learned  &  \bf0.56 &  0.32 &  0.26 &  0.47 &  0.35 &  0.29 &  0.56 &  0.53 &  0.43 &  0.15 &  0.17 &  0.14 &  \bf0.64 &  0.38 &  0.31 \\
Hybrid-Simulation & \bf 0.57 &  0.32 &  0.26 &  \bf 0.48 &  0.33 &  0.27 &  \bf 0.57 &  0.53 &  0.43 & \bf 0.28 &  0.19 &  0.15 &  \bf 0.68 &  0.43 &  0.35 \\
Hybrid-Learned &  0.56 &  0.32 &  0.26 &  0.47 &  0.35 &  0.29 &  0.56 &  0.53 &  0.43 &  0.19 &  0.15 &  0.12 &  0.64 &  0.38 &  0.31 \\
 \bottomrule
 %\multicolumn{16}{c}{\textit{Style Transfer Accuracy}}\\
 %\midruleHybrid-Simulation
 %& & & & & & & & & & & & & & &\\
%\end{tabular}
% \bigskip
%     \begin{tabular}{lrrr rrr rrr rrr rrr} 
%     \toprule
%     & \multicolumn{9}{c}{\bf Sentiment Transfer (reference-based)} &  \multicolumn{6}{c}{\bf Detoxification (reference-based)} \\
%                & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
%             \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
% \midrule
%  \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
%  \midrule
% BLEU & 0.18 & 0.22 & 0.18 & 0.17 & 0.16 & 0.13 & 0.19 & 0.20 & 0.16 & 0.10 & 0.10 & 0.08 & 0.18 & 0.18 & 0.14 \\
% COSINE\_SIM & \bf0.39 & 0.26 & 0.22 & \bf0.20 & 0.24 & 0.19 & \bf0.31 & 0.32 & 0.25 & \bf0.18 & 0.13 & 0.10 & \bf0.30 & 0.25 & 0.20 \\
% Masked BLEU & 0.13 & 0.19 & 0.15 & 0.16 & 0.16 & 0.13 & 0.18 & 0.17 & 0.13 & 0.11 & 0.11 & 0.09 & 0.16 & 0.15 & 0.12 \\
% COSINE\_SIM\_MASK & 0.25 & 0.21 & 0.17 & 0.15 & 0.16 & 0.13 & 0.24 & 0.28 & 0.22 & 0.17 & 0.13 & 0.10 & 0.24 & 0.22 & 0.18 \\
% METEOR & 0.31 & 0.22 & 0.18 & 0.12 & 0.13 & 0.10 & 0.16 & 0.18 & 0.14 & 0.11 & 0.09 & 0.08 & 0.23 & 0.17 & 0.14 \\
% ROUGE\_2 & 0.22 & 0.21 & 0.17 & 0.17 & 0.18 & 0.15 & 0.23 & 0.23 & 0.18 & 0.11 & 0.09 & 0.07 & 0.24 & 0.24 & 0.20 \\
% ROUGE\_L & 0.31 & 0.24 & 0.20 & 0.19 & 0.19 & 0.16 & 0.21 & 0.23 & 0.18 & 0.13 & 0.09 & 0.07 & 0.23 & 0.24 & 0.19 \\
% \midrule
% \multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
% \midrule
% PINC & -0.12 & -0.14 & -0.12 & -0.13 & -0.12 & -0.10 & -0.17 & -0.18 & -0.16 & -0.09 & -0.07 & -0.06 & -0.17 & -0.15 & -0.13 \\
% WMD & 0.25 & 0.26 & 0.21 & 0.21 & 0.22 & 0.18 & 0.25 & 0.27 & 0.21 & 0.11 & 0.08 & 0.07 & 0.19 & 0.19 & 0.15 \\
% BERTScore & 0.34 & 0.27 & 0.22 & 0.25 & 0.25 & 0.20 & 0.24 & 0.25 & 0.20 & 0.18 & 0.16 & 0.13 & 0.32 & 0.19 & 0.15 \\
% UDPIPE\_SMATCH & 0.16 & 0.20 & 0.16 & 0.19 & 0.19 & 0.16 & 0.18 & 0.18 & 0.14 & 0.16 & 0.15 & 0.12 & 0.15 & 0.14 & 0.12 \\
% AMR\_SMATCH & 0.28 & 0.27 & 0.22 & 0.22 & 0.20 & 0.17 & 0.25 & 0.24 & 0.19 & 0.12 & 0.09 & 0.07 & 0.18 & 0.17 & 0.14 \\
% S3BERT & \bf0.41 & 0.26 & 0.21 & 0.28 & 0.22 & 0.18 & 0.23 & 0.26 & 0.21 & 0.13 & 0.13 & 0.11 & 0.26 & 0.20 & 0.16 \\
% BLEURT & 0.31 & 0.25 & 0.20 & 0.31 & 0.31 & 0.25 & \bf0.42 & 0.41 & 0.32 & 0.15 & 0.17 & 0.13 & \bf0.35 & 0.23 & 0.19 \\
% TER & 0.35 & 0.23 & 0.19 & \bf0.39 & 0.26 & 0.21 & 0.22 & 0.21 & 0.17 & \bf0.24 & 0.10 & 0.09 & 0.23 & 0.14 & 0.12 \\
% TED &  -0.29 &  -0.23 &  -0.20 &  -0.35 & -0.26 & -0.22 &  -0.17 & -0.15 & -0.13 &  -0.40 &  -0.16 &  -0.13 &  -0.32 &  -0.20 & -0.17 \\
% %Hybrid\_Random &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  0.36 &  0.25 &  0.20 \\
%\bottomrule
   \end{tabular}            
\caption{Content preservation (reference-free). Pearson (PC), Spearman (SC) and Kendall's Tau (KC) correlations.}
\label{tab:automatic_evaluation_reference_free_with_content_preservation}
\end{table*}

%5.3 starts from here
\begin{table*}[!t]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lrrr rrr rrr rrr rrr} 
    \toprule
    & \multicolumn{9}{c}{\bf Sentiment Transfer } &  \multicolumn{6}{c}{\bf Detoxification } \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
    %\toprule
           \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
\midrule
 \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
 \midrule
Perplexity (GPT-2) &  \textbf{0.13} &  0.13 &  0.11 &  -0.11 &  -0.10 &  -0.08 &  -0.11 &  -0.07 &  -0.05 & \textbf{0.06} &  0.00 &  0.00 & \bf0.17 &  -0.13 &  -0.11  \\
Perplexity (MGPT) &  0.08 & 0.19 & 0.15 &  0.00 &  0.07 &  0.05 &  0.16 &  0.19 &  0.15 &  0.05 &  0.00 &  0.00 &  0.11 &  0.03 &  0.03  \\
% \midrule
% \multicolumn{16}{c}{\textit{Evaluation using LLMS}}\\
% \midrule
\hdashline
GPT4 &  \bf0.43 & 0.40 & 0.37 & \bf0.39 & 0.39 & 0.35 & \bf0.37 & 0.40 & 0.36 & \bf0.16 & 0.13 & 0.12 & \bf0.17 & 0.17 & 0.16  \\
LLAMA &  0.17 &  0.18 &  0.17 &  0.15 &  0.17 &  0.15 &  0.08 &  0.06 &  0.06 & \bf0.16 & 0.13 & 0.12 &  -0.01 &  -0.02 &  -0.01  \\
\midrule
\multicolumn{16}{c}{\textit{Newly applied}}\\
\midrule
Perplexity (Finetuned GPT-2) & \textbf{0.14} & 0.16 & 0.13 &  0.08 &  0.14 &  0.11 &  0.02 &  0.05 &  0.04 & \textbf{0.14} &  0.00 &  0.00 &  0.11 &  -0.06 &  -0.05  \\
Perplexity (Finetuned MGPT) &  0.04 &  0.08 &  0.07 & \textbf{0.17} & 0.15 & 0.12 & \textbf{0.23} &  0.21 & 0.16 &  0.00 & 0.03 & 0.03 & \textbf{0.23} &  0.04 &  0.03  \\
 \bottomrule
   \end{tabular}            
\caption{Fluency (reference-free). Pearson (PC), Spearman (SC) and Kendall's Tau (KC) correlations.}
\label{tab:fluency-results}
\end{table*}

\begin{table*}[!ht]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lrrr|rrr|rrr|rrr|rrr} 
    \toprule
    & \multicolumn{9}{c|}{\bf Sentiment Transfer } &  \multicolumn{6}{c}{\bf Detoxification } \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c|}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
    %\toprule
           \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\
\midrule
% Existing &  0.02 &  -0.04 &  -0.03 &  -0.04 &  -0.03 &  -0.02 &  -0.12 &  -0.09 &  -0.07 &  -0.04 &  -0.13 &  -0.10 &  -0.38 &  -0.39 &  -0.30  \\
% \hdashline
% GPT4 &  0.73 &  0.62 &  0.54 &  0.78 &  0.75 &  0.61 &  0.78 &  0.77 &  0.63 &  0.65 &  0.62 &  0.51 &  0.62 &  0.59 &  0.46  \\
% LLAMA &  0.08 &  0.16 &  0.13 &  0.02 &  0.01 &  0.01 &  0.01 &  0.01 &  0.00 &  0.18 &  0.14 &  0.11 &  0.27 &  0.23 &  0.19  \\
% \hdashline
% Ours1 &  0.14 &  0.11 &  0.09 &  0.47 &  0.39 &  0.31 &  0.35 &  0.36 &  0.26 &  0.04 &  0.10 &  0.08 &  0.19 &  0.32 &  0.25  \\
% Ours2 &  0.21 &  0.20 &  0.15 &  0.59 &  0.55 &  0.42 &  0.39 &  0.42 &  0.29 &  0.07 &  0.06 &  0.05 &  0.60 &  0.55 &  0.42  \\
Existing &  0.32 &  0.02 &  0.02 &  0.11 &  -0.02 &  -0.01 &  0.25 &  0.18 &  0.13 &  -0.04 &  -0.18 &  -0.14 &  0.07 &  -0.19 &  -0.14 \\
Human &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 &  1.00 \\
\hdashline
GPT4 &  \bf 0.73 &  0.62 &  0.54 & \bf 0.78 &  0.75 &  0.61 &  \bf 0.78 &  0.77 &  0.63 & \bf 0.65 &  0.62 &  0.51 & 0.62 &  0.59 &  0.46 \\
LLAMA &  0.08 &  0.16 &  0.13 &  0.02 &  0.01 &  0.01 &  0.01 &  0.01 &  0.00 &  0.18 &  0.14 &  0.11 &  0.27 &  0.23 &  0.19 \\
\hdashline
Ours1 &  0.57 &  0.33 &  0.26 &  0.59 &  0.54 &  0.43 &  0.54 &  0.57 &  0.42 &  0.38 &  0.44 &  0.34 &  0.47 &  0.43 &  0.32 \\ 
Ours2 &  0.68 &  0.40 &  0.31 &  0.72 &  0.68 &  0.53 &  0.59 &  0.59 &  0.42 &  0.41 &  0.38 &  0.29 & \bf 0.63 &  0.57 &  0.43 \\
 \bottomrule
   \end{tabular}            
\caption{Overall results (reference-free). Pearson (PC), Spearman (SC) and Kendall's Tau (KC) correlations.}
\label{tab:overall-results}
\end{table*}

% \begin{table*}[!ht]
% \addtolength{\tabcolsep}{-0.55pt}
% %\def\tblspc{\rule{0pt}{3.15ex}}
% \scriptsize\centering
%     \begin{tabular}{lll|rrr rrr rrr rrr rrr} 
%    % \toprule
%    % & \multicolumn{9}{c|}{\bf Sentiment Transfer } &  \multicolumn{6}{c}{\bf Detoxification } \\ [8ex]
%    % & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c|}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\ [8ex]
% 	%\multirow{2}{4em}{\bf Task} & \multirow{2}{4em}{\bf Languages} & \multirow{2}{4em}{\bf Approach} & \multicolumn{15}{c}{\bf Metrics} \\ [18ex]
%      %& &  & \multicolumn{15}{c}{\bf Metrics} \\ %[18ex]
%     %\midrule
%    \bf Task & \bf Languages & \bf Approach & \turnbox{90}{\bf BERTScore} & \turnbox{90}{\bf BLEURT} & \turnbox{90}{\bf COSINE\_SIM} &  \turnbox{90}{\bf Hybrid\_Learned\_CP} &  \turnbox{90}{\bf Hybrid\_Simulation\_CP} & \turnbox{90}{\bf Hybrid\_Simulation\_ST} & \turnbox{90}{\bf JS} & \turnbox{90}{\bf KL} & \turnbox{90}{\bf MGPT\_PPL} & \turnbox{90}{\bf MGPT\_FT\_PPL} & \turnbox{90}{\bf GPT2\_PPL} & \turnbox{90}{\bf GPT2\_FT\_PPL} & \turnbox{90}{\bf Sentence\_Accuracy} & \turnbox{90}{\bf TED} & \turnbox{90}{\bf TER}\\ %[3ex]
%     \midrule
%  \multirow{9}{4em}{Sentiment Transfer} & \multirow{3}{4em}{ English} & Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
%    & & Ours1 & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%    & & Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%  %  \hdashline
%    & \multirow{3}{4em}{Hindi} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours1 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark \\
%    & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%  %  \hdashline
%    & \multirow{3}{4em}{Bengali} & Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours1 & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%   \hdashline
%  \multirow{6}{4em}{Detoxification}  & \multirow{3}{*}{English} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours1 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark \\
%    & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%  %  \hdashline
%    & \multirow{3}{4em}{Hindi} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours1 & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%    & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
%  \bottomrule
%    \end{tabular}            
% \caption{Overall metrics and their component columns by task and language.}
% \label{tab:overall_metrics_details}
% \end{table*}

% \paragraph{Summary}
% The presence of strong correlations for GPT-4-based metrics and the hybrid methods underscores the importance of leveraging both advanced model outputs and sophisticated combination strategies. The negative correlations from COSINE highlight that not all embedding-based similarity metrics are suitable for capturing style intensity. Distribution-based metrics (KL, JS) and hybrid approaches offer promising paths forward.

% \textbf{GPT-4} and related variants provide a high upper bound on correlation with human judgments.
% \textbf{KL, JS, EMD} and hybrid metrics show that distributional and composite approaches can robustly model style transfer intensity.
% \textbf{Language and Task Differences}: While English and Bengali often yield higher correlations, Hindi results suggest more nuanced challenges.
% Overall, these results emphasize that advanced metrics, whether based on powerful LLMs like GPT-4 or on carefully chosen hybrid and distributional measures, are crucial for accurately reflecting human judgments of style transfer accuracy in a multilingual, reference-free setting.

\subsection{Content Preservation}
% The meta-evaluation results for content preservation metrics are presented across two tables based on their reference requirements. Table \ref{tab:automatic_evaluation_reference_free_with_content_preservation} presents results for reference-free metrics, which evaluate style transfer without requiring reference outputs. For completeness, the results of reference-based metrics, which utilize reference outputs in their evaluation, can be found in Table~\ref{tab:automatic_evaluation_reference_free_with_content_preservation_with_reference} in Appendix~\ref{appendix:additional_results}.
We present the meta-evaluation of content preservation metrics by their reference requirements: Table~\ref{tab:automatic_evaluation_reference_free_with_content_preservation} covers reference-free methods, while Table~\ref{tab:automatic_evaluation_reference_free_with_content_preservation_with_reference} in Appendix~\ref{appendix:additional_results} details reference-based approaches.


\paragraph{Previously Used:} \emph{BLEU} generally shows low alignment with human judgments, while \emph{Cosine Similarity} exhibits better performance in several tasks. \emph{Masked BLEU} and \emph{Masked Cosine Similarity} offer slight improvements over their unmasked counterparts, yet they still lag behind more recent methods. \emph{ROUGE-2} and \emph{ROUGE-L} provide moderate correlations but do not consistently outperform newer metrics.

\paragraph{Newly Applied:} \emph{BLEURT} remains consistently reliable, while \emph{BERTScore} also proves robust across various styles and languages. \emph{TER} and \emph{TED} offer competitive results, particularly for certain language-specific tasks. In contrast, \emph{PINC} displays negative correlations, suggesting it may be less suited for capturing style intensity shifts in these scenarios.

\paragraph{Novel:}
It outperforms or at least matches the performance of traditional metrics, though it generally falls behind the newly introduced text-based methods and LLM-driven approaches on average.

\paragraph{LLMs:} \emph{GPT-4} achieves higher correlations than traditional metrics across different styles and languages, demonstrating its strong ability to capture human-like judgments of text transformations. In contrast, \emph{LLAMA} tends to underperform, indicating considerable variability in how well different LLMs reflect stylistic and content-based shifts.

\paragraph{Hybrid:}
\emph{Hybrid-Simulation} achieves robust alignment with human ratings by unifying multiple signals into a single score, whereas \emph{Hybrid-Learned} shows comparable performance, albeit slightly trailing the simulation-based approach in some scenarios.
% Also, for both the tasks Bengali and English show higher correlation scores for most metrics than Hindi, possibly due to linguistic or stylistic complexities in Hindi texts.

\subsection{Fluency}
% Table \ref{tab:fluency-results} presents results for fluency evaluation.
% \paragraph{Perplexity-Based:} \emph{GPT-2 Perplexity} shows limited correlation with human judgments, with some minor positive and negative correlations across tasks. \emph{Finetuned GPT-2 Perplexity} marginally improves the results, but these small gains suggest that perplexity alone may not be sufficient to capture the full complexity of human fluency assessments in the context of TST. Similarly, \emph{MGPT Perplexity} and \emph{Finetuned MGPT Perplexity} show moderate improvements, particularly when fine-tuning is applied, highlighting the benefits of using multilingual models and target-style fine-tuning to improve perplexity metrics' alignment with human evaluations of fluency.
Table~\ref{tab:fluency-results} presents fluency evaluation results. \emph{GPT-2 Perplexity} displays limited correlations with human judgments, while \emph{Finetuned GPT-2 Perplexity} yields only marginal gains. \emph{MGPT Perplexity} and \emph{Finetuned MGPT Perplexity} provide moderate improvements, particularly under fine-tuning, underscoring the importance of multilingual modeling and style-specific training for better alignment with human fluency assessments.
\emph{LLMs:} \emph{GPT-4} demonstrates relatively strong correlations with human assessments of fluency for sentiment-related tasks, suggesting it captures fluidity and coherence more effectively when the stylistic shift involves changing sentiment. However, for detoxification tasks, its alignment with human judgments diminishes, indicating that removing toxicity poses different challenges for GPT-4. In contrast, \emph{LLaMA} exhibits generally weaker correlations and struggles in various settings, implying that its evaluations of fluency do not consistently match human perceptions.

Moreover, differences across languages persist; English often yields slightly better or at least more stable correlations, while Hindi and Bengali results vary more substantially.

\subsection{Overall Score}
Table~\ref{tab:overall_metrics_details} shows the single aggregate score. % aggregating - style transfer accuracy, content preservation, and fluency.

\paragraph{Previously Used:} 
% We find that traditional metrics are aggregated, they generally yield near-zero or even negative correlations in various languages and tasks. This suggests that simply merging conventional measures often fails to capture the multi-faceted nature of overall quality—encompassing style, content, and fluency—aligned with human judgments.
Aggregating traditional metrics often yields near-zero or negative correlations across various languages and tasks, indicating that simply merging these measures fails to capture the multi-faceted nature of overall quality—encompassing style, content, and fluency—in line with human judgments.

\paragraph{LLM:} 
% In contrast, GPT-4 consistently demonstrates strong alignment with human assessments of overall quality for both Sentiment Transfer and Detoxification tasks. Its robust performance reinforces earlier observations that GPT-4 effectively mirrors human perceptions, suggesting that its contextual understanding can capture the composite dimensions of text style transfer.
% Although it achieves some low to moderate positive correlations, LLaMA’s performance generally remains weaker than GPT-4. This disparity highlights that not all large language models exhibit the same capacity for evaluating comprehensive quality criteria in text style transfer.
\emph{GPT-4} consistently aligns well with human assessments of overall quality in both Sentiment Transfer and Detoxification, demonstrating its capacity to capture the multifaceted nature of text style transfer. \emph{LLaMA}, however, shows weaker correlations, indicating that not all large language models possess the same evaluative capabilities.

\paragraph{Ours:} Our approaches (\emph{Ours1} and \emph{Ours2}) provide noticeable improvements over existing methods. Although these do not surpass GPT-4, these clearly outperform many traditional and alternative measures.

\section{Conclusion}
We presented a comprehensive evaluation of existing and newly proposed metrics for two \textsc{TST} subtasks—\emph{Sentiment Transfer} and \emph{Text Detoxification}—in English, Hindi, and Bengali. We show that traditional metrics like BLEU and ROUGE often exhibit limited correlation with human judgments and are substantially surpassed by our own experimental metrics, hybrid approaches (ensembles) as well as prompted LLMs. %Moreover, hybrid approaches, which combine multiple metrics, show a possibility to outperform individual methods. %We also demonstrate the potential of leveraging large language models as evaluators, indicating the value of diverse and advanced metrics that capture style, content preservation, and fluency without reliance on references.
Our study is limited to two specific tasks and three languages, leaving open the question of how well these metrics generalize to other styles, languages, and domains.
% Additionally, scaling to larger datasets with fine-grained human annotations could enable the development of learned, multi-task evaluation metrics that jointly account for style, content, and fluency.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\onecolumn
\appendix

\section{Details - Hybrid approaches}
\label{appendix:hybrid}
In this section, we introduce our hybrid approaches by presenting both the selected metrics and their associated simulated weights, as well as the learned normalized feature importance. Further details on these weights, selected metrics, and feature scores can be found in Tables~\ref{tab:metrics_weight_simulation} and \ref{tab:Important_features-2} respectively.

\begin{table*}[!ht]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lrrrrrr|rrrrr} 
    \toprule
    & \multicolumn{6}{c|}{\bf Sentiment Transfer } &  \multicolumn{4}{c}{\bf Detoxification } \\
    \toprule
               & \multicolumn{3}{c}{\bf Simulation}& \multicolumn{3}{c|}{\bf Learned}& \multicolumn{2}{c}{\bf Simulation} & \multicolumn{2}{c}{\bf Learned}\\
    %\toprule
           \bf Metrics & \bf English & \bf Hindi & \bf Bengali & \bf  English & \bf Hindi & \bf Bengali & \bf English & \bf Hindi & \bf  English & \bf Hindi \\ 
% \midrule
BERTScore & 0.20 & 0.40 & 0.40 &  - & 0.36  & 0.14  & - & -  & -  & 0.30 \\
BERTScore\_IDF & -  &  - &  - &  0.27 &  0.35 &  - & - & -  & -  & 0.11 \\
BLEURT & 0.30  & 0.20  & 0.50  &  0.43 & 0.29  & 0.65  & - & 0.40 &  - & -\\
BLEU &  - & -  & -  &  - &  - &  - & - &  - & 0.34  & -\\
Masked BLEU &  - &  - & -  &  - & -  &  - & - &  - & 0.25  & -\\
COSINE & 0.50  & - &  0.10 &  0.30 & -  & 0.21 & 0.20 & 0.30 &  - & -\\
TER &  - & 0.40  &  - & -  & -  &  - & 0.10 &  0.30 & -  & 0.59 \\
TED &  - &  - & -  &  - &  - &  - & 0.70 &  - & 0.40  & - \\
% %
 \bottomrule
\end{tabular}            
% % \caption{Hybrid Simulation and Learned metric weights}
% % \label{tab:metrics_weight_simulation}
% % \end{table*}
\bigskip

% % \begin{table*}[t]
% % \addtolength{\tabcolsep}{-3.55pt}
% % \def\tblspc{\rule{0pt}{3.15ex}}
% % \scriptsize\centering
    \begin{tabular}{lrrrrrr|rrrrr} 
    \toprule
    & \multicolumn{6}{c|}{\bf Sentiment Transfer } &  \multicolumn{4}{c}{\bf Detoxification } \\
    \toprule
               & \multicolumn{3}{c}{\bf Simulation}& \multicolumn{3}{c|}{\bf Learned}& \multicolumn{2}{c}{\bf Simulation} & \multicolumn{2}{c}{\bf Learned}\\
%     %\toprule
           \bf Metrics & \bf English & \bf Hindi & \bf Bengali & \bf  English & \bf Hindi & \bf Bengali & \bf English & \bf Hindi & \bf  English & \bf Hindi \\ 
\midrule
EMD &  - &  - &  - &  - &  0.33 &  0.24 &  - &  - &  - &  -  \\
JS &  0.60 &  0.40 &  0.40 &  0.38 &  0.46 &  0.35 &  0.30 &  0.30 &  0.42 &  0.45  \\
KL &  0.15 &  0.20 &  0.30 &  0.38 &  - &  0.41 &  0.50 &  0.50 &  0.27 &  0.22  \\
Style\_Classifier\_Confidence &  0.25 &  0.40 &  0.30 &  0.24 &  0.21 &  - &  0.20 &  0.20 &  0.31 &  0.33  \\
% % BERTScore\_IDF & -  &  - &  - &  - &  - &  0.27 & 0.33 & -  & -  & 0.11 \\
% % BLEURT & 0.25  & 0.20  & 0.50  &  - & 0.40  & 0.43  &  &  0.65 &  - & -\\
% % BLEU &   & -  & -  &  - &  - &  - & - &  - & 0.35  & -\\
% % Masked BLEU &  - &  - & -  &  - & -  &  - & - &  - & 0.26  & -\\
% % COSINE & 0.50  &  - &  0.10 &  0.20 & -  &  0.30 & - & 0.21  &  - & -\\
% % TER &  - & 0.40  &  - & 0.40  & 0.30  &  - & 0.33 &  - & -  & 0.59 \\
% % TED &  - &  - & -  &  0.40 &  - &  - & - &  - & 0.40  & - \\
% % %
 \bottomrule
   \end{tabular}        
\caption{Hybrid Simulation - selected metrics and its weights.}
\label{tab:metrics_weight_simulation}
\end{table*}

% This is a section in the appendix. commentred features & wights+ 5.1-5.5

\begin{table*}[!ht]
%\addtolength{\tabcolsep}{-0.55pt}
%\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{l|rrr|rr} 
    \toprule
    & \multicolumn{3}{c|}{\bf Sentiment Transfer (CS) } &  \multicolumn{2}{c}{\bf Detoxification } \\
     \bf Metrics  & \bf English& \bf Hindi& \bf Bengali & \bf English& \bf Hindi\\
    %\toprule
          % \bf Metrics & \multicolumn{3}{c}{ \bf Feature Importance}& \multicolumn{2}{c}{\bf Feature Importance}\\
\midrule
BLEURT &  0.16 &  0.13 &  0.37 &  0.05 &  0.04  \\
COSINE &  0.11 &  0.08 &  0.12 &  0.08 &  0.04  \\
BERTScore\_IDF &  0.10 &  0.16 &  0.03 &  0.05 &  0.19  \\
BERTScore &  0.09 &  0.17 &  0.08 &  0.05 &  0.07  \\
S3BERT &  0.07 &  0.08 &  0.04 &  0.05 &  0.02  \\
WMD &  0.07 &  0.03 &  0.03 &  0.04 &  0.01  \\
AMR\_SMATCH &  0.06 & 0.02 &  0.02 &  0.05 &  0.02  \\
BLEU &  0.06 &  0.03 &  0.07 &  0.12 &  0.03  \\
ROUGE-L &  0.06 &  0.02 &  0.03 &  0.07 &  0.05  \\
Masked Cosine Similarity &  0.06 &  0.02 &  0.02 &  0.06 &  0.04  \\
Masked BLEU &  0.05 & 0.04 &  0.03 &  0.09 &  0.02  \\
METEOR &  0.03 & 0.04 &  0.04 &  0.05 &  0.02  \\
TED &  0.02 & 0.04 &  0.02 &  0.14 &  0.02  \\
SMATCH &  0.02 &  0.02 &  0.02 &  0.02 &  0.03  \\
TER &  0.02 &  0.16 &  0.04 &  0.03 &  0.38  \\
ROUGE-2 &  0.01 &  0.02 &  0.03 &  0.04 &  0.01  \\
PINC & 0.01 &  0.01 &  0.01 &  0.02 &  0.01  \\
 \bottomrule
   \end{tabular}     
\bigskip
    \begin{tabular}{l|rrr|rr} 
    \toprule
    & \multicolumn{3}{c|}{\bf Sentiment Transfer (SA)} &  \multicolumn{2}{c}{\bf Detoxification } \\
     \bf Metrics  & \bf English& \bf Hindi& \bf Bengali & \bf English& \bf Hindi\\
    %\toprule
          % \bf Metrics & \multicolumn{3}{c}{ \bf Feature Importance}& \multicolumn{2}{c}{\bf Feature Importance}\\
\midrule
KL &  0.34 &  0.17 &  0.33 &  0.21 &  0.18  \\
JS &  0.33 &  0.38 &  0.29 &  0.33 &  0.37  \\
Style\_Classifier\_Confidence &  0.21 &  0.17 &  0.18 &  0.25 &  0.26  \\
EMD &  0.11 &  0.27 &  0.20 &  0.21 &  0.18  \\
Sentence\_Accuracy &  0.01 &  0.00 &  0.00 &  0.00 &  0.01  \\
 \bottomrule
   \end{tabular}            
\caption{Hybrid-Learned - metrics and its learned feature importance scores (normalized).}
\label{tab:Important_features-2}
\end{table*}
%\bigskip
%\newpage
\begin{table*}[!ht]
\addtolength{\tabcolsep}{-0.55pt}
%\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
    \begin{tabular}{lll|rrr rrr rrr rrr rrr} 
   % \toprule
   % & \multicolumn{9}{c|}{\bf Sentiment Transfer } &  \multicolumn{6}{c}{\bf Detoxification } \\ [8ex]
   % & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c|}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\ [8ex]
	%\multirow{2}{4em}{\bf Task} & \multirow{2}{4em}{\bf Languages} & \multirow{2}{4em}{\bf Approach} & \multicolumn{15}{c}{\bf Metrics} \\ [18ex]
     %& &  & \multicolumn{15}{c}{\bf Metrics} \\ %[18ex]
    %\midrule
   \bf Task & \bf Languages & \bf Approach & \turnbox{90}{\bf BERTScore} & \turnbox{90}{\bf BLEURT} & \turnbox{90}{\bf Cosine Similarity} &  \turnbox{90}{\bf Hybrid\_Learned\_CP} &  \turnbox{90}{\bf Hybrid\_Simulation\_CP} & \turnbox{90}{\bf Hybrid\_Simulation\_ST} & \turnbox{90}{\bf JS} & \turnbox{90}{\bf KL} & \turnbox{90}{\bf Perplexity (MGPT)} & \turnbox{90}{\bf MGPT\_FT\_PPL} & \turnbox{90}{\bf Perplexity (GPT-2)} & \turnbox{90}{\bf GPT2\_FT\_PPL} & \turnbox{90}{\bf Sentence Accuracy} & \turnbox{90}{\bf TED} & \turnbox{90}{\bf TER}\\ %[3ex]
    \midrule
 \multirow{9}{4em}{Sentiment Transfer} & \multirow{3}{4em}{ English} & Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
   & & Ours1 & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
   & & Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 %  \hdashline
   & \multirow{3}{4em}{Hindi} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours1 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark \\
   & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 %  \hdashline
   & \multirow{3}{4em}{Bengali} & Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours1 & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
  \hdashline
 \multirow{6}{4em}{Detoxification}  & \multirow{3}{*}{English} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours1 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark \\
   & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 %  \hdashline
   & \multirow{3}{4em}{Hindi} &Existing & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours1 & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
   & &Ours2 & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{blue}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 \bottomrule
   \end{tabular}            
\caption{Overall Scores - language and task wise selected metrics.}
\label{tab:overall_metrics_details}
\end{table*}

\section{Additional Results (reference-based)}
\label{appendix:additional_results}
In addition to the reference-free evaluations shown in Tables~\ref{tab:automatic_evaluation_reference_free_with_style_transfer} and~\ref{tab:automatic_evaluation_reference_free_with_content_preservation}, the corresponding reference-based results are provided in Tables~\ref{tab:automatic_evaluation_reference_free_with_style_transfer_with_reference} and~\ref{tab:automatic_evaluation_reference_free_with_content_preservation_with_reference}, respectively.

\begin{table*}[!ht]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
%     \begin{tabular}{lrrr rrr rrr rrr rrr} 
%     \toprule
%     & \multicolumn{9}{c}{\bf Sentiment Transfer (reference free)} &  \multicolumn{6}{c}{\bf Detoxification (reference free)} \\
%                & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
%     %\toprule
%            \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
% \midrule
%  \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
%  \midrule
% Classifier\_Accuracy &  \bf0.51 &  0.49 &  0.48 & \bf0.61 &  0.61 &  0.59 &  \bf0.57 &  0.57 &  0.54 &  \bf0.36 &  0.36 &  0.35 &  0.38 &  0.37 &  0.36  \\
% EMD &  0.27 &  0.24 &  0.20 &  0.36 &  0.43 &  0.34 &  0.50 &  0.52 &  0.40 &  0.29 &  0.21 &  0.17 & \bf 0.47 &  0.53 &  0.43  \\
% % \midrule
% % \multicolumn{16}{c}{\textit{Evaluation using LLMS}}\\
% % \midrule
% \hdashline
% GPT4 & \bf0.92 & 0.81 & 0.79 & \bf0.87 & 0.84 & 0.79 & \bf0.82 & 0.83 & 0.77 & \bf0.74 & 0.72 & 0.65 & \bf0.74 & 0.74 & 0.68  \\
% GPT4\_bin\_acc &  0.89 &  0.78 &  0.77 &  0.84 &  0.83 & 0.80 &  0.77 &  0.78 &  0.74 &  0.61 &  0.61 &  0.59 &  0.60 &  0.61 &  0.59  \\
% LLAMA &  0.16 &  0.17 &  0.15 &  -0.11 &  -0.10 &  -0.09 & -0.17 &  -0.15 &  -0.13 &  0.20 &  0.18 &  0.17 &  0.20 &  0.16 &  0.15  \\
% LLAMA\_bin\_acc &  0.49 &  0.44 &  0.43 &  0.50 &  0.51 &  0.49 &  0.31 &  0.31 &  0.30 &  0.24 &  0.24 &  0.23 &  0.27 &  0.27 &  0.27  \\
% \midrule
% \multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
% \midrule
% Classifier\_Confidence &  0.51 &  0.43 &  0.35 &  0.66 &  0.57 &  0.46 &  0.59 &  0.52 &  0.40 &  0.39 &  0.32 &  0.25 &  0.41 &  0.38 &  0.30  \\
% KL\_DIS &  0.59 &  0.31 &  0.24 &  0.66 &  0.66 &  0.54 &  \textbf{0.62} &  0.62 &  0.50 &  \textbf{0.46} &  0.46 &  0.36 &  0.51 &  0.60 &  0.49  \\
% COSINE\_SIM &  -0.55 &  -0.44 & -0.36 &  -0.66 &  -0.67 &  -0.54 &  -0.53 &  -0.59 &  -0.46 &  -0.43 &  -0.40 &  -0.32 &  -0.48 &  -0.58 &  -0.47  \\
% JS\_DIS &  \bf 0.67 &  0.40 &  0.32 & \bf 0.69 &  0.67 & 0.55 &  \textbf{0.62} & 0.64 & 0.51 &  0.41 & 0.50 & 0.39 & \bf 0.53 &  0.60 & 0.50  \\
% \hdashline
% Hybrid\_Simulation  & \textbf{0.69} &  0.40 &  0.32 & \textbf{0.71} &  0.67 &  0.54 &  \textbf{0.62} & 0.64 & 0.51 &  \bf 0.44 &  0.47 &  0.37 &  0.53 &  0.61 &  0.49 \\
% Hybrid\_Learned  & 0.67 &  0.37 &  0.30 &  0.70 &  0.63 &  0.50 &  0.61 &  0.62 &  0.49 &  0.43 &  0.47 &  0.37 &  \textbf{0.55} & 0.62 &  0.50 \\
% %\bottomrule
%  \bottomrule
%  %\multicolumn{16}{c}{\textit{Style Transfer Accuracy}}\\
%  %\midrule
%  %& & & & & & & & & & & & & & &\\
% \end{tabular}
% \bigskip
    \begin{tabular}{lrrr rrr rrr rrr rrr} 
    \toprule
    & \multicolumn{9}{c}{\bf Sentiment Transfer (reference-based)} &  \multicolumn{6}{c}{\bf Detoxification (reference-based)} \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
            \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
\midrule
%COSINE\_SIM &  \textbf{0.32} &  0.32 &  0.26 &  0.59 &  0.60 &  0.49 &  0.34 &  0.45 &  0.35 &  0.28 &  0.32 &  0.25 &  0.30 &  0.32 &  0.26 \\
EMD &  -0.22 &  -0.27 &  -0.22 &  -0.28 &  -0.33 &  -0.26 &  -0.33 &  -0.37 &  -0.29 &  -0.28 &  -0.23 &  -0.18 &  -0.31 &  -0.28 &  -0.22 \\
KL\_DIS &  -0.30 & -0.36 & -0.29 & -0.62 &  -0.58 & -0.46 & -0.46 & -0.48 & -0.38 & -0.34 & -0.30 & -0.24 & -0.36 &  -0.28 &  -0.23 \\
Cosine Similarity & \bf0.32 & 0.32 & 0.26 & \bf 0.59 & 0.60 & 0.49 &  \bf 0.34 &  0.45 &  0.35 &  \bf 0.28 & 0.32 & 0.25 &  \bf 0.30 & 0.32 & 0.26 \\
JS\_SIM &  -0.29 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.47 & -0.37 & -0.28 &  -0.29 &  -0.23 &  -0.35 &  -0.28 &  -0.23 \\
% EMD &  -0.20 &  -0.25 &  -0.20 &  -0.31 &  -0.36 &  -0.29 &  -0.32 &  -0.36 &  -0.28 &  -0.35 &  -0.37 &  -0.29 &  -0.31 &  -0.28 &  -0.22 \\
% KL &  -0.32 &  -0.33 &  -0.26 &  -0.63 &  -0.57 &  -0.46 &  -0.43 &  -0.45 &  -0.36 &  -0.34 &  -0.40 &  -0.33 &  -0.36 &  -0.28 &  -0.23 \\
% COSINE &  0.33 &  0.31 &  0.25 &  0.60 &  0.60 &  0.48 &  0.34 &  0.44 &  0.36 &  0.31 &  0.38 &  0.30 &  0.30 &  0.32 &  0.26 \\
% JS &  -0.30 &  -0.32 &  -0.26 &  -0.63 &  -0.57 &  -0.46 &  -0.45 &  -0.48 &  -0.39 &  -0.28 &  -0.39 &  -0.32 &  -0.35 &  -0.28 &  -0.23 \\
% Sentence Accuracy & 0.11 & 0.17 & 0.17 & -0.08 & -0.07 & -0.07 & -0.04 & -0.03 & -0.03 & 0.36 & 0.36 & 0.35 & 0.38 & 0.37 & 0.36\\
% EMD & -0.22 & -0.27 & -0.22 & -0.28 & -0.33 & -0.26 & -0.33 & -0.37 & -0.29 & -0.28 & -0.23 & -0.18 & -0.31 & -0.28 & -0.22\\
% KL & -0.30 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.48 & -0.38 & -0.34 & -0.30 & -0.24 & -0.36 & -0.28 & -0.23\\
%COSINE & 0.32 & 0.32 & 0.26 & 0.59 & 0.60 & 0.49 & 0.34 & 0.45 & 0.35 & 0.28 & 0.32 & 0.25 & 0.30 & 0.32 & 0.26\\
% JS & -0.29 & -0.36 & -0.29 & -0.62 & -0.58 & -0.46 & -0.46 & -0.47 & -0.37 & -0.28 & -0.29 & -0.23 & -0.35 & -0.28 & -0.23\\
% %Style\_Classifier\_
% Confidence & -0.09 & -0.09 & -0.07 & 0.10 & 0.12 & 0.09 & 0.04 & 0.08 & 0.07 & 0.39 & 0.32 & 0.25 & 0.41 & 0.38 & 0.30\\
% BLEU &  &   &   &   &   &   &   &   &  &   &   &      &    &   &   \\
\bottomrule
   \end{tabular}            
\caption{Automatic metrics results reference-based: style transfer. Pearson Correlation: PC, Spearman Correlation: SC
Kendall Tau Correlation: KC}
\label{tab:automatic_evaluation_reference_free_with_style_transfer_with_reference}
\end{table*}

\begin{table*}[!ht]
\addtolength{\tabcolsep}{-3.55pt}
\def\tblspc{\rule{0pt}{3.15ex}}
\scriptsize\centering
%     \begin{tabular}{lrrr rrr rrr rrr rrr} 
%     \toprule
%     & \multicolumn{9}{c}{\bf Sentiment Transfer (reference free)} &  \multicolumn{6}{c}{\bf Detoxification (reference free)} \\
%                & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
%     %\toprule
%            \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
% \midrule
%  %\midrule
%  %\multicolumn{16}{c}{\textit{Content preservation}}\\
%  %\midrule
%  \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
%  \midrule
% BLEU & 0.24 & 0.22 & 0.18 & 0.24 & 0.19 & 0.15 & 0.32 & 0.31 & 0.25 & 0.14 & 0.13 & 0.11 & 0.45 & 0.37 & 0.31\\
% COSINE\_SIM & \bf0.54 & 0.27 & 0.22 & \bf0.33 & 0.24 & 0.20 & \bf0.43 & 0.40 & 0.32 & \bf0.28 & 0.19 & 0.15 & \bf0.59 & 0.45 & 0.38\\
% Masked BLEU & 0.21 & 0.21 & 0.17 & 0.15 & 0.12 & 0.10 & 0.23 & 0.24 & 0.19 & 0.15 & 0.15 & 0.12 & 0.45 & 0.39 & 0.32\\
% COSINE\_SIM\_MASK & 0.36 & 0.17 & 0.14 & 0.19 & 0.13 & 0.11 & 0.28 & 0.29 & 0.23 & 0.23 & 0.15 & 0.12 & 0.56 & 0.45 & 0.37\\
% METEOR & 0.38 & 0.25 & 0.21 & 0.20 & 0.18 & 0.14 & 0.33 & 0.27 & 0.22 & 0.16 & 0.10 & 0.08 & 0.54 & 0.34 & 0.28\\
% ROUGE\_2 & 0.24 & 0.19 & 0.16 & 0.19 & 0.20 & 0.16 & 0.28 & 0.30 & 0.24 & 0.17 & 0.11 & 0.09 & 0.41 & 0.37 & 0.31\\
% ROUGE\_L & 0.39 & 0.25 & 0.21 & 0.26 & 0.23 & 0.19 & 0.28 & 0.32 & 0.25 & 0.22 & 0.12 & 0.10 & 0.46 & 0.39 & 0.33\\
% %\midrule
% %\multicolumn{16}{c}{\textit{Evaluation using LLMS}}\\
% %\midrule
% \hdashline
% GPT4 & \bf0.42 & 0.36 & 0.35 & \bf0.39 & 0.41 & 0.39 & \bf0.51 & 0.54 & 0.48 & \bf0.46 & 0.31 & 0.30 & \bf0.46 & 0.42 & 0.40\\
% LLAMA & 0.24 & 0.26 & 0.24 & 0.32 & 0.28 & 0.26 & 0.32 & 0.38 & 0.35 & 0.25 & 0.11 & 0.10 & 0.28 & 0.16 & 0.16\\
% \midrule
% \multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
% \midrule
% PINC & -0.18 & -0.17 & -0.15 & -0.16 & -0.12 & -0.10 & -0.27 & -0.28 & -0.23 & -0.12 & -0.12 & -0.10 & -0.41 & -0.36 & -0.30\\
% WMD & 0.35 & 0.28 & 0.23 & 0.27 & 0.24 & 0.20 & 0.34 & 0.35 & 0.28 & 0.15 & 0.14 & 0.11 & 0.41 & 0.38 & 0.32\\
% BERTScore & \bf 0.50 & 0.31 & 0.26 & \bf 0.45 & 0.33 & 0.27 & 0.49 & 0.44 & 0.36 & 0.21 & 0.19 & 0.15 & \bf 0.62 & 0.38 & 0.31\\
% UDPIPE\_SMATCH & 0.25 & 0.24 & 0.20 & 0.18 & 0.20 & 0.17 & 0.26 & 0.30 & 0.25 & 0.16 & 0.15 & 0.12 & 0.34 & 0.31 & 0.26\\
% AMR\_SMATCH & 0.38 & 0.25 & 0.20 & 0.22 & 0.20 & 0.17 & 0.32 & 0.32 & 0.26 & 0.19 & 0.13 & 0.11 & 0.37 & 0.34 & 0.28\\
% S3BERT & 0.46 & 0.23 & 0.19 & 0.30 & 0.18 & 0.14 & 0.30 & 0.30 & 0.24 & 0.22 & 0.20 & 0.16 & 0.49 & 0.38 & 0.31\\
% BLEURT & 0.47 & 0.30 & 0.25 & 0.41 & 0.35 & 0.29 & \bf 0.56 & 0.53 & 0.42 & 0.18 & 0.17 & 0.14 & \bf 0.62 & 0.43 & 0.35\\
% TER & 0.42 & 0.26 & 0.22 & \bf 0.45 & 0.28 & 0.24 & 0.34 & 0.33 & 0.27 & 0.21 & 0.17 & 0.14 & 0.58 & 0.37 & 0.31 \\%&  41.2 &  52.2 &  79.1 &  39.8 &  57.0  \\
% TED &  0.43 &  0.24 &  0.22 &  0.42 &  0.29 &  0.25 &  0.20 &  0.28 &  0.24 &  \bf0.48 &  0.21 &  0.18 &  0.48 &  0.36 &  0.30 \\
% \hdashline
% % Hybrid\_Simulation  &  0.55 &  0.31 &  0.26 &  \bf0.48 &  0.33 &  0.27 &  \bf0.57 &  0.53 &  0.43 & \textbf{0.48} & 0.21 & 0.17 &  0.63 &  0.41 &  0.34 \\
% % Hybrid\_Learned  &  \bf0.56 &  0.32 &  0.26 &  0.47 &  0.35 &  0.29 &  0.56 &  0.53 &  0.43 &  0.15 &  0.17 &  0.14 &  \bf0.64 &  0.38 &  0.31 \\
% Hybrid\_Simulation & \bf 0.57 &  0.32 &  0.26 &  \bf 0.48 &  0.33 &  0.27 &  \bf 0.57 &  0.53 &  0.43 & \bf 0.28 &  0.19 &  0.15 &  \bf 0.68 &  0.43 &  0.35 \\
% Hybrid\_Learned &  0.56 &  0.32 &  0.26 &  0.47 &  0.35 &  0.29 &  0.56 &  0.53 &  0.43 &  0.19 &  0.15 &  0.12 &  0.64 &  0.38 &  0.31 \\
%  \bottomrule
%  %\multicolumn{16}{c}{\textit{Style Transfer Accuracy}}\\
%  %\midruleHybrid-Simulation
%  %& & & & & & & & & & & & & & &\\
% \end{tabular}
% \bigskip
    \begin{tabular}{lrrr rrr rrr rrr rrr} 
    \toprule
    & \multicolumn{9}{c}{\bf Sentiment Transfer (reference-based)} &  \multicolumn{6}{c}{\bf Detoxification (reference-based)} \\
               & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}& \multicolumn{3}{c}{\bf Bengali} & \multicolumn{3}{c}{\bf English}& \multicolumn{3}{c}{\bf Hindi}\\
            \bf Metrics & \bf PC & \bf SC & \bf KC & \bf  PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC & \bf PC & \bf SC & \bf KC \\ %& \bf AVG & \bf ACC & \bf CS & \bf BL & \bf AVG \\
\midrule
 \multicolumn{16}{c}{\textit{Previously used \& LLMs}}\\
 \midrule
BLEU & 0.18 & 0.22 & 0.18 & 0.17 & 0.16 & 0.13 & 0.19 & 0.20 & 0.16 & 0.10 & 0.10 & 0.08 & 0.18 & 0.18 & 0.14 \\
Cosine Similarity & \bf0.39 & 0.26 & 0.22 & \bf0.20 & 0.24 & 0.19 & \bf0.31 & 0.32 & 0.25 & \bf0.18 & 0.13 & 0.10 & \bf0.30 & 0.25 & 0.20 \\
Masked BLEU & 0.13 & 0.19 & 0.15 & 0.16 & 0.16 & 0.13 & 0.18 & 0.17 & 0.13 & 0.11 & 0.11 & 0.09 & 0.16 & 0.15 & 0.12 \\
Masked Cosine Similarity & 0.25 & 0.21 & 0.17 & 0.15 & 0.16 & 0.13 & 0.24 & 0.28 & 0.22 & 0.17 & 0.13 & 0.10 & 0.24 & 0.22 & 0.18 \\
METEOR & 0.31 & 0.22 & 0.18 & 0.12 & 0.13 & 0.10 & 0.16 & 0.18 & 0.14 & 0.11 & 0.09 & 0.08 & 0.23 & 0.17 & 0.14 \\
ROUGE-2 & 0.22 & 0.21 & 0.17 & 0.17 & 0.18 & 0.15 & 0.23 & 0.23 & 0.18 & 0.11 & 0.09 & 0.07 & 0.24 & 0.24 & 0.20 \\
ROUGE-L & 0.31 & 0.24 & 0.20 & 0.19 & 0.19 & 0.16 & 0.21 & 0.23 & 0.18 & 0.13 & 0.09 & 0.07 & 0.23 & 0.24 & 0.19 \\
\midrule
\multicolumn{16}{c}{\textit{Newly applied \& Novel}}\\
\midrule
PINC & -0.12 & -0.14 & -0.12 & -0.13 & -0.12 & -0.10 & -0.17 & -0.18 & -0.16 & -0.09 & -0.07 & -0.06 & -0.17 & -0.15 & -0.13 \\
WMD & 0.25 & 0.26 & 0.21 & 0.21 & 0.22 & 0.18 & 0.25 & 0.27 & 0.21 & 0.11 & 0.08 & 0.07 & 0.19 & 0.19 & 0.15 \\
BERTScore & 0.34 & 0.27 & 0.22 & 0.25 & 0.25 & 0.20 & 0.24 & 0.25 & 0.20 & 0.18 & 0.16 & 0.13 & 0.32 & 0.19 & 0.15 \\
UDPIPE\_SMATCH & 0.16 & 0.20 & 0.16 & 0.19 & 0.19 & 0.16 & 0.18 & 0.18 & 0.14 & 0.16 & 0.15 & 0.12 & 0.15 & 0.14 & 0.12 \\
AMR\_SMATCH & 0.28 & 0.27 & 0.22 & 0.22 & 0.20 & 0.17 & 0.25 & 0.24 & 0.19 & 0.12 & 0.09 & 0.07 & 0.18 & 0.17 & 0.14 \\
S3BERT & \bf0.41 & 0.26 & 0.21 & 0.28 & 0.22 & 0.18 & 0.23 & 0.26 & 0.21 & 0.13 & 0.13 & 0.11 & 0.26 & 0.20 & 0.16 \\
BLEURT & 0.31 & 0.25 & 0.20 & 0.31 & 0.31 & 0.25 & \bf0.42 & 0.41 & 0.32 & 0.15 & 0.17 & 0.13 & \bf0.35 & 0.23 & 0.19 \\
TER & 0.35 & 0.23 & 0.19 & \bf0.39 & 0.26 & 0.21 & 0.22 & 0.21 & 0.17 & \bf0.24 & 0.10 & 0.09 & 0.23 & 0.14 & 0.12 \\
TED &  -0.29 &  -0.23 &  -0.20 &  -0.35 & -0.26 & -0.22 &  -0.17 & -0.15 & -0.13 &  -0.40 &  -0.16 &  -0.13 &  -0.32 &  -0.20 & -0.17 \\
%Hybrid\_Random &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  - &  0.36 &  0.25 &  0.20 \\
\bottomrule
   \end{tabular}            
\caption{Automatic metrics results reference-based: content preservation. Pearson Correlation: PC, Spearman Correlation: SC
Kendall Tau Correlation: KC}
\label{tab:automatic_evaluation_reference_free_with_content_preservation_with_reference}
\end{table*}

\section{GPT-2 and MGPT Finetune Details}
\label{appendix:finetune}

We fine-tune both \emph{GPT-2}\footnote{\url{https://huggingface.co/openai-community/gpt2}} and \emph{mGPT}\footnote{\url{https://huggingface.co/ai-forever/mGPT}} using the same hyperparameter configuration obtained through few random optimization experiments. Specifically, we set the maximum token length to 512 and use the target-style training data from~\cite{mukherjee-etal-2024-large-language} for fine-tuning. Each model is trained for 10 epochs with a batch size of 2, a learning rate of \(1 \times 10^{-5}\), and a weight decay of \(0.01\).

\section{Prompt Details}
\label{appendix:prompt_details}
This section provides a collection of example prompts (in English) for the evaluation of Text Sentiment Transfer task (prompt details in Tables \ref{table:prompt-1}, \ref{table:prompt-2}, \ref{table:prompt-3} and \ref{table:prompt-4})

\begin{table*}
    \centering\small
    \begin{tabular}{p{0.07\linewidth} p{0.88\linewidth}}
      \toprule
      \textbf{Prompt} & {\color{cyan!80!yellow!80!black!100 } Sentiment transfer task: transfer the sentiment of a sentence (from positive to negative or negative to positive) while keeping the rest of the sentiment-independent content unchanged.}\\
    \\
    & {\color{green!100!yellow!70!black!100!} Please rate the sentiment transfer accuracy of the negative to positive sentiment transfer task between the following English source sentence S1 and the sentiment-transferred sentence S2. Use a scale of 1 to 5, where 1 indicates that the sentiment in S1 is completely identical to the sentiment in S2, and 5 indicates that the sentiment has been completely transferred to the target sentiment in S2.} \\
    & {\color{orange!50!yellow!90!black!100!} S1: so he can charge a bloody fortune for them.
    
    S2: so he can charge a fair amount of money for them.}\\
    & {\color{orange!50!yellow!90!black!100!}Sentiment transfer accuracy rating (on a scale of 1 to 5) =}\\
    \bottomrule
\end{tabular}
\caption{A Sample prompt for Sentiment Transfer Accuracy evaluation in Sentiment Transfer in English.
It contains {\color{cyan!80!yellow!80!black!100} task definition}, {\color{green!100!yellow!70!black!100!} instruction}, and {\color{orange!50!yellow!90!black!100!} input}.}
\label{table:prompt-1}
\end{table*}

\begin{table*}
    \centering\small
    \begin{tabular}{p{0.07\linewidth} p{0.88\linewidth}}
      \toprule
      \textbf{Prompt} & {\color{cyan!80!yellow!80!black!100 } Sentiment transfer task: transfer the sentiment of a sentence (from positive to negative or negative to positive) while keeping the rest of the sentiment-independent content unchanged.}\\
    \\
    & {\color{green!100!yellow!70!black!100!} Please act as a binary classifier to evaluate the sentiment transfer accuracy of the positive to negative sentiment transfer task in English. Determine whether the target sentiment has been successfully transferred to the generated sentence (S2) from the source sentence (S1).If the target sentiment has been successfully transferred to S2, output `1`. If the target sentiment has not been successfully transferred to S2, output `0`.} \\
    & {\color{orange!50!yellow!90!black!100!} S1: so he can charge a bloody fortune for them.
    
    S2: so he can charge a fair amount of money for them.}\\
    & {\color{orange!50!yellow!90!black!100!}Sentiment transfer accuracy classification (0 or 1) =}\\
    \bottomrule
\end{tabular}
\caption{A Sample prompt for Sentiment Transfer Accuracy (binary) evaluation in Sentiment Transfer in English.
It contains {\color{cyan!80!yellow!80!black!100} task definition}, {\color{green!100!yellow!70!black!100!} instruction}, and {\color{orange!50!yellow!90!black!100!} input}.}
\label{table:prompt-2}
\end{table*}

\begin{table*}
    \centering\small
    \begin{tabular}{p{0.07\linewidth} p{0.88\linewidth}}
      \toprule
      \textbf{Prompt} & {\color{cyan!80!yellow!80!black!100 } Sentiment transfer task: transfer the sentiment of a sentence (from positive to negative or negative to positive) while keeping the rest of the content unchanged.}\\
    & {\color{green!100!yellow!70!black!100!} Please rate the content preservation between the following English source sentence S1 and the sentiment-transferred sentence S2 for the negative to positive sentiment transfer task on a scale of 1 to 5, where 1 indicates very low content preservation and 5 indicates very high content preservation. To determine the content preservation between these two sentences, consider only the information conveyed by the sentences and ignore any differences in sentiment due to the negative to positive sentiment transfer.} \\
    & {\color{orange!50!yellow!90!black!100!} S1: so he can charge a bloody fortune for them.

    S2: so he can charge a fair amount of money for them.}\\
    & {\color{orange!50!yellow!90!black!100!}Content Preservation rating (on a scale of 1 to 5) =}\\
    \bottomrule
\end{tabular}
\caption{A sample prompt for Content Preservation evaluation in Sentiment Transfer in English.
It contains {\color{cyan!80!yellow!80!black!100} task definition}, {\color{green!100!yellow!70!black!100!} instruction}, and {\color{orange!50!yellow!90!black!100!} input}.}
\label{table:prompt-3}
\end{table*}

\begin{table*}
    \centering\small
    \begin{tabular}{p{0.07\linewidth} p{0.88\linewidth}}
      \toprule
   \textbf{Prompt} & {\color{green!100!yellow!70!black!100!} Please rate the fluency of the following English sentence S on a scale of 1 to 5, where 1 represents poor fluency, and 5 represents excellent fluency.} \\
    & {\color{orange!50!yellow!90!black!100!} S: so he can charge a fair amount of money for them.}\\
    & {\color{orange!50!yellow!90!black!100!} Fluency rating (on a scale of 1 to 5) =}\\
    \bottomrule
\end{tabular}
\caption{A same prompt for Fluency evaluation in Sentiment Transfer in English.
It contains {\color{green!100!yellow!70!black!100!} instruction}, and {\color{orange!50!yellow!90!black!100!} input}.}
\label{table:prompt-4}
\end{table*}


\section{Additional Statistics}
\label{appendix:statistics}
In this section, we provide additional statistics for the Text Sentiment Transfer task in English, focusing on reference-free evaluation metrics. Specifically, we present heatmaps illustrating the correlations between each pair of metrics for style transfer accuracy, content preservation, and fluency in Figures~\ref{fig:heatmap_sa}, \ref{fig:heatmap_cp}, and \ref{fig:heatmap_fl}, respectively. We also show the distribution of each metric’s values in Figures~\ref{fig:distribution_style_transfer}, \ref{fig:distribution_cs}, and \ref{fig:distribution_fluency} for style transfer accuracy, content preservation, and fluency, thereby offering a more comprehensive view of their behavior.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Sentence_Accuracy_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_style_accuracy_gpt4_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_style_accuracy_human_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_style_accuracy_llama_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Style_Classifier_Confidence_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_EMD_distribution.png}\hfill    
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_gpt4_bin_acc_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_JS_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_KL_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_llama_bin_acc_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_COSINE_distribution.png} 
    \caption{Style Transfer Accuracy - metrics' value distribution.}
    \label{fig:distribution_style_transfer}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en__ROUGE_2_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en__ROUGE_L_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_AMR_SMATCH_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_BERTScore_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_BLEU_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_BLEU_MASK_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_BLEURT_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_content_preservation_gpt4_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_content_preservation_human_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_content_preservation_llama_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_COSINE_SIM_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_COSINE_SIM_MASK_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_METEOR_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_S3BERT_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_SMATCH_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_TED_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_TER_distribution.png}\hfill 
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_WMD_distribution.png}\hfill
    \caption{Content Preservation- - metrics' value distribution.}
    \label{fig:distribution_cs}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Perplexity_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Perplexity_FT_distribution.png}\hfil
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Perplexity_GPT2_distribution.png}\hfill
    \includegraphics[width=0.30\linewidth]{figs/distr/sentiment_en_Perplexity_GPT2_FT_distribution.png}           
    \caption{Fluency - metrics' value distribution.}
    \label{fig:distribution_fluency}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/heatmap/sentiment_en_sa_heatmap.png}
    \caption{Sentence Accuracy - correlations' heatmap between the metrics.}
    \label{fig:heatmap_sa}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/heatmap/sentiment_en_cp_heatmap.png}
    \caption{Content Preservation - correlations' heatmap between the metrics.}
    \label{fig:heatmap_cp}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figs/heatmap/sentiment_en_fl_heatmap.png}
    \caption{Fluency - correlations' heatmap between the metrics.}
    \label{fig:heatmap_fl}
\end{figure*}

\end{document}
