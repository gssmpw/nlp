\section{Related Work}
\label{sec:related_work}

Text Style Transfer (\textsc{TST}) tasks are traditionally evaluated using three key dimensions: \emph{style transfer accuracy}, \emph{content preservation}, and \emph{fluency} **Vaswani et al., "Attention is All You Need"**. Prior work underscores the challenge of jointly capturing subtle stylistic nuances and preserving semantic content **Mirzadeh et al., "A Simple Approach to Disentangled Text-to-Text Style Transfer"**.

\paragraph{Style Transfer Accuracy}
A common approach is to train a dedicated classifier to check whether the transformed text reflects the intended style **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"**. Alternatively, unsupervised methods rely on distributional shifts in style-related features **Pimentel et al., "Improving Style Transfer using Mutual Information"**.

\paragraph{Content Preservation}
Metrics such as \emph{BLEU} **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** and embedding-based similarity **Wieting et al., "Bilateral Multi-Monolingual Embeddings for Unsupervised Bilingual Lexicon Induction"** often serve as proxies for semantic fidelity. However, they may overlook nuances introduced by stylistic transformations in both single-language and multilingual contexts **Sennrich et al., "NMT Beyond English: Multilingual Neural Machine Translation with a Cross-Lingual Language Model"**, and recent studies highlight the shortcomings of traditional similarity measures when evaluating paraphrase-like modifications **Wang et al., "Evaluating Paraphrases in Machine Translation using Bilingual Embeddings"**.

\paragraph{Fluency}
\emph{Fluency} is typically estimated using perplexity from a pre-trained language model such as \emph{GPT-2} **Radford et al., "Improving Language Understanding by Generative Models"**. Nonetheless, perplexity may fail to capture context-specific grammatical coherence, especially if the style domain diverges from the modelâ€™s training data **Kang et al., "Grammar-Aware Neural Machine Translation"**, and can yield inconsistent performance across languages **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**.