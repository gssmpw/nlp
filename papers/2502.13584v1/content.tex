\section{Introduction}
Active Electronically Scanned Array (AESA) radar systems deployed to fighter aircraft are required to perform surveillance (search) and tracking (revisit) \cite{white2008radar} to scan for new targets and update existing track information for use by the pilot. Determining the task that will generate the most information is referred to as sensor management \cite{hero2007foundations,hero2011sensor}.

Reinforcement learning (RL) is a machine learning approach used to train an autonomous agent to generate a set of actions $a_{k}$ based on information contained within an observation state $s_{k}$ and a reward calculated based on the impact of the policy on the environment. RL has been employed in many domains such as drone racing \cite{Kaufmann2023} or wilderness search and rescue \cite{Ewers2024} with RL outperforming state of the art algorithms. A common reason is that RL is able to learn underlying patterns within the data that traditional methods cannot.

Within radar task management, RL has been applied to the sensor management problem before. 
\cite{Shaghaghi2018} proposes a RL-based approach with monte carlo tree search whilst \cite{Qu2019} uses earliest start time with RL. However, both only consider the tracking portion and neither implement a full Multi-Target Tracking (MTT) simulation. \cite{Xiong2023} applies multi agent RL to a 2D simulation but also does not consider the search aspect. Therefore this paper proposes a RL solution to the problem in a full 3D MTT environment for both search and tracking. 

The methodology for this paper is introduced in \autoref{sect:method} with results then being presented in \autoref{sect:results}. A conclusion is drawn in \autoref{sect:conclusion_future_work}. 


% \cite{Xiong2023} - MARL tracking
% - 2D simulation
% - Infinite range beam with probability of detection simulation (right words?)
% >  Assume that all targets are well-separated such that the data association processing [4] is not required. 
% No complex MTT
% > In addition, assume that the number and the initial states of moving targets are known, which can be obtained by some initial tracking algorithms
% No search

% \cite{Shaghaghi2018} - RL radar task management
% - Discrete action space
% - Only tracking
% - No tracking-based simulation

% \cite{Qu2019} - RL radar task management using EST algorithm
% - Discrete action space
% - Only tracking
% - No tracking-based simulation



\section{Methodology}
\label{sect:method}
\subsection{Simulation}
The agent is trained in a basic but representative environment simulating a typical engagement with a observer platform scanning an area of interest. The RADAR is assumed to generate measurements in a field-of-view of $9\degree$. Since the sensor is assumed to be an AESA RADAR, the agent actions are considered to occur instantaneously, with no penalty for scanning sectors further away from the current line-of-sight. A fixed number of targets are simulated with all targets birthed at $t=t_{0}$ with deaths at $t=t_{f}$. A simulation time step of $50\text{ms}$ with the sensor generating observations at the same frequency as the MTT algorithm for simplicity.
% \inote{Jan - Clutter simulation parameters}
% \begin{itemize}
%     \item Single sensor
%     \begin{itemize}
%         \item Geometrical detection model -> is target within FOV cone
%         \item $9$deg FOV
%         \item Instant control action
%         \item $-\pi/3 \leq \psi,\theta \leq \pi/3$
%         \item Returns elevation bearing and range with noise covariance matrix $ = diag(1e-6rad, 1e-6rad, 25m)$
%     \end{itemize}
%     \item Multiple targets (fixed number, no births or deaths)
%     \begin{itemize}
%         \item $[x, \dot x, y, \dot y, z, \dot z]$
%         \item Samples from a multivariate norm
%         \item Velocity is updated using the same transitions as in the tracker
%     \end{itemize}
%     \item $50\si{\milli\second}$ time step
%     \item UKF predictor and updater
%     \item GNN with 2D assignment wit mahalanobis distance hypothesiser
%     \item Frobenius norm of covariance based deleter (threshold (5000))
%     \item Simple initiator % https://stonesoup.readthedocs.io/en/v1.4/stonesoup.initiator.html#stonesoup.initiator.simple.SimpleMeasurementInitiator
% \end{itemize}

\subsubsection{Multiple Target Tracker Feature Extraction}
The sensor management agent utilises knowledge of current tracks in the area-of-interest as observations to determine the agent policy. In the simplest case, the agent could take observations from RADAR returns. However, there are several limitations of this approach. The first is that RADAR observations are noise-corrupted and will contain clutter or anomalies which will degrade agent performance and lead to a requirement for increased training time. Most importantly, RADAR returns will provide information only when a sector is scanned, with no method of maintaining target information between sector revisits. To combat this, a MTT algorithm is implemented as part of the feature extraction process improve the quality of the observation data, by populating a track list of kinematic target states at the current timestep. This track list will be maintained using a constant-velocity model which is typical in industry applications \cite{li2003survey}. 
The MTT algorithm is implemented using the open-source Stone Soup package \cite{thomas2017open,barr2022stone} using an Unscented Kalman filter estimator \cite{julier2004unscented} for performing measurement updates. Track \textit{assignment} is performed with the nearest neighour algorithm using the 2D Munkres assignment algorithm and the Mahalanobis distance metric as the hypothesiser to populate the cost matrix. It is assumed that the targets birth at the start of the simulation and live throughout the duration of the scenario. Tracks are deleted from the track list if the Frobenius norm of the target covariance is above a set threshold. A constant-velocity target model is used as the predictor component with a state vector $\hat{\mathbf{x}}_{k}=\begin{bmatrix}x&\dot{x}&y&\dot{y}&z&\dot{z}\end{bmatrix}^{\top}\in\mathbb{R}^{6}$, with the 1D state transition matrix $\mathbf{F}_{k}$ and process noise covariance $\mathbf{Q}_{k}$ shown in (\ref{eq:process_model}). The measurement function $\boldsymbol{h}\left(\hat{\mathbf{x}}\right)\;:\;\mathbb{R}^{6}\rightarrow{}S^{2}\times{}\mathbb{R}^{1}$ maps the Cartesian target position to body-spherical coordinates of the target with respect to the observer platform $\left[\psi,\theta,r\right]^{\top}$. The measurements are generated with a noise distribution given by the measurement noise covariance matrix in (\ref{eq:measurement_model}).
\begin{equation}\label{eq:process_model}
\mathbf{F}_{k,\mathrm{1D}}=\begin{bmatrix}
    1&\Delta{t}\\0&1\end{bmatrix},\;\mathbf{Q}_{k,\mathrm{1D}}=\tilde{q}\begin{bmatrix}\frac{1}{3}\Delta{t}^{3}&\frac{1}{2}\Delta{t}^{2}\\\frac{1}{2}\Delta{t}^{2}&\Delta{t}\end{bmatrix}
\end{equation}
\begin{equation}\label{eq:measurement_model}
\mathbf{R}_{k}=\begin{bmatrix}
    \sigma^{2}_{\psi}&0&0\\0&\sigma^{2}_{\theta}&0\\0&0&\sigma^{2}_{r}\end{bmatrix}=\begin{bmatrix}
    (1\times{}10^{-3})^{2}&0&0\\0&(1\times{}10^{-3})^{2}&0\\0&0&5^2\end{bmatrix}
\end{equation}

\subsection{Scan History}

A scan is modeled as the bivariate Gaussian distribution with zero-correlation:
\begin{equation}
    p(\vec x, \vec \mu, \vec \sigma) = \frac{1}{2\pi\sigma_a\sigma_b} \exp{\left[ -\frac{1}{2}\left(
        \left(\frac{x_a-\mu_{a}}{\sigma_a}\right)^2 +  \left(\frac{x_b-\mu_{b}}{\sigma_b}\right)^2 
    \right)
    \right]} 
    \label{eq:bivariate_normal_distribution}    
\end{equation}

The initial scan value has a covariance of $\sigma_a = \sigma_b = \sigma_0$. 
This value is calculated such that the beam contains $95\%$ of \eqref{eq:bivariate_normal_distribution}. 
Applying a two-tailed critical z-value test with $\alpha=0.05$ gives us a z-value of $1.96$ giving $\sigma_0 = \frac{\mathrm{FoV}}{3.92}$.

In order to loose confidence in a scan over time, the covariance is diffused with every time step using:
\begin{gather}
    % \sigma_{i,1} = \sigma_{i,0} (1+\gamma) \\
    % \sigma_{i,2} = \sigma_{i,1} (1+\gamma) = \sigma_{i,0} (1+\gamma) (1+\gamma) \\
    \sigma_{t} = \sigma_{0} (1+\gamma)^t
    \label{eq:sigma_t}
\end{gather}

The ratio of the maximum value at $t=T$ to $t=0$ is $\zeta= \sigma_T^2 / \sigma_0^2$ which, when using \eqref{eq:sigma_t}, can be solve for $\gamma$ with 
$\gamma = \zeta^{-1/2T} - 1$.

% The ratio of the maximum value at $t=T$ to $t=0$ is $\zeta$ which is given by
% \begin{equation}
%     \zeta = \frac{p(\vec \mu, \vec \mu, \sigma_T)}{p(\vec \mu, \vec \mu, \sigma_0)}
%     = \frac{\sigma_T^2}{\sigma_0^2} 
%     \label{eq:zeta_derivation_0}
% \end{equation}
% Substituting \eqref{eq:sigma_t} into \eqref{eq:zeta_derivation_0} with $t=T$ gives
% \begin{equation}
%     \zeta = \frac{\sigma_0^2}{\sigma_0^2(1+\gamma)^{2T}} = (1+\gamma)^{-2T} \label{eq:zeta_wrt_gamma}
% \end{equation}
% Rearranging \eqref{eq:zeta_wrt_gamma} to solve for $\gamma$ gives
% \begin{equation}
%     \gamma = \frac{1}{\sqrt[2T]{\zeta}} - 1 \label{eq:gamma}
% \end{equation}

The maximum value of \eqref{eq:bivariate_normal_distribution} is at $\vec x = \vec \mu$ and with $\sigma_a = \sigma_b = \sigma$.
However, given the diffusion of the covariance this is not the maximum at all steps.
The scan history adds a single scan per step so the global maximum then becomes
\begin{equation}
    p_{\textrm{max},T} = \sum^T_{t=0} \frac{1}{2\pi\sigma_0^2(1+\gamma)^{2t}}
    \label{eq:bivariate_normal_distribution_max}
\end{equation}
% Applying the geometric series formula $\sum_{k=0}^n ar^k= a\frac{1-r^{n+1}}{1-r}$ and then simplifying to get the closed form solution gives
% \begin{equation}
%     p_{\textrm{max},T} = \frac{(1+\gamma)^2\left(1-(1+\gamma)^{-2(T+1)}\right)}{2\pi\sigma_0^2(\gamma^2+2\gamma)}
% \end{equation}

% The maximum value of \eqref{eq:bivariate_normal_distribution} is at $\vec x = \vec \mu$.
% \begin{equation}
% p(\vec \mu, \vec \mu, \vec \sigma) = 
% \frac{1}{2\pi\sigma_a\sigma_b} e^0
% \label{eqn:bivariate_normal_max_derivation_0}
% \end{equation}
% With $\sigma_a = \sigma_b = \sigma$, \eqref{eqn:bivariate_normal_max_derivation_0} becomes
% \begin{equation}
% p(\vec \mu, \vec \mu, \sigma) = 
% \frac{1}{2\pi\sigma^2}
% \label{eqn:bivariate_normal_max_derivation_0}
% \end{equation}
% which is the maximum value possible value of a given bivariate normal distribution. 
% However, given the diffusion of the covariance this is not the maximum at all steps.
% The look history adds a single look per step so the global maximum then becomes
% \begin{equation}
%     p_{\textrm{max},T} = \sum^T_{t=0} \frac{1}{2\pi\sigma_0^2(1+\gamma)^{2t}}
% \end{equation}
% Applying the geometric series formula $\sum_{k=0}^n ar^k= a\frac{1-r^{n+1}}{1-r}$ and then simplifying to get the closed form solution gives
% \begin{equation}
%     p_{\textrm{max},T} = \frac{(1+\gamma)^2\left(1-(1+\gamma)^{-2(T+1)}\right)}{2\pi\sigma_0^2(\gamma^2+2\gamma)}
% \end{equation}

A single scan is thusly modeled as the tuple $(\mu, t)$ with $\mu = [\psi,\theta]$ and $t$ is the time step at which the scan is added to the set of scans $L = [l_0,l_1,\dots,l_n]$. $L$ is modeled as a first-in first-out (FIFO) queue with a maximum length of $N_{SV}$. The scan value at a given point $\vec x$ after time $t=T$ is then 
\begin{equation}
    \mathrm{SV}(\vec x, T) = \sum^L_l p(\vec x, \mu_l, \sigma_0(1+\gamma)^{T-t_l})
\end{equation}
The scaled scan value is then $\mathrm{SSV}(\vec x, T) = \frac{\mathrm{SV}(\vec x, T)}{p_{\mathrm{max}, 4}}$.


\subsection{Action Space}

The action space is discrete and is defined by
\begin{gather}
   N_a =  \left\lceil\frac{\mathrm{IFOV}}{ \frac{\sqrt{2}}{2}\mathrm{FOV}}\right\rceil \label{eqn:action_steps_per_axis} \\
    \left\{a \in \mathbb{Z}^2 ~|~ 0 \leq a < N_a\right\}
\end{gather}
were $a$ is mapped into bearing $psi$ and elevation $theta$ with
\begin{equation}\label{eq:actionToBearings}
    [\psi, \theta] = \frac{\pi}{3}
    \left(
    \frac
        {2 \vec a }
        { N_a - 1}
    -1
    \right) \in [-\pi/3, \pi/3]
\end{equation}
The $\frac{\sqrt{2}}{2}$ term in \eqref{eqn:action_steps_per_axis} accounts for the circular shape of the sensor beam. This introduces overlapping actions within the searchable space but it prevents any unreachable areas. Without this term $21\%$ of the area would be unsearchable.

% \begin{figure}
%     \centering
%     \subfloat[$\mathrm{FOV}$\label{fig:beam_pattern_with_gaps}]{\includegraphics[width=0.45\linewidth]{fov_gaps.png}}
%     \subfloat[$\frac{\sqrt{2}}{2}\mathrm{FOV}$\label{fig:beam_pattern_without_gaps}]{\includegraphics[width=0.45\linewidth]{fov_overlapping.png}}
%     \caption{{\color{red} Probably not needed. Just takes up precious space.} The beam pattern in Fig. \ref{fig:beam_pattern_with_gaps} only has a $79\%$ infill whilst Fig. \ref{fig:beam_pattern_without_gaps} has complete coverage at the cost of regions of overlap.}
%     \label{fig:beam_patter_gaps_and_overlapping}
% \end{figure}

\subsection{Observation Space}
\label{sect:method_observation_space}

The observation space has two components: the track list $s_{TL}$, and the scan history $s_{SH}$. The track list is dynamic list of all the currently known tracks and the $i$th track is defined by
\begin{equation}
    s_{TL,i} = \begin{bmatrix}
        ||\vec p_i|| \\ \tan^{-1}\frac{p_{y,i}}{p_{x,i}} \\ \sin^{-1}\frac{p_{z,i}}{||\vec p_i||} \\ \vec v_{x_i} \\ \vec v_{y_i} \\\vec v_{z_i} \\||\vec c_i||_\mathrm{fro}
    \end{bmatrix}
    \left( 1/
    \begin{bmatrix}
        100000 \\ \frac{\pi}{3}\\ \frac{\pi}{3} \\ 100 \\ 100 \\ 100  \\ c_\mathrm{threshold}
    \end{bmatrix}^T
    \right)
    +
    \begin{bmatrix}
        -1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
    \end{bmatrix}
\end{equation}
The total observation is then an array containing all the tracks.
% \begin{equation}
%     s_{TL} = [\vec s_{TL,0},\cdots,\vec s_{TL,N_\mathrm{track}}]^T
% \end{equation}
However, implementation-wise this is actual a zero-padded array $s_{TL} = ([\vec s_{TL,0},\cdots,\vec s_{TL,N_\mathrm{track}}]~||~\mathbf{0}^{(N_\mathrm{track,max}-N_\mathrm{track}) \times 7})^T$ where $N_\mathrm{track,max}$ is the number of tracks at the current timestep, and $N_\mathrm{track,max}$ is the padding term which represents the best guess of the total amount of tracks. This is required as there might be multiple tracks per targets. However, this value should not be too large as a high track-to-target ratio implies poor tracking and searching performance and results in poor performance. In this work $N_\mathrm{track,max} = 15$.

The scan history is the rasterized scan value map defined by 
\begin{equation}
    s_{SH} = \left\{ SSV([\psi, \theta],t) \in \mathbb{R}^{48 \times 48} ~|~ \psi, \theta \in \left[-\frac{\pi}{3}, \operatornamewithlimits{\cdots}_{N-2}, \frac{\pi}{3}\right]\right\}
\end{equation}
such that $s_{SH}$ is an image in the shape of $(1,48,48)$.

\subsection{Rewards}

Minimizing the scan value implies better search performance giving
\begin{equation}
    r_{SV,t} = - SSV([\psi, \theta], t)
\end{equation}

The tracking performance is given by the reduction in covariance across all the tracks with detections. This further rewards efficient tracking by detecting multiple targets. This is calculated using
\begin{equation}
    r_{TL,t} = \sum \left\{ 
    ||P_t||_\mathrm{fro} - ||P_{t-1}||_\mathrm{fro} 
    ~|~
   D_t \bigcup D_{t-1} 
   \right\}
\end{equation}
where $D_t$ is the set of detections by the sensor at timestep $t$ and $P_t$ is the covariance matrix of the associated track for $D_t$.

The total reward is then the summation of reward components given by
\begin{equation}
    r_t =  r_{SV,t} + r_{TL,t}
\end{equation}

\subsection{Network Design}
\label{sect:network_design}

$s_{TL}$ is a varying sized matrix with the number of rows depending on the number of currently detected tracks. For non-temporal data structures, the use of bidirectional recurrent architecture has shown promising results \cite{Wang2020} with the Gated Regression Unit (GRU) \cite{Cahuantzi2023} being one of the best variants for lower complexity data. Recent developments, however, have been moving away from recurrent architectures in favour of transformers \cite{Vaswani2017} where the revolution lies in the Multi-Headed Self-Attention (MHSA) block that significantly accelerates training times. We therefore propose three architectures: PPO-Flat, PPO-BiGRU, PPO-BiGRU+MHSA.
BiGRU+MHSA still requires the use of a recurrent block to handle the sequence-to-vector encoding because MHSA only performs sequence-to-sequence operations. The core policy network is a Multi-Layer Perceptron (MLP) which requires a fixed-size input. However, this part of the architecture is only $50\%$ of the size of the standalone BiGRU architecture. Flat is a simple row-major matrix transformation from 2D to 1D with zero-value padding to account for the varying rt.

To effectively handle $s_{SH}$, we use the NatureCNN architecture from \cite{Mnih2015} which has shown excellent performance in image-based operations for reinforcement learning. 

The final latent-space encodings $z_{TL}$ and $z_{SH}$ are then concatenated into $z$ before being fed into the MLP core-network. 

\begin{figure*}[tb!]
    \centering
    \subfloat[
                $s_{TL}$ is flattened and concatenated with the NatureCNN output before being inputted into the core network.
                \label{fig:rl_architecture_flat}
            ]{
                \includegraphics[width=.32\linewidth,origin=c]{rl_architecture.flatten.pdf}
            }
    \hfill
    \subfloat[
                BiGRU architecture where $s_{TL}$ is passed into the recurrent network sequentially outputting a fixed-length latent representation.
                \label{fig:rl_architecture_rnn}
            ]{
                \includegraphics[width=.3\linewidth,origin=c]{rl_architecture.rnn.pdf}
            }
    \hfill
    \subfloat[
                Multi-headed self attention is used to highlight important tracks from $s_{TL}$. The BiGRU module is $50\%$ of the size as the standalone variant.
                \label{fig:rl_architecture_rnn_mhsa}
            ]{
                \includegraphics[width=.3\linewidth,origin=c]{rl_architecture.rnn_mhsa.pdf}
            }
    \caption{Agent architectures for the sensor management problem. A NatureCNN is used for feature extraction from the \textit{scan-history} and RNN or Flattening layers are used to transform the \textit{track-list} before concatenation.}
    \label{fig:enter-label}
\end{figure*}

\subsubsection{Behaviour Cloning}
\label{sect:behaviour_cloning}

Imitation learning (IL) is a powerful technique when coupled with RL. IL is commonly referred to as offline RL due to its similarities. In IL a dataset is used to train a policy whereas in RL this dataset is created online through interaction with the environment. The most ubiquitous IL method is behaviour cloning (BC) \cite{Foster2024} which collapses IL down to a supervised learning problem. In BC the states $s \in S$ are inputs to the policy with actions $a \in A$ being the labels. During training, the mean-squared loss between the estimate action $\hat a$ and the true action $a$ is minimized until $\pi_\mathrm{student} \approx \pi_\mathrm{teacher}$.

Radar search is a simple task which can be solved using a random policy. This gives good search performance but has no ability for tracking. However, this can still be used to pre-train the RL policy to ensure it is good at searching to begin with and can thus quickly detect targets potentially reducing training time. Therefore the RL policy tries to approximate the random policy such that $\pi_\theta \approx \pi_\mathrm{random}$ where $\pi_\mathrm{random} \sim \mathcal{U}(0,N_a)^{1 \times 2}$.
Without BC, the policy would first have to learn how to search before being able to receive the sparse rewards for correctly tracking. 

\subsubsection{Auto-encoder}
\label{sect:auto-encoder}
\autoref{sect:behaviour_cloning} outlines how the network is pre-trained using BC but only to maximize the search performance. This means that the track list feature extractor is left untrained. Thus, to also maximize the potential for tracking performance during RL, the aforementioned feature extractor is pre-trained using the Auto-Encoder (AE) paradigm \cite{Sarankumar2024}. The AE is a popular unsupervised training method which can train an encoder, or feature extractor in this case, to maximize the information throughput during encoding. The AE is has two components: the encoder $f_E$ and the decoder $f_D$. These are used to approximate $x$ with $\hat x = f_D(f_E(x))$. The mean squared error $\frac{1}{n} \sum^n_{i=0} (\hat x_i - x_i)^2$ is then used to minimze the difference between the true and approximate values. 
% \begin{gather}
%     f_E(x) \mapsto z \\
%     f_D(z) \mapsto \hat x \\
%     \hat x = f_D(f_E(x)) \\
%     \epsilon =  \frac{1}{n} \sum^n_{i=0} (\hat x_i - x_i)^2
% \end{gather}
In this work, $x \sim \mathcal{U}(-1,1)^{N_\mathrm{track,max}\times7}$ which is inline with the observation space definition of the track list in \autoref{sect:method_observation_space} resulting in a track list feature extractor that is able to encode the observation to a meaningful latent space.

\section{Results}
\label{sect:results}

\begin{figure*}[tb!]
    \centering    
    \subfloat[GOSPA distance is the overall metric with lower values being better.\label{fig:results_gospa_distance}]{\includegraphics[width=0.24\linewidth]{results/boxplot_sum_gospa_distance.png}}
    \hfill
    \subfloat[GOSPA localisation is the tracking error score if a truth and track can be associated.\label{fig:results_gospa_localisation}]{\includegraphics[width=0.24\linewidth]{results/boxplot_sum_gospa_localisation.png}}
    \hfill
    \subfloat[GOSPA false measures the amount of tracks without an associated ground truth.\label{fig:results_gospa_false}]{\includegraphics[width=0.24\linewidth]{results/boxplot_sum_gospa_false.png}}
    \hfill
    \subfloat[GOSPA missed measures the amount of ground truths without an associated track.\label{fig:results_gospa_missed}]{\includegraphics[width=0.24\linewidth]{results/boxplot_sum_gospa_missed.png}}
    \caption{GOSPA distance (\autoref{fig:results_gospa_distance}) and three of its components \cite{Rahmathullah2017}. GOSPA switching has been excluded as algorithms had a value of $0$ for this metric showing that any detected track was never falsely associated to a different ground truth throughout its existence.}
    \label{fig:results_gosap_all}
\end{figure*}

\subsection{Experimental Setup}
\label{sect:results_experimental_setup}
% Three generic baseline policies are implemented for comparison:
% \begin{itemize}
%     \item Static: $a_0 \sim \mathcal{U}(0,N_a)^{1 \times 2}$,~$a_t = a_0$.
%     \item Random: $a_t \sim \mathcal{U}(0,N_a)^{1 \times 2}$.
%     \item Coverage: $a_t = \left[ \mod(k, \sqrt{N_a}), \left\lfloor\frac{k}{\sqrt{N_a}}\right\rfloor \right],~k = \mod(t, N_a)$
% \end{itemize}

Three generic baseline policies are implemented for comparison: Static, where an initial action is sampled uniformly from the action space and then held constant throughout the episode; Random, where an action is sampled uniformly from the action space at each timestep; and Coverage, which scans each line sequentially.

% \inote{Update this if we have enough time to run a third model training step.}
The models were trained on a \texttt{Intel i7-9700} with $32\si{\giga\byte}$ of RAM and no dedicated GPU. Each model was trained twice over $\SI{1e7}{}$ steps with early termination if no improvement was recorded in an evaluation environment after $\SI{25e5}{}$ steps. After training each model was analysed over $100$ full simulations to collect the summary data. The core policy is of size $2 \times 128$ and NatureCNN outputs a feature of dimension $1\times128$ across all variations. BiGRU has a hidden dimension of $64$, whilst this is $32$ for the BiGRU aspect of BiGRU+MHSA. Both BiGRU variants output a feature dimension of $1\times64$ with flat having a feature dimension of $1\times 120$ due to the lack of any reduction in observation.


\subsection{Search Performance}
\label{sect:results_search}

% \inote{Reword or use table to avoid using a figure.}
% Static performing extremely poorly in mean search reward over a simulationw ith an average search reward of $-25.1790 \pm 6.4789$. Coverage performs the best with an average value of $-0.0502 \pm 0.0344$. The three PPO-based architectures and coverage are closely grouped together around $-0.1250$ showing that the five non-static algorithms are comparable at searching. 
% It is easy to see that the search aspect of the search and tracking problem is not difficult. Static only performs so poorly because it does no searching whatsoever and is the least optimal solution to this problem.

In our simulations, the 'Static' approach exhibited the lowest mean search reward at $-25.17 \pm 6.47$, indicating significant underperformance. Conversely, the 'Coverage' method demonstrated the highest average search reward at $-0.0502 \pm 0.0344$. Notably, the three PPO-based architectures and 'Coverage' exhibited closely clustered mean search rewards around $-0.13$, suggesting comparable search performance among these five algorithms.

These results highlight that the search component of the search-and-track problem itself is not inherently challenging. The substantial underperformance of 'Static' can be attributed to its lack of search behavior, rendering it the least optimal solution for this task.

% \begin{figure}[htb!]
%     \centering    
%     \includegraphics[width=0.7\linewidth]{results/boxplot_search.png}
%     \caption{Mean search reward on a symmetric log scale. Static is heavily penalized by the scan value as expected. Performance of the five other algorithms is comparable with coverage marginally being the best.}
%     \label{fig:results_search_reward}
% \end{figure}

\subsection{Tracking Performance}
\label{sect:results_track}

% \begin{figure}[htb!]
%     \centering    
%     \includegraphics[width=0.7\linewidth]{results/boxplot_cov_norm_mean.png}
%     \caption{Mean frobenius norm of the track covariance matrix over the duration of the simulation. This metric highlights the inability of static to track as expected, with random showing a wide range from best to worst in class.}
%     \label{fig:results_cov_norm_mean}
% \end{figure}

It is evident from \autoref{sect:results_search} that search is not the difficult aspect of this problem. Being able to search and track simultaneously, however, is what is important. Static, without any tracking capabilities, has the highest mean covariance value $2630.71 \pm 0.02
$. Random on the other hand has a lower value of $948.15 \pm 921.86$ but the largest standard deviation. This is expected since the policy has a chance of perfectly tracking a target as well as never tracking it due to the randomness. Coverage on the other hand performs similarly to PPO-Flat and PPO-BiGRU with values of $574.27 \pm 424.53$, $730.56 \pm 744.84$, and $744.24 \pm 739.27$ respectively. However, the $5^{th}$ percentile whisker for coverage is higher than all PPO algorithms showing that there is potential for more performance there. The best algorithm, is however PPO-BiGRU+MHSA with a mean covariance norm value  of $461.98 \pm 573.50$. 

The General Optimal Sub-Pattern Association (GOSPA) \cite{Rahmathullah2017} is a summary metric of various tracking criteria. The overall score, GOSPA distance, can be seen in \autoref{fig:results_gospa_distance} with static unexpectedly performing well. However, this is explained by \autoref{fig:results_gospa_localisation} and \autoref{fig:results_gospa_false} which show that static does not track. Therefore the overall score is low if few tracks exist and are therefore unable to perform poorly. PPO-BiGRU+MHSA again performs the best with a mean GOSPA distance of $\SI{1.16e7}{}$.
%11617961.2094 \pm 1903403.6679
This is due to the lowest GOSPA missed score (\autoref{fig:results_gospa_missed}) of $\SI{1.07e11}{}$
%108668500000.0000 \pm 26544467793.7710
even though the GOSPA localisation score (\autoref{fig:results_gospa_localisation}) was the highest of all algorithms with a score of $\SI{1.37e10}{}$.
%13786514681.7149 \pm 10828261378.1350$

\section{Conclusions and Future Work}
\label{sect:conclusion_future_work}

% The overall best performing algorithm was PPO-BiGRU+MHSA whilst the worst performing was static. The latter has no tracking or search capabilities and this outcome was hypothesized in \autoref{sect:results_experimental_setup}. The former algorithm, PPO-BiGRU+MHSA, did not perform best in search, however, it was the best at tracking. Interestingly the pure BiGRU variant, PPO-BiGRU, was only on par with the zero-value padding variant, PPO-Flat, implying that the MHSA was critical. The MHSA can evidently learn which track in the track list is most important which is key for effective feature extraction. Simply flattening the track list, as done by PPO-Flat, puts the burden on the core policy without the benefits of an optimised architecture resulting in sub-optimal performance. 

% This study has limited the action space to azimuth and elevation commands to the radar which is assumed to act instantaneously. In practice, the scenario could be expanded to enable the platform to move within the area of regard. The MTT algorithm implemented as part of the feature extractor generates a track list at time $k$. As such, the agent can only access the current state estimates to determine the next action. More modern MTT algorithms such as the trajectory-PMBM \cite{garcia2020trajectory} are able to generate trajectory histories rather than a single track state, potentially providing additional information about the environment such as birth locations that could inform the agent policy when incorporated into the feature extraction process.

PPO-BiGRU+MHSA exhibited the best overall performance, while the static algorithm, lacking tracking and search capabilities, performed the worst, as hypothesized in \autoref{sect:results_experimental_setup}. PPO-BiGRU+MHSA excelled in tracking but not search. Notably, PPO-BiGRU performed similarly to PPO-Flat, highlighting the critical role of the MHSA, which effectively learns the importance of tracks within the list. PPO-Flat, by flattening the track list, burdens the policy without the benefits of an optimized architecture.

This study limited actions to radar azimuth and elevation commands, assuming instantaneous action. In practice, platform movement within the area of regard should be considered. The current MTT algorithm provides only the current track list state at time k, limiting the agent's information. More advanced MTT algorithms, like trajectory-PMBM \cite{garcia2020trajectory}, generate trajectory histories, potentially providing valuable information like birth locations that could enhance the policy.