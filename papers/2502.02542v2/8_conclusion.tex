\section{Conclusion}

In this paper, we propose \sys, a novel indirect prompt injection attack applications applying reasoning LLMs to untrusted data. Our attack exploits input-dependent inference-time reasoning by introducing computationally demanding decoy problems without altering the expected answers, making it undetectable to users. Our experimental results show that \sys significantly disrupts reasoning efficiency, with attacks on FreshQA dataset increasing reasoning tokens up to \textbf{18$\times$} and attack on SQuAD dataset increasing the reasoning tokens by \textbf{46}$\times$. We evaluate a simple filtering defenses that can mitigate the attack and argue for adoption of defenses by the applications using reasoning LLMs.


\section*{Ethical Statement}

We have conducted this research with an emphasis on responsible research practices, mindful of both the computational and infrastructure costs associated with LLMs. Specifically, we have limited our study to a total of $20$ million tokens for OpenAI services and $10$ million tokens for DeepSeek. This represents a fraction of the daily computational load on these services, which are already under strain according~\cite{deepseek2025}. We aimed to minimize our impact on these infrastructures while still obtaining meaningful insights and allowing reproducible experiments. 

Understanding and mitigating resource overhead risks is essential for the long-term success of LLM applications. We have made our code and used prompts public to facilitate adoption of defenses by the applications relying on LLM reasoning models. By conducting this work we hope to contribute to sustainable, fair, and secure advances in AI research promoting \textit{computing for good}.

\section*{Acknowledgements}
This work was partially supported by the NSF grant CNS-2131910 and NAIRR 240392. We thank Vardan Verdiyan for insights on the decoy problem sets and Vitaly Shmatikov for providing connection to algorithmic complexity attacks.



