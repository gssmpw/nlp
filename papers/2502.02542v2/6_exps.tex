
\section{Evaluation}
\label{sec:exps}

\subsection{Experimental Setup}


\paragraphbe{Models.} We evaluate our attack on three closed-source (o1, o1-mini and o3-mini) and open-source (DeepSeek-R1) reasoning models. These models leverage advanced reasoning methods such as CoT, and are well-known for excelling on a range of complex tasks and benchmarks~\cite{guo2025deepseek, sun2023survey}.

\paragraphbe{Datasets.} We evaluate our attack using FreshQA~\cite{vu2023freshllms} and SQuAD~\cite{rajpurkar2018know}. FreshQA is a dynamic question-answering (QA) benchmark designed to assess the factual accuracy of LLMs by incorporating both stable and evolving real-world knowledge. The benchmark includes 600 natural questions categorized into four types: never-changing, slow-changing, fast changing, and false-premise. These questions vary in complexity, requiring both single-hop and multi-hop reasoning, and are linked to regularly updated Wikipedia entries. The original query consists of an average of 11.6$\pm$1.85 tokens. However, due to the randomness and the length of the context extracted from Wikipedia, the total input token count increases to an average of 11278.2$\pm$6011.49 tokens when the context is appended. This leads to a noticeable variation in input length.

SQuAD contains more than 100k questions based on more than 500 articles retrieved from Wikipedia. While the average length of a query in the dataset is similar to FreshQA with $11.5\pm3.4$ tokens, the context is significantly shorter and shows less variance in length. An average context in the dataset contains $117.5\pm37.3$ tokens.  Utilizing these two datasets allows us to study our attack and impact of factors like context length and complexity.

We select a subset of the dataset containing samples with ground-truth that changes infrequently and has lower likelihood of unintentional errors. To minimize costs and adhere to ethical considerations, we restrict our evaluations of different attack types, attack transferability and reasoning effort to five data samples from FreshQA. This ensures minimal impact on existing infrastructure while allowing us to test our attack methodologies. Subsequently, we study the impact of context-agnostic attacks on 100 samples from the FreshQA and SQuAD datasets across four models (o1, o1-mini, DeepSeek-R1, and o3-mini) and present a comprehensive analysis of the attack performance on a larger scale.







\paragraphbe{Evaluation Metrics.} 
Since we evaluated our attack using QA datasets, we measure \textbf{claim accuracy}~\citep{ min2023factscore}. This is done by using LLM-as-a-judge, where the model verifies claims in its output against a list of ground truths. A score of 1 is assigned if the claims align and 0 if they do not. For longer outputs, more sophisticated claim verification metrics could be used~\cite{song2024veriscore, wei2024long}. Additionally, since our attack introduces a decoy problem, we assess output stealthiness by measuring the presence of decoy-related information in the final output, which we refer to as \textbf{contextual correctness}. This metric evaluates how much of the output belongs to the context surround the user query versus the decoy task. We assign a score of 1 if the output contains only claims relevant to the user query's context, 0.5 if it includes claims for both contexts, and 0 if it consists entirely of decoy-related information. All the results were also manually reviewed for errors. Fig.~\ref{fig:cc evaluation prompt} and Fig.~\ref{fig:cc_score_example} in Appendix show the contextual correctness evaluation prompt and output examples respectively.



\input{tables/tab_main_tab}
\input{tables/tab_ablation}
\input{tables/tab_transferability}
\input{tables/tab_effort}
\input{tables/scaled_squad}






\subsection{Attack Setup}
To orchestrate the attack, we first retrieve context related to the question either directly from the dataset or using the links present in the dataset. We then inject manually written attack templates (discussed in sections \ref{subsec:manual_injection} and \ref{subsec:weaving_injection}) in the retrieved context and compare the model's responses to both the original and compromised contexts for evaluation. 
We select the best performing decoy problems from Table~\ref{tab:dataset_comparison} i.e Sudoku and MDP. For example of injection templates, refer to Figure~\ref{fig:context_agnostic_prompt_sudoku} and Figure~\ref{fig:context_agnostic_prompt} in Appendix~\ref{appendix: used_prompts}.
Finally, we utilize decoy-optimized context generated using Algorithm \ref{alg:ICL-genetic} to produce output and evaluate ICL-Genetic based attacks.

\subsection{Experimental Results}
We demonstrate the main experimental results of our \sys attack against o1 and DeepSeek-R1 models, demonstrating that all attack types significantly amplify the models' reasoning complexity. For the o1 model, Table~\ref{tab:o1_main_tab} shows that the baseline processing involves $751\pm410$ reasoning tokens. The ICL-Genetic (Agnostic) attack causes the largest increase -- an $18\times$ rise. Context-Agnostic and Context-Aware attacks also increase the token count significantly, by $9.7\times$ and over $2\times$, respectively. 

Similarly, Table~\ref{tab:deepseek_main_tab} shows that all attack types severely raise the number of reasoning tokens in the DeepSeek-R1 model. The baseline of $711\pm635$ tokens increase more than $10\times$ under the ICL-Genetic (Agnostic) attack. Other attacks, such as Context-Agnostic, Context-Aware, and ICL-Genetic (Aware), also lead to substantial increases in reasoning complexity. Overall, our results demonstrate that ICL-based attacks, especially those involving Context-Agnostic, severely disrupt reasoning efficiency for both models by drastically increasing reasoning token counts. This trend persists across all attack types. Similarly Tables~\ref{tab:Context_Agnostic_100samples_big_models} and~\ref{tab:Context_Agnostic_100samples_mini_models} show an increase in reasoning tokens across all four models tested on both the SQuAD and FreshQA datasets using the context-agnostic attack. We observe a $46\times$ increase in reasoning tokens for the SQuAD dataset on the o1 model. This highlights the effectiveness of our attack methodology across a diverse set of contexts and model families. Figure~\ref{fig:reasoning_content_example} in the Appendix gives an insight of how the decoy task causes increase in  reasoning steps for R1 model.

\pbe{ICL Ablation.}
Table~\ref{tab:o1_main_tab} and~\ref{tab:deepseek_main_tab} show that the results based on ICL outperform both context-agnostic and context-aware settings. In Table~\ref{tab:ablation_table}, we present an ablation study on ICL-Genetic with context-agnostic attack framework to evaluate the efficacy of each individual components and its contributions on crafting the final attack. It shows that while both ICL-Genetic and context-agnostic attacks independently have higher reasoning token count than baseline, both of them are lower than the attack  conducted by combining both techniques. We hypothesize that this occurs because the attack-agnostic samples used to generate the initial population allow the algorithm to narrow down the search space, thereby enabling it to take a more exploitative route in finding an effective injection.


\input{tables/filtering}
\input{tables/paraphrasing}

\pbe{Attack Transferability.}
We evaluate the transferability of \sys across o1, o1-mini, and DeepSeek-R1 models under the Context-Agnostic attack. Contexts optimized using the ICL-Genetic attack on a source model are applied to target models to assess transferability. The o1 model demonstrates strong transferability, achieving a 12$\times$ increase on DeepSeek-R1, exceeding the 10.5$\times$ increase from context optimized directly on DeepSeek-R1. Similarly, o1's transfer to o1-mini results in a 6.2$\times$ increase. DeepSeek-R1 also transfers effectively to o1 with an 11.4$\times$ increase but less so to o1-mini (4.4$\times$). In contrast, o1-mini shows moderate transferability with a 7.5$\times$ increase on DeepSeek-R1 and only 2.9$\times$ on o1. These findings demonstrate that context optimized from various source models can significantly increase reasoning tokens across different target models. 




\pbe{Reasoning Effort Tuning.} 
The o1 model API provides \textit{reasoning effort} hyperparameter that controls the size of thought in generating responses, with low effort yielding quick, simple answers and high effort producing more detailed explanations~\cite{openai_reasoning_effort, openai_reasoning_guide}. We use this parameter to evaluate our attack across different effort levels. Table~\ref{tab:effort_comparison} shows that the Context-Agnostic attack significantly increases reasoning tokens at all effort levels. For high effort, the token count rises over 12$\times$. Medium and low effort also show large increases, reaching up to 14$\times$. These results demonstrate that the attack disrupts the model's reasoning efficiency across tasks of varying complexity, with even low-effort tasks experiencing significant reasoning overhead.














 

    



