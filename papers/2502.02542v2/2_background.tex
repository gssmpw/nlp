\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/reasoning.pdf} %
    \caption{Application of Reasoning LLMs on untrusted contexts.}
    \label{fig:reasoning_llm} %
\end{figure}

\section{Background and Related Work} 

\renewcommand\thefootnote{}%


\subsection{Reasoning in LLM}
Language models (LMs) predict the probability distribution of words in a sequence, allowing them to comprehend and generate human-like text. The scaling of these models~\citep{merity2016pointer, devlin2018bert, brown2020language, mehta2023empirical, zhao2023survey} enabled modern LLMs to excel at complex tasks, but faced token-based cost challenges for complex tasks~\citep{liao2024attention, han2024token,wang-etal-2024-reasoning-token}.

Chain-of-thought (CoT) prompting~\citep{wei2022chain, kojima2022large}, on the other hand, guides LLMs to generate intermediate \emph{reasoning} steps in natural language that lead to more accurate and interpretable outcomes, which has significantly improved performance across various benchmarks~\citep{sun2023survey}. Tree-of-thought (ToT)~\citep{yao2024tree} generalizes CoT by exploring multiple reasoning paths in a tree structure $T$, allowing correcting errors by revisiting earlier decisions. Recent models like DeepSeek-R1~\citep{guo2025deepseek} specifically utilize large-scale reinforcement learning (RL) with the collection of long CoT examples to improve reasoning capabilities. Models that generate reasoning before producing the final answer are  called \textit{reasoning LLMs} (see Figure~\ref{fig:reasoning_llm}). 






















\subsection{Reasoning Model Deployment}

Reasoning models are already deployed in general-use applications, e.g., ChatGPT and DeepSeek chat, and recently Copilot~\cite{verge_microsoft_o1_copilot_2025}. These applications often integrate reasoning models into their services by making API calls to service providers like OpenAI, Azure, Fireworks, or DeepSeek, which host the models. These models are accessed via APIs, with pricing based on token usage. Output tokens, which include reasoning tokens (visible in DeepSeek-R1 but not in o1 or o1-mini), are significantly more expensive than input tokens, emphasizing the need for efficient token management. For instance, the o1 model charges \$15.00 per million input tokens and \$ 60.00 per million output tokens~\citep{openai_pricing}. In contrast, DeepSeek offers a more economical pricing structure, with input tokens priced at \$3.00 per million and output tokens at \$2.19 per million~\citep{deepseek_pricing}. DeepSeek's model weights are open and could be locally deployed. Nevertheless,   in both local deployment and API access, applications will pay for reasoning, either directly, e.g., API access, or indirectly, e.g., by requiring more GPUs to support the increased load. 

Crucially for our threat model, user-facing applications like ChatGPT, Copilot, and DeepSeek chat do \emph{not} charge users per query or amount of reasoning, instead providing free or fixed-cost access.









\subsection{The Importance of Operational Cost}

As AI adoption grows, recent studies~\citep{samsi2023words, luccioni2024power, varoquaux2024hype} are shifting the focus from training costs to the computational and environmental overhead of inference~\citep{samsi2023words}. Large-scale models, particularly multipurpose generative models, consume significantly more energy than task-specific models, with inference requiring 0.002kWh for text classification and up to 2.9kWh for image generation per 1,000 queries~\citep{luccioni2024power}. Although training modes like BLOOMz-7B require 5,686 kWh, inference surpasses training costs after 592 million queries, making large-scale deployment a major energy consumer~\citep{patterson2022carbon}. 









\subsection{Related Attacks}

\paragraphbe{Prompt Injection.} In prompt injection attacks, an adversary manipulates the input prompt of LLMs to influence the generated output. This manipulation can either \emph{directly} alter the input prompt~\citep{perez2022ignore, apruzzese2023real} or \emph{indirectly} inject malicious prompts into external sources retrieved by the model~\citep{greshake2023not}. By contrast, our work introduces a novel attack vector where the adversary's goal is not to provoke harmful output (that can trigger jailbreaking defenses~\cite{anthropic2025, zaremba2025trading}) but to increase the number of reasoning tokens, thereby inflating the financial cost of using deployed reasoning models.



\paragraphbe{Denial-of-service (DoS) Attacks.} DoS attacks come from networking and systems research~\citep{bellardo2003802, heftrig2024harder,martin2004denial}. The first DoS attacks on ML models~\citep{shumailov2021sponge} focused on maximizing energy consumption while not preserving correctness of user output. Other attacks~\citep{gao2024denial,chen2022nicgslowdown, gao2024inducing, geiping2024coercing} fine-tune LLMs with malicious data or create malicious prompts to generate outputs that are visible to users and can be easily flagged by them. Our attack aims to not modify the LLM answer,  but instead increase the hidden reasoning components of the output. \citet{zaremba2025trading} and \citet{chen2024not,wang2025underthinking} observed that reasoning LLMs tend to spend excessive time on simple problems or reducing safety guardrails, referring to this as \textit{nerd sniping} and \textit{overthinking}, respectively. In contrast, we propose indirect prompt injection attacks with decoys targeting amplification of inference cost by increasing the reasoning tokens generated by these models while maintaining answer stealthiness. 








