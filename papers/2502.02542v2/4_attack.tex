
\section{Attack Methodology}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/methods.pdf} %
    \caption{\sys attack methodology.}
    \label{fig:methods} %
\end{figure}

To satisfy our objectives, the attacker has to (1) select a hard problem, i.e. \emph{decoy} (Subsection~\ref{subsec:decoy}), (2) inject into the existing context (Subsections~\ref{subsec:weaving_injection} such that the output doesn't contain any tokens associated with the decoy task \ref{subsec:manual_injection}) , and (3) optimize the decoy task to further increase the reasoning tokens while maintaining output stealthiness (Subsection~\ref{subsec:optimization}), as illustrated in Figure~\ref{fig:methods}. 


\subsection{Decoy Problem Selection}
\label{subsec:decoy}

\input{tables/tab_decoy}
Reasoning LLMs allocate different numbers of reasoning tokens based on their assessment of a task's difficulty and confidence in the response~\cite{guo2025deepseek}. Leveraging this, we introduce \textbf{decoy problems} designed to increase reasoning token usage. However, the main challenge of selecting an effective decoy problem is accurately estimating problem complexity from the model's perspective. Table~\ref{tab:dataset_comparison} shows that tasks perceived as difficult by humans do not always result in higher token usage. For example, models generate solutions to IMO 2024 problems with an average of 8923 reasoning tokens, significantly fewer than the 21080 tokens required on average for a simple Sudoku puzzle across all three reasoning LLMs.


Our decoy problems are designed to trigger \textit{multiple} rounds of backtracking~\cite{yao2024tree}. The most effective decoys involve tasks with many small, verifiable steps. Examples include Sudoku puzzles and Finite Markov Decision Processes (MDPs), which require numerous operations, each validated against clear criteria, thereby increasing the model's reasoning complexity. 









\subsection{Context-Aware Injection}
\label{subsec:weaving_injection}

In this attack, the adversary modifies the context to create a contextual link between the previously selected decoy problem and the original user query. This forces reasoning LLMs to address the decoy problem as part of generating a response to the original query. Table~\ref{tab:attack_example_table} demonstrates this approach, showing that when the decoy task is effectively integrated into the original context, the model provides a response consistent with the original user query while significantly increasing the reasoning token count. The attack assumes a stronger threat model as it requires the adversary to have access to the user question and requires them to craft an injection which is unique to the query and the target context. This also reduces the transferability of the created injection to other users or contexts.

\subsection{Context-Agnostic Injection}
\label{subsec:manual_injection}
While the context generated through the context-aware injection attack may appear stealthy, it requires extensive manual curation for each sample, which is labor-intensive. We experimented with automating this process using LLMs like GPT-4o and o1, but the attack was not effective, indicating the need for further research.

Motivated by these findings, we propose \textbf{Context-Agnostic Injection} attack, which aims to create a general attack template that can be inserted into any context. This template is crafted without any knowledge of the user query. These templates prioritize being explicitly recognizable rather than blending with the input context. In this approach, the original context consist of a sequence of elements (e.g., pieces of information or statements). The attack modifies this context by injecting two components: a decoy task designed to significantly increase reasoning complexity, and a set of instructions guiding how to execute the task. The execution instructions are crucial since the reasoning models are fairly good at realizing that a decoy task is not relevant to the primary use query so inserting an instruction that explicitly instructs the model to execute the secondary task becomes crucial for a successful attack. Our results in section~\ref{sec:exps} show that even with a weaker threat model, when compared to context-aware injection, context-agnostic injections have a higher reasoning increase. This makes them a stronger attack while being cost-affective and transferable.


















\input{tables/attack_example}

\subsection{Decoy Optimization}
\label{subsec:optimization}




The adversary can further optimize their decoy injection attack, or find new adversarial contexts, by using an ICL based genetic algorithm, inspired by~\citet{monea2024llms}. In Algorithm~\ref{alg:ICL-genetic} (ICL-Genetic), we generate new adversarial context by using using our decoy injection sample and prompting an LLM to create multiple variants. These variants serve as the initial population for the genetic algorithm. 

We then perform multi-objective optimization by assigning a score to each variant based on two factors: (1) its effectiveness in increasing reasoning steps compared to the reasoning steps generated by the original context ($z$), and (2) its output stealthiness, ensuring that the decoy task's results do not appear in the final output. Variants with low scores or poor stealthiness are filtered out in subsequent generations.


\begin{algorithm}[tbp]
\caption{ ICL-Genetic attack algorithm }
\label{alg:ICL-genetic}
\begin{algorithmic}[0]
    \STATE \textbf{Require:} \\
        \quad Reasoning model: $\mathcal{P_{\theta}}$ \\
        \quad ICL capable model: $\mathcal{M_{\theta}}$ \\
        \quad Target context: $z$ \\
        \quad Dummy query: $q$ \\
        \quad Number of shots n \\
        \quad Number of rounds T \\
        \quad ICL-Genetic input prompt generator: $\text{w}_{\text{icl-prompt}}(.)$ \\
        \quad Buffer: $\mathcal{E} \leftarrow \emptyset$ or (manual samples, sample scores)\\
\end{algorithmic}
\begin{algorithmic}[1]
        \STATE Output $y, r$ = $\mathcal{P_\theta}($$z,q$$)$ 
        \STATE Initial population $G_0$ = $\mathcal{M_\theta}\big(\text{w}_{\text{icl-prompt}}(z,\mathcal{E},0)\big)$
        \STATE Initialize $\mathcal{E}_{\text{temp}}$
        \FOR{each $g \in G_0 $}
            \STATE Output $y^*, r^*$ = $\mathcal{P_\theta}(z^*,q)$
            \STATE Score s = $\text{log}_{10}(\frac{r^*}{r})$
            \STATE Append $(z^*,s)$ to  $\mathcal{E}_{\text{temp}}$
            \ENDFOR
        \STATE Add highest scoring n samples in $\mathcal{E}_{\text{temp}}$ to $\mathcal{E}$ 
        
    \FOR{$t = 1,2,3,...$ to T}
        \STATE New generation $z^*$  = $\mathcal{M_\theta}\big(\text{w}_{\text{icl-prompt}}(z,\mathcal{E},t)\big)$
        \STATE Output $y^*, r^*$ = $\mathcal{P_\theta}($$z^*,q$$)$
        \STATE Score s = $\text{log}_{10}(\frac{r^*}{r})$
         \STATE if {s $>$ 0 and $\mathbbm{1}_{y \approx y^{*}}$ then} %
        \STATE \quad add ($z^*$, s) to buffer 
        
    \ENDFOR
    \STATE \textbf{Output:} $z^*$ from the buffer with maximum s
\end{algorithmic}
\end{algorithm}




We use a logarithmic scale to calculate the score since ICL is highly sensitive to prompt composition and the selection of examples. Using a log scale reduces the algorithm's sensitivity to minor variations in reasoning steps, providing more stability during optimization. After generating and scoring the initial population, we use it as an in-context sample to produce the next generation. This process continues until either the desired score is achieved or the maximum number of iterations is reached. Only positive-scoring generations are added to the buffer, as negative-scoring samples have been shown to negatively impact performance in similar settings~\cite{monea2024llms}. 


















