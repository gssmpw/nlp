
\section{Introduction}

Inference-time reasoning boosts large language model (LLM) performance across a broad range of tasks, inspiring a new generation of \emph{reasoning LLMs} like OpenAI o1~\citep{jaech2024openai} and DeepSeek R1~\citep{guo2025deepseek}. While initially geared towards solving complex mathematical problems, reasoning LLMs are now integrated for general public use in apps like ChatGPT and DeepSeek. For instance, Microsoft has already integrated o1 into Copilot ~\citep{verge_microsoft_o1_copilot_2025} and made it freely available to its users. 



Internally, reasoning models produce chain-of-thought sequences, i.e., \emph{reasoning tokens}, that help in generating the final output. Having access to reasoning tokens is not necessary for the users of an application utilizing reasoning LLMs. Nevertheless, generated reasoning and output tokens impact the \emph{cost of inference} by increasing the time, energy, and financial overhead for every query. Applications that use reasoning LLM APIs are charged for generating \emph{both} reasoning and the answer. 
\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/attack_method.pdf} %
    \caption{Overview of \sys Attack.}
    \label{fig:attack_method} %
\end{figure}




In this paper, we propose an \sys attack\footnote{Code available at: \url{https://github.com/akumar2709/OVERTHINK_public}.} that forces a reasoning LLM to spend an \emph{amplified number of reasoning tokens} for an (unmodified) user input, without changing the expected answer (and therefore undetectable to the querying user).\footnote{Some service providers, e.g., ~\citet{openai_reasoning_guide}, even protect reasoning tokens by hiding them from the output, making modifications to the chain-of-thought difficult to observe.}  
Our attack is a form of \emph{indirect prompt injection}  that exploits the reasoning model's reliance on inference-time compute scaling. \sys is different from existing prompt injections~\citep{perez2022ignore, apruzzese2023real, greshake2023not} in that while previous attacks aim to alter the answer itself, \sys aims to instead increase the number of reasoning tokens without any impact on the final answer. \sys\ impacts many common applications of reasoning models, such as retrieval-augmented generation, which depend on  (often unvetted) public texts such as social media posts and Wikipedia articles that are vulnerable to adversarial modification.



The adversary can use \sys for various adversarial intentions, such as denial-of-service (already a significant problem for reasoning LLMs~\citep{deepseek2025}), amplified expenses for apps built on reasoning APIs, and slowdown of user inferences. We discuss the  consequences of \sys, including financial costs, energy consumption, and ethics in Section~\ref{sec:consequences}. 




Figure~\ref{fig:attack_method} depicts the \sys attack. We assume that the user asks an application that uses a reasoning LLM a question that could be answered by using adversary-controlled  public sources, \eg web pages, forums, wikis, or documents.
The key technique used in \sys is to inject some computationally demanding \emph{decoy} problem (e.g., Markov Decision Processes (MDP) or Sudoku problems) into the source that would be supplied as context to the reasoning LLM during inference. 


Our hypothesis is that reasoning models are trained to solve problems and follow instructions they discover, even if it does not align with the user's query in the context, as long as they do not contradict safety guardrails. In fact, our decoy problems are intentionally benign, yet they cause high computational overheads for a reasoning LLM. Furthermore, a user reading the compromised source will still be able to find the answer manually, and decoys could be ignored or considered as Internet junk (e.g., clickbaits, search engine optimization, hidden advertisements). 

Our attack contains three key stages: \textbf{(1)} picking a \emph{decoy} problem that results in a large number of reasoning tokens, but will not trigger safety filters; \textbf{(2)} integrating selected decoys into a compromised source (e.g., a wiki page) by either modifying the problem to fit the context
(context-aware) or by injecting a general template (context-agnostic); 
and,  \textbf{(3)} optimizing  the decoy tasks using an in-context learning genetic (ICL-Genetic) algorithm to select contexts with decoys that provide the highest reasoning tokens and maintain stealthiness in the answers to the user.






Our experimental results show that \sys significantly increases the reasoning complexity of reasoning LLMs across different attack types, models and datasets. For the o1 model, context-agnostic attack shows an increase of \textbf{46}$\times$ in reasoning tokens on SQuAD dataset. For FreshQA dataset, our ICL-Genetic (Agnostic) attack results in the largest increase, with an \textbf{18}$\times$ rise in reasoning tokens, while our Context-Agnostic and Context-Aware attacks cause 9.7$\times$ and over 2.0$\times$ increases, respectively. Similarly, for  DeepSeek-R1 we see an increase of up to \textbf{20}$\times$ on SQuAD dataset. For FreshQA dataset, the ICL-Genetic (Agnostic) attack leads to a more than \textbf{10}$\times$ increase in reasoning tokens, with other attacks also causing substantial amplification. Our results demonstrate that ICL-based attacks, particularly Context-Agnostic ICL-Genetic attacks, effectively and consistently disrupt reasoning efficiency across multiple reasoning LLMs, highlighting the robustness of our attack methods.
Finally, we discuss how simple defenses like filtering and paraphrasing can mitigate our attack and argue that application developers should always deploy them when leveraging reasoning LLMs.



 





