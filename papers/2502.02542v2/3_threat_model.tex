



\section{The \sys Attack}\label{OverthinkAttack}

We focus on applications that use reasoning LLMs on untrusted data. Our attack aims to increase inference costs by generating \emph{unnecessary} reasoning tokens while maintaining accuracy on generated answers. Reasoning tokens are often hidden or unimportant for simple tasks, whereas generated answers are presented to the user and may be flagged by the user. Our slowdown attack is inspired by algorithmic complexity attacks~\cite{crosby2003denial} and leverages indirect prompt injection~\cite{greshake2023not} to modify model's behavior.



\subsection{Threat Model.}  

\paragraphbe{Attack's Potential Scenarios.} 
Users might send questions to chatbots like chatGPT and DeepSeek, or AI assistants like CoPilot that already supports reasoning LLM. These applications can retrieve information from untrusted external sources  such as webpages or documents and provide them as inputs to the LLM along with user's query. 
While users enjoy an application for free or at a fixed cost, the application is responsible for costs associated with LLM answer generation and reasoning. An adversary can target to modify a webpage that the application retrieves data from or alter documents or emails used by an AI assistant. In all these cases, the user invests in getting the right answer on their query but might not be interested in detailed inspection of the source nor access to reasoning. This makes these scenarios a perfect candidate for our proposed attack.






\paragraphbe{Adversary's Target.}
In this attack, the adversary targets \emph{the application} that uses reasoning LLMs on external resources. Unlike existing prompt injection attacks, the adversary does not target the user directly, e.g. modify outputs; instead, the focus is on increasing application's costs to operate the reasoning model.

\paragraphbe{Adversary's Objectives.} 
The adversary aims to increase the target LLM's reasoning tokens for a specific class of user queries that rely on external context, which is integrated into the final input to the LLM. These attacks are applicable to instructions that depend on such external information (e.g., "\textit{summarize what people are saying about NVIDIA stock after DeepSeek}") but not to context-independent instructions (e.g., \textit{"write a story about unicorns"}). By manipulating the external context, the adversary ensures that the LLM answers the user's original query correctly while omitting any trace of the context manipulation.


\paragraphbe{Adversary's Capabilities.}
In this attack, the adversary has black-box access to the reasoning LLM and can access the external context retrieved by the LLM. We explore two different scenarios: the first where the adversary has access to the user query or similar queries, and the second where the adversary has no knowledge of the user query. In cases where the adversary cannot access the target LLM, we assume that they have access to a proxy LLM, on which they can perform black-box queries. 





\subsection{Problem Statement}



A \emph{reasoning LLM} \( P_{\theta} \) maps an input sequence \( x \), containing user's question q and external context \( z \), to an output sequence consisting of reasoning tokens \(r\) and answer tokens \( y \). Our attack aims to form an adversarial input prompt \(x^*\) with user's question $q$ and adversary-created context \( z^* \). The adversary aims to achieve the following:





\begin{enumerate}
    \item \textbf{Longer Reasoning}. 
    The length of the reasoning sequence \( 
    ||r^*|| \) is significantly increased, compared to outputs \(P_{\theta}(q,z)=[r, y]\) on context $z$ :  
    \[
    ||r^*|| \gg ||r||
    \]

    \item \textbf{Answer Stealthiness}. The answer \( y^* \) has to remain similar to the original output \( y \) and cannot include any information related to the modification included in \( z^* \).
    
\end{enumerate}
Therefore the attack objective can be formulated as:

\[
z^* = \underset{\tilde{z}}{\text{argmax}} \; \mathbb{E}\left[ ||r^*||\bigg|\mathbb{1}_{y^*\approx y} \right] \quad 
\]
where, $\tilde{z}$ is all possible variants of z leading to $y^*\approx y$. 








\subsection{Why Does This Attack Matter?} \label{sec:consequences}
With the increasing deployment of reasoning LLMs in daily applications, various cost-related aspects must be considered. First, output tokens, including reasoning tokens, are more expensive than input tokens. Thus, any increase in the number of these tokens can lead to additional expenses for applications. In addition, users might exhaust the output tokens specified in the API call, resulting in them paying for an empty response because the models generate more reasoning tokens than anticipated. Second, a higher number of reasoning tokens often results in longer running times, which can cause significant issues for time-sensitive tasks and applications. Third, LLM providers usually face resource limitations in delivering services to users. Due to these limitations, increased token usage and response times may delay other benign responses, impacting the ability to provide timely and appropriate outputs for other users. Finally, the increased response process also leads to higher resource consumption, contributing to unnecessary increases in carbon emissions.











