\section{Attack Limitations and Potential Defenses}


While our attack demonstrates both high success and output stealthiness, a key limitation is its low input stealthiness. As a result, if the defender is aware of this threat, the attack can be easily detected by straightforward methods. However, since defense solutions often need to be tailored to specific use cases, deploying it becomes a challenge for application developers rather than model developers like OpenAI or DeepSeek. Optimizing the injected context to enhance input stealthiness could be a potential future direction. In this section, we present and discuss some defense ideas.

\paragraphbe{Filtering.} 
As a potential defense, the application can filter irrelevant information from the external context and remove all unnecessary content. One approach is to divide the context into chunks and retrieve only the most relevant ones, reducing the likelihood of retrieving injected content. Alternatively, an LLM can be deployed to handle the filtering task. This approach has shown promising success in filtering out our injected problems as shown in Table~\ref{tab:filtering}. However, it is unclear how filtering affects the performance of the reasoning process, as the target model—o1 in this case—may already know the answer to the question, even if the filtered context lacks relevant information. The impact of filtering on reasoning performance could be further explored as future work. To evaluate filtering, we use GPT-4o as the LLM to filter relevant content. The corresponding prompt used for filtering is shown in Figure~\ref{fig:filter_prompt}.

\paragraphbe{Paraphrasing.} 
Another potential countermeasure against our attack is paraphrasing~\citep{jain2023baseline, gong2024paraphrasing}. Since the external context originates from untrusted sources, a reasonable practice for the application is to paraphrase the retrieved context. This can decrease the likelihood of a successful attack, especially trigger-based attacks that rely on specific keywords. Paraphrasing retains the main content while rephrasing the text to enhance clarity. The results of our attack after applying paraphrasing are illustrated in Table~\ref{tab:paraphrasing}. As shown in the table, the manual injection and ICL-Genetic with manual injection attacks still lead to a significant increase in reasoning tokens, while the other two attacks do not. We use GPT-4o to paraphrase the context, and the prompt used for this task is shown in Figure~\ref{fig:paraphrase_prompt}.

\paragraphbe{Caching.} 
Caching can be a potential defense as it minimizes the number of times the model generates the solution for the decoy task. Caching can be implemented in two main ways: exact-match caching and semantic caching. Exact-match caching stores responses to specific queries and retrieves them when the same query is repeated verbatim. Semantic caching, on the other hand, analyzes the meaning behind queries to identify and store responses for semantically similar inputs. In this defense strategy, when a query is received, a similar benign query is retrieved and used as input to the LLM, preventing the manipulated context from being included in the final input.

\pbe{Adaptive Reasoning.} Another approach is to adjust amount of reasoning depending on the model inputs, i.e. we can decide in advance how many reasoning tokens is worth spending based on the question and context. For example, OpenAI API models can control ``effort levels'' reasoning tokens (see Table~\ref{tab:effort_comparison}). However the context could also be manipulated to select expensive effort~\cite{shafran2025rerouting}. Instead, we could rely on the trusted context, \eg user's questions, estimating the effort to isolate from potentially harmful outputs, similar to~\cite{han2024token}.



