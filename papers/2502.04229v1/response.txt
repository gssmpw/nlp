\section{Related Works}
\noindent \textbf{Dataset Distillation.} DD aims to condense the richness of large-scale datasets into compact small datasets that effectively preserve training performance **Raghu et al., "A Deep Learning Framework for Data Efficient Neural Networks"**.
Coreset selection **Srinivas et al., "Data-Efficient Meta-Learning: Towards Controllable Episodic Training Sets"** is an early-stage research in data-efficient learning. Most methods rely on heuristics to select representatives. 
Unlike this paradigm, DD **Wang et al., "Learning to Synthesize Data for Efficient Neural Network Training"** aims to learn how to synthesize a tiny dataset that trains models to perform comparably to those trained on the complete dataset. Wang~\textit{et al.} **Wang et al., "Meta-Learning for Few-Shot Learning: A Survey"** first proposed a bi-level meta-learning approach, which optimizes a synthetic dataset so that neural networks trained on it achieve the lowest loss on the raw dataset.
% , and thus the performance of models trained by synthetic and real datasets is matched.

Following this research, many researchers have focused on reducing the computational cost of the inner loop by introducing closed-form solutions, such as kernel ridge regression **Sra et al., "K-Means in O(1) Time"**. 
Zhao~\textit{et al.} **Zhao et al., "Efficient Dataset Distillation via Gradient Matching"** proposed an approach that makes parameters trained on condensed data approximate the target parameters, formulating a gradient matching objective that simplifies the DD process from a parameter perspective. In **Djolli et al., "Differentiable Siamese Augmentation for Data Efficient Learning"**, the authors enhanced the process by incorporating Differentiable Siamese Augmentation~(DSA), which enables effective data augmentation on synthetic data and results in the distillation of more informative images. Additionally, Du~\textit{et al.} **Du et al., "Sequential Dataset Distillation for Neural Network Training"** proposed a sequential DD method to extract the high-level features learned by the DNN in later epochs. 
By combining meta-learning and parameter matching, Cazenavette~\textit{et al.} **Cazenavette et al., "Matching Training Trajectories: A Meta-Learning Approach for Efficient Neural Network Training"** proposed Matching Training Trajectories~(MTT) and achieved satisfactory performance. Besides, a recent work, TESLA **Chen et al., "TESLA: Memory-Efficient Dataset Distillation via Gradient Matching"**, reduced GPU memory consumption and can be viewed as a memory-efficient version of MTT. 
% Instead of matching gradients or parameters, recent works proposed to condense datasets by matching distributions**.

\noindent \textbf{Backdoor Attack.} Backdoor attacks introduce malicious behavior into the model without degrading its performance on the original task by poisoning the dataset. Gu~\textit{et al.} **Gu et al., "BadNets: Identifying Vulnerabilities in Deep Neural Networks"** introduced the backdoor threat in DL with BadNets, which injects visible triggers into randomly selected training samples and mislabels them as a specified target class. To enhance attack stealthiness, Chen~\textit{et al.} **Chen et al., "Blended Backdoor Attack: A Stealthy Data Poisoning Attack"** proposed a blended strategy to make poisoned images indistinguishable from benign ones, improving their ability to evade human inspection. Furthermore, subsequent works explored stealthier attacks: WaNet **Arag√≥n et al., "WaNet: A Wide-Area Network for Adversarial Examples"** used image warping; ISSBA **Khoury et al., "ISSBA: Image Steganography and Security via Backdoor Attack"** employed deep steganography; Feng~\textit{et al.} **Feng et al., "Frequency-Domain Backdoor Attacks on Neural Networks"** and Wang~\textit{et al.} **Wang et al., "Triggered Frequency Shifts for Stealthy Backdoor Attack"** embedded triggers in the frequency domain; Yang~\textit{et al.} **Yang et al., "Backdoor Attack via Measurement Domain Trigger"** injected the trigger into the measurement domain; and Color Backdoor **Li et al., "Color Backdoor: A Uniform Color Space Shift for Stealthy Backdoor Attack"** utilized uniform color space shifts as triggers. 

Although existing works have demonstrated the vulnerability of deep networks to backdoor attacks, the exploration of such vulnerabilities in the context of DD remains limited. Only a few studies have evaluated the security risks associated with DD **Li et al., "Security Risks and Vulnerabilities in Dataset Distillation"**. This highlights the urgent need for a deeper investigation into the potential threats and vulnerabilities specific to DD.