\chapter{Probability theory - Measure theory- Statistics}

In this chapter we give a short overview on probabilistic and statistic notions.
We will see that everything that has seen introduced so far can be used in the context of probability and statistics,
namely information theory. Such a mixture gave birth to the theory of information geometry.  

We will also recall some notions of distance or divergence between probabilities, which is important for the ``learning process''.

\section{Key Information for Statistics}

When studying a mathematical problem in statistics, several key pieces of information come into play:
\begin{enumerate}
    \item  {\bf Qualitative Description of the Observed Phenomenon:} 
    
    \item[] This refers to a detailed understanding of the nature and characteristics of the event or situation being observed, which can be represented through various inputs $\omega_i$ collected from the sample space $\Omega$.
\item  {\bf Information About the Unknown Distribution of Outcomes $\omega$}: 
\item[]
This involves understanding how different outcomes are distributed, which is fundamental to statistical analysis.
\item  {\bf A Priori Probabilities}: 
\item[]
This entails the probabilities associated with observing various phenomena, represented as $(\Omega, \mathbf{S}, P_{\theta})$. Here, $\mathbf{S}$ denotes a $\sigma$-algebra that organizes measurable sets for the outcomes.
\item  {\bf A Distribution on the Parameter Space $Q\{d\theta\}$}:
\item[] This refers to a probability distribution defined on the range of possible parameter values that could explain the outcomes, denoted by the parameter space $\Theta$.
\end{enumerate}

In this context, the first item is associated with a collection of inputs gathered from the sample space $\Omega$. The second item involves the probability distribution $P[\cdot]$, which comes from a family of distributions ${P_{\theta}}$, where the parameter $\theta$ belongs to the parameter space $\Theta$. 
Overall, this transforms the issue of mathematical statistics into a decision-making problem, as one must determine which pieces of information are most beneficial. Formally, every statistical problem relates to a measurable space $(\Omega, \mathbf{S})$, which captures the outcomes of the observed phenomenon, a family of probability distributions ${P_{\theta}}$, and potentially additional a priori knowledge about the unknown phenomenon linked to another measurable space $(\Omega', \mathbf{S'})$, which may concern hypotheses or actions taken.

\section{Probability }\label{sec:proba}
\subsection{Probability space }
The basic notion in probability theory is the one of random experiment.
Such an experiment is
mathematically described by a probability space characterized by the triplet
$(\Omega,\bS, P)$ where:

\begin{enumerate}
\item $\Omega$ is a sample space i.e. the space of all possible outcomes
(samples) $\omega$ of the
random experiment.


\item ${\bS}$ is the family of events. An event  $A\in
{\bS}$, is identified with a subset $ A$ of $\Omega$. 


 \item The probability associates to each event a real number between 0 and 1. A probability is
defined by a map from the space of events ${\bS}$ to the set $[0,1]$.
\end{enumerate}

\begin{definition}[$\sigma$-algebra]
A family ${\bS}$ of subsets of a sample space $\Omega$ is a $\sigma$-algebra defined by 
the following algebraic structures:

\begin{enumerate}
\item $\Omega \in {\bS}$, 
\item $ A \in {\bS}\Longrightarrow A^c \in {\bS},$
\item $A_1,A_2,\dots  \in {\bS}\Longrightarrow
\bigcup\limits_{i=1}^\infty A_i
\in {\bS}.$
\end{enumerate}
the pair $(\Omega,{\bS})$ is called measurable space.
\end{definition}

\begin{examples}
\ 

\begin{itemize}
\item The \emph{smallest} $\sigma$-algebra containing one $A\subset \Omega$ is $\bS=\{\emptyset, A,A^c,\Omega\}$.


\item  The \emph{largest}  $\sigma$-algebra is consisted of all subset  of $\Omega$. This  algebra served us in discret spaces, but is too large to be useful in general

\item If $\Omega$ is a topological space the $\sigma$-algebra generated by all the open subset of $\Omega$ is called the \emph{Borel's algebra}.
\end{itemize}
\end{examples}

\begin{ex}\label{Ex:6.1}
    Show that the $\sigma$-algebra generated by $n$ disjoint subsets of a set $\Omega$ has cardinality $2^n$
\end{ex}



 \begin{definition}
 A probability $P$ is the following map  $$P: {\bS}\to [0,1]$$ which
satisfies the following properties: 

\begin{enumerate}
\item $P[\Omega] = 1,$
\item $ P[\emptyset] = 0,$
\item $ P[\mathop{\cup}\limits_{i=1}^\infty A_i] =
\sum\limits_{i=1}^\infty P[A_i]\enspace {\mathrm if}\enspace  A_i\cap A_j =
\emptyset,\enspace
 \forall  i \not= j.$
\end{enumerate}
\end{definition}

\begin{itemize}
\item The triplet
$(\Omega,{\bS},P)$ is called a probability space.

\item An event $A \in {\bS}$ such that $P[A] = 0$ is called a $P$-null set.

\item An event $A \in {\bS}$ such that $P[A] = 1$  is said to be realized
almost surely (usually denote: ``a.s.'').
\end{itemize}

From this definition, we can obtain have  useful relations:
\begin{itemize}
 \item $P[A^c] = 1-P[A], \quad A\in {\bS}$
\item  $P[A\setminus B] = P[A] -P[B], \quad B\subseteq A \in {\bS}$, in
particular $P[B] \leq P[A]$
\item  $P[A\cup B] =P[A] +P[B]-P[A\cap B],\quad A\, B \in {\bS}$.
\end{itemize}

\section{Examples}
To  illustrate the  construction of probability spaces, let us discuss the following examples.

\subsection{Flipping coin examples: How biased is it?}
\begin{example}
    A fixed coin is flipped independently  $n$ times. The sequence of outcomes  
  $(\omega_1, \dots, \omega_n)$, consisting of heads  $H$ and tails $T$, might look like \[\underbrace{HTHTHHHHT\ldots THTTT)}_{n}.\] To determine whether a coin is biased, we need to analyze the probability of obtaining heads $H$ and tails $T$ over multiple independent flips.

  \, 
  
  The question is whether we can assume that the coin in question is not biased.   
\, 

    Equivalently, we can ask: does the probability of obtaining heads  $H$ deviate from $\frac{1}{2}$ by less than $10^{-3}$?
\end{example}

Now, consider the following situation:

\begin{example}
    The coin is flipped independently, but the probability distribution governing the outcomes is unknown. The only available information is a constant parameter $\theta $.  

    We do know, however, that the probability of observing any specific sequence  $\omega$  with $k(\omega$)  heads  $H$  follows the Bernoulli probability law (this is the Bayesian approach):
    \[
    P(\omega) = \theta^{k(\omega)} (1 - \theta)^{N-k(\omega)}.
    \]
    
    Consequently, the probability distribution $P[\cdot]$  is given by
    \[
    p_j(\theta) = \theta^{k({\omega}_j)} (1 - \theta)^{N- k ({\omega}_j)}, 
    \]
    where
    \[
    j=1, \dots, 2^{N},
    \]
    and
    \[
    \theta \in [0,1].
    \]
    
    This defines a parametric curve in the $(n - 1)$-dimensional simplex of all probability distributions on the sample space  $\Omega$, where $ n=2^N $.
    
    Now, suppose we impose a lexicographical order on all possible sequences of $ \omega$'s. Then, the number $ k({\omega}_j) $ corresponds to the number of zeros in the binary representation of the integer $ j - 1 $.
\end{example}

\,

To end our discussion on the example of the flipping coin, we observe the following. 

\begin{example}~
    \begin{itemize}
        \item For $N=1$, we obtain $n=2^1=2$.
         The unknown probability distribution  $P[\cdot]$ is represented by a  ``curve'' with parametric equation
    \[
    p_j(\theta) = \theta ^{k(\omega_j)}
    (1- \theta)^{1- k({\omega}_j)}, 
    \]
for all probability distributions on $\Omega$    
\[
    j=1,2;\quad \theta \in [0,1]
    \]
     in the $1$-dimensional simplex, that is on the segment.

      \item  For $N=2$, we obtain $n=4$.
         The probability distribution $P[\cdot]$ is represented by a curve with parametric equation:   
\[
p_j(\theta) = \theta^{k({\omega}_j)} (1- \theta)^{2-k({\omega}_j)},
\]
where
\[
 j= 1,...,4
\]
and
\[
 \theta \in [0,1].
\]
In this case, the parametric curve is a curve in the $3$-dimensional probabilistic simplex.

\, 

We explain how this curve behaves in relation to the representation of words in letters $H$ and $T$. 

We begin with $\theta=0$, which corresponds to the vertex $P[TT]=1$. Then, the curve goes through the center of our simplex, since we have $P[HH]=P[HT]=P[TH]=P[TT]=\frac{1}{4}$ and where $\theta =\frac{1}{2}$. The endpoint of the curve is given by the vertex $P[HH]$ which corresponds to $(\theta=1)$.
    \end{itemize}   
\end{example}

More generally, the flipping coin example can be re-contextualized as follows. 
\begin{example}
For finite probabilities, we may use the \emph{multinomial distribution},  with sample space $\Omega=
\{\omega_1,\dots,\omega_m\}$ and $\sigma$-algebra generated by ${\bS}=
{\mathcal P}(\Omega)$, which is the set of subsets of $\Omega$ generated by the
elementary one element sets $\{\omega_i\}$. The probability is defined by 
\begin{equation}
 p_k=p(\omega_k)=P[\{\omega_k\}],\quad \sum\limits_{k=1}^m p_k=1,
\end{equation}
with
\[ p=\sum\limits_{k=1}^m
p_k\,\delta_{\omega_k}\]
and this induces \begin{equation}
p=\sum\limits_{k=1}^{m-1}p_k\,\delta_{\omega_k}
 +(1-\sum\limits_{k=1}^{m-1} p_k)\,\delta_{\omega_m}.
\end{equation}

\end{example}


\subsection{Other examples}
\begin{example}
    In the case of countably infinite probabilities, let 
$\Omega=\{\omega_1,\omega_2,\dots\}$ and let the $\sigma$-algebra be generated
by the elementary sets $\{\omega_i\}$. The probability is defined by
its value on these sets such that:
\begin{equation}
 p_k=p(\omega_k)=P[\{\omega_k\}],\quad \sum\limits_{k=1}^\infty p_k=1.
\end{equation}

A typical example of such probability distribution is the Poisson
distribution,
\begin{equation}
p_k= e^{-\lambda}{\lambda^k\over k!}.
\end{equation}


 In the case of continuous probability distributions with respect to the Lebesgue measure, we consider the case where the sample space is identified to 
$\mathbb{R}$ with the Borel
$\sigma$-algebra ${\bS}$ being the subset of ${\mathcal P}(\Omega)$
generated by the intervals $(-\infty,a],$ where $ a\in {\mathbb R}$. The
probability is then given by 
\begin{equation}
P\big[ (-\infty,
a]\big] = \int\limits_{-\infty}^a
\rho(\omega)\, d\omega 
\end{equation}
with
\[\rho(\omega)\geq 0,\quad \int\limits_{-\infty}^\infty
\rho(\omega)\, d\omega =1.\]
A classical example is the Gaussian distribution: 

\begin{equation}
P\big[ (-\infty, a]\big]= \int_{-\infty}^a
\rho(x)\, dx\,,\quad
\rho(x) = \frac{1}{ \sigma\sqrt{2\pi}}
e^{-\displaystyle{\frac{(x-\mu)^2}{ 2\sigma^2}}}.
\end{equation}
\end{example}

\begin{ex}\label{Ex:BorelThm} Prove the following theorem on $\sigma$-Algebras and subcovers.
Let $(X, \mathcal{A}, \mu)$ be a measure space, and let $\mathcal{B} \subseteq \mathcal{A}$ be a sub-$\sigma$-algebra of $\mathcal{A}$. If $\{ A_n \}_{n \in \mathbb{N}}$ is a countable cover of $X$ by sets in $\mathcal{A}$, then there exists a subcover in $\mathcal{B}$ if and only if the measure $\mu$ on $\mathcal{A}$ is uniquely determined by its restriction to $\mathcal{B}$.
\end{ex}

\section{Measure}\label{sec:meas}
In many contexts, it is beneficial to consider a notion of measure that extends beyond the confines of probability theory. Unlike a probability measure--which by definition assigns a total measure of 1 to the entire space--a general measure does not require such normalization. By relaxing the normalization condition, we can define and work with measures that better reflect the intrinsic properties of the space under consideration. This generalization opens up new possibilities for both theoretical developments and practical applications, ranging from quantum field theory to ergodic theory and beyond.

\begin{definition}
 A measure $\lambda$ on the measurable space $(\Omega,{\bS})$ is a map, which maps events to real positive numbers such that:
\begin{enumerate}
\item $\lambda: {\bS}\to \bbR^+$,
 \item $\lambda[\emptyset] = 0,$
\item $\lambda[\mathop{\cup}\limits_{i=1}^\infty A_i] =
\sum\limits_{i=1}^\infty P[A_i]\enspace$ if $ A_i,\cap A_j = \emptyset, \,
 \forall\enspace i \not= j.$
\end{enumerate}
\end{definition}

\vspace{5pt}
\begin{itemize}
\item A measure $\lambda$ is said to be emph{bounded (or finite)} if  $\lambda[\Omega]<\infty $.
Hence a probability is a finite measure (bounded by 1).\\

\item A measure $\lambda$ is \emph{$\sigma$-finite} if 
\[\Omega =
\bigcup\limits_{k=1}^\infty
A_k, \, \text{ with  }\,  \lambda[A_k]< \infty.\] 
\end{itemize}


\begin{remark}
We draw the readers attention upon the fact that a $\sigma$-finite measure is in general not finite. An example is
the Lebesgue measure $\lambda$ on the real line, which is finite on all bounded
interval $\lambda[ a,b] = \mid b-a\mid$, but not finite.
\end{remark}

 \begin{ex}\label{Ex:Borel}
   If $X$ is a metric  space and $A$ and $B$ are two disjoint closed subsets of $X$, then there exists a continuous function $f(x)$ on $X$ with properties: 

   \,

   1) $0\leq f(x)\leq 1$

\, 

   2) \[\begin{cases}
       f(x)=0 & \text{for} \quad  x\in A \\
       f(x)=1 & \text{for} \quad x\in B
   \end{cases}\]
    
   \end{ex} 

\section{Measurable function - Random variables}\label{sec:rand}

\subsection{Definition}
\ 

Let us first consider  a random experiment described by the probability
space $(\Omega,\bS,P)$.  A real random variable is an application from the sample space
$\Omega$ to a subspace $E\in \bbR$  which preserves the
algebra of events. 
 
 \begin{definition}[Mesurable function]
  Let $(E, \cB)$ a mesurable space, a \emph{mesurable function} $f$ from
$(\Omega,\bS)$ to  $(E, \cB)$ is  such the inverse image 
of a mesurable set in $\cB$ is a mesurable set in  $\bS$
\[ 
f^{-1}(B)=\{\omega \in \Omega,\, X(\omega) \in B\} \in \bS, \quad \forall B\in
\cB.
\]
\end{definition}

\begin{definition}[Random variable]
  Let $(\Omega, \bS,P)$ a probability space, a $E$ valued \emph{random variable} $X$ is the class of $P$-ae ($P$-almost everywhere, i.e. up to set of $P$-measure $0$)  measurable function  from $(\Omega,\bS,P)$ to  $(E, \cB)$   such the inverse image of a measurable set in $\cB$ is a measurable set in  $\bS$

\end{definition}


\begin{itemize}
\item A real random variable on the measurable space $(\Omega,\bS)$ is a measurable
function $X$ with value in $E \subseteq\bbR$ (or $\overline\bbR=\bbR\cup\{-\infty,+\infty\}$) and $\cB$ is the $\sigma$-algebra generated by the topology on $E$. \\

\item If $E$ is finitely or infinitely many
countable we speak about finite or discrete random variables.\\

\item If $E$ is a vector space we speak about random vectors.
\end{itemize}

Let Let $(\Omega, \bS,P)$ a probability space, the integral over $\Omega$, if there exist, of a random variable $X$  is called the {\emph{expectation (or mean)} of $X$, and denoted 
\[
\bbE_P[X]=\int_\Omega X(\omega) P[d\omega].
\]

That is $\bbE_P[X]$ exist if $X\in L(\Omega,\bS,P)$.




\section{Dominating Measure}

\begin{definition}
If $ \mu $ and $ \nu$ are two measures on the same measurable space  $(\Omega,\bS)$,
$\mu $ is said to be \emph{absolutely continuous} with respect to $ \nu $ if $ \mu ( A)=0$ for every set 
$A$ for which $\nu (A)=0$. This is written as "$ \mu \ll \nu $". That is:
\[\
mu\ll \nu \text{ if and only if  \  for all } A \in \bS, \ (\nu (A)=0 \Leftarrow  \mu (\cA)=0).
\]

When $ \mu \ll \nu $,then $\nu $ is said to be \emph{dominating} $ \mu $.
\end{definition}

Absolute continuity of measures is:
\begin{itemize}
\item reflexive, 
\item transitive, 
\item not antisymmetric
\end{itemize}

\vspace{3pt}
If $ \mu \ll \nu $ and $ \nu \ll \mu$, the measures $ \mu $ and $\nu $ are said to be \emph{equivalent}. 
Thus absolute continuity induces a partial ordering of such equivalence classes.

\begin{ex}\label{Ex:domMeas}
   As a concrete example of a dominating measure,  let $\{P_k\}$ be a finite or countable family of probabilities on the same measurable space $(\Omega,\bS)$ and consider
   \begin{itemize}
       \item the arithmetic mean of a finite collection of probability measures:
       \[
       P_0[\cdot] = \left(\frac{1}{n} \sum_{k=1}^n P_k\right)[\cdot],
       \]
       \item or, in the case of a countable family, the weighted series:
       \[
       P_0[\cdot] = \left(\sum_{k=1}^{\infty} \frac{1}{2^k} P_k\right)[\cdot].
       \]
   \end{itemize}
   Prove that $P_k \ll P_0$
\end{ex}

\vspace{3pt}

If $ \mu $ is a signed or complex measure, it is said that $ \mu$  is absolutely continuous with respect to 
$\nu $if its \emph{variation } $ |\mu |$ satisfies$ |\mu |\ll \nu$ ; equivalently, if every set 
$\cA$ for which $ \nu (A)=0$ is $ \mu $-null.

\begin{theorem}[The Radon--Nikodym theorem]
If $\mu$ is absolutely continuous with respect to $\nu$, and both measures are $\sigma$-finite, then $\mu$ has a density, or "Radon--Nikodym derivative", with respect to 
$\nu$, which means that there exists a $\nu$-measurable function 
$\rho$ taking values in $ [0,+\infty )$, denoted by $ \rho=d\mu /d\nu$ , such that for any 
$\nu$-measurable set $\cA$ we have:
\[
 \mu (A)=\int _{A}\rho\,d\nu .
\]
\end{theorem}

If $\mu=P\ll \nu$ is a probability and $\nu$ is a positive $\sigma$-finite measure, the Radon--Nikodym derivative $\rho=\frac{dP}{d\nu}$ is a random variable, called density  (or distribution) of $P$ with respect to the measure $\nu$. We have

\[
\rho= \frac{dP}{d\lambda},\quad \quad P(A)=\int_{A}\rho d\nu,\  \forall \, A\subset \bS, \quad \int_{\Omega}\rho d\nu=1 . 
\]


In specific cases, a probability measure  $\mu$ can behave as a dominating measure for another measure $\nu$
if $\nu \ll\mu.$ 

\, 

\section{About Information Geometry}

The family of probability densities of (parametric) probability measures is denoted: 
\[\sfS= \{\rho_\theta \in L^1(\Omega, \bS,\mu) \quad \rho>0, \quad \mu - a.e \quad \int_{\Omega}\rho d\mu=1\}.\]
We consider the case when $\sfS$ is a smooth topological manifold.

Probability distributions in a statistical manifold of exponential type are such that there exist parameterizations $\theta$ satisfying:
\[
\rho_\theta = \exp \left( \sum_{i=1}^n \theta_i X^i- \psi (\theta)\right),
\]

where parameters $\theta$ and random variables $X=(X^i)_{i=1}^n$ are chosen adequately,
$\psi(\theta)$ is a \emph{potential function}. 


In information geometry, an important question is whether a given family of probability distributions can be recognized as an exponential family--or, more generally, if it exhibits an exponential structure. Recall that a probability distribution belongs to an exponential family if its density function can be written in the form:
\[\rho_\theta(\omega)=h(\omega)\exp\left( \eta(\theta)\cdot \sfT(\omega)-\sfA(\theta)\right),\]
where: 

\begin{itemize}
    \item $h(\omega)$is a base measure (Radom--Nikodym derivative w.r.t the dominated measure $\mu$),
    \item $\eta(\theta)$ represents natural parameters
    \item $\sfT(\omega)$ is a sufficient statistic,
    \item $\sfA(\theta)$ is the log-partition function ensuring normalization.
\end{itemize}

\vspace{5pt}
This representation is not merely a convenient rewriting; it endows the parameter space with a rich geometric structure. 
In particular, when a family of distributions is exponential, the Fisher information metric
 $g_{ij}=\bbE[ \partial_i \ln \rho \; \partial_j \ln \rho]$ and the dual affine connections
(which emerge naturally in this setting) yield a dually flat space. This flatness simplifies the study of geodesics, divergence functions, and many aspects of statistical inference.

A classical example is the normal (Gaussian) family. 
While the normal distribution indeed constitutes an exponential family, its precise exponential form depends critically on the choice of parametrization. 

\begin{ex}\label{Ex:Gauss}
The Normal/Gaussian distributions on ($\bbR, \bB,\lambda)$, where $\bB$ is the Borel algebra on $\bbR$ and $\lambda[(a,b)]=\vert b-a\vert$ the Lebesgue measure is defined by  the probability distribution with respect to the Lebesgue measure on $\bbR$ by: 
\[\rho_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp{-\frac{(x-\mu)^2}{2\sigma^2}}, \quad \mu\in \bbR,\ \sigma >0.\]
Is the normal family exponential? If yes, give a proof. 
\end{ex}


The specific choice of parametrization affects not only the expression of the density but also the form of the Fisher information metric and the associated affine connections. 
Thus, determining whether a family of probability distributions is of exponential type--and selecting an appropriate parametrization in cases like the normal family--is fundamental in information geometry. It allows one to exploit powerful geometric techniques to derive insights into the behavior of statistical models, facilitating tasks such as parameter estimation, hypothesis testing, and the study of statistical divergences.

\subsection{Tangent space to a manifold of probability distributions}
Let $u\in L^{2}(\Omega,\bS, P_{\theta})$ be a tangent vector to the manifold  $\sfS$ (of dimension $n$) of
probability distributions (we assume of exponential type) at the point $P_{\theta}$.
\[
\cT_{\theta}=\left\{ u\in L^{2}(\Omega, \bS, P_{\theta}); \quad \bbE_{P_{\theta}}[u]=0,\quad u=\sum_{i=1}^{d}u^i\partial_{i}\ell_{\theta}         \right\}
\]
where $\bbE_{P_{\theta}}[u]$  is the expectation value, with respect to the probability ${P_{\theta}}$.

The tangent space to the considered manifold $\sfS$ is (locally) isomorphic to the $n$-dimensional linear space generated by the family of (centered) random variables (called \emph{score vector}),
$( \partial_{i}\ell_\theta), \ \{i=1,\dots,n\}$ where $\ell_{\theta} = \ln \rho_{\theta}.$

\begin{remark}
Densities are positive random variables almost everywhere; the tangent vectors are signed measures (it is a real-valued measure) with vanishing mean value.  
\end{remark} 

\begin{ex}\label{Ex:esp0}
  Show that $\bbE_{P_{\theta}}[u]=0$.
\end{ex}

\begin{ex}\label{Gauss2}
Show that the tangent space $\cT_{\mu,\sigma} $ to the manifold of Gaussian distribution \eqref{Ex:Gauss} is spanned, in the $(\mu, \sigma)$-parametrization, by the two random variables

\begin{itemize}
    \item ${\partial_\mu \ln \rho (x) = \frac{\partial \ln \rho}{\partial \mu}(x)  =\frac{x-\mu }{
\sigma^2}}$
\item ${\partial_\sigma \ln \rho (x) = \frac{\partial \ln \rho }{ \partial \sigma}(x)  =\frac{(x-\mu)^2 }{\sigma^3}-{1\over \sigma}}.$
\item Deduce that the plane $\cT_{\mu,\sigma}$ consists of all the quadratic polynomials in $x$ whose
expectation vanishes: 
\[
T_{\mu,\sigma}= \{ Ax^2+Bx+C, \quad C = A(\sigma^2 +\mu^2) -B\mu\}.
\]
\end{itemize}
\end{ex}

\subsection{The Riemannian metric}
  In the basis $\partial_i\ell_{\theta}$ the (Fisher--Rao) metric is the covariance matrix of the score vector:
\[   {g_{i,j}(\theta)=\mathbb{E}_{P_{\theta}}[\partial_i\ell_{\theta}\partial_{j}\ell_{\theta}]   } \]
 with   
\[{g^{i,j}(\theta)=\mathbb{E}_{P_{\theta}}[a^{i}_{\theta}a^{j}_{\theta}]},\] 

where  $\{a^{i}\}$ form a dual basis to $\{\partial_j\ell_{\theta}\}$:

\[a^{i}_{\theta}(\partial_j\ell_{\theta})=\mathbb{E}_{P_{\theta}}[a^{i}_{\theta}\partial_j\ell_{\theta}]=\delta^i_{j}\]

and
\[
\mathbb{E}_{P_{\theta}}[a^{i}_{\theta}]=0.
\]


\subsection{Applying the notions of parallel transport}
Suppose that we perturb infinitesimally $\theta $ such that $ \theta'=\theta+d\theta$.
Consider the linear map

\[
m:\cT_{\theta+d\theta}\to \cT_{\theta}
\] (this map depends on $d\theta$ and reduces to the identity map as $d\theta$ tends to 0). 

The given vector $$u^i\partial'_i \in \cT_{\theta+d\theta}$$ is mapped to $(u^k+d\theta^i\Gamma_{ij}^ku^j)\partial_k\in\cT_{\theta}$. 

This construction allows to establish a correspondence between the vectors in $T_{\theta}$ and $T_{\theta'}$.

The function $\Gamma_{ij}^k(\theta)$ are the coefficients of the affine connection.

\subsection{Covariant derivative}

Let $\pi:E\to \sfS$ be a vector bundle. A covariant derivative (also knows as a connection) is an $\bbR$-bilinear map 

\[
\nabla: \Gamma(\cT\sfS)\times \Gamma(E)\to \Gamma(E) \quad (X,s)\mapsto \nabla_{(X,s)}
\]
Consider the intrinsic change in the $j$-th basis vector $\partial_j (\theta)$ as $\theta$ deforms into $\theta'$, 
in the direction of $\partial_i$. We obtain the following vector field:
\[
\nabla_{\partial i}\partial_j=\Gamma_{ij}^k(\theta)\partial_k(\theta).
\]
This is a covariant derivative of the vector field $\partial_j$ along $\partial_i$. It is determined from the coefficients of the affine connection (namely $\Gamma_{ij}^k(\theta)$).

\subsection{Amari--Chentsov tensor}

There exists a skewness tensor. It is a fully symmetric covariant tensor of rank 3: 

\[\cT\cM \times \cT\cM \times \cT\cM\to \mathbb{R},\] 

given by \[ T|_{P_{\theta}}(u,v,w)= \mathbb{E}_{P_{\theta}}[u_{\theta}v_{\theta}w_{\theta}] \] 

so that we have:  \[T_{ijk}(\theta)=\mathbb{E}_{P_{\theta}}[\partial_i\ell_{\theta}\partial_j\ell_{\theta}\partial_k\ell_{\theta}].\]


The skewness tensor was introduced to formalize the notion of statistical curvature, via the affine connections $\{\nabla^{\alpha}\}_{\alpha\in \mathbb{R}}$. 
We have the following:
\[\nabla^{\alpha}_{X}Y= \nabla^{0}_{X}Y- \frac{\alpha}{2}(\underbrace{T.g^{-1}}_{\overline{T}})(X,Y),\]
 for any couple of vector fields $X, Y$ over $\sfS$; 
$\nabla^{0}$ is the Levi--Civita connection and $(\cdot)$ is the “contraction” (of two tensors).   

\subsection{Statistical inference}
The coefficients of the affine connection are:

\[\Gamma_{ijk}^\alpha=\mathbb{E}[\{\partial_i\partial_j\ell(x,\theta)+\frac{1-\alpha}{2}\partial_i\ell(x,\theta)\partial_j\ell(x,\theta)\}\partial_k\ell(x,\theta)].\] 
This called the $\alpha$-connection. 

Recall that the third order Amari--Chentsov tensor is defined to be: 
\[T_{ijk}(\theta)=\mathbb{E}[\partial_i\ell_\theta\partial_j\ell_\theta\partial_k\ell_\theta].\]

We use this tensor to simplify calculations of the $\alpha$-connection i.e.: 
$$\Gamma_{ijk}^\alpha=\Gamma_{ijk}^1+\frac{1-\alpha}{2}T_{ijk}.$$

 The $\alpha$-connection has a meaning of its own, depending on $\alpha$.  It plays, for example, an important role in statistical inference. 


\section{Hints and solutions to Exercises}

Exercise \ref{Ex:6.1}. It is easy to show it using basic set theory.

\, 
 
Exercise \ref{Ex:BorelThm}.
~

1. \textbf{Second-Countability of $ X $:} \\
   By definition, $ X $ is second-countable, meaning it has a countable basis $ \mathcal{B} = \{B_n\}_{n \in \mathbb{N}} $ for its topology. That is, every open set in $ X $ can be written as a union of elements of $ \mathcal{B} $.

2. \textbf{Open Cover $ \mathcal{U} $:} \\
   Let $ \mathcal{U} = \{U_\alpha\}_{\alpha \in I} $ be an open cover of $ X $. This means:
   \[
   X = \bigcup_{\alpha \in I} U_\alpha.
   \]

3. \textbf{Constructing a Countable Subcover:} \\
   For each $ x \in X $, there exists some $ U_\alpha \in \mathcal{U} $ such that $ x \in U_\alpha $. \\
   Since $ \mathcal{B} $ is a basis, there exists some $ B_{n_x} \in \mathcal{B} $ such that:
   \[
   x \in B_{n_x} \subseteq U_\alpha.
   \]
   Let $ \mathcal{B}' = \{B_{n_x} \mid x \in X\} $. This is a subset of $ \mathcal{B} $, and since $ \mathcal{B} $ is countable, $ \mathcal{B}' $ is also countable.

4. \textbf{Extracting a Countable Subcover:} \\
   For each $ B_{n_x} \in \mathcal{B}' $, choose one $ U_\alpha $ such that $ B_{n_x} \subseteq U_\alpha $. Let $ \mathcal{U}' $ be the collection of all such $ U_\alpha $. \\
   Since $ \mathcal{B}' $ is countable, $ \mathcal{U}' $ is also countable.

5. \textbf{Verification that $ \mathcal{U}' $ is a Cover:} \\
   For any $ x \in X $, there exists $ B_{n_x} \in \mathcal{B}' $ such that $ x \in B_{n_x} \subseteq U_\alpha $ for some $ U_\alpha \in \mathcal{U}' $. \\
   Thus, $ x \in U_\alpha $, and since $ x $ was arbitrary, $ \mathcal{U}' $ covers $ X $.

6. \textbf{Conclusion:} \\
   We have constructed a countable subcover $ \mathcal{U}' $ of $ \mathcal{U} $, proving the theorem.

\,

  Exercise \ref{Ex:Borel}. 
   \, 
   
   If $\inf_{x\in A,y\in B} d(x,y)=\delta>0$ then the function $f$ can be chosen to be uniformly continuous. 

   Let $f(x)=d(x,A)/[d(x,A)+d(x,B)]$. It is easy to verify that $f$ satisfies both conditions 1 and 2. The other part of the theorem follows from the fact that $d(a,X)+d(x,B)\geq \delta$ and from the following fact that the function $d(x,A)$ satisfies the inequality \[|d(x,A)-d(y,A)|\leq d(x,y).\]
   In particularly, $d(x,A)$ is uniformally continuous. 

\, 

Exercise. \ref{Ex:domMeas}. The proof is omitted since it is considered in Ex. 7.2.
\, 

Exercise. \ref{Ex:esp0}. The proof is omitted since it is a straightforward calculation.

\, 

Exercise \ref{Ex:Gauss} and Ex. \ref{Gauss2}.
By an appropriate change of variables, for the Gaussian, this density can be rewritten in the exponential family form, 
revealing its natural parameters and sufficient statistics. 

