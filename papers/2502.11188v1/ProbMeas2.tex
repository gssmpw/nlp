\chapter{Categorical Structures in Probability and Measure Theory}
In this chapter, we embark on a journey into probability and statistics in a manner that may initially surprise the reader: through the lens of philosophy. Rather than taking the conventional route of axiomatic probability theory, we begin with a deeper reflection on the nature of mathematical structures and their underlying symmetries. Our point of departure is a generalization of Felix Klein’s revolutionary insight--his Erlangen Program--which reinterpreted geometry as the study of figures and their congruences under transformation groups.

\, 

We invite the reader to consider an analogous perspective in the realm of probability distributions. Just as Klein’s figures reside in a structured mathematical world shaped by transformations, we view probability distributions as abstract entities within a broader conceptual space. This space, much like Plato’s world of ideas, consists of idealized probabilistic objects related by structural equivalences. These equivalences, or congruences, form the foundation upon which we build a categorical framework for probability theory.

\, 

This shift in perspective naturally leads us to the introduction of categories of probability distributions, where morphisms arise from Markov kernels--stochastic mappings that embody the very notion of congruence in this setting. In this way, Markov kernels serve as the probabilistic analogues of Klein’s geometric transformations, revealing deeper structures in statistical theory that might otherwise remain hidden.

\, 

Through this approach, we illuminate the rich interplay between probability, statistics, and abstract mathematical structures, setting the stage for a unifying perspective that extends beyond classical interpretations.

\section{Philosophical Approach}
\subsection{}Let us begin by considering a broad and fundamental question: 
\begin{center} {\it what is, in general, the aim of geometry?}  
\end{center}

One can state succinctly that geometry is primarily concerned with:  
\begin{itemize}
    \item the study of spaces.
    \item The transformations of these spaces, namely, their motions or the actions of groups.  
    \item Entire classes of geometric objects.  
\end{itemize}

\, 

\subsubsection{}
The foundations of geometry, as formulated by Klein, are based on the study of geometric figures and the transformations that act upon them. This perspective, known as \textit{Erlangen’s program}, emphasizes the structural relationships between figures rather than their specific realizations. In its classical setting, this formalism provides a natural framework for understanding symmetries, invariants, and classification problems within geometry.

\,

\subsection{}
For us, however, we will think of those (Klein) figures as figures {\it in the sense of Plato}. Plato's world of geometric figures refers to his philosophical concept of the {\it World of Forms}, where perfect and unchanging geometric shapes exist, independently of the physical world.

\, 

In Plato's philosophy, as described in dialogues such as the \emph{Republic} and the \emph{Phaedo}, mathematical objects: circles, triangles, and other ideal forms do not exist merely as physical approximations but as pure, abstract entities in a higher, non-material realm.
\footnote{In {\it The Republic,} Plato’s ``Allegory of the Cave'' illustrates how ordinary humans perceive mere shadows (imperfect physical objects), while true philosophers seek to understand the Forms, including perfect geometric structures. A drawn triangle or sphere in the physical world is an imperfect shadow of the true ideal triangle or sphere that exists in the intelligible realm.}

\,

It is precisely {\it this concept} of Plato's world of figures/forms which is used in Klein's perception and which we will use also to describe geometric structures in probability and measure theory.  
 
\subsection{}
In the present work, we extend this viewpoint beyond its original domain by considering a formalism in which Klein’s approach is adapted to the realm of probability theory and statistics. The motivation for such a generalization arises naturally: if one seeks to construct a statistical geometry, it is necessary to first identify the appropriate notion of \textit{geometric figures} and the corresponding transformations that preserve the relevant structures. 

\,
\subsubsection{}
Our approach follows a twofold strategy. First, we identify probabilistic objects that play the role of geometric figures in this generalized setting. These may include, for instance, families of probability distributions, families of distributions, or spaces of statistical models. Second, we introduce transformations between these objects that preserve key statistical or probabilistic properties. 
\, 

The fundamental problem then becomes: 

\,

\emph{how does one define and classify these transformations, and what structures do they induce?}

\,
\subsubsection{}
From this perspective, Markov categories and other categories called $CAP$, $CAPH$, 
$FAMH$ emerge as natural generalizations of geometric figures in Klein’s sense, and their transforms. These transformations--given by statistical morphisms, stochastic maps, or Markov kernels--serve as the analogues of geometric transformations in classical geometry. This interpretation provides a unifying language in which statistical inference, decision theory, and information geometry can be understood in terms of an overarching geometric framework.

\subsubsection{}

The goal of this work is to explore this formalism systematically and to establish the necessary mathematical structures that allow for a coherent treatment of statistical geometry. In particular, we shall investigate the role of categorical constructions, invariants under transformations, and the emergence of new geometric notions adapted to probabilistic contexts. Ultimately, our approach seeks to trivialize Klein’s original vision in such a way that its extension to statistics appears as a natural and inevitable development.

\,
The Klein figure formalism admits a natural extension to probabilistic contexts, particularly in the study of parametrized families of probability distributions. This adaptation establishes a coherent theoretical framework that facilitates systematic analysis and operational flexibility within such probabilistic structures.

\subsection{Towards a generalization of Klein's Plato-Figures}
The most elementary class of geometric objects consists of subsets of a given space, referred to by Felix Klein as \textit{figures}.

\,

\begin{itemize}
    \item Two figures $A$ and $B$ are said to be \textit{congruent} if there exists a motion mapping $A$ onto $B$, along with an inverse motion mapping $B$ onto $A$. The geometer’s principal concern is to investigate the properties of figures that remain invariant under such motions--those properties that are identical for all congruent figures. 
    \item[]
    \item The central problem thus becomes the determination of \textit{invariants} of figures, namely, real-valued functions that take equal values on congruent figures. A complete system of such invariants allows for a full {\bf classification of geometric configurations. }
\end{itemize}

\, 

Since any figure can be regarded as a set of points, one is naturally led to a {\bf generalization of this concept } of geometry--extending beyond the study of figures in the classical sense. Alongside figures in Klein’s sense, which are simply sets, one may consider \textit{parametrized sets}, thereby introducing a new geometric framework to examine their intrinsic properties.

\subsection{}

Let $\Theta$ be an index set parametrizing such a collection, and let us construct an epimorphism from this parameter set onto the space under consideration. That is, for each parameter value $\theta \in \Theta$, a corresponding point of the considered figure is assigned. In general, a single point of the figure may be associated with multiple parameter values, reflecting a richer underlying structure.

\,

Following Klein's approach to geometry, we may consider parametrized sets as Klein figures, where the parametrization is defined in a specific way. Such examples include:
    \begin{itemize}
        \item finite ordered sequences of points,  
        \item parametrized smooth curves or surfaces,  
        \item smooth manifolds parametrized by locally smooth coordinates--these can be treated both as points or as smooth surfaces.
        \end{itemize}

Furthermore, in the scope of generalizing objects and structures, we can recall what a category is, as we will need it later. 

\,
\subsection{Categories as a classifying tool}
A category is a mathematical structure that captures relationships between objects in an abstract way. At its core, a category consists of objects and morphisms (arrows) between them, which satisfy rules of composition and identity. One should think of a category not just as a formalism but a lens through which mathematics reveals its hidden symmetries and harmonies.

To be more precise, A category 
$\mathscr{C}$ consists of the following data:
\begin{itemize}
    \item A collection of objects, denoted by ${\rm Ob}(\mathscr{C})$
       \item  For each pair of objects $X, Y \in {\rm Ob}(\mathscr{C})$, a set of morphisms from $X$ to $Y$.
\end{itemize}
These morphisms must satisfy the following axioms:

\,

{\bf (1) Composition} 

For any three objects $X,Y,Z$ there exists a composition map $\circ$ which assigns to each pair of morphisms $f:X\to Y$ and $g:Y\to Z$ a composite morphism 
\[g\circ f:X\to Z.\]

\,

{\bf (2) Identity Morphisms}

For each object $X$, there exists an identity morphism $id$ such that for all 
$f:X\to Y$ and $g:Y\to X$ we have 
\[id_Y\circ f=f\quad \text{and} \quad g\circ id_Y=g\]


\, 

{\bf (3) Associativity}

For any morphisms $f:X\to Y$, $g:Y\to Z$, and $h:Z\to W$
one requires:
\[h\circ(g\circ f)=(h\circ g)\circ f.\]


\, 

\section{Generalization of Klein's (Plato)-figures/Congruences}

\subsection{Generalized Klein Figures}
One extends Klein’s notion of figures, beyond these classical cases, namely in the framework of probabilities. 
The following examples provide examples of  generalized Klein figures.

\, 
    \begin{itemize}
        \item Sets of probability distributions of a given type;  
        \item Dominated families of probability distributions, namely, all finite families of probability distributions on finite $\sigma$-algebras.  
    \end{itemize}
(Indeed, any finite or countable family $\{P_k\}$ of probability measures on a finite $\sigma$-algebra is necessarily dominated, as it admits a dominating measure.)




\subsection{Markov Transformations}

Using the Klein figure formalism, an essential ingredient in constructing a geometric framework for probabilistic or statistical quantities is the study of transformations acting on generalized Klein figures. We now introduce such transformations.

\begin{definition}
    Let $(\Omega, \bS)$ and $(\Omega', \mathbf{\Sigma})$ be two measurable spaces. A \textit{Markov transition probability}, or equivalently, a \textit{Markov morphism}, is a real-valued function \[\Pi(\omega, \mathcal{A})\] defined for $\omega \in \Omega$ and $\mathcal{A} \in \mathbf{\Sigma}$, satisfying the condition that for each fixed $\mathcal{A}$, the function \[\omega \mapsto \Pi(\omega, \mathcal{A})\] is $\bS$-measurable.  
\end{definition}

This transition probability distribution describes a Markovian random transition from the measurable space $(\Omega, \bS)$ to $(\Omega', \mathbf{\Sigma})$. The following theorem establishes an extension property for such transformations.

\,

    Let us consider now, classical example of a statistical problems.


We illustrate the congruences, mentioned in the first section of this chapter on a concrete example, which is presented below. 

\begin{example}
 We are interested in estimating the parameters of a normal distribution based on a set of $N$ independent observations $\{x^{(1)},...,x^{(N)}\}$ of  the parameters of a normal distribution with a density given by:
\[
p(x;\mu,\sigma)= \frac{1}{\sqrt{2 \pi} \sigma} \exp{ - \frac{(x-\mu)^2}{2\sigma^2}}.
\]  
Here, 
$\mu$ represents the mean, and 
$\sigma^2$ denotes the variance of the distribution.

From a geometric perspective, the set of all possible normal distributions (with densities of the form given above) forms an infinite-dimensional manifold, as it includes all probability distributions on the real line that are absolutely continuous with respect to the Lebesgue measure. However, when we restrict our attention to the family of normal distributions parameterized by $(\mu,\sigma)$, we obtain a two-dimensional surface within this infinite-dimensional space. This surface consists of all normal distributions defined on the Borel 
$\sigma-$algebra of the real line.
%%%
\end{example}

\subsection{Klein's Congruences and the Universal property}
The following theorem provides a condition under which two families of probability distributions are considered congruent, meaning that they share an underlying structural equivalence in terms of how their probability measures are defined. This  gives rise to a certain notion of {\bf universal property}, since it is reminiscent of certain  universal properties in algebraic geometry. 

\, 
\begin{theorem}[Universal property]\label{T:Univ}
Let us consider two families of probability distributions, denoted as: $ w_1$ and $w_2$ which are defined as

\[w_i=\{P_{\theta}^{{i}}(d\omega^{(i)}), \quad \theta\in \Theta\},\] 

where these families are probability distributions indexed by a parameter $\theta$, meaning that for all $\theta\in \Theta$ we have a corresponding probability measure on the spaces $(\Omega^{1},{\bf S}^{(1)})$ and $(\Omega^{2},{\bf S}^{(2)})$, respectively.

\, 

The two families $w_1$ and $w_2$ are said to be {\bf congruent} if there exists:

\begin{enumerate}
    \item {\bf A measurable mapping:}
$\epsilon=\varphi_{i}(\omega)$ of the measure spaces $(\Omega^{i},{\bf S}^{(i)})$, on which they are defined, onto a {\bf finite} space \[(\mathscr{E},{\bf S}(\mathscr{E})).\]

\item {\bf A common family of probability distributions:}
    \[R_{\theta}(d\epsilon)\] on the space $\mathscr{E}$, which serves as an intermediate representation of the distributions in both families.
    \item {\bf A family of transition distributions:}
     $$Q^{(i)}(\epsilon;d\omega^{(i)})$$
that describes how the probability measure on $\mathscr{E}$ is "transferred" back to the original space $\Omega^{(i)}$.
This relation is formalized as: 
\[P^{(i)}_{\theta}\{\cdot\}=\int_{\mathscr{E}}R_{\theta}\{d\epsilon\}Q^{(i)}(\epsilon,\cdot),\] where the key consistency condition is such that \[ Q(\epsilon,\varphi^{-1}(\epsilon))=1.\]
\end{enumerate}
This ensures that each outcome 
$\omega^{(i)}_{j}$ belongs to the preimage of its corresponding 
$\epsilon$, meaning that 
correctly classifies elements into equivalence classes in 
$\mathscr{E}$.

\[
\begin{tikzcd}
{} & (\mathscr{E},{\bf S}(\mathscr{E})) \arrow{dr}{\varphi^{-1}_2} \\
(\Omega^{1},{\bf S}^{(1)}) \arrow{ur}{\varphi_1} \arrow{rr}{\varphi^{-1}_2\circ \varphi_1} && (\Omega^{2},{\bf S}^{(2)})
\end{tikzcd}
\]
\end{theorem}
%\begin{example}
%In a similar vein, let us consider another example concerning the structure of congruent families within an algebra with three atoms $(\omega_1, \omega_2, \omega_3)$.


%\end{example}
\begin{example}
Let us apply this theorem on congruences between families of distributions to a congruence between certain simplices, that contain them. The original families of probability distributions can be described within each simplex using the same barycentric coordinates. This allows us to focus our study on the structure of equivalent families, given by simplices in a special position. Such families will be called maximal.

More precisely, consider two simplices:

\begin{itemize}
    \item The first simplex, denoted as $Cap(\Omega',\mathbf{S'})$,  has vertices ${\bf e}_1,...,{\bf e}_n$ corresponding to the  outcomes   $(\omega_1,...,\omega_n)$.
    \item These outcomes are partitioned into 
$m$ subsets, denoted as $E_1,\cdots, E_m$.
\end{itemize}

For each face of the original simplex 
$Cap$, determined by a subset 
$E_i$ of its vertices, we select a point with barycentric coordinates $P_{E_i}^{(1)}[\omega]$, for each  $\omega \in E_i$ and for each $i\in \{1,\cdots,m\}$. We assume that all but one of these points are chosen in the interior of their respective faces.

\, 

The collection of these points 
$P_{E_i} [.]$ are the vertices of $m$-dimensional forms an 
$m$-dimensional subsimplex. This subsimplex is equivalent to a similarly constructed 
$m$-dimensional subsimplex in the second simplex. Through this construction, we establish a structural equivalence between the two families in terms of their representation within simplicial geometry.

%
\end{example}


\begin{theorem}
    Let $\Pi(\omega, \mathcal{A})$ be a transition probability distribution from the measure space $(\Omega, \mathbf{S})$ to $(\Omega', \mathbf{\Sigma})$. Then, by extending the probability distribution \[\Pi(\omega, \cdot)\quad \text{on}\quad \mathbf{\Sigma} \quad \text{for each }\quad \omega \in \Omega\]  to a probability distribution \[\Pi^*(\omega, \cdot) \quad \text{on}\quad \mathbf{\Sigma^*},\] we obtain a transition probability distribution from $(\Omega, \mathbf{S^*})$ to $(\Omega', \mathbf{\Sigma^*})$. 
    
    Consequently, for any probability distribution $P$, we have:
    \[
    (P\Pi)^*[\cdot] = (P^*\Pi^*)[\cdot].
    \]
\end{theorem}
\begin{remark}
We make a link between the statement outlined above and the universal property of Theorem \ref{T:Univ}.  Theorem \ref{T:Univ} says that there a {\bf bijective correspondence} is established by the operators $\Pi_{ij}$, which are given by 
\[\Pi_{ij}=\Pi^{(i)}\circ Q^{(j)},\]
where $\Pi^{(i)}$ is given by the mapping $\varphi_i(\omega)$.
\end{remark}

\begin{ex}\label{Ex:ProofThm7.2}
The proof is left as an exercise. However, since it is a very ambitious exercise, we suggest that the reader looks up the definition of an inner and outer measure. 
\end{ex}


\section{Markov Transformations and Associated Subcategories}

The following theorem shows the existence of a natural subcategory, which we denote by $CAP^*$, within the category $CAP$ of measurable spaces and Markov morphisms.

\begin{theorem}[Subcategory of Markov Morphisms]
Let $(\Omega, \mathbf{S})$ be a measurable space whose $\sigma$-algebra $\mathbf{S}$ is closed, and let $CAP$ denote the category of measurable spaces with Markov morphisms. Then the subclass of such spaces, together with all their Markov morphisms, forms a subcategory $CAP^*$ of $CAP$. Moreover, the natural extension of each probability measure from $\mathbf{S}$ to its closure $\mathbf{S}^*$ defines a functor mapping $CAP$ into $CAP^*$.
\end{theorem}

\begin{proof}
By definition, any collection of objects along with all the corresponding morphisms (when these morphisms are suitably restricted) forms a subcategory. The key point is that the operations of extending a probability measure and of applying a Markov morphism commute. Indeed, suppose that for a Markov morphism $\Pi_{01}$ we have a corresponding probability measure $P_\omega\{.\} = \Pi_{01}(\omega; \cdot)$, and let $\Pi_{12}$ be another Markov morphism. Then, for every $\omega \in \Omega$, one may show that
\[
\bigl( \Pi_{01} \circ \Pi_{12} \bigr)^* = \Pi_{01}^* \circ \Pi_{12}^*,
\]
in the sense that the extension of the composite is equal to the composite of the extensions. In particular, 
\[
(P_\omega \Pi_{12})^*\{.\} = (P_\omega^* \Pi_{12}^*)\{.\}.
\]
This commutativity property ensures that the extension procedure defines a functor from $CAP$ to $CAP^*$.
\end{proof}

\begin{remark}
The passage to the closure of the algebra in the category of measurable spaces is functorial.
\end{remark}

We now show that Markov morphisms form a category in a natural way.

\begin{lemma}
Let $\Pi(\omega';\omega'')$ be a Markov morphism from $(\Omega', \mathbf{S}')$ to $(\Omega'', \mathbf{S}'')$, and let $f(\omega'')$ be a measurable real function on $(\Omega'', \mathbf{S}'')$ with 
\[
f(\omega'') \in [0,1] \quad \text{for all } \omega'' \in \Omega''.
\]
Then, the function 
\[
g(\omega') = (\Pi^T f)(\omega') = \int_{\Omega''} \Pi(\omega'; d\omega'') \, f(\omega'')
\]
is a measurable function on $(\Omega', \mathbf{S}')$ and satisfies 
\[
g(\omega') \in [0,1] \quad \text{for all } \omega' \in \Omega'.
\]
\end{lemma}

\begin{proof}
The proof follows from the monotonicity and linearity of the integral, combined with the fact that $\Pi(\omega'; \cdot)$ is a probability measure for each fixed $\omega'$.
\end{proof}



\section{Dominated Families in the Category of Markov Morphisms}

We now state a fundamental theorem concerning dominated families (denoted \textbf{DomFam}) in the context of Markov morphisms.

\begin{theorem}
Relative to the category of Markov morphisms, the property of being dominated is absolutely invariant; that is, if a measure $\mu$ on $(\Omega, \mathbf{S})$ dominates another measure $\nu$ (denoted $\mu \gg \nu$), then for any Markov morphism $\Pi$ from $(\Omega,\mathbf{S})$ to $(\Omega',\mathbf{\Sigma})$ the induced measure satisfies 
\[
\mu\Pi \gg \nu\Pi.
\]
\end{theorem}

\begin{proof}
Let $\mu$ be a nonnegative measure on $(\Omega, \mathbf{S})$ such that $\mu \gg \nu$, meaning that for any set $A \in \mathbf{S}$, if $\mu(A)=0$, then $\nu(A)=0$. Let $\Pi(\omega; d\omega')$ be a transition distribution determining a Markov morphism from $(\Omega,\mathbf{S})$ to $(\Omega',\mathbf{\Sigma})$. 

To show that $\mu\Pi \gg \nu\Pi$, it suffices to prove that for any $\mathcal{A} \in \mathbf{\Sigma}$ with $(\mu \Pi)(\mathcal{A}) = 0$, we have also $(\nu \Pi)(\mathcal{A}) = 0$. For a fixed $\mathcal{A}$, define the set
\[
B = \{ \omega \in \Omega \mid \Pi(\omega; \mathcal{A}) > 0 \}.
\]
It is known that if $f: \Omega \to [0,1]$ is a measurable function with $\int_\Omega f(\omega)\, \mu(d\omega)=0$, then $\mu\{ \omega \mid f(\omega) > 0 \} = 0$. Hence, taking $f(\omega)=\Pi(\omega; \mathcal{A})$, we deduce that
\[
\mu(B)=0.
\]
Since $\mu \gg \nu$, it follows that $\nu(B)=0$. Furthermore, for $\omega \in \Omega \setminus B$, we have $\Pi(\omega; \mathcal{A}) = 0$. Thus,
\[
(\nu \Pi)(\mathcal{A}) = \int_{\Omega} \nu(d\omega) \, \Pi(\omega; \mathcal{A})
= \int_B \Pi(\omega; \mathcal{A}) \, \nu(d\omega) + \int_{\Omega \setminus B} \Pi(\omega; \mathcal{A}) \, \nu(d\omega) = 0.
\]
This completes the proof of the invariance of the domination relation.
\end{proof}

\begin{theorem}
Let $CAP$ denote the category of measurable spaces with Markov morphisms. Then the subclass of dominated measurable spaces $(\Omega, \mathbf{S})$ whose $\sigma$-algebras are closed, together with all their Markov morphisms, forms a complete subcategory $\mathsf{FAMD}$ of $CAP$. Moreover, the collection of all families of mutually absolutely continuous probability distributions forms a complete subcategory $\mathsf{FAMH}$ of $\mathsf{FAMD}$.
\end{theorem}
\begin{proof}
We organize the proof in two parts.

\, 

\textbf{(i) Dominated Families Form a Subcategory.}  
Suppose $Q$ is a probability measure on $(\Omega,\mathbf{S})$ that dominates a family $\{P_\theta\}_{\theta \in \Theta}$, i.e., for each $\theta$, we have $Q \gg P_\theta$. By the previous theorem, for any Markov morphism $\Pi$ from $(\Omega,\mathbf{S})$ to $(\Omega',\mathbf{\Sigma})$, the extended measure $Q\Pi$ dominates the family $\{P_\theta\Pi\}_{\theta \in \Theta}$. Thus, the property of domination is preserved under Markov morphisms, and the dominated families with all their Markov morphisms form a subcategory of $FAM$, which we denote by $\mathsf{FAMD}$.

\, 

\textbf{(ii) Mutual Absolute Continuity.}  
In a family of mutually absolutely continuous probability measures, each measure dominates every other. Since this domination relation is invariant under any Markov morphism, the subcategory $\mathsf{FAMH}$ of such families is complete.
Hence, we conclude that $\mathsf{FAMD}$ is a complete subcategory of $FAM$, and within it, $\mathsf{FAMH}$ forms a complete subcategory.
\end{proof}


\begin{remark}
The process of passing to the closure of the $\sigma$-algebra in the category of measurable spaces is functorial.
\end{remark}

We end this subsection with the following lemma. 
\begin{lemma}\label{L:7.2}
If $P(\cdot)\in CAP(\Omega,{\bf S})$ is a constructive probability distribution (i.e. such that the distribution can be constructed from the family $Q_\theta$
 then all distributions that it dominates are also constructible.  
\end{lemma}

\subsubsection{Conclusion}

In summary, our construction shows that the property of being dominated is invariant under Markov morphisms, and that dominated families and the families of mutually absolutely continuous distributions naturally form complete subcategories of the category of measurable spaces with Markov morphisms.


\subsubsection{Dominating Measures and Constructive Probabilities}

\begin{lemma}
    The measure 
    \[
    P_0\{.\} = \left(\frac{1}{n} \sum_{k=1}^n P_k\right) \{.\}
    \]
    is a dominating law.
\end{lemma}
\begin{ex}\label{Ex:LemmaDom}
Show that for each $P_k$ we have the bound: 
    \[
    P_k\{.\} \leq 2^k P_0\{.\}.
    \]

\end{ex}

\section{Homogeneous Klein Geometries and Geodesic Structures}

\begin{definition}
    A Klein geometry generated by an elementary category of topological spaces is said to be \textit{almost homogeneous} or \textit{quasi-homogeneous} if any two points $a \in A$ and $b \in B$ of any two objects $A$ and $B$ are totally arbitrarily approximable.
\end{definition}

\begin{lemma}
    Consider an almost-homogeneous Klein geometry. Every invariant function in the category of topological spaces over a scalar field is identically constant.
\end{lemma}

\begin{proof}
    The proof is evident.
\end{proof}

\subsection{Geodesic Structures}

\begin{definition}
    Let $\gamma(t)$ be a curve. We say that $\gamma(t)$ is \textit{geodesic} if the family of tangent vectors 
    \[
    X(\tau) = \frac{d}{dt} \Big|_{\tau}
    \]
    is parallel along the curve, that is, for all $t$:
    \[
    \nabla_{\frac{d}{dt}} \left(\frac{d}{dt}\right)_{p(t)} = 0.
    \]
\end{definition}

\begin{remark}
    Geodesics are the analogs of straight lines in Euclidean spaces.
\end{remark}

In local coordinates, the geodesic equation takes the form:
\[
\frac{d^2 x^{(k)}}{dt^2} + \sum_{i,j} \Gamma_{ij}^k \frac{dx^{(i)}}{dt} \frac{dx^{(j)}}{dt} =0.
\]

Since parametrization is essential, any other canonical parametrization $s = s(t)$ of $\gamma(t)$ must be an affine function of $t$, that is, $s(t) = at + b$.

\subsection{Torsion, Curvature, and Flat Connections}

The linear connection operator $\nabla$ is associated with two multi-linear operators $T(X,Y)$ and $R(X,Y)$:
\begin{itemize}
    \item The torsion operator:
    \[
    T(X,Y) = \nabla_{X}(Y) - \nabla_{Y}(X) - [X,Y].
    \]
    \item The curvature operator:
    \[
    R(X,Y)Z = \nabla_{X}(\nabla_{Y}Z) - \nabla_Y(\nabla_{X}Z) - \nabla_{[X,Y]}Z.
    \]
\end{itemize}
where $[X,Y] = XY - YX$ is the commutator.

\begin{remark}
    If both the torsion and curvature vanish identically, then:
    \begin{itemize}
        \item The connection is flat.
        \item A manifold with such a flat connection is locally affine.
        \item Any parallel translation on such a manifold depends only on its initial and final points.
        \item Parallel translation depends only on the homotopy class of paths.
    \end{itemize}
\end{remark}

\subsection{Totally Geodesic Submanifolds}

\begin{definition}
    A submanifold $N$ of a manifold $M$ equipped with a linear connection $\nabla$ is called \textit{totally geodesic} if, for any two points of $M$, it contains the whole geodesic passing through these points.
\end{definition}

\begin{lemma}
    A submanifold $N \subset M$ is totally geodesic if, for any tangent vector fields $X, Y$ on $N$, the vector field $Z = \nabla_X Y$ is also tangent to $N$.
\end{lemma}

\begin{proof}
    The proof is evident.
\end{proof}

\begin{remark}
    A submanifold $N \subset M$ is totally geodesic if every locally shortest curve in $N$ is also locally shortest in $M$.
\end{remark}

\section{Equivariant Connections in the Category of Topological Spaces}

Let us return to the framework of Klein geometry and consider a category of topological spaces, where morphisms reflect the fundamental structures of geometric transformations.

\begin{definition}
    Let $\mathcal{C}$ be a category of topological spaces, and let $\nabla$ be a linear connection on each object of $\mathcal{C}$. The connection $\nabla$ is said to be \textit{absolutely equivariant} if, for any morphism $\varphi: X \to Y$ in $\mathcal{C}$, the image of every geodesic in $X$ under $\varphi$ is a geodesic in $Y$.
\end{definition}

This definition encapsulates the idea that a natural connection should be preserved under morphisms of the category, ensuring that geodesic structures remain invariant under transformations.

\subsection{Geodesic Midpoints and Absolute Covariance}

To better understand the implications of an absolutely equivariant connection, let us recall the fundamental properties of geodesics in canonical coordinates.

\begin{itemize}
    \item Given two points on a geodesic, one can uniquely define the \textit{midpoint} of the geodesic segment connecting them.
    \item This midpoint is determined by a symmetric function of the two endpoints.
    \item If the connection $\nabla$ is absolutely equivariant in the category $\mathcal{C}$, then the midpoint function is necessarily \textit{absolutely covariant} with respect to pairs of points.
\end{itemize}

Thus, the notion of geodesic symmetry emerges naturally in the categorical framework: an equivariant connection ensures that midpoint operations behave consistently across all objects and morphisms of the category.

\chapter{Categorical and Geometric structures in Statistical Manifold Theory}

In the previous chapter, we introduced key ideas in probability and statistics and briefly alluded to a more structural perspective using category theory and differential geometry. However, these tools have not yet been systematically employed. In this chapter, we take a decisive step in that direction by developing a structured approach relating to category theory and differential geometry in the study of statistical manifolds.

\, 

The categorical viewpoint allows us to formalize probabilistic transformations, treating probability distributions as objects and Markov kernels as morphisms. At the same time, differential geometry provides powerful tools for analyzing the geometric structure of statistical manifolds, particularly through notions such as connections, curvature, and geodesics. By combining these perspectives, we gain deeper insight into the intrinsic structure of statistical models and their transformations.

\, 

This chapter is devoted to making these ideas explicit. We will establish the categorical framework for statistical manifolds and then show how geometric methods naturally arise within this setting. The interplay between these two formalisms will not only clarify fundamental structures but also pave the way for further developments in information geometry, learning theory, and beyond.

\, 

In order to rigorously define the notion of statistical manifolds, we introduce five fundamental collections of probability distributions, which serve as building blocks for our categorical formulation:

\begin{itemize}
    \item The collection $\mathsf{Cap}$,
    \item The collection $\mathsf{Capd}$,
    \item The collection $\mathsf{\mathsf{Caph}}$,
    \item The collection $\mathsf{\mathsf{Conh}}$,
    \item The collection $\mathsf{Var}$.
\end{itemize}

These collections are formally defined as follows:

\begin{definition}
    Let $(\Omega, \mathbf{S})$ be a measurable space. We define:
    \begin{itemize}
        \item $\mathsf{Cap}(\Omega, \mathbf{S})$ as the collection of all probability distributions on $(\Omega, \mathbf{S})$.
        
        \item $\mathsf{Capd}(\Omega, \mathbf{S}, \mathbf{Z})$ as the collection of all probability distributions on $(\Omega, \mathbf{S})$ that vanish on the ideal $\mathbf{Z}$.
    
        \item $\mathsf{\mathsf{Caph}}(\Omega, \mathbf{S}, \mathbf{Z})$ as the collection of all probability distributions on $(\Omega, \mathbf{S})$ that vanish on $\mathbf{Z}$ and only on $\mathbf{Z}$, when considered over an open simplex.
        
        \item $\mathsf{\mathsf{Conh}}(\Omega, \mathbf{S}, \mathbf{Z})$ as the collection of all nonnegative, mutually absolutely continuous probability measures on $(\Omega, \mathbf{S})$ that vanish on $\mathbf{Z}$ and only on $\mathbf{Z}$.
      
        \item $\mathsf{Var}(\Omega, \mathbf{S})$ as the collection of all probability distributions on $(\Omega, \mathbf{S})$.
    \end{itemize}
\end{definition}

The following result formalizes the correspondence between these spaces:

\begin{proposition}
    The collections $\mathsf{Var}(\Omega, \mathbf{S})$ and $\mathsf{Cap}(\Omega, \mathbf{S})$ are in one-to-one correspondence.
\end{proposition}

\begin{proof}
    The claim follows from the structural equivalence of probability distributions in both settings.
\end{proof}

\begin{proposition}
    The collections $\mathsf{Cap}, \mathsf{Capd}, \mathsf{\mathsf{Caph}}, \mathsf{\mathsf{Conh}}, \mathsf{Var}$ each form a manifold equipped with a corresponding atlas.
\end{proposition}

\begin{proof}
    See Chentsov, pp. 73-74.
\end{proof}

\subsection{The Category of Statistical Decision Rules}

A statistical problem is naturally associated with the measurable space $(\Omega, \mathbf{S})$ of sample outcomes, together with:
\begin{itemize}
    \item A family $\{P_{\theta}\}$ of admissible probability distributions on $(\Omega, \mathbf{S})$,
    \item An additional structure encoding prior information about the phenomenon,
    \item A measurable space $(\Omega', \mathbf{S'})$ of inferences (actions),
    \item A statistical test quantifying the quality of a given decision rule.
\end{itemize}

\begin{definition}
    A \emph{statistical decision rule} is a transition probability distribution $\Pi(\omega, d\epsilon)$ describing a Markov random transition from the sample space $(\Omega, \mathbf{S})$ to the inference space $(\Omega', \mathbf{S'})$.
\end{definition}

\begin{remark}
    Any transition probability distribution $\Pi(\omega, d\epsilon)$ may be interpreted as a decision rule within any statistical model whose sample space is $(\Omega, \mathbf{S})$ and whose inference space is $(\Omega', \mathbf{S'})$.
\end{remark}

Furthermore, given a Markov random transition from $\Omega'$ to $\Omega''$ described by a transition probability $\Pi(\omega', d\omega'')$, we obtain a \emph{Markov morphism} between the measurable spaces $(\Omega', \mathbf{S'})$ and $(\Omega'', \mathbf{S''})$.

\begin{theorem}
    The class of objects:
    \begin{itemize}
        \item The collection $\mathsf{Cap}(\Omega, \mathbf{S})$,
        \item The collection $\mathsf{Capd}(\Omega, \mathbf{S}, \mathbf{Z})$,
        \item The collection $\mathsf{\mathsf{Caph}}(\Omega, \mathbf{S}, \mathbf{Z})$,
        \item The collection $\mathsf{\mathsf{Conh}}(\Omega, \mathbf{S}, \mathbf{Z})$,
    \end{itemize}
    equipped with the system of Markov homeomorphisms defined by
    \[
    (P\Pi) \{.\} = \int \Pi(\omega;.) P\{d\omega\},
    \]
    forms the categories of statistical decisions $\mathsf{CAP}, \mathsf{CAPD}, \mathsf{\mathsf{Caph}}, \mathsf{\mathsf{Conh}}$, respectively, which are isomorphic to categories of statistical decision rules.
\end{theorem}

\begin{proof}
    The proof follows from the categorical structure imposed by the Markov homeomorphisms and the natural compatibility of the decision rule formulation.
\end{proof}

\subsection{Types of Manifolds in Theory of Statistical Manifolds}

In the theory of statistical manifolds, four types of manifolds are of particular interest:
\begin{enumerate}
    \item manifold $\mathsf{\mathsf{Conh}}$,
    \item manifold $\mathsf{\mathsf{Caph}}$,
    \item manifold $\mathsf{Capd}$
    \item manifold $\mathsf{Cap}$.
\end{enumerate}

     \subsubsection{Characteristics of the Manifold $\mathsf{\mathsf{Conh}}$} 
      
Consider the category $\mathsf{Caph}F$ of collections $\mathsf{\mathsf{Caph}}(\Omega,\mathbf{S},\mathbf{Z})$ with finite quotient algebras $\mathbf{S}/ \mathbf{Z}$. These collections are finite-dimensional manifolds. 
Any $\mathbf{Z}$-dominated measure on $(\Omega, \mathbf{S})$, where $\mathbf{S}/ \mathbf{Z}$ is finite, is completely determined by the vector
\[\boldsymbol{\mu}=(\mu\{A_1\},\mu\{A_2\},\cdots,\mu\{A_m\})=(\mu_1,\mu_2,\cdots,\mu_m).\]

\begin{definition}
Let $\mathsf{\mathsf{Conh}}(\Omega,\mathbf{S},\mathbf{Z})$ be the collection of all nonnegative, mutually absolutely continuous measures on $(\Omega,\mathbf{S})$ that vanish on $\mathbf{Z}$-sets and only there. Then if $S_m \simeq \mathbf{S}/\mathbf{Z}$, the correspondence
\[
\mu \{.\}  \longleftrightarrow \boldsymbol{\mu} = (\mu\{A_1\},\mu\{A_2\},\cdots,\mu\{A_m\}),
\]
and
\[
\mu_j > 0,\quad j\in\{1,\cdots,m\}.
\]
defines a chart of the entire cone $\mathsf{\mathsf{Conh}}$. This chart is called a \emph{natural chart}.
The atlas of charts ${\mathsf{Conh}}$ also includes other charts obtained from the natural chart through analytic or infinitely differential coordinate transformations. However, the natural chart holds a privileged position.
\end{definition}
\begin{proposition}
   The collection $\mathsf{Caph}(\Omega,\mathbf{S},\mathbf{Z})$ is selected from $\mathsf{Conh}(\Omega,\mathbf{S},\mathbf{Z})$
by the condition
\[
<\mu,I>=1,
\]
making it a hypersurface. Specifically, $\mathsf{Caph}(\Omega, \mathbf{S}, \mathbf{Z})$ is the intersection of the cone  $\mathsf{Conh}(\Omega,\mathbf{S},\mathbf{Z})$ where
$\mu_j > 0$ for $j=1,\cdots,m$, where $\mu_j= \mu\{A_j\}$, with the hyperplane defined by $<\mu,I>=1$.

The vectors $\mathbf{p}=(P\{A_1\},\cdots,P\{A_m\})$ corresponding to ${\bf P\{\cdot\}}$, exhaust the interior of the unit simplex 
exhaust the interior of the unit simplex
\[
\sum_{j=1}^m p_j\mathbf{e}_j, 
\]
where $p_j\geq 0$, $\sum_{j=1}^m p_j=1$, and  $\mathbf{e}_1=(1,0,\cdots,0),\mathbf{e}_2=(0,1,0,\cdots,0),\cdots, \mathbf{e}_m = (0,0,0,\cdots,0,1)$ are the vertices of the simplex.
\end{proposition}
\begin{definition}
The probabilities $P\{\mathbf{A}_j\}=p_j\{P\}$ simultaneously serve as:

\begin{itemize}
    \item {\bf Natural coordinates}: The bijection

\[
\mathbf{P} \longleftrightarrow \mathbf{p}=(p_1,\cdots,p_m),
\] identifies points in the ambient space.
     \item {\bf Barycentric coordinates}: Affine coordinates relative to the simplex vertices ${\bf e}_1,\cdots {\bf e}_m$, where 
     $\mathbf{p}=\sum_{j=1}^mp_j{\bf e}_j$.
\end{itemize}
\end{definition}
\subsection{Non-Independence of Natural Coordinates}
\begin{remark}
Let  $\mathsf{Caph}$  be a manifold and consider a surface within it. The natural coordinates $\mathbf{p}=(p_1,\cdots,p_m)$ are not local coordinates of manifold $\mathsf{Caph}$.
\end{remark}
\begin{proof}
 The coordinates  $p_1,\cdots,p_m$ are linearly dependent on the surface since $\sum_{j=1}^mp_j$. This reduces the intrinsic dimension to 
$m-1$, rendering one coordinate redundant.
 \end{proof}
 \subsection{Constructing Local Coordinates}
To define a local coordinate system for $\mathsf{Caph}$, we discard one component of {\bf p}.

For example, omitting $p_m$, we obtain the chart :
\[
P\{A_j\} > 0, j=1,\cdots,m, \quad  \sum_{j=1}^m P\{A_j\} =1,\quad \Longrightarrow\, P\{A_m\}=1-\sum_{j=1}^{m-1} P\{A_j\}.
\]
However, such charts lack invariance under permutations of the atoms $A_1,\cdots, A_m$. To resolve this:


\subsubsection*{Homogeneous Coordinates}
Introduce equivalence classes under scaling:
\[
\mathbf{P} \longleftrightarrow c \cdot \mathbf{p},\quad c>0,
\]
where {\bf p} is defined up to a positive multiplicative constant. This ensures permutation invariance.


\subsubsection*{Rectilinear Coordinates}

Another type of coordinates, which can be used to solve the problem of chart invariance, is to introduce the rectilinear coordinates.
Let  $f_j:\Omega\to \bbR$, $(j=0,\cdots,m-1)$ be ${\bf S}$-measurable functions with $f_0(\omega)=I(\omega)$. Assume the matrix $\bigg(f_j(\omega_i)\bigg)$ 
has full rank $m$. Define coordinates:

\[
t_j(P)= \mathbf{M}_P f_j{\omega}= \mathbb{E}_P[f_j] = \langle P,f_j \rangle, \quad (j= 1,\cdots,m-1),
\]

where $\mathbf{M}_Pf(\omega)$ is an expectation of a random variable $f(\omega)$, defined as a following integral:
\[
\mathbf{M}_P f(\omega)= \int_{\Omega} f(\omega)P\{d\omega\}= \langle P, f \rangle,
\]
where the function $f$ must be integrable with respect to the measure $P$ (or quasi-integrable).

\subsubsection*{Natural Coordinate System}
\begin{definition}
Let $\mathsf{Caph}$ be an $(m-1)$-dimensional manifold.
A local coordinate system $\mathbf{t}= (t_1,\cdots,t_{m-1})$ for $\mathsf{Caph}$ is called \emph{natural} if it is induced by the expectations:
    \[
    \mathbf{t} = \int_{\Omega} f(\omega)P\{d\omega\},
    \]
    and such that 
    \[
    t_j(P) =  \int_{\Omega} f_j(\omega)P\{d\omega\}, \quad j=1,\cdots, m-1,
    \]
  where $f_j(\omega)$ are linearly independent functionals and each $f_j$ is $P$-quasi-integrable.
\end{definition}


\subsection{Canonical Affine Coordinates in the Theory of Statistical Manifolds}

The introduction of canonical affine coordinates in statistical manifold theory arises naturally from fundamental geometric and probabilistic considerations. Let us first define the indicator function 
\[
\epsilon_j(\omega) = 
\begin{cases} 
1, & \text{if } \omega \in A_j, \\
0, & \text{otherwise}.
\end{cases}
\]
These functions are not necessarily linearly independent; however, any $m-1$ of them, together with the identity function $I(\omega)$, form a linearly independent set.

\begin{definition}
    A coordinate system $\mathbf{s}=(s^1,\dots,s^{m-1})$ on the open simplex $\mathsf{Caph}(\Omega, \mathbf{S}, \mathbf{Z})$ is called \emph{canonical affine} if
    \[
    P_{\mathbf{s}} \{ .\} = T(\mathbf{s}) P_0\{ .\},
    \]
    where $P_0$ is the origin and $\mathbf{s}$ is a covariant linear coordinate system of the group of translations.
\end{definition}

More explicitly, if $P_0\{.\}$ represents the probability distribution at the origin and the functions $g_0(\omega) = 1$, $g_1(\omega),\dots,g_{m-1}(\omega)$ define the coordinate axes, then:
\[
\frac{dP_{\mathbf{s}}}{d\mu}(\omega) 
= p(\omega;\mathbf{s}) = p(\omega, 0)\exp \left[\sum_{\alpha=1}^{m-1} s^{\alpha} g_{\alpha} (\omega) - \Psi(\mathbf{s})  \right].
\]
Here, $\mu\{.\}$ is an arbitrary $\mathbf{Z}$-positive measure vanishing on $\mathbf{Z}$-sets only, and the normalization factor $\exp[\Psi(\mathbf{s})]$ is given by:
\[
\exp[ \Psi(\mathbf{s})] = \int_{\Omega}\exp \left[ \sum_{\alpha = 1}^{m-1}  s^{\alpha} g_{\alpha} (\omega) \right] P_0\{ d \omega\}.
\]

\subsubsection{Tangent Vectors and Tangent Spaces}
To analyze the structure of statistical manifolds, we introduce tangent vectors and vector fields. Let $\{P_t\}$ be a family of probability distributions parameterized by a smooth curve $\mathbf{x}(t)$. The functional $(Y)_{P}$ of smooth functions $f(\mathbf{x})$ is defined as:
\[
\frac{d}{dt}f(\mathbf{x}(t)) \bigg|_{t =\theta},
\]
which represents a tangent vector at the point $P=P_{\theta}$.

\begin{lemma}
    \begin{itemize}
        \item The curve $P_t$ admits the following decomposition:
        \[
        P_t{.} = P_{\theta}{.} + \tau P'_{\theta}{.}+ o(\tau),
        \]
        where $\tau = t - \theta$, $P'_{\theta}$ is a charge of bounded variation with total measure zero, i.e., $P'_{\theta} {\Omega} =0$, and the norm of the remainder term vanishes faster than $\tau$.
        \item There exists a one-to-one, linear, continuous, and analytic correspondence between tangent vectors at a given point and charges with zero total measure.
    \end{itemize}
\end{lemma}

\begin{proof}
    Consider the representation $P_t\{.\} \leftrightarrow \mathbf{p}(t) = (p_1(t),\dots,p_{m-1}(t),p_m(t))$, where $p_m(t)= 1 - p_1(t) - \dots - p_{m-1}(t)$. Near $t = \theta$, we expand:
    \[
    p_j(t) = p_j(\theta) + \tau \dot{p}_j(\theta) + o(\tau).
    \]
    Adding these expansions and subtracting from unity, we obtain the corresponding expression for $p_m$. Defining the auxiliary charges:
    \[
    P'_{\theta} \{H\} = \sum_{A_j \preceq H} \dot{p}_j (\theta),
    \]
    we obtain:
    \[
    P_t\{H\}= P_{\theta}\{H\}+ \tau P'_{\theta}\{H\} + R_{t,\theta}\{H\}.
    \]
    Using the chain rule,
    \[
    \frac{d}{dt}f(\mathbf{p}(t))\bigg|_{t= \theta} = \sum_{j=1}^{m-1} \frac{\partial f}{\partial p_j} \bigg|_{\mathbf{p}(\theta)} \dot{p}_j(\theta).
    \]
    The result follows from continuity and the existence of fiberings of the simplex into smooth trajectories.
\end{proof}

\subsection{Vector Fields and Ceva Lines}

In classical differential geometry, vector fields are typically expanded in a coordinate basis $X_i = \frac{\partial}{\partial x^i}$. However, in statistical manifold theory, this approach is often inconvenient due to the transformation properties of probability distributions. Instead, we construct an alternative basis.

We define $m$ vector fields $Y_j$ such that:
\[
(Y_j)_{\mathbf{p}} \leftrightarrow  \mathbf{e}_j  - \mathbf{p}.
\]
Additionally, we introduce $m$ vector fields $X_j$ satisfying:
\[
X_j= p_j Y_j, \quad (X_j)_{\mathbf{p}} \leftrightarrow  p_j \mathbf{e}_j - p_j \mathbf{p}.
\]

To understand the structure of these vector fields, consider the simplex and an arbitrary segment within it. Among all possible segments, we identify those connecting a vertex $\mathbf{e}_i$ to a point on the opposite face. These segments, known as \emph{Ceva lines}, define a special class of trajectories in the simplex.


\subsection{Ceva Lines and Tangent Vector Fields}

Let $\mathbf{p}(t; q_2,\dots,q_m)$ be a Ceva line. More precisely, we define it as
\[
\mathbf{p}(t;q_2,\dots,q_m)= p_1(t)\mathbf{e}_1 + [1 - p_1(t)] \sum_{i=2}^m q_i \mathbf{e}_i.
\]

\begin{lemma}
The vector fields $X_i = \frac{\partial}{\partial x^i}$, where $(x^i,\dots,x^n)$ is the field of differentiation along the Ceva line with respect to the parameter $t$, satisfy the differential equation
\[
\dot{p}_i(t) = p_i(t) - [p_i(t)]^2 = p_i(t) [1 - p_i(t)].
\]
In particular, for $i=1$, we obtain
\[
p_1(t)= \frac{\exp\{t\}}{\exp\{t\}+1} = \frac{1}{1+\exp\{-t\}}.
\] 
\end{lemma}

\begin{remark}
    The vector field $Y_1$ corresponds to the choice of parameter $p_1(t) =1 - \exp\{ -t \}$. The remaining fields $X_j$
    and $Y_j$ are obtained analogously along the Ceva lines:
    \[
    \mathbf{p}^{(j)}(t) = p_j(t) \mathbf{e}_j + [ 1 - p_j(t)] \sum_{i \neq j} q_i \mathbf{e}_i,
    \]
    where $p_j(t)$ for $X_j$ satisfies the differential equation
    \[
    \dot{p}_j (t) = p_j (t) - [ p_j(t)]^2.
    \]
    Similarly, for the field $Y_j$, we have $p_j(t)= 1 - \exp\{-t\}p$.
\end{remark}

\begin{proof}
We compute $\dot{p_i}(t)$, which defines the tangent vector of differentiation with respect to $t$. For $i=1$, we have
\[
\dot{p}_1 (t)= \frac{\exp\{ - t\}}{(1+ \exp \{-t\})^2}= \frac{1}{1 + \exp \{ -t\}}- \frac{1}{(1 + \exp \{ -t\})^2} = p_1 (t) - [p_1(t)]^2.
\]
Additionally, we obtain
\[
\dot{p}_1(t)= - \dot{p}_1(t)q_i = -p_1(t) [1 - p_1(t)]q_i = - p_1 (t)  p_i (t).
\]
Since differentiation with respect to $t$ satisfies $\frac{d}{dt} \longleftrightarrow p_1(t) \mathbf{e}_1 - p_1(t)\mathbf{p}(t)$, we conclude the proof of the lemma. 
\end{proof}

\begin{lemma}
    The system of tangent vectors \[(X_j)_{\mathbf{p}},\quad j= 1,\dots,m\] is complete at each point of the manifold $\mathsf{Caph}$. That is, any tangent vector $(Z)_{\mathbf{p}}$ may be expanded in terms of the $(X_j)_{\mathbf{p}}$, and any $m-1$ vectors $(X_j)_{\mathbf{p}}$ form a basis. 
    
    \, 
    
    Every vector field $Z$ admits a unique smooth expansion:
    \[
    Z= \sum_{j=1}^m \zeta^j(\mathbf{p})X_j,
    \]
    subject to the additional condition
    \[
    \sum_{i=1}^m \zeta^i p_i = 0.
    \]
    All other expansions of the form $Z= \sum_{j=1}^m \eta^j X_j$ satisfy
    \[
    \eta^i(\mathbf{p})= \zeta^i(\mathbf{p})+ \phi(\mathbf{p}),
    \]
    where $\phi(\mathbf{p})$ is a scalar field.
\end{lemma}

\begin{remark}
    If we replace the condition $\sum_{i=1}^m \zeta^i p_i = 0$ by 
    \[
    \sum_{j=1}^m \zeta^j = 0,
    \]
    then the above lemma remains valid.
\end{remark}

\begin{proof}
Let $(Z)_{\mathbf{p}} \leftrightarrow \mu\{\cdot\}$, where $\mu\{\Omega\}=0$. Then
\[
\mathbf{\mu} = \sum_{j=1}^m \mu_j \mathbf{e}_j = \sum_{j=1}^m \mu_j \mathbf{e}_j - \sum_{j=1}^m \mu_j \mathbf{p} 
= \sum_{j=1}^m \mu_j (\mathbf{e}_j - \mathbf{p}) 
= \sum_{j=1}^m \frac{\mu_j}{p_j} p_j(\mathbf{e_j} - \mathbf{p})
\longleftrightarrow  \sum_{j=1}^m \frac{\mu_j}{p_j}(X_j)_{\mathbf{p}}.
\]

Since $(X_j)_{\mathbf{p}} \longleftrightarrow  p_j \mathbf{e_j} - p_j \mathbf{p}$, we obtain:
\[
\sum _{j=1}^m (X_j)_{\mathbf{p}} \longleftrightarrow  \sum_{j=1}^m  (p_j\mathbf{e_j} - p_j \mathbf{p}).
\]
Thus,
\[
\sum_{j=1}^m (X_j)_{\mathbf{p}}  \longleftrightarrow \sum_{j=1}^m p_j {\mathbf{e}_j} - \mathbf{p}\sum_j^m p_j 
  = \mathbf{p} -\mathbf{p} = \mathbf{0}.
\]
Since $\sum_{j=1}^m X_j = {\mathbf{0}}$, it follows that $X_k = - \sum\limits_{j \neq k} X_j$. Inserting this expression into $(Z)_{\mathbf{p}}$, we obtain an expansion in a basis of $m-1$ vectors.

Let $\sum\limits_j \zeta^j X_j = Z = \sum\limits_j \eta^j X_j$ be two expansions of $Z$. Then
\[
\sum_j[\zeta^j - \eta^j]X_j =\mathbf{0},
\]
or equivalently,
\[
(\zeta^1 - \eta^1)X_1 = - \sum_{i=2}^m (\zeta ^i - \eta^i)X_i.
\]
Since $X_2,\dots,X_m$ form a basis, we deduce:
\[
\zeta^1 - \eta^1 = \zeta^2 - \eta^2 =\dots= \zeta^m - \eta ^m = \phi(\mathbf{p}).
\]
From
\[
\sum_{j=1}^m \zeta^j (\mathbf{p})p_j = \sum_{j=1}^m \zeta^j(\mathbf{p})p_j - \phi(\mathbf{p}),
\]
we see that $\sum_{j=1}^m \zeta^j(\mathbf{p}) p_j=0$ if and only if $ \sum_{j=1}^m\zeta^j (\mathbf{p}) p_j = \phi(\mathbf{p})$.
Thus, the lemma is proved.
\end{proof}

\subsection{Affine Connections and Ceva Lines on Statistical Manifolds}

Let us now examine the interplay between the vector fields $X_j$ and canonical affine coordinates.


\begin{lemma}
In a canonical affine coordinate system aligned with the directions $\epsilon_j(\omega)$ for $j\neq k$, the coordinate curves parametrized by 
\[x^i= t,\quad  x^j = \text{const}, (j \neq i, k),\]

correspond to the Ceva lines of the $i$-th family. The tangent vector fields associated with these curves are given by \[X_i = \frac{\partial}{\partial x^i}.\]
\end{lemma}

\begin{proof}
    Consider the probability measure $P_0$ at the center of the simplex:
    \[
    P_0 (\cdot) \leftrightarrow  \left(\frac{1}{m}, \dots, \frac{1}{m}\right).
    \]
    Let $k=m$ and integrate with respect to $\mu$ over the atom $A_i$ in the expression
    \[
    \frac{d P_s}{d\mu} (\omega) = p(\omega;s) = p(\omega; 0)
    \exp\left\{\sum_{\alpha=1}^{m-1}s^{\alpha}g_{\alpha}(\omega)- \Psi(s)\right\},
    \]
    where $\mu$ is an arbitrary positive measure vanishing only on $\mathbb{Z}$-sets. Noting that $\epsilon_j(\omega) = 0$ on $A_i$ for $j\neq i$, we obtain 
    \[
    p_i (\mathbf{x}) = P_{\mathbf{x}}\{ A_i\} =P_0 \{ A_i\} \exp\left\{x^i - \Psi(\mathbf{x})\right\}
    \]
    for $i < m$,
    and
    \[
    p_m(\mathbf{x}) = P_0 \{ A_m\} \exp\{- \Psi(\mathbf{x})\},
    \]
    where
    \[
    \exp{\Psi(\mathbf{s})} = \int_{\Omega} \exp\left[\sum_{\alpha=1}^{m-1} s^{\alpha}g_{\alpha}(\omega)\right] P_0\{ d \omega\}.
    \]
    Thus,
    \[
    \exp[ \Psi(x) ] = 
    \sum_{j=1}^{m-1} \frac{1}{m} \exp[x^j].
    \]

    Consider the coordinates $(x_0, x^1, x^2,\dots, x^{m-1})$ and let only $x^1$ vary, that is, $x^1 = x_0^1 +t$, with $P_{x(t)}= R_t$ and $b_i(t) = B_t\{ A_i\}$, while $x^2, \dots, x^{m-1}$ remain fixed.

    Then we have
    \[
    b_1(t) = b_1 (0) \exp [ t - \Phi(t) ], \quad b_i(t) = b_i (0) \exp [ - \Phi (t)],
    \]
    where
    \[
    \Phi (t) = \Psi(x_0^1 + t, x_0 ^2, \dots, x_0^{m-1}) - \Psi (x_0^1, x_0 ^2,\dots,x_0^{m-1}).
    \]
    Summing over $i=2,\dots,m$, we obtain
    \[
    p(t; q_2,\dots,q_m) = p_1(t)\mathbf{e}_1 +[1 - p_1(t)]\sum_{i=2}^m q_i \mathbf{e}_i.
    \]
    This confirms that the coordinate line is a Ceva line.

It remains to calculate $b_1(t)$. Substituting our expression for $\exp{- \Phi(t)}$, we obtain
    \[
    \frac{b_1(t)}{b_1(0)} = \frac{1 - b_1(t)}{1-b_1(0)} \exp{t},
    \]
    and differentiating with respect to $t$,
    \[
    \dot{b}_1(t) = b_1(t) - [b_1(t)]^2.
    \]
    This establishes the required result. The remaining cases follow by permutation of atoms and coordinates.
\end{proof}

\section{Translation-Invariant Tensor Fields}

\begin{theorem}
A tensor field defined on the simplex $\mathsf{Caph}$ is invariant under the group of translations if and only if its components are constant with respect to the the canonical coordinate system.
\end{theorem}

\begin{proof}
Consider the canonical coordinate system $(z_1, z_2,\dots,z_{m-1})$. In these coordinates, the simplex becomes an $(m-1)$-dimensional affine space, where translations correspond to parallel shifts.

Since the group of parallel translations is both transitive and simply transitive, we conclude that carrying the components of a tensor unchanged across all points yields an invariant field.
Let $f(\mathbf{p})=c$ be an invariant scalar field. For any point $\mathbf{q}$, there exists a unique translation mapping $\mathbf{p}$ to $\mathbf{q}$, denoted by $T\mathbf{p}=\mathbf{q}$. Since $f^T = f$, we obtain
    \[
    f(\mathbf{q})= f^T(\mathbf{q})= f(T^{-1}(\mathbf{q}))= f(\mathbf{p})=c.
    \]
    Hence, $f(\mathbf{q})$ is necessarily constant.

    Now, let
    \[
    (Y)_{\mathbf{p}}= \sum_{j=1}^{m-1} a^j (\mathbf{p})\frac{\partial}{\partial z^j}\Big|_{\mathbf{p}}.
    \]
    Under translation, canonical coordinates shift as
    \[
    T: z^j \longrightarrow z^j + s^j,
    \]
    which implies
    \[
   T:  \left(\frac{\partial}{\partial z^j}\right)_{\mathbf{p}} \longrightarrow \left(\frac{\partial}{\partial z^j}\right)_{T\mathbf{p}}.
    \]
    If $Y$ is invariant, then
    \[
    \sum a^j (\mathbf{p})\frac{\partial}{\partial z^j} = \sum a^j (T\mathbf{p})\frac{\partial}{\partial z^j}.
    \]
    Since the $\frac{\partial}{\partial z^j}$ form a basis at each point, it follows that
    \[
    a^j (\mathbf{p})= a^j (\mathbf{q}).
    \]
    Hence, the theorem is proved.
\end{proof}

\begin{corollary}
    On the manifold $\mathsf{Caph}$, there exist translation-invariant Riemannian metrics that convert $\mathsf{Caph}$ into a Euclidean space.
\end{corollary}

\section{Flat Connections}

\begin{definition}
    A connection is said to be \textit{flat} if
    \[
    \nabla_{X_j}X_k = 0, \quad j,k=1,2,\dots,m.
    \]
\end{definition}

\begin{remark}
    The flat connection defined above is compatible with any translation-invariant Riemannian metric.
\end{remark}


\section{Exercises}
Some easy exercises. 
\begin{itemize}
\item Exercise 1. Determine the affine transformation which maps the points $(0,0)$, $(1,0)$ and $(0,1)$ to the points
\begin{itemize}
    \item $(0,-1)$, $(1,1)$, and $(-1,1)$, respectively,
    \item $(-4,-5)$, $(1,7)$, and $(2,-9)$, respectively.
\end{itemize}
\item Exercise 2.The triangle $\bigtriangleup ABC$  has vertices $A(2,0)$, $B(-3,0)$ and $C(3,-3)$, and the points $P(-1,-1)$, $Q(1,3)$ and $R(-\frac{1}{4},0)$ lie on $BC$, $CA$, and $AB$ respectively.
\begin{itemize}
    \item Determine the ratios in which $P$, $Q$, and $R$ divide the sides of the triangle,
     \item Determine whether or not the points $P$, $Q$,and $R$ are collinear.
\end{itemize}
\item Exercise 3. Consider a statistical manifolds of exponential type. Using the charts description proof that the structure of statistical manifold is affine and flat.
\end{itemize}

\,
\subsection{Some corrections}
Exercise \ref{Ex:ProofThm7.2}, correction.
\begin{proof}
    The proof is divided into three parts.  
 \subsection*{First part of the Proof}   
    Firstly, since any measure on $\mathbf{\Sigma}$ can be extended to a measure on $\mathbf{\Sigma^*}$, where it coincides with the induced inner and outer measures, it remains to show that for any fixed $\mathcal{A} \in \mathbf{\Sigma^*}$, the transition probability function $\omega \mapsto \Pi^*(\omega, \mathcal{A})$ is $\mathbf{S^*}$-measurable.
    
\subsection*{Second part of the Proof}
Having established the first part of the proof, we now proceed to the second step. Our aim is to show that the real-valued function  
\[
\Pi_{\mathcal{A}}(\omega) = \Pi(\omega; \mathcal{A})
\]
is $\mathbf{S^*}$-measurable for any fixed $\mathcal{A} \in \mathbf{\Sigma^*}$. To do so, it suffices to verify that for any probability measure $P$ on $\mathbf{S}$ and any real number $z \in [0,1]$, the outer and inner $P$-measures of the preimage 
\[
U = \{ \omega \mid \Pi_{\mathcal{A}}(\omega) < z\}
\]
of the half-open interval $[0, z)$ coincide, i.e., 
\[
\overline{P} [U ] = \underline{P} [U ],
\]
which ensures that $U$ is indeed $\mathbf{S^*}$-measurable.

Consider probability measures $P$ and $Q$. For any $\mathcal{A} \in \mathbf{\Sigma^*}$, there exist two sets $\mathcal{G}, \mathcal{F} \in \mathbf{\Sigma}$ satisfying:
\[
\mathcal{F} \subset \mathcal{A} \subset \mathcal{G},
\]
such that for every $\omega$, the following equalities hold:
\[
Q[\mathcal{G}] = \overline{Q} [\mathcal{A}] = \underline{Q}[\mathcal{A}] = Q[\mathcal{F}].
\]

Since probability measures are monotone, we obtain the inequalities:
\[
\Pi(\omega; \mathcal{G}) \geq \Pi^*(\omega; \mathcal{A}) \geq \Pi(\omega; \mathcal{F}).
\]
Therefore, the functions $\Pi(\omega; \mathcal{G})$ and $\Pi(\omega; \mathcal{F})$ are $\mathbf{S}$-measurable and equal almost everywhere,
since
\[
\int \Pi(\omega; \mathcal{G}) P\{d\omega\} = \int \Pi(\omega; \mathcal{F}) P\{d\omega\},
\]
except possibly on a $\mathbf{S}$-measurable $P$-null set $N$ where $P\{N\} = 0$.

From this, it follows that:
\[
\{ \omega \mid \Pi(\omega; \mathcal{G}) < z \} \subseteq \{ \omega \mid \Pi^* (\omega; \mathcal{A}) < z \}.
\]
Moreover, we also have:
\[
\{ \omega \mid \Pi^*(\omega; \mathcal{A}) < z \} \subseteq \{ \omega \mid \Pi(\omega; \mathcal{G}) < z \} \cup N.
\]
Thus, we conclude:
\[
\overline{P} [ U ] = \underline{P} \{ \omega \mid \Pi(\omega; \mathcal{G}) < z \},
\]
which proves the second part.

\subsection*{Third part of the Proof}

Finally, we proceed to the third and last step: proving that the measures $(P\Pi)^*$ and $P^*\Pi^*$, both defined on $(\Omega',\mathbf{\Sigma^*})$, coincide. That is, we claim that for any $\mathcal{A} \in \mathbf{\Sigma^*}$, we have:
\[
(P \Pi)^* \{\mathcal{A}\} = (P^* \Pi^*)\{\mathcal{A}\}.
\]

Let us choose sets $\mathcal{F}$ and $\mathcal{G}$ satisfying for a given  $\mathcal{A}$:
\[
\mathcal{F}  \subseteq \mathcal{A} \subseteq \mathcal{G}.
\]
Then, as before, we have:
\[
Q\{\mathcal{G} \} = Q^*\{ \mathcal{A}\} = Q\{ \mathcal{F} \},
\]
where $Q = P \Pi$. Given that the function $\Pi_{\mathcal{A}}^*(\omega) = \Pi^*(\omega; \mathcal{A})$ is $\mathbf{\Sigma^*}$-measurable, we obtain:
\[
(P\Pi)\{ \mathcal{G} \} = \int \Pi(\omega; \mathcal{G}) P[d\omega] =
\int \Pi^*(\omega; \mathcal{G}) P^*[d\omega].
\]
By monotonicity, we conclude:
\[
\int \Pi^*(\omega; \mathcal{G}) P^*[d\omega] \geq \int \Pi^*(\omega; \mathcal{A})P^*[d\omega].
\]
Thus, we deduce:
\[
(P^* \Pi^*)\{ \mathcal{A} \} = (P\Pi)\{\mathcal{G}\}.
\]
Since this holds for all $\mathcal{A} \in \mathbf{\Sigma^*}$, we obtain the desired equality:
\[
(P^* \Pi^*)\{\mathcal{A} \} = (P \Pi)^* \{ \mathcal{A}\}.
\]
This completes the proof.

\end{proof}
\, 

Exercise \ref{Ex:LemmaDom}.

    The measure $P_0$ is dominating since for each $P_k$ we have the bound 
    \[
    P_k\{.\} \leq 2^k P_0\{.\}
    \]
    and the inclusion of null sets:
    \[
    \mathbf{Z}_{P_0} \subset \mathbf{Z}_{P_k}.
    \]
    If we can show that $P_0$ is a constructive probability measure, the desired conclusion follows from Lemma \ref{L:7.2}. 

    Consider a measurable mapping $\omega = f_k(x)$, where $f_k : \mathbf{E} \to \Omega$ determines the constructive distribution 
    \[
    P_k\{.\} = \lambda \frac{1}{f_k(.)}.
    \]
    Define $f_0$ piecewise as follows: for $2^{-k} < x < 2^{-k+1}$, let
    \[
    f_0(x) = g_k(x) = f_k (2^k x -1),
    \]
    and set $f_0(0) = f_1(0)$. 

    This function is measurable, as it is composed of countably many measurable functions, each defined on a distinct measurable subset. Consequently, the $f_0$-preimage of any $\mathbf{S}$-set is a countable union of disjoint $g_k$-preimages. 

    Furthermore, we have the measure transformation:
    \[
    \lambda g_k^{-1}\{.\} = 2^{-1} \lambda f_k^{-1}\{.\},
    \]
    since the $g_k$-preimage is the $f_k$-preimage shifted by $2^{-k}$ and contracted by a factor of $2^k$. This completes the proof.
