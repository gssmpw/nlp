\section{Related Work}
\label{s:related_work}
Automatic question answering (QA) systems____ have significantly advanced in recent years, especially following the emergence of large language models (LLMs) that have transformed various tasks in fields such as Natural Language Processing (NLP)____, Information Retrieval (IR)____, and Computer Vision____. QA systems generally fall into two main categories: (1) Extractive and (2) Generative. Extractive systems____ identify answers within a passage, yielding a final answer as a passage span. In contrast, generative systems____ generate answers based on passages____, knowledge graphs____, or by simply leveraging LLMs____. Due to the impressive capabilities of LLMs, much of recent research has focused on generative QA systems.

The rise of LLMs has also spurred interest in other QA-related tasks, such as Question Generation (QG)____. The objective of QG is to generate questions based on passages____ or knowledge graphs____, focusing on extracting relevant entities to craft questions____. In particular, QG systems excel in generating factoid questions____ derived from these extracted entities. Some studies have advanced QG by generating multiple-choice questions____, introducing distractors____ as plausible but incorrect options. This does not only provide correct answers but also suggests alternatives, offering a more comprehensive question format.

In addition to question answering and question generation, a new task known as Automatic Hint Generation for QA has been recently introduced, initially presented by____. Previously, hint generation was explored primarily within the scope of intelligent tutoring systems focused on programming____ rather than QA. Hint Generation task aims to generate hints related to a questionâ€™s answer, helping users arrive at answers themselves____. Recent work on this task has introduced methods to improve various aspects, such as dataset creation____, hint generation approaches____, hint ranking____, and hint evaluation____. For a deeper exploration of hint generation and its challenges, we recommend the recent survey by____ which examines datasets, methods, and evaluation strategies for Hint Generation.

\subsection{Datasets} \label{ss:related_work_dataset}
Numerous datasets are available for QA tasks____, which can be divided into categories like Factoid QA____, Definition QA____, Yes/No QA____, Commonsense QA____, or Mathematical QA____. Our focus is on factoid QA datasets, which are foundational to QG and Hint Generation tasks. Popular factoid QA datasets include TriviaQA____, NaturalQuestions (NQ)____, and WebQuestions (WebQ)____. While these datasets feature real user questions, others like ChroniclingAmericaQA____ and ArchivalQA____ employ synthetic questions generated via QG tasks. Although most factoid QA research centers around extracting or generating answers, certain datasets such as TriviaHG, WikiHint, and HintQA include also hints to assist users in deducing answers themselves. TriviaHG____ contains hints for a subset of TriviaQA questions, HintQA____ covers test subsets of TriviaQA, NQ, and WebQ, and WikiHint____ provides hints extracted from Wikipedia by humans for questions generated by ChatGPT and extracted from SQuAD 2.0____ and NQ.

\subsection{Models} \label{ss:related_work_model}
A variety of models and methods are applied across QA systems, which can be categorized as Traditional____, Deep Learning-based ____, and LLM-based____. Traditional methods rely on syntactic structures like syntax trees, dependency trees, or WordNet____ to locate answers, whereas deep learning models have introduced a shift towards transformer-based architectures. For QG, there are rule-based methods____, but generative models like T5____ have become the standard____. In Hint Generation, early work by____ used rule-based algorithms and WikiData\footnote{\url{https://www.wikidata.org/}}; however, more recent studies employ generative models such as LLaMA____ and Copilot\footnote{\url{https://copilot.microsoft.com/}}. Hint generation approaches are either answer-aware, where hints are generated based on the question accompanied with its answer, or answer-agnostic, where the generation is based on only the question. For instance, TriviaHG and WikiHint use answer-aware methods, while HintQA employs an answer-agnostic approach.

\subsection{Evaluation} \label{ss:related_work_evaluation}
Several evaluation metrics exist for QA systems, with some relying on statistical measures____ like Exact Match and F1, while others utilize neural-based metrics like BertScore____ and BEM____. QG is often evaluated through metrics such as BLEU____, METEOR____, and ROUGE____, alongside human evaluations____. For Hint Generation, five key metrics have been applied: Relevance, Readability, Convergence, Familiarity____, and AnswerLeakage____.

\subsection{Frameworks} \label{ss:related_work_framework}
For QA, frameworks like PySerini ____, BERGEN ____, and Reranker ____ simplify pipeline implementation, providing modular components for customized QA solutions. In QG, libraries such as LMQG____ allow question generation based on input passages. However, no dedicated frameworks exist for Hint Generation and Hint Evaluation, making it challenging to contribute to the research in these areas.
To address this gap, we introduce \framework, the first framework designed for hint generation, enabling users to both effectively generate and evaluate hints.