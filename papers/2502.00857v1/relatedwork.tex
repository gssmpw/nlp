\section{Related Work}
\label{s:related_work}
Automatic question answering (QA) systems~\cite{10.1145/3560260, DBLP:journals/ftir/MaviJJ24, abdel2023deep} have significantly advanced in recent years, especially following the emergence of large language models (LLMs) that have transformed various tasks in fields such as Natural Language Processing (NLP)~\cite{10.5555/3495724.3495883, 10.5555/3648699.3648939}, Information Retrieval (IR)~\cite{sachan-etal-2022-improving, sun-etal-2023-chatgpt}, and Computer Vision~\cite{pmlr-v139-ramesh21a, 2024arXiv240608394W}. QA systems generally fall into two main categories: (1) Extractive and (2) Generative. Extractive systems~\cite{9991478, 2020arXiv201008983R} identify answers within a passage, yielding a final answer as a passage span. In contrast, generative systems~\cite{yu2023generate, Chen_Lin_Han_Sun_2024} generate answers based on passages~\cite{10.1007/s10791-023-09420-7, 2021arXiv211006393X}, knowledge graphs~\cite{wang-etal-2021-generative, 10.1007/978-3-031-43415-0_35}, or by simply leveraging LLMs~\cite{li-etal-2024-self-prompting}. Due to the impressive capabilities of LLMs, much of recent research has focused on generative QA systems.

The rise of LLMs has also spurred interest in other QA-related tasks, such as Question Generation (QG)~\cite{10.1007/s13748-023-00295-9, Goyal2024}. The objective of QG is to generate questions based on passages~\cite{10.1145/3468889} or knowledge graphs~\cite{9621874}, focusing on extracting relevant entities to craft questions~\cite{fabbri-etal-2020-template}. In particular, QG systems excel in generating factoid questions~\cite{Abdallah2023} derived from these extracted entities. Some studies have advanced QG by generating multiple-choice questions~\cite{2024arXiv240802114F}, introducing distractors~\cite{2024arXiv240201512A} as plausible but incorrect options. This does not only provide correct answers but also suggests alternatives, offering a more comprehensive question format.

In addition to question answering and question generation, a new task known as Automatic Hint Generation for QA has been recently introduced, initially presented by~\citet{jatowt_kg_hint}. Previously, hint generation was explored primarily within the scope of intelligent tutoring systems focused on programming~\cite{10.1145/3469885, 10.1007/978-3-540-69132-7_41, Kochmar2022, Price2019} rather than QA. Hint Generation task aims to generate hints related to a questionâ€™s answer, helping users arrive at answers themselves~\cite{jatowt_kg_hint}. Recent work on this task has introduced methods to improve various aspects, such as dataset creation~\cite{mozafari-triviahg, mozafari-hintqa}, hint generation approaches~\cite{mozafari-triviahg, mozafari-hintqa}, hint ranking~\cite{2024arXiv241201626M}, and hint evaluation~\cite{mozafari-triviahg}. For a deeper exploration of hint generation and its challenges, we recommend the recent survey by~\citet{2024arXiv240404728J} which examines datasets, methods, and evaluation strategies for Hint Generation.

\subsection{Datasets} \label{ss:related_work_dataset}
Numerous datasets are available for QA tasks~\cite{reddy-etal-2019-coqa, clark-etal-2020-tydi, yang-etal-2018-hotpotqa}, which can be divided into categories like Factoid QA~\cite{rajpurkar-etal-2018-know}, Definition QA~\cite{2015arXiv150602075B}, Yes/No QA~\cite{clark-etal-2019-boolq}, Commonsense QA~\cite{talmor-etal-2019-commonsenseqa}, or Mathematical QA~\cite{amini-etal-2019-mathqa}. Our focus is on factoid QA datasets, which are foundational to QG and Hint Generation tasks. Popular factoid QA datasets include TriviaQA~\cite{joshi-etal-2017-triviaqa}, NaturalQuestions (NQ)~\cite{kwiatkowski-etal-2019-natural}, and WebQuestions (WebQ)~\cite{berant-etal-2013-semantic}. While these datasets feature real user questions, others like ChroniclingAmericaQA~\cite{10.1145/3626772.3657891} and ArchivalQA~\cite{10.1145/3477495.3531734} employ synthetic questions generated via QG tasks. Although most factoid QA research centers around extracting or generating answers, certain datasets such as TriviaHG, WikiHint, and HintQA include also hints to assist users in deducing answers themselves. TriviaHG~\cite{mozafari-triviahg} contains hints for a subset of TriviaQA questions, HintQA~\cite{mozafari-hintqa} covers test subsets of TriviaQA, NQ, and WebQ, and WikiHint\cite{2024arXiv241201626M} provides hints extracted from Wikipedia by humans for questions generated by ChatGPT and extracted from SQuAD 2.0~\cite{rajpurkar-etal-2018-know} and NQ.

\subsection{Models} \label{ss:related_work_model}
A variety of models and methods are applied across QA systems, which can be categorized as Traditional~\cite{punyakanok2004natural}, Deep Learning-based \cite{10.1007/s00521-021-06748-3}, and LLM-based~\cite{2024arXiv240214320Z}. Traditional methods rely on syntactic structures like syntax trees, dependency trees, or WordNet~\cite{fernando2008semantic} to locate answers, whereas deep learning models have introduced a shift towards transformer-based architectures. For QG, there are rule-based methods~\cite{7732102}, but generative models like T5~\cite{10.5555/3455716.3455856} have become the standard~\cite{2020arXiv200501107E, yuan-etal-2023-selecting}. In Hint Generation, early work by~\citet{jatowt_kg_hint} used rule-based algorithms and WikiData\footnote{\url{https://www.wikidata.org/}}; however, more recent studies employ generative models such as LLaMA~\cite{2024arXiv240721783D} and Copilot\footnote{\url{https://copilot.microsoft.com/}}. Hint generation approaches are either answer-aware, where hints are generated based on the question accompanied with its answer, or answer-agnostic, where the generation is based on only the question. For instance, TriviaHG and WikiHint use answer-aware methods, while HintQA employs an answer-agnostic approach.

\subsection{Evaluation} \label{ss:related_work_evaluation}
Several evaluation metrics exist for QA systems, with some relying on statistical measures~\cite{jurafsky2013} like Exact Match and F1, while others utilize neural-based metrics like BertScore~\cite{Zhang2020BERTScore} and BEM~\cite{bulian-etal-2022-tomayto}. QG is often evaluated through metrics such as BLEU~\cite{10.3115/1073083.1073135}, METEOR~\cite{banerjee-lavie-2005-meteor}, and ROUGE~\cite{lin-2004-rouge}, alongside human evaluations~\cite{liu2020semantics}. For Hint Generation, five key metrics have been applied: Relevance, Readability, Convergence, Familiarity~\cite{mozafari-triviahg}, and AnswerLeakage~\cite{2024arXiv241201626M}.

\subsection{Frameworks} \label{ss:related_work_framework}
For QA, frameworks like PySerini \cite{10.1145/3404835.3463238}, BERGEN \cite{2024arXiv240701102R}, and Reranker \cite{2024arXiv240817344C} simplify pipeline implementation, providing modular components for customized QA solutions. In QG, libraries such as LMQG~\cite{ushio-etal-2023-practical} allow question generation based on input passages. However, no dedicated frameworks exist for Hint Generation and Hint Evaluation, making it challenging to contribute to the research in these areas.
To address this gap, we introduce \framework, the first framework designed for hint generation, enabling users to both effectively generate and evaluate hints.