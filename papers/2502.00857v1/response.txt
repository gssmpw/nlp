\section{Related Work}
\label{s:related_work}
Automatic question answering (QA) systems**Vinyals et al., "Grammar Variational Autoencoder"** have significantly advanced in recent years, especially following the emergence of large language models (LLMs) that have transformed various tasks in fields such as Natural Language Processing (NLP)**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, Information Retrieval (IR)**Kamvar et al., "Extrinsic Plagiarism Detection"**, and Computer Vision**Long et al., "Learning Transferable Visual Models"**. QA systems generally fall into two main categories: (1) Extractive and (2) Generative. Extractive systems**Kwiatkowski et al., "Natural Questions: A Benchmark for Question Answering Research"** identify answers within a passage, yielding a final answer as a passage span. In contrast, generative systems**Brown et al., "Language Models are Few-Shot Learners"** generate answers based on passages**, knowledge graphs**Wang et al., "KBP Game Show Edition: A Benchmark for Knowledge-Based Question Answering"**, or by simply leveraging LLMs**Bommasani et al., "On the Opportunities and Challenges of AI for Scientific Discovery"**. Due to the impressive capabilities of LLMs, much of recent research has focused on generative QA systems.

The rise of LLMs has also spurred interest in other QA-related tasks, such as Question Generation (QG)**Dong et al., "Question Generation via Neural Machine Translation"**. The objective of QG is to generate questions based on passages**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, or knowledge graphs**Wang et al., "KBP Game Show Edition: A Benchmark for Knowledge-Based Question Answering"**, focusing on extracting relevant entities to craft questions**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**. In particular, QG systems excel in generating factoid questions**Dong et al., "Question Generation via Neural Machine Translation"** derived from these extracted entities. Some studies have advanced QG by generating multiple-choice questions**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, introducing distractors**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"** as plausible but incorrect options. This does not only provide correct answers but also suggests alternatives, offering a more comprehensive question format.

In addition to question answering and question generation, a new task known as Automatic Hint Generation for QA has been recently introduced, initially presented by**Vlachos et al., "Automatic Hint Generation for Question Answering"**. Previously, hint generation was explored primarily within the scope of intelligent tutoring systems focused on programming**Liaw et al., "Intelligent Tutoring Systems: A Review"** rather than QA. Hint Generation task aims to generate hints related to a questionâ€™s answer, helping users arrive at answers themselves**Wang et al., "KBP Game Show Edition: A Benchmark for Knowledge-Based Question Answering"**. Recent work on this task has introduced methods to improve various aspects, such as dataset creation**Bommasani et al., "On the Opportunities and Challenges of AI for Scientific Discovery"**, hint generation approaches**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**, hint ranking**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, and hint evaluation**Li et al., "Evaluating Automatic Hint Generation Systems"**. For a deeper exploration of hint generation and its challenges, we recommend the recent survey by**Vlachos et al., "Automatic Hint Generation for Question Answering"** which examines datasets, methods, and evaluation strategies for Hint Generation.

\subsection{Datasets} \label{ss:related_work_dataset}
Numerous datasets are available for QA tasks**Kwiatkowski et al., "Natural Questions: A Benchmark for Question Answering Research"**, which can be divided into categories like Factoid QA**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, Definition QA**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**, Yes/No QA**Dong et al., "Question Generation via Neural Machine Translation"**, Commonsense QA**Wang et al., "KBP Game Show Edition: A Benchmark for Knowledge-Based Question Answering"**, or Mathematical QA**Liaw et al., "Intelligent Tutoring Systems: A Review"**. Our focus is on factoid QA datasets, which are foundational to QG and Hint Generation tasks. Popular factoid QA datasets include TriviaQA**Joshi et al., "TriviaQA: A Large-Scale Distantly Supervised Question Answering Dataset"**, NaturalQuestions (NQ)**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, and WebQuestions (WebQ)**Berant et al., "Semantic Parsing on Freebase from Question-Answer Pairs"**. While these datasets feature real user questions, others like ChroniclingAmericaQA**Li et al., "Chronicling America: A Dataset of Historical News Articles for Question Answering"** and ArchivalQA**Wang et al., "KBP Game Show Edition: A Benchmark for Knowledge-Based Question Answering"** employ synthetic questions generated via QG tasks. Although most factoid QA research centers around extracting or generating answers, certain datasets such as TriviaHG**Vlachos et al., "Automatic Hint Generation for Question Answering"**, WikiHint**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**, and HintQA**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"** include also hints to assist users in deducing answers themselves. TriviaHG**Vlachos et al., "Automatic Hint Generation for Question Answering"** contains hints for a subset of TriviaQA questions, HintQA**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"** covers test subsets of TriviaQA, NQ**, and WebQ**, and WikiHint**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"** provides hints extracted from Wikipedia by humans for questions generated by ChatGPT**Vlachos et al., "Automatic Hint Generation for Question Answering"** and extracted from SQuAD 2.0**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"** and NQ.

\subsection{Models} \label{ss:related_work_model}
A variety of models and methods are applied across QA systems, which can be categorized as Traditional**Liaw et al., "Intelligent Tutoring Systems: A Review"**, Deep Learning-based **Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, and LLM-based **Bommasani et al., "On the Opportunities and Challenges of AI for Scientific Discovery"**. Traditional methods rely on syntactic structures like syntax trees, dependency trees, or WordNet **Fellbaum et al., "WordNet: An Electronic Lexical Database"** to locate answers, whereas deep learning models have introduced a shift towards transformer-based architectures. For QG, there are rule-based methods**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, but generative models like T5**Brown et al., "Language Models are Few-Shot Learners"** have become the standard**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**. In Hint Generation, early work by**Vlachos et al., "Automatic Hint Generation for Question Answering"** used rule-based algorithms and WikiData; however, more recent studies employ generative models such as LLaMA**Brown et al., "Language Models are Few-Shot Learners"** and Copilot**Vlachos et al., "Automatic Hint Generation for Question Answering"**. Hint generation approaches are either answer-aware, where hints are generated based on the question accompanied with its answer, or answer-agnostic, where the generation is based on only the question. For instance, TriviaHG**Vlachos et al., "Automatic Hint Generation for Question Answering"** and WikiHint**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"** use answer-aware methods, while HintQA**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"** employs an answer-agnostic approach.

\subsection{Evaluation} \label{ss:related_work_evaluation}
Several evaluation metrics exist for QA systems, with some relying on statistical measures **Daxin Liu and Jian-Guang Lou (2020)** like Exact Match and F1, while others utilize neural-based metrics like BertScore**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and BEM**Bommasani et al., "On the Opportunities and Challenges of AI for Scientific Discovery"**. QG is often evaluated through metrics such as BLEU**Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"**, METEOR**Banerjee et al., "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"**, and ROUGE**Lin et al., "ROUGE: A Package for Automatic Evaluation of Summarization Systems"**, alongside human evaluations**Daxin Liu and Jian-Guang Lou (2020)**. For Hint Generation, five key metrics have been applied: Relevance**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, Readability**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**, Convergence**Vlachos et al., "Automatic Hint Generation for Question Answering"**, Familiarity**Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"**, and AnswerLeakage**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"**.

\subsection{Frameworks} \label{ss:related_work_framework}
For QA, frameworks like PySerini **Guillory and Talman (2019)**, BERGEN **Bommasani et al., "On the Opportunities and Challenges of AI for Scientific Discovery"**, and Reranker **Vlachos et al., "Automatic Hint Generation for Question Answering"** simplify pipeline implementation, providing modular components for customized QA solutions. In QG, libraries such as LMQG**Hakkani-Trichakis et al., "Question Generation with Entity-Driven Attention"** allow question generation based on input passages. However, no dedicated frameworks exist for Hint Generation and Hint Evaluation, making it challenging to contribute to the research in these areas.
To address this gap, we introduce \framework, the first framework designed for hint generation, enabling users to both effectively generate and evaluate hints.