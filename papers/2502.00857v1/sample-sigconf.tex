%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf]{acmart}
\documentclass[sigconf,natbib=true]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[SIGIR '25]{Make sure to enter the correct
  conference title from your rights confirmation email}{July 13--18,
  2025}{Padova, IT}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{libertine}
% \usepackage{emoji}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{xspace}

\definecolor{codegreen}{RGB}{93, 168, 128} % Light pleasant blue
\definecolor{codeblue}{RGB}{74, 74, 250}  % Pleasant light green
\definecolor{codered}{RGB}{209, 106, 114}
\definecolor{text}{RGB}{68,114,157}

\usepackage{listings}
\lstset{
  language=Python,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\color{codeblue},
  stringstyle=\color{codegreen},
  commentstyle=\color{gray},
  breakindent=0em,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  morekeywords={},
  rulecolor=\color{lightgray!40},
  backgroundcolor=\color{lightgray!5},
  frameround=tttt,
  frame=lines,
  framesep=5pt,
  tabsize=4,
  captionpos=b,
  numbers=left,
  numbersep=5pt,
  xleftmargin=.0\textwidth,  
  xrightmargin=.0\textwidth,
}

\newcommand{\blackcircle}[1]{%
    \raisebox{0.8pt}{\textcircled{\raisebox{-0.8pt}{#1}}}
}

\newcommand{\framework}{\textsc{HintEval}\xspace}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{HintEval: A Comprehensive Framework for Hint Generation and Evaluation for Questions}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Jamshid Mozafari}
\orcid{0000-0003-4850-9239}
\affiliation{%
  \institution{University of Innsbruck}
 \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{jamshid.mozafari@uibk.ac.at}

\author{Bhawna Piryani}
\orcid{0009-0005-3578-2393}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{bhawna.piryani@uibk.ac.at}

\author{Abdelrahman Abdallah}
\orcid{0000-0001-8747-4927}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \state{Tyrol}
  \country{Austria}
  }
\email{abdelrahman.abdallah@uibk.ac.at}

\author{Adam Jatowt}
\orcid{0000-0001-7235-0665}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
 \state{Tyrol}
  \country{Austria}
  }
\email{adam.jatowt@uibk.ac.at}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mozafari et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% \begin{abstract}
% Large Language Models (LLMs) are transforming how people find information, and many users turn nowadays to chatbots for obtaining answers to their questions. Despite the instant access to (usually) correct information that LLMs offer, it is still important to promote critical thinking and problem-solving skills. Automatic hint generation is a new task that aims to support humans in answering questions by themselves by creating hints that guide users toward answers without directly revealing them. In this context, hint evaluation focuses on measuring the quality of hints, helping to improve the hint generation approaches. However, resources for hint research are currently spanning different formats and datasets, while the evaluation tools are missing or are incompatible, making it hard for researchers to compare and test their models. To overcome these challenges, we introduce \framework
% \footnote{\url{https://github.com/DataScienceUIBK/HintEval}}
% , a Python library
% \footnote{\url{https://pypi.org/project/hinteval/}}
% that makes it easy to access diverse existing datasets and that provides a multiple approaches to generate and evaluate hints. \framework aggregates the existing, scattered resources into a single toolkit that supports a range of research goals and enables a clear, multi-faceted and reliable evaluation. The proposed library also includes detailed online documentation
% \footnote{\url{http://hinteval.readthedocs.io/}}
% , helping users quickly explore its features and get started. By reducing barriers to entry and encouraging consistent evaluation practices, \framework offers a major step forward for facilitating hint generation and analysis research within the NLP/IR community. 
% \end{abstract}

\begin{abstract}
Large Language Models (LLMs) are transforming how people find information, and many users turn nowadays to chatbots to obtain answers to their questions. Despite the instant access to abundant information that LLMs offer, it is still important to promote critical thinking and problem-solving skills. Automatic hint generation is a new task that aims to support humans in answering questions by themselves by creating hints that guide users toward answers without directly revealing them. In this context, hint evaluation focuses on measuring the quality of hints, helping to improve the hint generation approaches. However, resources for hint research are currently spanning different formats and datasets, while the evaluation tools are missing or incompatible, making it hard for researchers to compare and test their models. To overcome these challenges, we introduce \framework\footnote{\url{https://github.com/DataScienceUIBK/HintEval}}, a Python library\footnote{\url{https://pypi.org/project/hinteval/}}
that makes it easy to access diverse datasets and provides multiple approaches to generate and evaluate hints. \framework aggregates the scattered resources into a single toolkit that supports a range of research goals and enables a clear, multi-faceted, and reliable evaluation. The proposed library also includes detailed online documentation\footnote{\url{http://hinteval.readthedocs.io/}}, helping users quickly explore its features and get started. By reducing barriers to entry and encouraging consistent evaluation practices, \framework offers a major step forward for facilitating hint generation and analysis research within the NLP/IR community. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011072</concept_id>
       <concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338</concept_id>
       <concept_desc>Information systems~Retrieval models and ranking</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003359</concept_id>
       <concept_desc>Information systems~Evaluation of retrieval results</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software libraries and repositories}
\ccsdesc[500]{Information systems~Retrieval models and ranking}
\ccsdesc[500]{Information systems~Evaluation of retrieval results}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Hint Evaluation, Hint Generation, Python Package, Framework}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{s:introduction}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.25\columnwidth]{images/logo-new.png}
  \caption{\framework logo.}
  \label{fig:hinteval_logo}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{images/Sample.pdf}
  \caption{Example hints for a sample question with scoring metrics. The metrics Relevance, Convergence, Familiarity, and Answer Leakage are rated on a scale from 0 to 1, where 0 represents the lowest and 1 the highest value. Higher scores in Relevance, Convergence, and Familiarity indicate better results, while a lower score is preferable for Answer Leakage. The Readability metric is scored as 0 (Beginner), 1 (Intermediate), or 2 (Advanced), with lower values indicating better readability.}
  \label{fig:hint_sample}
\end{figure*}

In recent years, the widespread adoption of Large Language Models (LLMs)~\cite{2023arXiv231211805G, 2023arXiv230308774O, 2024arXiv240721783D} has transformed the landscape of Information Retrieval (IR) and Natural Language Processing (NLP)~\cite{2024arXiv240512819Q}, enabling users to pose diverse questions and receive immediate answers~\cite{DBLP:journals/ftir/MaviJJ24, karpukhin-etal-2020-dense, abdel2023deep, zhao2020sparta}. Although this change has made information access more efficient, it has also raised concerns about the possible impact on human cognitive skills, particularly the abilities to think critically, reason effectively, and retain information~\cite{Heersmink2024}. Extensive reliance on automated responses, especially from AI-powered systems, may discourage users from engaging deeply with problems and reduce opportunities for cognitive development~\cite{ALFREDO2024100215}. For instance, studies have shown that students are more likely to depend on AI-provided solutions rather than developing their problem-solving skills independently, which can negatively impact learning outcomes and skill retention~\cite{DARVISHI2024104967, app14104115}.

In response to these concerns, an idea has emerged to provide hints to questions rather than direct answers, encouraging in this way users to participate in the answer finding process themselves~\cite{hume1996hinting}. This approach, known as Hint Generation, aims to give subtle guidance that leads users toward the correct answer without explicitly revealing it, thereby promoting active learning and supporting cognitive engagement~\cite{2024arXiv240404728J}. For example, for the question, \emph{Who made the "I Am Prepared to Die" speech at the opening of the Rivonia Trial in April 1964?}, a good hint would be \emph{He was the first Black president of South Africa}, while \emph{He studied law at the University of Fort Hare} would not be a very useful hint.
Hint Evaluation, a complementary task to Hint Generation, aims to assess the quality and effectiveness of generated hints, ensuring they support meaningful guidance towards the correct answers~\cite{bandura2013role, usher2006sources}. Figure~\ref{fig:hint_sample} presents five hints along with their evaluations for an example question.
%,its answer (in two variants), and five hints along with their evaluations. 

The research in Hint Generation and Evaluation is however currently hindered by fragmented resources, as available datasets, methods, and evaluation tools are often in incompatible formats, making it difficult for researchers to develop, benchmark, and compare hint-based models consistently.
To address these challenges, we introduce \framework, the first comprehensive framework for hint generation and evaluation in the NLP and IR domains, with its logo shown in Figure~\ref{fig:hinteval_logo}. \framework simplifies access to a diverse collection of datasets and provides a standardized toolkit for generating and evaluating hints across multiple contexts. The framework integrates previously distributed resources and complements the evaluation metrics with several new ones, enabling clear and consistent evaluation practices that support a range of research objectives. Furthermore, \framework includes detailed online documentation to help users explore its features and begin using the framework with ease. By reducing entry barriers and promoting unified evaluation standards, \framework aims to advance research and application in hint-based learning support, fostering a more robust understanding of how hints can be optimized for educational and problem-solving tasks.
The main contributions of this paper are as follows:

\begin{itemize}
    \item We present \framework, the first Python-based library specifically designed for Hint Generation and Evaluation, consolidating essential resources for both tasks and providing a standardized platform for research.
   
    \item \framework includes access to multiple preprocessed and evaluated datasets, making it easier for researchers to generate and evaluate hints without extensive data preparation.

    \item \framework offers a range of evaluation metrics and tools for comprehensive hint assessment, standardizing the evaluation process across different research contexts and supporting both answer-aware and answer-agnostic approaches.

    \item \framework is accompanied by extensive online documentation, which helps users explore its features, and is freely available on PyPI and GitHub, making it accessible to researchers and practitioners.

\end{itemize}

The remainder of this paper is organized as follows: In Section 2, we review recent work on hint generation and evaluation that highlights limitations and the need for a unified framework. Section 3 presents the design and functionality of \framework, with detailed descriptions of its datasets, models, and evaluation metrics including also baseline performance with several LLMs. Finally, in Section 4, we conclude with insights into the potential impacts of \framework on the field and outline directions for future research.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{images/Framework.pdf}
  \caption{Workflow of the \framework: \blackcircle{1}Questions are loaded and converted into a structured dataset using the \texttt{Dataset} module. \blackcircle{2}Users can load preprocessed datasets as a structured dataset.\blackcircle{3}Hints can be generated for each question using the \texttt{Model} module and stored in the dataset object. \blackcircle{4}The \texttt{Evaluation} module assesses all generated hints and questions using various evaluation metrics, storing the results in the dataset object. \blackcircle{5}The updated dataset can be saved and reloaded as needed.}
  \label{fig:hinteval_framework}
\end{figure*}

\section{Related Work}\label{s:related_work}
Automatic question answering (QA) systems~\cite{10.1145/3560260, DBLP:journals/ftir/MaviJJ24, abdel2023deep} have significantly advanced in recent years, especially following the emergence of large language models (LLMs) that have transformed various tasks in fields such as Natural Language Processing (NLP)~\cite{10.5555/3495724.3495883, 10.5555/3648699.3648939}, Information Retrieval (IR)~\cite{sachan-etal-2022-improving, sun-etal-2023-chatgpt}, and Computer Vision~\cite{pmlr-v139-ramesh21a, 2024arXiv240608394W}. QA systems generally fall into two main categories: (1) Extractive and (2) Generative. Extractive systems~\cite{9991478, 2020arXiv201008983R} identify answers within a passage, yielding a final answer as a passage span. In contrast, generative systems~\cite{yu2023generate, Chen_Lin_Han_Sun_2024} generate answers based on passages~\cite{10.1007/s10791-023-09420-7, 2021arXiv211006393X}, knowledge graphs~\cite{wang-etal-2021-generative, 10.1007/978-3-031-43415-0_35}, or by simply leveraging LLMs~\cite{li-etal-2024-self-prompting}. Due to the impressive capabilities of LLMs, much of recent research has focused on generative QA systems.

The rise of LLMs has also spurred interest in other QA-related tasks, such as Question Generation (QG)~\cite{10.1007/s13748-023-00295-9, Goyal2024}. The objective of QG is to generate questions based on passages~\cite{10.1145/3468889} or knowledge graphs~\cite{9621874}, focusing on extracting relevant entities to craft questions~\cite{fabbri-etal-2020-template}. In particular, QG systems excel in generating factoid questions~\cite{Abdallah2023} derived from these extracted entities. Some studies have advanced QG by generating multiple-choice questions~\cite{2024arXiv240802114F}, introducing distractors~\cite{2024arXiv240201512A} as plausible but incorrect options. This does not only provide correct answers but also suggests alternatives, offering a more comprehensive question format.

In addition to question answering and question generation, a new task known as Automatic Hint Generation for QA has been recently introduced, initially presented by~\citet{jatowt_kg_hint}. Previously, hint generation was explored primarily within the scope of intelligent tutoring systems focused on programming~\cite{10.1145/3469885, 10.1007/978-3-540-69132-7_41, Kochmar2022, Price2019} rather than QA. Hint Generation task aims to generate hints related to a question’s answer, helping users arrive at answers themselves~\cite{jatowt_kg_hint}. Recent work on this task has introduced methods to improve various aspects, such as dataset creation~\cite{mozafari-triviahg, mozafari-hintqa}, hint generation approaches~\cite{mozafari-triviahg, mozafari-hintqa}, hint ranking~\cite{2024arXiv241201626M}, and hint evaluation~\cite{mozafari-triviahg}. For a deeper exploration of hint generation and its challenges, we recommend the recent survey by~\citet{2024arXiv240404728J} which examines datasets, methods, and evaluation strategies for Hint Generation.

\subsection{Datasets} \label{ss:related_work_dataset}
Numerous datasets are available for QA tasks~\cite{reddy-etal-2019-coqa, clark-etal-2020-tydi, yang-etal-2018-hotpotqa}, which can be divided into categories like Factoid QA~\cite{rajpurkar-etal-2018-know}, Definition QA~\cite{2015arXiv150602075B}, Yes/No QA~\cite{clark-etal-2019-boolq}, Commonsense QA~\cite{talmor-etal-2019-commonsenseqa}, or Mathematical QA~\cite{amini-etal-2019-mathqa}. Our focus is on factoid QA datasets, which are foundational to QG and Hint Generation tasks. Popular factoid QA datasets include TriviaQA~\cite{joshi-etal-2017-triviaqa}, NaturalQuestions (NQ)~\cite{kwiatkowski-etal-2019-natural}, and WebQuestions (WebQ)~\cite{berant-etal-2013-semantic}. While these datasets feature real user questions, others like ChroniclingAmericaQA~\cite{10.1145/3626772.3657891} and ArchivalQA~\cite{10.1145/3477495.3531734} employ synthetic questions generated via QG tasks. Although most factoid QA research centers around extracting or generating answers, certain datasets such as TriviaHG, WikiHint, and HintQA include also hints to assist users in deducing answers themselves. TriviaHG~\cite{mozafari-triviahg} contains hints for a subset of TriviaQA questions, HintQA~\cite{mozafari-hintqa} covers test subsets of TriviaQA, NQ, and WebQ, and WikiHint\cite{2024arXiv241201626M} provides hints extracted from Wikipedia by humans for questions generated by ChatGPT and extracted from SQuAD 2.0~\cite{rajpurkar-etal-2018-know} and NQ.

\subsection{Models} \label{ss:related_work_model}
A variety of models and methods are applied across QA systems, which can be categorized as Traditional~\cite{punyakanok2004natural}, Deep Learning-based \cite{10.1007/s00521-021-06748-3}, and LLM-based~\cite{2024arXiv240214320Z}. Traditional methods rely on syntactic structures like syntax trees, dependency trees, or WordNet~\cite{fernando2008semantic} to locate answers, whereas deep learning models have introduced a shift towards transformer-based architectures. For QG, there are rule-based methods~\cite{7732102}, but generative models like T5~\cite{10.5555/3455716.3455856} have become the standard~\cite{2020arXiv200501107E, yuan-etal-2023-selecting}. In Hint Generation, early work by~\citet{jatowt_kg_hint} used rule-based algorithms and WikiData\footnote{\url{https://www.wikidata.org/}}; however, more recent studies employ generative models such as LLaMA~\cite{2024arXiv240721783D} and Copilot\footnote{\url{https://copilot.microsoft.com/}}. Hint generation approaches are either answer-aware, where hints are generated based on the question accompanied with its answer, or answer-agnostic, where the generation is based on only the question. For instance, TriviaHG and WikiHint use answer-aware methods, while HintQA employs an answer-agnostic approach.

\subsection{Evaluation} \label{ss:related_work_evaluation}
Several evaluation metrics exist for QA systems, with some relying on statistical measures~\cite{jurafsky2013} like Exact Match and F1, while others utilize neural-based metrics like BertScore~\cite{Zhang2020BERTScore} and BEM~\cite{bulian-etal-2022-tomayto}. QG is often evaluated through metrics such as BLEU~\cite{10.3115/1073083.1073135}, METEOR~\cite{banerjee-lavie-2005-meteor}, and ROUGE~\cite{lin-2004-rouge}, alongside human evaluations~\cite{liu2020semantics}. For Hint Generation, five key metrics have been applied: Relevance, Readability, Convergence, Familiarity~\cite{mozafari-triviahg}, and AnswerLeakage~\cite{2024arXiv241201626M}.

\subsection{Frameworks} \label{ss:related_work_framework}
For QA, frameworks like PySerini \cite{10.1145/3404835.3463238}, BERGEN \cite{2024arXiv240701102R}, and Reranker \cite{2024arXiv240817344C} simplify pipeline implementation, providing modular components for customized QA solutions. In QG, libraries such as LMQG~\cite{ushio-etal-2023-practical} allow question generation based on input passages. However, no dedicated frameworks exist for Hint Generation and Hint Evaluation, making it challenging to contribute to the research in these areas.
To address this gap, we introduce \framework, the first framework designed for hint generation, enabling users to both effectively generate and evaluate hints.

\section{\framework}\label{s:hint_eval}

The goal of \framework is to create a tool that simplifies hint generation and evaluation. Leveraging Python's popularity in the machine-learning domain due to libraries like PyTorch~\cite{10.5555/3454287.3455008} and TensorFlow~\cite{10.5555/3026877.3026899}, \framework was developed in Python and made accessible via PyPI\footnote{\url{https://pypi.org/project/hinteval/}} for straightforward installation using:
\newline
\begin{lstlisting}[numbers=none]
$ pip install hinteval
\end{lstlisting}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/Docstring.pdf}
  \caption{A docstring for the \texttt{evaluate} function of the \texttt{Wikipedia} method within the \texttt{Familiarity} evaluation metric. The docstring begins with: \blackcircle{1}A detailed description of the function, followed by \blackcircle{2}Notes specific to the evaluation metric and the method. It includes \blackcircle{3}a comprehensive Example demonstrating usage, helping users understand how to effectively implement it. \blackcircle{4}The References section lists the scholarly publications referenced by the method, while the \blackcircle{5}See Also section provides links to related functions with similar functionality. \blackcircle{6}The Params section outlines the input parameters of the function, and \blackcircle{7}the Returns section specifies the expected output. This structure ensures clear, accessible, and thorough documentation for users.}
  \label{fig:function_docstring}
\end{figure}

This paper introduces version \texttt{0.0.1}, the initial release. Users are encouraged to use the latest version and consult the project repository\footnote{\url{https://github.com/DataScienceUIBK/HintEval}} for updates. Contributors can fork the repository and submit pull requests to improve the framework. Additionally, users can consult the online documentation\footnote{\url{https://hinteval.readthedocs.io/}} for a comprehensive guide on using the framework. Apart from the online documentation, we have included extensive docstrings along with examples for \textbf{all} functions and classes, providing inline documentation easily accessible within IDEs to enhance understanding and usability. Figure~\ref{fig:function_docstring} illustrates an example of a function's docstring for a method used in the familiarity evaluation metric (explained in Sec. \ref{ss:familiarity}).

\framework comprises three main modules: \textit{Datasets}, \textit{Models}, and \textit{Evaluation}. The \textit{Datasets} module provides functionality for creating a new dataset or downloading and loading preprocessed datasets. The \textit{Models} module focuses on generating hints, while the \textit{Evaluation} module offers methods for assessing questions, hints, and answers using various metrics. Figure~\ref{fig:hinteval_framework} overviews the workflow of the \framework. Detailed examples illustrating the framework's functionality will be provided in the following sections.

\subsection{Datasets}\label{ss:datasets}
With \framework, we 
aim to 
%address the aforementioned challenges by enabling 
empower users to be able to seamlessly work with multiple datasets using just a few lines of code. To achieve this, we designed an architecture that unifies diverse annotation styles and storage formats across datasets. This architecture consists of several components, as illustrated in Figure~\ref{fig:dataset_schema}.

The \texttt{Dataset} class encapsulates key attributes that describe dataset metadata, such as its name, the corresponding GitHub repository or reference paper, version, and a brief description providing additional context about the dataset. Similar to datasets in other domains, hint datasets are typically organized into subsets such as train, validation (or development), test, etc. The number and type of subsets depend on the specific dataset. For instance, TriviaHG~\cite{mozafari-triviahg} includes fourteen subsets, whereas KG-Hint~\cite{jatowt_kg_hint} only provides a test subset. The \texttt{.subsets} attribute represents these subsets for each dataset.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{images/Schema.pdf}
  \caption{Schema of the Dataset class, illustrating the objects used to represent a dataset in \framework. The arrows indicate a subclass relationship.}
  \label{fig:dataset_schema}
\end{figure}

Each subset comprises multiple instances, accessible using unique IDs referred to as \texttt{q\_id}. At the core of the dataset are these instances, which can be accessed through the \texttt{.instances} attribute of the subset. Each instance includes a question, its corresponding answers, and a set of hints. A question may have multiple valid answers, for example, the question \emph{Which country is the University of Harvard located in?} can be answered with \emph{USA}, \emph{United States}, or \emph{United States of America}. Figure~\ref{fig:hint_sample} illustrates an instance comprising a question, its 2 answers, and five hints.

The attributes \texttt{.question}, \texttt{.answers}, and \texttt{.hints} within an instance correspond to the \texttt{Question}, \texttt{Answer}, and \texttt{Hint} objects.

\input{tables/table_1}

The \texttt{Question} class represents various attributes associated with a question. The content of the question is stored in the \texttt{.question} attribute, while the \texttt{.question\_type} attribute specifies the type of the question, which is classified into major and minor types based on the classification schema introduced in the TREC Question Classification task by~\citet{10.3115/1072228.1072378}. Additionally, the \texttt{.entities} attribute contains a list of entities present in the question, and the \texttt{.metrics} attribute stores the evaluated metrics for the question. \framework includes a built-in function to automatically detect and assign question types to each question based on the method presented by~\citet{tayyar-madabushi-lee-2016-high}.

The \texttt{Answer} class represents the attributes related to an answer. The content of the answer is stored in the \texttt{.answer} attribute, while the \texttt{.entities} attribute lists entities associated with the answer. The \texttt{.metrics} attribute holds the values of the evaluation metrics specific to the answer.

The \texttt{Hint} class represents the attributes related to a hint. The content of the hint is contained in the \texttt{.hint} attribute, and the \texttt{.source} attribute indicates the source of the hint. Similar to the \texttt{Question} and \texttt{Answer} classes, the \texttt{.entities} attribute lists entities included in the hint, and the \texttt{.metrics} attribute stores the evaluated metrics for the hint.

The \texttt{.entities} and \texttt{.metrics} attributes are shared across the \texttt{Question}, \texttt{Answer}, and \texttt{Hint} classes. The \texttt{.entities} attribute contains a list of \texttt{Entity} objects, where each object is an instance of the \texttt{Entity} class. An \texttt{Entity} object consists of the content of the entity stored in the \texttt{.entity} attribute, the type of the entity in \texttt{.ent\_type}, and its position in the text specified by the \texttt{start\_index} and \texttt{end\_index} attributes. The entity types align with the Named Entity Recognition (NER) types defined in spaCy\footnote{\url{https://spacy.io/}}. \framework provides a built-in function to extract named entities from each question, answer, and hint by leveraging the spaCy NER pipeline.

The \texttt{.metrics} attribute is a dictionary containing the evaluation results for various metrics, with each metric represented as an instance of the \texttt{Metric} class. A \texttt{Metric} object consists of the name of the metric and its corresponding value.

\lstinputlisting[
    float,
	caption={This script retrieves the latest information about the available datasets, returns it as a \texttt{dict}, and also displays metadata for each dataset in the terminal.},
	label={lst:available_datasets},
	language=Python,
]{scripts/available_datasets.py}

\lstinputlisting[
    float,
	caption={This script creates a user-defined dataset named \emph{own\_dataset} consisting of two instances. Each instance contains one question (\texttt{Q}), two answers (\texttt{A}), and three hints (\texttt{H}). Both instances belong to the \emph{test} subset.},
	label={lst:create_dataset},
	language=Python,
]{scripts/create_dataset.py}

All classes also include a \texttt{.metadata} attribute designed to store additional dataset-specific information and features. For instance, WikiHint~\cite{2024arXiv241201626M} includes a unique ranking feature for each hint, stored in the \texttt{.metadata} attribute of the corresponding hints.  


The \framework provides preprocessed hint datasets from several prior studies~\cite{mozafari-triviahg, mozafari-hintqa, jatowt_kg_hint}, simplifying usage with just a few lines of code. At the time of writing this paper, we have collected, processed, and converted the available hint datasets into the \texttt{Dataset} class, enabling users to easily download and utilize them. Table~\ref{tbl:preproccesed_datasets} presents the statistics of the hint datasets included in \framework. To explore the available preprocessed datasets and retrieve information about them, users can use the script provided in Listing~\ref{lst:available_datasets}. To access the newest preprocessed datasets, simply set \texttt{update} to \texttt{True}; there is no need to upgrade \framework to the latest version.

Users have also the flexibility to create their own datasets from scratch using the \texttt{Dataset} class, in addition to utilizing preprocessed datasets. Listing~\ref{lst:create_dataset} provides a script demonstrating how to create a custom dataset. For comprehensive instructions, we recommend referring to the online documentation.

To import or export datasets, whether they are preprocessed datasets provided by \framework or user-defined datasets, the \texttt{Dataset} class offers several convenient functions. The standard approach for storing datasets is by using pickle files, as they are compressed, easy to transport, and inherently provide a level of encryption, addressing security concerns. However, if a user prefers to export a dataset to a JSON file or import it from a JSON file, the \texttt{Dataset} class includes dedicated functions to simplify this process. Additionally, users can directly load available datasets by specifying their names. The script in Listing~\ref{lst:import_export} demonstrates how to efficiently load and store hint datasets.

\lstinputlisting[
    float,
	caption={This script downloads the TriviaHG~\cite{mozafari-triviahg} dataset and loads it in line 3. Lines 5 and 6 demonstrate how to load and store the dataset as a pickle file, while lines 8 and 9 show how to import and export the dataset as a JSON file.},
	label={lst:import_export},
	language=Python,
]{scripts/import_export.py}


\subsection{Models}\label{ss:models}
The current version of \framework includes two built-in models
% \footnote{\framework supports running LLM models both locally and remotely, providing flexibility based on user preferences and infrastructure.}
: \emph{Answer-Aware} and \emph{Answer-Agnostic}.

The Answer-Aware model generates hints for a question when the ground-truth answer is provided. In other words, to utilize this model, the gold answer to each question must be available and passed to the model. This approach offers both advantages and disadvantages. On the positive side, the quality of the generated hints is generally higher~\cite{2024arXiv241201626M}. However, its limitation lies in requiring the answer, which might not always be feasible for certain questions. The TriviaHG~\cite{mozafari-triviahg} dataset has been generated using this model.
In \framework, the Answer-Aware model is implemented as the \texttt{AnswerAware} class, which includes a \texttt{generate} function. This function accepts a list of \texttt{instance} objects and populates them with the generated hints. Note that these instance objects must contain the answers to the questions. 

The Answer-Agnostic model generates hints for a question without requiring the ground-truth answer. The primary advantage of this model is its applicability to questions where answers are unavailable or unclear. However, a limitation is the potentially lower quality of the generated hints, as the model might fail to identify the correct answer~\cite{2024arXiv241201626M}. The HintQA dataset~\cite{mozafari-hintqa} was generated using this model.
In \framework, the Answer-Agnostic model is implemented through the \texttt{AnswerAgnostic} class, which features a \texttt{generate} function. This function takes a list of \texttt{instance} objects as input and generates hints for them. It is worth mentioning that the WikiHint\cite{2024arXiv241201626M} dataset is generated using both the Answer-Aware and Answer-Agnostic models.

Listing~\ref{lst:model} illustrates how to use the answer-aware and answer-agnostic models to generate hints for questions. The script serves as a basic example with minimal parameters. We note that \framework supports running LLM models both locally and remotely, providing flexibility based on user preferences and infrastructure. For more detailed guidance on customizing parameters and leveraging advanced features, readers are encouraged to refer to the online documentation.

\lstinputlisting[
    float,
	caption={This script generates five hints (default value) for \texttt{instance\_1} and \texttt{instance\_2} using the LLaMA 3.1 8B model with both Answer-Agnostic and Answer-Aware approaches. Lines 4–7 prepare two sample instances, lines 9–10 generate hints using the Answer-Agnostic model, lines 12–13 add answers to the instances, and lines 15–16 generate hints using the Answer-Aware model.},
	label={lst:model},
	language=Python,
]{scripts/model.py}

The \texttt{AnswerAware} and \texttt{AnswerAgnostic} classes inherit from a base class called \texttt{Model}. To extend this base class, users must implement the \texttt{generate} function. \framework allows users to create their own models by extending the \texttt{Model} class, implementing the \texttt{generate} function, and seamlessly integrating their custom models into the \framework ecosystem.

\subsection{Evaluation}\label{ss:evaluation}
%As the name \framework suggests, the primary focus of the framework is on the evaluation task. Specifically, 
The Hint Evaluation task assesses hints using several metrics. At the time of writing, \framework\ implements five main evaluation metrics—Relevance, Readability, Convergence, Familiarity, and Answer Leakage—which collectively comprise \textbf{fifteen} evaluation methods and \textbf{thirty-five} sub-methods. These metrics are described as follows:

\begin{description}
\item[\textbf{Relevance}] evaluates the semantic relationship between the hint and the question, ensuring the hint is relevant.
\item[\textbf{Readability}] measures how easy or difficult it is to understand a hint or question. This metric is important to ensure that hints guide the user effectively and are understandable without causing confusion.
\item[\textbf{Convergence}]  evaluates how well the hints narrow down potential answers to the question. In other words, it assesses how effectively the hints guide the user toward eliminating incorrect answers and focusing on the correct one.
\item[\textbf{Familiarity}] measures how common or well-known the information in the hints, questions, or answers is. It assesses whether the content is likely to be understood by the general public, making it easier for users to grasp the provided hints without needing specialized knowledge.
\item[\textbf{AnswerLeakage}] measures the extent to which a hint directly discloses the answer. It ensures that hints guide users without explicitly revealing the solutions. This metric is useful for evaluating whether the hints are subtle enough to assure problem-solving rather than giving away the answer.
\end{description}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{images/Evaluators.pdf}
  \caption{Evaluation metrics included in the \framework framework. \textcolor[rgb]{0.15, 0.26, 0.44}{Dark blue boxes} denote the primary evaluation metrics, \textcolor[rgb]{0.44, 0.60, 0.15}{green boxes} indicate methods associated with each metric implemented from scratch in \framework, and \textcolor[rgb]{0.44, 0.72, 0.92}{light blue boxes} highlight methods adopted from prior studies.}
  \label{fig:evaluators}
\end{figure}

\lstinputlisting[
    float,
	caption={This script shows the evaluation of five metrics for two instances, \texttt{instance\_1} and \texttt{instance\_2}. It uses \emph{rougeL} for relevance (Line 13), \emph{XGBoost} for readability (Line 14), \emph{Bert-base} for convergence (Line 15), \emph{Wikipedia} for familiarity (Line 16), and \emph{Lexical} for answer leakage (Line 17).},
	label={lst:evaluator},
	language=Python,
]{scripts/evaluate.py}

\input{tables/table_2}

Some of these evaluation metrics were initially introduced by~\citet{mozafari-triviahg}, who proposed automatic methods only for Convergence and Familiarity. However, they have not introduced any automatic methods for Relevance, Readability, or Answer Leakage nor have they evaluated the TriviaHG dataset for these metrics. Subsequently,~\citet{2024arXiv241201626M} introduced automatic methods for Relevance, Readability, and AnswerLeakage. Despite these advancements, the existing methods faced challenges, including high computational load, reliance on large language models, and complex codebases tailored to specific dataset schema and structures.

To address these issues, \framework re-implements all metrics in a generic manner, ensuring they work seamlessly with any dataset based on the \texttt{Dataset} class. This allows users to define their datasets using the \texttt{Dataset} class and to easily apply the evaluation metrics with minimal code.
% \footnote{\framework supports running LLM models for evaluation metrics either locally or remotely.}

In addition to re-implementing metrics from the previous studies, \framework introduces a range of new methods for evaluating each metric, spanning from simple and lightweight approaches to more complex and resource-intensive ones. This design allows users to select methods based on available resources
%. There is 
with a trade-off between the simplicity and accuracy. 

Figure~\ref{fig:evaluators} provides an overview of all evaluation metrics and their methods included in \framework. Table~\ref{tbl:metrics_comparison} compares these methods for evaluation metrics across various features, while Table~\ref{tbl:evaluated_metrics} presents the average values of the evaluated metrics across different datasets and their subsets for all the metrics and their methods. Moreover, Listing~\ref{lst:evaluator} demonstrates how users can apply the evaluation metrics to their instances and datasets. The following sections provide detailed descriptions of the methods used for each evaluation metric:

\input{tables/table_3}

\subsubsection{Relevance}\label{ss:relevance}
The \emph{Rouge} method~\cite{lin-2004-rouge} evaluates the overlap between the evaluated hints and questions. It includes Rouge-1, which measures unigram overlap for word-level similarity, Rouge-2, which measures bigram overlap for span-level similarity, and Rouge-L, which evaluates the longest common subsequence for sequence alignment.
The \emph{Non-Contextual} (NonCtx) method~\cite{2019arXiv190901059M} computes relevance by measuring the similarity between hints and questions using fixed word embeddings. This method has two variants, Glove 6B~\cite{pennington-etal-2014-glove}, trained on 6 billion tokens from diverse text sources, and Glove 42B~\cite{pennington-etal-2014-glove}, trained on 42 billion tokens for more robust embeddings\footnote{On the WikiQA dataset~\cite{yang-etal-2015-wikiqa}, the MAP and MRR for the Glove 6B-based method are 80.64\% and 85.71\%, respectively, while for the Glove 42B-based method, they are 82.14\% and 86.47\%.}.
The \emph{Contextual} (Ctx) method~\cite{laskar-etal-2020-contextualized} uses embeddings from pre-trained transformer models~\cite{BasMozafari} to evaluate relevance between hints and questions, capturing deeper relationships between words through contextual understanding. Two supported variants are BERT-base~\cite{devlin-etal-2019-bert, 9122302} and RoBERTa-large\footnote{On the WikiQA dataset~\cite{yang-etal-2015-wikiqa}, the MAP and MRR for BERT-base are 91.2\% and 94.4\%, respectively, while for RoBERTa-large, they are 94.6\% and 97.4\%.}~\cite{2019arXiv190711692L}.  
The \emph{LLM} method~\cite{es-etal-2024-ragas} measures relevance of a hint to a question using the Answer Relevancy metric. In this approach, the hint is treated as an answer and the question as a prompt, with LLMs acting as a judge.

\subsubsection{Readability}\label{ss:readability}
The \emph{Traditional}~\cite{lee-lee-2023-lftk} method evaluates readability using classic formulas based on sentence length, word length, and complexity. The supported methods include the Gunning Fog Index~\cite{Gunning1952}, Flesch Reading Ease~\cite{rudolf_franz_flesch_1948}, Coleman-Liau Index~\cite{Coleman1975}, SMOG Index~\cite{d9397c09-9d7e-3784-b191-6efaa0fd35d0}, and Automated Readability Index~\cite{Senter1967}, which estimate readability levels required to understand the text.
The \emph{Machine Learning} (ML)~\cite{liu-lee-2023-hybrid} method evaluates readability using trained models that predict scores based on text features. These models are trained on the OneStopEnglish (OSE) dataset~\cite{vajjala-lucic-2018-onestopenglish}, which maps texts to specific readability levels including Beginner (0), Intermediate (1), and Advanced (2). Supported methods include XGBoost~\cite{10.1145/2939672.2939785}, a gradient boosting algorithm known for speed and accuracy, and Random-Forest~\cite{Breiman2001}, an ensemble method that builds multiple decision trees to improve prediction accuracy\footnote{The accuracy and F1 scores for Random-Forest are 46.61\% and 44.66\%, respectively, while for XGBoost, they are 48.22\% and 46.59\%.}.
The \emph{Neural Network} (NN)~\cite{liu-lee-2023-hybrid} method uses pre-trained transformer models to evaluate readability by capturing the deeper context and structure of text for nuanced analysis. Supported methods include BERT-base and RoBERTa-large\footnote{On the OneStopEnglish  dataset~\cite{vajjala-lucic-2018-onestopenglish}, the accuracy and F1 scores for BERT-base are 59.26\% and 58.2\%, respectively, while for RoBERTa-large, they are 62.03\% and 61.61\%.}.
The \emph{LLM}~\cite{naous-etal-2024-readme} method evaluates readability by leveraging large language models. These models provide highly accurate and context-aware readability scores, offering a deeper understanding of how easily text can be comprehended.

\subsubsection{Convergence}\label{ss:convergence}
The \emph{Specificity} (Spec)~\cite{10.1145/3477495.3531734} method evaluates the degree to which a hint is specific or general. The supported methods include BERT-base and RoBERTa-large, trained on the dataset from~\citet{Ko_Durrett_Li_2019}, which has been designed to detect the specificity or generality of a sentence\footnote{The accuracy and F1 scores for BERT-base are 81.56\% and 81.55\%, respectively, while for RoBERTa-large, they are 83.64\% and 83.63\%.}.
The \emph{Neural Network} (NN) method utilizes pre-trained transformer models to assess how effectively a hint narrows down possible answers. Here, the supported methods include BERT-base and RoBERTa-large, which were fine-tuned with the convergence values of the hints from the TriviaHG~\cite{mozafari-triviahg} training set and evaluated on its test set\footnote{The Pearson correlation between the ground-truth values and the predicted values is 56.1\% for BERT-base and 61.1\% for RoBERTa-large.}.
The \emph{LLM}~\cite{mozafari-triviahg} method utilizes large language models to generate candidate answers for a given question. It then evaluates the relationship between each hint and the candidate answers to calculate the convergence value. The supported models include LLaMA-3-8B~\cite{2024arXiv240721783D} and LLaMA-3-70B~\cite{2024arXiv240721783D}.

\subsubsection{Familiarity}\label{ss:familiarity}
The \emph{Word Frequency} (Freq) method evaluates text familiarity by analyzing word frequency using the C4~\cite{dodge-etal-2021-documenting} corpus as a reference. This approach provides insights into how frequently words are used in everyday language. By normalizing the frequency of each word, it generates a value between 0 and 1 to represent the familiarity of each word. This method then calculates the average of these values to produce a familiarity score for each sentence. Supported variants include an approach with stop-words, which considers all words, and one without stop-words, which excludes them to focus on more meaningful terms.
The \emph{Wikipedia} (Wiki)~\cite{mozafari-triviahg} method, on the other hand, assesses familiarity by analyzing the popularity of entities mentioned in the text. It uses the number of views of corresponding Wikipedia pages to determine how well-known the referenced people, places, or concepts are to the general public.

\subsubsection{AnswerLeakage}\label{ss:answer_leakage}
The \emph{Lexical} (Lex) method evaluates the similarity between the hint and the answer at the word level, focusing on explicit word overlap without considering deeper contextual meaning. The supported variants include an approach with stop-words, which is more permissive, and one without stop-words, which is more precise in identifying relevant overlap.
The \emph{Contextual} (Ctx)~\cite{2024arXiv241201626M} method uses embeddings to evaluate semantic similarity between the hint and the answer, capturing nuanced relationships even when different words convey the same idea. It computes similarity scores for each pair of words using contextualized word embeddings and selects the maximum similarity as the final value. This method supports SentenceBERT~\cite{reimers-gurevych-2019-sentence} models for generating contextualized word embeddings.

Evaluation metric classes in \framework are built upon a base class called \texttt{Evaluation}. Users can extend this base class by defining the \texttt{evaluate} function. This design empowers users to create custom evaluation metrics and methods by inheriting from the \texttt{Evaluation} class, implementing the required \texttt{evaluate} function, and seamlessly integrating their custom solutions into \framework.

\section{Conclusion}\label{s:conclusion}
We introduce \framework, the first framework for Hint Generation and Evaluation designed to address fragmented resources and inconsistent evaluation practices. By unifying datasets, models, and evaluation metrics into a single toolkit, \framework provides standardized methodologies, flexibility for diverse research requirements, and tools that ensure reproducibility and consistency. It supports five core metrics—Relevance, Readability, Convergence, Familiarity, and Answer Leakage—offering methods that range from lightweight to resource-intensive, catering to varied user needs.

\framework simplifies workflows for hint-oriented research, bridging gaps in resources and fostering advancements in question-answering, and problem-solving systems. Its open-source nature and comprehensive documentation make it a useful tool for the IR and NLP communities. Researchers can leverage \framework to develop innovative hint-based systems while maintaining evaluation consistency. By encouraging collaboration and standardization, the framework aims to create a foundation for future breakthroughs in hint generation and question answering research. 

Future efforts will focus on expanding the framework with additional datasets, models, and metrics, improving compatibility with emerging language models, and introducing advanced techniques for hint generation and evaluation to further drive innovation and support critical thinking, active learning, and user engagement.


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
