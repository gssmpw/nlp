\section{Related Work}
\subsection{Molecular Representation Learning}
Traditional molecular modeling approaches have predominantly relied on specialized architectures that directly operate on molecular structures for tasks such as property prediction~\cite{guo2021few,stark20223d}, molecule generation~\cite{gong2024text,kim2024data}, and reaction prediction~\cite{liu2024reactxt}. With the advent of the transformer architecture~\cite{vaswani2017attention}, the field has witnessed a shift towards representation learning through pre-training and fine-tuning paradigms. Early transformer-based approaches focused on learning from SMILES~\cite{weininger1988smiles} string representations. For example, MolBERT~\cite{li2021mol} adapted the BERT~\cite{Devlin2019BERTPO} architecture to recognize different SMILES~ string representations of compounds, while ChemBERTa~\cite{chithrananda2020chemberta} employed masked language modeling (MLM) on text-SMILES datasets. More recent approaches have explored richer molecular representations and transfer learning. MolT5~\cite{edwards_translation_2022} finetunes a pre-triend T5 language model for moleculecular translation. %\suhang{MolT5~\cite{edwards_translation_2022} finetunes a pre-triend general-purpose language models XXX for xxx}, 
MolCA~\cite{liu_molca_2023} introduced a cross-model projector to effectively fine-tune LLMs on select downstream tasks, %\suhang{then why not use MolCA for molecular graph representation learning in ICL??? You might want to either delete this or write it in a way that will not lead reviewers to ask the question I raised, e.g., explicitly write done fine-tuning LLM part}
while 3D MolM enhanced existing datasets by incorporating 3D conformational information generated using GPT-3.5.

Despite their effectiveness in molecular representation learning and analysis, these pre-training and fine-tuning approaches face the following limitations:
\begin{inparaenum}[(a)]
    \item requirements for significant computational resources during pre-training,
    \item need for task-specific fine-tuning and separate training for each task, and
    \item limited flexibility in adapting to new molecular tasks.
\end{inparaenum}
\subsection{In-Context Learning for Molecular Tasks}
ICL has emerged as a promising alternative for the pre-train/fine-tune paradigm, enabling general-purpose language models to perform various tasks through demonstration-based prompting. Instead of fine-tuning, ICL provides demonstrations in the prompt, which allows the LLM to learn from them and generate more accurate responses.   
Despite the effectiveness of ICLs in various applications~\cite{dong2022survey}, the work on ICL for molecular tasks is still in its early stage and there are very few works~\cite{li_empowering_2024,guo_what_2023}. %~\suhang{can we add this: The work on in-context learning for molecular tasks is still in its early stage and there are very few works~(cite these works)} 
Recently, MoleReGPT~\cite{li_empowering_2024} introduced dual approaches for molecular tasks. For molecular captioning, MoleReGPT utilizes Morgan fingerprint similarity, i.e., Scaffold, which compares the presence of specific substructures encoded in the Morgan fingerprint vector. %\suhang{and encode them in a vector???} encoded in a vector, i.e. Morgan fingerprint. %, and for molecule generation, it uses BM25 %\suhang{can you introduce more details of Morgan similarity and probably Scaffold since we used it as a baselines???} 
Guo et al.~\cite{guo_what_2023} established a benchmark across eight molecular tasks, evaluating various LLMs using random and scaffold-based sample selection. However, existing ICL approaches for molecular tasks have several limitations:
\begin{inparaenum}[(a)]
    \item insufficient capture of bond connectivity and atomic features present in molecular graphs,
    \item limited exploration of graph-aware contrastive learning for demonstration selection and
    \item primarily focus on large and commercial language models, such as GPT4.
\end{inparaenum}

While GNNs have demonstrated promise in capturing molecular structure in fine-tuned model such as  MolCA~\cite{liu_molca_2023}, their potential for enhancing ICL demonstration selection remains underexplored. Our work addresses this gap by introducing \texttt{GAMIC}, the first approach to leverage Morgan-based graph alignment, achieving SOTA performance on benchmark molecular ICL tasks. This novel direction addresses the limitations of existing methods while maintaining computational efficiency central to the ICL paradigm.

% \subsection{Pretrained Models}

% Traditional molecular modeling approaches have predominantly relied on specialized architectures that directly operate on molecular structures for tasks such as molecule property prediction~\cite{}, molecule discovery~\cite{}, and predicting the outcome of reactions between various molecules~\cite{}. The advent of the Transformer~\cite{} has motivated various undertakings to apply a representation learning paradigm of pretraing-finetuning to various molecular tasks. MolBert~\cite{} used BERT architecture to train against a large set of compounds and train it to recognize different representations of a SMILES~\cite{} string. ChemBERTa~\cite{} utilizes a text-SMILES dataset similarly using a masked language-modeling (MLM) based approach. 

% On the other, more recent approaches such as MolT5~\cite{} leverage the pre-training provided to general-purpose language models as a starting point for the additional subject-specific pre-training. This has demonstrated a higher capability in more specialized tasks, specifically molecule-text translation. MolCA~\cite{liu_molca_2023} utilized a graph-based encoder to capture the molecule and connect it to a textual representation. 3D Molm utilized a large pertaining set generated using GPT 3.5 to enhance the existing dataset to align graph conformation in 3D. However, methods based on the pretrain-finetung paradigm require significant pre-training and targeted fine-tuning for each task. Additionally, research will have to train individual models for each task separately. 

% \subsection{In-Context Learning}
% In-context learning is a recent paradigm that has demonstrated a capacity to train a general-purpose LM by including a list of demonstrations in the prompt. This affords LMs the flexibility to perform various tasks, while avoiding the computational requirements of pertaining, and fine-tuning. MoleReGPT~\cite{li_empowering_2024} applies two different approaches for Cap2Mol and Mol2Cap, namely Morgan similarity and BM25, respectively. Guo et al~\cite{guo_what_2023} published a benchmark on eight molecular tasks and provided results using multiple LMs using random and scaffold-based sample selection. However, neither approach captures the bond connectivity and atomic features of the graph representation of a molecule. While Graph Neural Network shows the promising ability to capture bond connectivity and atomic features, no research has explored the usefulness of molecular ICL. Prior research has enhanced model performance using graph-aware contrastive learning approaches, but they fail to explore its usefulness for ICL, particularly for few-shot demonstration sampling. Additionally, cutting-edge research fails to apply robust heuristic technique such MMR-based which as capable of further improving ICL demonstration selection.  To the best of your knowledge, we are the first to explore this direct for the task of molecular ICL.