@article{jin2018learning,
  title={Learning multimodal graph-to-graph translation for molecular optimization},
  author={Jin, Wengong and Yang, Kevin and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:1812.01070},
  year={2018}
}
@inproceedings{guo2021few,
  title={Few-shot graph learning for molecular property prediction},
  author={Guo, Zhichun and Zhang, Chuxu and Yu, Wenhao and Herr, John and Wiest, Olaf and Jiang, Meng and Chawla, Nitesh V},
  booktitle={Proceedings of the web conference 2021},
  pages={2559--2567},
  year={2021}
}
@article{weininger1988smiles,
  title={SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author={Weininger, David},
  journal={Journal of chemical information and computer sciences},
  volume={28},
  number={1},
  pages={31--36},
  year={1988},
  publisher={ACS Publications}
}
@article{chithrananda2020chemberta,
  title={ChemBERTa: large-scale self-supervised pretraining for molecular property prediction},
  author={Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  journal={arXiv preprint arXiv:2010.09885},
  year={2020}
}
@article{li2021mol,
  title={Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction},
  author={Li, Juncai and Jiang, Xiaofei},
  journal={Wireless Communications and Mobile Computing},
  volume={2021},
  number={1},
  pages={7181815},
  year={2021},
  publisher={Wiley Online Library}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@inproceedings{edwards2021text2mol,
  title={Text2mol: Cross-modal molecule retrieval with natural language queries},
  author={Edwards, Carl and Zhai, ChengXiang and Ji, Heng},
  booktitle={EMNLP},
  year={2021}
}
@inproceedings{jiang2024enhancing,
  title={Enhancing Cross Text-Molecule Learning by Self-Augmentation},
  author={Jiang, Yinuo and Zhuang, Xiang and Ding, Keyan and Zhang, Qiang and Chen, Huajun},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9551--9565},
  year={2024}
}
@article{shi2024prediction,
  title={Prediction of chemical reaction yields with large-scale multi-view pre-training},
  author={Shi, Runhan and Yu, Gufeng and Huo, Xiaohong and Yang, Yang},
  journal={Journal of Cheminformatics},
  volume={16},
  number={1},
  pages={22},
  year={2024},
  publisher={Springer}
}
@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}
@article{liu2023molxpt,
  title={Molxpt: Wrapping molecules with text for generative pre-training},
  author={Liu, Zequn and Zhang, Wei and Xia, Yingce and Wu, Lijun and Xie, Shufang and Qin, Tao and Zhang, Ming and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2305.10688},
  year={2023}
}
@article{tong2022blood,
  title={Blood--brain barrier penetration prediction enhanced by uncertainty estimation},
  author={Tong, Xiaochu and Wang, Dingyan and Ding, Xiaoyu and Tan, Xiaoqin and Ren, Qun and Chen, Geng and Rong, Yu and Xu, Tingyang and Huang, Junzhou and Jiang, Hualiang and others},
  journal={Journal of Cheminformatics},
  volume={14},
  number={1},
  pages={44},
  year={2022},
  publisher={Springer}
}
@article{liu2024reactxt,
    title={ReactXT: Understanding Molecular “Reaction-ship” via Reaction-Contextualized Molecule-Text Pretraining},
    author={Liu, Zhiyuan and Shi, Yaorui and Zhang, An and Li, Sihang and Zhang, Enzhi and Wang, Xiang and Kawaguchi, Kenji and Chua, Tat-Seng},
    booktitle={Findings of the Association for Computational Linguistics: {ACL} 2024},
    publisher={Association for Computational Linguistics},
    year={2024},
    url={https://openreview.net/forum?id=V-ejDfLiwe}
}
@article{kim2024data,
  title={Data-efficient molecular generation with hierarchical textual inversion},
  author={Kim, Seojin and Nam, Jaehyun and Yu, Sihyun and Shin, Younghoon and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2405.02845},
  year={2024}
}
@inproceedings{gong2024text,
  title={Text-guided molecule generation with diffusion language model},
  author={Gong, Haisong and Liu, Qiang and Wu, Shu and Wang, Liang},
  booktitle={AAAI},
  volume={38},
  number={1},
  pages={109--117},
  year={2024}
}
@inproceedings{stark20223d,
  title={3d infomax improves gnns for molecular property prediction},
  author={St{\"a}rk, Hannes and Beaini, Dominique and Corso, Gabriele and Tossou, Prudencio and Dallago, Christian and G{\"u}nnemann, Stephan and Li{\`o}, Pietro},
  booktitle={International Conference on Machine Learning},
  pages={20479--20502},
  year={2022},
  organization={PMLR}
}
@article{rogers2010extended,
  title={Extended-connectivity fingerprints},
  author={Rogers, David and Hahn, Mathew},
  journal={Journal of chemical information and modeling},
  volume={50},
  number={5},
  pages={742--754},
  year={2010},
  publisher={ACS Publications}
}
@misc{qian_can_2023,
	title = {Can {Large} {Language} {Models} {Empower} {Molecular} {Property} {Prediction}?},
	abstract = {Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at {\textbackslash}url\{https://github.com/ChnQ/LLM4Mol\}.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Qian, Chen and Tang, Huayi and Yang, Zhirui and Liang, Hong and Liu, Yong},
	month = jul,
	year = {2023},
	note = {arXiv:2307.07443 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\RI48AC5P\\2307.html:text/html;Qian et al_2023_Can Large Language Models Empower Molecular Property Prediction.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\2UXHMJ6E\\Qian et al_2023_Can Large Language Models Empower Molecular Property Prediction.pdf:application/pdf},
}
@inproceedings{NEURIPS2020_1457c0d6,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{scarselli2008graph,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}
@inproceedings{das-etal-2021-case,
    title = "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
author = "Das, Rajarshi  and
      Zaheer, Manzil  and
      Thai, Dung  and
      Godbole, Ameya  and
      Perez, Ethan  and
      Lee, Jay Yoon  and
      Tan, Lizhen  and
      Polymenakos, Lazaros  and
      McCallum, Andrew",
    booktitle = "EMNLP",
    month = nov,
    year = "2021",

}
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}
@article{hadi2023survey,
  title={A survey on large language models: Applications, challenges, limitations, and practical usage},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}
@article{lim2020scaffold,
  title={Scaffold-based molecular design with a graph generative model},
  author={Lim, Jaechang and Hwang, Sang-Yeon and Moon, Seokhyun and Kim, Seungsu and Kim, Woo Youn},
  journal={Chemical science},
  volume={11},
  number={4},
  pages={1153--1164},
  year={2020},
  publisher={Royal Society of Chemistry}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{wu_moleculenet_2018,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{guo_what_2023,
  title={What can large language models do in chemistry? a comprehensive benchmark on eight tasks},
  author={Guo, Taicheng and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh and Wiest, Olaf and Zhang, Xiangliang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59662--59688},
  year={2023}
}

@misc{balaji_gpt-molberta_2023,
	title = {{GPT}-{MolBERTa}: {GPT} {Molecular} {Features} {Language} {Model} for molecular property prediction},
	shorttitle = {{GPT}-{MolBERTa}},
	abstract = {With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention mechanisms show that GPT-MolBERTa is able to pick up important information from the input textual data, displaying the interpretability of the model.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Balaji, Suryanarayanan and Magar, Rishikesh and Jadhav, Yayati and Farimani, Amir Barati},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03030 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\ZPJDTTJV\\2310.html:text/html;Balaji et al_2023_GPT-MolBERTa.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\EB7V2IXZ\\Balaji et al_2023_GPT-MolBERTa.pdf:application/pdf},
}

@misc{liu_scientific_2024,
	title = {Scientific {Language} {Modeling}: {A} {Quantitative} {Review} of {Large} {Language} {Models} in {Molecular} {Science}},
	shorttitle = {Scientific {Language} {Modeling}},
	abstract = {Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Liu, Pengfei and Tao, Jun and Ren, Zhixiang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04119 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},

}

@misc{he_g-retriever_2024,
	title = {G-{Retriever}: {Retrieval}-{Augmented} {Generation} for {Textual} {Graph} {Understanding} and {Question} {Answering}},
	shorttitle = {G-{Retriever}},
	abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination. (Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
	month = mar,
	year = {2024},
	note = {arXiv:2402.07630 [cs]},
	keywords = {Computer Science - Machine Learning},

}
@inproceedings{Beltagy2019SciBERT,
  title={SciBERT: Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  year={2019},
  booktitle={EMNLP},
  Eprint={arXiv:1903.10676}
}
@inproceedings{li2024molm,
    title={3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models},
    author={Li, Sihang and Liu, Zhiyuan and Luo, Yanchen and Wang, Xiang and He, Xiangnan and Kawaguchi, Kenji  and Chua, Tat-Seng and Tian, Qi},
    booktitle={ICLR},
    year={2024},
    url={https://openreview.net/forum?id=xI4yNlkaqh}
}
@article{kipf2016variational,
  title={Variational graph auto-encoders},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1611.07308},
  year={2016}
}
@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@misc{pei_leveraging_2024,
	title = {Leveraging {Biomolecule} and {Natural} {Language} through {Multi}-{Modal} {Learning}: {A} {Survey}},
	shorttitle = {Leveraging {Biomolecule} and {Natural} {Language} through {Multi}-{Modal} {Learning}},
	abstract = {The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in {\textbackslash}url\{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling\}.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Pei, Qizhi and Wu, Lijun and Gao, Kaiyuan and Zhu, Jinhua and Wang, Yue and Wang, Zun and Qin, Tao and Yan, Rui},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01528 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Quantitative Biology - Biomolecules},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\ET4UHINW\\2403.html:text/html;Pei et al_2024_Leveraging Biomolecule and Natural Language through Multi-Modal Learning.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\7Q9W3BLK\\Pei et al_2024_Leveraging Biomolecule and Natural Language through Multi-Modal Learning.pdf:application/pdf},
}

@article{li_empowering_2024,
  title={Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective},
  author={Li, Jiatong and Liu, Yunqing and Fan, Wenqi and Wei, Xiao-Yong and Liu, Hui and Tang, Jiliang and Li, Qing},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@misc{bran_chemcrow_2023,
	title = {{ChemCrow}: {Augmenting} large-language models with chemistry tools},
	shorttitle = {{ChemCrow}},
	abstract = {Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Bran, Andres M. and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D. and Schwaller, Philippe},
	month = oct,
	year = {2023},
	note = {arXiv:2304.05376 [physics, stat]},
	keywords = {Statistics - Machine Learning, Physics - Chemical Physics},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\GLIZLLL5\\2304.html:text/html;Bran et al_2023_ChemCrow.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\VYTV4Z4N\\Bran et al_2023_ChemCrow.pdf:application/pdf},
}

@misc{ye_drugassist_2023,
	title = {{DrugAssist}: {A} {Large} {Language} {Model} for {Molecule} {Optimization}},
	shorttitle = {{DrugAssist}},
	abstract = {Recently, the impressive performance of large language models (LLMs) on a wide range of tasks has attracted an increasing number of attempts to apply LLMs in drug discovery. However, molecule optimization, a critical task in the drug discovery pipeline, is currently an area that has seen little involvement from LLMs. Most of existing approaches focus solely on capturing the underlying patterns in chemical structures provided by the data, without taking advantage of expert feedback. These non-interactive approaches overlook the fact that the drug discovery process is actually one that requires the integration of expert experience and iterative refinement. To address this gap, we propose DrugAssist, an interactive molecule optimization model which performs optimization through human-machine dialogue by leveraging LLM's strong interactivity and generalizability. DrugAssist has achieved leading results in both single and multiple property optimization, simultaneously showcasing immense potential in transferability and iterative optimization. In addition, we publicly release a large instruction-based dataset called MolOpt-Instructions for fine-tuning language models on molecule optimization tasks. We have made our code and data publicly available at https://github.com/blazerye/DrugAssist, which we hope to pave the way for future research in LLMs' application for drug discovery.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Ye, Geyan and Cai, Xibao and Lai, Houtim and Wang, Xing and Huang, Junhong and Wang, Longyue and Liu, Wei and Zeng, Xiangxiang},
	month = dec,
	year = {2023},
	note = {arXiv:2401.10334 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\58CMESA9\\2401.html:text/html;Ye et al_2023_DrugAssist.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\YV4Y7KIE\\Ye et al_2023_DrugAssist.pdf:application/pdf},
}

@inproceedings{liu_molca_2023,
    title={MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter},
    author={Liu, Zhiyuan and Li, Sihang and Luo, Yanchen and Fei, Hao and Cao, Yixin and Kawaguchi, Kenji and Wang, Xiang and Chua, Tat-Seng},
    booktitle={EMNLP},
    year={2023},
    url={https://openreview.net/forum?id=14WRhMNq7H}
}
@article{song2024towards,
  title={Towards Cross-Modal Text-Molecule Retrieval with Better Modality Alignment},
  author={Song, Jia and Zhuang, Wanru and Lin, Yujie and Zhang, Liang and Li, Chunyan and Su, Jinsong and He, Song and Bo, Xiaochen},
  journal={arXiv preprint arXiv:2410.23715},
  year={2024}
}
@article{su2022molecular,
  title={A molecular multimodal foundation model associating molecule graphs with natural language},
  author={Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2209.05481},
  year={2022}
}

@misc{li_towards_2024,
	title = {Towards {3D} {Molecule}-{Text} {Interpretation} in {Language} {Models}},
	abstract = {Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. We release our codes and datasets at https://github.com/lsh0520/3D-MoLM.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Li, Sihang and Liu, Zhiyuan and Luo, Yanchen and Wang, Xiang and He, Xiangnan and Kawaguchi, Kenji and Chua, Tat-Seng and Tian, Qi},
	month = mar,
	year = {2024},
	note = {arXiv:2401.13923 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Quantitative Biology - Biomolecules},
	file = {arXiv.org Snapshot:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\E2P97LCT\\2401.html:text/html;Li et al_2024_Towards 3D Molecule-Text Interpretation in Language Models.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\DVQAW7MP\\Li et al_2024_Towards 3D Molecule-Text Interpretation in Language Models.pdf:application/pdf},
}

@article{wu_moleculenet_2017,
	title = {{MoleculeNet}: a benchmark for molecular machine learning †{Electronic} supplementary information ({ESI}) available. {See} {DOI}: 10.1039/c7sc02664a},
	volume = {9},
	issn = {2041-6520},
	shorttitle = {{MoleculeNet}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5868307/},
	abstract = {A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms., Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
	number = {2},
	urldate = {2024-05-23},
	journal = {Chemical Science},
	author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
	month = oct,
	year = {2017},
	pmid = {29629118},
	pmcid = {PMC5868307},
	pages = {513--530},
	file = {Wu et al_2017_MoleculeNet.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\TVMN8YWC\\Wu et al_2017_MoleculeNet.pdf:application/pdf},
}

@article{pubchem,
  title={PubChem 2019 update: improved access to chemical data},
  author={Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},
  journal={Nucleic acids research},
  volume={47},
  number={D1},
  pages={D1102--D1109},
  year={2019},
  publisher={Oxford University Press}
}
@article{wang2021chemical,
  title={Chemical-reaction-aware molecule representation learning},
  author={Wang, Hongwei and Li, Weijiang and Jin, Xiaomeng and Cho, Kyunghyun and Ji, Heng and Han, Jiawei and Burke, Martin D},
  journal={ICLR},
  year={2022}
}
@inproceedings{zhang_motif-based_2021,
	title = {Motif-based {Graph} {Self}-{Supervised} {Learning} for {Molecular} {Property} {Prediction}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/85267d349a5e647ff0a9edcb5ffd1e02-Abstract.html},
	abstract = {Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being finetuned for specific tasks. However, most existing self-supervised pretraining frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs)  often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pretraining framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.},
	urldate = {2024-08-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {ZHANG, ZAIXI and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Chee-Kong},
	year = {2021},
	pages = {15870--15882},
	file = {ZHANG et al_2021_Motif-based Graph Self-Supervised Learning for Molecular Property Prediction.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\NAWYWV53\\ZHANG et al_2021_Motif-based Graph Self-Supervised Learning for Molecular Property Prediction.pdf:application/pdf},
}
@article{gat,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}
@article{ahneman2018predicting,
  title={Predicting reaction performance in C--N cross-coupling using machine learning},
  author={Ahneman, Derek T and Estrada, Jes{\'u}s G and Lin, Shishi and Dreher, Spencer D and Doyle, Abigail G},
  journal={Science},
  volume={360},
  number={6385},
  pages={186--190},
  year={2018},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{edwards_translation_2022,
	title = {Translation between {Molecules} and {Natural} {Language}},
	author = {Edwards, Carl and Lai, Tuan and Ros, Kevin and Honke, Garrett and Cho, Kyunghyun and Ji, Heng},
	month = nov,
	year = {2022},
	booktitle = "EMNLP",
}
@article{reizman2016suzuki,
  title={Suzuki--Miyaura cross-coupling optimization enabled by automated feedback},
  author={Reizman, Brandon J and Wang, Yi-Ming and Buchwald, Stephen L and Jensen, Klavs F},
  journal={Reaction chemistry \& engineering},
  volume={1},
  number={6},
  pages={658--666},
  year={2016},
  publisher={Royal Society of Chemistry}
}
@article{wang2023openchat,
  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={ICLR},
  year={2024}
}
@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={CoLM},
  year={2024}
}

@inproceedings{jin_hierarchical_2020,
	title = {Hierarchical {Generation} of {Molecular} {Graphs} using {Structural} {Motifs}},
	url = {https://proceedings.mlr.press/v119/jin20a.html},
	abstract = {Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.},
	language = {en},
	urldate = {2024-09-22},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jin, Wengong and Barzilay, Dr Regina and Jaakkola, Tommi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4839--4848},
	file = {Jin et al_2020_Hierarchical Generation of Molecular Graphs using Structural Motifs.pdf:C\:\\Users\\ITA223.490INFRA6JQJ\\Zotero\\storage\\CWSH8XFP\\Jin et al_2020_Hierarchical Generation of Molecular Graphs using Structural Motifs.pdf:application/pdf},
}


% BibTeX entries
@article{guo2023what,
  title={What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks},
  author={Guo, Taicheng and Guo, Kehan and Nan, Bozhao and Liang, Zhengwen and Guo, Zhichun and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2305.18365},
  year={2023}
}

@article{liu2022multimodal,
  title={Multi-modal molecule structure-text model for text-based retrieval and editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and others},
  journal={arXiv preprint arXiv:2212.10789},
  year={2022}
}
@article{wang2022molecular,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556/",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {\textquotedblleft}fantastic{\textquotedblright} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks."
}
@article{liu2023molca,
  title={MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter},
  author={Liu, Zhiyuan and Li, Sihang and Luo, Yanchen and others},
  journal={arXiv preprint arXiv:2310.12798},
  year={2023}
}
@article{bajusz2015tanimoto,
  title={Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations?},
  author={Bajusz, D{\'a}vid and R{\'a}cz, Anita and H{\'e}berger, K{\'a}roly},
  journal={Journal of cheminformatics},
  volume={7},
  pages={1--13},
  year={2015},
  publisher={Springer}
}
@article{lowe2017chemical,
  title={Chemical reactions from US patents (1976-Sep2016)},
  author={Lowe, Daniel},
  journal={Figshare},
  year={2017}
}

@article{wang2024comprehensive,
  title={A comprehensive survey of small language models in the era of large language models: Techniques, enhancements, applications, collaboration with llms, and trustworthiness},
  author={Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, Tzuhao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and others},
  journal={arXiv preprint arXiv:2411.03350},
  year={2024}
}

@article{kearnes2021open,
  title={The open reaction database},
  author={Kearnes, Steven M and Maser, Michael R and Wleklinski, Michael and others},
  journal={Journal of the American Chemical Society},
  volume={143},
  number={45},
  pages={18820--18826},
  year={2021},
  publisher={ACS Publications}
}