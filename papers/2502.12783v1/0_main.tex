\documentclass[10pt, conference, letterpaper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{url}
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
\usepackage{algorithmic}
\usepackage{algorithm} 
\usepackage[french,boxed,vlined,linesnumbered,inoutnumbered,rightnl,algo2e]{algorithm2e}
%\usepackage[algo2e]{algorithm2e} 
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{booktabs}
%\usepackage{subfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{threeparttable}
\usepackage{booktabs}
\newcommand*{\note}[1]{\textcolor{red}{#1}}
\newcommand*{\modify}[1]{\textcolor{blue}{#1}}
\newcommand{\algname}{FedHC\xspace}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\renewcommand{\algorithmiccomment}[1]{ $\triangleright$ #1}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\setlength{\abovecaptionskip}{0.05cm}
\begin{document}


%\title{An Adaptive Quantity Approach for Minimizing Cost and Delay in Satellite Clustered Federated Learning}

%\title{An Adaptive Quantization Approach for Cost and Latency Minimization in Satellite Clustered Federated Learning Framework \note{[TENTATIVE]}\\


%\title{An Adaptive Quantization Approach for Multi Objective Optimization in Satellite Clustered Federated Learning Framework}

%\title{Satellite Clustered Federated Learning Framework with Adaptive Quantization \note{[TENTATIVE]}\\

\title{\algname: A Hierarchical Clustered Federated Learning Framework for Satellite Networks \\

%\thanks{Identify applicable funding agency here. If none, delete this.}
}

%\author{Anonymous Authors}

%\begin{comment}
\author{\IEEEauthorblockN{Zhuocheng Liu$^{1}$, Zhishu Shen$^{1}$\textbf{\IEEEauthorrefmark{2}}\thanks{\IEEEauthorrefmark{2} Corresponding author (z\_shen@ieee.org).}, Pan Zhou$^{1}$, Qiushi Zheng$^{2}$, and Jiong Jin$^{2}$}

\IEEEauthorblockA{\textsuperscript{$^{1}$}School of Computer Science and Artificial Intelligence, Wuhan University of Technology, China\\
}


\IEEEauthorblockA{\textsuperscript{$^{2}$}School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Australia\\
}
%\IEEEauthorblockA{E-mail: \{\note{xxx}, \note{xxx}, yjl\}@whut.edu.cn, z\_shen@ieee.org, xxx@xxxx.com, jiongjin@swin.edu.au}
}
%\end{comment}

\maketitle

\begin{abstract}
%With the proliferation of data-driven services, the volume of data that needs to be processed by satellite networks has significantly increased. Federated learning (FL) is well-suited for handling big data processing at distributed, resource-constrained satellite environments. However, ensuring the convergence performance of FL models within dynamically distributed satellite networks is challenging while minimizing FL processing time and energy cost. To this end, we propose a hierarchical clustered federated learning framework named \algname. In this framework, a satellite-clustered parameter server (PS) selection algorithm is deployed at the cluster aggregation stage to partition nearby satellites into distinct clusters, designating the satellite cluster center client as the PS to accelerate the model aggregation. Several communicable cluster PS satellites are then selected through ground stations to aggregate global parameters to support the FL process. Moreover, a meta-learning-driven satellite re-clustering algorithm is introduced to enhance adaptability to dynamic satellite cluster changes. The extensive experiments conducted on satellite networks testbed demonstrate that \algname can significantly reduce processing time (up to 3x) and energy cost (up to 2x) against other comparative methods, while ensuring the accuracy of the model.

With the proliferation of data-driven services, the volume of data that needs to be processed by satellite networks has significantly increased. Federated learning (FL) is well-suited for big data processing in distributed, resource-constrained satellite environments. However, ensuring its convergence performance while minimizing processing time and energy consumption remains a challenge. To this end, we propose a hierarchical clustered federated learning framework, \algname. This framework employs a satellite-clustered parameter server (PS) selection algorithm at the cluster aggregation stage, grouping nearby satellites into distinct clusters and designating a cluster center as the PS to accelerate model aggregation. Several communicable cluster PS satellites are then selected through ground stations to aggregate global parameters, facilitating the FL process. Moreover, a meta-learning-driven satellite re-clustering algorithm is introduced to enhance adaptability to dynamic satellite cluster changes. The extensive experiments on satellite networks testbed demonstrate that \algname can significantly reduce processing time (up to 3x) and energy consumption (up to 2x) compared to other comparative methods while maintaining model accuracy.

%Low Earth Orbit (LEO) satellites need to process vast amounts of communication data from the ground daily. Federated Learning (FL) is well-suited for handling this data at the edge, where resources are limited. FL only requires the transmission of trained model parameters during data processing, reducing communication needs while achieving good accuracy. However, maintaining the convergence performance of FL models in a dynamically distributed satellite architecture, while minimizing communication time and energy consumption, remains a critical challenge.In this work, we propose a novel FL framework called hierarchical clustered quantitative federated learning (\algname)., to address the aforementioned issues while ensuring FL learning accuracy. \algname is built on two stages of operation: satellite cluster aggregation stage and ground station aggregation stage. We use K-means clustering algorithm to divide nearby satellites into different clusters, and specify the client of the satellite cluster center with good communication as the parameter server to accelerate the aggregation speed of the model. Aggregate global model parameters from different satellite clusters to perform FL. After the aggregation process is completed, the ground station selects a communicable cluster parameter server satellite to aggregate global parameters. Conduct model experiments on real satellite distribution data to verify the effectiveness of \algname. The results indicate that \algname can significantly reduce communication time and energy consumption with the same accuracy.
\end{abstract}

\begin{IEEEkeywords}
Satellite networks, hierarchical clustered federated learning, distributed computing%, multi-objective optimization
\end{IEEEkeywords}

\input{1_introduction}
%\input{2_relatedwork}
\input{3_model}
\input{4_algorithm}
\input{5_experiment}
\input{6_conclusion}


\begin{comment}
\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.
\end{comment}



\bibliographystyle{ieeetr} 
\bibliography{ref}

\begin{comment}
%表格暂时写在上面，方便对照
\begin{table}[ht]
\centering
\caption{Notations and Descriptions\note{[REMOVE THIS TABLE AT LAST]}}
\begin{tabular}{|c|l|}
\hline
\textbf{Inputs} & \textbf{Description} \\ \hline
$C$ & the set of candidate clients in FL \\ \hline
$K$ & the set of clusters \\ \hline
$D_i$ & Each client $c_i$ holds a local dataset size \\ \hline
$p_k$ & the cluster head of cluster satellite $k$ \\ \hline
$S_k$ & the clients belongs to cluster $k$ \\ \hline
$G$ & ground station number $G$ \\ \hline
$g_{i}^{p_k}$ & the cluster$p_k$ belongs to ground station i  \\ \hline
$t^{cmp}_i$ & computation time of client $i$ for one intra-cluster aggregation \\ \hline
$t^{com}_i$ & communication time of client $i$ for one intra-cluster aggregation \\ \hline
$t^{com}_k$ & communication time of cluster head $l_k$ for one inter-cluster aggregation \\ \hline
$n_k$ & Maximum selected number of clients per cluster per global round \\ \hline
\textbf{Outputs} & \textbf{Description} \\ \hline
$x^k_i$ & whether client $i$ belongs to cluster $k$ \\ \hline
$y^t_i$ & whether or not select client $i$ at global round $t$ \\ \hline
$\tau_{i}^{k}$ & local iterations given to client ci in the k-th communication round \\ \hline
\end{tabular}
\end{table}
\end{comment}

\end{document}
