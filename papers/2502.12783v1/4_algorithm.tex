\section{Hierarchical Clustered 
Federated Learning } \label{sec:algorithm} 

\subsection{Overview}

\begin{figure}[tb!]
\centerline{\includegraphics[width=1\linewidth]{Figure/Algorithm.png}}
\caption{Overview of the proposed framework \algname.}
\label{fig:overview}
\end{figure}

\begin{algorithm}
\begin{algorithmic}[1]
\caption{\algname for satellite networks}\label{alg:decentralized_FL}
    \REQUIRE{Information of satellite networks, $K$, $C$}
    \ENSURE{Global model $w_{G}$}

\STATE Conduct satellite-clustered parameter server selection algorithm;  \label{line:1}

\STATE \COMMENT{Satellite client initialization model parameters}  \label{line:2}
\FOR {$\forall$ satellites}
     \STATE Initialize global model parameter $w_0$;
\ENDFOR  \label{line:5}

%\STATE All satellites: Initialize global model parameter $w^{(a)}_0$;

\FOR {each FL round $m \in M$}  \label{line:6}
    \STATE \COMMENT{Train local model (in-orbit computing)}
    \FOR {each satellite $i \in C$ in parallel}
        \STATE $w_{m,\lambda+1}^{i} = w_{m,\lambda}^{i} - \eta \nabla \tilde{f}_i(w_{m,\lambda}^{i})$;
    \ENDFOR \label{line:9}
    %\STATE \note{Conduct adaptive weight quantification algorithm to obtain $w{_{m}^{i}}^{\prime}$;}
    \STATE \COMMENT{Aggregate satellite cluster models}
    \FOR {each satellite $i \in C^k$ in the same cluster}  \label{line:12}
        \STATE $w_{m+1} = w_m + \sum_{i \in C^k_i} p_i w_m$;   \label{line:13}
    \STATE \COMMENT{Check if it is necessary to reassemble clusters}  \label{line:14}
        \STATE Calculate dropout rate: $d_r = \frac{C^d}{C^k}$
        \IF {$d_r > Z$}
            \STATE Re-cluster the satellites
        \ENDIF  \label{line:18}
    \ENDFOR
\ENDFOR  
\STATE \COMMENT{Aggregate global model at the ground station}  \label{line:21}
\FOR {each PS $G_{k}, k \in {K}$ in different satellite clusters}
    \STATE $w_{G} =  \sum_{k \in {K}} \frac{D_k}{D} w_m^k$;  
\ENDFOR  \label{line:23}
\RETURN $w_{G}$.
\end{algorithmic}
\end{algorithm}

\figurename~\ref{fig:overview} illustrates the flowchart of our proposed \algname. The clustering FL clustering process in \algname includes two stages: \textbf{satellite cluster aggregation stage} (Step 1-3) and \textbf{ground station aggregation stage} (Step 4). In the satellite cluster aggregation stage, a clustering algorithm is introduced to divide the satellites into distinct clusters. Within each satellite cluster $K_i^a$, the algorithm selects a satellite near the cluster center with strong communication capabilities to act as the PS. The PS is responsible for aggregating model parameters from satellites within its cluster and establishing communication with the corresponding ground stations (Step 1). During the local training process, satellites may dynamically join or leave a cluster, necessitating re-aggregation. To address this issue, MAML is introduced to adjust the initial model parameters of the newly joined satellites, allowing them to better adapt to the tasks of the new cluster (Step 2). This method accelerates the overall convergence of the satellite PS aggregation process. Following each training round, the PS combines parameters from all satellites within its cluster and distributes the updated, aggregated parameters back to them (Step 3).

%This approach facilitates the acceleration of the overall convergence of the satellite PS aggregation process. After each training round, the PS aggregates the parameters of the satellites in its cluster and distributes the \note{Do we need to add ''updated" here?} aggregated parameters to them (Step 3). 

After a specified number of training rounds in the satellite cluster, the ground station aggregation stage starts. In this stage, the ground station communicates with visible satellite clusters to aggregate their model parameters of the respective satellite clusters they are affiliated. Finally, the ground station returns the trained model parameters to the respective satellite clusters (Step 4).

As detailed in Algorithm~\ref{alg:decentralized_FL},  we introduce a satellite-clustered parameter server selection algorithm to partition the original satellite network into distinct satellite clusters based on the satellite network information. For each cluster, the algorithm selects a satellite near the cluster center with robust communication capabilities as the PS (line \ref{line:1}). Then we initialize the global model parameters $w_0$ for all satellite clients within each cluster (lines \ref{line:2}-\ref{line:5}). During each round of FL aggregation, the local satellite client first performs local training to update the global model parameters $w_{m,\lambda+1}^{i}$ after training round $\lambda$ (lines \ref{line:6}-\ref{line:9}). These parameters are then transmitted to their cluster's PS for aggregation. 

After $m$ rounds of training in each satellite cluster, each PS forwards its aggregated parameters to its associated ground station for global aggregation, producing the updated model parameters $w_{m+1}$ (lines \ref{line:12}-\ref{line:13}). During global aggregation, satellite clusters monitor whether the number of dropped-out satellites $C^d$ exceeds a predefined threshold, triggering re-clustering when necessary (lines \ref{line:14}-\ref{line:18}). Finally, ground station broadcasts the global parameters to all affiliated satellites in their clusters, completing the hierarchically clustered FL process (lines \ref{line:21}-\ref{line:23}). 




%As shown in Algorithm~\ref{alg:decentralized_FL}, based on the satellite network information, we introduce a satellite-clustered parameter server selection algorithm to partition the original satellite network into distinct clusters. Within each satellite cluster, the algorithm selects a satellite near the cluster center with strong communication capabilities to serve as the PS (line \ref{line:1}). Then the global model parameters are initialized for all satellite clients within the satellite cluster (lines \ref{line:2}-\ref{line:5}). During each round of FL aggregation, the local satellite client first performs local training and obtains the trained global model parameters $w_{m,\lambda+1}^{i}$ after training round $\lambda$ (lines \ref{line:6}-\ref{line:9}). The satellite clients then transmit their global parameters $w_{m,\lambda+1}^{i} $ to satellite PS for aggregate global model parameters. After completing $m$ rounds of training in each satellite cluster, the global model parameters are transmitted to the ground station associated with the satellite PS for final aggregation, yielding updated aggregated global model parameters $w_{m+1}$ (lines \ref{line:12}-\ref{line:13}). During global aggregation, satellite clusters need to assess whether the number of dropped-out satellites $C^d$ exceeds a critical threshold, necessitating re-clustering (lines \ref{line:14}-\ref{line:18}). Finally, the ground station broadcasts the global parameters to all affiliated satellites in the cluster, completing the hierarchically clustered FL process (lines \ref{line:21}-\ref{line:23}). \note{please double check this paragraph to avoid the meaning changes after my rewrite}

To accelerate the convergence of the global model, we assign weights to clients based on the quality of their model updates. The quality is evaluated using the loss value of the client's local model. Let $L_i$ denote the loss value of the client $i$. The weight $p_i$ assigned to client $i$ is given by:
\begin{equation}
p_i = \frac{\frac{1}{L_i}}{\sum_{ i \in C^k} \frac{1}{L_i}}
\label{eq:p}
\end{equation}

\begin{comment}
The local updates from the satellite clients are then weighted and aggregated according to the weight parameter $p_i$ by:
\begin{equation}
w_{m+1} = w_m + \sum_{i \in U_k} p_i Q_m^i(w_{m+1}^{i} - w_m)
\end{equation}
\end{comment}
%If $p_i$ is 0, the client has not been selected by PS.

\subsection{Satellite-clustered parameter server selection algorithm}

We introduce a satellite-clustered parameter server selection algorithm that partitions the original satellite network topology into a predefined number of clusters $K$, optimizing the clustering process. Our algorithm iteratively refines the cluster centroids and the membership of associated satellites. Initially, $K$ centroids are randomly selected from the satellite location data. These locations are typically derived from geographic coordinates based on the satellite location parameters, i.e., inclination and orbital altitude. Each satellite is assigned to the nearest cluster centroid using the Euclidean distance metric, thereby forming initial clusters. The Euclidean distance between a satellite position vector
$\mathbf{C}^i = \{C_{1}^i, C_{2}^i, \ldots, C_{n}^i \}$ and a centroid $\mathbf{K}^j = \{K_{1}^j, K_{2}^j, \ldots, K_{n}^j \}$ is calculated as:
\begin{equation}
d(\mathbf{C}^i, \mathbf{K}^j) = \sqrt{\sum_{k=1}^{n} (C_{k}^i - K_{k}^j)^2}
\end{equation}

In the next update step, our algorithm recalculates the centroids by computing the mean position of all satellites assigned to each cluster. This process effectively repositions the centroids to more accurately represent the distribution of their associated satellites. For each cluster $K_k^i$, the new centroid $\mathbf{K}^j$ is obtained by:
\begin{equation}
\mathbf{K}^j = \frac{1}{|K^j|} \sum_{\mathbf{C}^i \in K^j} \mathbf{C}^i
\end{equation}
where $|K^j|$ represents the number of satellites in cluster $K^j$. The iterative process continues until the centroids stabilize, indicating their positions no longer change significantly between iterations. This indicates that the algorithm has converged to a local optimum. The convergence criterion is given by:
\begin{equation}
\sum_{j=1}^{|K|} \|\mathbf{K}_{\text{new}}^j - \mathbf{K}_{\text{old}}^j\|^2 < \epsilon
\end{equation}
where $|K|$ represents the number of clusters, and $\epsilon$ is a small positive number indicating stability in centroid positions.
The satellite nearest to the cluster centroid is designated as the PS for the respective cluster.


\subsection{Meta-learning-driven satellite re-clustering algorithm}

In dynamic satellite federated learning, the diverse training objectives of satellite clients, combined with their frequent network participation changes, can hinder model convergence and increase resource consumption. As a result, achieving acceptable performance requires substantial time and a large number of data samples.
%In dynamic satellite federated learning, the diverse training goals of satellite clients, coupled with their frequent network participation changes, can impede model convergence, resulting in heightened resource utilization. This necessitates a significant investment of time and data samples to attain acceptable performance. %the different training objectives of satellite clients, along with their frequent joining or leaving the network, can slow down model convergence, leading to increased resource consumption. This requires a large amount of time and data samples to achieve acceptable results. 

To address this challenge, we propose a satellite re-clustering algorithm based on meta-learning, extending the original satellite-clustered parameter server selection algorithm. When a new satellite joins the network, it inherits model updates from the head node of a specified cluster, rather than starting training from scratch. The core idea of the MAML approach is to identify a set of meta-initialization parameters that enable the model to achieve strong performance with just one or two gradient updates, even with a small number of new task examples.

First, we sample satellite clients from different clusters denoted as $ S = \{S_1, S_2, \dots, S_n\} $. Each satellite client is assigned a task $\textit{TK}_i$, which consists of a dataset $D_i$  and a loss function $L_{S_i}(w)$. The objective is to minimize the loss of the model on the task $\textit{TK}_i$. Then, an inner-loop adaptation is performed for each selected satellite node to fine-tune the global model $w$ by:
\begin{equation}
w_i' = w - \alpha \nabla_{w} L_{S_i}(w)
\end{equation}
where $\alpha$ is the local learning rate. Finally, an outer-loop meta-update is applied to aggregate the model updates from different satellite nodes, updating the global initialization by:
\begin{equation}
w^{t+1} = w^{t} - \beta \sum_{i \in S} \nabla_{w} L_{S_i}(w_i')
\end{equation}
where $\beta$ is the meta-learning rate, $w^{t}$ is the parameter of the current global model.

