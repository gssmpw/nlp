\section{Introduction}

%With the rapid development of communication technology and the proliferation of Internet of Things (IoT) devices, the computing task collected at the network edge devices has significantly increased. Distributed computing has emerged as a promising paradigm to exploit the abundant and massive computation resource of edge devices collaboratively~\cite{ChenINFOCOM23}. Among distributed computing schemes, federated learning (FL) only requires the transmission of trained model parameters during the data processing. This scheme can maintain data localization at the end-users throughout the collaborative learning process of the global model, by which enhances the security of user data. Numerous existing research have already focused on the implementation of FL in diverse wireless networks at the ground~\cite{LiaoINFOCOM23,LiTWC24}.


With the rapid development of communication technology and the proliferation of Internet of Things (IoT) devices, the computing task collected at the network edge devices has significantly increased. However, existing terrestrial wireless networks cover only a limited area of the Earth due to the geographic and economical constraints on deploying commercial mobile network infrastructure. Satellite networks serve as a viable complement to terrestrial networks, offering global coverage and seamless connectivity to support diverse computing tasks from both ground-based and space-based sources~\cite{MahboobCST24,PengISCC24}. %\textcolor{blue}{Satellites continuously collect high-resolution Earth observation images and sensor data every day, which can support a wide range of applications, such as fire detection [to be update ...] and disaster prediction [...]. Among these, post-disaster building damage assessment is considered one of the most important applications.}
Federated learning (FL) is a promising distributed learning paradigm for satellite networks, with significant potential for widespread deployment. This approach improves data processing efficiency by enabling distributed data analysis on independent satellites without requiring raw data sharing, thus preserving data privacy~\cite{ChenWC22,ShenCSUR23,FontanesiCST25}. The effectiveness of FL in supporting basic data processing tasks on real satellite computing platforms has already been demonstrated in~\cite{TangTMC24}. While FL addresses data sharing and privacy concerns, it is essential to design an FL framework tailored for satellite networks to achieve efficient collaborative model training.


%Although FL mitigates data sharing and privacy concerns, challenges remain in managing processing time and energy consumption, particularly in resource-constrained satellite networks with dynamically changing topologies.

The traditional method to conduct FL in satellite clusters typically involves two training phases. First, local training is performed on individual satellite clients. Then, collaborative model training takes place through centralized \textbf{parameter servers (PS)} on the ground. During this process, each client trains its model using its local data and transmits intermediate results, such as gradients and weights, between satellites for global model aggregation~\cite{ChenPIMRC23}. Although this FL method eliminates the need for raw data sharing to address privacy concerns, aggregating all intermediate results at a centralized PS in resource-limited satellite networks can lead to substantial communication overhead and energy consumption. Additionally, satellite network topology is constantly changing due to the high orbital speeds of satellites. Depending on the satellite type, the orbital period can range from 2 to 24 hours. While ground-based PS have abundant processing and communication resources to facilitate the FL process, communication with individual satellites is limited to specific time windows throughout the day~\cite{MathIN24}.

To address this issue, recent research has focused on implementing PS on satellites to enable distributed FL process. Most current research that considers satellites as PS typically groups satellites within the same orbit into clusters, performing FL within each group. For example, Zhai \textit{et al.} designed a distributed FL framework utilizing the Ring all-reduce algorithm to synchronize FL models and accommodate the circular topology of satellite constellations~\cite{ZhaiTMC24}. Similarly, Yan \textit{et al.} optimized the number of satellites and orbits in each cluster to minimize the convergence time of distributed FL~\cite{YanTVT24}. Distributed FL aggregates model parameters through peer-to-peer communication between clients without relying on a PS. However, these methods can lead to excessive communication between clients, resulting in inefficient use of satellite resources and increased communication costs. Cluster FL combines the advantages of both traditional and distributed FL by grouping clients into distinct clusters and performing FL independently within each cluster~\cite{LiuICC20,FedCE23}. Designing an FL solution that effectively harnesses large-scale, resource-constrainted, and fast-moving satellite networks is crucial for minimizing processing time and energy usage while ensuring model accuracy.



%Developing an FL solution that efficiently leverages the massive resource-constrained \note{vast yet resource-constrained?} and rapidly moving satellite network is crucial for minimizing processing time and energy consumption while maintaining satisfactory model accuracy.

%Razmi \textit{et al.} proposed an satellite resources  utilization scheme in a low Earth orbit (LEO) satellite cluster network~\cite{RazmiTOC24}. 
%In addition, the varying computing capabilities of edge devices require adaptive and efficient quantization techniques to ensure model performance. 
%Furthermore, a weight parameter has been incorporated to dynamically assess the necessity of quantizing the global parameters transmitted by the model, and to determine the appropriate number of quantization bits based on gradient changes.
%Here, the delay aggregation quantization (DAQ)~\cite{FartashNeurIPSs20} is employed to bypass unnecessary parameter uploads. This is achieved by estimating gradient innovation, which represent the differences between the current unquantized gradient and the previously quantized gradient.

To this end, we propose a hierarchical clustered federated learning framework, \algname, designed to optimize both processing time and energy consumption in satellite networks. The problem of clustered FL optimization with parameter server selection is NP-hard~\cite{XuIoT24}. To address this, we divide the FL process into two stages: 

\textbf{In the satellite cluster aggregation stage}, the framework first captures the location information of satellite clients and applies a clustering algorithm to group nearby satellites into distinct clusters. The satellite at the cluster center is designated as the PS to accelerate model aggregation. 
Each cluster independently aggregates local model parameters before contributing to the global FL process.
%Different satellite clusters aggregate the global model parameters for performing FL. \note{seems unnecessary}

\textbf{In the ground station aggregation stage}, the ground station selects a subset of cluster PS satellites that can communicate to aggregate global parameters. However, due to the high-speed movement of satellites, the network topology continuously changes, potentially altering cluster compositions. 
%\note{please help me rewrite this sentence to make sure the same meanings as the previous version, This dynamic environment may necessitate re-clustering, which necessitates a re-assessment of the cluster composition.} 
In such a dynamic environment, re-clustering becomes unavoidable, compelling a re-assessment of the clusterâ€™s composition to adapt to shifting conditions. After re-clustering, mismatches between existing and newly added satellites may lead to slower convergence and increased energy consumption. To tackle this issue, we integrate model-agnostic meta-learning (MAML) after re-clustering. MAML enables the model to rapidly adapt to new cluster parameters by leveraging prior experience from different clusters. This adaptation helps maintain high performance in dynamic network environments while reducing convergence time and energy consumption.

The main contributions of this paper are as below:
\begin{itemize}
    %\item \textcolor{blue}{For disaster remote sensing data within satellite regions, we propose a self-supervised federated learning framework for efficient satellite image training. Additionally, our framework incorporates a tailored clustering strategy to reduce processing time and energy costs when deploying FL in dynamic satellite networks.}
    \item A hierarchical clustered FL framework \algname is proposed to reduce the processing time and energy consumption in dynamic satellite networks. \algname organizes the satellite FL process into two stages: satellite cluster aggregation and ground station aggregation. Initially, satellite clusters train FL models locally, and then global model updates are consolidated at the ground station, minimizing communication time.


    %a hierarchical clustered Federated Learning (FL) framework to address the challenges of processing time and energy consumption in dynamic satellite networks. This framework organizes the satellite FL procedure into two phases: 1) intra-cluster aggregation, where satellite clusters independently train localized FL models, and 2) ground station aggregation, where global model updates are consolidated at the ground station. By prioritizing local training within clusters before transmitting refined updates to the ground, the framework minimizes communication overhead and accelerates convergence in resource-constrained orbital environments

    

    \item A meta-learning-driven satellite re-clustering algorithm is developed to enhance adaptability to dynamic satellite cluster changes during the FL process. This algorithm enables newly joined satellites to leverage knowledge from previous cluster members, facilitating rapid adaptation to evolving network conditions and accelerating model convergence.
    
    %we integrate meta-learning into the existing federated learning framework, allowing new nodes to leverage the experience of previous ones. This enables efficient scenario adaptation and significantly accelerates model convergence.
    
    \item The effectiveness of \algname is verified by the numerical experiments. The obtained results demonstrate that \algname outperforms other comparative methods in terms of accuracy, processing time and energy consumption. 

\end{itemize}

%The remainder of this paper is organized as follows:  related work is summarized in Section~\ref{sec:relatedwork}. Section~\ref{sec:model} presents the system model and formulates the optimization problem. Section~\ref{sec:algorithm} describes our proposed clustered FL framework \algname and its associated algorithms. Section~\ref{sec:evaluation} shows the experimental results that validates the performance of \algname. Finally, conclusion with the future direction is provided in Section~\ref{sec:conclusion}.