\section{Experimental Evaluation}~\label{sec:evaluation}

\subsection{Experimental setup}

We evaluate the performance of the proposed \algname using our established testbed, which simulates LEO satellite networks while considering the orbital movement of the satellites. The satellites are evenly distributed across each orbit, operating at an altitude of 1300 km with an inclination angle of $53^{\circ}$. The base station maintains a minimum elevation angle of $10^{\circ}$. Each client is trained using a small batch SGD with a batch size of 64, employing the LeNet model. The initial learning rate is set to 0.01, and clients train the FL model using the MNIST dataset and the CIFAR-10 dataset. The original dataset is partitioned into different subsets corresponding to the number of satellite clients. After dividing the satellite clients into $K$ clusters, local training is conducted for 500 rounds within each cluster. The communication and computation parameters in satellite networks follow those outlined in~\cite{ZhuJSAC23, ZhangIoT23}.
%The CPU cycle frequency of the satellite is $ f_i = 50 \, \text{GC/s}  (GC =  10^9 $ cycles)~\cite{ZhuJSAC23}, the transmission bandwidth is $ B_i = 27 \, \text{GHz} $ (Ka-band), the satellite transmission power is $ p_i = 30 \, \text{dBW} $~\cite{ZhangIoT23}, and the noise power density is $ N_0 = -174 \, \text{dBm/Hz} $~\cite{ZhuJSAC23}. 
We configure identical learning rates of $10^{-3}$ for both the inner ($\alpha$) and outer loops ($\beta$) in the satellite re-clustering algorithm. In terms of satellite mobility, we assume that satellites at the same latitude maintain their relative positions, and the ground station can connect at least one satellite cluster throughout the FL process. 


\begin{figure}[tb!]
	\centering
	\begin{minipage}[b]{.493\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{Figure/MNIST.png}
		\subcaption{MNIST}\label{fig:1_acc}
	\end{minipage}
        \begin{minipage}[b]{.493\columnwidth}
		\centering
		
		\includegraphics[width=\columnwidth]{Figure/CIFAR10.png}
		\subcaption{CIFAR-10}\label{fig:3_acc}
	\end{minipage}

	\caption{Accuracy performance with different datasets.}
    
	\label{fig:accuracy}
\end{figure}



%\subsection{Comparative methods}
We validate the effectiveness of our proposed \algname against the following three comparative methods.
\begin{itemize}

    %\item\textbf{FedAvg~\cite{McMahanPMLR17}}: is a traditional distributed learning algorithm
    
    %两个架构是否要引参考文献，但是这两个架构参考文献为一篇{Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation}
    
    \item\textbf{C-FedAvg}~\cite{ChenPIMRC23}: A centralized FL method based on FedAvg, where all data collected from each client is transmitted to a designated central satellite server for centralized learning.
    
    %\item\textbf{Distributed Learning Architecture~\cite{ChenPIMRC23}}: In distributed learning, LEO clients perform local training independently, without exchanging information through Inter-Satellite Links.

    \item\textbf{H-BASE}~\cite{LiuICC20}: A clustered FL method that randomly selects clients and performs training over a fixed number of intra-cluster aggregation iterations.
    
    \item\textbf{FedCE}~\cite{FedCE23}: A clustered FL method that partitions clients into clusters based on the distribution characteristics of their data.
    

\end{itemize}

%For the parameters in the comparison methods, we follow the optimal settings recommended in the original literature. 


%!!!排版参照图4，两两一排!!!

\subsection{Experimental results}




%横坐标为train round
%纵坐标为accuracy和loss
% k=3,4,5（6张图） 5个方法5条线  （折线图）
\textbf{Training convergence}: 
To validate the effectiveness of \algname, we fix the number of satellite clients at 800 and divide the satellite network into different clusters, i.e., $K$= 3, 4, 5. Then, we compare the performance of our method against other methods on the MNIST dataset after 300 rounds, and on the CIFAR-10 dataset after 1000 rounds. This evaluation aims to demonstrate the accuracy and convergence speed of \algname, highlighting its advantages over baseline methods. Figure~\ref{fig:accuracy} compares model accuracy across methods under various clustering configurations, evaluated over a fixed number of training rounds before full convergence. \algname consistently achieves higher accuracy than baseline methods within the same training round, particularly outperforming methods that do not incorporate meta-learning under satellite re-clustering conditions. 
%\figurename~\ref{fig:accuracy} shows the model accuracy of various methods under different clustering conditions, chosen based on a fixed number of rounds before complete convergence. As observed, \algname outperforms other baseline methods in terms of accuracy with the same number of rounds, while also demonstrating an improvement in convergence speed, especially compared to methods that do not incorporate meta-learning under satellite re-clustering conditions. 
In the following experiments, we present the results of all methods upon reaching the specified converged target accuracy thresholds  (MNIST: 80\%, CIFAR-10: 40\%), following the methodology outlined in~\cite{TMC23}.


%In the subsequent experiments, we fixed all algorithms to calculate different criteria when the MNIST dataset reaches the target accuracy of 80\% and the CIFAR-10 dataset reaches the target accuracy of 40\% following~\cite{TMC23}.

%\textbf{FL processing time and energy cost:} The FL processing time includes the computation time and communication time of the satellites calculated by Equation~\ref{eq:time}, while the energy cost of satellites encompasses both the transmission energy consumption and the energy consumption for global model parameter aggregation following Equation~\ref{eq:energy}.

%In the subsequent experiments, we fix all algorithms to calculate different criteria upon reaching the target accuracy (= 80%)

\begin{comment}
\begin{table}[h!]
\centering
\begin{tabular}{c|c|c|c}
\hline
Method & \( F_{\text{bld}} \) & \( F_{\text{dmg}} \) & \( F_{\text{total}} \) \\
\hline
Centralized-FedAvg       & 0.62 & 0.80 & 0.746 \\
H-BASE & 0.68 & 0.82 & 0.778 \\
FedHC w/o re-clustering algorithm    & 0.66 & 0.78 & 0.744 \\
FedHC    & \textbf{0.66} & \textbf{0.78} & \textbf{0.744} \\
\hline
\end{tabular}
\caption{Performance Comparison of Different Schemes}
\label{tab:performance_comparison}
\end{table}
\end{comment}

\begin{comment}
\begin{table*}[h!]
\centering
\caption{Performance comparison of different methods  under various clustering conditions ($K$).}
\label{tab:results}
\begin{threeparttable}
\begin{tabular}{l|lllllllll|lllllllll}
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{9}{c|}{MNIST}                                                                                                                                      & \multicolumn{9}{c}{CIFAR}                                                                                                                                      \\ \cline{2-19} 
\multicolumn{1}{c|}{}                        & \multicolumn{3}{c|}{\textit{K=3}}                   & \multicolumn{3}{c|}{\textit{K=4}}                   & \multicolumn{3}{c|}{\textit{K=5}}                   & \multicolumn{3}{c|}{\textit{K=3}}                   & \multicolumn{3}{c|}{\textit{K=4}}                   & \multicolumn{3}{c}{\textit{K=5}}                   \\ \cline{2-19} 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c|}{TEC} & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c|}{TEC} & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c|}{TEC} & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c|}{TEC} & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c|}{TEC} & \multicolumn{1}{c}{PT}   & \multicolumn{2}{c}{TEC} \\ \hline
C-FedAvg                                     &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l}{}    \\
H-BASE                                       &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l}{}    \\
XXXXXX                                       &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l}{}    \\
FedHC                                        &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l|}{}    &                          & \multicolumn{2}{l}{}    \\ \hline
\end{tabular}
   \begin{tablenotes}
       \footnotesize
       \item[1] PT: Processing Time ($10^3$ secs), TEC: Total Energy Cost (J), TC: Transmission Energy Cost (J)
     \end{tablenotes}
     \end{threeparttable}
  
\end{table*}
\end{comment}


\begin{table}[tb!]
\centering
\caption{Performance comparison of different methods.}
\label{tab:results}
\begin{threeparttable}
\begin{tabular}{l|ccl|ccl|ccl}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Method\\ (\textbf{MNIST})\end{tabular}}} & \multicolumn{3}{c|}{\textit{K=3}}                    & \multicolumn{3}{c|}{\textit{K=4}}                   & \multicolumn{3}{c}{\textit{K=5}}                  \\ \cline{2-10} 
\multicolumn{1}{c|}{}                                                                             & Time                       & \multicolumn{2}{c|}{Energy}  & Time                      & \multicolumn{2}{c|}{Energy}  & Time                     & \multicolumn{2}{c}{Energy}  \\ \hline
C-FedAvg~\tnote{1}                                                                                          &     8184                     & \multicolumn{2}{c|}{3697}     &  8184                       & \multicolumn{2}{c|}{3697}     &         8184               &  \multicolumn{2}{c}{3697}     \\
H-BASE                                                                                            & 4734                     & \multicolumn{2}{c|}{2301} & 5687                    & \multicolumn{2}{c|}{2633} & 5055                   & \multicolumn{2}{c}{2761} \\
FedCE                                                                                            & 3945                     & \multicolumn{2}{c|}{1889} & 3696                        & \multicolumn{2}{c|}{1923}     &  2906                      & \multicolumn{2}{c}{2160}     \\
FedHC                                                                                             & \textbf{3124}                        & \multicolumn{2}{c|}{\textbf{1720}}     & \textbf{3554}                        & \multicolumn{2}{c|}{\textbf{1863}}     & \textbf{2527}                       & \multicolumn{2}{c}{\textbf{1889}}     \\ \hline\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Method\\ (\textbf{CIFAR-10})\end{tabular}}} & \multicolumn{3}{c|}{\textit{K=3}}                    & \multicolumn{3}{c|}{\textit{K=4}}                   & \multicolumn{3}{c}{\textit{K=5}}                  \\ \cline{2-10} 
\multicolumn{1}{c|}{}                                                                             & \multicolumn{1}{c}{Time}   & \multicolumn{2}{c|}{Energy}  & Time                      & \multicolumn{2}{c|}{Energy}  & Time                     & \multicolumn{2}{c}{Energy}  \\ \hline
C-FedAvg~\tnote{1}                                                                                          & \multicolumn{1}{c}{13477}     & \multicolumn{2}{c|}{6637}     & \multicolumn{1}{c}{13477}    & \multicolumn{2}{c|}{6637}     & \multicolumn{1}{c}{13477}   & \multicolumn{2}{c}{6637}     \\
H-BASE                                                                                            & \multicolumn{1}{c}{9346}     & \multicolumn{2}{c|}{6841}     & \multicolumn{1}{c}{10129}    & \multicolumn{2}{c|}{5381}     & \multicolumn{1}{c}{12003}   & \multicolumn{2}{c}{5016}     \\
FedCE                                                                                            & \multicolumn{1}{c}{8315}     & \multicolumn{2}{c|}{5397}     & \multicolumn{1}{c}{7931}    & \multicolumn{2}{c|}{4674}     & \multicolumn{1}{c}{7258}   & \multicolumn{2}{c}{4451}     \\
FedHC                                                                                             & \multicolumn{1}{c}{\textbf{7873}}     & \multicolumn{2}{c|}{\textbf{5013}}     & \multicolumn{1}{c}{\textbf{7509}}    & \multicolumn{2}{c|}{\textbf{4259}}     & \multicolumn{1}{c}{\textbf{6942}}   & \multicolumn{2}{c}{\textbf{4148}}     \\ \hline
\end{tabular}
   \begin{tablenotes}
       \footnotesize
       %\item[1] PT: Processing Time (secs), TEC: Total Energy Cost (J).
       \item[1] As a centralized method, C-FedAvg achieves uniform performance across all scenarios since it operates without distributed collaboration ($K$=1). 
       
       %C-FedAvg records a same performance for all cases since it is a centralized method, i.e., $K$ = 1.
     \end{tablenotes}
     \end{threeparttable}
  
\end{table}


\textbf{FL processing time and energy consumption:} Table~\ref{tab:results} presents the total processing time (seconds, by Equation~\ref{eq:time}) and total energy consumption (J, by Equation~\ref{eq:energy}) across various clustering configurations and datasets. The results demonstrate that \algname outperforms all baselines on both datasets. In terms of processing efficiency, our method significantly reduces overall processing time compared to the three baseline methods. For example, with $K$=5 on the MNIST dataset, \algname demonstrates a twofold reduction in processing time against C-FedAvg. This improvement stems from hierarchical cluster FL, which deploys multiple parameter servers to enable parallelized model training across clusters, thereby drastically reducing communication time. This is because hierarchical cluster FL deploys multiple parameter servers, enabling parallelized model training across clusters and drastically reducing communication time. Compared to H-BASE and FedCE, which rely on static clustering or slower convergence strategies, \algname reduces processing time by up to 40\%. This improvement is driven by the meta-learning-driven satellite re-clustering algorithm, which accelerates FL convergence.


Meanwhile, for $K$=5 in the MNIST dataset, our method reduces total energy consumption by over twofold compared to C-FedAvg. In other cases, \algname consistently consumes approximately 1.5 times less energy than C-FedAvg. These results demonstrate the superior energy efficiency of our hierarchical clustered FL framework over centralized C-FedAvg. Furthermore, our method exhibits a significant advantage over other federated clustering methods, primarily due to two key innovations. First, the satellite-clustered parameter server selection algorithm optimizes transmission energy by strategically choosing parameter servers with favorable geographical positions and communication capabilities. Second, the meta-learning-driven satellite re-clustering algorithm accelerates model convergence, drastically reducing energy consumption during training. These features make our framework particularly well-suited for resource-constrained satellite networks, where efficient communication and rapid convergence are critical for minimizing energy consumption. 


%Table~\ref{tab:results} summarizes the processing time, total energy cost under different clustering conditions and different datasets. It is worth noting that, in the MNIST dataset, when K = 5, our method achieves more than twice the total energy savings compared to the traditional C-FedAvg. In other scenarios, the energy cost of the algorithm is also approximately 1.5 times lower than that of traditional C-FedAvg. The result indicates that hierarchical clustered FL can enhance the performance of \algname over the traditional centralized FedAvg. Compared to the other two federated clustering learning methods, our approach shows a more significant advantage in energy consumption. This is because our federated clustering framework employs a satellite cluster parameter server selection algorithm, which chooses satellites with optimal geographical locations and communication conditions as parameter servers, thereby saving transmission energy.And our meta-learning-based satellite re-clustering approach significantly reduces energy costs during the transmission process by enabling faster model convergence. This is particularly beneficial for satellite communication scenarios with limited communication resources.


%The processing speed advantage, combined with the energy savings detailed earlier, underscores the dual efficiency gains of our hierarchical FL framework. 


%When calculating the training time for FL learning, we observe that the proposed \algname significantly reducing the overall training time compared to the other three comparative methods. As shown in Table~\ref{tab:results}, when $K$=5, the training time of \algname is around two times lower than C-FedAvg. This is because hierarchical cluster FL sets up multiple parameter servers, which can simultaneously train the global model between different clusters, by which reduces the communication time during the FL process. Compared with H-BASE and FedCE, our communication time is significantly shorter. This improvement is due to the acceleration of FL convergence achieved through meta learning. As a result, our proposed \algname reaches the same level of accuracy more quickly than other methods.
