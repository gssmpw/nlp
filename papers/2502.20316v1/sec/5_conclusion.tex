
\section{Conclusion}
\label{sec:conclusion}
% In summary, NOMAE is a novel self-supervised learning technique that adapts masked autoencoder to sparse 3D lidar point clouds of automotive scenarios. 
% The reconstruction of only the local neighborhood keeps computation tractable and avoiding information leakage at the same time. 
% A multi-scale masking prepares the SSL pretext task for objects of varying sizes in the downstream tasks, such as pedestrians and trucks. 
% The proposed hierarchical mask generation supports the reconstruction of both coarse and fine patterns. 
% Experiments underline the benefit of proposed ingredients, achieving state-of-the-art performance in multiple benchmarks. 

In this work, we proposed NOMAE, a novel multi-scale self-supervised learning framework for large-scale point clouds. Observing the large-scale nature of LiDAR point clouds, NOMAE reconstructs only local neighborhoods, keeping the computation tractable at higher voxel resolutions, avoiding information leakage, and learning a localized representation suitable for diverse downstream perception tasks. Enabled by its efficiency, NOMAE utilizes multiple scales in the pre-training, enabling the model to learn both coarse and fine representations. A novel hierarchical mask generation scheme balances the pre-training of coarse and fine features, which is important for objects of different sizes, such as pedestrians and trucks
%prepares the model for objects of varying sizes in the downstream tasks, such as pedestrians and trucks.
We presented experimental results that underline the benefit of our proposed contributions, achieving state-of-the-art performance on multiple benchmarks.

\noindent\textbf{Limitations}: NOMAE is sensitive to the density of the point cloud and future work will investigate the proposed method for sparse 3D sensors such as radar. 
Additionally, NOMAE does not utilize the temporal nature of LiDAR data which can open the door for further performance improvement. 
Furthermore, the application of high-resolution sparse 3D representations in the encoders should be further investigated for the object detection task, where 2D bird's eye view and low-resolution 3D approaches are still dominant.
