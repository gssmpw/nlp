\input{figure_latex/framework}

\section{Related Work}
%-------------------------------------------------------------------------
 
%{\parskip=2pt
%\noindent\textbf{Automotive 3D Scene Understanding}: In recent years many works have explored the problem of automotive lidar 3D scene understanding. Sparse-CNN\,\citep{Liu2015SparseCN} was utilized for both semantic segmentation~\citep{ChoyGS2019mink,tang2020spvnas} and 3D object detection~\citep{Sun2020SparseRE,Yin2020Centerbased3O,yang2019std,fan2022fsd,yan2018second,zhou2018voxelnet,chen2023voxelnext,deng2021voxelrcnn} due to effectiveness and efficiency on sparse 3D point clouds. Transformer architectures are a common choice among more recent approaches for lidar point clouds, such as SST\,\citep{fan2022sst} or stratified transformer\,\citep{lai2022stratified}. Point transformer\,\citep{zhao2021pointtransformer, wu2022pointtransv2, Wu2023PointTV} utilizes smart and compute-efficient attention mechanisms. The latest version of point transformer PTv3\,\cite{Wu2023PointTV} extends recent serialization based methods from~\citep{Wang2023OctFormerOT,liu2023flatformer,Wu2023PointTV} with partition-based pooling, showing impressive performance on many leaderboards.}

% The latest version of point transformer PTv3~\cite{Wu2023PointTV} extends the previous work with the recent serialization based methods from~\citep{Wang2023OctFormerOT,liu2023flatformer,Wu2023PointTV}, showing impressive performance on many leaderboards. 

In this section, we review the existing works related on point cloud SSL and Automotive LiDAR SSL.

{\parskip=2pt
\noindent\textbf{3D Self-Supervised Learning}:} The success of generative self-supervised learning in natural language processing and computer vision has inspired several works~\citep{pang2022pointmae, Wang2020UnsupervisedPC, liu2022maskpoint, AbouZeid2023Point2VecFS} to explore masked auto-encoders for 3D point clouds. 
These approaches are typically tailored towards small-scale single object recognition tasks, where a standard ViT~\citep{Dosovitskiy2020AnII} architecture suffices to encode the point cloud. 
For example, PointMAE~\citep{pang2022pointmae} explicitly reconstructs point cloud patches using the Chamfer distance. 
MaskPoint~\citep{liu2022maskpoint} discriminates between the reconstructed points and noise points. 
In OcCo~\citep{Wang2020UnsupervisedPC}, point cloud completion is performed on occluded regions. 
Most prominently, Point2Vec~\citep{AbouZeid2023Point2VecFS} reconstructs the encoded features of a teacher model for the masked patches. 
Alternatively, contrastive methods can be employed with point clouds to distinguish multiple partial views~\citep{xie2020pointcontrast} or point-level correspondences~\citep{Hou2020ExploringD3}. 
These pioneering works achieve promising results on object-scale and room-scale 3D point clouds but are not usable for large-scale automotive LiDAR point clouds due to their inefficient scaling with the size of the point cloud. 
In contrast, our work focuses on large-scale automotive LiDAR point clouds and employs efficient hierarchical architectures. 
Additionally, our work proposes self-supervision for multiple feature levels, contrary to the single-scale supervision employed in these works. 

{\parskip=2pt
\noindent\textbf{Automotive LiDAR Self-Supervised Learning}: 
The focus of existing works on automotive large-scale point clouds is computational efficiency. 
One common technique is to reduce 3D point clouds to a 2D bird's-eye-view (BEV) grid, using pillar architectures~\citep{xv2023mvjar,yang2023gd-mae,hess2022voxelmae,tian2023geomae, boulch2023also}. 
\citep{hess2022voxelmae} reconstructs 3D points inside masked 2D pillars and \citep{xv2023mvjar} additionally predicts the order of the 2D BEV pillars. 
GeoMAE~\citep{tian2023geomae} predicts centroids and 3D sub-occupancy in the pretraining. 
GD-MAE~\citep{yang2023gd-mae} utilizes a generative decoder to reconstruct the whole scene to alleviate the leakage of positional information to the decoder. ALSO~\citep{boulch2023also} reconstructs the surface occupancy of the point cloud. UniPAD~\citep{Yang2023UniPADAU} is a pioneering work that generates coarse 3D features of the whole scene and uses a neural rendering approach for supervision.
Occupancy-MAE~\citep{min2022OccupancyMAESP} proposes to utilize the occupancy as a compressed representation of the point cloud and reconstruct the occupancy in the 3D space using a masked point cloud. 
Despite the significant performance achieved by them, reconstructing occupancy or features over the entire 3D volume is an expensive task, which allows supervision only on a coarse-scale.
Furthermore, the large computational cost prohibits employing modern 3D scene understanding architectures with high voxel resolutions.
As a result, the state of the art for self-supervised learning lags behind the performance of a fully supervised training of transformer architectures from scratch. 
In contrast, this paper addresses fully sparse SSL as a remedy, scaling well with higher 3D voxel resolutions.} 