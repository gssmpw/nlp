\section{Technical Approach}

Fig.~\ref{fig:framework} presents an overview of our proposed framework for self-supervised representation learning. The network consists of an encoder (to be trained), a token upsampling module, and multiple decoders for the hierarchical masked voxel reconstruction. We employ PTv3~\citep{Wu2023PointTV} as the encoder. In contrast to earlier works, we maintain a sparse feature space and generate fine-grained features only for the visible voxels using an upsampling module. 
We employ a sparse decoder for masked voxel reconstruction. This decoder is designed to be simple and lightweight, as described in Sec.~\ref{sec:decoder}, allowing us to deploy a separate decoder instance at each feature scale in the multi-scale pretext (MSP), which is further detailed in Sec.~\ref{sec:MSP}. 
The input point clouds are first voxelized and then masked before being fed into the encoder.  Our masking strategy ensures that there is adequate masking coverage while also maintaining a sufficient number of occupied voxels in the reconstructed neighborhoods at multiple hierarchical scales, as explained in Sec.~\ref{sec:HMG}.

\subsection{Encoder and Token Upsampling}
\label{sec:encoder}
This input point cloud $\mathcal{P}$ is first voxelized to obtain the set of all occupied voxels $\mathcal{V}$, which is then split into a set of visible voxels $\mathcal{V}_\text{v}$ and masked voxels $\mathcal{V}_\text{m}$, as detailed in Sec.~\ref{sec:HMG}. 
A sparse transformer encoder $\mathbb{E}$ based on PTv3~\citep{Wu2023PointTV} is used to encode the features $V$ at the positions of $\mathcal{V}_\text{v}$ to generate the set of tokens
\begin{align}
    F(s) &= \mathbb{E}(V)(s).
\end{align}
PTv3 employs partition-based pooling on the tokens to generate more abstract representations for coarser resolutions, similar to pooling in CNNs. 
$F(s)$ is the features (tokens) at the $s$-th scale level of PTv3 and $s\in\{0,...,S-1\}$. 

NOMAE uses an upsampling module $\mathbb{M}_\text{u}$ to propagate the abstract encoding of coarser resolution tokens to the tokens of finer resolution while keeping the representation sparse. This is similar to a feature pyramid network for CNNs. $\mathbb{M}_\text{u}$ consists of a single PTv3 transformer block at every scale, which is very lightweight. 

\subsection{Neighboring Decoder}
\label{sec:decoder}
Prior work on self-supervised learning for large-scale point clouds reconstruct exact locations of points~\citep{hess2022voxelmae}, geometrical properties~\citep{tian2023geomae} or voxel ordering~\citep{xv2023mvjar}, for each masked voxel $\mathcal{V}_\text{m}$. 
This requires passing $\mathcal{V}_\text{m}$ to the decoder, causing information leakage. 
\citep{min2022OccupancyMAESP,yang2023gd-mae} avoid this well-known information leakage by reconstructing the scene as a whole, which is computationally expensive. In contrast, NOMAE reconstructs the occupancy $O(v_\text{n},s)$ of all voxels $v_\text{n} \in \mathcal{V}_\text{n}$ within a certain neighborhood $n$ of visible voxels $\mathcal{V}_\text{v}$ at scale $s$. 
To achieve this, we employ a decoder $\mathbb{D}_\text{s}$ consisting of sparse convolution layers. 
\begin{align}
    \tilde{O}(v_\text{n},s) &= \mathbb{D}_\text{s}(\mathbb{M}_\text{u}(F)(s)),
\end{align}
where $\tilde{O}$ is the networks prediction of $O$.
Visible voxels of $\mathcal{V}_\text{v}$ are excluded from $\mathcal{V}_\text{n}$. 
An example is depicted in Fig.~\ref{fig:HMG_MSP_example}. 

\input{figure_latex/HMG_NeighOcc}

We note that $\mathcal{V}_\text{n}$ will not cover masked voxels in $\mathcal{V}_\text{m}$ that are not nearby visible voxels. This is an approximation that we make in our approach and we observed that LiDAR points in outdoor point clouds typically lie in the proximity of other points on the surfaces of objects. Hence, sufficient number of masked voxels in $\mathcal{V}_\text{m}$ are included in $\mathcal{V}_\text{n}$. Faraway isolated points do not contribute to the loss, which improves performance, as evaluated in the ablation study presented in Sec.~\ref{sec:ablations_neigh_size}. Our interpretation is that such isolated points belong to strongly occluded or masked objects and are infeasible to reconstruct, hence affecting the pretraining. 
This approximation allows us to avoid reconstructing large volumes of unoccupied space without leaking information about $\mathcal{V}_\text{m}$ to the decoder $\mathbb{D}_\text{s}$ at the same time. 

\subsection{Multi-Scale Pretext}
\label{sec:MSP}
GeoMAE~\citep{tian2023geomae} is the only prior work that exploits multiple hierarchical scales in the reconstruction during self-supervised training. 
Most other works~\citep{min2022OccupancyMAESP,yang2023gd-mae,hess2022voxelmae,tian2023geomae,boulch2023also} use a single scale for their pretraining tasks. This is unexpected since it is common practice in automated driving perception models to attach task heads to feature representations at different scales. The intuition is that finer resolutions are more suitable for small objects, such as pedestrians, while coarser resolutions are more suitable for larger objects, such as trucks. However, the multi-scale reconstruction in GeoMAE~\citep{tian2023geomae} is derived from a single feature map. 
%We find that it is better to base the multi-scale reconstruction on different scales of the feature encoding to account for the intuition that different feature scales are suitable for different object types. 
In our approach, we use $s$ instances $\mathbb{D}_\text{s}$ of the decoder architecture $\mathbb{D}$ with separate weights. Coupled with the neighborhood size being scale-dependent, this implicitly encourages the coarse-grained features to contain information from a larger area, while the fine-grained features contain more localized details.\looseness=-1

\subsection{Hierarchal Mask Generator}
\label{sec:HMG}
Generally, the scale of the masking can be different from the scale of reconstruction. For example, a random mask can be generated on a coarse scale and then upsampled to match the resolution of the reconstruction. However, experiments in \citep{hess2022voxelmae,yang2023gd-mae,tian2023geomae} showed that random masking on the same scale as the reconstruction scale performs best, which is intuitive. Consequently, we generate random masks for multiple scales in the MSP. These masks should be consistent to avoid information leakage, i.e., a voxel that is masked on a coarser scale should not be visible on a finer scale~\citep{zhang2022pointm2ae}. A straightforward manner to generate consistent masks would be to create a random mask for the finest scale and then derive the masks of coarser scales by defining a coarse-scale voxel as masked if all its corresponding fine-scale sub-voxels are masked. However, this would lead to rapidly decreasing masking ratios when moving to coarser scales because a single visible sub-voxel is sufficient to mask a coarser-scale voxel visible. Another alternative is masking at the coarsest scale and upsampling the mask~\citep{zhang2022pointm2ae} to finer scales which leads to a more consistent masking ratio across scales. However, we found that this approach rapidly decreased the size of reconstructed neighborhoods $n$ moving to finer scales.

Hence, we propose the masking scheme depicted in Fig.~\ref{fig:HMG}. We mask the coarsest scale first, using a random sampling of all occupied voxels $\mathcal{V}$, and the probability that a voxel is masked equals masking ratio $r$. Next, we take all voxels at scale $s-1$ within visible voxels of the previous, coarser, scale $s$ and repeat the sampling of additional masked voxels at scale $s-1$ with probability $r$.Consequently, the total masking ratio at scale $r_\text{t}(s)$ is approximately
\begin{align}
    \label{equ:total_masking_ratio}
    r_\text{t}(s) \approx 1-(1-r)^{S-s+1}.
\end{align}
By doing so, we ensure that coarser scales have a sufficient number of masked voxels without reducing the size of reconstructed neighborhoods at finer scales.
Only the visible voxels $\mathcal{V}_\text{v}$ of the finest scale are fed to the encoder backbone $\mathbb{E}$. An ablation study presented in Sec.~\ref{sec:ablation_MSP_HMG} quantifies the positive effect of HMG on the pretraining and downstream task performance. 

\subsection{Pretrainging Loss}
We use the Binary Cross Entropy loss (BCE) as the occupancy loss per scale. The final loss $L$ is the average of all single scale losses $L(s)$:
\begin{equation}
    L = \frac{1}{S} \sum_{s=0}^{S-1} \frac{1}{|\mathcal{V}_\text{n}(s)|} \sum_{v \in \mathcal{V}_\text{n}(s)} \text{BCE}(\tilde{O}(v,s),O(v,s)),
\end{equation}
with the ground truth occupancy $O(v,s) \in \{0,1\}$. 

