\clearpage
% \setcounter{page}{1}
\maketitlesupplementary


\section{Implementation Details}
\label{sec:imp_details}
In this section, we report further details of our implementation for both pretraining and finetuning. 
We use the Pointcept~\citep{pointcept2023} framework for pretraining and finetuning semantic segmentation. 
We performed the fine-tuning for object detection in the MMDetection3D framework~\citep{mmdet3d2020}.
\subsection{Training Settings}
\input{table/appendix/semseg_settings}
In Tab.~\ref{tab:appendix_semseg_settings}, we report the training details for both pretraining and semantic segmentation finetuning. For the criteria, the weights between brackets represent the weight of the loss in the final objective. During finetuning, we employ LLRD~\cite{he2022mae}, more specifically, we decay the learning rate of every block in the encoder exponentially with a factor of $0.65$ compared to the next block. For the downsampling blocks, we utilize the same learning rate as the next transformer block in the architecture. 

\input{table/appendix/objdet_settings}
In Tab.~\ref{tab:appendix_objdet_settings}, we highlight the pretraining and finetuning details for object detection experiments. We follow the standard 10 sweeps aggregation for nuScenes object detection~\citep{li2022uvtr}. To this end, we pretrain NOMAE on the aggregated point clouds and use a masking ratio of 85\% to accommodate for the denser point cloud. For other training settings, we utilize the same implementation details as our object detection baseline UVTR~\citep{li2022uvtr}.
\label{sec:further_ablaitons}
\subsection{Model Settings}
For the encoder $\mathbf{E}$, we use the same implementation details as the Encoder of PTv3~\citep{Wu2023PointTV}. For the upsampling module $\mathbf{M}_u$, we use a PTv3 decoder as a baseline and experiment with different sizes. We found empirically that reducing the size of the upsampling module improves the downstream performance after finetuning. Hence, we utilize a single transformer block per resolution in the upsampling module.

The neighboring decoder uses sparse convolution layers to generate representations for neighboring voxels $\mathcal{V}_n$ and a single sub-manifold convolution layer to convert the representation into occupancy predictions. 
Sec.~\ref{sec:neigh_dec_ablations} gives further details of the hyperparameter choices of the neighborhood decoder. 
\input{table/appendix/class_wise_results}
\subsection{Data Augmentations}
\input{table/appendix/data_augmentation}
We adopt common data augmentation methods used in~\citep{Wu2023PointTV,li2022uvtr} during fine-tuning semantic segmentation and object detection. 
Pretraining uses the same data augmentations as semantic segmentation finetuning. 
Details about the data augmentations used are reported in Tab.~\ref{tab:appendix_data_augmentation}.

\section{Further Ablations}
In this section, we present further ablation studies on the nuScenes semantic segmentation validation set. 
We evaluate the settings by freezing weights in the encoder after pretraining and using NonLP as described in Sec.~\ref{sec:nonLP}.

\input{table/appendix/batch_size}

{\parskip=2pt
\noindent\textbf{Pretraining Batch Size}: 
\label{sec:ablations_batch_size}
This experiment investigates the effect of different batch sizes for our SSL task. Tab.~\ref{tab:appendix_batch_size} compares the downstream performance of models pretrained with different batch sizes and the same number of epochs (leading to a different number of parameter update steps). It can be observed that for NOMAE, the performance increases with decreasing the batch size (even for a single scale pretraining), and it reaches a maximum of 8 examples per batch before further decreasing. We attribute the higher performance at lower batch sizes to an increased number of parameter update steps. The results also indicate that NOMAE can benefit from a longer training schedule.}

{\parskip=2pt
\noindent\textbf{Neighboring Decoder Design}: 
\label{sec:neigh_dec_ablations}
\input{figure_latex/neigh_dec}
This experiment investigates the effect of design choices for the neighboring decoder. Fig.~\ref{fig:appendix_neigh_dec} shows the general architecture of the neighboring decoder. 
It consists of multiple sparse convolution layers to yield representations for $\mathcal{V}_\text{n}$ from $\mathcal{V}_\text{v}$, followed by sub-manifold convolution layers to predict the occupancy from the representation. 

First, we vary both the network depth $m$ and the size of the neighborhood expansion per layer $k$ (layer receptive field). 
The total size of the neighborhood is given by
\begin{equation}
    n=2m(k-1) + 1.
\end{equation}
Tab.~\ref{tab:appendix_neigh_dec} shows that a shallower decoder helps to learn a better representation compared to a deeper one with the same neighborhood size. 
%However, we could not implement an even shallower network due to the limitations of the current Spconv library~\citep{spconv2022}.

Furthermore, we investigate the number of sparse submanifold convolution layers $c$ in the decoder. 
$c=0$ indicates that the sparse convolutions must directly predict the neighboring occupancy $\tilde{\mathcal{O}}(s)$. 
Tab.~\ref{tab:appendix_neigh_dec_head} shows that having at least one sparse submanifold convolution layer improves the downstream performance.
Adding more layers performs on par with the single layer despite the extra computation cost.
}
\subsection{Detailed Results}
In this section, we report detailed results for nuScenes semantic segmentation on the test set, in comparison to the validation set.
In particular, we examine the influence of the class \textit{bicycle} on the overall performance and its impact on the test mIoU score. 

In Tab.~\ref{tab:appendix_classwise_results}, we observe that NOMAE outperforms the previous baseline PTv3~\citep{Wu2023PointTV} on almost all classes. 
The performance of the \textit{bicycle} class is lower than for other classes for both PTv3 and NOMAE. 
We attribute this to \textit{bicycle} being a rare class in the nuScenes dataset. 
On the test set, however, state-of-the-art approaches achieve relatively high scores for the \textit{bicycle} class, thereby optimizing the mIoU score, which averages class-wise results irrespectively of the object's frequency in the dataset. We did not take any special measures to address this for minority classes during fine-tuning NOMAE. 
As a consequence, the final mIoU score of NOMAE is on par but slightly lower than the best leaderboard submission, while achieving similar or better performance than PTv3~\citep{Wu2023PointTV} on the other classes. 
As a result, NOMAE sets the new state-of-the-art regarding frequency-weighted IoU (fwIoU) on the test set. 

\input{table/appendix/neigh_dec}
\input{table/appendix/neigh_dec_head}

\section{Generalization to Different Architectures}
\label{sec:generalization}
In this section, we present more quantitative results on the Nuscenes Semseg task for pretraining and finetuning different architecutres (MinkUnet\,\cite{ChoyGS2019mink}, OACNN\,\cite{Peng2024OACNNsOS} and OctFormer\,\cite{Wang2023OctFormerOT}). We choose another transformer based architecture and two different CNN based ones. For fair comparison with the older architectures we reproduce their results for training from scratch using the modern optimizers and schedulers which already improves their performance by whopping 5.3 points. For the modern ones, the results are consistent with the original works. In Tab.~\ref{tab:appendix_different_architectures} we can see that NOMAE consistently improves the downstream performance by $\sim 1.4-2.2$ mIoU points compared to training from scratch. It's worth mentioning that such improvement comes without tuning any of the pretraining hyperparameters, demonstrating the superb generalization capabilities of NOMAE pretraining. We hypothesize that the generalization capabilities arises from the diverse feature learning promoted by our MSP (see Sec.~\ref{sec:MSP})

\begin{table}[h]
    \centering
    \footnotesize
    \vspace{-0.6em}
    % \addtolength{\tabcolsep}{-0.5em}
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Model} & \multicolumn{2}{c}{Scratch(paper)} & \multicolumn{2}{c}{Scratch(ours)} & \multicolumn{2}{c}{+ NOMAE}\\
    & \textbf{mIoU} & \textbf{mACC} & \textbf{mIoU} & \textbf{mACC}& \textbf{mIoU} & \textbf{mACC} \\
    \midrule
      MinkUnet\,\cite{ChoyGS2019mink} &73.3&-&78.6&83.9&80.1&86.2\\ % number of original paper
      OACNNs\,\cite{Peng2024OACNNsOS}   &78.9&-&78.8&85.8&80.9&86.4\\ 
      % Octformer\,\href{https://ojs.aaai.org/index.php/AAAI/article/view/25121/24893}{[44]} &-&-&78.4&86.9&80.2&\textbf{88.2}\\
      Octformer\,\cite{Wang2023OctFormerOT} &-&-&79.4&87.0&81.6&\textbf{88.8}\\ 
      PTv3\cite{Wu2023PointTV} &80.4&-&80.4&87.3&\textbf{81.8}&87.7\\ 
    \bottomrule
    \end{tabular}
    \caption{Results of pretraining and finetuning different architectures on Nuscenes Semseg val set compared to training from scratch.}
    \label{tab:appendix_different_architectures}
    \vspace{-1.1em}
\end{table}

\section{Multi-Scale SSL}
In this section we compare between our formulation of multi-scale SSL (assigning different granularity reconstruction targets to different feature levels at different scales, and the formulation of multi-scale SSL in GeoMAE\,\cite{tian2023geomae} (assigning different granularity reconstruction targets to a single feature level). Intuitively, our formulation encourages feature diversity between different feature levels. While GeoMAE's formulation improves the semantics of a single layer level, yet it doesn't promote feature diversity between different layers and can make said layer a bottleneck. In Tab\,\ref{tab:appendix_msp_vs_geomae} we quantify the improvement from each formulation using neighborhood occupancy as the pretext. The results show that the gain from MSP is $\sim3\times$ GeoMAEâ€™s multi-scale (MS) on nuScenes Semseg val set. This demonstrates the importance of assigning different tasks to different feature levels for an effective SSL on large-scale point clouds.

\begin{table}[h]
    \centering
    % \addtolength{\tabcolsep}{0.5em}
    \footnotesize
    \begin{tabular}{cccc}
\toprule
        &SS $2^s=8$ NOMAE& +GEO-MAE's MS& +our MSP\\
        \midrule
        NonLP mIoU&68.3 & 69.7 & \textbf{72.6}\\
        \bottomrule
    \end{tabular}
    % \vspace{-1.6em}
    \caption{Results on nuScenes val set for NonLP mIoU using Neighbourhood occupancy as pretext task and different Multi-Scale formulations}
    \label{tab:appendix_msp_vs_geomae}
\end{table}

\section{NOMAE Improves Sample Efficiency}
The results of our baseline PTv3 in Tab.\,\ref{tab:sota_semseg} are taken from the original paper~\cite{Wu2023PointTV}, To demonstrate that the improvement from NOMAE pretraining is not merely due to faster convergence of the pretrained weights, 
we report additional results for training the model from scratch using longer schedules of $50$, $100$, and $200$ epochs. 
The results in Tab.\,\ref{tab:appendix_longer_scratch} show no significant improvement from longer training, confirming that just 50 pretraining epochs of NOMAE followed by 50 epochs of finetuning improves sample efficiency and reaches higher final performance. We would like to highlight to the reader that we did not experiment with longer pretraining schedules and we suspect that further improvement can be brought by it.

\begin{table}[h]
    \centering
    \footnotesize
    % \vspace{-0.7em}
    \begin{tabular}{cccccc}
    \toprule
        PTv3Sup,50 ep~\cite{Wu2023PointTV}& Sup, 100ep& Sup, 200ep & NOMAE+50ep\\
        \midrule
        80.4 & 80.7 & 80.6& \textbf{81.8}\\
        \bottomrule
    \end{tabular}
    \caption{Results on Nuscenes SemSeg val set for longer training from scratch compared to NOMAE pretraining}
    \label{tab:appendix_longer_scratch}
    % \vspace{-1em}
\end{table}

\input{figure_latex/appendix_semseg_qualitative}
\input{figure_latex/appendix_objdet_qualitative}
\section{Additional Object Detection Results}
In this section we report more results on the object detection downstream task. We use the same pretrained model as in Sec.\,\ref{sec:benchmarking_results}, however we finetune the model using 100\% of the labelled data. Additionally we use all modern strategies commonly used for object detection such as CBGS\,\cite{zhu2019cbgs}, and Copy-Paste augmentation\,\cite{yan2018second}. We compare between NOMAE pretraining followed by 5 epochs fineuning and traning from scratch for 20 epochs (any longer schedule does not provide improvement). We also perform the same comparison using the commonly used CNN based SparseEncoder backbone.
The results in Tab.\ref{tab:appendix_objdet} shows that NOMAE brings consistent improvement to both NDS and mAP irrespective of the backbone architecture. Using NOMAE pretraining UVTR+PTv3 backbone achieves new SOTA for object detection on nuScenes val set.

\begin{table}[h]
    \centering
    \footnotesize
    % \caption{Objdet mAP and NDS for 20\% labeled data, no CBGS, and no object sample}
    % \addtolength{\tabcolsep}{-0.25em}
    % \vspace{-0.7em}
    \begin{tabular}{l|c|c}
    \toprule
        NDS/mAP&{Scratch 20ep}&{NOMAE +5ep}\\
        
        \midrule
        UVTR+PTv3&69.3/64.1& \textbf{71.2}/\textbf{67.0} \\
        UVTR+SparseEncoder~[37]&67.7/60.9& 70.4/65.3 \\
        \bottomrule
    \end{tabular}
    % \vspace{-1.0em}
    \caption{Results on nuScenes object detection val set for training from scratch compared to NOMAE pretraining.}
    \label{tab:appendix_objdet}
\end{table}

\section{Qualitative Analysis}
\label{sec:further_qualitative}
In this section, we present more qualitative results. 
Fig.~\ref{fig:appendix_semseg_qualitative} shows results for semantic segmentation and Fig.~\ref{fig:appendix_objdet_qualitative} object detection. 
For semantic segmentation, we observe the general trend of NOMAE correctly classifying smaller objects that are misclassified by the baseline (examples (c), (e), and (g)). Additionally, the boundaries between different semantic classes are more pronounced in the case of NOMAE (examples (a), (d), and (f)), which shows the importance of self-supervision at finer resolutions. We can also observe that NOMAE helps with the mixing between similar classes, such as trucks and buses in example (b), as well as different ground types in other examples.  
NOMAE also more accurately estimates the orientation of the objects and has higher true positive detections in the case of object detection.
