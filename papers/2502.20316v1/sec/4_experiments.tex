\section{Experiments}
\label{sec:experiments}

\worrynomore{In this section, we discuss the datasets, metrics, and the evaluation protocol that we use for benchmarking. We compare our proposed approach with state-of-the-art methods in the benchmarks and present extensive ablation studies to demonstrate the novelty of our contributions.}

 \begin{table*}
    \centering
    \footnotesize
    \caption{Comparison of LiDAR semantic segmentation performance on the nuScenes and Waymo Open datasets. For the first time, an SSL pretraining method outperforms strong supervised learning models. Methods marked with $^{*}$ are our implementation.}
    \vspace{-0.5em}
    \input{table/semseg}
    \vspace{-0.5em}
    \label{tab:sota_semseg}
\end{table*}

\subsection{Datasets and Evaluation Metrics}
%This section introduces the datasets and metrics used for pretraining and evaluating our approach.

{\parskip=2pt
The \textbf{nuScenes} dataset~\citep{caesar2020nuscenes} is a challenging dataset due to the sparsity of the LiDAR point cloud. It consists of 700 driving sequences for training, 150 for validation, and 150 for testing, with annotations for a variety of tasks. We evaluate both semantic segmentation and object detection on the nuScenes dataset. We use the mean intersection over union (mIoU) as the main evaluation metric for semantic segmentation and the nuScenes detection score (NDS) and mean average precision (mAP) for 3D object detection.}

{\parskip=2pt
The \textbf{Waymo Open Dataset}~\citep{sun2020wod} is a large-scale autonomous driving dataset. 
It consists of 798 driving sequences for training, 202 validation sequences, and 150 test sequences. 
% The dataset features 3D point clouds from $5$ LiDAR sensors, however, we use only the point cloud of a single LiDAR sensor for our method. 
% Yet, we compare against any state-of-the-art method, including methods using all $5$ LiDARs, which puts us into a slight disadvantage. 
%\TODO{which hyperparameter settings? I think the reason is that we are not developing a method for semi labeled frames} 
% Our reason to use only a single LiDAR is to keep the hyperparameter settings in the experiments as consistent as possible across datasets. 
%For simplicity, we keep the settings in the SSL pretext and the fine-tuning equal
We use the mean intersection over union (mIoU) and mean accuracy (mAcc) as the main metrics for evaluating the semantic segmentation performance.}

\subsection{Task Heads}

This section discusses the methods to evaluate the effectiveness of the pretraining and the quality of the learned representation.

{\parskip=2pt
\noindent\textbf{Fine-tuning}: 
\label{sec:fine-tune}
In our comparisons with state-of-the-art methods, fine-tuning follows the self-supervised pre-training of the encoder. For this purpose, a task-specific head is added with randomly initialized weights, and both the encoder $\mathbb{E}$ and the task-specific head are trained using the annotated dataset. We use the same head as PTv3~\citep{Wu2023PointTV} for the semantic segmentation tasks and the same head as our baseline UVTR~\citep{li2022uvtr} for the object detection task. 
We use layer-wise learning rate decay (LLRD)~\cite{he2022mae} to avoid forgetting the SSL representations in the encoder.}

{\parskip=2pt
\noindent\textbf{Non-linear Probing}: 
\label{sec:nonLP}
% The purpose of our ablation studies is to evaluate the learned representation from our pretext task, on the semantic segmentation task. 
% Therefore, the encoder is kept frozen after pre-training, i.e., no fine-tuning. 
% Furthermore,following Probe3D~\citep{ElBanani2024ProbingT3} and the earlier insights of earlier works~\citep{he2022mae,chen2021simsiam}, a small voxel-wise multilayer perceptron (MLP) probes the features of voxels individually. Using an voxel-wise MLP avoids that the representation learning happens in the head while still probing the stronger but non-linear features, correlating better with transformer performance~\citep{he2022mae,chen2021simsiam}. 
% The feature tokens of all scales are aggregated (concatenated) before passing them to the MLP, after up-sampling tokens of coarser scales. 
% This MLP (the non-linear probing, NonLP) is trained for a few epochs using annotated data. 
The purpose of the ablation study is to evaluate the learned representation from our SSL approach on the downstream semantic segmentation task. 
Therefore, the encoder is kept frozen after pre-training, i.e., no fine-tuning. 
Following Probe3D~\citep{ElBanani2024ProbingT3} and the insights of earlier works~\citep{he2022mae,chen2021simsiam}, we use a multi-scale non-linear probe (NonLP) instead of the commonly used linear probing protocol. NonLP aggregates the feature tokens of all scales after up-sampling tokens of coarser scales before passing them to a voxel-wise small MLP. NonLP avoids that the representation learning happens in the head. Still, it is probing the stronger but non-linear features, correlating better with transfer performance~\citep{he2022mae,chen2021simsiam}.
Similar to the commonly used linear probe, NonLP is trained for a few epochs using annotated data.}

\subsection{Implementation Details}
\label{sec:implementation_details}
We perform the experiments for semantic segmentation in the Pointcept~\cite{pointcept2023} framework and in the MMDetection3D~\citep{mmdet3d2020} framework for the object detection task. We use PTv3~\cite{Wu2023PointTV} as the encoder $\mathbb{E}$, unless stated otherwise. We use a single NVIDIA A100 GPU for the pretraining. 
We use $S=4$ for the reconstruction and masking scales, corresponding to target voxel sizes of $\{0.05,0.10,0.20,0.40\}$ meters. The input voxel dimension is $0.05$ meters. The masking ratio $r_\text{s}$ of the finest scale is $70\%$ for nuScenes and $85\%$ for Waymo. In the self-supervised pre-training, the decoder $\mathbb{D}$ consists of sparse convolution layers~\citep{spconv2022} of kernel size $5$, followed by a single sparse submanifold convolution layer to generate $\tilde{O}$. We use common augmentation techniques such as rotation, scaling, and jittering from the semantic segmentation literature~\cite{ChoyGS2019mink,tang2020spvnas,Wu2023PointTV} during pretraining. 
% For object detection we use MMDetection3D~\citep{mmdet3d2020} codebase. We use our encoder as the backbone in a UVTR~\citep{li2022uvtr} detector and fine-tune the detector using the same settings as in UVTR~\citep{li2022uvtr} with the addition of the LLRD and without the use of CBGS~\citep{zhu2019cbgs} strategy and cut-and-paste~\citep{zhou2018voxelnet} augmentation.

\def\objdetsec{tf} % write tt for sota and tf for alternative

\if\objdetsec
\input{table/objectdet_sota}
\else
\input{table/objectdet_ssl}
\fi

\subsection{Benchmarking Results}
\label{sec:benchmarking_results}

In this section, we present the benchmarking results for both 3D semantic segmentation and 3D object detection.

{\parskip=2pt
\noindent\textbf{3D Semantic Segmentation}: 
%At the date of submission, the best-performing model on the leaderboards for the LiDAR semantic segmentation task on both nuscenes and Waymo Open benchmarks is PTv3\cite{Wu2023PointTV}, which we use as our baseline. 
% Tab.~\ref{tab:sota_semseg} summarizes the best-performing methods on the nuScenes and Waymo Open leaderboards for the LiDAR semantic segmentation task. 
% Our model is fine-tuned as described in Sec.~\ref{sec:fine-tune}. 
% The self-supervised pretext of this paper adds  $0.8$ and \TODO{} mIoU points on the nuScenes validation and test set over the previous state of the art. 
% On the Waymo Open dataset, the improvement of our method is $0.2$ and \TODO{} points on the validation and test set, respectively.  
% It sets a new SOTA for nuScenes and Waymo, even with a single LiDAR's point cloud.
Tab.~\ref{tab:sota_semseg} summarizes the best-performing methods on the nuScenes and Waymo Open Dataset leaderboards for the LiDAR semantic segmentation task. The results include the most performant self-supervised pretraining methods. We fine-tuned our model as described in Sec.~\ref{sec:fine-tune}. We observe that our proposed self-supervised pretraining achieves a mIoU score of $81.8$ on the nuScenes validation set, adding $1.4$ mIoU points over the baseline and setting the state-of-the-art. The results on the nuScenes semantic segmentation test set are on par with the strong PTv3 baseline for the mIoU score and outperforms it in the frequency-weighted IoU (fwIoU) score by 0.4$\%$. 
The improvement on the test set is lesser than the validation dataset because NOMAE has relatively poor performance for the minority class \textit{bicycle}, and we did not make special adaptations for the test set submission.}

On the Waymo Open dataset val set, our method achieves a mIoU score of $72.3$ and mAcc of $75.2$. NOMAE improves by $1.0$ and $2.0$ points in the mIoU and mAcc, respectively, over the baseline PTv3~\citep{Wu2023PointTV}. The current version of the semantic segmentation challenge is relatively new for the Waymo Open dataset, with only a few submissions. With 70.3$\%$ mIoU score on the test set, NOMAE sets a new state-of-the-art on the Waymo Open Dataset single frame semantic segmentation challenge.

\begin{table}
    \centering
    \footnotesize
    \caption{Comparison of different SSL methods using the same backbone with NonLP. The iteration time (iter.time) is computed using a batch size of 1. Sup.res is the finest resolution of pretraining supervision.}
    \vspace{-0.5em}
    \input{table/other_semseg}
    \vspace{-1.0em}
    \label{tab:other_semseg}
\end{table}

{\parskip=2pt
\noindent\textbf{3D Object Detection}:} 
\if\objdetsec
We present results for object detection on the nuScenes validation set using the fine-tuning approach described in Sec.~\ref{sec:fine-tune}. Other training settings are the same as the baseline UVTR~\citep{li2022uvtr}. In our experiment, we use a larger voxel size of \SI{0.1}{\meter} compared to the more commonly used \SI{0.075}{\meter}. We also do not use test-time augmentation or model ensembling. 

Tab.~\ref{tab:objectdet_sota} shows that our approach achieves 69.7 and 63.7 NDS and mAP, improving by 3.3 NDS points and 2.8 mAP points over the UVTR-L-V0.1 ~\citep{Wu2023PointTV} baseline. Putting NOMAE close to state-of-the-art methods despite the larger voxel size. This showcases the ability of NOMAE to learn a representation suitable for object detection.
\else
We present results for object detection on the nuScenes validation set using the fine-tuning approach described in Sec.~\ref{sec:fine-tune}. We follow the experiment setup of GD-MAE
~\citep{yang2023gd-mae} and UniPAD~\citep{Yang2023UniPADAU}, utilizing only 20\% of the annotated frames during fine-tuning, without the use of CBGS~\citep{zhu2019cbgs} or Copy-and-Paste\citep{yan2018second} augmentation. 
We adopt the same training settings as the baseline UVTR~\citep{li2022uvtr} and do not use test-time augmentation or model ensembling.  

Tab.~\ref{tab:objectdet_ssl} shows that our approach achieves 60.9 and 54.4 NDS and mAP scores respectively, improving by 14.2 in NDS points and 15.4 in mAP points over the UVTR-L~\citep{Wu2023PointTV} baseline, and by 5.1 NDS points and 5.6 mAP over the closest contrastive SSL method Learning-from-2D~\citep{Yang2023UniPADAU}. The significant improvement demonstrates the effectiveness of our proposed localized multi-scale SSL for 3D object detection with limited annotated data. 
\fi

\subsection{Comparison with SSL Methods}
In this experiment, we compare the performance of our proposed method with the self-supervised pretraining methods of Occupancy-MAE~\citep{min2022OccupancyMAESP} and GeoMAE~\citep{tian2023geomae}, with the same encoder architecture of PTv3~\citep{Wu2023PointTV}. 
Tab.~\ref{tab:sota_semseg} shows that our re-implementation of Occupancy-MAE~\citep{min2022OccupancyMAESP} and GeoMAE~\citep{tian2023geomae} with the state-of-the-art PTv3~\citep{Wu2023PointTV} backbones (marked with $^{*}$) outperforms the results reported in the original papers by $0.3$ and $7.1$ mIoU points respectively for semantic segmentation with fine-tuning on the nuScenes dataset. 

Tab.~\ref{tab:other_semseg} shows the results of non-linear probing (NonLP). 
We observe that the NonLP performance in Tab.~\ref{tab:other_semseg} correlates with the fine-tuning results in Tab.~\ref{tab:sota_semseg}. Furthermore, the relative performance improvement of NOMAE over the baselines is higher for NonLP in comparison to fine-tuning, which indicates richer representation learning. Additionally, the time of a single training step (iteration time) is lowest for NOMAE, despite the multi-scale pretraining and a much finer resolution of the pretraining supervision. We note that the iteration time of NOMAE is $10$ms for the pretraining with a single scale. 



\begin{table}
    \centering
    \footnotesize
    \caption{Ablation study on the various components in NOMAE for semantic segmentation on the nuScenes validation set. Lines with * are with fine-tuning, and all the other results are with NonLP. For more details refer to Sec.~\ref{sec:incremental_design}}
    \vspace{-0.5em}
    \input{table/incremental}
    \vspace{-1.0em}
    \label{tab:incremental}
    
\end{table}
% We note that fine-tuning performance is still worse than training PTv3 from scratch, which we attribute to the lower learning rate in the LLRD fine-tuning strategy.  

\subsection{Ablation Study}
\label{sec:ablations}
In this section, we present ablation studies on the nuScenes semantic segmentation validation set to investigate the design choices of the proposed method. We performed the experiments using the pre-trained frozen encoder using NonLP. Please refer to Sec.~\ref{sec:nonLP} for further details. 

{\parskip=2pt
\noindent\textbf{Detailed Study of NOMAE}: 
\label{sec:incremental_design}
This experiment evaluates the improvement due to our proposed contributions, and the results are presented in Tab.~\ref{tab:incremental}. 
We start from our implementation of Occupancy-MAE~\citep{min2022OccupancyMAESP} as in Tab.~\ref{tab:other_semseg}. Reconstructing only the local neighborhood $\mathcal{V}_\text{n}$ of visible voxels $\mathcal{V}_\text{v}$ increases the NonLP mIoU from 59.0$\%$ to 66.7$\%$. This requires replacing the Occupancy-MAE decoder with our proposed upsampling module and neighborhood decoder. Adding the multi-scale reconstruction (MSP) from Sec.~\ref{sec:MSP} further improves the mIoU to 70.1 for naive mask construction and to 70.5 for masking strategy from Point-M2AE~\cite{zhang2022pointm2ae}, as opposed to single-scale reconstruction in Occupancy-MAE. Our proposed hierarchical mask generation from Sec.~\ref{sec:HMG} yields an improvement of 72.46 mIoU, as further investigated in Sec.~\ref{sec:ablation_MSP_HMG}. Moreover, increasing the reconstruction neighborhood size from $5$ to $9$ improves the mIoU to $74.8$, as further investigated in Sec.~\ref{sec:ablations_neigh_size}. Reducing the batch size to 8 (further investigated in Sec.~\ref{sec:further_ablaitons} of the supplementary material) yields the final NonLP performance of 74.8$\%$ and fine-tuning performance of 81.8$\%$ mIoU score.}

\input{table/mask_block_size}

{\parskip=2pt
\noindent\textbf{Mask Block Size, MSP and HMG}: 
\label{sec:ablation_MSP_HMG}
This experiment investigates different reconstruction and mask scales $s$ for our SSL task. Tab.~\ref{tab:mask_block_size} compares the performance of reconstructing different {\sl single} scales with the proposed multi-scale pretext (MSP) from Sec.~\ref{sec:MSP}. The experiment of MSP without hierarchical mask generation (HMG) uses either a random mask at the finest scale, which is pooled to generate masks of coarser scales, as described in the naive solution in Sec.~\ref{sec:HMG}, or the method of Point-M2AE~\cite{zhang2022pointm2ae}.}

It can be observed that no single-scale occupancy task is suitable for all object types. For example, trucks benefit from a coarser occupancy reconstruction, while pedestrians prefer a finer resolution in the pretraining task, except for the very fine scales of $2^s\in\{1,2\}$. MSP combines coarse and fine tasks, thereby maximizing the overall mIoU. We observe that HMG achieves an additional improvement of 2.4 and 2.1$\%$ mIoU over random mask at the finest scale and the mask generation proposed in Point-M2AE~\citep{zhang2022pointm2ae} respectively. This underlines the importance of proper training examples at all scales.

\input{figure_latex/neigh_size}

{\parskip=2pt
\noindent\textbf{Neighborhood Size}:}
\label{sec:ablations_neigh_size}
% For example, $n=5$ and $s=1$ means that a total volume of $5\cdot 0.05\cdot2^1~\text{m}$ is covered in all 3 (x,y,z) dimensions. 
% Experiments in Fig.~\ref{fig:neigh_size} are conducted With the use of single scale, without MSP and HMG. 
% We note that $n \rightarrow \infty$ corresponds to a reconstruction of the complete volume as in Occpuancy-MAE~\cite{min2022OccupancyMAESP}.
% We choose \TODO{$s=xxx$}, such that this experiment is consistent with the design in \cite{min2022OccupancyMAESP}.
Fig.~\ref{fig:neigh_size} investigates the effect of the neighborhood size (number of voxels) to generate $\mathcal{V}_\text{n}$ from $\mathcal{V}_\text{v}$. For example, $n=5$ indicates that a total of 5 voxels are covered in all 3 (x,y,z) dimensions for every scale. Experiments use MSP and HMG. Since every scale has a different voxel size, the reconstructed volume depends on the scale. We observe a maximum of the NonLP performance for $n=9$. 
Our interpretation is that a smaller neighborhood does not cover sufficient LiDAR measurements for reconstruction, while a larger neighborhood hinders local representations in the pretraining. Further, we observe that limiting the reconstruction size performs consistently better than reconstructing the whole space, which would correspond to the method of Occupancy-MAE~\citep{min2022OccupancyMAESP}.
\input{figure_latex/masking_ratio}
\input{figure_latex/qualitative}

{\parskip=2pt
\noindent\textbf{Masking Ratio}: 
\label{sec:masking_ratio}
Fig.~\ref{fig:masking_ratio} investigates the effect of different masking ratios on the quality of the representations for semantic segmentation on the Waymo and nuScenes datasets. This experiment uses a constant $r(s)=r$ to achieve the total masking ratio $r_\text{t}$ according to Eq.~\ref{equ:total_masking_ratio}. 
We observe that the optimal total masking ratio is $70\%$ for the nuScenes and $85\%$ for the Waymo Open dataset. This is intuitive since the point cloud density in Waymo (ca. $180$k points per frame) is higher than the relatively sparse nuScenes (ca. $34$k points per frame) LiDAR point clouds.}
%We investigate the effect of different masking ratios on the quality of the representations, on Waymo Open and nuScenes semantic segmenations. Table.~\ref{tab:masking_ratio} shows the results. We tune the $r_t$ and compute $r_s$ using Equ.,~\ref{equ:total_masking_ratio} assuming constant $r_s$ across scales. from the results we observe that for nuScenes performance peaks around 70\% and $85\%$ total masking for Waymo. Which seems to suggest that the optimal masking ratio is dataset dependent and depends on the density of the point cloud. Waymo having on average 
%\approx$180k points per frame compared to arounf $\approx$34k in nuScenes.

{\parskip=2pt
\noindent\textbf{Data Efficient nuScenes}: 
\label{sec:data_efficient_nuscenes}
Tab.~\ref{tab:model_efficiency} analyzes the performance under limited annotated data for semantic segmentation on the nuScenes dataset. The sub-sampling of the data is performed sequence-wise, meaning that all frames of a sequence are either included or excluded. For example, $0.1\%$ indicates that only one of the $1000$ scenes of the nuScenes dataset is used, namely \textsl{scene-0392}. 
%The number of learning steps are adapted for every experiment. 
In every experiment, all sequences of the training set are used for the self-supervised pertaining. We observe that NOMAE (fine-tune) consistently outperforms training from scratch, which demonstrates the method's ability to benefit from unannotated data. Furthermore, we observe that in experiments with very little data, NonLP performs similarly to fine-tuning. This suggests that NOMAE can benefit from the unannotated data to learn a sufficiently strong representation in the encoder, such that the fine-tuning of the encoder has little benefit.}

\input{table/nuscenes_data_efficient}
%We investigate the performance of NOMAE under limited annotated data on the nuScenes semantic segmentation task. We follow the standard practice of uniformly sampling the scenes (not the frames) to account for the sequential nature of the LiDAR data~\cite{meyer2019lasernet,Gao2024LeveragingID}. We construct the annotation limited splits using the sampled scenes. We conduct our experiments on 4 different splits \{0.1\%,1\%,10\%,50\%\} with the 0.1\% split being represented by 'scene-0392'. After pretraining on all the scenes We evaluate both NonLP and fine-tuning mAP on nuScenes validation set using only the limited number of annotated training scenes. We scale the number of epochs per split to have suffiecent learning steps. We observe that NOMAE (fine-tune) consistently outperforms training from scratch, which showcases the method's ability to beneift from unannotaed data. futhermore, we observe that in the extremly data-scare experiments,NoneLP seems to perform similar to fine-tuning suggesting that NOMAE can beneift from large volumes of unannotated data to learn a sufficiently strong representation without further fine-tuning using annotated data.


%\subsubsection{Neight Decoder}
%\subsubsection{Numbers of Blocks in Decoder}
%\subsubsection{Attention Window size}

\subsection{Qualitative Evaluations}
We present qualitative comparisons in Fig.~\ref{fig:qualitative}. In (a) and (b) for semantic segmentation, we observe that NOMAE improves the accuracy by reducing class mix-up and by improving the boundaries between objects. In (a), we observe that the baseline mis-segments the truck while NOMAE accurately recognizes it. In (b), we see that NOMAE recognizes the smaller object missed by the baseline (the bottom left pole). We can also see that it fails to recognize some drivable areas in (a). In Example (c), we visualize the detections from NOMAE. We see that compared to the baseline, NOMAE is able to more accurately estimate the orientation of the objects and has higher true positive detections. We also see both models hallucinating in further away regions (on the left), and NOMAE fails to detect the pedestrian on the left. More qualitative results are presented in Sec.~\ref{sec:further_qualitative} of the supplementary material. 

