\section{Introduction}
\label{sec:intro}

% Automated driving and driver assitance systems often employ range sensors that produce 3D point clouds, such as LiDAR and radar for environment perception. The depth information readily available by those sensors makes them   

\input{figure_latex/teaser}

Sensors that generate point clouds, such as LiDARs or radars, have become a cornerstone in automated driving as they provide high-resolution three-dimensional representations of the environment~\cite{schramm2024bevcar}. The rich spatial information represented in point clouds enables vehicles to accurately detect and classify objects, navigate complex environments, and enhance safety through real-time situational awareness. However, annotated point cloud datasets are significantly smaller than their image-based counterparts, which makes learning large-scale perception models extremely challenging. Self-supervised learning (SSL)~\citep{bao2021beit,chen2020simclr,he2020moco,chen2021simsiam,he2022mae,tian2023spark,chen2020improved,gao2022convmae,gosala2023skyeye,lang2024self,lang2023self}, through contrastive learning or masked modeling, provides an effective solution to this problem by learning meaningful representations from vast amounts of unlabeled data. SSL also reduces the reliance on arduous annotation processes while improving performance and generalization. Pioneering works~\citep{liu2022maskpoint,pang2022pointmae,yu2022pointbert,zhang2022pointm2ae} have successfully employed SSL to small-scale indoor point clouds, with more recent efforts extending it to large-scale outdoor point clouds~\citep{hess2022voxelmae,xv2023mvjar,tian2023geomae}. 

Outdoor point clouds, however, pose a unique challenge for masked modeling as most of the measured 3D volume is empty space. Current approaches resort to reconstructing precise point locations within occupied voxels~\citep{hess2022voxelmae,xv2023mvjar,tian2023geomae}, but this often leaks information to the decoder, signaling that the queried voxel is occupied.  Recent methods~\citep{yang2023gd-mae,min2022OccupancyMAESP} attempt to overcome this problem by reconstructing the entire scene, but the computational complexity and class imbalance caused by the large number of empty voxels limit these approaches to either 2D bird's-eye-view representations or coarse-grained 3D reconstructions.
%The computational complexity and the class imbalance of a large number of empty voxels limit these approaches to a 2D bird's eye view representation or coarse-grained 3D reconstruction. 
%GD-MAE~\citep{yang2023gd-mae} and Occupancy-MAE~\citep{min2022OccupancyMAESP} try to overcome this issue by reconstructing the whole scene by going from a sparse to a dense feature space. 
%In order to reduce complexity and address the imbalance introduced by the high number of empty voxels, they resort to a 2D bird's eye view representation or coarse-grained 3D reconstruction, respectively. 

In this work, we propose \textbf{N}eighborhood \textbf{O}ccupancy \textbf{MAE} (NOMAE), the first multi-scale sparse self-supervised learning framework for LiDAR point clouds that directly addresses the problem of 3D point cloud sparsity in masked modeling. The novelty in NOMAE lies in the concept that the occupancy of fine-grained 3D voxels is only evaluated (loss) in the neighborhood of visible (not masked) occupied voxels. This is motivated by the fact that LiDAR points are typically clustered in the proximity of other LiDAR points in outdoor driving scenarios. Thereby, we avoid leaking information about masked voxels to the decoder and eliminate the need for dense feature spaces or reconstructing large unoccupied areas. This makes our approach lightweight and usable with more modern sensors that have finer resolutions as well as state-of-the-art 3D transformer architectures.
Our framework employs self-supervision at multiple scales, made feasible by the lightweight nature of our reconstruction task. 
%We use masking and reconstruction at multiple scales simultaneously, which further improves the quality of our learned representation and the performance on downstream tasks. 
To facilitate this, we introduce a hierarchical mask generation module that is suitable for multi-scale SSL. We perform extensive experiments with NOMAE on the competitive nuScenes~\citep{caesar2020nuscenes} and Waymo Open~\citep{sun2020wod} datasets that demonstrate state-of-art pretraining performance for multiple downstream tasks (illustrated in Fig.~\ref{fig:teaser}). 
%Additionally, we investigate the effect of the proposed ingredients in ablation studies. 
Our main contributions are as follows:
\begin{itemize}
    \item The novel \textit{localized reconstruction} self-supervised learning framework for point clouds, \textbf{N}eighborhood \textbf{O}ccupancy \textbf{MAE}.
    \item A multi-scale SSL strategy, where different feature levels are supervised at different scales.
    \item A novel mask generating scheme suitable for multi-scale SSL.
    \item Extensive benchmarking on two standard autonomous driving datasets, achieving state-of-the-art results across two perception tasks.
    \item Comprehensive ablation studies to highlight the impact of our proposed contributions.
\end{itemize}


% Automated driving and driver assistance systems often use sensors that produce 3D point clouds, such as LiDAR and radar. 
% There are a number of popular benchmark datasets for LiDAR perception algorithms, including panoptic tracking, semantic segmentation, and 3D object detection~\cite{caesar2020nuscenes,geiger2012kitti,sun2020wod}. 
% These benchmarks are still dominated by supervised approaches despite the large progress of the self-supervised learning (SSL) research community. 
% This paper investigates the specifics of SSL in the automotive context and derives novel suitable SSL task, thereby outperforming existing supervised and self-supervised lidar approaches on the Nuscenes~\cite{caesar2020nuscenes} and Waymo Open dataset benchmarks~\cite{sun2020wod}. 

% \input{figure_latex/teaser}

% % should we really argue so much with the usage of unlabeled data? We might be expected to show experiments with unlabeled data, if we would. 

% Self-supervised learning gained significant traction in the visual representation learning ~\citep{bao2021beit,chen2020simclr,he2020moco,chen2021simsiam,he2022mae,tian2023spark,chen2020improved,gao2022convmae}, where masked modeling~\citep{bao2021beit,he2022mae,gao2022convmae} and particularly masked auto-encoders~\citep{he2022mae} led to large performance improvements over supervised learning. 
% Pioneering works~\citep{liu2022maskpoint,pang2022pointmae,yu2022pointbert,zhang2022pointm2ae} successfully transferred these SSL concepts to small-scale point clouds and first research~\citep{hess2022voxelmae,xv2023mvjar,tian2023geomae} on large-scale (automotive) point clouds. 

% A large challenge of masked modeling with automotive point clouds is that most of the measured 3D volume is empty space. 
% First research resorts to reconstructing precise point locations within occupied voxels~\citep{hess2022voxelmae,xv2023mvjar,tian2023geomae}.
% This comes with the issue of information leaking to the decoder, that the queried voxel must be occupied. 
% GD-MAE~\citep{yang2023gd-mae} and Occupancy-MAE~\citep{min2022OccupancyMAESP} try to overcome this issue by reconstructing the whole scene, by going from a sparse to a dense feature space. 
% In order to reduce complexity and address the imbalance introduced by the high number of empty voxels, they resort to a 2D bird's eye view representation or coarse-grained 3D reconstruction, respectively. 
% With heavy compute requirements, these papers outperform their respective 2D or coarse 3D baseline methods but still lag behind transformer-based models with fine-grained 3D representations, which are currently at the top of the leaderboards. 

% Building on top of these pioneering works, this paper addresses the 3D point clouds sparsity in 3D masked modeling by the following novel concept:
% \begin{itemize}
%     \item The occupancy of fine-grained 3D voxels is only evaluated (loss) in the neighborhood of visible (not masked) occupied voxels. 
%     This is motivated by the fact that lidar points are usually clustered in the proximity of other LiDAR points in automotive scenarios. 
%     \item Thereby, we avoid the need of a dense feature space and that large regions of unoccupied space must be reconstructed. 
%     This makes our approach lightweight and usable for fine resolutions as well as state-of-the-art transformer architectures. 
%     \item At the same time, the positions of masked voxels are not leaked to the decoder, which solely uses information from visible voxels.
%     \item The lightweight nature of our reconstruction task allows us to apply masking and reconstruction at multiple scales, which further improves the quality of our learned representation and the performance on the downstream tasks.
%     \item For this purpose, we present a hierarchical mask generation module that is suitable for our SSL task at multiple scales. 
% \end{itemize}
% Experiments with self-supervised learning using the state-of-the-art PTv3~\citep{Wu2023PointTV} architecture, on the competitive nuScenes and Waymo Open datasets demonstrate the superior performance of this approach and investigate the effect of the proposed ingredients in ablation studies. 



















\iffalse
Self-supervised Learning have gained significant importance recently in the deep learning community for its visual representation learning capabilities from unlabeled data~\citep{bao2021beit,chen2020simclr,he2020moco,chen2021simsiam,he2022mae,tian2023spark,chen2020improved,gao2022convmae}.
Masked modeling~\citep{bao2021beit,he2022mae,gao2022convmae} ,with masked autoencoders~\citep{he2022mae} as an example, is one direction of self-supervised learning that saw huge increase in applications due to the improved downstream performance when finetuned compared to training from scratch. Maksed autoencoders learn high level semantics by reconstructing an input example using only a highly masked version of the example. Many works have adapted masked auto-encoders to work for small scale point clouds~\citep{liu2022maskpoint,pang2022pointmae,yu2022pointbert,zhang2022pointm2ae}. By tokenizing the point cloud using furthest point sampling (FPS) to select token centers and k-nearest neighbors (KNN) to construct the patches followed by a mini-PointNet~\citep{qi2017pointnet} to construct the tokens, Such tokens can be readily be used by the transformer architecture in the masked autoencoder. . However, large scale point clouds such as automotive data~ collected using lidar have proven challenging\cite{caesar2020nuscenes,geiger2012kitti,sun2020wod}. the large size of the point clouds, sparsity and non uniform point distribution prevented direct adaptation.

To enable masked modeling for large scale point clouds and remedy some of their irregularity, point clouds are first voxelized and some voxels are masked with the Target of reconstructing the points in the masked voxels~\citep{hess2022voxelmae,xv2023mvjar} or some features about them~\citep{tian2023geomae} using the visible voxels.
while such approaches have proven effective, they still suffer from the leak of the location of the hidden voxels to reconstruct their internal points.

Another direction it to reconstruct features, such as occupancy or the internal points, for the whole scene~\citep{min2022OccupancyMAESP, yang2023gd-mae}. GD-MAE~\citep{yang2023gd-mae} reconstructs the points inside non-visible voxels using a generative decoder before applying the loss only on the masked voxels (ignoring empty voxels). While in Occupancy-MAE~\citep{min2022OccupancyMAESP}, occupancy of all the non visible voxels are predicted and the Focal Loss~\citep{Lin2017FocalLF} is used to penalize false predicitons and to reduce the effect of the large amount of empty voxels.
Despite the improved performance, such approaches suffer from the huge computational capabilities required to compute the output over the whole 3D space. Leading to such methods only being applicable for coarse-grained supervision or 2D pillar representation (limiting the learned representation capability) and only training smaller encoders (due to the heavy computational cost needed for the decoder.


In this paper we present a novel approach for self-supervised learning on large scale autonomous point clouds that utilizes the non uniform distribution in large scale datasets. We observe that the points in the clouds are clustered around regions where objects are located. with most of the occupied voxels being in a close neighborhood of another occupied voxel. building from this, we propose NeighOcc (Neighbouring Occupancy as a pretext task). More specifically, the encoder is presented by a 3D masked point cloud and produces a hierarchal representation, a light weight decoder converts such representation into a scale of choice, followed by a light weight neighboring occupancy head which predicts occupancy of voxels in the neighbourhood of the masked point cloud.

This eliminates the need for the computationally expensive decoder to reconstruct the 3D space without leaking any new positinal informatoin to the decoder. Additionally, with the decoder focusing more on regions where objects are most likely to exist, the encoder have to learn a more useful internal representation without being hindered by the huge amount of free space (non occupied voxels).
empowered by the light weight design and motivated by the varying sizes of objects in the large scale point clouds, we propose utilizing multiple pretext tasks which differ mainly in the size of the voxel grid we compute occupancy over. To enable the simultaneous training of the different tasks, we design a novel masking strategy ,Hierarchal masking, which ensures that the sizes of the masked regions in the point cloud are uniformly distributed over the sizes needed by all the simultaneous tasks.

Extensive experiments conducted on the competitive
nuScenes\citep{caesar2020nuscenes}~ and Waymo~\citep{sun2020wod} datasets, the results demonstrate the superiority and generalization capability of the representation learned by the proposed method. Additionally finetuning the pretrained weights leads to improved performance of the state of the art PTv3~\citep{Wu2023PointTV}. bla bla (write numbers for the improvement). Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a light weight semantically strong self supervised learning task based on occupancy. which achieves a better learned representation that SOTA methods using a SOTA architecture
    \item enabled by the low cost of the decoder needed in our framework, we propose a multiscale learning objective which consists of combining different scaled replicas of our proposed task. And design a masking strategy to enable the efficient simultaneous training of the replicas.
    \item We conduct comprehensive experiments on the
        nuScenes and Waymo datasets, wherein our method surpasses other two methods in the quality of the learned representation measured by the non-linear probing performance. And after finetuning surpasses SOTA performance of supervised and self supervised pretraining on the task of semantic segmentation. Which provides convincing evidence for the effectiveness of our approach.
\end{itemize}
\fi
