\begin{figure*}
  \centering
  % \begin{subfigure}{0.68\linewidth}
  %   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %   \caption{An example of a subfigure.}
  %   \label{fig:short-a}
  % \end{subfigure}
  % \hfill
  % \begin{subfigure}{0.28\linewidth}
  %   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %   \caption{Another example of a subfigure.}
  %   \label{fig:short-b}
  % \end{subfigure}
  % \begin{subfigure}{\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    % \includegraphics[width=\textwidth]{figure/framework_green.pdf}
    \includegraphics[width=1.0\textwidth]{figure/framework.pdf}
    % \includegraphics[width=\textwidth]{figure/framework.jpg}
  % \end{subfigure}
  \vspace{-2em}
  \caption{Overview of the proposed NOMAE approach. The input point cloud is first voxelized and masked by the hierarchal mask generator. The encoder $\mathbb{E}$ processes the visible voxels $\mathcal{V}_{\text{v}}$ to yield a hierarchical representation. The upsampler $\mathbb{M}_\text{u}$ then fuses the multi-scale representations to capture high-level features at each scale. For each feature scale, a separate neighboring decoder predicts occupancy in $\mathcal{V}_{\text{n}}$, corresponding to the immediate neighborhood of the visible voxels. The combination of independent learning tasks across multiple feature scales and the localized predictions by the neighboring decoders enables learning representations that are well-suited for 3D point clouds.}
  \label{fig:framework}
  \vspace{-0.5em}
\end{figure*}