\section{Related work}
\label{sbs:related}
%{\bf PW comment 4: Compare to Talagrand work, Also add literature on Auction theory, EM algorithms, hypergraph covering (hardness), anything else?}
Talagrand, "Concentration of Measure Phenomenon" has extensively studied problems of the form $\mathbb{E}[\sup_{t\in T}\{X_t\}]$, where $X_t$ are Gaussian. However, we are not aware of any of his work for the corresponding problems \basic, \gena, \genb, or \genbcorr, where we have (1) a budget on the total variance of the underlying Gaussian variables, and (2) the variables may have non-zero means. That said, our main structural lemma (Lemma~\ref{lem:eps-contribution}) does rely on a chaining argument (as do many other results in this area), which has been discussed thoroughly in Chapter 2 of Boucheron, Lugosi, and Massart, "Concentration Inequalities Using the entropy Method". However, a direct application of methods in Chapter 2 of Boucheron, Lugosi, and Massart, "Concentration Inequalities Using the entropy Method" would give an upper bound of $O(\eps\sqrt{\log n})$ as opposed to $O(\eps\sqrt{\log \frac{1}{\eps}})$ (cf. Lemma~\ref{lem:eps-contribution}).

% A chaining argument (see Boucheron, Lugosi, and Massart, "Concentration Inequalities Using the entropy Method") is typically used to upper-bound $\mathbb{E}[\max_i\{X_i\}]$ of random variables $\{X_i\}$ by approximating the maximum as a sum over maximums of smaller subsets of random variables, since the maxima of each subset can be better approximated. Our argument in Lemma~\ref{lem:eps-contribution} follows a similar idea: we group variables that are close to each other in variance, bound the maximum over each group of variables, and then bound the overall maximum by a weighted sum over maxima of each group. However, crucially, since the total variance is bounded, we are able to replace the $\log n$ factor by a $\log\frac{1}{\eps}$ by upper bounding the weighted contribution of each group more tightly (Equation~\ref{eqn:crux} in Lemma~\ref{lem:eps-contribution}).

Our approximation algorithms corresponding to Theorems~\ref{thm:PTAS-correlated} and~\ref{thm:polylog-approx} rely crucially on Lemma~\ref{lem:eps-contribution}. We are able to obtain a PTAS because Lemma~\ref{lem:eps-contribution} implies that most of the variables will be set to zero in the optimal solution (a version of that result is in Theorem~\ref{thm:random-graph-ordering}). The logarithmic approximation algorithm on the other hand is from a reduction to submodular maximization, which is NP-hard but admits an efficient approximation algorithm Schrijver, "Combinatorial Optimization", 
Many variants of submodular maximization have been studied before Bach, "Optimization Methods for Large-Scale Machine Learning", including for non-monotone submodular functions Sviridenko, "A Note on Maximizing a Monotone Submodular Function", monotone submodular functions Calinescu, Chekuri, and Pal, "Maximin Rates in Combinatorial Optimization", and submodular maximization with a matroid constraint Iwata, "A Simple O(1)-Approximation Algorithm for Max-Min Matroid Portfolios".