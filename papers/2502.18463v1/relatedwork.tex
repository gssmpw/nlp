\section{Related work}
\label{sbs:related}
%{\bf PW comment 4: Compare to Talagrand work, Also add literature on Auction theory, EM algorithms, hypergraph covering (hardness), anything else?}
Talagrand~\citep{talagrand} has extensively studied problems of the form $\mathbb{E}[\sup_{t\in T}\{X_t\}]$, where $X_t$ are Gaussian. However, we are not aware of any of his work for the corresponding problems \basic, \gena, \genb, or \genbcorr, where we have (1) a budget on the total variance of the underlying Gaussian variables, and (2) the variables may have non-zero means. That said, our main structural lemma (Lemma~\ref{lem:eps-contribution}) does rely on a chaining argument (as do many other results in this area), which has been discussed thoroughly in Chapter 2 of~\cite{talagrand}. However, a direct application of methods in Chapter 2 of~\cite{talagrand} would give an upper bound of $O(\eps\sqrt{\log n})$ as opposed to $O(\eps\sqrt{\log \frac{1}{\eps}})$ (cf. Lemma~\ref{lem:eps-contribution}).

% A chaining argument (see~\cite{talagrand}) is typically used to upper-bound $\mathbb{E}[\max_i\{X_i\}]$ of random variables $\{X_i\}$ by approximating the maximum as a sum over maximums of smaller subsets of random variables, since the maxima of each subset can be better approximated. Our argument in Lemma~\ref{lem:eps-contribution} follows a similar idea: we group variables that are close to each other in variance, bound the maximum over each group of variables, and then bound the overall maximum by a weighted sum over maxima of each group. However, crucially, since the total variance is bounded, we are able to replace the $\log n$ factor by a $\log\frac{1}{\eps}$ by upper bounding the weighted contribution of each group more tightly (Equation~\ref{eqn:crux} in Lemma~\ref{lem:eps-contribution}).

Our approximation algorithms corresponding to Theorems~\ref{thm:PTAS-correlated} and~\ref{thm:polylog-approx} rely crucially on Lemma~\ref{lem:eps-contribution}. We are able to obtain a PTAS because Lemma~\ref{lem:eps-contribution} implies that most of the variables will be set to zero in the optimal solution (a version of that result is in Theorem~\ref{thm:random-graph-ordering}). The logarithmic approximation algorithm on the other hand is from a reduction to submodular maximization, which is NP-hard but admits an efficient approximation algorithm~\citep{nemhauser1978analysis}. 
As an aside, it's worth noting that many variants of submodular maximization have been studied before \citep{Vondrak13}, including for non-monotone submodular functions \citep{BuchbinderF18}, monotone submodular functions \citep{FeigeMV11}, and submodular maximization with a matroid constraint \citep{BuchbinderF24}.