\clearpage
\onecolumn
\appendix

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}

%	\part{Appendix} % Start the appendix part
%	\renewcommand{\contentsname}{}
\renewcommand{\contentsname}{Appendix}
\tableofcontents

\clearpage


\section{Related Work}\label{app:related_work}
\subsection{Text-to-Image Generation}
With the development of vision-language models~\citep{radford2021clip} and diffusion models~\citep{sohl2015deep, ho2020denoising}, great advancements recently have been made in text-to-image generation~\citep{nichol2021glide, ho2022imagen, zhang20232dsurvey}. In particular, Stable Diffusion~\citep{stable_diffusion} is a notable framework that trains the diffusion models on latent space, leading to reduced complexity and detailed preservation. In addition, with the emergence of text-to-2D models, more applications have been developed, \eg, spatial control~\citep{voynov2023sketch, zhang2023controlnet}, concept control~\citep{gal2022image, ruiz2022dreambooth}, and image editing~\citep{brooks2023instructpix2pix}.

\subsection{Text-to-3D Generation}
The success of the 2D generation is incredible.
However, it is challenging to transfer image diffusion models to 3D because of the difficulty of 3D data collection. 
Fortunately, Neural Radiance Fields (NeRF)~\citep{mildenhall2020nerf, barron2022mip} provided new insights for 3D-aware generation, where only 2D multi-view images are needed in 3D scene reconstruction. 
Combining prior knowledge from text-to-2D models, several methods, such as DreamField~\citep{jain2021dreamfields}, DreamFusion~\citep{poole2022dreamfusion}, and SJC~\citep{wang2022sjc}, have been proposed to generate 3D objects guided by text prompts~\citep{li20233dsurvey}.
However, the vanilla score distillation sampling loss~\citep{poole2022dreamfusion} suffers from issues such as over-saturation, over-smoothing, and Janus problems, \etc. 
Recently, several works have proposed improvements to enhance generation quality~\citep{wang2023prolificdreamer, yu2023text, zhu2023hifa, katzir2023noise, chung2023luciddreamer, wu2024consistent3d, zhuo2025vividdreamer}.
Additionally, with the availability of large 3D datasets~\citep{deitke2023objaverse, deitke2024objaverse}, some works~\citep{liu2023zero, shi2023mvdream, liu2024one, liu2023syncdreamer, long2024wonder3d} leverage multi-view information to address the Janus problem more effectively.
Moreover, the recent advancement of text-to-3D generation also inspired multiple applications that include but are not limited to text-guided scene generation~\citep{cohen2023set, hollein2023text2room}, text-guided 3D editing~\citep{Ayaan2023instructnerf, kamata2023instruct3d}, and text-guided avatar generation~\citep{cao2023dreamavatar, jiang2023avatarcraft,han2023headsculpt,zhou2024headstudio}.

\subsection{Learning from Human Preferences}
Learning from human preferences is essential for improving the alignment and performance of generative models across various domains, including large language models (LLMs)~\citep{bai2022training,bai2022constitutional,leerlaif}, large multimodal models (LMMs)~\citep{2023llavarlhf,yu2024rlhf,yu2024rlaifv}, text-to-image diffusion models~\citep{black2023training,lee2023aligning,clark2023directly,xu2024imagereward,wallace2024diffusion,fan2024reinforcement,zhang2024hive}, and text-to-3D generation~\citep{xie2024carve3d,ye2025dreamreward}.
Reinforcement Learning from Human Feedback (RLHF)~\citep{christiano2017deep} has been proven effective in refining LLMs to better align with human preferences. 
It often involves collecting human feedback datasets, training a reward model, and fine-tuning the language model using reinforcement learning.
InstructGPT~\citep{ouyang2022training} is a notable work that employs a two-stage fine-tuning strategy to align GPT-3 with human instructions, which leads to more coherent and contextually appropriate outputs.

The success of RLHF in LLMs has inspired the applications in text-to-image diffusion models to enhance image generation quality and align with human preferences. In more detail, RWR~\citep{lee2023aligning} first introduces human feedback-based reward fine-tuning for diffusion models, which fine-tunes Stable Diffusion~\citep{rombach2022high} using log probabilities of the denoising process. 
ImageReward~\citep{xu2024imagereward} proposes a reward model specifically for text-to-image tasks and further develops reward feedback Learning for refining diffusion models. 
DiffusionDPO~\citep{wallace2024diffusion} uses DPO to optimize diffusion models using human comparative data, while 
DPOK~\citep{fan2024reinforcement} integrates policy optimization with KL regularization for improved alignment.
Recently, advancements in multi-view diffusion models have facilitated significant progress in text-to-3D generation. For instance,
Carve3D~\citep{xie2024carve3d} enhances text-to-3D generation with a Multi-view Reconstruction Consistency (MRC) metric for improved consistency and quality.
DreamReward~\citep{ye2025dreamreward} improves text-to-3D models using human feedback. 
It collects a 3D dataset with human annotations, trains Reward3D as a multi-view reward model, and introduces DreamFL to create 3D assets aligned with human preferences.
However, these works rely heavily on large-scale datasets to train a reward model or utilize it as preference feedback, which is very expensive for 3D generation.

\section{Additional Implementation Details}\label{app:algorithm_flow}

\iffalse
\subsection{Text-to-3D Optimization}
DreamDPO is an optimization-based text-to-3D generation method.
It begins by initializing a 3D representation $\theta$, such as NeRF ~\cite{mildenhall2020nerf}.
In each training iteration, the optimization process involves three key steps:
(1) Pair Collection: Pairwise examples are generated online by applying different Gaussian noise.
(2) Preference Ranking: A reward model or a large multimodal model (LMM) ranks the generated examples based on their alignment with the desired text prompt.
(3) Online Feedback: A piecewise reward loss is calculated using the pairwise preferences, and the 3D representation $\theta$ is updated accordingly.
DreamDPO guides the optimization process with human preference, reusing in 3D assets with improved alignment to the input text and enhanced texture and geometry quality.


\textbf{STEP1: Pair Collection.}
Given a sampled camera pose $c$, an RGB image $\mathbf{x}$ is rendered from the 3D representation $g(\theta, c)$.
Then, two Gaussian noise $\bm{\epsilon}^1$ and $\bm{\epsilon}^2$ are added to $\mathbf{x}$ at timestep $t$, resulting a pairwise noisy images $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$:
\begin{equation}
    \begin{aligned}
    \mathbf{x}_t^1 &= \alpha_t \mathbf{x}_0 + \sigma_t \bm{\epsilon}^1, \\
    \mathbf{x}_t^2 &= \alpha_t \mathbf{x}_0 + \sigma_t \bm{\epsilon}^2,
    \end{aligned}
\end{equation}
where $\mathbf{x}_0 = \mathbf{x}$, $\alpha_t$ and $\sigma_t$ are hyperparameters satisfying $\alpha_0 \approx 1, \sigma_0 \approx 0, \alpha_0 \approx 0, \sigma_0 \approx 1$.
Then we feed the pairwise noisy images into a pre-trained text-to-image diffusion model $\bm{\epsilon}_\phi$~\cite{shi2023mvdream,rombach2022high} and generate corresponding prediction:
\begin{equation}
    \begin{aligned}
       \hat{\mathbf{x}}_0^1 = \frac{\mathbf{x}_t^1 - \sqrt{1 - \alpha_t}\epsilon_\theta(\mathbf{x}_t^1;y,t)}{\sqrt{\alpha_t}},  \\
       \hat{\mathbf{x}}_0^2 = \frac{\mathbf{x}_t^2 - \sqrt{1 - \alpha_t}\epsilon_\theta(\mathbf{x}_t^2;y,t)}{\sqrt{\alpha_t}},
    \end{aligned}
\end{equation}
where $\hat{\mathbf{x}}_0^1$ and $\hat{\mathbf{x}}_0^2$ are the predicted $x_0$ in a single-step for $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$, respectively~\citep{song2020denoising}.

\textbf{STEP2: Preference Ranking.}
Then, we use a ranking model $r(\cdot)$ to score the prediction online, yielding a preferred response $\mathbf{x}_t^{\text{win}}$ and a less preferred one $\mathbf{x}_t^{\text{lose}}$.
It is worth noting that DreamDPO supports both reward model~\citep{xu2024imagereward,wu2023human} and LMM-based AI annotator~\citep{bai2023qwen,yang2023dawn}, where reward model are used as default.
Instruction questions can also be incorporated into LMM-based ranking models to provide explicit guidance.

\textbf{STEP3: Online Feedback.}
Using the preference pairs $(\mathbf{x}_t^{win}, \mathbf{x}_t^{lose})$, DreamDPO updates the 3D representation by minimizing the reward loss, and its gradient is given by:
\begin{equation}
\nabla_\theta \mathcal{L}_{\mathrm{Reward}} \triangleq
\begin{cases}
\mathbb{E}_{t, \epsilon} \left[ w(t) \left( \epsilon^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) \right) \frac{\partial x}{\partial \theta} \right], & s_\text{gap} < \tau \\
\mathbb{E}_{t, \epsilon} \left[ w(t) \left( \left( \epsilon^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) - \epsilon^{\text{win}} \right) - \left( \bm{\epsilon}^1_\phi(\mathbf{x}_t^{\text{lose}}; y, t) - \epsilon^{\text{lose}} \right) \right) \frac{\partial x}{\partial \theta} \right], & \text{otherwise}
\end{cases}
\end{equation}
where $\tau = 0.001$ is a predefined threshold, and $s_\text{gap} = r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y)$ indicates the discrepancy of preference score between $\mathbf{x}_t^{\text{win}}$ and $\mathbf{x}_t^{\text{lose}}$.
For LMM-based ranking model, we use $\tau = 1$ in optimization.

% $s_\text{gap}$ can be calculated by:
% \begin{equation}
%     s_\text{gap} = \frac{r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y)}{r(\mathbf{x}_t^{\text{lose}}, y)},
% \end{equation}
% For LMM-based ranking model, $s_\text{gap}$ is calculated differently as:
% \begin{equation}
%     s_\text{gap} = r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y),
% \end{equation}
% with $\tau = 1$.
\fi

\iffalse
\subsection{The Derivation of DreamDPO}
\zzl{
We provide a more detailed derivation of DreamDPO.
Given the preference pair $(\mathbf{x}_t^{\text{win}},\mathbf{x}_t^{\text{lose}})$, DreamDPO aims to yields a preferred 3D asset.
To perform this optimization, we need a differentiable loss function where preferred images have low loss and less preferred images have high loss.
Therefore, we investigate the Diffusion-DPO loss~\citep{wallace2024diffusion}, which re-formulates Direct Preference Optimization (DPO)~\citep{rafailov2024direct} to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective:
}
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{Diff-DPO}}&(\phi) = -\mathbb{E}_{t} \log \sigma \left( -\beta w(t) \right) \left( \right. \\
    &\| \bm{\epsilon}^\text{win} - \epsilon_\phi(\mathbf{x}_t^\text{win}, t) \|_2^2 - \| \bm{\epsilon}^\text{win} - \epsilon_{\text{ref}}(\mathbf{x}_t^\text{win}, t) \|_2^2 \\ 
    -&\left. \left. \left( \| \bm{\epsilon}^\text{lose} - \epsilon_\phi(\mathbf{x}_t^\text{lose}, t) \|_2^2 - \| \bm{\epsilon}^\text{lose} - \epsilon_{\text{ref}}(\mathbf{x}_t^\text{lose}, t) \|_2^2 \right) \right) \right),
\end{aligned}
\label{eq:diff-dpo}
\end{equation}
\zzl{
where $\sigma$ is the sigmoid function, $\epsilon_{\text{ref}}$ is a frozen reference model, $\beta$ is a hyperparameter controls regularization.
$\mathcal{L}_{\text{Diff-DPO}}(\phi)$ encourages $\epsilon_\phi$ to improve more at denosing $\mathbf{x}_t^\text{win}$ than $\mathbf{x}_t^\text{lose}$.
}

% [formulation of the reward loss]
\zzl{

% To construct a reference model for text-to-3D generation, a straightforward approach is to firstly train a 3D asset using the baseline method.
% However, this approach is inefficient as it doubles the training time. 
Moreover, directly using the loss function may fail to produce realistic samples due to poor conditioning at small noise levels~\citep{poole2022dreamfusion}.
Therefore, we consider the gradient of $\mathcal{L}_{\text{Diff-DPO}}$, omitting the U-Net Jacobian term to derive an effective gradient for optimizing the 3D representation with preference data:
}
\begin{equation}
    \nabla_\theta \mathcal{L}_{\text{Reward}}  \triangleq 
    \mathbb{E}_{t, \epsilon} \left[ \log \sigma \left( -\beta w(t) \right)  \left( \left( \bm{\epsilon}_\phi(\mathbf{x}_t^{\text{win}}; y, t) - \epsilon^{\text{win}} \right) - \left( \bm{\epsilon}_\phi(\mathbf{x}_t^{\text{lose}}; y, t) - \bm{\epsilon}^{\text{lose}} \right)\right) \frac{\partial \mathbf{x}}{\partial \theta} \right].
\end{equation}
\zzl{
As a result, $\nabla_\theta \mathcal{L}_{\text{Reward}}$ avoids the expensive computation of both the reference model and the U-Net Jacobian term. 
Additionally, we simplify the weight function, resulting in \cref{eq:gradient_reward_loss}.
Meanwhile, \cref{eq:gradient_reward_loss} is the gradient of \cref{eq:reward}. 
Compared to \cref{eq:diff-dpo}, \cref{eq:reward} serves as an optimization objective inspired by SimPo~\citep{meng2024simpo}, eliminating the need for a reference model.
}
% Diffusion-DPO~\citep{wallace2024diffusion} aligns diffusion models to human preference by directly optimizing on human comparison data.
\fi

\subsection{Pseudo-Code for DreamDPO}
A more detailed pseudo-code for DreamDPO is presented in~\cref{alg:dreamdpo}.

\begin{algorithm}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \caption{Pseudo-code for DreamDPO}
    \label{alg:dreamdpo}
    \begin{algorithmic}[1]
        \REQUIRE Text-to-image diffusion model $\bm{\epsilon}_\phi$. Ranking model $r$. Learning rate $\eta$ for 3D representation parameters. A prompt $y$. Evaluating threshold of score gap $\tau$.
        \STATE Initialization A 3D representation presenting with NeRF $\theta$
        \WHILE{not converged}
        \STATE Randomly sample a camera pose $c$, pairwise 2D noise $\bm{\epsilon}^1$ and $\bm{\epsilon}^2$, and timestep $t \sim \text{Uniform}(\left\{ 1, \cdots, T\right\})$.
        \STATE Render at pose $c$ to get a image $\mathbf{x}_0$.
        \STATE Add noise $\bm{\epsilon}^1$ and $\bm{\epsilon}^2$ to $\mathbf{x}_0$ and get $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$, respectively.
        \STATE Denoise with the predicting noise:
        \begin{equation} 
    \begin{aligned}
       \hat{\mathbf{x}}_0^1 = \frac{\mathbf{x}_t^1 - \sqrt{1 - \alpha_t}\bm{\epsilon}_\theta(\mathbf{x}_t^1;y,t)}{\sqrt{\alpha_t}}, \\
       \hat{\mathbf{x}}_0^2 = \frac{\mathbf{x}_t^2 - \sqrt{1 - \alpha_t}\bm{\epsilon}_\theta(\mathbf{x}_t^2;y,t)}{\sqrt{\alpha_t}}.
       \nonumber
    \end{aligned}
\end{equation}
        \STATE Score the prediction $(\hat{\mathbf{x}}_0^1, \hat{\mathbf{x}}_0^2)$ online via a rank model $r$, yielding the pairwise comparison $(\mathbf{x}_t^{\text{win}}, \mathbf{x}_t^{\text{lose}})$.
        \STATE Compute the score gap:
        \begin{equation}\notag
            s_\text{gap} = r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y).
        \end{equation}
        \IF{$s_\text{gap} < \tau$ }
        \STATE
        \begin{equation}
            \nabla_\theta \mathcal{L}_{\mathrm{Reward}} = \mathbb{E}_{t} \left[ w(t) \left( \bm{\epsilon}^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) \right) \frac{\partial \mathbf{x}}{\partial \theta} \right],
            \nonumber
        \end{equation}
        \ELSE
        \STATE 
        \begin{equation}
            \nabla_\theta \mathcal{L}_{\mathrm{Reward}} = \mathbb{E}_{t} \left[ w(t) \left( \left( \bm{\epsilon}^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) - \bm{\epsilon}^{\text{win}} \right) - \left( \bm{\epsilon}^1_\phi(\mathbf{x}_t^{\text{lose}}; y, t) - \bm{\epsilon}^{\text{lose}} \right) \right) \frac{\partial \mathbf{x}}{\partial \theta} \right].
            \nonumber
        \end{equation}
        \ENDIF
        \item $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}_{\mathrm{Reward}}$.
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

\subsection{Details of LMM-based Pairwise Comparison}
\label{app:lmms}
We detail the implementation of the LMM-based pairwise comparison.
We use the large visual-language model ``qwen-vl-plus-latest'' from QwenVL~\citep{bai2023qwen} as the default LMM.
Given pairwise examples, we conduct the comparison query sequentially.
For each query, we first insert a predefined ``yes" or ``no" question into the comparison prompt, such as ``\textit{Is the leaf shouting?}'' for the prompt ``\textit{A shouting leaf.}'' 
Then, the LMM performs visual question answering based on the provided image and query. 
Finally, we extract the number of ``yes" responses as the score.
The following prompts are used for the queries:
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, 
    title=Comparison Query, width=\textwidth, 
    boxrule=1pt, arc=2mm, auto outer arc,
    sharp corners=south]
\textcolor{blue}{[Task Description]}: You are an expert in evaluating the alignment between a given text description and an image. Your task is to answer each of the alignment questions with either ``Yes" or ``No" based on the image. Provide your responses in the format specified below.

\textcolor{orange}{[Evaluation Instruction]:} 

1. Carefully analyze the provided image and answer questions based on the image.

2. For each question, answer with either ``Yes" or ``No". Do not provide explanations or additional information.

\textcolor{olive}{[Evaluation Question(s)]:} 

Q1: \{Question\}

...

\textcolor{magenta}{[Output Format]:}

A1: [Yes/No]

...
\end{tcolorbox}


\subsection{Details of Text-to-Avatar Generation}\label{app:avatar}
We detail the toy exploration of text-to-avatar generation using DreamDPO. 
Specifically, we integrate the reward loss into HeadStudio~\citep{zhou2024headstudio}, a Gaussian-based avatar generation framework. 
HeadStudio is an optimization-based method that utilizes a score sampling loss~\citep{katzir2023noise} with ControlNet~\citep{zhang2023controlnet} to optimize an animatable head prior model. 
By replacing the score sampling loss with the reward loss, we leverage ControlNet to generate avatars.


\section{Supplementary Experimental Settings}

\subsection{Details of Measurement Metrics}\label{app:metrics}
In the main paper, we employ two evaluation strategies to demonstrate the superiority of the proposed method. Here we supplementary the details of the measurements.

\textbf{Evaluation with ImageReward.}
ImageReward~\citep{xu2024imagereward} is a text-to-image human preference reward model.
Due to its effectiveness, it has been broadly used for human preference evaluation in text-to-image generation~\citep{fan2024reinforcement} and text-to-3D generation~\citep{ye2025dreamreward}.
Given a (text, image) pair, it extracts image and text features, combines them with cross-attention, and uses an MLP to generate a scalar for preference comparison.
For each 3D asset, we uniformly render 120 RGB images from different viewpoints.
Afterward, the ImageReward score is computed from the multi-view renderings and averaged for each prompt.

\textbf{Evaluation with GPTEval3D.}
We utilize GPTEval3D~\citep{wu2024gpt}, which is a comprehensive benchmark for text-to-3D generation evaluation. GPTEval3D includes 13 baseline methods $\mathcal{M}$, 110 text prompts, and 5 criteria that are text-asset alignment, 3D plausibility, texture details, geometry details, and texture-geometry coherency respectively.
For a new method, GPTEval3D employs GPT-4V to compare 3D assets generated by this new method and one of the baseline methods with the same input text prompt.
These pairwise comparison results are then used to calculate the Elo rating for each model.
Specifically, let $\mathbf{A}$ be a matrix where $\mathrm{A}_{ij}$ represents the number of times that the $i$-th model outperforms the $j$-th model in comparisons. 
The Elo ratings for the models are computed by optimizing the following objective:
\begin{equation}
    \sigma = \arg\max\limits_{\sigma} \mathop{\sum}\limits_{i \neq j} \mathrm{A}_{ij} \log \left(1 + 10^{(\sigma_j - \sigma_i)/400}\right),
\end{equation}
where $\sigma_i \in \mathbb{R}$ is the Elo rating of the $i$-th model.
In this work, we calculate Elo ratings within the existing tournament, initializing, and freezing baseline scores as specified in the official code\footnote{\href{https://github.com/3DTopia/GPTEval3D/blob/main/data/tournament-v0/config.json}{https://github.com/3DTopia/GPTEval3D/blob/main/data/tournament-v0/config.json}}. For interested readers, please refer to~\citep{wu2024gpt}.


\section{Supplementary Experimental Results}

\subsection{More Qualitative Results}\label{app:qualitative_results}
We present additional qualitative results in \cref{fig:supp_comp_mvdream} and \cref{fig:supp_comp_mvdream2}. 
The comparisons demonstrate that our method generates human preferred 3D assets, with improved text alignment and enhanced geometric and texture details.

\begin{figure*}[ht]
    \centering
    % \vspace{-2.0em}
    \includegraphics[width=1.0\linewidth]{figures/supp_comp_mvdream.pdf}
    % \vspace{-2.0em}
    \caption{More qualitative results using DreamDPO.}
    \label{fig:supp_comp_mvdream}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/supp_comp_mvdream2.pdf}
    \caption{More qualitative results using DreamDPO.}
    \label{fig:supp_comp_mvdream2}
\end{figure*}

% \subsection{More Ablation Studies}