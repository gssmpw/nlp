\section{Introduction}
\label{sec:intro}
3D content generation is pivotal in driving innovation across diverse fields, including product design, medical imaging, scientific visualization, and the rapidly growing domains of virtual and augmented reality~\citep{li2023generative}. Despite its extensive applications, it remains challenging to create high-quality 3D content, which requires substantial time and effort, even for professionals. In response, \textit{text-to-3D generation} has emerged as a solution by automating 3D generation from textual descriptions, which archives remarkable advancements in the field~\citep{poole2022dreamfusion,wang2023prolificdreamer,wang2022sjc,yu2023text,wang2024prolificdreamer,shi2023mvdream,katzir2023noise,chung2023luciddreamer,wu2024consistent3d}. Nevertheless, some researchers~\citep{xie2024carve3d,ye2025dreamreward} emphasize that 3D content generated by existing methods often fails to align with \textit{human preferences} fully, highlighting the need for continued refinement and innovation in these methods.

Previous work~\citep{ye2025dreamreward} has leveraged \textit{reward models} to integrate human preferences into the generation process, leading to enhanced 3D generation outcomes. The core idea is to regularize the generated 3D content to achieve a high \textit{pointwise score} from the reward model. Despite these improved results, several issues remain to be addressed. First, it heavily depends on the reward model's ability to accurately evaluate the \textit{pointwise quality} of generated content, which places significant demands on the reward model. Second, since the reward model can only provide \textit{quality-relevant} scores, it lacks the flexibility to enable controllability from other perspectives. The issues reduce the applicability and adaptability of the current method. This falls short of meeting diverse requirements or providing broader control in 3D generation, which is never our desideratum.

To relieve the issues of prior work and better align 3D generation with human preferences, we propose DreamDPO. Essentially, DreamDPO is an optimization-based method for text-to-3D generation. It achieves alignment through direct preference optimization, leveraging preferences derived from either \textit{reward models} or \textit{large multimodal models}. Specifically, DreamDPO operates by initializing a 3D representation~\citep{mildenhall2020nerf,kerbl3Dgaussians} and optimizing it through a three-step iterative process. First, \textit{pairwise examples} are constructed \textit{on-the-fly} by applying different Gaussian noise. Second, a reward model or a large multimodal model \textit{ranks} these examples based on their matching with the input text prompt or \textit{specific instructions}, which matches human preferences about the pairwise examples\footnote{Reward models are trained using pairwise data reflecting human preferences, while large multimodal models are inherently aligned with these preferences~\citep{sun2023aligning}.}. Finally, a reward loss is computed from the \textit{pairwise preferences}, which guides the update of the 3D representation. By incorporating human preferences into the optimization loop, DreamDPO generates 3D assets that achieve superior alignment with textual inputs, along with enhanced texture and geometry quality.


DreamDPO can be justified as follows. While absolute quality evaluation inherently provides a ranking on pairwise examples, a ranking requires only the scores to distinguish \textit{relative preferences}, but need not be perfectly accurate~(\textit{c.f.}, \citep{zhang2024generating}). DreamDPO takes advantage of this distinction, and changes the previous \textit{score-guided} optimization~\citep{ye2025dreamreward} to \textit{preference-guided} optimization. Therefore, it lowers the demand for precise scoring and requires only distinguishable scores. Additionally, DreamDPO can make use of the preferences provided by large multimodal models. By constructing preferred and less preferred examples based on specific instructions about the attributes of generation content~(\eg., the object number and motion), it directs the optimization process to align more closely with the desired outcomes.  This strategy enhances adherence to instructions and introduces \textit{fine-grained controllability}, meeting diverse requirements effectively. Moreover, we conduct a series of experiments to justify our claims. Empirical results demonstrate that our method flexibly accommodates either reward models or large multimodal models, which enables the generation of higher-quality and more controllable 3D content. 

Before delving into details, we clearly emphasize our contribution as follows.
\begin{itemize}
    \item Conceptually, we propose DreamDPO that shifts the paradigm of text-to-3D generation by reducing dependence on precise absolute quality evaluation. Instead, it leverages distinguishable relative preferences and integrates large multimodal models, which achieves remarkable alignment with human preferences while enabling fine-grained controllability.
    \item Technically, DreamDPO pioneers a three-step optimization process, combining online pair construction, preference ranking, and ranking-driven updates. This innovative design ensures superior alignment with input prompts, significantly enhancing the quality and adaptability of generated 3D content.
    \item Empirically, extensive results establish DreamDPO as a new benchmark for text-to-3D generation. Both quantitative and qualitative results are thoroughly analyzed, supported by detailed discussions and ablation studies. DreamDPO outperforms 13 state-of-the-art methods, achieving the best quantitative performance across two key metrics while delivering highly impressive and controllable qualitative results.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{sec:pre} introduces some background knowledge relevant to this work. Section~\ref{sec:method} presents the technical details of our proposed DreamDPO. Experimental results are analyzed and discussed in Section~\ref{sec:exp}. Conclusions are given in Section~\ref{sec:clu}.



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overview_v3.pdf}
    % \vspace{-2.0em}
    \caption{Overview of our method.
    DreamDPO first constructs pairwise examples, then compares their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D presentation with a preference-driven loss function. The loss function pulls the \texttt{win} example $\mathbf{x}_t^{\text{win}}$ closer and pushes the \texttt{lose} example $\mathbf{x}_t^{\text{lose}}$ away. 
    As a piecewise objective, it selectively pushes $\mathbf{x}_t^{\text{lose}}$ only when the preference score gap $s_\text{gap}$ exceeds a threshold $\tau$, preventing chaotic gradients from overly similar $\mathbf{x}_t^{\text{lose}}$.
    }
    % \vspace{-10pt}
    \label{fig:overview}
\end{figure*}