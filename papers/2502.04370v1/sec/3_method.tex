\section{Method}
\label{sec:method}

\textbf{Overview.} DreamDPO is an optimization-based text-to-3D generation method. It begins by initializing a 3D representation, \eg, NeRF~\citep{mildenhall2020nerf}. In each training iteration, the optimization procedure involves three key steps: (1) \textit{pairwise example construction}: pairwise examples are generated online by applying different Gaussian noises during the diffusion process; (2) \textit{pairwise example comparison}: a reward model or a large multimodal model~(LMM) compares the generated examples based on their alignment with the desired text prompt; and (3) \textit{preference-guided optimization}: a piecewise reward loss is calculated using the pairwise comparison, and the 3D representation is updated accordingly. Overall, our DreamDPO guides the optimization process with human preferences, leading to 3D assets with improved alignment to input text and enhanced texture/geometry quality. The framework overview of our method is provided in \cref{fig:overview}. A complete algorithm flow of our method can be checked in Appendix~\ref{app:algorithm_flow}. 
% We detail DreamDPO below.

\input{table/qua_comp}

\subsection{Algorithm Details}

\textbf{Pairwise example construction.} Given a sampled camera pose, an RGB image $\mathbf{x}$ can be rendered from the 3D representation using renderers. Then two different Gaussian noise $\bm{\epsilon}^1$ and $\bm{\epsilon}^2$ are added to $\mathbf{x}$ at timestep $t$, resulting in pairwise noisy images $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$:
% \vspace{-1.0em}
\begin{equation}
    \mathbf{x}_t^1 = \alpha_t \mathbf{x}_0 + \sigma_t \bm{\epsilon}^1, \ \ \mathbf{x}_t^2 = \alpha_t \mathbf{x}_0 + \sigma_t \bm{\epsilon}^2,
\end{equation}
where $\mathbf{x}_0 = \mathbf{x}$, $\alpha_t$ and $\sigma_t$ are hyperparameters satisfying $\alpha_0 \approx 1, \sigma_0 \approx 0, \alpha_0 \approx 0, \sigma_0 \approx 1$~(\cf, \citep{sohl2015deep,ho2020denoising}). Afterward, we feed the pairwise noisy images into a pre-trained text-to-image diffusion model $\bm{\epsilon}_\phi$~\citep{shi2023mvdream,rombach2022high} and generate corresponding predictions:
\begin{equation}
    \begin{aligned}
       \hat{\mathbf{x}}_0^1 = \frac{\mathbf{x}_t^1 - \sqrt{1 - \alpha_t}\bm{\epsilon}_\phi(\mathbf{x}_t^1;y,t)}{\sqrt{\alpha_t}},  \\
       \hat{\mathbf{x}}_0^2 = \frac{\mathbf{x}_t^2 - \sqrt{1 - \alpha_t}\bm{\epsilon}_\phi(\mathbf{x}_t^2;y,t)}{\sqrt{\alpha_t}},
    \end{aligned}
\end{equation}
where $\hat{\mathbf{x}}_0^1$ and $\hat{\mathbf{x}}_0^2$ are predicted $\mathbf{x}_0$ of a single step for $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$, respectively~\citep{song2020denoising}.

\textbf{Pairwise comparison}. After pair construction, at step $t$, we utilize a rank model denoted by $r(\cdot)$ to compare $\mathbf{x}_t^1$ and $\mathbf{x}_t^2$. This yields a preferred prediction $\mathbf{x}_t^{\text{win}}$ and a less preferred one $\mathbf{x}_t^{\text{lose}}$, where $\mathbf{x}_t^{\text{win}}=\mathbf{x}_t^1$ and $\mathbf{x}_t^{\text{lose}}=\mathbf{x}_t^2$, or vice versa. It is worth noting that our DreamDPO supports both reward models~\citep{xu2024imagereward,wu2023human} and LMM-based AI annotators~\citep{bai2023qwen,yang2023dawn}, where reward models are used as default.


\textbf{Preference-guided optimization.} 
The proposed method leverages the pairwise comparison
$(\mathbf{x}_t^{\text{win}},\mathbf{x}_t^{\text{lose}})$ to enable efficient sampling via optimization to yield human preferred 3D assets.
To achieve this, we need a differentiable loss function, where preferred images have low losses and less preferred images have high losses. To this end, inspired by~\citep{rafailov2024direct,meng2024simpo,wallace2024diffusion}, we reformulate SimPO~\citep{meng2024simpo} to eliminate the need for a reference model and derive a differentiable objective:
\begin{equation}
    \mathcal{L}_{\text{Reward}} = -\mathbb{E}_{t} \left[w(t)\left(  \| \bm{\epsilon}^\text{win} - \bm{\epsilon}_\phi(\mathbf{x}_t^\text{win};y,t) \|_2^2 
    - \| \bm{\epsilon}^\text{lose} - \bm{\epsilon}_\phi(\mathbf{x}_t^\text{lose};y,t) \|_2^2 \right) \right],
\end{equation}
where $\bm{\epsilon}^\text{win}$ and $\bm{\epsilon}^\text{lose}$ denote Gaussian noise for $\mathbf{x}_t^\text{win}$ and $\mathbf{x}_t^\text{lose}$ respectively. 
% where $\beta$ is a hyperparameter controls regularization.
Intuitively, $\mathcal{L}_{\text{Reward}}$ encourages $\bm{\epsilon}_\phi$ to pull $\mathbf{x}_t^\text{win}$ closer and push $\mathbf{x}_t^\text{lose}$ further away.

% [formulation of the reward loss]
Following~\citep{poole2022dreamfusion}, we consider the gradient of $\mathcal{L}_{\text{Reward}}$ and omit the U-Net Jacobian term for effective optimization.
It leads to the following gradient for optimizing 3D representations with preference pairwise comparisons:
\begin{equation}\label{eq:gradient_reward_loss}
    \nabla_\theta \mathcal{L}_{\text{Reward}}  = 
    \mathbb{E}_{t} \left[ w(t) \left( \bm{\epsilon}_\phi(\mathbf{x}_t^{\text{win}}; y, t) - \bm{\epsilon}^{\text{win}} \right)
    - \left( \bm{\epsilon}_\phi(\mathbf{x}_t^{\text{lose}}; y, t) - \bm{\epsilon}^{\text{lose}} \right) \frac{\partial \mathbf{x}}{\partial \theta} \right].   
\end{equation}
% [why you use the gap. Its advantages ]
However, in practice, the gradient in \cref{eq:gradient_reward_loss} fails to produce realistic results (refer to \cref{fig:abl_score_gap}).
Delving into the optimization process, we observe that the pairwise comparison results can be overly similar, leading to nearly equal scores.
In this case, directly pushing $\mathbf{x}_t^\text{lose}$ away leads to chaotic gradients.
To address this, we introduce a piecewise optimization loss that selectively pulls $\mathbf{x}_t^\text{win}$ when the preference score gap $s_\text{gap}$ is small.
The gradient of the final loss is defined as:
\begin{equation}
    \nabla_\theta \mathcal{L}_{\text{Reward*}} :=
\begin{cases}
\mathbb{E}_{t} \left[ w(t) \left( \bm{\epsilon}^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) \right) \frac{\partial \mathbf{x}}{\partial \theta} \right], & s_\text{gap} < \tau, \\
\mathbb{E}_{t} \left[ w(t) \left( \mathbf{\Delta}_t^{\text{win}} - \mathbf{\Delta}_t^{\text{lose}}
\right) \frac{\partial \mathbf{x}}{\partial \theta} \right], & \text{otherwise}.
\end{cases}
\label{eq:reward}
\end{equation}
where $\tau=0.001$ is a pre-defined threshold, $\mathbf{\Delta}_t^{\text{win}} :=\bm{\epsilon}^s_\phi(\mathbf{x}_t^{\text{win}}; y, t) - \bm{\epsilon}^{\text{win}}$, and $\mathbf{\Delta}_t^{\text{lose}} := \bm{\epsilon}^1_\phi(\mathbf{x}_t^{\text{lose}}; y, t) - \bm{\epsilon}^{\text{lose}}$.
Note that $s_\text{gap} := r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y)$ indicates the discrepancy of preference scores between $\mathbf{x}_t^{\text{win}}$ and $\mathbf{x}_t^{\text{lose}}$. 
Instruction questions can also be incorporated into LMM-based ranking models to provide explicit guidance (see empirical evaluations in \S\ref{sec:refined_analysis}). 

% \begin{equation}
%     s_\text{gap} = (r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y))/r(\mathbf{x}_t^{\text{lose}}, y).
% \end{equation}
% For the LLM-based ranking model, $s_\text{gap}$ is calculated differently as
% \begin{equation}
%     s_\text{gap} = r(\mathbf{x}_t^{\text{win}}, y) - r(\mathbf{x}_t^{\text{lose}}, y),
% \end{equation}


% \subsection{Theoretical Insights}

