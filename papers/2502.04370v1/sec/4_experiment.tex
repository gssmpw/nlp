\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/comp_all.pdf}
    % \vspace{-2.5em}
    \caption{
    Qualitative comparisons on the benchmark of GPTEval3D~\protect\citep{wu2024gpt}.
    Existing methods struggle with text matching, as marked in red.
    DreamDPO improves text matching, which provides better human preference results.
    (Zoom in to see the details.)
    }
    % \vspace{-1.3em}
    \label{fig:quan_comp_all}
\end{figure*}

\section{Experiments}
\label{sec:exp}
In this section, a series of experiments are conducted to justify our claims. We first detail experiment setups~(\S\ref{sec:exp_setups}). The comprehensive results and comparison with previous advanced methods are then presented and discussed~(\S\ref{sec:exp_comparison}). Finally, we carry out an analysis study to further elaborate on and discuss the superiority of our method~(\S\ref{sec:refined_analysis}).

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/comp_mvdream_v2.pdf}
    % \vspace{-2.0em}
    \caption{
    Qualitative comparisons with MVDream~\protect\citep{shi2023mvdream}. 
    DreamDPO performs well across short to long prompts, offering better human preference results, marked in red.~(Zoom in to see the details.)
    }
    \label{fig:quan_comp_backbone}
\end{figure}


\subsection{Experimental Setups}\label{sec:exp_setups}

\textbf{Datasets and measurements.} We here evaluate the proposed method with 110 prompts from GPTEval3D~\citep{wu2024gpt}, which covers a range of creativity and complexity use cases. Based on this, two evaluation strategies are exploited. 
(1) We utilize a text-to-image reward model named ImageReward~\citep{xu2024imagereward} to evaluate human preference for 3D assets.
We calculate the average preference score across 120 rendered images of a 3D asset and its corresponding text prompt.
(2) We use GPT-4V to perform pairwise comparisons with baselines, generating Elo ratings that align with human judgments on text alignment, 3D plausibility, and texture-geometry coherence, \etc. More details of the two measurements can be found in Appendix~\ref{app:metrics}.


\textbf{Baselines.} Following GPTEval3D~\citep{wu2024gpt}, we benchmark our method against 13 baselines categorized into text-guided and image-guided approaches respectively. Specifically, the text-guided group includes DreamFusion~\citep{poole2022dreamfusion}, DreamGaussian~\citep{tang2023dreamgaussian}, Instant3D~\citep{li2023instant3d}, Fantasia3D~\citep{chen2023fantasia3d}, Latent-NeRF~\citep{metzer2023latent}, Magic3D~\citep{lin2023magic3d}, MVDream~\citep{shi2023mvdream}, Point-E~\citep{nichol2022point}, ProlificDreamer~\citep{wang2024prolificdreamer}, Shap-E~\citep{jun2023shap}, and SJC~\citep{wang2023score}. Besides, the image-guided group includes SyncDreamer~\citep{liu2023syncdreamer} and Wonder3D~\citep{long2024wonder3d}.

\textbf{Implementation.} We conduct experiments using PyTorch~\citep{paszke2019pytorch} and threestudio~\citep{threestudio2023}, with MVDream~\citep{shi2023mvdream} as the backbone of our method. Note that we use PyTorch auto-differentiation to compute analytic normals for geometry evaluation in GPTEval3D and do not use the Lambertian shading trick~\citep{lin2023magic3d} due to memory limitation. We follow the training strategy of MVDream and use HPSv2~\citep{wu2023human} as the default reward model.
The optimization process takes around two hours on a single NVIDIA RTX A6000 GPU.


%Our method is implemented in PyTorch and threestudio~\citep{threestudio2023}, with MVDream~\citep{shi2023mvdream} as the backbone. Notably, we use PyTorch auto-differentiation to compute analytic normals for geometry evaluation in GPTEval3D and do not use Lambertian shading trick~\citep{lin2023magic3d} due to memory limitation. We follow the training strategy of MVDream and use HPSv2~\citep{wu2023human} as the default reward model. The entire optimization process takes around 2 hours on a single NVIDIA A6000 (48GB) GPU.

\subsection{Comparison with Prior Methods}\label{sec:exp_comparison}

\subsubsection{Qualitative Comparisons}
We conduct two qualitative evaluations, which include comparing 13 benchmarks of GPTEval3D, MVDream~\citep{shi2023mvdream}, and DreamReward~\citep{ye2025dreamreward}. The evaluations show improvements in text alignment, generation stability, and texture-geometry details, respectively.
As seen in \cref{fig:quan_comp_all}, while the comparing baselines produce high-fidelity results, they often fail in text alignment, as marked in red.
For instance, in the prompt ``\textit{A small, rustic cabin sits alone in a peaceful, snow-covered forest}'', most existing methods miss key elements like the forest (the first row in \cref{fig:quan_comp_all}).
In contrast, our method accurately captures both objects, showcasing its effectiveness for improving text alignment.
Further comparisons with MVDream, are shown in \cref{fig:quan_comp_backbone}.
Although MVDream is capable of generating multiview consistent 3D assets, it struggles with long prompts (\eg, the second and fourth rows in \cref{fig:quan_comp_backbone}).
Instead, our method performs well across both short to long prompts.
Lastly, the comparisons with DreamReward demonstrate that our method not only improves text alignment (\eg, ensuring ``\textit{leaves a trail of flowers}'' appears under the bicycle shown in the first row in \cref{fig:quan_comp_baseline}) but also enhances geometric and texture details (\eg, generating a more luxuriant oak shown in the second row in \cref{fig:quan_comp_baseline}).
More visualization results can be found in Appendix~\ref{app:qualitative_results}.


\subsubsection{Quantitative Comparisons}
We provide extensive quantitative comparison results to justify our claims.
Human preference evaluations are first conducted using ImageReward, as illustrated in the first column of \cref{tab:qua_comp}.
Our method achieves competitive performance compared to existing methods, highlighting the benefits of preference-guided optimization via our reward loss.
Then we perform a comprehensive evaluation using GPTEval3D. The results indicate that our method outperforms previous state-of-the-art (SOTA) methods, and ranks first across all metrics. Specifically, our method achieves improvements in text-asset alignment (+28.4), 3D plausibility ($+24.4$), text-geometry alignment (+25.8), texture details (+48.4), geometry details (+41.4), and overall performance (+24.4). 
It showcases the superiority of the proposed method in enhancing text and geometry details while maintaining 3D consistency.
% Finally, we report the CLIP score for text-image alignment evaluation, which further affirms the capability of our method.


\begin{figure}[t]
  \centering
  % First subfigure
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/abl_backbone.pdf}
    \caption{The analysis of backbone.
    We present the results of DreamDPO using Stable Diffusion v2.1 (SD2.1)~\citep{rombach2022high}.
    DreamDPO demonstrates effective performance with SD2.1, highlighting its potential to leverage more advanced backbone diffusion models for further improvements.}
    \label{fig:abl_backbone}
  \end{minipage}\hfill
  % Second subfigure
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/abl_imagereward.pdf}
    \caption{The analysis of reward models.
    We present the results of DreamDPO using ImageReward~\citep{xu2024imagereward}.
    DreamDPO demonstrates effective performance with ImageReward, highlighting its potential to leverage stronger reward models to further enhance generation quality.}
    \label{fig:abl_imagereward}
  \end{minipage}
\end{figure}


\begin{figure}[t]
    \centering    
    \includegraphics[width=1.0\linewidth]{figures/abl_score_gap.pdf}     
    % \vspace{-2.0em}
    \caption{
    The analysis of the score gap threshold $\tau$.     
    We conduct 2D toy experiments with $\tau$ ranging from $0.01$ to $0$.
    The results indicate that a small but non-zero $\tau$ effectively filters out overly similar \texttt{lose} examples, leading to more detailed outputs.
    }   
    % \vspace{-1.0em}
    \label{fig:abl_score_gap} 
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/comp_dreamreward.pdf}
    % \vspace{-2.5em}
    \caption{
    Qualitative comparisons with DreamReward~\citep{ye2025dreamreward}.
    DreamDPO improves both text matching (marked in red) and geometric/texture details.
    % (Zoom in to see the details.)
    }
    % \vspace{-1.0em}
    \label{fig:quan_comp_baseline}
\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/abl_qwenv2.pdf}
    % \vspace{-2.0em}
    \caption{
    The generation results of DreamDPO with large multi-modal models (LMMs).
    We explore the potential of our method to leverage LMMs, such as QwenVL~\protect\citep{bai2023qwen} for explicit guidance in correcting the number and attribute of 3D assets. The left corner shows the details of pairwise comparisons using the LMM, including the question and win/lose criteria.
    By carefully designing the question, DreamDPO can leverage both \texttt{win} and \texttt{lose} examples to guide optimization. (Zoom in to see the details.)
    }
    % \vspace{-1.0em}
    \label{fig:abl_qwenv2}
\end{figure*}


\begin{figure}[t]
  \centering
  % First subfigure
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/abl_negative_example.pdf}
    \caption{The analysis of pairwise example construction.
    We compare (1) different noises: adding different Gaussian noises with the same timesteps, and (2) difference timesteps: adding the same Gaussian noise with different timesteps.}
    \label{fig:abl_negative_example}
  \end{minipage}\hfill
  % Second subfigure
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/application.pdf}
    \caption{The further application of DreamDPO.
    We conduct toy experiments on text-to-avatar generation by combining DreamDPO with Gaussian-based avatar generation framework~\citep{zhou2024headstudio}. More details can be checked in Appendix~\ref{app:avatar}.}
    \label{fig:application}
  \end{minipage}
\end{figure}


\subsection{More Analyses and Justifications}\label{sec:refined_analysis}


% Ours (MVDream) v.s. Ours (SD2.1)
\textbf{Evaluation on different backbones. }
We here further investigate the impact of backbone selection on our method. The performance of DreamDPO using Stable Diffusion v2.1~(SD2.1)~\citep{rombach2022high} is provided. Note that as the previous method ProlificDreamer~\citep{wang2023prolificdreamer} also utilizes SD2.1, we compare DreamDPO with SD2.1 against ProlificDreamer. As shown in~\cref{fig:abl_backbone}, our method can perform effectively with SD2.1 and achieve competitive results compared to ProlificDreamer. Importantly, DreamDPO does not need LoRA training and is \textit{more efficient} than ProlificDreamer. 
%While SD2.1 provides superior texture and geometry details compared to MVDream, it lacks 3D consistency awareness. Therefore, we choose MVDream as our default backbone.



% Ours (HPSv2) v.s. Ours (ImageReward)
\textbf{Evaluation on different reward models.} We study the impact of reward model selection on our method. Specifically, ImageReward~\citep{xu2024imagereward} is used, which is an image-based reward model with a similarity function comparable to HPSv2~\citep{wu2023human}. As shown in \cref{fig:abl_imagereward}, the results demonstrate that our method performs effectively across different reward models, demonstrating its flexibility and scalability. For instance, in the prompt ``\textit{A mug filled with steaming coffee}'', both HPSv2 and ImageReward successfully capture the coffee, and ImageReward places greater emphasis on the steam.
While ImageReward demonstrates improvement over the baseline, HPSv2 yields superior results due to its better generalization across diverse image distributions~\citep{wu2023human}. 
Therefore, we adopt HPSv2 as our default reward model. Also, it highlights the potential of leveraging stronger reward models, \textit{e.g.}, Reward3D~\citep{ye2025dreamreward} and VisionReward~\citep{xu2024visionreward}, to further enhance generation quality.



\textbf{Evaluation on different score gaps.} We investigate the impact of the score gap $\tau$. Specifically, 2D toy experiments with $\tau$ range from $0.01$ to $0$ are conducted. We provide results in~\cref{fig:abl_score_gap}, which show that a smaller $\tau$ produces more detailed outputs. Note that a small $\tau$ means choosing to \texttt{lose} examples with scores close to \texttt{win} samples, and focusing the training process on hard cases. However, we observe that $\tau = 0$ (the last column in \cref{fig:abl_backbone}) results in a chaotic gradient.
To balance high-fidelity generation and stable training, we suggest using \textit{a small but non-zero} $\tau$, which excludes overly similar \texttt{lose} examples.


% Same timestep but different noise v.s. Same noise but different timestep
\textbf{Evaluation on different pair examples.} The influence of different pair example generation methods is studied. Specifically, we compare: (1) \textit{different noises}, by adding different Gaussian noises with the same timesteps; (2) \textit{difference steps}, by adding the same Gaussian noise with different timesteps. As shown in~\cref{fig:abl_negative_example}, using different Gaussian noise yields better results than different timesteps. We attribute that noisy latents with different timesteps are easier to distinguish, making them less effective as challenging examples.
It highlights the importance of generating meaningful and challenging \texttt{lose} examples.  Accordingly, we adopt different noises as the default setting.


% LMM-based AI Feedback
\textbf{Ranking model design.} We explore the potential of our method to leverage large multi-modal models (LMMs) for explicit guidance. Instead of relying on a reward model, we use large visual-language models, such as QwenVL~\citep{bai2023qwen}, to rank paired results. Specifically, we extract ``yes'' or ``no'' questions from the text prompt, query the LMM with rendered paired examples, and count the ``yes'' responses to calculate the reward score. 
Then, we use \cref{eq:reward} with threshold $\tau = 1$ for 3D assets generation.
As shown in \cref{fig:abl_qwenv2}, our method effectively improves text alignment by using LMM to guide the optimization with user instruction (\eg, correcting the \textit{number} and \textit{attribute} of 3D assets). Additionally, our method flexibly supports using \texttt{lose} examples to guide optimization. 
For example, given the prompt ``\textit{A dancing elephant}'', the baseline generates an elephant standing rather than dancing. 
By setting ``\textit{elephant stays on the ground}'' as the \texttt{lose} example and pushing it away, our method encourages the elephant to lift its leg, leading to a dancing pose. It highlights the potential of our method to integrate LMMs into 3D generation.
More implementation details of the LMMs-based comparison can be found in Appendix~\ref{app:lmms}.


% Application in Avatar Generation
\textbf{Further application.} 
To showcase the potential of our method, we provide empirical results on text-to-avatar generation. In specific, we replace the score sampling loss in HeadStudio~\citep{zhou2024headstudio} which is a 3DGS~\citep{kerbl3Dgaussians} based avatar generation framework, with our reward loss. 
As illustrated in \cref{fig:application}, our method achieves great generation results. This underscores the broader applicability of our method to various generation tasks, \eg, 4D generation~\citep{bahmani20244d} and scene generation~\citep{zhang2024text2nerf}, \etc.

% As illustrated in \cref{fig:application}, our method achieves remarkable results that demonstrates improved text alignment and enhanced texture and geometry.