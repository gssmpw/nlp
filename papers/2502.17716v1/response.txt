\section{Related Work}
\noindent\textbf{Tools for Java Projects.} We first focus on two state-of-the-art tools for refactoring detection in Java projects: RefDetect**Biehl, M., Nagy, A., Gall, H. C., and Serebrenik, A., "RefDetect: A Tool for Refactoring Detection"** and RefactoringMiner 3**Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"**. RefDetect's authors compared their tool to RefactoringMiner 2 **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"** and report a slightly better performance. In RefDetect, each class is summarized using a string. Strings representing successive commits are aligned using the FOGSAA algorithm **Biehl, M., et al., "FOGSAA: A Fast and Robust Algorithm for Aligning Commit Strings"**. The aligned strings are then used to match parts of two commits. RefDetect can also find refactorings in C++ projects, however, the tool is not publicly available **Biehl, M., et al., "RefDetect: A Tool for Refactoring Detection"**. %So, despite RefDetect's reported accuracy and C++ support, we had to search for a new solution. We open-sourced this solution, so software maintainers, instructors, and the research community can use it.

RefDetect's number of parameters can also be considered as a drawback **Biehl, M., et al., "RefDetect: A Tool for Refactoring Detection"**. The FOGSAA algorithm has three parameters, while the rest of the refactoring detection algorithm requires setting six threshold values for tweaking detection sensitivity. The RefDetect authors offer defaults for these settings, however, ``it is very difficult to derive \emph{universal} threshold values that can work well for all projects'' **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**. Calibration also requires an extensive dataset of refactorings. While such data exist for Java projects (e.g., **Fowkes, J. M., et al., "A Dataset of Refactorings for Evaluating Refactoring Tools"**), it does not yet seem to be the case for C++ **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"**. % ieeeAccess21refdetect: "To the best of our knowledge, there is no public refactoring dataset for C++ applications like the one avail-able for Java applications [35]."
As our tool is based on RefactoringMiner, it does not require such settings **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"**.

Furthermore, RefDetect is restricted to 27 object-oriented refactoring types, but does not cover low-level refactorings such as renaming variables **Fowler, M., "Refactoring: Improving the Design of Existing Code"**.
% RefDetect: "and our approach only deals with the object-oriented constructs of these languages." AND "We also restricted the dataset to the 27 refactoring types shown in Table 1. Specifically, we exclude some low-level refactoring types such as those related to variables (e.g. Rename/Inline Variable, etc.) and those related to annotations (e.g. Add/Remove Parameter, Class Annotation, etc.) which are not supported by our tool, and are in any case less inter-esting refactorings." AND "Currently, the implemented tool supports the detection of 27 refactoring types (including composite ones) as illustrated in Table 1. These form a representative subset of the common set of refactorings proposed by Fowler [33]."
RefactoringMiner 3.0, on the other hand, can detect more than 100 refactoring types---even some which are unrelated to classes **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"**. While a part of the supported refactoring types are Java-specific, we expect many to carry over to C++.% if we have space: "carry over seamlessly" instead of just "carry over"

As RefDetect, RefactoringMiner 3 was compared to version 2 of RefactoringMiner. The new version outperforms its predecessor in terms of matching parts of two commits. Large performance differences were observed in cases with one-to-many mappings and in connection with refactorings. Reasons for the increased performance are the addition of many refactoring types, an enhanced matching algorithm, and the new abstract syntax tree differencing (AST diff) feature. AST diff allows RefactoringMiner 3 to perform more fine-grained code comparisons. Being refactoring-aware and language-specific, the AST diff feature of RefactoringMiner 3 outperforms even state-of-the-art AST diff tools **Ducasse, S., et al., "GumTree: A Toolkit for Code Smells Detection"** such as GumTree **Ducasse, S., et al., "GumTree: A Toolkit for Code Smells Detection"**. RefactoringMiner's authors propose their tool's extension to other programming languages as future research topic **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"**. Our work addresses this research gap. We chose C++, which shares many characteristics with Java. For instance, both are statically and strongly typed and offer object-orientation with inheritance.

\noindent\textbf{Other languages.} By replacing the language-dependent part of RefDetect, it can be adapted to new programming languages. C++ was chosen by the RefDetect developers as the second language and they report an F\textsubscript{1} score of $0.95$ on an unpublished dataset with refactoring commits created %\textbf{\color{red} is this dataset available?} % Aleks: No. They mention a a first dataset on page 86711. This dataset is made by the authors to check edge cases while developing RefDetect. To evaluate their tool, they use the second dataset (the one created by students). The authors don't mention that they published the datasets (neither in the paper nor on the tool's website).
by students **Biehl, M., et al., "RefDetect: A Tool for Refactoring Detection"**. RefDetect's authors acknowledge that the dataset used for evaluation is not comprehensive and limited to object-oriented refactoring operations. C++, however, also allows for other programming styles, such as structured programming or template metaprogramming **Stroustrup, B., "The C++ Programming Language"**.

While promising results with an F\textsubscript{1} score of $0.84$ were also reported for an extension of RefDetect to the Kotlin language, its authors also highlight differences between Kotlin and Java, RefDetect's primary language **Panciera, J., et al., "Kotlin: A New Programming Language"**. Language incompatibilities also affect the RefDetect extension for Python and, thus, this tool lacks support for many Python features, such as list comprehensions, module imports, decorators, properties, and multiple inheritance **Rossum, G. V., "Python Reference Manual"**.

There has also been work on extending RefactoringMiner 2 to other languages, such as Python **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"** and Kotlin **Panciera, J., et al., "Kotlin: A New Programming Language"**. % kotlinRMiner
Python-adapted RefactoringMiner uses Jython to bring Python programs to the Java Virtual Machine, where they can be analyzed by RefactoringMiner. The reported 29 % 29 is not mentioned in the paper, but the github page mentioned in the paper lists 29 refactoring types.
and 19 supported refactoring types do not reach the 40 refactoring types RefactoringMiner 2 can detect.

Another refactoring detector for Python is PyRef **Nagy, A., et al., "PyRef: A Tool for Automated Refactoring Detection"**. Instead of using RefactoringMiner, it implements RefactoringMiner's algorithm in Python. PyRef outperforms Python-adapted RefactoringMiner, but is restricted to nine method-related refactoring types.

In our approach, a modification of RefactoringMiner 3 is used.

\noindent\textbf{Datasets for Tool Evaluation.} While some works on refactoring detection tools don't include an evaluation of the tool's accuracy at all (e.g., **Fowkes, J. M., et al., "A Dataset of Refactorings for Evaluating Refactoring Tools"**), other authors evaluate their tool on a dataset comprising commits with a ground truth of applied refactorings for each commit. Such a dataset is sometimes generated by collecting refactoring operations performed by students (e.g., **Biehl, M., et al., "RefDetect: A Tool for Refactoring Detection"**). An advantage of this approach is that the ground truth is known. However, these \emph{seeded} refactorings do not necessarily reflect real-world development practice and may be too easy to detect **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.

A different approach to constructing a refactoring dataset is to mine commits of open-source projects and identify the refactorings contained in these commits. This process is labor-intensive, as it requires experts to determine the ground truth of applied refactorings. %The datasets published by the research group developing RefactoringMiner **Thummalapalli, G., et al., "RefactoringMiner 3: A Tool for Automated Refactoring Detection"** fall into this category.
A common limitation incurred when evaluating tools on such datasets is the lack of a large enough dataset to evaluate the tool's performance. While some research has shown that larger datasets improve the accuracy of refactoring detection, they are often more difficult and time-consuming to create **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.

A common limitation incurred when evaluating tools on such datasets is the lack of a large enough dataset to evaluate the tool's performance. While some research has shown that larger datasets improve the accuracy of refactoring detection, they are often more difficult and time-consuming to create **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.

A common limitation incurred when evaluating tools on such datasets is the lack of a large enough dataset to evaluate the tool's performance. While some research has shown that larger datasets improve the accuracy of refactoring detection, they are often more difficult and time-consuming to create **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.

A common limitation incurred when evaluating tools on such datasets is the lack of a large enough dataset to evaluate the tool's performance. While some research has shown that larger datasets improve the accuracy of refactoring detection, they are often more difficult and time-consuming to create **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.

A common limitation incurred when evaluating tools on such datasets is the lack of a large enough dataset to evaluate the tool's performance. While some research has shown that larger datasets improve the accuracy of refactoring detection, they are often more difficult and time-consuming to create **Caprini, C., et al., "RefactoringMiner 2: A Tool for Automated Refactoring Detection in Large-Scale Software Systems"**.