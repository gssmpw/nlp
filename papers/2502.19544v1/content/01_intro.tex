\section{Introduction} \label{intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/fig1.pdf}
    \caption{
    Aggregate performance on \textbf{50 manipulation tasks} from Meta-World and \textbf{22 locomotion tasks} from DMControl with pixel inputs.
    We pre-train one \textit{single} world model per benchmark on \textit{non-curated} offline data, which can be fine-tuned to largely improve RL's sample efficiency.
    With 150k online samples, WPT obtains 35.65\% and 35\% higher normalized scores compared to DrQ v2 and Dreamer v3 using the same samples and matches baselines' results obtained with higher sample budgets (500k samples for DMControl and 1M samples for Meta-World). 
    }
    \label{fig:aggreated_performance}
\end{figure}

\begin{figure*}[t] 
\centering
\includegraphics[width=0.99 \textwidth]{fig/mpt_overview.pdf}
\caption{Overview of generalist world model pre-training (WPT). Facing non-curated offline datasets with reward-free, non-expert, and multi-embodiment data, we train a task and embodiment-agnostic world model. The pre-trained world model improves the sample efficiency of RL training over a wide range of tasks.}
\label{fig:overview}
\end{figure*}

With the success of scaling laws in vision and language~\cite{brown2020language,he2022masked,kirillov2023segment,touvron2023llama}, the robotics community has started investigating the use of large-scale offline datasets for robot learning~\cite{brohan2022rt,walke2023bridgedata,brohan2023rt,o2023open,khazatsky2024droid,team2024octo}.
Large-scale offline datasets enable the trained agents to learn a wide range of skills as well as help the agent adapt to new tasks with limited samples~\cite{team2024octo}.

To leverage offline datasets for decision-making, imitation learning (IL)~\cite{brohan2022rt,team2024octo,doshi2024scaling} and offline RL~\cite{levine2020offline,kumar2020conservative,fujimoto2021minimalist,kumar2022pre} are commonly used. 
However, these methods have limitations in terms of both data and algorithmic aspects.
IL assumes the availability of expert data from human demonstration or specialist RL agents, which usually involves an expensive data collection procedure. 
Further to this, agents learned by IL are usually sensitive to environmental perturbations such as action delay, lighting conditions, sensor noise, etc~\cite{chae2022robust,zare2023survey}. 
Offline RL aims to learn agents from offline data but suffers from training instability, especially for pixel-based observations~\cite{kumar2020conservative,lu2022challenges}. 
Besides the training stability issue, offline RL requires task-specific datasets, labeled with rewards. 
This poses another challenge for many real-world applications, e.g., robotic manipulation, to utilize offline datasets for \emph{new} tasks, it is necessary to retrospectively annotate the image-based offline data with rewards, which can be challenging and laborious. 
Furthermore, fine-tuning an offline RL agent, so-called offline-to-online RL~\cite{nair2020awac,lee2022offline,zhao2022adaptive,yu2023actor,nakamoto2024cal}, requires additional tricks to stabilize training and prevent performance collapse caused by the distributional shift in the fine-tuning stage.

In contrast to IL and offline RL, which require curated datasets, many real-world scenarios have data from various sources.
To this end, we focus on the more realistic setting where the offline dataset includes non-curated mixed-quality data consisting of \textit{reward-free} and \textit{non-expert} data, collected by multiple agents with varying embodiments. 
This setting has a minimal requirement for offline data, which significantly expands the pool of usable data. The primary research question we aim to answer is:

\textit{What is the best way to leverage non-curated offline data for efficient multi-task \& multi-embodiment learning?}

\definition[Non-curated data]{Non-curated data encapsulates all observation-action trajectories, i.e., it can consist of reward-free and non-expert data for any embodiment.}

Existing methods for leveraging offline data (e.g. IL and offline RL) typically fail given non-curated data.
For example, IL would require manually filtering the dataset to select expert trajectories and offline RL would require retrospective reward labeling.
With such non-curated data, a common strategy is to pre-train a visual encoder~\cite{schwarzer2021pretraining,nair2022r3m,parisi2022unsurprising,xiao2022masked,yang2021representation,shang2024theia}. However, visual pre-training alone fails to fully leverage the rich information that exists in the offline dataset, e.g., dynamics model, informative states, or action prior, etc.

Facing the non-curated offline dataset, training a world model seems a ``natural" choice, as it holds the promise of better generalization and better sample efficiency. 
However, model-based approaches are not commonly used in the offline-to-online setting~\cite{rafailov2023moto,wu2024pre}. 
This is partially due to the performance margin between model-based and model-free approaches not being significant~\cite{yu2020mopo,kidambi2020morel} while model-based approaches involve higher training complexity.
In addition, in the typical offline-to-online RL setting, training an offline agent with model-based approaches can be unstable, especially for pixel inputs~\cite{lu2022challenges}, and strong regularization is required during the online training stage~\cite{rafailov2023moto}.

In this paper, we show that with careful design choices, world model pre-training unlocks more useable offline data and better fits the pre-train and fine-tune paradigm used for scaling. Our key insights are {\em i)} instead of pre-training a policy using model-based offline RL as done in previous work~\cite{yu2020mopo,kidambi2020morel,rafailov2023moto}, learning a world model is stable and scalable, {\em ii)} unlike previous methods~\cite{hafner2019dream,hafner2020mastering,hafner2023mastering,hansen2023td} which learns a world model per task, by simply padding actions to unify the action spaces, a single world model can be pre-trained on the non-curated offline data consisting of data sources from different tasks and different embodiments, {\em iii)} via fine-tuning, a pre-trained generalist world model can boost RL's sample efficiency over a wide range of tasks and embodiments, {\em iv)} during fine-tuning, experience rehearsal, and execution guidance can largely improve the performance.

%%%%%%% Table for policy learning comparison %%%%%%%%%
\captionsetup{skip=2pt}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{table*}[t]
    \caption{Comparison with different policy learning methods that leverage offline data.}
    \label{tab:related_work}
    \centering

    \setlength{\tabcolsep}{7.5pt}
    \begin{tabularx}{\textwidth}{lcccccH}
        \specialrule{1pt}{1pt}{2.5pt}
        & \bf Offline RL & \bf Off2On RL & \bf RLPD & \bf MT IL & \bf MT Offline RL & \bf WPT (ours) \\
        \specialrule{1pt}{1pt}{2.5pt}
        Reward-free offline data & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark \\
        Non-expert offline data & \cmark & \cmark & \cmark & \xmark & \cmark & \cmark \\
        X-embodiment offline data & \xmark & \xmark & \xmark & \cmark & \cmark & \cmark \\
        Continual Improvement & \xmark & \cmark & \cmark & \xmark & \xmark & \cmark \\
        Training stability & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark \\
        \specialrule{1pt}{1pt}{2pt}
    \end{tabularx}
\end{table*}
%%%%%%% Table Ends Here %%%%%%%%

We extensively evaluate WPT on 72 pixel-based continuous control tasks covering locomotion and manipulation with different action spaces, hard exploration, high dimensions, and complex dynamics. 
Under a limited sample budget (150k samples), WPT outperforms previous SOTA methods by a decent margin. 
Specifically, WPT obtains 35.65\% and 35\% higher normalized scores compared to DrQ v2 and Dreamer v3 under the same sample budget and matches baselines' results obtained with higher sample budgets (500k samples for DMControl and 1M samples for Meta-World.).
For example, WPT enables an agent learning to control an Ant robot to walk forward within 100 trails, while widely used learning-from-scratch baselines need \textbf{10-30$\times$} samples. 
We further demonstrate that, without any modifications, WPT also helps task adaptation, where the agent is required to continually adapt its skill to new tasks.
Our results demonstrate strong performance with a large margin over model-free methods when leveraging \textit{non-curated} offline data. 
We hope that our results motivate further investigation of model-based approaches for using offline data. 
In summary, our contributions include:
\vspace{-0.3cm}
\begin{itemize}[noitemsep,leftmargin=.225in]
    \item We propose a new and more realistic setting for leveraging offline data where the offline data consists only of reward-free and non-expert multi-embodiment data.
    \item We show that generalist world model pre-training along with a series of careful design choices achieves strong performance in 72 visuomotor tasks spanning 6 embodiments.\looseness-2
    \item To facilitate further research with non-curated offline data, we open-source our datasets, including 60K trajectories and 10M transitions from the DMControl and Meta-World benchmarks.
\end{itemize}
