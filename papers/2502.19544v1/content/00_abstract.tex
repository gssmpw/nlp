\begin{abstract}
Sample-efficient robot learning is a longstanding goal in robotics.
Inspired by the success of scaling in vision and language, the robotics community is now investigating large-scale offline datasets for robot learning. 
However, existing methods often require expert and/or reward-labeled task-specific data, which can be costly and limit their application in practice.
In this paper, we consider a more realistic setting where the offline data consists of \textbf{reward-free} and \textbf{non-expert} \textbf{multi-embodiment} offline data.
We show that generalist world model pre-training (WPT), together with retrieval-based experience rehearsal and execution guidance, enables efficient reinforcement learning (RL) and fast task adaptation with such non-curated data.
In experiments over \textbf{72 visuomotor tasks}, spanning \textbf{6 different embodiments}, covering hard exploration, complex dynamics, and various visual properties, WPT achieves 35.65\% and 35\% higher aggregated score compared to widely used learning-from-scratch baselines, respectively.

\end{abstract}