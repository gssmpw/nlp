%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%        Experiments          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments} \label{section:experiments}

In this section, we investigate our WPT in two different RL settings. In the first setting, we focus on how the proposed WPT helps to improve the sample efficiency of RL training. In the second setting, we investigate whether WPT enables fast task adaptation, which is particularly important to empower the agent's life-long learning ability. Lastly, we conduct an ablation study to investigate the role of proposed design choices.

\subsection{Dataset}
Our dataset consists of data from two different benchmarks: DeepMind Control Suite (DMControl) and Meta-World, visualized in~\cref{fig:overview} and~\cref{app:task_visualization} for the full list. For DMControl, we include 10k trajectories covering 5 embodiments collected by \textit{unsupervised RL agents}~\cite{pathak2017curiosity,rajeswar2023mastering}, which are trained via curiosity without any task-related information. For Meta-World, we collect 50k trajectories spinning 50 tasks by executing the pre-trained RL agents from {TDMPCv2}~\cite{hansen2023td} with injected Gaussian noise with $\sigma = [0.1, 0.3, 0.5, 1.0, 2.0]$ to mimic imperfect demonstrations. In total, the offline dataset consists of 60k trajectories with 10M state-action pairs from 6 different embodiments.   

\subsection{WPT Enables Sample-efficient Fine-tuning}


\textbf{Tasks}~~ 
We evaluate our method on six selected \emph{pixel}-based continuous control tasks from DMControl and Meta-World.
The chosen tasks cover different challenges in RL. Except for learning from \textbf{high-dimensional} observations, we include a \textbf{hard exploration} version of Cheetah Run by setting zero rewards below a reward threshold and applying an action penalty. We include Walker Run and Quadruped Walk for \textbf{complex dynamics}. We further include challenging manipulation tasks from MetaWorld.

\textbf{Baselines}~~
In our experiments, we consider four different baselines for leveraging reward-free offline data. 
Note that as these baselines are not designed to handle multi-embodiment offline data, we preprocess the offline data to only contain task-related data for the baselines. 
Although this is an unfair comparison for WPT as baselines eliminate the difficulty raised with the large non-curated dataset, we still outperform these baselines by a large margin on tested tasks. The compared baselines are:
\vspace{-.2cm}
\begin{itemize}[noitemsep,leftmargin=.225in]
    \item \textbf{R3M} pre-trains visual representation aiming to boost policy learning. We use a ResNet-18 backbone and train additional policy and value heads via RL. We include this baseline to compare with a representative approach for visual pre-training. 
    \item \textbf{UDS} suggests to label offline data with zero. UDS is an offline RL method, which cannot be directly used for comparison. We instead set zero reward for offline data and use RLPD~\cite{ball2023efficient} for policy training.    
    \item \textbf{ExPLORe} labels offline data with UCB reward. We follow the implementation from~\citet{li2023accelerating} but use ensembles for reward estimation, which has better performance.
    \item \textbf{JSRL-BC}~\cite{uchendu2023jump} collects online data with both the training policy and a prior policy trained on offline data via IL. We include JSRL-BC to compare with different ways of using offline data.
\end{itemize}

%%%%%% Task adaptation plot %%%%%%
\begin{figure*}[t] 
\label{fig:adaptation}
\centering
\includegraphics[width=0.98\textwidth]{fig/crl.pdf}
\vskip -0.15in
\caption{WPT enables fast task adaptation. We train an RL agent to control an Ant robot from DMControl to finish a series of tasks incrementally. WPT outperforms the widely used baseline by a decent margin by properly leveraging non-curated offline data.}
\label{fig:adaption}
\end{figure*}


%%%% Ablation study plots %%%%%%

\begin{figure*}[t]
  \centering
  \begin{subfigure}
    \centering
    \includegraphics[width=0.49\linewidth]{fig/iqm-optimality-gap.pdf}
    \label{fig:sub1}
  \end{subfigure}
  \hfill
  \begin{subfigure}
    \centering
    \includegraphics[width=0.45\linewidth]{fig/sample-eff.pdf}
    \label{fig:sub2}
  \end{subfigure}
  \vskip -0.1in
  \caption{Ablation study on the role of each component. ``P'' represents world model pretraining, ``ER'' means experience rehearsal, and ``G'' represents execution guidance.  Together with the proposed retrieval-based experience rehearsal and execution guidance, world model pre-training boosts RL performance on a wide range of tasks.}
  \label{fig:ablation}
  % \vskip -0.1in
\end{figure*}


\textbf{Results}~~
\Cref{fig:finetune} shows comparison results with baselines. Our method outperforms or matches \emph{all} compared baselines by a large margin.
Compared to R3M, WPT stresses the importance of world model pre-training as well as reusing offline data during fine-tuning compared to representation learning only. 
R3M fails to improve sample efficiency on most of the tested tasks. The same conclusion is obtained in~\citet{hansen2022pre}.  
UDS and ExPLORe reuse offline data by labeling offline data with zero rewards and UCB rewards respectively. 
The labeled offline data is concatenated with online data for off-policy updates. 
We find UDS only shows slightly better performance on the Walker Run task compared to R3M and JSRL-BC, showing the ineffectiveness of directly labeling offline data with zero rewards. 
ExPLORe shows better performance on 2/3 locomotion tasks compared to other baselines and shows signs of meaningful progress on two challenging manipulation tasks. 
However, WPT still outperforms ExPLORe by a large margin. 
Compared to ExPLORe, WPT demonstrates the superiority of world model pre-training as well as execution guidance.

Compared to JSRL-BC, our method also demonstrates a clear performance boost.
JSRL-BC's performance heavily depends on the distribution of offline datasets. 
It can achieve good performance if a good prior actor can be extracted from the offline data. 
However, in our case, since we aim to lift the assumption of expert trajectories, the pre-trained BC agent usually fails to work well in the target task. 
As a result, JSRL-BC is only slightly better than the rest baselines on the Cheetah Run Hard, Assembly, and Shelf Place tasks while achieving similar results on the rest. 
In contrast, WPT works well on leveraging non-expert offline data.
For example, the Quadruped Walk shows that WPT can nicely benefit from exploratory offline data collected by unsupervised RL, which enables it to control the Ant robot to walk forward from pixel inputs with only 100 trials.

In~\cref{fig:aggreated_performance} and~\cref{app:full_results}, we further compare our method with two widely used learning-from-scratch baselines, Dreamerv3 and DrQv2, on 72 tasks. 
We show that by leveraging the non-curated offline data, WPT can clearly improve the sample efficiency on a wide range of tasks. Furthermore, our method achieves promising performance on several hard exploration tasks, while learning-from-scratch baselines fail.

\subsection{WPT Enables Fast Task Adaptation}

In many real-world applications, we want the robot to continuously adapt to new tasks. In this section, we investigate whether WPT benefits task adaptation. 
In this setting, the agent is required to incrementally solve a sequence of tasks. 
The setting is very similar to continual reinforcement learning (CRL) or life-long RL~\cite{parisi2019continual,khetarpal2022towards} but with a limited set of tasks for simplicity. 
Note that CRL has a broad scope; assumptions and experiment setups vary among methods, which makes it difficult to set up a fair comparison with other methods. 
Instead of proposing a SOTA CRL method, we aim to demonstrate that our method offers a simple yet general recipe to leverage previous data that also fits the CRL setting.

\textbf{Setup \& baselines}~~
We set our continual multi-task adaption experiment based on the Ant robot from the DeepMind Control Suite. 
Specifically, the agent needs to sequentially solve stand, walk, run, jump, roll, and roll fast tasks. 
We train 300K environment steps for each task. To solve one task, the agent accesses all previous experiences as well as the model's weights trained on previous tasks. 
To have a fair comparison, i.e., having comparable model parameters and eliminating the potential effects from pre-training on other tasks, we pre-train a small world model only on the Ant domain. 
We compare our method with PackNet~\cite{mallya2018packnet}, which is a common baseline for continual learning. 
PackNet iteratively prunes the actor's parameters by keeping the weights with a larger magnitude while re-initializing the rest. By doing so, the actor maintains skills learned previously, which may help the learning of new tasks. 
For each new task, we fine-tune the actor model by iterative pruning while randomly initializing the critic model as rewards are not shared among tasks.

\textbf{Results}~~
\Cref{fig:adaption} shows policy performance on incremental task adaption. As we can see WPT outperforms baselines by a large margin.
Compared to PackNet, WPT shows great potential for pre-training of world models with non-curated offline data together with experience rehearsal and actor prior. For the tested Ant domain, WPT enables the agent to adapt to new tasks within only 100 trials on all six tasks. With the limited sample budget, PackNet only achieves $\sim20-60\%$ episodic returns of WPT. We would argue the diversity of the non-curated offline data contributes to WPT's supreme performance on task adaptation. 

\subsection{Ablations} \label{experiment:abaltion}
In this section, we investigate the role of each component of our method. We use the same set of tasks used in~\cref{fig:finetune}.
As shown in~\cref{fig:ablation}, world model pre-training shows promising results when the offline dataset consists of diverse trajectories, such as data collected by exploratory agents, while fails to work well when the offline data distribution is relatively narrow as in the Meta-World tasks. 
This is due to {\em i)} world model pre-training alone failing to fully leverage the rich action prior in the offline data and {\em ii)} the distributional shift between offline and online data hurts the world model fine-tuning. The proposed retrieval-based experience rehearsal and execution guidance helps the agent benefit from action prior in the offline data and enriches the online sample diversity, which together enables WPT to achieve strong performance on a wide range of tasks.
