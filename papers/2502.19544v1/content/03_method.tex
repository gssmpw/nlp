
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%        Method               %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}

In this section, we detail our two-stage approach, which consists of {\em (i)} world model pre-training, which learns a generalist (i.e. multi-task \& embodiment) world model, given offline data, which rather importantly, includes reward-free and non-expert data, and {\em (ii)} RL-based fine-tuning which leverages the pre-trained world model and online interaction in an offline-to-online fashion. See \cref{fig:overview} and \cref{alg:wpt}.

\subsection{Problem Setup}
In this paper, we assume the agent can access a static offline dataset $\mathcal{D}_{\text{off}}$, which consists of trajectories $\{\tau_{\text{off}}^i\}_{i=1}^{N_{\text{off}}}$, where $\tau_{\text{off}}^i$ includes observations and actions $\{o^i_t, a^i_t\}_{t=0}^T$ collected by \textit{unknown} behavior policies. 
This means that {\em (i)} rewards $r^i_t$ are unknown, {\em (ii)} $\mathcal{D}_{\text{off}}$ does not necessarily include expert trajectories, and {\em (iii)} datasets consist of multi-embodiment data. 
The agent interacts with the environment to collect labeled trajectories $\tau_{\text{on}}^i = \{o^i_t, a^i_t, r^i_t\}_{t=0}^T$ and stores them in an online dataset $\mathcal{D}_{\text{on}} = \{\tau_{\text{on}}^i\}_{i=1}^{N_{\text{on}}}$. 
The goal of this paper is to learn a high-performance policy from both $\mathcal{D}_{\text{off}}$ and $\mathcal{D}_{\text{on}}$ whilst minimizing the amount of online interaction ($N_{on}$) by leveraging the unlabeled offline data $\mathcal{D}_{\text{off}}$. 
Removing the requirement of rewards and expert trajectories enables the agent to leverage a large set of diverse datasets. 

\subsection{Multi-Embodiment World Model Pre-training} 
\label{method:pretrain}
 
During pre-training, instead of training one model per task -- as done in previous work~\cite{hafner2019dream,hafner2020mastering,hafner2023mastering,hansen2023td} -- we show that a single multi-task \& embodiment world model more effectively leverages the offline data. 
Compared to methods~\cite{yang2021representation,schwarzer2021pretraining,yuan2022pre,parisi2022unsurprising,ze2023visual} that only pre-train representations, pre-training a world model creates a general understanding of the environment, which we show can further boost agents' performance during fine-tuning.


We adopt a widely used world model in the literature, the recurrent state space model (RSSM)~\citep{hafner2019learning} by making several modifications: {\em (i)} we remove the task-related losses, {\em (ii)} we pad the action with zeros to unify the action dimension of different embodiments, and {\em (iii)} we scale the model size to 280M. 
Given these modifications, we show that RSSMs can successfully learn the dynamics of multiple embodiments and can be fine-tuned for mastering different tasks.
Our first stage consists of pre-training the following components:
\begin{align*}
\text{Sequence model} &: h_t = f_\theta (h_{t-1}, z_{t-1}, a_{t-1}) \\
\text{Encoder} &: z_t \sim q_\theta(z_t|h_t, o_t) \\
\text{Dynamics predictor} &: \hat{z}_t \sim p_\theta(z_t|h_t) \\
\text{Decoder} &: \hat{o}_t \sim d_\theta(\hat{o}_t | h_t, z_t).
\end{align*}
The models $f_\theta$, $q_\theta$, $p_\theta$ and $d_\theta$ are optimized jointly by minimizing:
\begin{multline}
  \label{eq:wm objective}
    \mathcal{L}(\theta) = \mathbb{E}_{p_\theta, q_\theta}\Big[ \sum_{t=1}^T \underbrace{-\ln p_\theta(o_t|z_t, h_t)}_\text{pixel reconstruction loss} \\
     +  \beta \cdot \underbrace{\text{KL} \big( q_\theta(z_t|h_t, o_t) \ || \ p_\theta(z_t|h_t) \big)}_\text{latent state consistency loss} \Big].
\end{multline}

The first term minimizes the reconstruction error while the second term enables the latent dynamics learning. 
Note that we removed the task-related objectives, e.g., reward prediction and continue prediction. 
We note that there is plenty of room to improve the world model's pre-training, e.g., by leveraging recently developed self-supervised training methods~\cite{eysenbach2023contrastive}, or advanced architectures~\cite{vaswani2017attention,gu2021efficiently}. These improvements are orthogonal to our method and we leave it as future work.

\subsection{RL-based Fine-tuning}
In our fine-tuning stage, we allow the agent to interact with the environment to collect new data $\tau_{\text{on}}^i = \{o^i_t, a^i_t, r^i_t\}_{t=0}^T$.
The newly collected data is used to learn a reward function via supervised learning whilst also fine-tuning the world model with~\cref{eq:wm objective}.
For simplicity, we represent the concatenate of $h_t$ and $z_t$ as $s_t = [h_t, z_t]$. The actor and critic are trained with imagined trajectories $\tilde{\tau} (s, a)$ generated by rolling out the policy $\pi_\phi (a|s)$ in the world model $p_\theta$ starting from the initial state distribution $p_0(s)$, which are samples from replay buffer.
Following DreamerV3, the critic $v_{\phi}(V_t^\lambda \mid s_t)$ learns to approximate the distribution 

over the $\lambda$-return $V_t^\lambda$ calculated as,
\begin{equation} \label{eq:critic_return}
 \underbrace{V^\lambda_t}_{\lambda-\text{return}} = \hat{r}_t + \gamma~\begin{cases} (1-\lambda) v_{t+1}^\lambda + \lambda V^\lambda_{t+1} &\text{if}~~ t < H \\  v_H^\lambda &\text{if}~~ t=H \end{cases},
\end{equation}
where $v_t^\lambda = \mathbb{E}[ v_\phi(V_t^\lambda \mid s_t)]$ denotes the expected value of the distribution predicted by the critic.
It is trained using maximum likelihood by minimizing
\begin{equation} \label{eq:critic_loss}
\mathcal{L}(v_\phi) = \mathbb{E}_{p_\theta,\pi_\phi} \left [ - \sum_{t=1}^{H-1} \mathrm{ln} \ v_{\phi}(V_t^{\lambda} \mid s_t) \right].
\end{equation}
The actor $\pi_\phi(a_t|s_t)$ is then updated by maximizing the $\lambda$-return regularized by actor entropy $\mathbf{H}[a_t|s_t]$:
\begin{align} \label{eq:actor}
    \mathcal{L}(\pi_\phi) &= \mathbb{E}_{p_\theta, \pi_\phi}\left[ 
    \sum^{H-1}_{t=1} \left( -v_t^\lambda - \eta \cdot \mathbf{H}[a_t|s_t] \right) \right].
\end{align}


In practice, we found that using the pre-trained world model alone fails to work well in many cases, especially for hard-exploration tasks.
To identify the reasons, we need to rethink the role of each component in policy updates. 
Given the policy update rule in~\cref{eq:actor}, the policy update is infected by the distribution of imagined trajectories $\tilde{\tau}(s,a)=p_0(s)\prod_{t=0}^{H-1}\pi_\phi(a_t|s_t)p_\theta(s_{t+1}|s_t,a_t)$ and the reward model $r_\theta(s_t, a_t)$, i.e., the initial state distribution $p_0(s)$, the policy $\pi_\phi(a|s)$, the dynamics model $p_\theta(s_{t+1}|s_t, a_t)$, and the reward model $r_\theta(s_t, a_t)$. 
Intuitively, as shown in~\cref{fig:illustration}, the policy will learn a good policy if the imagined trajectories $\tilde{\tau}$ contain ``promising'' data for policy learning and the reward function $r_\theta(s_t,a_t)$ can correctly label those trajectories. 
The world model pre-training gives a better initialization of $p_\theta(s_{t+1} | s_t, a_t)$. 
However, when the online data distribution is narrow -- as in hard exploration tasks -- the initial state distribution $p_0(s)$ can also be narrow, which prevents the actor from reaching high-reward states during imagination. 
Also, the reward function $r_\theta(s,a)$ trained on the narrow online data distribution will incorrectly assign rewards for transitions generated by the world model pre-trained on a broader distribution. 
That is, the agent fails to improve the policy even if ``promising'' trajectories exist in $\tilde{\tau}$, since incorrect rewards are assigned. 
Furthermore, fine-tuning the world model $p_\theta(s_{t+1}|s_t, a_t)$ on the narrow distribution causes catastrophic forgetting.
In the following, we improve the agents' performance by reusing non-curated offline data for {\em i)} augmenting the initial state distribution $p_0(s)$, {\em ii)} collecting diverse online trajectories, and {\em iii)} improving the reward predictions. See \cref{fig:illustration} for an illustrative motivation.


\paragraph{Retrieval-based Experience Rehearsal}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{fig/mpt-model-rollout.pdf}
\caption{Motivation of using experience rehearsal and execution guidance. During fine-tuning, experience rehearsal augments the initial state $p_0(s)$ used for model rollout, which enables the model rollout to start from informative states. Execution guidance helps the agent to collect data that is close to the offline data distribution, which enriches the diversity of the collected data and improves the reward model's accuracy on a broader state distribution.}
\label{fig:illustration}

\end{figure}


Experience replay is a simple yet effective approach to prevent catastrophic forgetting~\citep{khetarpal2022towards} as well as improving policy learning~\citep{ ball2023efficient,li2023accelerating}. 
In the RL setting, \citet{ball2023efficient} demonstrates promising results by replaying offline data to improve sample efficiency but requires the offline data to be reward-labeled. ExPLORe~\cite{li2023accelerating} reuses single-embodiment but reward-free offline data to encourage exploration by labeling offline data with UCB rewards. 
Unlike the replay scheme used in ExPLORe, where the offline datasets are relatively small and well-structured, directly replaying the non-curated offline data can be challenging. 
Our dataset can be $\sim100\times$ larger and consist of different tasks and embodiments. 
As such, directly labeling the offline dataset with UCB reward or even replaying the dataset can be infeasible. 

To enable the usage of the non-curated offline data, we propose a simple retrieval-based approach to filter the trajectories that are close to the downstream tasks. 
Specifically, we retrieve a subset of trajectories from the non-curated offline data based on the neural feature distance between the online samples and the trajectories in the offline dataset. 
We calculate the distance as :
\begin{equation} \label{eq:retrieval}
    \mathbf{D} = ||\text{e}_\theta(o_{\text{on}}) - \text{e}_\theta(o_\text{off}) ||_2 ,
\end{equation}
where $\text{e}_\theta$ is a feature extractor and $o_{\text{on}}$ and $o_{\text{off}}$ are images from the initial observation of trajectories from the online buffer and the offline dataset, respectively. 
In practice, we found that neural representation matters for retrieval and the encoder learned via world model pre-training works better than the general visual model R3M~\cite{nair2022r3m}.

To efficiently search over the whole dataset, we pre-build the key-value pairs between the trajectories ID and its neural features and use Faiss~\cite{douze2024faiss} for similarity search. 
Once the key-value database is built, the buffer retrieval procedure only takes seconds. 
The retrieved offline buffer, although reward-free, contains embodiment-specific data and is feasible to use. 
The retrieved offline data can be used to prevent catastrophic forgetting in the world model and to augment the initial state distribution $p_0(s)$ used for trajectory imagination. 

%%%%%%%%% Experiments Results %%%%%%%%%%
%%% Fine-tuning results %%%

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\textwidth]{fig/main_plot_new.pdf}
\vskip -0.1in
\caption{WPT enables sample-efficient RL training. We include 6 tasks from DMControl and Meta-World with pixel inputs. The mean and the corresponding 95\% confidence interval across 3 seeds are plotted. Full results on 72 tasks are included in~\cref{app:full_results}.}

\label{fig:finetune}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Execution Guidance via Prior Actors}
The common practice in RL training is to initialize the replay buffer with a random policy and gradually collect new data by interacting with the environment using the training policy.
When using a pre-trained world model, it is preferred to start the training from a distribution that is close to the offline data distribution.
This has three benefits:
First, offline data usually contains useful information for policy training, such as near-expert trajectories or a broader state-action distribution.
Second, the distribution shift between the offline and online data may ruin the pre-trained weights, so bringing the initial distribution close to the offline data is a good strategy to prevent this.
Thirdly, the reward function can get better predictions for states that are close to the offline data distribution. This improves policy learning as the initial states used to generate the imagined trajectories $\tilde{\tau}$ are augmented with offline samples.

To guide the online data collection such that it remains close to the offline data distribution, we simply train a prior actor $\pi_{\text{bc}}$ via behavior cloning on the \textit{retrieved} buffer. During online data collection, we switch between the policy prior $\pi_{\text{bc}}$ and the RL agent $\pi_\phi$ according to a pre-defined schedule.
Specifically, at the beginning of an episode, we decide whether to use $\pi_\text{bc}$ according to the scheduled probability. If the $\pi_\text{bc}$ is selected to use, we then randomly select the starting time step $t_\text{bc}$ and duration $H$ of using $\pi_{\text{bc}}$. 
$\pi_\phi$ is then used for the rest of the time steps.
This procedure is similar to JSRL~\cite{uchendu2023jump} but with a couple of differences: 
{\em i)} WPT leverages non-curated offline data while JSRL uses task-specific offline data  and
{\em ii)} WPT shows the superiority of a model-based approach for leveraging offline data while JSRL is a model-free approach.
As a result, we show that WPT obtains better performance than JSRL on tested tasks.
The full algorithm is presented in~\cref{app:algo}.

