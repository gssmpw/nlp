\section{Related Work}

In this section, we review methods which leverage offline data, including pre-training in the context of RL and the so-called generalist agents. 
See \cref{tab:related_work} for a comparison of what types of data the methods can use.

\textbf{RL with task-specific offline datasets}~~
Leveraging offline data is a promising direction to improve the sample efficiency in RL. 
One representative method is Offline RL.
Offline RL trains agents from offline data without interacting with the environment. 
It usually constrains the distance between the learned policy and behavior policies in different ways~\citep{kumar2019stabilizing,kumar2020conservative,wu2019behavior,kostrikov2021offline,kostrikov2021boffline,fujimoto2021minimalist,uchendu2023jump}. 
However, the performance of the learned policy is highly affected by the quality of offline datasets~\citep{yarats2022don}. 
To continue improving the policy, offline-to-online RL~\citep{nair2020awac,zhao2022adaptive,lee2022offline,yu2023actor,rafailov2023moto} was proposed. 

Offline RL and offline-to-online RL commonly suffer from training instability and require additional treatments~\cite{lee2022offline,lu2022challenges}. RLPD~\cite{ball2023efficient} shows that off-policy RL delivers strong performance by directly concatenating the offline data with online data. However, RLPD still requires reward-labeled task-specific offline data and hasn't discussed the multi-embodiment cases. Recently, ExPLORe~\cite{li2023accelerating} labels reward-free offline data with approximated upper confidence bound (UCB) of rewards to solve hard exploration tasks. However, ExPLORe obtains good performance when the offline data includes near-expert data of the target tasks, while we consider a more general setting with non-curated offline data.

\textbf{RL with multi-task offline datasets}~~
Recently, several works have applied offline RL in a multi-task setting~\cite {kalashnikov2021mt,yu2021conservative,kumar2022pre,julian2020never,hansen2023td}, but require known rewards. ~\citet{georgiev2024pwm} pre-trains a world model for multi-task RL but is designed for state-based inputs and requires reward-labeled offline data for world model pre-training.

To lift the assumption of known rewards, human labeling~\citep{cabi2019scaling,singh2019end}, inverse RL~\citep{ng2000algorithms,abbeel2004apprenticeship} or generative adversarial imitation learning~\citep{ho2016generative} can be applied. However, this requires human labor or expert demonstrations.
\citet{yu2022leverage} directly sets zero rewards for unlabelled data, which introduces additional bias. 

Another stream of work leverages in-the-wild data for RL training. Most methods focus on representation learning. \citet{stooke2021decoupling,yang2021representation,schwarzer2021pretraining,shah2021rrl,yuan2022pre,wang2022vrl3,parisi2022unsurprising,sun2023smart,ze2023visual} investigate different representation learning methods and show promising performance boosts during fine-tuning.


\textbf{Generalist Agents}~~ 
RL methods usually perform well on a single task~\cite{vinyals2019grandmaster,andrychowicz2020learning}, however, this contrasts with humans that can perform multiple tasks well. 
Recent works have proposed generalist agents that master a diverse set of tasks with a single agent~\citep{reed2022generalist,brohan2022rt,team2024octo}. 
These methods typically resort to scalable models and large datasets and are trained via imitation learning~\citep{brohan2022rt,brohan2023rt,o2023open,khazatsky2024droid}. 
In contrast, we seek to train a generalist world model and use it to boost RL performance for multiple tasks and embodiments.\looseness-1

\textbf{World models}~~
World models learn to predict future observations or states based on historical information. World models have been widely investigated in online model-based RL~\citep{haRecurrentWorldModels2018,hafner2019dream,micheli2022transformers,alonso2024diffusionworldmodelingvisual}.
Recently, the community has started investigating scaling world models~\citep{haRecurrentWorldModels2018}, for example, \citet{hu2023gaia,pearce2024scaling,wu2025ivideogpt,agarwal2025cosmos} train world model with Diffusion Models or Transformers. 
However, these models are usually trained on demonstration data. 
In contrast, we explore the offline-to-online RL setting -- closely fitting the successful pre-train and then fine-tune paradigm -- and we focus on leveraging reward-free and non-expert data to increase the amount of available data for pre-training.
