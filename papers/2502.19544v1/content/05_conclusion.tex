%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%        Conclusion           %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
% 
We propose WPT, a simple yet efficient approach to leverage non-curated offline data. 
We show that the generalist world model pre-trained on non-curated data can boost RL training spanning multiple tasks and multiple embodiments. 
Together with retrieval-based experience rehearsal and execution guidance, WPT outperforms baselines on a wide range of tasks, including 22 locomotion tasks and 50 manipulation tasks.
WPT unlocks ample sources of offline data and our results show that leveraging this non-curated data leads to strong performance.
Nevertheless, our method can be improved in multiple ways, for example, by extending our methods to real-world applications as well as proposing novel world model architectures.