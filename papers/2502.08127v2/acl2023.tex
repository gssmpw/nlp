% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}
\usepackage{array}
\usepackage{booktabs}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{tcolorbox}
\usepackage{listings}
\tcbuselibrary{listingsutf8} % Ensure compatibility
% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{7cm}
%
% and set <dim> to something 5cm or larger.

\title{Fino1: On the Transferability of Reasoning‑Enhanced LLMs to Finance}
%\title{Fino1: Large Language Models for Finance Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\small
\author{
  Lingfei Qian \\
  The Fin AI \\
  \texttt{lfqian94@gmail.com} \\\And
  Weipeng Zhou \\
  The Fin AI \\\And
  Yan Wang \\
  The Fin AI \\\AND 
  Xueqing Peng \\
  The Fin AI \\
  %\texttt{xueqing.peng2024@gmail.com} \\
  \And
  Yi Han \\
  Georgia Institute of Technology\\\And
  Jimin Huang \\
  The Fin AI \\\AND
  Qianqian Xie\thanks{\phantom{h}Corresponding author} \\
  The Fin AI \\
  \texttt{xqq.sincere@gmail.com} \\
\And
  Jianyun Nie \\
  University of Montreal \\
  %\texttt{nie@iro.umontreal.ca}
}


\begin{document}
\maketitle
\begin{abstract}
% Recent advancements in large language models (LLMs) have demonstrated impressive general reasoning capabilities. However, their effectiveness in financial reasoning—a critical requirement for real-world applications in the financial domain—remains largely underexplored.
While large language models (LLMs) have shown strong general reasoning capabilities, their effectiveness in financial reasoning—which is crucial for real-world financial applications—remains underexplored. 
In this study, we conduct a comprehensive evaluation of 24 state-of-the-art general and reasoning-focused LLMs across four complex financial reasoning tasks involving financial text, tabular data, and equations. 
We assess key capabilities such as numerical reasoning, tabular interpretation, financial terminology comprehension, long-context understanding, and equation-based problem solving. 
% Our findings reveal that while improvements in data quality and pretraining benefit financial reasoning, general-domain techniques such as chain-of-thought (CoT) fine-tuning do not consistently translate to gains in financial tasks. Furthermore, all existing reasoning strategies struggle with long-context and multi-table scenarios.
% Our analysis reveals that while data quality and pretraining help, general techniques like chain-of-thought (CoT) fine-tuning offer limited gains in financial settings, especially for long-context and multi-table tasks. 
% To address these challenges, we propose two domain-adapted models, \textbf{Fino1-8B} and \textbf{Fino1-14B}, trained using chain-of-thought (CoT) fine-tuning and reinforcement learning with domain-specific reasoning paths. Our models are trained on a carefully curated dataset that integrates high-quality examples from diverse sources, covering a wide range of formats including financial reports, tables, equations, and structured XBRL texts. Despite using only modest amounts of training data, our models achieve an {8–10\% performance improvement}, outperforming several advanced LLMs—including GPT reasoning-focused models (GPT-o1 and GPT-o3-mini), the latest {GPT-4.5}, and DeepSeek reasoning models (V3 and R1). These results highlight the effectiveness of domain-specific reasoning training and demonstrate the strong practical value of our models, particularly in resource-constrained deployment scenarios.
Our analysis reveals that while data quality and pretraining contribute to performance, general techniques like chain-of-thought (CoT) fine-tuning offer limited gains in financial tasks. To address this, we propose two domain-adapted models, \textbf{Fino1-8B} and \textbf{Fino1-14B}, trained with CoT fine-tuning and reinforcement learning using domain-specific reasoning paths. Our models are trained on a carefully curated dataset integrating high-quality examples from diverse sources, covering financial reports, tables, equations, and structured XBRL texts. Despite limited training data, they achieve an {7–9\% performance improvement}, outperforming several advanced LLMs—including GPT-o1, GPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1)—demonstrating strong practical value in resource-constrained scenarios.
% rivaling models up to 70B parameters—offering practical value in resource-constrained scenarios. 
Our findings highlight the need for domain-specific adaptations in financial reasoning, and we release all datasets, models, and code for future research%\footnote{\url{https://anonymous.4open.science/r/Fino1-CE1F/README.md}
\footnote{\url{https://github.com/The-FinAI/Fino1}}. %we release part of the code through a anonymous link.}.

% To address these limitations, we introduce two domain-adapted financial reasoning models, Fino1-8B and Fino1-14B, developed through CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. 
% Despite being trained on only thousands of financial examples, our models achieve a performance improvement of 8–10\% across all tasks, surpassing all baseline models with similar parameter sizes and rivaling or even exceeding the performance of models with up to 70B parameters. Notably, our models demonstrate strong potential for deployment in real-world financial applications, especially under resource-constrained settings where deploying extremely large models is not feasible.
% Our results underscore the importance of domain-specific adaptation for financial reasoning tasks and point to future research directions, including enhanced multi-table reasoning, long-context processing, and deeper understanding of financial terminology. All our datasets, models, and codes will be made publicly available.
% Our findings from comprehensive evaluations underscore the necessity of domain-specific reasoning adaptations for financial tasks.
% %as domain-specific fine-tuning yields stable and significant improvements. 
% Additionally, we highlight potential directions for developing more advanced reasoning models in the financial domain, including multi-table reasoning, long-context processing, and financial terminology comprehension. 
%Furthermore, we introduce a leaderboard for benchmarking future datasets and models\footnote{\url{https://huggingface.co/spaces/TheFinAI/open-finllm-reasoning-leaderboard}}.
%All our datasets, models\footnote{\url{https://huggingface.co/TheFinAI}}, and codes\footnote{\url{https://github.com/The-FinAI/Fino1}} are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models\footnote{\url{https://huggingface.co/spaces/TheFinAI/open-finllm-reasoning-leaderboard}}.

\end{abstract}


\section{Introduction}
Advancements in large language models (LLMs) have demonstrated remarkable performance across various natural language pocessing tasks, including content generation\cite{van2024adapted, zhang2024systematic}, language translation \cite{xu2024contrastive}, and sentiment analysis \cite{xing2024designing, miah2024multimodal}. More recently, reasoning-enhanced models such as OpenAI’s o1 and DeepSeek’s R1 have been developed to extend LLM capabilities in complex reasoning \cite{guo2025deepseek, jaech2024openai}. These models exhibit significant improvements, particularly in mathematical and logical tasks that require complex reasoning \cite{temsah2024openai, zhong2024evaluation}. However, despite their success in general reasoning, their performance in the financial domain remains largely unexplored. 


Financial reasoning is essential in the financial domain, where professionals must extract key insights from lengthy reports and multi-table documents. It requires not only numerical computation but also a deep understanding of financial terminology, regulations, and economic concepts~\cite{xie2023pixiu, xie2024finben}. Unlike general reasoning tasks, financial reasoning involves (1) domain-specific language comprehension, (2) reasoning over financial concepts and numbers, and (3) interpreting structured data such as financial tables~\cite{xie2024open}. Evaluating LLMs in this context is critical for closing the gap between general AI reasoning and real-world financial applications.
% Financial reasoning play an important role for financial domain where reasoning over long financial reports and across tables is required  for financial workers to get key insights of financial documents effeiciently.  inherently demand rigorous reasoning, requiring not only numerical computations but also a deep contextual understanding of financial terminology, regulations, and economic principles~\cite{xie2023pixiu, xie2024finben}. Unlike general reasoning tasks, financial reasoning necessitates (1) comprehension of domain-specific terminology, (2) the ability to establish and manipulate relationships between financial concepts and numbers to derive accurate conclusions, and (3) an understanding of the structure and content of financial tables \cite{xie2024open}. Given the specialized nature of financial decision-making, assessing the reasoning capabilities of LLMs in this domain is crucial for bridging the gap between general AI reasoning and financial applications. Effective financial reasoning requires models to integrate textual, numerical, and structured data, highlighting the need for domain-specific adaptations to enhance AI-driven financial analysis.


To address this gap, we firstly comprehensively evaluate the performance of existing powerful reasoning models and conduct an in-depth analysis of their capabilities in financial tasks, to provide insights into the strengths and limitations of existing reasoning models in financial applications. We focus on the following key aspects:
(1) \textbf{Transferability of general-domain reasoning enhancements} – Assessing whether reasoning-enhanced LLMs, originally optimized for general-domain reasoning, also demonstrate improved performance in financial reasoning tasks.
(2) \textbf{Impact across financial tasks} – Analyzing how the reasoning enhancement process influences LLM performance across different types of financial tasks.
(3) \textbf{Scaling and performance gap} – Investigating the differences between smaller reasoning models and large-scale LLMs in financial reasoning.
(4) \textbf{Limits of LLMs in complex financial tasks} – Evaluating the extent to which LLMs can handle extremely complex financial reasoning challenges.

% we evaluate 16 models  including xxx 
% list some xxx 
% on xx datasets covering xxx tasks.
% to assess xx abilities of LLMs

% then specific 
% xx dataset on xx task

% We evaluated the performances of 16 large language models from the GPT \cite{achiam2023gpt}, LLaMA \cite{dubey2024llama}, DeepSeek \cite{liu2024deepseek}, and Qwen \cite{yang2024qwen2} families on three financial datasets, FinQA \cite{chen2021finqa}, DM-Simplong \cite{zhao2024docmath}, and XBRL-Math \cite{wang2025finnlp}, which incorporate financial text, tabular data, and equations as inputs. Our selection includes both general purpose and reasoning-enhanced models, such as DeepSeek-V3 \cite{liu2024deepseek} and DeepSeek-R1 \cite{guo2025deepseek}, spanning from smaller models like LLaMA 3-8B-Instruct \cite{dubey2024llama} to high-capacity models like GPT-o1 \cite{jaech2024openai}, ensuring a comprehensive assessment across different computational scales.
% These three financial reasoning datasets are designed to evaluate distinct aspects of financial understanding. Collectively, they assess models’ abilities in numerical reasoning, tabular data interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving.
% Through this evaluation, our objective is to examine the capacity of LLMs to process financial concepts, reason over structured and unstructured financial data, and perform complex mathematical operations, offering deeper insight into their strengths and limitations in financial analysis tasks.

% From experimental results and analysis, we have the following findings:

% (1) General reasoning enhancement strategies, such as those employed in DeepSeek-R1 and GPT-o1, do not consistently improve performance in financial domain tasks. In fact, these models exhibit a performance decline compared to general-purpose models like DeepSeek-V3 and GPT-4o, suggesting challenges in understanding financial terminology and reasoning over long contexts and multi-table data.

% (2) While improvements in pretraining methods and increased training data in general domains, as well as increasing model size all contributes to performance gains. For financial tasks, model performance tends to plateau at an upper boundary. Specifically, after reaching 70B or even 32B parameters, financial reasoning tasks show no significant improvement.

% (3) The performance of different reasoning enhancement strategies in financial tasks can vary significantly. For instance, general-domain techniques like Chain-of-Thought and reflection-tuning in GPT-o1, designed for math and code, show no improvements in financial reasoning. In contrast, results show that DeepSeek and Qwen-Math incorporate enhance formula computations and structured data parsing using tool-integrated reasoning (TIR) and the process reward model (PRM), and knowledge distilled from general-purpose models. While these methods improve numerical accuracy in financial reports and market data, their impact on financial terminology comprehension, long-context modeling, and multi-table inference remains limited.

% We evaluated 16 large language models from the GPT~\cite{achiam2023gpt}, LLaMA~\cite{dubey2024llama}, DeepSeek~\cite{liu2024deepseek}, and Qwen~\cite{yang2024qwen2} families on three financial reasoning benchmarks: FinQA~\cite{chen2021finqa}, DM-Simplong~\cite{zhao2024docmath}, and XBRL-Math~\cite{wang2025finnlp}. These datasets involve financial text, tabular data, and equations, testing models on numerical reasoning, financial terminology, long-context understanding, and equation-based problem solving. Our evaluation includes both general-purpose and reasoning-enhanced models, ranging from smaller (e.g., LLaMA 3-8B-Instruct~\cite{dubey2024llama}) to high-capacity models (e.g., GPT-o1~\cite{jaech2024openai}). Experiments results demonstrate the following key insigts: (1) Reasoning-enhanced models like DeepSeek-R1 and GPT-o1 do not consistently outperform general-purpose models in financial tasks, likely due to challenges in financial terminology and multi-table reasoning. (2) Performance gains from larger model sizes and improved pretraining plateau beyond 32B or 70B parameters. (3) Reasoning strategies tailored for math and code (e.g., CoT, reflection tuning) offer limited benefits, while tool-integrated reasoning (TIR), process reward models (PRM), and knowledge distillation in DeepSeek and Qwen-Math enhance formula computation but still fall short in handling complex financial contexts and terminology.

We evaluated 24 LLMs from the \textbf{GPT}~\cite{achiam2023gpt}, \textbf{LLaMA}~\cite{dubey2024llama}, \textbf{DeepSeek}~\cite{liu2024deepseek}, and \textbf{Qwen}~\cite{yang2024qwen2} families on four financial reasoning benchmarks: \textbf{FinQA}~\cite{chen2021finqa}, \textbf{DM-Simplong}~\cite{zhao2024docmath}, \textbf{XBRL-Math}~\cite{wang2025finnlp} and \textbf{DM-Complong}. 
% 
These datasets span financial texts, tables, and equations, enabling evaluation of models in numerical reasoning, domain-specific terminology comprehension, long-context processing, and equation-based reasoning tasks.
Our evaluation encompasses both general-purpose and reasoning-enhanced models, ranging from smaller-scale models (e.g., LLaMA 3-8B-Instruct\cite{dubey2024llama}) to high-capacity models (e.g., GPT-o1\cite{jaech2024openai}), as well as reasoning-augmented models incorporating strategies such as data sampling and financial Chain-of-Thought (CoT) construction.
Experimental results reveal three key insights:
(1) Reasoning-enhanced models like DeepSeek-R1 and GPT-o1 do not consistently outperform general-purpose models on financial tasks, likely due to challenges related to domain-specific terminology and multi-table reasoning;
(2) Performance gains from larger model sizes and improved pretraining tend to plateau beyond 32B or 70B parameters;
% (3) Math- and code-oriented reasoning techniques (e.g., CoT, reflection tuning) yield limited benefits in financial contexts, while approaches such as tool-integrated reasoning (TIR), process reward models (PRM), and knowledge distillation (as in DeepSeek) improve formulaic computation but remain insufficient for more complex financial reasoning tasks.

To develop reasoning models tailored for financial domain, we build novel financial reasoning-enhanced large language models Fino1-8B and Fino1-14B based on LLaMA-3.1-8B-Instruct\cite{dubey2024llama} and Qwen2.5-14B-Instruct\cite{yang2024qwen2}, chosen for their strong performance among smaller-scale LLMs. To enhance reasoning capabilities, we apply CoT fine-tuning and reinforcement learning (RL), inspired by the training strategies of HuatuoGPT-o1~\cite{chen2024huatuogpt}.
We leverage a diverse set of high quality financial reasoning datasets—including FinQA~\cite{chen2022finqadatasetnumericalreasoning}, TATQA~\cite{zhu2021tatqaquestionansweringbenchmark}, DocMath (Simpshort and Compshort)~\cite{zhao2024docmathevalevaluatingmathreasoning} , DocFinQA~\cite{reddy2024docfinqalongcontextfinancialreasoning}, and BizBench-QA~\cite{krumdick2024bizbench}—covering tasks such as long-context question answering, complex reasoning over financial documents, and table-text integration. 
%Reasoning paths are constructed using GPT-4o, enabling our 8B and 14B models to surpass—the performance of advanced closed-source models such as GPT-o3-mini and GPT-o1, and approach SOTA LLMs such as DeepSeek-V3 and DeepSeek-R1, and GPT-4o.
We construct high-quality reasoning paths using GPT-4o, combined with a novel data filtering process that selects more challenging samples for training—particularly for the 14B model—ensuring the model learns from difficult, domain-specific reasoning cases rather than generic QA pairs. Experimental results show that this targeted training strategy enables our models to surpass the performance of advanced closed-source models such as GPT-o3-mini and GPT-o1, and approach most powerful LLMs like DeepSeek-V3, DeepSeek-R1, and GPT-4o, demonstrating that domain-specific reasoning-enhanced training with limited, high-quality data can be highly effective.

To the best of our knowledge, this work presents the first comprehensive study of reasoning-enhanced LLMs for financial tasks involving diverse inputs such as financial documents, tables, equations, and structured XBRL texts. First, we conduct a systematic evaluation of 24 general-purpose and reasoning-enhanced LLMs across multiple financial reasoning benchmarks. Second, we introduce \textbf{Fino1}, the first suite of domain-specific, reasoning-enhanced LLMs, trained using GPT-4o-distilled reasoning paths with a novel data curation and filtering strategy. Third, our results show that domain-specific reasoning training with limited high-quality data consistently outperforms general-domain strategies and even surpasses larger closed-source models. We release all models, datasets, and code to support future research in financial AI.

% To the best of our knowledge, this work presents the first comprehensive study on the reasoning capabilities of enhanced LLMs for financial reasoning tasks involving diverse inputs, including financial reports, tables, equations, and structured XBRL texts. First, we conduct a systematic evaluation of 24 general-purpose and reasoning-enhanced LLMs across multiple financial benchmarks, assessing key capabilities such as numerical reasoning, tabular interpretation, financial terminology comprehension, long-context understanding, and equation-based problem solving. Second, we propose \textbf{Fino1}, the first suite of domain-specific, reasoning-enhanced LLMs for financial applications. Our models are trained using GPT-4o-distilled reasoning paths with a novel data filtering and backtracking refinement strategy, enabling effective adaptation of general-purpose LLMs to financial tasks. Third, experimental results show that domain-specific reasoning training with limited high-quality data significantly outperforms general-domain reasoning strategies and even surpasses larger and closed-source models. We release all our models, datasets, and code to facilitate future research in financial AI.

% To the best of our knowledge, we are the first to systematically explore the reasoning capabilities of enhanced LLMs in factual reasoning tasks across various financial inputs, including financial reports, tables, equations, and structured XBRL texts. Additionally, we investigate the effectiveness of reasoning enhancement strategies using financial data in adapting general-domain models for financial tasks.
% Overall, our results indicate that reasoning enhancement strategies developed for general domains do not consistently deliver stable or effective improvements in financial reasoning tasks, whereas domain-specific enhancements with financial data lead to significant performance gains. Additionally, our comparisons show that larger training datasets do not necessarily result in better performance. We also highlight key directions for advancing reasoning-enhanced LLMs in the financial domain, including improving models’ understanding of financial knowledge and terminology, enhancing multi-table and long-context reasoning capabilities, and designing diverse, domain-specific reasoning strategies to boost performance on financial tasks.

% For a comprehensive evaluation of models, we assess their ability to understand financial terminology, numerical data, and reasoning over tables, long contexts, and equations. To achieve this, we conduct experiments on three financial datasets: FinQA, DM-Simplong, and XBRL-Math, each posing distinct challenges in financial reasoning. We evaluate models from the GPT, LLaMA, DeepSeek, and Qwen families, covering both general-purpose LLMs and reasoning-enhanced LLMs designed for logical reasoning and mathematical tasks. Our selection includes models of various scales, ranging from small models like LLaMA 3-8B to the some of the most powerful models like GPT-o1, ensuring a diverse assessment of model performance across different capacities.




\section{Methods}
\subsection{Datasets}

To evaluate the financial reasoning capabilities of reasoning-oriented and general LLMs, we select four diverse benchmark tasks. \textbf{(1) FINQA}~\cite{chen2021finqa} is a large-scale dataset that emphasizes complex numerical reasoning in financial reports, requiring the integration of structured tables and unstructured text, along with a deep understanding of domain-specific terminology. \textbf{(2) DocMath (Simplong, Complong)}~\cite{zhao2024docmath} evaluates LLMs’ numerical reasoning abilities over long, specialized documents with multi-tiered tables, testing their capacity to extract and reason over extended contexts. The \textit{simplong} subset contains relatively easier questions, while the \textit{complong} subset includes more complex questions that require advanced reasoning across tables and context. \textbf{(3) XBRL-Math}~\cite{wang2025finnlp} targets reasoning over structured financial data in XBRL filings, which involve standardized taxonomies, hierarchical disclosures, and formulas, challenging models to interpret intricate numerical relationships.

% To evaluate the overall capabilities of existing reasoning-oriented and general LLMs in handling financial reasoning tasks, we select three tasks with different attributes: 

% (1) \textbf{FINQA}~\cite{chen2021finqa}: FinQA is a large-scale dataset designed to facilitate research in numerical reasoning within the financial domain. It consists of expert-annotated question-answer pairs and is distinctive in its focus on complex numerical reasoning, requiring the integration of both structured data (such as tables) and unstructured data (such as textual descriptions) found within financial reports. The dataset contains a vast number of financial terms, posing a challenge for models to understand domain-specific terminology and extract relevant information accurately.

% (2) \textbf{DocMath (simplong)}~\cite{zhao2024docmath}: The DocMath-Eval dataset is a comprehensive benchmark designed to evaluate the numerical reasoning capabilities of LLMs, with a primary focus on long contexts consisit of specialized documents and tables. In particular, the simpllong subset is specifically designed to evaluate LLMs' numerical reasoning abilities over long financial or specialized documents that contain multiple and multi-tiered tables. It challenges models to extract relevant information from extended contexts and perform reasoning across complex tabular structures.

% (3) \textbf{XBRL-Math}~\cite{wang2025finnlp}: This dataset is designed to evaluate the numerical reasoning capabilities of LLMs in the context of XBRL (eXtensible Business Reporting Language) filings. Different from regular financial reports, XBRL files are well structured that encode financial data using standardized taxonomies, ensuring consistency and interoperability across regulatory filings. It contains structured financial documents with US GAAP XBRL tags, equations, and multi-tiered numerical relationships. This dataset challenges models to extract, interpret, and reason across interconnected financial terms and formulas, such as APR = ((Fees + Interest) / Principal) × (365 / Days in Loan Term). Understanding XBRL taxonomy, hierarchical disclosures, and numerical dependencies is essential for accurate financial data analysis, making this dataset a benchmark for evaluating LLMs' ability to handle complex financial reasoning tasks.

\begin{table}[h]
    \centering
    \small
    \renewcommand{\arraystretch}{0.5} % Adjust row spacing
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{c c c c}  
        \hline
        \textbf{Dataset} & \textbf{Size} & \textbf{Data Types} & \textbf{Average Token} \\
        \hline
        FinQA & 1100 & Tables and Texts & 1,128 \\
        DM-Simplong & 100 & Tables and Texts & 4,330 \\
        DM-Complong & 300 & Tables and Texts & 39,983\\
        XBRL-Math & 90 & Texts and Equations & 397 \\
        \hline
    \end{tabular}
    \caption{Overview of the datasets used in the study.}
    \label{tab:datasets}
\end{table}

More details of the datasets are listed in Table \ref{tab:datasets}. By conducting experiments on these three datasets, we aim to evaluate the model's ability in various aspects, including understanding financial terms, extracting relevant numbers and entities from financial reports from different sources, and reasoning over long contexts and multiple tables.

% \subsection{Evaluated Models}
% To conduct a comprehensive evaluation of existing LLMs, we select models from the GPT, LLaMA, DeepSeek, and Qwen families and other reasoning enhanced models based on these models which includ different enhancement stratigies like data sampling and financial reasoning SFT, which have demonstrated effectiveness in various natural language processing tasks. Our selection includes models of different scales, ranging from small-sized models (8B) to large models (70B), as well as even larger ones like DeepSeek-R1 (approximately 650B). Additionally, we incorporate the most advanced closed-source models, such as GPT-4o and GPT-o1.
% Finally, all the models we use are listed following:
% (1) GPT-4o \cite{hurst2024gpt}: OpenAI’s latest flagship model, featuring multimodal capabilities, improved efficiency, and real-time reasoning across text, image, and audio modalities.
% (2) GPT-o1 \cite{jaech2024openai}: A highly optimized variant within OpenAI’s GPT series, designed for enhanced reasoning and problem-solving with strong step-by-step thought processes.
% (3) GPT-o3-mini: A compact version of the o3 model, optimized for efficiency while maintaining robust language processing abilities.
% (4) DeepSeek-V3 \cite{liu2024deepseek}: DeepSeek's Mixture-of-Experts (MoE) model with 671 billion parameters, activating 37 billion per token for efficient reasoning tasks.
% (5) DeepSeek-R1 \cite{guo2025deepseek}: DeepSeek's first-generation reasoning model, achieving performance comparable to OpenAI's o1 across math, code, and reasoning tasks.
% (6) Qwen2.5-72B-Instruct \cite{yang2024qwen2}: Alibaba's open-source model demonstrating superior performance in Chinese and English tasks with advanced instruction tuning.
% (7) Qwen2.5-72B-Instruct-Math \cite{yang2024qwen2}: A specialized variant fine-tuned for mathematical reasoning, achieving strong results in complex problem-solving tasks.
% (8) Llama-series Models \cite{dubey2024llama}: This is the most widely adopted open-source large language model family, setting industry benchmarks for accessibility, efficiency, and performance. These models are trained on extensive datasets incorporating diverse multilingual and domain-specific data, leveraging high-quality pretraining and instruction tuning methodologies. In our experiments, we mainly focus on instruction finetune versions of these models ranging from 8B to 70B.
% (8) DeepSeek-R1-Distill Models \cite{guo2025deepseek}: This series of models is supervised fine-tuned on data distilled from DeepSeek-R1, leveraging its reasoning capabilities, with backbone models selected from Llama3.3-70B-Instruct, Qwen 2.5-32B, Qwen 2.5-14B, and Llama3.1-8B to balance efficiency and performance in instruction-following tasks.
% (9) Other reasoning enhanced Models\cite{muennighoff2025s1, ye2025limo, finr1}: S1-32B, Limo, FinR1-7B, these models use different straitigies for reasoning enhancement, like data sampling, and cot building with financial qa tasks.  

% \begin{table*}[t]
%     \centering
%     \scriptsize
%     \begin{tabular}{>{\raggedright}p{4cm} >{\centering}p{1.2cm} >{\centering}p{1.2cm} >{\centering}p{1.2cm} >{\centering}p{1cm} >{\centering\arraybackslash}p{3cm} }
%         \hline
%         \textbf{Model Name} & \textbf{Parameters} & \textbf{Reasoning Enhanced} & \textbf{Context Window Size} & \textbf{Close/Open Source} & \textbf{Reasoning Enhanced Training Data} \\
%         \hline
%         GPT-4o & Unknown & No & 128k & Closed & - \\
%         GPT-o1 & Unknown & Yes & 128k & Closed & Public and properity data (Human-annotated CoT, MCTS-assisted Synthetic data)  \\ 
%         GPT-o3-mini & Unknown & Yes & 128k & Closed & Public and properity data \\ 
%         \hline
%         DeepSeek-V3 & 671B & No & 128k & Open & -  \\
%         DeepSeek-R1 & 671B & Yes & 128k & Open & Cold-start data generation, post-processing data\\
%         Qwen2.5-72B-Instruct & 72B & No & 128k & Open & -  \\
%         Qwen2.5-72B-Instruct-Math & 72B & Yes & 128k & Open & Synthetic data from Qwen, high-quality mathematical data, CoT, TIR\\
%         DeepSeek-R1-Distill-Llama-70B & 70B & Yes & 128k & Open & Distilled from R1 \\
%         Llama3-70B-Instruct & 70B & No & 8k & Open & - \\
%         Llama3.1-70B-Instruct & 70B & No & 128k & Open & -  \\
%         Llama3.3-70B-Instruct & 70B & No & 128k & Open & -  \\
%         \hline
%         DeepSeek-R1-Distill-Qwen-32B & 32B & Yes & 128k & Open & Distilled from R1 \\
%         DeepSeek-R1-Distill-Qwen-14B & 14B & Yes & 128k & Open & Distilled from R1  \\
%         DeepSeek-R1-Distill-Llama-8B & 8B & Yes & 128k & Open & Distilled from R1  \\
%         Llama3-8B-Instruct & 8B & No & 8k & Open & -  \\
%         Llama3.1-8B-Instruct & 8B & No & 128k & Open & -  \\
%         \hline
%     \end{tabular}
%     \caption{Summary of evaluated large language models, including their parameter sizes, reasoning capabilities, input limits, source availability, and reasoning enhancement strategies. MCTS refers to Monte Carlo Tree Search. CoT refers to chain-of-thought reasoning. TIR means tool-integrated reasoning.}
%     \label{tab:LLMs}
% \end{table*}

\subsection{Evaluated Models}

To conduct a comprehensive evaluation, we select a diverse set of LLMs from the \textbf{GPT}, \textbf{LLaMA}, \textbf{DeepSeek}, and \textbf{Qwen} families, along with reasoning-enhanced variants that incorporate strategies such as data sampling and domain-specific fine-tuning. Our selection covers models of various scales, from smaller models (8B) to large-scale models (70B), and even ultra-large models like \textbf{DeepSeek-R1} ($\sim$650B). We also include advanced closed-source models such as \textbf{GPT-4o} and \textbf{GPT-o1} as strong baselines.

The evaluated models are summarized as follows:
\textbf{GPT-4o}~\cite{hurst2024gpt}: OpenAI’s flagship model with multimodal capabilities and improved real-time reasoning.
\textbf{GPT-o1}~\cite{jaech2024openai}: A reasoning-optimized variant in the GPT series with strong step-by-step problem-solving ability.
\textbf{GPT-o3-mini}: A compact version of GPT-o3, optimized for efficiency with robust language understanding.
\textbf{DeepSeek-V3}~\cite{liu2024deepseek}: A 671B Mixture-of-Experts (MoE) model with 37B active parameters per token, designed for efficient reasoning.
\textbf{DeepSeek-R1}~\cite{guo2025deepseek}: A first-generation reasoning model with competitive performance on math, code, and reasoning tasks.
\textbf{Qwen2.5-72B-Instruct}~\cite{yang2024qwen2}: A strong multilingual model with advanced instruction tuning.
\textbf{Qwen2.5-72B-Instruct-Math}: A math-specialized variant fine-tuned for complex problem solving.
\textbf{LLaMA-series}~\cite{dubey2024llama}: Widely adopted open-source models (8B–70B) with strong instruction-tuned variants used in our experiments.
\textbf{DeepSeek-R1-Distill}~\cite{guo2025deepseek}: Distilled models from DeepSeek-R1, fine-tuned using LLaMA and Qwen backbones to balance performance and efficiency.
\textbf{Other reasoning-enhanced models}: Includes {S1-32B}\cite{muennighoff2025s1}, {Limo}~\cite{ye2025limo}, and {FinR1-7B}~\cite{finr1}, incorporating reasoning strategies such as financial CoT construction and task-specific fine-tuning.




\begin{table}[t]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt} % 缩小列间距
    \renewcommand{\arraystretch}{0.9} % 缩小行距
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{>{\raggedright}p{3.4cm} >{\centering}p{1.1cm} >{\centering}p{1.1cm} >{\centering}p{1.1cm} >{\centering}p{0.9cm} >{\centering\arraybackslash}p{3cm}}
        \hline
        \textbf{Model Name} & \textbf{Parameters} & \textbf{Enhanced for Reasoning Tasks} & \textbf{Context Window Size} & \textbf{Close/Open Source} & \textbf{Reasoning Enhanced Training Data} \\
        \hline
        GPT-4o & Unknown & No & 128k & Closed & - \\
        GPT-o1 & Unknown & Yes & 128k & Closed & Public and proprietary data (Human-annotated CoT, MCTS-assisted Synthetic data)  \\ 
        GPT-o3-mini & Unknown & Yes & 128k & Closed & Public and proprietary data \\
        DeepSeek-V3 & 671B & No & 128k & Open & -  \\
        DeepSeek-R1 & 671B & Yes & 128k & Open & Cold-start data generation, post-processing data \\
        GPT-4.5 & Unknown & No & 128k & Closed & - \\
        \hline
        Qwen2.5-72B-Instruct & 72B & No & 128k & Open & -  \\
        Qwen2.5-72B-Instruct-Math & 72B & Yes & 128k & Open & Synthetic data from Qwen, high-quality mathematical data, CoT, TIR \\
        DeepSeek-R1-Distill-Llama-70B & 70B & Yes & 128k & Open & Distilled from R1 \\
        Llama3-70B-Instruct & 70B & No & 8k & Open & - \\
        Llama3.1-70B-Instruct & 70B & No & 128k & Open & - \\
        Llama3.3-70B-Instruct & 70B & No & 128k & Open & - \\
        \hline
        Qwen2.5-32B-Instruct & 32B & No & 128k & Open & - \\
        DeepSeek-R1-Distill-Qwen-32B & 32B & Yes & 128k & Open & Distilled from R1 \\
        Limo & 32B & Yes & 128k & Open & Public and proprietary data with Qwen filteration\\
        S1-32B & 32B & Yes & 128k & Open & Public and proprietary data + original data, filtered by quality, difficulty, diversity \\
        Qwen/QwQ-32B & 32B & Yes & 128k  & Open & - \\
        \hline
        Qwen2.5-14B-Instruct & 14B & No & 128k & Open & - \\
        DeepSeek-R1-Distill-Qwen-14B & 14B & Yes & 128k & Open & Distilled from R1 \\
        DeepSeek-R1-Distill-Llama-8B & 8B & Yes & 128k & Open & Distilled from R1 \\
        Llama3-8B-Instruct & 8B & No & 8k & Open & - \\
        Llama3.1-8B-Instruct & 8B & No & 128k & Open & - \\
        Qwen2.5-7B-Instruct & 7B & No & 128k & Open & - \\
        FinR1-7B & 7B & Yes & 128k & Open & 60k CoT data for financial tasks\\
        \hline
    \end{tabular}
    }
    \caption{Summary of evaluated large language models, including their parameter sizes, reasoning capabilities, input limits, source availability, and reasoning enhancement strategies. MCTS refers to Monte Carlo Tree Search. CoT refers to chain-of-thought reasoning. TIR means tool-integrated reasoning.}
    \label{tab:LLMs}
\end{table}



\subsection{Evaluation Settings}

% Each dataset provides LLMs with a context comprising financial reports, tables, equations, and symbols, along with a question. Following prior work~\cite{zhao2024docmath}, we use a generic prompt (see appendix \ref{app:prompt} for more details) to guide model responses. Models must extract relevant information and perform mathematical reasoning based on the context. Outputs may vary due to formatting differences (e.g., percentages, rounding). Reasoning-focused models like DeepSeek-R1 and GPT-o1 typically produce step-by-step answers. Inspired by DocMath, we adopt an \textit{LLM-as-judge} evaluation: GPT-3.5-Turbo extracts final answers, and correctness is assessed via mathematical comparison.

% For large models (e.g., DeepSeek-V3, DeepSeek-R1, GPT series), evaluations are conducted via APIs from TogetherAI\footnote{\url{https://www.together.ai}} and OpenAI\footnote{\url{https://platform.openai.com}}. For models up to 72B, we use the VLLM framework on 8×A100 GPUs (80GB each), with a maximum generation length of 512 tokens.

% \subsection{Evaluation Settings}
In each dataset, LLMs are presented with a context containing financial reports, tables, equations, and symbols, along with a question. Following prior work~\cite{zhao2024docmath}, we use a generic prompt (see appendix \ref{app:prompt} for more details) to instruct models to answer based on the given context. Models must extract relevant information and perform mathematical reasoning using numbers and concepts from the context. While outputs are typically in mathematical form, variations in formats (e.g., percentages, rounding) may cause differences. Reasoning-focused models like DeepSeek-R1 and GPT-o1 often generate step-by-step reasoning. Inspired by DocMath \cite{zhao2024docmath}, we adopt an \textit{LLM-as-judge} evaluation approach: GPT-3.5-Turbo extracts final answers, followed by a mathematical comparison to assess correctness.

For large models (e.g., DeepSeek-V3, DeepSeek-R1, GPT series), we run evaluations via APIs from TogetherAI\footnote{\url{https://www.together.ai}} and OpenAI\footnote{\url{https://platform.openai.com}}, using versions GPT-o1-2024-12-17, GPT-o3-mini-2025-01-31, and GPT-4o-2024-08-06. For models up to 72B parameters, we use the VLLM framework on 8 A100 GPUs (80GB each), with a max generation length of 512 tokens.

% In each dataset, the LLMs are presented with a context consisting of financial reports, financial tables, equations, and mathematical symbols, along with a question. Insired by previous work \cite{zhao2024docmath}, a generic prompt is used to instruct the model to answer the question based on the provided context. The models are required to extract relevant information and perform mathematical computations using numbers and concepts derived from the context. This necessitates strong logical reasoning abilities within financial contexts. The results are typically presented in a mathematical format, but due to variations in numerical representation—such as percentages and different rounding strategies—the final outputs can differ. Furthermore, reasoning-focused models, such as DeepSeek-R1 and GPT-o1, tend to provide a step-by-step reasoning process before arriving at the final answer. Inspired by DocMath, we employ an LLM-as-judge approach to evaluate model performance. First, an LLM (GPT-3.5-Turbo) is used to extract the final answer from the model’s output. Then, a mathematical comparison method is applied to determine whether the result is correct.

% For large models like DeepSeek-V3 and DeepSeek-R1, or closed-source models like the GPT series, we conduct our experiments using APIs provided by togetherAI\footnote{https://www.together.ai} and OpenAI\footnote{https://platform.openai.com}. Specifically, we utilize GPT-o1-2024-12-17, GPT-o3-mini-2025-01-31, and GPT-4o-2024-08-06 for our evaluations.
% For other models with 72B parameters or fewer, we generate results using the VLLM framework on 4 A100 GPUs (80GB memory per GPU), with a maximum generation length of 512 tokens.



\subsubsection{Reasoning Data Curation}

To enhance domain-specific reasoning capabilities in LLMs, we curate a high-quality training dataset by integrating diverse financial reasoning benchmarks. Our dataset combines a wide range of reasoning tasks—covering numerical computation, table-text integration, cross-table correlation, and long-context understanding—reflecting the complexity of real-world financial documents.

Specifically, we incorporate data from several well-established benchmarks, including {FinQA}~\cite{chen2022finqadatasetnumericalreasoning}, {DocFinQA}~\cite{reddy2024docfinqa}, {TAT-QA}~\cite{zhu2021tatqaquestionansweringbenchmark}, {DocMath-Eval}~\cite{zhao2024docmathevalevaluatingmathreasoning}, and {BizBench-QA}~\cite{krumdick2024bizbench}. These datasets collectively provide rich and diverse coverage across task types and difficulty levels, supporting the development of robust financial reasoning models.
{FinQA} provides QA pairs combining textual and tabular financial data; {DocFinQA} extends this to long-context documents; {TAT-QA} focuses on complex numerical reasoning; {DocMath-Eval} (CompLong and SimpShort subsets) targets reasoning over multi-table and single-table scenarios; and {BizBench-QA} enriches the corpus with code-based financial reasoning tasks.
Please see Appendix~\ref{appendix:datasets} for detailed descriptions of each dataset.


% \subsubsection{Reasoning Data Curation}

% To enhance the domain-specific reasoning abilities of LLMs, we curate a high-quality financial training dataset by integrating diverse reasoning-enhanced data sources. Our dataset is built from well-established financial QA benchmarks, covering a range of reasoning challenges, including long-context retrieval, numerical reasoning, tabular interpretation, and multi-table financial analysis.

% We primarily leverage \textbf{FinQA}~\cite{chen2022finqadatasetnumericalreasoning}, a dataset with over 5,000 open-ended QA pairs extracted from financial reports and tables. FinQA provides a strong foundation for financial numerical reasoning by integrating both textual and tabular data, making it well-suited for training reasoning-based models.

% To incorporate long-context financial reasoning, we use \textbf{DocFinQA}~\cite{reddy2024docfinqa}, which extends FinQA by pairing questions with full-length financial filings. This increases the average context length from under 700 words to approximately 123,000 words, providing a more realistic evaluation of LLMs’ ability to handle large-scale financial documents.

% For complex numerical reasoning over structured financial data, we integrate \textbf{TAT-QA}~\cite{zhu2021tatqaquestionansweringbenchmark}, a large-scale dataset extracted from real financial reports. TAT-QA blends tabular and textual reasoning, requiring skills such as arithmetic operations, comparisons, and numerical span selection.

% To further assess mathematical reasoning within financial contexts, we include \textbf{DocMath-CompLong} and \textbf{DocMath-SimpShort} from \textbf{DocMath-Eval}~\cite{zhao2024docmathevalevaluatingmathreasoning}. DocMath-CompLong presents complex numerical reasoning challenges across quarterly and annual reports with multiple tables, while DocMath-SimpShort focuses on localized numerical reasoning within short documents containing a single table, derived from FinQA and TAT-QA.

% Additionally, we incorporate \textbf{BizBench-QA}~\cite{krumdick2024bizbench}, which provides 14,400 financial QA pairs covering diverse financial reasoning tasks, including SEC filings, financial code interpretation (CodeFinQA, CodeTAT-QA), and numerical span identification. This dataset enriches our training corpus with code-based reasoning and mathematical problem-solving from multiple perspectives.

% By integrating these diverse, high-quality financial reasoning datasets, we construct a robust training set that enables LLMs to develop advanced financial reasoning skills, spanning long-context comprehension, numerical analysis, and structured financial document understanding.


% \subsection{Fino1: LLMs for Financial Reasoning}
% To address limitations of general and reasoning ehanced LLMs on financial reasoning, we developed Fino1, a novel suite of reasoning-enhanced LLMs specifically designed for complex financial reasoning tasks, incoporating domain-specific reasoning ehanced training.

% \subsubsection{Reasoning Data Curation}

% To enhance the domain-specific reasoning abilities of LLMs, we first construct high-quality financial training data with domain-specific reasoning paths based on the FinQA dataset. 
% FinQA~\cite{chen2022finqadatasetnumericalreasoning} consists of over 5,000 open-ended question-answer pairs derived from financial reports and tables. This comprehensive dataset integrates both textual and tabular financial data, presenting a moderate level of difficulty, making it well-suited for training reasoning-based models.

% Building on FinQA~\cite{chen2022finqadatasetnumericalreasoning}, DocFinQA~\cite{reddy2024docfinqa} introduces a long-context financial QA benchmark by pairing FinQA-style questions with full-length financial filings. It increases the average context length from under 700 words to approximately 123,000 words, better simulating the scale and complexity of real-world financial documents. This enables evaluation of long-context retrieval and reasoning in LLMs.

% TAT-QA~\cite{zhu2021tatqaquestionansweringbenchmark} introduces a large-scale dataset extracted from real financial reports that blends tabular and textual data, simulating complex, real-world QA scenarios. It emphasizes numerical reasoning skills such as arithmetic operations, counting, and comparison.

% DocMath-CompLong and DocMath-SimpShort from DocMath-Eval~\cite{zhao2024docmathevalevaluatingmathreasoning} are two benchmark subsets designed to evaluate numerical reasoning in domain-specific contexts. DocMath-CompLong, constructed from quarterly and annual financial reports, presents a challenging setting that requires reasoning across long documents with multiple tables. In contrast, DocMath-SimpShort, built from TAT-QA~\cite{zhu2021tatqaquestionansweringbenchmark} and FinQA~\cite{chen2022finqadatasetnumericalreasoning}, focuses on localized numerical reasoning within short documents containing a single table.

% The Bizbench-QA~\cite{krumdick2024bizbench} dataset contains 14,400 financial question-answer pairs in the training set, covering tasks from SEC-NUM, CodeFinQA, CodeTAT-QA, and FinCode. These tasks collectively address financial mathematical reasoning from multiple perspectives, such as code generation and numerical span identification. 

% DocFinQA~\cite{reddy2024docfinqa} is a long-context financial reasoning dataset with 5,740 question-answer pairs in the training set. As an extension of FinQA, it formulates questions using full financial documents instead of selected text snippets, enabling more realistic and context-rich reasoning.


\subsection{Data Collection and Reasoning Path Construction}

To ensure a balanced dataset of both short and long context samples, we collect all data from the {FinQA} train set, {TATQA}, {DocMath-CompLong}, and {DocMath-SimpShort}. Additionally, we sample 1,000 examples each from {BizBench-QA} and {DocFinQA} to balance short and long reasoning tasks. In total, we gather {9,851} financial QA samples for reasoning path construction.
Inspired by~\cite{ye2025limo, muennighoff2025s1}, which suggest that simple QA pairs may not effectively improve reasoning capabilities, we apply a filtering strategy to retain more challenging examples. Specifically, we use {Qwen-14B-Instruct} to generate answers for each question and employ {GPT-3.5-Turbo} as a verifier. QA pairs correctly answered by Qwen-14B are removed, resulting in a more difficult training subset of {3,472} QA pairs for optimizing more advanced models.

\subsection{Reasoning Path Generation and Refinement}

For reasoning path generation, we use \textbf{GPT-4o}, chosen for its strong performance across datasets (Table~\ref{tab:llm_performance_full}) and its stability in generating high-quality outputs. Following the CoT refinement framework from {HuatuoGPT-o1}~\cite{chen2024huatuogpt}, we employ a verifier-based iterative process to refine the reasoning paths generated by GPT-4o. If a reasoning path is rejected by the verifier, refinement is guided by one of the following strategies:
(1) Backtracking: Revisiting previous steps to correct logical inconsistencies.
(2) Exploring New Paths: Generating alternative reasoning routes when the current path fails.
(3) Verification: Cross-checking intermediate steps to ensure logical soundness.
(4) Correction: Making targeted adjustments to rectify specific errors.
The CoT refinement process begins with an initial prompt to generate a reasoning path and final answer. If the verifier identifies the answer as incorrect, one of the above strategies is applied iteratively until a correct and logically consistent answer is produced.

\subsubsection{Model Training}

We develop our financial reasoning models based on curated reasoning paths, focusing on smaller-sized models for efficient real-world deployment. We select \textbf{LLaMA-3.1-8B-Instruct} and \textbf{Qwen2.5-14B-Instruct} as backbone models, given their strong performance among small and mid-sized LLMs.
We adopt a two-stage training approach. In the first stage, we perform {Supervised Fine-Tuning (SFT)}, using questions as input and reasoning paths with final answers as output. For the {8B model}, we use only {FinQA} reasoning paths to assess the impact of domain-specific fine-tuning. For the {14B model}, we use the full set of sampled and filtered data to leverage its greater capacity for diverse and complex reasoning.
In the second stage, we apply {RL} using a verifier-based sparse reward mechanism and optimize with {Proximal Policy Optimization (PPO)}, following~\cite{chen2024huatuogpt}. A reward model evaluates the correctness and coherence of the reasoning path. If errors are found, the model iteratively refines its outputs through \textit{backtracking}, \textit{alternative path exploration}, \textit{verification}, and \textit{correction}, improving logical consistency and equation-solving accuracy.
During SFT, models are trained for three epochs with a learning rate of 5e-6 and a maximum sequence length of 8192 tokens. {LoRA} is applied for efficient parameter updates. In the RL stage, training continues for three epochs with a reduced learning rate of 5e-7, guided by the reward model to refine reasoning quality via PPO.


\section{Results}
% \subsection{Overall Results Analysis}
% The resutls of all these models on three dataset are listed in Table \ref{tab:llm_performance_full}. 
% Among all models, DeepSeek-R1 achieves the highest overall performance (68.93), driven by its strong XBRL-Math results, with its distilled variants, DeepSeek-R1-Distill-Llama-70B and DeepSeek-R1-Distill-Qwen-32B, closely following. Among large-scale models, GPT-4o performs well across datasets but is outperformed by DeepSeek-R1 due to its lower XBRL-Math score.
% General-purpose 70B models, such as Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct, achieve strong results, with Llama3.3-70B showing significant improvement over its predecessors. Surprisingly, these models also surpass the reasoning-focused GPT-o1 and approach GPT-o3-mini, suggesting that general-purpose reasoning strategies do not always translate effectively to financial tasks.
% In the 8B category, Fino1-8B (61.03) outperforms other models, demonstrating robust task optimization. Notably, reasoning-enhanced models exhibit mixed results: while they improve XBRL-Math scores, they tend to perform worse on FinQA and DM-Simplong. This suggests that different reasoning strategies yield varying benefits across financial reasoning tasks, emphasizing the importance of task-specific optimization.
% \subsection{Overall Results Analysis}

% Table \ref{tab:llm_performance_full} summarizes model performance across three datasets. DeepSeek-R1 achieves the highest overall score (68.93), driven by strong XBRL-Math results, with its distilled variants performing similarly. GPT-4o performs well overall but falls short in XBRL-Math. General-purpose 70B models like Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct also perform strongly, with Llama3.3-70B showing clear improvement over earlier versions and even outperforming reasoning-focused models like GPT-o1 and approaching GPT-o3-mini. In the 8B category, Fino1-8B (61.03) leads, reflecting effective task-specific optimization. Overall, reasoning-enhanced models improve XBRL-Math scores but often underperform on FinQA and DM-Simplong, emphasizing the need for task-specific reasoning strategies in financial tasks.

%\subsection{Overall Results Analysis}

% Table~\ref{tab:llm_performance_full} presents the performance of various LLMs across four financial reasoning benchmarks. Our models, {Fino1-8B} and {Fino1-14B}, demonstrate strong results despite their smaller sizes. Notably, {Fino1-14B} achieves an average score of {60.25}, outperforming many larger models such as {LLaMA-3.3-70B-Instruct (56.04)} and {Qwen2.5-72B-Instruct (53.71)}. It also surpasses several reasoning-enhanced and closed-source models, including {GPT-o1-preview}, {GPT-4.5}, and {GPT-o3-mini}, while approaching the performance of top-performing models like {DeepSeek-R1 (60.87)} and {DeepSeek-V3 (61.30)}.


Table~\ref{tab:llm_performance_full} presents the performance of various LLMs across four financial reasoning benchmarks: FinQA, DM-Simplong, XBRL-Math, and DM-Complong. Our models, {Fino1-8B} and {Fino1-14B}, demonstrate strong and consistent results across tasks, outperforming several larger and closed-source models.
{Fino1-14B} achieves an average score of {60.25}, surpassing many larger-scale models such as {LLaMA-3.3-70B-Instruct (56.04)}, {Qwen2.5-72B-Instruct (53.71)}, {GPT-o1-preview (54.05)}, {GPT-o3-mini (57.89)}, and even {GPT-4.5 (60.43)}. It also {achieves the best performance among all models on two benchmarks—DM-Simplong (60.00) and XBRL-Math (86.67)}. These results demonstrate that high-quality, domain-specific reasoning training can enable smaller models to rival—and even surpass—the performance of significantly larger and proprietary LLMs.


% Table~\ref{tab:llm_performance_full} presents the performance of various LLMs across four financial reasoning benchmarks: FinQA, DM-Simplong, XBRL-Math, and DM-Complong. Our models, \textbf{Fino1-8B} and \textbf{Fino1-14B}, demonstrate strong and consistent results across tasks, outperforming several larger and closed-source models.
% \textbf{Fino1-14B} achieves an average score of \textbf{60.25}, surpassing many larger-scale models such as \textbf{LLaMA-3.3-70B-Instruct (56.04)}, \textbf{Qwen2.5-72B-Instruct (53.71)}, and \textbf{GPT-o1-preview (54.05)},\textbf{GPT-4.5 (60.43)} and \textbf{GPT-o3-mini (57.89)}, while approaching top-performing models like \textbf{DeepSeek-R1 (60.87)} and \textbf{DeepSeek-V3 (61.30)}.  demonstrating that high-quality reasoning training can rival the capabilities of larger and even closed-source models.

{Fino1-8B} also performs competitively with an average score of {50.77}, surpassing other 8B models and even some 14B and 32B models. Both models excel on {XBRL-Math}, where {Fino1-14B} achieves the top score of {86.67}.
Moreover, {Fino1-14B} and {Fino1-8B} significantly outperform {FinR1-7B}, a financial reasoning-enhanced model trained on a much larger 60K task-aligned dataset, demonstrating that {larger data volume does not necessarily lead to better performance}, and that \textbf{carefully curated, high-quality reasoning data—even in smaller quantities—can yield superior results}.




% DeepSeek-R1 achieves the best overall performance with an average score of 68.93, driven primarily by its strong performance on XBRL-Math. Notably, its leading performance is followd by its distilled versions, DeepSeek-R1-Distill-Llama-70B and DeepSeek-R1-Distill-Qwen-32B, indicating that distillation effectively preserves task-specific capabilities while improving computational efficiency. 
% Among large-scale and proprietary models, GPT-4o demonstrates strong overall performance across all three datasets; however, its lower performance on XBRL-Math leads to an overall score that falls short of DeepSeek-R1.

% When comparing models around the 70B scale, general-purpose models such as Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct achieve competitive results. Interestingly, Llama3.3-70B shows a substantial improvement over its predecessors, Llama3-70B and Llama3.1-70B, suggesting significant advancements in its design and training data. Additionally, these leading 70B models also surpass the reasoning-focused GPT-o1 and approach the performance of GPT-o3-mini, indicating that reasoning enhancement strategies designed for general-purpose tasks do not always translate directly to improvements in financial reasoning.

% In the 8B category, Fino1-8B (61.03) achieves the highest overall score, outperforming other 8B models such as Llama3-8B-Instruct (39.95) and DeepSeek-R1-Distill-Llama-8B (53.36). This suggests that Fino1-8B is particularly well-optimized for the evaluated tasks, demonstrating robust generalization capabilities despite its smaller scale. These findings reinforce the advantage of DeepSeek models, particularly in mathematical reasoning, while also indicating that model distillation and optimization strategies can bridge performance gaps across model sizes.

% An interesting observation emerges when comparing the performance of reasoning-enhanced models with their base versions across different tasks. Specifically, models such as GPT-4o versus GPT-o1, DeepSeek-V3 versus DeepSeek-R1, Qwen2.5-72B-Instruct versus Qwen2.5-72B-Instruct-Math, and Llama3.1-8B-Instruct versus DeepSeek-R1-Distill-Llama-8B exhibit a consistent trend: reasoning-enhanced models tend to perform worse on FinQA and DM-Simplong, while showing significant improvements on XBRL-Math. For instance, while GPT-o1 consistently lags behind in XBRL-Math, DeepSeek and Qwen-series reasoning-enhanced models gain substantial improvements in this dataset. This suggests that different reasoning enhancement strategies yield varying levels of effectiveness depending on the format and nature of financial reasoning tasks, highlighting the need for task-specific optimization rather than a one-size-fits-all approach to reasoning augmentation.





% More specifically, Qwen2.5-72B-Instruct achieves the highest score in FinQA (73.38), while GPT-4o leads in DM-Simplong (60.0). DeepSeek-R1 and its distilled variants perform best in XBRL-Math, with DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70B both scoring 86.67. Qwen2.5-72B-Instruct-Math achieves 83.33 in XBRL-Math but performs lower in DM-Simplong (42.0). Among the LLaMA models, Llama3.3-70B-Instruct attains the highest scores across all datasets, with 68.15 in FinQA, 54.0 in DM-Simplong, and 70.0 in XBRL-Math. The DeepSeek-R1-Distill-Qwen-32B model scores 65.48 in FinQA, 55.0 in DM-Simplong, and 84.44 in XBRL-Math. Smaller models, such as Llama3-8B-Instruct and DeepSeek-R1-Distill-Llama-8B, show lower scores across all datasets, with Llama3-8B-Instruct scoring 41.97 in FinQA and 29.0 in DM-Simplong, while DeepSeek-R1-Distill-Llama-8B achieves 45.96 in FinQA and 33.0 in DM-Simplong.

% \begin{table*}[h]
%     \centering
%     \small
%     \renewcommand{\arraystretch}{1.2} % Adjust row height
%     \begin{tabular}{lcccc}
%         \toprule
%         \textbf{Models} & \textbf{FinQA} & \textbf{DM-Simplong} & \textbf{XBRL-Math}&\textbf{Average} \\
%         \midrule
%         GPT-4o & 72.49 & 60.00 & 72.22 & 68.24\\
%         GPT-o1 & 49.07 & 56.00 & 74.44 & 59.84\\
%         GPT-o3-mini & 60.87 & 59.00 & 76.67 & 65.51\\  \hline
%         DeepSeek-V3 & 73.20 & 53.00 & 76.67 & 67.62\\
%         DeepSeek-R1 & 65.13 & 53.00 & 86.67 & 68.93\\
%         Qwen2.5-72B-Instruct & 73.38 & 59.00 & 67.78 & 66.72\\
%         Qwen2.5-72B-Instruct-Math & 69.74 & 42.00 & 83.33 & 65.69\\
%         DeepSeek-R1-Distill-Llama-70B & 66.73 & 53.00 & 86.67 & 68.80\\ 
%         Llama3-70B-Instruct & 58.92 & 41.00 & 56.67 & 52.20\\
%         Llama3.1-70B-Instruct & 63.18 & 48.00 & 63.33 & 58.17\\
%         Llama3.3-70B-Instruct & 68.15 & 54.00 & 70.00 & 64.05\\  \hline
%         DeepSeek-R1-Distill-Qwen-32B & 65.48 & 55.00 & 84.44 & 68.97\\
%         DeepSeek-R1-Distill-Qwen-14B & 63.27 & 44.00 & 84.44 & 63.90\\
%         DeepSeek-R1-Distill-Llama-8B & 45.96 & 33.00 & 81.11 & 53.36\\
%         Llama3-8B-Instruct & 41.97 & 29.00 & 48.89 & 39.95\\
%         Llama3.1-8B-Instruct & 54.13 & 34.00 & 62.22 & 50.12\\
%         \hline
%         Fino1-8B&60.87 & 40.00 & 82.22 & 61.03\\
%         \bottomrule
%     \end{tabular}
%     \caption{Performance of different LLMs on three tested financial datasets.}
%     \label{tab:llm_performance}
% \end{table*}

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{4pt}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Models} & \textbf{FinQA} & \textbf{DM-Simplong} & \textbf{XBRL-Math} & \textbf{DM-Complong} & \textbf{Average} \\
        \midrule
        GPT-4o & 72.49 & 60.00 & 72.22 & 39.33 & 61.01 \\
        GPT-o1-preview & 49.07 & 56.00 & 74.44 & 36.67 & 54.05 \\
        GPT-o3-mini & 60.87 & 59.00 & 76.67 & 35.00 & 57.89 \\
        DeepSeek-V3 & 73.20 & 53.00 & 76.67 & \textbf{42.33} & \textbf{61.30} \\
        DeepSeek-R1 & 65.13 & 53.00 & 86.67 & 38.67 & 60.87 \\
        GPT-4.5 & 68.94 & 59.00 & 74.44 & 39.33 & 60.43 \\
        \midrule
        Meta-Llama-3-70B-Instruct & 58.92 & 41.00 & 56.67 & 13.67 & 42.57 \\
        Llama-3.1-70B-Instruct & 63.18 & 48.00 & 63.33 & 34.33 & 52.21 \\
        Llama-3.3-70B-Instruct & 68.15 & 54.00 & 70.00 & 32.00 & 56.04 \\
        Qwen2.5-72B-Instruct & \textbf{73.38} & 59.00 & 67.78 & 14.67 & 53.71 \\
        Qwen2.5-Math-72B-Instruct & 69.74 & 42.00 & 83.33 & 5.00 & 50.02 \\
        DeepSeek-R1-Distill-Llama-70B & 66.73 & 53.00 & 86.67 & 30.67 & 59.27 \\
        \midrule
        Qwen2.5-32B-Instruct & 73.11 & 56.00 & 65.56 & 30.00 & 56.17 \\
        Qwen/QwQ-32B & 61.22 & 46.00 & 84.44 & 20.00 & 52.92 \\
        DeepSeek-R1-Distill-Qwen-32B & 65.48 & 55.00 & 84.44 & 24.67 & 57.40 \\
        Limo & 63.44 & 45.00 & 61.11 & 15.33 & 46.22 \\
        S1-32B & 66.81 & 53.00 & 84.44 & 24.00 & 57.06 \\

        \midrule
        Qwen2.5-14B-Instruct & 67.44 & 59.00 & 57.78 & 26.67 & 52.72 \\
        DeepSeek-R1-Distill-Qwen-14B & 63.27 & 44.00 & 84.44 & 21.00 & 53.18 \\
        DeepSeek-R1-Distill-Llama-8B & 45.96 & 33.00 & 81.11 & 15.67 & 43.94 \\
        Meta-Llama-3-8B-Instruct & 41.97 & 29.00 & 48.89 & 6.00 & 31.47 \\
        Llama-3.1-8B-Instruct & 54.13 & 34.00 & 62.22 & 14.30 & 41.16 \\
        Qwen2.5-7B-Instruct & 55.37 & 41.00 & 42.22 & 17.67 & 39.07 \\
        FinR1-7B & 58.74 & 37.00 & 30.00 & 13.67 & 34.85 \\
        \midrule

        Fino1-8B & 60.87 & 40.00 & 82.22 & 20.00 & 50.77 \\
        Fino1-14B & 70.01 & \textbf{60.00} & \textbf{86.67} & 24.33 & 60.25\\
        \bottomrule
    \end{tabular}
    }
    \caption{Performance of different LLMs on four financial reasoning datasets.}
    \label{tab:llm_performance_full}
\end{table}

Beyond overall performance, the comparison across models provides additional insights that can guide the development of LLMs for financial applications:

(1) \textbf{General reasoning enhancements do not necessarily improve financial task performance.}  
Models like {GPT-o1} and {DeepSeek-R1}, though strong in general reasoning, underperform on financial benchmarks, highlighting the importance of domain-specific knowledge and structured numerical reasoning. Moreover, reasoning enhancement strategies based on general-domain data, as seen in {S1-32B} and {Limo}, often fail to improve and sometimes even degrade performance compared to their backbone models (e.g., {Qwen2.5-32B}). These results underscore that {high-quality, domain-aligned reasoning paths are more effective than general reasoning augmentation}, and tailored financial training is crucial for optimal model performance.


% (1) \textbf{General reasoning enhancements do not necessarily translate to improvements in financial tasks}. While models fine-tuned for general reasoning, such as {GPT-o1} and {DeepSeek-R1}, demonstrate strong performance in mathematical and logical reasoning tasks, they do not consistently outperform other models in financial benchmarks. For example, {GPT-o1 scores 49.07 in FinQA and 56.0 in DM-Simplong}, trailing behind models like GPT-4o that are not explicitly optimized for general reasoning. Also, DeepSeek-R1 also under performan V3 in these tasks. This suggests that financial reasoning requires more than just enhanced logical and mathematical capabilities; it demands a deep understanding of {domain-specific terminology, structured financial statements, and numerical patterns}. One possible explanation for this phenomenon is that reasoning-focused fine-tuning, which emphasizes general problem-solving strategies, may inadvertently weaken a model’s retention of financial knowledge. This is particularly relevant for tasks requiring precise comprehension of financial terms, balance sheet structures, and industry-specific metrics, where models without targeted financial fine-tuning may struggle. Additionally, reasoning-enhanced models might prioritize general logical inference over domain-specific factual recall, which could lead to a decrease in performance when interpreting specialized financial contexts. These findings indicate that while general reasoning improvements enhance broad problem-solving skills, they do not necessarily translate into domain-specific advantages, highlighting the need for specialized financial training in LLMs.

% (2) \textbf{Different reasoning enhancement strategies yield varying improvements across financial tasks.} Comparing models like {GPT-4o} vs. {GPT-o1}, {Qwen2.5-72B-Instruct} vs. {Qwen2.5-72B-Instruct-Math}, and Llama variants vs. {DeepSeek-R1}, we observe that reasoning-enhanced models do not consistently improve financial performance. For example, {GPT-o1} trails {GPT-4o} in FinQA (49.07 vs. 72.49) and DM-Simplong (56.00 vs. 60.00), while {Qwen2.5-Math} excels in XBRL-Math (67.78~$\rightarrow$~83.33) but drops sharply in FinQA (69.74~$\rightarrow$~42.00) and DM-Simplong (59.00~$\rightarrow$~42.00). Similarly, {DeepSeek-R1-Distill-Llama-8B} achieves strong XBRL-Math scores (81.11) but offers limited gains elsewhere. These results suggest that reasoning strategies like chain-of-thought or math fine-tuning benefit structured tasks but fail to generalize to broader financial reasoning, highlighting the need for task-specific adaptation.

% (2) \textbf{Reasoning enhancements vary in effectiveness across financial tasks.} Models like {GPT-o1} perform worse than {GPT-4o} on FinQA (49.07 vs. 72.49) and DM-Simplong (56.00 vs. 60.00). {Qwen2.5-Math} improves in XBRL-Math (67.78~$\rightarrow$~83.33) but drops sharply in FinQA (69.74~$\rightarrow$~42.00) and DM-Simplong (59.00~$\rightarrow$~42.00). {DeepSeek-R1-Distill-Llama-8B} also excels in XBRL-Math (81.11) but shows limited gains elsewhere. These results suggest that strategies like CoT or math fine-tuning help on structured tasks but fail to generalize, highlighting the need for task-specific adaptation in financial reasoning.


(2) \textbf{Reasoning enhancements vary in effectiveness across financial tasks.} Models like {GPT-o1} perform worse than {GPT-4o} on FinQA (49.07 vs. 72.49) and DM-Simplong (56.00 vs. 60.00). {Qwen2.5-Math} improves in XBRL-Math (67.78~$\rightarrow$~83.33) but drops sharply in FinQA (69.74~$\rightarrow$~42.00) and DM-Simplong (59.00~$\rightarrow$~42.00). {DeepSeek-R1-Distill-Llama-8B} also excels in XBRL-Math (81.11) but shows limited gains elsewhere. These results suggest that strategies like CoT or math fine-tuning may offer benefits for structured tasks but often fail to generalize effectively to broader financial reasoning. Compared with Qwen2.5-72B-Instruct and its math enhanced version, we can see that while TIR and synthetic building appears to help in equation-heavy tasks like XBRL-Math, its impact on other types of financial tasks is less clear and may be influenced by multiple factors such as task structure, data modality, or domain complexity.

% (2) \textbf{Reasoning enhancements vary in effectiveness across financial tasks.} Models like {GPT-o1} perform worse than {GPT-4o} on FinQA (49.07 vs. 72.49) and DM-Simplong (56.00 vs. 60.00). {Qwen2.5-Math} improves in XBRL-Math (67.78~$\rightarrow$~83.33) but drops sharply in FinQA (69.74~$\rightarrow$~42.00) and DM-Simplong (59.00~$\rightarrow$~42.00). {DeepSeek-R1-Distill-Llama-8B} also excels in XBRL-Math (81.11) but shows limited gains elsewhere. These results suggest that strategies like CoT or math fine-tuning help on structured tasks but fail to generalize, highlighting the need for task-specific adaptation in financial reasoning. 
% While TIR contributes positively in equation-intensive tasks such as XBRL-Math, it does not consistently improve performance across broader financial tasks.


% (2) \textbf{Different reasoning enhancement strategies yield varying improvements depending on the financial reasoning task format}. 
% By comparing the performance of GPT-4o vs. GPT-o1, Qwen2.5-72B-Instruct vs. Qwen2.5-72B-Instruct-Math, and Llama models vs. the DeepSeek-R1 distilled Llama models, we observe that reasoning-enhanced models do not always improve financial task performance. GPT-o1 underperforms GPT-4o in FinQA (49.07 vs. 72.49) and DM-Simplong (56.00 vs. 60.00), suggesting that its reasoning strategies, such as chain of thought and reflection-tuning, are optimized for general problem-solving rather than financial-specific logic. Similarly, Qwen2.5-72B-Instruct-Math, despite achieving a substantial XBRL-Math boost (67.78 → 83.33), experiences sharp declines in FinQA (69.74 → 42.00) and DM-Simplong (59.00 → 42.00), indicating that mathematical reasoning enhancements help structured financial data but do not generalize well to broader financial tasks. In the 8B category, DeepSeek-R1-Distill-Llama-8B (53.36) outperforms Llama3.1-8B-Instruct (50.12) and Llama3-8B-Instruct (39.95), excelling in XBRL-Math (81.11) but showing limited improvements in other financial tasks.

% These results suggest that different reasoning strategies target distinct aspects of financial reasoning. DeepSeek-R1 and Qwen2.5-72B-Instruct-Math demonstrate strong performance in symbolic and mathematical reasoning, excelling in structured equation-based tasks. However, they do not generalize well to financial question-answering tasks like FinQA and DM-Simplong. Similarly, reasoning augmentations in GPT-o1 fail to provide consistent improvements, likely due to the absence of relevant financial reasoning data, highlighting the challenge of adapting broad reasoning techniques to financial logic.


% Despite its reasoning enhancements, GPT-o1 does not provide noticeable improvements over GPT-4o in financial tasks, with its scores in FinQA (49.07) and DM-Simplong (56.0) significantly lower than GPT-4o (72.49, 60.0). This suggests that GPT-o1's reasoning strategies, such as chain of thought and reflection-tuning, may be optimized for general problem-solving rather than financial-specific logic.
% In contrast, while DeepSeek-R1 and Qwen2.5-70B-Instruct-Math also see a decline in FinQA, they achieve a substantial boost in XBRL-Math, likely due to their effectiveness in financial reasoning abilities in more structured XBRL tests and equations through reinforcement learning optimizations like GRPO (Group Relative Policy Optimization) or by leveraging tool-integrated reasoning and symbolic computation enhancements.
% These results suggest that different reasoning strategies target distinct aspects of financial reasoning. Enhancements designed for symbolic and mathematical reasoning (such as those in DeepSeek-R1 and Qwen-Math) may offer advantages in understanding equations and well-structured text inputs, whereas general reasoning augmentations in GPT-o1 do not directly translate to improvements in financial-domain tasks. This underscores the importance of developing more specialized and well-designed reasoning fine-tuning strategies for financial data structures, as improvements in one type of reasoning may not generalize across different financial task formats.

(3) \textbf{Larger model size offers limited benefits in financial reasoning.} While larger models typically excel in general NLP tasks, their gains in financial tasks are not proportional. For example, {DeepSeek-70B} (66.73 FinQA, 53 DM-Simplong, 86.67 XBRL-Math) performs similarly to the smaller {DeepSeek-R1-Distill-Qwen-32B} (65.48, 55, 84.44), despite having less than half the parameters. Across models like {Qwen2.5-72B}, {DeepSeek-R1-Distill-Llama-70B}, and {GPT-4o}, performance tends to plateau around 70B. This may reflect the structured nature of tasks like XBRL-Math, where arithmetic precision outweighs complex language reasoning, enabling smaller models to compete effectively.

% (3) \textbf{Scaling model size does not always lead to better financial task performance.} While larger models usually excel in general NLP tasks, their gains in financial tasks are not proportional. For example, {DeepSeek-70B} (66.73 in FinQA, 53 in DM-Simplong, 86.67 in XBRL-Math) performs similarly to the smaller {DeepSeek-R1-Distill-Qwen-32B} (65.48, 55, 84.44), despite having less than half the parameters. Across models like {Qwen2.5-72B}, {DeepSeek-R1-Distill-Llama-70B}, {GPT-4o}, and others, performance tends to plateau around 70B parameters. This may be due to tasks like XBRL-Math emphasizing structured data and arithmetic precision over complex language reasoning, allowing smaller models to compete effectively. Thus, in structured financial reasoning, larger size offers limited additional benefits.

% (3) \textbf{Scaling model size does not always lead to performance gains in financial tasks}.
% While larger models typically achieve better results in standard NLP tasks, their performance in financial tasks does not always scale proportionally with size.
% For instance, DeepSeek-70B (66.73 in FinQA, 53 in DM-Simplong, and 86.67 in XBRL-Math) performs similarly to its smaller distilled variant DeepSeek-R1-Distill-Qwen-32B (65.48, 55, 84.44), despite the latter having less than half the parameters. A broader comparison, including Qwen2.5-72B-Instruct, DeepSeek-R1-Distill-Llama-70B, GPT-4o, DeepSeek-V3, and DeepSeek-R1, reveals that performance stabilizes once model size reaches approximately 70B parameters.
% One possible explanation for this trend is the nature of XBRL-Math, which consists of structured financial tables and numerical equations that prioritize precise mathematical computations over complex language-based reasoning. In such structured, numerically driven tasks, smaller models can achieve competitive performance by leveraging arithmetic operations and pattern recognition rather than advanced language modeling. This suggests that the advantage of larger models diminishes in financial decision-making tasks where structured data comprehension and numerical reasoning play a more crucial role than language-based inference.


(4) \textbf{Improved pre-training data and strategies in general domains also benefit financial tasks.} {Llama 3.1-8B-Instruct} shows a notable performance boost over {Llama 3-8B-Instruct} (avg. 50.12 vs. 39.95), with gains in XBRL-Math (62.22 vs. 48.89) and FinQA (54.13 vs. 41.97). Similarly, {Llama 3.1-70B-Instruct} outperforms {Llama 3-70B} (58.17 vs. 52.20), especially in DM-Simplong (48.00 vs. 41.00), while {Llama 3.3-70B-Instruct} achieves the highest overall performance (64.05), driven by improvements in FinQA (68.15) and XBRL-Math (70.00). These gains stem from larger, more diverse training corpora (e.g., 15T+ tokens in Llama 3.1) and enhanced post-training techniques like online preference optimization. Such strategies improve general reasoning while also boosting performance in domain-specific tasks like financial reasoning.

% (4) \textbf{Improving Pre-training Data and Strategies in General Domains Also Benefits Financial Tasks}.
% Compared with Llama 3-8B-Instruct, Llama 3.1-8B-Instruct demonstrates a significant performance boost, increasing its average score from 39.95 to 50.12, with substantial improvements in XBRL-Math (62.22 vs. 48.89) and FinQA (54.13 vs. 41.97). Similarly, among the 70B-Instruct models, Llama 3.1-70B-Instruct outperforms Llama 3-70B-Instruct (58.17 vs. 52.20), with the largest improvement in DM-Simplong (48.00 vs. 41.00), while Llama 3.3-70B-Instruct achieves the highest overall performance (64.05 vs. 58.17), excelling in FinQA (68.15 vs. 63.18) and XBRL-Math (70.00 vs. 63.33), highlighting the benefits of dataset expansion and refined training strategies. 

% More specifically, Llama 3.1 incorporates a more extensive dataset, covering over 15 trillion tokens across multiple languages and domains, leading to notable performance gains. Llama 3.3 further benefits from advancements in post-training techniques, such as online preference optimization, which enhances core performance while significantly reducing training and inference costs. These refined optimization strategies in pre-training not only improve general capabilities but also enhance performance in domain-specific tasks.

(5) \textbf{Existing reasoning models struggle with long contexts and cross-table correlations.} Financial tasks often require integrating information from lengthy reports, structured tables, and equations, yet current models show clear limitations. For example, {GPT-4o} performs well on FinQA (72.49) but drops on DM-Simplong (60.00), indicating challenges in combining textual and symbolic reasoning. Models like {DeepSeek-R1} and {DeepSeek-R1-Distill-Qwen-32B} excel in XBRL-Math but fail to generalize. Performance on {DM-Complong} remains low across models (e.g., {LLaMA-3.3-70B: 32.00}). Smaller models like {DeepSeek-R1-Distill-Llama-8B} also perform poorly with 33.00 for DM-Simplong. These results highlight the need for improved long-context modeling and structured data integration in financial reasoning.

% (5) \textbf{Existing reasoning models struggle with long financial contexts and cross-table correlations.} Many financial tasks require extracting and synthesizing information from lengthy reports, structured tables, and equations, yet current models show limitations. For instance, {GPT-4o} performs well in FinQA (72.49) but drops in DM-Simplong (60.00), indicating difficulty in combining textual and symbolic reasoning. Similarly, models like {DeepSeek-R1} and {DeepSeek-R1-Distill-Qwen-32B} score high in XBRL-Math (86.67 and 84.44) but fail to generalize to broader tasks. Reasoning-enhanced models still struggle with long-range dependencies and multi-table reasoning. Notably, smaller models like {DeepSeek-R1-Distill-Llama-8B} drop from 81.11 (XBRL-Math) to 33.0 (DM-Simplong), revealing poor performance in complex contexts. Overcoming these limitations requires better long-context modeling, structured data integration, and multi-modal document understanding.


% (5) \textbf{Existing reasoning models struggle with long financial contexts and cross-table correlations}. Many financial reasoning tasks require models to extract and synthesize information from lengthy reports, structured tables, and numerical equations. However, performance discrepancies across datasets suggest that current LLMs struggle with this challenge. For example, models that perform well in {FinQA}, which primarily involves textual financial reasoning, do not necessarily excel in {DM-Simplong}, which integrates numerical and symbolic information. {GPT-4o achieves 72.49 in FinQA but drops to 60.0 in DM-Simplong}, indicating difficulties in integrating mathematical symbols with textual reasoning. Furthermore, models that score highly in {XBRL-Math}, such as {DeepSeek-R1-Distill-Qwen-32B (84.44)} and {DeepSeek-R1 (86.67)}, highlight a growing but incomplete capability to process structured financial tables.  
% Notably, all reasoning-enhanced models fail to show improvements in handling long financial contexts and multi-table correlations. This is particularly concerning given that financial reports are inherently long and include complex interlinked tables. Though smaller models perform well in structured inputs like XBRL-Math, their performance drops significantly—by nearly 30\%—when handling long contexts. For example, {DeepSeek-R1-Distill-Llama-8B achieves 81.11 in XBRL-Math but drops to 33.0 in DM-Simplong}. This suggests that while smaller models can recognize structured patterns, they struggle with long-range dependencies and multi-step reasoning. Addressing these limitations requires improvements in long-context retention, structured data comprehension, and multi-modal document understanding to enhance financial reasoning capabilities.
(6) \textbf{Financial reasoning enhancement improves performance across all tasks.} Compared to {LLaMA 3.1-8B-Instruct} (41.16), {DeepSeek-R1-Distill-Llama-8B} shows a modest gain (43.94). In contrast, our {Fino1-8B}, trained on GPT-4o-distilled reasoning paths with backtracking strategies, achieves a substantial improvement, reaching an average score of {50.77}, and consistently outperforms baselines on FinQA (60.87 vs. 54.13), DM-Simplong (40.00 vs. 34.00), and XBRL-Math (82.22 vs. 62.22).
{Fino1-14B} further boosts performance, achieving an average score of {60.25}, outperforming other 14B models (e.g., {Qwen2.5-14B: 52.72}) and even rivaling state-of-the-art models like {DeepSeek-R1 (60.87)} and {GPT-4o (61.01)}. Notably, although {Fino1-8B} and {Fino1-14B} were trained with limited financial CoTs, both models show improvements across all tasks, highlighting the effectiveness of high-quality reasoning paths and structured training strategies such as backtracking, path exploration, and verification.

% (6) \textbf{Financial reasoning enhancement improves performance across all tasks.} Compared to {LLaMA 3.1-8B-Instruct} (41.16), {DeepSeek-R1-Distill-Llama-8B} shows a modest gain (43.94). In contrast, our {Fino1-8B}, trained on GPT-4o-distilled reasoning paths with backtracking strategies, achieves a substantial improvement, reaching {50.77}. It consistently outperforms baselines on FinQA (60.87 vs. 54.13), DM-Simplong (40.00 vs. 34.00), and XBRL-Math (82.22 vs. 62.22).
% {Fino1-14B} further boosts performance, achieving an average score of {60.25}, outperforming other 14B models (e.g., {Qwen2.5-14B: 52.72}) and even rivaling state-of-the-art models like {DeepSeek-R1 (60.87)} and {GPT-4o (61.01)}. Notably, although {Fino1-8B} and Fino1-14B was with limited financial CoTs, it improves across all tasks, highlighting the effectiveness of high-quality reasoning paths and structured training strategies like backtracking, path exploration, and verification.


% (6) \textbf{Reasoning enhancement based on financial data improves performance across all datasets.} Compared to \texttt{Llama 3.1-8B-Instruct}, \texttt{DeepSeek-R1-Distill-Llama-8B} (with reasoning data distilled from DeepSeek-R1) shows a modest gain (50.12 $\rightarrow$ 53.36). In contrast, our model \texttt{Fino1-8B}, trained with reasoning paths distilled from \texttt{GPT-4o} using a backtracking strategy, achieves a substantial boost, reaching an average score of 61.03—a 10.91-point increase. \texttt{Fino1-8B} outperforms baselines in FinQA (60.87 vs. 54.13), DM-Simplong (40.00 vs. 34.00), and XBRL-Math (82.22 vs. 62.22), demonstrating the effectiveness of reasoning-driven training.
% Notably, although training data came solely from FinQA, the model improved across all tasks, suggesting that domain-specific reasoning enhancements enhance overall comprehension of financial terminology, long-context reasoning, table understanding, and equation interpretation. These results highlight the value of combining strategies like backtracking, path exploration, verification, and correction—refining reasoning paths to improve logical consistency, computational accuracy, and robustness in financial reasoning.

% Overall, these insights demonstrate that financial reasoning presents unique challenges distinct from general NLP and mathematical reasoning. The variability in performance across different tasks underscores the necessity for more financially specialized training methodologies and architectural improvements to enhance financial comprehension, structured data processing, and multi-step numerical reasoning.
Overall, these insights highlight that financial reasoning poses unique challenges distinct from general NLP and mathematical reasoning. The performance variability across tasks underscores the need for specialized training strategies and architectural improvements to better support financial comprehension, structured data processing, and multi-step numerical reasoning.

% (6) \textbf{Reasoning enhancement basd on financial data improve performance of all datasets}.
% Compared with Llama 3.1-8B-Instruct, DeepSeek-R1-Distill-Llama-8B, which incorporates reasoning enhancement data distilled from DeepSeek-R1, shows only a modest improvement (50.12 to 53.36). However, our model, Fino1-8B, trained with reasoning paths distilled from GPT-4o using a backtracking strategy to iteratively refine the reasoning process, achieves a substantial performance boost across all datasets, with an average score of 61.03—a 10.91-point increase over Llama 3.1-8B-Instruct. Notably, Fino1-8B outperforms its counterparts in FinQA (60.87 vs. 54.13), DM-Simplong (40.00 vs. 34.00), and particularly in XBRL-Math (82.22 vs. 62.22), demonstrating the effectiveness of reasoning-driven training.

% It is important to note that our training utilized reasoning paths generated exclusively from FinQA, yet the model exhibited improvements across all datasets. This suggests that financial domain-specific reasoning enhancement data can significantly improve models' comprehensive understanding of financial terminology, long-context comprehension, table-based reasoning, and equation interpretation. Furthermore, these results highlight the effectiveness of a combination of reasoning path-building strategies, including backtracking, exploring new paths, verification, and correction, in financial tasks. By refining reasoning paths through iterative corrections, these strategies enhance logical consistency, computational accuracy, and overall robustness, demonstrating a promising approach for advancing financial reasoning capabilities in language models.




%----------

% Based on the above findings, we observe that developing a powerful reasoning model for financial tasks presents unique challenges distinct from general-domain reasoning. Our analysis suggests three key areas for improvement in financial reasoning models:  

% (1) \textbf{Enhancing models' understanding of financial knowledge and terminology}.  While reasoning-enhanced models demonstrate strong performance in general domains, they often struggle with financial tasks due to a lack of domain-specific knowledge. Future work should explore strategies that incorporate financial corpora, structured financial reports, and regulatory documents to improve models' comprehension of domain-specific terminology and contextual nuances.  

% (2) \textbf{Improving models' ability to handle multi-table reasoning and long financial contexts}. Our results highlight that even large models exhibit significant performance drops when processing long financial reports and cross-referencing multiple tables. To address this, future reasoning stratigies should be focused on inferring logics among tables or menorization mechanisms to enhance long-context understanding.  

% (3) \textbf{Deeper analysis of reasoning-enhanced strategies in models like GPT-o1 and DeepSeek-R1}.  
% Although GPT-o1 and DeepSeek-R1 are designed with advanced reasoning enhancements, they fail to show consistent improvements across financial tasks. GPT-o1 utilizes "chain-of-thought" prompting, generating intermediate reasoning steps to enhance structured problem-solving. However, this reasoning strategy does not lead to improvements in any of the financial benchmarks, as GPT-o1 underperforms compared to GPT-4o across all tasks, scoring 49.07 in FinQA and 56.0 in DM-Simplong. In contrast, DeepSeek-R1 introduces self-reflection capabilities through reinforcement learning, enabling the model to iteratively evaluate and refine its reasoning processes without supervised data. While this approach helps DeepSeek-R1 achieve strong performance in XBRL-Math (86.67), indicating improved structured numerical reasoning, it also leads to a drop in FinQA (65.13), suggesting challenges in financial text-based reasoning. These results highlight a fundamental difference between the two models — GPT-o1's structured reasoning approach does not translate to domain-specific improvements, whereas DeepSeek-R1's reinforcement learning strategy enhances structured data processing but remains limited in free form textual financial reasoning. Understanding these trade-offs is essential for designing reasoning strategies tailored to financial tasks.

% Developing robust financial reasoning models requires both domain adaptation and architectural advancements to bridge the gap between general reasoning capabilities and financial-specific challenges. Future work should focus on integrating financially specialized pretraining, multi-table comprehension techniques, and long-context optimization to build models capable of handling complex financial documents and reasoning tasks effectively.
%----------------
% \section{Related Work} 
% Development of LLMs for general purpose, 
% development of reasoning enhanced models like R1 and O1, and their performance outperformance existing general purpose LLMs in mathmatical tasks and xxx tasks.

% Development of financial LLMs and some tasks
% emphize the importance of reasoning abilities in financial tasks 
% (stock trading, methematical)


\section{Conclusion}  
This study presents a comprehensive evaluation of 24 general-purpose and reasoning-augmented LLMs on financial reasoning tasks, encompassing financial text understanding, numerical computation, and structured data interpretation. Our findings show that existing models, despite strong performance on general reasoning benchmarks, often fail to generalize to financial domains—particularly in handling long contexts, multi-table reasoning, and domain-specific terminology. To bridge this gap, we curated a high-quality, diverse financial reasoning dataset and introduced \textbf{Fino1}, a suite of domain-adapted models trained with GPT-4o-generated reasoning paths using chain-of-thought fine-tuning and reinforcement learning. Despite being trained on limited data, Fino1 models outperform several larger and advanced LLMs, including GPT-o1, GPT-o3-mini, GPT-4.5, and DeepSeek models (V3 and R1). These results highlight the effectiveness of domain-specific reasoning strategies and demonstrate the practical value of tailored model adaptation for real-world financial applications.

% In this study, we comprehensively evaluated 24 recent reasoning-augmented and general-purpose LLMs on financial tasks, assessing their performance in text understanding, numerical reasoning, and structured data processing. Our findings show that while reasoning-enhanced models perform well on general tasks, they often fail to generalize to financial reasoning due to limited domain adaptation. Models optimized for structured numerical reasoning, such as those excelling in XBRL-Math, still struggle with financial text comprehension and long-context understanding. To address these limitations, we introduced Fino1, a reasoning-enhanced LLM based on Llama-3.1-8B-Instruct, trained with reasoning paths generated by GPT-4o using only the FinQA dataset. Despite training on a single dataset, Fino1 achieved a ~10\% performance boost across three financial benchmarks, demonstrating the value of domain-specific reasoning data. Future directions include (1) enhancing financial knowledge adaptation through targeted pretraining, (2) improving multi-table reasoning and long-context processing, and (3) refining domain-specific reasoning strategies via structured data reasoning, retrieval-augmented methods, and domain-aware model design.

% In this study, we evaluated the performance of 16 recent reasoning-augmented LLMs and general-purpose LLMs on financial tasks, analyzing their capabilities in text understanding, numerical reasoning, and structured data processing. Our results indicate that while reasoning-enhanced models demonstrate strong performance in general tasks, they often fail to generalize effectively to financial reasoning due to insufficient domain adaptation. Additionally, models optimized for structured mathematical reasoning perform well in numerical tasks like XBRL-Math but struggle with financial text comprehension and long-context processing, highlighting the need for financial domain-specific adaptations.
% To address these limitations, we presented Fino1, a reasoning-enhanced LLMs based on Llama-3.1-8B-Instruct, leveraging reasoning paths curated from GPT-4o using only FinQA, a dataset with 5,000 financial reasoning samples. Our results demonstrate that, despite being trained on a single dataset, Fino1 achieved an overall performance improvement of approximately 10\% across three financial benchmarks. This significant improvement underscores the potential of financial domain-specific data in enhancing LLMs' financial reasoning capabilities.
% Future work can focus on enhancing financial knowledge adaptation, improving multi-table reasoning, and handling long-context financial documents. Additionally, refining domain-specific reasoning strategies through structured data reasoning, retrieval-augmented approaches, and domain-aware pretraining will be critical for bridging the gap between general reasoning capabilities and financial-specific challenges.

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}
%# \bibliography{custom} % No need to add .bib extension





% To address these limitations, future work will focus on scaling fine-tuning efforts to larger model sizes, expanding evaluation tasks to broader financial applications, diversifying training data with multiple financial datasets, and improving reasoning path construction through human-verified CoT annotations and model-ensemble strategies. Additionally, optimizing model training approaches, including retrieval-augmented learning and reinforcement learning with financial expert feedback, will be crucial for advancing financial reasoning models to handle complex, real-world financial decision-making tasks.



%\section*{Acknowledgements}
\newpage
\appendix
\label{sec:appendix}
\section{Related Work}
Our work builds upon three main research directions: the development of general-purpose and reasoning-enhanced language models, applications of LLMs in finance, and studies on reasoning capabilities in financial tasks.
\subsection{General-Purpose and Reasoning-Enhanced LLMs}
Recent years have witnessed significant advancements in large language models which demonstrated remarkable capabilities across various tasks, including natural language understanding, generation, and complex reasoning. Building upon these foundations, researchers have developed specialized reasoning-enhanced models such as DeepSeek-R1 and GPT-o1, which incorporate techniques like chain-of-thought prompting \cite{wei2022chain}, self-reflection \cite{zhang2023automatic}, and reinforcement learning from human feedback \cite{ouyang2022training}. These models have shown superior performance in mathematical reasoning \cite{lewkowycz2022solving}, symbolic manipulation \cite{drori2023neural}, and logical inference tasks \cite{chen2023automatic}.
\subsection{LLMs in Financial Applications}
The application of LLMs in finance has gained significant attention, with models being adapted for tasks such as financial text analysis \cite{yang2020finbert}, market sentiment prediction \cite{yang2023fingpt}, and automated trading strategies \cite{kou2024automate}. Recent work has focused on developing finance-specific models like BloombergGPT \cite{wu2023bloomberggpt}, FinGPT \cite{yang2023fingpt} and PIXIU \cite{xie2023pixiu}, OpenFinLLMs~\cite{xie2024open}, which are pretrained on financial corpora to better understand domain-specific terminology and concepts. These models have demonstrated effectiveness in tasks such as financial sentiment analysis, earnings call analysis, and regulatory compliance checking.

\subsection{Reasoning Capabilities in Financial Tasks}
Financial reasoning presents unique challenges that combine numerical computation, domain knowledge, and logical inference. Recent studies have explored LLMs' capabilities in financial reasoning tasks, including numerical reasoning over financial documents \cite{chen2021finqa}, mathematical problem-solving in financial contexts \cite{srivastava2024evaluating}, and structured data interpretation \cite{liu2025findabench}. These works highlight the importance of both domain expertise and reasoning abilities in financial applications. Our work extends these research directions by systematically evaluating how reasoning enhancements in state-of-the-art LLMs translate to financial domain tasks, providing insights into the effectiveness of different reasoning strategies and identifying areas for improvement in financial reasoning capabilities.


\section{Details Data Source}
\label{appendix:datasets}

We provide here detailed descriptions of the datasets used to construct our training and evaluation corpus for financial reasoning enhancement.

\begin{itemize}
    \item \textbf{FinQA}~\cite{chen2022finqadatasetnumericalreasoning}: FinQA consists of over 5,000 open-ended question-answer pairs derived from financial reports and tables. It integrates both textual and tabular data, covering numerical reasoning tasks such as arithmetic operations, comparison, and multi-step calculations, making it well-suited for training reasoning-enhanced models.

    \item \textbf{DocFinQA}~\cite{reddy2024docfinqa}: An extension of FinQA designed for long-context reasoning. It pairs FinQA-style questions with full-length financial filings, increasing the average context length from under 700 words to approximately 123,000 words. This dataset better simulates real-world document scale and complexity, enabling robust evaluation of retrieval and reasoning over long financial documents.

    \item \textbf{TAT-QA}~\cite{zhu2021tatqaquestionansweringbenchmark}: A large-scale financial QA dataset created from real financial reports. It combines both textual and tabular data and emphasizes diverse numerical reasoning tasks, including arithmetic operations, counting, sorting, and comparison. TAT-QA reflects real-world challenges in financial document interpretation.

    \item \textbf{DocMath-Eval}~\cite{zhao2024docmathevalevaluatingmathreasoning}: This benchmark suite evaluates mathematical reasoning in domain-specific contexts. We utilize two subsets:
    \begin{itemize}
        \item \textbf{DocMath-CompLong}: Constructed from quarterly and annual reports, it involves reasoning across long documents and multiple tables, presenting a challenging multi-step numerical reasoning setting.
        \item \textbf{DocMath-SimpShort}: Built from FinQA and TAT-QA, this subset focuses on localized numerical reasoning tasks over short documents containing a single table.
    \end{itemize}

    \item \textbf{BizBench-QA}~\cite{krumdick2024bizbench}: Contains 14,400 financial QA pairs across multiple task types, including SEC-NUM, CodeFinQA, CodeTAT-QA, and FinCode. These tasks span code generation, numerical span identification, and financial mathematical reasoning from various perspectives, enriching the training corpus with broader financial task coverage.
\end{itemize}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{case1.png}
    \caption{Error case 1.}
    \label{fig:case1}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{case2.png}
    \caption{Error case 2.}
    \label{fig:case2}
\end{figure*}
\section{Error Analysis}
To further investigate why reasoning-enhanced models like DeepSeek-R1 and GPT-o1 underperform in financial tasks compared to general-purpose models such as GPT-4o and DeepSeek-V3, we conduct an error analysis using the FinQA dataset since these reasoning-enhanced models exhibit a significant performance drop on FinQA.

We focus on cases where DeepSeek-R1 produces incorrect answers while DeepSeek-V3 generates correct ones. Two common error patterns emerge. First, DeepSeek-R1 tends to over-reason and does not strictly adhere to instructions, as illustrated in Figure \ref{fig:case1}. In this case, the question requires a sum, but the model provides excessive details by breaking down the calculation instead of directly summing the values as instructed.

Second, compared to DeepSeek-V3, DeepSeek-R1 appears to lack financial sensitivity. For instance, in Figure \ref{fig:case2}, it fails to recognize critical financial nuances, leading to an incorrect response. Specifically, DeepSeek-R1 misinterprets the concept of "average share price" in the UK Employee Share Purchase Plan by incorrectly identifying it as the employee purchase price (\$4,078 per share) rather than the actual compensation expense per share (\$719 per share). This mistake stems from an inability to correctly differentiate between compensation expense per share and the full fair market value (FMV) calculation, resulting in an overestimated figure.
In contrast, DeepSeek-V3 correctly understands that the compensation expense per share represents the direct financial impact per issued share, allowing it to provide the accurate answer. This highlights a fundamental gap in DeepSeek-R1’s financial reasoning capabilities, particularly in recognizing the accounting conventions used to report share-based compensation.





\section*{Limitations}
Our study has several limitations that highlight areas for future improvement. First, our model scale is limited, as we only fine-tuned an 8B model (Fino1), while larger models (e.g., 70B) could potentially benefit more from reasoning enhancements. Second, our evaluation scope is restricted, covering only three financial reasoning tasks (FinQA, DM-Simplong, and XBRL-Math), which do not fully capture the breadth of financial NLP applications such as forecasting, financial sentiment analysis, and fraud detection. Third, our fine-tuning relies on a single dataset, FinQA, for reasoning path construction, limiting the model’s exposure to different financial reasoning patterns; incorporating additional datasets could improve generalization. Fourth, our reasoning path construction approach is simplified, as we generate paths using a single method (GPT-4o), whereas exploring multiple reasoning path generation strategies—such as ensemble approaches or human-annotated paths—could lead to more robust financial reasoning capabilities. 

\subsection{Future Direction}
Developing effective reasoning models for financial tasks poses distinct challenges compared to general-domain reasoning. Our findings suggest three key directions for improvement: (1) Enhancing financial knowledge and terminology understanding through domain-specific pretraining using financial corpora, structured reports, and regulatory documents; (2) Improving multi-table reasoning and long-context comprehension by developing strategies that better infer logic across tables and optimize memory mechanisms for processing lengthy financial documents; and (3) Refining reasoning-enhanced strategies, as current approaches like GPT-o1’s chain-of-thought prompting and DeepSeek-R1’s self-reflection via reinforcement learning yield mixed results—highlighting the need for more tailored reasoning strategies to address both structured numerical and unstructured textual financial reasoning.

\section*{Ethics Statement}

While our study demonstrates significant advancements in financial reasoning with LLMs, Fino1 remains a research prototype and is not yet suitable for real-world financial applications. Our model inherits the well-documented limitations of large language models, including hallucinations, sensitivity to input phrasing, and potential biases in financial data sources, which could affect reliability in critical financial contexts. Furthermore, Fino1 has not undergone rigorous testing for high-stakes financial decision-making, such as investment strategies, regulatory compliance, or risk assessment, where incorrect outputs could lead to severe consequences. Additionally, the model’s reliance on a single dataset (FinQA) and a single reasoning path construction method (GPT-4o) limits its robustness and adaptability to the complex and evolving nature of financial environments. As with all AI-driven financial models, careful human oversight remains essential, and future work must focus on enhancing factual consistency, mitigating biases, improving domain adaptation, and ensuring alignment with regulatory frameworks before such models can be considered for real-world financial deployment.

\clearpage
\onecolumn
\section{Prompt}
\label{app:prompt}
\begin{tcolorbox}[colback=lightgray!10, colframe=black, title=Prompt Template for DocMath-CompLong task]
\fontsize{8pt}{9pt}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily, frame=none]
You are a financial expert, you are supposed to answer the given question based on the provided financial document context. You need to first think through the problem step by step, documenting each necessary step. Then you are required to conclude your response with the final answer in your last sentence as 'Therefore, the answer is {final answer}'. The final answer should be a numeric value.
###Context
<Full document here>

### Input
<the question based on the document>

Let's think step by step to answer the given question.

### Output
\end{lstlisting}
\end{tcolorbox}


\begin{tcolorbox}[colback=lightgray!10, colframe=black, title=Prompt Template for  DocMath-Simplong task]
\fontsize{8pt}{9pt}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily, frame=none]
You are a financial expert, you are supposed to answer the given question based on the provided financial document context. You need to first think through the problem step by step, documenting each necessary step. Then you are required to conclude your response with the final answer in your last sentence as 'Therefore, the answer is {final answer}'. The final answer should be a numeric value.

###Context
<simple long context here>

### Input
<the question based on the context>

Let's think step by step to answer the given question.

### Output
\end{lstlisting}
\end{tcolorbox}


\begin{tcolorbox}[colback=lightgray!10, colframe=black, title=Prompt Template for FinQA task]
\fontsize{8pt}{9pt}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily, frame=none]
Please answer the given financial question based on the context.
Context: <context>
Question: <the question based on the context>
Answer:
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!10, colframe=black, title=Prompt Template for XBRL-Math task]
\fontsize{8pt}{9pt}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily, frame=none]
You are a financial expert tasked with carefully reading, analyzing, and answering the following eXtensible Business Reporting Language. Please follow the steps below:

INPUT: Read the eXtensible Business Reporting Language (XBRL) question: <XBRL related question>, formula: <the formula related to this question>, and the explanation: <term explanation>. Provide only the final answer which is the numerical result of the calculation. For formulas like ROI, provide percentages. Never use the percent symbol in percentages.

OUTPUT:
\end{lstlisting}
\end{tcolorbox}

% Provide precise answers to detailed questions about financial data extraction and application using XBRL (eXtensible Business Reporting Language) filings, a standardized digital format for sharing and analyzing financial information. This task covers five areas: defining XBRL terms, domain-specific queries, financial math, numeric queries, and providing the correct US GAAP XBRL tags (e.g., 'US GAAP XBRL tag for revenue' should be answered as 'us-gaap:RevenueFromContractWithCustomerExcludingAssessedTax'). Ensure responses strictly match the correct answer without additional explanation. When answering questions about XBRL, it's essential to follow a structured approach.
%\label{sec:appendix}

%This is a section in the appendix.

\end{document}
