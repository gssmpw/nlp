\section{Related Works}
Continual Learning (CL), or lifelong learning, enables models to incrementally learn new tasks while preserving prior knowledge. Class-incremental learning~\cite{masana2022class} refers to scenarios, where tasks correspond to subsets of a multi-class classification problem, with the final model integrating knowledge from all tasks.  

Regularization-based methods mitigate forgetting by constraining updates to critical parameters. Elastic Weight Consolidation (EWC)~\cite{Kirkpatrick2017overcoming} penalizes changes to important weights, while Synaptic Intelligence (SI)~\cite{Zenke2017continual} accumulates importance measures to protect significant parameters. Learning Without Forgetting (LWF)~\cite{lwf} employs knowledge distillation to maintain prior knowledge, and Learning Without Memorizing (LWM) enhances this approach by incorporating attention maps.  

Replay-based methods retain past data for training on new tasks. iCaRL~\cite{Rebuffi2017iCaRL} combines exemplar memory with nearest-mean classification, enabling recognition of both old and new classes. Synthetic data generation~\cite{sarmin2024syntheticdatarevisitingprivacyutility} can enhance privacy in replay but remains vulnerable to targeted attacks and requires sufficient training samples~\cite{lu2023machine}.  

Recently, NICE~\cite{Gurbuz2024nice} emerged as a competitive replay-free class-incremental method. It preserves essential neurons by reducing their flexibility and distinguishes tasks using activation patterns. However, its reliance on heavily post-processed activation samples limits interpretability and may discard valuable information.  

Dynamic architecture methods address saturation by expanding network capacity. Progressive Neural Networks~\cite{Rusu2016progressive} add new columns per task, linking them to previous ones for knowledge transfer. Meta-learning approaches, such as meta-experience replay~\cite{Riemer2019learning}, optimize knowledge transfer while reducing interference.  

Explainability-driven methods have also tackled CL. LRP-based approaches~\cite{explain_not_forget} freeze neurons for task-incremental learning but do not address class-incremental challenges. ICICLE~\cite{rymarczyk2023icicle} mitigates interpretability drift, but requires network pretraining and a dedicated architecture, limiting applicability. PiECL~\cite{graphexp} focuses on temporal graph learning, making it less generalizable to broader CL settings.