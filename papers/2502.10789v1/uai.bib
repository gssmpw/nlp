@article{French1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in Cognitive Sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@article{pham2021dualnet,
  title={Dualnet: Continual learning, fast and slow},
  author={Pham, Quang and Liu, Chenghao and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16131--16144},
  year={2021}
}

@INPROCEEDINGS{emnist,

  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},

  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 

  title={EMNIST: Extending MNIST to handwritten letters}, 

  year={2017},

  volume={},

  number={},

  pages={2921-2926},

  keywords={NIST;Databases;Training;Benchmark testing},

  doi={10.1109/IJCNN.2017.7966217}}

@online{fashionmnist,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}
@article{burgt2020explainable,
  title={Explainable AI in banking},
  author={Burgt, Joost van der},
  journal={Journal of Digital Banking},
  volume={4},
  number={4},
  pages={344--350},
  year={2020},
  publisher={Henry Stewart Publications}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Baolin and Ng, Andrew Y and others},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  number={2},
  pages={4},
  year={2011},
  organization={Granada}
}



@article{tinyimagenet,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Yann and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}
@article{facil,
  title={Class-Incremental Learning: Survey and Performance Evaluation on Image Classification},
  author={Masana, Marc and Liu, Xialei and Twardowski, Bartłomiej and Menta, Mikel and Bagdanov, Andrew D. and van de Weijer, Joost},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi={10.1109/TPAMI.2022.3213473},
  year={2023},
  volume={45},
  number={5},
  pages={5513-5533}}
}

@inproceedings{cifar10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

@inproceedings{10.1145/3664647.3680719,
author = {Hao, Xuze and Ni, Wenqian and Jiang, Xuhao and Tan, Weimin and Yan, Bo},
title = {Addressing Imbalance for Class Incremental Learning in Medical Image Classification},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680719},
doi = {10.1145/3664647.3680719},
abstract = {Deep convolutional neural networks have made significant breakthroughs in medical image classification, under the assumption that training samples from all classes are simultaneously available. However, in real-world medical scenarios, there's a common need to continuously learn about new diseases, leading to the emerging field of class incremental learning (CIL) in the medical domain. Typically, CIL suffers from catastrophic forgetting when trained on new classes. This phenomenon is mainly caused by the imbalance between old and new classes, and it becomes even more challenging with imbalanced medical datasets. In this work, we introduce two simple yet effective plug-in methods to mitigate the adverse effects of the imbalance. First, we propose a CIL-balanced classification loss to mitigate the classifier bias toward majority classes via logit adjustment. Second, we propose a distribution margin loss that not only alleviates the inter-class overlap in embedding space but also enforces the intra-class compactness. We evaluate the effectiveness of our method with extensive experiments on three benchmark datasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our approach outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {2467–2476},
numpages = {10},
keywords = {class imbalance, class incremental learning, medical image classification},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

  



@article{vellido2020importance,
  title={The importance of interpretability and visualization in machine learning for applications in medicine and health care},
  author={Vellido, Alfredo},
  journal={Neural computing and applications},
  volume={32},
  number={24},
  pages={18069--18083},
  year={2020},
  publisher={Springer}
}

@ARTICLE{10444954,
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
  year={2024},
  volume={46},
  number={8},
  pages={5362-5383},
  keywords={Task analysis;Training;Surveys;Testing;Complexity theory;Stability analysis;Visualization;Continual learning;incremental learning;lifelong learning;catastrophic forgetting},
  doi={10.1109/TPAMI.2024.3367329}}


@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
} 

@article{Litjens2017survey,
  title={A survey on deep learning in medical image analysis},
  author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen AWMM and van Ginneken, Bram and S{\'a}nchez, Clara I},
  journal={Medical Image Analysis},
  volume={42},
  pages={60--88},
  year={2017},
  publisher={Elsevier}
}

@article{Samek2017explainable,
  title={Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models},
  author={Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert},
  journal={arXiv preprint arXiv:1708.08296},
  year={2017}
}

@article{Bach2015lrp,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PLOS ONE},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science}
}

@article{Holzinger2017need,
  title={We need to open the black box of machine learning: Technical solutions for the problems of explainability and interpretability},
  author={Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S and Kell, Douglas B},
  journal={arXiv preprint arXiv:1712.09923},
  year={2017}
}

@inproceedings{Wang2017chestxray,
  title={ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
  author={Wang, Xiaosong and Peng, Yong and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2097--2106},
  year={2017}
}

@inproceedings{Codella2018skin,
  title={Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)},
  author={Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M Emre and Sch{\"o}ffmann, Klaus and Ciompi, Francesco and Gutman, David and Helba, Brian and Kittler, Harald and Halpern, Allan},
  booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
  pages={168--172},
  year={2018},
  organization={IEEE}
}



@article{Parisi2019continual,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural Networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
}

@article{Kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Katharina and Quan, John and Ramalho, Tiago and Grabska-Barwi{\'n}ska, Agnieszka and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@inproceedings{derakhshani2022lifelonger,
  title={Lifelonger: A benchmark for continual disease classification},
  author={Derakhshani, Mohammad Mahdi and Najdenkoska, Ivona and van Sonsbeek, Tom and Zhen, Xiantong and Mahapatra, Dwarikanath and Worring, Marcel and Snoek, Cees GM},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={314--324},
  year={2022},
  organization={Springer}
}

@article{lu2023machine,
  title={Machine learning for synthetic data generation: a review},
  author={Lu, Yingzhou and Shen, Minjie and Wang, Huazheng and Wang, Xiao and van Rechem, Capucine and Fu, Tianfan and Wei, Wenqi},
  journal={arXiv preprint arXiv:2302.04062},
  year={2023}
}


@misc{sarmin2024syntheticdatarevisitingprivacyutility,
      title={Synthetic Data: Revisiting the Privacy-Utility Trade-off}, 
      author={Fatima Jahan Sarmin and Atiquer Rahman Sarkar and Yang Wang and Noman Mohammed},
      year={2024},
      eprint={2407.07926},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2407.07926}, 
}

@inproceedings{rymarczyk2023icicle,
  title={Icicle: Interpretable class incremental continual learning},
  author={Rymarczyk, Dawid and van de Weijer, Joost and Zieli{\'n}ski, Bartosz and Twardowski, Bartlomiej},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1887--1898},
  year={2023}
}

@inproceedings{NEURIPS2021_01894d6f,
 author = {Sun, Yiyou and Guo, Chuan and Li, Yixuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {144--157},
 publisher = {Curran Associates, Inc.},
 title = {ReAct: Out-of-distribution Detection With Rectified Activations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{10.1145/3630050.3630178,
author = {Nascita, Alfredo and Cerasuolo, Francesco and Aceto, Giuseppe and Ciuonzo, Domenico and Persico, Valerio and Pescap\'{e}, Antonio},
title = {Explainable Mobile Traffic Classification: the Case of Incremental Learning},
year = {2023},
isbn = {9798400704499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630050.3630178},
doi = {10.1145/3630050.3630178},
abstract = {The surge in mobile network usage has contributed to the adoption of Deep Learning (DL) techniques for Traffic Classification (TC) to ensure efficient network management. However, DL-based classifiers still face challenges due to the frequent release of new apps (making them outdated) and the lack of interpretability (limiting their adoption). In this regard, Class Incremental Learning and eXplainable Artificial Intelligence have emerged as fundamental methodological tools. This work aims at reducing the gap between the DL models' performance and their interpretability in the TC domain. In this study, we examine from different perspectives the differences between classifiers when trained from scratch and incrementally. Using Deep SHAP, we derive global explanations to emphasize disparities in input importance. We comprehensively analyze base classifiers' behavior to understand the starting point of the incremental process and examine updated models to uncover architectures' features resulting from the incremental training.The analysis is based on MIRAGE19, an open dataset focused on mobile app traffic.},
booktitle = {Proceedings of the 2023 on Explainable and Safety Bounded, Fidelitous, Machine Learning for Networking},
pages = {25–31},
numpages = {7},
keywords = {class incremental learning, continual learning, eXplainable artificial intelligence, traffic classification},
location = {Paris, France},
series = {SAFE '23}
}

  



@article{medmnistv2,
    title={MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification},
    author={Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
    journal={Scientific Data},
    volume={10},
    number={1},
    pages={41},
    year={2023},
    publisher={Nature Publishing Group UK London}
}

@inproceedings{Zenke2017continual,
  title={Continual learning through synaptic intelligence},
  author={Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  pages={3987--3995},
  year={2017},
  organization={PMLR}
}

@article{Simonyan2014VeryDC,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556},
  url={https://api.semanticscholar.org/CorpusID:14124313}
}

@inproceedings{Rebuffi2017iCaRL,
  title={iCaRL: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}

@inproceedings{Gurbuz2024nice,
  title={{NICE}: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning},
  author={Gurbuz, Omer Efe and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}

@inproceedings{Rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  booktitle={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@inproceedings{vgg,
  title={Video object forgery detection algorithm based on VGG-11 convolutional neural network},
  author={Gan, Yanfen and Yang, Jixiang and Lai, Wenda},
  booktitle={2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)},
  pages={575--580},
  year={2019},
  organization={IEEE}
}


@misc{resnet,
      title={Demystifying ResNet}, 
      author={Sihan Li and Jiantao Jiao and Yanjun Han and Tsachy Weissman},
      year={2017},
      eprint={1611.01186},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1611.01186}, 
}

@ARTICLE{lenet,

  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},

  journal={Proceedings of the IEEE}, 

  title={Gradient-based learning applied to document recognition}, 

  year={1998},

  volume={86},

  number={11},

  pages={2278-2324},

  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},

  doi={10.1109/5.726791}}


@inproceedings{Riemer2019learning,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{read2021classifier,
  title={Classifier chains: A review and perspectives},
  author={Read, Jesse and Pfahringer, Bernhard and Holmes, Geoffrey and Frank, Eibe},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={683--718},
  year={2021}
}
@ARTICLE{graphexp,

  author={Peng, Ciyuan and Tang, Tao and Yin, Qiuyang and Bai, Xiaomei and Lim, Suryani and Aggarwal, Charu C.},

  journal={IEEE Transactions on Neural Networks and Learning Systems}, 

  title={Physics-Informed Explainable Continual Learning on Graphs}, 

  year={2024},

  volume={35},

  number={9},

  pages={11761-11772},

  keywords={Data models;Task analysis;Learning (artificial intelligence);Adaptation models;Predictive models;Physics;Learning systems;Continual learning (CL);explainable artificial intelligence (XAI);physics-informed learning;temporal graphs},

  doi={10.1109/TNNLS.2023.3347453}}


@InProceedings{explain_not_forget,
author="Ede, Sami
and Baghdadlian, Serop
and Weber, Leander
and Nguyen, An
and Zanca, Dario
and Samek, Wojciech
and Lapuschkin, Sebastian",
editor="Holzinger, Andreas
and Kieseberg, Peter
and Tjoa, A. Min
and Weippl, Edgar",
title="Explain to Not Forget: Defending Against Catastrophic Forgetting with XAI",
booktitle="Machine Learning and Knowledge Extraction",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="1--18",
abstract="The ability to continuously process and retain new information like we do naturally as humans is a feat that is highly sought after when training neural networks. Unfortunately, the traditional optimization algorithms often require large amounts of data available during training time and updates w.r.t. new data are difficult after the training process has been completed. In fact, when new data or tasks arise, previous progress may be lost as neural networks are prone to catastrophic forgetting. Catastrophic forgetting describes the phenomenon when a neural network completely forgets previous knowledge when given new information. We propose a novel training algorithm called Relevance-based Neural Freezing in which we leverage Layer-wise Relevance Propagation in order to retain the information a neural network has already learned in previous tasks when training on new data. The method is evaluated on a range of benchmark datasets as well as more complex data. Our method not only successfully retains the knowledge of old tasks within the neural networks but does so more resource-efficiently than other state-of-the-art solutions.",
isbn="978-3-031-14463-9"
}

@inproceedings{parmar2019review,
  title={A review on random forest: An ensemble classifier},
  author={Parmar, Aakash and Katariya, Rakesh and Patel, Vatsal},
  booktitle={International conference on intelligent data communication technologies and internet of things (ICICI) 2018},
  pages={758--763},
  year={2019},
  organization={Springer}
}

@inproceedings{lwm,
  title={Learning without memorizing},
  author={Dhar, Prithviraj and Singh, Rajat Vikram and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5138--5146},
  year={2019}
}


@article{masana2022class,
  title={Class-incremental learning: survey and performance evaluation on image classification},
  author={Masana, Marc and Liu, Xialei and Twardowski, Bart{\l}omiej and Menta, Mikel and Bagdanov, Andrew D and Van De Weijer, Joost},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={5},
  pages={5513--5533},
  year={2022},
  publisher={IEEE}
}

@inproceedings{exqcx,
  title={Ecq x: explainability-driven quantization for low-bit and sparse DNNs},
  author={Becking, Daniel and Dreyer, Maximilian and Samek, Wojciech and M{\"u}ller, Karsten and Lapuschkin, Sebastian},
  booktitle={International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers},
  pages={271--296},
  year={2020},
  organization={Springer}
}

@article{lwf,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@article{ewc,
  title={Elastic weight consolidation (EWC): Nuts and bolts},
  author={Aich, Abhishek},
  journal={arXiv preprint arXiv:2105.04093},
  year={2021}
}

@misc{hatefi2024pruningexplainingrevisitedoptimizing,
      title={Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers}, 
      author={Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin},
      year={2024},
      eprint={2408.12568},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.12568}, 
}

