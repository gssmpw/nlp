\section{Related Works}
Continual Learning (CL), or lifelong learning, enables models to incrementally learn new tasks while preserving prior knowledge. Class-incremental learning____ refers to scenarios, where tasks correspond to subsets of a multi-class classification problem, with the final model integrating knowledge from all tasks.  

Regularization-based methods mitigate forgetting by constraining updates to critical parameters. Elastic Weight Consolidation (EWC)____ penalizes changes to important weights, while Synaptic Intelligence (SI)____ accumulates importance measures to protect significant parameters. Learning Without Forgetting (LWF)____ employs knowledge distillation to maintain prior knowledge, and Learning Without Memorizing (LWM) enhances this approach by incorporating attention maps.  

Replay-based methods retain past data for training on new tasks. iCaRL____ combines exemplar memory with nearest-mean classification, enabling recognition of both old and new classes. Synthetic data generation____ can enhance privacy in replay but remains vulnerable to targeted attacks and requires sufficient training samples____.  

Recently, NICE____ emerged as a competitive replay-free class-incremental method. It preserves essential neurons by reducing their flexibility and distinguishes tasks using activation patterns. However, its reliance on heavily post-processed activation samples limits interpretability and may discard valuable information.  

Dynamic architecture methods address saturation by expanding network capacity. Progressive Neural Networks____ add new columns per task, linking them to previous ones for knowledge transfer. Meta-learning approaches, such as meta-experience replay____, optimize knowledge transfer while reducing interference.  

Explainability-driven methods have also tackled CL. LRP-based approaches____ freeze neurons for task-incremental learning but do not address class-incremental challenges. ICICLE____ mitigates interpretability drift, but requires network pretraining and a dedicated architecture, limiting applicability. PiECL____ focuses on temporal graph learning, making it less generalizable to broader CL settings.