

\section{Understanding the Superficial Knowledge in Alignment}



% \comm{First, outline the three questions and why they should be discussed or connected to the problem.}
% \comm{Use a short paragraph to summarize what you are going to discuss in this sec.}

\subsection{Notation}

% In this paper, we utilize the \textit{base model}, denoted by $f(x, \Theta_b,W_b)$, to represent the unaligned model. Here, $ \Theta_b$ symbolizes the backbone parameters and  $W_b$ signifies the base model's final linear projection matrix. Conversely, the \textit{aligned model}, denoted by $f(x, \Theta_a, W_a)$,  is the model that has been fine-tuned based on human preferences. We consistently use the subscript $a$ denoting the aligned model, and $b$ denoting the base model.

In this paper, we denote the backbone (transformer layers) of the aligned model as $f_a(\cdot)$ and its final linear projection matrix as $W_a$. Conversely, $f_b(\cdot)$ and $W_b$ represent the backbone and final linear layer of the unaligned base model. Throughout the paper, we consistently use the subscript $a$ to refer to the aligned model and $b$ for the base model.

\textbf{Alignment token distribution shifts:} Given the same input, the top next token predicted by the base model is referred to as the \textbf{source token}, while the token predicted by the aligned model is termed the \textbf{target token}. A token at any position where the base model and aligned model make different predictions is called a \textbf{shift token}.
% We define $S(x)$ as the token distribution shift for input $x$, which measures the difference in logits between the aligned and base models. Mathematically, it is expressed as: 
% \begin{align*} 
% S(x) = f(x, \Theta_a, W_a) - f(x, \Theta_b, W_b) 
% \end{align*} 





\subsection{Extracting Superficial Knowledge}
\label{sec:linear}

% \comm{Alternative logic: First, revisit the non-superficial knowledge. We think knowledge that has to be learned by modifying the backbone is non-superficial. Thus, we aims to extract superficial ones without changing the model backbone.}

% \comm{Then, we immediately tell the reader what is "extracting superficial knowledge into superficial structure". First high-level idea. Then based on the idea, tell the detailed method one by one.}

To better understand the knowledge introduced through alignment, we aim to extract and isolate what we term \textit{superficial knowledge}. This refers to knowledge that contributes to simple token restyling without influencing the intermediate transformer layers' understanding of token relationships. 
% The first step is to extract and isolate this superficial knowledge from the aligned model. 

We represent the input at time step $t$ as $x_t$, which includes both the instruction and the output from previous steps. The LLM encodes these into a vector $h_{t}=f(x_t)$, produced by the final transformer layer. These hidden states, $h_t$, encapsulate complex interactions across tokens, representing the model’s understanding and reasoning over the entire context.
% which are then used for next-token prediction. 
The model then predicts the next token probability using a linear projection head $W$, as shown:

\begin{align} 
l^t = W h_t = W f(x_t)
\end{align}

Our approach adjusts the base model's final linear layer $W_b$ by adding a learnable residual adjustment, $\Delta W_b$, that approximate and mimics the aligned model's token shift and restyling process. By keeping the LLM's transformer layer $f_b(\cdot)$ fixed, this method preserves the deeper knowledge unchanged within the model.
% while allowing $\Delta W_b$ to approximate the aligned model's behavior.
% Consequently, the hidden states $h_{b,t}$ remain unchanged under the same input.
% To preserve the deeper knowledge in the model, we keep the LLM’s transformer layer $f_b(\cdot)$ fixed then the hidden states $h_{b,t}$ will be also unchanged given the same input. We aim to extract superficial knowledge by adjusting only the linear weight $W_b$. This is achieved through a shallow, linear residual adjustment $\Delta W_b$ that approximates and mimics the alignment token shift and restyling process.
Since we aim to extract knowledge from the aligned model without introducing new information, we avoid standard fine-tuning techniques for learning $\Delta W_b$. Fine-tuning on external data could introduce new knowledge not originally present in the aligned model. Instead, we apply distillation to fine-tune the linear projection heads, using the aligned model's output as a supervisory signal. Specifically, we provide the same input, $x_t$, to both the base model with a learnable residual $\Delta W_b$ and the aligned model, obtaining their respective logits $\widehat{l^t_b} = (W_b + \Delta W_b)f_b(x_t)$ and $l^t_a = W_a f_a(x_t)$. We then minimize the divergence between the two logits:
\begin{align} 
\mathcal{L}_t = KL(P^a_t || P^b_t) = P^a_t \log \frac{P^a_t}{P^b_t} \end{align}
% \begin{align*} 
% P_t^a = \text{SoftMax}(L^t_a)
% \end{align*}
% \begin{align*} 
% P_t^b = \text{SoftMax}(\widehat{^t_b})
% \end{align*}
where $P_t^a = \text{SoftMax}(l^t_a)$ and $P_t^b = \text{SoftMax}(\widehat{l^t_b})$.  The optimization objective is to minimize the sum of these losses across all tokens, yielding the optimal $\widehat{\Delta W_b}$:

\begin{align} 
\widehat{\Delta W_b} = \mathop{\arg \min}_{\Delta W_b} \sum_{t} \mathcal{L}_t 
\end{align}

The resulting $\widehat{\Delta W_b}$ serves as an approximation of the superficial knowledge in the alignment process. By applying the optimized $\widehat{\Delta W_b}$ to the base model, we effectively integrate only the superficial knowledge. This modified version is referred to as the "base model with superficial knowledge."


\subsection{Is Alignment Primarily Superficial?}
\label{sec:align_comp}
\begin{figure}[h]
\centering
\vskip -0.4in
  \includegraphics[width=0.45\textwidth]{figs/kl2.png}
  \caption{KL divergence between the base model and the aligned model, and between the base model with superficial knowledge and the aligned model}
\label{fig:kl}
\end{figure}

%  analyze superficial knowledge to
We then try to address the question posed earlier: What proportion of alignment does superficial knowledge constitute, and is alignment entirely superficial?

% \textbf{Experimental Setup.} 
To address this, 
% we initially distill a shallow linear projection head to extract superficial knowledge and 
we evaluate the base model, aligned model, and base model with only superficial knowledge on various downstream tasks to gauge the importance of superficial knowledge. We use four datasets, each curated to evaluate different aspects of alignment: 1. The GSM dataset \cite{gsm}, comprising mathematical tasks, is utilized to analyze reasoning ability. 2. The Toxigen dataset \cite{toxigen}, which includes both neutral and toxic questions, focuses on evaluating the model's ability to avoid generating toxic content. 3. The Advbench dataset \cite{advbench}, featuring harmful questions, is used to assess safety. 4. The TruthfulQA dataset \cite{truthfulqa} assesses the model’s capability in providing factual responses.   In our experiments, we use both LLaMA2 as the base models, with LLaMA2-chat serving as the aligned models, the results are presented in Table~\ref{tab:ana7}. Additional results for Mistral and Qwen are included in Appendix~\ref{app:additional}. For more details about the training process and experiment setup, please refer to Appendix~\ref{app:setup}. 


\begin{table*}[t]

\begin{center}

% \begin{small}
\begin{tabular}{ccccccc}
\toprule
\multirow{3}{*}{Model} & GSM($\uparrow$) & Toxigen($\downarrow$) & \multicolumn{2}{c}{Advbench($\downarrow$)} & TruthfulQA($\uparrow$)  \\
  ~ & (reasoning) & (toxicity) &  \multicolumn{2}{c}{(safety)} &  (factuality) \\
% \midrule
~&ACC & ToxiScore & HarmRate & HarmScore & \% Info+True \\
\midrule
 7B                   & 0.037         & 0.77        & 0.66        &          3.84        & 0.34 \\
7B-Chat(Aligned)  & 0.230(+0.193) & 0.00(-0.77) & 0.00(-0.66) & 1.00(-2.84) & 0.68(+0.34) \\
 7B+Urial           & 0.049(+0.012) & 0.00(-0.77) & 0.07(-0.59) & 1.29(-2.55) & 0.41(+0.07) \\
 7B+LIMA           & 0.058(+0.021) &  0.86(+0.11) & 0.84(+0.18) & 4.63(+0.79) & 0.42(+0.08) \\
% \rowcolor{LightCyan}
 \textbf{7B+Superficial}        & \textbf{0.140(+0.103)} & \textbf{0.00(-0.77)} & \textbf{0.00(-0.66)} & \textbf{1.00(-2.84)} & \textbf{0.66(+0.32)} \\
\midrule
 13B  & 0.066         & 0.85        & 0.80        & 4.34 & 0.23 \\
 13B-Chat(Aligned)                       & 0.324+(0.258) & 0.00(-0.85) & 0.00(-0.80) & 1.00(-3.34) & 0.71(+0.48) \\
 13B+Urial                           & 0.177(+0.111) & 0.00(-0.85) & 0.05(-0.75) & 1.23(-3.11) & 0.50(+0.27) \\
13B+LIMA                           & 0.114(+0.048) & 0.91(+0.06) & 0.82(+0.02) & 4.61(+0.27) & 0.51(+0.28) \\
% \rowcolor{LightCyan}
  \textbf{13B+Superficial}        & \textbf{0.226(+0.160)} & \textbf{0.00(-0.85)} & \textbf{0.00(-0.80)} & \textbf{1.00(-3.34)} & \textbf{0.55(+0.32)} \\
\bottomrule
\end{tabular}
% \end{small}
\end{center}
\caption{ Superficial knowledge is sufficient for safety and detoxifying but remains a gap for more knowledge-intensive tasks. Evaluation is based on LLaMA2. $\uparrow$ means the metric is higher the better, and $\downarrow$ means the metric is lower the better.}\label{tab:ana7}
\end{table*}


% \begin{table}[h]
% \vskip -0.3in
% \caption{Evaluation on LLaMA2-13b}
% \label{tab:ana13}
% \begin{center}
% \begin{tabular}{ccccccc}

% \toprule
%  \multirow{3}{*}{Model} & GSM($\uparrow$) & Toxigen($\downarrow$) & \multicolumn{2}{c}{Advbench($\downarrow$)} & TruthfulQA($\uparrow$)  \\
%   ~ & (reasoning) & (toxicity) &  \multicolumn{2}{c}{(safety)} &  (factuality) \\
% % \midrule
% ~&ACC & ToxiScore & HarmRate & HarmScore & \% Info+True \\
% \midrule
%  Base  & 0.066         & 0.85        & 0.80        & 4.34 & 0.23 \\
%  Aligned                       & 0.324+(0.258) & 0.00(-0.85) & 0.00(-0.80) & 1.00(-3.34) & 0.71(+0.48) \\
%  Base+Urial                           & 0.177(+0.111) & 0.00(-0.85) & 0.05(-0.75) & 1.23(-3.11) & 0.50(+0.17) \\
% Base+LIMA                           & - & - & 0.82(+0.02) & 4.61(+0.27) & - \\
% \rowcolor{LightCyan}
%   Base+Superficial        & 0.226(+0.160) & 0.00(-0.85) & 0.00(-0.80) & 1.00(-3.34) & 0.55(+0.22) \\
% \bottomrule


% \end{tabular}

% \end{center}
% \end{table}



\subsubsection{Superficial knowledge indeed takes a large proportion of the alignment, particularly in the front part of the response.} 
The results in Table\ref{tab:ana7} show that simply adding superficial knowledge to the model enables achieving  most performance gains achieved through alignment. This includes eliminating the risk of generating unsafe or toxic responses, and reclaiming an average of 58\% and 78\% of the performance improvements in GSM and TruthfulQA. These gains surpass those achieved by other simple methods, such as LIMA \cite{lima} and Urial \cite{urial}, as our approach more thoroughly captures the scope of superficial knowledge.
Additionally, 
% to explore the distribution of superficial knowledge within model responses, 
we visualized the relationship between position and KL divergence of next token probabilities of the base model vs. aligned model and base model + superficial knowledge vs. aligned model across 100 test samples, shown in Figure~\ref{fig:kl}. The figure reveals that superficial knowledge could considerably reduces the KL divergence between the base and aligned models, highlighting its critical role in alignment. Moreover, we found the initial positions (e.g., the first 10 tokens) in each response may contain the most alignment knowledge, as indicated by significantly different distributions between the base and aligned models at these positions. However, this knowledge is predominantly superficial, as evidenced by the shallow linear projection head can readily assimilate, driving the KL divergence near zero   at these positions.  In contrast, the knowledge in later positions is more complex and less readily captured by the linear projection head, indicating a deeper level of knowledge.


\subsubsection{Alignment is not merely superficial knowledge}. Although superficial knowledge contributes significantly to model alignment, our results suggest that alignment is not solely comprised of superficial elements. This is evident from the persistent performance gap between the base model equipped with superficial knowledge and the fully aligned model, particularly in knowledge-intensive tasks such as GSM and TruthfulQA. Additionally, the KL divergence between the base model with superficial knowledge and the aligned model cannot be minimized to zero, further indicating that deeper, more complex knowledge  also play a critical role in complete model alignment.

\begin{table*}[t]
% \comm{To add the implication of the figure.}


\begin{center}
\begin{small}
\begin{tabular}{|p{13.5cm}|}
\toprule
\textbf{Question:} Toula went to the bakery and bought various types of pastries. She bought 3 dozen donuts which cost \$68 per dozen, 2 dozen mini cupcakes which cost \$80 per dozen, and 6 dozen mini cheesecakes for \$55 per dozen. How much was the total cost? \\
\textbf{Correct Answer:} \$694 \\
\midrule
\textbf{Base Model:} \\
\#\#\# Solution \\
\$68 + 80 + 55 = 203\$\\
\#\#\# Explanation\\
The total cost is \$203.\\
\#\#\# Hints\\
- The total cost is the sum of the cost of each item...\\

\midrule
\textbf{Aligned Model:} \\
\textcolor{red}{To find} the total cost \textcolor{red}{of} the pastries, we need to \textcolor{red}{calculate} the \textcolor{red}{cost} of each type of pastry and then add them up.\\
\textcolor{red}{1}. \textcolor{red}{Don}uts: 3 dozen donuts \textcolor{red}{x} \$68 per dozen = \$204\\
\textcolor{red}{2}. Mini cupcakes: 2 dozen mini cupcakes x \$80 per dozen = \$160\\
3. Mini cheesecakes: 6 dozen mini cheesecakes x \$55 per dozen = \$330\\
Total cost = \$\textcolor{red}{2}04 + \$160 + \$330 = \$\textcolor{red}{6}94\\
\textcolor{red}{There}fore, the total cost of the pastries \textcolor{red}{that} Toula bought \textcolor{red}{at} the bakery \textcolor{red}{is} \$694.\\

\midrule
\textbf{Alignment Token Shift (Base$\rightarrow$Aligned):} (\#\# $\rightarrow$ To)  ( solve $\rightarrow$ find )  (,$\rightarrow$ of) (find$\rightarrow$ calculate)  (total$\rightarrow$ cost ) (The$\rightarrow$1 ) (The$\rightarrow$ Don ) (cost$\rightarrow$ x ) ( ' ' $\rightarrow$ 2 ) (6 $\rightarrow$ 2 ) \textcolor{blue}{(8 $\rightarrow$ 6 )} ( \#\# $\rightarrow$ There ) ( is $\rightarrow$ that ) ( was $\rightarrow$ at ) ( was $\rightarrow$ is ) \\
\midrule
\textbf{Base Model + Superficial Knowledge:}\\
\textcolor{red}{To find} the total cost \textcolor{red}{of} the \textcolor{red}{items T}oula bought \textcolor{red}{at} the bakery, we need to \textcolor{red}{add} the cost of each item \textcolor{red}{she} bought.\\
The cost of 3 dozen donuts \textcolor{red}{=} 3 \textcolor{red}{x} \$68 = \$204 \\
\textcolor{red}{The} cost of 2 dozen mini cupcakes = 2 x \$80 = \$160 \\
The cost of 6 dozen mini cheesecakes = 6 x \$55 = \$330 \\
\textcolor{red}{There}fore, the total cost of the items Toula bought at the bakery is\textcolor{red}{: \$}204 + \$160 + \$330 = \$894 \\
\textcolor{red}{So}, the total cost of the items Toula bought at the bakery is \$894. \\
\midrule
\textbf{Alignment Token Shift (Base$\rightarrow$Base+Superficial Knowledge):} (\#\# $\rightarrow$ To)  (solve $\rightarrow$ find )  (,$\rightarrow$ of)  ( past $\rightarrow$ items )  (,$\rightarrow$ T) (,$\rightarrow$ at)  (find$\rightarrow$ add ) (.$\rightarrow$she ) (is$\rightarrow$ = )  ( * $\rightarrow$ x ) (' ' $\rightarrow$ The ) (The $\rightarrow$ There ) (\$ $\rightarrow$ : ) (' ' $\rightarrow$ \$ ) (\#\# $\rightarrow$ So )  \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\caption{Examples of responses from the base model, aligned model, and base model with superficial knowledge. Tokens highlighted in \textcolor{red}{red} indicate token shifts, where the top token generated by the model differs from that of the base model when given the same input at the current step.}\label{tab:example1}
\end{table*}

To better illustrate the distinction between superficial and deeper knowledge, we analyze response examples to observe the changes that occur during inference when only superficial knowledge is applied, and what cannot be captured by superficial knowledge alone. We input the same questions into the base model, the aligned model, and the base model augmented with superficial knowledge. One example from the GSM test set is presented in Table~\ref{tab:example1}. In the responses shown, tokens highlighted in \textcolor{red}{red} indicate  token shifts, where the top token generated by the current model differs from that of the base model when given the same input at the current step. Additionally, we display the corresponding source shift tokens for each shift token.

\subsubsection{Restyle Patterns in Extracted Superficial Knowledge.} As demonstrated in Table~\ref{tab:example1}, incorporating superficial knowledge noticeably changes the model's response style. The base model often provides direct but sometimes inaccurate answers, while the aligned model adopts a more structured, step-by-step approach, typically organizing points sequentially (e.g., 1, 2, 3, 4). This structured restyling is what we define as superficial knowledge.  In the given example, the base model augmented with superficial knowledge follows a more logical, stepwise structure, resulting in more reasonable and coherent answers. This structured response pattern enables the aligned model to provide correct answers more consistently.
Moreover, when examining token shifts between the base model and the base model equipped with superficial knowledge, we observed that both source and target shift tokens predominantly focus on stylistic elements used for organizing responses. For example, '\#\# $\rightarrow$ To'  leads model to recall the target of the question.  'The $\rightarrow$ There(fore)' push model to summarize the findings.  
These shifts greatly help model to organize the response. Additionally, as previously noted, initial positions hold the most alignment knowledge, which is largely superficial. This is clearly demonstrated in the example where the phrase 'To find' significantly alters the answer style, marking a crucial contribution from alignment. More examples will be provided in Appendix~\ref{app:example}.


\subsubsection{What is essential for alignment other than superficial knowledge? The ability to reason and integrate context may count.} As demonstrated earlier, superficial knowledge alone cannot cover all aligned knowledge, and there remains a performance gap between a base model equipped with superficial knowledge and an aligned model. This gap exists because the aligned model is superior in its ability to reason and integrate context compared to the base model, as shown in Table~\ref{tab:example1}. The base model with superficial knowledge ultimately provides the incorrect answer due to a calculation error: it miscalculates '\$204 + \$160 + \$330 = \$894'. In contrast, the aligned model does not exhibit this error, as demonstrated by the token shift pair \textcolor{blue}{(8 $\rightarrow$ 6)}.  The mathematical calculations require a high level of integration and understanding of token relationships, which cannot be achieved through a simple shallow linear projection head (superficial knowledge). This also underscores that alignment is more than merely superficial knowledge.


% Additionally, when we feed the question and output from the aligned model (prior to the final answer of 694) into the base model, the base model still fails to generate the correct answer, as demonstrated by the token shift pair \textcolor{blue}{(8 $\rightarrow$ 6)}. 














% ================================

\section{Using Superficial Knowledge for A Good Purpose}
\label{sec:good_purpose}
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\textwidth]{figs/main4.jpg}
%   \caption{main}
%   \label{fig:main}
% \end{figure}

After gaining a basic understanding of superficial knowledge in alignment, we will highlight several benefits of extracting and isolating this knowledge.
% Superficial knowledge, which can be succinctly represented by a linear model, primarily comprises style patterns that may generally benefit various models. This prompts an investigation into whether the extracted superficial knowledge  can be transferred across models, potentially enabling offsite alignment for larger models. Furthermore, as demonstrated in \cite{alignattack}, safety in alignment is notably vulnerable, indicating that it can be easily compromised by additional fine-tuning. We also explored whether safety compromised in this way can be restored using the superficial knowledge we extracted.


\subsection{Weak-to-Strong Superficial Alignment}\label{sec:transfer}





\begin{figure}[h]
% \vspace{-0.1in}
\centering
  \includegraphics[width=0.45\textwidth]{figs/cover2.png}
  \caption{Trade-off between informativeness(Shift token cover rate) and transferability(Transfer space similarity) with diffrent $k$}
  % \caption{Trade-off between informativeness and transferability with diffrent $k$}
  \label{fig:cover}
  \vspace{-0.2in}
\end{figure}

Initially, since the essence of superficial knowledge lies in restyling, and this restyling pattern may be universal across models, we explore the possibility of transferring superficial knowledge between models.
% We observe that the extracted superficial knowledge can indeed be transferred across models of different sizes, enabling weak-to-strong offsite tuning.
% \textbf{Key challenges: finding a transferable input space.} 
A major challenge in achieving effective transferability is identifying a generalizable input space for superficial knowledge modeling. 

As described in Section\ref{sec:linear}, we store the superficial knowledge of alignment within a linear weight, $\widehat{\Delta W_b}$. However, this weight cannot be directly applied to other models, as it is intrinsically tied to the last hidden state space, which is not generalizable across models. To overcome this limitation and enable effective knowledge transfer, we identify a more universally applicable yet equally informative input space for extracting superficial knowledge: the  logits space. Since models from the same family typically share the same vocabulary, regardless of model size, the logits space offers a consistent input structure. Moreover, it can effectively capture the contextual knowledge stored in the hidden states.

However, in our experiments, we found that using the full output logits is not an optimal choice. Employing only the top-$k$ logits as input (i.e., setting all logits ranked beyond $k$ to 0) yields better transferring results. This can be attributed to two main reasons. First, the most critical information tends to be concentrated in the top-$k$ logits, as significant target-shift tokens are often found among the top-ranked tokens of the base model. Second, tail tokens typically contain more random information, and while they might capture additional details, such patterns are not consistent across models and do not transfer effectively.

To substantiate these points, we computed two metrics within the top-$k$ logits space and the full logits space. The first metric, \textit{shift token cover rate}, measures the proportion of top-$k$ tokens predicted by the base logits that encompass the target shift tokens (i.e., the top-1 token predicted by the aligned model). As $k$ increases, the \textit{shift token cover rate} correspondingly rises. The second metric, the  \textit{transfer space similarity}, evaluates the similarity of the top-$k$ token logit spaces across models with different size. We collected 1,000 logit samples from both LLaMA2-7b and LLaMA2-13b using identical inputs, denoted as $\mathbf{L}_{7b}$ and $\mathbf{L}_{13b}$, respectively. We performed Singular Value Decomposition (SVD) on these samples: $\mathbf{L}_{7b}=U_{7b}S_{7b}V_{7b}^T$ and $\mathbf{L}_{13b}=U_{13b}S_{13b}V_{13b}^T$, where $V_{*} \in \mathbb{R}^{|\mathcal{V}|\times 1000}$ represents the base vectors for the logits space, and $|\mathcal{V}|$ is the vocabulary size. The similarity between $V_{7b}$ and $V_{13b}$ was  calculated using the formula:
\begin{align}
\text{Similarity} = \frac{\|V_{13b}^T V_{7b}\|_F}{\sqrt{\|V_{13b}\|_F \|V_{7b}\|_F}}
\end{align}
This similarity assesses the subspace similarity between the top-$k$ token logit spaces of LLaMA2-7b and LLaMA2-13b.


In Figure~\ref{fig:cover}, we plot the relationship between \textit{shift token cover rate} and \textit{transfer space similarity}.   As the value of $k$ increases, we observe a decrease in \textit{transfer space similarity} and a corresponding increase in \textit{shift token cover rate}, indicating a potential trade-off between informativeness and transferability.  An appropriate value for $k$ may be selected based on this trade-off. Further details will be discussed in Appendix~\ref{app:topk}.



\begin{table*}[t]
% \vskip -0.1in
\begin{center}

% \begin{small}
% \scalebox{0.95}{
\begin{tabular}{ccccccc}
\toprule
\multirow{3}{*}{Model} & GSM($\uparrow$) & Toxigen($\downarrow$) & \multicolumn{2}{c}{Advbench($\downarrow$)} & TruthfulQA($\uparrow$)  \\
  ~ & (reasoning) & (toxicity) &  \multicolumn{2}{c}{(safety)} &  (factuality) \\
% \midrule
~&ACC & ToxiScore & HarmRate & HarmScore & \% Info+True \\
\midrule
 7B                   & 0.037         & 0.77        & 0.66        &          3.84        & 0.34 \\
 7B+Superficial        & 0.140(+0.103) & 0.00(-0.77) & 0.00(-0.66) & 1.00(-2.84) & 0.66(+0.32) \\
 % \rowcolor{LightCyan}
7B+Superficial-BB-7B        & 0.111(+0.074) & 0.00(-0.77) & 0.00(-0.66) & 1.00(-2.84) & 0.46(+0.12) \\

\midrule
 13B  & 0.066         & 0.85        & 0.80        & 4.34 & 0.23 \\
  13B+Urial                           & 0.177(+0.111) & 0.00(-0.85) & 0.05(-0.75) & 1.23(-3.11) & 0.50(+0.27) \\
13B+LIMA                           & 0.114(+0.048) & 0.91(+0.06) & 0.82(+0.02) & 4.61(+0.27) & 0.51(+0.28) \\
13B+Superficial        & 0.226(+0.160) & 0.00(-0.85) & 0.00(-0.80) & 1.00(-3.34) & 0.55(+0.32) \\
  
  % \rowcolor{LightCyan}
  13B+Superficial-BB-7B        & 0.168(+0.102) & 0.00(-0.85) & 0.00(-0.80) & 1.03(-3.31) & 0.55(+0.32)  \\
\bottomrule
\end{tabular}
% }

% \end{small}
\end{center}\caption{Superficial knowledge can be transferred across models. Evaluation is based on LLaMA2. $\uparrow$ means the metric is higher the better, and $\downarrow$ means the metric is lower the better.}\label{tab:bb}

\end{table*}
% \vskip -0.2in

% To better achieve transferability, we extract superficial knowledge from alignment using a linear model, with top-$k$ logits as input.  We  approximate and mimic the token distrubution shift with a  linear weight $W_{trans}$:

To enhance transferability, we extract superficial knowledge from model alignment using a linear model, with the top-$k$ logits as input. We approximate and model the token distribution shift using a linear transformation,$W_{trans}$, as follows:
\begin{align}
l_a^t - l_b^t = W_{trans} \cdot \text{topk}(l_b^t)
\end{align}
Here,  $l_a^t$ and $l_b^t$ represent the logits output of the aligned model and the base model at step $t$. The function $\text{topk}(\cdot)$ sets all logits ranked beyond the $k$-th position to zero. We optimize the linear weight $W_{trans}$ using distillation techniques outlined in Section~\ref{sec:linear}. The superficial knowledge extracted through this process is referred to as Black-box Superficial Knowledge (denoted as Superficial-BB).

% is defined such that it sets the values of all logits ranked beyond the $k$-th position to zero. We apply similar distillation techniques described in section~\ref{sec:linear} to optimize the linear weight $W$. And we refer to the superficial knowledge extracted in this manner as Black-box Superficial Knowledge (denoted as Superficial-BB).


% \textbf{Experimental Setup:} 
% In our experiments, we first extracted Black-Box Superficial knowledge from LLaMA2-7b-Chat, and then applied it to both LLaMA2-7b and LLaMA2-13b. 

In our experiments, we extracted Black-Box Superficial knowledge from LLaMA2-7b-Chat, and then applied it to both LLaMA2-7b and LLaMA2-13b. The evaluation results on  downstream tasks are listed in Table~\ref{tab:bb}. 

\textbf{Experiment Results.} We found that although there may be some loss of knowledge modeling due to the information gap between top-$k$ logit space and hidden states space, the black-box linear model still retains much of the superficial knowledge. By attaching the knowledge, we can largely recover the alignment performance, such as eliminating risks of generating harmful responses and improving accuracy in math and factual answering tasks.
Moreover, the black-box superficial knowledge is transferable. When applying the superficial knowledge extracted from LLaMA2-7b-chat to LLaMA2-13b, it still demonstrates strong performance,  reducing the risk of generating harmful responses and increasing accuracy in math tasks from 0.066 to 0.168, and in factual questions from 0.23 to 0.55. The performance gains brought by the extracted superficial knowledge to LLaMA2-13B even surpass that to LLaMA2-7B, this may due to the larger model's superior capability to utilize the superficial knowledge.

The transferability of superficial knowledge can  be utilized in offsite alignment settings, where there may not be sufficient resources to align the full larger model directly. By aligning a smaller model and transferring the extracted superficial knowledge to a larger model,  we can achieve superficial alignment, and the performance could surpass that of other simple alignment methods such as Urial and LIMA.


% This transfer demonstrates that lightweight linear heads can effectively propagate alignment capabilities from a less powerful model to a stronger one, thus circumventing the substantial tuning costs typically associated with larger models. Furthermore, in contrast to proxy tuning, which necessitates inference on three separate models at each step, LinAlign requires only the operation of a lightweight linear head to achieve alignment, significantly enhancing operational efficiency.






\subsection{Recoverable Superficial Safety}\label{sec:restore}

\begin{table*}[t]
\begin{center}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Model} &\multicolumn{2}{c}{Advbench($\downarrow$)} & MMLU($\uparrow$)  \\
~& HarmRate & HarmScore & ACC \\
\midrule
LLaMA2-7b-Chat                             & 0.00 & 1.00 & 0.465 \\
LLaMA2-7b-Chat-Finetuned                    & 0.96 & 4.91 & \textbf{0.466} \\
LLaMA2-7b-Chat-Finetuned (+Urial)       & 0.93 & 4.85 & 0.459 \\
LLaMA2-7b-Chat-Finetuned (+Superficial-BB)       & \textbf{0.08} & \textbf{1.38} & 0.456 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Restoring safety using extracted superficial knowledge after fine-tuning disruptions. $\uparrow$ means the metric is higher the better, and $\downarrow$ means the metric is lower the better.}\label{tab:plug}
\end{table*}
% \vskip -0.2 in

As noted by \cite{alignattack}, safety in alignment is easily disrupted through additional fine-tuning, which can result in the generation of harmful or toxic responses. This raises the question of whether there is also a simple method to restore alignment. Superficial knowledge emerges as a promising candidate due to its simplicity. To explore this, we initially extracted superficial knowledge from the aligned model. The superficial knowledge was still extracted in a black-box manner, considering that the hidden state spaces of the base model and the fine-tuned aligned model are likely to differ. Subsequently, when the safety of the model was compromised by fine-tuning, we attempted to reintegrate the extracted superficial knowledge into the fine-tuned model.

% \textbf{Experimental Setup.}
In our experiments, we use LLaMA2-7b as the base model and LLaMA2-7b-chat as the aligned model to extract superficial knowledge. Following the setup from \cite{alignattack}, we utilize their selected identity shift dataset to fine-tune the LLaMA2-7b-chat model, which represents the most effective benign fine-tuning attack described in their paper. This fine-tuning process induces the model to generate harmful responses. We evaluate the fine-tuned model using the advbench dataset. Additionally, in Appendix~\ref{app:other_recover}, we also explore less aggressive fine-tuning tasks to provide a more comprehensive analysis.

\textbf{Experiment Results.} The results are shown in Table~\ref{tab:plug}. We found that after fine-tuning, the harmful response rate of the model increased dramatically from 0\% to 96\%. However, after restoring the superficial knowledge, most of the performance was regained, and the harmful rate dropped to 8\%. This also indicates that the fine-tuning process may potentially damage the superficial knowledge in alignment. Yet, our extraction method allows for the preservation of this knowledge within a linear model, enabling easy restoration without compromising the model's original utility, as demonstrated by evaluation performance on MMLU. 
Whenever the model is disrupted by fine-tuning, the extracted knowledge can be reapplied without additional training.
In contrast, other superficial methods such as Urial fail to restore the fine-tuned model effectively, as the finetuned model with Urial still produces many harmful responses.







