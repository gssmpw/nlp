\onecolumn
\newpage

\input{sec/related}

\section{Experiment Setup} \label{app:setup}

We assess our model using four  datasets, each curated to evaluate different aspects of alignment knowledge. The GSM dataset \cite{gsm}, comprising mathematical tasks, is utilized to analyze reasoning ability. Meanwhile, the Toxigen dataset \cite{toxigen}, which includes both neutral and toxic questions, focuses on evaluating model's ability to avoid generating toxic content. The Advbench dataset \cite{advbench}, featuring harmful questions, is used to evaluate safety. Additionally, the TruthfulQA dataset \cite{truthfulqa} assesses the model’s capability in providing factual responses. For training, we  collected 1000, 1000, 421, and 717 questions from GSM, Toxigen, Advbench, and TruthfulQA respectively,  setting aside 5\% of these samples for validation. The lr is set to 0.0001. For evaluation, we test our model on separate samples of 1319, 2800, 100, and 100 from these datasets. 





\textbf{Evaluation metrics.} Following the approaches described in \cite{tulu, proxytuning}, we extract the last number in the model's response to serve as the final answer and calculate accuracy (ACC) to evaluate GSM performance. We employ the toxicity classifier based on roberta-large from \cite{toxigen} to assess the toxicity of generated responses. Additionally, we use two open-source fine-tuned LLaMA\footnote{\url{https://huggingface.co/allenai/truthfulqa-truth-judge-llama2-7B}}\footnote{\url{https://huggingface.co/allenai/truthfulqa-info-judge-llama2-7B}} to evaluate the truthfulness and informativeness of the model responses, reporting the percentage of responses that are both truthful and informative (\% Info + True) on Truthfulqa\cite{lin2021truthfulqa}. For the advbench dataset, following \cite{alignattack}, we employ GPT to assess the harmfulness of model responses on a scale of 1-5 (where a higher score indicates greater harmfulness), with the harmRate indicating the fraction of test cases that receive the maximum harmfulness score of 5.

\textbf{Implementation.} We implemented our method with PyTorch. The experiments were conducted on a server equipped with AMD EPYC 7702 64-Core Processor, 512GB Memory, and NVIDIA RTX A6000 GPU (48GB Memory).
% \\comm{To add: type of compute workers, memory, time of execution}
The evaluation and training time  for each experiment is not more than 5 hours, respectively. During inference,  we set `do\_sample` to False, and evaluate in a single run.

\section{Superficial Knowledge in Mistral and Qwen}\label{app:additional}

\begin{table*}[h]

\begin{center}

% \begin{small}
\begin{tabular}{ccccccc}
\toprule
\multirow{3}{*}{Model} & GSM($\uparrow$) & Toxigen($\downarrow$) & \multicolumn{2}{c}{Advbench($\downarrow$)} & TruthfulQA($\uparrow$)  \\
  ~ & (reasoning) & (toxicity) &  \multicolumn{2}{c}{(safety)} &  (factuality) \\
% \midrule
~&ACC & ToxiScore & HarmRate & HarmScore & \% Info+True \\
\midrule
 Mistral                   & 0.224         & 0.86        & 0.92        &          4.76        & 0.33 \\
Mistral-Instruct(Aligned)  & 0.440(+0.216) & 0.00(-0.86) & 0.06(-0.86) & 1.51(-3.25) & 0.75(+0.42) \\
\midrule
 Mistral+Urial           & 0.235(+0.011) & 0.00(-0.86) & 0.10(-0.82) & 1.43(-3.33) & 0.45(+0.12) \\
 Mistral+LIMA           & 0.014(-0.210) &  0.70(-0.16) & 0.68(-0.24) & 3.90(-0.86) & 0.28(-0.05) \\
% \rowcolor{LightCyan}
 Mistral+Superficial        & 0.277(+0.053) & 0.00(-0.86) & 0.12(-0.80) & 1.62(-3.14) & 0.64(+0.31) \\
\bottomrule
\end{tabular}
% \end{small}
\end{center}
\caption{Evaluation based on Mistral-7B-v0.3. $\uparrow$ means the metric is higher the better, and $\downarrow$ means the metric is lower the better.}
\label{tab:anamistral}
\end{table*}



\begin{table*}[h]

\begin{center}

% \begin{small}
\begin{tabular}{ccccccc}
\toprule
\multirow{3}{*}{Model} & GSM($\uparrow$) & Toxigen($\downarrow$) & \multicolumn{2}{c}{Advbench($\downarrow$)} & TruthfulQA($\uparrow$)  \\
  ~ & (reasoning) & (toxicity) &  \multicolumn{2}{c}{(safety)} &  (factuality) \\
% \midrule
~&ACC & ToxiScore & HarmRate & HarmScore & \% Info+True \\
\midrule
 Qwen                   & 0.638         & 0.81        & 0.29        &          2.20        & 0.40 \\
Qwen-Instruct(Aligned)  & 0.723(+0.085) & 0.00(-0.81) & 0.00(-0.29) & 1.00(-1.10) & 0.74(+0.34) \\
\midrule
 Qwen+LIMA           & 0.491(-0.147) &  0.94(+0.13) & 0.17(-0.12) & 1.75(-0.45) & 0.44(+0.04) \\
% \rowcolor{LightCyan}
 Qwen+Superficial        & 0.670(+0.032) & 0.00(-0.81) & 0.00(-0.29) & 1.00(-1.10) & 0.65(+0.25) \\
\bottomrule
\end{tabular}
% \end{small}
\end{center}
\caption{Evaluation based on Qwen-3b. $\uparrow$ means the metric is higher the better, and $\downarrow$ means the metric is lower the better.}
\label{tab:anamistral}
\end{table*}

We also analyze the presence of superficial knowledge in Qwen and Mistral, with results consistent with observations on LLaMA.  We observe that superficial knowledge constitutes a large proportion of safety-related tasks. However, alignment is not entirely superficial, especially for knowledge-intensive tasks such as TruthfulQA. Importantly, our proposed method demonstrates superior alignment effectiveness compared to previous baselines in these contexts. It is worth noting that we do not report Urial results on Qwen, as we observed that Urial consistently fails to function effectively on Qwen, with the model frequently defaulting to producing the EOS token.




\section{Strategies for Selecting Transferable Input Spaces}\label{app:topk}

\begin{figure*} % "l" for left, "0.5\textwidth" for the image width
\centering
% \vskip -0.3in
  \includegraphics[width=0.4\textwidth]{figs/k_acc.png}
  \caption{K vs Token Accuracy}
\label{fig:topk}
\end{figure*}

In Section~\ref{sec:transfer}, we discuss the potential trade-off between informativeness and transferability in the context of  input spaces. An optimal value for $k$ may be selected based on this trade-off. Increasing $k$ to include more information from logits can also introduce additional noise, which might reduce the model's transferability. Next, we present our strategies for selecting appropriate $k$ values.

We trained linear heads on LLaMA-7b to extract superficial knowledge with various values of $k$. Training utilized logits collected from the Toxigen datasets, with logits specifically from LLaMA-7b. The token prediction accuracy was then evaluated on validation samples using logits from both LLaMA-7b and LLaMA-13b. We refer to the token accuracy measured on LLaMA-7b as validation accuracy, and the accuracy on LLaMA-13b as validation transfer accuracy. This approach helps quantify the trade-offs between richer logit information and potential transfering noise impacts as $k$ increases.  The relationship between $k$ and  accuracies is illustrated in Figure \ref{fig:topk}. Our findings indicate that below a certain threshold (e.g., 500, as shown in the figure), increasing $k$ enriches the information base, thereby enhancing both validation accuracy and validation transfer accuracy. However, surpassing this threshold, the test accuracy plateaus and the transfer accuracy declines, as the most significant information resides in the top logits, while the tail logits, being closer to random noise, may introduce elements that are not generalizable across different models. To identify an optimal $k$ that is broadly effective across models, we might select $k$ based on validation transfer accuracy.


\section{Recoverability of alignment under other fine-tuning scenarios}\label{app:other_recover}

\begin{table*}[h]
\begin{center}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Model} &\multicolumn{2}{c}{Advbench($\downarrow$)}  \\
~& HarmRate & HarmScore \\
\midrule
LLaMA2-7b-Chat                                  & 0.00 & 1.00 \\
LLaMA2-7b-Chat-Finetuned (Alpaca)	            & 0.20 & 1.81  \\
LLaMA2-7b-Chat-Finetuned (Alpaca)+Superficial	& \textbf{0.05} & \textbf{1.30} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Restoring safety using extracted superficial knowledge after fine-tuning disruptions on Alpaca datasets.}\label{tab:plug_other}
\end{table*}
We additionally experimented with less aggressive fine-tuning tasks in ~\cite{alignattack}, i.e. finetuning on Alpaca. While the attack’s impact was weaker in this case, our method still effectively realigned the model’s safety capabilities.

\section{Extra Examples for Understanding Knowledge in Alignment}\label{app:example}

\begin{table*}[htp]

\begin{center}

\begin{tabular}{|p{14cm}|}
\toprule
\textbf{Question:} Greg has an alarm set to ring three times a day as a reminder. When the alarm goes off, it continues to ring until Greg turns it off. The first time it went off today, it rang four times. The second time it went off, it rang for three times as long as the first time. The third time, it rang for half as long as the second time. How many times did the alarm ring in all? \\
\textbf{Correct Answer:} 12 \\
\midrule
\textbf{Base Model:} \\
\#\#\# Explanation\#\#\# Solution \\
The alarm rings for \$4\$ times the first time, \$3\$ times the second time, and \$2\$ times the third time. The total number of times the alarm rings is: \\
4 + 3 + 2 \&= 9 \\
9 + 2 \&= 11\\
… \\
87 + 2 \&= 89 \\
\midrule
\textbf{Aligned Model:} \\
\textcolor{red}{Let'}s \textcolor{red}{break} down the \textcolor{red}{information} given in the \textcolor{red}{problem:}\\
\textcolor{red}{1}. The alarm rings three times a day.\\
2. \textcolor{red}{The first} time it went off today, it rang four times.\\
3. The second time it went off, it rang for three times as long as the first time\textcolor{red}{, which means} it rang for \textcolor{red}{4 x} 3 = 12 times.\\
4. The third time \textcolor{red}{it} went off, it rang for half as long as the second time, which means it rang for 12 \textcolor{red}{/} 2 = 6 times.\\
\textcolor{red}{So}, the alarm rang \textcolor{red}{a} total of 4 + 12 + 6 = 22 times \textcolor{red}{today}.\\
\midrule
\textbf{Alignment Token Shift Examples:} (\#\# $\rightarrow$ Let)  ( \$ $\rightarrow$ ' )  (say$\rightarrow$ break) (problem$\rightarrow$ information)  (question$\rightarrow$ problem ) (.$\rightarrow$: ) (The$\rightarrow$ 1 ) (When$\rightarrow$ The ) (alarm$\rightarrow$ first ) (.$\rightarrow$ , ) (so$\rightarrow$ which ) (is$\rightarrow$ means ) (1$\rightarrow$ 4 ) (times$\rightarrow$ x ) (,$\rightarrow$ it ) (x$\rightarrow$ / ) (Now$\rightarrow$ So ) (' '$\rightarrow$ a ) (.$\rightarrow$ today ) \\
\midrule
\textbf{Base Model + Shallow Align:}\\
\textcolor{red}{Let'}s \textcolor{red}{break} \textcolor{red}{this} down \textcolor{red}{step} by step\textcolor{red}{:}\\
\textcolor{red}{1}. The first time \textcolor{red}{the Al}arm went off, it rang for 4 times.\\
2. The second time the Alarm went off, it rang for 3 times as long as the first time\textcolor{red}{, which} is 3 \textcolor{red}{x} 4 = 12 seconds.\\
3. The third time the Alarm went off, it rang for half as long as the second time, which is 12 / 2 = 6 seconds.\\
So, \textcolor{red}{in} total, the Alarm rang for 4 + 12 + 6 = 22 seconds. </s>\\
\midrule
\textbf{Shallow Align Token Shift Examples:} (\#\# $\rightarrow$ Let)  ( \$ $\rightarrow$ ' )  (say$\rightarrow$ break), (down$\rightarrow$ this)  (into$\rightarrow$ step ), (.$\rightarrow$: ) (The$\rightarrow$ 1 ) (it$\rightarrow$ the ),(alarm$\rightarrow$ Al )  (.$\rightarrow$, )  (so$\rightarrow$ which)  (times$\rightarrow$ x )  (the$\rightarrow$in )  \\
\bottomrule
\end{tabular}

\end{center}
\caption{Examples of responses from the Base Model, Aligned Model, and Base Model with Superficial Knowledge (\textcolor{red}{Red} color shows the shift position).}
\label{tab:example2}
\end{table*}

\textbf{Analysis for Table~\ref{tab:example2}:} The superficial knowledge primarily alters the response style, guiding the model to deconstruct the question. This stylistic  ultimately leads to the correct answers.





\newpage


\begin{table*}[h]

\begin{center}

\begin{tabular}{|p{14cm}|}
\toprule
\textbf{Question:} James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week? \\
\textbf{Correct Answer:} 540 \\
\midrule
\textbf{Base Model:} \\
180 meters \\ 
\#\#\# Solution \\
The total distance run is: \\
\$\textbackslash dfrac {3 \textbackslash times 3 \textbackslash times 60} {1000} = 180\$\\
\#\#\# Explanation \\
The total distance run is:\\
\$\textbackslash dfrac {3 \textbackslash times 3 \textbackslash times 60} {1000} = 180\$\\
\#\#\# Hints\\
- 1000 is the number of meters in a kilometer.\\
- 60 is the number of meters in a sprint.\\
- 3 is the number of sprints per week.\\

\midrule
\textbf{Aligned Model:} \\
\textcolor{red}{To find out} how many meters \textcolor{red}{James} runs \textcolor{red}{in} a week, we need to \textcolor{red}{multiply} the number of sprints he runs \textcolor{red}{each} week by the \textcolor{red}{distance of} each sprint. \\
\textcolor{red}{James} runs 3 sprints 3 times a week\textcolor{red}{,} so he runs \textcolor{red}{a} total of \textcolor{red}{3 x} 3 = 9 sprints \textcolor{red}{per} week.\\
Each sprint is 60 meters long, so \textcolor{red}{James} runs a total of 9 x 60 = 540 meters per week.\\
\midrule
\textbf{Alignment Token Shift Examples:} (\#\# $\rightarrow$ To)  ( solve $\rightarrow$ find )  (the$\rightarrow$ out) (he$\rightarrow$ James)  (a$\rightarrow$ in ) (find$\rightarrow$multiply ) (by$\rightarrow$ each ) (number$\rightarrow$ distance )  (he$\rightarrow$ of ) (\$\$$\rightarrow$ James ) (.$\rightarrow$ , ) (' '$\rightarrow$ a ) (9$\rightarrow$ 3 ) (s$\rightarrow$ x ) (each$\rightarrow$ per ) (he$\rightarrow$ James ) \\
\midrule
\textbf{Base Model + Shallow Align:}\\
\textcolor{red}{To determine} the total \textcolor{red}{distance of running for James in} a week, we need to \textcolor{red}{multiply} the \textcolor{red}{number} of sprints he runs \textcolor{red}{per} week by the distance he runs in each sprint. \\
James runs 3 sprints per week, and each sprint is 60 meters long, so the total distance he runs in a week is\textcolor{red}{:} \\
3 \textcolor{red}{x} 60 = 180 meters \\
\textcolor{red}{So}, \textcolor{red}{James} runs \textcolor{red}{a} total of 180 meters per week. \\

\midrule
\textbf{Shallow Align Token Shift Examples:} (\#\# $\rightarrow$ To)  ( solve $\rightarrow$ determine )  (number$\rightarrow$ distance ) (run$\rightarrow$ of ) (the$\rightarrow$ running ) (,$\rightarrow$ for ) (the$\rightarrow$ James ) (,$\rightarrow$ in ) (find$\rightarrow$ multiply ) (distance$\rightarrow$ number ) (by$\rightarrow$ per ) (\$\$$\rightarrow$ James )  (' '$\rightarrow$ per )(.$\rightarrow$ , ) (' '$\rightarrow$ : ) (*$\rightarrow$ x )(There$\rightarrow$ So )(the$\rightarrow$ James )(' '$\rightarrow$ a ) \\
\bottomrule
\end{tabular}

\end{center}
\caption{Examples of responses from the Base Model, Aligned Model, and Base Model with Superficial Knowledge (\textcolor{red}{Red} color shows the shift position).}
\label{tab:example3}
\end{table*}

\textbf{Analysis for Table~\ref{tab:example3}:} Superficial knowledge alters the response style, but fails to produce correct answers due to the lack of integration of '3 times' in the question.


