\section{Related Work}

\textbf{Aligning LLMs with human preference.}
Large Language Models (LLMs) have demonstrated superior capabilities across various NLP tasks, yet they pose several challenges. These include the potential to disseminate misleading information, pursue unsuitable objectives, and generate content that may be perceived as harmful or biased\cite{mozes2023use, chang2024survey}. To address these issues, alignment was proposed to regulate LLMs with human preferences and values~\cite{rlhf, dpo, spin}. A prevalent method of alignment is Reinforcement Learning from Human Feedback (RLHF). This approach uses reward models, which serve as proxies for human judgments, to supervise an LLM~\cite{macglashan2017interactive, xue2023reinforcement, yuan2023rrhf, zhu2023principled}. However, RLHF is generally more complex than supervised learning, exhibiting optimization instability and sensitivity to hyperparameters. In recent developments, there has been a significant shift towards employing closed-form losses that directly utilize offline human preferences~\cite{song2024preference, ethayarajh2024kto}, such as Direct Preference Optimization (DPO)~\cite{dpo} that simplifies the optimization objectives.
% DPO simplifies the optimization process by eliminating the need for a separate reward learning phase, which is a standard step in traditional preference-based reinforcement learning approaches. 
% Another  approach is PRO~\cite{song2024preference}, which enhances LLMs by enabling them to analyze and rank multiple dimensions and positions of candidate responses based on human preferences. 
% Additionally, KTO  directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences. ~\cite{ethayarajh2024kto}.
% These approaches, however, often require extensive tuning of the full LLM parameters, which can be resource-intensive.
Though extensive resources are devoted, the alignment is not very robust and can be easily removed by jailbreaking prompts.
Such limitation motivates us to understand the alignment toward improving it.



\textbf{Superficial alignment.}
Recent studies have shown that only a few samples are sufficient to align a large language model (LLM)~\cite{lima, chen2023alpagasus, lee2023platypus}, leading to the Superficial Alignment Hypothesis. This hypothesis suggests that an aligned LLM's knowledge is largely derived from pre-training, with alignment mainly imparting superficial adjustments. Additionally, Urial\cite{urial} demonstrated that alignment can be achieved through in-context learning. However, these studies only show that alignment can be accomplished using superficial methods to a certain degree, without fully validating the hypothesis or assessing the extent to which alignment is superficial. In this paper, we explore the superficiality of knowledge introduced during alignment, investigate the proportion of superficial knowledge involved, and analyze what is truly learned throughout the alignment process, offering our insights on the Superficial Alignment Hypothesis.


% Recent studies suggest that alignment in models is often superficial, and a limited number of data points might be adequate for learning. This alignment is primarily characterized by style shifts~\cite{urial, lima, chen2023alpagasus, lee2023platypus}. Urial~\cite{urial} further demonstrates that style-guided prompts, which incorporate common summarized styles, can effectively facilitate these style shifts. In our paper, we delve deeper into style shifts in alignment, discovering that these shifts can be captured using a lightweight linear model head. Additionally, we propose that using distillation allows for the autonomous extraction of style shifts, thereby minimizing the need for manual intervention.
