
\section{Introduction}\label{sec:intro2}


Recent years have witnessed significant advancements of large language models (LLMs) in various tasks~\citep{hendrycks2021measuring,cobbe2021training,chen2021codex,welbl2017crowdsourcing}%\comm{cite}
. Although LLMs acquire extensive world knowledge, they meanwhile cast serious risks to the society. For example, LLMs are easily prompted to generate toxic, misleading, or harmful content~\citep{wei2024jailbroken,zou2023universal,qi2023fine}. To ensure that the behaviors of LLMs adhere to human values and preferences, aligning LLMs to follow instructions based on human feedback~\cite{ipo,spin, rlhf, dpo, sppo} is essential. To obtain satisfactory alignment, the tuning of an LLM usually demands a non-trivial amount of data and computation resources.


Despite the considerable efforts invested in tuning LLMs~\citep{touvron2023llama,falcon}, it has been surprisingly discovered that alignment might be attainable at lower costs or through simpler methods~\citep{lima,chen2023alpagasus,lee2023platypus,urial}. For example, using only a few selected training examples can significantly improve alignment performance, approaching levels achieved through extensive tuning. Furthermore, Urial~\citep{urial} found that alignment often results in "stylistic token shifts," and by employing in-context learning (ICL)
\citep{brown2020language,wei2022chain} 
with a few restyling examples, alignment can be improved without any further tuning. These findings give rise to the \textit{Superficial Alignment Hypothesis}\citep{lima}, which suggests that a model may acquire most of its knowledge and abilities during pre-training, while alignment primarily involves superficial adjustments.

However, current methods support this hypothesis primarily through informal observations and indirect implications (i.e., because alignment can be achieved through superficial methods, it is hypothesized to be superficial). There remains a lack of rigorous, deep analysis regarding the extent to which alignment relies on superficial knowledge and whether alignment is purely superficial.

% However, we should not jump to conclusions regarding this hypothesis. Existing works only demonstrated that superficial methods can significantly contribute to alignment and approach performance by full tuning.
% These studies do not conclusively prove that alignment is inherently superficial. Since they cannot guarantee that the knowledge introduced by simple methods such as LIMA~\cite{lima} and Urial~\cite{urial} \comm{Should mention that Urial is ICL} is entirely superficial \comm{lacks evidence}. 
% \comm{We can use attention shift to show that LIMA and Urial change the casualty.}
% For example, Duan \etal \cite{duan2023exploring} shows that in-context learning can modify the internal causality of LLM inference. Thus, ICL potentially introduces confounding factors and causes significant changes in its hidden representations. It may also inadvertently introduce new knowledge into the model. Therefore, it is still plausible that whether deep, non-superficial knowledge coexists with superficial knowledge in alignment and plays a crucial role. The current research is insufficient to verify this, which underscores the need for a more nuanced understanding of the various types of knowledge that contribute to model alignment.

% The current research is insufficient to reject the opinion of coexistence, which underscores the need for a more nuanced understanding of the various types of knowledge that contribute to model alignment.

\begin{figure*}[t]
  \centering
  % \includegraphics[width=\textwidth]{figs/main6.jpg}
  \includegraphics[width=\textwidth]{figs/main3.drawio.png}
  \caption{We extract superficial knowledge from an aligned model into a shallow linear projection head. The upper left shows the potential advantages brought by the extracted superficial knowledge, and the upper right shows the WordCloud of source shift tokens and target shift tokens, which primarily involves stylistic words.}
  \label{fig:main}
\end{figure*}

To fill this gap, we first formalize the previously vague concept of superficial knowledge. We define \textit{superficial knowledge} as the type of knowledge that can be easily acquired through simple token restyling, without requiring modifications to the model's understanding of the underlying causal relationships between tokens and the process of knowledge extraction and compression. In contrast, \textit{deep knowledge} pertains to the model's ability to capture token relationships and extract meaningful insights from the data.

% And then we propose to extract and separate the superficial knowledge from the alignment.  

% Notably, knowledge in large language models (LLMs) is intricately complex, involving reasoning, integration, and mutual influence among contexts. This knowledge is deeply embedded within the parameters of LLMs. To further our analysis, we define \textit{superficial knowledge} as the type of knowledge that can be easily acquired without necessitating modifications to the model's understanding of the underlying causal relationships between tokens. 

We propose a method to extract and isolate superficial knowledge from the alignment process. To ensure the extracted knowledge remains superficial, we restrict our modifications to shallow, simple structures - specifically, the linear projection head of the LLM. This affects only the final token selection process, without altering the intermediate token merging or self-attention mechanisms. By doing so, we avoid disrupting the deep knowledge associated with internal token interactions. Furthermore, to ensure that no new knowledge is introduced into the model and to focus exclusively on analyzing the knowledge derived from alignment, we employ distillation to finalize the extraction process.


With the extracted and separated superficial knowledge, we can quantify the \emph{superficial portion of alignment} by comparing the aligned model with a base model augmented only with superficial knowledge across benchmarks in math, safety, toxicity, and truthfulness. Our key findings are twofold:


% \textbf{(1)} Superficial knowledge indeed takes a large proportion in the alignment, particularly in safety and detoxifying task. These superficial knowledge primarily consists of stylistic patterns, which significantly aid the model in organizing responses. By utilizing extracted superficial knowledge alone, we can remove the 100\% safety and toxicity risks and reclaim 58\% and 78\% performance improvements on average in math and truthfulness tasks, respectively. The performance gains from this superficial knowledge alone  exceed those from other simple methods, such as LIMA \cite{lima} and ICL \cite{urial}, as we more comprehensively cover the breadth of superficial knowledge.

\textbf{(1)} Superficial knowledge constitutes a significant portion of the alignment, especially in safety and detoxification tasks. This knowledge primarily consists of stylistic patterns that help the model structure its responses. By leveraging superficial knowledge alone, we can completely eliminate safety and toxicity risks while achieving average performance improvements of 58\% in math and 78\% in truthfulness tasks. The gains from superficial knowledge surpass those from simpler methods like LIMA~\citep{lima} and ICL~\citep{urial}, as our approach more comprehensively covers the breadth of superficial knowledge.


% \textbf{(2)} Yet, alignment is not totally superficial. There remains a noticeable disparity between superficial knowledge and fully aligned knowledge in knowledge-intensive tasks such as math and truthfulQA tasks. We further demonstrate that the knowledge beyond the superficial may be related to the ability for reasoning and incorporating context in section~\ref{sec:align_comp}.

\textbf{(2)} However, alignment is not entirely superficial. A clear gap remains between superficial knowledge and fully aligned knowledge, particularly in knowledge-intensive tasks such as math and truthfulQA. As we demonstrate in section~\ref{sec:align_comp}, this gap likely relates to the model’s capacity for reasoning and contextual understanding, which goes beyond superficial patterns.


% \textbf{(3)} We also discovered that superficial knowledge in alignment primarily consists of stylistic patterns, which significantly aid the model in organizing responses. Besides, we find the style patterns, located at the forefront of responses, mark a most crucial contribution to alignment, which is consistent with prior findings \cite{urial}. 


In addition, since our extracted superficial knowledge is stored in a simple and modular structure, we have also discovered several useful properties of superficial knowledge. We further demonstrate the \textit{Superficial Advantage (SA)}—the benefits of isolating superficial knowledge alone.

\textbf{SA1: Weak-to-Strong Superficial Alignment.} Our experiments reveal that the extracted superficial knowledge is transferable across models. This transferability can be leveraged for offsite alignment of larger models—superficial knowledge extracted from a smaller, weaker model can be applied to a larger, stronger model. This allows for plug-and-play alignment of the larger model without requiring extensive tuning.

\textbf{SA2: Recoverable Superficial Safety.} Previous work~\citep{alignattack,wei2024jailbroken} has shown that safety mechanisms can be easily compromised, such as through slight fine-tuning on as few as 10 samples. However, with our extracted superficial knowledge, we can re-attach the lightweight structure encapsulating this knowledge to a de-aligned LLM and successfully recover 88\% of the alignment effects without compromising MMLU accuracy.











