\section{Related Work}
\label{Related Work}
This section first reviews the object pose and size estimation methods, dividing them into instance-level and category-level methods, and then reviews recent diffusion model-based methods and explains how our proposed method differs from them. Finally, we review the object pose estimation-based robotic grasping methods.

\vspace{-1.25em}
\subsection{Instance-Level Methods}
Instance-level methods are trained on known objects \cite{SSD-6D} and can be mainly divided into three categories: correspondence-based, template-based, and direct regression-based. Correspondence-based methods can be further divided into 2D-3D correspondence and 3D-3D correspondence. 2D-3D correspondence methods\cite{5,6} first define the keypoints between RGB image and object CAD model. This is followed by training a model to predict the 2D keypoints and using the Perspective-n-Points (PnP) algorithm to solve the object pose. 3D-3D correspondence methods\cite{7,8} define the keypoints on the object CAD model directly and use the observed point cloud to predict the predefined 3D keypoints. Next, they apply the least squares algorithm to solve the object pose. However, most correspondence-based methods rely heavily on rich texture information and may not work well when applied to textureless objects.

\par There are some point cloud-based template methods, which are based on point cloud registration \cite{9,10}. Specifically, the template is the object CAD model with the canonical pose, and the purpose of these methods is to find the optimal relative pose that aligns the observed point cloud with the template. Besides these methods, RGB-based template methods\cite{11,12} also exist, which require collecting and annotating object images from various perspectives during the training phase to create templates. After that, these methods train a template matching model to find the closest template to the observed image and use the template pose as the actual pose of the object. Overall, template-based methods can be effectively applied to textureless objects, however, the template-matching process is generally time-consuming.

\par With the rapid advancement of deep learning technology, direct regression-based methods\cite{13,14,15,16,17} have recently gained popularity. These methods use the ground-truth object poses for supervision and train models to regress the object pose end-to-end. Specifically, DenseFusion\cite{13} fuses the RGB and depth features and proposes a pixel-level dense fusion network for pose regression. FFB6D\cite{14} further designs a bidirectional feature fusion network to fully fuse the RGB and depth features. GDR-Net\cite{15} proposes a geometry-guided network for end-to-end monocular object pose regression. HFF6D\cite{16} designs a hierarchical feature fusion framework for object pose tracking in dynamic scenes. Although instance-level methods have achieved high accuracy, they are restricted to fixed instances, meaning that they only work for specific objects on which they are trained.

\vspace{-1em}
\subsection{Category-Level Methods}
Research in the domain of category-level methods has received substantial attention given their potential for generalization to unknown objects within the given object categories. 
NOCS\cite{18} introduces a normalized object coordinate space, providing a standardized representation for a category of objects, and recovers object pose using the Umeyama algorithm. SPD\cite{19} leverages shape prior deformation to solve the problem of diverse shape variations between intra-class objects. Due to the superior performance achieved by SPD, other prior-based methods are also subsequently proposed. CR-Net\cite{20} designs a recurrent framework for iterative residual refinement to improve the shape prior-based deformation and coarse to fine object pose estimation. SGPA\cite{21} utilizes the structure similarity between the shape prior and the observed intra-class unknown object to dynamically adapt the shape prior. 6D-ViT\cite{22} introduces Pixelformer and Pointformer networks, based on the Transformer architecture, to extract more refined features of the observed objects. STG6D\cite{23} goes a step further and fuses the difference features between the shape prior and the observed objects, enabling more refined deformation. RBP-Pose\cite{24} designs a geometry-guided residual object bounding box projection network to solve the insufficient pose-sensitive feature extraction.
%SSP-Pose\cite{25} introduces an end-to-end method via integrating shape prior into a direct pose estimation network, avoiding the use of the undifferentiable Umeyama algorithm.
CATRE\cite{26} proposes a pose refinement method based on the alignment of the observed point cloud and the shape prior, which can be used to further refine the object pose estimated by the above methods. GeoReF \cite{25} builds upon CATRE \cite{26} to tackle the geometric variation issue by incorporating hybrid scope layers and learnable affine transformations. Although prior-based methods significantly improve accuracy, constructing CAD model libraries is cumbersome and time-consuming. 

\par Besides these prior-based methods, DualPoseNet\cite{27} introduces a dual pose encoder with refined learning of pose consistency and regresses object pose via two parallel pose decoders. FS-Net\cite{28} proposes a shape-based 3D graph convolution network and performs decoupled regression for translation, rotation, and size. GPV-Pose\cite{29} harnesses geometry-guided point-wise voting to enhance the learning of category-level pose-sensitive features. HS-Pose\cite{30} further proposes a hybrid scope feature extraction network, addressing the limitations associated with the size and translation invariant properties of 3D graph convolution. IST-Net\cite{31} explores the necessity of shape priors for category-level pose estimation and proposes an implicit space transformation-based prior-free method. VI-Net\cite{32} addresses the problem of poor rotation estimation by decoupling rotation into viewpoint and in-plane rotations. While these methods do not depend on shape priors, they still require large amounts of real-world annotated data for training, which hinders their practical applicability.

\par To address the problem of insufficient real-world training data, CPPF\cite{33} performs pose estimation in the wild by introducing a category-level point pair feature voting method. SAR-Net\cite{34} proposes to explore the shape alignment of each intra-class unknown object against its corresponding shape prior without using real-world training data. SSC6D\cite{35} proposes a self-supervised method using DeepSDF\cite{36} for deep implicit shape representation. UDA-COPE\cite{37} utilizes a teacher-student self-supervised learning framework to achieve domain adaptation. RePoNet\cite{38} proposes a self-supervised method based on pose and shape differentiable rendering. DPDN\cite{39} designs a parallel deep prior deformation-based domain generalization learning scheme. More recently, TTA-COPE\cite{40} introduces a test-time adaptation method, which initially trains the model on labeled synthetic data and subsequently utilizes the pretrained model for test-time adaptation in real-world data during inference. Nevertheless, the performance of these methods is limited by the huge domain gap between the rendered synthetic domain and the real world.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Fig2.jpg}
\vspace{-2em}
\caption{Some visualizations of the reverse diffusion process, representing the diffusion from Gaussian noise poses to objects poses in the observed scene.}
\label{Fig2}
\vspace{-1em}
\end{figure*}

\vspace{-1em}
\subsection{Diffusion Model-Based Methods}
\par More recently, diffusion models gained popularity in object pose estimation. In terms of instance-level methods, DiffusionReg \cite{DiffusionReg} proposes a point cloud registration framework leveraging the SE(3) diffusion model. This model gradually perturbs the optimal rigid transformation of a pair of point clouds by continuously injecting perturbations through the SE(3) forward diffusion process. The SE(3) reverse denoising process is then used to progressively denoise, approaching the optimal transformation for precise pose estimation. 6D-Diff \cite{6D-Diff} develops a diffusion-based framework that formulates 2D keypoint detection as a denoising process, enabling more accurate 2D-3D correspondences. As for category-level methods, GenPose \cite{42} introduces a score-based diffusion model to tackle the multi-hypothesis issue in symmetric objects and partial point clouds. Their approach first uses the score-based diffusion model to generate multiple pose candidates and then employs an energy-based diffusion model to eliminate abnormal poses. DiffusionNOCS \cite{DiffusionNOCS} first diffuses the NOCS map of the object using multi-modal input as a condition, and then uses an offline registration algorithm to align and solve the object pose.

\par In general, DiffusionReg \cite{DiffusionReg} and 6D-Diff \cite{6D-Diff} are instance-level methods. GenPose \cite{42} and DiffusionNOCS \cite{DiffusionNOCS} mainly focus on 6-DoF pose (excluding 3D size). Moreover, GenPose does not focus on solving the problem of domain generalization, and the diffusion target of DiffusionNOCS is the NOCS map. Different from the above methods, we aim to develop a category-level 9-DoF object pose estimation method suitable for real-world robotic applications using only rendered synthetic data for training. This approach faces two main challenges: \textbf{1)} The significant domain gap between synthetic and real-world data, which adversely affects the performance of conventional regression models. We propose using a denoising diffusion probabilistic model to frame object pose estimation as a generative process. The diffusion model performs extensive sampling on the Markov chain, which can effectively expand the distribution of the synthetic pose data, making the data distribution more uniform \cite{Ed-sam}, thus contributing to reducing the impact of the domain gap on the pose estimation model. \textbf{2)} Efficient pose estimation is crucial due to the limited computational resources available in robotics. To address this problem, we design a simple yet effective network structure, employing lightweight baseline networks (ResNet18 \cite{52} for RGB image and PointNet \cite{53} for point cloud) and using only global features as conditions. Additionally, given the sparsity of object pose data (only 15 values), our approach differs from dense diffusion tasks like image generation and can be efficient with fewer diffusion steps.

\vspace{-1em}
\subsection{Object Pose Estimation-Based Robotic Grasping}
To investigate the application of object pose estimation technology for robotic grasping, Zhang \emph{et al.}\cite{41} developed a practical robotic grasping system based on pose estimation with protective correction. GenPose \cite{42} proposes a score-based diffusion method for 6-DoF object pose estimation and explores its application for robotic manipulation. Liu \emph{et al.}\cite{43} introduced a difference-aware shape adjustment method based on fine segmentation. They also built a robotic grasping platform to verify the practical performance of the pose estimation. For applications where depth images are not practical, e.g., under strong or low light conditions or for transparent and reflective objects, Wolnitza \emph{et al.}\cite{44} proposed a monocular method for 3D object reconstruction and object pose estimation and used it for robotic grasping. BDR6D\cite{45} is another method that first predicts the depth information from a monocular image and then utilizes a bidirectional depth residual network for pose estimation. The proposed method is then deployed with a UR5 robot to perform grasping and manipulating tasks. More recently, STG6D\cite{23} develops a robotic continuous grasping system via a category-level method and proposes a pre-defined vector orientation-based grasping strategy. DGPF6D\cite{46} introduces a contrastive learning-guided shape prior-free category-level method for domain-generalized robotic picking. Yu \emph{et al.}\cite{47} proposed a self-supervised-based category-level object pose estimation method for robotic grasping. Chen \emph{et al.}\cite{48} explored a sim-to-real method by iterative self-training for robotic bin picking.

\par Side-stepping from the above object pose estimation methods for robotic grasping, this paper proposes a DDPM-based novel paradigm for domain-generalized category-level 9-DoF object pose estimation, redefining the pose estimation process from a generative perspective. Leveraging the latent generalization ability of the diffusion model, the proposed method achieves training solely with rendered synthetic images for generalization to real-world robotic grasping scenarios.

\vspace{-0.5em}