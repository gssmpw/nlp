@article{galic2021global,
  title={Global Process Mining Survey 2021: Delivering Value with Process Analytics-Adoption and Success Factors of Process Mining},
  author={Galic, Gabriela and Wolf, Marcel},
  journal={Technical report, Deloitte Global},
  year={2021}
}

@inproceedings{he2017drain,
  title={Drain: An online log parsing approach with fixed depth tree},
  author={He, Pinjia and Zhu, Jieming and Zheng, Zibin and Lyu, Michael R},
  booktitle={2017 IEEE international conference on web services (ICWS)},
  pages={33--40},
  year={2017},
  organization={IEEE}
}

@book{van2022process,
  title={Process mining handbook},
  author={van der Aalst, Wil MP and Carmona, Josep},
  year={2022},
  publisher={Springer Nature}
}

@inproceedings{van2015pm,
  title={PM: a process mining project methodology},
  author={Van Eck, Maikel L and Lu, Xixi and Leemans, Sander JJ and Van Der Aalst, Wil MP},
  booktitle={International conference on advanced information systems engineering},
  pages={297--313},
  year={2015},
  organization={Springer}
}

@misc{dotnet,
    author = {{.NET Platform}},
    title = {{.NET Platform}},
    howpublished = {\url{https://github.com/dotnet}},
    note = {Accessed: 2024-02-14}
}

@article{stepanov2024extracting,
  title={Extracting high-level activities from low-level program execution logs},
  author={Stepanov, Evgenii V and Mitsyuk, Alexey A},
  journal={Automated Software Engineering},
  volume={31},
  number={2},
  pages={41},
  year={2024},
  publisher={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{elnaggar2021prottrans,
  title={Prottrans: Toward understanding the language of life through self-supervised learning},
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={10},
  pages={7112--7127},
  year={2021},
  publisher={IEEE}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@article{iandola2020squeezebert,
  title={SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},
  author={Iandola, Forrest N and Shaw, Albert E and Krishna, Ravi and Keutzer, Kurt W},
  journal={arXiv preprint arXiv:2006.11316},
  year={2020}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@misc{adamw2024,
    author = {{AdamW}},
    title = {{AdamW}},
    howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html}},
    note = {Accessed: 2024-03-18}
}

@misc{procfiler2024,
    author = {{Procfiler}},
    title = {{Procfiler}},
    howpublished = {\url{https://github.com/PM-IDE/Procfiler}},
    note = {Accessed: 2024-02-14}
}

@inproceedings{nedelkoski2020self,
  title={Self-attentive classification-based anomaly detection in unstructured logs},
  author={Nedelkoski, Sasho and Bogatinovski, Jasmin and Acker, Alexander and Cardoso, Jorge and Kao, Odej},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={1196--1201},
  year={2020},
  organization={IEEE}
}

@article{huang2020hitanomaly,
  title={Hitanomaly: Hierarchical transformers for anomaly detection in system log},
  author={Huang, Shaohan and Liu, Yi and Fung, Carol and He, Rong and Zhao, Yining and Yang, Hailong and Luan, Zhongzhi},
  journal={IEEE transactions on network and service management},
  volume={17},
  number={4},
  pages={2064--2076},
  year={2020},
  publisher={IEEE}
}

@inproceedings{du2017deeplog,
  title={Deeplog: Anomaly detection and diagnosis from system logs through deep learning},
  author={Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={1285--1298},
  year={2017}
}

@inproceedings{guo2021logbert,
  title={Logbert: Log anomaly detection via bert},
  author={Guo, Haixuan and Yuan, Shuhan and Wu, Xintao},
  booktitle={2021 international joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@article{lee2023lanobert,
  title={Lanobert: System log anomaly detection based on bert masked language model},
  author={Lee, Yukyung and Kim, Jina and Kang, Pilsung},
  journal={Applied Soft Computing},
  volume={146},
  pages={110689},
  year={2023},
  publisher={Elsevier}
}