\section{Related Work}
\subsection{3D Face Reconstruction}
Reconstructing 3D faces from 2D input images has received widespread attention over the past few decades \cite{STATEOF3D}.
Model-free approaches \cite{retinaface, dou2017end, feng2018joint, wu2020unsupervised, ruan2021sadrnet} regress 3D vertices directly or optimize a Signed Distance Function \cite{SDF} for image fitting.
These techniques commonly require explicit 3D supervision during training, which limits their expressiveness due to inherent constraints in data creation and the differences between synthetic and real images \cite{dou2017end, zeng2019df2net}.

With the development of statistical face models 3DMM, many methods for estimating coefficients of these models have emerged, employing a fixed linear shape space in an analysis-by-synthesis manner, such as BFM \cite{BFM}, FaceWarehouse \cite{Facewarehouse}, FLAME \cite{li2017learning}, 3DDFA-v3~\cite{wang20243d}, \etc. Existing methods can be generally categorized into optimization-based \cite{aldrian2012inverse, bas2017fitting} and learning-based approaches \cite{ExpNet, inversefacenet, zielonka2022towards}.
Optimization-based methods require iterative optimization for each new image, which is time-consuming. With the rise of deep learning, learning-based methods have become mainstream, prompting many works to leverage various supervisory signals from different image domains, such as 2D keypoints \cite{facescape, Deep3DReconstruct, shang2020self} and 2D face contours \cite{counter} for self-supervised training. However, for commonly used 2D key points, the sparsity and limited accuracy of the predicted points result in constrained supervision, particularly when facial expressions and head poses are complex. This often leads to misalignment between the 3D mesh and the input image. Photometric constraints are especially effective for image-domain data; however, they are vulnerable to alignment errors and rely on the quality of the rendered image.

To enable supervisory signals from the image domain to assist in reconstructing accurate 3D meshes, it is essential to obtain a precise representation of facial appearance or texture. \cite{lin2020towards} enhances the initial 3DMM texture during the estimator training process, while \cite{booth20183d} utilizes a 3DMM for shape estimation, supplemented by a PCA appearance model learned from in-the-wild images. \cite{gecer2019ganfit} expands on this concept by employing a GAN to model facial appearance more effectively. Additionally, \cite{tewari2021learning, tran2019towards} learns nonlinear models of shape and expression during the self-supervised training of a estimator.

Most of these studies generate renderings using linear statistical models and Lambertian reflectance \cite{lambertian}. In contrast, SMIRK introduces an innovative neural rendering module that tries addresses the domain gap between the input and the synthesized output. By reducing this discrepancy, SMIRK enhances the supervision signal within an analysis-by-synthesis framework. 
However, SMIRK heavily depends on randomly sampling some pixels from the source image. Although this approach ensures that facial structure information is not leaked, the supervision signal provided by the sampled pixels is not strong enough, resulting in significant differences between the reconstructed 2D image and the source image. The intermediate generated mesh also deteriorates accordingly, failing to align perfectly with the real image. We propose to use implicit appearance token that decouple from input image, representing semantic information such as facial texture and details, to provide stronger guidance, enabling the generalization of more accurate 2D images and corresponding meshes.


\subsection{Disentangled Face Representation Learning}
The development of disentangled facial representation learning  \cite{Infogan,beta-vae,orthogonal} has greatly benefited from advances in Generative Adversarial Networks (GANs) and self-supervised learning, particularly in the areas of image generation and facial editing. Early studies primarily focused on separating facial geometric structures from texture features. For example, \cite{liu2015deep, LIA, face2facevid} used an autoencoder model to disentangle identity and motion. While the 3D Morphable Model (3DMM) excels in modeling facial geometry \cite{PIRenderer}, its texture modeling capabilities are limited, resulting in generated facial images that lack realistic texture details. This limitation arises mainly because the linear texture model of 3DMM is overly simplified. To address this, \cite{Mofa, StyleHEAT} combined 3DMM with GANs to generate high-quality textures, overcoming the shortcomings of traditional 3DMM. \cite{Deep3DReconstruct} further improved the detail and visual quality of the generated images by disentangling facial expressions, lighting, and textures. However, these methods face a dilemma: non-3DMM-based approaches struggle to model 3D-level information, while the facial textures included in 3DMM do not correspond to human perception. In this paper, we use 3DMM to obtain facial geometry information and facial tokenizer to derive high-level texture representations that align with human perception, achieving a balance between both approaches.


\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{Images/main_fig_v2.pdf}
\vspace{-1.5em}
\caption{The framework of our pipeline.}
\vspace{-.5em}
\label{fig:overal_arch}
\end{figure}