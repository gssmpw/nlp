
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{bbding}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage[accsupp]{axessibility} 
\newcommand{\lj}[1]{\textcolor{blue}{#1}}
% \newcommand{\rev}[1]{\textcolor{blue}{#1}}
\newcommand{\rev}[1]{\textcolor{black}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\usepackage{xspace}
\newcommand{\ie}{\textit{i.e.,}\xspace}
\newcommand{\eg}{\textit{e.g.,}\xspace}

% %加粗
% \usepackage{bm}

% %% lihao package
% \usepackage{graphicx} 


\title{TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy

\author{Yunfei Liu\textsuperscript{1}\thanks{Equal contribution.},\quad Lei Zhu\textsuperscript{1,2}\footnotemark[1]~~\thanks{The work was done during Lei\&Ailing’s internship at the International Digital Economy Academy.}~, \quad Lijian Lin\textsuperscript{1},\quad Ye Zhu\textsuperscript{1},\quad Ailing Zhang\textsuperscript{1,2}\footnotemark[2]~,\quad Yu Li\textsuperscript{1,}\thanks{Corresponding author.} \\
\textsuperscript{1}International Digital Economy Academy \\ \textsuperscript{2}Peking University, Shenzhen Graduate School\\
\texttt{\{liuyunfei,zhulei,linlijian,zhuye,zhangailing,liyu\}@idea.edu.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\etc}{\textit{etc.}}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\yf}[1]{\textcolor[rgb]{0.8,0.0,0.1}{#1}}   % Modification by Yunfei

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{Images/teaser_fig1_v2.pdf}
  \caption{Given the input image (a), TEASER predicts hybrid parameters for 3D facial reconstruction. The explicit parameters can be used to reconstruct precise 3D facial expressions (b). The implicit parameters (\ie appearance token) guides high-fidelity face images generation (c). TEASER can be easily adapted to various applications, \eg, expression modification, as shown in the top row of (d), or changing facial appearance through token swapping, as shown in the bottom row of (d).}
  \label{teaser}
\end{figure}

\begin{abstract}
\vspace{-3mm}
3D facial reconstruction from a single in-the-wild image is a crucial task in human-centered computer vision tasks. 
While existing methods can recover accurate facial shapes, there remains significant space for improvement in fine-grained expression capture. 
Current approaches struggle with irregular mouth shapes, exaggerated expressions, and asymmetrical facial movements. 
We present TEASER (Token EnhAnced Spatial modeling for Expressions Reconstruction), which addresses these challenges and enhances 3D facial geometry performance.
TEASER tackles two main limitations of existing methods: insufficient photometric loss for self-reconstruction and inaccurate localization of subtle expressions. 
We introduce a multi-scale tokenizer to extract facial appearance information. Combined with a neural renderer, these tokens provide precise geometric guidance for expression reconstruction.
Furthermore, TEASER incorporates a pose-dependent landmark loss to further improve geometric performance.
Our approach not only significantly enhances expression reconstruction quality but also offers interpretable tokens suitable for various downstream applications, such as photorealistic facial video driving, expression transfer, and identity swapping.
Quantitative and qualitative experimental results across multiple datasets demonstrate that TEASER achieves state-of-the-art performance in precise expression reconstruction. 
Code and demos are available at \color{blue}{\url{https://tinyurl.com/TEASER-project}}.
\end{abstract}


\section{Introduction}

3D facial reconstruction from a single image is a key component in several human-centered applications, including digital avatar creation, immersive telecommunication in AR/VR/XR, social media, \etc Generally, these methods estimate the corresponding coefficients of the 3D Morphable Model (3DMM)~\cite{3DMM} for precise facial feature positioning and expression reconstruction. Due to the scarcity of large-scale paired data for single image-to-3D face mapping, many optimization-based methods~\cite{bas2017fitting,romdhani2005estimating} or deep learning-based regression approaches~\cite{ExpNet, EMOCA, Deep3DReconstruct, Complete3DMH, Mofa} commonly rely on self-supervised learning, adhering to the analysis-by-synthesis training paradigm.

Recent analysis-by-synthesis methods typically begin by using a network to estimate facial geometry, facial texture and environmental illumination from the input image. Then a differentiable renderer is employed to render a pseudo-image based on the extracted information. The loss between this pseudo-image and the input image is calculated as a supervision signal, allowing the network to be trained without requiring 3D data. Although these methods have achieved good shape reconstruction performance, they tend to fail in capturing facial expressions under certain scenarios, \eg, the input image contains extreme facial movements and expressions, including asymmetrical, exaggerated, or subtle motions perceptible to the human eye. The reasons are: 1) The differentiable renderer requires facial texture and environmental illumination. However, since the texture color on the face is often coupled with environmental illumination, estimating these two parameters simultaneously is challenging. 2) The pseudo-image generated by differentiable renderer is often overly smoothed, resulting in the loss of important facial details such as texture and expressions. And 3) the differentiable renderer struggle to model complex facial scenarios such as shadows, occlusions, and highly reflective facial surfaces, limiting the effectiveness of photometric loss.
Some methods have attempted to constrain the 3DMM parameter search space by incorporating networks from other facial tasks, such as lip reading~\cite{filntisis2023spectre}, emotion estimation~\cite{EMOCA}, face recognition~\cite{gecer2019ganfit}, and GAN discriminators~\cite{otto2023perceptual}, \etc However, these approaches provide indirect supervision for expression reconstruction and require a delicate balancing of weights among different loss components.

Recently, SMIRK~\cite{SMIRK} introduced a UNet-based image generator to upgrade the traditional differentiable renderer. By circumventing the limitations of differentiable rendering, SMIRK improves the perceptual quality of reconstructed facial expressions. However, it heavily relies on randomly sampled pixels from the input images, resulting in low-quality outputs characterized by excessive smoothing and noticeable artifacts. Consequently, a considerable domain gap remains between the input and the synthesized images, as illustrated at $\text{4}^{th}$ column in Fig.~\ref{teaser}.

Inspired by this approach, we propose Token EnhAnced Spatial modeling for Expressions Reconstruction (TEASER). Our method first extracts a hybrid representation combining explicit facial parameters (3DMM coefficients) and implicit appearance token. This token then guide a newly designed neural renderer to generate more faithful face images.
Our novel neural renderer addresses issues in the neural renderer from SMIRK, thereby providing more accurate photometric loss and precise expression reconstruction. Furthermore, the extracted tokens not only enhance the stability of the neural renderer but also exhibit good interpretability, enabling TEASER to be applied to various downstream tasks including face editing, face animation, and identity swapping.

Specifically, TEASER introduces a multi-scale tokenizer that extracts complex facial representations from the input image, including self-occlusion shadows, glasses, lighting, and various skin tone, \etc, compressing them into a compact representation called \textit{Token}. Subsequently, we design a Token-guided facial neural renderer that fuses Tokens at different scales to reconstruct high-fidelity input images based on facial mesh geometry.
To train the tokenizer in an self-supervised manner, we propose a novel token cycle loss. To further improve the generalization of the method in subtle facial expression scenarios, we introduce a pose-dependent landmark loss and a region loss to improve facial expression details and face reconstruction performance in gaze/mouth areas, respectively.

In summary, the main contributions of this paper are as follows: 1) We propose TEASER, a novel approach that achieves more accurate facial expression reconstruction by predicting a hybrid representation of faces from a single image. 2) We design a multi-scale facial appearance tokenizer and introduce a token-guided neural renderer to generate high-fidelity facial images. The extracted token is interpretable and highly disentangled, enabling various downstream applications. 3) We develop a token cycle constraint for self-supervised training of the tokenizer. Additionally, we introduce pose-dependent landmark loss and region loss to further enhance the quality of expression reconstruction and facial image reconstruction. 4) TEASER achieves the state-of-the-art performance including quantitative and qualitative results on multiple benchmark datasets. Rigorous experiments also demonstrate the efficiency of different components in TEASER. Furthermore, we showcase impressive results in various face editing and animation tasks.

\section{Related Work}
\subsection{3D Face Reconstruction}
Reconstructing 3D faces from 2D input images has received widespread attention over the past few decades \cite{STATEOF3D}.
Model-free approaches \cite{retinaface, dou2017end, feng2018joint, wu2020unsupervised, ruan2021sadrnet} regress 3D vertices directly or optimize a Signed Distance Function \cite{SDF} for image fitting.
These techniques commonly require explicit 3D supervision during training, which limits their expressiveness due to inherent constraints in data creation and the differences between synthetic and real images \cite{dou2017end, zeng2019df2net}.

With the development of statistical face models 3DMM, many methods for estimating coefficients of these models have emerged, employing a fixed linear shape space in an analysis-by-synthesis manner, such as BFM \cite{BFM}, FaceWarehouse \cite{Facewarehouse}, FLAME \cite{li2017learning}, 3DDFA-v3~\cite{wang20243d}, \etc. Existing methods can be generally categorized into optimization-based \cite{aldrian2012inverse, bas2017fitting} and learning-based approaches \cite{ExpNet, inversefacenet, zielonka2022towards}.
Optimization-based methods require iterative optimization for each new image, which is time-consuming. With the rise of deep learning, learning-based methods have become mainstream, prompting many works to leverage various supervisory signals from different image domains, such as 2D keypoints \cite{facescape, Deep3DReconstruct, shang2020self} and 2D face contours \cite{counter} for self-supervised training. However, for commonly used 2D key points, the sparsity and limited accuracy of the predicted points result in constrained supervision, particularly when facial expressions and head poses are complex. This often leads to misalignment between the 3D mesh and the input image. Photometric constraints are especially effective for image-domain data; however, they are vulnerable to alignment errors and rely on the quality of the rendered image.

To enable supervisory signals from the image domain to assist in reconstructing accurate 3D meshes, it is essential to obtain a precise representation of facial appearance or texture. \cite{lin2020towards} enhances the initial 3DMM texture during the estimator training process, while \cite{booth20183d} utilizes a 3DMM for shape estimation, supplemented by a PCA appearance model learned from in-the-wild images. \cite{gecer2019ganfit} expands on this concept by employing a GAN to model facial appearance more effectively. Additionally, \cite{tewari2021learning, tran2019towards} learns nonlinear models of shape and expression during the self-supervised training of a estimator.

Most of these studies generate renderings using linear statistical models and Lambertian reflectance \cite{lambertian}. In contrast, SMIRK introduces an innovative neural rendering module that tries addresses the domain gap between the input and the synthesized output. By reducing this discrepancy, SMIRK enhances the supervision signal within an analysis-by-synthesis framework. 
However, SMIRK heavily depends on randomly sampling some pixels from the source image. Although this approach ensures that facial structure information is not leaked, the supervision signal provided by the sampled pixels is not strong enough, resulting in significant differences between the reconstructed 2D image and the source image. The intermediate generated mesh also deteriorates accordingly, failing to align perfectly with the real image. We propose to use implicit appearance token that decouple from input image, representing semantic information such as facial texture and details, to provide stronger guidance, enabling the generalization of more accurate 2D images and corresponding meshes.


\subsection{Disentangled Face Representation Learning}
The development of disentangled facial representation learning  \cite{Infogan,beta-vae,orthogonal} has greatly benefited from advances in Generative Adversarial Networks (GANs) and self-supervised learning, particularly in the areas of image generation and facial editing. Early studies primarily focused on separating facial geometric structures from texture features. For example, \cite{liu2015deep, LIA, face2facevid} used an autoencoder model to disentangle identity and motion. While the 3D Morphable Model (3DMM) excels in modeling facial geometry \cite{PIRenderer}, its texture modeling capabilities are limited, resulting in generated facial images that lack realistic texture details. This limitation arises mainly because the linear texture model of 3DMM is overly simplified. To address this, \cite{Mofa, StyleHEAT} combined 3DMM with GANs to generate high-quality textures, overcoming the shortcomings of traditional 3DMM. \cite{Deep3DReconstruct} further improved the detail and visual quality of the generated images by disentangling facial expressions, lighting, and textures. However, these methods face a dilemma: non-3DMM-based approaches struggle to model 3D-level information, while the facial textures included in 3DMM do not correspond to human perception. In this paper, we use 3DMM to obtain facial geometry information and facial tokenizer to derive high-level texture representations that align with human perception, achieving a balance between both approaches.


\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{Images/main_fig_v2.pdf}
\vspace{-1.5em}
\caption{The framework of our pipeline.}
\vspace{-.5em}
\label{fig:overal_arch}
\end{figure}

\section{Method}
\label{sec:method}
This paper introduces TEASER, a novel approach that aims at extracting accurate hybrid representations from images, including explicit FLAME parameters for precise 3D facial expression geometry modeling and implicit tokens for capturing complex facial appearances. The overall network framework, as illustrated in Fig.~\ref{fig:overal_arch}, comprises three main components: 1) a Multi-scale Facial Appearance Tokenizer (MFAT) for capturing facial appearance token at different scales, 2) a Geometry Extractor that leverages existing techniques to extract parameters for FLAME modeling 3D geometric of facial expressions, and 3) a Token-guided Face Synthesizer (TFS) that generates facial images that are both aligned with the mesh image and rich in detail and fidelity, based on explicit facial mesh images and implicit tokens.
We begin by introducing preliminaries and notation definitions in Sec.~\ref{sec:preliminary}, followed by detailed descriptions of network modules, loss functions in subsequent subsections.

\subsection{Preliminary}\label{sec:preliminary}
Given an input image $I$, many existing methods employ FLAME to model the expressive 3D facial geometry. 
FLAME generates 3D vertices $V \in \mathbb{R}^{5023\times 3}$ using shape coefficients $\beta \in \mathbb{R}^{300}$, expression coefficients $\psi \in \mathbb{R}^{50}$, jaw rotation $\theta_j \in \mathbb{R}^3$, head pose $\theta_h \in \mathbb{R}^3$, eye closure blendshapes $b \in \mathbb{R}^2$, and camera parameters $c \in \mathbb{R}^3$. In addition to geometry-related parameters, these methods typically predict appearance-related coefficients for facial modeling, including texture parameters $\gamma \in \mathbb{R}^{50}$ and scene lighting using Spherical Harmonics (SH) $l \in \mathbb{R}^{9\times 3}$. $\gamma$ is used to generated the albedo $b \in \mathbb{R}^{5023\times 3}$ for each vertex in $V$ through a FLAME-Tex model.
To ensure the accuracy of extracted parameters, these method introduce self-supervised constraints between $I$ and reconstructed image $\hat{I}$, which is generated by a differentiable renderer based on Lambertian reflectance. Mathematically, the overall process is

\begin{equation*}
\{\beta, \psi, \theta_j, \theta_h, b, c, \gamma, l\} = \mathcal{E}(I)\text{,} \hspace*{2mm} V = \text{FLAME}(\beta, \psi, \theta_j, \theta_h, b)\text{,}
\tag{
{\begin{minipage}[b][0pt][b]{.56em}\begin{equation}\label{eq:encode}\end{equation}\end{minipage}},\,\hspace*{2mm}%
{\begin{minipage}[b][0pt][b]{.50em}\begin{equation}\label{eq:flame}\end{equation}\end{minipage}}\hspace*{-1.1mm}
}
\end{equation*}

\begin{equation*}
C = \text{FLAME-Tex}(\gamma)\text{,} \hspace*{2mm} \hat{I} = \mathcal{R}(V, C, l, c)\text{.}
\tag{
{\begin{minipage}[b][0pt][b]{.56em}\begin{equation}\label{eq:flametex}\end{equation}\end{minipage}},\,\hspace*{2mm}%
{\begin{minipage}[b][0pt][b]{.50em}\begin{equation}\label{eq:diffrender}\end{equation}\end{minipage}}\hspace*{-1.1mm}
}
\end{equation*}

Unlike previous methods that predict facial texture PCA coefficients $\gamma$ and simplified environmental lighting $l$, TEASER introduces an implicit latent code, namely token $T$, to represent complex facial appearance. Simultaneously, TEASER incorporates a novel neural renderer $\mathcal{R}^*$ that takes both explicitly rendered mesh image and implicit token, generating high-fidelity facial images that bridge the domain gap between the input and synthesized output $\hat{I}$. Consequently, we re-write Eqn.~\ref{eq:encode}, Eqn.~\ref{eq:flametex} and Eqn.\ref{eq:diffrender} as follows:

\begin{equation}\label{eq:encode_new}
    \{\beta, \Psi, \theta, T\} = \mathcal{E}(I),
\end{equation}
\begin{equation}\label{eq:diffrender_mesh}
    \hat{I} = \mathcal{R}^*(I_v, I_b, T) \text{, where} \hspace*{2mm} I_v = \mathcal{R}(V, C_0, l_0, c).
\end{equation}
Here, for brevity, we combine jaw pose and expression parameters into a joint expression representation $\Psi = \{\psi, \theta_j, b\}$. We also combine camera motion and head pose to global transformation parameters, \ie $\theta = \{c, \theta_h\}$.
$C_0$ and $l_0$ are predefined vertices color and environmental lighting, respectively. $I_b$ is the background image where the face area is masked out.
Following previous work, we treat head pose $\theta_h$ as a rigid transformation and use a weak perspective camera model.

\subsection{Hybrid Facial Representation}

\textbf{Multi-scale Appearance Tokenizer.}
We design a Multi-scale Appearance Tokenizer (MFAT, $\mathcal{E}_T$) to capture appearance features with rich facial appearance information. Given an input image $I$, we first employ a multi-layer CNN-based image encoder $E$ to extract image features. The extracted features at different layers contain varying levels of semantic information. Thus, we propose to use all these features to capture both high-dimensional semantics and fine-grained texture details for a more comprehensive facial representation. Specifically, we adopt an average pooling layer $\mathcal{P}$ followed by a fully connected layer $\mathcal{F}$ after each layer to project these features into a unified space. Finally, we concatenate these multi-scale features together as a comprehensive facial appearance token $T$. The computational process can be formulated as follows:
\begin{equation} \label{eq:encoder_token}
    T = \mathcal{E}_T(I) = z_1 \odot \cdots \odot z_K,
\end{equation}
where $\odot$ is channel-wise concatenation, $z_i = \mathcal{F}_i(\mathcal{P}(x_i)), i \in \{1, \cdots, K\}$. $x_i$ is the feature map from $i$-th stage in image encoder $E$. By doing this, our appearance token $T$ incorporates multi-level information from the input image, resulting in a more detailed and precise facial representation.



\textbf{Facial Geometry Extractor.}
To extract the geometric information from the input image, we follow \cite{SMIRK} and use three identical encoders $\mathcal{E}_\beta, \mathcal{E}_\Psi, \mathcal{E}_\theta$ to predict the face shape $\beta$, expression $\Psi$, and transpose parameters $\theta$, respectively. Formally,
\begin{equation} \label{eq:encoder_geo}
\beta =  \mathcal{E}_{\beta}(I),  \quad \Psi =  \mathcal{E}_{\Psi}(I), \quad \theta =  \mathcal{E}_{\theta}(I).
\end{equation}


\subsection{Token guided Face Synthesizer}


We argue that an effective neural renderer should possess two key characteristics: a) The generated facial images must maintain strict alignment with the facial geometry (\textit{i.e}., the rendered mesh image). This alignment provides accurate spatial guidance for expression geometry reconstruction when computing self-supervised losses. b) The generated faces must be of high fidelity, minimizing the gap with the input image to reduce errors associated with photometric loss. To achieve these objectives, we introduce a Token-guided Face Synthesizer (TFS). Our innovation lies in implementing a facial neural rendering network controlled by hybrid information, combining explicit face geometry and implicit appearance tokens.
Specifically, our input contains three parts: 1) face geometric information $I_v$, which is generated from differentiable rasterization step on the reconstructed FLAME vertices $V$, as provided at Eqn.~\ref{eq:diffrender_mesh}. 2) Facial appearance token $T$, which is extracted from $\mathcal{E}_T$. $T$ implicitly contains skin colors, face texture, complex environment illumination, \etc And 3) Non-face background $I_b$. We mask out the face part to obtain the background region.
Based on these inputs, the generation process of our TFS can be represented as:
\begin{equation}
\hat{I} = \text{TFS}(I_v \odot I_b, T).
\end{equation}
$\hat{I}$ represents the generated image, which follows the geometric information of $I_v$, incorporates the appearance of $T$, and uses $I_b$ as the background.
TFS is built on a U-Net structure for multi-scale feature extraction, which firstly takes face geometric $I_v$ and the non-face background $I_b$ as input. 
As for the appearance token $T$, we adopt two techniques to incorporate $T$ into TPS to guide the face generation process.
Initially, we adopt adaptive instance normalization (AdaIN \cite{ADAIN}) to modulate the generation with $T$, which provides global style guidance to TPS. To better inject high-frequency details, inspired by ControlNet~\cite{ControlNet}, we create a new token decoder $D$ by referring to the decoder, where each block is followed by a zero-convolution layer. Each feature map extracted by each block of $D$ is added to the features from the UNet decoder. Combining AdaIN and token decoder $D$, TPS effectively captures both global style information and detailed high-frequency features, leading to more refined face generation results with rich facial details.
For the detailed network architecture please refer to the supplementary materials.


\subsection{Loss functions}
To reconstruct 3D facial mesh from image $I$, we design a learning framework to minimize the total loss $\mathcal{L}$ as follows:
\begin{equation} \label{loss_total}
    \mathcal{L} = \lambda_{ec} \mathcal{L}_{ec} + \lambda_{lmk} \mathcal{L}_{lmk} + \lambda_{tc} \mathcal{L}_{tc} + \lambda_{pdl} \mathcal{L}_{pdl} + \lambda_{rg} \mathcal{L}_{rg} + \lambda_{ic} \mathcal{L}_{ic},
\end{equation}
where $\mathcal{L}_{ec}$ is the augmented expression consistency loss, aiming to reduce over-compensation errors and promoting diverse expressions, we borrow it from~\cite{SMIRK}. $\mathcal{L}_{lmk}$ is the landmark loss, which is inherit from~\cite{MICA}.

\textbf{Token Consistency Loss.}
Due to the lack of ground-truth tokens, we adopt a token consistency loss $\mathcal{L}_{tc}$ to conduct self-supervised training in a cycle training manner. Given an input image $I$, we first extract its appearance token $T$ and geometric parameters $\beta, \Psi, \theta$. To enhance the stability of tokens across diverse expressions, we propose to augment the expression parameter $\Psi$ to $\Psi_{aug}$ by randomly changing certain values. The augmented $\Psi_{aug}$ and the original facial features are fed into our TFS to generate face images $\hat{I}_{aug}$ with different expressions to $I$. Finally, the $\mathcal{L}_{tc}$ computes the mean-squared error between the original Token $T$ with the token predicated on $\hat{I}_{aug}$: 
\begin{equation} \label{eq:loss_token_cc}
    \mathcal{L}_{tc}=\sum_{i=1}^{N} \left\| \mathcal{E}_T(\text{TFS}(\mathcal{R}(V^{'}_i, C_0, l_0, c)\odot I_b, T)) - T \right\|_2,
\end{equation}
where $V^{'}_i=\text{FLAME}(\beta, \Psi_{aug}, \theta)$, $N$ denotes the number of times we randomly augment $\Psi_{aug}$. This loss guarantees the predicted token to be consistent under different expressions.


\textbf{Pose-dependent Landmark Loss.}
We observed that commonly used facial landmark detection methods~\cite{DECA,MICA} in previous approaches often yield inaccurate results, particularly in distinguishing the boundary between the lower lip and lower teeth, or when dealing with complex expressions. To address this, we employ a state-of-the-art 2D facial landmark detector~\cite{} that accurately captures facial features under challenging conditions, including complex expressions, large head poses, and asymmetrical mouth shapes. 

To tackle the issue of non-correspondence between 2D and 3D cheek contour landmarks due to pose variations, we deviate from conventional dynamic landmark marching~\cite{zhu2015high}. Instead, we introduce a novel pose-dependent landmark loss that adapts to the specific head orientation $\theta$, ensuring more robust and accurate facial reconstruction across diverse poses.
\begin{equation} \label{eq:loss_pdl}
    \mathcal{L}_{pdl} = \left\|M_L(\theta_y) L - M_L(\theta_y) L_v \right\|_2,
\end{equation}
where $L \in \mathbb{R}^{203\times 2}$ and $L_v \in \mathbb{R}^{203\times 3}$ are 2D landmarks detected from $I$ and 3D landmark selected from face mesh $V$, respectively. $M_L$ is a pose-dependent mask, which is defined as 
\begin{equation}
    M_L(\theta_y) = \mathbbm{1}_{\theta_y < -\epsilon}M^{left}_L + \mathbbm{1}_{\theta_y > \epsilon}M^{right}_L + \mathbbm{1}_{-\epsilon \leq \theta_y \leq \epsilon}M^{front}_L,
\end{equation}
where $\mathbbm{1}$ is indicator function, $\theta_y$ denotes the yaw angle of head pose, $\epsilon$ is a tunable parameter that determines the range of frontal face. \rev{We empirically set $\epsilon=0.05$ in our experiments.} $M^{left}_L, M^{right}_L, M^{front}_L$ are masks for visible landmarks under different head poses.

\textbf{Region Loss.}
Previous neural facial renderer~\cite{SMIRK} tends to generate faces with misaligned gaze information, lack of wrinkle details, and color shifts in the lips. Although these complex representations can be naturally addressed through our extracted implicit appearance tokens, the generated faces still exhibit minor discrepancies compared to the input images. We attribute this issue to the small size of these perceptually sensitive regions, making it challenging for the neural renderer to learn. To address this, we employ a masking strategy to calculate the local loss in the mouth and eye areas. Specifically, we parse facial landmarks to obtain masks for the eye region ($M_e$) and mouth region ($M_m$).The formula for this landmark-guided region loss is as follows:
\begin{equation} \label{eq:loss_region}
\mathcal{L}_{rg} = \left\| (M_m + M_e) \cdot I - (M_m + M_e) \cdot \hat{I} \right\|_2.
\end{equation}

\textbf{Photometric Loss and Perceptual Loss.}
Similar to previous approaches~\cite{Deep3DReconstruct,DECA}, we apply common image-level supervision, including photometric loss $\mathcal{L}_{pho}$ and VGG perceptual loss $\mathcal{L}_{pho}$, to ensure consistency between the generated image $\hat{I}$ and the input image $I$:
\begin{equation}
\mathcal{L}_{ic} = \lambda_{pho}\mathcal{L}_{pho} +  \lambda_{per}\mathcal{L}_{per}.
\end{equation}


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Images/case1-v4.pdf}
\vspace{-1.5em}
\caption{Visual comparison of 3D face reconstruction with SOTA methods.}
\vspace{-.5em}
\label{fig:3d_mesh_cmp}
\end{figure}


\section{Experiment}
\label{sec:exp}

\subsection{Experimental setup}

\textbf{Training Datasets.} 
We use the following datasets for training: FFHQ \cite{FFHQ}, CelebA \cite{CELEBA}, and LRS3 \cite{LRS3}. Since LRS3 is video dataset, we randomly sample images from each video frames from during training. 
We crop the frames based on mediapipe landmark~\cite{} and resize them to the resolution of $224 \times 224$. \rev{We follow~\cite{SMIRK} and separate different videos for training and testing.}

\textbf{Implementation Details.} 
Our model is implemented in PyTorch~\cite{imambi2021pytorch}. All models are trained on one NVIDIA RTX 3090 GPU and the batchsize is 16.
We use MobileNet-V3~\cite{MOBILENET} as the image encoder in MFAT for the balance between performance and efficiency.
We use a learning rate of 0.001 to train our model with the Adam optimizer.
We set the number of scales in MFAT to 4 and set the dimension of all tokens to 256.
In our loss function, we set $\lambda_{ec} = 1.0$, $\lambda_{lmk} = 100$, $\lambda_{tc} = 5.0$, $\lambda_{rg} = 10.0$, $\lambda_{ic} = 10.0$, $\lambda_{pdl} = 500.0$, $\lambda_{pho} = 1.0$,  $\lambda_{per} = 1.0$.
We utilize a pre-trained geometry encoder from MICA~\cite{MICA}. During the training process, $\mathcal{E}_\beta$ and $\mathcal{E}_\theta$ remain frozen to maintain stability and leverage the robustness of the pre-trained components. We alternately train the encoders (including the geometry encoder and MFAT) and TFS during the training process. This design helps reduce the effect of the neural renderer compensating for the encoders. For more training details please refer to the supplemental materials.


\textbf{SOTA Methods.}
We compare with the following publicly available methods, including DECA~\cite{DECA}, EMOCA~\cite{EMOCA}, and SMIRK~\cite{SMIRK}, which use the FLAME model~\cite{FLAME}. We also involves the comparisons with Deep3DFace~\cite{Deep3DReconstruct} and 3DDFA-v3~\cite{wang20243d}, which use BFM model~\cite{BFM}.

\textbf{Evaluation Metrics.} 
Recent methods~~\cite{EMOCA,SMIRK} have reported that using 3D facial geometry to measure the accuracy of facial expression reconstruction is ill-posed. 
Therefore, we evaluate the quality of final reconstructed 2D images and videos. The insight is that if the quality of these reconstructed 2D images is high, then in our alternative training mode, the intermediate generated mesh will also align more closely with the input image. We use the Peak Signal-to-Noise Ratio (\textbf{PSNR}), Fréchet Inception Distance (\textbf{FID}) \cite{FID}, cosine similarity of identity (\textbf{CSIM}), Average Expression Distance (\textbf{AED}), and Average Pose Distance (\textbf{APD}) to evaluate image-level reconstruction performance. We use warp-error \cite{WARPINGERROR} and calcute flicker level between adjacent frames to evaluate video-level reconstruction. For more details please refer to the supplementary materials.


\begin{table*}[htbp]
\begin{center}
    \vspace{-5mm}
    \caption{Quantitative comparisons of the reconstructed image on LRS3 and HDTF test dataset. }
    \label{tab:data_cmp}
    \scalebox{0.9}[0.9]{
    \begin{tabular}{cccccccc|cc}
        \toprule[1.1pt]
        
        & \multicolumn{ 1}{l}{} & \multicolumn{6}{c}{Images} & \multicolumn{2}{c}{Videos} \\
        & \multicolumn{ 1}{l}{Methods} & LPIPS $\downarrow$ & FID $\downarrow$ & CSIM $\uparrow$ & PSNR $\uparrow$ & AED $\downarrow$ & APD $\uparrow$ & warp-error$\downarrow$ & flicker $\downarrow$  \\
        \hline
        \specialrule{0em}{1pt}{1pt}
        
        \multirow{3}{*}{\rotatebox{90}{LRS3}}
        & SMIRK & $0.109$ & $25.39$ & $0.729$ & $29.14$ & $0.147$ & $0.056$ & $1.245$ & $1.588$ \\
        & 3DDFA-V3 & $0.181$ & $56.18$ & $0.604$ & $25.78$ & $0.141$ & $0.054$ & $0.903$ & $1.372$ \\
            & Ours & $\textbf{0.077}$ & $\textbf{19.41}$ & $\textbf{0.804}$ & $\textbf{30.67}$ & $\textbf{0.114}$ & $\textbf{0.044}$ & $\textbf{0.801}$ & $\textbf{1.167}$ \\
            
        \hline
        \specialrule{0em}{1pt}{1pt}
        \multirow{3}{*}{\rotatebox{90}{HDTF}}

            & SMIRK & $0.114$ & $35.27$ & $0.732$ & $27.39$ & $0.138$ & $0.125$ & $1.958$ & $1.484$  \\
            & 3DDFA-V3  & $0.399$ & $104.90$ & $0.597$ & $12.87$ & $0.144$ & $0.068$ & $1.736$ & $0.870$  \\
            & Ours & $\textbf{0.081}$ & $\textbf{30.19}$ & $\textbf{0.826}$ & $\textbf{28.35}$ & $\textbf{0.036}$ & $\textbf{0.029}$ & $\textbf{1.422}$ & $\textbf{0.662}$ \\			
        \bottomrule[1.1pt]
    \end{tabular}  
    }
\end{center}
\end{table*}


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Images/mesh-recon-cmp.pdf}
\vspace{-1.5em}
\caption{Visual comparison of estimated expression and its corresponding reconstructed images.}
\vspace{-.5em}
\label{fig:2d_image_rec}
\end{figure}


\subsection{Qualitative and Quantitative Results}

\subsubsection{Quantitative Comparison}
We evaluate the image quality of our reconstructed images in two test datasets to validate the effectiveness of our method and the result is shown in Tab.\ref{tab:data_cmp}. Compared with 3DDFA-v3 and SMIRK across two benchmark datasets, our method demonstrates a significant advantage over the two methods.
Specifically, higher LPIPS and lower FID values indicate that the perceptual difference between the 2D images reconstructed by our method and the input images is smaller. TEASER gets the highest CSIM value, suggesting that the extracted tokens contain rich facial appearance information. The better AED and APD results indicate that the expressions and head poses in our reconstructed images are more accurate.
In the evaluation of videos, the reduction in warp-error and flicker values demonstrates that our generated videos exhibit better stability. This improvement is attributed to the high consistency of our tokens compared to the randomly sampled pixels in SMIRK, allowing for smoother transitions between video frames, which plays a crucial role in video generation.

\subsubsection{Qualitative Comparison}
\textbf{3D Mesh Accuracy.}
Fig.~\ref{fig:3d_mesh_cmp} presents a visual comparison of the 3D meshes from different methods. Our approach, along with SMIRK, outperforms Deep3DFace, DECA, and EMOCA in capturing more vivid facial expressions. However, our method excels over SMIRK by producing more precise and nuanced expressions. Notably, our method excels in modeling subtleties such as the mouth corner movements, intricate mouth shapes, and degree of eye openness.

\textbf{Reconstruction Facial Image Quality.}
Fig.\ref{fig:2d_image_rec} visually compares the reconstructed image with 3DDFA-v3 and SMIRK.
It can be seen that the quality of our reconstructed images far surpasses these two SOTA methods. There are significant improvements in image clarity, appearance details (including eye gaze, wrinkles, sharpness of teeth), and the alignment of lip shape and color. Additionally, we show that our 3D mesh and 2D reconstructed images are almost perfectly aligned. 

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Images/ablation_study.pdf}
\vspace{-1.5em}
\caption{Visual results of ablation study. Left: impact of token consistency loss. Middle: impact of region loss. Right: impact of proposed landmark loss.}
\vspace{-.5em}
\label{fig:ablation_study}
\end{figure}


\begin{table*}[!t]
  \centering
  \caption{Ablation study on the LRS3 test dataset.}
  \vspace{1mm}
  \label{tab:ablation_study}
  \centering
  \scalebox{0.9}[0.9]{
    \begin{tabular}{rl|cccccc}
    \toprule[1.1pt]
     Version & Description   & LPIPS $\downarrow$ & FID $\downarrow$ & CSIM $\uparrow$ & PSNR $\uparrow$ & AED $\downarrow$ & APD $\downarrow$  \\
    \hline 
    V1 & Baseline & $0.109$ & $25.39$ & $0.729$ & $29.14$ & $0.147$ & $0.056$  \\
       & V1 (+ token $T'$) & $0.095$ & $25.05$ & $0.723$ & $28.39$ & $0.144$ & $0.055$  \\
    V2 & V1 (+ token $T$) & $0.089$ & $20.46$ & $0.740$ & $28.86$ & $0.137$ & $0.054$  \\
    V3 & V2 (+ $\mathcal{L}_{tc}$.) & $0.087$ & $19.96$ & $0.755$ & $29.04$ & $0.133$ & $0.054$   \\
    V4 & V3 (+ $\mathcal{L}_{rg}$) & $0.086$ & $20.30$ & $0.762$ & $29.17$ & $0.123$ & $0.046$ \\
    V5 & V4 (+ Zero Conv.) & $0.078$ & $19.11$ & $0.798$ & $30.51$ & $0.118$ & $0.045$  \\
    Final & V5 (+ $\mathcal{L}_{pdl}$) & $\textbf{0.077}$ & $\textbf{19.41}$ & $\textbf{0.804}$ & $\textbf{30.67}$ & $\textbf{0.114}$ & $\textbf{0.044}$ \\ 
    \bottomrule[1.1pt]
    \end{tabular}
  }
\end{table*}

\subsection{Ablation Study}
\textbf{Ablation on the Significance of Appearance Token.}
In order to fully demonstrate the effectiveness of the facial tokens, we first conducted an ablation study on them. We take SMIRK as the baseline model. 
As shown in Fig.\ref{fig:ablation_study}, the tokens enhance the reconstructed 2D image in three aspects. First, the overall clarity of the image is improved. Second, the shape of the mouth on the face gradually approaches the original image. Additionally, compared to the baseline, there is a significant increase in gaze accuracy and teeth clarity, resulting in more accurate meshes (specifically reflected in the position of the eye contours and the shape of the mouth). We also remove multi-scale strategy in MFAT, and use only the final output of the token encoder as the facial appearance token, denoted as $T'$. It is worth noting that we removed the mask from the baseline that could leak facial color information, resulting in a slight decrease in both PSNR and CSIM metrics. As seen in Tab.\ref{tab:ablation_study}, the inclusion of the token significantly and consistently improves other evaluation metrics. 

\textbf{Effect of the Token Consistency Loss and Region Loss.}
As shown in the second row of Fig.\ref{fig:ablation_study} and Tab.\ref{tab:ablation_study}. Compared to the baseline, our token have learned more information representing facial details without adding any constraints. However, there are still some shortcomings, as seen in the third column in Fig.~\ref{fig:ablation_study}, such as incorrect gaze and over smooth teeth. After adding region loss, the gaze and the density of the beard in the reconstructed images show significant improvement. This is because under the constraint of region loss, our tokens pay more attention to the mouth and eye areas, and under the constraint of cycle loss, the information extracted by the tokens is closer to the input image, thus improving fidelity.


\textbf{Effect of Pose-dependent Landmark Loss.}
In order to further enhance TEASER's performance, including modeling complex mouth shape and improving the accuracy of mouth closure issue, we introduce pose-dependent landmark loss. In the third row of Fig.\ref{fig:ablation_study}, $\mathcal{L}_{pdl}$ compensates for a minor flaw that was present before its addition, such as more accurate lip closure.


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Images/swap_token_and_shape.pdf}
\vspace{-1.5em}
\caption{Application of our token in face editing. Left: we only replace tokens $T$ in the first row. Right: we replace both tokens $T$ and the FLAME shape parameters $\beta$.}
\vspace{-.5em}
\label{swap_token_and_shape}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=1\textwidth]{Images/drive_case.pdf}
\vspace{-1.5em}
\caption{Application of our token in expression transfer.}
\vspace{-.5em}
\label{fig3}
\end{figure}


\subsection{Token Analysis}


\textbf{Token Interpretability via t-SNE Clustering Visualization.}

\begin{wrapfigure}[13]{r}{2in}
  \vspace{-8mm}
  \centering\includegraphics[width=\linewidth]{Images/tokens_visualization.pdf}
    \caption{Different subjects' tokens visualization via t-SNE.}
    \label{tokens_visualization}
\end{wrapfigure}

To validate our motivation, we investigate the multi-scale tokens that we extracted. We extract multi-scale tokens from each frame of videos of several different individuals, and perform clustering on these tokens across various scales. The clustering results are presented in Fig.\ref{tokens_visualization}. We mark the tokens from the same person with the same color, and it can be seen that tokens from the same individual coalesce into distinct groups, with clear boundaries between these groups, and this is the case for all four scales of tokens. Given that the expressions and head poses vary in every frame within the same video, the ability to cluster them supports our hypothesis that \emph{the tokens encapsulate identity-aware facial texture information}. The slight variations within the same category of tokens are attributed to the changes in texture that occur when expressions change.

\textbf{Token Transfer for Identity Swapping and Face Animation.}
Since our tokens can represent facial textures, we apply them to the task of face swapping, with the results shown in  Fig.~\ref{swap_token_and_shape}. In the top row of Fig.~\ref{swap_token_and_shape}, we only transferred the target's token to the source, and it can be observed that the swapped face retains the expressions, poses, and shapes consistent with the source, while the textures, such as skin and eyebrow color, are taken from the target. To more closely align with the definition of face swapping, we transferred the target's tokens and the FLAME shape parameters to the source. 
As shown in the bottom row of Fig.~\ref{swap_token_and_shape}, the resulting swapped identity is almost identical to the target, while the expressions and other non-identity information remain consistent with the source.

\section{Conclusion}
In this paper, we propose a hybrid representation approach to enhance the accuracy of facial expression reconstruction. Our method designs a multi-scale facial appearance encoder to extract implicit facial appearance tokens, while employing a geometry encoder to capture explicit facial geometry information. In addition, we introduce a token-guided facial neural synthesizer, which generates accurate and stable facial images based on the spatial information from facial meshes. This provides finer and more precise supervision signals for facial expression reconstruction, enabling the capture of detailed facial expressions from a single image. 
Our results demonstrate the interpretability of the implicit tokens and highlight several potential applications, holding promising implications for visual effects and other human-centered tasks. Overall, we contribute a novel facial expression extraction method and a hybrid signal-driven facial generation network, paving the way for future tasks in complex 3D facial capture and tracking.


% \newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\renewcommand{\thefigure}{A-\arabic{figure}}
\renewcommand{\thetable}{A-\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}

\rev{
\section{Outline}
This supplementary document provides more analyses and results that were not included in the main paper due to space limitations. The contents are organized as follows.
\begin{itemize}
    \item Section~\ref{sec:supp_arch}: Detailed architecture of our Token-guided Face Synthesizer.
    \item Section~\ref{sec:supp_impl}: More implementation details about training, evaluation and loss design.
    \item Section~\ref{sec:supp_more_results}: More results on different 3D benchmarks, ablation studies, network parameters analysis, and inferecne speed details.
    \item Section~\ref{sec:supp_understanding_token}: Detailed explanation and analysis of our multi-scale appearance token and more cluster results of appearance token on different person videos.
    \item Section~\ref{sec:supp_discussions}: More discussions about our limitations, future work and ethical considerations.
\end{itemize}
}

\section{Architecture details}\label{sec:supp_arch}
\textbf{Token-guided Face Synthesizer.} 
As shown in Fig~\ref{fig:enter-label}, the proposed TFS consists of a UNet-based face generator (\ie, an encoder $E$, a bottleneck block, and a generator $\mathcal{G}$), 4 MLP-layers, and a 4-layer token decoder. There are 4 blocks in $E$ and $\mathcal{G}$. Each block contains an up/down-sampling layer and two convolutional layers. Given a rendered mesh $I_v$ and a background $I_b$, the encoder $E$ first extracts multi-level features. These features are fed into the generator $\mathcal{G}$ by skip-connection. To better inject the appearance token $T$ to guide the generation process, we use two ways. Specifically, we first use AdaIN to modulate each block in $\mathcal{G}$, which provides a global style guidance for TPS. According to Sec~3.2 in the main paper, $T$ consists of $K$ ($K=4$) sub-tokens. 
In TFS, we adopt $K$ MLP-layers to map these $K$ sub-tokens into AdaIN normalization parameters. Each of these parameters is fed into different layers in $\mathcal{G}$ to modulate features from $\mathcal{G}$, respectively. Meanwhile, we design a multi-layer token decoder $D$, where each block is followed by a zero-convolution layer. 
The token decoder $D$ takes the whole multi-level token $T$ as input.
Each feature map extracted by each block of $D$ is added to the features after AdaIN.
Combining
AdaIN and token decoder D, TPS effectively captures both global style information and detailed
high-frequency features, leading to more refined face generation results with rich facial details.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/TFS-arch.pdf}
    \caption{Detailed architecture of Token-guided Face Synthesizer.}
    \label{fig:enter-label}
\end{figure}

\section{More implementation details}\label{sec:supp_impl}

\subsection{Training details.}
Our training process is divided into two stages. In the first stage, we only optimize the geometry encoders, without incorporating the tokenizer and TFS, for coarse FLAME parameters estimation. In this stage, the total loss is set as $\mathcal{L} = \mathcal{L}_{lmk}$.
In the second stage, similar to SMIRK~\cite{SMIRK}, we alternately train the encoders (including the geometry encoder and MFAT) and TFS during the training process. This design helps reduce the effect of the neural renderer compensating for the encoders. Specifically, for each iteration, we first freeze the TFS and update the encoders. Then, we freeze the encoders and update the TFS only. By alternating the training of the encoders and TFS, we avoid the joint optimization of these two components. Additionally, the frozen part acts as a regularizer for the other training component, enhancing generalization.

\subsection{Evaluation metrics details}
To validate our motivation and demonstrate that the 2D images we reconstructed have higher quality, we utilized several image-level evaluation metrics. Additionally, we also assessed the stability of the reconstructed videos, which is greatly beneficial for subsequent practical application such as face reenactment and identity swapping. 
\textbf{(i) Image Reconstruction:}
For image reconstruction quality, the Peak Signal-to-Noise Ratio (\textbf{PSNR}) measures the low-level similarity of the generated images to the ground-truth images. The Fréchet Inception Distance (\textbf{FID}) \cite{FID} metric is employed to measure the dissimilarity between distributions of generated and real images. Of particular importance, we compute the cosine similarity of identity (\textbf{CSIM}) features to assess the fidelity of identity preservation. These features are derived from the pretrained face recognition model ArcFace \cite{ARCFACE}. Furthermore, following the previous work PIRenderer \cite{PIRenderer}, the Average Expression Distance (\textbf{AED}) and Average Pose Distance (\textbf{APD}) metrics are employed to scrutinize the impact of pose and expression imitation.
\textbf{(ii) Video Reconstruction:} We use warp-error \cite{WARPINGERROR} to evaluate the temporally consistency of reconstructed video. We determine the flicker level of the entire video by calculating the average brightness difference between adjacent frames.

\subsection{Loss details}

Previous methods for single-image face geometry reconstruction often heavily rely on 2D facial landmark detection due to the lack of necessary 3D annotations. These 2D landmarks are used to constrain the accuracy of the projected mesh. However, we have observed that the commonly used 2D facial landmarks are inaccurate for many scenarios. For example, in Fig.~\ref{fig:lmk_loss}, asymmetric mouth shapes or exaggerated facial expressions often lead to erroneous mouth landmarks with widely used detectors such as the 68-point facial detector~\cite{DECA} or Mediapipe~\cite{MICA,SMIRK}, which in turn degrades the accuracy of face reconstruction. In our experiment, we set $\epsilon=0.05$ in Eq.~\ref{eq:loss_pdl}.

Our investigation revealed that the facial landmark detector provided by InsightFace~\cite{insightface} delivers accurate and robust facial keypoints. However, since these keypoints are 2D, they can only detect visible facial contours. The corresponding mesh landmarks are often fixed at specific semantic locations, such as the jawline and the sides of the nose. In cases of large head poses, this leads to significant discrepancies between the 3D mesh landmarks and the detected 2D landmarks.
To address this issue, we designed a head-pose-based landmark loss that only constrains the visible mesh landmarks. This provides more precise supervision signals. The landmarks selected under different head poses are dynamically adjusted based on visibility. \rev{Therefore, our selected facial keypoints focus on the eyes, nose, and mouth regions. We empirically observed that: 1) Yaw angle significantly affects the accuracy of nose and jawline keypoints, 2) Pitch angle has minimal impact on keypoint loss. Please find more details at the right in Fig.~\ref{fig:lmk_loss}}

Although our experiments demonstrate that this loss term brings significant improvements to the model, we used a relatively hard mask to directly determine effective landmarks based on different head poses. Designing a smoothly varying confidence score to apply to all landmarks could potentially make better use of all landmark information, including eyebrows. Therefore, we plan to further explore a stronger landmark loss in the future.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/lmk_loss.pdf}
    \caption{(Left) We use InsightFace~\cite{insightface} to detect 203 2D facial landmarks, which get more accurate mouth landmarks. (Right) Different masks for landmark loss under different head poses.}
    \label{fig:lmk_loss}
\end{figure}

\section{More Results}\label{sec:supp_more_results}


\rev{
\begin{table}
  \vspace{-6mm}
  \centering
  \caption{\rev{Numerical comparison on 3D facial geometry benchmark.}}
  \label{tab:rlt_stirling}
  \scalebox{0.75}[0.75]{
    \begin{tabular}{lccccccccc}
    \toprule[1.1pt]
    \multicolumn{ 1}{l}{} & \multicolumn{3}{c}{NoW benchmark} & \multicolumn{3}{c}{Stirling benchmark} & \multicolumn{3}{c}{FaceWareHouse $\times e2$} \\
     Method   & Median $\downarrow$ & Mean $\downarrow$ & Std. $\downarrow$ & Median $\downarrow$ & Mean $\downarrow$ & Std. $\downarrow$ & Median $\downarrow$ & Mean $\downarrow$ & Std. $\downarrow$  \\
    \hline 
    Deep3DFace*{\small (~\cite{Deep3DReconstruct})}  & $1.11$ & $1.41$ & $1.21$ & \textbf{$0.99$} & $1.27$ & $1.15$ & $2.98$ & $4.11$ & $3.93$  \\
    3DDFA-V2~{\small (\cite{3ddfav2})}              & $1.09$ & $1.38$ & $1.18$ & $1.20$ & $1.55$ & $1.45$ & $3.12$ & $3.94$ & $3.87$  \\
    DECA~{\small (\cite{DECA})}                     & $1.23$ & $1.57$ & $1.39$ & $1.03$ & $1.32$ & $1.18$ & $3.09$ & $4.05$ & $3.91$  \\
    SMIRK~{\small (\cite{SMIRK})}                   & $0.99$ & $1.22$ & $1.02$ & $1.01$ & $1.08$ & $1.05$ & $2.87$ & $3.98$ & $3.89$  \\
    TEASER (Ours)                                   & \textbf{$0.92$} & \textbf{$1.10$} & \textbf{$0.99$} & $1.00$ & \textbf{$1.07$} & \textbf{$1.04$} & \textbf{$2.78$} & \textbf{$3.87$} & \textbf{$3.81$}  \\
    \bottomrule[1.1pt]
    \end{tabular}
  }
\end{table}
}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/supp-stirling.pdf}
    \caption{Visual mesh results on stiring result dataset. From left to right: input images, reconstructed mesh from SMIRK, and ours.}
    \label{fig:supp_stirling}
\end{figure}

\rev{\textbf{More results on 3D benchmarks.}}
In Fig.~\ref{fig:supp_stirling}, we present a comparison of our method with SMIRK on the Stirling benchmark~\cite{Stirling} for facial expression reconstruction. It is visually evident that our method reconstructs facial meshes with greater accuracy, particularly in terms of mouth closure. Although we did not specifically focus on optimizing the 3D face shape estimation, the improved accuracy of expression reconstruction in our approach allows us to achieve performance on par with other methods on the Stirling benchmark (as reported at Tab.~\ref{tab:rlt_stirling}). \rev{Both the Stirling and NoW benchmarks primarily focus on face shape reconstruction accuracy, which is not our main contribution, our method still shows competitive performance in Tab.\ref{tab:rlt_stirling}.}
\rev{
To better demonstrate our effectiveness on 3D expression reconstruction, we compared our method on FaceWareHouse~\cite{Facewarehouse}, a dataset that provides 3D geometry meshes with different expressions across 150 subjects. We used the now evaluation codebase~\footnote{\url{https://github.com/soubhiksanyal/now_evaluation}} with standard (non-metrical) evaluation to calculate `median/mean/std` metrics. All images from FaceWareHouse are used for benchmarking. Note that methods marked with * in Table~\ref{tab:rlt_stirling} used additional 3D datasets during training, making direct comparisons with our method unfair. Nevertheless, our method still achieves comparable results.}

\rev{In Table~\ref{tab:supp_more_cmp_lrs3}, we have provided more quantitative comparisons on LRS3 test set with other advanced method under same condition. Our method consistently achieves better performance across metrics.}

\begin{table}
  \centering
  \caption{\rev{More numerical comparisons against SOTA methods.}}
  \label{tab:supp_more_cmp_lrs3}
  \scalebox{0.9}[0.9]{
    \begin{tabular}{lcccccc}
    \toprule[1.1pt]
     Methods   & LIPIS $\downarrow$ & FID $\downarrow$ & CSIM $\uparrow$ & PSNR $\uparrow$ & AED $\downarrow$ & APD $\downarrow$ \\
    \hline 
    DECA{\small (~\cite{DECA})}            & 0.442 & 151.6 & 0.209 & 11.90 & 0.151 & 0.057  \\
    % 3DDFA-v2~{\small (\cite{EMOCA})}       & - & - & - & - & - & - \\
    3DDFA-v3~{\small (\cite{wang20243d})}  & 0.181 & 25.18 & 0.604 & 25.78 & 0.141 & 0.054  \\
    SMIRK~{\small (\cite{SMIRK})}       & 0.109 & 25.39 & 0.729 & 29.14 & 0.147 & 0.056  \\
    TEASER (Ours)                       & 0.077 & 19.41 & 0.804 & 30.67 & 0.114 & 0.044  \\
    \bottomrule[1.1pt]
    \end{tabular}
  }
\end{table}


\rev{\textbf{More ablation study results.}}
In Fig.~\ref{fig:supp_ablation_study}, we demonstrate the effectiveness of the token consistency loss. With this loss, we can reconstruct more faithful facial images, capturing clearer facial details, such as gaze direction, sharpness of the teeth, and finer details of glasses.

\rev{\textbf{Number of network parameters analysis.}}
\rev{To further analysis our token enhanced face renderer can generated superior face quality, not dependent on more trainable parameters, we experimented with increasing SMIRK's renderer parameters to match or exceed our renderer size. We found that increasing the learnable parameters of the renderer did not significantly improve the performance of SMIRK. Even with these larger models, our method still achieves better results, as shown in the Table.~\ref{tab:supp_param_analysis}.}

\begin{table}
  \vspace{-6mm}
  \centering
  \caption{\rev{Analysis on number of trainable parameters in our renderer. Our method shows consistently superior performance compared to SMIRK with different scale of model size.}}
  \label{tab:supp_param_analysis}
  \scalebox{0.9}[0.9]{
    \begin{tabular}{lccccccc}
    \toprule[1.1pt]
     Methods   & \# Params. & LIPIS $\downarrow$ & FID $\downarrow$ & PSNR $\uparrow$ & CSIM $\uparrow$ & AED $\downarrow$ & APD $\downarrow$ \\
    \hline 
    SMIRK         & 31.4M & 0.109 & 25.39 & 0.729 & 29.14 & 0.147 & 0.056  \\
    SMIRK \small{(res\_block=6)}       & 36.1M & 0.108 & 25.31 & 0.731 & 29.12 & 0.146 & 0.056 \\
    SMIRK \small{(res\_block=6, init\_feat=40)}       & 56.4M & 0.108 & 25.32 & 0.728 & 29.16 & 0.147 & 0.055 \\
    TEASER (Ours)  & 32.7M & 0.077 & 19.41 & 0.804 & 30.67 & 0.114 & 0.044  \\
    \bottomrule[1.1pt]
    \end{tabular}
  }
\end{table}

\rev{\textbf{Inference speed.}}
\rev{On an RTX 3090, our method runs at an overall speed of 20.43 FPS. Both the Encoder and Face Renderer operate above real-time, with the Encoder at 43.94 FPS and the Token-guided Face Synthesizer at 45.77 FPS.
Our model can be easily converted to a ONNX model, after which the overall speed increases to 29.77 FPS.}

\section{\rev{Understanding appearance tokens}} \label{sec:supp_understanding_token}

In Fig.~\ref{fig:supp_token_viz}, we present more interpretable results of the tokens. Using videos corresponding to different identities, we extracted tokens and observed that the extracted tokens cluster effectively around the same identity. This demonstrates that the tokens extracted by TEASER exhibit strong interpretability and disentanglement with respect to identity.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Images/ablation_study-supp.pdf}
    \caption{More results about effectiveness of token consistency loss.}
    \label{fig:supp_ablation_study}
\end{figure}

\rev{Fig.~\ref{fig:supp_diff_token_effect} illustrates how tokens at different scales affect face appearance. We extract multi-scale appearance tokens from the target image, while the source image provides face geometry. Specifically, $z_1$ represents low-level tokens, and $z_4$ represents high-level tokens. It can be observed that low-level tokens primarily influence skin tone, while high-level tokens affect semantic details such as gender, eyebrow shape, \etc}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Images/swap_token_diff_scale.pdf}
    \caption{\rev{Understanding how tokens at different scales affect face appearance.}}
    \label{fig:supp_diff_token_effect}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Images/supp-tokens_visualization.pdf}
    \caption{More results about token visualization.}
    \label{fig:supp_token_viz}
\end{figure}

\section{Discussions} \label{sec:supp_discussions}

\subsection{Limitations and Future work.} 
The 3D geometry encoder of our method inherits several limitations that are specific to FLAME-based head reconstruction methods. While we do address some of the limitations that stem from the use of a parametric FLAME model, we are still subject to misalignment on extreme poses and strongly occluded faces. Our face synthesis stage cannot generate overly complex facial occlusions, such as hands, microphones, bangs, and other facial accessories. Future work can explore using semantic segmentation to filter out non-facial regions when calculating the loss, in order to improve the accuracy and robustness of the synthesis-by-neural-rendering scheme. Our work can also be applied to future SOTA talking head generation tasks, providing accurate expression information and a general facial renderer.

\subsection{Ethical Considerations.} 
This research is conducted with a focus on advancing the accuracy and realism of 3D expression capture technologies for legitimate and ethical applications, such as enhance user experience in VR/AR/XR and improving visual effects in the social media. 
\rev{Specifically, the ability to edit faces and transfer expressions could facilitate the creation of deepfake videos, which might be used to manipulate the likeness of individuals without their consent. This includes generating highly realistic but fabricated content, such as making individuals appear to say or do things they never did or placing their likenesses in misleading or harmful contexts. Such misuse poses risks to privacy, consent, and trust in digital media, as well as broader societal implications, such as spreading misinformation or damaging reputations.}
We strongly object to the misuse of our method for any harmful or deceptive purposes. Our research is intended to support the scientific community and ethical industry practices. We encourage ongoing dialogue and regulation to ensure that developments in this area are employed in ways that uphold individual rights and align with community values. 
\rev{Future work should also prioritize an in-depth exploration of these issues, including robust mitigation strategies, transparent usage guidelines, and technical safeguards to prevent misuse.
}

\end{document}
