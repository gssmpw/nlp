\section{Evaluation}
\label{sect:results}




\subsection{Evaluation Setup}

\textbf{Implementation}. We implement \system in CUDA and PTX assembly on the basis of QServe~\cite{lin2024qserve} and TensorRT-LLM~\cite{trtllm} system. The specialized CUDA kernels are compiled into PyTorch extensions for better flexibility and compatibility with the purely PyTorch-based serving interface. 


\textbf{Testbed}. 
Our primary experiments are conducted on a server equipped with 8 NVIDIA A100 80GB GPUs, 2 AMD EPYC 7763 CPUs (128 cores), and 2TB of memory. Unless explicitly stated, all experiments utilize the A100 GPUs. Additionally, we perform some evaluations on a cloud instance with a single NVIDIA L40S 48GB GPU to assess system performance across different GPU architectures. All evaluations use PyTorch 2.5.0 with CUDA 12.4 and cuDNN 9.2.0.
\input{text/table/5-results/longbench}
\input{text/figure/5-results/main_NIAH}
\input{text/table/5-results/ruler}



\textbf{Models}. 
To comprehensively assess system performance across various LLM architectures, we utilize the widely adopted GQA-based model Llama-3-8B \cite{dubey2024llama}, the MHA-based model Llama-2-7B \cite{touvron2023llama2}, and the smaller-scale model Minitron-4B \cite{Minitron}. Additionally, to support long-context inference, we employ the context-extended Llama-3-8B version Gradient \cite{gradientlongcontextllama3}.


\textbf{Metrics}.
Our primary focus is on serving throughput. For the prefilling stage, we use \emph{time-to-first-token} (TTFT) as a key metric, while for the decoding stage, we emphasize minimizing the \emph{per-token generation latency}.



\textbf{Baselines}. 
We consider the following systems as baselines, using their latest versions\footnote{vLLM 0.6.3} to ensure a fair comparison. We activated W8A8 precision for baselines if available.

\begin{itemize}[leftmargin=*, itemsep=-3pt, topsep=-5pt, ]
  \item \emph{vLLM}~\cite{vllm}, one of the most popular LLM serving systems featuring PagedAttention.
  
  \item \emph{QServe}~\cite{lin2024qserve}, efficient LLM serving system featuring W4A8KV4 quantization.
  
  \item \emph{MInference} \cite{jiang2024minference}, the state-of-the-art long-context prefilling stage acceleration system.
  
  \item \emph{DuoAttention} \cite{xiao2024duoattention}, a strong long-sequence LLM inference framework with static sparse attention.
  
\end{itemize}



Additionally, we compare our approach with the state-of-the-art long-context decoding stage acceleration system, \emph{Quest} \cite{tang2024quest}. Since Quest only supports MHA models, we conduct and discuss this comparison in Table~\ref{tab:results:e2e_quest}.



\subsection{End-to-end Accuracy}
\label{sect:results:acc_eval}






We evaluate the accuracy of our hybrid block-sparse mechanism with LongBench~\cite{bai2023longbench} tasks, the Needle-in-a-Haystack (NIAH)~\cite{LLMTest_NeedleInAHaystack} pressure tests, as well as the challenging RULER~\cite{nvidia_ruler} benchmarks.  Table~\ref{tab:results:long_bench} compares the LongBench accuracy between \system and dense baseline. Results show that \system well preserves the performance of two models across different test sets. Figure~\ref{fig:evaluation:main_niah} showcases the NIAH evaluation results of our system, where \system also achieves the same level of accuracy compared to the dense baseline. In Table~\ref{tab:results:main_ruler}, we test \system with RULER benchmarks. Unless otherwise specified, we convert half of the attention heads into streaming heads and keep token budget for dynamic sparsity to 4096 for the benchmarks.



\input{text/figure/5-results/main_results}

\input{text/figure/5-results/main_results_ctx}

\subsection{End-to-end Efficiency}

\textbf{Decoding Efficiency}. Figure~\ref{fig:evaluation:main_speed} presents the efficiency benchmarking results for the decoding stage. We use the same sparsity configurations as in \sect{sect:results:acc_eval}. Compared with the state-of-the-art serving systems, \system demonstrates significant and consistent efficiency improvements across different GPU platforms and model architectures. On Llama-3-8B and Minitron-4B, \system achieves 1.5$\times$ average speedup over vLLM. For MHA-based model Llama-2-7B, \system runs more than 2.0$\times$ faster than baselines on average. Additionally, we demonstrate that \system also functions well on other GPU devices such as L40S with Ada Lovelace Architecture. \system achieves up to 1.7$\times$ speedup over vLLM.

\textbf{Prefilling Efficiency}. In Figure~\ref{fig:evaluation:main_speed_prefill}, we compare the prefilling speed of \system against 4 baselines on Llama-3-8B and Llama-2-7B. \system maintains superior prefilling throughput across different sequence lengths. For instance, on Llama-2-7B, \system achieves an average of 1.8$\times$ higher prefilling throughput over vLLM. \system is also compatible with the prefilling dynamic sparsity in MInference, which we activated after 128K sequence length. 


\input{text/table/5-results/e2e_quest}
\subsection{End-to-End Comparison with Quest}

We also compares our system against Quest~\cite{tang2024quest} in Table~\ref{tab:results:e2e_quest}. Across different sequence lengths, \system consistently outperforms Quest in both prefilling (1.6-2.1$\times$ speedup) and decoding stages (1.3-1.5$\times$ speedup). 

