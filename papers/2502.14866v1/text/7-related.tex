\section{Related Work}
\label{sect:related}

\textbf{LLM Serving Systems}. Various systems have been developed to enhance LLM deployment efficiency. Orca~\cite{orca} uses iteration-level scheduling and selective batching for distributed systems. vLLM~\cite{vllm} introduces PagedAttention, inspired by virtual memory, to optimize KV cache management. TensorRT-LLM~\cite{trtllm} is the industryâ€™s leading solution also featuring in-flight batching and PagedAttention inspired by vLLM. LightLLM~\cite{lightllm} further reduces memory waste in PagedAttention by introducing TokenAttention. SGLang~\cite{sglang} advances LLM programming with a domain-specific language and RadixAttention. LMDeploy~\cite{lmdeploy} improves deployment with persistent batching and blocked KV cache. Nanoflow~\cite{zhu2024nanoflow} features intra-device scheduling and asynchronous CPU scheduling, while QServe~\cite{lin2024qserve} improves LLM serving throughput through \texttt{W4A8KV4} quantization and system codesign. MLC-LLM~\cite{mlcllm} accelerates deployment on edge devices via compiler-based optimizations. Inspired by contextual sparsity~\cite{liu2023deja}, PowerInfer~\cite{song2023powerinfer,xue2024powerinfer} deploys LLMs on memory-constrained devices via offloading.


\textbf{Sparse Attention}. BigBird~\cite{zaheer2020big} reduces attention complexity by blending local, global, and random attention masks. Subsequent methods like StreamingLLM~\cite{xiao2023efficient}, H2O~\cite{zhang2024h2o}, and TOVA~\cite{oren2024transformers} simplify attention patterns by discarding KV caches mid-way through the context. However, these approaches struggle to retain the original models' long-context capabilities due to limited global context modeling. Recent works like DuoAttention~\cite{xiao2024duoattention}, RetrievalAttention~\cite{liu2024retrievalattention}, and SeerAttention~\cite{gao2024seerattention} address this issue by introducing retrieval heads~\cite{wu2024retrieval} or combining full attention with local attention heads. Quest~\cite{tang2024quest} introduces dynamic, query-aware sparsity for accelerated decoding, while MInference~\cite{jiang2024minference} extends similar ideas to the prefilling stage. FastGen~\cite{ge2023model} optimizes decoding by profiling attention heads to discard tokens. PQCache~\cite{zhang2024pqcache} and ShadowKV~\cite{sun2024shadowkv} further advance the selective attention methods with product quantization and low-rank decomposition. Additionally, LongLoRA~\cite{chen2023longlora} finetunes short-context LLMs to long-context ones after converting global attention to shifted sparse attention. 



