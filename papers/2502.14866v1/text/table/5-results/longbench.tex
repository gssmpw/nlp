\begin{table}[t]
\centering
\small
\caption{\textbf{Accuracy evaluation on LongBench}~\cite{bai2023longbench}. We compare our method with vanilla dense attention on 2 models and 10 different benchmarks.}
\scalebox{1.0}
{
\begin{tabular}{ccccc}
\toprule
Model & \multicolumn{2}{c}{Llama-3-8B} & \multicolumn{2}{c}{Llama-2-7B} \\
\midrule
Benchmark & Dense & \system & Dense & \system \\
\midrule
2WikiMQA  & 30.3 & 31.6 & 35.4 & 35.1 \\ 
DuReader  & 30.3 & 30.8 & 25.4 & 24.7 \\ 
HotpotQA  & 41.7 & 42.7 & 47.4 & 49.6 \\ 
MultiNews & 27.7 & 27.7 & 26.6 & 26.6 \\ 
Qasper    & 31.7 & 29.3 & 32.6 & 29.5 \\ 
QMSum     & 23.8 & 24.0 & 21.0 & 21.3 \\  
SamSum   & 41.2 & 39.3 & 41.8 & 41.5 \\ 
TriviaQA  & 84.9 & 83.7 & 86.2 & 86.5 \\ \midrule
\textbf{Average}   & \textbf{38.9} & \textbf{38.6} & \textbf{39.5} & \textbf{39.4}
\\
\bottomrule
\end{tabular}
}
\label{tab:results:long_bench}
\end{table}

