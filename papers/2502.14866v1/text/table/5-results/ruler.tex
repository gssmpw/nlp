\begin{table}[t]
\centering
\caption{\textbf{Accuracy evaluation on RULER}~\cite{nvidia_ruler}. We evaluate the accuracy of Llama-3-8B on RULER benchmarks, including challenging tasks such as multi-hop tracing and aggregation to test behaviors beyond searching from context. LServe-$N$ denotes that the token budget for dynamic sparsity is $N$. Note that for long-context inputs, latency is not dominated by attention alone in \system, with page selector and GEMM also contributing to it. Experiments reveal that LServe-8192 is only up to 6\% slower than LServe-4096 when the sequence length exceeds 128K.}
\scalebox{0.9}
{
\begin{tabular}{ccccccc}
\toprule
Llama-3-8B  & 32K  & 64K  & 128K & 160K	& 192K & 256K \\ \midrule
Dense       & 90.5 & 86.8 & 83.8 & 79.3 & 79.6 & 79.4 \\ \midrule
LServe-4096 & 91.0 & 85.6 & 81.0 & 79.0 & 76.1 & 75.7 \\ \midrule
LServe-8192 & 91.8 & 86.1 & 81.7 & 81.2 & 79.7 & 79.1 \\
\bottomrule
\end{tabular}
}
\label{tab:results:main_ruler}
\end{table}

