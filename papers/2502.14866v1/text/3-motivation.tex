\subsection{Motivation}
\label{sect:motivation}
\input{text/figure/3-motivation/attention_ratio}


Serving long-sequence LLMs is challenging due to the high cost of attention. Figure~\ref{fig:motivation:attention_ratio} profiles the latency breakdown of Llama-3-8B with a batch size of 1 across various sequence lengths on the A100 GPU. In both the prefilling and decoding stages, attention kernels account for at least 50\% of the runtime at sequence lengths over 64k, rising to 75\% at 128k. According to QServe~\cite{lin2024qserve}, the ratio of attention kernels in end-to-end runtime will increase as the batch size scale up. Therefore, in real-world serving scenarios, optimizing the attention becomes increasingly critical. 

Accelerating attention in long-sequence LLMs requires a deep understanding of attention kernel implementation on GPUs, as illustrated in Figure~\ref{fig:motivation:attention_flow}. During the prefilling stage, the attention kernel is parallelized across batch size, attention heads, and query tokens, with query tokens set to 1 in the decoding stage. In both stages, the computation along the KV token dimension remains sequential. In each iteration, a block (depicted as a grid with an orange contour in Figure~\ref{fig:motivation:attention_flow}) is computed collaboratively by all threads in the current thread block. Although skipping certain computation within each block is possible, it yields minimal speedup. This is due to the lockstep execution of threads within a GPU warp, where faster threads are forced to wait for slower ones. 

That said, rather than focusing on sparsity within each iteration, a more effective way to accelerate attention is to \textbf{reduce the number of sequential iterations} along the KV token dimension. This approach leads to our unified \textit{block sparse attention} formulation, where attention computation is skipped in a blockwise manner. In this scheme, aside from the most recent KV block, each block is either fully computed or entirely skipped during the prefilling stage. During decoding, each sequence contains only one query token, reducing the dimensionality of each orange-contoured grid to 1$\times P$, where $P$ represents the page size (i.e., the number of KV tokens per page). We will detail \system's sparsity pattern selection in Section~\ref{sect:method}. 

\input{text/figure/3-motivation/attention_flow}

Additionally, because the decoding stage is memory-bound, KV cache quantization also contributes to speed improvements. Quantization is orthogonal to block sparsity, as it reduces the \textit{runtime of each iteration}, while sparsity reduces the \textit{number of iterations}. %

















