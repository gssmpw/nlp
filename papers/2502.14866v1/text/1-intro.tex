\section{Introduction}
\label{sect:intro}



Large Language Models (LLMs) have dramatically transformed the field of artificial intelligence. With expanding context window lengths, LLMs now demonstrate remarkable performance across diverse long-sequence applications~\cite{google2024gemini}, including multi-turn conversations, long document analysis~\cite{zhang2024benchmarking, goyal2020evaluating, huang2021efficient}, multi-modal understanding~\cite{xue2024longvila, liu2024visual, lin2024vila}, and code completion~\cite{li2023starcoder, lozhkov2024starcoder}. Many of these applications require processing hundreds of thousands of context tokens in real-world settings, presenting unique challenges. In particular, the demand for fast prefilling, or minimizing the time to the first token, and the burden on the decoding phase due to the large KV (key-value) caches necessary for such contexts, represent significant hurdles.

\input{text/figure/teaser}


Long-sequence LLMs are about more than just an extended context. The recently announced OpenAI o1~\cite{openai2024o1} demonstrates exceptional capabilities in complex reasoning tasks, such as deciphering, mathematics, and coding. These advancements are achieved through inference-time scaling and the generation of extensive chains of thought~\cite{wei2022chain}. According to \citet{qin2024o1}, o1's internal reasoning process can extend to \textbf{20k} tokens for mathematical problems, making it the first known \textit{long-generation} LLM. Contrary to the conventional belief that the prefilling stage dominates runtime in long-sequence LLMs, when Llama-3-8B~\cite{dubey2024llama} is run using TensorRT-LLM~\cite{trtllm} with 256k input tokens and 20k output tokens (comparable to o1’s reasoning trace length), the prefilling time is 116 seconds, while decoding takes 540 seconds — almost \textbf{5$\times$} longer.





To enhance the efficiency of long-sequence LLMs, it is essential to optimize both the prefilling and decoding stages rather than focusing on just one. Beyond model architectural modifications in the pre-training stage~\cite{ainslie2023gqa, brandon2024reducing}, existing acceleration solutions for long-sequence LLMs primarily address efficiency from two angles. The first approach centers on KV cache quantization, where methods such as QServe~\cite{lin2024qserve}, KIVI~\cite{liu2024kivi}, and KVQuant~\cite{hooper2024kvquant} employ low-bit quantization to reduce memory usage and I/O traffic, potentially increasing generation throughput. However, these quantization techniques do not lower the number of computations performed in the attention loop, resulting in suboptimal generation speeds as sequence lengths grow. The second approach utilizes approximate sparse attention to improve long-sequence LLM performance. For example, StreamingLLM~\cite{xiao2023efficient}, H2O~\cite{zhang2024h2o}, and TOVA~\cite{oren2024transformers} apply static masking mechanisms to reduce attention complexity, though at the expense of accuracy in long-context tasks and irregular KV cache memory layouts. DuoAttention~\cite{xiao2024duoattention} advances this strategy by pruning attention computations at a coarser granularity using an optimization-based approach. Other methods, such as MInference~\cite{jiang2024minference} and Quest~\cite{tang2024quest}, implement dynamic sparse attention to accelerate either the prefilling \textbf{\textit{or}} decoding stage. However, these approaches do not reduce KV cache memory consumption and lack a unified framework to address efficiency challenges in \textbf{\textit{both}} stages simultaneously.

To this end, we introduce \textbf{\system}, an efficient system for serving long-sequence LLMs that leverages hybrid sparse attention. Recognizing that not all tokens hold equal importance, \system integrates multiple hardware-friendly, structured sparsity patterns into a \textbf{unified block sparse attention} framework (see Figure~\ref{fig:method:blocksparse}). Block-level sparsity accelerates attention computation by processing the KV history in discrete blocks. By skipping blocks, we directly reduce the number of sequential iterations, resulting in measured speedups during both the prefilling and decoding stages.

Building on the unified block sparse attention framework, \system further illustrates acceleration opportunities from \textit{static} and \textit{dynamic} sparsity. 

For \textit{static} sparsity, inspired by DuoAttention~\cite{xiao2024duoattention}, we modify the attention masks in the original model by converting half of the attention heads into $\Lambda$-shaped masks, transforming these attention heads into \textit{streaming heads}. Additionally, we fuse the computation of streaming and standard attention heads into unified GPU kernels for both the prefilling and decoding stages, translating theoretical computation and memory savings that translate to up to 1.7$\times$ measured speedup. 

For \textit{dynamic} sparsity, we observe that query-centric sparsity~\cite{tang2024quest} allows for nearly lossless KV compression: the required number of KV tokens to maintain long-context capabilities remains constant (e.g., 4096), regardless of context length. To optimize efficiency, we design a hierarchical page selector to identify important KV pages for each query token, reusing the selection results across tokens to reduce page selection overhead by 4$\times$. 

Our key observation is that \textbf{static and dynamic sparsity patterns are orthogonal} in long-sequence LLMs. By unifying static and dynamic sparsity with KV cache quantization into a single GPU kernel, \system achieves compounded efficiency benefits from each individual optimization for decoding stage attention.

We benchmark \system across three long-sequence LLMs—Llama-3-8B, Minitron-4B, and Llama-2-7B—at context lengths up to 512k tokens. Compared to state-of-the-art frameworks like vLLM~\cite{vllm}, QServe~\cite{lin2024qserve}, MInference~\cite{jiang2024minference}, and DuoAttention~\cite{xiao2024duoattention}, \system accelerates prefilling stage by up to \textbf{2.9$\times$} and achieves an average of \textbf{1.3$\times$-2.1$\times$} speedup in the decoding stage. Furthermore, \system accomplishes these speedups while retaining the long-context capabilities of the original dense, floating-point models, demonstrating that hybrid attention sparsity is a free lunch for long-sequence LLM serving.





























