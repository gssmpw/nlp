\section{Conclusion}
\label{sect:conclusion}

We introduce \system, an efficient serving system for long-sequence LLMs that leverages hybrid sparse attention. By incorporating \textit{unified block sparse attention}, we achieve significant acceleration of the attention mechanism for both prefilling and decoding stages in long-sequence models. We further show that head-level static sparsity and query-aware dynamic sparsity are orthogonal and can be effectively combined with minimal impact on accuracy. \system surpasses state-of-the-art systems, delivering an average of \textbf{1.3$\times$-2.1$\times$} speedup in the decoding stage and up to \textbf{2.9$\times$} speedup in the prefilling stage, preserving the models' long-context capabilities.




\section*{Acknowledgements}
We thank MIT-IBM Watson AI Lab, MIT AI Hardware Program, MIT Amazon Science Hub, National Science Foundation, and Hyundai for supporting this research. We also thank June Yang, Bo Li, and Kaiyu Xie for their helpful discussions. 


\newpage
