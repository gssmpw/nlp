\section{Background and Motivation}
\label{sect:background}



\subsection{Background}

\paragraph{LLM Inference.} LLMs are transformer-based architectures with stacked identical layers, each containing attention blocks, feed-forward networks (FFN), and normalization components. %
LLM inference involves two stages: an initial \emph{prefilling} stage that handles multiple tokens concurrently, followed by auto-regressive \emph{decoding} stage where only one token will be processed for each request in a decoding step.


\myparagraph{Attention.} The attention mechanism exchanges information across tokens. It first transforms input $\mathbf{x}$ through linear projections to generate query vectors $\mathbf{q}\in\mathbb{R}^{N\times HD}$, and key-value pairs $\mathbf{k},\mathbf{v}\in\mathbb{R}^{N\times \hat{H}D}$, where $\hat{H}$ represents the key/value head count. Traditional multi-head attention (MHA) maintains $H = \hat{H}$, and contemporary architectures 
~\cite{touvron2023llama2,jiang2023mistral,jiang2024mixtral} employ grouped-query attention (GQA)~\cite{ainslie2023gqa}
where $H = n\hat{H} (n\in \mathbb{Z})$ to shrink the size of KV cache. The current $\mathbf{k}$ and $\mathbf{v}$ is then concatenated with KV cache from $S$ preceding tokens, yielding $\mathbf{K}, \mathbf{V}\in\mathbb{R}^{(S+N)\times \hat{H}D}$. The attention computation can then be formulated as follows:

\vspace{-14pt}
\begin{equation}
\small
\mathbf{S}_{h} = \frac{\mathbf{q}_{h}\mathbf{K}_{\hat{h}}^T}{\sqrt{D}}, 
\hspace{5pt}
\mathbf{o}_{h} = \text{softmax}\left(\mathbf{S}_{h}\right)\mathbf{V}_{\hat{h}}, 
\hspace{5pt}
\hat{h}=\left\lfloor\frac{h}{n}\right\rfloor
\label{eqn:background:attn}
\end{equation}
Therefore, the complexity of attention can be expressed as $O\left(N(S+N)HD\right)$, which increases quadratically in the prefilling stage and linearly in the decoding stage with respect to sequence length. When $S$ is long, both decoding stage and prefilling stage are bounded by attention.


\myparagraph{Paged Attention.} In LLM serving, the generation length of each sequence is highly variable and unpredictable. Padding all sequences to the maximum length results in considerable memory waste and fragmentation. To address this, vLLM~\cite{vllm} introduces PagedAttention, a KV cache management algorithm inspired by operating systems' virtual memory. Instead of allocating a continuous memory buffer for each sequenceâ€™s KV cache, PagedAttention partitions the cache into fixed-size blocks (or pages), each holding KV data for a set number of consecutive tokens (typically 16 to 64). A page block table records the physical address of each page, allowing the PagedAttention kernel to use indirect addressing to retrieve KV features. TensorRT-LLM~\cite{trtllm} and QServe~\cite{lin2024qserve} implement quantized page attention to reduce memory bandwidth usage during the decoding stage, resulting in further generation speedups.



















