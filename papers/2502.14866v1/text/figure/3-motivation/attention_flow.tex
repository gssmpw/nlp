
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/3-motivation/attention-flow.pdf}
    \caption{\textbf{Attention calculation on GPUs}: In both the decoding and prefilling stages, each query token iterates over all key and value tokens sequentially in a \textit{block-by-block} manner. Skipping KV blocks reduces the number of sequential iterations, directly accelerating attention.} %
    \label{fig:motivation:attention_flow}
    \vspace{-7pt}
\end{figure}
