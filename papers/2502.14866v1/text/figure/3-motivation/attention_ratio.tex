
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figure/3-motivation/motivation_attention_ratio.pdf}
    \caption{
    Latency breakdown of LLM inference for both prefilling and decoding stage. As sequence length increases, attention dominates both stages due to its quadratic complexity in prefilling stage and linear complexity during decoding stage. In contrast, GEMM exhibits linear complexity during prefilling stage and constant complexity during decoding stage. Latency numbers measured with Llama-3-8B on NVIDIA A100 GPU.
    } \label{fig:motivation:attention_ratio}
\end{figure}
