\section{Analysis}
\label{sect:analysis}

\input{text/figure/6-analysis/prefilling_attention}
\input{text/figure/6-analysis/our_larger_page}
\input{text/figure/6-analysis/selector_overhead}

In this section, we present in-depth analysis on our design choices in the \system system from both the accuracy and the efficiency perspective. We also scrutinize the sources of performance gains in \sect{sect:results}.

\subsection{Prefilling Stage Sparse Attention Kernel}

We benchmark the performance of our block sparse attention kernel for the prefilling stage in Figure~\ref{fig:ana:prefilling_attention}. Compared with the implementation by MInference~\cite{jiang2024minference}, our kernel consistently achieves 1.3$\times$ speedup at the same sparsity level. Oracle stands for the theoretical upper-bound speedup ratio: $\text{Latency}_{\text{oracle}} = \text{Latency}_{\text{dense}} * (1-\text{sparsity})$.


\subsection{Effectiveness of Hierarchical Paging}



We use the Needle-in-a-Haystack ~\cite{LLMTest_NeedleInAHaystack} test to demonstrate that the hierarchical paging design effectively maintains the model's long-context capability on larger page blocks without increasing the token budget. In contrast to the performance drop observed with increased page granularity in Figure~\ref{fig:ana:naive-larger-page}, \system leverages a hierarchical page structure to decouple the pruning algorithmâ€™s page granularity from the physical memory layout of the KV cache. This approach enables our sparse attention mechanism to remain both accurate and hardware-efficient. Figure~\ref{fig:ana:our_larger_page} highlights this improvement: with a page size of 64 and the same token budget, \system achieves accuracy comparable to the baseline algorithm~\cite{tang2024quest}, which prunes history tokens at a granularity of 16.

\subsection{Mitigating Page Selection Overhead}


\input{text/table/6-analysis/reusable_accuracy}

\paragraph{Reusable Page Selection.} During decoding, although the attention kernel maintains constant complexity due to a capped number of historical KV tokens, the complexity of the page selector still scales linearly with sequence length. As illustrated in Figure~\ref{fig:ana:selector_overhead}, for a sequence length of 128K and a 4K token budget for sparse attention, the page selector (0.24 ms) is already twice as slow as the sparse attention kernel (0.12 ms). With our reusable page selector, however, \system significantly reduces page selection overhead by a factor of $C$, where $C$ is the reuse interval. We further show that \system is resilient to different reuse interval choices. Table~\ref{tab:ana:reusable_accuracy} demonstrates no significant performance degradation until the reuse interval exceeds 8, so we set it to 4 by default in \system.

\paragraph{Context Pooling Overhead.} To enable page selection during decoding, we must calculate representative features using min-max pooling in the prefilling stage. It is important to note that a single pooling kernel executes under \textbf{1 ms}, while the entire prefilling stage completes in approximately 17 seconds with 128K context length. Consequently, the context pooling overhead is negligible.

\subsection{Sparse Attention Kernel for Decoding Stage}

\input{text/figure/6-analysis/decoding_attention_kernel}


We analyze the effectiveness of different sparsity patterns in decoding attention. In Figure~\ref{fig:ana:decoding_attn_kernel}, we apply \textit{static} sparsity by converting 50\% of attention heads to streaming heads, achieving a \textbf{1.3-1.7$\times$} speedup across various input sequence lengths. Additionally, we introduce dynamic sparsity with a fixed KV budget of 4096 tokens, which bounds the computational complexity of decoding attention to a \textbf{constant}, delivering a \textbf{30$\times$} speedup over the dense baseline for an input length of 256K.  Although sparsity selection introduces minor overhead for shorter sequences, this is mitigated by reusable page selection. Additionally, we also perform the end-to-end ablation study in Section \ref{sect:End-to-End Ablation}.



\subsection{End-to-End Speedup Breakdown}
\label{sect:End-to-End Ablation}


\input{text/figure/6-analysis/case_study}
In Figure~\ref{fig:ana:case_study}, we highlight the sources of performance improvement in \system. By leveraging static sparsity, \system achieves end-to-end speedups of up to \textbf{1.7$\times$} over the dense baseline. Additionally, dynamic sparsity, aided by a reusable page selector, significantly reduces generation latency, yielding a \textbf{7.7$\times$} speedup for sequence lengths of 256K. Lastly, \system configures sparse patterns through offline profiling, effectively avoiding slowdowns from dynamic sparsity at shorter context lengths.

























