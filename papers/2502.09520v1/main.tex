\documentclass[journal]{IEEEtran}
% Include your settings
\input{Settings/Settings}



% Include abbreviations file
\input{Backmatter/Abbreviations}

% Include new_commands file
\input{Backmatter/New_Commands}
%

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{SQ-GAN: Semantic Image Communications\\ Using Masked Vector Quantization}


\author{Francesco~Pezone{\small{$^{1}$}},
        Sergio~Barbarossa{\small{$^{2}$}},~\IEEEmembership{Fellow ~IEEE}
        and~Giuseppe~Caire{\small{$^{3}$}},~\IEEEmembership{Fellow~IEEE,}% <-this % stops a space
\thanks{$^{1}$ CNIT - National Inter-University Consortium for Telecommunications, Parma, Italy}% <-this % stops a space
\thanks{$^{2}$ Sapienza University of Rome, Italy}
\thanks{$^{3}$ Technical University of  Berlin,  Germany}% <-this % stops a space
\thanks{{E-mail: francesco.pezone.ds@gmail.com, sergio.barbarossa@uniroma1.it, caire@tu-berlin.de}}% <-this % stops a space
}


% The paper headers
%\markboth{IEEE Transactions on Cognitive Communications and Networking (submitted)}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}


% make the title area
\maketitle
\begin{abstract}
This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach integrating generative models to optimize image compression for semantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic semantic segmentation 
and a new specifically developed semantic-conditioned adaptive mask module (SAMM) to selectively encode semantically significant features of the images. SQ-GAN outperforms state-of-the-art image compression schemes 
such as JPEG2000 and BPG across multiple metrics, including perceptual quality and semantic segmentation accuracy on the post-decoding reconstructed image, at extreme low compression rates expressed in bits per pixel. The code is available at \textit{\url{https://github.com/frapez1/SQ-GAN}}
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Semantic Communication, VQ-GAN, Data Augmentation, Semantic-Aware Discriminator
\end{IEEEkeywords}



\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% MAIN %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\IEEEPARstart{T}{he} recent surge of intelligent and interconnected devices is leading the transition to new communication paradigms. 
%%%% Beginning new text 
The upcoming sixth generation (6G) of wireless networks is poised to redefine the communication paradigm by integrating advanced Artificial Intelligence (AI) and networking \cite{yang2020artificial}. One of the most promising advancements in this context is semantic communication, which shifts the focus from raw data transmission to task-oriented and context-aware knowledge representation. Instead of transmitting all information indiscriminately, semantic communication prioritizes the meaning and relevance of the data, for a given application, and enhancing communication efficiency \cite{Strinati20206G}, \cite{Xie2021Deep, gunduz2022beyond, xie2022task, yang2022semantic}.

Recent breakthroughs in deep learning, generative AI, and self-supervised learning have further strengthened the potential of semantic communications. The use of generative AI for next generation networks has been recently proposed in \cite{barbarossa2023semantic}, \cite{tao2024wireless}, \cite{van2024generative}.
A key innovation in this domain is the integration of deep generative models that can synthesize high-fidelity multimedia content once properly conditioned. This capability shifts the communication paradigm: rather than transmitting full-resolution data, the transmitter selects and encodes only the most essential semantic features needed to trigger the generative model at the receiver side  \cite{tang2024evolving}. The receiver then reconstructs a meaningful representation, conditioned on the received information, and tailored to the application at hand.

To illustrate this concept, consider a vehicular communication scenario, where an autonomous vehicle transmits visual information from its onboard cameras to a roadside unit (RSU) responsible for making real-time context-aware safety decisions, possibly integrating data from multiple vehicles or RSU sensors. Instead of sending full video streams, the transmitter extracts key semantic attributes, such as the locations of pedestrians, vehicles, traffic signs, and road boundaries, and transmits compressed feature representations. At the receiver side, a generative model reconstructs an application-specific representation, ensuring that the RSU can accurately interpret the environment and make fast, informed decisions.
%%%% End new text
%Traditional communication schemes, treating data in an agnostic way, are being replaced by task-oriented schemes more targeted to the specific needs of new applications such as autonomous driving, augmented reality, and the Internet of Things \cite{Lu2014ConnectedVehicles, Fortino2014IoT}. Moreover, the increase in device-to-device communication has introduced new challenges, as multiple devices now need to communicate efficiently and effectively with each other. In this scenario, classical performance metrics such as symbol-error probability or human perceptual quality in audio, images, and video, may lead to inefficiencies in system design. 
%In vision tasks, for example, even if two images may be quite different on a pixel-by-pixel level, they can still convey the same meaning and information at the semantic level, which may be sufficient to perform a specific task. As a consequence, a task-oriented system does not necessarily require a highly accurate pixel-by-pixel image reproduction as long as the relevant information is transmitted. This idea stands at the basis of the concept of \textit{Semantic communication}, which has gained significant traction in recent years (e.g., see \cite{Qiao2021Semantic, Strinati20206G}). In particular, by leveraging the semantic content of messages, it is possible to compress and send only those parts of the data that help the receiver to reconstruct not the original data but one with the same meaning. This approach can effectively reduce bandwidth requirements and improve robustness against noise and interference \cite{Xie2021Deep}.

Deep learning has played a pivotal role in advancing semantic communication systems \cite{Liu2021Deep}. The development of end-to-end learning frameworks enables joint optimization of encoding and decoding processes, allowing systems to learn joint source/channel coding \cite{xu2023deep} or learn efficient representations of semantic content directly from data \cite{Weng2021Semantic, Xie2021DeepLearningEnabled}. Generative models, particularly \glspl{gan}, have been instrumental in improving the quality and efficiency of data representation and transmission \cite{goodfellow2014generative, Liu2024novel}. Specifically, integrating vector quantization techniques with generative models has shown great potential in learning discrete latent representations that are both compact and semantically rich \cite{Oord2017VQ-VAE, Esser2O21Taming}. Models like the \gls{vqvae}, the \gls{vqgan} and the \gls{maskvqvae} \cite{Huang2023MaskedVQ-VAE} have demonstrated remarkable capabilities in image synthesis and compression tasks.

In this context, we propose the \gls{sqgan} model, a novel \gls{vqgan}-based approach that merges the strengths of generative models and semantic communication for efficient data transmission. This architecture is designed to compress and reconstruct an image $\x$ such that its \gls{ssm} $\s$ is preserved. The \gls{sqgan} makes use of masked vector quantization \cite{Huang2023MaskedVQ-VAE} to prioritize semantically significant information, thereby reducing redundancy and enhancing communication efficiency.

The \gls{sqgan} framework operates by selectively encoding the most relevant features of the input data based on their semantic importance. This selective encoding is achieved through a newly designed and trained 
adaptive masking mechanism that learns to identify and preserve critical information while reducing redundancy. The masked vector quantization not only compresses data effectively but also ensures that semantically important information is preserved, making it highly suitable for applications where bandwidth and computational resources are limited. The overall scheme is a hierarchical coding scheme, where different objects in the image have different importance, depending on the task subsuming the exchange of information. 
%It is appealing to notice that the overall approach is reminiscent of the standard approach in image coding, where the image in the pixel domain processed through a linear transform (e.g., wavelet, discrete-cosine), and a suitably designed bit-allocation mechanism selects a subset of ``significant'' transform coefficients for quantization, while the ``weak'' coefficients are discarded. At a high-level and in qualitative non-rigorous terms, we may say that in conventional image coding, the most significant coefficients are generally the  high-energy ones, whose impact on the reproduction mean-square error is larger.
The proposed approach can be seen as a generalization of hierarchical image coding \cite{shapiro1992embedded}, where the linear transform stage, e.g. a wavelet transform, is replaced by a non-linear learned ``encoder'' stage, guided by the segmentation map of the image, obtained after prior selection of the classes of objects deemed relevant for the application. The encoder produces a real-valued latent space tensor, which plays the role of the transform coefficients in conventional image coding schemes. The new adaptive masking mechanism 
is trained to select the elements in the latent space tensor that are most significant for the preservation of the image semantic content. Only these selected elements are then quantized, using vector quantization with the learned codebook. Finally, the decoder part of the scheme is formed by some learned blocks, trained to reconstruct the original image by minimizing an error function that includes its \gls{ssm}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Semantic_MQ-GAN/General_schema.pdf}
    \caption[General communication scheme]{Schematic representation of the overall coding/decoding scheme.}
    \label{fig: NOT Communication scheme}
\end{figure*}

The key innovations in the proposed \gls{sqgan} scheme are: (i) a novel \gls{gan}-based model that integrates semantic conditioning and compression directly into its architecture, (ii) the \gls{samm} that selects and encodes the most semantically relevant features of the input data based on the \gls{ssm}, (iii) a specifically designed data augmentation technique in the training phase %\textcolor{red}{[IN TRAINING?]}
to enhance the semantically relevant classes, and (iv) the inclusion (in training) of a semantic-aware discriminator to force the model to give more importance to the semantically relevant regions over the non-relevant ones.

This work is organized as follows. \sref{sec: notation} introduces the scope and general overview of the proposed architecture. \sref{ch: SQGAN} introduces in more detail the different parts of the proposed \gls{sqgan} model. \sref{sec: SQGAN training} discusses the training method with data augmentation for the enhancement of semantic relevant classes and the semantic-aware discriminator employed to optimize the \gls{sqgan}. Finally, \sref{sec: SQGAN numerical results} presents numerical experiments demonstrating the effectiveness of \gls{sqgan} compared to state-of-the-art 
image compression techniques such as \gls{jpeg2000} and \gls{bpg}. %\textcolor{red}{[IN THE ABSTRACT YOU ALSO TALK ABOUT AN EXAMPLE ONLINE OPTIMIZATION FOR EDGE COMPUTING ... IS IT STILL THERE? IT SHOULD BE MENTIONED HERE .. ]}


\section{Problem definition and general architecture}\label{sec: notation}
\thispagestyle{plain}

The objective of this work is to develop an image encoder that efficiently compresses an image $\x$ 
along with its semantic segmentation map $\s$, in order to enable the reconstruction of an image $\hat{\x}$ that preserves the semantic content of the original image, even at very low \gls{bpp} (see Fig.~\ref{fig: NOT Communication scheme}). 
The model leverages both the original image $\x$ and its corresponding \gls{ssm} $\s$, which provides pixel-wise class labels indicating the semantic class of each pixel in the image. 
The \gls{ssm} $\s$ is obtained using a \gls{sota} \gls{ssmodel} \cite{Wang2022internimage}. 
%possibly with the aid/integration of different sensors beyond the camera taking the image: 
%Our scheme makes no assumptions on how $\s$ is generated along $\x$ and therefore it can  integrate third-parties semantic segmentation tools.  

The encoder in \gls{sqgan} jointly processes $\x$ and $\s$ and produces two bit streams, which are then 
used to reconstruct $\hat{\x}$ and $\hat{\s}$. To evaluate the semantic quality of $\hat{\x}$, we also define
the ``generated'' \gls{ssm} $\tilde{\s}$, which is obtained by applying a a \gls{sota} \gls{ssmodel} (e.g., the same which was used to generate $\s$ from $\x$) to the reconstructed image $\hat{\x}$. 
Notice that, in general, $\s$, $\hat{\s}$, and $\tilde{\s}$, are different. 

By jointly encoding and decoding $\x$ and $\s$, and incorporating the reconstruction of $\hat{\s}$ in the decoder, 
it is possible to directly optimize the preservation of semantic information during the training process and use $\hat{\s}$ to condition the reconstruction $\hat{\x}$. This conditioning ``pushes'' the decoder to align $\hat{\x}$
with the semantic structure of the original image $\x$.  

As anticipated above, the ``generated'' \gls{ssm} $\tilde{\s}$ is obtained by applying a pre-trained \gls{sota} \gls{ssmodel}  to the reconstructed image $\hat{\x}$. 
By comparing $\tilde{\s}$ to the original $\s$, we can evaluate how well the reconstructed image $\hat{\x}$ retains semantic information recognizable by external models.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Semantic_MQ-GAN/Schema_Semantic_MQGAN.pdf}
    \caption[Scheme of the proposed SQ-GAN]{Encoder and decoder detailed structure of the proposed SQ-GAN scheme. The ``channel'' here may represent transmission or storage, depending on the application.}
    \label{fig: SQGAN Scheme masked Sementic VQ-GAN}
\end{figure*}

The overall architecture of the proposed \gls{sqgan} scheme is shown in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN}, where
the original image $\x$ and the corresponding 
original \gls{ssm} $\s$ are both represented by 3-dimensional tensors. 
%In the context of tensors, the term "shape" refers to the dimensions of the tensor, that is, how many elements exist in each dimension. The term "size"  refers to the total number of elements present in the tensor.
The image $\x$ is a tensor of size $3 \times H \times W$, where $3$ refers to the RGB channels, 
$H$ is the height and $W$ is the width of the frame (in pixels).
In this work, we have used images of size $H = 256$ and $W = 512$.
The \gls{ssm} $\s$ is a tensor of size $n_c \times H \times W $, where $n_c$ refers to the number of semantic classes, $H$ is the height and $W$ is the width of the frame. 
\textcolor{black}{Each $n_c$-dimensional vector in pixel position $(h,w)$ represents the 1-hot encoding of the corresponding semantic class of the pixel. This representation is quite standard in this setting and has proven to be convenient in 
a large number of works on semantic-conditioned image generation.}


%Leveraging ideas from \gls{vqvae} \cite{Oord2017VQ-VAE, Huang2023MaskedVQ-VAE} and \gls{vqgan} \cite{Esser2O21Taming},  the generator $G$ in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} consist the compression algorithm model, the discriminator $D_{disc}$ is responsible for the adversarial training and considered in \sref{sec: SQGAN training}. This networks are further divided in two sub-networks each: $G_\x$ and $G_\s$ and,  $D_{disc}^\x$ and $D_{disc}^\s$, respectively. 

The compression and reconstruction of the image $\x$ and of its \gls{ssm} $\s$ is accomplished by two parallel (and mutually interacting) pipelines. We use apex/subscripts $\x$ and $\s$ to indicate blocks of the 
specific pipeline, \textcolor{black}{in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} they are represented by the beige and green color, respectivelly.} In particular, the scheme is formed by two encoder-decoder pairs, $E_\x$-$D_\x$ and $E_\s$-$D_\s$, respectively. The semantic encoder $E_\s$ take as input the original \gls{ssm} $\s$ and outputs 
a latent tensor $\z^\s=E_\s(\s)$ of shape $C \times H_{16} \times W_{16} $ 
where $C=256$, $H_{16}=\frac{H}{16}$ and $W_{16}=\frac{W}{16}$. 
At the same time the \gls{ssm} $\s$ and the image $\x$ are combined by the proposed \gls{spe} to influence the feature extraction of the image with the \gls{ssm}. This new representation $\h_{SemPE}(\x, \s)$ is then encoded 
by $E_\x$ to obtain a latent tensor $\z^\x = E_\x(\h_{SemPE}(\x, \s))$ with the same shape as $\z^\s$.
Both tensors $\z^\x$ and $\z^\s$  are composed of $K=H_{16} \times W_{16}$ latent vectors $\z_k^\x$ and $\z_k^\s$ 
in $C$ dimensions. 
%The entries of these vectors are real numbers (e.g., in practice, this is accomplished by  using floating-point arithmetic or integer arithmetic over a very large number of bits).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%For training, leveraging ideas from \gls{vqvae} \cite{Oord2017VQ-VAE, Huang2023MaskedVQ-VAE} and \gls{vqgan} %\cite{Esser2O21Taming},  we identify the pair 
%
%the generator $G$ in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} consist the compression algorithm model, the %discriminator $D_{disc}$ is responsible for the adversarial training and considered in \sref{sec: SQGAN training}. This 5networks are further divided in two sub-networks each: $G_\x$ and $G_\s$ and,  $D_{disc}^\x$ and $D_{disc}^\s$, %respectively. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \gls{samm} blocks select the most semantically relevant latent vectors. The number of selected latent vectors depends on a masking fraction variable $m^\x$, or $m^\s$, that can be set as a parameter.  Hence,  
only $N_\x=m^\x K$ and $N_\s=m^\s K$ selected latent vectors from $\z^\x$ and $\z^\s$ are quantized, respectively. 

%The generator $G_\x$ and $G_\s$ make use of vector quantization codebooks $\C_\x$ and $\C_\s$, respectively. 
Vector quantization (VQ) is applied separately to each selected vector, where the quantization codebook is formed by 
$J$ learnable $C$-dimensional vectors (quantization codewords) denoted by $\e_j^\x$ and $\e_j^\s$, $j \in \{1, \ldots, J\}$, respectively. Therefore, each quantized latent vector is identifies by a binary index of 
$\log_2 J$ bits.  In this work we chose $J=1024$ and the \gls{l2} distance as the quantization 
metric (more in \sref{sec: SQGAN quantization}. The quantized latent tensors are denoted by $\z_q^\x$ and $\z_q^\s$, respectively.
The list of quantization indices $\e_j^\x$ and $\e_j^\s$, including the positions of the discarded non-relevant indices, are binary encoded and from the binary encoded stream to be stored or transmitted. 

At the receiver side, the binary stream is decoded and formatted into latent space tensors $\z_q^\x$ and $\z_q^\s$. Following the same procedure as in \cite{Huang2023MaskedVQ-VAE}, the latent tensors contain the quantized 
vectors in the positions where these vectors were selected and effectively quantized, and 
a ``placeholder'' codeword $M_\x$ or $M_\s$ in the positions these were discarded. These placeholder codewords
are not necessarily all-zero vectors, and can be learned in the traning phase in order to faclitate reconstruction. 
These latent space tensors are then processed by the \gls{adm} module to produce the reconstructed latent space tensors $\hat{\z}^\x$ and $\hat{\z}^\s$, respectively.

Finally, the latent tensor $\hat{\z}^\s$ is processed by the decoder $D_\s$ to produce the reconstructed $\hat{s}$ and
the latent tensor $\hat{\z}^\x$ is processed by the decoder $D_\x$ to produce the reconstructed $\hat{x}$ 
with conditioning provided by $\hat{s}$.


\textcolor{black}{In the next sections we will examine the detailed architecture of the blocks in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN}, and the overall training method. 
In particular, it is important to clarify that the training of the proposed \gls{sqgan} is performed via an adversarial approach based on 
two discriminators. However, in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} only the structure of the generator is represented. In fact, the generator is the part used at the running time, while the discriminators will only play a role in training. }


\section{Masked Semantic VQ-GAN (SQ-GAN)}\label{ch: SQGAN}
This section describes in detail the individual blocks of the proposed \gls{sqgan} architecture.

\subsection{Semantic Encoder}\label{sec: SQGAN semantic Encoder}
The encoder $E_\s$ maps the $n_c \times H \times W$ \gls{ssm} $\s$ to a latent tensor $\z^\s=E_\s(\s)$ with shape $C \times H_{16} \times W_{16}$, where \textcolor{black}{$C=256$ is the hyperparameter representing the number of channels} and $H_{16}$ and $W_{16}$ are the height and width.
$E_\s$ is formed by a repeated sequence of two \glspl{resblock} and one down-scaling layer, that will be referred to as \gls{resblockdown}. The average pooling down-scaling layer halves the height $H$ and width $W$ of the \gls{ssm}. 
%After the first \gls{resblockdown}, the intermediate latent representation will have a shape of $\frac{H}{2}=H_{2}$ and $\frac{W}{2}=W_{2}$. 
By applying 4 cascaded stages of \gls{resblockdown}, the final latent tensor has shape $H_{16} \times W_{16}$. 
The value of 4 \glspl{resblockdown} staged has been chosen via extensive ablation study
to achieve a good balance between compression and reconstruction accuracy. 
In particular, changing to 3 or 5 stages has effects on the performance of the whole architecture, causing poor compression in the first case and poor semantic preservation in the second. 
%A total if 3 consecutive \glspl{resblockdown} will guarantee better reconstruction quality paying the price of higher \glspl{bpp}. On the other hand, the 5 \glspl{resblockdown} will compress more at the expense of the reconstruction quality. For this reason, after training the model with a different number of consecutive \glspl{resblockdown} it was concluded that 4 represent the best trade-off between compression and reconstruction.\\
The output of the last \gls{resblockdown} is processed by the final multi-head self-attention layer of the encoder \textcolor{black}{as in \cite{Huang2023MaskedVQ-VAE}}.
% This concludes the description of the semantic encoder $E_\s$. A block composed of four consecutive \gls{resblockdown} and one multi-head self-attention layer.  This block takes as input the \gls{ssm} $\s$ and outputs a latent tensor $\z^\s = E_\s(\s)$. The output shape of $\z^\s$ is $C \times H_{16} \times W_{16}$, where $C$ is a hyperparameter usually set to $C=256$.

\subsection{Image Encoder}\label{sec: SQGAN image Encoder}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Semantic_MQ-GAN/Schema_Masker_SQGAN_vs_MQGAN.pdf}
    \caption[\acrshort{samm} architecture scheme]{Architectural diagram of the \acrshort{amm} as in \cite{Huang2023MaskedVQ-VAE} (left), and the proposed \acrshort{samm} employing the \acrshort{spade} layer to introduce the \acrshort{ssm} conditioning (right).}
    %{Comparison between AMM on the left and the proposed SAMM on the right. The strength of the SAMM is the conditioning on the SSM $\s$ via the \gls{spade} layer.}
    \label{fig: SQGAN SemAdaptiveMask module vs old}
\end{figure*}

The image encoder $E_\x$ maps the image $\x$ to the latent representation $\z^\x$ \textcolor{black}{of shape $C \times H_{16} \times W_{16}$},  with conditioning provided by $\s$.  $E_\x$ has the same structure of $E_\s$. 
The main difference lies in the conditioning process. In fact, not all the parts of the image $\x$ have the same semantic meaning and relevance: for example, in assisted driving applications, 
pedestrians are more important than the sky. 

To help the encoder $E_\x$ to assign different importance to different parts, the image $\x$ is modified before encoding. The proposed method draws inspiration from the version of the \gls{pe} introduced by Dosovitskiy et al. in transformer networks for vision tasks \cite{Dosovitskiy2021ViT}. This scheme divides the image into $16 \times 16$ patches and 
assigns a specific \gls{pe} vector to every patch in order to allow the transformer to correctly interpret the relative positions of the patches in the frame. In our case, \gls{pe} as proposed in \cite{Dosovitskiy2021ViT} is not needed since,  unlike transformer architectures, the spatial correlations are preserved in convolution-based architectures. However, the idea of assigning different weights to different semantic regions of the image 
is used to improve the overall performance. Hence, we propose a variation of \gls{pe}, referred to as \gls{spe}. 
The goal of \gls{spe} is to provide the encoder $E_\x$ with a suitable transformation of $\x$ and $\s$ 
that can be used to influence the feature extraction process. To this purpose, $\s$ is processed by \gls{spe}, 
a two layer \gls{cnn} designed to take into account the semantic classes of adjacent pixels. 
%
%and provide a meaningful, learnable transformation of the \gls{ssm} \textcolor{black}{of shape $3 \times H \times W$}. 
In parallel, the image $\x$ is processed by a one-layer \gls{cnn} called $InputNet_\x$. 
Both outputs of \gls{spe} and \gls{cnn} are tensors of shape $128 \times H \times W$.
Then, as in classical \gls{pe}, the input of the encoder is formed by the elementwise sum ${\rm InputNet}_\x(\x) + {\rm SemPE}(\s)$. 
This is then processed by the encoder $E_\x$ that produces 
the latent tensor $\z^\x = E_\x(\h_{SemPE}(\x, \s))$ of shape $C \times H_{16} \times W_{16}$. 

\subsection{Semantically Conditioned Adaptive Mask Module}\label{sec: SQGAN SemAdaptiveMask}

The latent tensors $\z^\x$ and $\z^\s$ can be interpreted as collections of $K =H_{16} W_{16}$ 
vectors in $\mathbb{R}^C$. The next step is to select the most relevant $N_\x=m_\x K$ vectors in 
$\z^\x$ and $N_\s=m_\s K$ vectors in $\z^\s$, where the relevance masking fraction $m_\x \in (0,1]$ and $m_\s \in (0,1]$ are design parameters.  Such selection is performed by the \gls{samm} blocks.
The following discussion will consider only the latent tensor $\z^\x$
since the operations introduced in this section, in \sref{sec: SQGAN quantization} and in \sref{sec: SQGAN ADM} apply symmetrically to the $\s$ pipeline. 

\gls{samm} is a variation of the \gls{amm} introduced in \cite{Huang2023MaskedVQ-VAE}. The idea is to assign to each of the $K$ latent vectors a relevance score and then select the $N_\x$ vectors with the highest score. 
In the proposed \gls{samm}, the relevance score is conditioned on the \gls{ssm} $\s$ and the masking fraction $m_\x$ can be adjusted dynamically. This allows the network to use the same weights to compress images at different levels of compression. The differences in architecture between \gls{amm} and \gls{samm} are shown in \fref{fig: SQGAN SemAdaptiveMask module vs old}.
While the classic \gls{amm}, on the left, is more suitable for general purpose applications, the \gls{samm} on the right is designed to take into account the semantic class of the different regions of the image. The new \gls{samm}  enforces this conditioning thanks to the \gls{spade} normalization layer \cite{Park2019SPADE}. 
\gls{samm} takes as input the latent tensor $\z^\x$ and the \gls{ssm} $\s$ and outputs a relevance score $\alpha_k^\x \in [0,1]$ for each latent vector $\z_k^\x$. The final step involves the multiplication of the selected latent vectors by their respective relevance scores. This is done to allow backpropagation to flow through \gls{samm} and train its parameters, as discussed in \cite{Huang2023MaskedVQ-VAE}.
%the process of selecting the elements with the highest relevance score is not differentiable and without the product it would be impossible to train the \gls{samm}.\\
%This completes the masking process where, starting from the output of the encoder, a list of the most relevant $N_\x$ latent vectors is obtained alongside with a list with the relative position of these vectors in the latent tensor $\z^\x$.\\


\subsection{Quantization and Compression}\label{sec: SQGAN quantization}

After selecting the $N_\x$ latent vectors with the highest score, the selected vectors are suitably quantized, while
the non-relevant $K-N_\x$ latent vectors are just dropped.
The vector quantization process follows the same steps as the classic \gls{maskvqvae} \cite{Huang2023MaskedVQ-VAE}. 
A learnable codebook $\C_\x = \{ \e_j^\x: j = 0, \ldots, J-1\}$ is used, 
where in this application we found convenient to 
use $J=1024$ codewords of dimensionality $C=256$.
For each selected score-scaled relevant vector $\z_k^{'\x}= \alpha_k^\x \z_k^\x$, the codeword at minimum distance is found, i.e., the codeword index $j$ is selected such that
\begin{equation}
    j = \underset{i \in \{1, \dots, J\}}{\text{argmin}} \|\z_k^{'\x} - \e_i^\x\|^2,
    \label{eq: GM vq-vae quantization}
\end{equation}
Then, the sequence of quantization indices is binary encoded using entropy coding.
Every latent vector selected for quantization is located in some position in a $H_{16} \times W_{16}$ shaped array. 
Therefore, the quantized array is formed by $N_\x$ positions containing quantization indices (i.e., integers from 0 to $J-1$ selected as in \eqref{eq: GM vq-vae quantization}) and $K-N_\x$ ``empty'' positions corresponding to the discarded vectors. We assign to all discarded positions an additional special index (conventionally denoted as -1), 
such that we can interpret the quantized array as discrete information source over an alphabet of size $J+1 = 1025$.
The index -1 appears with probability $1-\frac{N_\x}{K} = 1 - m_\x$, while the other indices from 0 to $J-1$ 
appear with some probabilities $\beta_j$ such that $\sum_{j=0}^{J-1} \beta_j = m_\x$. 
In general, this probability is unknown and depends on the specific image $\x$ to be encoded. 
However, using the fact that entropy is maximized by the uniform probability distribution, we obtain an upper bound to the length of the entropy-coded binary sequence by assuming $\beta_j = \frac{m_\x}{J}$.
Therefore, the entropy coding rate for the index sequence is upper-bounded by:
\begin{equation}
    R_\x = h_2\!\left (m_\x\right) + m_x \log_2(J),
\end{equation}
where $h_2(p) = -(1-p)\log_2(1 - p) - p \log_2(p)$ is the binary entropy function. 
Since the index sequence length is $K$, the number of bits necessary to represent such a sequence is therefore upper-bounded by $B_\x = R_\x K$. Notice that this is indeed a brute-force bound, and in fact any suitable 
more refined compression scheme (e.g., using arithmetic coding coupled with Krichevsky-Troﬁmov probability sequential estimation) would achieve a lower rate. 
\begin{comment}
Use this reference: 
R.E. Krichevsky and V.K. Troﬁmov, “The Performance of Universal Encoding,” IEEE Trans. Inform. Theory, vol. IT-27, pp. 199-207, March 1981.
\end{comment}
However, in this work this value is further approximated. To maintain a linear relationship between the number of bits and the number of relevant latent vectors the condition $h_2(p) \leq 1$ is used. 
By substituting $h_2(p)=1$ the final amount of bits is expressed as follows:
\begin{equation}
    B_\x = K + \log_2(J) N_\x = K(1+ m_\x \log_2(J)).
\end{equation}
As will be pointed out in \sref{sec: SQGAN numerical results}, even under this coarse upper bound, 
the \gls{sqgan} consistently outperforms competing state-of-the-art image compression algorithms. 
By considering the same approach for the $N_s$ relevant vectors in $\z_\s$, $J = 1024$, 
and normalizing by the total number of pixels $H W$, we can express the overall coding rate in \gls{bpp} as a function of $m_\x$ and $m_\s$ as: \footnote{In our results, when referring to \gls{bpp}, it is generally intended this total \gls{bpp} 
unless when differently specified.}

\begin{equation}
    {\rm BPP} = \frac{B_\x + B_\s}{H W} = \frac{1}{256}[10 (m_\x + m_\s) + 2].
    \label{eq: SQGAN BPP}
\end{equation}
(notice that $K/(HW) = H_{16} W_{16}/(HW) = 1/256$).

\subsection{Tensor Reconstruction and Adaptive De-Masking Module}\label{sec: SQGAN ADM}

At the receiver, the codebooks $\C_\x$ and $\C_\s$ are stored in the decoder. The bit stream is decoded and the array of 
quantization indices is retrieved. Then, a tensor of shape  $C \times H_{16} \times W_{16}$ is obtained by 
placing quantization codewords $\e_j^\x$ in correspondence of indices $j \in \{0, \ldots, J-1\}$ and a placeholder codeword denoted as $\bM_\x$ in correspondence of all indices equal to -1 (i.e., in the position of the discarded latent space vectors). A similar operation is done for the $\s$ pipeline, where the placeholder is denoted by $\bM_\s$. 
The placeholders are also learned in the trainign phase. 

The resulting tensor is processed by the Adaptive De-Masking Module (\gls{adm}). This step is similar to what introduced in \gls{maskvqvae} \cite{Huang2023MaskedVQ-VAE} and makes use of a direction-constrained self-attention mechanism to gradually let the information flow from the relevant $\e_j^\x$ to the non-relevant placeholder $\bM_\x$. This can be interpreted as a sort of  non-linear ``interpolation'' of the latent space tensor in the discarded positions. 
The output $\hat{\z}^\x$ of the \gls{adm} is then used as the input of the decoder $D_\x$. Similarly, the output 
$\hat{\z}^\s$ of the \gls{adm} in the $\s$ pipeline forms the input of the decoder $D_\s$.
At this point, the flows for the two pipelines for $\x$ and for $\s$ diverge again, as described in the next two subsections.

\subsection{Semantic Decoder}\label{sec: SQGAN semantic Decoder}

The structure of the decoder $D_\s$ is similar to a mirrored version of the encoder $E_\s$. The first input layer is composed of the multi-head self-attention layer, after which the series of \glspl{resblockup} is placed. Every \gls{resblockup} is composed of two consecutive \glspl{resblock} and one up-scaling layer. The up-scaling is performed by copying the value of one element in the corresponding up-scaled $2\times2$ patch. The goal is to gradually transform the latent tensor $\hat{\z}^\s$ from a shape of $C\times H_{16} \times W_{16}$ back to the original shape of $n_c \times H \times W$, the same as the \gls{ssm} $\s$. This decoding and up-scaling process is completed by using four consecutive \glspl{resblockup}. 
The final reconstructed \gls{ssm} $\hat{\s}$ is obtained \textcolor{black}{by the standard practice of applying the argmax operator  to assign each pixel to a specific semantic class \cite{Ronneberger2015Unet}}. The reconstructed \gls{ssm} $\hat{\s}$ can now be used to condition the reconstruction $\hat{\x}$ of the image $\x$.


\subsection{Image Decoder}\label{sec: SQGAN image Decoder}

The reconstruction of $\x$ is obtained by applying the decoder $D_\x$ to $\hat{z}^\x$  and to the reconstructed \gls{ssm} $\hat{\s}$. This decoder has a structure similar to the mirrored version of the encoder $E_\x$, with some key conceptual differences. The input multi-head self-attention layer \textcolor{black}{has the same structure as the one used in the semantic decoder.}. The sequence of four consecutive \gls{resblockup} is modified to incorporate the conditioning via the \gls{ssm}. The modifications focus on the normalization layers within the \glspl{resblock}, replacing every normalization layer with the \gls{spade} layer. This new layer is responsible for enforcing the structure of the \gls{ssm} during the decoding phase \cite{Park2019SPADE}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training and Inference}\label{sec: SQGAN training}

\textcolor{black}{So far we have provided a structural and functional description of the blocks that form the encoder (transmitter) and decoder (receiver) in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN}. This specifies the role and action of each block at runtime, but does not say how the blocks are trained. In this section, we focus on the training phase. We consider the whole proposed model in \fref{fig: SQGAN Scheme masked Sementic VQ-GAN} as composed of two pipelines: one is used for the compression and reconstruction of $\s$, denoted by $G_\s$ (depicted in beige), the other for the compression and reconstruction of the $\x$, denoted by $G_\s$ (depicted  in green).  The training of these generators is obtained in an adversarial way, according to the GAN principle, i.e., the generators are coupled with corresponding discriminators, denoted as $D_{disc}^\s$ and $D_{disc}^\x$, respectively, for the sole purpose of training. The training scheme for $G_\s$ is shown in Fig.~\ref{fig: SQGAN Gen_sem sqgan}, and the one for $G_\x$ is shown in Fig.~\ref{fig: SQGAN Gen_img sqgan}. At runtime (i.e., when the trained model is applied to a generic image for compression and reconstruction) the discriminators play no role.}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Semantic_MQ-GAN/G_ssm.pdf}
    \caption[\acrshort{sqgan} training pipeline of the  \acrshort{ssm} generator]{Schematic representation of the semantic generator network $G_\s$ training pipeline.}
    \label{fig: SQGAN Gen_sem sqgan}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Semantic_MQ-GAN/G_img.pdf}
    \caption[\acrshort{sqgan} training pipeline of the image generator]{Schematic representation of the image generator network $G_\x$ training pipeline.}
    \label{fig: SQGAN Gen_img sqgan}
\end{figure*}

The two pipelines are intended to reconstruct objects in distinct domains -- the image domain and the \gls{ssm} domain -- each requiring different loss functions. 
After intensive numerical experimentation, we realized that
training the entire network as a monolithic entity (i.e., of both pipelines coupled together as in Fig.~\ref{fig: SQGAN Scheme masked Sementic VQ-GAN})  is not efficient. 
Additionally, it is useful to consider the masking fractions $m_\s$ and $m_\x$ as variable rather than fixed as in \cite{Huang2023MaskedVQ-VAE}. This allows \gls{sqgan} to compress images and \glspl{ssm} at various compression levels, but also increases the training complexity. 
Hence, to effectively train \gls{sqgan}, we devised a multi-step approach consisting of three stages: (i) train $G_\s$ using the original $\s$, (ii) train $G_\x$ with the original $\x$ and $\s$ and (iii) fine-tune the entire network, denoted by $G$, using the original $\x$, original $\s$, and the reconstructed $\hat{\s}$ by freezing $G_\s$'s parameters and only fine-tuning $G_\x$'s parameters. 
Initially training $G_\s$ ensures that the \gls{ssm} is accurately reconstructed independently of the rest. At the same time, training $G_\x$ with the original $\x$ and $\s$ allows learning the conditional dependencies required for image reconstruction based on a reliable \gls{ssm}. However, since the decoder part in $G_\x$ is later required to be conditioned by the reconstructed $\hat{\s}$ rather than the original $\s$, the fine-tuning step is crucial to adapt $G_\x$ to mitigate the imperfections of $\hat{\s}$ that are not present in $\s$.

In addition to the multi-step training approach, another crucial challenge is the identification and preservation of semantically relevant information. To address this problem the training process has been further improved by proposing and incorporating a data augmentation step, adding semantically relevant classes, and a {\em semantic-aware discriminator network}. The idea is to emphasize the importance of semantically relevant classes. For example, our numerical experiment we focused on dashboard camera views for assisted driving applications, where the semantic classes of interest are 
(for example) "traffic signs", "traffic lights", and  "pedestrians". 
These typically occupy a small portion of the frame and are not always present in every frame.
In contrast, non-relevant classes such as "sky", "vegetation", and "street", appear much more frequently and occupy large portion of the images. Therefore, data augmentation is used to prevent common but irrelevant semantic classes to dominate the training. 

This section is divided as follows. \sref{sec: SQGAN Data Augmentation} will present the data augmentation process discussing commonly used techniques and the proposed semantic relevant classes enhancement method. \sref{sec: SQGAN training G_s} will focus on the training of $G_\s$ while \sref{sec: SQGAN training G_x} will focus on the training of $G_\x$ with the introduction of the proposed semantic-aware discriminator. Finally, \sref{sec: SQGAN training G} will focus on the final fine-tuning process. 
\textcolor{black}{The dataset used in training is the Cityscapes dataset \cite{Cordts2016Cityscapes} composed of 2975 pairs of images and associated SSMs.}

\subsection{Data Augmentation}\label{sec: SQGAN Data Augmentation}
\begin{figure*}[!t]
    \centering
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/traffic_sign_nonagum_image.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/traffic_sign_agum_image.png}}\\[1mm]
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/traffic_sign_nonagum_ssm.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/traffic_sign_agum_ssm.png}}
    \caption[Data Augmentation for \acrshort{sqgan}]{Effect of combined data augmentation techniques, including rotation, cropping and the proposed semantic relevant classes enhancement. On the left, the original image and \gls{ssm} and on the right the resulting augmented version.}
    \label{fig: SQGAN Data augmentation}
\end{figure*}


This section introduces a novel data augmentation technique, the  semantic relevant classes enhancement, designed to solve the problem of underrepresented but semantically relevant classes in datasets. 
%The core idea lies in the assumption that some specific classes, i.e. "traffic signs" and "traffic lights", are crucial in applications like autonomous driving but unfortunately underrepresented. In fact they often occupy a small portion of the frame and are not always present in every frame. Enhancing their representation is fundamental for improving the model's ability to efficiently reconstruct these critical classes.
This new data augmentation technique is fundamentally different from previously proposed methods (e.g., \cite{Konushin2021dataAug1, Jockel2021dataAug2}). These approaches often focus on swapping existing objects -- for example, replacing a "turn left" sign with a "stop" sign -- or employ complex \gls{nn} architectures to introduce new objects into images. These approaches can lead to increased computational costs or fail to adequately address the under-representation of critical classes. In contrast, the proposed technique is straightforward, efficient, and fast.

For the sake of illustration, let us focus on data augmentation to address the under-representation of "traffic signs" and "traffic lights". The process is based on the use of mini-batches of pairs $(\x,\s)$.
For each image in a mini-batch, the \gls{ssm} is used to identify all instances of "traffic signs" and "traffic lights" present in that image. Then, each image and its corresponding \gls{ssm} is augmented 
by adding more instances of these critical classes. This is achieved by copying "traffic signs" and "traffic lights" from other images within the same mini-batch and pasting them into the current image and \gls{ssm}. By increasing the presence of these objects, the model will be more exposed to them during training.

The process begins by collecting all "traffic signs" and "traffic lights" from the images and \glspl{ssm} in the current mini-batch. For each image in the mini-batch, a random number $n$ between $0$ and $25$ is selected, representing the number of objects to add. Then, $n$ objects are randomly chosen from the collected set and are carefully placed into the image and its \gls{ssm}. The placement is done to avoid overlapping with existing instances of the same classes or with the other relevant class. An example of an augmented pair $(\x,\s)$ is represented in \fref{fig: SQGAN Data augmentation} with the original pair on the left and the data augmented version on the right. Other classic data augmentation techniques like  cropping, rotation and color correction are also applied. 
However, it is interesting to notice the increase in "traffic signs" (yellow), and "traffic lights" (orange) in the augmented $\x$ and $\s$.


\subsection{Training $G_\s$}\label{sec: SQGAN training G_s}

After data augmentation, the training of the sub-network $G_\s$ is implemented as depicted in \fref{fig: SQGAN Gen_sem sqgan}, based on an adversarial approach \glspl{vqgan}.\\
The input $\s$ and output $\hat{\s}$ tensors of $G_\s$ have shape $n_c \times H \times W$. We considered the following loss function:
\begin{equation}
    \Loss_{\text{SQ-GAN}}^\s = \Loss_{\text{WCE}}  + \lambda_{\text{GAN}} \Loss_{\text{GAN}} + \lambda_{\text{vq}} \Loss_{\text{vq}} + \lambda_{\text{commit}} \Loss_{\text{commit}},
\end{equation}
where all the terms are defined as in \cite{Esser2O21Taming}, with the exception of the weighted cross-entropy 
loss $\Loss_{\text{WCE}}$, defined as: 
\begin{equation} 
    \Loss_{\text{WCE}}(\s, \hat{\s}) = - \sum_{(h,w)} \text{w}_{\s_{(h,w)}} \s_{(h,w)} \log\left( \s_{(h,w)} \cdot \hat{\s}_{(h,w)} \right),
\end{equation}
%
%
%
where $\s_{(h,w)}$ and $\hat{\s}{(h,w)}$ denote the one-hot encoded vectors at pixel location $(h,w)$
in $\s$ and $\hat{\s}$, respectively, and $\text{w}_{\s_{(h,w)}} \geq 0$ is the weight associated with the semantic class at that pixel. 
This choice is motivated by the nature of the \gls{ssm}, which is a pixel-wise classification map where each pixel belongs to a specific semantic class. 
The weights are assigned to emphasize the importance of semantically relevant classes over non-relevant ones, encouraging the model to focus more on reconstructing the critical classes. Specifically, the weight factors are set as follows:

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\text{w} = 1$  & for the relevant classes "traffic signs" and \\ & "traffic lights". \\
    \textbullet \;\; $\text{w} = 0.85$  & for the classes "people" and "rider". \\
    \textbullet \;\; $\text{w} = 0.20$  & for the non-relevant classes "sky" and \\ & "vegetation". \\
    \textbullet \;\; $\text{w} = 0.50$  & for all the other classes.
\end{tabular}\\

The other important component involved in training is the discriminator network $D_{disc}^\s$ \cite{isola2017image2image}.  This is composed of a convolutional layer, batch normalization layer and a leaky ReLU \cite{Bing2015Rectified} repeated 3 times and followed by the last convolutional 
layer that maps the output to a single number. This value is the output of the discriminator used to classify the \gls{ssm} as real or fake.
During training, the masking fraction $m_\s$ is varied randomly, selected from a set of values ranging from 5\% to 100\% with and expected value of 35\%. This approach allows the model to learn to compress the \gls{ssm} at various levels of compression.
The sub-network $G_\s$ is trained using the Adam optimizer \cite{Kingma2015Adam} with a learning rate of $10^{-4}$ and a batch size of 8. The training is conducted for 200 epochs with early stopping to prevent overfitting.

\subsection{Training $G_\x$}\label{sec: SQGAN training G_x}

The training of the sub-network $G_\x$ follows a similar approach. As said before, at this stage 
$G_\x$ is trained using the original image $\x$ and the original \gls{ssm} $\s$, as illustrated in \fref{fig: SQGAN Gen_img sqgan}.
Again, an adversarial approach typical of \glspl{vqgan} is employed for training, with the loss function defined as:
\begin{align}
    \Loss_{\text{SQ-GAN}}^\x = & \Loss_{WL_2}  + \Loss_{\text{perc}} + \lambda_{\text{GAN}} \Loss_{\text{GAN}} + \lambda_{\text{vq}} \Loss_{\text{vq}} \\ &+ \lambda_{\text{commit}} \Loss_{\text{commit}},
\end{align}
with the last three terms as in \cite{Esser2O21Taming} and the first two described as follows.
The weighted $\ell_2$ loss $\Loss_{WL_2}$, defined as:
\begin{equation} 
\Loss_{\text{W}l_2}(\x, \hat{\x}) = \frac{1}{H W}\sum_{(h,w)} \text{w}_{\s_{(h,w)}} \| \x_{(h,w)} - \hat{\x}_{(h,w)} \|^2, \label{eq: weighted_l2_loss} 
\end{equation}
is designed to adjusts the importance of different semantic classes in the image. 
In \eqref{eq: weighted_l2_loss}, $\x_{(h,w)}$ and $\hat{\x}{(h,w)}$ are the pixel values at location $(h,w)$ of the tensors $\x$ and $\hat{\x}$, respectively, and $\text{w}_{\s_{(h,w)}}$ is the weight associated with the semantic class at that pixel, as given by the \gls{ssm} $\s$. In our experiments, we used 
the following set of weights: 

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\text{w} = 1$    & for the relevant classes "traffic signs" and \\ &  "traffic lights". \\
    \textbullet \;\; $\text{w} = 0.55$ & for the classes "people" and "rider". \\
    \textbullet \;\; $\text{w} = 0$    & for the non-relevant classes "sky" and \\ & "vegetation". \\
    \textbullet \;\; $\text{w} = 0.15$ & for all other classes.
\end{tabular}\\

Among all possible choices of the weights, the most interesting is the one concerning the weights for "sky" and "vegetation", set to zero. This choice has been made to force the network to neglect the pixel-by-pixel reconstruction of these classes. In fact, the real color of the sky and trees in the image are semantically irrelevant. 

The perceptual loss is instead the part that ensures that the reconstructed image maintains visual similarity to the original, preventing unrealistic alterations such as an unnatural sky color. To quantify this loss, we used  \gls{lpips} \cite{Zhang2018LPIPS}, which evaluates the difference between two images in a latent space representation. The idea is that, if two images convey similar semantic content, like for example presence of relevant objects, as pedestrians, cars or traffic lights, each with the right shape, and well positioned relatively to each other, we should have a low (semantic) distortion, irrespective of the difference at the pixel level. LPIPS, instead of computing pixel-wise differences between $\x$ and $\hat{\x}$,   
compares the latent features extracted from the images by employing a deep neural network (DNN) pre-trained for object recognition. The DNN
is somehow substituting a human to quantify the perceptual difference between two images. 
Formally, LPIPS is defined as:
\begin{equation}
\label{LPIPS}
    \Loss_{\text{perc}} = \sum_{l\in L} \frac{1}{H_l W_l} \sum_{(h_j,w_j)} \left\| \text{w}_l \odot  \left( \phi_i(\x) - \phi_i(\hat{\x})  \right) \right\|_2^2
\end{equation}
where $\phi_l(\cdot)$ denotes the activation from the $l$-th layer of a pre-trained network $\phi$, $L$ is the set of layers used for feature extraction, $H_l$ and $W_l$ are the height and width of the feature map at layer $l$, and $\text{w}_l$ are learned weights that adjust the contribution of each layer. The operator $\odot$ denotes element-wise multiplication between the weight $\text{w}_l$ and all the elements of the difference $\left( \phi_i(\x) - \phi_i(\hat{\x})  \right)$ in any location coordinate $(h_j,w_j)$. By comparing the feature representations at multiple layers, \glspl{lpips} captures perceptual differences at different scales and abstraction levels. Lower \gls{lpips} values indicate better reconstruction quality. 

Another important novel aspect of the proposed approach is the 
adversarial loss $\mathcal{L}_{\text{GAN}}$ involving a new semantic-aware discriminator network. 
The discriminator $D_{disc}^\x$ plays a crucial role in adversarial training by determining whether an image reconstructed by $G_\x$ is real or fake.  However, a potential drawback is that the discriminator might focus on non-relevant parts of the image to produce its classification score. 
For instance, it might prioritize vegetation details, and classify images as real only if the leaves on the trees have a certain level of detail. This will force the generator $G_\x$ to reconstruct images with better vegetation details to fool the discriminator. Unfortunately, this is not optimal for the structure of the \gls{sqgan}. Giving more importance to the vegetation will decrease the importance of other classes, thus causing the \gls{samm} module  to select the wrong latent vectors as relevant.

In recent years, several works have proposed ways to modify the discriminator by introducing 
various conditioning. For example, \cite{Oluwasanmi2020condDiscr} conditioned the discriminator on the \gls{ssm} to improve \gls{sseg} retention, while \cite{Chen2020ssd} enforced the discriminator to focus on high-frequency components. However, these methods do not adequately address the issue at hand. 
To achieve the desired performance, it is essential to adjust the discriminator to minimize its focus on non-relevant regions. To this end, we make the following observations about the discriminator's behavior:
\begin{itemize}
    \item For a (generic) trained discriminator $D_{disc}$ and two images $\x$ and $\y$, $D_{disc}(\x)$ is likely similar to $D_{disc}(\y)$ if both images originate from the same data distribution, i.e., $p_{\x} = p_{\y}$. However, $D_{disc}(\x) = D_{disc}(\y)$ is not guaranteed unless the images are identical or indistinguishable by the discriminator.
    \item If $p_{\y}$ differs from $p_{\x}$, $D_{disc}(\y)$ will likely differ from $D_{disc}(\x)$. The output difference is primarily influenced by the aspects of $p_{\y}$ that deviate from $p_{\x}$.
    \item The greater the difference between $p_{\x}$ and $p_{\y}$, the higher the uncertainty in predicting $D_{disc}(\y)$ based on $D_{disc}(\x)$.
\end{itemize}
Based on these insights, a technique is proposed to reduce the discriminator's focus on non-relevant semantic classes.
The approach consists in artificially modifying the reconstructed image $\hat{\x}$ before it is evaluated by the discriminator. This modification aims to minimize the differences in non-relevant regions between $\x$ and $\hat{\x}$, bringing the data distribution of these regions of $\hat{\x}$ closer to the real distribution of $\x$. For example, if the generator reconstructs a tree with dark green leaves when in reality they are light green, the color in $\hat{\x}$ will be artificially shifted toward light green. 

This artificial editing is performed by considering the residual between the real and reconstructed images, defined as $\br = \x - \hat{\x}$. By masking this pixel-wise difference between the two images and adding back a fraction of the residual to $\hat{\x}$ it is possible to obtain the new image
$\hat{\x}_{rel} = \hat{\x} + \w_{rel} \odot \br$, 
where $\w_{rel}$ is the re-scaling relevance tensor. In our experiments, we have used the following values:

\vspace{0.2cm}
\begin{tabular}{ll}
    \textbullet \;\; $\w_{rel} = 0.90$    & for the class "sky". \\
    \textbullet \;\; $\w_{rel} = 0.80$ & for the class "vegetation". \\
    \textbullet \;\; $\w_{rel} = 0.40$    & for the class "street". \\
    \textbullet \;\; $\w_{rel} = 0$ & for the other classes.
\end{tabular}\\

For example, this means that  80\% of the difference between the shades of green leaves is removed before presenting the image to the discriminator. The modified vegetation in $\hat{\x}_{rel}$ will appear much closer to the real light green in $\x$, thus reducing the discriminator's focus on this non-relevant region.
It is important to acknowledge that this approach negatively impacts the generator's ability to accurately reproduce these specific non-relevant classes. Nevertheless, the perceptual loss still considers the entire image, guiding the generator to reconstruct the "sky," "vegetation," and "streets" to maintain overall realism.

The sub-network $G_\x$ is trained for different masking fractions $m_\x$. These values are selected from a finite set ranging from $5\%$ to $100\%$ with an expected value of $35\%$.
The structure of $D_{disc}^\x$ is the same of $D_{disc}^\s$ and the training is performed using the Adam optimizer with a learning rate of $10^{-4}$ with batch size of $8$. The model is trained for $200$ epochs with early stopping.


\subsection{Fine-tuning $G$}\label{sec: SQGAN training G}

The final step involves fine-tuning the entire network $G$ by freezing the parameters of $G_\s$ and updating only those of $G_\x$. This fine-tuning addresses the scenario where the original \gls{ssm} $\s$ is unavailable at the receiver, that is when the \gls{sqgan} is used for \gls{sc}.
Notice that the fine-tuning step is {\em essential} since the quality of $\hat{\s}$ is influenced by the masking fraction $m_\s$, and this dependency is missed in the separate training steps described above.
The loss function and the semantic-aware discriminator $D_{disc}^\x$ remain identical to those used in the training of $G_\x$. Fine-tuning is conducted over 100 epochs with early stopping to prevent overfitting. During each iteration, both masking fractions $m_\x$ and $m_\s$ are randomly selected from the same distribution. The Adam optimizer is employed with a learning rate of $10^{-4}$ and a batch size of 8.



\section{Results} \label{sec: SQGAN numerical results}

This section focuses on the performances of the proposed \gls{sqgan} evaluated using 500 pairs of images and \glspl{ssm} from the validation set of the Cityscape dataset \cite{Cordts2016Cityscapes}.

In \sref{sec: SQGAN result samm},  \textcolor{black}{we illustrate the function and role of the SAMM in selecting features of $\x$ and $\s$ that are semantically relevant. Additionally, we also show the effect of the masking fractions on the final reconstruction.} In \sref{sec: SQGAN result comparison} the performance of the proposed \gls{sqgan} is compared with state of the art compression algorithms like \gls{bpg} and \gls{jpeg2000}.

\subsection{SAMM function}\label{sec: SQGAN result samm}
\begin{figure*}[!t]
    \centering
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/real_image.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/masked_image.png}}\\[1mm]
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/real_ssm.png}}%
    \hspace{5mm}
    \adjustbox{valign=t}{\includegraphics[width=0.45\textwidth]{Figures/Semantic_MQ-GAN/masked_ssm.png}}
    \caption[Visual representation of the effect of the \acrshort{samm} module]{Visual representation of the latent tensor selection of the \acrshort{samm} projected in the image and \acrshort{ssm} domain. In both cases the masking has been fixed to $m_\x=m_\s=0.20$ and the region considered semantically relevant are shown on the right.}
    \label{fig: SQGAN masked images}
\end{figure*}

\begin{figure*}[!t]
    \centering
    % Text boxes above columns
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        Original & 
        $m_\x=0.95, \;m_\s=0.15$ & 
        $m_\x=0.55, \;m_\s=0.55$ & 
        $m_\x=0.15, \;m_\s=0.95$ \\
    \end{tabular}
    \\% Adjust space between text and images

    % Images with corresponding SSM overlayed on the top row
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image1) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image1.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image4) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/img_x95_s15.png}};
            \node[anchor=south east,inner sep=0] at (image4.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/masking/intern_ssm_x95_s15.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image3) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/img_x55_s55.png}};
            \node[anchor=south east,inner sep=0] at (image3.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/masking/intern_ssm_x55_s55.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image2) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/img_x15_s95.png}};
            \node[anchor=south east,inner sep=0] at (image2.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/masking/intern_ssm_x15_s95.png}};
        \end{tikzpicture} \\

        % new row for SSM
        \includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/real_ssm.png} & 
        \includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/ssm_x95_s15.png} & 
        \includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/ssm_x55_s55.png} & 
        \includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/masking/ssm_x15_s95.png} \\
    \end{tabular}
    
    \caption[Visual comparison between different results at different masking fractions]{Visual comparison between the same image and \acrshort{ssm} at different masking reactions $m_\x$ and $m_\s$. The original image and \acrshort{ssm} are shown on the left. The upper row shows the reconstructed $\hat{\x}$ and the generated \gls{ssm} using the \gls{sota} \gls{ssmodel} INTERN-2.5 \cite{Wang2022internimage}. The bottom row shows the reconstructed \gls{ssm} $\hat{\s}$. All pairs $(\hat{\x}, \hat{\s})$ are obtained at $0.05$\gls{bpp}.}
    \label{fig: SQGAN visual result changing masking}
\end{figure*}
\gls{sqgan} adopts various techniques to force the model to correctly reconstruct the relevant regions of the image. The weighted loss function based on the semantic classes, the semantic-aware discriminator $D_{disc}^\x$, and the semantic relevant classes enhancement data augmentation have been designed with this idea in mind. Their scope is to guide \gls{samm} to identify and select the relevant latent vectors $\z_k^\x$ and $\z_k^\s$. 

Since the selection is performed on the latent tensor, \textcolor{black}{to understand the effect of SAMM, we project back from the latent tensor to the image (pixel) domain \cite{Huang2023MaskedVQ-VAE}. \fref{fig: SQGAN masked images} shows such a projection for a specific example.} \footnote{The projection showed in figure \ref{fig: SQGAN masked images} was implemented by leveraging the convolutional structure of the two encoders that allows to trace back the spatial correlation between the latent tensor and the original frame.} 
In the left column the original $\x$ and $\s$ are represented, while the right column shows which are the regions associated to the latent vectors that \gls{samm} considers more relevant. In this example the masking fractions are chosen as $m_\x=m_\s=0.20$.

It is immediately evident that $G_\x$ and $G_\s$ consider different regions as relevant. The \gls{samm} in $G_\s$ focuses on regions with the most change in semantic classes. The street, building and sky require very few associated latent vectors to be represented. On the contrary the parts of the \gls{ssm} containing relevant classes are strongly  preferred. 

In a similar way, the \gls{samm} in $G_\x$ shows a strong preference for relevant classes like cars and people. However, it also focuses on areas previously ignored, such as the street and buildings. In fact, thanks to the conditioning on the \gls{ssm}, the sub-network $G_\x$ knows the location of every object and their shape and can focus on different aspects like colors and textures. For this reason the streets and sky will require some latent vectors to reconstruct the colors correctly. However, most of the latent vectors are selected from the truly relevant regions.

In both cases, \gls{samm} tends to prefer regions that contain more semantically relevant objects. This is a direct effect of the various techniques adopted to train the model as described in \sref{sec: SQGAN training}. \\
\begin{figure*}[!h]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/mIoU_mIoU_vs_m_s.pdf}
        \caption[\acrshort{ssm} retention as a function of the masking fraction]{\acrshort{ssm} retention  evaluated between the true $\s$ and the reconstructed $\hat{\s}$ with the \gls{miou} metric as a function of the masking fraction $m_\s$. As the masking fraction $m_\s$ increases the network $G_\s$ is able to better reconstruct the \gls{ssm}. However, the increase of performances reaches a plateau from $m_\s \geq 0.2$ ($BPP_\s=0.011$\gls{bpp}).}
        \label{fig: SQGAN miou vs m_s}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/LPIPS_3d.png}
        \caption[\acrshort{lpips} as a function of the masking fractions]{\acrshort{lpips} evaluated between $\x$ and $\hat{\x}$ as the masking fractions $m_\s$ e $m_\x$ vary.}
        \label{fig: SQGAN lpips 3d plot}
    \end{minipage}
\end{figure*}
\begin{figure*}[!h]
    \centering
    % Text boxes above columns
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        Original & 
        $0.019$\textit{BPP} & 
        $0.038$\textit{BPP} & 
        $0.078$\textit{BPP} \\
    \end{tabular}
    \\% Adjust space between text and images

    % Images with corresponding SSM overlayed
    \begin{tabular}{>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth} 
                    @{\hspace{1mm}}>{\centering\arraybackslash}m{0.24\textwidth}}
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image1) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image1.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image2) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/image_0194.png}};
            \node[anchor=south east,inner sep=0] at (image2.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/ssm_0194.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image3) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/image_0389.png}};
            \node[anchor=south east,inner sep=0] at (image3.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/ssm_0389.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image4) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/image_0780.png}};
            \node[anchor=south east,inner sep=0] at (image4.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/comparisons/img/ssm_0780.png}};
        \end{tikzpicture} \\
        % new row for BPG
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image5) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/real_image.png}};
            \node[anchor=south east,inner sep=0] at (image5.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/real_ssm.png}};
        \end{tikzpicture} & 
        % Empty slot for (row 2, col 2) 
        & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image7) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/comparisons/bpg/image_0383.png}};
            \node[anchor=south east,inner sep=0] at (image7.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/comparisons/bpg/ssm_0383.png}};
        \end{tikzpicture} & 
        \begin{tikzpicture}
            \node[anchor=south west,inner sep=0] (image8) at (0,0) {\includegraphics[width=\linewidth]{Figures/Semantic_MQ-GAN/comparisons/bpg/image_0785.png}};
            \node[anchor=south east,inner sep=0] at (image8.south east) {\includegraphics[width=0.45\linewidth]{Figures/Semantic_MQ-GAN/comparisons/bpg/ssm_0785.png}};
        \end{tikzpicture} \\
    \end{tabular}
    
    \caption[Visual comparison between \acrshort{bpg} and \acrshort{sqgan}]{Visual comparison at different compression rates between the proposed \acrshort{sqgan} (TOP) and the classical \acrshort{bpg} (BOTTOM). The \glspl{ssm} shown are generated from $\hat{\x}$ via the \acrshort{sota} \acrshort{ssmodel} INTERN-2.5 \cite{Wang2022internimage}. The proposed model is able to reconstruct images with higher semantic retention and lower values of \acrshort{bpp} compared with \acrshort{bpg}. The \acrshort{bpg} algorithm is not able to compress images at lower values than $0.038$ \acrshort{bpp}, thus the comparison is limited to $0.038$ and $0.078$ \acrshort{bpp}.}
    \label{fig: SQGAN visual comparison sqgan bpg}
\end{figure*}


\textcolor{black}{Next, we examine the effect of different masking fractions on the quality of the reconstructed image and SSM.} In fact increasing $m_\x$ and $m_\s$ is expected to increase the overall quality of $\hat{\x}$ and $\hat{\s}$ respectively, while decreasing is expected to do the opposite. \fref{fig: SQGAN visual result changing masking} shows visually how masking fractions influence the reconstructed outputs.

The column on the left shows the original $\x$ and $\s$. The other columns show on top the reconstructed $\hat{\x}$ and the generated \gls{ssm} obtained form $\hat{\x}$ via the INTERN-2.5 \gls{ssmodel} \cite{Wang2022internimage}. The bottom row shows the reconstructed $\hat{\s}$. These examples are obtained by fixing the compression level to a total amount of $0.05$ \gls{bpp} and by letting the masking fractions vary, such that $m_\x + m_\s = 1.1$ (constant).

The quality of $\hat{\x}$ is significantly affected by the fidelity of $\hat{\s}$, which depends heavily on $m_\s$, more than $m_\x$. A reduction in object detail within $\hat{\s}$ directly limits the ability of $\hat{\x}$ to retain such details. This is evident by comparing the bottom row $\hat{\s}$ with the generated \gls{ssm} in the upper row: any detail absent in $\hat{\s}$ is also absent in the \gls{ssm}. \\
Conversely, the influence of $m_\x$ on the overall quality shows expected behavior. Increasing $m_\x$ enhances image fidelity, particularly for non-relevant details. Relevant features are prioritized and reconstructed effectively even at low $m_\x$, but finer details, such as building windows, are better retained when $m_\x=0.95$. This highlights the model's capability to prioritize relevant features before addressing non-relevant ones.\\

To assess the quality of the reconstructed SSM, we use the mean Intersection over Union (mIoU) parameter, defined as:
\begin{equation} 
\text{mIoU} = \frac{1}{n_c} \sum_{i=1}^{n_c} \frac{|\s_i \cap \s'_i|}{|\s_i \cup \s'_i|}, 
\end{equation}
where $n_c$ represents the number of semantic classes, and $\s_i$ and $\s'_i$ are pixel sets of class $i$ in $\s$ and the predicted $\s'$, respectively. This parameter measures how well two segmentation maps overlap with each other. A value close to $1$ indicates a high semantic content retention in $\hat{\x}$. 

The impact of $m_\s$ on the mIoU is illustrated 
in \fref{fig: SQGAN miou vs m_s}. 
%Variations of $m_\s$ have a significant impact on the quality of the reconstructed SSM. For example, $\hat{\s}$ reconstructed at $m_\s=0.55$ and $m_\s=0.95$ appear nearly identical. 
It is useful to notice that, for $m_\s \geq 0.55$, the value of mioU remains almost constant denoting that, above that value, additional latent vectors do not add relevant semantic features.
%implies redundancy in selecting additional latent vectors, as sufficient semantic information is preserved.}
For very low values of $m_\s$ the performance is poor, but, as soon as $m_\s \geq 0.20$, the model is able to reconstruct $\hat{\s}$ with an acceptable level of semantic retention. To give a term of comparison, the value of $m_\s =0.20$  corresponds to a compression of $BPP_\s=0.011$ \gls{bpp}. This means that at this low value of \gls{bpp} the model is already able to preserve valuables details in the \gls{ssm}. Increasing $m_\s$ beyond the value of 0.55 (equivalent to $BPP_\s=0.025$ \gls{bpp}) does not provide further improvement. We interpret this saturation as follow: beyond this value, all semantically relevant latent vectors have been selected and quantized. Adding other vectors will only increase the amount of redundant information, without improving the reconstructed \gls{ssm}. This behavior is quite different from what observed and discussed previously about $m_\x$, where even for values close to $1$, the improvements were still visible on the final output.
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/FID_FID_vs_BPP.pdf}
        \caption*{(a)} % Caption under image without adding to list
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/lpips_lpips_vs_BPP.pdf}
        \caption*{(b)}
    \end{subfigure}
    
    \vspace{0.05mm} % Add space between rows

    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/mIoU_internimage_mIoU_internimage_vs_BPP.pdf}
        \caption*{(c)}
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Semantic_MQ-GAN/plots/psnr_psnr_vs_BPP.pdf}
        \caption*{(d)}
        \label{fig: SQGAN psnr vs bpp}
    \end{subfigure}

    \caption[Performance comparison between the \acrshort{sqgan} and classical compression algorithms]{Performance comparison between \acrshort{bpg}, \acrshort{jpeg2000} and \acrshort{sqgan} in term of semantic metrics and the classic pixel-by-pixel \acrshort{psnr}.}
    %[Comparison performances between \acrshort{sqgan} and classical compression algorithms]{Performance comparisons between \gls{sqgan} and classical compression algorithms. The non-semantic metric \gls{psnr} (d) is the only metric where \gls{bpg} performs better. In all the semantic-related metrics assessing image quality, such as \gls{fid} (a), \gls{lpips} (b), and \gls{miou} (c), the proposed model outperforms classical algorithms.}
    \label{fig: SQGAN all metrics comparison}
\end{figure*}


%Similar to the result in \fref{fig: SQGAN miou vs m_s} 
It is also interesting to see how both $m_\x$ and $m_\s$ affect $\hat{\x}$. For this purpose, in \fref{fig: SQGAN lpips 3d plot} we report the value of \gls{lpips} to measure the distance between $\x$ and $\hat{\x}$. As expected, the performance along the $m_\s$ axis has a strong influence on the output only for values of $m_\s \leq 0.55$. Instead, the influence of $m_\x$ is observed along the whole range from 0 to 1. This is consistent with the visual results shown in \fref{fig: SQGAN visual result changing masking}.


\subsection{Visual Results and Comparisons with SOTA image compression}\label{sec: SQGAN result comparison}
In this section the results of the proposed \gls{sqgan} are compared with SOTA compression algorithms \gls{bpg} and \gls{jpeg2000}. A visual comparison between \gls{bpg} and \gls{sqgan} is shown in \fref{fig: SQGAN visual comparison sqgan bpg}. 

The top row represents the reconstructed image $\hat{\x}$ obtained with the proposed \gls{sqgan} and the associated generated \gls{ssm} via the INTERN-2.5 \gls{ssmodel}. The bottom row shows the reconstructed image obtained by using the \gls{bpg} algorithms and the associated generated \gls{ssm}. 

We notice that \gls{sqgan} can reach lower values of \gls{bpp}. In particular, in this case BPG cannot compress images at \gls{bpp} lower than $0.038$. Furthermore, while \gls{bpg} uses precious resources to reconstruct semantically irrelevant details such as the windows of the buildings,  \gls{sqgan} focuses on the relevant parts. This is further evidenced by the amount of semantic retention of the generated \gls{ssm}. For example, the cyclist riding the bike is still visible and correctly identified by \gls{ssmodel}, at all levels of BPP in \fref{fig: SQGAN visual comparison sqgan bpg}. In contrast, the cyclist completely disappears in the generated SSM out of the BPG reconstructed image at all levels of BPP in \fref{fig: SQGAN visual comparison sqgan bpg}.\\
As a comparison term, to obtain a level of semantic retention similar to the one obtained by \gls{sqgan} at $0.038$ \gls{bpp}, \gls{bpg} requires a rate of $0.280$ \gls{bpp}.\\

As a further parameter used to quantify the (semantic) distortion between the original and reconstructed images, we used the Fréchet Inception Distance (FID), evaluated over a batch of images  \cite{Heusel2017FID}.
The value of FID
is defined as \cite{Heusel2017FID}: 
\begin{align}
    \text{FID} &= \left\| \mu_{\phi}(\mathbf{X}) - \mu_{\phi}(\hat{\mathbf{X}}) \right\|_2^2 \\
    &+ \text{Tr}\left( \Sigma_{\phi}(\x) + \Sigma_{\phi}(\hat{\mathbf{X}}) - 2 \left( \Sigma_{\phi}(\mathbf{X}) \Sigma_{\phi}(\hat{\mathbf{X}}) \right)^{1/2} \right)
\end{align}
where $\mathbf{X}$ and $\hat{\mathbf{X}}$ are the sets of real and reconstructed images, respectively; $\mu_{\phi}(\mathbf{X})$ and $\Sigma_{\phi}(\mathbf{X})$ are the mean and covariance of the features extracted from the real images $\mathbf{X}$ using a pre-trained Inception-v3 model $\phi$ \cite{Szegedy2015Inceptionv3};  $\mu_{\phi}(\hat{\mathbf{X}})$ and $\Sigma_{\phi}(\hat{\mathbf{X}})$ are the corresponding statistics from the reconstructed images $\hat{\mathbf{X}}$. Rather than directly comparing images pixel by pixel, the FID compares the distributions of the latent representations of the original and reconstructed images obtained using a convolutional neural network (Inception v3) trained for image recognition. 
%and then it assesses the difference between the distributions of the representations of $\mathbf{X}$ and $\hat{\mathbf{X}}$ in the latent space. 
From its definition, it is a parameter that is robust against translation, rotation, or change of scale of relevant objects. 
A low value of FID indicates that a DNN operating over the two batches of original and reconstructed images extract representations that are statistically equivalent.

%It provides a measure of the overall quality and diversity of the reconstructed images, assessing how well they match the statistics of the original images. Lower \gls{fid} values indicate better reconstruction quality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It is now useful to compare SQ-GAN 
with state-of-the-art image encoders, like JPEG2000, and BPG, using the following four metrics: FID, LPIPS, Peak Signal-to-Noise Ratio (PSNR), and mIoU. The first three metrics are evaluated between the original $\x$ and the reconstructed images $\hat{\x}$, while the \gls{miou} value is evaluated between $\s$ and the \gls{ssm} $\tilde{\s}$ generated via the \gls{sota} INTERN-2.5 \gls{ssmodel}. The comparison is reported in \fref{fig: SQGAN all metrics comparison}, where we show the above metrics vs. the BPP value.

From \fref{fig: SQGAN all metrics comparison}, SQ-GAN significantly outperforms JPEG2000 and BPG, in terms of parameters that somehow reflect a sort of semantic distortion, like, LPIS, FID, and mIoU. At the same time, SQ-GAN is outperformed by BPG in terms of PSNR, i.e. a standard metric that makes a pixel-by-pixel comparison. 
Furthermore, SQ-GAN is able to reach very low BPP, in the order of $0.01$ BPP, with still relatively good values of the semantic metrics FID, LPIPS, and mIoU.   It is interesting to notice that, at $0.038$ \gls{bpp} the \gls{bpg} algorithm can reconstruct images with a \gls{psnr} of 26, but it fails to preserve the \gls{ssm}. As shown in \fref{fig: SQGAN visual comparison sqgan bpg}, while \gls{bpg} reconstructs the the buildings’ windows more accurately, only the first two cars are detected and the rest of the objects are lost.

In summary, \fref{fig: SQGAN all metrics comparison} shows the main feature of the proposed method: In all applications where the task is to compress in order to preserve the semantic content of the image, without necessarily being able to reconstruct images that are very similar to the original ones at the pixel level, SQ-GAN significantly outperforms state-of-the art image encoders.
%The first important advantage of the proposed \gls{sqgan} is its ability to compress at very lower rate. Moreover, on semantic metrics like \gls{fid}, \gls{lpips}, and \gls{miou}
%\footnote{In this case the \gls{miou} is evaluated between $\s$ and $\tilde{\s}$.}  
%the \gls{sqgan} consistently outperforms classical algorithms for the same compression rate (BPP).  As it may be expected, the only metric where \gls{bpg} outperforms \gls{sqgan} is the \gls{psnr}. 
%This is not a surprise since this is a classical pixel-by-pixel metric that does not consider the overall visual quality but only the distance in the pixel domain. However, in a \gls{sc} framework, not all pixels have the same importance. Performing better on pixel-by-pixel metrics and not on semantic relevant metrics is not an advantage. 

%Overall, the proposed \gls{sqgan} demonstrates remarkable performances. The model compresses images at very low \gls{bpp} while preserving their semantic content. Furthermore, the relevant semantic content can be tuned in the training phase according to the specific task/application.

%In summary, the proposed \gls{sqgan} achieves remarkable \gls{sc} image compression by effectively balancing low \gls{bpp} and high semantic retention. The integration of the \gls{samm} and \gls{spe} allows for selective prioritization of semantically relevant regions, ensuring that critical classes such as "traffic signs" and "traffic lights" are accurately reconstructed. The introduction of the semantic-aware discriminator further enhances the model's ability to focus on important semantic details while minimizing attention on less relevant areas. Additionally, the semantic relevant classes enhancement data augmentation technique plays a crucial role in addressing the challenge of underrepresented classes, thereby improving the overall robustness and effectiveness of the compression process.

%Experimental results on the Cityscapes dataset demonstrate that \gls{sqgan} consistently outperforms classical compression algorithms like \gls{bpg} and \gls{jpeg2000} across various metrics, including \gls{miou}, \gls{fid} and \gls{lpips}. These findings highlight \gls{sqgan}'s superior capability in preserving semantic content while maintaining efficient compression rates. The visual comparisons further corroborate the quantitative results, showcasing \gls{sqgan}'s ability to retain essential semantic details even at lower \gls{bpp} levels.

%These advancements establish \gls{sqgan} as a potent tool for applications requiring both high compression efficiency and meticulous preservation of semantic information.





\section{Conclusions}
This work presents \gls{sqgan}, a new approach for simultaneous encoding of images and their semantic maps, aimed at preserving the image semantic content, even at very low values of bit per pixel. 
%The design is made so that the reconstructed image retains the semantic content and are visually appealing and understandable to a human operator. 
The comparison of \gls{sqgan} with state-of-the-art encodes has been carried out using a set of performance indicators, like FID, LPIPS, and mIoU, that somehow measure the semantic distortion between the original image (or semantic map) and the reconstructed image (or semantic map). Using these performance indicators, extensive evaluations on the Cityscapes dataset show that
SQ-GAN significantly outperforms JPEG2000 and BPG, in terms of rate-(semantic) distortion tradeoff. At the same time, SQ-GAN is clearly outperformed by BPG in terms of conventional pixel-based indicators, such as the PSNR. 
%Extensive evaluations on the Cityscapes dataset establish that \gls{sqgan} outperforms conventional algorithms like \gls{bpg} and \gls{jpeg2000} across metrics such as \gls{miou}, \gls{fid}, and \gls{lpips}. Qualitative results illustrate \gls{sqgan}'s ability to retain crucial semantic details at low \gls{bpp} levels, demonstrating its suitability for applications demanding efficient compression and meticulous semantic preservation.
In summary, the method is appealing whenever compression is performed having a clear task in mind, motivating the compression like, for example, object recognition from images collected from remote sensors. 
%A field of application could be IoT, where sensors collect data to monitor an environment and compress the observed data in order to enable only the recovery of important information (semantic content) of the observed scene. 

%exhibit significant improvements with respect to state of the art image encoders, in terms of the trade-off between compression rate and semantic distortion, having  exceptional capability in achieving semantically-aware image compression by effectively balancing low \gls{bpp} with high semantic retention. The integration of the \gls{samm} and \gls{spe} modules enables the prioritization of semantically significant regions, ensuring accurate reconstruction of critical classes, which can be emphasized during training depending of the specific task at hand. The semantic-aware discriminator and the novel data augmentation strategy proposed for training further enhance the model's robustness, addressing challenges posed by underrepresented classes.
Further investigations are necessary to simplify the coding strategy and to generalize the approach to video coding. 



\section*{Acknowledgment}
%\textcolor{red}{The work of G. aire was supported by BMBF Germany in the program of ``Souverän. Digital. Vernetzt.'' Joint Project 6G-RIC (Project IDs 16KISK030). Barbarossa's work was funded by the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (PE00000001 - program RESTART) and Huawei Technology France SASU, under agreement N.TC20220919044}\\
This work was supported by BMBF Germany in the program of ``Souverän. Digital. Vernetzt.'' Joint Project 6G-RIC (Project IDs 16KISK030), the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (PE00000001 - program RESTART), Huawei Technology France SASU, under agreement N.TC20220919044, and the SNS-JU-2022 project ADROIT6G under agreement n. 101095363.




 
% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}
\bibliography{Backmatter/Bibliography}





% that's all folks
\end{document}

