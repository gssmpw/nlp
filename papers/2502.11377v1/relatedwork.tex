\section{Related Work}
\subsection{World Models}

Model-based RL improves sample efficiency over model-free RL by learning an approximate model for the transition dynamics of the environment, allowing for policy training without interacting with the environment itself. However, obtaining accurate world models is not straightforward because the learned model can easily accumulate errors exponentially over time. To alleviate this issue, \cite{Chua2018} designs ensembles of stochastic dynamics models to attempt to incorporate uncertainty. The Dreamer architecture~\cite{Hafner2019,Hafner2020,Hafner2023} learns a model of the environment via reconstructing the input from a latent space using the recurrent state-space model (RSSM). The RSSM incorporates the Gated Recurrent Unit (GRU) network~\cite{Cho2014} and the Variational Autoencoder (VAE)~\cite{Kingma2013} for modeling. With this generative world model, the policy is trained with imagined trajectories in this learned latent space.  \cite{Robine2023} and \cite{Micheli2022} leverage the Transformer architecture~\cite{Vaswani2017} to autoregressively model the world dynamics and similarly train the policy in latent imagination. 
Our work is built on top of the Dreamer architecture, but the idea of explicit modeling of hidden parameters has the potential to be combined with other architectures.
% \sehoon{Morgan, try to avoid some vauge words, like ``use''. It is better to be specific. It is also better to avoid the repetition of the same words}

\subsection{Randomized Approaches without Explicit Modeling}
One of the most popular approaches to deal with uncertain or parameterized dynamics is domain randomization (DR), which aims to improve the robustness of a policy by exposing the agent to randomized environments. It has been effective in many applications, including manipulation \cite{Peng2018, Tobin2017, Zhang2016, James2017}, locomotion \cite{Peng2020, Tan2018}, autonomous driving \cite{Tremblay2018}, and indoor drone flying \cite{Sadeghi2016}. 
% \sehoon{assign citations to the proper application category}. 
Domain randomization has also shown great success in deploying trained policies on actual robots, such as performing sim-to-real transfer for a quadrupedal robot in \cite{Tan2018} and improving performance for a robotic manipulator in \cite{Peng2018}. \cite{rigter2024waker} and \cite{yamada2023twist} both incorporate DR for increased robustness in a world model setting. While DR is highly effective in many situations, it tends to lead to an overly conservative policy that is suboptimal for challenging problems with a wide range of transition or reward functions. 

\subsection{Domain Adaptation}
Since incorporating additional observations is often beneficial \cite{Kim2020ObservationSM}, another common strategy for dealing with variable environments is to incorporate the hidden environmental parameters into the policy for adaptation. This privileged information of the hidden parameters can be exploited during training, but at test time, system identification must occur online. For model-free RL, researchers typically train a universal policy conditioned on hidden parameters and estimate them at test time by identifying directly from a history of observations~\cite{Yu2017,Kumar2021,Nahrendra2023}. Another option is to improve state estimation while training in diverse environments, which similarly allows for adaptation without needing to perform explicit system identification~\cite{Ji2022}. 
% \sehoon{Morgan, check the descriptions and adjust them.} 
For model-based RL, the problem of handling variable physics conditions is handled in multiple ways. A few research groups \cite{Nagabandi2018,Sæmundsson2018} propose using meta-learning to rapidly adapt to environmental changes online. \cite{Wang2021} uses a graph-based meta RL technique to handle changing dynamics. \cite{Ball2021} used data augmentation in offline RL to get zero-shot dynamics generalization. The most applicable methods for our work are those that use a learned encoder to estimate a context vector that attempts to capture the environmental information. Then, this context vector is used for conditioning the policy and forward prediction, as in \cite{Wang2022,Lee2020,Huang2021,Seo2020}.

% Another common choice for dealing with a variable environment is to attempt to incorporate environmental information into the policy for adaptation. During training, privileged information can be used, but at test time, system identification must occur online.

% For model-free RL, \cite{Yu2017} and \cite{Kumar2021} both use privileged environmental information to learn a universal policy during training and then use a history of observations to train an encoder to estimate the parameters during testing. \cite{Ji2022} simultaneously trains a policy and estimation network in a changing environment to improve performance and avoid explicitly having to perform system identification.  \cite{Nahrendra2023} similarly employs an observation history to estimate velocity and as an input to a VAE that attempts to reconstruct the privileged state information.

% For model-based RL, the problem of handling variable physics conditions is handled in multiple ways. \cite{Nagabandi2018} and \cite{Sæmundsson2018} propose using meta-learning to rapidly adapt to dynamics changes online. \cite{Wang2021} uses a graph-based meta RL technique to handle changing dynamics. \cite{Ball2021} used data augmentation in offline RL to get zero-shot dynamics generalization. The most applicable methods for our work are the problems that use a learned encoder to estimate a context vector that attempts to capture the environmental information and is used to condition the policy and for forward prediction, as in \cite{Wang2022}, \cite{Lee2020}, \cite{Huang2021}, \cite{Seo2020}.