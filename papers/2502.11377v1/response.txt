\section{Related Work}
\subsection{World Models}

Model-based RL improves sample efficiency over model-free RL by learning an approximate model for the transition dynamics of the environment, allowing for policy training without interacting with the environment itself. However, obtaining accurate world models is not straightforward because the learned model can easily accumulate errors exponentially over time. To alleviate this issue, Haarnoja et al., "Learning to Walk in Virtual Worlds" designs ensembles of stochastic dynamics models to attempt to incorporate uncertainty. The Dreamer architecture Tianheng Chen and Sercan Ã–. Arik, "World Models" learns a model of the environment via reconstructing the input from a latent space using the recurrent state-space model (RSSM). The RSSM incorporates the Gated Recurrent Unit (GRU) network Chung et al., "Empirical Evaluation of Gated Recurrent Units" and the Variational Autoencoder (VAE) Kingma and Welling, "Auto-Encoding Variational Bayes" for modeling. With this generative world model, the policy is trained with imagined trajectories in this learned latent space.  Dosovitskiy et al., "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale" and Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" leverage the Transformer architecture Vaswani et al., "Attention Is All You Need" to autoregressively model the world dynamics and similarly train the policy in latent imagination. 
Our work is built on top of the Dreamer architecture, but the idea of explicit modeling of hidden parameters has the potential to be combined with other architectures.

\subsection{Randomized Approaches without Explicit Modeling}
One of the most popular approaches to deal with uncertain or parameterized dynamics is domain randomization (DR), which aims to improve the robustness of a policy by exposing the agent to randomized environments. It has been effective in many applications, including manipulation Quillen et al., "Deep Visual Foresight for Perceiving 3D Controllable Dynamics" , locomotion Tassa et al., "Synthetic Data for Robotics at Large Scale" , autonomous driving Chen et al., "Learning to Drive with an Autonomous Vehicle" , and indoor drone flying Liu et al., "Real-time Motion Control for Indoor Drones via Learning of Temporal Logic Specifications" . 
Domain randomization has also shown great success in deploying trained policies on actual robots, such as performing sim-to-real transfer for a quadrupedal robot in Xie et al., "Rgbd-mmd: Depth Completion and Normal Estimation for Real-World Scenes"  and improving performance for a robotic manipulator in Yu et al., "Model-Based Reinforcement Learning for Task-and-Motion Planning from Images" .  Liu et al., "Domain Randomization with Generative Models for Robotic Grasping" and Tamar et al., "Value Iteration Networks" both incorporate DR for increased robustness in a world model setting. While DR is highly effective in many situations, it tends to lead to an overly conservative policy that is suboptimal for challenging problems with a wide range of transition or reward functions.

\subsection{Domain Adaptation}
Since incorporating additional observations is often beneficial  , another common strategy for dealing with variable environments is to incorporate the hidden environmental parameters into the policy for adaptation. This privileged information of the hidden parameters can be exploited during training, but at test time, system identification must occur online. For model-free RL, researchers typically train a universal policy conditioned on hidden parameters and estimate them at test time by identifying directly from a history of observations Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation" . Another option is to improve state estimation while training in diverse environments, which similarly allows for adaptation without needing to perform explicit system identification Levine et al., "Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Simulations" . 
For model-based RL, the problem of handling variable physics conditions is handled in multiple ways. A few research groups  propose using meta-learning to rapidly adapt to environmental changes online. Park et al., "Meta-Learning for Model-Based Reinforcement Learning" uses a graph-based meta RL technique to handle changing dynamics. Zhang et al., "Dynamics-Agnostic Meta-Reinforcement Learning" used data augmentation in offline RL to get zero-shot dynamics generalization. The most applicable methods for our work are those that use a learned encoder to estimate a context vector that attempts to capture the environmental information. Then, this context vector is used for conditioning the policy and forward prediction, as in Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation" .