\section{Related Work}
Although LLMs are highly effective in performing various natural language processing (NLP) tasks, they often struggle with tasks that require extensive real-world knowledge, particularly when dealing with long-tail facts, i.e., facts associated with less common entities____. This limitation highlights the need for augmenting LLMs with non-parametric knowledge or integrating this knowledge into the model's parameters.

Non-parametric knowledge can significantly enhance LLM performance, particularly when the required knowledge is rare or involves multiple relationships between entities____.  However, external knowledge sources can also potentially mislead LLMs when answering questions about well-known entities, as powerful LLMs have already internalized this information within their parameters____.

RAG is also not a universal solution. %due to the introduction of harmful noise, which can degrade LLM performance____.
On the one hand, models have been shown to rely more on nonparametric knowledge____. However, LLMs still face challenges in discriminating highly semantically related information and can easily be distracted by this irrelevant and misleading content____. Furthermore, RAG methods introduce latency in response time, as retrieval must be performed, particularly in the case of iterative RAG approaches____. These involve a multi-step process, including retrieval followed by augmentation____.


Recent studies have revealed that LLMs acquire their knowledge predominantly during the pre-training phase____. However, pre-training a model each time you want to incorporate new information appears to be excessive. It has been shown that attempting to acquire new knowledge through supervised fine-tuning can actually lead to an increase in hallucinations relative to the existing knowledge. Furthermore, LLMs tend to struggle when trying to integrate new knowledge via fine-tuning, instead primarily learning how to make better use of their pre-existing knowledge____. 

Low-Rank Adaptation, or LoRA____ freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, thus greatly reducing the number of trainable parameters for downstream tasks.

There have been several papers on the continuous knowledge editing through various training and evaluation mechanisms. ____ have shown that almost all SoTA editing methods have a trade-off between accuracy, recall, and hallucination.