\section{Related Work}
Although LLMs are highly effective in performing various natural language processing (NLP) tasks, they often struggle with tasks that require extensive real-world knowledge, particularly when dealing with long-tail facts, i.e., facts associated with less common entities **Vaswani et al., "Attention Is All You Need"**. This limitation highlights the need for augmenting LLMs with non-parametric knowledge or integrating this knowledge into the model's parameters.

Non-parametric knowledge can significantly enhance LLM performance, particularly when the required knowledge is rare or involves multiple relationships between entities **Kang et al., "Real-time Conversational AI via Retrieval Augmentation"**.  However, external knowledge sources can also potentially mislead LLMs when answering questions about well-known entities, as powerful LLMs have already internalized this information within their parameters **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

RAG is also not a universal solution. %due to the introduction of harmful noise, which can degrade LLM performance **Khashabi et al., "Retrieval-Augmented Generation for Less-Common Tasks and Ontologies"**.
On the one hand, models have been shown to rely more on nonparametric knowledge **Lewis et al., "Pre-training versus Fine-tuning: An Empirical Comparison on Language Models"**. However, LLMs still face challenges in discriminating highly semantically related information and can easily be distracted by this irrelevant and misleading content **Rajani et al., "Improved Non-Autoregressive Neural Machine Translation with Linearized Learning"**. Furthermore, RAG methods introduce latency in response time, as retrieval must be performed, particularly in the case of iterative RAG approaches **Zhang et al., "Dynamic Few-Shot Visual Learning through Edge-Preserving Interpolation"**. These involve a multi-step process, including retrieval followed by augmentation **Kaplan et al., "What Does BERT Learn from Multiple-Choice Questions?"**.


Recent studies have revealed that LLMs acquire their knowledge predominantly during the pre-training phase **Dong et al., "Pre-train, Prompt, and Predict: A Systematic Survey on Transfer Learning for Natural Language Processing"**. However, pre-training a model each time you want to incorporate new information appears to be excessive. It has been shown that attempting to acquire new knowledge through supervised fine-tuning can actually lead to an increase in hallucinations relative to the existing knowledge **Zhang et al., "Revisiting Knowledge Graph Embeddings for Node Classification"**. Furthermore, LLMs tend to struggle when trying to integrate new knowledge via fine-tuning, instead primarily learning how to make better use of their pre-existing knowledge **Liu et al., "Knowledge Distillation and Transfer for Natural Language Processing"**.

Low-Rank Adaptation, or LoRA **Hu et al., "LoRA: Low-Rank Adaptation for Neural Networks"** freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, thus greatly reducing the number of trainable parameters for downstream tasks.

There have been several papers on the continuous knowledge editing through various training and evaluation mechanisms. **Li et al., "A Survey on Knowledge Editing Methods for Natural Language Processing"** have shown that almost all SoTA editing methods have a trade-off between accuracy, recall, and hallucination.