% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{DBLP:journals/corr/abs-1709-00103,
	title        = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
	author       = {Victor Zhong and Caiming Xiong and Richard Socher},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1709.00103},
	url          = {http://arxiv.org/abs/1709.00103},
	eprinttype   = {arXiv},
	eprint       = {1709.00103},
	timestamp    = {Mon, 13 Aug 2018 16:48:41 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1709-00103.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{Aho:72,
	title        = {The Theory of Parsing, Translation and Compiling},
	author       = {Alfred V. Aho and Jeffrey D. Ullman},
	year         = 1972,
	publisher    = {Prentice-Hall},
	address      = {Englewood Cliffs, NJ},
	volume       = 1
}
@book{APA:83,
	title        = {Publications Manual},
	author       = {{American Psychological Association}},
	year         = 1983,
	publisher    = {American Psychological Association},
	address      = {Washington, DC}
}
@article{lapata,
	title        = {Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts},
	author       = {Huang, Wenyu and Zhou, Guancheng and Lapata, Mirella and Vougiouklis, Pavlos and Montella, Sebastien and Pan, Jeff Z},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2405.06524}
}
@misc{pandora,
	title        = {Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models},
	author       = {Jinyang Wu and Feihu Che and Chuyuan Zhang and Jianhua Tao and Shuai Zhang and Pengpeng Shao},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.13533},
	eprint       = {2408.13533},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{popqa,
	title        = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
	author       = {Mallen, Alex  and Asai, Akari  and Zhong, Victor  and Das, Rajarshi  and Khashabi, Daniel  and Hajishirzi, Hannaneh},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {9802--9822},
	doi          = {10.18653/v1/2023.acl-long.546},
	url          = {https://aclanthology.org/2023.acl-long.546},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs{'} strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.}
}
@misc{allenzhu2024physicslanguagemodels32,
	title        = {Physics of Language Models: Part 3.2, Knowledge Manipulation},
	author       = {Zeyuan Allen-Zhu and Yuanzhi Li},
	year         = 2024,
	url          = {https://arxiv.org/abs/2309.14402},
	eprint       = {2309.14402},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{role,
	title        = {On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models},
	author       = {Dongyang Li and Junbing Yan and Taolin Zhang and Chengyu Wang and Xiaofeng He and Longtao Huang and Hui Xue and Jun Huang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.16367},
	eprint       = {2406.16367},
	archiveprefix = {arXiv},
	primaryclass = {cs.IR}
}
@article{Chandra:81,
	title        = {Alternation},
	author       = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year         = 1981,
	journal      = {Journal of the Association for Computing Machinery},
	volume       = 28,
	number       = 1,
	pages        = {114--133},
	doi          = {10.1145/322234.322243}
}
@inproceedings{andrew2007scalable,
	title        = {Scalable training of {L1}-regularized log-linear models},
	author       = {Andrew, Galen and Gao, Jianfeng},
	year         = 2007,
	booktitle    = {Proceedings of the 24th International Conference on Machine Learning},
	pages        = {33--40}
}

@book{Gusfield:97,
	title        = {Algorithms on Strings, Trees and Sequences},
	author       = {Dan Gusfield},
	year         = 1997,
	publisher    = {Cambridge University Press},
	address      = {Cambridge, UK}
}
@article{rasooli-tetrault-2015,
	title        = {Yara Parser: {A} Fast and Accurate Dependency Parser},
	author       = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
	year         = 2015,
	journal      = {Computing Research Repository},
	volume       = {arXiv:1503.06733},
	url          = {http://arxiv.org/abs/1503.06733},
	note         = {version 2}
}
@article{Ando2005,
	title        = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	author       = {Ando, Rie Kubota and Zhang, Tong},
	year         = 2005,
	month        = dec,
	journal      = {Journal of Machine Learning Research},
	publisher    = {JMLR.org},
	volume       = 6,
	pages        = {1817--1853},
	issn         = {1532-4435},
	acmid        = 1194905,
	issue_date   = {12/1/2005},
	numpages     = 37
}
@article{DBLP:journals/compsec/ChenCWCYJLL24,
	title        = {A survey of large language models for cyber threat detection},
	author       = {Yiren Chen and Mengjiao Cui and Ding Wang and Yiyang Cao and Peian Yang and Bo Jiang and Zhigang Lu and Baoxu Liu},
	year         = 2024,
	journal      = {Comput. Secur.},
	volume       = 145,
	pages        = 104016,
	doi          = {10.1016/J.COSE.2024.104016},
	url          = {https://doi.org/10.1016/j.cose.2024.104016},
	timestamp    = {Wed, 28 Aug 2024 10:56:25 +0200},
	biburl       = {https://dblp.org/rec/journals/compsec/ChenCWCYJLL24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2102-03315-ARC,
	title        = {Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer {AI2} Reasoning Challenge},
	author       = {Sumithra Bhakthavatsalam and Daniel Khashabi and Tushar Khot and Bhavana Dalvi Mishra and Kyle Richardson and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord and Peter Clark},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2102.03315},
	url          = {https://arxiv.org/abs/2102.03315},
	eprinttype   = {arXiv},
	eprint       = {2102.03315},
	timestamp    = {Fri, 12 Mar 2021 08:37:07 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2102-03315.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/MoskovskiyPP24,
  author       = {Daniil Moskovskiy and
                  Sergey Pletenev and
                  Alexander Panchenko},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case
                  of Text Detoxification},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {14361--14373},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.839},
  timestamp    = {Mon, 18 Nov 2024 09:06:00 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/MoskovskiyPP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/LinHE22-TruthfulQA,
	title        = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
	author       = {Stephanie Lin and Jacob Hilton and Owain Evans},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022},
	publisher    = {Association for Computational Linguistics},
	pages        = {3214--3252},
	doi          = {10.18653/V1/2022.ACL-LONG.229},
	url          = {https://doi.org/10.18653/v1/2022.acl-long.229},
	editor       = {Smaranda Muresan and Preslav Nakov and Aline Villavicencio},
	timestamp    = {Mon, 01 Aug 2022 16:27:45 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/LinHE22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:journals/corr/abs-2009-03300-MMLU,
	title        = {Measuring Massive Multitask Language Understanding},
	author       = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
	year         = 2021,
	booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=d7KBjmI3GmQ},
	timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2403-14608,
	title        = {Parameter-Efficient Fine-Tuning for Large Models: {A} Comprehensive Survey},
	author       = {Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2403.14608},
	doi          = {10.48550/ARXIV.2403.14608},
	url          = {https://doi.org/10.48550/arXiv.2403.14608},
	eprinttype   = {arXiv},
	eprint       = {2403.14608},
	timestamp    = {Fri, 12 Apr 2024 20:43:45 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2403-14608.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:journals/corr/abs-2106-09685-LoRA,
	title        = {LoRA: Low-Rank Adaptation of Large Language Models},
	author       = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen{-}Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year         = 2022,
	booktitle    = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=nZeVKeeFYf9},
	timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2312-10549,
	title        = {Catastrophic Forgetting in Deep Learning: {A} Comprehensive Taxonomy},
	author       = {Everton Lima Aleixo and Juan Gabriel Colonna and Marco Cristo and Everlandio Fernandes},
	year         = 2024,
	journal      = {J. Braz. Comput. Soc.},
	volume       = 30,
	number       = 1,
	doi          = {10.5753/JBCS.2024.3966},
	url          = {https://doi.org/10.5753/jbcs.2024.3966},
	timestamp    = {Fri, 20 Sep 2024 14:02:37 +0200},
	biburl       = {https://dblp.org/rec/journals/jbcs/AleixoCCF24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{kadavath2022languagemodelsmostlyknow,
	title        = {Language Models (Mostly) Know What They Know},
	author       = {Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zac Hatfield-Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
	year         = 2022,
	url          = {https://arxiv.org/abs/2207.05221},
	eprint       = {2207.05221},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{DBLP:journals/corr/abs-2305-17553-counterfactplus,
	title        = {Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark},
	author       = {Jason Hoelscher{-}Obermaier and Julia Persson and Esben Kran and Ioannis Konstas and Fazl Barez},
	year         = 2023,
	journal      = {CoRR},
	volume       = {abs/2305.17553},
	doi          = {10.48550/ARXIV.2305.17553},
	url          = {https://doi.org/10.48550/arXiv.2305.17553},
	eprinttype   = {arXiv},
	eprint       = {2305.17553},
	timestamp    = {Mon, 05 Feb 2024 20:18:50 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2305-17553.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/naacl/SunXZLD24-Head-to-Tail,
	title        = {Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? {A.K.A.} Will LLMs Replace Knowledge Graphs?},
	author       = {Kai Sun and Yifan Ethan Xu and Hanwen Zha and Yue Liu and Xin Luna Dong},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {311--325},
	doi          = {10.18653/V1/2024.NAACL-LONG.18},
	url          = {https://doi.org/10.18653/v1/2024.naacl-long.18},
	editor       = {Kevin Duh and Helena G{\'{o}}mez{-}Adorno and Steven Bethard},
	timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
	biburl       = {https://dblp.org/rec/conf/naacl/SunXZLD24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/semweb/AuerBKLCI07-dbpedia,
	title        = {DBpedia: {A} Nucleus for a Web of Open Data},
	author       = {S{\"{o}}ren Auer and Christian Bizer and Georgi Kobilarov and Jens Lehmann and Richard Cyganiak and Zachary G. Ives},
	year         = 2007,
	booktitle    = {The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, {ISWC} 2007 + {ASWC} 2007, Busan, Korea, November 11-15, 2007},
	publisher    = {Springer},
	series       = {Lecture Notes in Computer Science},
	volume       = 4825,
	pages        = {722--735},
	doi          = {10.1007/978-3-540-76298-0\_52},
	url          = {https://doi.org/10.1007/978-3-540-76298-0\_52},
	editor       = {Karl Aberer and Key{-}Sun Choi and Natasha Fridman Noy and Dean Allemang and Kyung{-}Il Lee and Lyndon J. B. Nixon and Jennifer Golbeck and Peter Mika and Diana Maynard and Riichiro Mizoguchi and Guus Schreiber and Philippe Cudr{\'{e}}{-}Mauroux},
	timestamp    = {Wed, 07 Dec 2022 23:12:15 +0100},
	biburl       = {https://dblp.org/rec/conf/semweb/AuerBKLCI07.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nips/LewisPPPKGKLYR020-RAG,
	title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	author       = {Patrick S. H. Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{\"{u}}ttler and Mike Lewis and Wen{-}tau Yih and Tim Rockt{\"{a}}schel and Sebastian Riedel and Douwe Kiela},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	url          = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	editor       = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
	timestamp    = {Tue, 19 Jan 2021 15:57:07 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20-few-shot-learning,
	title        = {Language Models are Few-Shot Learners},
	author       = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert{-}Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	editor       = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
	timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{roee,
	title        = {Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?},
	author       = {Zorik Gekhman and Gal Yona and Roee Aharoni and Matan Eyal and Amir Feder and Roi Reichart and Jonathan Herzig},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {7765--7784},
	url          = {https://aclanthology.org/2024.emnlp-main.444},
	editor       = {Yaser Al{-}Onaizan and Mohit Bansal and Yun{-}Nung Chen},
	timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/GekhmanYAEFRH24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{huang2024prompting,
	title        = {Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts},
	author       = {Huang, Wenyu and Zhou, Guancheng and Lapata, Mirella and Vougiouklis, Pavlos and Montella, Sebastien and Pan, Jeff Z},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2405.06524}
}
@inproceedings{rubin2021learning,
	title        = {Learning To Retrieve Prompts for In-Context Learning},
	author       = {Ohad Rubin and Jonathan Herzig and Jonathan Berant},
	year         = 2022,
	booktitle    = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022},
	publisher    = {Association for Computational Linguistics},
	pages        = {2655--2671},
	doi          = {10.18653/V1/2022.NAACL-MAIN.191},
	url          = {https://doi.org/10.18653/v1/2022.naacl-main.191},
	editor       = {Marine Carpuat and Marie{-}Catherine de Marneffe and Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
	timestamp    = {Mon, 01 Aug 2022 16:27:57 +0200},
	biburl       = {https://dblp.org/rec/conf/naacl/RubinHB22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{eval-harness,
	title        = {A framework for few-shot language model evaluation},
	author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	year         = 2024,
	month        = 07,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.12608602},
	url          = {https://zenodo.org/records/12608602},
	version      = {v0.4.3}
}
@article{llama3_tech_report,
	title        = {The Llama 3 Herd of Models},
	author       = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al{-}Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur{\'{e}}lien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Rozi{\`{e}}re and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia{-}Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr{\'{e}}goire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and et al.},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2407.21783},
	doi          = {10.48550/ARXIV.2407.21783},
	url          = {https://doi.org/10.48550/arXiv.2407.21783},
	eprinttype   = {arXiv},
	eprint       = {2407.21783},
	timestamp    = {Mon, 26 Aug 2024 08:08:35 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2407-21783.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{roberts-etal-2020-much,
	title        = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
	author       = {Roberts, Adam  and Raffel, Colin  and Shazeer, Noam},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5418--5426},
	doi          = {10.18653/v1/2020.emnlp-main.437},
	url          = {https://aclanthology.org/2020.emnlp-main.437},
	editor       = {Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.}
}
@inproceedings{lee-etal-2022-plug,
	title        = {Plug-and-Play Adaptation for Continuously-updated {QA}},
	author       = {Lee, Kyungjae  and Han, Wookje  and Hwang, Seung-won  and Lee, Hwaran  and Park, Joonsuk  and Lee, Sang-Woo},
	year         = 2022,
	month        = may,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2022},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {438--447},
	doi          = {10.18653/v1/2022.findings-acl.37},
	url          = {https://aclanthology.org/2022.findings-acl.37},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs{'} efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task{---}Continuously-updated QA (CuQA){---}in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.}
}
@article{carlino2001knowledge,
	title        = {Knowledge spillovers: cities’ role in the new economy},
	author       = {Carlino, Gerald A},
	year         = 2001,
	journal      = {Business Review Q},
	volume       = 4,
	number       = 1,
	pages        = {17--24}
}
@article{Kirkpatrick_2017,
	title        = {Overcoming catastrophic forgetting in neural networks},
	author       = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	year         = 2017,
	month        = mar,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {Proceedings of the National Academy of Sciences},
	volume       = 114,
	number       = 13,
	pages        = {3521–3526},
	doi          = {10.1073/pnas.1611835114},
	issn         = {1091-6490},
	url          = {http://dx.doi.org/10.1073/pnas.1611835114}
}
@article{guo2023evaluating,
	title        = {Evaluating large language models: A comprehensive survey},
	author       = {Guo, Zishan and Jin, Renren and Liu, Chuang and Huang, Yufei and Shi, Dan and Yu, Linhao and Liu, Yan and Li, Jiaxuan and Xiong, Bojian and Xiong, Deyi and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.19736}
}
@misc{wang2024wiserethinkingknowledgememory,
	title        = {WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models},
	author       = {Peng Wang and Zexi Li and Ningyu Zhang and Ziwen Xu and Yunzhi Yao and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.14768},
	eprint       = {2405.14768},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{joshi2017triviaqa,
	title        = {TriviaQA: {A} Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	author       = {Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
	year         = 2017,
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers},
	publisher    = {Association for Computational Linguistics},
	pages        = {1601--1611},
	doi          = {10.18653/V1/P17-1147},
	url          = {https://doi.org/10.18653/v1/P17-1147},
	editor       = {Regina Barzilay and Min{-}Yen Kan},
	timestamp    = {Fri, 06 Aug 2021 00:40:58 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/JoshiCWZ17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Zhou2024ASO,
	title        = {A Survey on Data Augmentation in Large Model Era},
	author       = {Yue Zhou and Chenlu Guo and Xu Wang and Yi Chang and Yuan Wu},
	year         = 2024,
	journal      = {ArXiv},
	volume       = {abs/2401.15422},
	url          = {https://api.semanticscholar.org/CorpusID:267311830}
}
@inproceedings{krayko-etal-2024-efficient,
	title        = {Efficient Answer Retrieval System ({EARS}): Combining Local {DB} Search and Web Search for Generative {QA}},
	author       = {Krayko, Nikita  and Sidorov, Ivan  and Laputin, Fedor  and Galimzianova, Daria  and Konovalov, Vasily},
	year         = 2024,
	month        = nov,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
	publisher    = {Association for Computational Linguistics},
	address      = {Miami, Florida, US},
	pages        = {1584--1594},
	doi          = {10.18653/v1/2024.emnlp-industry.116},
	url          = {https://aclanthology.org/2024.emnlp-industry.116/},
	editor       = {Dernoncourt, Franck  and Preo{\c{t}}iuc-Pietro, Daniel  and Shimorina, Anastasia},
	abstract     = {In this work, we propose an efficient answer retrieval system **EARS**: a production-ready, factual question answering (QA) system that combines local knowledge base search with generative, context-based QA. To assess the quality of the generated content, we devise comprehensive metrics for both manual and automatic evaluation of the answers to questions. A distinctive feature of our system is the Ranker component, which ranks answer candidates based on their relevance. This feature enhances the effectiveness of local knowledge base retrieval by 23{\%}. Another crucial aspect of our system is the LLM, which utilizes contextual information from a web search API to generate responses. This results in substantial 92.8{\%} boost in the usefulness of voice-based responses. **EARS** is language-agnostic and can be applied to any data domain.}
}
@inproceedings{belikova-etal-2024-jellybell,
	title        = {{J}elly{B}ell at {T}ext{G}raphs-17 Shared Task: Fusing Large Language Models with External Knowledge for Enhanced Question Answering},
	author       = {Belikova, Julia  and Beliakin, Evegeniy  and Konovalov, Vasily},
	year         = 2024,
	month        = aug,
	booktitle    = {Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Bangkok, Thailand},
	pages        = {154--160},
	url          = {https://aclanthology.org/2024.textgraphs-1.15/},
	editor       = {Ustalov, Dmitry  and Gao, Yanjun  and Panchenko, Alexander  and Tutubalina, Elena  and Nikishina, Irina  and Ramesh, Arti  and Sakhovskiy, Andrey  and Usbeck, Ricardo  and Penn, Gerald  and Valentino, Marco},
	abstract     = {This work describes an approach to develop Knowledge Graph Question Answering (KGQA) system for TextGraphs-17 shared task. The task focuses on the fusion of Large Language Models (LLMs) with Knowledge Graphs (KGs). The goal is to select a KG entity (out of several candidates) which corresponds to an answer given a textual question. Our approach applies LLM to identify the correct answer among the list of possible candidates. We confirm that integrating external information is particularly beneficial when the subject entities are not well-known, and using RAG can negatively impact the performance of LLM on questions related to popular entities, as the retrieved context might be misleading. With our result, we achieved 2nd place in the post-evaluation phase.}
}
@article{WangAUG,
	title        = {A Comprehensive Survey on Data Augmentation},
	author       = {Zaitian Wang and Pengfei Wang and Kunpeng Liu and Pengyang Wang and Yanjie Fu and Chang{-}Tien Lu and Charu C. Aggarwal and Jian Pei and Yuanchun Zhou},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2405.09591},
	doi          = {10.48550/ARXIV.2405.09591},
	url          = {https://doi.org/10.48550/arXiv.2405.09591},
	eprinttype   = {arXiv},
	eprint       = {2405.09591},
	timestamp    = {Thu, 13 Jun 2024 21:48:07 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2405-09591.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/acl/HuangCWYLSYS24,
	title        = {Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal},
	author       = {Jianheng Huang and Leyang Cui and Ante Wang and Chengyi Yang and Xinting Liao and Linfeng Song and Junfeng Yao and Jinsong Su},
	year         = 2024,
	booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand, August 11-16, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {1416--1428},
	doi          = {10.18653/V1/2024.ACL-LONG.77},
	url          = {https://doi.org/10.18653/v1/2024.acl-long.77},
	editor       = {Lun{-}Wei Ku and Andre Martins and Vivek Srikumar},
	timestamp    = {Tue, 24 Sep 2024 10:55:50 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/HuangCWYLSYS24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{voznyuk-konovalov-2024-deeppavlov,
    title = "{D}eep{P}avlov at {S}em{E}val-2024 Task 8: Leveraging Transfer Learning for Detecting Boundaries of Machine-Generated Texts",
    author = "Voznyuk, Anastasia  and
      Konovalov, Vasily",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Tayyar Madabushi, Harish  and
      Da San Martino, Giovanni  and
      Rosenthal, Sara  and
      Ros{\'a}, Aiala},
    booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.semeval-1.257/",
    doi = "10.18653/v1/2024.semeval-1.257",
    pages = "1821--1829",
    abstract = "The Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection shared task in the SemEval-2024 competition aims to tackle the problem of misusing collaborative human-AI writing. Although there are a lot of existing detectors of AI content, they are often designed to give a binary answer and thus may not be suitable for more nuanced problem of finding the boundaries between human-written and machine-generated texts, while hybrid human-AI writing becomes more and more popular. In this paper, we address the boundary detection problem. Particularly, we present a pipeline for augmenting data for supervised fine-tuning of DeBERTaV3. We receive new best MAE score, according to the leaderboard of the competition, with this pipeline."
}


@article{10.1145/3698590,
	title        = {Knowledge Editing for Large Language Models: A Survey},
	author       = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
	year         = 2024,
	month        = oct,
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	doi          = {10.1145/3698590},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3698590},
	note         = {Just Accepted},
	abstract     = {Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME), also known as Knowledge Editing or Model Editing, has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
	keywords     = {Model Editing, Knowledge Update, Fine-tuning, Large Language Models}
}
@article{roberts2020much,
	title        = {How much knowledge can you pack into the parameters of a language model?},
	author       = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2002.08910}
}
@inproceedings{DBLP:conf/emnlp/YaoWT0LDC023,
	title        = {Editing Large Language Models: Problems, Methods, and Opportunities},
	author       = {Yunzhi Yao and Peng Wang and Bozhong Tian and Siyuan Cheng and Zhoubo Li and Shumin Deng and Huajun Chen and Ningyu Zhang},
	year         = 2023,
	booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
	publisher    = {Association for Computational Linguistics},
	pages        = {10222--10240},
	doi          = {10.18653/V1/2023.EMNLP-MAIN.632},
	url          = {https://doi.org/10.18653/v1/2023.emnlp-main.632},
	editor       = {Houda Bouamor and Juan Pino and Kalika Bali},
	timestamp    = {Fri, 12 Apr 2024 13:11:51 +0200},
	biburl       = {https://dblp.org/rec/conf/emnlp/YaoWT0LDC023.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1803-05457,
	title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning Challenge},
	author       = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1803.05457},
	url          = {http://arxiv.org/abs/1803.05457},
	eprinttype   = {arXiv},
	eprint       = {1803.05457},
	timestamp    = {Mon, 13 Aug 2018 16:48:43 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1803-05457.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/ijcai/LiuCLHWZ20,
	title        = {LogiQA: {A} Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
	author       = {Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
	year         = 2020,
	booktitle    = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI} 2020},
	publisher    = {ijcai.org},
	pages        = {3622--3628},
	doi          = {10.24963/IJCAI.2020/501},
	url          = {https://doi.org/10.24963/ijcai.2020/501},
	editor       = {Christian Bessiere},
	timestamp    = {Tue, 15 Oct 2024 16:43:28 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/LiuCLHWZ20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/emnlp/FarahaniJ24,
	title        = {Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models},
	author       = {Mehrdad Farahani and Richard Johansson},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {16966--16977},
	url          = {https://aclanthology.org/2024.emnlp-main.943},
	editor       = {Yaser Al{-}Onaizan and Mohit Bansal and Yun{-}Nung Chen},
	timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/FarahaniJ24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2404-03302,
	title        = {How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?},
	author       = {Siye Wu and Jian Xie and Jiangjie Chen and Tinghui Zhu and Kai Zhang and Yanghua Xiao},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2404.03302},
	doi          = {10.48550/ARXIV.2404.03302},
	url          = {https://doi.org/10.48550/arXiv.2404.03302},
	eprinttype   = {arXiv},
	eprint       = {2404.03302},
	timestamp    = {Thu, 08 Aug 2024 08:05:58 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2404-03302.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

