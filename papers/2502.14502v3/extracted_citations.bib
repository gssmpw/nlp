@inproceedings{DBLP:conf/emnlp/FarahaniJ24,
	title        = {Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models},
	author       = {Mehrdad Farahani and Richard Johansson},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {16966--16977},
	url          = {https://aclanthology.org/2024.emnlp-main.943},
	editor       = {Yaser Al{-}Onaizan and Mohit Bansal and Yun{-}Nung Chen},
	timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/FarahaniJ24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/naacl/SunXZLD24-Head-to-Tail,
	title        = {Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? {A.K.A.} Will LLMs Replace Knowledge Graphs?},
	author       = {Kai Sun and Yifan Ethan Xu and Hanwen Zha and Yue Liu and Xin Luna Dong},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {311--325},
	doi          = {10.18653/V1/2024.NAACL-LONG.18},
	url          = {https://doi.org/10.18653/v1/2024.naacl-long.18},
	editor       = {Kevin Duh and Helena G{\'{o}}mez{-}Adorno and Steven Bethard},
	timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
	biburl       = {https://dblp.org/rec/conf/naacl/SunXZLD24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/abs-2106-09685-LoRA,
	title        = {LoRA: Low-Rank Adaptation of Large Language Models},
	author       = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen{-}Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year         = 2022,
	booktitle    = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=nZeVKeeFYf9},
	timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2404-03302,
	title        = {How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?},
	author       = {Siye Wu and Jian Xie and Jiangjie Chen and Tinghui Zhu and Kai Zhang and Yanghua Xiao},
	year         = 2024,
	journal      = {CoRR},
	volume       = {abs/2404.03302},
	doi          = {10.48550/ARXIV.2404.03302},
	url          = {https://doi.org/10.48550/arXiv.2404.03302},
	eprinttype   = {arXiv},
	eprint       = {2404.03302},
	timestamp    = {Thu, 08 Aug 2024 08:05:58 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2404-03302.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{allenzhu2024physicslanguagemodels32,
	title        = {Physics of Language Models: Part 3.2, Knowledge Manipulation},
	author       = {Zeyuan Allen-Zhu and Yuanzhi Li},
	year         = 2024,
	url          = {https://arxiv.org/abs/2309.14402},
	eprint       = {2309.14402},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@article{huang2024prompting,
	title        = {Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts},
	author       = {Huang, Wenyu and Zhou, Guancheng and Lapata, Mirella and Vougiouklis, Pavlos and Montella, Sebastien and Pan, Jeff Z},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2405.06524}
}

@inproceedings{krayko-etal-2024-efficient,
	title        = {Efficient Answer Retrieval System ({EARS}): Combining Local {DB} Search and Web Search for Generative {QA}},
	author       = {Krayko, Nikita  and Sidorov, Ivan  and Laputin, Fedor  and Galimzianova, Daria  and Konovalov, Vasily},
	year         = 2024,
	month        = nov,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
	publisher    = {Association for Computational Linguistics},
	address      = {Miami, Florida, US},
	pages        = {1584--1594},
	doi          = {10.18653/v1/2024.emnlp-industry.116},
	url          = {https://aclanthology.org/2024.emnlp-industry.116/},
	editor       = {Dernoncourt, Franck  and Preo{\c{t}}iuc-Pietro, Daniel  and Shimorina, Anastasia},
	abstract     = {In this work, we propose an efficient answer retrieval system **EARS**: a production-ready, factual question answering (QA) system that combines local knowledge base search with generative, context-based QA. To assess the quality of the generated content, we devise comprehensive metrics for both manual and automatic evaluation of the answers to questions. A distinctive feature of our system is the Ranker component, which ranks answer candidates based on their relevance. This feature enhances the effectiveness of local knowledge base retrieval by 23{\%}. Another crucial aspect of our system is the LLM, which utilizes contextual information from a web search API to generate responses. This results in substantial 92.8{\%} boost in the usefulness of voice-based responses. **EARS** is language-agnostic and can be applied to any data domain.}
}

@misc{pandora,
	title        = {Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models},
	author       = {Jinyang Wu and Feihu Che and Chuyuan Zhang and Jianhua Tao and Shuai Zhang and Pengpeng Shao},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.13533},
	eprint       = {2408.13533},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inproceedings{popqa,
	title        = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
	author       = {Mallen, Alex  and Asai, Akari  and Zhong, Victor  and Das, Rajarshi  and Khashabi, Daniel  and Hajishirzi, Hannaneh},
	year         = 2023,
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {9802--9822},
	doi          = {10.18653/v1/2023.acl-long.546},
	url          = {https://aclanthology.org/2023.acl-long.546},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
	abstract     = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs{'} strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.}
}

@inproceedings{roee,
	title        = {Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?},
	author       = {Zorik Gekhman and Gal Yona and Roee Aharoni and Matan Eyal and Amir Feder and Roi Reichart and Jonathan Herzig},
	year         = 2024,
	booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16, 2024},
	publisher    = {Association for Computational Linguistics},
	pages        = {7765--7784},
	url          = {https://aclanthology.org/2024.emnlp-main.444},
	editor       = {Yaser Al{-}Onaizan and Mohit Bansal and Yun{-}Nung Chen},
	timestamp    = {Thu, 14 Nov 2024 17:20:55 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/GekhmanYAEFRH24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{role,
	title        = {On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models},
	author       = {Dongyang Li and Junbing Yan and Taolin Zhang and Chengyu Wang and Xiaofeng He and Longtao Huang and Hui Xue and Jun Huang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.16367},
	eprint       = {2406.16367},
	archiveprefix = {arXiv},
	primaryclass = {cs.IR}
}

@misc{wang2024wiserethinkingknowledgememory,
	title        = {WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models},
	author       = {Peng Wang and Zexi Li and Ningyu Zhang and Ziwen Xu and Yunzhi Yao and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},
	year         = 2024,
	url          = {https://arxiv.org/abs/2405.14768},
	eprint       = {2405.14768},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

