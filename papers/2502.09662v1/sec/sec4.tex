\subsection*{Data collection and annotation}\label{subsec4-1}
The established CCS-127K dataset is a highly diverse collection of cervical cytology specimens, enriched with both cell and slide annotations. In total, we compiled 128,423 cytological specimens from 48 different centers. The data collection process involved several key steps. First, the specimens were prepared using commonly employed cytology sedimentation methods, including natural, membrane, and centrifugal techniques. Next, all liquid-based cytology specimens were digitized into WSIs using various imaging protocols, which contributed to the diversity of cytology images within the CCS-127K dataset. Four distinct scanners with varying specifications were employed: the Pannoramic MIDI (3DHISTECH Ltd.), SQSL-510 (Shenzhen Shengqiang Technology Ltd.), Aperio AT2 (Leica Microsystems Ltd.), and HDS-MS-200A (Xiamen Heidstar Ltd.). 
Subsequently, quality control procedures were conducted, resulting in a final tally of 127,471 specimens available for development and evaluation within the Smart-CCS system. During data annotation, cytologists provided both cell-level and WSI-level annotations according to TBS guidelines \cite{nayar2015bethesda}. For cell-level annotations, cytologists first identified RoIs containing abnormal cells and then utilized bounding boxes to delineate all abnormal cells within the RoI. These annotations consisted of six major categories of abnormalities: ASC-US, LSIL, ASC-H, HSIL, SCC, and AGC,  and were used to develop the abnormal cell detector. It is noteworthy that the AGC category encompasses typical glandular cells exhibiting epithelial cell abnormalities, including AGC-NOS, AGC-FN, AIS, and ADC. This study specifically focuses on cancer screening, and consequently, organism categories from TBS were excluded from further analysis. For slide-level annotations, we obtained corresponding diagnostic results from each participating center. These results were assessed by two cytologists, each possessing over ten years of experience, who examined the cytology specimens using a microscope. In case of discrepancies, a third senior cytologist, with over fifteen years of experience, rendered the final decision. The slide-level labels comprised seven cytology grades: NILM, ASC-US, LSIL, ASC-H, HSIL, SCC, and AGC. Additionally, we gathered 738 corresponding histological diagnostic results from prospective cohorts to further evaluate the system's diagnostic capabilities. When processing biopsy results, benign and inflammatory findings were classified as negative, while carcinoma cases were categorized as CIN3 for diagnostic evaluation.

\subsection*{Data preprocessing}\label{subsec4-2}
Each cervical digital slide typically reaches a giga-pixel resolution of up to 100,000 $\times$ 100,000 pixels, which needs to be tiled into patches for model input to fulfill the computation needs, shown in Extended Data Fig. \ref{SF_preprocessing}. 
During slide preprocessing, we employed OpenSlide library \cite{goode2013openslide} and CLAM toolbox \cite{lu2021data} for foreground extraction and slide tiling. The CLAM toolbox first used thresholding segmentation to remove a significant amount of irrelevant background, and then performed non-overlapping patching at \(1,200 \times 1,200\) pixels in the foreground. The final statistics of WSI and patch are summarized in Extended Data Table. \ref{ST_patches}.

\subsection*{Quality control for slides and patches}\label{subsec4-3}
Strict quality control is essential for both cytologist screening and AI system development. There are several factors contributing to the reduced quality, including sample preparation, staining reagents, and scanning protocols, which may impede the effective representation of deep features. In clinical practice, cytologists often refer to TBS criteria for sample adequacy assessment \cite{nayar2015bethesda}, including minimum squamous cellularity criteria, transformation zone component, obscuring factors, and interfering substances, along with imaging. Following this, we established the exclusion criteria, shown in Extended Data Fig. \ref{SF_qualitycontrol}, considering a) preparation (impurities, dried specimen, interfering labeling), b) staining (overstaining, uneven staining), c) scanning (out-of-focus, severe artifacts), d) minimum cellularity. We additionally employed a threshold-based foreground segmentation algorithm to assess cytology patch quality, effectively excluding those with insufficient foreground proportion. As a result, we excluded 952 WSIs and 46,136 patches from the CCS-127K dataset to ensure high-quality cytology data for system development and evaluation. 

\subsection*{Large-scale self-supervised cytology pretraining}\label{subsec4-3}
Large-scale self-supervised pretraining has made significant advancements in computational pathology, benefiting greatly from large-scale publicly available histology datasets \cite{huang2023visual,lu2024visual}. For instance, CtransPath was pretrained using 32K WSIs (4.2M patches) from public TCGA and PAIP datasets \cite{wang2022transformer}, and UNI was pretrained by 100K WSIs (100M patches) collected from private data and the GTEx consortium \cite{chen2024towards}.
This resulted in the development of generalizable pretrained models that promote downstream histological tasks. However, the inaccessibility of cytological data significantly hinders large-scale pretraining in computational cytology. To overcome this limitation, we collected more than 127K cytology WSIs with 227 million patches to support large-scale pretraining in Smart-CCS.

 The structure of visual pretraining is provided in Fig. \ref{F3_method}(a), we utilized the SOTA self-supervised learning framework, DINOv2 \cite{oquabdinov2}. This framework employs a student-teacher network, where the student model is supervised by pseudo labels from the teacher model. Thus, the network output distributions are aligned for knowledge distillation. This pretraining framework relies on alignment strategies, primarily achieved through reconstruction loss and alignment loss between the outputs of the student and teacher networks. Specifically, given a batch of cytology patches, they are first randomly augmented, including color jittering, gaussian blur, and solarization, to generate multiple augmented patches, which are then subjected to global crop and multiple local crops. The global crop patches are randomly masked for masked image modeling.
Then, the global crops without masks serve as inputs to the teacher network, while the global crops with masks and local crops serve as inputs to the student network, outputting crop prototypes. Subsequently, prototype alignment involves reconstruction loss and alignment loss to update the student network. Masked image modeling in reconstruction loss utilizes the student's output to predict the teacher's output, encouraging the student network to learn semantic and contextual information based on the surrounding pixels of the masked regions. Regarding alignment loss, aggregated crop prototypes (both global crop with masks and local crop) from the student are aligned with the corresponding global crops without masks through minimizing cross-entropy loss. Finally, the teacher network is updated by the exponential moving average of the student network. More pretraining details and settings are presented in the Extended Data Table. \ref{ST_pretrain_para}.

Leveraging such self-supervised pretraining, the generalizable cytological knowledge can be captured and accumulated from large-scale diverse data for general cytology understanding. After pretraining, the pertained model serves as a feature extractor for downstream cancer screening tasks. In that case, the extracted features can efficiently generalize and adapt to task-specific whole slide image classification tasks equipped with supervision information, resulting in strong and consistent performance in multi-center retrospective and prospective validations.

\subsection*{Cytology WSI classification model finetuning}\label{subsec4-4}
The objective of this study is to develop a trustworthy and generalizable WSI classification model for cervical cancer screening. 
Existing cytology WSI classification studies have consistently adhered to the two-step WSI classification approach, specifically focusing on abnormal cell detection and slide-level aggregation \cite{lin2021dual,wang2024artificial,zhu2021hybrid,yu2023ai}. This aligns with the clinical setting where cytologists first identify suspicious cells and then provide patient diagnostic results. 
These studies are consistent in the first step, developing a strong abnormal cell detector for atypical or malignant cell detection, while different in the second step with diverse designs of cell aggregation. Some studies introduced morphological segmentation of atypical cells (e.g., ASC-US) for refined guidance towards final diagnosis \cite{zhu2021hybrid,yu2023ai}. However, pixel-level annotation of cytoplasm and nuclei entails significant labor burden for system development. 
Other designs involving cell statistics or rule-based WSI classifiers resulted in screening performance highly dependent on detector performance, as weak classifiers limit generalization capability \cite{lin2021dual,wang2024artificial}. Additionally, permutations and combinations of models can increase the model complexities \cite{zhu2021hybrid,wu2024development}. Recently, another stream followed histology researches and adopted multi-instance learning (MIL) scheme for WSI classification \cite{zhao2024less,jin2024hmil}. 
This stream ignores cell information and just relies on slide-level supervision. Therefore, leveraging both cell-level supervision from detection and slide-level supervision from MIL is the main consideration in algorithm design. 

The proposed cancer screening algorithm is provided in Fig. \ref{FE1}(b), consisting of two main networks, i.e., abnormal cell detection and slide-level classification. Given a cytology WSI, local patches are extracted from the WSI using a non-overlapping sliding window of 1,200$\times$1,200 pixels. 
These patches are then inferred by the selected DDETR \cite{zhu2020deformable} to identify abnormal cells as abnormality candidates with predicted information including locations, types, and confidences. The first step primarily suppresses diagnosis-irrelevant information such as background and normal cells, while the second stage aggregates informative candidates and learns diagnosis-related representations. In the second step, after sliding through the entire WSI, a bag is constructed by aggregating selected cell images, which are cropped from WSI based on the predicted locations, classes and confidences. Subsequently, predicted abnormal cell images in the WSI bag are mapped into generalizable slide features through the frozen feature encoder from the pretraining stage and propagated through the classification head to obtain the final diagnosis results. 
This cytology WSI classification algorithm provides both abnormal cells and suggested diagnostic results. More techniques about the two steps are detailed as follows.

\noindent \textbf{Abnormal cell detection.} In abnormal cell detection, the cytology images are first augmented and then mapped into multi-scale feature maps through the backbone with 1/8, 1/16, 1/32, 1/64 of the original size. The multi-scale feature maps serve as the input to the transformer structure, which includes encoder and decoder layers. Each encoder layer comprises the following components, positional encoding, residual connection, layer normalization, feedforward neural network \cite{dosovitskiy2020image}, and the proposed multi-scale deformable attention module (MSDAttn). This module, designed for capturing deformable information, is a variant of multi-head self-attention \cite{dosovitskiy2020image} and implemented as follows,
\begin{equation}
\operatorname{MSDAttn}\left(\boldsymbol{z}_q, \hat{\boldsymbol{p}}_q,\left\{\boldsymbol{x}^l\right\}_{l=1}^L\right)=\sum_{m=1}^M \boldsymbol{W}_m\left[\sum_{l=1}^L \sum_{k=1}^K A_{m l q k} \cdot \boldsymbol{W}_m^{\prime} \boldsymbol{x}^l\left(\phi_l\left(\hat{\boldsymbol{p}}_q\right)+\Delta \boldsymbol{p}_{m l q k}\right)\right]
\end{equation}
where $M$, $L$, and $K$ are the number of attention head, feature level, and sampling point, respectively. $\boldsymbol{z}_q$ and $\hat{\boldsymbol{p}}_q$ denote content feature and the normalized coordinates of the reference point for each query element $q$, and $\boldsymbol{x}^l$ are the multi-scale feature maps.  $A_{m l q k}$ and $\Delta \boldsymbol{p}_{m l q k}$ are the attention weight and the sampling offset projected from $\boldsymbol{z}_q$. $\phi_l\left( \cdot \right)$ represent a scaling function. $\boldsymbol{W}_m$ and $\boldsymbol{W}_m^{\prime}$ are learnable weights. 
In the decoder, there are the same components with the object queries input to constrain the maximum number of detections. The final prediction head, a feed-forward network acts as the classification and regression head, predicting the probability and bounding box coordinates. The above multi-scale deformable design is highly aligned with the challenges of cancer screening, 1) small object detection, such as naked nuclei cells, 2) multi-scale for multi-resolution cytology images, 3) diverse morphological characteristics. 

In this study, we also evaluated other SOTA object detection models, namely Faster R-CNN \cite{ren2015faster}, YOLO-v3 \cite{redmon2018yolov3}, RetinaNet \cite{lin2017focal}, DETR \cite{carion2020end}. In the cancer screening algorithm, we first train the abnormal cell detector using the cell-level CCS-Cell dataset with instance annotations, and then adopt the trained detector to screen abnormal cells in the first step of CCS algorithm.

\noindent \textbf{Whole slide image classification.} In the second step, WSI aggregation, thousands of detected abnormal cells with each WSI are aggregated for the final WSI classification.
Regarding cell selection and aggregation, we consider the top-k confident cell images for each abnormal class, ASC-US, LSIL, ASC-H, HSIL, SCC and AGC, generating an WSI feature bag to represent original WSI. Subsequently, each cell image in the bag is augmented as input for the classifier. 

 In terms of WSI classifier, cell images are first mapped into 1024-dimensional features by our pretrained vision transformer with frozen weights. All mapped instance features in the WSI bag are concatenated to form a feature bag with size of $n$×1024 (where $n$ is the number of instances), serving as the generalized representation of the original WSI. 
 Finally, the feature bag of abnormal cells passes through trainable classification heads to obtain slide-level predictions. In the ablation experiments, we compared several SOTA classification heads, including MIL-based models such as MeanMIL, MaxMIL, ABMIL \cite{ilse2018attention}, CLAM \cite{lu2021data}, DSMIL \cite{li2021dual}, TransMIL \cite{shao2021transmil} and S4MIL \cite{fillioux2023structured}. 

\subsection*{Test-time adaptation with prototype alignment}\label{subsec4-7pro}
To tackle the complex clinical settings, we introduced adaptation techniques into our Smart-CCS diagram to adapt the developed model in the inference stage, further enhancing system robustness.
The satisfactory performance of deep learning models relies on a strong assumption that the training and testing data are drawn from the same or similar distribution, namely independently and identically distributed (IID) \cite{quinonero2022dataset}. However, for AI systems, cross-center data variability poses a significant challenge in clinical deployment, often leading to severe performance degradation  \cite{asif2021towards}. This issue primarily stems from variations in specimen preparation and imaging protocols across different hospitals, which can cause shifts in cytology WSI data. These shifts may manifest as differences in resolution, staining intensity, or overall image appearance, potentially introducing biases toward specific staining and imaging protocols.

Test-time adaptation considers the settings involving adapting a trained task-specific model to the current test batch before making predictions. Thus, we introduce the test-time adaptation into Smart-CCS to optimize the trained model by dynamically updating model weights during inference. To be more concrete, we propose a test-time adaptation approach with prototype alignment, which enhances the adaptability of the trained WSI classification model by leveraging both the current test batch and source supervision information. This adaptation strategy ultimately improves the model's performance in real-world clinical scenarios.
Firstly, we extract WSI features using pretrained extractor from retrospective data, with size of $n$×1024 (where $n$ is the number of instances).
The top-$k$ confident WSI features for each category constitute a designed class-wise prototype bank that stores the diagnostic supervision information from the retrospective samples. 
Subsequently, for WSIs from current clinical scenarios, the model infers abnormal cells as candidates. 
For adaptation, given the current batch $x$ with $m$ samples, the $i_{th}$ sample containing $n$ cell instances is augmented to improve the diversity of cell instances \cite{wang2022continual}. The current batch $x$ with its augmented view $\tilde{x}$ are transformed into features $f$ and $\tilde{f}$. Then, we build a mean teacher framework to distill the alignment knowledge from source to current test samples. The teacher and student are initialized by the trained slide classifier. Subsequently, the knowledge is distilled through feature alignment loss and prediction alignment loss. The feature alignment loss aligns current batch $f$ with augmented view $\tilde{f}$ and prototypes via contrastive learning. Here, the positive pairs in $J^{+}$ are built by current sample $I$ with corresponding augmentation, and the nearest prototypes computed by the cosine similarity. Then, the feature alignment loss is formed as, 
\begin{equation}
\mathcal{L}_{Falign}= - \sum_{i \in I}  \sum_{j^{+} \in J^{+}} \log \frac{\exp \left(\frac{Sim(z_i, z_{j^{+}}) }{ \tau}\right)}{\sum_{j \in J} \exp \left(\frac{Sim(z_i \cdot z_j)}{ \tau}\right)},
 \end{equation}
where $Sim(z_{i},z_{j})=z_{i} \cdot z_{j} /(\left\|z_{i}\right\|\left\|z_{j}\right\|)$. 



$z_{i}$ represents the $i_{th}$ embedding after projection, and $\tau$ is a tunable temperature hyper-parameter.
For the denominator, $J=J^{+}+J^{-}$ denotes the total number of positive ($J^{+}$) and negative ($J^{-}$) samples. In terms of the prediction alignment loss, student and teacher networks output predictions $\tilde{y}^{s}_{ic}$ and $\tilde{y}^{t}_{ic}$ for given inputs $x_{i}$. Then, the consistency loss $\mathcal{L}_{Palign}$ is employed to align these predictions, formed as,
 \begin{equation}
\mathcal{L}_{Palign}=  -\sum_{c=1}^C \tilde{y}^{s}_{ic} \log \tilde{y}^{t}_{ic}
 \end{equation}
where $C$ is the total number of classes.
Finally, the student network is updated using the overall loss. The teacher network is updated stably using an exponential moving average. To keep prediction smooth, the ensemble of the teacher output and the student output serves as the final prediction after test-time adaptation.

In conclusion, pretraining leverages general cytology information, finetuning specializes the pretrained model for cancer screening specific tasks, and adaptation further optimizes the finetuned model for clinical evaluation settings. Together, these three techniques within Smart-CCS significantly advance the generalizability of cervical cancer screening.

\subsection*{Model training and implementation}\label{subsec4-6}
In the experiment, we utilized Python (v3.9.0) and PyTorch (v2.0.0) \cite{paszke2019pytorch} complied with CUDA (v12.1)(\url{https://pytorch.org}) for model training and evaluation. In pretraining stage, we employed DINOv2 (\url{https://github.com/facebookresearch/dinov2}) for self-supervised pretraining using 2$\times$8 80GB NVIDIA H800 GPU cards. To adapt the size of cell instances, we reduced the local crop scale to 0.02 and set the local crop number to 8. Other pretraining parameters are listed in Extended Data Table. \ref{ST_pretrain_para}. For WSI patching and preprocessing, we employed OpenSlide-python (v1.3.1) and in-house library to load cytology WSIs. Then, the CLAM toolkit (\url{https://github.com/mahmoodlab/CLAM}) was utilized for tiling WSI into 1,200$\times$1,200 patches. 
In training stage, we utilized 8$\times$ 24GB NVIDIA GeForce RTX 3090 GPUs to conduct experiments. For detector development, we implement and compare models under the MMDetection library (v3.0.0) (\url{https://github.com/open-mmlab/mmdetection}). To accelerate cell prediction inference within WSI, we employed distributed data parallelism techniques. The parameters of the employed detector and classifier are detailed in Extended Data Table. \ref{ST_det_para}.

\subsection*{Evaluation metrics and setting}\label{subsec4-8}
We conducted large-scale evaluation of cell-level and WSI-level tasks for cancer screening. In cell-level evaluation, we reported overall performance using metrics AP50 (average precision under 0.50 IoU threshold), mAP (mean average precision under IoU from 0.5 to 0.95, step 0.05), and mAR (mean average recall under IoU from 0.5 to 0.95, step 0.05). We also described the AP50 for fine-grained per-class evaluations. 



In WSI-level evaluation, we reported the performance of subgroups including ECA, ASC-US+, LSIL+, HSIL+. The cytological screening results from different subgroups guide subsequent diagnostic procedures and follow-up actions. For instance, if the screening result falls under the ECA subgroup—includes any category of squamous or glandular abnormalities—the next step is to perform an HPV test. If the result falls under the LSIL+ subgroup (indicating LSIL or more severe findings), a colposcopy is recommended to further evaluate the cervical tissue. Therefore, we summed the predicted logits of each class in the subgroup to obtain binary predictions. In retrospective and prospective studies, we adopted metrics consisting of ACC (accuracy), AUC (Area Under ROC Curve), and F1 Score to evaluate the overall performance of WSI classification, and provided sensitivity and specificity metrics. In the experiments, we tuned heyeparameters to ensure gradual convergence during pretraining and finetuning (Extended Data Table. \ref{ST_pretrain_para}-\ref{ST_det_para}). The checkpoint achieving the best performance on the validation set was used for subsequent evaluation experiments.

\subsection*{Statistical analysis}\label{subsec4-11}
We report a 0.95 confidence interval (CI) for statistical analysis. The feature visualization is implemented using t-SNE in scikit-learn (v1.0.2). 