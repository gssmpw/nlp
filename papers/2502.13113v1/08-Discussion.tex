\section{Additional Related Work}

\autoref{tables:notation} shows prior works on hierarchical and heterogeneous accelerators. Other works like MAESTRO~\cite{kwon2019understanding}, Eyeriss~\cite{eyeriss2016isca} and OMEGA~\cite{garg2021understanding} have proposed taxonomies to classify scheduling strategies. Herald~\cite{kwon2021heterogeneous} proposes a heterogeneous accelerator framework based on cross-node-only leaf, but partitions the workload based on optimal spatial parallelism strategies. Works like Sparseloop~\cite{sparseloop}, %Orojenesis~\cite{orojenisis}, 
LayoutLoop~\cite{feather}
CimLoop~\cite{cimloop} have also modified Timeloop to enable exploration in new domains like sparsity, data layout, compute-in-memory and so on.

%\vspace{-1mm}
\section{Conclusion}
\label{sec:discussion}

Mixed-reuse applications are an emerging trend in the field of AI, with transformers being the most popular. Hierarchical and heterogeneous architectures have is an emerging way to run operations of different arithmetic intensities. In this work, we propose a taxonomy to classify hierarchical and heterogeneous processors (HHP's) based on the position of compute across different levels of memory hierarchy and on the location on (or absence of) heterogeneity. We modify timeloop to enable blackbox mapping on sub-accelerators in a hierarchical or heterogeneous and study the performance and energy of these architectures for transformer workloads. We find that the performance and energy trends across HHP's greatly vary depending on the type of transformer (encoder-only or decoder-only), memory bandwidth partitioning and even the energy breakdown varies across workloads and architectures.  We envision that the taxonomy and proposed framework will enable exploration of new accelerators for mixed-reuse applications.

\section*{Acknowledgment}
This work was supported by ACE, one of the seven centers in JUMP 2.0, a
Semiconductor Research Corporation (SRC) program sponsored by DARPA.