\section{Background}
\label{sec:background}

\subsection{Mapping}

Mapping refers to the a set of loop-transformations to schedule the workload on spatial accelerator. Mapping comprises of different aspects like loop permutation, parallelization strategies, tiling strategies (slicing a dimension into multiple loop nests depending on buffer sizes) etc. Mapping strategies mainly affect the static utilization of the PE-array, and the reuse achieved.

\subsection{Mixed-reuse Workloads}

Mixed-reuse workloads refer to workloads which have both high arithmetic intensity and low arithmetic intensity operations. Unlike earlier DNNs which have high- intensity operations or scientific applications like CG which have low- intensity operations, mixed-reuse workloads have both high- and low-intensity operations. One of the examples is encoder-only transformer (e.g. BERT) where Q/K/V generation and deprojection layers are high arithmetic intensity operations while logit and attend have relatively low arithmetic intensity. However, in BERT, there are sequential dependencies between high- and low-reuse operations, and the only operations that can overlap are logit ($P=QK^T$) and value generation operations ($V=IW_v$).

 \insertFigure{roofline}{Roofline in heterogeneous accelerator with high- and low-reuse sub-acceelrators compared to homogeneous accelerator with total area and memory bandwidth.}


Decoder-only transformers (e.g. GPT-3, Llama), on the other hand, have pre-fill and decode stages, and can be pipelined at granularity of single batch. Prefill stage in terms of einsums, is similar to the encoder-only transformer with Q/K/V generation, logit, attend, deprojection and feed-forward network. In the decode stage, new tokens are generated for each query one at a time. As a result, the sequence length on query side is 1, and the process is repeated until all tokens are generated. As a result, decode stage consists of multiple tensor operations with smaller aspect ratios with lower MACs as well as proportionally lower arithmetic intensity. These attention layers are repeated serially for each token. Since prefill and decode stages for a group of tokens can be decoupled in a coarse grained manner, mapping prefill stage on a high-reuse accelerator and decode stage on a low-reuse accelerator provides much better decoupling of high- and low-reuse operations than encoder-only models. Throughout this work, we discuss the impact of this decoupling on homogeneous, heterogeneous and hierarchical accelerators.
