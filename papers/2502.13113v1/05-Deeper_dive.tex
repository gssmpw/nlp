\section{Deeper Dive: Analysis of HHP's}

%\TODO{Map-space}

The \HHPName taxonomy provides a structured way of classifying the architectures based on the relative location of sub-accelerators and levels of memory hierarchy that have compute.

\subsection{Pros and Cons of Heterogeneous Architectures}

Heterogeneous accelerators are good at decoupling high-reuse and low-reuse accelerators and they make it easier for the mappers to assign high-reuse task to one unit and low-reuse task to another unit. However, for an area normalized setup, both individual sub-accelerators have lower compute roof compared to a homogeneous accelerator with same number of resources. 

If the workload is sufficiently compute-bound or if the memory bandwidth is sufficiently high, homogeneous accelerator utilizes its complete compute roof to achieve undivided throughput. However, the heterogeneous setup is beneficial when the compute-roof gap between the operations is as significant as 1-2 orders of magnitude. In that case, the homogeneous accelerator wastes its compute roof, while a perfect overlap between heterogeneous sub-accelerators leads to better utilization.

Another factor that affects the performance of heterogeneous accelerator is the dependency graph. For example, BERT only has a few opportunities to overlap two operations in an intra-cascade fashion (e.g., value matrix generation and logit operations). Moreover, typically the maximum sequence length is less than the hidden dimension of the model ($L_{max}<d_{model}$), which implies that the compute volume of one BMM operation is lower than one GEMM operation. Moreover, the number of GEMM operations in an attention layer itself is twice the number of BMM operations. This exacerbates the compute volume gap beween high and low-reuse operations in BERT. Thus, we observe that, with sufficiently high memory bandwidth, heterogeneous accelerators are less effective on workloads like BERT.
GPT on the other hand, has pre-fill and decode stages that can be overlapped in an inter-cascade fashion. Decode stage has lower reuse than prefill stage. These sub-cascades can be executed independently, and take advantage of the overlap that heterogeneous sub-accelerators offer, thus minimizing the wastage of compute roof in homogeneous accelerators.

\subsection{Impact of heterogeneity position}

The biggest advantage of hierarchical architecture is ease of decoupling high- and low-reuse sub-accelerators. This is especially useful when high- and low-reuse operations have different shapes. Intra-node architecture on the other extreme has sub-accelerators coupled with most of the parallelism strategies being common due to the shape. For example, in a RaPiD-like~\cite{rapid} setup, the only parallelism dimension that is different is the number of rows in the PE array and the Special Function Unit array. This easily works for element-wise operations followed by DNN layers, since they use the input generated by the previous operation, preserving the problem dimensions across sub-accelerators. However, repurposing it for two different operations with different reuse strategies poses mapping challenges, as we also see in one of the results in~\autoref{sec:eval}. Cross-depth heterogeneous accelerators have the lowest energy, since they avoid data movement across an entire level of memory hierarchy.

\insertWideFigureScaled{framework}{Evaluation framework built on Timeloop~\cite{timeloop}.%\TK{where is Timeloop model?}\RG{Its internally used by the mapper, the stats reported are for the optimal mapping.}
}{0.85}


%\subsection{Pros and Cons of Compound Heterogeneity}

\subsection{Mapping on HHP's}
\label{sec:mapping}

In this work, while we run the timeloop mappers independently on sub-accelerators, we make use of spatial parallelism mapping constraints to capture relative position of heterogeneity. For example, in a RaPiD~\cite{rapid}-like\footnote{Note in actual RaPiD accelerator, the functional units are used for element-wise and high precision operations, but the concept of two sub-accelerators in one node can be used to compute mixed-reuse cascades.} intra-node accelerator, the number of columns per sub-accelerator are equal, and the same dimension can be parallelized across columns. Cross-depth accelerator on the other hand, allows for complete decoupling of spatial parallelism strategies.

Since we partition the workload in an operation-by-operation manner and assign it to sub-accelerators based on reuse, the operation mapping search can be carried out independently on sub-accelerators, which we refer to as \textit{blackbox} mapping. Therefore the design-space is additive and not multiplicative. For example, in case of one high-reuse operation and one low-reuse operation, the design-space is $O(High+Low)$ and not $O(High\times Low)$


\subsection{Resource Partitioning}

For heterogeneous accelerators, our framework (\autoref{sec:expt}) partitions the compute roof (number of PEs), L1 and L2 resources, and memory bandwidth. We observe that the performance of heterogeneous accelerators is highly sensitive to memory bandwidth. For cascade(s) where the latency of low-reuse operations dominate (e.g. GPT3), low-reuse sub-accelerators should get a larger portion of the memory bandwidth. However, if the number of high-reuse operations is larger (e.g. BERT), allocating higher memory bandwidth to low-reuse operations, can affect the performance of high-reuse operations which primarily determine the latency. In such cases, the bandwidth partitioning is governed by two conflicting forces - (1) high-reuse operations dominate the cascade, and (2) low-reuse operations are in need of higher portion of the bandwidth, to improve the operation latency. Across the memory hierarchy, we partition the LLB in the ratio of compute roof, as high-reuse operations benefit from more on-chip memory space while low-reuse operations peak their arithmetic intensity even with low on-chip reuse space. In case of hierarchical accelerator, L1 is used purely by the high-reuse sub-accelerator, and is not partitioned.