\section{Introduction}
\label{sec:introduction}

Artificial intelligence (AI) applications consist of  cascades of tensor operations of various shapes and sizes~\cite{kwon2018maeri,kwon2019understanding,flat}. As a result, these cascades have varying arithmetic intensities, i.e. reuse. An obvious example of this is transformers~\cite{attention} with multi-head attention using batched matrix multiplication (BMM) and layer normalization which have lower reuse than GEMMs involved in Q,K,V generation and feedforward layers. Autoregressive decoding~\cite{gpt4} has a decode phase which has less reuse than BMMs in the summarization phase~\cite{neupim}. In domains such as AR/VR, operations have differences in the range of arithmetic intensities, and even individual AI models (task) have different overall reuse trends~\cite{xrbench}. Recent applications like neurosymbolic AI~\cite{neurosymbolic} combine probabilistic models and DNN models. Therefore, AI workloads exhibit mixed reuse (i.e., a mix of high and low reuse). 

Earlier accelerators~\cite{kwon2018maeri,tpu-isca,eyeriss2016isca,nvdla} and mapping frameworks~\cite{kao2020gamma,cosa,kwon2019understanding,interstellar,timeloop,pipeorgan} were able to exploit reuse on earlier CNN layers with cubic aspect ratios. However, with the emergence of DNN application domains such as AR / VR~\cite{xrbench} and applications such as GNNs~\cite{hamilton2017inductive} and transformers, applications have operations with low arithmetic intensity and hence prior works have used inter-operation fusion/pipelining~\cite{garg2021understanding,flat,flashattention,pipeorgan,tileflow,isca-pip,isos,tangram} which involves reusing the intermediate tiles by staging them on-chip. However, the arithmetic intensities of even a fused cascade of a few operations are low. For example, in the decode stage of autoregressive decoding, the arithmetic intensity of the operators is 1-2 orders of magnitude lower than the arithmetic intensity required to saturate the datapath~\cite{neupim}. These works also miss the opportunity of hiding low-reuse operations behind high-reuse operations to keep datapath utilization high.

To this end, hierarchical heterogeneous processors (HHP's, term coined by Symphony~\cite{symphony}) have emerged as an attractive solution, particularly for mixed-intensity workloads, where low-intensity operators can be hidden behind high-intensity operators. Recent works NeuPIM~\cite{neupim} and Duplex~\cite{duplex} proposed heterogeneous accelerators with different sub-accelerators at different levels of memory hierarchy. Symphony~\cite{symphony} proposed another hierarchical heterogeneous accelerator for sparse and dense processing on one accelerator substrate.
Mapping different operations on different sub-accelerators decouples low-reuse operations from the critical path by overlapping it with high-reuse operations, thus mitigating its impact. 
Thus, HHP's can concurrently running high-reuse and low-reuse operations uses resources in a complementary fashion. High-reuse operations require low memory bandwidth but high on-chip memory to reuse. Low-reuse operations require a higher memory bandwidth and a lower on-chip memory, just enough to hide the latency.~\autoref{fig:roofline} shows the compute roof and bandwidth partitioning example on a roofline.

With different HHP's being proposed, we aim to classify the HHP's under a common taxonomy, and explore the impact of architectural choices on performance and energy efficiency. 

%\TK{I think at this point you should mention that given the increasing set of HHPs being proposed, the goal of this work is to bring the diff HHPs under a common umbrella. And what that will enable ? And that way the diff aspects you have disvussed in next two paras can be discussed more systematically - wrt arch choices, mapping, etc}

Prior works have heterogeneous architectures with vastly different hardware organizations.
 The most popular example of heterogeneous architecture is the NVIDIA tensor core chip~\cite{blackwell} where there is heterogeneity between streaming multiprocessors and tensor cores. Moreover, this is the most tightly coupled form of heterogeneity, since both tensor core and streaming multiprocessors are governed by a single FSM controller. NeuPIM~\cite{neupim} and Duplex~\cite{duplex} are at the other end of the spectrum where the heterogeneous sub-accelerators are at different levels of memory hierarchy. This essentially means that for a system with NPU and in-DRAM/near-DRAM processor, the only shared resource is the DRAM, while for a near-LLB (Last Level Buffer) accelerator, the only shared resources are last-level buffers and DRAM. Another major difference between these architectures is that NVIDIA tensor-cores have compute only at the leaves of the memory hierarchy, while architectures like NeuPIM have compute across memory hierarchy.



Differences in these architectures have an impact on the performance of different workloads. For example, NeuPIM-like accelerators are more desirable for workloads with clearly marked high- and low-reuse operations running concurrently on separate sub-accelerators (e.g. prefill and decode phases of decoder-only transformers for workloads like GPT3~\cite{gpt3} and Llama~\cite{llama}). On another extreme, for traditional DNN's with sufficiently cubic aspect ratios, a homogeneous accelerator with compute only at the leaves, provides the highest undivided throughput. Moreover, with sufficiently high memory bandwidth, a homogeneous accelerator slightly edges a heterogeneous accelerator for an encoder-only mixed reuse transformer (e.g. BERT~\cite{bert}) where the arithmetic intensity differences gap between high- and low-reuse operations is less than an order of magnitude, and the dependencies make it hard to achieve perfect decoupling of high-reuse and low-reuse operations. Another factor that influences these architectures is mapping, particularly mapping constraints, coupled with the microarchitecture. For example, a heterogeneous accelerator could be sensitive to the mapping constraints in low-reuse sub-accelerator, but less sensitive to mapping constraints in the high-reuse sub-accelerator (because of sufficient dimension sizes), which is the case with decoder-only transformers.

%In this work, we aim to provide a systematic approach to characterize hardware organization of different hierarchical and/or heterogeneous accelerators.

In this work, \textit{we propose a \HHPNamenospace\footnote{\underline{H}eterogeneous and Hier\underline{AR}chical \underline{P}rocessors}, a taxonomy to classify the heterogeneous and hierarchical  accelerators.}
This provides a systematic approach to characterize hardware organization of different hierarchical and/or heterogeneous accelerators.
Specifically, the taxonomy classifies the accelerators based on relative location of sub-accelerators w.r.t. their FSM and memory hierarchy and on the basis of whether the compute is only at the leaf or throughout memory hierarchy.
We can also use the taxonomy to derive a new class of accelerators, which prior work has not exhibited.


We modify Timeloop~\cite{timeloop} and develop a wrapper around it to model the performance for various hierarchical and/or heterogeneous accelerator configurations described by our taxonomy. We compare several different hierarchical and/or heterogeneous architecture design points for various mixed-reuse tensor workloads.
We also explore the detailed impact of \textit{partitioning resources (e.g. memory bandwidth) across sub-accelerators} in a configuration.
