\section{Insights and Motivation}
\label{sec:motivation}

\subsection{Impact of Paritioning across Sub-accelerators}


As discussed, one of the ways of partitioning the workload on HHP sub-accelerators is based on reuse, with a high-reuse accelerator running high reuse component decoupled from the low reuse accelerator running low reuse component. The case for partitioning the workload by reuse is that high-reuse and low-reuse operations use resources in a complementary fashion.

\subsubsection{Impact on roofline}

 High-reuse operations in the application are highly compute-bound and end up saturating the bandwidth even with a fraction of peak memory bandwidth. $BW_{high-reuse}=BW_{peak}\times \frac{AI_{tipping}}{AI_{op}}$. On the other hand, low-reuse operations are bounded by memory bandwidth and performance in the memory-bound region is proportional to the memory bandwidth, and hence these operations are more likely to use the memory bandwidth.~\autoref{fig:roofline} shows how the roofline is split in heterogeneous architectures with a high-reuse and low-reuse sub-accelerator compared to a plain homogeneous accelerator with same total compute roof. High-reuse sub-accelerator has a higher compute roof and requires a lower memory bandwidth, as a result of which it can be compute-bound even if the tipping point were higher. For a low-reuse accelerator, the performance is directly proportional to memory bandwidth. So in order to minimize major dark silicon in situations where high-reuse sub-accelerator is idle waiting for low-reuse accelerator to finish, a low-reuse accelerator is granted a higher portion of the memory bandwidth. One of the major caveats with this kind of split though is the overlap opportunity. It is possible that only 20\% of the whole cascade of operations can be run on high- and low-reuse sub-accelerators independently. In this case, the underutilization due to dark silicon is significant. However, if cascade can be decoupled into larger independent sub-cascades, mapping high- and low-reuse operations on a heterogeneous accelerator is beneficial compared to a homogeneous accelerator, since the latter gets massively underutilized during a low-reuse sub-cascade of operations.

% \insertFigure{roofline}{Roofline in heterogeneous accelerator with high- and low-reuse sub-acceelrators compared to homogeneous accelerator with total area and memory bandwidth.}

 \insertFigureScaled{resources}{Partitioning of LLB resources.}{0.9}

 

 \subsubsection{Impact on LLB utilization}
 
LLB usage of these operations trends in the opposite direction to roofline. High-reuse operations need vast LLB space in order to make use of the reuse opportunity due to the large dimensions of the matrix, while low reuse operations easily hit the peak arithmetic intensity when the smallest tensor fits the LLB requiring a fraction of LLB space, as~\autoref{fig:resources}(b) shows. Therefore, high-reuse and low-reuse partitioning is a synergetic way to allocate operations.

\insertWideFigureScaled{partitioning}{(a) Intra-cascade partitioning within the encoder model. (b) Inter-cascade partitioning of the decoder model into prefill and decode phases with the prefill phase mapped on high-reuse sub-accelerator and decode phase mapped on low-reuse sub-accelerator.}{.95}


\subsection{Examples of Workload Partitioning}

\textbf{Intra-Cascade Partitioning:}
\autoref{fig:partitioning}(a) shows example workload partitioning strategies in high- and low-reuse operations. High-reuse sub-accelerator runs the high-reuse operations (e.g. GEMMs in encoder models and prefill phase of decoder models etc.) while low-reuse sub-accelerator runs the low-reuse operations (e.g. Multi-head attention, entire decode phase of decoder models). As far as load distribution is concerned, since the high-reuse sub-accelerator has a larger number of PEs, prioritizing keeping the high-reuse sub-accelerator busy to the best extent possible avoids major dark silicon, even if low-reuse sub-accelerator is idle in cases where there are more high-reuse operations.

\textbf{Inter-cascade Partitioning:}
Inter-cascade partitioning has been popularized by prior works on heterogeneous dataflow accelerators\cite{kwon2021heterogeneous,qin2022enabling} in the context of having heterogeneity in terms of different dataflows used.  Within transformers themselves, HHP benefits from inter-cascade reuse as ~\autoref{fig:partitioning}(b) shows. Prefill and decode stages are mapped on high- and low-reuse sub-accelerators respectively. Unlike BERT, L and A operations in the prefill stage, are mapped on high-reuse accelerator here since the decode stage has 1-2 orders of magnitude lower reuse. \autoref{fig:partitioning} shows this difference on a roofline.

