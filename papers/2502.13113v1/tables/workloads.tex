\begin{table}[h]
\begin{scriptsize}
    \centering
    \caption{Transformer workload configuration. Sequence lengths for decoder-only transformers are listed as prefill/decode}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         \textbf{Workload} & \textbf{Models} & \textbf{Partitioning} & {$\mathbf{d_{model}}$} &
         \textbf{Seq length} \\\hline
         Encoder (translation) & BERT-large & Intra-cascade & 1024 & 256\\\hline
         Decoder (chatbot~\cite{genz}) & Llama-2 & Inter-cascade & 4096 & 3000/1000\\\hline
         Decoder (chatbot~\cite{genz}) & GPT3  & Inter-cascade & 12288 & 3000/1000\\\hline
          
    \end{tabular}
    \label{tables:workloads}
\end{scriptsize}
\end{table}