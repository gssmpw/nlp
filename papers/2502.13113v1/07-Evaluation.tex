\insertWideFigureScaled{eval-perf}{Speedup of various architecture configurations normalized to leaf+homogeneous. We zoom in on the utilization of homogeneous and cross-node heterogeneous accelerators on BERT.}{1}

\insertFigureScaled{eval-energy}{Energy of various architecture configurations.}{0.99}

\insertFigureScaled{eval-efficiency}{Energy per joule for various architecture configurations normalized to leaf+homogeneous}{0.99}


\insertFigureScaled{eval-energy2}{On-chip energy (excluding DRAM) breakdown by sub-accelerators running high-reuse and low-reuse operations.}{1}

\insertFigureScaled{eval-bw}{Impact of 50/50 bandwidth partitioning in case of decoder-only transformer.}{0.8}


\section{Results}
\label{sec:eval}

\subsection{Performance}

~\autoref{fig:eval-perf} shows performance of HHP's for transformer models. The performance of each workload depends on multiple factors, and there is no one size fits all architecture for all applications. Under the normal bandwidth of 2048 bits/cycle, encoder-only workload performs better on homogeneous configuration compared to heterogeneous counterpart since the number of high-reuse operations is larger, and the overlap potential is lower. We zoom into the utilization over time by homogeneous and cross-node accelerators on BERT, and observe that the ability to overlap high-reuse and low-reuse accelerator can affect the performance. We also find that the relative speedup of homogeneous accelerator deteriorates with lower bandwidth. We find that this is because of lower utilization of the PEs when the low-reuse workload is run. However, in case of decoder only workloads, the performance trends are reversed. This is because of better decoupling of high- and low-reuse subcascades in the form of prefill and decode stages. Heterogeneous sub-accelerators are able to utilize the PEs in a better way by parallelizing high- and low-reuse cascades. The low-reuse sub-cascade is slower than high-reuse sub-cascade. However, we also observe that tight coupling of sub-accelerators in intra-node configuration leads to lower utilization in case of decoder-only workload. On lowering the memory bandwidth, we find that the low-reuse sub-accelerator of the heterogeneous configuration gets hit harder, since it is using only a portion of the memory bandwidth in parallel with the high-reuse sub-accelerator. This trend is completely opposite of the impact of lowering bandwidth on encoder-only workload, since in case of encoder-only workload, low-reuse accelerator was already running at lower compute roof with 100\% utilization of that sub-accelerator and was not affected by lowering the bandwidth. In case of the decoder-only model, the achieved utilization was 19\% of the compute-roof and was further hit after lowering the bandwidth by 4x. 

\subsection{Energy}

~\autoref{fig:eval-energy} shows the energy of various architectures broken down across levels of memory hierarchy. The heterogeneous accelerator configuration has lower overall energy consumption than the homogeneous accelerator configuration for encoder and decoder workloads by roughly 10\% and 20\% respectively. For encoder-only models, the energy is dominated by register-file (RF) which shows good overall reuse, while for decoder-only models, the energy is significantly dominated by DRAM accesses, showing less local reuse opportunities. Hierarchical+cross-depth architecture has the least energy since the low-reuse sub-accelerator avoids data movement at one level of the memory hierarchy. For encoder-only cascades, the difference in the energy mainly stem from the register file energies, while in case of decoder-only cascades, the major parameters the differences stem primarily from LLB energies, and L1 energy in case of hierarchical+cross-depth heterogeneous accelerator.

\subsection{Multiplications Per Joule}

~\autoref{fig:eval-efficiency} shows multiplications per joule (energy efficiency) of various accelerator configurations. Cross-depth heterogeneous accelerator is the most energy efficient architecture, while the homogeneous architecture (with same number of total PEs as heterogeneous sub-accelerator) is the least energy efficient. The energy efficiency difference is more pronounced in case of GPT, where low-reuse operations dominate the energy.

\subsection{Energy Breakdown Across Sub-accelerators}

~\autoref{fig:eval-energy2} shows the on-chip energy breakdown across sub-accelerators in case of intra-node, cross-node and cross-depth heterogeneity. For encoder-only model (BERT), we observe that high-reuse operations contribute significantly to the energy, while for decoder-only models (GPT3 and Llama-2), we observe that low-reuse operations contribute more significantly to the energy.

\subsection{Sensitivity Study - Bandwidth Partitioning}

We also compare the bandwidth partitioning in case of decoder-only workload. We originally allocated 75\% memory bandwidth to low-reuse sub-accelerator in case of decoder-only workload because decode stage is significantly slower. However, allocating 50\% bandwidth to each sub-accelerator naively erodes the advantage heterogeneous configuration had by overlapping high- and low-reuse operations as~\autoref{fig:eval-bw} shows.

\subsection{Summary of Key Trends}

The key observations can be summarized as follows:

\squishlist
\item For encoder-only model (BERT), homogeneous accelerator has better performance since intra-cascade partition can depend on the dependency graph. For decoder-only models (GPT and Llama), heterogeneous accelerator performs better because of the ability to overlap high- and low-reuse operations. Decoder-only models significantly dominate latency.
\item Hierarchical+Cross-depth accelerators have the lowest energy and the highest energy efficiency.
\item The energy is dominated by DRAM in case of decoder models and register file in case of encoder models
\item Heterogeneous accelerators are sensitive to memory bandwidth partitioning.
\item On-chip energy is dominated by high-reuse operations for applications like BERT, while it is dominated by low-reuse operations for applications like Llama.
\squishend