\insertWideFigure{taxonomy}{Various examples hierarchical and/or heterogeneous processors described by \HHPName taxonomy. The square and chevron shapes represent different sub-accelerator architectures, while C represents the FSM controller tied to the sub-accelerators.}

\input{tables/notation}


\section{\HHPName Taxonomy}
\label{sec:taxonomy}


\autoref{fig:taxonomy} shows the proposed \HHPName taxonomy for classifying heterogeneous and/or hierarchical processors. The building blocks (sub-accelerators) in the figure are represented by square, chevron and circle, with change in shape representing heterogeneity. The box with the C represents the position of the FSM controller.



\subsection{Classification of HHP's}

We categorize them on the basis of two axes.

(1) \textbf{Leaf-only}\footnote{Many terms in the taxonomy are derived from treating memory hierarchy as a tree-like structure with DRAM being at the root and L1 being at the leaves with L2 at the intermediate level.}\textbf{ vs hierarchical:
}

\textit{Leaf-only:} Leaf-only implies traditional architectures with computation only at leaves of the memory hierarchy, i.e. closest to the L1 buffer.  Most of the accelerators fall into this category and some of the examples include TPUv1~\cite{tpu-isca}, TPUv4~\cite{tpuv4}, Herald~\cite{kwon2021heterogeneous}, NVIDIA B100 GPU's~\cite{blackwell} etc. \autoref{fig:taxonomy}a)-c) represent leaf-only accelerators where compute is close to only L1.

\textit{Hierarchical:} Hierarchical implies that the computation is distributed across multiple levels of the memory hierarchy. Heterogeneous architectures with processing-in-memory (NeuPIM~\cite{neupim}, Duplex~\cite{duplex}, etc.) are a major example of this. Another example, of hierarchical architectures is Symphony~\cite{symphony} which has "logical elements" for compute across the levels of the memory hierarchy. ~\autoref{fig:taxonomy}d)-h) show examples of hierarchical accelerators with sub-accelerators across memory hierarchy.

(2) \textbf{Location (or absence}\footnote{in case of homogeneous architectures}\textbf{) of heterogeneity:} 

\textit{Homogeneous: }One of the subcategories of this is homogeneous accelerator, where heterogeneity is absent. An example of this is TPUv1~\cite{tpu-isca}. The rest of the categories are based on the source of heterogeneity in the overall structure. \autoref{fig:taxonomy}a) and e) show examples of homogeneity.

\textit{Intra-node heterogeneous}: The finest-grained source of heterogeneity is intra-node heterogeneity(with tree-like treatment of memory hierarchy). In this category, sub-accelerators share a common FSM. An example of this from real-systems is NVIDIA B-100 where the tensor-core operates on the same program counter as the Streaming multiprocessor (SM). Another example of intra-node heterogeneity is a RaPiD core~\cite{rapid} with a 2-D array of MAC units along with a 1-D array to do specialized vector operations that require higher bit precision. \autoref{fig:taxonomy}c) and g) show examples of intra-node heterogeneity.

\textit{Cross-node heterogeneous}: Heterogeneity can also occur across nodes of a tree as opposed to within node. This is the most common form of heterogeneity among the architectures that are not homogeneous, and an example of this in the traditional sense is a CPU-GPU heterogeneous system. Among accelerators, examples include Symphony~\cite{symphony}, Herald~\cite{kwon2021heterogeneous}, AESPA~\cite{qin2022enabling}, TPUv4~\cite{tpuv4}, etc. \autoref{fig:taxonomy}b) and f) show examples of cross-node heterogeneity.

\textit{Cross-depth heterogeneous}: The coarsest form of heterogeneity occurs across different levels of memory hierarchy. An example of this is NeuPIM~\cite{neupim} which consists of an NPU at the leaves for GEMMs and uses processing-in-memory (DRAM) for vectors. \autoref{fig:taxonomy}d) shows a different example of this with sub-accelerators at L1 and L2.

\textit{Compound heterogeneous:} The above sources of heterogeneity are not mutually exclusive, and it is possible to use this framework to derive a new kind of architeture, with multiple sources of heterogeneity. \autoref{fig:taxonomy}h) shows an example of a combination of cross-node and cross-depth heterogeneity, where leaves have different sub-accelerators, and both of these are different from the accelerator at L2. Another example of compound heterogeneous accelerator could include a combination of cross-depth and intra-node heterogeneity with just one of the sub-accelerators having intra-node heterogeneity.

\textbf{Example datapoints:} Architectures are described using the two axes discussed above. For example, leaf-only + cross-node heterogeneous architectures, as the name suggests, exhibit cross-node heterogeneity and only have compute at the leaves as~\autoref{fig:taxonomy}b) shows with squares and chevrons at different nodes among the leaves. Another example is hierarchical+cross-depth heterogeneous, as~\autoref{fig:taxonomy}d) shows, where chevrons and squares are at different levels of the memory hierarchy. Note, that cross-depth heterogeneous is the only category that cannot have a leaf-only counterpart, since compute at atleast two levels is needed for cross-depth heterogeneity. \autoref{fig:taxonomy}c) shows leaf-only+intra-node heterogeneous architecture, where heterogeneity is within a node. Here, square and chevron share a common FSM. An example of this is a GPU with a tensor core. In~\autoref{sec:eval}, we mainly focus on configurations in~\autoref{fig:taxonomy} (a-d), since they cover all the axes, while showing through ~\autoref{fig:taxonomy} (e-h) that we can classify more complex architectures like Symphony~\cite{symphony} and derive new categories using the taxonomy.



\subsection{Describing Existing Works}

\autoref{tables:notation} shows some examples of existing architectures, described using the \HHPName taxonomy. The table only describes existing work and does not cover some of the combinations from~\autoref{fig:taxonomy}. These combinations are plausible, but have not been exhibited in prior works. Most of the earlier accelerators fall under the leaf-only+homogeneous category. These works include simpler accelerators with fixed-dataflow like TPUv1~\cite{tpu-isca} and Eyeriss~\cite{eyeriss2016isca}, and flexible accelerators that support multiple dataflows (e.g. Flexagon~\cite{flexagon} and MAERI~\cite{kwon2018maeri}). The next most common category is leaf-only+heterogeneous accelerators. Herald~\cite{kwon2021heterogeneous} was one of the earlier works to propose a heterogeneous accelerator with each sub-accelerator being good for different CONV operation shapes. AESPA~\cite{qin2022enabling} proposed a similar cross-node heterogeneous accelerator but for SpGEMMs. These works can be described as leaf-only+cross-node heterogeneous accelerators. Leaf only+ Intra-node heterogeneous accelerators are also popular. These works typically consist of the sub-accelerators sharing a common FSM. For example, in NVIDIA B100, SM's and tensor-core share an FSM. This is the most tightly coupled form of heterogeneity. Hierarchical accelerators have recently emerged in popularity. NeuPIM~\cite{neupim} and Duplex~\cite{duplex} directly target transformers with mixed-reuse operations and exhibit cross-depth heterogeneity. Symphony~\cite{symphony} is hierarchical and exhibits cross-node heterogeneity. However, even though, the sub-accelerators have different FSM's, unlike herald, sub-accelerators are cross-node within a cluster, and that cluster is then repeated across the level. Herald on the other hand consists of entire sub-accelerator in one local area.



