\section{EdgeMLBalancer Approach}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{Images/architecture.jpg}
    \caption{Architecture of EdgeMLBalancer}
    \label{fig:architecture}
\end{figure*}

We use the running example presented in the Section II to explain the approach. The goal of our EdgeMLBalancer approach is to enable efficient and adaptive real-time object detection on resource-constrained edge devices. By leveraging self-adaptation, our approach dynamically adjusts system behavior, such as model selection based on real-time CPU usage, to optimize resource utilization while maintaining high inference accuracy, as shown in \textit{Figure} \ref{fig:architecture}.

To achieve this, our system leverages a modular architecture. In the rest of this section, we will explain our approach using the running example presented in Section II. The \textit{Managing System} is at the core of this architecture, relying on the MAPE-K loop \cite{b13} to monitor runtime metrics, analyze performance, and select models based on CPU usage and accuracy trade-offs using an epsilon-greedy strategy. This design allows for a dynamic adaptation to changing environmental conditions, aiming to achieve a trade-off to balance performance, energy (through CPU), and inference accuracy.

\subsection{MAPE-K Feedback Loop}

The approach makes use of the Monitor-Analyze-Plan-Execute-Knowledge (MAPE-K) loop, enabling continuous monitoring, decision-making, and adaptation based on real-time data. Below, each component of the MAPE-K loop is described in detail.

\subsubsection{\textbf{Monitor}}

The \textit{monitoring component} operates as a data aggregation and reporting layer, ensuring the \textit{Managing System} has access to the latest operational information about the system and its environment. The metrics collected by the \textit{Monitor} are categorized into two groups as shown in \textit{Figure} \ref{fig:architecture}: \textit{ML Component Metrics} and \textit{Non-ML Component Metrics}. 

\noindent
\textbf{ML Component Metrics:}

It focuses on metrics directly related to the performance of the object detection models (can be extended to other classes of models as well). The metrics collected are:

\begin{itemize}
    \item Confidence Score (\textit{C}): Measures the reliability of the model's predictions for each object of the given frame. For a frame \textit{i} with \(d\) detected objects, the confidence score \(C_i\) is computed as: 
    \[C_i = \frac{\sum_{j=1}^{d} c_j}{d}
    \] 
    For instance, if a frame contains three detections with confidence scores of 0.85, 0.75, and 0.9, then the overall confidence score for that frame is computed using the above formula as 0.833. This score helps evaluate the model's accuracy and stability, where stability refers to the model's ability  consistently to produce reliable predictions across varying conditions and workloads. 

    % \item Number of Detections (\textit{D}): Represents the total count of objects detected in a frame, providing quantitative measure of the model's effectiveness in identifying relevant objects. For a frame \textit{i}, the detected count \(D_i\) is expressed as: 
    % \[D_i = n_i\]
    % where \(n_i\) is the total number of detected objects in frame \textit{i}.
\end{itemize}

Apart from the Confidence score (which is the main metric used in the context of this study), other metrics that can be monitored include the number of detections, model size, throughput (in terms of detections per unit of time), etc.

\noindent
\textbf{Non-ML Component Metrics:}

It captures metrics related to the system resource usage, and data from the managed system and its environment, emphasizing the computational constraints of edge devices. The metrics collected are:

\begin{itemize}
    \item CPU Usage (\textit{U}): Monitors percentage of processor utilization for model inference performed on each frame, providing real-time insights into the computational load imposed by the active model. High CPU usage can indicate bottlenecks, necessitating a switch to a less resource-intensive model. 
    % \item Battery Consumption (\textit{B}): Tracks the battery usage during inference, measured in milliampere-hours (mAh). 


\end{itemize}

The metrics measured are focused on our study and this can be extended to other metrics like measuring memory usage, battery usage, network utilization, etc.

The set of monitored metrics Metrics (\(M_\text{data}\)) capturing system resource usage \(U_i\), model performance \(C_i\), and the current model in use \(m_i\). For each frame \textit{i}, these metrics are collected continuously, allowing the system to maintain real-time awareness of both model performance and resource utilization. 
\begin{align*}
M_{\text{data}} = \{(m_i, C_i, U_i) | i = 1,2,...,n\}
\end{align*}
\textit{Example} In the running example section of traffic detection, \(M_\text{data}\) for frame \(i\) might include a CPU usage of \(13\%\), an inference accuracy of \(54.42\%\) for \(3\) detected objects, and the model name "EfficientDet Lite2" (refer Section II).

This granular real-time monitoring provides the basis for the \textit{Analyzer} to evaluate trends and adapt the system dynamically. This component not only monitors but also logs the metrics data into the \textit{Log Repository} in \textit{Knowledge} (discussed in later part of this section), facilitating a thorough performance review. 
% By bridging the gap between operational data and decision-making, the \textit{Monitor} plays a crucial role in enabling the self-adaptive behavior of the framework. 



\subsubsection{\textbf{Analyzer}}

It plays a critical role in assessing the performance of the current model and computing scores that guide model selection in the \textit{Planner}. The \textit{Analyze} component consists of two submodules as shown in \textit{Figure} \ref{fig:architecture}: the \textit{Data Preprocessor} and the \textit{Score Generator}.

The \textit{Data Preprocessor} begins by cleaning and organizing the raw data gathered from \textit{Monitor}. For each frame \textit{i}, metrics such as confidence score (\(C_i\)), and CPU usage (\(U_i\)) are collected and then the aggregation is performed over the window of last \textit{n} frames, to capture overall performance trends, with the average values \(C_\text{avg}, U_\text{avg}\) calculated as:
\[C_\text{avg} = \frac{1}{n} \sum_{i=1}^{n} C_i \quad 
U_\text{avg} = \frac{1}{n} \sum_{i=1}^{n} U_i\]

The aggregated metrics for each model, represented as (\(C_\text{avg}, U_\text{avg}\)), are then structured into a preprocessed dataset, which forms as input for the \textit{Score Generator}.

\textit{Example} Considering the data from the example mentioned in the \textit{Monitor component} and with historical data, the aggregated data for the frame is calculated to be \(U_\text{avg} = 18\%\), and \(C_\text{avg} = 55.94\%\). Now this aggregated data forms as an input to the score generator.

The \textit{Score Generator} evaluates the preprocessed data to compute a performance score (\(S\)) \cite{b5} for each model. This score combines real-time (\(C_i, U_i\)) and historical metrics (\(U_\text{avg}, C_\text{avg}\)) to evaluate the trade-offs between computational efficiency and detection accuracy. The performance score \(S_{m_i}\) for a model \(m_i\) is calculated as:
\[
S_{m_i} = \arg\min(U_i, U_\text{avg}) \times (1 - \frac{C_\text{avg}}{C_i})
\]
The score calculation serves two purposes. One, \textit{efficiency assessment}, minimizing the CPU usage term (\(\min(U_i, U_\text{avg})\)), the system favors models with lower computational demands. Other, \textit{accuracy assessment}, where the confidence ratio (\((1 - \frac{C_\text{avg}}{C_i})\)) highlights models that are maintaining or improving their accuracy relative to historical performance. 

\textit{Example} Assume \( U_\text{avg}, C_\text{avg}\) values from the \textit{data preprocessor example} are the inputs for historical data, and  \(U_i, C_i\) are the inputs for current values. Now using the \(S_{m_i}\) formula, the computed score for the "EfficientDet Lite2" model is \(S_{m_i} = -0.3627\). The negative score reflects slight decline in the confidence, but the low CPU usage reduces the penalty, making it a better score. 

This \textit{Score} is then given as an input to the \textit{Planner} component and also logs this data into the \textit{Score Table} in \textit{Knowledge} component (discussed later in this section). The \textit{Analyze} component transforms raw runtime data into actionable performance score, bridging the gap between monitoring and planning.

\subsubsection{\textbf{Planner}}

This component in our approach serves as the decision-making hub responsible for selecting the optimal model for inference. It uses the performance scores generated in the \textit{Analyze} component and determines which model will be deployed next. By leveraging the \textit{Epsilon-Greedy Strategy}, the \textit{Plan} component balances the need for exploration (testing alternative models) and exploitation (using the best-performing model) to trade-off between adaptability, efficiency, and fairness. 

\noindent
\textbf{Model Selector Engine:}

At the core of the \textit{Plan} component is the \textit{Model Selector Engine} as shown in \textit{Figure} \ref{fig:architecture}, which evaluates all available models stored in the \textit{Score Table}. The engine considers the performance scores for each model, reflecting their performance under the current operational conditions. The decision-making process is guided by the \textit{Epsilon-Greedy Strategy}, ensuring that no model is starved and that system performance is continually optimized. The \textit{Model Selector Engine} operates as follows:

\begin{enumerate}
    \item Retrieves performance scores \(S\) for all models from the Score Table.
    \item Applies Epsilon-Greedy Strategy (discussed later in this component), to select the next model for inference.
    \item Updates the \textit{Executor} component with the selected model and its operational context. 
\end{enumerate}

\noindent
\textbf{Epsilon-Greedy Strategy:}

This is a well-known probabilistic decision-making mechanism, primarily used in adaptive systems, to balance \textit{exploration \& exploitation} effectively \cite{b38}\cite{b39}. Its core idea is to ensure the system does not rely exclusively on the currently best-performing model (exploitation) but also occasionally tests alternative models (exploration) to discover potentially better options as system conditions evolve. In the running example section of traffic detection on edge devices, where workloads, CPU availability, and environmental factors fluctuate, epsilon-greedy enables adaptability by periodically switching models that may perform better under new conditions. This strategy presented in the \textit{Algorithm}\ref{alg:selection}, ensures fairness and prevents overutilization of any single model, balances performance with long-term adaptability, and minimizes overhead, making it ideal for resource-constrained environments. It operates as follows: 

\begin{enumerate}
    \item For each inference cycle, a random number is generated as shown in \textit{Algorithm} \ref{alg:selection} (line 4).
    \item Exploration: With a probability \(\epsilon\), the system selects a model randomly from the available repository, as in \textit{Algorithm} \ref{alg:selection} [lines 5-6]. This exploration step evaluates underutilized models and updates their performance metrics. 
    \item Exploitation: With a probability \(1-\epsilon\), the system selects the model with the lowest performance score from the \textit{Score Table}, as in \textit{Algorithm} \ref{alg:selection} [lines 7-11].
\end{enumerate}

\noindent
\textit{Example} In the running example section of traffic detection, given the performance scores for the available 4 models as \(S_\text{efficientdet-lite0} = -0.25, S_\text{efficientdet-lite1} = -0.30, S_\text{efficientdet-lite2} = -0.36, S_\text{ssd-mobilenet-v1} = -0.28\), the random value \(p\) determines the model selection in \textit{Algorithm} \ref{alg:selection}[line 4]. If \(p = 0.3\), then the algorithm executes exploitation [lines 7-12], and selecting \(m_\text{selected} = \text{"EfficientDet Lite2"}\) in [line 14], as it has the lowest score. Conversely, if \(p = 0.08\), then algorithm executes exploration [lines 5-6], randomly selecting \(m_\text{selected} = \text{"EfficientDet Lite0"}\) in [line 14]. 


The \textit{Plan} component concludes by forwarding the selected model to the \textit{Executor} component for inference. The epsilon-greedy being a light-weight algorithm, allows you to easily select and dynamically adapt to evolving workloads and operational conditions, optimizing performance while maintaining resource efficiency. 

\subsubsection{\textbf{Executor}}

The \textit{Execute} component of the MAPE-K loop is responsible for enacting the decisions made in the \textit{Plan} component by deploying the selected model for inference. This component ensures seamless integration between model selection and real-time object detection, maintaining system responsiveness and adaptability. The \textit{Execute} component performs two key functions: \textit{Model Activation and Execution} and \textit{Feedback Logging}.

Upon receiving the selected model (\(m_\text{selected}\)) from the \textit{Plan} component as in \textit{Algorithm} \ref{alg:selection} [line 14], the \textit{Execute} component activates and loads the corresponding TensorFlow Lite model from the \textit{Model Repository} in \textit{Knowledge}. The selected model processes each incoming frame by performing \textit{Object Detection} and \textit{Post-Processing} as explained in the running example section. 

If the selected model differs from the previously active model (\(m_\text{active} \neq m_\text{selected}\)), the currently active model is deactivated to free resources, and the new model is initialized. This ensures smooth transitions during model switches maintaining system responsivesness and reliability. 

Following each inference, the \textit{Execute} component logs metrics (\(M_\text{data}\)) into the \textit{Knowledge Log Repository}. These logs serve as updated inputs for the Monitor, enabling the system to continuously adapt to changing operational conditions while refining model selection in future cycles by feedback logging. 

\subsubsection{\textbf{Knowledge}}

The \textit{Knowledge} component serves as the central repository, storing critical data required for adaptive decision-making. It supports all the components by maintaining up-to-date records of models, performance scores, operational logs, ensuring informed and efficient system adaptation. It consists of three submodules as shown in \textit{Figure} \ref{fig:architecture}: the \textit{Model Repository}, the \textit{Score Table}, and \textit{Log Registry}. 

The \textit{Model Repository}  houses a variety of preloaded object detection models \textit{M}, where each model \(m_i\)  in \textit{M} represents different configurations, including EfficientDet Lite0, Lite1, Lite2, and SSD MobileNet V1 \cite{b36}\cite{b37}. Each model represents a unique trade-off between computational efficiency and detection accuracy. For instance, SSD MobileNetV1 is generally more suitable for scenarios requiring low latency, and limited computational resources, offering faster inference times with moderate accuracy [9][10]. In contrast, EfficientDet Lite0 provides higher accuracy but demands more computational power, leading to increased CPU usage and energy consumption [9][10].  This enables quick model initialization during execution and supports model comparison during analysis. 

The \textit{Score Table} maintains the performance scores for each model, dynamically updated after every analysis cycle. These scores guide the \textit{Planner} in selecting the optimal model. Additionally, the \textit{Log Registry} records runtime metrics such as CPU usage, battery usage, confidence scores, and number of detections, offering historical data for performance trend analysis. It plays a vital role in enabling efficient model selection, trend analysis, and runtime  updates, making it cornerstone of the self-adaptive framework. 

\textit{Example} In the running example section of traffic detection, all the 4 available models and its configurations are stored in \textit{Model Repository}, enabling the system to quickly load and evaluate models when required. The given performance scores computed in \textit{score generator example} and forwarded to \textit{planner example} are stored and dynamically updated in the \textit{Score Table}, based on which we select the model if it is exploitation in \textit{Planner} component. \textit{Log Registry} records the runtime metrics for historical analysis.

\begin{algorithm}
\begin{small}
\caption{Planner: Algorithm for Model Selection with Epsilon-Greedy}\label{alg:selection}
\begin{algorithmic}[1]
\STATE \textbf{procedure} FORMULATOR(\(m_\text{active}\), \(U_\text{active}\), \(C_\text{active}\)) \(\triangleright\) Input: Current active model (\(m_\text{active}\)), its CPU usage (\(U_\text{active}\)), and its inference confidence (\(C_\text{active}\))
\STATE \textbf{Initialize:}
\STATE \hspace{1em} $M \gets \{ m_1, m_2, \dots, m_n \}$
\STATE \hspace{1em} $p \sim \textit{random}(0, 1)$

\IF{ \(p \leq \epsilon \textbf{ then}) \)\(\triangleright\) Exploration: Randomly select model excluding the best model }
    \STATE \(M_{\text{next}} \gets random(M)\)
\ELSE 
    \STATE \(\triangleright\) Exploitation: Select the best model based on Score
    \FOR{each \(m_{\text{i}} \in M\)}
        \STATE Get Calculated Score from Analyzer component
        \STATE \(M_{\text{next}} \gets \arg\min(S)\)
    \ENDFOR
\ENDIF

\STATE Return selected model: \(m_\text{selected}\)
\end{algorithmic}
\end{small}
\end{algorithm}
