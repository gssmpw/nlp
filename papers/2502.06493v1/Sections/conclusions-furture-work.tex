\section{Conclusions and Future Work}

 By balancing model-switching using computational efficiency, accuracy, and resource fairness, the approach addresses key challenges in edge AI systems, including adaptability and sustainability. Our findings highlight significant improvements in model usage fairness, computational efficiency, and detection accuracy compared to other approaches. 
%Moreover, integration of ML-balanced dynamic model-switching mechanisms reduces resource starvation while maintaining robust system performance under varying runtime conditions. 
Further, the results validate the potential of adaptive strategies to optimize edge-AI systems for compute-conscious, high-performance applications. We believe that our study shows promising results and forms the basis for future exploration to switch between models on edge devices. However, further studies are needed, which will be part of our future work.

Building upon this study, future work includes but is not limited to: (i) integrating Large Language Models (LLMs) to complement vision-based tasks (already in the pipeline), (ii) extending it to other Android devices and other edge devices like raspberry pi 4, Nvidia Jetson nano etc., and (iii) then extending the approach to hybrid edge-cloud architectures would enable optimized workload distribution between edge devices and cloud infrastructure, balancing real-time constraints with computational efficiency. 

