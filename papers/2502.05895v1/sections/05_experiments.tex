\section{Experiments} \label{sec:experiments}

% \textbf{Dataset} For evaluation, we use the Dreambooth~\citep{DB} dataset. It contains $30$ concepts of different categories, including pets, interior decoration, toys, backpacks, etc. For each concept, we used $25$ contextual text prompts, which include accessorisation, appearance, and background modification. For each concept, we generate $10$ images per prompt. In total, there are $750$ unique concept-prompt pairs and a total of $7500$ images for robust evaluation. Additionally, in Appendix~\ref{app:add_example}, we compare methods in challenging contexts using 10 long and complicated prompts.

\textbf{Dataset} We use the Dreambooth~\citep{DB} dataset for evaluation, which contains $30$ concepts from various categories such as pets, interior decoration, toys, and backpacks. To ensure consistency across models, we standardized the data preprocessing procedure (see Appendix~\ref{app:data}). For each concept, we used $25$ contextual text prompts, which include accessorization, appearance, and background modification. For each concept, we generated $10$ images per prompt. In total, there are $750$ unique concept-prompt pairs and a total of $7500$ images for robust evaluation. Additionally, in Appendix~\ref{app:long_prompts}, we compared methods in challenging contexts using 10 long and complicated prompts.

\textbf{Evaluation Metrics}
To estimate the concept's identity preservation, we use CLIP Image Similarity (IS) between real and generated images as in ~\citep{DB}. Higher values for this metric typically indicate better subject fidelity. We also provide DINO for concept fidelity in Appendix~\ref{app:add_dino}. However, it is important to note that as the generated images align more with the contextual prompt, they tend to resemble the original images less. So, even if the identity of the concept is preserved perfectly, the metric will be lower. To evaluate the alignment between generated images and contextual prompts we calculate the CLIP Text Similarity (TS) of the prompt and generated images~\citep{TI}.

\textbf{Selecting base fine-tuning model} At the initial stage, it is essential to choose a foundational fine-tuning model to facilitate the comparison of different sampling methods. To this end, we train five distinct models for each concept, implementing diverse fine-tuning parameterizations~\citep{DB, svdiff}, optimizing text embeddings~\citep{TI, CD}, and leveraging a pre-trained hypernetwork~\citep{elite}. A comprehensive description of the model training and inference procedures can be found in Appendix~\ref{sec:training-details}. We primarily focus on the Stable Diffusion-2-base model in our experiments, but we also include results from other backbones (SD-XL, PixArt-alpha) to further validate our findings in Appendix~\ref{app:add_backbones}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[trim={0 4cm 0 4cm},clip,width=\linewidth]{imgs/examples_new.pdf}
  \vspace{-0.20in}
  \caption{Examples of the generation outputs for different sampling methods.}
  \label{fig:examples}
  \vspace{-0.21in}
\end{figure*}
For each model, we conduct a complete evaluation of the Mixed sampling by varying the parameter \(\omega_s\) within the range of 0 to 7.0, while deriving \(\omega_c\) as \(7.0 - \omega_s\). Figure~\ref{fig:all_mixed} illustrates the Mixed sampling Pareto frontiers for all aforementioned methods. The method shows the expected behaviour as the superclass guidance scale increases the text similarity improves as well, but the more diverse generation we get, the more we lose on the image similarity. Notably, the results indicate that the Mixed sampling method significantly enhances text similarity across all models. Furthermore, for each model, it is feasible to select a value for \(\omega_s\) such that image similarity remains relatively unchanged, while text similarity is markedly improved.

The Pareto frontier obtained from the SVDiff model achieves a favorable balance between text and image similarity; therefore, this model was chosen for subsequent evaluations of various sampling methods. We complement our analysis with results for different samplings on top of Dreambooth in Appendix~\ref{app:dreambooth}.

\textbf{Computational efficiency of sampling methods}
% Switching sampling maintains the same number of U-Net calls and batch size as typical inference with the concept. In contrast, Mixed, Multi-stage, Masked, and Photoswap sampling require a batch size that is twice as large. Lastly, ProFusion necessitates the same batch size as Mixed sampling but performs twice as many U-Net inferences compared to all other sampling methods.
% Switching sampling keeps the same number of U-Net calls and batch size as Base sampling. Conversely, Mixed, Multi-stage, Masked, and Photoswap sampling double the required batch size. Lastly, ProFusion necessitates the same batch size as Mixed sampling but performs twice as many U-Net inferences compared to all other sampling methods.
Switching sampling requires the same number of U-Net calls as Base sampling. Mixed, Multi-stage, Masked, and Photoswap methods double the computational cost by performing two U-Net inferences per step to integrate concept and superclass trajectories. ProFusion further quadruples the number of U-Net inferences per step compared to Base sampling.

\textbf{Proposed sampling techniques analysis}
% In Figure~\ref{fig:multi-stage}, the Pareto frontiers for Mixed and Switching sampling are illustrated. For the Switching sampling, the curve is obtained by varying the switching step $t_{sw} = [1, 3, 5, 7, 10, 20, 30, 40]$. We observe that the Switching sampling curves lie below the Mixed sampling curve and exhibit lower values of image similarity. This indicates that Switching sampling impacts concept identity more negatively.
In Figure~\ref{fig:multi-stage}, the Pareto frontiers for Mixed and Switching sampling are shown. For the Switching sampling, the curve is generated by varying the switching step $t_{sw} = [1, 3, 5, 7, 10, 20, 30, 40]$. We observe that Switching sampling curves lie below the Mixed sampling curve and have lower values of image similarity. This indicates that Switching sampling more negatively affects concept identity.

In addition, we evaluated Multi-stage sampling with various hyperparameters. In Figure~\ref{fig:multi-stage}, each Multi-stage sampling curve is generated by fixing the switching step while varying the superclass guidance scale $\omega_s = [1.0, 3.0, 5.0]$. The graphs show that the curves for Multi-stage sampling fall between the Mixed and Switching Pareto Frontiers. Only curves with high values of the switching step cross the Pareto frontier of Mixed sampling; however, these points correspond to very low image similarity values, thereby compromising concept identity.

Figure~\ref{fig:masked} presents the Pareto frontier for different Masked sampling hyperparameters. For all curves, we fixed the following hyperparameters: \( t_{sw} = 3, \omega_s = 3.5, \omega_c = 3.5 \), as these parameters correspond to the optimal point for Multi-stage and Mixed samplings. Each curve for Masked sampling is derived by varying the quantile \( q = [ 0.3, 0.5, 0.7, 0.9 ] \), which controls the mask binarization threshold. Mask sampling curve falls between the Mixed and Switching Pareto Frontiers and does not facilitate an increase in concept fidelity while maintaining a high quality of alignment with the text. This limitation can be explained by the noisiness of the cross-attention masks, especially during the early stages of generation (see Appendix~\ref{app:cross_attn}), which hampers the precise distinction of the concept. Furthermore, this method is overparameterized, making its practical application challenging.


\begin{figure*}[!ht]
\centering
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/profusion_photoswap.pdf}
  \vspace{-0.20in}
  \captionof{figure}{Pareto frontiers curves for Photoswap~\citep{photoswap} and ProFusion~\citep{profusion}.}
  \label{fig:profusion-photoswap}
\end{minipage}
\hfill
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/all_methods.pdf}
  \vspace{-0.20in}
  \captionof{figure}{The overall results of different sampling methods against main personalized generation baselines.}
  \label{fig:all-methods}
\end{minipage}
  \vspace{-0.16in}
\end{figure*} 

\textbf{Comparison with existing sampling methods}
To fairly compare our results with Photoswap~\citep{photoswap} and ProFusion~\citep{profusion}, which were initially proposed alongside fixed fine-tuning methods, we reimplemented both approaches using the same fixed SVDiff models to eliminate any influence from differing training methods.

We will first discuss the Photoswap method. In Figure~\ref{fig:profusion-photoswap}, the Pareto front for this method is illustrated. This curve was obtained by varying three hyperparameters: ($t_{SF}, t_{CM}, t_{SM}$) = [(1, 10, 15), (5, 15, 20), (10, 20, 25)], with the last combination representing the optimal values proposed in the original work~\citep{photoswap}. 

% As shown in Figure~\ref{fig:profusion-photoswap}, the curve for this method is nearly indistinguishable from that of Switching sampling. This leads us to conclude that altering the self and cross-attention maps across all layers of the U-Net affects generation almost equally as using the entire noise prediction from the superclass trajectory.
As shown in Figure~\ref{fig:profusion-photoswap}, this method's curve is nearly identical to that of Switching sampling. This leads us to conclude that altering the self and cross-attention maps across all layers of the U-Net affects generation almost equally as using the entire noise prediction from the superclass trajectory.

Additionally, the ProFusion Pareto frontiers are shown in Figure~\ref{fig:profusion-photoswap}. Since Mixed sampling is part of the ProFusion, we evaluated it in the same way by fixing all parameters and varying $\omega_s$. We assessed this method using two levels of fusion step intensity $r$ and constructed a distinct curve with fixed $\omega_s=3.5$ and various $r = [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]$. As observed, with decreasing fusion step intensity $r$, the curve converges more closely to the Mixed sampling curve. However, when the fusion step intensity is high, this method significantly improves concept preservation and results in image similarity even higher than Base sampling.

\begin{figure*}[t!]
  \centering
  \includegraphics[trim={0 4cm 0 4cm},clip,width=\linewidth]{imgs/main_examples_new.pdf}
  % \vspace{-0.24in}
  \caption{Examples of generation results for Mixed and ProFusion sampling methods compared to the main personalized generation baselines.}
  \label{fig:main_examples}
\end{figure*}

\textbf{User study}
In addition to CLIP metrics, we also conducted a human evaluation. For each sampling method, we took the optimal point in terms of CLIP metric and visual generation assessment and generated \num{16000} pairs comparing different sampling techniques and base personalization methods (Dreambooth (DB), Custom Diffusion (CD), Textual Inversion (TI) and ELITE) with Mixed Sampling as a strong and effective baseline. See Appendix~\ref{app:us} for more details.

Given an original image of the concept, a text prompt, and $2$ generated images (Mixed versus the competitor's), we asked users to answer the following questions: 1) "Which image is more consistent with the text prompt?" to evaluate text similarity 2) "Which image better represents the original image?" for image similarity 3) "Which image is generally better in terms of alignment with the prompt and concept identity preservation?" to evaluate the general impression. We provide an example of a comparison in Appendix~\ref{app:us}.

\begin{table*}[ht!]
\centering
\caption{User study results of the pairwise comparison of SVDDiff with Mixed sampling method versus other baselines.
The values in the table show the win rate. "TS" stands for text similarity, "IS" stands for image similarity, and "All" stands for overall impression. *Sampling method is applied on top of the SVDDiff fine-tuning method.}
	\label{table:user_study}
\resizebox{\textwidth}{!}{

\begin{tabular}{lccccccccccccccc}
\toprule
 & Base* & Switching* & Multi-stage* & Masked* & Photoswap* & ProFusion*  & DB & TI & ELITE & CD \\
\midrule
TS & $0.52$ & $0.51$ & $0.51$ & $0.51$ & $0.53$ & $0.49$ & $0.74$ & $0.67$ & $0.64$ & $0.51$ \\
%IS & $\textbf{0.763}$ & $0.693$ & $0.716$ & $0.733$ & $0.714$ & $0.718$  \\
IS & $0.37$ & $0.47$ & $0.50$ & $0.59$ & $0.70$ & $0.35$ & $0.40$ & $0.74$ & $0.73$ & $0.53$ \\
All & $0.41$ & $0.48$ & $0.50$ & $0.59$ & $0.69$ & $0.37$ & $0.59$ & $0.77$ & $0.75$ & $0.53$ \\
\bottomrule
\end{tabular}
}
  % \vspace{-0.14in}
\end{table*}

Combining the user study results (Table~\ref{table:user_study}) and the insights from Figure~\ref{fig:all-methods}, which illustrates the improvements of the examined techniques against the main personalized generation baselines, we find that all sampling methods improve the performance of the fine-tuned model in either concept or context preservation.

% \textbf{A framework for selecting sampling method}
% Standard sampling is effective but sometimes fails to align well with the text prompt. Fortunately, there are alternative sampling methods that can enhance text similarity. 

% The simplest and most cost-effective option is Switching Sampling. This method increases text similarity without adding to the computational load. However, it can compromise the preservation of concepts. 

% Mixed Sampling addresses this issue more effectively and generally provides stable results while maintaining both concept and context (see Figure~\ref{fig:examples}). The trade-off is that it requires double the batch size compared to Switching Sampling. 

% Another viable option is Masked Sampling, which can yield favorable outcomes in situations where Mixing and Switching struggle to balance context and concept. However, it demands careful tuning of hyperparameters and may produce inconsistent results because of the cross-attenttion masks noisiness.

% Finally, ProFusion not only enhances text similarity but also preserves a high level of concept preservation (see Figure~\ref{fig:main_examples}), as indicated by user feedback. The downside is that it requires twice the U-Net inference compared to Mixed Sampling and require careful selection of a big number of hyperparameters.

\input{sections/05_experiments_framework_new}
