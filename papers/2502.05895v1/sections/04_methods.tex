\section{Methods}
\label{sec:method}
Given a model $\varepsilon_{\theta}$, fine-tuned by (\ref{eq:finetuning}) for a specific concept, we can identify two distinct sampling approaches, each maximizing one of the objectives: concept fidelity or editability:

Sampling with concept (Base sampling):
\begin{equation} 
  \label{eq:concept_sampling}
  \tilde{\varepsilon}_{\theta}(p^C) = \varepsilon_{\theta} + \omega(\varepsilon_{\theta}(p^C) - \varepsilon_{\theta}) = \varepsilon_{\theta} + \omega \Delta\varepsilon_{\theta}^{C}
\end{equation}
Sampling with superclass:
\begin{equation}
  \label{eq:superclass_sampling}
  \tilde{\varepsilon}_{\theta}(p^S) = \varepsilon_{\theta} + \omega( \varepsilon_{\theta}(p^S) - \varepsilon_{\theta}) = \varepsilon_{\theta} + \omega \Delta\varepsilon_{\theta}^{S}
\end{equation} 
% Here, $p^C$ represents a concept prompt embedding (for example, \textit{"a V* with a city in the background"}) and $p^S$ indicates a superclass prompt embedding (\textit{"a backpack with a city in the background"}) where the concept token $V^*$ is replaced by a superclass token (\textit{"backpack"}).
Here, $p^C$ represents a concept prompt embedding (for example, \textit{"a V* with a city in the background"}), and $p^S$ indicates a superclass prompt embedding (\textit{"a backpack with a city in the background"}) where the concept token $V^*$ has been replaced by a superclass token (\textit{"backpack"}).

\begin{figure*}[ht!]
  \centering
  \vspace{-0.02in}
  \includegraphics[trim={0 7cm 0 7cm},clip,width=\linewidth]{imgs/sampling_new.pdf}
  \vspace{-0.20in}
  \caption{\textbf{Visualization of Different Sampling Strategies.} (a) Usual sampling with concept reproduces the concept but does not align closely with the text prompt. (b) Generation with superclass effectively captures the context obtained from the prompt but produces a random superclass representative (e.g., dog). (c-d) Mixed and Switching sampling strategies improve context preservation while maintaining the concept's identity.}
  \label{fig:sampling}
  \vspace{-0.20in}
\end{figure*}

The extended fine-tuning of the model \(\varepsilon_{\theta}\) enhances its ability to accurately reproduce the concept generated via~(\ref{eq:concept_sampling}). However, this improvement comes at the cost of overlooking the contextual information supplied by the prompt $P^C$ (see Figure~\ref{fig:sampling}a). Conversely, the generation via~(\ref{eq:superclass_sampling}) ensures the highest alignment with the text prompt, though at the expense of preserving the concept's identity (see Figure~\ref{fig:sampling}b).

% This raises the question of whether we can integrate the two sampling strategies~(\ref{eq:concept_sampling}) and~(\ref{eq:superclass_sampling}) to obtain the optimal balance between the high fidelity of the learned concept identity and its adaptability to various contexts.

This consideration raises the question of whether we can integrate the two sampling strategies~(\ref{eq:concept_sampling}) and~(\ref{eq:superclass_sampling}) to obtain the optimal balance between the high fidelity of the learned concept identity and its adaptability to various contexts.

\subsection{Mixed sampling} \label{sec:mixed_sampling}
One reasonable approach for incorporating superclass into the generation process~\citep{profusion} is to modify the sampling strategy by adding guidance to the superclass prompt (see Figure~\ref{fig:sampling}c):
\begin{align}\label{eq:mixed_sampling}
    \tilde{\varepsilon}^{MX}_{\theta}(p^S, p^C) = \varepsilon_{\theta} + \omega_s \Delta\varepsilon_{\theta}^{S} + \omega_c\Delta\varepsilon_{\theta}^{C}
\end{align}
% By adjusting the ratio between the concept guidance scale $\omega_c$ and the superclass guidance scale $\omega_s$, we can either amplify or diminish the influence of the concept or superclass, thus varying the trade-off between concept and context fidelity. In Figure~\ref{fig:visual}, you can observe how the generated output alters with increasing superclass influence. For instance, in the teapot example, as we raise the superclass guidance scale, the context, which was initially poorly represented through sampling with the concept, gradually becomes more accurate. However, excessive superclass influence may result in a loss of concept identity preservation, as illustrated in the dog example.

Adjusting the ratio between the concept guidance scale $\omega_c$ and the superclass guidance scale $\omega_s$ amplifies or diminishes the influence of the concept or superclass, varying the trade-off between concept and context fidelity. Figure~\ref{fig:visual} shows how the generated output changes with increasing superclass influence. For instance, in the teapot example, as we raise the superclass guidance scale, the context, which was initially poorly represented through sampling with the concept, gradually becomes more accurate. However, excessive superclass influence may reduce concept identity preservation, as shown in the dog example.

\subsection{Switching sampling} \label{sec:switching_sampling}
Another solution for how to combine the superclass sampling trajectory with the concept sampling trajectory is to condition several steps on the superclass prompt embedding $p^S$, then at the \textit{switching step} $t_{sw}$ switch to the concept prompt embedding $p^C$  (see Figure~\ref{fig:sampling}d). In this case~(\ref{eq:sampling}) will be rewritten in the following form:
\begin{align}\label{eq:switching_sampling}
    \tilde{\varepsilon}^{SW}_{\theta}(p^S, p^C, t_{sw}) = \varepsilon_{\theta} +
    \begin{cases}
         \omega\Delta\varepsilon_{\theta}^{S}, & t > T - t_{sw}\\
        \omega\Delta\varepsilon_{\theta}^{C}, &\text{otherwise}
    \end{cases} 
\end{align}
By increasing the \textit{switching step} $t_{sw}$, we can amplify the influence of the superclass and thus improve context preservation.
Up to 10 steps can effectively recover context that has been poorly generated through Base sampling, as demonstrated in the teapot example in Figure~\ref{fig:visual}. Nonetheless, this strategy may result in notable degradation of the concept's identity. The effect of the superclass can be so intense that the concept loses its original attributes and takes on excessive characteristics from the superclass, as evidenced by the dog example in Figure~\ref{fig:visual}.

% This sampling procedure is similar to Photoswap~\cite{photoswap} approach adapted to the personalization task. The main difference is that in switched sampling we take the noise predictions entirely from the superclass trajectory for the first $t_{sw}$ steps, whereas in Photoswap only the self- and cross-attention maps and features are taken from the superclass for the first $t_{sw}$ steps. However, as we show in Section~\ref{sec:experiments}, the results of these two methods are almost indistinguishable.

This sampling procedure is similar to Photoswap~\cite{photoswap} but adapted to the personalization task. The main difference is that switched sampling takes noise predictions entirely from the superclass trajectory for the first $t_{sw}$ steps, whereas Photoswap uses only self- and cross-attention maps, and features are taken from the superclass for the first $t_{sw}$ steps. However, as we show in Section~\ref{sec:experiments}, the results of both methods are almost indistinguishable.

The aforementioned methods can be flexibly combined, we refer to this type of sampling as \textit{multi-stage sampling}:
\begin{align}\label{eq:multistage_sampling}
\tilde{\varepsilon}^{MS}_{\theta}(p^S, p^C) = \varepsilon_{\theta} +
    \begin{cases}
         (\omega_s + \omega_c)\Delta\varepsilon_{\theta}^{S} &t > T - t_{sw}\\[-0pt]
         \omega_s \Delta\varepsilon_{\theta}^{S} + \omega_c\Delta\varepsilon_{\theta}^{C}&\text{otherwise}
    \end{cases}
\end{align}
This combination enables a greater influence of the superclass on the generated output and enhances alignment with the text prompt. However, it is important to consider that as the influence of the superclass increases, the more the concept's identity is lost.

\subsection{Masked sampling}

\begin{figure*}[t!]
  \centering
  \includegraphics[trim={0 6.7cm 0 6.7cm},clip,width=\linewidth]{imgs/visual_new_v2.pdf}
  \vspace{-0.19in}
  \caption{\textbf{Effects of Superclass Influence on Different Sampling Methods.} 
  % For Mixed Sampling, the influence is adjusted by varying the superclass guidance scale $\omega_s = [1.0, 3.5, 5.0]$ with $\omega_c = 7.0 - \omega_s$. For Switching Sampling, we vary the switching step $t_{sw} = [3, 7, 20]$ . For Masked Sampling, the mask is modified by altering the thresholding quantile $q = [0.3, 0.5, 0.9]$. 
  For Mixed Sampling, the influence is adjusted by varying the superclass guidance scale $\omega_s$ with $\omega_c = 7.0 - \omega_s$. For Switching Sampling, we vary the switching step $t_{sw}$ . For Masked Sampling, the mask is modified by altering the concept mask thresholding quantile $q$.
  }
  \label{fig:visual}
  \vspace{-0.19in}
\end{figure*}

Sampling with a superclass prompt hinders the preservation of concept identity, whereas sampling with a concept prompt disrupts contextual adaptation. To address this challenge, restricting the image regions impacted by each sampling approach could be beneficial. This can be effectively achieved through masking.

Suppose at each diffusion step we could obtain a concept mask $M_t$, then we can use it in the Mixed sampling. Specifically, we apply this mask to the concept trajectory, ensuring it only influences relevant regions:
\begin{align}\label{eq:masked_base}
    \varepsilon^{M}_{\theta}(p^S, p^C) = \varepsilon_{\theta} + \omega\Delta\varepsilon_{\theta}^{C} \odot M_t + \omega\Delta\varepsilon_{\theta}^{S} \odot \overline{M_t} 
\end{align}
Moreover, to enhance the alignment between regions inside and outside the mask, and to gently amplify the influence of the superclass within the mask -- especially in cases where prompts alter the object's appearance (like color or outfit) -- we can apply Mixed sampling within the mask:
\begin{align}\label{eq:masked_mixed}
    &\varepsilon^{M}_{\theta}(p^S, p^C) = \varepsilon_{\theta} + \\ &+ \omega_c\Delta\varepsilon_{\theta}^{C} \odot M_t + 
    \omega_s\Delta\varepsilon_{\theta}^{S} \odot M_t +(\omega_c + \omega_s)\Delta\varepsilon_{\theta}^{S} \odot \overline{M_t}  \notag
\end{align}
The generation process starts with Mixed sampling for a limited number of steps, thereby enhancing the robustness of mask generation. Then, we apply masked sampling as described in (\ref{eq:masked_mixed}), using the concept mask $M_t(q)$. This mask is derived by averaging the cross-attention maps associated with the concept identifier token across all U-Net layers and binarizing it using a threshold determined by the quantile $q$:
\begin{equation}\label{eq:masked_sampling}
\tilde{\varepsilon}^{M}_{\theta}(p^S, p^C) = 
    \begin{cases}
         \tilde{\varepsilon}^{MX}_{\theta}(p^S, p^C, \omega_c^0, \omega_s^0), & t > T - t_{sw}\\
         \varepsilon^{M}_{\theta}(p^S, p^C, \omega_c, \omega_s, q),
    &\text{otherwise,}
    \end{cases} 
\end{equation}
where $\varepsilon^{M}_{\theta}(p^S, p^C, \omega_c, \omega_s, q)$ is computed as in (\ref{eq:masked_mixed}). 

Equation~\ref{eq:masked_sampling} summarizes the complete Masked sampling algorithm. Increasing the quantile $q$ reduces the area influenced by the concept, thereby expanding the region impacted by the superclass (see Appendix~\ref{app:cross_attn}) and enhancing the influence of the context, as illustrated in Figure~\ref{fig:visual}.

\subsection{Other approaches}
\textbf{ProFusion} The main contribution of the Profusion~\citep{profusion} sampling method is a novel technique to ensure the concept's preservation combined with Mixed Sampling. A sampling step in this approach consists of the following stages: (1) we predict $x_t \rightarrow \tilde{x}_{t-1}$ through the usual diffusion backward sampling process with concept (2) after that we make a forward diffusion step $\tilde{x}_{t-1}\rightarrow \tilde{x_t}$ (3) finally, we again make a backward step with the Mixed sampling  $\tilde{x_{t}} \rightarrow x_{t-1}$. The first two steps define Fusion Step and have a hyperparameter $r$ that controls its intensity (e.g. the influence on the result). In case $r=0$ we get Mixed sampling.

\textbf{Photoswap} In this method, the author proposes to replace self-attention features, cross-attention maps, and self-attention maps in the concept trajectory with maps from the superclass at several initial steps. Thus, the method has three hyperparameters: (1) $t_{SF}$ the number of initial steps during which the self-attention features are replaced, (2) $t_{CM}$ the same parameter for cross-attention maps, and (3) $t_{SM}$ for self-attention maps.

\subsection{Evaluation protocol for sampling techniques}
The study of sampling methods involves several key steps. 

% The first step is to select a fundamental fine-tune model on the basis of which we can compare different sampling techniques. For each model, we propose constructing a complete Pareto front of the Mixed sampling. We chose Mixed sampling as our baseline because it is the simplest efficient method, characterized by a single hyperparameter.

The first step is to select a fundamental fine-tuned model that will be used as a baseline for comparing different sampling techniques. For each model, we propose to construct the full Pareto front of Mixed sampling, which we selected as our baseline because it is the simplest yet efficient method, defined by a single hyperparameter.

% It is essential to select a model whose Pareto frontier exhibits a sufficiently large length; this allows for a clearer distinction between the varying parameters. Additionally, this front should lie within the optimal balance between concept fidelity and editability comparing to other fine-tuning methods. By doing so, we can examine sampling not only in scenarios where the model performs poorly but also ensure that sampling does not undermine performance in cases where the model excels.

It is crucial to select a model whose Pareto frontier is of sufficient length, enabling a clearer distinction between the varying parameters. Additionally, this frontier should lie within the optimal balance between concept fidelity and editability compared to other fine-tuning methods. This ensures that we can study sampling in scenarios where the model performs poorly while also confirming that it does not degrade performance when the model excels.

\begin{figure*}[ht!]
\centering
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/multi-stage.pdf}
  \vspace{-0.19in}
  \captionof{figure}{Pareto Frontier curves for Mixed, Switching and Multi-stage Sampling methods. Each Multi-stage sampling curve is generated by fixing the switching step while varying the superclass guidance scale $\omega_s = [1.0, 3.0, 5.0]$.}
  \label{fig:multi-stage}
\end{minipage}%
\hfill
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/masked.pdf}
  \vspace{-0.19in}
  \captionof{figure}{Pareto frontiers curves for  Masked sampling. Each Masked sampling curve is derived by varying the quantile \( q = [ 0.3, 0.5, 0.7, 0.9 ] \), which controls the mask binarization threshold; \( t_{sw} = 3, \omega_s = 3.5\) are fixed.}
  \label{fig:masked}
\end{minipage}
\vspace{-0.19in}
\end{figure*}

\begin{figure}[h!]
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/all_mixed.pdf}
  \vspace{-0.23in}
    \caption{Mixed sampling Pareto frontiers for different fine-tuning methods.}
    \label{fig:all_mixed}
    \vspace{-0.24in}
\end{figure}
Once the base model is chosen, we fix it and proceed to compare different sampling techniques. For each method, we demonstrate its behaviour at different hyperparameter values. We illustrate the optimal points with generation examples and prove our findings with a user study.

It is important to note that the choice of sampling that maximizes editability can be approached in different ways. For example, one option is to use the model weights before fine-tuning, $\theta^{\text{orig}}$, in (\ref{eq:superclass_sampling}) instead of the fine-tuned weights, $\theta$. Additionally, we can vary the superclass prompts. One extreme option is to remove the superclass token entirely, allowing the model to focus solely on the scene's context (e.g., $p^{\hat{S}} = \textit{"with a city in the background"}$). These hyperparameters affect all sampling methods simultaneously. We analyze this dependency in Appendix~\ref{app:hyper_theta}.