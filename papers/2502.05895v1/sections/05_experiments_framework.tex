\textbf{A framework for selecting sampling method}
In this section, we provide an overall analysis of the performance of different sampling methods in terms of concept fidelity, alignment with a text prompt, and computational efficiency. In our conclusions, we rely mainly on the results of the user study, as current studies show that the CLIP metrics do not always match human perception. In case the user study doesn't reveal the difference between the performance of different methods, we draw conclusions based on the metrics and visual examples.

According to the Figure~\ref{fig:examples} Base sampling sometimes fails to align well with the text prompt. Fortunately, there are alternative sampling methods that can enhance text similarity. 

% здесь проблема что у нас и base comparable, при этом ему мы очень значительно по IS проигрываем, выходит что мы ничего не улучшаем? По картинкам улучшаем поэтому ссылаюсь на них

% As the user study and CLIP-metrics show Mixed, Switching, Multi-stage, and Masked sampling show comparable performance in terms of text similarity. The simplest and most cost-effective option is Switching sampling. This method increases text similarity without adding to the computational load. However, sometimes it can compromise the preservation of concepts. 
As shown by the user study and CLIP metrics, Mixed, Switching, Multi-stage, and Masked sampling achieve comparable performance in terms of TS. Among these, Switching sampling is the simplest and most cost-effective option. It improves TS without increasing computational load but may occasionally compromise concept preservation.

% Mixed Sampling addresses this issue more effectively and generally provides stable results while maintaining both concept and context (see Figure~\ref{fig:examples}). The trade-off is that it requires double the batch size compared to Switching Sampling. 
Mixed Sampling addresses this issue more effectively, generally providing stable results while preserving both concept and context (Figures~\ref{fig:examples},~\ref{fig:add_ex}). The trade-off is that it requires double the batch size compared to Switching Sampling. 

Another viable option is Masked Sampling, which can yield better concept fidelity outcomes in situations where Mixing and Switching struggle to balance context and concept. However, it demands careful tuning of hyperparameters and may produce inconsistent results because of cross-attention mask noisiness.

% Finally, ProFusion not only enhances text similarity but also preserves a high level of concept preservation (see Figure~\ref{fig:main_examples}), as indicated by user feedback. The downside is that it requires twice the U-Net inference compared to Mixed Sampling and require careful selection of many hyperparameters.
Finally, ProFusion enhances text similarity while maintaining a high level of concept preservation (Figures~\ref{fig:main_examples},~\ref{fig:add_ex_all}), as indicated by user feedback. The downside is that it requires twice the U-Net inference compared to Mixed Sampling and requires careful selection of many hyperparameters.
