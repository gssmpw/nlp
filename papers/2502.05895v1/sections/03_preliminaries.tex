\section{Preliminaries}
\textbf{Stable Diffusion Model} As a base model in this work, we use Stable Diffusion~\citep{stablediffusion}, one of the most widely used diffusion models in research. Stable Diffusion is a large text-to-image model trained on pairs $(x, P)$, where $x$ is an image and $P$ is a text prompt describing it. Stable Diffusion includes the CLIP~\citep{clip} text encoder $E_T$, which is used to obtain the text conditional embedding $p = E_T(P)$, the encoder $E$, which transforms the input image into the latent space $z = E(x)$, the decoder $D$, which reconstructs the input image from the latent $x \approx D(z)$, and a U-Net-based~\citep{unet} conditional diffusion model $\varepsilon_{\theta}$. The denoising process is performed in the latent space. With a randomly sampled noise $\varepsilon \sim N(0,I)$, the time step $t$, and the coefficients controlling the noise schedule we obtain a noisy latent code: $z_t = \alpha_t z + \sigma_t \varepsilon$. The goal of U-Net $\varepsilon_{\theta}$ is to predict the noise from the noisy latent. We assume that $\varepsilon_{\theta}$ depends implicitly on $z_{t}$, although we will omit the dependency in our notation.
\begin{equation}\label{eq:training}
\min_\theta \mathbb{E}_{p, z, \varepsilon, t}\left[\left\|\varepsilon-\varepsilon_\theta(p)\right\|_2^2\right]
\end{equation}
During inference, a random noise $z_T \sim N(0,I)$ is denoised step by step to $z_0$, using DDIM sampling~\cite{ddim}:
$z_{t-1}=\text{DDIM}(t, z_t, \varepsilon_\theta(p)), \; t = T, \dots, 1$. The resulting image is obtained through the decoder as $D(z_0)$. 

\textbf{Classifier-free guidance} A commonly used technique to improve generation quality of conditional diffusion models post-training is classifier-free sampling~\citep{classfree}. Given the current noisy sample $z_t$ and condition $p$, the diffusion model outputs the predictions of the conditional noise $\varepsilon_{\theta}(E_T(p))$ and unconditional noise $\varepsilon_{\theta}$ (conditioned on null text). Then, an updated prediction
\begin{equation}\label{eq:sampling}
\tilde{\varepsilon}_{\theta}(p) = \varepsilon_{\theta} + \omega( \varepsilon_{\theta}(p) - \varepsilon_{\theta}) = \varepsilon_{\theta} + \omega \Delta \varepsilon_{\theta}^{p}
\end{equation}
will be used to sample $z_{t-1}$, where $\omega$ is a guidance scale. $\Delta\varepsilon_{\theta}^{p}$ denotes the classifier-free guidance noise difference.


\textbf{Finetuning for Personalized Text-to-Image Generation} Let $\mathbb{C} = \{ x\}_{i=1}^N$ be a small image set of images with a specific concept. A special text token $V^*$ can be bound to it, using the following fine-tuning objective:
\begin{equation}\label{eq:finetuning}
\min_\theta \mathbb{E}_{z=\mathcal{E}(x), x \in \mathbb{C}, \varepsilon, t}\left[\left\|\varepsilon-\varepsilon_\theta(p^{C})\right\|_2^2\right]
\end{equation}
where $p^C = E_T(P^C)$ is a text embedding of the prompt $P^C = $\textit{"a photo of a V*"}.