\textbf{A framework for selecting sampling method}
In this section, we analyze the performance of different sampling methods in terms of concept fidelity, alignment with a text prompt, and computational efficiency. Our conclusions are based primarily on the user study results, as research suggests that CLIP metrics do not always align with human perception. When the user study does not reveal significant differences between methods, we rely on quantitative metrics and visual examples to guide our recommendations.

\textbf{Step 1: Assess Text Alignment Needs}
If Base sampling fails to align adequately with the text prompt — as illustrated in Figure~\ref{fig:examples} — consider alternative methods that improve text similarity. Mixed, Switching, Multi-stage, and Masked sampling all achieve comparable text similarity, as demonstrated by both user study results and CLIP metrics.

\textbf{Step 2: Evaluate Computational Efficiency} Switching sampling is the simplest and most cost-effective option, improving TS without increasing computational load, though it may compromise concept preservation. Mixed sampling, requiring twice as many U-Net inferences of Base or Switching, offers more stable results with better concept and context preservation (see Figures~\ref{fig:examples},~\ref{fig:add_ex}).

\textbf{Step 3: Prioritize Concept Fidelity} Switching, Mixed, and Masked samplings may struggle to achieve better concept fidelity. Meanwhile, ProFusion excels in both text similarity and concept preservation, as supported by user feedback (Figures~\ref{fig:main_examples} and~\ref{fig:add_ex_all}). The trade-off is computational cost: ProFusion requires four times more U-Net inference than Base sampling and requires careful hyperparameter selection.

\textbf{Step 4: Make an Informed Decision}

\begin{itemize}
    \item \textbf{Switching sampling} matches the computational load of Base sampling, improving text similarity at the expense of concept fidelity.
    \item \textbf{Mixed sampling} requires twice as many U-Net inferences of Base sampling and outperforms Switching in balancing text similarity and concept preservation.
    \item \textbf{ProFusion} requires four times the U-Net inferences of Base sampling, achieving the highest concept fidelity but requiring extensive hyperparameter tuning and offering less text similarity improvement than Mixed.
\end{itemize}
