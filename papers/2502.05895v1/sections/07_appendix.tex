%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Related Work} \label{app:related_work}
\textbf{Personalized Generation} 
Due to the considerable success of large text-to-image models \cite{ramesh2022hierarchical, ramesh2021zero, saharia2022photorealistic, rombach2022high}, the field of personalized generation has been actively developed. The challenge is to customize a text-to-image model to generate specific concepts that are specified using several input images. Many different approaches \cite{DB, TI, CD, svdiff, ortogonal, profusion, elite, r1e} have been proposed to solve this problem and can be divided into the following groups: pseudo-token optimization \cite{TI, profusion, disenbooth, r1e}, diffusion fune-tuning \cite{DB, CD, profusion}, and encoder-based \cite{elite}. The pseudo-token paradigm adjusts the text encoder to convert the concept token into the proper embedding for the diffusion model. Such embedding can be optimized directly \cite{TI, r1e} or can be generated by other neural networks \cite{disenbooth, profusion}. Such approaches usually require a small number of parameters to optimize but lose the visual features of the target concept. Diffusion fine-tuning-based methods optimize almost all \cite{DB} or parts \cite{CD} of the model to reconstruct the training images of the concept. This allows the model to learn the input concept with high accuracy, but the model due to overfitting may lose the ability to edit it when generated with different text prompts. To reduce overfitting and memory usage, lightweight parameterizations \cite{svdiff, r1e, lora} have been proposed that preserve edibility but at the cost of degrading concept fidelity. Encoder-based methods \cite{elite} allow one forward pass of an encoder that has been trained on a large dataset of many different objects to embed the input concept. This dramatically speeds up the process of learning a new concept and such a model is highly editable, but the quality of recovering concept details may be low. Generally, the main problem with existing personalized generation approaches is that they struggle to simultaneously recover a concept with high quality and generate it in a variety of scenes.

\textbf{Sampling strategies}
Much research has been devoted to sampling techniques for text-to-image diffusion models, focusing not only on personalized generation but also on image editing. In this paper, we address a more specific question: how can the two trajectories -- superclass and concept -- be optimally combined to achieve both high concept fidelity and high editability? The ProFusion paper \cite{profusion} considered one way of combining these trajectories (Mixed sampling), which we analyze in detail in our paper (see Section \ref{sec:mixed_sampling}) and show its properties and problems. In ProFusion, authors additionally proposed a more complex sampling procedure, which we observed to be redundant compared to Mixed sampling, as can be seen in our experiments (see Section \ref{sec:experiments}). In Photoswap \cite{photoswap}, authors consider another way of combining trajectories by superclass and concept, which turns out to be almost identical to the Switching sampling strategy that we discuss in detail in Section \ref{sec:switching_sampling}. We show why this strategy fails to achieve simultaneous improvements in concept reconstruction and editability. In the paper, we propose a more efficient way of combining these two trajectories that achieves an optimal balance between the two key features of personalized generation: concept reconstruction and editability.

\section{Training details} \label{sec:training-details}
The Stable Diffusion-2-base model is used for all experiments. For the Dreambooth, Custom Diffusion, and Textual Inversion methods, we used the implementation from \url{https://github.com/huggingface/diffusers}.

\textbf{SVDiff} We implement the method based on \url{https://github.com/mkshing/svdiff-pytorch}. The parameterization is applied to all Text Encoder and U-Net layers. The models for all concepts were trained for $1600$ using Adam optimizer with $\text{batch size} = 1$, $\text{learning rate} = 0.001$, $\text{learning rate 1d} = 0.000001$, $\text{betas} = (0.9, 0.999)$, $\text{epsilon} = 1e\!-\!8$, and $\text{weight decay} = 0.01$. 

\textbf{Dreambooth} All query, key, and value layers in Text Encoder and U-Net were trained during fine-tuning. The models for all concepts were trained for $400$ steps using Adam optimizer with $\text{batch size} = 1$, $\text{learning rate} = 2e\!-\!5$, $\text{betas} = (0.9, 0.999)$, $\text{epsilon} = 1e\!-\!8$, and $\text{weight decay} = 0.01$. 

\textbf{Custom Diffusion} The models for all concepts were trained for $1600$ steps using Adam optimizer with $\text{batch size} = 1$, $\text{learning rate} = 0.00001$, $\text{betas} = (0.9, 0.999)$, $\text{epsilon} = 1e\!-\!8$, and $\text{weight decay} = 0.01$. 

\textbf{Textual Inversion} The models for all concepts were trained for $10000$ steps using Adam optimizer with $\text{batch size} = 1$, $\text{learning rate} = 0.005$, $\text{betas} = (0.9, 0.999)$, $\text{epsilon} = 1e\!-\!8$, and $\text{weight decay} = 0.01$. 

\textbf{ELITE} We used the pre-trained model from the official repo \url{https://github.com/csyxwei/ELITE} with $\lambda=0.6$ and inference hyperparams from the original paper.

\clearpage
\section{Superclass and concept trajectory choice}\label{app:hyper_theta}

 \begin{wrapfigure}{r}{0.45\textwidth}
    % \centering
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/mixed_noft_nosup.pdf}
    \caption{The Pareto frontiers for original Mixed sampling and Mixed sampling in the Superclass, NoFT, and Empty Prompt setups. Mixed NoFT and Mixed Empty Prompt configurations overlap with the Pareto frontier of the original mixed sampling, but primarily in regions associated with low image similarity, which compromises concept fidelity.} \label{fig:mixed_noft_ep}
    \vspace{-0.14in}
\end{wrapfigure}

There are multiple ways to define sampling with maximized textual alignment to the prompt. However, the arbitrary choice can harm the alignment between Base sampling~\ref{eq:concept_sampling} and the selected trajectory. We use the Sampling with superclass (\ref{eq:superclass_sampling}) as it's the default choice in the literature and guarantees the maximized alignment between noise predictions $\tilde{\varepsilon}_{\theta}(p^C)$ and $\tilde{\varepsilon}_{\theta}(p^S)$. 

The several natural ways to adjust Sampling with superclass can be presented by varying $\theta$ and $p^{S}$ in (\ref{eq:superclass_sampling}). We explore two additional options with decreased alignment with (\ref{eq:concept_sampling}): (1) NoFT -- weights of base model $\theta^{\text{orig}}$ instead fine-tuned weights, (2) Empty Prompt -- prompt without any reference to a concept, even to its superclass category, i.e. $p^{\hat{S}} = \textit{"with a city in the background"}$ instead of $p^{S} = \textit{"a backpack with a city in the background"}$.

To validate the robustness of our framework for sampling method selection, we employ the original experimental protocol, supplementing the results shown in Figures~\ref{fig:examples} and~\ref{fig:profusion-photoswap}. Our analysis of Figures~\ref{fig:multi-stage_noft_ep} and~\ref{fig:masked_noft_ep} reveals that trajectories generated under the NoFT and Empty Prompt configurations (second and third columns, respectively) maintain identical method ordering to those produced by Superclass sampling ((\ref{eq:superclass_sampling}), first column).

Notably, Figure~\ref{fig:mixed_noft_ep} shows that Empty Prompt configuration demonstrates weaker alignment with Base sampling compared to NoFT, particularly at higher values of the superclass guidance scale $\omega_{s}$. This divergence manifests as reduced concept fidelity for Empty Prompt under large $\omega_{s}$. These findings highlight a practical adjustment: prioritizing smaller $\omega_{s}$ values in Empty Prompt setup preserves concept fidelity without altering the framework’s core selection logic. 

A key limitation of increased misalignment is the gradual erosion of superclass category information from generated images, which can lead to semantically inconsistent outputs. For instance, Figure~\ref{fig:examples_noft_ep} illustrates how the Mixed Empty Prompt setup, despite the strong animal prior in Base sampling, can produce human-like features in an image of a cat described as \textit{"in a chef outfit"}. This suggests that when superclass information is weakened, the model may introduce unexpected visual artifacts, impacting the fidelity of the intended concept.

Concept sampling (\ref{eq:concept_sampling}) can also be adjusted to better capture a concept’s visual characteristics, further decoupling fidelity from editability. For example, this can be achieved by (1) using the weights of a highly overfitted model (e.g., DreamBooth) or (2) selecting a prompt that omits contextual details, such as $p^{\hat{C}} = \textit{"a photo of V*"}$ instead of $p^{C} = \textit{"a V* with a city in the background"}$. Combining superclass sampling under NoFT or Empty Prompt with Base sampling configured via (1) or (2) could enhance both image and text similarity. We leave this direction for future work.

\begin{figure}[b]
    \centering
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/multi-stage_original.pdf}
    \hfill
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/multi-stage_noft.pdf}
    \hfill
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/multi-stage_nosup.pdf}
    \caption{Pareto Frontier Curves for Mixed, Switching, and Multi-Stage Sampling Methods in the Superclass, NoFT and Empty Prompt setups.
The NoFT and Empty Prompt configurations (second and third columns, respectively) preserve the same method ordering as those produced by Superclass sampling (first column).} \label{fig:multi-stage_noft_ep}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/masked_profusion.pdf}
    \hfill
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/masked_noft.pdf}
    \hfill
    \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=0.32\linewidth]{imgs/masked_nosup.pdf}
    \caption{Pareto Frontier Curves for Mixed, Switching, Masked, and ProFusion Sampling Methods in the Superclass, NoFT, and Empty Prompt setups.
The NoFT and Empty Prompt configurations (second and third columns, respectively) preserve the same method ordering as those produced by Superclass sampling (first column).} \label{fig:masked_noft_ep}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{imgs/examples_noft_ep.pdf}
    \caption{Examples of the generation outputs for Mixed and ProFusion sampling methods for their optimal metrics point in the Superclass, NoFT, and Empty Prompt (EP) setups.} \label{fig:examples_noft_ep}
\end{figure}

\clearpage

\begin{figure}[ht!]
  \centering
  \includegraphics[trim={0 5cm 0 5cm},clip,width=0.95\linewidth]{imgs/us_example_new.pdf}
  \caption{An example of a task in the user study}
  \label{fig:us_ex}
  \vspace{-0.19in}
\end{figure}

\section{Data preparation}\label{app:data}
For each concept, we used inpainting augmentations to create the training dataset. We took an original image and automatically segmented it using the Segment Anything model on top of the CLIP cross-attention maps. Then we crop the concept from the original image, apply affine transformations to it, and inpaint the background. We used $10$ augmentation prompts, different from the evaluation prompts, and sampled $3$ images per prompt, resulting in a total of $30$ training images per concept. We commit to open-source the augmented datasets for each concept after publication.

\section{User Study}\label{app:us}

An example task from the user study is shown in Figure~\ref{fig:us_ex}. In total, we collected 48,864 responses from 200 unique users for 16,000 unique pairs. For each task, users were asked three questions: 1) "Which image is more consistent with the text prompt?" 2) "Which image better represents the original image?" 3) "Which image is generally better in terms of alignment with the prompt and concept identity preservation?" For each question, users selected one of three responses: "1", "2", or "Can't decide."

\section{Complex Prompts Setting}\label{app:long_prompts}

We conduct a comparison of different sampling methods using a set of complex prompts. For this analysis, we collected 10 prompts, each featuring multiple scene changes simultaneously, including stylization, background, and outfit:

\adjustbox{max width=\linewidth}{
\begin{lstlisting}
live_long = [
  "V* in a chief outfit in a nostalgic kitchen filled with vintage furniture and scattered biscuit",
  "V* sitting on a windowsill in Tokyo at dusk, illuminated by neon city lights, using neon color palette",
  "a vintage-style illustration of a V* sitting on a cobblestone street in Paris during a rainy evening, showcasing muted tones and soft grays",
  "an anime drawing of a V* dressed in a superhero cape, soaring through the skies above a bustling city during a sunset",
  "a cartoonish illustration of a V* dressed as a ballerina performing on a stage in the spotlight",
  "oil painting of a V* in Seattle during a snowy full moon night",
  "a digital painting of a V* in a wizard's robe in a magical forest at midnight, accented with purples and sparkling silver tones",
  "a drawing of a V* wearing a space helmet, floating among stars in a cosmic landscape during a starry night",
  "a V* in a detective outfit in a foggy London street during a rainy evening, using muted grays and blues",
  "a V* wearing a pirate hat exploring a sandy beach at the sunset with a boat floating in the background",
]

object_long = [
  "a digital illustration of a V* on a windowsill in Tokyo at dusk, illuminated by neon city lights, using neon color palette",
  "a sketch of a V* on a sofa in a cozy living room, rendered in warm tones",
  "a watercolor painting of a V* on a wooden table in a sunny backyard, surrounded by flowers and butterflies",
  "a V* floating in a bathtub filled with bubbles and illuminated by the warm glow of evening sunlight filtering through a nearby window",
  "a charcoal sketch of a giant V* surrounded by floating clouds during a starry night, where the moonlight creates an ethereal glow",
  "oil painting of a V* in Seattle during a snowy full moon night",
  "a drawing of a V* floating among stars in a cosmic landscape during a starry night with a spacecraft in the background",
  "a V* on a sandy beach next to the sand castle at the sunset with a floaing boat in the background",
  "an anime drawing V* on top of a white rug in the forest with a small wooden house in the background",
  "a vintage-style illustration of a V* on a cobblestone street in Paris during a rainy evening, showcasing muted tones and soft grays",
]
\end{lstlisting}
}

The results of this comparison are presented in Figures~\ref{fig:add_long},~\ref{fig:add_long_metrics}. We observe that Base sampling may struggle to preserve all the features specified by the prompts, whereas advanced sampling techniques effectively restore them. The overall arrangement of methods in the metric space closely mirrors that observed in the setting with simple prompts.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{imgs/long_prompts_examples.pdf}
  \caption{Additional examples of the generation outputs for different sampling methods with \textbf{complex prompts}. We highlight parts of the prompt that are missing in Base sampling while appearing in other methods.}
  \label{fig:add_long}
\end{figure}

\clearpage
\section{Dreambooth results}\label{app:dreambooth}

We conduct additional analysis of different sampling methods in combination with Dreambooth. Figure~\ref{fig:add_db_metrics} shows that Mixed Sampling still overperforms Switching and Photoswap,  while Multi-stage and Masked struggle to provide an additional improvement over the simple baseline. Figure~\ref{fig:add_db} shows that all methods allow for improvement TS with a negligent decrease in IS while Mixed Sampling provides the best IS among all samplings.

\begin{figure*}[!ht]
\centering
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/long_prompts.pdf}
  \caption{CLIP metrics for different sampling methods estimated on \textbf{complex prompts}.}
  \label{fig:add_long_metrics}
\end{minipage}
\hfill
\begin{minipage}{.477\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/db_samplings.pdf}
  \caption{CLIP metrics for different sampling strategies on top of a Dreambooth fine-tuning method.}
  \label{fig:add_db_metrics}
\end{minipage}
\end{figure*} 

\begin{figure}[h!]
  \centering
  \includegraphics[trim={0 1cm 0 1cm},clip,width=\linewidth]{imgs/db_sampling_examples.pdf}
  \caption{Additional examples of the generation outputs for different sampling methods on top of a Dreambooth fine-tuning method.}
  \label{fig:add_db}
\end{figure}

\section{PixArt-alpha \& SD-XL}\label{app:add_backbones}
We conducted a series of experiments using different backbones. For SD-XL~\cite{podell2023sdxlimprovinglatentdiffusion}, we used SVDDiff as the fine-tuning method, while PixArt-alpha~\citep{chen2023pixartalphafasttrainingdiffusion} employed standard Dreambooth training. Hyperparameters for Switching, Masked, and ProFusion were selected in the same manner as in the experiments with SD2.

Figures~\ref{fig:pixart} and~\ref{fig:sdxl} demonstrate that Mixed Sampling follows a similar pattern to SD2, improving TS without a significant loss in IS. Notably, Mixed Sampling for SD-XL achieves simultaneous improvements in both IS and TS. ProFusion exhibits behavior consistent with SD2, enhancing IS more effectively than Mixed Sampling but performing worse at improving TS while also requiring twice the computational resources. 

\begin{figure}[h]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/pixart.pdf}
  \captionof{figure}{CLIP metrics for different sampling methods estimated on PixArt model.}
  \label{fig:pixart}
\end{minipage}%
\hfill
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/sdxl.pdf}
  \captionof{figure}{CLIP metrics for different sampling methods estimated on SD-XL model.}
  \label{fig:sdxl}
\end{minipage}
\end{figure}

\clearpage
\section{Cross-Attention Masks}\label{app:cross_attn}

\begin{figure}[h!]
  \centering
  \includegraphics[trim={3cm 0cm 3cm 0cm},clip,width=\linewidth]{imgs/cross_attention_masks.pdf}
  \caption{Visualization of the cross-attention masks for Masked sampling examples. Here, $q$ defines the thresholding quantile and $t$ the denoising step.}
  \label{fig:cross_attn_add_ex}
\end{figure}

\clearpage
\section{Additional Examples}\label{app:add_example}

\begin{figure}[h!]
  \centering
  \includegraphics[trim={0 2cm 0 2cm},clip,width=\linewidth]{imgs/additional_examples.pdf}
  \caption{Additional examples of the generation outputs for different sampling methods.}
  \label{fig:add_ex}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[trim={0 2cm 0 2cm},clip,width=\linewidth]{imgs/additional_examples_all.pdf}
  \caption{Additional examples of the generation outputs for Mixed and ProFusion sampling methods in comparison to the main personalized generation baselines.}
  \label{fig:add_ex_all}
\end{figure}

\clearpage
\section{DINO Image Similarity}\label{app:add_dino}

We compare CLIP-IS (left column) and DINO-IS~\citep{oquab2024dinov2learningrobustvisual} (right column) in Figures~\ref{fig:profusion_photoswap_dino},~\ref{fig:all_methods_dino}. We observe that despite the choice of metric, different sampling techniques and finetuning strategies have the same arrangement. The most noticeable difference is that SVDDiff superiority over ELITE and TI is more pronounced. That strengthens our motivation to select SVDDiff as the main backbone.

\begin{figure}[h]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/profusion_photoswap.pdf}
\end{minipage}%
\hfill
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/profusion_photoswap_dino.pdf}
\end{minipage}
\caption{Pareto frontiers curves for Photoswap~\citep{photoswap} and ProFusion~\citep{profusion}.}\label{fig:profusion_photoswap_dino}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/all_methods.pdf}
\end{minipage}%
\hfill
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[trim={3cm 10cm 3cm 10cm},clip,width=\linewidth]{imgs/all_methods_dino.pdf}
\end{minipage}
\caption{The overall results of different sampling methods against main personalized generation baselines.}\label{fig:all_methods_dino}
\end{figure}
