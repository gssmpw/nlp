\section{Related Work} \label{sec:related_work}
\textbf{Personalized Generation} 
Due to the considerable success of large text-to-image models \cite{ramesh2022hierarchical, ramesh2021zero, saharia2022photorealistic, rombach2022high}, the field of personalized generation has been actively developed. The challenge is to customise a text-to-image model to generate specific concepts that are specified using several input images. Many different approaches \cite{DB, TI, CD, svdiff, ortogonal, profusion, elite, r1e} have been proposed to solve this problem and they can be divided into the following groups: pseudo-token optimisation \cite{TI, profusion, disenbooth, r1e}, diffusion fune-tuning \cite{DB, CD, profusion}, and encoder-based \cite{elite}. The pseudo-token paradigm adjusts the text encoder to convert the concept token into the proper embedding for the diffusion model. Such embedding can be optimized directly \cite{TI, r1e} or can be generated by other neural networks \cite{disenbooth, profusion}. Such approaches usually require a small number of parameters to optimise but lose the visual features of the target concept. Diffusion fine-tuning based methods optimise almost all \cite{DB} or parts \cite{CD} of the model to reconstruct the training images of the concept. This allows to learn the input concept with high accuracy, but the model due to overfitting may lose the ability to edit it when generated with different text prompts. To reduce overfitting and the memory used, different lightweight parameterizations \cite{svdiff, r1e, lora} have been proposed that preserve edibility but at the cost of degrading concept fidelity. Encoder-based methods \cite{elite} allow one forward pass of an encoder that has been trained on a large dataset of many different objects to embed the input concept. This dramatically speeds up the process of learning a new concept and such a model is highly editable, but the quality of recovering concept details may be low. Generally, the main problem with existing personalised generation approaches is that they struggle to simultaneously recover a concept with high quality and generate it in a variety of scenes.

\textbf{Sampling strategies}
Much work has been devoted to the study of sampling for text-to-image diffusion models, not only in the task of personalised generation, but also in image editing. In this paper, we investigate a narrower question: how we can optimally combine the two trajectories on superclass and concept to simultaneously have high concept fidelity and high editability. The ProFusion paper \cite{profusion} considered one way of combining these trajectories (mixed sampling), which we analyse in detail in our paper (see Section \ref{sec:mixed_sampling}), and show its properties and problems. In ProFusion, authors additionally proposed a more complex sampling procedure, which we observed to be redundant compared to mixed sampling, as can be seen in our experiments (see Section \ref{sec:experiments}). In Photoswap \cite{photoswap} authors consider another way of combining trajectories by superclass and concept, which turns out to be almost identical to the switching sampling strategy that we discuss in detail in Section \ref{sec:switching_sampling}. We show why this strategy fails to achieve simultaneous improvements in concept reconstruction and editability. In the paper, we propose a more efficient way of combining these two trajectories that achieves an optimal balance between the two key features of personalised generation: concept reconstruction and its editability.
