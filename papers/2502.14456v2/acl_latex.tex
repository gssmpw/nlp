% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}  % 智能列宽调整
\usepackage{float}     % 表格定位控制
\usepackage{ragged2e}  % 文本对齐
\usepackage{enumitem}   % For compact itemize lists
\usepackage{epstopdf}
\epstopdfsetup{outdir=./}



% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{url} % 用于处理邮箱地址


% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{authblk}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ \textit{#1}}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}
\usepackage{listings}
\usepackage{enumitem}
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    framesep=3mm
}
\title{Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
  Ran Ding\textsuperscript{1*} \and
  Ziyu Zhang\textsuperscript{1*} \and
  Ying Zhu\textsuperscript{1}\thanks{These authors contributed equally to this work.} \and
  Ziqian Kong\textsuperscript{2} \and
  Peilan Xu\textsuperscript{1}\thanks{Corresponding author.} \\
  
  \textsuperscript{1}Nanjing University of Information Science and Technology, Nanjing, China \\
  \textsuperscript{2}Hangzhou Dianzi University, Hangzhou, China\\
   \textsuperscript
  \url{{202283460001, 202283460036, 202283460055}@nuist.edu.cn}, \\
  \url{kzq@hdu.edu.cn}, \url{xpl@nuist.edu.cn}
}


%\author{
%  \textbf{Ran Ding\textsuperscript{1}},
%  \textbf{Ziyu Zhang\textsuperscript{1}},
%  \textbf{Ying Zhu\textsuperscript{1}},
%  \textbf{Ziqian Kong\textsuperscript{1}},
%\\
%  \textsuperscript{1}Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China \\

%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
To enhance tourists' experiences and immersion, this paper proposes a narrative-driven travel planning framework called NarrativeGuide, which generates a geoculturally-grounded narrative script for travelers, offering a novel, role-playing experience for their journey. In the initial stage, NarrativeGuide constructs a knowledge graph for attractions within a city, then configures the worldview, character setting, and exposition based on the knowledge graph. Using this foundation, the knowledge graph is combined to generate an independent scene unit for each attraction. During the itinerary planning stage, NarrativeGuide models narrative-driven travel planning as an optimization problem, utilizing a genetic algorithm (GA) to refine the itinerary. Before evaluating the candidate itinerary, transition scripts are generated for each pair of adjacent attractions, which, along with the scene units, form a complete script. The weighted sum of script coherence, travel time, and attraction scores is then used as the fitness value to update the candidate solution set. Experimental results across four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate significant improvements in narrative coherence and cultural fit, alongside a notable reduction in travel time and an increase in the quality of visited attractions. Our study highlights that incorporating external evolutionary optimization effectively addresses the limitations of large language models in travel planning.

\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated significant success in various generation tasks, such as role-playing \cite{wang2023rolellm}. These applications not only offer a convenient alternative to human labor but also enhance the user's narrative immersion \cite{ahn2024timechara, lu2024large}, such as the educational chatbot \cite{wang2024book2dial} and the sales agent \cite{chang2024injecting}. Moreover, in the tourism domain, some studies \cite{wei2024tourllm, vasic2024llm, helmy2024navigating} have explored employing LLMs as virtual tour guides. Although these systems offer increased convenience, they do not necessarily improve the overall user experience. This is because tourists' modes of travel remain unchanged, limiting the potential for deeper immersion, and LLMs often lack robust itinerary planning capabilities.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figure1.pdf}
  \caption{Comparison between narrative-driven travel and traditional tourism. In traditional tourism (top figure), tourists typically search encyclopedias or consult virtual guides to obtain information about attractions. Narrative-driven travel (bottom figure) immerses tourists in a personalized storyline, where they assume roles within a script based on the geocultural background of the attractions. Guided by the \textit{NarrativeGuide}.}
  \label{fig:experiments1}
\end{figure}

Indeed, the powerful story creation capabilities of LLMs have the potential to transform the tourism industry \cite{wang2023open, mirowski2023co}. By combining LLM-driven story generation with agent-based role-playing, narratives can be effortlessly brought to life \cite{han2024ibsen, wu2024role}. Accordingly, we propose the concept of narrative-driven travel planning, as illustrated in Figure \ref{fig:experiments1}. By generating a geocultural-grounded script, tourists can assume the roles of characters within the narrative, thereby enhancing their immersive experience.

Unlike existing tasks in script generation and virtual tour guiding, narrative-driven travel planning faces two primary challenges. First, for the task of generating a travel guide script, LLMs should incorporate geocultural references from authentic tourist attractions to ensure an immersive experience. Second, itineraries must satisfy tourists' constraints, such as travel duration, while optimizing narrative coherence. However, recent research highlights the planning limitations of LLMs. For instance, the TravelPlanner benchmark \cite{xietravelplanner} reveals that LLMs struggle to meet user requirements, achieving a success rate of only 0.6\%.

\textbf{Contributions.} We model narrative-driven travel planning as an optimization problem. The objective is to select a subset of attractions within a city and determine an itinerary that traverses them, thereby optimizing the narrative coherence, travel time, and attraction score. To address this, we propose \textit{NarrativeGuide}, a framework that integrates geocultural knowledge graphs with genetic algorithms (GA). First, we construct a knowledge graph incorporating historical, cultural, and geographical information for each attraction and generate an independent narrative script for each attraction. Then, we apply a GA-based optimization approach. In each iteration, a new sequence of attraction visits is generated, transition scripts are added to ensure narrative coherence, and their narrative coherence is evaluated. Finally, the itinerary with the optimal weighted sum of script quality, travel time, and attraction satisfaction is selected along with its corresponding travel script. We evaluate our approach using different LLMs across four cities, Nanjing, Yangzhou, Paris, and Berlin. Experimental results demonstrate that \textit{NarrativeGuide} significantly improves script quality compared to baseline methods and enhances itinerary planning by reducing travel time and selecting more popular attractions.

%To address these challenges, this paper proposes a novel approach that integrates virtual script generation based on a city landmark knowledge graph with real-world itinerary planning (e.g., TSP). The main contributions are:
%\begin{itemize}
%    \item \textbf{A new script creation framework:} Generating virtual scripts based on a knowledge graph of city attractions, and integrating the virtual storyline with the background of real-world tourist spots.
%    \item \textbf{A multi-level generation and optimization mechanism:} By generating landmark scripts, transitions, and applying smoothness scores and genetic algorithms for optimization, we improve script coherence and practicality.
%    \item \textbf{Experimental validation and algorithm comparison:} Experiments in cities like Nanjing, Yangzhou, Paris, and Berlin demonstrate the effectiveness of our approach in enhancing script quality and meeting real-world constraints.
%\end{itemize}

\section{Related Work}

\subsection{Long-Form Script Generation}

Long-form narrative generation is a key research area in natural language processing, aiming to produce coherent and creative stories. \citet{guo2018long} introduced LeakGAN, which combines generative adversarial networks (GANs) with policy gradients to guide long-text generation. \citet{yao2019plan} introduced the "Plan-and-Write" framework, which divides the story generation into two stages: planning and writing. \citet{you2023eipe} proposed the "EIPE-text" method, which refines plans iteratively using an evaluation mechanism to produce more coherent narratives.  In the domain of scriptwriting, \citet{mirowski2023co} developed the Dramatron system, which leverages large language models (LLMs) to co-write movie and theatre scripts. Dramatron generates coherent scripts by hierarchically creating titles, characters, story beats, location descriptions, and dialogues.

\subsection{Automatic Itinerary Planning}

Numerous studies have addressed automated travel itinerary planning, employing various methods to tackle the problem. Some studies use exact algorithms, such as \citet{verbeeck2014extension}, which applies a branch-and-cut approach to solve self-guided tour planning. Since travel itinerary planning is NP-hard \cite{liao2018using, castro2015fast, gavalas2013cluster}, approximation methods are often employed to enhance solution efficiency. Consequently, metaheuristic algorithms are commonly used. For instance, \citet{abbaspour2009itinerary} employed a genetic algorithm to address itinerary planning, focusing on time and multimodal transport constraints. \citet{zhang2024cooperative} use a cooperative co-evolutionary algorithm for cross-city itinerary planning, while \citet{chen2023application} apply an improved ant colony algorithm, considering restaurant and hotel selections. More recently, some researchers have explored the use of LLMs for itinerary planning. For example, \citet{singh2024personal} leverage LLMs for personalized travel itinerary planning, and \citet{li2023everywheregpt} utilize the ChatGPT model to enable users to generate travel plans and suggestions based on keywords.

\section{Method}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{pipline.pdf}
  \caption{The pipeline of the proposed NarrativeGuide. This framework consists of two stages. The first stage, preliminary script preparation, involves constructing a knowledge graph based on the historical, cultural, and geographical background of various attractions in the city. Using this foundation, NarrativeGuide generates a worldview and character settings, followed by the exposition and independent sub-scripts for each attraction. The second stage, evolutionary itinerary optimization, begins by generating multiple candidate itineraries and their corresponding transition scripts. Each itinerary is then evaluated based on script coherence, travel time, and attraction satisfaction. Finally, GA is employed to optimize the itinerary.}
  \label{fig:experiments}
\end{figure*}

Given a travel itinerary for a tourist, LLMs can directly generate a narrative script for the journey. However, this approach encounters challenges such as attention sink and difficulties in maintaining global consistency. To address these issues, this paper adopts a segmented planning approach, dividing the complete narrative script into scene units based on individual attractions. By pre-configuring the worldview and character settings, the logical consistency of each attraction's independent narrative script is ensured. After the tourist selects a segment of the itinerary, transition scripts are generated for pairs of adjacent attractions, ultimately forming a complete travel narrative script. Moreover, the challenge of narrative-driven travel planning lies not only in the quality of the script but also in the ability to plan the itinerary. To this end, we model the problem as an optimization task and use the GA to determine the final itinerary, optimizing the script score, travel time, and attraction satisfaction. Figure 2 illustrates the pipeline of the proposed NarrativeGuide framework.

\subsection{The Optimization Model}

We model narrative-driven travel planning as an optimization problem. To formalize this, we define an undirected, connected, and weighted graph \( G = (V, E) \), where the vertex set \( V = \{v_1, v_2, \dots, v_n\} \) represents the attraction, and the edge set \( E = \{e_1, e_2, \dots, e_m\} \) represents the relationships between the attractions. Each edge \( e_k \in E \) connects two distinct vertices \( v_i \) and \( v_j \) (\(i \neq j\)) and is associated with an attribute vector \( \mathbf{w}(v_i, v_j) \), which encodes the historical or cultural connections between these two attractions, along with geographical attributes such as travel time. Each vertex \( v_i \in V \) is also associated with an attribute vector \( \mathbf{w}(v_i) \), which encapsulates information about the attraction, including its historical background, cultural significance, main attractions, geographical location, ticket price, and other relevant details.

The objective is to select a subset \( S \subset V \) and determine an optimal visiting sequence \( \textbf{x} = (x_1, x_2, \dots, x_k) \) for the selected subset. This arrangement is designed to maximize the tourist's experience, such as the coherence of the corresponding narrative script, the quality of the attractions, and the travel time. The objective function can be expressed as follows:

\begin{equation}
\label{eq:fitness}
    \begin{aligned}
    \max_{\textbf{x}} F(\textbf{x}) = & w_1 f_1(\textbf{x}) + w_2 \sum_{i=1}^{k-1} f_2(x_i, x_{i+1})^{-1} \\
    & + w_3 \sum_{i=1}^{k} f_3(x_i)
    \end{aligned}
\end{equation}
where $f_1(\textbf{x})$ represents the smoothness score, $f_2(x_i, x_{i+1})$ represents the travel time between attractions $x_i$ and $x_{i+1}$, and $f_3(x_i)$ represents the popularity of attraction $x_i$, \( w_1 \), \( w_2 \), and \( w_3 \) are weighting factors that control the relative importance of each component in the optimization. To transform the problem into a maximization problem, we take the reciprocal of the travel time \( f_2 \) as the second term in the objective function \( F(\textbf{x}) \).

\subsection{Geoculturally-Grounded Narrative Script Generation}

To create an immersive experience for tourists, the narrative script must be grounded in the geocultural context of the attractions. Therefore, we initially construct a knowledge graph by extracting information about  attractions from Wikipedia and inputting it into the LLM. The LLM is responsible for summarizing this information into five key attributes, i.e., historical background, cultural significance, historical stories, main attractions, and geographical location. In the knowledge graph, each attraction is represented as a node, and each node is associated with an attribute vector that includes the aforementioned five attributes of the attraction, collectively referred to as the attraction information. Subsequently, we input these attributes into the LLM to extract historical or cultural connections between the attractions. These connections are used as edges to connect the nodes, and each edge is associated with an attribute vector that includes historical or cultural relevance. In this manner, we construct a weighted and connected knowledge graph that enables the generation of geoculturally-grounded narrative scripts, as depicted in Fig.~\ref{fig:kg}.

% In the scenic spot script generation module, the first step is to generate the world view and character settings. To achieve this, we extract detailed information about the scenic spots from the previously constructed knowledge graph and use a Large Language Model (LLM) to generate the corresponding settings.

\begin{figure}[ht]
  \includegraphics[width=\columnwidth]{enlarge1.pdf}
  \caption{Knowledge graph of attraction information.}
  \label{fig:kg}
\end{figure}

Consider an itinerary \( \textbf{x} = (x_1, x_2, \dots, x_k) \), we generate a narrative script for the tourist in a multi-level manner, as outlined in Algorithm 1. First, we create a personalized worldview and character settings for the tourist. Then, we generate the exposition, which immerses the tourist in their role. Next, based on the geocultural information of each attraction, we generate an independent sub-script for each attraction, treating them as scene units. Finally, for the itinerary \( \textbf{x} \), we create a transition script for each pair \( x_i, x_{i+1} \), considering their cultural, historical, and geographical relationships, ensuring smooth scene transitions and maintaining the tourist's immersion.

% \begin{algorithm}
%\caption{Attraction Script Generation Algorithm}
%\label{alg:script_generation}
%\begin{algorithmic}[1]
%\State \textbf{Initialize} knowledge graph \( G \), attractions \( V \), and LLM \( \mathcal{M} \)
%\State Generate worldview \( \mathcal{W} \) and characters \( \mathcal{C} \) using \( \mathcal{M} \)
%\State Generate opening script \( S_{\text{open}} \) using \( \mathcal{M} \), \( \mathcal{W} \), and \( \mathcal{C} \)
%\For{each attraction \( v_i \) in \( V \)}
%    \State Generate scripts \( S_i \) for \( v_i \) using \( \mathcal{M} \), \( \mathcal{W} \), and \( \mathcal{C} \)
%\EndFor
%\For{each pair of connected attractions \( (v_i, v_j) \) in \( G \)}
%    \State Generate transition script \( T_{ij} \) between \( S_i \) and \( S_j \) using \( \mathcal{M} \)
%\EndFor
%\State Select the best scripts \( \{S^*\} \) and transitions \( \{T^*\} \) using a genetic algorithm
%\State \textbf{Return} \( S_{\text{open}} \), \( \{S^*\} \), and \( \{T^*\} \)
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}
\caption{Narrative Script Generation for Tourist Itinerary}
\label{alg:script_generation}
\begin{algorithmic}[1]
\State \textbf{Initialize} knowledge graph \( G \), attractions \( V \), and itinerary \( \textbf{x} \)
\State Generate world view \( \mathcal{W} \) and character settings \( \mathcal{C} \);
\State Generate exposition \( S_0 \) based on \( \mathcal{W} \), \( \mathcal{C} \);
\For{each attraction \( v_i \) in \( V \)}
    \State Generate scene unit \( S_i \) for \( v_i \) based on \( \mathcal{W} \) and \( \mathcal{C} \);
\EndFor
\For{$i \gets 1$ \textbf{to} $k-1$}
    \State Generate transition script \( T_{ij} \) between scripts \( S_{x_i} \) and \( S_{x_{i+1}} \);
\EndFor
\State \textbf{Return} Narrative script $\{S_0, S_{x_1}, T_{12}, S_{x_2}, \dots, T_{k-1, k}, S_{x_k}\}$
\end{algorithmic}
\end{algorithm}

\textbf{Worldview and Character Setting.} 
We instruct the LLM to generate a worldview, denoted as $\mathcal{W}$, by integrating the storylines and cultural backgrounds of attractions. This process follows a predefined format and an example worldview provided as reference. The LLM is tasked with producing a foundational description of the fictional world, encompassing its history, culture, and geographical features, while ensuring consistency with the background of the attractions. Additionally, it defines world rules that align with these elements.

The generated worldview $\mathcal{W}$ serves as the basis for creating two characters, i.e., the user character and the guide character. Using $\mathcal{W}$, a predefined character setting format, and example character profiles, the LLM determines the names, identities, personality traits, background stories, and relationships with the user or travel purposes for both characters. The complete character settings are represented as $\mathcal{C} = \{C_u, C_g\}$, where $C_u$ and $C_g$ correspond to the user character and guide character, respectively.

\textbf{Exposition.} The LLM synthesizes the worldview $\mathcal{W}$ and character settings $\mathcal{C}$ to generate an engaging exposition, denoted as $S_0$. This introduction establishes the narrative framework by presenting the initial encounter between the user and the guide, defining the journey’s starting point and purpose, and offering a glimpse into the forthcoming adventure. The LLM is tasked with crafting $S_0$ to ensure coherence with the predefined elements, effectively setting the stage for the unfolding storyline.

\textbf{Geoculturally-Grounded Attraction Script.} We begin by extracting detailed attraction information \(v_i\) from the knowledge graph. Using this data, we construct a comprehensive prompt that integrates \(v_i\), the overarching worldview \(\mathcal{W}\), and character settings \(\mathcal{C}\). The prompt specifies the script structure, divided into “Intro,” “Development,” “Climax,” and “Ending”, along with the desired narrative style, character interactions, and key plot elements. This structured prompt guides the LLM in generating complete and coherent attraction scripts. Following these guidelines, the LLM produces multiple scripts \(\mathcal{S}_i\) that align with the specified criteria.  

\textbf{Transition Script.} After designing a travel route \( S = \{S_0, S_{x_1}, S_{x_2}, \ldots, S_{x_k}\} \) consisting of multiple attraction scripts, we focus on generating the transitional script \( T_{ij} \) for adjacent attraction scripts \( S_{x_i} \) and \( S_{x_j} \) using the LLM. To guide the LLM, we establish several requirements: a common cultural theme, a time-space portal triggered by historical events, clear reasoning for the scene transitions, and consistency in character goals. Based on these instructions, the LLM generates \( T_{ij} \). 

Next, we combine \( S_{x_i} \), \( T_{ij} \), and \( S_{x_j} \) and input them back into the LLM for evaluation. The LLM assesses the transitional script \( T_{ij} \) according to a predefined questionnaire, considering four aspects, i.e., narrative coherence, character interaction, spatiotemporal consistency, and immersion (each dimension contains three sub-questions). The average evaluation score serves as the smoothness score for \( T_{ij} \), providing valuable data for travel planning. Upon completion, we obtain the full travel script \( T(\textbf{x}, S) = \{S_0, S_{x_1}, T_{12}, S_{x_2}, \ldots, T_{k-1,k}, S_{x_k}\} \).

\subsection{Genetic Algorithm for Narrative-Driven Travel Planning}

NarrativeGuide utilizes the GA to determine the final itinerary. The following sections introduce the algorithm from two aspects: the encoding scheme and the update of candidate solutions.

\paragraph{Encoding Scheme and Population Initialization.}
In the GA, each candidate solution represents a candidate itinerary, and the dimension of the solution indicates the upper bound of the number of attractions that can be visited. Since a single attraction may correspond to multiple different scripts, once the sequence of attractions is determined, it is necessary to further specify the script number for each attraction. To achieve this, the encoding consists of two main pieces of information: the attraction number and the script choice for each attraction. Therefore, a two-dimensional encoding scheme is used, where the first row contains the attraction numbers or 0 (indicating a placeholder that does not correspond to any attraction). The second row contains the script choices for the corresponding attractions. Fig. \ref{fig:coding} provides an encoding example, representing an itinerary from attraction 1 $\to$ attraction 2 $\to$ attraction 3, with corresponding scripts 3, 2, and 1, respectively.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{coding.pdf}
    \caption{The encoding example of a candidate solution in GA.}
    \label{fig:coding}
\end{figure}

Each candidate solution in the population is initialized as follows. For the first row, each position is randomly selected from the set of attractions \( V \), with a certain probability of being set to 0. For the second row, if a position corresponds to a visited attraction, a random script is selected from the available scripts for that attraction; otherwise, the value is set to 0.

\paragraph{Update of Candidate Solutions}
The population update process consists of two core steps, i.e., crossover and mutation. To prevent duplicate attractions during these operations, adjustments are made to the crossover and mutation strategies. Although the encoding is two-dimensional, with attraction numbers and their corresponding scripts located in the same column, only the attraction numbers need to be checked for duplication. Therefore, the encoding is treated as one-dimensional for both crossover and mutation operations.

In the crossover strategy, two crossover points, \( n_1 \) and \( n_2 \) (\( n_1 < n_2 \)), are selected. The encoding between columns \( n_1 \) and \( n_2 \) is exchanged between two individuals. During this exchange, we check for duplicate attraction numbers within the segment, ensuring no duplication occurs with other parts of the encoding. If any duplication is found, the exchange is aborted.

In the mutation strategy, a mutation point \( m \) is selected, and a random attraction number, not already present in the encoding, is chosen. The script choice for this attraction is then selected randomly.

The pseudocode of the algorithm is shown in \ref{GA}.
\begin{algorithm}
    \caption{Genetic Algorithm for Narrative-Driven Travel Planning}
    \label{GA}
    \begin{algorithmic}[1]
        \State \textbf{Initialize population $P = \{\mathbf{x}_1, \dots, \mathbf{x}_\lambda\}$}
        \While{Termination condition not satisfied}
        \For {$i \gets 1$ \textbf{to} $\lambda$}
        \State Perform Crossover operation;
        \State Perform Mutation operation;
        \State Generate new $\mathbf{x}'$;
        \State $\{S_0, S_{x'_1}, T_{12}, \dots, T_{k-1, k}, S_{x'_k}\} = T(\mathbf{x}', S)$;
        \State Evaluate the fitness of $\mathbf{x}'$, $f(\mathbf{x'})$;
        \State If $\mathbf{x}'$ is better than $\mathbf{x}$, replace $\mathbf{x}$;
        \EndFor
        \EndWhile
        \State \textbf{Return} $\textbf{x}^* \gets \arg\min(f(\textbf{x}))$
    \end{algorithmic}
\end{algorithm}


\section{Experiment}

In this section, we conduct tests in four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany. We evaluate both the quality of the scripts generated by the proposed algorithm and its ability to plan travel itineraries. The section begins with a description of the experimental setup, followed by an analysis of the results. The source code is available at \url{https://github.com/EvoNexusX/2025DingNarrativeGuide}.

\subsection{Experimental Design}
The experiment consists of two parts. The first part aims to evaluate the quality of the generated travel scripts based on predefined criteria. For this, we use OpenAI's GPT-4 model as the evaluation model. The model receives as input the algorithm-generated narrative scripts, which represent travel itineraries for various destinations, including descriptions of attractions, historical and cultural context, character interactions, and attraction information. First, the model is paired with relevant attraction information, followed by the use of custom evaluation prompts. The evaluation focuses on four aspects: narrative coherence, character interaction, spatiotemporal consistency, cultural fit. Weight adjustment rules are applied based on script length: a factor of 0.7 for scripts under 1500 characters, no adjustment for scripts between 1500 and 7000 characters, and a factor of 1.2 for scripts exceeding 7000 characters. The detailed evaluation criteria and weights are included in Appendix A.

The second part of the experiment aims to evaluate the quality of the generated travel itineraries. The comparison focuses on two metrics, i.e., travel time and attraction score. The attraction score is calculated as the product of the number of reviews and the rating for each attraction on the Ctrip website. Each model is tested ten times, and the average values are used for comparison.

% In this experiment, the objective is to evaluate the quality of the generated travel scripts according to predefined criteria. The process involves data collection, script evaluation using GPT-4, and scoring based on four main dimensions. The inputs for the experiment include two components: travel scripts, which represent travel itineraries for various destinations with descriptions of attractions, historical and cultural context, and character interactions, and attraction information, a dataset containing details about various cities' tourist attractions, including name, historical background, cultural significance, and geographic location.

% The generated travel scripts are evaluated using OpenAI's GPT-4 model by extracting script content, pairing it with relevant attraction information, and using custom evaluation prompts. The evaluation covers four dimensions: plot coherence, character interaction, time and space coherence, and cultural fit. Weight adjustment rules are applied based on script length, with a factor of 0.7 for scripts under 1500 characters, no adjustment for scripts between 1500 and 7000 characters, and a factor of 1.2 for scripts over 7000 characters. 

\subsection{Experimental Results}
\subsubsection{Script Quality}
Table \ref{tab:script_score} presents the experimental results of the proposed algorithm across different LLMs, including Deepseek-v3, GPT-4o, GPT-4o-mini, GPT-4, and Qwen2.5-max. We use pure GPT-4 as the baseline algorithm for comparison. The evaluation dimensions include narrative coherence (NC), character interaction (CI), spatiotemporal consistency (SC), cultural fit (CF), and the overall score. Here, we generate Chinese scripts for Nanjing and Yangzhou in China, while English scripts are generated for Paris and Berlin in France and Germany, respectively. 

\begin{table}[ht]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{llccccc}
\toprule
\textbf{City} & \textbf{Model} & \textbf{NC} & \textbf{CI} & \textbf{SC} & \textbf{CF} & \textbf{Overall} \\
\midrule
\multirow{6}{*}{Berlin} & Baseline & 7.40 & 2.40 & 8.00 & 7.50 & 25.30 \\
 & Deepseek-v3 & 7.92 & 2.52 & 9.12 & 9.00 & 28.56 \\
 & GPT-4o-mini & 10.08 & 2.52 & 8.88 & 10.20 & \textbf{31.68} \\
 & GPT-4o & 9.36 & 2.52 & 9.12 & 9.60 & 30.60 \\
 & GPT-4 & 8.88 & 2.88 & 8.40 & 9.00 & 29.16 \\
 & Qwen2.5-max & 8.88 & 2.52 & 9.12 & 9.00 & 29.52 \\
\midrule
\multirow{6}{*}{NanJing} & Baseline & 5.18 & 1.68 & 5.18 & 5.25 & 17.29 \\
 & Deepseek-v3 & 7.40 & 2.10 & 7.00 & 7.50 & 24.00 \\
 & GPT-4o-mini & 7.40 & 2.10 & 7.00 & 7.00 & 23.50 \\
 & GPT-4o & 7.40 & 2.10 & 7.00 & 7.50 & 24.00 \\
 & GPT-4 & 7.20 & 2.10 & 7.00 & 7.50 & 23.80 \\
 & Qwen2.5-max & 7.40 & 2.10 & 7.60 & 7.50 & \textbf{24.60} \\
\midrule
\multirow{6}{*}{Paris} & Baseline & 7.40 & 2.40 & 7.00 & 7.50 & 24.30 \\
 & Deepseek-v3 & 8.88 & 2.88 & 9.12 & 9.00 & 29.88 \\
 & GPT-4o-mini & 8.64 & 2.52 & 8.40 & 9.00 & 28.56 \\
 & GPT-4o & 7.40 & 2.40 & 7.40 & 7.50 & 24.70 \\
 & GPT-4 & 9.84 & 2.88 & 9.12 & 9.00 & 30.84 \\
 & Qwen2.5-max & 9.84 & 2.88 & 9.60 & 10.80 & \textbf{33.12} \\
\midrule
\multirow{6}{*}{YangZhou} & Baseline & 5.18 & 1.68 & 5.18 & 5.25 & 17.29 \\
 & Deepseek-v3 & 7.40 & 2.10 & 7.40 & 7.50 & 24.40 \\
 & GPT-4o-mini & 8.16 & 2.52 & 8.40 & 8.40 & \textbf{27.48} \\
 & GPT-4o & 7.40 & 2.10 & 7.00 & 7.50 & 24.00 \\
 & GPT-4 & 7.00 & 2.10 & 7.00 & 7.00 & 23.10 \\
 & Qwen2.5-max & 7.40 & 2.10 & 7.00 & 7.50 & 24.00 \\
\bottomrule
\end{tabular}
}
\caption{The experimental results of the proposed algorithm across Deepseek-v3, GPT-4o, GPT-4o-mini, GPT-4, and Qwen2.5-max are compared with the baseline GPT-4.}
\label{tab:script_score}
\end{table}

From the results in Table \ref{tab:script_score}, it can be observed that the proposed algorithm outperforms the baseline method across all four cities and various LLMs. Among the LLMs tested, Qwen2.5-max performed the best in Nanjing (China) and Paris (France), while GPT-4o-mini showed the best results in Yangzhou (China) and Berlin (Germany). Compared to the baseline algorithm, the scores improved by 28\%–59\%, demonstrating that the proposed algorithm significantly enhances LLM-driven narrative-based travel planning tasks. Furthermore, the experimental results indicate that better LLM performance did not lead to a significant improvement in script generation tasks. In fact, our approach decomposes a large itinerary script into several scene units (representing individual attractions) and constructs the complete narrative script through transition scripts. This reduces the demand for LLMs' long-text generation capabilities.

By comparing the scores across the four detailed metrics, we can observe significant improvements in narrative coherence (NC), cultural fit (CF), and spatiotemporal consistency (SC), while the improvement in character interaction (CI) was relatively modest. The improvements in narrative coherence and spatiotemporal consistency can be attributed to the evolutionary algorithm's optimization of the itinerary, which considers both the geographical proximity of attractions and their cultural and historical relevance. The enhancement in cultural fit arises from the algorithm's approach of assigning an independent narrative to each attraction, ensuring that the attraction's script aligns with its cultural background. However, when generating the overall script, the occurrence of hallucinations may reduce the cultural fit score.

Moreover, the experiments also indicate that the language of the script (Chinese versus English) has a significant impact on the quality of the generated scripts. As shown in Table \ref{tab:script_score}, the overall scores for Chinese scripts (Nanjing and Yangzhou) were consistently lower than those for English scripts (Berlin and Paris). This suggests inherent differences in how models process historical content across various linguistic and cultural contexts.

\subsubsection{Travel Itinerary Planning}

This section compares the proposed NarrativeGuide with several representative LLMs, including GPT-4o, GPT-4o-mini, and Qwen2.5-max, focusing on their travel itinerary planning capabilities. The comparison primarily examines travel time and the quality of the planned attractions, as shown in Tables \ref{tab:com_time} and \ref{tab:score}. Note that the base LLM used in NarrativeGuide is GPT-4o.

\begin{table}[ht]
    \scriptsize
    \centering
    \begin{tabular}{lcccc}
        \toprule
        & NarrativeGuide    & GPT-4o-mini & GPT-4o & Qwen2.5-max \\ \midrule  
        Nanjing  & \textbf{0.384} & 1.659      & 1.935  &1.854\\  \midrule
        Yangzhou & \textbf{0.786} & 2.656      & 1.645  &1.288\\  \midrule
        Paris    & \textbf{0.249} & 3.283      & 3.143  &3.566\\  \midrule
        Berlin   & \textbf{0.212} & 3.836      & 5.886  &3.595\\  \bottomrule
    \end{tabular}
    \caption{The travel time (h) of NarrativeGuide with GPT-4o are compared with the baseline GPT-4o, GPT-4o-mini, and Qwen2.5-max.}
    \label{tab:com_time}
\end{table}

\begin{table}[ht]
    \scriptsize
    \centering
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        & NarrativeGuide     & GPT-4o-mini & GPT-4o & Qwen2.5-max\\  \midrule
        Nanjing  & \textbf{3.79E+05} & 1.65E+05     & 8.87E+04 & 9.63E+04 \\  \midrule
        Yangzhou & \textbf{4.24E+05} & 9.40E+04      & 1.28E+05 & 1.71E+05 \\  \midrule
        Paris    & \textbf{5.17E+04}  & 1.08E+04      & 9.55E+03  & 9.55E+03\\  \midrule
        Berlin   & \textbf{1.30E+04}  & 4.11E+03       & 3.25E+03 & 3.67E+03 \\ \bottomrule
    \end{tabular}}
    \caption{The attraction score of NarrativeGuide with GPT-4o are compared with the baseline GPT-4o, GPT-4o-mini, and Qwen2.5-max.}
    \label{tab:score}
\end{table}

From the results in Table \ref{tab:com_time}, it is evident that with the introduction of GA optimization, the algorithm tends to recommend attractions that are clustered together, significantly reducing travel time. For example, for the city of Berlin, the travel time for the itinerary recommended by GPT-4o is 27 times greater than that of the NarrativeGuide. These results once again highlight that pure LLMs lack the capability for itinerary planning and are unable to suggest a reasonable travel plan. Additionally, the results in Table \ref{tab:score} further support this conclusion. The itineraries generated by NarrativeGuide have higher attraction scores, indicating that they feature popular destinations. However, this advantage is not as significant as the travel time reduction, as LLMs possess enough internal knowledge to recommend popular attractions. Yet, due to the inability to collect real-time data from the real world, this outcome is based on prior knowledge rather than updated, accurate data. Overall, the use of GA as an external planner in NarrativeGuide proves to be beneficial, significantly enhancing the ability of LLMs to address real-world problems and meet practical demands.

\section{Conclusion}
\label{sec:bibtex}
This study introduces \textit{NarrativeGuide}, a novel framework that combines geocultural knowledge graphs with evolutionary algorithms to improve narrative-driven travel planning. By grounding script generation in real-world attractions and optimizing itineraries using GA, our approach addresses the dual challenges of narrative coherence and practical travel constraints. Experimental evaluations across four cities, i.e., Nanjing, Yangzhou, Paris, and Berlin, show significant improvements: script quality metrics, including narrative coherence, cultural fit, and spatiotemporal consistency, increased by 28\%–59\% compared to baseline methods, while travel time was reduced by up to 27-fold in cities such as Berlin. The framework’s integration of LLM-generated scene units with GA-driven itinerary optimization ensures both immersive storytelling and efficient route planning, overcoming the limitations of traditional LLMs in handling real-world constraints.


\section*{Limitations}

\paragraph{Data Dependency} The quality of generated scripts heavily relies on the completeness and accuracy of the knowledge graph, which may limit scalability to regions with sparse cultural or historical data.

\paragraph{Character Interaction} While narrative coherence and cultural fit were strengths, character interaction scores remained suboptimal (e.g., 1.68–2.88), indicating a need for deeper modeling of dynamic character behaviors.

\paragraph{Language and Cultural Gaps} A performance disparity (23\%) was observed between English and Chinese scripts, suggesting potential biases in LLMs’ handling of non-Western cultural contexts.

\paragraph{Algorithm Scalability} The genetic algorithm’s efficiency may degrade for large-scale cities or highly complex constraints (e.g., multi-day itineraries).

\paragraph{User Personalization} The current framework prioritizes narrative fluency over individualized preferences, such as varying travel interests or activity types. Future work could incorporate adaptive user profiling to address this gap.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliography{custom}
\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abbaspour and Samadzadegan(2009)}]{abbaspour2009itinerary}
RA~Abbaspour and F~Samadzadegan. 2009.
\newblock Itinerary planning in multimodal urban transportation network.
\newblock \emph{Journal of Applied Sciences}, 9(10):1898--1906.

\bibitem[{Ahn et~al.(2024)Ahn, Lee, Lim, Kim, Yun, Lee, and Kim}]{ahn2024timechara}
Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, and Gunhee Kim. 2024.
\newblock Timechara: Evaluating point-in-time character hallucination of role-playing large language models.
\newblock \emph{arXiv preprint arXiv:2405.18027}.

\bibitem[{Castro et~al.(2015)Castro, S{\"o}rensen, Vansteenwegen, and Goos}]{castro2015fast}
Marco Castro, Kenneth S{\"o}rensen, Pieter Vansteenwegen, and Peter Goos. 2015.
\newblock A fast metaheuristic for the travelling salesperson problem with hotel selection.
\newblock \emph{4or}, 13:15--34.

\bibitem[{Chang and Chen(2024)}]{chang2024injecting}
Wen-Yu Chang and Yun-Nung Chen. 2024.
\newblock Injecting salesperson's dialogue strategies in large language models with chain-of-thought reasoning.
\newblock \emph{arXiv e-prints}, pages arXiv--2404.

\bibitem[{Chen et~al.(2023)Chen, Wu, Ye, Lee, Huang, Lin, and Wang}]{chen2023application}
Shuo-Tsung Chen, Tsung-Hsien Wu, Ren-Jie Ye, Liang-Ching Lee, Wen-Yu Huang, Yi-Hong Lin, and Bo-Yao Wang. 2023.
\newblock Application of ant colony optimization computing to a recommended travel itinerary planning system with repeatedly used nodes.
\newblock \emph{Applied Sciences}, 13(24):13221.

\bibitem[{Gavalas et~al.(2013)Gavalas, Konstantopoulos, Mastakas, Pantziou, and Tasoulas}]{gavalas2013cluster}
Damianos Gavalas, Charalampos Konstantopoulos, Konstantinos Mastakas, Grammati Pantziou, and Yiannis Tasoulas. 2013.
\newblock Cluster-based heuristics for the team orienteering problem with time windows.
\newblock In \emph{Experimental Algorithms: 12th International Symposium, SEA 2013, Rome, Italy, June 5-7, 2013. Proceedings 12}, pages 390--401. Springer.

\bibitem[{Guo et~al.(2018)Guo, Lu, Cai, Zhang, Yu, and Wang}]{guo2018long}
Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018.
\newblock Long text generation via adversarial training with leaked information.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32.

\bibitem[{Han et~al.(2024)Han, Chen, Lin, Xu, and Yu}]{han2024ibsen}
Senyu Han, Lu~Chen, Li-Min Lin, Zhengshan Xu, and Kai Yu. 2024.
\newblock {IBSEN}: Director-actor agent collaboration for controllable and interactive drama script generation.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1607--1619.

\bibitem[{Helmy et~al.(2024)Helmy, El-Din, Mohamed, Kader, Ramadan, Kamal, and Selim}]{helmy2024navigating}
Mona Helmy, Yomna~Safaa El-Din, Omar~Tarek Mohamed, Omar Salah~Abdel Kader, Shehab~Adel Ramadan, Amr~Essam Kamal, and Mohamed Reda~Mohamed Selim. 2024.
\newblock Navigating the world with an intelligent tourist guide using generative ai.
\newblock In \emph{2024 International Telecommunications Conference (ITC-Egypt)}, pages 1--6. IEEE.

\bibitem[{Li(2023)}]{li2023everywheregpt}
Bohao Li. 2023.
\newblock Everywheregpt: An ai travel planning assistant based on chatgpt.
\newblock In \emph{Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering}, pages 995--1003.

\bibitem[{Liao and Zheng(2018)}]{liao2018using}
Zhixue Liao and Weimin Zheng. 2018.
\newblock Using a heuristic algorithm to design a personalized day tour route in a time-dependent stochastic environment.
\newblock \emph{Tourism Management}, 68:284--300.

\bibitem[{Lu et~al.(2024)Lu, Yu, Zhou, and Zhou}]{lu2024large}
Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024.
\newblock Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment.
\newblock \emph{arXiv e-prints}, pages arXiv--2401.

\bibitem[{Mirowski et~al.(2023)Mirowski, Mathewson, Pittman, and Evans}]{mirowski2023co}
Piotr Mirowski, Kory~W Mathewson, Jaylen Pittman, and Richard Evans. 2023.
\newblock Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals.
\newblock In \emph{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, pages 1--34.

\bibitem[{Singh et~al.(2024)Singh, Verma, Wang, Bharadwaj, Fashandi, Ferreira, and Lee}]{singh2024personal}
Harmanpreet Singh, Nikhil Verma, Yixiao Wang, Manasa Bharadwaj, Homa Fashandi, Kevin Ferreira, and Chul Lee. 2024.
\newblock Personal large language model agents: A case study on tailored travel planning.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pages 486--514.

\bibitem[{Vasic et~al.(2024)Vasic, Fill, Quattrini, and Pierdicca}]{vasic2024llm}
Iva Vasic, Hans-Georg Fill, Ramona Quattrini, and Roberto Pierdicca. 2024.
\newblock {LLM}-aided museum guide: Personalized tours based on user preferences.
\newblock In \emph{International Conference on Extended Reality}, pages 249--262. Springer.

\bibitem[{Verbeeck et~al.(2014)Verbeeck, Vansteenwegen, and Aghezzaf}]{verbeeck2014extension}
C{\'e}dric Verbeeck, Pieter Vansteenwegen, and E-H Aghezzaf. 2014.
\newblock An extension of the arc orienteering problem and its application to cycle trip planning.
\newblock \emph{Transportation research part E: logistics and transportation review}, 68:64--78.

\bibitem[{Wang et~al.(2024)Wang, Macina, Daheim, Chowdhury, and Sachan}]{wang2024book2dial}
Junling Wang, Jakub Macina, Nico Daheim, Sankalan~Pal Chowdhury, and Mrinmaya Sachan. 2024.
\newblock Book2dial: Generating teacher-student interactions from textbooks for cost-effective development of educational chatbots.
\newblock \emph{arXiv e-prints}, pages arXiv--2403.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Lin, Yu, Hu, and Karlsson}]{wang2023open}
Yuxin Wang, Jieru Lin, Zhiwei Yu, Wei Hu, and B{\"o}rje~F Karlsson. 2023{\natexlab{a}}.
\newblock Open-world story generation with structured knowledge enhancement: A comprehensive survey.
\newblock \emph{Neurocomputing}, page 126792.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Peng, Que, Liu, Zhou, Wu, Guo, Gan, Ni, Yang et~al.}]{wang2023rolellm}
Zekun~Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et~al. 2023{\natexlab{b}}.
\newblock Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2310.00746}.

\bibitem[{Wei et~al.(2024)Wei, Yang, Wang, Mao, Xu, and Ning}]{wei2024tourllm}
Qikai Wei, Mingzhi Yang, Jinqiang Wang, Wenwei Mao, Jiabo Xu, and Huansheng Ning. 2024.
\newblock Tourllm: Enhancing llms with tourism knowledge.
\newblock \emph{arXiv e-prints}, pages arXiv--2407.

\bibitem[{Wu et~al.(2024)Wu, Wu, Jiang, Liu, Hong, Zhao, and Zhang}]{wu2024role}
Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Jiale Hong, Hai Zhao, and Min Zhang. 2024.
\newblock From role-play to drama-interaction: An llm solution.
\newblock \emph{arXiv e-prints}, pages arXiv--2405.

\bibitem[{Xie et~al.()Xie, Zhang, Chen, Zhu, Lou, Tian, Xiao, and Su}]{xietravelplanner}
Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu~Su.
\newblock Travelplanner: A benchmark for real-world planning with language agents.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Yao et~al.(2019)Yao, Peng, Weischedel, Knight, Zhao, and Yan}]{yao2019plan}
Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019.
\newblock Plan-and-write: Towards better automatic storytelling.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 7378--7385.

\bibitem[{You et~al.(2023)You, Wu, Liang, Mao, Wu, Cao, Cai, Guo, Xia, Wei et~al.}]{you2023eipe}
Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, et~al. 2023.
\newblock Eipe-text: Evaluation-guided iterative plan extraction for long-form narrative text generation.
\newblock \emph{arXiv e-prints}, pages arXiv--2310.

\bibitem[{Zhang et~al.(2024)Zhang, Xu, Wang, and Luo}]{zhang2024cooperative}
Ziyu Zhang, Peilan Xu, Zhaoguo Wang, and Wenjian Luo. 2024.
\newblock Cooperative coevolution for cross-city itinerary planning.
\newblock In \emph{International Conference on Intelligent Information Processing}, pages 382--391. Springer.

\end{thebibliography}


\appendix
\clearpage
\onecolumn
\section*{Appendix A: Evaluation Criteria and Weights}
\label{sec:appendix}
\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|p{4cm}|p{5cm}|p{2.5cm}|p{2.5cm}|p{6cm}|}
\hline
\textbf{Dimension} & \textbf{Criteria} & \textbf{Score Range} & \textbf{Weight} & \textbf{Description} \\
\hline
\textbf{1. Narrative Coherence} & Event Logic & 0-10 & 0.4 & Logic of event connections and cause-effect relationships \\
                           \hline
                           & Attraction Relevance & 0-10 & 0.4 & Connection of attractions to the overall plot \\
                           \hline
                           & Transition Smoothness & 0-10 & 0.2 & Smoothness and naturalness of transitions between events and locations \\
\hline
\textbf{2. Character Interaction} & Dialogue Authenticity & 0-10 & 0.3 & Authenticity of dialogue in relation to character identities and historical/cultural context \\
                                   \hline
                                   & Cultural-Driven Actions & 0-10 & 0.4 & Actions of characters based on cultural/historical context \\
                                   \hline
                                   & Metaphorical Dialogue & 0-10 & 0.3 & Use of dialogue that adds deeper, symbolic meanings related to the attractions or historical context \\
\hline
\textbf{3. Spatiotemporal Consistency} & Spatiotemporal Corridor & 0-10 & 0.6 & Logic of time/space transitions and their relevance to the storyline and attractions \\
                                      \hline
                                      & Route Rationality & 0-10 & 0.4 & Historical and geographical logic in selecting travel paths \\
\hline
\textbf{4. Cultural Fit} & Cultural Depth & 0-10 & 0.5 & Depth of cultural integration in the narrative (impact on decisions, symbolism, etc.) \\
                          \hline
                          & Multi-Dimensional Linkage & 0-10 & 0.5 & Complexity of connections between historical, cultural, and geographical elements across attractions \\
\hline
\end{tabular}
}
\caption{Evaluation Criteria and Weights}
\end{table}

\section*{Appendix B: Prompt}

\subsection*{1. Generating Worldview}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|p{13cm}|}
\hline
\textbf{\small Description} & \textbf{\small Content} \\
\hline
\textbf{\small Table Input} & \small Location: \{item['location']\} \\
                             & \small Features/Culture/History/Legends: \{item['features']\} \\
\hline
\textbf{\small Requirement} & \small Construct a travel script worldview based on the table. Connect various attractions' storylines and describe basic information about this world. \\
\hline
\textbf{\small Example} & \begin{minipage}[t]{\linewidth}
\small
Travel Script Worldview Setting \\
Name: Time Journey: Dream Hunting in Jinling \\
Background: \\
At the intersection of modern technology and ancient wisdom exists a secret organization - 'Time Guardians'. This group consists of individuals who can travel through historical periods to protect cultural heritage. They can access a parallel world called 'Historical Realm' that preserves the most glorious cultural legacies and captivating stories from history. \\
In this realm, Nanjing (known as 'Jinling') is a mysterious place full of historical charm. Each attraction represents a temporal node containing rich history and hidden passages to other eras. These passages only appear during specific historical moments, and the Time Guardians' mission is to guide travelers through these nodes while protecting cultural heritage from temporal erosion.
\end{minipage} \\
\hline
\end{tabular}
}
\caption{Worldview Generation Template}
\end{table}



\subsection*{2. Generating Characters}
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.0} % Reduce row height
\begin{tabular}{|l|p{12cm}|}
\hline
\small \textbf{Description} & \small \textbf{Content} \\
\hline
\small \textbf{Worldview} & \small \{worldview\} \\
\hline
\small \textbf{Character 1: User's Role} & \small Name: \textit{\{Example: Lin Yi\}} \\
                                       & \small Identity: \textit{\{Example: Traveler\}} \\
                                       & \small Personality: \textit{\{Example: Curious, observant\}} \\
\hline
\small \textbf{Character 2: Guide} & \small Name: \textit{\{Example: Murong Yun\}} \\
                                   & \small Identity: \textit{\{Example: Time Guardian\}} \\
                                   & \small Expertise: \textit{\{Example: Temporal navigation\}} \\
\hline
\small \textbf{Example} & 
\begin{minipage}[t]{\linewidth}
\small
\textbf{Character Settings:}
\begin{itemize}[leftmargin=*,nosep]
\item[\textendash] \underline{Traveler:} Lin Yi (Modern history enthusiast)
\item[\textendash] \underline{Guide:} Murong Yun (Time Guardian)
\item[$\triangleright$] Goal: Protect cultural heritage nodes
\item[$\triangleright$] Key traits: Temporal navigation abilities
\end{itemize}
\end{minipage} \\
\hline
\end{tabular}
\caption{Character Generation Template}
\label{tab:character}
\end{table}


\subsection*{3. Generating Exposition}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|p{12cm}|}
\hline
\small \textbf{Description} & \small \textbf{Content} \\
\hline
\small \textbf{Worldview} & \small \{worldview\} \\
\hline
\small \textbf{Characters} & \small \{characters\} \\
\hline
\small \textbf{Requirements} & 
\begin{minipage}[t]{\linewidth}
\small
Brief introduction of worldview \\
First meeting between user and guide \\
Journey starting point and purpose \\
Preview of upcoming travels \\
Make the opening engaging and intriguing
\end{minipage} \\
\hline
\small \textbf{Example} & 
\begin{minipage}[t]{\linewidth}
\small
\textbf{Travel Script Opening} \\
\underline{Worldview}: \{worldview\} \\
\underline{Characters}: \{characters\} \\
\underline{Key Elements}: 
\newline Introduced the "Historical Realm" parallel world
\newline Established mentor-protégé relationship in first encounter
\newline Set journey goal: Protect cultural heritage nodes
\newline Foreshadowed conflicts with temporal erosion
\end{minipage} \\
\hline
\end{tabular}
}
\caption{Opening Script Generation Template}
\label{tab:opening}
\end{table}


\subsection*{4. Generating Script for Attraction}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|p{10cm}|}
\hline
\small \textbf{Description} & \small \textbf{Content} \\
\hline
\small \textbf{Background and Worldview Setting} & \small \{worldview\} \\
\hline
\small \textbf{Character Setting} & \small \{characters\} \\
\hline
\small \textbf{Attraction Setting} & 
\begin{minipage}[t]{\linewidth}
\small
\textbf{Location}: \{attraction['name']\} \\
\textbf{Historical Context}: \{attraction['history']\} \\
\textbf{Cultural Features}: \{attraction['culture']\} \\
\textbf{Legends}: \{attraction['legends']\}
\end{minipage} \\
\hline
\small \textbf{Script Requirements} & \small Four-act structure: Intro/Development/Climax/Ending \newline Historical accuracy with emotional character arcs \newline Adventure elements with environmental interactions \newline Action-driven climax with tangible conflicts \newline Self-contained narrative resolution \\
\hline
\small \textbf{Special Requirements} & \small Temporal transition effects between eras \newline Cultural symbolism in dialogue/actions \newline Consistent character voices \newline Skip redundant introductions \newline Explicit section markers \\
\hline
\end{tabular}
}
\caption{Attraction Script Generation Template}
\label{tab:attraction}
\end{table}

\subsection*{5. Generate Transition Scripts}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|>{\RaggedRight\arraybackslash}p{10cm}|}
\hline
\small \textbf{Description} & \small \textbf{Content} \\
\hline
\small \textbf{Current Scenic Spot Script} & \small \{script1\} \\
\hline
\small \textbf{Next Scenic Spot Script} & \small \{script2\} \\
\hline
\small \textbf{Transition Script} & 
\begin{minipage}[t]{\linewidth}
\small
\textbf{Example Transition}: 
\newline Used shared cultural motif (e.g., "Dragon Gate" legend) 
\newline Introduced time portal triggered by historical event
\newline Added guide's explanation linking both locations
\newline Maintained character goals across transition
\end{minipage} \\
\hline
\end{tabular}
}
\caption{Transition Script Generation Template}
\label{tab:transition}
\end{table}

\subsection*{6. Score with Transition Script}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|>{\RaggedRight\arraybackslash}p{10cm}|}
\hline
\small \textbf{Description} & \small \textbf{Content} \\
\hline
\small \textbf{Previous Script} & \small \{previous\_script\} \\
\hline
\small \textbf{Transition Script} & \small \{transition\_script\} \\
\hline
\small \textbf{Next Script} & \small \{next\_script\} \\
\hline
\small \textbf{Combined Script} & \small \{combined\_script\} \\
\hline
\small \textbf{Survey Questions} & \small \{survey\_text\} \\
\hline
\small \textbf{Scoring Requirement} & 
\begin{minipage}[t]{\linewidth}
\small
\textbf{Example Scoring}: \\
4,5,3,2,3,4,1,2,3,1,3,3 \\
\textbf{Interpretation}: 
\newline First 3 scores: Plot Coherence (Q1-Q3) 
\newline Next 3: Character Interaction (Q4-Q6) 
\newline Next 3: Spatiotemporal Coherence (Q7-Q9) 
\newline Last 3: Immersion (Q10-Q12)
\end{minipage} \\
\hline
\end{tabular}
}
\caption{Script Scoring Template}
\label{tab:scoring}
\end{table}



\section*{Appendix C: Fluency Survey}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|p{6cm}|p{5cm}|}
\hline
\small \textbf{Category} & \small \textbf{Question} & \small \textbf{Rating (1-5)} \\ 
\hline
\multirow{3}{*}{\small Plot Coherence} 
& \small 1. Plot continuity across transition scripts & \small 1: Fragmented – 5: Seamless \\ 
\cline{2-3}
& \small 2. Logical story progression & \small 1: Forced – 5: Natural \\ 
\cline{2-3}
& \small 3. Utilization of prior plot elements & \small 1: Incoherent – 5: Coherent \\ 
\hline
\multirow{3}{*}{\small Character Interaction} 
& \small 4. Consistency with character profiles & \small 1: Inconsistent – 5: Faithful \\ 
\cline{2-3}
& \small 5. Dialogue/action naturalness & \small 1: Artificial – 5: Organic \\ 
\cline{2-3}
& \small 6. Engagement level & \small 1: Disconnected – 5: Immersive \\ 
\hline
\multirow{3}{*}{\small Spatiotemporal Coherence} 
& \small 7. Historical/geographical accuracy & \small 1: Anachronistic – 5: Authentic \\ 
\cline{2-3}
& \small 8. Environmental consistency & \small 1: Jarring – 5: Continuous \\ 
\cline{2-3}
& \small 9. Transition clarity & \small 1: Confusing – 5: Intuitive \\ 
\hline
\multirow{3}{*}{\small Immersion} 
& \small 10. Narrative depth & \small 1: Superficial – 5: Layered \\ 
\cline{2-3}
& \small 11. Multimedia support & \small 1: Distracting – 5: Enhancing \\ 
\cline{2-3}
& \small 12. Innovative storytelling & \small 1: Generic – 5: Original \\ 
\hline
\end{tabular}
}
\caption{Transition Script Evaluation Criteria}
\label{tab:fluency}
\end{table}

\appendix
\clearpage



\end{document}
