\section{Proofs}
\label{sec:proofs}
\subsection{Proof of \cref{thm:grad-proj-is-adapter}}
\label{sec:proof-grad-proj-is-adapter}

\gradProjIsAdapter*
\begin{proof}
To show that the two optimization trajectories are equivalent, we will use induction to show that after every optimizer step $t \ge 0$ we have that
the optimizer states are equivalent, i.e., 
$\optimState{\Lambda}{t} = \optimState{\linearMap{S}\Theta}{t}$,
which in turn allows us to show that the networks are identical, i.e.,
$\Theta^{(t)} = \Theta^{(0)} + \linearMap{S}^{\adj}\Lambda^{(t)}$.


Note that at initialization, since $\Lambda^{(0)} = \vzero$, we have that
\begin{align*}
    \Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(0)} = \Theta^{(0)} + \linearMap{S}^\adj \vzero = \Theta^{(0)},
\end{align*}
which implies that our reparameterized network is identical to our original network.
By assumption we also have that the optimizer states are equal
$\optimState{\Lambda}{0} = \optimState{\linearMap{S}\Theta}{0}$. Now assume that this is true for $t \le k$, i.e., for all $t \le k$
\begin{align}
    \Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(t)} &= \Theta^{(t)} & \text{(Neural networks equivalent)}
    \label{eq:inductive-assumption-param} \\
    \optimState{\Lambda}{t} &= \optimState{\linearMap{S}\Theta}{t}. &\text{(Optimizer states equivalent)}
    \label{eq:inductive-assumption-optim-states}
\end{align}

Now note that for $k+1$,
\begin{align}
\Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(k+1)} 
= \Theta^{(0)} + \linearMap{S}^\adj (\Lambda^{(k)} + \update{\Lambda}{k})  =  \Theta^{(k)} + \linearMap{S}^\adj \update{\Lambda}{k} 
\label{eq:parameter-unroll-partial}
\end{align}
where we used \cref{eq:reparameterized-optim-definition} in the first equality, and \cref{eq:inductive-assumption-param} for the second equality.
Also by \cref{eq:inductive-assumption-param} and by the chain rule, we have that the gradients of the loss function at timestep $t$ to the (effective) parameters of the networks are the same, i.e.,
$
    \cot{\Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(k)}} = \cot{\Theta^{(k)}}.
$
In particular, this means that
\begin{align}
\cot{\Lambda^{(k)}} = \linearMap{S} \cot{\linearMap{S}^\adj \Lambda^{(k)}} 
= \linearMap{S} \cot{\Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(k)}} 
= \linearMap{S} \cot{\Theta^{(k)}}
\label{eq:transformed-gradient-equivalence}
\end{align}
and in turn, expanding $\update{\Lambda}{k}$,
\begin{align}
   (\update{\Lambda}{k}, \optimState{\Lambda}{k + 1}) = \optimizer( \cot{\Lambda}^{(k)}, \optimState{\Lambda}{k})  
   = \optimizer(\linearMap{S} \cot{\Theta^{(k)}}, \optimState{\linearMap{S}\Theta}{k}) 
   = (\update{\linearMap{S}\Theta}{k}, \optimState{\linearMap{S}\Theta}{k + 1}),
   \label{eq:optimizer-output-equivalence-next-step}
\end{align}
where the first equality is the optimizer we use to train the reparameterized model in \cref{eq:reparameterized-optim-definition},
in the second equality we use \cref{eq:transformed-gradient-equivalence} and \cref{eq:inductive-assumption-optim-states}. But by \cref{eq:parameter-unroll-partial},
\begin{align}
\Theta^{(0)} + \linearMap{S}^\adj \Lambda^{(k+1)} 
    = \Theta^{(k)} + \linearMap{S}^{\adj}\Delta^{(k)}_{\Lambda}    = \Theta^{(k)} + \linearMap{S}^{\adj}\Delta^{(k)}_{\linearMap{S}\Theta}  = \Theta^{(k+1)}.
   \label{eq:parameter-equivalence-next-step}
\end{align}
\Cref{eq:optimizer-output-equivalence-next-step} proves optimizer states are equivalent for optimizer step $k+1$, whereas \cref{eq:parameter-equivalence-next-step} establishes the networks are equivalent for optimizer step $k+1$, thus completing the proof.
\end{proof}


\subsection{Proof of \cref{thm:kron-factored-proj-is-mora}}
\label{sec:proof-kron-factored-proj-is-mora}

\kronFacProjIsMora*
\begin{proof}
From \cref{thm:grad-proj-is-adapter}, we know that training a linear layers using the gradient transformation $\linearMap{S} = \mR^\adj \otimes \mL$ corresponds to using the reparameterization:
\begin{align*}
\Theta &= \Theta^{(0)} + (\mR^\adj \otimes \mL)^\adj \Lambda
\end{align*}
and training $\Lambda$ instead, using the same optimizer. Letting $\vec\mA = \Lambda$, we then have
\begin{align*}
\vec\mW &= \vec{\mW^{(0)}} + (\mR \otimes \mL^\adj) \vec{\mA} \\
&= \vec{\mW^{(0)}} + \vec{ \mL^\adj \mA \mR^\adj}
\end{align*}
where in the first equation we used the fact that $(\mM \kron \mN)^\adj = \mM^\adj \kron \mN^\adj$ and in the second equation we used $\vec{\mM \mN \mO} = (\mO^\adj \kron \mM) \vec{\mN}$.
Taking $\vecinv{\cdot}$ in both sides completes the proof.
\end{proof}

\subsection{Proof of \cref{thm:galore-is-lora}}
\label{sec:proof-galore-is-lora}


We now state a more general version of \cref{thm:galore-is-lora}, and then prove it.

\begin{corollary}[Galore is one-sided LoRA (General)]
Let $\mW^{} \in \R^{m \times n}$ be the parameter matrix of a linear layer with corresponding gradient matrix $\cot{\mW} \in \R^{m \times n}$.
Consider training $\mW$ with $\optimizer$ using GaLore, i.e., where we linearly transform the gradient matrix with a matrix $\mP$,
\begin{align*}
    \widetilde{\cot{\mW}} = \begin{cases}
        \Matrix{P} \cot{\mW} & m \le n \,\,\,\,\,\, \text{(i.e., apply from the left)}\\
        \cot{\mW} \mP & m > n \,\,\,\,\,\, \text{(i.e., apply from the right)}\\
    \end{cases}
\end{align*}
and then apply our optimizer on it, before transforming our update back to parameter space via $\mP^\adj$, viz.,%
\begin{align*}
   (\update{\mW}{t}, \optimState{\mW}{t+1}) &= \optimizer(\vec{\widetilde{\cot{\mW}}^{(t)}}, \optimState{\mW}{t}) \\
   \mW^{(t+1)} = &\begin{cases}
   \mW^{(t)} + \mP^{\adj}\vecinv{\update{\mW}{t}} & m \le n \\
   \mW^{(t)} + \vecinv{\update{\mW}{t}}\mP^{\adj} & m > n \\
   \end{cases}
\end{align*}
where $\mP$ is an arbitrary matrix of size $\R^{d \times m}$ (if $m \le n$) or $\R^{n \times d}$ (otherwise) and $d \le \min(m, n)$ controls the dimensionality of the transformation.
Then the optimizer trajectory of this network is equivalent to a network trained with the reparameterization:
\begin{align*}
\mW = \begin{cases}
\mW^{(0)} + \mP^\adj \mA & m \le n \\
\mW^{(0)} + \mA \mP^\adj & m > n,
\end{cases}
\end{align*}
i.e., adding LoRA adapters where one side is frozen to $\mP^\adj$ and only the other side, $\mA$, is learned.
\end{corollary}


\begin{proof}
Define 
\begin{align*}
\linearMap{S} = \begin{cases}
    \mI_n \kron \mP & m \le n \\
    \mP^\adj \kron \mI_m & m > n \\
\end{cases}
\end{align*}
where $\mI_m$ is the $m \times m$ identity matrix, and similarly for $\mI_n$.
Then note that 
\begin{align*}
\vec{\widetilde{\cot{\mW}}} &= \begin{cases}
    \vec{\Matrix{P} \cot{\mW}} & m \le n \\
    \vec{\cot{\mW} \mP} & m > n \\
\end{cases} \\
&= \linearMap{S} \vec{\cot{\mW}}
\end{align*}
using $\vec{\mM \mN \mO} = (\mO^\adj \kron \mM) \vec{\mN}$ as before.
imilarly, we have that $\vec{\mW^{(t+1)}} = \vec{\mW^{(t)}} + \linearMap{S}^\adj \update{\mW}{t}$. Hence, by \cref{thm:grad-proj-is-adapter}, we have that training a network with GaLore is equivalent to introducing a parameter $\mA$ and optimizing using the reparameterization $\vec{\mW} = \vec{\mW^{(0)}} + \linearMap{S} \vec{\mA}$.
Observe that this choice of $\linearMap{S}$ is a special case of \cref{thm:kron-factored-proj-is-mora} where $\mL = \mP$ and $\mR = \mI$ (if $m \le n$) or $\mL = \mI$ and $\mR = \mP$ (if $m > n$).
Thus, the reparameterization corresponds to LoRA with one of the two adapter matrices frozen to $\mP$.
\end{proof}


\section{Weight Decay}
\label{app:weight-decay}

\Cref{thm:galore-is-lora} establishes an equivalence between GaLore and LoRA when stateful optimizers are in play (\cref{eq:optim-definition}).
While Adam~\citep{adam} can be straightforwardly recast as a stateful optimizer, it turns out that weight decay, as is traditionally implemented in, e.g., AdamW~\citep{adamw}, breaks this symmetry as it does not fit our definition of a stateful optimizer.
From the perspective of our definition, the problem is that optimizer steps with AdamW are not solely a function of the observed gradients up until this point, but also the actual values of the parameters.
This is important, since automatic differentiation libraries traditionally distinguish \emph{trainable} and \emph{non-trainable} parameters, with weight decay being applied to the former.
Since the duality  in \cref{thm:galore-is-lora} changes what the trainable parameters are, this means that the weight decay is applied differently in the gradient transformation view and in the linear adapter view.

For example, consider taking a linear layer with weight $\Theta$, and training it with an optimizer that applies weight decay.
When training this layer with a linear gradient transformation $\mS$, after a single optimizer step, our new weight is given by
\begin{align}
   (\update{\Theta}{0}, \optimState{\Theta}{1}) &= \text{OptimizerWithoutWeightDecay}( \mS \cot{\Theta}^{(0)}, \optimState{\Theta}{0}) \\
   \Theta^{(1)} &= \Theta^{(0)} + \mS^\adj \update{\Theta}{0} - \lambda \Theta^{(0)}
\end{align}
where $\lambda \in \mathbb{R}^{+}$ is our weight decay penalty.
Similarly, following \cref{thm:grad-proj-is-adapter}, our linear layer's effective weight after a single optimizer step, in the adapter view, is given by 
\begin{align}
   \Theta_\text{effective}^{(1)} &= \Theta^{(0)} + \mS^\adj (\Lambda^{(0)} + \update{\Lambda}{0} - \lambda \Lambda^{(0)}) \\
   &= \Theta^{(0)} + \mS^\adj (\Lambda^{(0)} + \update{\Lambda}{0}) - \lambda \mS^\adj \Lambda^{(0)} \\
   &= \Theta^{(0)} + \mS^\adj \update{\Theta}{0} - \lambda \mS^\adj \Lambda^{(0)}
\end{align}
where the final equality follows from \cref{thm:grad-proj-is-adapter}.
Note that in general, we do not have that $\Theta^{(0)} = \mS^\adj \Lambda^{(0)}$, which means the optimizer trajectories may diverge.

An alternative interpretation for the above is that weight decay can be seen (roughly) as placing a Gaussian prior on the trainable parameters, but since the set of trainable parameters is different under each view, the equivalence does not immediately hold.
We note that it is not outright clear if one implementation of weight decay is superior, so further research is required in this regard.
In our experiments, for simplicity, we leave the application of weight decay untouched, i.e., we (implicitly) use the implementation of weight decay that naturally arises from the adapter or gradient transformation views.

\paragraph{Maintaining the equivalence.} If one truly cares about preserving the optimizer trajectory even when training with weight decay, practically all one has to do is adjust the application of the weight decay so that it reflects the behavior of weight decay in the gradient transformation view or in the linear adapter view.
Adjusting the linear adapter weight decay application to match the gradient transformation weight decay application is fairly simple: one just has to compute the effective weight at every timestep, as done above, and decay the frozen base weights directly.
The converse is possible but slightly trickier, since the application of weight decay in the linear adapter view requires one to know what $\Lambda^{(t)}$ is, which may require the introduction of additional optimizer state. For example, one could store $\Theta^{(0)}$ and solve\footnote{Note that since the right hand side lies in $\range(\mS^\adj)$, this linear system will have a solution.} $\mS^\adj \Lambda_\text{effective}^{(t)} = \Theta^{(t)} - \Theta^{(0)}$ on every optimizer state,\footnote{ In the distributed case, this might not incur any additional cost, since $\Theta^{(0)}$ would already need to be stored. But this would require solving a linear system on every iteration.} or one could store and continually update $\Lambda_\text{effective}$ as part of the optimizer state.

\section{Experimental Setup}
\label{sec:architectural-details}

The details of the two architectures we consider are shown in \cref{tab:architecture-details}.
We include discussion on some additional details below.

\paragraph{Gradient accumulation.} The experimental setup in the original GaLore paper  did not perform gradient accumulation, which meant that the maximum sequence length had to be short enough (e.g., 256) such that a single batch could contain a large-enough number of sequences for accurate gradient estimation.  Our experiments are instead conducted in the  standard setting where we assume gradients  are accumulated across multiple microbatches.
In this case, the reparameterization of GaLore as LoRA has the additional benefit of straightforwardly allowing for gradient accumulation in the lower-dimensional space.  Concretely, the most straightforward implementation of GaLore\footnote{E.g., the official implementation in \url{https://github.com/jiaweizzhao/GaLore}.} will lead to gradient accumulation in the original parameter space, which would consume substantially more memory. In contrast, in the LoRA formulation the gradients of $\mA$ are accumulated \emph{after} applying the gradient transformation, providing substantial memory savings without {any} additional code.\footnote{One can implement this optimization in the GaLore form, but this requires additional code. The issue is that most deep learning frameworks will compute the gradients of a parameterized function, and the user then separately passes these as input to an optimizer. If gradients are only transformed in the optimizer, then the automatic differentiation module cannot figure out that only the smaller transformed gradient is needed, and not the full gradient.
We suspect that a sufficiently good compiler should in principle recover this optimization if one ensures that entire training steps (viz., all the gradient accumulation steps and optimizer step) are compiled jointly.}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
                   & \tinyB & \largeB \\ \midrule
Layers             & 12   & 24   \\
Heads              & 16   & 16   \\
Embed.\ dim.        & 1024 & 2048 \\
Intermediate dim. & 2816 & 5472 \\
Head dim.          & 64   & 128   \\
Query groups       & 16    & 16   \\
Batch size         & 0.5M & 1M   \\ \midrule
Warmup tokens      & 0.5B & 1B   \\
Total tokens       & 5B   & 10B \\
\bottomrule
\end{tabular}
\caption{Description of the two architectural settings we consider for our experiments: a \tinyB setting which we conduct most of our analyses and ablations on, and a \largeB setting which we use to evaluate our techniques in more realistic, large-scale setting.\vcram{-2mm} }
\label{tab:architecture-details}
\end{table}


\section{Additional results}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{latex/images/projection_both.pdf}
    \caption{Full results for gradient reconstruction error (i.e., $\Vert \cot{\Theta} - \mS^\top \mS \cot{\Theta}\Vert^2$) (left) and cosine similarity  (i.e., $\cos(\cot{\Theta}, \mS^\top \mS \cot \Theta)$) of the various transformations across training steps with the \tinyB model. The projections with lowest reconstruction error (measured either by L2 error or cosine similarity with the unprojected stochastic gradient) do not give the best downstream performance (see \cref{tab:full-results}).}
    \label{fig:reconstruction_figure_full}
\end{figure}
