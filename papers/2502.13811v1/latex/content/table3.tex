\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{llSS}
\toprule
Method & Projection Init.   &                                    {200M}      & {1B}        \\ \midrule
Dist. Training (DiLoCo) & $-$                                     & 18.00018958 & 12.7678594  \\ 
Dist.\ ReLoRA (LTE) & $-$                                     & 20.97318755 &  13.71558336 \\ 
 \midrule
Identical Random   & $\mP_i = \mP_j$ & 21.50739451 & 14.27987079 \\
Independent Random   & $\expect[ \mP_i \mP_j^\top] = \mathbf{0}$ & 20.11480801 & 13.65508717 \\
Distributed Random & $\mP_i  \mP_j^\top = \mathbf{0}$  & 19.81358752 & 13.51449892 \\
\bottomrule
\end{tabular}
\vcram{-2mm}
\caption{Results of the distributed training experiments, where four workers are trained independently and synchronized every 500 steps, following DiLoCo~\citep{diloco}. We use random semi-orthogonal matrices for the distributed (one-sided) LoRA experiments. For the (re)initializations of worker-specific projections $\{\mP_k\}_{k=1}^K$, \emph{identical} shares the projection matrix across workers, \emph{independent} initializes each worker's projection independently, and \emph{distributed} initializes the worker projections such that they are all orthogonal to one another. The top two rows are our baselines, viz., DiLoCo and a distributed variant of ReLoRA, which is similar to LTE~\citep{lte}.}
\vcram{-7mm}
\label{tab:result-distributed}
\end{table}
