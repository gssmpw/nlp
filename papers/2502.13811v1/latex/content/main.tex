\section{Introduction}

Training neural networks, in particular large language models (LLMs), can be extremely memory-intensive.
Standard approaches for LLM training use gradient accumulation across multiple batches and optimizers such as Adam~\citep{adam}, which maintains estimates of the first and second moments of the (stochastic) gradient. 
The amount of GPU memory needed for standard training is then roughly four times the amount of memory needed to store the model (assuming  the gradients/optimization states are kept in the same precision as the model parameters).



In response, a wealth of literature has developed around  memory-efficient training methods.
Most of these fall into one of two families.
The first involves modifications to the model parameterization, in particular by introducing ``adapters'' to the model architecture that have a small number of additional parameters, and only tuning those adapters \citep{houlsby2019parameter, li-liang-2021-prefix,lora}.  
Adapters such as LoRA increase the total number of  parameters but reduce the number of trainable parameters, resulting in overall memory savings.
While LoRA was originally introduced for memory-efficient \emph{finetuning}, recent works such as ReLoRA~\citep{relora}, LoRA-the-Explorer~\citep[LTE;][]{lte}, and Flora~\citep{hao2024flora} find that LoRA can even enable memory-efficient \emph{pretraining} if the adapters are periodically merged back into the full model (and then reinitialized).


The second family of methods involves more direct changes to the optimization strategy, either by designing optimizers that store fewer extra bits of information per parameter~\citep{anil2019memory, adafactor}, or (broadly) compressing the gradients, e.g., via quantization~\citep{signsgd, adam8bit,li2024memory} or low-rank approximations~\citep{gooneratne2020low,huang2023low}. 
For LLMs, GaLore~\citep{galore} has recently emerged as a promising gradient compression approach for memory-efficient pretraining. GaLore transforms the gradient matrices of linear layers via projections derived from an SVD of the gradient matrix, and then performs optimization in this projected space. 

Is there a relationship between methods that directly transform/compress gradients, and adapter-based methods that reparameterize the underlying model into frozen and trainable components? In the case of GaLore and LoRA, recent works find that the answer is \emph{yes}, in particular showing that training a LoRA adapter with one side frozen can be seen  as a form of gradient compression where the gradient matrices are {sketched} to a lower dimensional space with random matrices~\citep{hao2024flora} or through SVD-based projections~\citep{loeschckeloqt}. 




In this work, we show that the  connection between GaLore and  LoRA is more general by proving that training a neural network by applying an \emph{arbitrary linear transformation} to the gradient vector is equivalent (in the sense that the resulting models are the same and have the same optimization trajectory) to reparameterizing the neural network through a \emph{linear adapter} that additively modifies the original parameters, and then only training the adapter. 
When applied to (vectorized) matrices with a particular Kronecker factorization of the linear map, our results recover the equivalence between GaLore and one-sided LoRA.  

Our empirical experiments study this  connection between linear gradient transformations and adapter-based reparameterizations  in the context of memory-efficient LLM training.
First, we perform a comparison across gradient projection-based and LoRA-based approaches for memory-efficient training and find that randomly sketching gradients works particularly well. 
We  then exploit the adapter view of  projected-gradient training\footnote{Note that this notion of performing gradient descent with projections of the gradient is distinct from \emph{projected gradient descent} (PGD) from the optimization literature.} by developing a QLoRA-style \citep{qlora} approach to GaLore-style training. We next  show that the gradient projection view of LoRA adapters can improve  distributed training of LLMs with parallel LoRA adapters \citep{lte} by suggesting an initialization scheme of worker-specific LoRA adapters  tailored for distributed training. These results collectively demonstrate that this \textit{duality} between linear gradient transformations and 
adapter-based reparameterizations is a productive lens with which to view neural network
optimization, since it unifies several existing approaches and suggests new techniques for improving training efficiency and performance.






















\vcram{-2mm}
\section{Background}
\vcram{-1mm}

\subsection{Memory Characteristics of LLM Training}
\vcram{-1mm}
\label{sec:memory-characteristics}

LLM training makes use of accelerators like GPUs, which requires  storing important data in rapidly accessible, on-device memory.\footnote{While offloading to CPU is theoretically possible, bandwidth limitations often make this infeasible in practice.}
The bulk of this memory consumption can be broken down into four main categories.

\noindent\textbf{Model parameters.} 
We must keep the model's parameters in memory, since these are used in various stages of the training process (e.g., to compute gradients). 
Here, it is useful to distinguish \emph{trainable} parameters (which get updated regularly during training) from \emph{non-trainable} parameters, which are not updated during training but may still be used in gradient computation. 


    
\noindent\textbf{Gradients.} 
LLMs are trained using (variants of) stochastic gradient descent, which requires an estimate of the gradient of the loss function with respect to each trainable parameter. 
Standard LLM training uses a large number of samples to estimate the gradient, which necessitates gradient accumulation across multiple mini-batches of data.\footnote{While there are methods that perform an optimizer step as soon as a gradient is estimated (thus eliminating the need to allocate memory for gradient accumulation; \citealp[e,g., LOMO,][]{lomo}), this is not standard in LLM training since it can place restrictions on sequence length: for example GaLore with LOMO only trains on 256-length sequences. We thus train with gradient accumulation in the present work.}




\noindent\textbf{Optimizer states.}
In addition to the gradient itself, most optimizers used in LLM training persist other state across steps. Adam~\citep{adam} and AdamW~\citep{adamw} maintain running averages of the gradient and the gradient squared (i.e., an estimate of first- and second-order  moments), which require two floats per trainable parameter. 
Examples of techniques that reduce optimizer memory include 8-bit Adam~\citep{adam8bit}, which stores Adam states in lower precision, and AdaFactor~\citep{adafactor}, which modifies Adam to use fewer floats per parameter.
    
\noindent\textbf{Activations.}
LLM gradients are almost always obtained using reverse-mode automatic differentiation~\citep{griewank}. 
This consists of building a description of the LLM during a forward pass, in terms of a computation graph of its operations, and storing all (possibly intermediate) results required to subsequently compute the gradients of the neural network. 
The simplest way to reduce activation memory is by breaking batches into smaller microbatches and performing more gradient accumulation steps. 
Other techniques include gradient checkpointing~\citep{chen2016training,checkmate}, which trades off compute for activation memory by recomputing quantities during the backward pass, and random projections~\citep{bershatsky,wta-rcs}, which produce stochastic estimators of gradients based on sketched activations. 


This work is mostly concerned with training LLMs in memory-constrained regimes where the model, optimizer, and gradient memory dominate, since activation storage can be made small by, e.g., reducing the (micro) batch size.

\vcram{-1mm}
\subsection{LoRA and GaLore}
\vcram{-1mm}
This paper centers mainly around two memory-efficient training techniques: low-rank adapters~\citep[LoRA;][]{lora} and gradient low-rank projections~\citep[GaLore;][]{galore}.
LoRA reparameterizes the model's linear layers as $\mY = (\mW + \mA\mB)\mX$, where $\mW$ is the model's original weight matrix and $\mA,\mB$ are  matrices such that $\operatorname{rank}(\mA\mB) < \operatorname{rank}(\mW)$. 
$\mW$ remains frozen and only $\mA,\mB$ are optimized; thus, while the {total} number of model parameters is {increased}, the number of {trainable parameters} is {decreased}, which can lead to memory savings. Recent works obtain even further memory savings by working with a compressed version of $\mW$~\citep{qlora,guo2024lq,li2023loftq}.
While LoRA was originally proposed in the context of memory-efficient {finetuning}, ReLoRA~\citep{relora}, LoRA-the-Explorer~\citep{lte}, and Flora~\citep{hao2024flora} show that by periodically merging the low-rank components with the full weights and reinitializing them, LoRA can enable reasonably performant memory-efficient {pretraining} from scratch. 

GaLore provides an alternative approach to memory-efficient pretraining. 
Instead of reparameterizing the {weights} to be a combination of full-rank and low-rank matrices---which increases the number of model parameters---GaLore performs a low-rank compression  of the \emph{gradient} \emph{matrix}  $\cot{\mW}$ instead.
The optimizer update is performed in this lower dimensional space, and the updated parameters are uncompressed back into the full parameter space before the next forward pass.
Specifically, given a gradient matrix $\cot{\mW} \in \R^{m \times n}$, GaLore uses a matrix $\mP \in \R^{k \times m}$ (with $k < m$) to transform the gradient via $\mP \cot{\mW} \in \R^{k \times n}$, feeds this compressed gradient into a regular optimizer to obtain a pseudo-parameter update $\Delta \in \R^{k \times n}$, and then updates the original parameters via  $\mP^\adj {\Delta}$.
In practice, $\mP$  is given by the top singular vectors of $\cot{\mW}$, where
in order to amortize the cost of SVD, $\mP$ is updated only every so often.
As with LoRA, GaLore reduces the memory needed to store the optimizer states, since optimization happens in the lower dimensional space.




\vcram{-2mm}
\section{Duality between Linear Gradient Transformations and Adapters}
\vcram{-2mm}
\label{sec:theory}

In this section, we prove that training a neural network using  linear transformations of the gradient is equivalent to reparameterizing the neural network using specific linear adapters.
We  begin with the general case, where all parameters are treated as arbitrary vectors~(\cref{thm:grad-proj-is-adapter}).
We then show how applying a Kronecker-factored linear transformation to the gradients of linear layers of the network is equivalent to training the model with a version of LoRA which inserts a trainable square matrix between the LoRA matrices (\cref{thm:kron-factored-proj-is-mora}).
From this, we further show that specializing to a specific choice of Kronecker-factored transformation establishes an equivalence between GaLore~\citep{galore} and ``one-sided'' LoRA~\citep{lora} where one of the LoRA adapters is initialized in a particular way and remains frozen (\cref{thm:galore-is-lora}); this recovers the equivalence established in recent work \citep{hao2024flora,loeschckeloqt}.



\vcram{-1mm}
\subsection{General Case}
\vcram{-1mm}
Let $f(\mX; \Theta)$ be a neural network over input $\mX$ with trainable parameters $\Theta \in \mathbb{R}^{d}$, and further let $\cot \Theta^{} \in \mathbb{R}^{d}$ be the gradient of some differentiable loss function $\Loss$ of the network $f$ with respect to $\Theta$, computed on a random data minibatch.
We use the superscript $\left(\cdot\right)^{(t)}$ to specify a particular quantity's value after $t$ optimizer steps, e.g.,  $\Theta^{(t)}$ are the network's parameters after $t$ optimizer steps.
We are also interested in the \textit{optimization trajectory} of a model   $(\Theta^{(0)}, \Theta^{(1)}, \ldots, \Theta^{(t)})$.
Our  results show that two optimization trajectories---one from training with gradient projections, and one from training with adapters---are equivalent.

Typical approaches to neural network optimization use optimizers that maintain a state $\optimState{\Theta}{t}$ and obtain $\Theta^{(t+1)}$ via,
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
   (\update{\Theta}{t}, \optimState{\Theta}{t + 1}) &= \optimizer( \cot{\Theta}^{(t)}, \optimState{\Theta}{t}) \\
   \Theta^{(t+1)} &= \Theta^{(t)} + \update{\Theta}{t}.
    \label{eq:optim-definition}
\end{align}
For example, Adam\footnote{We omit bias correction for simplicity; one can easily handle it by adding the current timestep to our optimizer state. This change would also allow us to add learning rate schedules.}~\citep{adam} maintains first- and second-moment estimates of the gradient entries in its state $\optimState{\Theta}{t} = (\vmu_\Theta^{(t)}, \vnu_\Theta^{(t)})$, and the optimizer update is:
\setlength{\abovedisplayskip}{\baselineskip}
\setlength{\belowdisplayskip}{\baselineskip}
\[
\begin{array}{r@{\;}l}
    \text{\footnotesize \( \vmu_{\Theta, i}^{(t + 1)} \)}&\text{\footnotesize \(= (1 - \beta_1) \cot{\Theta}_i^{(t)} + \beta_1 \vmu_{\Theta, i}^{(t)} \) }\\[1pt]
    \text{\footnotesize \( \vnu_{\Theta, i}^{(t + 1)} \)}& \text{\footnotesize \(= (1 - \beta_2) (\cot{\Theta}_i^{(t)})^2 + \beta_2 \vnu_{\Theta, i}^{(t)} \) }
\end{array}
\;
\vcenter{\hbox{\( \text{\footnotesize \( \update{\Theta, i}{t} = - \gamma \frac{\mu_{\Theta, i}^{(t + 1)}}{\sqrt{\nu_{\Theta, i}^{(t + 1)}} + \epsilon} \) } \)}}
\]
where $\gamma \in \R^+$ is the learning rate, $\beta_1, \beta_2 \in [0, 1)$ control the exponential moving averages of the gradient moments, and $\epsilon$ is present for numerical stability.
In this case, the dimensionality of the optimizer states is proportional to the dimensionality of our gradient estimate $\dim(\cot{\Theta}^{(t)}) = d$, i.e., 
$\update{\Theta}{t + 1} \in \R^{d}, \optimState{\Theta}{t} \in \R^{2d}$.

\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
Now consider optimizing $\Theta$ with \emph{linearly transformed gradient dynamics}, where the gradient $\cot \Theta$ is mapped to an $r$-dimensional space by a matrix $\mS \in \R^{r \times d}$. 
In this case, we can use the transpose of the linear map to go back into the original parameter space resulting in the following update:
\begin{align*}
   (\update{\linearMap{S}\Theta}{t}, \optimState{\linearMap{S}\Theta}{t+1}) &= \optimizer(\linearMap{S}\cot{\Theta}^{(t)}, \optimState{\linearMap{S}\Theta}{t}) \\
    \Theta^{(t+1)} &= \Theta^{(t)} + \linearMap{S}^{\adj}\update{\linearMap{S}\Theta}{t},
\end{align*}
where we have used the subscript $\linearMap{S}\Theta$ to emphasize the fact that the optimizer is now operating on a different space, i.e., as if we were optimizing on $\R^r$, instead of the original parameter space, $\R^d$.
For example, if we were using Adam as our optimizer, then this change would cause the dimensionality of the optimizer update and states to be proportional to $r$ instead of $d$, viz., $\update{\linearMap{S}\Theta}{t + 1} \in \R^{r}, \optimState{\linearMap{S}\Theta}{t} \in \R^{2r}$.

Let us further consider a \emph{reparameterization} of the neural network parameters as  $f(\mX; \Theta^{} + \linearMap{S}^\adj \Lambda)$ with $\Lambda \in \mathbb{R}^{r}$. Specifically, suppose that we keep $\Theta^{}$ and $\mathbf{S}$ fixed, and only optimize $\Lambda$, resulting in the following update:
\begin{subequations}
\label{eq:reparameterized-optim-definition}
\begin{align}
   (\update{\Lambda}{t}, \optimState{\Lambda}{t+1}) &= \optimizer( \cot{\Lambda}^{(t)}, \optimState{\Lambda}{t}) \\
    \Lambda^{(t+1)} &= \Lambda^{(t)} + \update{\Lambda}{t}.
\end{align}
\end{subequations}
Because the above is adapting a neural network  indirectly via another vector $\Lambda$ that is linearly mapped to the original parameter space, we refer to this as using a \emph{linear adapter}, akin to the usage of ``adapter'' in the parameter-efficient finetuning literature \citep{houlsby2019parameter,lora}.
Since $\mS^\adj \in \R^{d \times r}$,  we have $\dim(\update{\Lambda}{t}) = \dim(\update{\linearMap{S}\Theta}{t})$ and $\dim(\optimState{\Lambda}{t}) = \dim(\optimState{\linearMap{S} \Theta}{t})$, i.e., 
the output and states of our optimizers have the exact same dimension in both cases.
This is not a coincidence: we now show that optimizing this linear adapter when $\Lambda$ is initialized to $\vzero$  is 
\emph{equivalent} to optimizing $\Theta$ in the original neural network with linearly transformed gradient dynamics. (All proofs are in \cref{sec:proofs}).
\setlength{\abovedisplayskip}{\baselineskip}
\setlength{\belowdisplayskip}{\baselineskip}


\begin{restatable}[Equivalence of gradient transformations and linear adapters]{theorem}{gradProjIsAdapter}
 Suppose we are given initial parameters $\Theta^{(0)}$ and state $\xi^{(0)}_{\linearMap{S}\Theta}$.
Let $\Theta^{(t)}$ be the parameters  after $t$ update steps with the linearly transformed gradient dynamics with $\linearMap{S}$. 
Now consider a linear adapter which reparameterizes the model as $\Theta^{(0)} + \mS^\top \Lambda^{}$, where $\Lambda^{(0)}$ is initialized to $\vzero$ and the optimizer state $\optimState{\Lambda}{0}$ is initialized to $\optimState{\linearMap{S}\Theta}{0}$, and only $\Lambda$ is optimized.
Then we have $\Theta^{(t)} = \Theta^{(0)} + \linearMap{S}^{\adj}\Lambda^{(t)}$ for all $t$, i.e., the optimization trajectories are equivalent.
\label{thm:grad-proj-is-adapter}
\end{restatable}

\begin{remark}
The above only requires that the reparameterized model is equivalent to the original model at initialization, and can therefore be straightforwardly extended to cases where the adapter is not initialized to $\mathbf{0}$, as long as we have $\Theta^{(0)} = \tilde{\Theta} + \mathbf{S}^\top  \Lambda^{(0)}$ for some $\tilde{\Theta}$ and $\Lambda^{(0)}$.
\end{remark}

\begin{remark}
The above theorem holds for any optimizer of the form in \cref{eq:optim-definition}, e.g, Adam~\citep{adam}.
Notably, AdamW~\citep{adamw} does not fit this definition due to the way that weight decay is applied.
See \cref{app:weight-decay} for a discussion about weight decay, and what adjustments are required to preserve the equivalence. 
\end{remark}
\vcram{-2mm}
\subsection{Kronecker-factored Gradient Transformations}
\label{sec:special-cases}
\vcram{-1mm}
The formulation in \cref{thm:grad-proj-is-adapter} assumes very little about the neural network being trained and the gradient transformation (or, equivalently, linear adapter) being applied, which makes it difficult to enable practical memory savings. Concretely, consider applying an arbitrary linear transformation to just a single linear layer of a neural network with parameters $\mW \in \R^{m \times n}$, i.e., $f(\mX ; \Theta) = \mW \mX$. In this case we have $\Theta = \vec{\mW} \in \R^{mn}$, and thus arbitrary linear maps of the form $\linearMap{S} \in \R^{r \times mn}$ require $O(mnr)$ memory to store.
This cost is already non-trivial for a single linear layer of moderate size, and becomes rapidly intractable if we consider applying gradient transformations to the entirety of a model's parameters.
As such, practical applications need to consider matrices $\mS$ that are efficient to store in memory (and also efficient to apply to $\Theta$). 

To this end, we consider \emph{Kronecker-factored} linear maps of the form $\linearMap{S} = \mR^\adj \otimes \mL$ where $\mL \in \R^{d_L \times m}, \mR \in \R^{n \times d_R}, d_L d_R = r$. This particular parameterization of $\mS$ reduces the memory requirement to $O(d_Lm + nd_R)$ and FLOPs to $\min\{O(d_L m n + d_L n d_R), \, O(m n d_R + d_L m d_R)\}$ (since $\mS\cot{\Theta} = \vec{\mL \cot{\mW} \mR}$), which can be memory-efficient if $d_L, d_R$ are small enough. 
We now show applying \cref{thm:grad-proj-is-adapter} to such an $\linearMap{S}$ establishes an equivalence between training with gradients transformed by ${\mL \cot{\mW} \mR}$, and  reparameterizing the linear layer as $\mW + \mL^\adj \mA \mR^\adj$ and only training $\mA \in \R^{d_L \times d_R}$.


\begin{restatable}[Kronecker-factored parameterization of the linear map]{proposition}{kronFacProjIsMora}
     Let $\mW^{} \in \R^{m \times n}$ be the parameter matrix of a linear layer with corresponding gradient matrix $\cot{\mW} \in \R^{m \times n}$. Further let $\Theta = \vec{\mW}$ and $\cot{\Theta} = \vec{\cot{\mW}}$. Consider training $\Theta$ as above with $\linearMap{S} = \mR^\adj \otimes \mL$, i.e., by transforming the gradient matrix via $\mL \cot{\mW}\mR$.
Then the optimizer trajectory of $\mW$ is equivalent to reparameterizing the model as
$
\mW = \mW^{(0)} + \mL^\adj \mA \mR^\adj,
$
and then just training $\mA$ (after initializing $\mA^{(0)} = \vzero$).
\label{thm:kron-factored-proj-is-mora}
\end{restatable}
\begin{remark}
\Cref{thm:kron-factored-proj-is-mora} shows that MoRA~\citep{mora}, LoRA-XS~\citep{balazy2024lora}, and PMSS \cite{wang2024pmss}, which are recent approaches to parameter-efficient finetuning which reparameterize a linear layer as $\mW + \mB \mA \mC$ and only train $\mathbf{A}$, can  be interpreted as training the model with linearly-transformed gradients where the linear transformation has a Kronecker factorization.
\end{remark}

Finally, as a simple corollary we now show that one can set $\linearMap{S}$ in a way that recovers GaLore, which in reparameterized form corresponds to one-sided LoRA, i.e., fixing one of the adapter matrices and only the training the other.


\input{latex/content/table1}


\begin{restatable}[GaLore is one-sided LoRA]{corollary}{galoreIsLora}
Let $\mW^{} \in \R^{m \times n}$ be the parameter matrix of a linear layer with corresponding gradient matrix $\cot{\mW} \in \R^{m \times n}$.
Without loss of generality, assume $m \le n$. 
Now consider training $\mW$ with $\optimizer$ using GaLore, i.e., where we linearly transform the gradient matrix with a matrix $\mP$
and then apply our optimizer on it, before transforming our update back to parameter space via $\mP^\adj$, viz.,
\begin{align*}
   (\update{\mW}{t}, \optimState{\mW}{t+1}) &= \optimizer(\vec{\mP {\cot{\mW}}^{(t)}}, \optimState{\mW}{t}) \\
   \mW^{(t+1)} &= \mW^{(t)} + \mP^{\adj}\vecinv{\update{\mW}{t}}
\end{align*}
where $\mP$ is an arbitrary matrix of size $\R^{d \times m}$ and $d \le m$ controls the dimensionality of the transformation.
Then the optimizer trajectory of this network is equivalent to a network trained with the reparameterization $\mW = \mW^{(0)} + \mP^\adj \mA$,
where only $\mA$ is learned.
\label{thm:galore-is-lora}
\end{restatable}
\vcram{-1mm}
\begin{remark}
The original GaLore work advocates for swapping out the gradient transformation every 200 optimizer steps. 
This does not break the equivalence in \cref{thm:galore-is-lora}.
In the adapter formulation, recomputing the gradient transformation corresponds to merging the learned adapter into the frozen weights, updating the frozen part of the adapter, and resetting the learned part to zero.
This effectively amounts to ReLoRA~\citep{relora}, where one side of the adapter is kept frozen throughout training.
\end{remark}


While \cref{thm:kron-factored-proj-is-mora,thm:galore-is-lora} focus on the case of a single linear layer, it is straightforward to generalize them to multiple linear layers.
For example, one could treat the parameters of all layers as a single vector living in the product space of the individual layers' parameter spaces, and define the gradient transformation map $\linearMap{S}$ on that space as applying the correct projection to each of the layer's parameters individually.
In practice, this can be implemented by modifying the optimizer step to apply a separate linear transformation to each layer.


Finally, we note that
 \citet{hao2024flora} and \citet{loeschckeloqt} also show that  training LoRA adapters with one side frozen with ordinary SGD is equivalent to applying a linear transformation to the gradient matrix, as in \cref{thm:galore-is-lora}.  Our \cref{thm:grad-proj-is-adapter} can be thus be seen as a generalization of these recent results, where we show that this equivalence generalizes to arbitrary parameters of the neural network and  other types of stateful optimizers.





\section{Empirical Study}


The equivalences in \cref{sec:theory} are agnostic to the choice of left and right transformations in $\mS = \mR^\adj \kron \mL$.
However, one might expect that the choice of $\mL$ and $\mR$ should matter for downstream performance.
Hence, in the following sections, we first explore how the choice of  $\linearMap{S}$ affects pretraining\footnote{We target the pretraining setting as the gap between ordinary training and memory-efficient training methods is typically larger in pretraining than it is in finetuning.} performance, and how by viewing gradient transformations as adapters, we further improve  memory efficiency by combining the technique with QLoRA-style \citep{qlora} training (\cref{sec:applications-galore-to-lora}).
We then show how the converse is also useful: by viewing LoRA adapters through the lens of gradient transformations, we can improve distributed training of LoRA adapters by coordinating the LoRA adapter initialization across different workers (\cref{sec:applications-lora-to-galore}).



\noindent\textbf{Experimental setup.}
We consider two moderate-scale language modeling settings: a \tinyB setting (training on 5B tokens) and a \largeB setting (training on 10B tokens).\footnote{While this is not large by modern standards, due to our limited compute resources this is the largest setting at which we can feasibly perform experiments.}
We use a Transformer++~\citep{llama} architecture and train on the SlimPajama~\citep{slimpajama} dataset, tokenized using the Llama-2~\citep{llama2} tokenizer, using sequences of length 2048.
All numbers we report are perplexity on a disjoint (validation) set of SlimPajama.
We use AdamW~\citep{adamw} with weight decay $0.1$, $\beta_1 = 0.9$ and $\beta_2 = 0.95$.
We  warm up the learning rate to $4 \times 10^{-4}$, before decaying it via a cosine decay schedule to
$1 \times 10^{-4}$.
We conduct all training in \texttt{bfloat16} precision.
See \cref{sec:architectural-details} for more details.


\subsection{Study 1: Memory-Efficient Pretraining}

\label{sec:applications-galore-to-lora}



The discussion in \cref{sec:theory} establishes a direct link between GaLore and one-sided LoRA. 
But how should we set $\mS$ in practice?
From the perspective of accurate gradient estimation, it would  be ideal to have $\mS^{\adj}\mS \approx \mI_{}$, since in the vanilla SGD case this would be equivalent to performing SGD with \emph{sketched gradients}, where $\mS^{\adj}\mS\cot{\Theta} \approx \cot{\Theta}$~\citep{murray-rand-nla}. For the GaLore case with $\mS = \mI \otimes \mP$, this amounts to setting $\mP$ such that $\mP\mP^\adj \approx \mI$, which could be achieved by, e.g., using random sketching matrices with the property $\mathbb{E}[\mP\mP^\top] = \mathbf{I}$.\footnote{This sketching view of LoRA  provides a possible perspective on why one-sided LoRA finetuning works well in practice \citep{zhang2023lora,zhu2024asymmetry,hayou2024lora}.} As noted by \citet{hao2024flora}, using a random sketching matrix can enable further savings as only the seed needs to be persisted across optimization steps. We thus experiment with a variety of sketching matrices for LoRA-based pretraining as shown in  \cref{tab:full-results}.



Another benefit of the adapter parameterization of gradient projections is that it allows us to be more memory efficient by quantizing the base weights as done in QLoRA~\citep{qlora}. Specifically, given the adapter parameterization $\Theta + \mS^\top \Lambda$ we can quantize $\Theta$ and only train $\Lambda$, thus enabling further memory savings. Finally, the adapter parameterization has the additional benefit of reducing the number of trainable parameters being registered for automatic differentiation, which allows for gradient accumulation to happen in a lower dimensional space. (See  \cref{sec:architectural-details} for more discussion.)





 

\input{latex/content/table2}










 





\noindent\textbf{Results.}
The results are shown in \cref{tab:full-results}, where we follow the original GaLore paper and use a rank of $256$ for the \tinyB model and a rank of $512$ for the \largeB model,\footnote{The only exceptions are for the double-sided methods. For the two-sided Gaussian,  we set the rank as to match the number of trainable parameters in the one-sided Gaussian approach. For the two-sided SVD, we use the same rank as in two-sided Gaussian, which incurs more memory since the projection matrices must be persisted across optimization steps.} and further merge the  adapters into the full weights and reinitialize them every 200 steps. 
We see that one-sided transformations, regardless of their nature, perform somewhat similarly at both \tinyB and \largeB scale, suggesting that  using a random gradient transformation matrix may be a more economical alternative to using the top singular vectors derived from the gradient as in GaLore.
We also find that ReLoRA performs comparably to one-sided gradient transformations, suggesting that the additional flexibility of ReLoRA (i.e., optimizing two sides of a LoRA adapter instead of only one side) is not necessary.
Interestingly, using two-sided Gaussian gradient transformations, which are similar in spirit to recent approaches for memory-efficient finetuning~\citep{mora,balazy2024lora,wang2024pmss}, degrades performance when memory consumption is matched to one-sided methods;
two sided SVD-based projections fare slightly better but still trail behind one-sided methods and incur a much larger memory cost, since two projection matrices must be persisted.
While \citet{galore} report no gap between GaLore and full pretraining, we did not find this to be true on our setup,\footnote{Which is different from theirs in many ways, e.g., we train on longer sequences with gradient accumulation using \texttt{bfloat16}.} and instead observe a non-trivial gap between regular (full) training and these memory-efficient pretraining methods.


\input{latex/content/reconstruction_figure}

When adding quantization to the base weights (where we use groups of size 256), we find that, across the board, 8-bit integer quantization can be performed without major performance degradation, whereas 4-bit NormalFloat quantization  begins to incur a penalty (4-bit integer did even worse).
Finally, we find that QGaLore~\citep{qgalore}, which quantizes the weights to \texttt{INT8} precision and trains these \texttt{INT8} weights directly using an SVD gradient transformation, underperforms QLoRA-style approach to quantized GaLore training.

\noindent\textbf{Analysis.}
We have motivated our experiments with sketching matrices from the perspective of accurate gradient compression, i.e., we use $\mS$ to compress the gradient, and then $\mS^\adj$ to decompress it.
From this compression viewpoint, one may then wonder whether different gradient transformations exhibit different reconstruction capabilities, and whether this ultimately dictates the performance of the resulting model. As shown in \cref{fig:reconstruction_figure}, we find that the gradient reconstruction error does not correlate with performance. As expected, methods that perform SVD on the gradients have low reconstruction error (since SVD explicitly minimizes a reconstruction objective), but as shown in \cref{tab:full-results}, SVD performs similarly to sketching matrices, which have higher reconstruction error. We believe that the relationship between the nature of the gradient transformation and downstream performance is fairly complex, and merits further investigation. \cref{fig:reconstruction_figure_full} (appendix) shows similar results for cosine similarity instead of squared error.





\vcram{-2mm}
\subsection{Study 2: Distributed Pretraining}
\vcram{-2mm}
\label{sec:applications-lora-to-galore}

\input{latex/content/table3}




Our second experiment targets distributed pretraining of LLMs across poorly-connected and resource-constrained workers, which is important for many applications of interest, from federated training of LLMs to scaling up LLMs across data centers that are not co-located, i.e., where techniques like FSDP are not possible. DiLoCo~\citep{diloco} is a recent effective approach that has workers train independently for some number of iterations using an inner optimizer, then uses the average change in parameters from each worker as a ``pseudo-gradient'' on an outer optimizer that updates a global copy of the parameters (i.e., as in federated learning;~\citealp{mcmahan2017communication,reddi2020adaptive}). This  approach has since been scaled up to train 10B LLMs across distributed workers \cite{opendiloco}.




However, DiLoCo still assumes that each worker has enough memory to perform a full forward/backward pass on the model, i.e., it does not target memory efficiency. A memory-efficient distributed approach that is of particular interest in light of the equivalence in \cref{sec:theory} is LoRA-the-explorer~\citep[LTE;][]{lte}, which can be seen as an extension of ReLoRA to the distributed setting. LTE has $K$ independent workers train separate LoRA adapters for a small number  of {local} steps, and then performs a {global} step by averaging the adapters across workers. The globally-averaged adapter is then merged into the base weights, and optimization continues  by resetting and traiing the worker-specific LoRA adapters.

 We will now describe how the equivalence in \cref{sec:theory} can be used to derive an improved version of LTE, which trains only one side of the LoRA adapter in each worker, but initializes the frozen side in a worker-aware manner.
Consider a one-sided analogue of LTE, where the weight $\mW^{(g, l)}_k$ for the $k$th worker after $g$ global and $l$ local updates is
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align*}
    \mW^{(g, l)}_k = \mW^{(g, 0)}_k + {\mP^{(g)}_k}^\adj \mA^{(g, l)}_k
\end{align*}
and only $\mA$ is trained. The global step is given by,
\begin{align*}
    \mW^{(g + 1, 0)}_k 
    &= \mW^{(g, 0)}_k + \frac{1}{K} \sum_{k = 1}^K {\mP^{(g)}_k}^\adj \mA^{(g, L)}_k
\end{align*}
where we have assumed that the global step is performed after $L$ local steps.
After a global step, we would also reset $\mA$ by setting $\mA_k^{(g + 1, 0)} = \vzero$ and similarly swap out $\mP_k$ for another (e.g., random) matrix for all $k$.

By \cref{thm:galore-is-lora}, local steps must correspond to training the worker weights using a gradient transformation, 
\begin{align*}
    \mW^{(g, l)}_k = \mW^{(g, 0)}_k + {\mP^{(g)}_k}^\adj \Delta_{\mP \mW}^{(g, l)},
\end{align*}
where we use $\Delta_{\mP \mW}^{(g, l)}$ to denote the optimizer update that was performed in the lower dimensional transformed space.
Further, a global step in this view can be equivalently seen as defining a global pseudo-gradient $\varDelta^{(g)}$ as the average of for the local pseudo-gradients $\{\varDelta^{(g)}_1, \ldots, \varDelta^{(g)}_K\}$,
\begin{align*}
    \varDelta^{(g)} &= \frac{1}{K} \sum_{k = 1}^K \varDelta_k^{(g)}, \,\,\,\,
    \varDelta_k^{(g)} = \mW^{(g, L)}_k - \mW^{(g, 0)}_k 
\end{align*}
The global weight update is then given by a step using the global pseudo-gradient,
\begin{align*}
    \mW^{(g + 1, 0)}_k = \mW^{(g, 0)} + \varDelta^{(g)}.
\end{align*}
This can be straightforwardly generalized to the use of different learning rates and more advanced optimizers.

\setlength{\abovedisplayskip}{\baselineskip}
\setlength{\belowdisplayskip}{\baselineskip}



One approach to initialize/reset the frozen side of worker-specific LoRA adapters (i.e., the gradient projections $\mP_{k}^{}$) is to sample a projection and broadcast it to all workers. However, the GaLore--LoRA duality suggests a different scheme. \cref{thm:grad-proj-is-adapter} shows that training with linear gradient transformations only optimizes a \emph{subspace} of the full model, namely $\range(\mS^\adj)$ where $\mS^\top = \mI \kron \mP^\top$ in the GaLore case. This suggests a different approach to distributed LoRA training, wherein the frozen part of each LoRA adapter, $\mS_1, \ldots, \mS_K$, is initialized differently, so that the sum of their ranges allows a larger subspace to be trained. 
For example, we could sample random semi-orthogonal matrices $\mP_1, \ldots, \mP_K$ uniformly at random, assign each worker $\mS_i = \mI \kron \mP^\top_i$, and they will likely each cover different portions of the space.
An even stronger strategy would be to demand that $\range(\mS^\top_1), \ldots, \range(\mS^\top_K)$ must be orthogonal, which can be realized by keeping the $\mP_i$'s as semi-orthogonal, but enforcing that $\mP_i^\top \mP_j = \mI$ (e.g., by generating a random $m \times m$ orthogonal matrix and having each worker take a different $d \times m$ submatrix.)
Intuitively, this ensures that no worker is duplicating the work of another, since their projections are pairwise orthogonal. We experiment with such \emph{identical random}, \emph{independent random}, \emph{distributed random} initialization schemes.





\input{latex/content/table4}

\noindent\textbf{Results.}
The results for the main set of experiments are shown in \cref{tab:result-distributed}. 
 We consider two baselines: (i) DiLoCo~\citep{diloco}, which has each worker training independently for $500$ steps before computing a pseudo-gradient that is used to update the global parameters using SGD with Nesterov momentum~\citep{nesterov}, and (ii) distributed ReLoRA, which is an analog of ReLoRA but adapted to train like DiLoCo, i.e., one trains the LoRA adapter for $500$ steps and defines the adapter weight as the pseudo-gradient for the Nesterov step; this is very close to LTE.\footnote{LTE can be seen as using SGD as the optimizer on the pseudo-gradients, but we found this led to worse results in preliminary experiments.} Our distributed GaLore experiments make use of random semi-orthogonal projections since the distributed random initialization for it is easy to compute, and does not add significant communication overhead.\footnote{Each worker just needs the seed used to sample the orthogonal matrix, and the indices of the rows  it will keep.}
As in \cref{sec:applications-galore-to-lora},  distributed GaLore leads to degradations at both \tinyB and \largeB scales compared to the full distributed training baseline (i.e., DiLoCo). However, our distributed random initialization scheme, where workers are ``aware'' of each, performs  well, thus demonstrating the utility of the gradient transformation--adapter duality from \cref{sec:theory}.




\noindent\textbf{Analysis.} We perform a study at the 200M scale over how the number of workers and rank affect performance. Intuitively, larger ranks lead to a larger subspace being trained by each worker (and, in the limit, we should recover something akin to DiLoCo when there is no rank reduction), so we would expect performance to improve as we increase the rank.
Indeed, the results for this ablation (shown in \cref{tab:sweep-distributed}) confirm this intuition, likely because DiLoCo benefits from more workers to get a better estimate of the pseudo-gradient for the outer optimizer step.
More surprisingly, we find that this gap is largely bridged by ensuring that different gradient transformations are assigned to each worker, with the distributed initialization once again performing the best. It would be interesting to further study how the effectiveness of the distributed initialization scheme changes as we go to more extreme settings (e.g., hundreds of extremely low-rank workers).




\vcram{-2mm}
\section{Discussion and Limitations}
\vcram{-2mm}
The preceding studies focus on two situations in which the duality between linear adapters and gradient transformations offers practical insights.
We believe there are many other avenues that merit further exploration.
For instance, \cref{thm:grad-proj-is-adapter} makes no assumptions about the structure of $\mS$; while we only considered Kronecker-factorized matrices, other linear maps that admit efficient storage and computation  would be interesting to explore.
Regardless of the structure of $\mS$, as discussed in \cref{sec:applications-galore-to-lora}, what characterizes a good $\mS$ is not clear but has a large impact. 
It may be possible to \emph{learn} a good $\mS$ with meta-learning-style approaches, which can be seen as \emph{learning an optimizer}~\citep[][\emph{i.a.}]{andrychowicz2016learning,li2016learning,wichrowska2017learned,bello2017neural}.\footnote{In the GaLore/LoRA case, learning $\mP$ in this meta-learning sense is different from learning $\mP$ in the ordinary LoRA sense, i.e., when both $\mP$ and $\mA$ are trained with gradient descent against the same loss function.} 
Finally, while we focused on linear gradient transformations, where we proved exact equivalence with a linear adapter parameterization, it may be possible to establish approximate equivalence between non-linear gradient transformations and other types of adapters.

Our work has several limitations. Due to  compute constraints, we were only able to scale our experiments to \largeB, which is small by industry standards. While our duality results are more general, our experiments primarily focus on the special case of the GaLore--LoRA duality.
We chose to focus primarily on a wide array of gradient transformations, but forgo a study of the interaction between such transformations and the choice of optimizer, projection reinitialization schedule, etc.
Ultimately, we believe that our results signal that these techniques could be applied at larger scales, especially when performing distributed training in memory-constrained regimes.

\vcram{-1mm}
\section{Related work}
\vcram{-1mm}
\noindent\textbf{Memory-efficient training.}
There is a growing body of research focused on memory-efficient LLM training. This work explores the connections among GaLore~\citep{galore}, LoRA~\citep{lora}, QLoRA~\citep{qlora}, and ReLoRA~\citep{relora}. Various approaches in low-rank adaptations have been proposed to enhance these techniques~\citep{renduchintala2023tied,sheng2023s,zhang2023lora,xia2024chain,wang2023multilora,hao2024flora,wang2024pmss}, including efforts to train models from scratch~\citep{kamalakara2022exploring,wang2023cuttlefish,zhao2023inrank}. Broadly, memory-efficient training also encompasses methods such as adapters~\citep{houlsby2019parameter,mahabadi2021parameter}, which insert trainable layers  and prompt tuning~\citep{li-liang-2021-prefix,lester-etal-2021-power}, which optimizes continuous prompts. Additionally, its combination with quantization techniques~\citep{kwon-etal-2022-alphatuning} and other methods that update subparts of the parameter vector~\citep{guo2021diff,zaken2021bitfit,NEURIPS2021_cb2653f5} are also relevant.


\noindent\textbf{Memory-reduction via randomization.}
Randomization has been used in other contexts to reduce memory consumption in automatic differentiation.
\citet{adelman} and \citet{wta-rcs} perform row/column subsampling to reduce the amount of computation and memory required to compute gradients.
\citet{bershatsky} also explores Gaussian projections, but in the context of reducing activation memory by sketching them.
\citet{randomized-ad} construct gradient estimators by computing the gradient on a subsample of the paths in the computation graph.
More tangentially, MeZO~\citep{mezo} amounts to sketching the gradient of a neural network by performing forward-mode automatic differentiation on random vectors.

\vcram{-2mm}
\section{Conclusion}
\vcram{-2mm}
We proved a general equivalence between training an LLM with linear transformations of gradients and training with additive linear adapters, and showed the GaLore--LoRA equivalence is a special case of this result.
We then used this equivalence to derive more memory-efficient and performant methods for LLM pretraining, including combinations of quantization and gradient-projection methods and improved initialization for distributed adapter pretraining.

\section*{Impact statement}
The last few years have seen widespread interest in LLMs.
Perhaps the most salient finding from the race to build the best LLMs is that increasing parameter counts in tandem with data is of paramount importance.
This makes it very hard to train competitive LLMs unless one has the best and latest hardware, which offers the most memory capacity and thus the ability to actually train these models in practice.
Our research targets exactly this setting, offering a mathematical connection between two methods at the cornerstone of memory-efficient training, and showing how this connection can lead to further improvements in memory-efficiency and distributed training. 

\section*{Acknowledgements}
We thank Shannon Zejiang Shen, Li Du, Aniruddha Nrusimha, Jeremy Bernstein, Jyothish Pari, Sami Jaghouar, and Johannes Hagemann for helpful discussions and feedback. This study was supported by MIT-IBM Watson AI Lab.


