\begin{table}[t]
\footnotesize
\centering
\vcram{-2mm}
\begin{tabular}{lSSS}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{(Rank, Workers)} \\  \cmidrule(l){2-4}
                               & { (128, 8)}  & { (256, 4)}  & { (512, 2)}  \\ \midrule
 Dist. Training (DiLoCo)                                        & 17.80935333 & 18.00018958 & 18.56305978 \\
Dist. ReLoRA (LTE)                                        & 23.76163306 & 20.97318755 & 19.53845727 \\
Identical Random     & 23.95595272 & 21.50739451 & 20.31663561 \\
Independent Random & 20.64390346 & 20.11480801 & 19.97153946 \\
Distributed Random & 20.31710547 & 19.81358752 & 19.65598032 \\
\bottomrule
\end{tabular}
\vcram{-2mm}
\caption{Results of the distributed pretraining experiments as we vary the rank of the gradient projections and number of workers. For the DiLoCo baseline, we only vary the number of workers; note that the for the distributed ReLoRA baseline (which is similar to LTE,~\citealp{lte}), we have double the number of trainable parameters as in the one-sided methods.
}
\label{tab:sweep-distributed}
\vcram{-5mm}
\end{table}
