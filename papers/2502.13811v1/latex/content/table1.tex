\begin{table*}[t]
\footnotesize
\centering
\vcram{-2mm}
\begin{tabular}{lllllclc}
\toprule
Method                 & Adapter Parameterization & Trained & Frozen & Persisted   \\ \midrule
Baseline                  & $\mW$ & $\mW$ & $-$ &   $\mW$ \\
ReLoRA \cite{relora} & $\mW + \mB\mA$ & $\mA,\mB$ &  $\mW$ &  $\mW, \mA, \mB$  &   \\                           

Gradient SVD \citep[GaLore; ][]{galore} & $\mW + \mP^\top\mA$, \,\, $\mP^\top = \operatorname{SVD}(\cot{\mW})$ & $\mA$ & $\mW,\mP$ & $\mW, \mP, \mA$                               \\
Gaussian \citep[Flora;][]{hao2024flora} &           $\mW + \mP^\top\mA$, \,\,  $\mP \sim k \mathcal{N}(\mathbf{0}, \mathbf{I})$ & $\mA$ & $\mW,\mP$ & $\mW, \mA$          \\ 
Rademacher &  $\mW + \mP^\top\mA$, \,\,  $\mP \sim k \operatorname{Unif}(\{-\mathbf{1}, \mathbf{1}\})$ & $\mA$ & $\mW,\mP$ & $\mW, \mA$                \\ 
Random Semi-orthogonal      &  $\mW + \mP^\top\mA$, \,\,  $\mP^\top \mP = k\mI$ & $\mA$ & $\mW,\mP$   & $\mW,\mP,\mA$          \\ 
Two-sided Gaussian &  $\mW + \mL^\top\mA\mR^\top$,  \,\,  $\mL, \mR \sim k \mathcal{N}(\mathbf{0},\mathbf{I})$ & $\mA$ & $\mW, \mL,\mR$ & $\mW, \mA$        \\
Two-sided Gradient SVD &  $\mW + \mL^\top\mA\mR^\top$, $\mL^\adj, \,\,   \mR^\adj = \operatorname{SVD}(\cot{\mW})$ & $\mA$ & $\mW,\mL,\mR$ & $\mW, \mL, \mR, \mA$        \\
\bottomrule
\end{tabular}
\vcram{-2mm}
\caption{A summary of methods tested for our pretraining experiments, where we list the gradient transformation method (which is not relevant for Baseline/ReLoRA) and the corresponding adapter parameterization. We also break down the reparameterized model into trained and frozen components, alongside the the set of components that need to be persisted in memory (for methods that make use of easy-to-materialize random sketching matrices, viz., Gaussian, Rademacher, one only needs to persist the seeds for the gradient transformation). Random semi-orthogonal matrices---a tall/wide  matrix whose columns/rows are orthonormal vectors---are also random but are not straightforwardly materializable from a seed, and hence may need to be persisted across optimization steps. In the Gaussian and Rademacher cases, we use $k$ as shorthand for the constant that rescales the distribution so that $\expect[\mP \mP^\adj] = \mI$.
}
\label{tab:methods}
\vcram{-4mm}
\end{table*}
