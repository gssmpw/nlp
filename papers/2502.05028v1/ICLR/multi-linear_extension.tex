Compared to discrete optimization, continuous optimization has a plethora of efficient tools and algorithmic frameworks. As a result, a common approach in discrete optimization is based on a continuous relaxation to embed the corresponding discrete problem into a solvable continuous optimization. In the subsequent section,we will present a canonical relaxation technique for submodular functions, known as \emph{multi-linear extension}~\citep{calinescu2011maximizing,chekuri2014submodular}. To better illustrate this extension, we suppose $|\V|=n$ and set $\V:=[n]=\{1,\dots,n\}$  throughout this paper. 
\begin{definition}\label{def1:multi-linear}
For a set function $f:2^{\V}\rightarrow\R_{+}$, we define its multi-linear extension  as 
	\begin{equation}
	\label{equ:multi-linea}
	F(\x)=\sum_{\mathcal{A}\subseteq\V}\Big(f(\mathcal{A})\prod_{a\in\mathcal{A}}x_{a}\prod_{a\notin\mathcal{A}}(1-x_{a})\Big)=\E_{\mathcal{R}\sim\x}\Big(f(\mathcal{R})\Big),
\end{equation} where $\x=(x_{1},\dots,x_{n})\in [0,1]^{n}$ and $\mathcal{R}\subseteq\V$ is a random set that contains each element $a\in\V$ independently with probability $x_{a}$ and excludes it with probability $1-x_{a}$. We write $\mathcal{R}\sim\x$ to denote that $\mathcal{R}\subseteq\V$ is a random set sampled according to $\x$. 
\end{definition}
From the Eq.\eqref{equ:multi-linea}, we can view multi-linear extension at any point $\x\in[0,1]^{n}$ as the expected utility of independently selecting each action $a\in\V$ with probability $x_{a}$.  With this tool, we can cast the previous discrete problem Eq.\eqref{equ:problem_t} into a continuous maximization which learns the selected probability for each action $a\in\V$, that is, for any $t\in[T]$, we consider the following continuous optimization:\vspace{-0.2em}
	\begin{equation}\label{equ:continuous_max}
		\max_{\x\in[0,1]^{n}} F_{t}(\x),\ \ \text{ s.t.}\ \  \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N,\vspace{-0.3em} 
	\end{equation}where $F_{t}(\x)$ is the multi-linear extension of $f_{t}$.
When $f_{t}$ is submodular, the maximization problem Eq.\eqref{equ:continuous_max} is both non-convex and non-concave~\citep{bian2020continuous}. Thanks to recent advancements in optimizing complex neural networks, a large body of empirical and theoretical evidence has shown that numerous gradient-based algorithms, such as projected gradient methods and Frank Wolfe, can efficiently address the general non-convex or non-concave problem. Specifically, under certain mild assumptions, many first-order gradient algorithms can converge to a stationary point of the corresponding non-convex or non-concave objective~\citep{nesterov2013introductory,lacoste2016convergence,jin2017escape,agarwal2017finding,hassani2017gradient}. Motivated by these findings, we proceed to investigate the stationary points of the multi-linear extension of submodular functions.
	\subsection{Characterizing stationary points}
	We begin with the definition of a stationary point for maximization problems.
	\begin{definition}A vector $\x\in\C$ is called a stationary point for the differentiable function $G: [0,1]^{n}\rightarrow\R_{+}$ over the domain $\C\subseteq[0,1]^{n}$ if $\max_{\y\in\C}\langle\y-\x,\nabla G(\x)\rangle\le 0$.
	\end{definition}
Stationary points are of great interest as they characterize the fixed points of a multitude of gradient-based methods. Next, we quantify the performance of the stationary points of multi-linear extension relative to the maximum value, i.e., 
	\begin{theorem}[Proof is deferred to \cref{append:proof1}]\label{thm:1} If $f:2^{\V}\rightarrow\R_{+}$ is a monotone submodular function with curvature $c$, then for any stationary point $\x$ of its multi-linear extension $F:[0,1]^{n}\rightarrow\R_{+}$ over domain $\C\subseteq[0,1]^{n}$, we have
	\begin{equation*}
			F(\x)\ge\Big(\frac{1}{1+c}\Big)\max_{\y\in\C}F(\y).
		\end{equation*}
	\end{theorem}
	%To our regret, \citet{hassani2017gradient} showed that the stationary point of some special multi-linear extension of the submodular function only can guarantee a conservative approximation to the global maxima. Generally, we can conclude that
	\begin{remark}
		The ratio $\frac{1}{1+c}$ is tight for the stationary points of the multi-linear extension of submodular function with curvature $c$, because there exists a special instance of multi-linear extension with a $(\frac{1}{2})$-approximation stationary point when $c=1$~\citep{hassani2017gradient}.
  	\end{remark}
  		\begin{wrapfigure}{r}{0.24\textwidth}
      \includegraphics[width=0.24\textwidth]{ICLR/Figure/Compare.pdf}
  		\captionsetup{font=scriptsize}
  		\caption{$\frac{1}{1+c}$ v.s. $\frac{1-e^{-c}}{c}$.}\label{figure:2}
 		\vspace{-1.0em}
  	\end{wrapfigure}
Theorem~\ref{thm:1} suggests that applying various gradient-based methods directly to multi-linear extension only can ensure a $\frac{1}{1+c}$-approximation guarantee. However, the known tight approximation ratio for maximizing a monotone submodular function with curvature $c$ is $\frac{1-e^{-c}}{c}$~\citep{vondrak2010submodularity,bian2017guarantees}. As depicted in Figure \ref{figure:2}, there exists a non-negligible gap between $\frac{1}{1+c}$ and $\frac{1-e^{-c}}{c}$. The question arises: \emph{Is it feasible to bridge this significant gap?} Recently, numerous studies have successfully leveraged a classic technique named \emph{Non-Oblivious Search}~\citep{alimonti1994new,khanna1998syntactic,filmus2012power,filmus2014monotone} to output superior solutions by constructing an effective surrogate function. Inspired by this idea, we also aspire to devise a surrogate function that can enhance the approximation guarantees for the stationary points of multi-linear extension. In line with the works~\citep{zhang2022boosting, zhang2024boosting,wan2023bandit}, we consider a type of surrogate function  $F^{s}(\x)$ whose gradient at point $\x$ assigns varying  weights to the gradient of multi-linear extension at  $z*\x$, given by $\nabla F^{s}(\x)=\int_{0}^{1} w(z)\nabla F(z*\boldsymbol{x})\mathrm{d}z$ where $w(z)$ is the positive weight function over $[0,1]$ and $*$ denotes the multiplication of scalars and vectors.
After carefully
selecting the weight function $w(z)$, we can have that:
	\begin{theorem}[Proof is deferred to \cref{append:proof2}]\label{thm:2}If the weight function $w(z)=e^{c(z-1)}$ and the function  $F:[0,1]^{n}\rightarrow\R_{+}$ is the multi-linear extension of a monotone submodular function $f:2^{\V}\rightarrow\R_{+}$ with curvature $c$, we have, for any $\x,\y\in[0,1]^{n}$,
    \begin{equation}\label{equ:boosting1}
				\langle\y-\x,\nabla F^{s}(\x)\rangle=\left\langle\y-\x,\int_{0}^{1}e^{c(z-1)}\nabla F(z*\x)\mathrm{d}z\right\rangle\ge\Big(\frac{1-e^{-c}}{c}\Big)F(\y)-F(\x).
		\end{equation}
	\end{theorem}
\begin{remark}
Theorem~\ref{thm:2} illustrate that the stationary points of surrogate function $F^{s}(\x)$ can provide a better $\Big(\frac{1-e^{-c}}{c}\Big)$-approximation than the stationary points of the original multi-linear extension $F$. 
\end{remark}
\begin{remark}
Unlike prior work on surrogate functions regarding the multi-linear extension of submodular functions~\citep{zhang2022boosting,zhang2024boosting}, Theorem~\ref{thm:2} takes into account the impact of curvature. Specifically, when the curvature $c=1$,  our result Eq.\eqref{equ:boosting1} is consistent with those  of ~\citet{zhang2022boosting,zhang2024boosting}. To the best of our knowledge, we are the first to explore the stationary points of the multi-linear extension of submodular functions with different curvatures.
\end{remark}
\subsection{Constructing an Unbiased Gradient for surrogate function}\label{sec:construct_gradient_surrogate_function}
In this subsection, we present how to estimate the gradient $\nabla F^{s}(\x)=\int_{0}^{1}e^{c(z-1)}\nabla F(z*\boldsymbol{x})\mathrm{d}z$ using the function values of $f$.
Given that  $F$ is the multi-linear extension of set function $f$, we can show $\frac{\partial F(\x)}{\partial x_{i}}=\E_{\mathcal{R}\sim\x}\Big(f(\mathcal{R}\cup\{i\})-f(\mathcal{R}\setminus\{i\})\Big)$ \citep{calinescu2011maximizing}. That is to say, the partial derivative of multi-linear extension $F$ at each variable $x_{i}$ equals the expected marginal contribution for the action $\{i\}$. Consequently, after sampling a random number $z$ from the probability distribution of r.v. $\mathcal{Z}$ where $P(\mathcal{Z}\le b)=(\frac{c}{1-e^{-c}})\int_{0}^{b}e^{c(z-1)}\mathrm{d}z=\frac{e^{c(b-1)}-e^{-c}}{1-e^{-c}}$ for any $b\in[0,1]$ and then generating a random set $\mathcal{R}$ according to $z*\x$, we can estimate $\nabla F^{s}(\x)$ by the following equation:
\vspace{-0.5em}
\begin{equation}\label{equ:gradient_surrogate}\widetilde{\nabla}F^{s}(\x)=\Big(\frac{1-e^{-c}}{c}\Big)\Big(f(\mathcal{R}\cup\{1\})-f(\mathcal{R}\setminus\{1\}),\dots,f(\mathcal{R}\cup\{n\})-f(\mathcal{R}\setminus\{n\})\Big)
\end{equation}\vspace{-1.0em}