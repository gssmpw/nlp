Given the core role of Bregman divergence in the mirror ascent method, we begin with an in-depth
review of this concept, that is, 
	\begin{definition}[Bregman Divergence]
		Let $\phi: \Omega\rightarrow\R$ is a continuously-differentiable, $1$-strongly convex function defined on a convex set  $\Omega\subseteq[0,1]^{n}$. Then the Bregman divergence with respect to $\phi$ is defined as:
  \vspace{-0.1em}
	\begin{equation}\label{equ:Bregman}
\D_{\phi}(\x,\y): =\phi(\x)-\phi(\y)-\langle\nabla\phi(\y),\x-\y\rangle.
		\end{equation}
	\end{definition}
	\vspace{-0.1em}
	Two well-known examples of Bregman divergence include the Euclidean distance, which arises from the choice of $\phi(\x)=\frac{\|\x\|_{2}^{2}}{2}$ and the Kullback-Leibler (KL) divergence, associated with $\phi(\x)=\sum_{i=1}^{n}x_{i}\log(x_{i})$. Note that both forms of $\phi(\x)$ allow for a coordinate-wise decomposition. Without loss of generality, we make the following assumption.\vspace{-0.3em}
	\begin{assumption}\label{ass:2} $\phi(\x)$ is dominated by a one-dimensional strongly convex differentiable function $g:[0,1]\rightarrow\R$, that is, $\phi(\x)=\sum_{i=1}^{n}g(x_{i})$ where $\x=(x_{1},\dots,x_{n})$.
	\end{assumption}\vspace{-0.3em}
	Under this assumption, we can re-define the Bregman divergence between two $n$-dimensional vectors $\mathbf{b}$ and $\mathbf{c}$ as: $\D_{g,n}(\mathbf{b},\mathbf{c}):=\sum_{i=1}^{n}\Big(g(b_{i})-g(c_{i})-g'(c_{i})(b_{i}-c_{i})\Big)$ where $g'$ denotes the derivative of $g$. Specially, we also have $\D_{\phi}(\x,\y)=\D_{g,n}(\x,\y)$ from Eq.\eqref{equ:Bregman}. Building on these foundations, we now introduce the Multi-Agent Online Boosting Mirror Ascent (\textbf{MA-OSMA}) algorithm for MA-OSM problem, as detailed in Algorithm~\ref{alg:BDOMA}.
	
	In Algorithm~\ref{alg:BDOMA}, at every time step $t \in [T]$, each agent $i \in\N$ maintains a local variable $\x_{t,i}\in[0,1]^{|\V|}$, which, to some extent, reflects agent $i$'s current beliefs regarding all actions in $\V$. The core of \textbf{MA-OSMA} algorithm is primarily composed of four interleaved components: Rounding, Information aggregation, Surrogate gradient estimation and Probabilities update. Specifically, at every iteration $t\in[T]$, each agent $i$ first selects an action $a_{t,i}$ from $\V_{i}$ based on its current preferences $\x_{t,i}$. Subsequently, agent $i$ receives $\x_{t,j}$ from all neighboring agents and then computes the aggregated beliefs $\y_{t,i}$ as the weighted average of $\x_{t,j}$ for $j\in\N_{i}$, where $\N_{i}$ denotes the neighbors of agent $i$. Next, agent $i$ estimates the gradient of the surrogate function of the multi-linear extension of $f_{t}$ at each coordinate $a\in\V_{i}$ by employing the methods outlined in Section~\ref{sec:construct_gradient_surrogate_function}. That is, agent $i$ initially samples a random number $z_{t,i}$ from the random variable $\mathcal{Z}$, where $P(\mathcal{Z}\le b)=\frac{e^{c(b-1)}-e^{-c}}{1-e^{-c}}$ for any $b\in[0,1]$, and then approximates $[\nabla F_{t}^{s}(\x_{t,i})]_{a}$ as $\frac{1-e^{-c}}{c} \big(f_{t}(\mathcal{R}_{t,i}\cup\{a\})-f_{t}(\mathcal{R}_{t,i}\setminus\{a\})\big)$ for any $a\in\V_{i}$ where $\mathcal{R}_{t,i}$ is a random set according to $z_{t,i}*\x_{t,i}$. Finally, each agent $i$ adjusts the probabilities of actions in $\V_{i}$ through a mirror ascent along the direction $[\widetilde{\nabla} F_{t}^{s}(\x_{t,i})]_{\V_{i}}$. As for other actions not in $\V_{i}$, 
   their probabilities are straightforwardly updated using the aggregated beliefs $\y_{t,i}$.
   
    The key novelty of Algorithm~\ref{alg:BDOMA} is twofold: first, it integrates a surrogate gradient estimation for the multi-linear extension of $f_{t}$, ensuring a tight approximation guarantee; second, it adopts a divide-and-conquer strategy to update the probabilities of all actions in Lines 12-13, which only requires agents to evaluate the marginal benefits of actions within their own action sets. These tactics not only effectively reduce the computational burden for each agent but also partially offset the practical errors caused by the limited observational capabilities of each agent.
     \begin{algorithm}[t]
		\caption{Multi-Agent Online Surrogate Mirror Ascent~(\textbf{MA-OSMA})}\label{alg:BDOMA}
		\begin{algorithmic}[1]
			\STATE{\bf Input:} Number of iterations $T$, the set of agents $\N$, communication graph $G(\N,\mathcal{E})$,
			weight matrix $\W=[w_{ij}]\in\R^{N\times N}$, $1$-strongly decomposable convex function $\phi(\x)=\sum_{i=1}^{n}g(x_{i})$, the curvature $c\in[0,1]$, step size $\eta_{t}$ for $t\in[T]$;
			\STATE {\bf Initialized:} for any agent $i\in\N$, let $[\x_{1,i}]_{j}=\frac{1}{|\V_{i}|},\ \forall j\in\V_{i}\ \text{ and }\  [\x_{1,i}]_{j}=0,\ \forall j\notin\V_{i}$
			\FOR{$t\in[T]$}
			\FOR{$i\in\N$}
			\STATE Compute $\text{SUM}:=\sum_{a\in\V_{i}}[\x_{t,i}]_{a}$\ \ \ \  \COMMENT{Rounding (Lines 5-6)}
			\STATE Select an action $a_{t,i}$ from the set $\V_{i}$ with probability $\frac{[\x_{t,i}]_{a}}{\text{SUM}}$
			\STATE Exchange $\x_{t,i}$ with each neighboring node $j\in\mathcal{N}_{i}$\ \ \ \COMMENT{Information aggregation (Lines 7-8)}
			\STATE Aggregate the information by setting $ \y_{t,i}=\sum_{j\in\mathcal{N}_{i}\cup\{i\}}w_{ij}\x_{t,j}$%\ \ \ \ \ \COMMENT{Rounding (Lines 7-9)}
			\STATE Sampling a random number $z_{t,i}$ from r.v. $\mathcal{Z}$\ \ \ \COMMENT{Surrogate gradient estimation (Lines 9-11)}
			\STATE Sampling a random set $\mathcal{R}_{t,i}\sim z_{t,i}*\x_{t,i}$
			\STATE Compute $[\widetilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a}:=\frac{1-e^{-c}}{c} \big(f_{t}(\mathcal{R}_{t,i}\cup\{a\})-f_{t}(\mathcal{R}_{t,i}\setminus\{a\})\big)$ for any $a\in\V_{i}$ 
			\STATE Update $	[\x_{t+1,i}]_{a}=[\y_{t,i}]_{a},\ \forall a\notin\V_{i}$\ \ \ \ \ \COMMENT{ Update the probabilities of actions (Lines 12-13)}
			\STATE Update the probabilities of actions of agent $i$ itself  by 
			\begin{equation}\label{equ:mirror_projection}
				[\x_{t+1,i}]_{\V_{i}}:=\mathop{\arg\min}_{\sum_{k=1}^{n_{i}}b_{k}\le1}\Bigg(-\langle[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{\V_{i}},\mathbf{b}\rangle+\frac{1}{\eta_{t}}\D_{g,n_{1}}(\mathbf{b}, [\y_{t,i}]_{\V_{i}})\Bigg),
			\end{equation} where $n_{i}=|\V_{i}|$ and $\mathbf{b}=(b_{1},\dots,b_{n_{i}})\in[0,1]^{n_{i}}$
			\ENDFOR
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}	
    \subsubsection{Regret Analysis for Algorithm~\ref{alg:BDOMA}}
     In this subsection, we present theoretical results for the proposed method \textbf{MA-OSMA}. 
    We begin by introducing some standard assumptions about the communication graph $G(\N,\mathcal{E})$, weight matrix $\mathbf{W}\in\R^{N\times N}$,  Bregman divergence $\mathcal{D}_{\phi}$ and the surrogate gradient estimation $\tilde{\nabla}F_{t}^{s}$.
	\begin{assumption}\label{ass:1}
	The graph $G$ is connected, i.e., there exists a path from any agent $i\in\N$ to any other agent $j\in\N$. Moreover, the weight matrix $\mathbf{W}=[w_{ij}]\in\R^{N\times N}$ is symmetric and doubly 
	stochastic with positive diagonal, i.e., $\W^{T}=\W$ and $\W\one_{N}=\one_{N}$, where $N$ is the number of agents. 
\end{assumption}
% i.e., there exists a path from any agent $i\in\N$ to any other agent$j\in\N$
\begin{remark}
	The connectivity of communication graph $G$ implies the uniqueness of $\lambda_{1}(\W)=1$ and also warrants that other eigenvalues of $\W$ are strictly less than one in magnitude~\citep{nedic2009distributed,horn2012matrix,yuan2016convergence}. In detail, regarding the eigenvalue of $\W$,i.e., $1=\lambda_{1}(\W)\ge\lambda_{2}(\W)\ge\dots\ge\lambda_{N}(\W)\ge-1$, then $\beta<1$,where $\beta=\max(|\lambda_{2}(\W)|,|\lambda_{N}(\W)|)$ is the second largest magnitude of the eigenvalues of $\W$.
\end{remark}
	\begin{assumption}\label{ass:3}
	Let $\x$ and $\{\y_{i}\}_{i=1}^{N}$ be vectors in $[0,1]^{n}$, the Bregman divergence satisfies the separate convexity in the following sense $\D_{\phi}\left(\x,\sum_{i=1}^{N}\alpha_{i}\y_{i}\right)\le\sum_{i=1}^{N}\D_{\phi}(\x,\alpha_{i}\y_{i})$, where $\sum_{i=1}^{N}\alpha_{i}=1$.
\end{assumption}
\begin{remark}
	The separate convexity~\citep{bauschke2001joint} is commonly satisfied for most used cases of Bregman divergence. For example, the Euclidean distance and KL-divergence.
\end{remark}
\begin{assumption}\label{ass:3+}
	The Bregman divergence satisfies a Lipschitz condition, i.e., there exists a constant $K$ such that for any $\x,\y,\z\in[0,1]^{n}$, we have $|\D_{\phi}(\x,\z)-\D_{\phi}(\y,\z)|\le K\|\x-\y\|$.
	\end{assumption}
\begin{remark}
	When the function $\phi$ is Lipschitz with respect to $\|\cdot\|$, the Lipschitz condition on the Bregman divergence is automatically satisfied. Thus, this assumption evidently holds for Euclidean distance. However, KL divergence is not satisfied with Assumption~\ref{ass:3+}, as its gradient will approach infinity on the boundary. %However, the gradient of KL divergence will approach infinity on the boundary. Luckily, we can tackle this drawback by mixing a uniform distribution to keep away from boundaries.
\end{remark}
\begin{assumption}\label{ass:4}For any $t\in[T]$ and $\x\in[0,1]^{n}$, the stochastic gradient $\widetilde{\nabla}F_{t}^{s}(\x)$ is bounded and unbiased, i.e., $\E(\widetilde{\nabla}F^{s}_{t}(\x)|\x)=\nabla F^{s}_{t}(\x)\ \ \text{and}\ \  \E(\|\widetilde{\nabla}F^{s}_{t}(\x)\|_{*}^{2})\le G^{2}$.	Here, $\|\cdot\|_{*}$ is the dual norm of  the general norm $\|\cdot\|$. Moreover, $F_{t}$ is also $L$-smooth, i.e., $\|\nabla F^{s}_{t}(\x)-\nabla F^{s}_{t}(\y)\|_{*}\le L\|\x-\y\|$.
	\end{assumption}\vspace{-0.2em}
A detailed discussion regarding Assumption~\ref{ass:4}  will be presented in the \cref{sec:discussion_on_A5}. Now we are ready to show the main theoretical result about Algorithm~\ref{alg:BDOMA}.
	\begin{theorem}[Proof is deferred to \cref{appendix:1}]
		\label{thm:final_one}
		Consider our proposed Algorithm~\ref{alg:BDOMA}, if Assumption \ref{ass:2}-\ref{ass:4} hold and each set function $f_{t}$ is monotone submodular with curvature $c$ for any $t\in[T]$, then we can conclude that, when $\alpha=\frac{1-e^{-c}}{c}$,	\vspace{-0.7em}
		\begin{equation}\label{equ:thm1_equation_uncomplete}
				\E\Big(\textbf{\emph{Reg}}^{d}_{\alpha}(T)\Big)\le C_{1}\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+KNC_{2}\sum_{t=1}^{T}\frac{|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|}{\eta_{t+1}}+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t},
		\end{equation} where $\mathcal{A}_{t}^{*}$ is any maximizer of Eq.\eqref{equ:problem_t}, $\Delta$ is the symmetric difference of two sets, $C_{1}=(4G+LDG)N^{\frac{3}{2}}$, $\|\x\|\le C_{2}\|\x\|_{1}$ for $\x\in[0,1]^{n}$, $D=\sup_{\x,\y\in\C}\|\x-\y\|$, $R^{2}=\sup_{\x,\y\in\C}\mathcal{D}_{\phi}(\x,\y)$, and $\C$ is the constraint set of Eq.\eqref{equ:continuous_max}. 
	\end{theorem}
\begin{remark} According to the definition of symmetric difference, i.e., $S\Delta T=(S\setminus T)\cup(T\setminus S)$, we can know that the value  $|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|$  quantifies the deviation between the optimal strategy set at time $t+1$ and the one at time $t$, which, to a certain extent, reflects the environmental fluctuations.
\end{remark}
\begin{remark}\label{Remark:final}
	From Eq.\eqref{equ:thm1_equation_uncomplete}, if we set $\eta_{t}=O\left(\sqrt{\frac{(1-\beta)C_{T}}{T}}\right)$ where $C_{T}:=\sum_{t=1}^{T}|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|$is the deviation of maximizer sequence, we have that $\sum_{t=1}^{T}\E\Big(f_{t}(\mathcal{A}_{t})\Big)\ge\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}_{t}^{*})-O\left(\sqrt{\frac{C_{T}T}{1-\beta}}\ \right)$, which means that Algorithm~\ref{alg:BDOMA} can attain a dynamic regret bound of $O(\sqrt{\frac{C_{T}T}{1-\beta}})$ against a  $(\frac{1-e^{-c}}{c})$-approximation to the best comparator in hindsight. %To the best of our knowledge, this is the first result
%with a tight $(\frac{1-e^{-c}}{c})$-approximation guarantee for MA-OSM problem.
\end{remark}