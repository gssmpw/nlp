\textbf{Notations.} Throughout this paper, $\R$ and $\R_{+}$  denote the set of real numbers and non-negative real numbers, respectively. For any positive integer $K$, $[K]$ stands for the set $\{1,\dots, K\}$. Let $\|\cdot\|$ represent a norm for vectors and its dual norm be denoted by $\|\cdot\|_{*}$. Specially, $\|\cdot\|_{1}$ and $\|\cdot\|_{2}$ denote the $l_{1}$ norm and $l_{2}$  norm for vectors, respectively.  $\langle\cdot,\cdot\rangle$ denotes the inner product. The lowercase boldface (e.g. $\x$) denotes a column vector with a suitable dimension and the uppercase boldface (e.g. $\W$) for a matrix. The $i$-th component of a vector $\x$ will be denoted $x_{i}$ and the element in the $i$-th row of the $j$-th column of a matrix $\W$ will be denoted by $w_{ij}$. Moreover, $\lambda_{i}(\W)$ denotes the $i$-th largest eigenvalue of matrix $\W$. $\mathbf{I}_{n}$ and $\mathbf{1}_{n}$ represent the identity matrix and the $n$-dimensional vector whose all entries are $1$, respectively. Additionally, for any vector $\x\in\R^{n}$ and $S\subseteq[n]$, the $[\x]_{S}$ denotes the projection of $\x$ onto the set $S$, i.e., $[\x]_{S}=(x_{i_1},\dots,x_{i_{|S|}})\in\R^{s}$ for any $S=\{i_{1},\dots,i_{|S|}\}\subseteq[n]$.
	
	\textbf{Submodularity and curvature.} Let $\V$ be a finite set and $f:2^{V}\rightarrow\R_{+}$ be a set function mapping subsets of $\V$ to the non-negative real line.  The function $f$ is said to be submodular iff $f(S\cup\{e\})-f(S)\ge f(T\cup\{e\})-f(T)$ for any $S\subseteq T\subseteq V$ and $e\in V\setminus T$. In this paper, we will consider submodular functions that are \emph{monotone}, meaning that for any $S\subseteq T\subseteq V$, $f(S)\le f(T)$, and \emph{normalized}, that is, $f(\emptyset)=0$. To better reflect the diminishing returns property of submodular functions, \citet{conforti1984submodular} introduced the concept of \emph{curvature}, which is defined as $c:=1-\min_{S\subseteq\V, e\notin S}\frac{f(S\cup\{e\})-f(S)}{f(\{e\})}$. Moreover, we can infer  $c\in[0,1]$ for submodular functions. 
	\vspace{-0.1em}
\subsection{Problem Formulation}\label{sec:Problem_Formulation}
In this subsection, we introduce the multi-agent online submodular maximization problem, commonly abbreviated as multi-agent submodular coordination.

In MA-OSM, we generally consider a group of $N$ different agents denoted as $\N=\{1,2,\ldots,N\}$, interacting over a connected communication graph $G(\N,\mathcal{E})$. In addition, each agent $i$ within $\N$ is equipped with a \emph{unique} and \emph{discrete} set of actions, denoted by $\V_{i}$. This implies that these action sets are mutually disjoint, i.e., $\V_{i}\cap\V_{j}=\emptyset$ for any two distinct agents $i\in\N$ and $j\in\N$. At each time step $t\in[T]$, every agent $i\in\N$ separately selects an action $a_{t,i}$ from the individual action set $\V_{i}$. After committing to these choices, the environment reveals a monotone submodular function $f_{t}$ defined over the aggregated action space $\V:=\cup_{i\in\N}\V_{i}$. Then, the agents receive the utility $f_{t}(\cup_{i\in\N}\{a_{t,i}\})$. As a result, the objective of agents at any given moment is to maximize their collective gains as much as possible, that is to say, we need to solve the following submodular maximization problem in a multi-agent manner at each round: 
 \begin{equation}\label{equ:problem_t}
\max f_{t}(\mathcal{A}),\ \ \text{ s.t.}\ \  |\mathcal{A}\cap\V_{i}|\le1,\forall i\in\N.
\end{equation} 
Compared to the standard \emph{centralized}  submodular maximization problem, this MA-OSM problem brings additional challenges: \textbf{1) Unpredictable Objectives and Actions:} Agents must make decisions at each moment without prior knowledge of future submodular utility functions and the insight into other agents' actions; \textbf{2) Limited Feedback:} In many real-world scenarios, each agent is typically endowed with a narrow perceptual or detection scope, which only allows it to sense the environmental changes within its surroundings.  For instance, in the target tracking problem of Figure~\ref{figue_intro_target}, every sensor usually overlooks these targets beyond its sensing circle. Broadly speaking, the local information observed by one agent is inadequate for precisely assessing the actions of most other agents who are not in close vicinity. To capture this, various studies~\citep{xu2023online,rezazadeh2023distributed,robey2021optimal,qu2019distributed} related to MA-OSM problem commonly confine each agent $i\in\N$ to a local marginal gain oracle $\mathcal{O}_{t}^{i}:\V_{i}\times2^\V\rightarrow\R_{+}$ after  $f_{t}$ is revealed, where $\mathcal{O}_{t}^{i}(a,\mathcal{A}):=f_{t}(\mathcal{A}\cup\{a\})-f_{t}(\mathcal{A})$  for any $a\in\V_{i}$ and $\mathcal{A}\subseteq\V$. This restriction means that, at each time $t\in[T]$, agents only can receive the limited feedback about the marginal evaluations of actions within their own action set, rather than the full information of $f_{t}$. In this paper, we also impose this restriction on each agent.


%This restriction reflects the fact that the local information observed by one agent  is difficult to precisely assess the actions of most other agents who may be distant. 




%In general, the local information observed by every agent can provide a precise assessment of its own actions but may cause substantial errors when used to evaluate the actions of others. As a result,  In other words, at each time $t\in[T]$,  each agent only can get access to the marginal evaluations for the actions within its own action set, rather than the full information of $f_{t}$.  This requirement means that, at each time $t\in[T]$, each agent only can get access to the marginal evaluations for the actions within its own action set, rather than the full information of $f_{t}$. 


%This restriction reflects the fact that the local information can offer a more precise assessment of an agent's action compared to those of other agents.  %Note that this definition assumes that local information can perfectly approximate the marginal benefit, without considering estimation errors.


%In other words, at each time $t\in[T]$,  each agent only can have access to the marginal evaluations for the actions within its own action set, rather than the full information of $f_{t}$. 

%Note that this assumption is a compromise for the finite perceptual range of agents, as we know that the local environmental information around each agent can provide a better evaluation for its own action set, in contrast with other actions belonging to other agents. This approach acknowledges that localized environmental data can more effectively evaluate an agent's action set compared to actions of other agents. as we know that the local environmental information around each agent can provide a better evaluation for its own action set, in contrast with other actions belonging to other agents. 



%The private attribute  of each action set $\mathcal{V}_i$ means that every agent $i \in \mathcal{N}$ only can partially observe the global  constraints in Eq.\eqref{equ:problem_t}, i.e.,  $\{\mathcal{A} \subseteq \mathcal{V}: |\mathcal{A} \cap \mathcal{V}_i| \leq 1, \forall i \in \mathcal{N}\}$. We will systematically address these issues  in the upcoming sections of this paper.

Given the NP-hardness of maximizing a  submodular function subject to a general constraint~\citep{vondrak2013symmetry,bian2017guaranteed}, we adopt the dynamic \emph{$\alpha$-regret} to evaluate the algorithm performance for MA-OSM problem in this paper, which is defined as
follows~\citep{kakade2007playing,streeter2008online,chen2018online}:\vspace{-1.0em}
\begin{equation*}
\textbf{Reg}^{d}_{\alpha}(T)=\alpha\sum_{t=1}^{T}f_{t}(\mathcal{A}_{t}^{*})-\sum_{t=1}^{T}f_{t}(\cup_{i\in\N}\{a_{t,i}\}),
\end{equation*}\vspace{-0.5em}where $\mathcal{A}_{t}^{*}$ is the maximizer of Eq.\eqref{equ:problem_t} and  $a_{t,i}$ is the action taken via the agent $i\in\N$ at time $t\in[T]$.