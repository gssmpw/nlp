		The primary computational burden of Algorithm~\ref{alg:BDOMA} lies in Line 13, where each agent is tasked with a single constrained mirror projection. Despite that this projection can be done very efficiently in linear time using standard methods described in \citep{pardalos1990algorithm,brucker1984n}, the optimal solution to Eq.\eqref{equ:mirror_projection} admits an analytical expression when KL-divergence is selected as the metric. That is, we have the following theorem, whose proof is deferred to \cref{append:proof4}.
	\begin{theorem}\label{thm:projection}
		Let $m$ be a positive integer and  $g(x)=x\log(x)$. Then, the optimal solution $\x$ to the problem $\min_{\|\mathbf{b}\|_{1}\le1, \mathbf{b}\in[0,1]^{m}}\Big(\langle\mathbf{z},\mathbf{b}\rangle+\D_{g,m}(\mathbf{b}, \mathbf{y})\Big)$ satisfies the following conditions: if $\sum_{i=1}^{m}y_{i}\exp(-z_{i})\le1$, $x_{i}=y_{i}\exp(-z_{i})$; otherwise,  $x_{i}=\frac{y_{i}\exp(-z_{i})}{\sum_{i=1}^{m}y_{i}\exp(-z_{i})}$ $\forall i\in[m]$.
		\end{theorem}
		However, KL divergence does not meet with the Lipschitz condition in Assumption~\ref{ass:3+}, as its gradient approaches infinity on the boundary.
		 Fortunately, this drawback can be circumvented by mixing a uniform distribution. As a result, we get the \emph{projection-free} Multi-Agent Online Surrogate Entropic Ascent (\textbf{MA-OSEA}) algorithm for the MA-OSM problem, as shown in Algorithm~\ref{alg:BDOEA}. Similarly, we also can verify the following regret bound for \textbf{MA-OSEA} algorithm.
		\begin{theorem}[Proof deferred to \cref{append:proof5}]
		\label{thm:final_one1}
		Consider our proposed Algorithm~\ref{alg:BDOEA}, if Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3} and \ref{ass:4} hold, $\|\cdot\|$ is $l_{1}$ norm and each set function $f_{t}$ is monotone submodular with curvature $c$, then we can conclude that, when $\alpha=\frac{1-e^{-c}}{c}$,
\begin{equation}\label{equ:th2_uncomplete_equation}
		\E\Big(\textbf{\emph{Reg}}^{d}_{\alpha}(T)\Big)\le C_{1}\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}(\beta-\beta\gamma)^{t-\tau}\eta_{\tau}\Big)+\frac{NC_{2}}{\eta_{T+1}}+C_{2}\sum_{t=1}^{T}\frac{|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|}{\eta_{t+1}}+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}+\sum_{t=1}^{T}\frac{C_{3}}{\eta_{t}}+GD\gamma,
 		\end{equation} where $\mathcal{A}_{t}^{*}$ is any maximizer of Eq.\eqref{equ:problem_t},  $C_{1}=(4G^{2}+LDG)N^{\frac{3}{2}}$, $C_{2}=N\log(\frac{n}{\gamma})$, $C_{3}=2N^{2}\gamma$, $D=\sup_{\x,\y\in\C}\|\x-\y\|_{1}$ and $\C$ is the constraint set of Eq.\eqref{equ:continuous_max}.
		
		% Moreover, if we set $\eta_{t}=O(\frac{(1-\beta)C_{T}}{\sqrt{T}})$ and $\gamma=O(T^{-\frac{3}{2}})$ where $C_{T}=\sum_{t=1}^{T}\|\one_{\mathcal{A}_{t+1}^{*}}-\one_{\mathcal{A}_{t}^{*}}\|$, we have that
	%	\begin{equation*}
		%	\sum_{t=1}^{T}\E\Big(f_{t}(\mathcal{A}_{t})\Big)\ge\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}_{t}^{*})-O\Big(\sqrt{\frac{C_{T}T}{1-\beta}}\ \Big).
	%	\end{equation*}
	\end{theorem}
	\begin{remark}\label{Remark:final1}
		From Eq.\eqref{equ:th2_uncomplete_equation}, if we set $\eta_{t}=O\left(\sqrt{\frac{(1-\beta)C_{T}}{T}}\right)$ and $\gamma=O(T^{-2})$ where $C_{T}=\sum_{t=1}^{T}|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|$, we have that $\sum_{t=1}^{T}\E\Big(f_{t}(\mathcal{A}_{t})\Big)\ge\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}_{t}^{*})-\widetilde{O}\left(\sqrt{\frac{C_{T}T}{1-\beta}}\right)$.
	\end{remark}
			\begin{algorithm}[t]
			\caption{Multi-Agent Online Surrogate Entropic Ascent~(\textbf{MA-OSEA})}\label{alg:BDOEA}
			\begin{algorithmic}[1]
				\STATE{\bf Input:} Number of iterations $T$, the set of agents $\N$, communication graph $G(\N,\mathcal{E})$,
				weight matrix $\W=[w_{ij}]\in\R^{N\times N}$, $1$-strongly decomposable convex function $\phi(\x)=\sum_{i=1}^{n}x_{i}\log(x_{i})$, the curvature $c\in[0,1]$, step size $\eta_{t}$ for $t\in[T]$,mixing parameter $\gamma$;
				\STATE {\bf Initialized:} for any agent $i\in\N$, let $[\x_{1,i}]_{j}=\frac{1}{|\V_{i}|},\ \forall j\in\V_{i}\ \text{ and }\  [\x_{1,i}]_{j}=0,\ \forall j\notin\V_{i}$
				\FOR{$t\in[T]$}
				\FOR{$i\in\N$}
				\STATE Compute $\text{SUM}:=\sum_{a\in\V_{i}}[\x_{t,i}]_{a}$\ \ \ \  \COMMENT{Rounding (Lines 5-6)}
				\STATE Select an action $a_{t,i}$ from the set $\V_{i}$ with probability $\frac{[\x_{t,i}]_{a}}{\text{SUM}}$
				\STATE Compute $\hat{\x}_{t, i}:=(1-\gamma)\x_{t, i}+\frac{\gamma}{n}\one_{n}$;\ \ \ \ \  \COMMENT{Mixing (Line 7)}
				\STATE Exchange $\hat{\x}_{t,i}$ with each neighboring node $j\in\mathcal{N}_{i}$\ \ \ \COMMENT{Information aggregation (Lines 8-9)}
				\STATE Aggregate the information by setting $ \y_{t,i}=\sum_{j\in\mathcal{N}_{i}\cup\{i\}}w_{ij}\x_{t,j}$%\ \ \ \ \ \COMMENT{Rounding (Lines 7-9)}
				\STATE Sampling a random number $z_{t,i}$ from r.v. $\mathcal{Z}$\ \ \ \COMMENT{Surrogate gradient estimation (Lines 10-12)}
				\STATE Sampling a random set $\mathcal{R}_{t,i}\sim z_{t,i}*\x_{t,i}$
				\STATE Compute $[\widetilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a}:=\frac{1-e^{-c}}{c} \big(f_{t}(\mathcal{R}_{t,i}\cup\{a\})-f_{t}(\mathcal{R}_{t,i}\setminus\{a\})\big)$ for any $a\in\V_{i}$ 
				\STATE Update $	[\x_{t+1,i}]_{a}=[\y_{t,i}]_{a},\ \forall a\notin\V_{i}$\ \ \ \ \ \COMMENT{ Update the probabilities of actions (Lines 13-18)}
				\STATE Compute $\text{SUM}_{1}:=\sum_{a\in\V_{i}}\Big(	[\y_{t,i}]_{a}\exp(\eta_{t}[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a})\Big)$
				\IF{$\text{SUM}_{1}\le 1$}
				\STATE  $[\x_{t+1,i}]_{a}:=[\y_{t,i}]_{a}\exp(\eta_{t}[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a})$ for any $a\in\V_{i}$
				\ELSE \STATE $[\x_{t+1,i}]_{a}:=[\y_{t,i}]_{a}\exp(\eta_{t}[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a})/\text{SUM}_{1}$ for any $a\in\V_{i}$
				\ENDIF
				\ENDFOR
				\ENDFOR
			\end{algorithmic}
		\end{algorithm}