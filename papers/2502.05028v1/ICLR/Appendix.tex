\appendix
\section{Additional Discussions} 
\subsection{More Details on Related Works}\label{Appendix:related_work}
\textbf{Single-Agent Submodular Maximization,} Maximization of submodular functions has recently found numerous applications in machine learning, operations research and economics, including data summarization~\citep{lin2010multi,lin2011class,wei2013using,wei2015submodularity}, dictionary learning~\citep{das2018approximate}, product recommendation~\citep{kempe2003maximizing,el2009turning,mirzasoleiman2016fast,wang2022empirical}, federated learning~\citep{balakrishnan2022diverse,FedSub,zhao2024a,zhao2024huber} and in-context learning~\citep{kumari2024end,fancombatting,jin2024learning,jin2024visual,jin2024apeer}. When considering the simple cardinality constraint, the classical works\citep{fisher1978analysis,nemhauser1978analysis} show that the
greedy algorithm can achieve a tight $(1-1/e)$-approximation guarantee for monotone submodular maximization problem. As for the general matroid constraint, a continuous greedy algorithm with $(1-1/e)$-approximation guarantee is presented in \citep{calinescu2011maximizing,chekuri2014submodular}. Especially when the submodular function has curvature $c\in[0,1]$, \citet{vondrak2010submodularity} pointed out that the continuous greedy algorithm also can achieve an improved ($\frac{1-e^{-c}}{c}$)-approximation guarantee.

\textbf{Multi-Agent Submodular Maximization.} Multi-agent submodular maximization(MA-SM) problem involves coordinating multiple agents to collaboratively maximize a submodular utility function. A commonly used solution for MA-SM problem heavily depends on the distributed implementation of the classic \emph{sequential greedy} method~\citep{fisher1978analysis}, which can ensure a $(\frac{1}{1+c})$-approximation~\citep{conforti1984submodular} when the submodular function has curvature $c\in[0,1]$. However, this distributed algorithm requires each agent to have full access to the decisions of all previous agents, thereby forming a \emph{complete} directed communication graph. Subsequently, several studies~\citep{grimsman2018impact,gharesifard2017distributed,marden2016role} have investigated how the topology of the communication network affects the performance of the distributed greedy method. Particularly, \citet{grimsman2018impact} pointed out that the worst-case performance of the distributed greedy algorithm will deteriorate in proportion to the size of the largest independent group of agents in the communication graph. In order to overcome these challenges, various studies \citep{rezazadeh2023distributed,robey2021optimal,du2022jacobi} utilized the multi-linear extension to design algorithms for solving MA-SM problem. Specifically, \citet{du2022jacobi} devised
a multi-agent variant of gradient ascent for MA-SM problem, which can attain $\frac{1}{2}OPT-\epsilon$ over connected communication graph at the cost of $O(\frac{n}{\epsilon^{2}})$ value queries to the submodular function where $OPT$ is the optimal value. After that, to improve the  $\frac{1}{2}$-approximation, \citet{robey2021optimal} developed a multi-agent variant of continuous greedy method\citep{calinescu2011maximizing,chekuri2014submodular} with tight $(1-1/e)$-approximation. However, this multi-agent continuous greedy\citep{robey2021optimal} requires the exact knowledge of the multi-linear extension function, which will lead to the exponential query complexity. To tackle this drawback, \citet{rezazadeh2023distributed} also proposed a stochastic variant of continuous greedy method\citep{calinescu2011maximizing,chekuri2014submodular}, which considers the curvature $c$ of submodular objectives and can enjoy $(\frac{1-e^{-c}}{c})OPT-\epsilon$ at the expense of $O(\frac{n\log(\frac{1}{\epsilon})}{\epsilon^{3}})$ value queries to the submodular function. In our Algorithm~\ref{alg:BDOMA} and Algorithm~\ref{alg:BDOEA}, if any incoming objective function $f_{t}$ corresponds to some submodular function $f$ and we return the set $\cup_{i\in\N}\{a_{t,i}\},\forall t\in[T]$  with probability $\frac{1}{T}$, we also can obtain two methods with the tight ($\frac{1-e^{-c}}{c}$)-approximation guarantee for MA-SM problem. Notably, in sharp contrast with $O(\frac{n\log(\frac{1}{\epsilon})}{\epsilon^{3}})$ value queries in \citep{rezazadeh2023distributed}, the aforementioned variants of Algorithm~\ref{alg:BDOMA} and Algorithm~\ref{alg:BDOEA} only require inquiring the submodular objective $O(\frac{n}{\epsilon^{2}})$ and $O(\frac{n\log(\frac{1}{\epsilon})}{\epsilon^{2}})$ times to attain $(\frac{1-e^{-c}}{c})OPT-\epsilon$, respectively. We present a detailed comparison of our proposed \textbf{MA-OSMA} and \textbf{MA-OSEA} with previous studies for MA-SM problem in Table~\ref{tab:result1}.


Because the multi-linear extension of a submodular set function belongs to the general continuous DR-submodular functions, we also review related works about decentralized continuous DR-submodular maximization. As for more detailed content on continuous DR-submodular maximization, please refer to \citet{NEURIPS2023_c041d58d,DBLP:conf/iclr/PedramfarNQA24}.

\textbf{Decentralized Continuous DR-submodular Maximization.} A differentiable function $F:[0,1]^{n}\rightarrow\R_{+}$ is {\it DR-submodular} if $\nabla F(\x)\le\nabla F(\y)$ for any $\x\ge\y$. \cite{mokhtari2018decentralized} is the first to study the decentralized continuous DR-submodular maximization problem, which showed an algorithm titled DeCG achieving $(1-1/e)OPT-\epsilon$ 
 for \emph{monotone} and \textit{deterministic}  continuous DR-submodular maximization after $O(\frac{1}{\epsilon^{2}})$ iterations  and $O(\frac{1}{\epsilon^{2}})$ communications.  When only an unbiased estimate of gradient is available, \cite{mokhtari2018decentralized}  also presented a decentralized method named DeSCG for monotone cases, which attains $(1-1/e)OPT-\epsilon$ with $O(\frac{1}{\epsilon^{3}})$ communications after $O(\frac{1}{\epsilon^{3}})$ iterations. Next, \cite{xie2019decentralized} presented a deterministic DeGTFW and a stochastic DeSGTFW by applying the gradient tracking techniques\citep{pu2021distributed2}  to DeCG and  DeSCG respectively,  both of which achieve the $(1-1/e)$-approximation with a faster $O(\frac{1}{\epsilon})$ convergence rate and a lower $O(\frac{1}{\epsilon})$ communications. After that, \cite{gao2023convergence} utilized the  variance reduction technique\citep{fang2018spider} to propose two sample-efficient algorithms, namely, DeSVRFW-gp and DeSVRFW-gt, both of which can reduce the sample complexity of DeSGTFW from $O(\frac{1}{\epsilon^{3}})$ to  $O(\frac{1}{\epsilon^{2}})$. Lastly, several studies\citep{zhu2021projection,zhang2023communication,liao2023improved} extended these aforementioned offline decentralized frameworks to time-varying DR-submodular objectives. It's important to note that the decentralized submodular maximization problem  significantly differs from the multi-agent scenario emphasized in this paper. In the context of decentralized optimization, we typically assume that each local node maintains its own local utility function and the collective goal is to optimize the sum of these local functions. In contrast, within the scope of this paper, we assume that all agents share a common submodular function but each agent is restricted to accessing a unique set of actions.
 
\begin{table}[h]
\renewcommand\arraystretch{1.35}
	\centering	
	\resizebox{0.9\textwidth}{!}{
		\setlength{\tabcolsep}{1.0mm}{
			\begin{tabular}{ccccc}
				\toprule[1.0pt]
				Method&Utility&Graph&Query Complexity&Reference \\
				\hline
				Greedy Method &$(\frac{1}{1+c})OPT$ &\textbf{complete} &$n$&\citet{conforti1984submodular}\\
				Greedy Method &$(\frac{1}{1+\alpha(G)})OPT$ &connected &$n$&\citet{grimsman2018impact}\\
    	 Projected Gradient Method&$\frac{1}{2}OPT-\epsilon$ &connected &$O\big(\frac{n}{\epsilon^{2}}\big)$&\citet{du2022jacobi}\\
       CDCG&$(1-\frac{1}{e})OPT-\epsilon$ &connected &$O\big(\frac{n2^{n}}{\epsilon}\big)$&\citet{robey2021optimal}\\
        Distributed-CG&$(\frac{1-e^{-c}}{c})OPT-\epsilon$ &connected &$O\Big(\frac{n\log(\frac{1}{\epsilon})}{\epsilon^{3}}\Big)$&\citet{rezazadeh2023distributed}\\
				\rowcolor{cyan!18}
		Algorithm~\ref{alg:BDOMA}&$(\frac{1-e^{-c}}{c})OPT-\epsilon$ &connected &$O\big(\frac{n}{\epsilon^{2}}\big)$ &Theorem~\ref{thm:final_one} \& Remark~\ref{Remark:final}\\
				\rowcolor{cyan!18}
			Algorithm~\ref{alg:BDOEA}&$(\frac{1-e^{-c}}{c})OPT-\epsilon$ &connected &$O\Big(\frac{n\log(\frac{1}{\epsilon})}{\epsilon^{2}}\Big)$ &Theorem~\ref{thm:final_one1} \& Remark~\ref{Remark:final1}\\
				\midrule[1.0pt]
			\end{tabular}
	}}\caption{Comparison with prior works for Multi-Agent Submodular Maximization Problem. $OPT$ is the optimal value, $c$ is the curvature of submodular objective, $n:=|\V|$ the total number of all available actions, $\alpha(G)$ is the number of nodes in the largest independent set in communication graph $G$ where $\alpha(G)\ge1$. Note that the column of Query Complexity only considers the setting that each agent selects one action.}\label{tab:result1}
\end{table}
\subsection{More Discussions on Assumption~\ref{ass:4}}\label{sec:discussion_on_A5}
In this subsection, we show the Assumption~\ref{ass:4} is well-established in both $l_{1}$ norm and $l_{2}$ norm.

At first, we review that, in \cref{sec:construct_gradient_surrogate_function}, we define that:
\begin{equation}\label{equ:appendix}\widetilde{\nabla}F^{s}(\x)=\Big(\frac{1-e^{-c}}{c}\Big)\Big(f(\mathcal{R}\cup\{1\})-f(\mathcal{R}\setminus\{1\}),\dots,f(\mathcal{R}\cup\{n\})-f(\mathcal{R}\setminus\{n\})\Big),
\end{equation}where $z$ is sampled from the probability distribution of the random variable $\mathcal{Z}$ where $P(\mathcal{Z}\le b)=(\frac{c}{1-e^{-c}})\int_{0}^{b}e^{c(z-1)}\mathrm{d}z=\frac{e^{c(b-1)}-e^{-c}}{1-e^{-c}}$ for any $b\in[0,1]$  and $\mathcal{R}\sim z*\x$. 

From Eq.\eqref{equ:appendix} and the submodularity of set function $f$, we can know that  $\E(\widetilde{\nabla}F^{s}(\x)|\x)=\nabla F^{s}$ and $\E(\|\widetilde{\nabla}F^{s}(\x)\|_{\infty})\le\frac{1-e^{-c}}{c}m_{f}$ where $m_{f}=\max_{a\in\V}f(\{a\})$ denotes maximum singleton value.

Moreover, \citet{hassani2017gradient} recently have shown that the multi-linear extension $F$ of a submodular function $f$ is $m_{f}$-smooth under $l_{1}$ norm, that is, $\|\nabla F(\x)-\nabla F(\y)\|_{\infty}\le m_{f}\|\x-\y\|_{1}$. From the definition of surrogate function, we also know that $\nabla F^{s}(\x)=\int_{0}^{1}e^{c(z-1)}\nabla F(z*\boldsymbol{x})\mathrm{d}z$ such that 
\begin{equation*}
    \begin{aligned}
        \|\nabla F^{s}(\x)-\nabla F^{s}(\y)\|_{\infty}&\le\int_{0}^{1}e^{c(z-1)}\|\nabla F(z*\x)-\nabla F(z*\y)\|_{\infty}\mathrm{d}z\\
        &\le\int_{0}^{1}ze^{c(z-1)}m_{f}\|\x-\y\|_{1}\mathrm{d}z\\
        &=\frac{e^{-c}+c-1}{c^{2}}m_{f}\|\x-\y\|_{1}.
    \end{aligned}
\end{equation*}
Therefore, we can conclude that the surrogate function $F^{s}$ of the multi-linear extension of a submodular function $f$ with curvature $c\in[0,1]$ is $\Big(\frac{e^{-c}+c-1}{c^{2}}m_{f}\Big)$-smooth  under $l_{1}$ norm. Due to $\|\cdot\|_{2}\le\sqrt{n}\|\cdot\|_{\infty}$ and $\|\cdot\|_{1}\le\sqrt{n}\|\cdot\|_{2}$, we also can show that $\E(\|\widetilde{\nabla}F^{s}(\x)\|_{2})\le\sqrt{n}\frac{1-e^{-c}}{c}m_{f}$ and $  \|\nabla F^{s}(\x)-\nabla F^{s}(\y)\|_{2}\le\Big(n\frac{e^{-c}+c-1}{c^{2}}m_{f}\Big)\|\x-\y\|_{2}$.

With the previous results, in Assumption~\ref{ass:4}, we can set $G=\Big(\frac{1-e^{-c}}{c}\Big)\max_{a\in\V,t\in[T]}\Big(f_{t}(\{a\})\Big)$ and $L=\Big(\frac{e^{-c}+c-1}{c^{2}}\Big)\max_{a\in\V,t\in[T]}\Big(f_{t}(\{a\})\Big)$ under $l_{1}$ norm. As for $l_{2}$ norm, we consider $G=\sqrt{n}\Big(\frac{1-e^{-c}}{c}\Big)\max_{a\in\V,t\in[T]}\Big(f_{t}(\{a\})\Big)$ and $L=n\Big(\frac{e^{-c}+c-1}{c^{2}}\Big)\max_{a\in\V,t\in[T]}\Big(f_{t}(\{a\})\Big)$.
\subsection{More Discussions on Experiments}
In this subsection, we highlight some additional details about the experiments. 

At first, all experiments are performed in Python 3.6.5 using CVX optimization tool~\citep{grant2014cvx} on a MacBook Pro with Apple M1 Pro and 16GB RAM. Then, when considering the random graph, we set the weight matrix $\W$ as follow: if the edge $(i,j)$ is an edge of the graph, let $w_{ij}=1/(1+\max(d_{i},d_{j}))$ where $d_{i}$ and $d_{j}$ are the degree of agent $i$ and $j$, respectively. If $(i,j)$ is not an edge of the graph and $i\neq j$, then $w_{ij}=0$. Finally, we set $w_{ii}=1-\sum_{j\in\mathcal{N}_{i}}w_{ij}$. 

As for the complete graph, we set $w_{ij}=\frac{1}{N}$ where $N$ is the number of agents. In \textbf{MA-OSMA} and \textbf{MA-OSEA} algorithms, we set $c=1$ and $\eta_{t}=\frac{1}{\sqrt{T}}$.

\section{Proof of Theorem~\ref{thm:1}} \label{append:proof1}
	We begin by reviewing some basic properties about the multi-linear extension of a submodular function.
	\begin{lemma}[\citet{calinescu2011maximizing,bian2020continuous}]\label{lemma:th1:1}When $f:\V\rightarrow\R_{+}$ is a monotone submodular function, its multi-linear extension $F(\x)=\sum_{\mathcal{A}\subseteq\V}\Big(f(\mathcal{A})\prod_{a\in\mathcal{A}}x_{a}\prod_{a\notin\mathcal{A}}(1-x_{a})\Big)$ has the following properties:
	\begin{enumerate}
		\item  $F$ is monotone, that is, $\frac{\partial F(\x)}{\partial x_{i}}\ge 0$ for any $i\in[n]$ and $\x\in[0,1]^{n}$;
		\item  $F$ is concave along any non-negative direction $\textbf{d}\in\R_{+}^{n}$;
		\item   $\frac{\partial F(\x)}{\partial x_{i}}=\E_{\mathcal{R}\sim\x}\Big(f(\mathcal{R}\cup\{i\})-f(\mathcal{R}\setminus\{i\})\Big)$,
		\item  $\nabla F(\x)\ge\nabla F(\y)$ for any $\x\le\y$ and $\x,\y\in[0,1]^{n}$.
	\end{enumerate}
	\end{lemma}
	With this lemma, we can show the following lemma.
	\begin{lemma}[\citet{calinescu2011maximizing,bian2020continuous}]\label{lemma:thm1:2}when  $F$ is the multi-linear extension of a monotone submodular function $f$, we can conclude that
		\begin{equation*}
	  \langle\y-\x,\nabla F(\x)\rangle\ge F(\x\lor\y)+F(\x\land\y)-2F(\x),
		\end{equation*} where $\x\land\y:=\min(\x,\y)$ and $\x\lor\y:=\max(\x,\y)$ are component-wise minimum and component-wise maximum, respectively. 
	\end{lemma}
	\begin{proof}
	From the second property about the concavity in Lemma~\ref{lemma:th1:1} and $ \y\lor\x\ge\x$ and $\x\land \y\le\x$, we first have that
	\begin{equation}
	\begin{aligned}
	&\langle\y\lor \x -\x, \nabla F(\x)\rangle \ge F(\y\lor \x)-F(\x), \\
	&\langle \x\land \y -\x, \nabla F(\x)\rangle \ge F(\x\land\y)-F(\x).
	\end{aligned}
\end{equation}
Due to $\x+\y=\x\lor\y+\x\land\y$, we therefore that that $\langle\y-\x,\nabla F(\x)\rangle=\langle\y\lor \x -\x, \nabla F(\x)\rangle +\langle \x\land \y -\x, \nabla F(\x)\rangle\ge F(\x\lor\y)+F(\x\land\y)-2F(\x)$.
\end{proof}
From the third property in Lemma~\ref{lemma:th1:1} and the definition of curvature $c\in[0,1]$, that is, $c:=1-\min_{S\subseteq\V, e\notin S}\frac{f(S\cup\{e\})-f(S)}{f(\{e\})}$, we can conclude that, when $f$ is monotone submodular, $f(S\cup\{e\})-f(S)\ge(1-c)f(\{e\})$ for any $S\subseteq\V, e\notin S$ such that, for any $\x\in[0,1]^{n}$,
\begin{equation*}
\frac{\partial F(\x)}{\partial x_{i}}=\E_{\mathcal{R}\sim\x}\Big(f(\mathcal{R}\cup\{i\})-f(\mathcal{R}\setminus\{i\})\Big)\ge(1-c)f(\{i\})=(1-c)\frac{\partial F(\textbf{0}_{n})}{\partial x_{i}},
\end{equation*} where $\textbf{0}_{n}$ is $n$-dimensional zero vector and we general suppose $f(\emptyset)=0$.

As a result, we can show that 
\begin{lemma}\label{lemma:thm1:3}
when  $F$ is the multi-linear extension of a monotone submodular function $f$ with curvature $c$, we have 
\begin{equation*}
F(\x\lor\y)\ge(1-c)\Big(F(\x)-F(\x\land\y)\Big)+F(\y).
\end{equation*}
\end{lemma}
\begin{proof}
\begin{equation*}
	\begin{aligned}
		F(\x\lor\y)-F(\y)&=\int_{z=0}^{1}\Big\langle\nabla F\Big(\y+z(\x\lor\y-\y)\Big),\x\lor\y-\y\Big\rangle\mathrm{d}z\\
		&\ge(1-c)\langle\nabla F(\textbf{0}_{n}),\x\lor\y-\y\rangle\\
		&\ge(1-c)\langle\nabla F(\x\land\y),\x\lor\y-\y\rangle\\
		&=(1-c)\langle\nabla F(\x\land\y),\x-\x\land\y\rangle\\
		&\ge(1-c)\Big(F(\x)-F(\x\land\y)\Big),
	\end{aligned}
\end{equation*} where the first inequality follows from $\nabla F(\y+z(\x\lor\y-\y))\ge(1-c)\nabla F(\textbf{0}_{n})$; the second inequality comes from the fourth property in Lemma~\ref{lemma:th1:1}, i.e., $\nabla F(\textbf{0}_{n})\ge\nabla F(\x\land\y)$. The second equality is due to $\x+\y=\x\lor\y+\x\land\y$ and the final inequality from the concavity along non-negative direction.
\end{proof}
Merging Lemma~\ref{lemma:thm1:3} into Lemma~\ref{lemma:thm1:2}, we finally have that $  \langle\y-\x,\nabla F(\x)\rangle\ge F(\y)-(1+c)F(\x)$ such that if $\x$ is a stationary point for $F$ over the domain $\C\subseteq[0,1]^{n}$, we have that $ 0\ge \langle\y-\x,\nabla F(\x)\rangle\ge F(\y)-(1+c)F(\x)$ for any $\y\in\C$, so $F(\x)\ge\frac{1}{1+c}\max_{\y\in\C}F(\y)$.

\section{Proof of Theorem~\ref{thm:2}} \label{append:proof2}
In this section, we show the proof of Theorem~\ref{thm:2}.
Before going into the details, we firstly prove the following lemma.
\begin{lemma}\label{lemma:thm2:1}
	when  $F$ is the multi-linear extension of a monotone submodular function $f$ with curvature $c$, we have, for any $\y,\x\in[0,1]^{n}$,
	\begin{equation*}
		\langle\y,\nabla F(\x)\rangle\ge F(\y)-cF(\x).
	\end{equation*}
\end{lemma}
\begin{proof}
Due to $\x+\y=\x\lor\y+\x\land\y$, we have that 
\begin{equation*}
	\begin{aligned}
		\langle\y,\nabla F(\x)\rangle&=\langle\x\lor\y-\x,\nabla F(\x)\rangle+\langle\x\land\y,\nabla F(\x)\rangle\\
		&\ge F(\x\lor\y)-F(\x)+(1-c)\langle\x\land\y,\nabla F(\textbf{0}_{n})\rangle\\
		&\ge F(\x\lor\y)-F(\x)+(1-c)F(\x\land\y)\\
		&\ge F(\y)-cF(\x),
	\end{aligned}
\end{equation*} where the first inequality follows from $\nabla F(\x)\ge(1-c)\nabla F(\textbf{0}_{n})$ and the concavity along non-negative direction, i.e., $\langle\x\lor\y-\x,\nabla F(\x)\rangle\ge F(\x\lor\y)-F(\x)$; the final inequality comes from Lemma~\ref{lemma:thm1:3}.
\end{proof}
Next, we verify Theorem~\ref{thm:2}.
		\begin{proof}
			Firstly, we prove that
			\begin{equation}\label{equ:thm2:1}
				\begin{aligned}
					&\left\langle\x,\int_{0}^{1}e^{c(z-1)}\nabla F(z*\x)\mathrm{d}z\right\rangle\\&=	\int_{0}^{1}e^{c(z-1)}\langle\x,\nabla F(z*\x)\rangle\mathrm{d}z\\
					&=\int_{0}^{1}e^{c(z-1)}\mathrm{d}\Big(F(z*\x)\Big)\\
					&=e^{c(z-1)}*F(z*\x)|_{z=0}^{z=1}-\int_{0}^{1}F(z*\x)\mathrm{d}\Big(e^{c(z-1)}\Big)\\
					&=F(\x)-c\int_{0}^{1}e^{c(z-1)}F(z*\x)\mathrm{d}z.
					\end{aligned}
				\end{equation}
			Then, we show an upper bound for $\langle\y,\int_{0}^{1}e^{c(z-1)}\nabla F(z*\x)\mathrm{d}z\rangle$, 
			\begin{equation}\label{equ:th1_2}
				\left\langle\y,\int_{0}^{1}e^{c(z-1)}\nabla F(z*\x)\mathrm{d}z\right\rangle=\int_{0}^{1}e^{c(z-1)}\langle\y,\nabla F(z*\x)\rangle\mathrm{d}z\ge\int_{0}^{1}e^{c(z-1)}(F(\y)-cF(z*\x))\mathrm{d}z,
				\end{equation}  where the inequality follows from Lemma~\ref{lemma:thm2:1}.
	Combining Eq.\eqref{equ:thm2:1} with Eq.\eqref{equ:th1_2}, we have 
		\begin{equation*}
			\left\langle\y-\x, \int_{0}^{1}e^{c(z-1)}\nabla F(z*\x)\mathrm{d}z\right\rangle\ge \Big(\int_{0}^{1}e^{c(z-1)}\mathrm{d}z\Big)F(\y)-F(\x)=\Big(\int_{0}^{1}e^{c(z-1)}\mathrm{d}z\Big)F(\y)-F(\x).
			\end{equation*}
Note the $\int_{0}^{1}e^{c(z-1)}\mathrm{d}z=\frac{1-e^{-c}}{c}$.
	\end{proof}
	
 \section{Proof of Theorem~\ref{thm:final_one}}\label{appendix:1}
	In this section, we show the proof of Theorem~\ref{thm:final_one}.
	Before going into the details, we firstly review a standard lemma for the mirror projection.
	\begin{lemma}[\citet{chen1993convergence,jadbabaie2015online}]\label{lemma:1}
		Let $\phi:[0,1]^{n}\rightarrow\R$ be a $1$-strongly convex function with respect to the norm $\|\cdot \|$  and $D_{\phi}(\x,\y)$ represent the Bregman divergence with respect to $\phi$, respectively. Then,  any update of the form 
		\begin{equation*}
			\x^{+}=\min_{\y\in\C}\langle\textbf{b},\y\rangle+\mathcal{D}_{\phi}(\y,\x),
		\end{equation*} satisfies the following inequality
		\begin{equation*}
			\langle\x^{+}-\z,\textbf{b}\rangle\le \mathcal{D}_{\phi}(\z,\x)-\mathcal{D}_{\phi}(\z,\x^{+})-\mathcal{D}_{\phi}(\x^{+},\x),
		\end{equation*} for any $\z\in\C$ where $\C$ is a  convex domain  in $[0,1]^{n}$.
	\end{lemma} 
	Next, we verify that each local variable $\x_{t,i}$ of Algorithm~\ref{alg:BDOMA} is included in the constraint of continuous problem Eq.\eqref{equ:continuous_max}  for any $t\in[T]$ and $i\in\N$.
	\begin{lemma}\label{lemma:included_convex_constaint}
	In Algorithm~\ref{alg:BDOMA}, if we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption~\ref{ass:1} holds,we have that, for any $t\in[T]$ and $i\in\N$,  we have that, for any $t\in[T]$ and $i\in\N$, $\x_{t,i}\in\C$ and $\y_{t,i}\in\C$.
	\end{lemma}
	\begin{proof}
	We prove this lemma by induction.
	At first, from the Line 2 in Algorithm~\ref{alg:BDOMA}, we know that $\x_{1,i}\in\C$ for any $i\in\N$. Moreover, due to Assumption~\ref{ass:1} and Line 8, we also can infer $\y_{1,i}=\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\x_{1,j}\in\C$ for any $i\in\N$.
	For any $1\le t<T$, if we assume $\x_{t,i}\in\C$ and $\y_{t,i}\in\C$ for any $i\in\N$. Then from the Line 12 in Algorithm~\ref{alg:BDOMA}, we know $\sum_{a\in\V_{j}}[\x_{t+1,i}]_{a}=\sum_{a\in\V_{j}}[\y_{t,i}]_{a}\le1$ for any $j\neq i$ and $i\in\N_{i}$. Furthermore, Line 13 implies that $[\x_{t+1,i}]_{\V_{i}}$ is the projection over the constraint $\sum_{k=1}^{n_{i}}b_{k}\le1$, so $\sum_{a\in\V_{i}}[\x_{t+1,i}]_{a}\le1$. we can conclude that $\x_{t+1,i}\in\C$. Due to  Assumption~\ref{ass:1}, we can further show $\y_{t+1,i}\in\C$.
	\end{proof}
	With Lemma~\ref{lemma:included_convex_constaint}, we integrate the probability update procedures for  actions $i\in\V_{i}$ and those not in $\V_{i}$ within Algorithm~\ref{alg:BDOMA}, i,e, Lines 12-13.
		\begin{lemma}\label{lemma:simplify}
		In Algorithm~\ref{alg:BDOMA}, if we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption~\ref{ass:2} and \ref{ass:1} hold,we have that, for any $t\in[T]$ and $i\in\N$, 
		\begin{equation*}
			\x_{t+1,i}=\mathop{\arg\min}_{\x\in\C}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{\phi}(\x, \y_{t,i})\Big), 
		\end{equation*}where $\odot$ denotes the coordinate-wise multiplication, i.e.,the $i$-th element of vector $\x\odot\y$ is $x_{i}y_{i}$, and $\one_{\V_{i}}$ denotes a $n$-dimensional vector where the entries at $\V_{i}$ is equal to $1$ and all others are $0$.
	\end{lemma}
	\begin{proof}
		Firstly, we define the solution of the problem $\min_{\x\in\C}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{\phi}(\x, \y_{t,i})\Big)$ as $\textbf{o}^{*}\in[0,1]^{n}$. Due to Assumption~\ref{ass:2},
		\begin{equation*}
			\begin{aligned}
				\textbf{o}^{*}&=\mathop{\arg\min}_{\x\in\C}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{\phi}(\x, \y_{t,i})\Big)\\
				&=\mathop{\arg\min}_{\x\in\C}\Big(-\langle[\widetilde{\nabla} F_{t}^{s}(\x_{t,i})]_{\V_{i}},[\x]_{\V_{i}}\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{g,n_{i}}([\x]_{\V_{i}}, [\y_{t,i}]_{\V_{i}})+\frac{1}{\eta_{t}}\mathcal{D}_{g,n-n_{i}}([\x]_{\V\setminus\V_{i}}, [\y_{t,i}]_{\V\setminus\V_{i}})\Big).
			\end{aligned}
		\end{equation*}
As a result, we can conclude that
\begin{equation}\label{equ:simplify}
\begin{aligned}
	&[\textbf{o}^{*}]_{\V_{i}}=\mathop{\arg\min}_{\sum_{a\in\V_{i}}b_{a}\le1}\Big(-\langle[\widetilde{\nabla} F_{t}^{s}(\x_{t,i})]_{\V_{i}},\textbf{b}\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{g,n_{i}}(\textbf{b}, [\y_{t,i}]_{\V_{i}})\Big),\\
&[\textbf{o}^{*}]_{\V\setminus\V_{i}}=\mathop{\arg\min}_{\sum_{a\in\V_{j}}z_{a}\le1, \forall j\in\N\setminus\{i\}}\Big(\mathcal{D}_{g,n_{i}}(\textbf{z}, [\y_{t,i}]_{\V\setminus\V_{i}})\Big)=[\y_{t,i}]_{\V\setminus\V_{i}}, \ \  \ (\text{Note that}\ \y_{t,i}\in\C\text{\ (See lemma~\ref{lemma:included_convex_constaint})})
\end{aligned}
\end{equation} where  $\textbf{b}\in[0,1]^{n_{i}}$ and $\textbf{z}\in[0,1]^{n-n_{i}}$.
From the Eq.\eqref{equ:simplify} and Lines 12-13 in Algorithm~\ref{alg:BDOMA}, we get the $\textbf{o}^{*}=\x_{t+1,i}$.
	\end{proof}
In the following part, we define some commonly used symbols for the proof of Theorem~\ref{thm:final_one}: 
	\begin{equation*}
		\begin{aligned}
			&\bar{\x}_{t}:=\frac{\sum_{i=1}^{N}\x_{t,i}}{N},\ \ \ \x_{t}^{cate}:=[\x_{t,1};\x_{t,2};\dots;\x_{t,N}]\in\R^{n*N};\\
			&\bar{\y}_{t}:=\frac{\sum_{i=1}^{N}\y_{t,i}}{N},\ \ \ \y_{t}^{cate}:=[\y_{t,1};\y_{t,2};\dots;\y_{t,N}]\in\R^{n*N};\\
			&\mathbf{r}_{t,i}:=\x_{t+1,i}-\y_{t,i},\ \ \ \mathbf{r}_{t}^{cate}:=[\mathbf{r}_{t,1};\mathbf{r}_{t,2};\dots;\mathbf{r}_{t,N}]\in\R^{n*N};
		\end{aligned}
	\end{equation*}
With these symbols, we can verify that 
	\begin{lemma}\label{lemma:2} If we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption \ref{ass:2} and \ref{ass:4} hold, we have that
		\begin{equation*}
			\E(\|\mathbf{r}_{t,i}\|)=\E(\|\x_{t+1,i}-\y_{t,i}\|)\le G\eta_{t}.
		\end{equation*}
	\end{lemma}
	\begin{proof}
		According to Lemma~\ref{lemma:simplify}, we have that 
		\begin{equation*}
			\x_{t+1,i}=\mathop{\arg\min_{\x\in\C}}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{\phi}(\x, \y_{t,i})\Big),
		\end{equation*} where $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$.  
		
		From the Lemma~\ref{lemma:1}, we have
		\begin{equation}\label{equ:lemma21}
			\eta_{t}\langle	\x_{t+1,i}-\x,-\tilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\rangle\le \mathcal{D}_{\phi}(\x,\y_{t,i})-\mathcal{D}_{\phi}(\x,\x_{t+1,i})-\mathcal{D}_{\phi}(\x_{t+1,i},\y_{t,i}),
		\end{equation} for any $\x\in\C$.
		If we set $\x=\y_{t,i}$\footnote{Note that we prove $\y_{t,i}\in\C$ in Lemma~\ref{lemma:included_convex_constaint}.} in Eq.\eqref{equ:lemma21}, we have that
		\begin{equation*}
			\eta_{t}\langle	\x_{t+1,i}-\y_{t,i},\tilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\rangle\ge\mathcal{D}_{\phi}(\y_{t,i},\x_{t+1,i})+\mathcal{D}_{\phi}(\x_{t+1,i},\y_{t,i})\ge\|\x_{t+1,i}-\y_{t,i}\|,
		\end{equation*} where the final inequality follows from the $1$-strongly convex of $\phi$.
		
		From the Young inequality, we have that $\eta_{t}\langle	\x_{t+1,i}-\y_{t,i},\tilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\rangle\le\frac{\|\x_{t+1,i}-\y_{t,i}\|}{2}+\frac{\|\eta_{t}\tilde{\nabla} F_{t}^{A}(\x_{t,i})\odot\one_{\V_{i}}\|_{*}}{2}$ such that $\E(\|\x_{t+1,i}-\y_{t,i}\|)\le\E(\|\eta_{t}\tilde{\nabla}F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\|_{*})\le\eta_{t}G$ from the Assumption~\ref{ass:4}.
			\end{proof}
With this Lemma~\ref{lemma:2}, we next derive an upper bound about the deviation between $\x_{t+1,i},\y_{t+1,i}$ and the average $\bar{\x}_{t+1}$.
	\begin{lemma}\label{lemma:3}
		Under the Assumption \ref{ass:2}, \ref{ass:1}  and \ref{ass:4}, we have that, for any $t\in[T]$ and $i\in\N$, 
		\begin{equation*}
			\begin{aligned}
				&\E(\|\x_{t+1,i}-\bar{\x}_{t+1}\|)\le \sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}\eta_{\tau}G,\\
				&\E(\|\y_{t+1,i}-\bar{\x}_{t+1}\|)\le \sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}\eta_{\tau}G,
			\end{aligned}
		\end{equation*} where $\beta=\max(|\lambda_{2}(\W)|,|\lambda_{N}(\W)|)$ is the second largest magnitude of the eigenvalues of the weight matrix $\W$.
	\end{lemma}
	\begin{proof}
		From the definition of $\mathbf{r}_{t,i}$, we can conclude that 
		\begin{equation}\label{lemma31}
			\x_{t+1,i}=\mathbf{r}_{t,i}+\y_{t,i}=\mathbf{r}_{t,i}+\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\x_{t,j},
		\end{equation} where the final equality follows from Line 8 in Algorithm~\ref{alg:BDOMA}. 
		
		As a result, from the Eq.\eqref{lemma31}, we can show that
		\begin{equation}\label{lemma32}
			\begin{aligned}
				\x_{t+1}^{cate}& =\mathbf{r}_{t}^{cate}+(\W\otimes\mathbf{I}_{n})\x_{t}^{cate}\\
				&=\sum_{\tau=1}^{t}(\W\otimes\mathbf{I}_{n})^{t-\tau}\mathbf{r}_{\tau}^{cate}\\
				&=\sum_{\tau=1}^{t}(\W^{t-\tau}\otimes\mathbf{I}_{n})\mathbf{r}_{\tau}^{cate},
			\end{aligned}
		\end{equation} where the symbol $\otimes$ denotes the Kronecker product.
		
		If we also define $\bar{\x}_{t}^{cate}=[\bar{\x}_{t};\bar{\x}_{t};\dots;\bar{\x}_{t}]\in\R^{n*N}$ and from the Eq.\eqref{lemma32}, we also have that
		\begin{equation}\label{lemma33}
			\begin{aligned}				\bar{\x}_{t+1}^{cate}&=\left(\frac{\one_{N}\one_{N}^{T}}{N}\otimes\mathbf{I}_{N}\right)\x_{t+1}^{cate}\\
				&=\sum_{\tau=1}^{t}\left(\frac{\one_{N}\one_{N}^{T}}{N}\otimes\mathbf{I}_{n}\right)\mathbf{r}_{\tau}^{cate}.
			\end{aligned}
		\end{equation}
		Then, from the Eq.\eqref{lemma32} and Eq.\eqref{lemma33}, we have that , for any $i\in\N$,
		\begin{equation}\label{lemma34}
			\x_{t+1,i}-\bar{\x}_{t+1}=\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}\left([\W^{t-\tau}]_{ij}-\frac{1}{N}\right)\mathbf{r}_{\tau,j}.
		\end{equation}
		Eq.\eqref{lemma34} indicates that
		\begin{equation*}
			\begin{aligned}
				\E\left(\left\|\x_{t+1,i}-\bar{\x}_{t+1}\right\|\right)&=\E\left(\left\|\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}([\W^{t-\tau}]_{ij}-\frac{1}{N})\mathbf{r}_{\tau,j}\right\|\right)
                \\&\le\E\left(\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}\left|[\W^{t-\tau}]_{ij}-\frac{1}{N}\right |\left\|	\mathbf{r}_{\tau,j}\right\|\right)\\&\le\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}\left|[\W^{t-\tau}]_{ij}-\frac{1}{N}\right|\eta_{\tau}G\le\sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}\eta_{\tau}G,
			\end{aligned}
		\end{equation*} where the second inequality comes from Lemma~\ref{lemma:2} and the final inequality follows from $\sum_{j\in\N_{i}\cup\{i\}}|[\W^{t-\tau}]_{ij}-\frac{1}{N}|\le\sqrt{N}\beta^{t-\tau}$(See Proposition 1  in \citep{nedic2009distributed}).
		Due to $\y_{t+1,i}=\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\x_{t+1,j}$ we also can have $\E(\|\y_{t+1,i}-\bar{\x}_{t+1}\|)\le\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\E(\|\x_{t+1,j}-\bar{\x}_{t+1}\|)\le \sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}\eta_{\tau}G$.

	\end{proof}
	\begin{lemma}\label{lemma:4} 
			Consider our proposed Algorithm~\ref{alg:BDOMA}, if Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3},\ref{ass:4} hold and each set function $f_{t}$ is monotone submodular with curvature $c$ for any $t\in[T]$, then we can conclude that,
		\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}(\bar{\x}_{t})\Big)\\
				&\le(3G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\mathcal{D}_{\phi}(\x^{*}_{t},\y_{t,i})-\mathcal{D}_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t},
			\end{aligned}
		\end{equation*} where $\x_{t}^{*}$ is the optimal solution of Eq.\eqref{equ:continuous_max} and $D=\sup_{\x,\y\in\C}\|\x-\y\|$ where $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$.
	\end{lemma}
	\begin{proof}
		From the Eq.\eqref{equ:boosting1} in Theorem~\ref{thm:2}, we have that
		\begin{equation}
			\label{equ:lemma_result_0}
			\begin{aligned}
				\Big(\frac{1-e^{-c}}{c}\Big)F_{t}(\x_{t}^{*})-F_{t}(\bar{\x}_{t})&\le\langle\nabla F_{t}^{s}(\bar{\x}_{t}),\x_{t}^{*}-\bar{\x}_{t}\rangle\\
				&=\underbrace{\langle\nabla F_{t}^{s}(\bar{\x}_{t})-\sum_{i\in\N}\Big(\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\Big),\x_{t}^{*}-\bar{\x}_{t}\rangle}_{\text{\textcircled{1}}}\\
				&\quad+\underbrace{\sum_{i\in\N}\langle\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x_{t}^{*}-\x_{t,i}\rangle}_{\text{\textcircled{2}}}\\
				&\quad +\underbrace{\sum_{i\in\N}\langle\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x_{t,i}-\bar{\x}_{t}\rangle}_{\text{\textcircled{3}}},
			\end{aligned}
		\end{equation} where $\odot$ denotes the coordinate-wise multiplication, i.e.,the $i$-th element of vector $\x\odot\y$ is $x_{i}y_{i}$, and $\one_{\V_{i}}$ denotes a $n$-dimensional vector where the entries at $\V_{i}$ is equal to $1$ and all others are $0$.
		
		For \textcircled{1}, we have
		\begin{equation}\label{equ:lemma_result_1}
			\begin{aligned}
				&\langle\nabla F_{t}^{s}(\bar{\x}_{t})-\sum_{i\in\N}\Big(\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\Big),\x_{t}^{*}-\bar{\x}_{t}\rangle\\
				&\le\left\|\nabla F_{t}^{s}(\bar{\x}_{t})-\sum_{i\in\N}\Big(\nabla  F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\Big)\right\|_{*}\left\|\x_{t}^{*}-\bar{\x}_{t}\right\|\\
				&\le\|\x^{*}_{t}-\bar{\x}_{t}\|\sum_{i\in\N}\Big(\|[\nabla F_{t}^{s}(\bar{\x}_{t})]_{\V_{i}}-[\nabla F_{t}^{s}(\x_{t,i})]_{\V_{i}}\|_{*}\Big)\\
				&\le\|\x^{*}_{t}-\bar{\x}_{t}\|\sum_{i\in\N}\Big(\|\nabla F_{t}^{s}(\bar{\x}_{t})-\nabla F_{t}^{s}(\x_{t,i})\|_{*}\Big)\\
				&\le\|\x^{*}_{t}-\bar{\x}_{t}\|\sum_{i\in\N}\Big(L\|\bar{\x}_{t}-\x_{t,i}\|\Big)\\
				&\le LD\sum_{i\in\N}\Big(\|\bar{\x}_{t}-\x_{t,i}\|\Big)\\
				&\le LDG\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau},
			\end{aligned}
		\end{equation} where  the fourth inequality follows from Assumption~\ref{ass:4}; the fifth comes from  $D=\sup_{\x,\y\in\C}\|\x-\y\|$ and the final inequality from Lemma~\ref{lemma:3}.
		
		For \textcircled{3}, from  Assumption~\ref{ass:4} and  Lemma~\ref{lemma:3}, 
		we have, 
		\begin{equation}\label{equ:lemma_result_3}
			\E\Big(\sum_{i\in\N}\langle\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x_{t,i}-\bar{\x}_{t}\rangle\Big)\le G\sum_{i\in\N}\E\Big(\|\x_{t,i}-\bar{\x}_{t}\|\Big)\le\sum_{\tau=1}^{t}GN^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}.
		\end{equation}
		As for \textcircled{2},
		we have, %+\x_{t+1,i}-\y_{t,i}+\y_{t,i}-\x_{t,i}
		\begin{equation}\label{equ:lemma_result_2}
			\begin{aligned}
				&\E\Big(\sum_{i\in\N}\langle\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x_{t}^{*}-\x_{t,i}\rangle\Big)\\
				&=\E\Bigg(\E\Big(\sum_{i\in\N}\langle\nabla F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x^{*}_{t}-\x_{t,i}\rangle|\x_{t,i}\Big)\Bigg)\\
				&=\E\Bigg(\E\Big(\sum_{i\in\N}\langle\tilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x^{*}_{t}-\x_{t,i}\rangle|\x_{t,i}\Big)\Bigg)\\
				&=\underbrace{\sum_{i\in\N}\E\Big(\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x^{*}_{t}-\x_{t+1,i}\rangle\Big)}_{\text{\textcircled{4}}}\\
				&+\underbrace{\sum_{i\in\N}\E\Big(\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x_{t+1,i}-\y_{t,i}\rangle\Big)}_{\text{\textcircled{5}}}\\
				&+\underbrace{\sum_{i\in\N}\E\Big(\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\y_{t,i}-\x_{t,i}\rangle\Big)}_{\text{\textcircled{6}}}.
			\end{aligned}
		\end{equation}
		For \textcircled{4}, from Lemma~\ref{lemma:simplify}, we know that
		\begin{equation*}
			\x_{t+1,i}=\mathop{\arg\min}_{\x\in\C}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{\phi}(\x, \y_{t,i})\Big).
		\end{equation*} 
		
		So, from the Lemma~\ref{lemma:1}, we can show that
		\begin{equation}\label{equ:lemma41}
			\eta_{t}\langle	\x_{t+1,i}-\x,-\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}}\rangle\le \D_{\phi}(\x,\y_{t,i})-\D_{\phi}(\x,\x_{t+1,i})-\D_{\phi}(\x_{t+1,i},\y_{t,i}),
		\end{equation} for any $\x\in\C$.
		If we set $\x=\x^{*}_{t}$ in Eq.\eqref{equ:lemma41},  we have  
		\begin{equation}
			\label{equ:lemma_result_4}
			\begin{aligned}
				\text{\textcircled{4}}&\le \frac{1}{\eta_{t}}\sum_{i\in\N}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})-\D_{\phi}(\x_{t+1,i},\y_{t,i})\Big)\\
				&\le \frac{1}{\eta_{t}}\sum_{i\in\N}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)-\sum_{i\in\N}\E\left(\frac{\|\x_{t+1,i}-\y_{t,i}\|}{2\eta_{t}}\right).
			\end{aligned}
		\end{equation}
		For \textcircled{5}, by Young's inequality, we have that
		\begin{equation}
			\label{equ:lemma_result_5}
			\begin{aligned}
				\text{\textcircled{5}}&\le\sum_{i\in\N}\E\left(\frac{\|\x_{t+1,i}-\y_{t,i}\|}{2\eta_{t}}\right)+\sum_{i\in\N}\E\left(\frac{\eta_{t}}{2}\|\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\otimes\one_{\V_{i}}\|_{*}\right),\\
				&\le\sum_{i\in\N}\E\left(\frac{\|\x_{t+1,i}-\y_{t,i}\|}{2\eta_{t}}\right)+\frac{\eta_{t}NG}{2}.
			\end{aligned}
		\end{equation}
		For \textcircled{6}, we have that,
		\begin{equation}
			\label{equ:lemma_result_6}
			\begin{aligned}
				\text{\textcircled{6}}&\le\Big(\sum_{i\in\N}\E\Big(\|\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\otimes\one_{\V_{i}}\|_{*}\|\x_{t,i}-\y_{t,i}\|\ |\ \x_{t,i},\forall i\in\N\Big)\Big)\\
				&\le \Big(\E(\|\tilde{\nabla} F_{t}^{A}(\x_{t,i})\|_{*})\Big)\Big(\sum_{i\in\N}\E(\|	\x_{t,i}-\bar{\x}_{t}\|)+\E(\|	\y_{t,i}-\bar{\x}_{t}\|)\Big)\\
				&\le2\sum_{\tau=1}^{t}GN^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}.
			\end{aligned}
		\end{equation}
		Merging Eq.\eqref{equ:lemma_result_1},\eqref{equ:lemma_result_2},\eqref{equ:lemma_result_3},\eqref{equ:lemma_result_4},\eqref{equ:lemma_result_5} and \eqref{equ:lemma_result_6} into Eq.\eqref{equ:lemma_result_0}, we have that
		\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)F_{t}(\x^{*}_{t})-F_{t}(\bar{\x}_{t})\le\langle\nabla F_{t}^{s}(\bar{\x}_{t}),\x^{*}_{t}-\bar{\x}_{t}\rangle\\
				&\le(3G+LD)\Big(\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{1}{\eta_{t}}\sum_{i\in\N}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)+\frac{\eta_{t}NG}{2}.		
			\end{aligned}
		\end{equation*}
As a result, we get the result in Lemma~\ref{lemma:4}.
	\end{proof}
	Next, we prove an upper bound of $\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)$, that is, 
	\begin{lemma}\label{lemma:5} If Assumption \ref{ass:2}-\ref{ass:4} hold, we have that
		\begin{equation*}
		\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\le\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|,
		\end{equation*} where $\x_{t}^{*}$ is the optimal solution of Eq.\eqref{equ:continuous_max}, $R^{2}:=\sup_{\x,\y\in\C}\mathcal{D}_{\phi}(\x,\y)$, and $\C$ is the constraint set in Eq.\eqref{equ:continuous_max}. 
	\end{lemma}
	\begin{proof}
		\begin{equation*}
			\begin{aligned}
				&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\\
				&=\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Big(\frac{1}{\eta_{t}}\E(\D_{\phi}(\x^{*}_{t},\y_{t,i}))-\frac{1}{\eta_{t+1}}\E(\D_{\phi}(\x^{*}_{t+1},\y_{t+1,i}))\Big)}_{\text{\textcircled{1}}}\\
				&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\D_{\phi}(\x^{*}_{t+1},\y_{t+1,i})-\D_{\phi}(\x^{*}_{t},\y_{t+1,i})\Big)\Bigg)}_{\text{\textcircled{2}}}\\
				&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t+1,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\Bigg)}_{\text{\textcircled{3}}}\\
				&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Big(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\Big)\E\Big(\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)}_{\text{\textcircled{4}}}.
			\end{aligned}
		\end{equation*}
		Firstly, we have \textcircled{1}$\le\frac{N R^{2}}{\eta_{1}}$ and \textcircled{2}$\le \sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|$ from Assumption~\ref{ass:3+}.
		
		Then, from the separate convexity, we have 
		\begin{equation*}
			\begin{aligned}
				&\text{\textcircled{3}}=\sum_{t=1}^{T}\sum_{i\in\N}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\D_{\phi}(\x^{*}_{t},\y_{t+1,i})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\Bigg)\\
				&=\sum_{t=1}^{T}\sum_{i\in\N}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\D_{\phi}(\x^{*}_{t},\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\x_{t+1,j})-\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\Bigg)\\		
				&\le\sum_{t=1}^{T}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\sum_{i\in\N}\sum_{j\in\N_{i}\cup\{i\}}\Big(w_{ij}\D_{\phi}(\x^{*}_{t},\x_{t+1,j})\Big)-\sum_{i\in\N}\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\Bigg)\\		
				&=\sum_{t=1}^{T}\Bigg(\frac{1}{\eta_{t+1}}\E\Big(\sum_{i\in\N}\Big((\sum_{j\in\N_{i}\cup\{i\}}w_{ji})\D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)-\sum_{i\in\N}D_{\phi}(\x^{*}_{t},\x_{t+1,i})\Big)\Bigg)\\		
				&=0,
			\end{aligned}
		\end{equation*} where the first inequality follows from Assumption~\ref{ass:3}, and the third inequality is due to $w_{ij}=w_{ji}$.
		
		Moreover, we have \textcircled{4}$\le NR^{2}\Big(\frac{1}{\eta_{T+1}}-\frac{1}{\eta_{1}}\Big)$. We finally get 
		\begin{equation*}
		\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(D_{\phi}(\x^{*},\y_{t,i})-D_{\phi}(\x^{*},\x_{t+1,i})\Big)\le \frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|.
		\end{equation*}
	\end{proof}
	As a result, we can prove the following Lemma:
	\begin{lemma}\label{thm:3}
	If Assumption \ref{ass:2}-\ref{ass:4} hold and each set function $f_{t}$ is monotone submodular with curvature $c$ for any $t\in[T]$, then
		\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\Big)\\
				&\le(4G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.		
			\end{aligned}
		\end{equation*}
	\end{lemma}
	\begin{proof}
		From the Lemma~\ref{lemma:4} and  Lemma~\ref{lemma:5}, we have that
		\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}(\bar{\x}_{t})\Big)\\
				&\le (3G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.	
			\end{aligned}
		\end{equation*}
		From the Assumption~\ref{ass:4}, we also can show that $|F_{t}(\x)-F_{t}(\y)|\le G\|\x-\y\|$ for any $t\in[T]$ such that we have $|F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)-F_{t}(\bar{\x}_{t})|\le G\|\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}-\bar{\x}_{t}\|\le G\sum_{i\in\N}\|\x_{t,i}-\bar{\x}_{t}\|\le G\Big(\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)$. Thus, we have that
\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}})\Big)\\
&\le \Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}(\bar{\x}_{t})\Big)+\big|\sum_{t=1}^{T}\E\Big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)-F_{t}\big(\bar{\x}_{t}\big)\Big)\big|\\
            &\le (4G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\x^{*}_{t+1}-\x^{*}_{t}\|+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.	
			\end{aligned}
		\end{equation*}
\end{proof}
From \citet{calinescu2011maximizing,chekuri2014submodular}, we know that the optimal value of continuous problem Eq.\eqref{equ:continuous_max} is equal to the optimal value of the corresponding discrete submodular maximization Eq.\eqref{equ:problem_t}, so we can set $\x^{*}_{t}:=\one_{\mathcal{A}_{t}^{*}}$ where $\mathcal{A}_{t}^{*}$ is the maximizer of Eq.\eqref{equ:problem_t}.

Next, we show a relationship between $\E\big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\big)$ and $\E\big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\big)$.
	\begin{lemma}\label{lemma:rounding1}
			If the function $f_{t}$ is monotone submodular and $a_{t,i}$ is the action taken by the agent $i\in\N$ at time $t$, then we have 
		\begin{equation*}
		\E\Big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big)\ge\E\Big(\big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\Big).  
		\end{equation*}
	\end{lemma}
\begin{proof}
	We prove this lemma by induction on $N=|\N|$. When $N=1$, for any $t\in[T]$, we have that
	\begin{equation}\label{equ:rounding1}
		\begin{aligned}
			\E(F_{t}(\x_{t,1}))&\le \E(F_{t}(\frac{\x_{t,1}}{\|\x_{t,1}\|_{1}})\le \E_{\mathcal{R}\sim\frac{\x_{t,1}}{\|\x_{t,1}\|_{1}}}\Big(f_{t}(\mathcal{R})\Big)\\
			&\le \E_{\mathcal{R}\sim\frac{\x_{t,1}}{\|\x_{t,1}\|_{1}}}\Big(\sum_{a\in\mathcal{R}}f_{t}(a)\Big)\\
			&=\sum_{a\in\V}\frac{[\x_{t,1}]_{a}}{\|\x_{t,1}\|_{1}}f_{t}(a)\\
			&=\E(f_{t}(a_{t,1})),	\end{aligned}
	\end{equation} where the first inequality follows from the monotonicity of $f_{t}$ and $\|\x_{t,1}\|_{1}\le1$; the second one from  the submodularity of $f_{t}$ and Line 5-6 in Algorithm~\ref{alg:BDOMA}.
	
Now let $N>1$. For any $\mathcal{R}\subseteq\V$, we denote $\mathcal{R}_{1}=\mathcal{R}\setminus\V_{N}$ and $s_{i}=\sum_{a\in\V_{i}}[\x_{t,i}]_{a}$ for any $i\in\N$. Then, we have that\begin{equation*}
	\begin{aligned}
		\E\Big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\Big)&\le 	\E\Big(F_{t}\big(\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}\big)\Big)=\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R})\Big)\\
		&=\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1})+f_{t}(\mathcal{R})-f_{t}(\mathcal{R}_{1})\Big)\\
		&=\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R})-f_{t}(\mathcal{R}_{1})\Big)+\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1})\Big)\\
	&=\E\Bigg(\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R})-f_{t}(\mathcal{R}_{1})\Big|\mathcal{R}_{1}\Big)\Bigg)+\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1})\Big)\\
	&\le\E\Bigg(\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1}\cup\{a_{t,N}\})-f_{t}(\mathcal{R}_{1})\Big|\mathcal{R}_{1}\Big)\Bigg)+\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1})\Big)\\	&=\E_{\mathcal{R}\sim\sum_{i\in\N}\frac{\x_{t,i}}{s_{i}}\odot\one_{\V_{i}}}\Big(f_{t}(\mathcal{R}_{1}\cup\{a_{t,N}\})\Big)\\
	&\le \E\Big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big),
	\end{aligned}
\end{equation*} where the first inequality follows from the monotonicity of $f_{t}$ and $s_{i}\le1$ and we get the second inequality follows from repeating the proof in Eq.\eqref{equ:rounding1} because $f_{t}(\mathcal{R})-f_{t}(\mathcal{R}_{1})=f_{t}(\mathcal{R})-f_{t}(\mathcal{R}\setminus\V_{N})$ is a submodular function over $\V_{N}$ for any fixed $\mathcal{R}\subseteq\V$ and $a_{t,N}$ is selected from the set $\V_{N}$ according to $\frac{[\x_{t,N}]_{\V_{N}}}{s_{N}}$.
\end{proof}
Merging Lemma~\ref{lemma:rounding1} into Lemma~\ref{thm:3}, we can get that
	\begin{equation*}
	\begin{aligned}
		&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}^{*}_{t})-\sum_{t=1}^{T}\E\Big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big)\\
		&\le(4G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KN} {\eta_{t+1}}\|\one_{\mathcal{A}_{t+1}^{*}}-\one_{\mathcal{A}_{t}^{*}}\|+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.		
	\end{aligned}
\end{equation*}
We know that all norms in finite-dimensional real space are equivalent~\citep{lax2014functional,zhao2024minimax}, so $\|\x\|\le C_{2}\|\x\|_{1}$.
Finally,  we can verify the Eq.\eqref{equ:thm1_equation_uncomplete} in Theorem~\ref{thm:final_one}, that is,
	\begin{equation*}
	\begin{aligned}
		&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}^{*}_{t})-\sum_{t=1}^{T}\E\Big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big)\\
		&\le(4G+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}\eta_{\tau}\Big)+\frac{NR^{2}}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{KNC_{2}} {\eta_{t+1}}|\mathcal{A}_{t+1}^{*}\Delta\mathcal{A}_{t}^{*}|+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.		
	\end{aligned}
\end{equation*}
	\section{Proof of Theorem~\ref{thm:projection}}\label{append:proof4}
	In this section, we prove the Theorem~\ref{thm:projection}.
	\begin{proof}
	When $g(x)=x\log(x)$, we know that, for any $\mathbf{b},\mathbf{y}\in(0,1)^{m}$ 
	\begin{equation*}
	\D_{g,m}(\mathbf{b}, \mathbf{y})=\sum_{i=1}^{m}\Big(b_{i}\log(\frac{b_{i}}{y_{i}})\Big)-\sum_{i=1}^{m}b_{i}+\sum_{i=1}^{m}y_{i}.
	\end{equation*}
	Next, we consider the Lagrangian function, for any fixed $\y,\mathbf{z}\in(0,1)^{n}$,
	\begin{equation*}
L(\mathbf{b},\lambda)=\sum_{i=1}^{m}z_{i}b_{i}+\sum_{i=1}^{m}\Big(b_{i}\log(\frac{b_{i}}{y_{i}})\Big)-\sum_{i=1}^{m}b_{i}+\sum_{i=1}^{m}y_{i}+\lambda(\sum_{i=1}^{m}b_{i}-1).
	\end{equation*}
Then, we have that
\begin{equation}\label{equ:dual_lamba}
	\frac{\partial L(\mathbf{b},\lambda)}{\partial b_{i}}=z_{i}+\log(\frac{b_{i}}{y_{i}})+\lambda,\ \forall i\in[m].
\end{equation}
Setting all equations in Eq.\eqref{equ:dual_lamba} to $0$, we can get $b_{i}=y_{i}\exp(-z_{i})\exp(-\lambda)$ for any $i\in[m]$ such that $L(\lambda)=-\sum_{i=1}^{m}\Big(y_{i}\exp(-z_{i})\Big)\exp(-\lambda)-\lambda$. When $\sum_{i=1}^{m}\Big(y_{i}\exp(-z_{i})\Big)\le 1$, $L(0)=\max_{\lambda\ge0}L(\lambda)$ such that the optimal solution $b_{i}^{*}=y_{i}\exp(-z_{i})$. Similarly, when $\sum_{i=1}^{m}\Big(y_{i}\exp(-z_{i})\Big)>1$,
\begin{equation*}
L\Bigg(\log\Big(\sum_{i=1}^{m}\big(y_{i}\exp(-z_{i})\big)\Big)\Bigg)=\max_{\lambda\ge0}L(\lambda),
\end{equation*} so $b_{i}^{*}=y_{i}\exp(-z_{i})\exp\Bigg(-\log\Big(\sum_{i=1}^{m}\big(y_{i}\exp(-z_{i})\big)\Big)\Bigg)=\frac{y_{i}\exp(-z_{i})}{\sum_{i=1}^{m}\big(y_{i}\exp(-z_{i})\big)}$.
\end{proof}
	\section{Proof of Theorem~\ref{thm:final_one1}}\label{append:proof5}
In this  section, we  present the proof of Theorem~\ref{thm:final_one1}. Specially, we assume $\|\cdot\|$ is $l_{1}$ norm in this section.
Like the Appendix~\ref{appendix:1}, we can show that 
\begin{lemma}\label{lemma:included_convex_constaint2}
	In Algorithm~\ref{alg:BDOEA}, if we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption~\ref{ass:1} holds,we have that, for any $t\in[T]$ and $i\in\N$,  we have that, for any $t\in[T]$ and $i\in\N$, $\x_{t,i}\in\C$ and $\y_{t,i}\in\C$.
\end{lemma}
\begin{lemma}\label{lemma:simplify2}
	In Algorithm~\ref{alg:BDOEA}, if we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption~\ref{ass:2} and \ref{ass:1} hold,we have that, for any $t\in[T]$ and $i\in\N$, 
	\begin{equation*}
		\x_{t+1,i}=\mathop{\arg\min}_{\x\in\C}\Big(-\langle\widetilde{\nabla} F_{t}^{s}(\x_{t,i})\odot\one_{\V_{i}},\x\rangle+\frac{1}{\eta_{t}}\mathcal{D}_{KL}(\x, \y_{t,i})\Big), 
	\end{equation*}where $\odot$ denotes the coordinate-wise multiplication, i.e.,the $i$-th element of vector $\x\odot\y$ is $x_{i}y_{i}$, and $\one_{\V_{i}}$ denotes a $n$-dimensional vector where the entries at $\V_{i}$ is equal to $1$ and all others are $0$.
\end{lemma}
\begin{lemma}\label{lemma::2} In Algorithm~\ref{alg:BDOEA}, if we set the constraint $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$ and Assumption \ref{ass:2} and \ref{ass:4} hold, we have that
	\begin{equation*}
		\E(\|\mathbf{r}_{t,i}\|_{1})=\E(\|\x_{t+1,i}-\y_{t,i}\|_{1})\le G\eta_{t}.
	\end{equation*}
\end{lemma}
Moreover, we also define the following new symbols for Algorithm~\ref{alg:BDOEA}:
	\begin{equation*}
		\begin{aligned}
			&\bar{\x}_{t}=\frac{\sum_{i=1}^{N}\x_{t,i}}{N},\ \ \ \x_{t}^{cate}=[\x_{t,1};\x_{t,2};\dots;\x_{t,N}]\in\R^{n*N};\\
			&\bar{\y}_{t}=\frac{\sum_{i=1}^{N}\y_{t,i}}{N},\ \ \ \y_{t}^{cate}=[\y_{t,1};\y_{t,2};\dots;\y_{t,N}]\in\R^{n*N};\\
			&\overline{\hat{\x}}_{t}=\frac{\sum_{i=1}^{N}\hat{\x}_{t,i}}{N},\ \ \ \hat{\x}_{t}^{cate}=[\hat{\x}_{t,1};\hat{\x}_{t,2};\dots;\hat{\x}_{t,N}]\in\R^{n*N};\\
			&\mathbf{r}_{t,i}=\x_{t+1,i}-\y_{t,i},\ \ \ \mathbf{r}_{t}^{cate}=[\mathbf{r}_{t,1};\mathbf{r}_{t,2};\dots;\mathbf{r}_{t,N}]\in\R^{n*N};
		\end{aligned}
	\end{equation*}
Then, we also can show that
	\begin{lemma}\label{lemma::3}
		In Algorithm~\ref{alg:BDOEA}, under the Assumption \ref{ass:2}, \ref{ass:1}  and \ref{ass:4}, we have that, for any $t\in[T]$ and $i\in\N$, 
		\begin{equation*}
			\begin{aligned}
				&\E((\|\x_{t+1,i}-\bar{\x}_{t+1}\|_{1})\le \sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}(1-\gamma)^{t-\tau}\eta_{\tau}G,\\
				&\E((\|\y_{t+1,i}-\bar{\x}_{t+1}\|_{1})\le \sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}(1-\gamma)^{t+1-\tau}\eta_{\tau}G+\gamma D,
			\end{aligned}
		\end{equation*} where $\beta=\max(|\lambda_{2}(\W)|,|\lambda_{N}(\W)|)$ is the second largest magnitude of the eigenvalues of the weight matrix $\W$ and $D=\sup_{\x,\y\in\C}\|\x-\y\|_{1}$ where $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$.
	\end{lemma}
		\begin{proof}
		From the definition of $\mathbf{r}_{t,i}$ and $\hat{\x}_{t,i}$, we can conclude that 
		\begin{equation}\label{lemma:31}
			\begin{aligned}
				\x_{t+1,i}&=\mathbf{r}_{t,i}+\y_{t,i}=\mathbf{r}_{t,i}+\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\hat{\x}_{t,j}\\
				&=\mathbf{r}_{t,i}+\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\bigg((1-\gamma)\x_{t,i}+\frac{\gamma}{n}\one_{n}\bigg).
			\end{aligned}		
		\end{equation} where the final equality follows from step 8 in Algorithm~\ref{alg:BDOEA}. 
		
		As a result, from the Eq.\eqref{lemma:31}, we can show that
		\begin{equation}\label{lemma:32}
			\begin{aligned}
				&\x_{t+1}^{cate}=\mathbf{r}_{t}^{cate}+(\W\otimes\mathbf{I}_{n})\bigg((1-\gamma)\x_{t}^{cate}+\frac{\gamma}{n}\one_{nN}\bigg)\\
				&=\mathbf{r}_{t}^{cate}+(1-\gamma)(\W\otimes\mathbf{I}_{n})\x_{t}^{cate}+\frac{\gamma}{n}\one_{nN}\\
				&=\sum_{\tau=1}^{t}(1-\gamma)^{t-\tau}(\W\otimes\mathbf{I}_{n})^{t-\tau}\bigg(\mathbf{r}_{\tau}^{cate}+\frac{\gamma}{n}\one_{nN}\bigg)\\
				&=\sum_{\tau=1}^{t}(1-\gamma)^{t-\tau}(\W^{t-\tau}\otimes\mathbf{I}_{n})\mathbf{r}_{\tau}^{cate}+\sum_{\tau=1}^{t}\frac{\gamma(1-\gamma)^{t-\tau}}{n}\one_{nN}.
			\end{aligned}
		\end{equation}
		If we define $\bar{\x}_{t}^{cate}=[\bar{\x}_{t};\bar{\x}_{t};\dots;\bar{\x}_{t}]\in\R^{n*N}$ and from the Eq.\eqref{lemma:32}, we also have that
		\begin{equation}\label{lemma:33}
			\begin{aligned}
				&\bar{\x}_{t+1}^{cate}=(\frac{\one_{N}\one_{N}^{T}}{N}\otimes\mathbf{I}_{N})\x_{t+1}^{cate}\\
				&=\sum_{\tau=1}^{t}(1-\gamma)^{t-\tau}(\frac{\one_{N}\one_{N}^{T}}{N}\otimes\mathbf{I}_{n})\mathbf{r}_{\tau}^{cate}+\sum_{\tau=1}^{t}\frac{\gamma(1-\gamma)^{t-\tau}}{n}\one_{nN}.
			\end{aligned}
		\end{equation}
		Then, from the Eq.\eqref{lemma:32} and Eq.\eqref{lemma:33}, we have that , for any $i\in\N$,
		\begin{equation}\label{lemma:34}
			\x_{t+1,i}-\bar{\x}_{t+1}=\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}(1-\gamma)^{t-\tau}([\W^{t-\tau}]_{ij}-\frac{1}{N})\mathbf{r}_{\tau,j}.
		\end{equation}
		Eq.\eqref{lemma:34} indicates that
		\begin{equation*}
			\begin{aligned}
				\E(\|\x_{t+1,i}-\bar{\x}_{t+1}\|_{1})&=\E(\|\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}(1-\gamma)^{t-\tau}([\W^{t-\tau}]_{ij}-\frac{1}{N})\mathbf{r}_{\tau,j}\|_{1})\\
				&\le\E(\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}(1-\gamma)^{t-\tau}|[\W^{t-\tau}]_{ij}-\frac{1}{N}|\|	\mathbf{r}_{\tau,j}\|_{1})\\
				&\le\sum_{\tau=1}^{t}\sum_{j\in\N_{i}\cup\{i\}}|[\W^{t-\tau}]_{ij}-\frac{1}{N}|(1-\gamma)^{t-\tau}\eta_{\tau}G\\
				&\le\sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}(1-\gamma)^{t-\tau}\eta_{\tau}G,
			\end{aligned}
		\end{equation*} where the second inequality comes from Lemma~\ref{lemma::2} and the final inequality follows from $\sum_{j\in\N_{i}\cup\{i\}}|[\W^{t-\tau}]_{ij}-\frac{1}{N}|\le\sqrt{N}\beta^{t-\tau}$.
		Due to $\y_{t+1,i}=\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\hat{\x}_{t+1,j}$ we also can have 
		\begin{equation*}
			\begin{aligned}
				\E((\|\y_{t+1,i}-\bar{\x}_{t+1}\|_{1})&\le\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\E((\|\hat{\x}_{t+1,j}-\bar{\x}_{t+1}\|_{1})\\
				&=\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\E((\|\big((1-\gamma)\x_{t,i}+\frac{\gamma}{n}\one_{n}\big)-\bar{\x}_{t+1}\|_{1})\\
				&=(1-\gamma)\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\E((\|\x_{t,i}-\bar{\x}_{t+1}\|_{1})+\gamma\sum_{j\in\N_{i}\cup\{i\}}w_{ij}\E(\|\frac{1}{n}\one_{n}-\bar{\x}_{t+1}\|_{1})\\
				&\le\sum_{\tau=1}^{t}\sqrt{N}\beta^{t-\tau}(1-\gamma)^{t+1-\tau}\eta_{\tau}G+\gamma D.
			\end{aligned}
		\end{equation*}
	\end{proof}
	Like the Lemma~\ref{lemma:4}, we can present a similar lemma for Algorithm~\ref{alg:BDOEA},  that is,
		\begin{lemma}\label{lemma::4} 
			Consider our proposed Algorithm~\ref{alg:BDOEA}, if Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3},\ref{ass:4} hold and each set function $f_{t}$ is monotone submodular with curvature $c$ for any $t\in[T]$, then we can conclude that,
		\begin{equation*}
			\begin{aligned}
				&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*_{t}})-\sum_{t=1}^{T}\E\Big(F_{t}(\bar{\x}_{t})\Big)\le(2G+LD)\Big(\sum_{t=1}^{T}\sum_{i\in\N}\|\x_{t,i}-\bar{\x}_{t}\|_{1}\Big)\\&+G\Big(\sum_{t=1}^{T}\sum_{i\in\N}\|\y_{t,i}-\bar{\x}_{t}\|_{1}\Big)+\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(D_{KL}(\x^{*},\y_{t,i})-D_{KL}(\x^{*},\x_{t+1,i})\Big)+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t},
			\end{aligned}
		\end{equation*} where $\x_{t}^{*}$ is the optimal solution of Eq.\eqref{equ:continuous_max} and $D=\sup_{\x,\y\in\C}\|\x-\y\|$ where $\C=\{\x\in[0,1]^{n}: \sum_{a\in\V_{i}}x_{a}\le1,\forall i\in\N\}$.
	\end{lemma}
	Then, we derive an upper bound for  $\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{KL}(\x^{*}_{t},\y_{t,i})-\D_{KL}(\x^{*}_{t},\x_{t+1,i})\Big)$, i.e., 
		\begin{lemma}\label{lemma::5} 
				If Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3} and \ref{ass:4} hold, we have that
		\begin{equation*}
			\begin{aligned}
			&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{KL}(\x^{*}_{t},\y_{t,i})-\D_{KL}(\x^{*}_{t},\x_{t+1,i})\Big)\\&\le\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log\left(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}}\right)+\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)\Bigg)+\sum_{t=1}^{T}\frac{2N^{2}\gamma}{\eta_{t}}.
			\end{aligned}
		\end{equation*}
	\end{lemma}
	\begin{proof}
From \cite{nesterov2013introductory}, we know that, for any two vector $\x,\y\in\R^{n}_{+}$, 
\begin{equation*}
	\D_{KL}(\x,\y)=\sum_{j=1}^{n}[\x]_{j}\log\left(\frac{[\x]_{j}}{[\y]_{j}}\right)-\sum_{j=1}^{n}[\x]_{j}+\sum_{j=1}^{n}[\y]_{j}.
\end{equation*}
Thus, we can show that
		\begin{equation*}
			\begin{aligned}
				&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{KL}(\x^{*}_{t},\y_{t,i})-\D_{KL}(\x^{*}_{t},\x_{t+1,i})\Big)\\
				&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log\left(\frac{[\x_{t+1,i}]_{j}}{[\y_{t,i}]_{j}}\right)+\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)\Bigg)\\
				&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log\left(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}}\right)+\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log\left(\frac{[\x_{t+1,i}]_{j}}{[\hat{\x}_{t+1,i}]_{j}}\right)+\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)\Bigg).
			\end{aligned}
		\end{equation*}
	From Line 7 in Algorithm~\ref{alg:BDOEA}, we know that $[\hat{\x}_{t+1,i}]_{j}=(1-\gamma)[\x_{t+1,i}]_{j}+\frac{\gamma}{n}$. So if  $[\x_{t+1,i}]_{j}\le\frac{1}{n}$, $[\x_{t+1,i}]_{j}\le[\hat{\x}_{t+1,i}]_{j}$ or $\log(\frac{[\x_{t+1,i}]_{j}}{[\hat{\x}_{t+1,i}]_{j}})\le0$. As for $[\x_{t+1,i}]_{j}>\frac{1}{n}$ and $\gamma\le\frac{1}{2}$,
	\begin{equation*}
	\begin{aligned}
		\log(\frac{[\x_{t+1,i}]_{j}}{[\hat{\x}_{t+1,i}]_{j}})&=	\log(\frac{[\x_{t+1,i}]_{j}}{(1-\gamma)[\x_{t+1,i}]_{j}+\frac{\gamma}{n}})=\log\Big(1+\frac{\gamma([\x_{t+1,i}]_{j}-\frac{1}{n})}{(1-\gamma)[\x_{t+1,i}]_{j}+\frac{\gamma}{n}}\Big)\\&\le \frac{\gamma([\x_{t+1,i}]_{j}-\frac{1}{n})}{(1-\gamma)[\x_{t+1,i}]_{j}+\frac{\gamma}{n}}\le2\gamma, 
	\end{aligned}
	\end{equation*} where the final inequality follows from 
		$[\x_{t+1,i}]_{j}-\frac{1}{n}\le[\x_{t+1,i}]_{j}\le2(1-\gamma)[\x_{t+1,i}]_{j}\le2\Big((1-\gamma)[\x_{t+1,i}]_{j}+\frac{\gamma}{n}\Big)$.
		
		Then, we have 
		\begin{equation*}
			\begin{aligned}
				&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Big(\D_{KL}(\x^{*}_{t},\y_{t,i})-\D_{KL}(\x^{*}_{t},\x_{t+1,i})\Big)\\
				&\le\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}})+\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)\Bigg)+\sum_{t=1}^{T}\frac{2N^{2}\gamma}{\eta_{t}}.
			\end{aligned}
		\end{equation*}
	\end{proof}
	\begin{lemma}\label{lemma::11} If Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3} and \ref{ass:4} hold, we have that
	\begin{equation*}
		\begin{aligned}
		\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}})\Bigg)\le\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}]\|_{1}.
		\end{aligned}
	\end{equation*}
\end{lemma}
\begin{proof}
	\begin{equation*}
		\begin{aligned}
			&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}})\Bigg)\\&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)\\
			&=\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Bigg(\frac{1}{\eta_{t}}\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t,i}]_{j}})\Big)-\frac{1}{\eta_{t+1}}\E\Big(\sum_{j=1}^{n}[\x_{t+1}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)\Bigg)}_{\text{\textcircled{1}}}\\
			&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t+1}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)\Bigg)}_{\text{\textcircled{2}}}\\
			&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)}_{\text{\textcircled{3}}}\\
			&+\underbrace{\sum_{t=1}^{T}\sum_{i\in\N}\Big(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\Big)\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)}_{\text{\textcircled{4}}}.
		\end{aligned}
	\end{equation*}
	Firstly, from the  convexity of function $\log(\frac{1}{x})$, we have 
	\begin{equation*}
		\begin{aligned}
			&\text{\textcircled{3}}=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)\\
			&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{\sum_{k\in\N_{i}\cup\{i\}}w_{ik}[\hat{\x}_{t+1,k}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)\\
			&\le\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\sum_{k\in\N_{i}\cup\{i\}}w_{ik}\log(\frac{1}{[\hat{\x}_{t+1,k}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)\\
			&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\Big(\sum_{k\in\N_{i}\cup\{i\}}w_{ki}\Big)\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\hat{\x}_{t+1,i}]_{j}})\Big)\Bigg)\\
			&=0.
		\end{aligned}
	\end{equation*}
	Then, for \textcircled{1}, we can show that
	\begin{equation*}
\text{\textcircled{1}}\le\sum_{i\in\N}\frac{1}{\eta_{1}}\E\Big(\sum_{j=1}^{n}[\x_{1}^{*}]_{j}\log(\frac{1}{[\y_{1,i}]_{j}})\Big)\le\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{1}},
	\end{equation*} where the final inequality follows from $[\y_{1,i}]_{j}\ge\frac{\gamma}{n}$ such that $\log(\frac{1}{[\y_{1,i}]_{j}})\le\log(\frac{n}{\gamma})$ and $\sum_{j=1}^{n}[\x_{1}^{*}]_{j}\le N$.
	
	As for \textcircled{2}, we can have that
	\begin{equation*}
		\begin{aligned}
			&\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}[\x_{t+1}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)-\E\Big(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)\Bigg)\\
			&=\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t+1}}\Bigg(\E\Big(\sum_{j=1}^{n}\Big([\x_{t+1}^{*}]_{j}-[\x_{t}^{*}]_{j}\Big)\log(\frac{1}{[\y_{t+1,i}]_{j}})\Big)\Bigg)\\
			&\le\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}]\|_{1}, 
		\end{aligned}
	\end{equation*} where the final inequality follows from  $\log(\frac{1}{[\y_{t+1,i}]_{j}})\le\log(\frac{n}{\gamma})$.
	Moreover, we have \textcircled{4}$\le N^{2}\log(\frac{n}{\gamma})\Big(\frac{1}{\eta_{T+1}}-\frac{1}{\eta_{1}}\Big)$. We finally get 
	\begin{equation*}
		\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}[\x_{t}^{*}]_{j}\log(\frac{[\hat{\x}_{t+1,i}]_{j}}{[\y_{t,i}]_{j}})\Bigg)\le\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}]\|_{1}.
	\end{equation*}
\end{proof}
Next, from Line 13-18 in Algorithm~\ref{alg:BDOEA}, we know that, for any $i\in\N$, when $\sum_{a\in\V_{i}}\Big(	[\y_{t,i}]_{a}\exp(\eta_{t}[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a})\Big)\le1$, we can have $[\x_{t+1,i}]_{a}\ge[\y_{t,i}]_{a}$. As for $\sum_{a\in\V_{i}}\Big(	[\y_{t,i}]_{a}\exp(\eta_{t}[\tilde{\nabla} F_{t}^{s}(\x_{t,i})]_{a})\Big)>1$, we have $\sum_{a\in\V_{i}}[\x_{t+1,i}]_{a}=1$ such that $\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)=\sum_{a\in\V_{i}}[\y_{t+1,i}]_{a}-1\le0$. As a result,  we can conclude that
 \begin{equation*}
\sum_{t=1}^{T}\sum_{i\in\N}\frac{1}{\eta_{t}}\E\Bigg(\sum_{j=1}^{n}\Big([\y_{t,i}]_{j}-[\x_{t+1,i}]_{j}\Big)\Bigg)\le0.
 \end{equation*}

	
Finally, we get these result 
\begin{equation*}
	\begin{aligned}
		&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}(\bar{\x}_{t})\Big)\le(3G^{2}+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}(1-\gamma)^{t-\tau}\eta_{\tau}\Big)\\&+G\gamma D+\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}\|_{1}+\sum_{t=1}^{T}\frac{2N^{2}\gamma}{\eta_{t}}.+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.
			\end{aligned}
\end{equation*}
Like Lemma~\ref{thm:3}, we also can verify that 
	\begin{lemma}\label{thm:31}
	In Algorithm~\ref{alg:BDOEA}, if Assumption \ref{ass:2},\ref{ass:1},\ref{ass:3},\ref{ass:4} hold and each set function $f_{t}$ is monotone submodular with curvature $c$ for any $t\in[T]$, then
	\begin{equation*}
		\begin{aligned}
			&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}F_{t}(\x^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\Big)\le(4G^{2}+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}(1-\gamma)^{t-\tau}\eta_{\tau}\Big)+G\gamma D\\&+\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}\|_{1}+\sum_{t=1}^{T}\frac{2N^{2}\gamma}{\eta_{t}}+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.
		\end{aligned}
	\end{equation*}
\end{lemma}


From \citet{calinescu2011maximizing,chekuri2014submodular}, we know that the optimal value of continuous problem Eq.\eqref{equ:continuous_max} is equal to the optimal value of the corresponding discrete submodular maximization Eq.\eqref{equ:problem_t}, so we can set $\x^{*}_{t}:=\one_{\mathcal{A}_{t}^{*}}$ where $\mathcal{A}_{t}^{*}$ is the maximizer of Eq.\eqref{equ:problem_t}.


Next, like Lemma~\ref{lemma:rounding1}, we also can show a relationship between $\E(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big))$ and $\E\big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)$.
\begin{lemma}
	If the function $f_{t}$ is monotone submodular and $a_{t,i}$ is the action taken via the agent $i\in\N$ at time $t$, then we have 
	\begin{equation*}
		\E\Big(f_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big)\ge\E\Big(\big(F_{t}\big(\sum_{i\in\N}\x_{t,i}\odot\one_{\V_{i}}\big)\Big).  
	\end{equation*}
\end{lemma}
Finally,  we get
\begin{equation*}
	\begin{aligned}
		&\Big(\frac{1-e^{-c}}{c}\Big)\sum_{t=1}^{T}f_{t}(\mathcal{A}^{*}_{t})-\sum_{t=1}^{T}\E\Big(F_{t}\big(\cup_{i\in\N}\{a_{t,i}\}\big)\Big)\le(4G^{2}+LDG)\Big(\sum_{t=1}^{T}\sum_{\tau=1}^{t}N^{\frac{3}{2}}\beta^{t-\tau}(1-\gamma)^{t-\tau}\eta_{\tau}\Big)+G\gamma D\\&+\frac{N^{2}\log(\frac{n}{\gamma})}{\eta_{T+1}}+\sum_{t=1}^{T}\frac{N\log(\frac{n}{\gamma})}{\eta_{t+1}}\|\x_{t+1}^{*}-\x_{t}^{*}\|_{1}+\sum_{t=1}^{T}\frac{2N^{2}\gamma}{\eta_{t}}+\frac{NG}{2}\sum_{t=1}^{T}\eta_{t}.
	\end{aligned}
\end{equation*}
