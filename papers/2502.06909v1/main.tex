\documentclass[journal,comsoc]{IEEEtran}
\usepackage{stfloats}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem} 
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{corollary}[theorem]{Corollary} 
\newtheorem{definition}{Definition}
\usepackage[cmintegrals]{newtxmath}
\interdisplaylinepenalty=2500
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{hyperref}
\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=black,
	pdfstartview=Fit,
	breaklinks=true}

\usepackage{tikz,xcolor,hyperref}
\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
		\draw[lime, fill=lime] (0,0) 
		circle [radius=0.16] 
		node[white] {{\fontfamily{qag}\selectfont \tiny ID}};    \draw[white, fill=white] (-0.0625,0.095) 
		circle [radius=0.007];    \end{tikzpicture}
	\hspace{-2mm}}
\foreach \x in {A, ..., Z}{
	\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}


\begin{document}

\title{Satisfaction-Aware Incentive Scheme for Federated Learning in Industrial Metaverse: DRL-Based Stackbelberg Game Approach}

\author{Xiaohuan Li, 
        Shaowen Qin, 
        Xin Tang,
        Jiawen Kang,
        Jin Ye, 
        Zhonghua Zhao,
        and Dusit Niyato, \textit{Fellow, IEEE}
        \thanks{This work was supported in part by the National Natural Science Foundation of China under Grant U22A2054, U23A20313, in part by the Guangxi Natural Science Foundation of China under Grant 2024JJA170165, and in part by the Graduate Study Abroad Program of Guilin University of Electronic Technology under Grant GDYX2024001. ({\itshape Corresponding author: Jin Ye}.)}
        \thanks{Xiaohuan Li is with the Guangxi University Key Laboratory of Intelligent Networking and Scenario System (School of Information and Communication, Guilin University of Electronic Technology), Guilin 541004, China, and also with National Engineering Laboratory for Comprehensive Transportation Big Data Application Technology (Guangxi), Nanning 530001, China (e-mails: lxhguet@guet.edu.cn).}
        \thanks{Shaowen Qin, Xin Tang and Zhonghua Zhao are with the Guangxi University Key Laboratory of Intelligent Networking and Scenario System (School of Information and Communication, Guilin University of Electronic Technology), Guilin 541004, China (e-mails: shaowen211@gmail.com; tangx@mails.guet.edu.cn; gietzzh@guet.edu.cn).}
        \thanks{Jiawen Kang is with the School of Automation, Guangdong University of Technology, Guangzhou 510006, China (e-mail: kavinkang@gdut.edu.cn).}
        \thanks{Jin Ye is with Guangxi Key Laboratory of Multimedia Communications and Network Technology, Nanning 530000, China, and also with School of Computer and Electronic Information, Guangxi University, Nanning 530000, China (e-mail: yejin@gxu.edu.cn). }
        \thanks{Dusit Niyato is with the College of Computing and Data Science, Nanyang Technological University, Singapore (e-mail: dniyato@ntu.edu.sg).}
        }

\maketitle

\begin{abstract}
Industrial Metaverse leverages the Industrial Internet of Things (IIoT) to integrate data from diverse devices, employing federated learning and meta-computing to train models in a distributed manner while ensuring data privacy. Achieving an immersive experience for industrial Metaverse necessitates maintaining a balance between model quality and training latency. Consequently, a primary challenge in federated learning tasks is optimizing overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency. Additionally, the satisfaction function is incorporated into the utility functions to incentivize node participation in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for industrial Metaverse. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves at least 23.7\% utility compared to existing schemes without compromising model accuracy. 

\end{abstract}
	
\begin{IEEEkeywords}
Metaverse, Federated Learning, Age of Information, Incentive Scheme, Stackelberg Game.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
Metaverse is undoubtedly one of the most popular and promising intelligent applications \cite{b1,b2,b3}, creating a collective virtual shared space that merges digital and physical realities, enabling users to interact with each other and digital environments in immersive, and real-time settings. Metaverse can also be combined with the Industrial Internet of Things (IIoT), giving rise to industrial Metaverse. For example, NVIDIA \cite{b6} developed an open platform called Omniverse, which supports multi-user real-time 3D simulation and visualization of physical properties in a shared virtual space for industrial applications such as automotive design. The meta-computing plays a crucial role in integrating distributed computing resources within the IIoT to build industrial Metaverse. It enables distributed data processing, storage, and computation across heterogeneous systems, bridging the gap between virtual and physical spaces \cite{b47}. This provides a seamless platform for industrial Metaverse, delivering a realistic, persistent, and smooth interaction experience for users.

However, traditional distributed learning approaches may face certain limitations due to the large number of devices and data in industrial Metaverse enabled by meta-computing. First, IIoT nodes need to share data for model training, which increases the risk of privacy leakage. Second, the large volume of data transmission may result in latency issues. Fortunately, Federated Learning (FL) \cite{b8,b9} can effectively address these problems. FL is a distributed learning scheme proposed by Google to optimize global machine learning models without moving data out of local devices. In the industrial Metaverse, which uses meta-computing and incorporates FL, each IIoT node participates in training a shared AI model using its own dataset, and then uploads its local model to the server to build a new global model \cite{b10,b50}, thus achieving the goal of building a high-quality Metaverse while ensuring privacy.

To ensure an immersive experience in industrial Metaverse, high-quality global models must be developed. This requires vast amounts of real-time sensing data and significant computational resources from nodes \cite{b11}. Furthermore, achieving an immersive experience in the Metaverse demands not only high-quality models but also low-latency interactions. Balancing the reduction of latency with the enhancement of quality is crucial to maintaining the overall performance of Metaverse applications. However, due to the large number of nodes and the complex structure of the industrial Metaverse, designing a methodology that effectively balances quality and latency presents a significant challenge.

To address these challenges, we define a satisfaction function based on data size, Age of Information (AoI) \cite{b40}, and latency, aiming to balance training latency and model quality. This satisfaction function is then incorporated into the utility function, transforming the utility optimization problem into a Stackelberg game. However, traditional methods such as backward induction, convex optimization, and nonlinear programming require lots of participant information to solve for game equilibrium, which is not practical for industrial Metaverse. The nodes in the industrial Metaverse are not only large-scale but also operate independently. To preserve privacy, these participants are often unwilling to share their private information, and thus lack the a priori information required by traditional methods. Fortunately, Deep Reinforcement Learning (DRL) can learn optimal strategies based on experience alone, eliminating the need for prior information. We propose a DRL-based method to obtain game equilibrium effectively, ensuring participant privacy. The main contributions of this paper are as follows:

% Contributions
\begin{itemize}

\item We design a new meta-computing framework based on FL to provide dynamic resource scheduling and optimization solutions for the industrial Metaverse. The framework incorporates an FL incentive scheme designed for the industrial Metaverse, enabling resource allocation and optimization while ensuring the requirements of the demand side, thereby improving learning efficiency.

\item We introduce a new metric named “Satisfaction” that balances training latency and model quality in the industrial Metaverse. Focusing on the real-time relevance and quality of data, we use factors such as data size, AoI, and service latency as the key factors to measure the contribution of nodes in the industrial Metaverse. We design a satisfaction-aware incentive scheme that constructs utility functions for nodes and servers to incentivize resource sharing among nodes.

\item We transform the optimization problem involving two utility functions into a Stackelberg game model and prove the uniqueness of the Stackelberg Equilibrium (SE). We then use the DRL algorithm to learn the SE without requiring private information from other agents, relying solely on past experiences.

\end{itemize}

The rest of the paper is organized as follows. Section II reviews some important literature related to this paper. Section III gives the system model of the paper. Section IV details the satisfaction design. Section V introduces the incentive scheme based on the satisfaction. Section VI introduces the Stackelberg game based on DRL. Section VII presents the experimental parameters and related results. The last section concludes the paper.

\section{Related Works}
\subsection{Quality of Metaverse Applications}
The definition of quality in the Metaverse varies across different services. The authors in \cite{b16} defined the quality of perception in VR as a measure of users' subjective feelings about their immersion in the virtual world and designed a double Dutch auction mechanism to determine optimal pricing and allocation rules, facilitating fast trades of VR services between users and providers. The authors in \cite{b17} proposed a novel metric called Meta-Immersion, which combines objective KPIs with subjective perceptions of Metaverse users, allowing for distinct expressions in different virtual scenarios. They also developed an attention-aware rendering capacity allocation scheme to enhance QoE. The authors in \cite{b18} used collected synchronization data and the value decay rate of the digital twin to determine synchronization strength, maximizing its payoff, and employed a dynamic Stackelberg game to obtain optimal control. The authors in \cite{b19} proposed a novel metric called Age of Migration Task (AoMT) to quantify the task freshness of Vehicle Twins' migration, aiming to ensure the quality of migration within the Metaverse, and designed an AoMT-based contractual model to effectively incentivize Vehicle Twins' migration. Additionally, most existing Metaverse QoE evaluation studies primarily focus on subjective aspects such as video \cite{b12}, audio \cite{b13}, 3D models, and AR/VR QoE \cite{b14}.

However, Metaverse immersion is primarily influenced by image quality and response speed \cite{b41,b42}, while model quality and latency in FL directly affect virtual image quality and response speed in the Metaverse \cite{b43}, which leads to reduced immersion. Therefore, the quality metrics defined in the above studies are insufficient for scenarios requiring high immersion, particularly those involving the integration of the industrial Metaverse with FL. Immersive experiences in the industrial Metaverse require a high degree of real-time interaction, which exacerbates the efficiency issues in FL. Thus, identifying quality evaluation methods applicable to the integration of the industrial Metaverse and FL is the focus of this paper.

\subsection{Incentive Schemes for FL}
\textbf{Node contribution evaluation:} Node contribution evaluation in FL focuses on the extent to which each participating node contributes to the overall learning process and outcome, enabling FL to achieve higher performance with minimal rewards \cite{b44}. The authors in \cite{b24} used historical learning records to estimate nodes' learning quality and an exponential forgetting function to assign weights, designed quality-aware incentives and model aggregation methods to improve learning effectiveness. The authors in \cite{b25} designed the quality-aware node selection framework AUCTION, which included factors such as data size, data quality, and learning budget within nodes that influence learning quality. These factors were encoded into an attention-based neural network to enhance strategy performance. The authors in \cite{b26} introduced reputation as a node evaluation metric to improve the reliability of FL tasks in mobile networks, resulting in a reliable worker selection scheme. Shapley values are also used in FL to measure how much a participant contributes to the overall model training quality. The authors in \cite{b27} leveraged Shapley value calculation to build FedCoin, a blockchain-based peer-to-peer payment system that ensures realistic and fair profit distribution.

\textbf{Incentive Schemes:} Auction-based schemes are commonly applied in FL due to their simplicity of construction. The authors in \cite{b30} proposed an incentive mechanism named FMore, designed for multidimensional procurement auctions, to encourage more low-cost, high-quality edge nodes to participate in learning. The authors in \cite{b24} proposed an FL system called FAIR, where reverse auctions were modeled to encourage quality users to participate in FL. The Stackelberg game, a sequential model in game theory, is suitable for incentive design in FL. The authors in \cite{b33} proposed an FL-based crowdsourcing framework using incentive-driven interactions and analyzed it with a two-stage Stackelberg game. The authors in \cite{b34} designed a dynamic incentive scheme based on the Stackelberg game to adaptively adjust node selection. Reinforcement Learning (RL) adapts to the environment and optimizes the strategy through continuous iteration in a dynamic setting \cite{b31}. The authors in \cite{b32} proposed a DRL-based mechanism to optimize pricing for servers and training policies for edge nodes, incentivizing participation. Additionally, several studies have integrated game theory with DRL. The authors in \cite{b46} formulated the interaction between edge servers and data centers as a multi-leader multi-follower Stackelberg game, employing the MADDPG \cite{b55} algorithm to achieve optimal strategies. The authors in \cite{b45} modeled providers in the Metaverse as a Stackelberg game and used MALPPO to optimize Vehicular Twin migration. Several papers also proposed incentives for FL in the Metaverse. The authors in \cite{b35} proposed a decentralized FL framework with privacy preservation and an age-based model to incentivize IIoT Metaverse data sensing. The authors in \cite{b36} proposed a Metaverse optimization framework minimizing energy, time and accuracy trade-offs, with a resource allocation algorithm for device contributions.

 \begin{figure*}[!t]
	\centerline{\includegraphics[width=0.95\textwidth]{fig1.jpg}}
	\caption{A meta-computing framework based on FL for industrial Metaverse.The framework enables resource scheduling and task management for IIoT devices, where the server employs a Satisfaction-aware incentive mechanism to coordinate nodes for efficient task execution.}
	\label{fig1}
\end{figure*}

However, none of these works identify the potential factors affecting FL quality related to the freshness of information, nor do they consider the overall budgetary investment. Therefore, designing an FL incentive mechanism based on model quality that meets the needs of the industrial Metaverse is another focus of this paper.

\section{System Model}
In order to promote efficient task processing and decision making in the industrial Metaverse, we design a meta-computing framework based on FL in Fig. \ref{fig1}. The device management module contains multiple edge nodes. Its primary purpose is to collect data from production devices, integrate the computational, storage, and communication resources of the edge nodes, map these resources to the server, convert them into objects that can be easily accessed by the resource scheduler, and then train FL models locally based on the incentive scheme developed by the task manager. The resource scheduler module contains several virtual edge nodes that constantly monitor changes in the configuration details of physical nodes, simulate possible states of the nodes, and dynamically perform resource optimization. The task management module, located on the server, accepts requests from users, decomposes tasks, and designs incentive schemes based on their conditional constraints. The zero-trust computing management module performs global aggregation for FL via the blockchain, and the identity and access management module ensures users have the appropriate permissions to access the data. Since the zero-trust computing management and identity and access management modules are not the focus of this paper, detailed discussions are omitted. Comprehensive analyses of these modules are available in \cite{b47}. The main notations used in this paper are shown in Table \ref{tab1}.

\begin{table}[t]
\caption{Summary of Main Notations}
\begin{center}
	\begin{tabular}{|p{1.2cm}|p{5.5cm}|}
		\hline
        \textbf{Notation}     &\textbf{Description}  \\ \hline
        $a_i$                 & The duration from the end of data collection to the beginning of the next period of data collection for node $i$         \\ \hline
		$c_i$                      &  The time spent by the node $i$ to collect and process the model training data         \\ \hline
		$d_i$               & The size of data collected per unit time period for node $i$        \\ \hline
        $r_i$                      & The satisfaction unit reward provided by the server for node $i$          \\ \hline
		$\sigma_i$                    & The unit cost required to maintain the update cycle for node $i$          \\ \hline
		$\theta _i$                    & The period of update buffer data for node $i$  \\ \hline
        $D_{i}$                      & The data size collected by node $i$          \\ \hline
        $E_i$                 &The service latency for node $i$            \\ \hline
		$G_i$                        & Server satisfaction with node $i$        \\ \hline
        $I_{all}$                      & The set of IIoT nodes involved          \\ \hline
        $Q_i$                &The quality of the model provided by node $i$       \\ \hline
        $R_i$                 &The reward given to node $i$ by the server           \\ \hline
        $T$                 &The task duration            \\ \hline
        $U_i$                 &The utility of node $i$            \\ \hline
        $V_i$                 &The utility obtained by the server from node $i$           \\ \hline
	\end{tabular}
\label{tab1}
\end{center}
\end{table}

We represent the set of IIoT nodes involved as ${I_{all}} = \{1,...,i,...,I\}$ and the task duration as $T$. Upon task arrival, each model training is iterated $K$ times to minimize global losses, where $K$ is specified by the server. Assume that there are $I$ nodes with local data sets $\{D'_1,D'_2,...,D'_I\}$. We define $D_i\triangleq|D'_i|$, where $|\cdot|$ denotes the size of the data set with meta-computing, each node downloads a shared global model $\omega$ from the server and trains the model using its local data. The node then uploads the new local model update to the server. Therefore, the total size of data samples from $I$ nodes is $\sum\nolimits_{i=1}^{I}{{D}_{i}={D}_{all}}$. The loss function of the node $i$ with the data set $D'_i$ is
\begin{equation}
\begin{split}
{{F}_{i}}(\omega )\triangleq \frac{1}{{{D}_{i}}}\sum\limits_{s\in {{D}_{i}}'}{{{f}_{i}}(\omega )},\label{eq30} 
\end{split}
\end{equation}
where $f_i(\omega)$ is the loss function on the data sample $s$. The goal is to optimize the global loss function $F(\omega)$ by minimizing the weighted average of every node $i$’s local loss function $F_i(\omega)$ on its local training samples
\begin{equation}
\begin{split}
F(\omega )\triangleq \frac{\sum\nolimits_{i=1}^{I}{{{D}_{i}}{{F}_{i}}(\omega )}}{D_{all}},\label{eq31} 
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\omega *=\arg \min F(\omega ).\label{eq32} 
\end{split}
\end{equation}

Due to the inherent complexity of many machine learning models, it is hard to find a closed-form solution. Therefore, it is often solved using gradient descent techniques \cite{b37}. Given that IIoT nodes may be reluctant to provide new sensing data for time-sensitive FL tasks in the industrial Metaverse, reliable incentives are greatly needed to encourage users to share new sensing data, which is discussed in the next section.

\section{Satisfaction: Quality Control for Industrial Metaverse}
The structure of the industrial Metaverse is complex, containing multiple types of industrial devices and involving a large number of industrial nodes, where the trade-off between low latency and high model quality must be carefully considered. Among them, latency directly affects screen lag and response time in virtual environments. High latency causes the screen to fail to update in time, resulting in users experiencing lags or delays when interacting with the environment \cite{b41,b42}. In other words, user interaction in the Metaverse is highly dependent on low latency, and any delay will significantly reduce user satisfaction. On the other hand, degraded model quality means that the information or imagery received by users is not realistic enough, which may lead to a lack of synchronization between the virtual environment and the user's actual behavior. Furthermore, model quality will greatly disturb users' decisions. With information from a declining quality model, users may make inappropriate responses. Traditional quality metrics in FL do not adequately capture the requirements of the industrial Metaverse. Therefore, we propose a satisfaction metric $G_i$, to balance quality and latency. The satisfaction metric $G_i$ of the task $i$ is denoted as
\begin{equation}
\begin{split}
{G_{i}} = \tau {Q_{i}} - \lambda{E_{i}},\label{eq10} 
\end{split}
\end{equation}
where $\tau$ and $\lambda$ are the conversion parameters for quality and latency, respectively.

Since the timeout of gradient updates can negatively impact the learning results, we use AoI to measure the freshness of the model in order to ensure its quality. AoI, as a valid measure of information freshness, denotes the latency of the information from its generation to the completion of the model training after it has been uploaded to the server \cite{b35}, and it can enhance the performance of time-critical applications and services. In FL, we assume that requests arrive at the beginning of each cycle. We focus on FL with a data cache buffer on the node and AoI \cite{b38}. The node $i$ periodically updates its cached data with an update period $\theta _i$ independently. It is denoted as
\begin{equation}
	{\theta _i} = {c_i}t + {a_i}t, \label{eq1}
\end{equation}
where ${c_i}t  ({c_i} \in N)$ is the time spent by the node $i$ to collect and process the model training data; ${a_i}t ({a_i} \in N)$ is the duration from the end of data collection to the beginning of the next phase of data collection. It may contains  service time period and idle time period.

When the request arrives during the data collection phase or at the beginning of phase $(c_i + 1)t$, the AoI is $t$. This is the minimum  AoI value. For requests arriving in phase $l \times t$, where $l \ge (c_i + 2)$, the AoI will be $[l - (c_i + 1) + 1]t$. We suppose that $t$ is fixed and that the update cycle ${\theta _i}$ is affected by ${c _i}$ and ${a _i}$. Let us consider a case with an adjustable update phase, i.e., when ${a_i} = a$ is fixed, ${c_i} = \frac{{{\theta _i}}}{t} - a$, we use ${\theta _i}$ instead of ${c _i}$. Therefore, the average AoI of the node $i$ is
\begin{equation}
\begin{split}
{{\bar A}_i} &= \frac{t}{{{c_i} + {a_i}}}[{c_i} + 1 + \frac{{({a_i} - 1)({a_i} + 2)}}{2}]\\&
= \frac{{t{\theta _i}}}{{{\theta _i} - at}} + \frac{{{t^2}}}{{{\theta _i} - at}}(\frac{{{a^2} - a}}{2}),\label{eq3} 
\end{split}
\end{equation}
when ${\theta _i} > at$ , ${\bar A_i}({\theta _i})$ is a concave function about ${\theta _i}$.

In addition to AoI, traditional FL experiences service latency \cite{b39}, defined in this paper as ${E_{i}}$. Unlike AoI, service latency refers to the duration from when the node receives a request until it uploads a local model. It includes both the data collection period and the model training period.
The probability of a request arriving is uniformly distributed across periods, i.e., $\frac{1}{T}$. If the request arrives in the $n$th period of the data collection cycle, the service delay is ${c_i}t + t - (n - 1)t$. If the request arrives in any of the remaining periods, the service latency is $t$. Therefore, the average service latency is
\begin{equation}
\begin{split}
{{\bar E}_i} &= \frac{{{c_i}}}{{{c_i} + {a_i}}}[({c_i}t + t) + \cdots + ({c_i}t + t - ({c_i} - 1)t)] + \frac{{{a_i}}}{{{c_i} + {a_i}}}t\\&
= \frac{{{c_i}}}{{{c_i} + {a_i}}}[\frac{{{c_i}}}{2}({c_i} + 3)] + \frac{{{a_i}}}{{{c_i} + {a_i}}}t\\&
= \frac{{{{({\theta _i} - at)}^3}}}{{2t{\theta _i}}} + \frac{{3{{({\theta _i} - at)}^2}}}{{2{\theta _i}}} + \frac{{a{t^2}}}{\theta }.
\label{eq4} 
\end{split}
\end{equation}

From Eq. \eqref{eq3} and \eqref{eq4}, we observe a trade-off between service delay and AoI when selecting the cycle length ${\theta _i}$. Intuitively, a lower ${\theta _i}$, which means that a shorter cycle length and more frequent data updates, results in a lower average AoI. However, service latency also increases as updates take time.

To ensure user satisfaction, we must maintain the freshness of the model while guaranteeing its quality. This requires that we cannot rely solely on raw data to evaluate node contributions. Therefore, we define the model quality contributed by each node as the ratio of data size to AoI, which is denoted as
\begin{equation}
\begin{split}
{Q_i} = \rho \frac{{{D_i}}}{{{A_i}}} = \rho \frac{{Td({\theta _i} - at)}}{{{\theta _i}(t{\theta _i} + {t^2}(\frac{{{a^2} - a}}{2}))}} ,\label{eq9} 
\end{split}
\end{equation}
where $\rho $ is the parameter used to adjust the quality.

In the context of combining the industrial Metaverse with FL, the final aggregated global model depends on the local models contributed by individual nodes, while the size of the raw data ${D_i}$ reflects the node's contribution to the overall FL training process. ${D_i}$ is denoted as ${D_i} = \frac{T}{{{\theta _i}}}d$, where $d$ is the amount of data collected per unit time period.

\section{Satisfaction-aware incentives scheme}
\subsection{Utility Model and Problem Formulation}
To meet the quality and latency requirements of industrial Metaverse tasks, we formulate the corresponding utility functions and optimization objectives.
Node $i$ is incentivized to handle incoming task requests, and each participating node receives a monetary reward ${R_i}$ from the server. Therefore, the utility of node $i$ is the difference between the reward ${R_i}$ and the cost ${C_i}$ of participating in the FL task. The utility $U_i$ can be expressed as
\begin{equation}
\begin{split}
{U_i} = {R_i} - {C_i},\label{eq5} 
\end{split}
\end{equation}
where the cost of the FL training task is defined as
${C_i} = \frac{{{\sigma _i}}}{{{\theta _i}}}$, where ${\sigma _i}$ is the unit cost required to maintain the update cycle, ${\theta _i}$ with respect to data collection, computation and transmission. 

Additionally, to incentivize nodes to participate in FL and provide higher-quality local models, the server will provide corresponding rewards ${R_i}$, which is denoted as ${R_i} = {r_i}\ln (\frac{1}{{{\theta _i}}})$, where ${r_i}$ is a unit award. The utility that the server receives from node $i$ is defined as the difference between the satisfaction gain $\beta {G_i}$ obtained by the server and the payoff ${R_i}$ given to the node, expressed as
\begin{equation}
\begin{split}
{V} =\sum\limits_{i\in {{I}_{all}}} (\beta {G_i} - {R_i}),\label{eq12} 
\end{split}
\end{equation}
where $\beta $ is the profit per unit of satisfaction.

In order to meet the quality and latency requirements of industrial Metaverse tasks, we define the goal of the incentive mechanism as maximizing server utility function $V$ and node utility function $U_i$.
The optimization problem for the node utility  is formulated as follows:
\begin{equation}
\begin{split}
  & \text{P1: }\underset{{{\theta }_{i}}}{\mathop{\max }}\, {{U}_{i}} \\ 
 & s.t. {{\theta }_{i}}\ge \theta _{i}^{\min },
\label{eq41} 
\end{split}
\end{equation}
which is subject to a minimum update cycle $\theta_i^{\max}$.
The optimization problem for the server utility is formulated as follows:
\begin{equation}
\begin{split}
  & \text{P2: }\underset{{{r}_{i}}}{\mathop{\max }}\,{{V}_{}} \\ 
 & s.t.{{A}_{i}}\le A_{i}^{\max }, \\ 
 & {{D}_{i}}\le D_{i}^{\max }, \\ 
 & \sum\limits_{i\in I}{{{r}_{i}}}\le {{R}^{\max }},  
\label{eq42} 
\end{split}
\end{equation}
which is subject to a maximum tolerable AoI constraint ${A_i}^{\max}$ , the maximum tolerable service delay constraint ${D_i}^{\max}$ , and a budget constraint ${R^{\max}}$.

\subsection{Stackelberg Game Analysis}
Since the goal of the server and the node is to maximize their respective reward functions Eq. \eqref{eq41} and \eqref{eq42}, We model the interaction between the server and the node as a two-part Stackelberg game, where the server, as a leader, determines the reward strategy ${r_i}$ and the node, as a follower, responds with ${\theta _i}$. The Stackelberg game can be defined in terms of strategies as
\begin{equation}
\begin{split}
\Omega  = \{ (SP \cup {\{ i\} _{i \in I}}),(r_i,\theta _i),(V_i,U_i)\} .\label{eq14} 
\end{split}
\end{equation}
The policy consists of three parts, $(SP \cup {\{ i\} _{i \in I}})$ denotes the set of servers and corresponding nodes, $({r _i},{\theta _i})$ denotes the set of policies, and $({V_i},{U _i})$ denotes the set of utilities.
We denote ${\theta ^*}$ as the optimal update period provided by the node, i.e., ${\theta ^*} = [\theta _1^*,...,\theta _i^*,...,\theta _I^*] $, and ${r_i^*}$ as the optimal reward decision of the server.
\begin{definition}
(Stackelberg Equilibrium): There exists an optimal update cycle $\theta _i^*$ and an optimal reward $r_i^*$. A policy $(\theta _i^*, r_i^*)$ is considered a Stackelberg equilibrium if and only if it satisfies the following 
\begin{equation}
\begin{split}
\forall \theta _i^{},U_i^{}(\theta _i^*,r_{i}^*) \ge U_i^{}(\theta _i^{},r_{i}^*)\\
\forall r_i^{},V(\theta _i^*,r_{i}^*) \ge U_i(\theta _i^*,r_i).\label{eq15}
\end{split}
\end{equation}
\end{definition}

We analyze the node's optimal decision using a typical inverse induction method. Subsequently, we compute the first-order and second-order derivatives of ${U _i}$ with respect to ${\theta _i}$ as follows:
\begin{equation}
\begin{split}
  & \frac{\partial U_{i}^{{}}}{\partial \theta _{i}^{{}}}=\frac{{{\sigma }_{i}}}{\theta _{i}^{2}}-\frac{{{r}_{i}}}{{{\theta }_{i}}} \\ 
 & \frac{{{\partial }^{2}}U_{i}^{{}}}{\partial \theta _{i}^{2}}=\frac{{{r}_{i}}{{\theta }_{i}}-2{{\sigma }_{i}}}{\theta _{i}^{3}},  \label{eq16}
\end{split}
\end{equation}
where ${{\theta }_{i}}<\frac{2{{\sigma }_{i}}}{{{r}_{i}}}$, $\frac{{{\partial }^{2}}U_{i}^{{}}}{\partial \theta _{i}^{2}}<0$, and $U_{i}^{j}$ are convex functions. Solving the first-order derivative optimality condition$\frac{\partial U_{i}^{{}}}{\partial \theta _{i}^{{}}}=0$ is obtained as $\theta _{i}^{*}=\frac{{{\sigma }_{i}}}{{{r}_{i}}}$. By setting ${{\theta }_{i}}={{\theta }_{i,\max }}$ and ${{\theta }_{i}}={{\theta }_{i,\min }}$, we can get the upper and lower bounds of $\theta$ with respect to each node $i$. Based on this, the optimal update response is obtained as
\begin{equation}
\theta _i^* = \left\{ \begin{array}{l}
{\theta _{i,\max }},{\rm{                        }}{\theta _i} > {\theta _{i,\max }},\\
\frac{{{\sigma _i}}}{{{r_i}}},{\rm{                          }}{\theta _{i,\min }} < {\theta _i} < {\theta _{i,\max }},\\
{\theta _{i,\min }},{\rm{                        }}{\theta _i} < {\theta _{i,\min }}.
\end{array} \right.\label{eq18}
\end{equation}
Bringing $\theta _{i}^{*}$ into Eq. (\ref{eq12}), $V$ re-expresses as
\begin{equation}
\begin{array}{ll}
V=&\sum\limits_{i \in I} (\beta \tau \rho \frac{{Td({\sigma _i} - atr)}}{{{\sigma _i}(t\frac{{{\sigma _i}}}{r} + {t^2}(\frac{{{a^2} - a}}{2}))}} \\&- 
\beta \lambda \frac{{({{(\frac{{{\sigma _i}}}{r} - at)}^3} + 3t{{(\frac{{{\sigma _i}}}{r} - at)}^2} + 2a{t^3})r}}{{2t{\sigma _i}}} - r\ln (\frac{r}{{{\sigma _i}}})).
\end{array} 
\label{eq19}
\end{equation}

The server utility optimization problem with feasibility constraints is reformulated as
\begin{equation}
\begin{split}
\underset{r}{\mathop{\max }}\,&V\\
 s.t. &\max \{A_{i}^{{}}\}\le {{A}_{i}}^{\max }, \\ 
 & \max \{D_{i}^{{}}\}\le {{D}_{i}}^{\max }.
 \label{eq20}
\end{split}
\end{equation}

The first-order and second-order derivatives of ${{V}_{i}}$ with respect to $r_i$ are obtained as follows:
\begin{equation}
\begin{split}
\begin{array}{l}
\frac{{\partial {V_i}}}{{\partial {r_i}}} =  - \frac{{2T\rho d\tau \beta  \cdot \left( {\left( {{a^3} - {a^2}} \right){t^2}{r^2} + 4a\sigma tr - 2{\sigma ^2}} \right)}}{{\sigma t \cdot {{\left( {\left( {{a^2} - a} \right)tr + 2\sigma } \right)}^2}}}\\
\qquad \; \:{\rm} + \frac{{\lambda \beta  \cdot \left( {\left( {{a^3} - 3{a^2} - 2a} \right){t^3}{r^3} + \left( {3 - 3a} \right){\sigma ^2}tr + 2{\sigma ^3}} \right)}}{{2\sigma t{r^3}}} - \ln \left( {\frac{r}{\sigma }} \right) - 1,\\
\frac{{{\partial ^2}{V_i}}}{{\partial {r_i}^2}} =  - \frac{{8Ta \cdot \left( {a + 1} \right)\rho d\tau \sigma \beta }}{{{{\left( {\left( {{a^2} - a} \right)tr + 2\sigma } \right)}^3}}} - \frac{{3\lambda \sigma \beta \Lambda }}{{t{r^4}}} - \frac{1}{r},
\end{array}
\label{eq21}
\end{split}
\end{equation}
where $a>1$, ${{\theta }_{i}}>at$, $\theta _{i}^{*}=\frac{{{\sigma }_{i}}}{{{r}_{i}}}>at$, and then $r<\frac{{{\sigma }_{i}}}{at}<\frac{{{\sigma }_{i}}}{(a-1)t}$, we obtain $\Lambda =\sigma -\left( a-1 \right)tr>0$. The second order derivative $\frac{{{\partial }^{2}}V_{i}^{{}}}{\partial {{r}_{i}}^{2}}<0$ is finally obtained. It shows that the objective function in the problem is convex with respect to ${{r}_{i}}$ and the problem in Eq. \eqref{eq19} is a convex optimization problem, which can be solved by existing convex optimization tools to find the optimal solution of ${{r}_i^{*}}$ .

\begin{theorem}
There exists a unique stackelberg equilibrium in the proposed game $(\theta _{i}^{*},r_{i}^{*})$.
\end{theorem}

\begin{proof}
Based on the given reward policy $r$, each node has an optimal update policy $\theta _{i}^{*}$, which is unique due to the convex characterization of the payoff function ($\frac{{{\partial }^{2}}U_{i}^{{}}}{\partial \theta _{i}^{2}}<0$). Next, the server has a unique optimal policy under the best response of all nodes ($\frac{{{\partial }^{2}}V_{i}^{{}}}{\partial {{r}_{i}}^{2}}<0$). The unique equilibrium is reached since the final policy $(\theta _{i}^{*},r_{{}}^{*})$ maximizes the node's utility and the server's utility, respectively. $\qedsymbol$
\end{proof}

\section{DRL-based Stackelberg Game Approach}
\subsection{DRL for Stackelberg Game}
Traditional heuristic algorithms require full information about the game environment. However, due to the non-cooperative relationship, each game player is not willing to disclose its private information. DRL aims to learn decision-making based on past experiences, current states, and given rewards. To address decision-making problems with continuous action spaces, this paper employs the MADDPG algorithm \cite{b55}, a DRL algorithm for use in a multi-intelligence environment. We describe the detailed definition of each term as follows.

\textbf{State Space:} In the current decision round $t$, the state space is defined by the price strategy $R^t = \{r^t_1, \dots, r^t_i, \dots, r^t_I\}$ assigned by the server to each edge node, and the caching strategy $\Theta^t = \{\theta^t_1, \dots, \theta^t_i, \dots, \theta^t_I\}$ of the edge node. Formally, the state space at round $t$ is represented as $S^t \overset{\Delta}{=} \{R^t, \Theta^t\}$.

\textbf{Partially Observable Space:} For privacy protection reasons, the agents at the edge nodes cannot observe the complete state of the environment and can only make decisions based on their localized observations in the formulated partially observable space. At the beginning of each training round $t$, the server first decides on its strategy based on its own historical strategy, which can be regarded as the observation space of the server: $o^t_{server} \overset{\Delta}{=} \{R^{t-L}, \Theta^{t-L}, \dots, R^{t-1}, \Theta^{t-1}\}$. Then, edge node $i$ determines its caching strategy based on the historical pricing strategy of the server and the historical update strategies $\Theta^t_{-i} = \{\theta^t_1, \dots, \theta^t_{i-1}, \theta^t_{i+1}, \dots, \theta^t_I\}$ of the other edge nodes. Therefore, the observation space of edge node $i$ is denoted as $o^t_{node} \overset{\Delta}{=} \{R^{t-L}_i, \Theta^{t-L}_{-i}, \dots, R^{t-1}_i, \Theta^{t-1}_{-i}\}$.

\textbf{Action Space:} After receiving the observation $o^t_{server}$, the server agent must take an action $x^t_{server} = r^t$ with the goal of utility maximization. Considering the bid limit $r^{max}$, the action space is defined as $r\in[0, r^{max}]$. The edge node determines a caching strategy $x^t_{node} = \theta^t$ after receiving the observation $o^t_{node}$.

\textbf{Reward Function:} After all agents take actions, each agent receives an immediate reward $e_t$ corresponding to the current state and the action taken. The reward functions of the nodes and server are aligned with the utility functions in Eq. \eqref{eq5} and \eqref{eq12}.

In each training cycle, the server determines a payment strategy. Upon observing the payment strategy from the server, the edge nodes determine their feedback. After the server receives the optimal training strategies from the edge nodes, it proceeds to determine the payment strategy. The server also updates the strategy and value function based on the rewards from each training cycle.

\subsection{MADDPG Algorithm}
\begin{figure}[t!]
	\centerline{\includegraphics[width=0.49\textwidth]{fig2-4a.jpg}}
        \caption{DRL algorithm for Stackelberg game.}
	\label{fig11a}
\end{figure}

\begin{figure}[b]
	\centerline{\includegraphics[width=0.5\textwidth]{fig2-4b.jpg}}
        \caption{Details of the DRL Controller.}
	\label{fig11b}
\end{figure}

The process of the DRL-based Stackelberg game is illustrated in Fig. \ref{fig11a}, where the server serves as the leader and nodes act as the followers. In each training cycle, the server agent observes the state $o^t_{server}$ and determines the action $r^t_{server}$, while the node agent observes the state $o^t_{node}$ and determines the action $\theta^t_{node}$. Subsequently, the current state transitions to the next, and the agents receives the reward. The detailed components of the DRL controller for each agent are shown in Fig. \ref{fig11b}. The replay buffer is used to store transitions, including the current state, action, reward, and next state, collected during interactions with the environment. These stored transitions are sampled in batches to decorrelate sequential data and stabilize the training process. The actor and critic network comprising three fully connected layers. The actor network takes the current state as input and outputs the corresponding action by generating a policy. The critic network evaluates the action taken by the actor network and provides a value estimation to guide policy improvement. Both the actor and critic networks are updated using two separate optimization modules. The policy optimizer updates the parameters of the actor network based on the policy gradient, while the value optimizer minimizes the temporal difference error to refine the critic network's value estimations.

The actor network is responsible for outputting a deterministic action $x_t$  based on the current environment state $s_t$. The actor network $\mu(o_t|\phi^\mu)$ is defined by the parameters $\phi^\mu$. The parameters $\phi^\mu$ of the actor network are updated using the policy gradient formula, given as
\begin{equation}
\begin{split}
\nabla_{\phi^\mu}J\approx E_{o_t\sim\rho^\beta}\left[\nabla_{\phi^\mu}Q\left(o_t,\mu\left(o_t\middle|\phi^\mu\right)\middle|\phi^Q\right)\right],\label{eq104}
\end{split}
\end{equation}
where $\rho^\beta$ denotes the state distribution under the behavior policy $\beta$, while $\nabla_{\phi^\mu}Q\left(o_t,\mu\left(o_t\middle|\phi^\mu\right)\middle|\phi^Q\right)$ represents the gradient of the value function for the action chosen by the actor network in the state $o_t$.

The critic network $Q(o_t,x_t|\phi^Q)$ is defined by the parameters $\phi^Q$. It is trained by minimizing the mean squared error between the predicted Q-values and the target Q-values, with its loss function $L$ defined as
\begin{equation}
\begin{split}
L\left(\phi^Q\right)=E_{\left(o_t,x_t,e_t,o_{t+\mathbb{1}}\right)\sim\mathbb{D}}\left[\left(y_t-Q\left(o_t,x_t\middle|\phi^Q\right)\right)^2\right],\label{eq105}
\end{split}
\end{equation}
where $\mathbb{D}$ represents the experience replay buffer, and $y_t$ is the target Q-value.

\begin{algorithm}[!b]
\caption{MADDPG-based Solution for Stackelberg Game}\label{alg1}
\begin{algorithmic}[1]
    \STATE Initialize maximum episodes $E$, maximum time steps $T$ in an episode, batch size $B$
    \FOR{Each agent}
        \STATE Initialize critic network $Q(s_t,x_t|\phi^Q$) and actor $\mu(s_t|\phi^\mu)$ with weights $\phi^Q$ and $\phi^\mu$
    \ENDFOR    
    \WHILE{$e\le E$}
        \STATE Initialize a random process $N$ for action exploration
        \STATE Receive initial observation state $o_1$
        \WHILE{$t\le T$}
            \STATE Select action $x_t=\ \mu(o_t|\phi^\mu)+N_t$ according to the current policy
            \STATE Execute action $x_t$, and observe reward $e_t$ and observe new state $o_{t+1}$
            \STATE Store transition $(o_t,x_t,e_t,o_{t+1})$ in $B$
            \STATE Sample a random minibatch of $N$ transitions\\ $(o_i,x_i,e_i,o_{i+1})$ from $B$
            \STATE Set $y_i$ by Eq. \eqref{eq106}
            \STATE Update critic by minimizing the loss formula Eq. \eqref{eq105}
            \STATE Update the actor policy using the sampled policy gradient formula Eq. \eqref{eq104}
            \STATE Update the target networks by Eq. \eqref{eq107}
            \STATE $t=t+1$
        \ENDWHILE
        \STATE $e=e+1$
    \ENDWHILE       
\end{algorithmic}
\end{algorithm}

The DDPG employs Temporal Difference (TD) learning to update the critic network, with the target value $y_t$ calculated based on the Bellman equation as
\begin{equation}
\begin{split}
y_t=e_t+\gamma Q\left(o_{t+1},\mu\left(o_{t+1}\middle|\phi^{\mu^\prime}\right)\middle|\phi^{Q^\prime}\right),
 \label{eq106}
\end{split}
\end{equation}
where $e_t$ is the immediate reward, $\gamma$ is the discount factor, and $\phi^{\mu^\prime}$ and $\phi^{Q^\prime}$ represent the parameters of the target actor network and target critic network, respectively. These target network parameters are softly updated at a slow rate $\tau$ towards the main network parameters $\phi^\mu$ and $\phi^Q$ to ensure stability during the training process
\begin{equation}
\begin{split}
\phi\gets\tau\phi+(1-\tau)\phi^\prime.
 \label{eq107}
\end{split}
\end{equation}

By alternately updating the actor and critic networks using the above loss function and gradient update formulas, the policy gradually converges to the optimal strategy that maximizes the cumulative reward.

The MADDPG-based solution for the Stackelberg game is presented in Algorithm~\ref{alg1}. In each training step, the MADDPG algorithm samples and stores experiences from $I$ agents and then samples a batch of $B$ experiences from the stored memories to train each agent. The MADDPG algorithm utilizes a deep neural network to construct both an actor network and a critic network, each comprising an input layer, a hidden layer, and an output layer. The primary factor influencing the time complexity is the dimensionality of the network architecture. Let $S$ represent the dimensionality of the state space, $L$ represent the dimensionality of the action space, and $H$ represent the number of neurons in the hidden layer. The computational complexity of the actor network is $O(SH + HL + H^2)$, while the complexity of the critic network is $O((S + L)H + H^2 + H)$. Since the target actor and critic networks have the same architecture, the complexity for a single agent is $O(2(SH + HL + H^2) + 2((S + L)H + H^2 + H))$. As a result, the overall time complexity of the algorithm is $O(2IB(2H^2 + 2(S + L)H + H))$.

\section{Simulation Results}
To verify the impact of the proposed satisfaction-aware incentive scheme on FL in the industrial Metaverse, this paper conducts experiments using Python 3.7 and TensorFlow 1.15, and evaluates the performance of the scheme. The value ranges of the simulation parameters are shown in Table \ref{tab2}. We take image classification tasks as an example of industrial Metaverse applications. In the physical space, the robots perform image classification tasks without loss of generality. We use the MNIST dataset, consisting of 60,000 samples for the training set and 10,000 samples for the test set. To evaluate the performance of the proposed scheme, we compare it with four reinforcement learning algorithms: MADDPG \cite{b55}, MAPPO \cite{b53}, MASAC \cite{b54}, and MADQN \cite{b56}, which are DDPG algorithm, PPO algorithm, SAC algorithm, and DQN algorithm in multi-agent environment.
\begin{table}[htbp]
\caption{Training Parameters Information}
\begin{center}
	\begin{tabular}{|p{5cm}|p{1.5cm}|}
		\hline
		\textbf{Parameter}                      & \textbf{Set up}         \\ \hline
		Total number of nodes $I$                 & [5,25]          \\ \hline
		Duration from the end of data collection to the beginning of the next phase of data collection $a$                & [1,8]          \\ \hline
		Duration of training missions $T$                    & 10          \\ \hline
		Total model training time period $t$                    & 1  \\ \hline
		Update data volume $d$                        & [10,80]          \\ \hline
		Parameters of model quality $\rho $                        & [3,7]        \\ \hline
        Satisfaction Unit Profit $\beta $                &3       \\ \hline
	\end{tabular}
\label{tab2}
\end{center}
\end{table}

\subsection{Satisfaction and Utility Analysis}
Fig. \ref{fig2} illustrates the impact of update cycles on satisfaction. In the early stages of the update cycle, the AoI decreases rapidly with an increase in the update cycle, significantly improving model quality and leading to a rapid increase in satisfaction. However, as the update cycle reaches a certain threshold, the impact of increasing latency becomes apparent, causing satisfaction to peak and then decline. This indicates the existence of an optimal update cycle for optimizing node satisfaction. Moreover, satisfaction decreases with an increase in parameter $a$ and increases with an increase in parameter $d$.

\begin{figure}[!t]
	\centerline{\includegraphics[width=0.5\textwidth]{fig3a.pdf}}
        \caption{Satisfaction for different update cycle.}
	\label{fig2}
\end{figure}

\begin{figure}[!t]
	\centerline{\includegraphics[width=0.5\textwidth]{fig4.pdf}}
        \caption{Server utility for different schemes and numbers of selected nodes.}
	\label{fig3}
\end{figure}

Fig. \ref{fig3} shows a comparative analysis of server utility under different incentive schemes with the maximum budget constraint ($R^{max}=50$). The utility of our scheme reaches its peak when 15 nodes are chosen, indicating an optimal balance between the number of nodes and the budget constraint. While increasing the number of nodes usually results in higher utilities, due to the budget limit, utilities decrease after the optimal point. The results suggest that our scheme consistently outperforms other strategies across all node numbers, indicating an optimized balance between cost and quality incentives to maximize utility within the given budget. The quality-first and price-first strategies show competitive performance, although neither dominates across all node numbers. The randomized pricing and randomized node combination strategies yield lower utility, which may suggest a less effective node selection or pricing model under these approaches.

Fig. \ref{fig5} illustrates the comparison of normalized server utility under different budget constraints. The figure shows three bar sets corresponding to different maximum budget constraints: $R^{max}=50$, $R^{max}=100$, and $R^{max}=150$. The number of selected nodes ranges from 5 to 25. As the number of selected nodes increases, the server utility generally increases under each budget constraint. However, the extent of the utility increase varies with the budget limit. For the lowest budget ($R^{max}=50$), the utility increases with the number of nodes but then levels off, indicating that beyond a certain point, additional nodes do not increase utility within the budget constraint. As the budget constraint increases from $R^{max}=100$ to $R^{max}=150$, utility continues to increase with the number of nodes, suggesting that higher budgets can effectively utilize more nodes to generate utility. The highest utility is observed under the constraint $R^{max}=150$, particularly with 100 nodes. This means that the server can utilize a higher budget to maximize its utility potential. The server's ability to generate utility is significantly affected by the number of nodes it can support, which in turn is limited by its budget.

Fig. \ref{fig6} illustrates the comparative analysis of server strategies under different conditions. The unit cost per node ranges from 1 to 5. Fig. \ref{fig6a} shows the server bids under different unit satisfaction margins $\sigma$. As the node unit cost increases, the server bids decrease for all values of $\sigma$. Higher satisfaction margins result in a sharp decrease in bids. This suggests that as the node cost increases, servers are inclined to bid higher for nodes with higher satisfaction margins. Fig. \ref{fig6b} shows the server's bid for different data sizes $d$. As in Fig. \ref{fig6a}, all three lines exhibit a decreasing trend, indicating that as node cost increases, the server's willingness to bid declines. This decrease is more significant at larger data volumes, implying that servers favor nodes with larger data sizes when bidding. Fig. \ref{fig6} shows that both unit satisfaction profit and data size significantly affect server strategy. As the node cost increases, servers tend to place lower bids, and higher satisfaction and larger data sizes amplify this effect.

\begin{figure}[!t]
	\centerline{\includegraphics[width=0.5\textwidth]{fig5.pdf}}
        \caption{Server utility for different number of selected nodes and budget constraints.}
	\label{fig5}
\end{figure}

\begin{figure}[!t]
	\centering
	\subfigure[]{
		\includegraphics[width=0.5\textwidth]{fig6a.pdf}\label{fig6a}
	}
        \subfigure[]{
		\includegraphics[width=0.5\textwidth]{fig6b.pdf}\label{fig6b}
	}
        \caption{Normalized server bids for different parameters and unit cost of node.}
	\label{fig6}
\end{figure}

\subsection {FL Performance Comparison}
Fig. \ref{fig8} compares the total training time of different algorithms with varying total numbers of nodes. As shown in the figure, in the absence of incentives, the total training time of the global model decreases as the number of terminal devices increases. This is because the total computational resources increase as the number of devices increases. In contrast, in the incentive mechanism algorithm proposed in this paper, as the number of nodes increases, the budget available for each node decreases, which leads to a decrease in the total computational power, resulting in increased total training time. Notably, the randomized node combinations and the randomized selection algorithms show the most significant increase in training time as the node count grows, suggesting potential inefficiencies in these methods when dealing with larger sets of nodes. However, our scheme scales more efficiently, maintaining lower training times across all node numbers compared to the other strategies.

Fig. \ref{fig9} shows the accuracy comparison of FL under different schemes. The evaluation benchmarks the accuracy for 5, 15, and 25 nodes selected from a pool of 25 candidate nodes. The results indicate that our scheme achieves accuracy comparable to or better than FedAvg across all node sizes, suggesting that our scheme effectively manages the trade-offs between computation, communication, and model quality. The quality-first and price-first schemes show varying performance, with quality-first generally maintaining higher accuracy, suggesting that quality-first prioritizes model performance over cost. Conversely, price-first seems to prioritize cost-saving, potentially sacrificing accuracy.

\begin{figure}[!t]
	\centerline{\includegraphics[width=0.5\textwidth]{fig7.pdf}}
        \caption{FL training time for different number of selected nodes.}
	\label{fig8}
\end{figure}

\subsection{Solution Comparison}
Fig. \ref{fig15} compares the performance of different DRL algorithms in the Stackelberg game. In Fig. \ref{fig15a}, as the number of selected clients increases, the server utility exhibits a rise-then-fall trend, reaching its peak at 15 clients. This trend occurs because a moderate number of clients provides sufficient computational resources to optimize the training performance.  However, as the number of clients continues to increase, the limited total budget results in less resource allocation per node, leading to a decline in overall utility. Of the four algorithms, MADDPG consistently achieves the highest utility, closely approximating the SE. In contrast, the utilities of MAPPO, MASAC, and MADQN are slightly lower, showing a noticeable gap. This demonstrates that MADDPG excels in balancing resource allocation and client selection to maximize server utility. In Fig. \ref{fig15b}, as the node unit cost increases, server bids gradually decrease because the server must lower its bids to optimize resource allocation under budget constraints. The server bids in MADDPG remain consistent with the SE, whereas the server bids in MAPPO, MASAC, and MADQN diverge from the SE. Fig. \ref{fig15} shows that MADDPG consistently approximates the SE more closely than MAPPO, MASAC, and MADQN, indicating its superior performance in this Stackelberg game setting.

\begin{figure}[t]
	\centerline{\includegraphics[width=0.5\textwidth]{fig8.pdf}}
        \caption{FL accuracy for different number of selected nodes.}
	\label{fig9}
\end{figure}

\begin{figure}[!t]
	\centering
	\subfigure[]{
		\includegraphics[width=0.5\textwidth]{fig9a.pdf}\label{fig15a}
	}
        \subfigure[]{
		\includegraphics[width=0.5\textwidth]{fig9b.pdf}\label{fig15b}
	}
	\caption{Normalized server utility for different DRL algorithm and number of selected nodes.}
	\label{fig15}
\end{figure}

\section{Conclusion}
In this paper, we have designed a satisfaction-aware FL incentive scheme for the industrial Metaverse. The scheme integrates a satisfaction function that incorporates data size, AoI, and latency into the utility function and formulates this utility optimization problem as a two-stage Stackelberg game. We have employed a DRL approach to learn the equilibrium of the Stackelberg game, with the goal of maximizing the server's utility by identifying the optimal equilibrium. Experimental results have demonstrated that, compared to traditional methods, the scheme effectively balances model quality and update latency through efficient resource allocation, enhances FL performance, ensures real-time performance and high efficiency of FL in the Metaverse, and better addresses the needs of the industrial Metaverse. In the future, we will extend our meta-computing framework to incorporate efficient asynchronous FL, with the aim of enhancing learning efficiency in the industrial Metaverse.

\bibliographystyle{unsrt}
\bibliography{main}

%\vspace{-20pt}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{LXH}}]{Xiaohuan Li}
%was born in Chongqing, China. He received the B.Eng. and M.Sc. degrees from the Guilin University of Electronic Technology, Guilin, China, in 2006 and 2009, respectively, and the Ph.D. degree from the South China University of Technology, Guangzhou, China, in 2015. He was a Visiting Scholar with the Université de Nantes, France, in 2014. He is currently a Professor with the School of Information and Communication, Guilin University of Electronic Technology and Research fellow with the National Engineering Laboratory of Application Technology of Integrated Transportation Big Data(Beihang University). His current research interests include wireless sensor networks, vehicular networks, and cognitive radios.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{QSW}}]{Shaowen Qin}
%received the B.S. degree from Guilin University of Electronic Technology, Guilin, China, in 2018, and the M.S. degree from Guangxi University, Nanning, China, in 2024. She is currently pursuing the Ph.D. degree in information and communication engineering at Guilin University of Electronic Technology. Her main research interests include federated learning and the Industrial  Internet of Things.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{TX}}]{Xin Tang}
%received the B.S. and M.S. degrees from the Guilin University of Electronic Technology, Guangxi, China, in 2011 and 2015, respectively. Since 2015, he worked in the China Mobile Communications Corporation Guangxi Branch. In 2016, he joined the Institute of Information Technology of Guilin University of Electronic Technology, where he is a senior engineer. He is currently pursuing the Ph.D. degree in information and communication engineering with the Guilin University of Electronic Technology. His current research interests include edge computing, multi-agent system, and intelligent transportation system.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{KJW}}]{Jiawen Kang}
%received the Ph.D. degree from Guangdong University of Technology, China, in 2018. He has been a postdoc at Nanyang Technological University, Singapore from 2018 to 2021. He currently is a full professor at Guangdong University of Technology, China. His research interests mainly focus on blockchain, security, and privacy protection in wireless communications and networking.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{YJ}}]{Jin Ye}
%received the Ph.D. degree with School of Science and Engineering, Central South University in 2008. She is currently a professor with school of Computer, Electronics and Information, Guangxi University. She worked as a visiting scholar in Department of Computer Science and Engineering at the University of Minnesota, Twin Cities in 2018. She also is the Member of China Computer Federation. Her main research interests include network protocol design, data center networks.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{ZZH}}]{Zhonghua Zhao}
%received the M.S. degree from Guilin University of Electronic Technology, Guilin, China. He is currently a professor with the School of Information and Communication, Guilin University of Electronic Technology, Guilin, China, and has published 18 academic papers. His current research interests include vehicular networks, edge computing, and intelligent transportation systems.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{DN}}]{Dusit Niyato}
%(M'09-SM'15-F’17) is a professor in the School of Computer Science and Engineering, at Nanyang Technological University, Singapore. He received B.Eng. from King Mongkuts Institute of Technology Ladkrabang (KMITL), Thailand in 1999 and Ph.D. in Electrical and Computer Engineering from the University of Manitoba, Canada in 2008. His research interests are in the areas of sustainability, edge intelligence, decentralized machine learning, and incentive mechanism design. 
%\end{IEEEbiography}

\end{document}