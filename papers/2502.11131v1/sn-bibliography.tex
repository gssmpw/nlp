%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\usepackage{lmodern}
\usepackage{anyfontsize}

\usepackage{subcaption}
%\usepackage{subcaption}
\usepackage{natbib}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
%\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%
\bibliographystyle{sn-basic}
\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Yuqi} \sur{Liu}}\email{211224027@cupl.edu.cn}

\author*[1]{\fnm{Yan} \sur{Zheng}}\email{zhengyan@cupl.edu.cn}
%\equalcont{These authors contributed equally to this work.}

%\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
%\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{School of Information Management for Law}, \orgname{China University of Political Science and Law}, \orgaddress{\street{27 Fuxue Road}, \city{Beijing}, \postcode{102249}, \country{China}}}

%\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

%\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. 
In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in \url{https://github.com/liuyuqi123study/RankSVM_for_SLR}}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Information Retrieval, Optimization, Natural Language Processing, Legal Intelligence}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle
                                
\section{Introduction}\label{sec1}

In recent years, Legal Artificial Intelligence (AI) has attracted attention from both AI researchers and legal professionals(\cite{greenleaf2018building}). Legal AI mainly means applying artificial intelligence technology to help with legal tasks. Benefiting from the rapid development of AI, especially natural language processing (NLP) techniques, Legal AI has had a lot of achievements in real law applications (\cite{zhong-etal-2020-nlp,Shao2020BERTPLIMP,surden2019artificial}). 

Among legal AI tasks, similar case retrieval (SCR) is a representative legal AI application, as the appeal to similar sentences for similar cases plays a pivotal role in promoting judicial fairness(\cite{Shao2020BERTPLIMP}). 

As demonstrated in a lot of work, there are mainly two approaches to do information retrieval. 1) Traditional IR models(e.g.BM25 which is a probabilistic retrieval model(\cite{BM25}) using keywords. 2) More advanced techniques for IR using pre-trained models with deep learning skills, and the latter has achieved promising results in some commonly used benchmark datasets(\cite{ma2021lecard}).When using pre-trained language models, there have been a lot of variations trying to incorporate the structural information in legal documents to help with the retrieval(\cite{hu2022bert_lf,Shao2020BERTPLIMP,DBLP:journals/corr/ChungGCB14,10.1145/3609796,10.1016/j.ipm.2024.103729,feng-etal-2022-legal,10.1145/3569929}) or to utilize LLMs to boost similar case retrieval~\cite{zhou2023boostinglegalcaseretrieval}.


However, compared with attention paid to language models, there is less focus on the classifier used for the final ranking. As it is pointed out in an early paper~\cite{10.1145/1148170.1148205}, there are two important factors in document retrieval and one of them is that to have high accuracy on top-ranked documents is crucial for an IR system. So we try to look into better classifiers in the hope of improving ranking performance. As a problem of ranking by criterion of relevance, however, SCR is reduced to a 2-class classification problem following a pointwise path in a lot of work, which means the related classifier is only used to produce one label for a query-candidate pair. It is also mentioned by other papers that ~\cite{10.1007/978-3-031-10986-7_43}the fine-tuned BERT model does not compare which case candidate is more similar to the query case.

In our paper, we reintroduce pairwise methods to do the final ranking. Under pairwise settings, classifiers care about the relative order between two documents, which is closer to the actual ranking task(\cite{10.1561/1500000016}). Among pairwise approaches, we pay extra attention to the Ranking Support Vector Machine(RankSVM) method as a representative method. The advantage of RankSVM is that it aims to minimize the ranking loss and can also mitigate the negative influence of the class-imbalance issue(\cite{10.5555/2981345.2981385,DBLP:journals/corr/WuZ16}) which is common for SCR tasks. It is also mentioned in some papers (\cite{WU202024}) that the setting of binary labels compared with multi labels could help mitigate the accumulated errors from thresholds learning in RankSVM. 


So we are inspired to use RankSVM to explore its performance on similar case retrieval tasks and measure the performance using NDCG as our main metric (Normalized Discounted Cumulative Gain).

In our experiments, we test different kinds of retrieval models when combined with RankSVM to examine their retrieval performance on different similar case retrieval datasets LeCaRDv1(\cite{ma2021lecard}) and LeCardv2(\cite{li2023lecardv2largescalechineselegal}). Our method is illustrated in Figure \ref{fig:GA}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{Graph_abstract_svm_SLR.png}
    \caption{Illustration of Our Method}
    \label{fig:GA}
\end{figure}

\section{Materials and Methods}\label{sec2}
\subsection{Language Models for SCR}
In this paper, we mainly examine three language models: BERT model, BERT+LEVEN, and Lawformer language model, which respectively represent basic language model, knowledge incorporated model, and language model with long inputs. BERT(\cite{devlin2019bertpretrainingdeepbidirectional}) is a commonly used language model with a maximum window length of 512, and many researchers have done many implementations that have a lot to do with how they understand structural information in legal documents and concatenate them with models that could deal with limited length (\cite{Shao2020BERTPLIMP,hu2022bert_lf,10.1007/978-3-031-10986-7_43}). So in our experiments, we both examine the basic BERT model and the BERT model with structural information extracted from legal documents. We use the code from LEVEN(\cite{yao-etal-2022-leven}) as an example where it incorporates event detection. 

Lawformer is a representative model that people propose to deal with longer inputs (\cite{xiao2021lawformer})which is based on longformer architecture (\cite{DBLP:journals/corr/abs-2004-05150}). Lawformer could process long texts up to respectively 509 for query documents and 3072 for candidate documents. 

Some work(\cite{xu2024rankmambabenchmarkingmambasdocument}) moves on to examine another new pre-trained model Mamba(\cite{gu2024mambalineartimesequencemodeling}) that could deal with long inputs as well, where MAMBA achieves competitive performance compared with transformer-based models using the same training recipe in the document ranking task. So we also measure its performance on the LeCaRDv1 dataset.

\subsection{Datasets and Data Preprocessing}
In our experiments, we use two benchmark datasets that are commonly used for similar case retrieval tasks--LeCaRDv1 and LeCaRDv2. Here we mainly examine the difference between these two datasets. LeCaRDv1 dataset(A Chinese Legal Case Retrieval Dataset)(\cite{ma2021lecard}) and LeCaRDv2 dataset(\cite{li2023lecardv2largescalechineselegal}) are two datasets specifically and widely used as benchmarks for similar case retrieval tasks since their release. We first check the difference in the number of query files and candidate files, which is shown in Table \ref{tab:details}. While LeCaRDv1 contains 107 queries and 10,700 candidates, LeCaRDv2 contains more queries and candidates. At the same time, it is worth attention that while LeCaRDv1 has a separate candidate pool(folder) of size 100 for each query, there is no subfolder for LeCaRDv2 queries. Moreover, there lies some divergence in their judging criteria. While LeCaRDv1 focuses on the fact part of a case involving key circumstances and key elements, LeCaRDv2 argues that characterization, penalty, and procedure should all be taken into consideration. Even though they say in their paper that there is no explicit mapping function between the Overall Relevance and the sub-relevance, the calculation could be described as follows, see \ref{relev}.
\begin{table}[ht]
\caption{Details about LeCaRDv1 and LeCaRDv2 datasets}
\centering
\begin{tabular}{c c c} 
\toprule
datasets&\textbf{LeCaRDv1}	& \textbf{LeCaRDv2}\\
\midrule
\#candidate cases/query		& 100&55192\\
\midrule
\#average relevant cases per query&10.33&20.89\\	
\midrule
ratio of relevance candidates&0.1033&0.0004\\
\bottomrule
\end{tabular}
\label{tab:details}
\end{table}

\begin{equation}
    relevance_{v2}=relevance_{v1}+relevance_{penalty}+relevance_{procedure.}
\label{relev}
\end{equation}

As it is known, a legal document could usually be partitioned into 3 parts--Parties' Information, Facts, Holding, and Decision, which is illustrated in Figure \ref{fig:illus}. Here we follow the common practice to only take fact parts of those documents as inputs, see equation \ref{input} and \ref{input_v2}. 
\begin{equation}
    input_{v1}=(query['q'],cand['ajjbqk'])
    \label{input}
\end{equation}
\begin{equation}
    input_{v2}=(query['fact'],cand['fact'])
    \label{input_v2}
\end{equation}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{case_illustration.pdf}
    \caption{Illustration of Structure of A Case}
    \label{fig:illus}
\end{figure}

Moreover, we examine the fact parts in both datasets to be used as our inputs, see Figure \ref{fig:lengths of datasets} and Figure \ref{fig:lengths of datasets_v2}, from which we could tell that many of them are actually longer than the input limit of the BERT model as well as the Lawformer or MAMBA model. 
At the same time, as we use the same number of bins in the histogram for both LeCaRDv1 and LeCaRDv2 datasets,  it could be seen that while the LeCaRDv2 dataset has query files of longer lengths, it has candidate files of shorter lengths. As it is not



\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{histogram_v1_candidates.pdf}
         \caption{Lengths of Candidates in LeCaRDv1 Dataset}
         \label{fig:Lengths of Candidates in LeCaRDv1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{histogram_v1_queries.pdf}
         \caption{Lengths of Queries in LeCaRDv1 Dataset}
         \label{fig:Lengths of Queries in LeCaRDv1 Dataset}
     \end{subfigure}
        \caption{Length of Documents in LeCaRDv1 Dataset}
        \label{fig:lengths of datasets}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{v2_candidate.pdf}
         \caption{Lengths of Candidates in LeCaRDv2 Dataset}
         \label{fig:Lengths of Candidates in LeCaRDv2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{v2_query.pdf}
         \caption{Lengths of Queries in LeCaRDv2 Dataset}
         \label{fig:Lengths of Queries in LeCaRDv2 Dataset}
     \end{subfigure}
        \caption{Length of Documents in LeCaRDv2 Dataset}
        \label{fig:lengths of datasets_v2}
\end{figure}

%Compared with its precedent LeCaRDv1, LeCaRDv2 ~\cite{li2023lecardv2largescalechineselegal} has 800 queries and 55192 candidates. It is said that it covers 50 charges which are more than 20 covered by LeCaRDv1.
%Besides, LeCaRDv2 dataset changes its criterion of relevance from just basic facts to  It also argues that the pooling strategy employed in LeCaRDv1 is too naive, aimed at a different setting that emphasizes directly retrieving relevant cases from a large legal corpus so there's no subfolder in the dataset.

With different benchmark datasets, we conduct different data preprocessing. We also conduct visualization to check LeCaRDv2's length of documents. See Figure \ref{fig:lengths of datasets_v2} to check their distribution. It could be told that case documents in LeCaRDv2 dataset are shorter .However, as there is no subfolder for LeCaRDv2 dataset in its original repository \url{https://github.com/THUIR/LeCaRDv2}, out of consideration for memory and the potential risk of probable overfitting if we conduct experiments with all negative samples among the corpus, we conduct our experiments trying to follow the practice on LeCaRDv1 dataset by building a subfolder of 130 candidates file for each query, among which 30 are those that are labeled relevant which is also similar with the practice taken in LeCaRDv2 baseline where they have negative samples with the ratio of positives and negatives at 1:32. It is shown by our result that it is a wise choice. We also follow different train-test splits. For LeCaRDv1, we follow the 5-fold validation while for LeCaRDv2 we took 640 instances as our training dataset and 160 as our test dataset.

At the same time, the data preprocessing does differ from model to model. For Lawformer, we basically follow the practice in its paper to have candidate files of length 3072 and query files of length 509. For LEVEN code, what we do is we detect their queries and candidate files to get their trigger words and relevant event types. According to the original paper, there are As a result, for non-trigger tokens, it just feeds the sum of token embeddings and position embeddings into the BERT model. For trigger words, we define an event type embedding for each event type and add the corresponding embedding to each input.
\begin{equation}
    inputx_{without_event}=func_{tokentoids}('[CLS]'+query+'[SEP]'+cand+'[SEP]')
\end{equation}
\begin{equation}
    inputx_{use_event}=id_{CLS}+id_{query}+id_{sep}+id_{candidate}+id_{sep}
\end{equation}
\begin{equation}
    id_{events}=[0]+id_{query_event}+[0]+id_{cand_event}+[0]
\end{equation}

\subsection{Fine-tuning with Pretrained Model And Feature Extraction}

When we do fine-tuning, we basically follow the practice taken in LEVEN code \url{https://github.com/thunlp/LEVEN}. For BERT-related models, we add a two-class classification layer after the pooled output of BERT denoted as [CLS] and do fine-tuning with the training set. 

For Lawformer, the concrete steps are basically the same as BERT. In terms of  hyperparameters, we basically follow what is used in LEVEN code since  according to our experiments those are better than the original Lawformer code when applied to a smaller batch size. The hyperparameters used are shown in Table \ref{table:Parameters for BERT model}.

\begin{equation}
output=func_{twoclassclassification}([CLS])
\end{equation}
%As we said before, when doing fine-tuning, in the BERT model,BERT based LEVEN model, and lawformer model, we have a fully connected layer after the CLS token to act as a classifier. 

To extract features, we just use the output of CLS of every query-candidate pair as our features. According to the dimension of CLS, we have 648 features for each query-candidate pair. For further experiments, we only pick the models with the best performance on the validation dataset or test dataset for feature extraction. In the LeCaRDv1 dataset,when training on each fold,we iterate the training dataset for 5 epochs and choose the epoch with the best performance compared with other epochs on the respective validation dataset. After training and validating on each fold, we report the average value for the 5 folds. In the LeCaRDv2 dataset, we iterate the training dataset for 5 epochs and choose the epoch with the best performance on the test dataset to extract features. Our model is only trained from the training data.

The result of RankSVM is used to rank the similarity of candidate-query pairs under the same candidate files.
\begin{equation}
 features=[CLS]
\end{equation}
\subsection{RankSVM}
Since RankSVM is a relatively classic method developed from ordinal-regression SVM to do information retrieval(\cite{Joachims1998MakingLS,jakkula2006tutorial}), we try to apply RankSVM(\cite{Joachims1998MakingLS,zheng2019ranksvm}) for similar case retrieval task in this work. The mathematical formulation for RankSVM is shown below. Given n training queries ${q_i}^n_{i=1}$, their associated document pairs $(x_u^{(i)},x_v^{(i)})$ and the corresponding ground truth label $y_{u,v}^{(i)}$, where a linear scoring function is used without complicated kernel, i.e., $f(x)=w^Tx$.
%The intuitive idea of RankSVM is that it changes the classic classification setting to solve the ordinal regression problem based on the relationship of different labels.

The basic form of RankSVM is shown as follows.
\begin{equation}min\frac{1}{2}\||w\||^2+C\sum^n_{i=1}\sum_{u,v:y^{(i)}_{u,v}=1}\xi_{u,v}^{(i)}\end{equation}
\begin{equation}s.t. w^T(x_u^{(i)}-x_v^{(i)})\geq1-\xi_{u,v}^{(i)}, if y_{u,v}^{(i)}=1\end{equation},
\begin{equation}\xi_{u,v}^{(i)}\geq 0,i=1,\dots,n.
\end{equation}

In our experiment, we mainly use the method developed in the paper by Joachims(\cite{Joachims1998MakingLS}), where the algorithm just has 1 slack variable and it is solved using its dual form. Different algorithms for RankSVM are suitable for different applications. According to our experiment, the result with the algorithm we chose is better than other settings.(e.g., see this work(\cite{10.1145/1102351.1102399}) Also, we use different values of C, and they are 0.001, 0.05, 0.01, 0.02, 0.05, 0.1, 0.5, 1. We report the best result for each model.
%The main advantages of the method are 1) An efficient and effective method for selecting the working set, which is an active part of the optimization problem. 2) Successive "shrinking" of the optimization problem. This exploits the property that many SVM learning problems have i) much fewer support vectors(SV) than training examples. ii) many SVs which have an $\alpha_i$ at the upper bound C.3) computational improvements like caching and incremental updates of the gradient and the termination criteria. 

%In their method, they reformulated the problem as follows:

%\begin{equation}min:W(\alpha)=-\sum^l_{i=1}\alpha_i+\frac{1}{2}\sum^l_{i=1}\sum^l_{j=1}y_iy_j\alpha_i\alpha_jk(x_i,x_j)\end{equation}
%\begin{equation}subject\enspace to:\quad\sum^l_{i=1}y_i\alpha_i=0\end{equation}
%\begin{equation}\forall i:0\leq\alpha_i\leq C\end{equation}.

%To select a good working set, it is needed to find a steepest feasible direction $d$ of descent which has only $q$ non-zero elements; those non-zero elements will compose the current working set, satisfying
%The algorithm itself consists of a general decomposition algorithm,selecting a good working set, and shrinking.

It is worth mentioning that here we don't need a threshold and we just get results in scores and do ranking. Under the setting of predicting with a threshold, as it is said, there will be an implicit presumption that when in training and test, its input has the same data distribution and accumulated error(\cite{WU202024}) and may impede model performance.



%For LeCaRDv1, we  adopt top-k Normalized Discounted Cumulative Gain(NDCG@K) as our main evaluation metric. 

According to the original LeCaRDv1 and LeCaRDv2 paper, there are 4 classes of relevance. Here we just process it as a two-class classification. When processing labels, as long as two cases are relevant, we view the label of the pair as positive. 

\begin{table}[ht]
\caption{Parameters of Different Models}
\centering
\begin{tabular}{ c c c c}
\toprule
Model&BERT-Based&Lawformer&MAMBA\\
\midrule
  Algorithm& 1-slack algorithm (dual)& 1-slack algorithm (dual)& 1-slack algorithm (dual) \\
\midrule
Norm&l1-norm&l1-norm&l1-norm\\
\midrule
Learning Rate&1e-5&2.5e-6(-1)/5e-6(v2)&1e-5\\
\midrule
Optimizer&Adamw&Adamw&Adamw\\
\midrule
Training batch size&16&2(v1)/4(v2)&16\\
\midrule
Evaluating batch size&32&32&32\\
\midrule
Step size&1&1&1\\
\bottomrule
\end{tabular}
\label{table:Parameters for BERT model}
\end{table}

%\begin{table}[h]
%\caption{Parameters of Lawformer based model}
%\centering
%\begin{tabular}{ c c}
%\toprule
%Parameters\\
%\midrule
 % Algorithm& 1-slack algorithm (dual) \\
%\midrule
%Norm&l1-norm\\
%\midrule
%Learning Rate&2.5e-6(v1)/5e-6(v2)\\
%\midrule
%Optimizer&Adamw\\
%\midrule
%Training batch size&2(v1)/4(v2)\\
%\midrule
%Evaluating batch size&32\\
%\midrule
%Step size&1\\
%\bottomrule
%\end{tabular}
%\label{table:Parameters for Lawformer model}
%\end{table}
\section{Result}
\subsection{Results on LeCaRDv1}

To test the effectiveness of our method, we conduct our experiments mainly using BERT, BERT+Event, BERT+Event+RankSVM, BERT+RankSVM, Lawformer, Lawformer+RankSVM. The result is shown in Table \ref{table:LeCaRDv1-2}.

From the result on BERT, we could see that RankSVM helps improve performance especially on NDCG, which proves that our model is better at putting highly relevant cases earlier in a similar case list. To better understand the advantage of RankSVM, we finish visualization based on the training and testing dataset on fold 0 to see the score computed by original BERT and BERT+RankSVM. Specifically, we use features extracted by BERT as our input and compress them into two dimensions using t-SNE(\cite{JMLR:v9:vandermaaten08a}). After that, we color them according to similarity scores computed by relative models or labels. See Figure \ref{fig:v1_train} and \ref{fig:v1_test} for reference.
\begin{table}[ht]
\caption{Results on LeCaRDv1 Dataset}

\begin{tabular}{ccccccc}
\toprule
  Model&NDCG@10&NDCG@20 & NDCG@30\\
  \midrule
 BERT  &0.7896  &0.8389 & 0.9113\\
 BERT+RankSVM&\textbf{0.7963} &\textbf{0.8504} & \textbf{0.9166}\\
\midrule
BERT+LEVEN& 0.7881 &0.8435 & 0.9136 \\
BERT+LEVEN+RankSVM&\textbf{0.7931} &\textbf{0.8452} &\textbf{0.9164}\\
\midrule
MAMBA&0.7469&0.8013&0.892\\
\midrule
Lawformer&0.7592&0.8169&0.8993\\
Lawformer+RankSVM&\textbf{0.7738}&\textbf{0.8258}&\textbf{0.9073}\\
\bottomrule
\end{tabular}

\label{table:LeCaRDv1-2}

\end{table}
\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{label_train.pdf}
         \caption{Visualization of Relevance Labels}
         \label{fig:labels_v1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{BERT_train.pdf}
         \caption{Visualization of BERT Features}
         \label{fig:BERT_v1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{RankSVM_BERT.pdf}
         \caption{Visualization of Rank-SVM Scores}
         \label{fig:RankSVM_v1}
     \end{subfigure}
        \caption{Visualization of Labels/Features/Scores on LeCaRDv1 Training Dataset}
        \label{fig:v1_train}
\end{figure}

From the visualization result, we could see that RankSVM performs better by differentiating the relevance level of each pair continuously, while BERT treats the relevance level more discretely.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{label_test.pdf}
         \caption{Visualization of Relevance Labels}
         \label{fig:labels_v1_test}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{BERT_test.pdf}
         \caption{Visualization of BERT Features}
         \label{fig:BERT_test_v1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{RankSVM_test.pdf}
         \caption{Visualization of RankSVM}
         \label{fig:RankSVM_v1_test}
     \end{subfigure}
        \caption{Visualization of Labels/Features/Scores on LeCaRDv1 Test Dataset}
        \label{fig:v1_test}
\end{figure}

What's more, the mechanism of RankSVM is actually maximizing ROC(Receiver Operating Characteristic curve)(\cite{Ataman2005OptimizingAU}). So we use the model of BERT comparing the results with and without RankSVM to show how ROC changes. For the BERT model, we use the first fold and the C is 0.01, and the visualization result is shown in Figure \ref{fig:ROC_curve_v1}. It could be seen that RankSVM helps improve the result of BERT on the AUC score, and it does an even better job in the test dataset. It proves our guess before that it could help reduce overfitting.
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ROC_training.pdf}
         \caption{ROC Curve for LeCaRDv1 Training Dataset}
         \label{fig:ROC_v1_train}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{test_set_v1_roc.pdf}
         \caption{ROC Curve for LeCaRDv1 Test\\ Dataset}
         \label{fig:ROC_v1_test}
     \end{subfigure}
        \caption{Three simple graphs}
        \label{fig:ROC_curve_v1}
\end{figure}
Additionally, to explore SVM's performance on longer text for retrieval, we also benchmark its performance using Lawformer. It could also be seen that all NDCG-related metrics get improved. 

We also conduct experiments using MAMBA. As the performance is not competent enough, we don't do further combinations with RankSVM.


\subsection{Results on LeCaRDv2}
The result is shown in Table \ref{table:LeCaRDv2 Result}.According to the table, we could find that the NDCG related metrics are generally improved similar to LeCaRDv1, except for the BERT model. To understand its performance, here we also do visualization using features extracted from the BERT model following the same steps as the LeCaRDv1 dataset, see Figure \ref{fig:Visualization_on_v2_train} and \ref{fig:Visualization_on_v2_test}.
\begin{table}[ht]
\caption{Results on LeCaRDv2 Dataset}
\centering

\begin{tabular}{c c c c c} 
\toprule
 Model&NDCG@10&NDCG@20&NDCG@30\\
\midrule
 BERT&\textbf{0.8351}&\textbf{0.8764}&\textbf{0.9348}\\
\midrule
BERT+LEVEN&0.8117&0.8455&0.9184 \\
\midrule
BERT+RankSVM&0.793&0.8582&0.9247\\
\midrule
BERT+LEVEN+RankSVM&\textbf{0.8078}&0.8548&0.9227\\
\midrule
Lawformer&0.7968&0.8461&0.919\\
\midrule
Lawformer+RankSVM&\textbf{0.812}&0.8574&0.9247\\
\bottomrule
\end{tabular}
\label{table:LeCaRDv2 Result}
\end{table}
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{train_v2.pdf}
         \caption{Visualization of Relevance Labels}
         \label{fig:v2_label}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{train_v2_svm.pdf}
         \caption{Visualization of RankSVM Scores}
         \label{fig:v2_SVM}
     \end{subfigure}
        \caption{Visualization of Labels/Scores on LeCaRDv2 Training Dataset}
        \label{fig:Visualization_on_v2_train}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{test_label_v2.pdf}
         \caption{Visualization of Relevance Labels}
         \label{fig:v2_label_test}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{test_svm_v2.pdf}
         \caption{Visualization of RankSVM Scores}
         \label{fig:v2_SVM_test}
     \end{subfigure}
        \caption{Visualization of Labels/Score on LeCaRDv2 Test Dataset}
        \label{fig:Visualization_on_v2_test}
\end{figure}

It is shown by the figure with real labels that BERT features fail to distinguish these two classes explicitly this time as there are a lot of overlaps between the two classes compared with what is extracted from the LeCaRDv1 dataset, while RankSVM tries to learn from those features that are not distinguished between these two classes continuously. The failure of BERT may be owing to the changed standards of relevance in LeCaRDv2 and the shorter document length of the LeCaRDv2 dataset. It explains why RankSVM added to BERT fails to help improve NDCG metrics here.
\section{Discussion}
\subsection{Impact of Length of Text}
Even after we changed the length of the input data from 512 to 800 using the MAMBA model and Lawformer model, we could see that the interesting thing here is that there is no apparent increase in metrics compared with BERT, which is also mentioned in other works(\cite{deng2024learning}).

But we could also argue that there lies some space for hyperparameter tuning in the future.
\subsection{Error Analysis}
%Even RankSVM has demonstrated the ability to increase the performance on LeCaRDv1 and LeCaRDv2 with the NDCG metric, it fails to increase the performance shown by recall metrics which is also interestingly not taken in other papers as a metric~\cite{deng2024learning}. The concrete result of the recall metric is shown in the appendix. We could also argue that maybe it is because of the similarity standard changes in LeCaRDv2.

To explore further why RankSVM fails to improve the performance on BERT-based models regarding the LeCaRDv2 dataset, we conduct experiments using multi-class BERT classification to extract features out of our speculation that RankSVM on LeCaRDv2 needs language models that provide more information and end up getting all NDCG-related metrics improved again, see table \ref{table:Result when using multi-class classification}. So our speculation holds as RankSVM still works for BERT on the LeCaRDv2 dataset under this setting. Recently, there have been more research studies on improving RankSVM, and we leave that to explore in the future.
\begin{table}[ht]
\caption{Result on LeCaRDv2 when we do multi-class classification}
\centering
\begin{tabular}{c c c c}
\toprule
Model&NDCG@10&NDCG@20&NDCG@30\\
\midrule
  BERT Multiclass&0.791&0.8443&0.9181 \\
\midrule
BERT Multiclass+RankSVM&0.8053&0.8352&0.9222\\
\bottomrule
\end{tabular}
\label{table:Result when using multi-class classification}
\end{table}


\section{Conclusion}\label{sec13}
In our paper, we examine the performance of RankSVM when combined with other language models using binary labels to serve a similar case retrieval task on the LeCardv1 dataset and the LeCardv2 dataset. We come to the conclusion that RankSVM with binary labels could help improve the performance of models by improving their concrete ranks, and we also try to give some explanation from the feature perspective. Also, the RankSVM could help fight overfitting, which results from an imbalance class common in this task. Our findings point to the potential of improving SCR task performance from an optimization perspective. There still lies some work that needs to be done to explore the appropriate model to improve performance on the recall metric on the LeCaRDv2 dataset and how the performance of RankSVM will change when we extract features with larger batch sizes. We also leave the investigation regarding RankSVM combined with other models and datasets for future work.

\backmatter


%\begin{appendices}

%\section{Section title of first appendix}\label{secA1}



%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
