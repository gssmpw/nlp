\section{Related Work}
\label{Sec:related_work}

\textbf{Automatic app maturity ratings}: The evaluation of mobile apps often involves various perspectives. In particular, identifying mobile app development is consistent with what is stated in the privacy policy concerning online advertising and tracking ~\cite{nguyen2022freely, nguyen2021measuring}, aiding developers in crafting child-friendly apps concerning both content and privacy aspects~\cite{hu2015protectingcikm, liccardi2014can}. However, fewer studies aimed at mobile app maturity rating. Therefore, there is growing concern regarding inappropriate content and maturity ratings in mobile apps, which are linked to privacy concerns. Early work by Chen et al.~\cite{chen2013isthisapp} proposed Automatic Label of Maturity ratings (ALM), a text-mining-based semi-supervised algorithm that uses app descriptions and user reviews to determine maturity ratings. The authors used the content rating from Apple App Store as the reference standard for a given app. However, this method uses keyword matching while ignoring semantic analysis. Using a similar approach for ground truth establishment, Hu et al.~\cite{hu2015protectingcikm} proposed a text feature-based SVM classifier for content rating prediction with an online training element. The previous two methods solely depend on text features despite having access to other modalities. Liu et al.~\cite{liu2016identifying} and Chenyu et al.~\cite{zhou2022automatic} extended these works by incorporating image and APK features to identify childrenâ€™s apps. However, features were limited to extracting text using OCR software, colour distribution of the icon and screenshots, and permissions and APIs. More recently, Sun et al.~\cite{sun2023not} identified discrepancies in content ratings of the same app in different geographic regions by defining rating system mappings between geographical regions. However, this research focuses on single modalities or multiple modalities but treats them independently. \\ 
% \vspace{-3mm}

\noindent\textbf{Vision-Language (VL) models}:  Early image-based contrastive representations have made advancements, nearly achieving the performance levels seen in supervised baselines across various downstream tasks such as image classification and retrieval~\cite{chen2020simple, zbontar2021barlow}. Driven by the success of contrastive learning in intra-modal tasks, there has been a growing interest in developing multi-modal objectives (e.g., Vision-Language), enabling the model to comprehend and exploit cross-modal associations.
Pioneering works such as CLIP~\cite{clip} and ALIGN~\cite{align} bridged the gap between the vision and language modalities by learning language and vision encoders jointly with a symmetric cross-entropy loss which is an adaptation of InfoNCE loss~\cite{oord2018representation} for cross-model pairs. CLIP optimises the cosine similarity between text and image embeddings, while ALIGN employs a similar contrastive learning setting with noisy training data. Zhai et al.~\cite{LiT} tuned the text encoder using image-text pairs while keeping the image encoder frozen. The rich embeddings that these methods learn are later adapted to various application domains such as video-text retrieval~\cite{fang2021clip2video, portillo2021straightforward}, image generation~\cite{nichol2021glide}, and visual assistance~\cite{massiceti2023explaining}. 
However, \cite{agarwal2021evaluating, luccioni2024stable} point out the challenges in adapting Large Multi-modal Models (LMMs) for different domains when the downstream task deviates from the originally pre-trained task. To the best of our understanding, ours is the first work to leverage the advances in VL-language models to detect content compliance malpractices specific to mobile apps. 

