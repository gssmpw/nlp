

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/fig_methodology.pdf}
    \caption{(a): Vision-language model architecture during the training stage. (b): Custom transformer block with cross-attention. (c) Pipeline for the downstream task of content rating classification.}
    \label{fig:model_architecture}
    % \vspace{-0.32cm}
\end{figure*}

% \vspace{-0.4cm}
\section{Methodology}
\label{Sec:methodology}

We propose a customised vision-language (VL) model that can be trained end-to-end with image and language data for learning joint representations directly via image patches and raw text tokens. Fig.~\ref{fig:model_architecture}(a) gives an overview of our model. It uses mobile app creatives such as app icons/screenshots and app descriptions as the paired inputs to generate joint embeddings that are useful for the downstream task of content rating prediction. We discuss our dataset in detail in Sec.~\ref{Sec:dataset}.


At a high level, we adapt two image encoder-based backbones to learn image content and style features separately. These two encoded features are merged together as image features. Then, we employ a text backbone to encode text features in the corresponding image-text pair belonging to an app. A cross-modal module then extracts relationships between image and textual features to produce the final image and textual embedding representations. We use pair-wise Sigmoid contrastive loss to learn the model parameters.

\subsection{Encoding Visual Information}
\label{sec:visual_info}
App icons and screenshots are the most prominent static-visual information the prospective app users first observe. In some cases, the style of an app icon alone is enough to distinguish many popular apps. As an example, app icons from Google would likely contain four colours: red, yellow, green and blue (\textbf{cf.} Fig.~\ref{fig:style_content_difference}). However, encoding such information 
is challenging as they do not contain features a generalised encoder such as CLIP was pre-trained on. To address this, we introduce trainable content and style encoder modules that work together to generate the final image embedding vector. Their ablation studies are further discussed in Sec.~\ref{Subsec: ablations}. 


\begin{figure}[th]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/content_style_disparity.pdf}
    \caption{Disparity between the content and style of app icons and screenshots.}
    \label{fig:style_content_difference}
    % \vspace{-0.5cm}
\end{figure}


\subsubsection{Style Encoder}
\label{sec:style_enc}

Style information (e.g., texture, colour schemes, artistic choices) is crucial when predicting content rating as it adds context beyond the objects or elements present in images.
Kid-specific apps often use cartoonish characters, which are often associated with distinctive shapes and distinctive colour palettes and textures (e.g., bright colours, glitter texture, non-complex surface reflections), while dark tones (e.g. blood) or provocative lighting would be inappropriate for younger audiences, even if the content seems neutral~\cite{liu2016identifying}.


In Fig.~\ref{fig:style_content_difference}, we provide some example cases of apps with a significant disparity between the content and style of their creatives relative to the app category and content rating. The app icon and screenshots for \emph{The Puzzle Cakes} showcase cake related content and would have to rely on the style to associate it with a puzzle game. Conversely, \emph{Pocket Love} and \emph{Dirty Crown Scandal} employ a similar animated, cartoonish style but are aimed at distinctly different audiences based on their content ratings. Furthermore, there can be inter-app content and style disparities as in the examples of  \emph{The Virus} and \emph{Spin the Bottle Game}. In both cases, the colour themes of the app icons are very different from the screenshots.  

Therefore, we embedded a separate style encoder module. We use the CLIP image encoder as the base network for the style encoder, employing a masked representation learning task. This involves two identical networks, as illustrated in Fig.\ref{fig:model_architecture} (a). 
First, we uniformly sample an image $i$ from the dataset $D$ and generate an augmented image $x_s = t_s(i)$ by applying an image augmentation $t_s \sim T$. Next, we randomly mask three $3\times3$ patches to produce the masked image $x_s^{'}$.
The masked image is provided for one network called \emph{online}, while the unmasked image passes through another network called \emph{target}, a slow-moving average network. This allows the network to focus on the features that are invariant to masking. The target network $\theta_t$ uses an exponential moving average (EMA) of the online network $\theta_s$ to learn lower semantic features~\cite{assran2022masked, he2022masked}. More precisely, given a target decay rate $\tau \in [0, 1]$, after each training step, we update the target network weights using,
\begin{equation}
\label{eq:ema}
 \theta_t \leftarrow \tau \theta_t + (1 - \tau) \theta_s.
\end{equation}
The EMA introduces stability by averaging the networkâ€™s weights over time, smoothing out rapid updates that occur during the training process. This slows down the learning of fast-changing, higher-level features and enables the capturing of lower-level feature. From the masked image $x_s^{'}$, the online network outputs a representation $q_s^{'} = f_{\theta_s}(x_s^{'})$. The target network outputs $q_s = f_{\theta_t}(x_s)$ from the augmented view $x_s$. Finally we define the mean squared error between the embeddings $q_s$ and $q_s^{'}$ 
\begin{equation}
\label{eq:l2}
 \mathcal{L}_{mse} = ||q_s - q_s^{'}||_2^2.
\end{equation} 
and is added to the final loss, which we discuss later in Sec.~\ref{subsec: loss fn}. Furthermore, the generated style embeddings from the target network are scaled down using the hyperparameter $\alpha$ and added to the embeddings generated by the content backbone. 

\subsubsection{Content Encoder}
As shown in Fig.~\ref{fig:model_architecture}(a), our image content backbone, $f_{\theta_c}$, is built on the CLIP image encoder and remains active throughout the training process. This branch operates on augmented images of 224x224 resolution, which could be derived from an app icon or screenshots. We follow the augmentation settings defined in CLIP to generate different views of the image. The content backbone outputs visual content features, $q_c$ and are combined with visual style features, $q_s$ given by the style encoder. We represent this combination using the equation $q_i=q_c+\alpha. q_s$ where $\alpha$ is empirically selected as $0.1$. 

\subsection{Encoding Textual Information}
The app description provides an overview of the app's functionalities, features, and content to the prospective audience. Often, the text is summarised as app users are reluctant to read lengthy texts (capped at 4,000 characters; the average length of an app description in the top 20,000 apps of Google Play Store is 2,169 words), and Google Play mandated it to be general audience friendly. We perform randomised text chunking with four or more consecutive sentences randomly extracted from the long app description to be paired with respective app visuals.
We used a 110 million parameter RoBERTa text transformer with maximum $256$ tokens marked as text backbone in Fig.~\ref{fig:model_architecture}(a) to encode this information while the model parameters are kept frozen during the training. 

\subsection{Image-Text Cross Attention}
\label{ssec:cross attention}

Typical image captioning datasets such as MS-COCO~\cite{lin2014microsoft} and Flickr30k~\cite{young2014image} have a strong correlation between the captions and the images.
In contrast, app icons/screenshots and descriptions can exhibit a larger semantic gap, especially in the context of the content rating prediction problem because: 1) the description may not perfectly reflect what is depicted in the app icon or screenshots, 2) these modalities may not always contain useful information related to the content/age rating, and 3) App images display complex variations within the same content rating class. Therefore, we employ a stack of cross attention layers~\cite{vaswani2017attention} to align visual and textual tokens to fill the correlation gap between image patches and words. As shown in Fig.~\ref{fig:model_architecture}(b), our custom cross-attention module initially has a self-attention layer followed by a cross-attention layer. This layer induces text information to the image features. The \emph{query} values are derived from the previous image content encoder layer, and the memory \emph{keys} and \emph{values} are obtained from the hidden layers of the text encoder and vise versa. Text-to-image cross attention allows every patch of the image to attend over all tokens in the input sequence. Conversely, in \emph{image-to-text cross attention}, the roles are reversed, allowing every token of the input sequence to attend over all patches in the content image.
Finally, we introduce an additional linear projection layer, which outputs the final visual and textual embeddings.

\vspace{-0.15cm}
\subsection{Loss Function}
\label{subsec: loss fn}

Given a paired image and text sample $(i, j)$ from dataset $D$, two transformations $t_c$ and $t_s$ are drawn from a distribution of image augmentation $T$ (cf.~\ref{sec:visual_info}), to produce two distinct views $x_c = t_c(i)$ and $x_s = t_s(i)$ of the image $i$. These views serve as inputs to the image content backbone $f_{\theta_c}$ and the image target encoder $f_{\theta_t}$ in the style encoder block, respectively.
The views $x_c$ and $x_s$ are first encoded by $f_{\theta_c}$ and $f_{\theta_t}$ into their representations $q_c = f_{\theta_c}(x_c)$ and $q_s = f_{\theta_t}(x_s)$, which are then linearly combined to get the representation $q_i$. The text $j$ is encoded by a text backbone, $g_{\phi}$ into their representation $q_j = g_{\phi}(j)$. Then, these representations, $q_j$ and $q_i$ are mapped by the custom cross attention modules onto the embeddings $z_i$ and $z_j$. The Sigmoid Contrastive Loss~\cite{zhai2023sigmoid} is computed at the embedding level on $z_i$ and $z_j$. 

As defined in Eq.~\ref{eq:sigcl}, we adopt a supervised Contrastive Loss, more specifically, Sigmoid Contrastive loss (SigCL) proposed by~\cite{zhai2023sigmoid} with content rating label, where we consider image-text pairs with the same rating as positive pairs. This enables it to distinguish between data points not just based on data similarity but also according to their categories. Additionally, the SigCL benefits over Unified Contrastive Loss (UniCL)~\cite{yang2022unified} in a multi-modal setting because, when $N$ image-text pairs from the same content rating category are presented in a batch, UniCL is bounded and the maximum softmax value per pair is limited to $1/N$.  
Meanwhile, SigCL varies between 0 and 1 for each positive pair. We defined the SigCL between $z_i$ and $z_j$ embeddings along the batch $B$ as, 

\begin{multline}
\label{eq:sigcl}
    \mathcal{L}_{SigCL} =-\frac{1}{|P|} \sum_{i,j\in P} log \frac{1}{1+ e^{(-\tau z_i.z_j + b)}}\\
    -\frac{1}{|B|} \sum_{i\in|B|}\sum_{j\in|B|\backslash \{P\}} log \frac{1}{1+ e^{(\tau z_i.z_j - b)}}
\end{multline}

where $y_i$ and $y_j$ are the labels for a given image and text pair and $P = \{k|k\in B, y_i=y_j\}$, which represents the image text pairs coming from the same content rating. The $b$ in Eq.~\ref{eq:sigcl} alleviates the heavy imbalance coming from the many negatives. We also employ Euclidean distance loss, $L_{mse}$ to learn low level information such as texture and colour in image data. Specifically, we take the augmented image $x_s$ and generate a masked version of it. The two views are encoded by the $target$ and $online$ networks described in Sec.~\ref{sec:style_enc} into representations $y_s$ and $y'_s$, and optimise them using  $L_{mse}$. The entire network is optimized by minimising the following loss function:%\vspace{-0.3cm}
\begin{equation}
\label{eq:finallos}
    \mathcal{L} = \mathcal{L}_{SigCL}+\lambda \mathcal{L}_{mse}
\end{equation}
where $\lambda$ is a positive constant trading off the importance of the first and second terms in the loss L.

