\section{Results}
\label{sec:results}

In this section, we first present the performance results of our model compared against several state-of-the-art baseline architectures representing image embeddings, text embeddings and image-text multi-modal embeddings.
Next, in the ablation study, we demonstrate the effect of different components of our approach by removing each component separately. Further, we evaluate the effect of using symmetric cross entropy loss (SCE)~\cite{clip}, which is used for CLIP pre-training, uniCL loss~\cite{yang2022unified} and SigCL loss~\cite{zhai2023sigmoid}. 


\subsection{Performance Comparison with Baselines}



\begin{table}
    \centering
    \caption{ Performance Evaluation. Linear classification on top
of the frozen image and text representation and supervised baselines}
    \label{tab: performance comparison}
    % \vspace{-0.4cm}
    \begin{tabular}{llccccc}
    \hline
    &            & $P_m$ & $R_m$ & $P_w$ & $R_w$ & Acc \\ 
    
    \multicolumn{7}{l}{\emph{1) Image embeddings only}} \\ 
    \hline
    &   ResNet50    & 26.48 & \textbf{45.54} & 44.07 & 38.60 & 38.61\\
    &   ViT         & 28.33 & 35.25 & 45.18 & 46.06 & 46.07 \\
    &   BLIP        & 40.48 & 32.74 & 48.65 & 45.18 & 45.19 \\
    \cline{2-7}
    &   CLIP        & 56.88 & 42.57 & 51.24 & 50.02 & 50.03   \\
    &   CLIP-f.t.   & 56.64 & 42.39 & 51.01 & 49.80 & 49.79 \\
    &   Ours        & \textbf{61.18} & 45.14 & \textbf{53.10} & \textbf{51.14} & \textbf{51.15} \\
    \cline{2-7}
    % \vspace{-0.2cm}
    % &&&&&&\\
    \multicolumn{7}{l}{\emph{2) Text embeddings only}}\\
    \hline
    &   BERT        & 45.28 & \textbf{47.94}& 51.06 & 49.32 & 49.33 \\
    &   RoBERTa     & 52.35 & 46.86 & 49.82 & 49.83  & 49.84 \\
    &   BLIP        & 38.10 & 41.20 & 45.11 & 39.75 & 39.76 \\
    \cline{2-7}
    &   CLIP        & 51.08 & 42.62 & 47.44 & 47.65 & 47.46   \\
    &   CLIP-f.t.   & 49.25 & 41.63 & 46.69 & 47.08 & 50.18 \\
    &   Ours        & \textbf{60.58} & 43.31 & \textbf{52.54} & \textbf{50.45} & \textbf{50.46} \\
    \cline{2-7}
    % \vspace{-0.2cm}
    % &&&&&&\\
    
    \multicolumn{7}{l}{\emph{3) Image-Text embeddings only}}\\
    \hline
    &   ViT+RoBERTa & 33.33 & 33.87  & 48.45 & 50.78 & 50.78 \\
    &   BLIP        & 33.26 & 32.59 & 47.73 & 49.13 & 49.13 \\
    \cline{2-7}    
    &   CLIP        & 53.66 & 45.44 & 49.97 & 50.11 & 50.12   \\
    &   CLIP-f.t.   & 57.70 & 45.56 & 50.60 & 50.19 & 50.18 \\
    &   Ours        & \textbf{61.57} & \textbf{46.63} & \textbf{54.80} & \textbf{53.09} & \textbf{53.09} \\
    \hline
    
    \end{tabular} 
\end{table}

We evaluate the performance of our method against pre-trained and fine-tuned CLIP models in three settings as shown in Tab.~\ref{tab: performance comparison}; 1) using only CLIP image embeddings ($z_i$), 2) using only CLIP text embeddings ($z_j$), and 3) using both CLIP image and text embeddings ($z_i$ and $z_j$). For 1) and 2), we modify the experimental setup explained in Sec.~\ref{subsec: content rating prediction} such that either $z_i$ or $z_j$ would only propagate via a single MLP network before obtaining the softmax scores. In 1) and 3), we take the majority voting output considering all images that come with an app, but not in 2). This is because one app has only one text description.

\begin{figure}[h]   
    \centering
    \includegraphics[width=0.97\linewidth]
    {figures/fig_confusion_matrix.drawio.pdf}
    \caption{Confusion matrix comparing our method against baselines - using image-text embeddings.}
    \label{fig:confution_matrix}
\end{figure}


For the pre-trained CLIP model baselines, we initialise the original parameters for the model and freeze them before training the content rating classifier. For fine-tuning CLIP, we use our training dataset consisting of app icons, screenshots and the respective app descriptions to fine-tune the already pre-trained CLIP model for an additional 13 epochs using a batch size of 384. Next, we freeze the fine-tuned model parameters to train the content rating classifier. 


As detailed in Tab.~\ref{tab: performance comparison}, our model achieves an accuracy of $\sim$53.1\% in the image-text embedding classifier setting and outperforms the pre-trained CLIP by a relative percentage increase of $\sim$$5.9\%$ and the fine-tuned CLIP by a relative percentage increase of $\sim$$5.8\%$. We also report other metrics, such as precision and recall in macro ($m$) and weighted ($w$) settings for further comprehension. 
Despite a relative decrease in accuracy by 3.7\% for the image-only classifier and 5.0\% for the text-only classifier, our methods outperform CLIP and CLIP-ft across all evaluated metrics. We attribute this improved performance to the integration of cross-attention mechanisms and a style encoder, which enable our model to learn richer visual and textual features compared to models leveraging two separate encoders for each modality.

We further portray the confusion metrics in Fig.~\ref{fig:confution_matrix}, comparing our method against baselines of CLIP and CLIP-ft, where our predictions are less scattered off-diagonal in the upper and lower triangles. 
Based on these results, we can conclude that our method outperforms both CLIP and CLIP-ft among all three settings depicted in Tab.~\ref{tab: performance comparison} and the best results are obtained when both image and text embeddings are considered.


For completeness, we also consider standard image and text baselines that have been supervised fine-tuned on our training dataset. These models are trained end-to-end using app creatives (app icons/screenshots and app descriptions ) to predict content rating labels directly. 
Specifically, we present image-only classification results using ResNet50~\cite{he2016deep}, ViT~\cite{dosovitskiy2020image}, and BLIP~\cite{li2022blip} image encoders as the backbone of the classifiers. For text classification results, we use BERT, RoBERTa and BLIP text encoder-based classifiers. For image-text classification results, we use a concatenation of ViT+RoBERTa encoder and BLIP text and image concatenated encoder-based classifiers.


ResNet50 and ViT both perform poorly compared to our method on average by 20.2\%. Comparatively, BERT and RoBERTa only demonstrate an average accuracy drop of $\sim$6.6\%. This disparity in performance can be attributed to several factors. App descriptions sometimes provide explicit, detailed information about the content, usage, and target audience, directly reflecting the attributes relevant to content rating. 

ViT and RoBERTa concatenated classifier has an accuracy of 50.78\% which is still 4.35\% under performing than our method. Additionally, BLIP image or text or image and text encoder classifiers are also under performing than our method in average by 15.81\%. These results again establish that, despite training an end-to-end model to perform content rating classification, it is not effective compared to our method in producing more meaningful embeddings that subsequently produce better content rating classifications.


\subsection{Ablation Studies}
\label{Subsec: ablations}

\noindent{\textbf{Ablations with respect to loss functions:}
We alter our contrastive loss function in Eq.~\ref{eq:sigcl} in several ways to experiment with different loss functions in SSL, such as SCE loss, UniCL, and SigCL. We compare linear classification results trained on the combination of image and text representations. The results of Tab.~\ref{tab: loss_comparisson} show that our method trained on SigCL outperforms SCE and UniCL methods by 9.3\% and 3.0\%, respectively.} \\


\begin{table}
    \centering
    \caption{Performance with different losses}
    \vspace{-0.3cm}
    \label{tab: loss_comparisson}
    \begin{tabular}{llccc}
        \hline
        & Metric &SCE & UniCL &Ours\\
        \hline
        Macro & Precision & 56.36 & 60.29 & \textbf{61.57}\\
         & Recall &38.52& 43.45 & \textbf{46.63}\\
         & F1 Score &38.17 & 45.23 & \textbf{49.25}\\
         \hline
        Weighted & Precision &49.74 & 52.63 & \textbf{54.80}\\
         & Recall &48.59 & 51.51 &\textbf{53.09}\\
         & F1 Score & 46.77& 50.50 & \textbf{52.31}\\
        \cline{2-5}
          & Accuracy & 48.59 & 51.52 &\textbf{53.09}\\
        \hline      
    \end{tabular}   
    \vspace{-0.4cm}
\end{table}

\noindent{\textbf{Ablation of model components:} To observe the effect of the style encoder, we evaluate our model with only the content-encoder as presented in Tab.~\ref{tab: deactivation_test}. The macro average precision observes a gain of 1.67\% without the style encoder (i.e., less likely to make false predictions but recalls less: -6.58\%), yet, all the other metrics indicate better performance with our method (macro average F1 score: +7.43\%, accuracy: +4.65\%). Next, we augmented our methodology without cross-attention and replaced it with a self-attention block. Our method achieves better performance in all the metrics compared to this setting. Removing cross-attention disproportionately affects classes, as indicated by a larger drop in the weighted F1 score compared to the macro F1 score. This suggests that cross-attention is crucial for maintaining performance in majority classes like G, PG, and M, helping the model effectively distinguish between these content ratings. 
Overall, ablation study results show that style backbone added with text-image cross-attention contributes to the increased performance.}
         
\begin{table}
    \centering
    \caption{Effect of incorporating style encoder and cross attention}
    \label{tab: deactivation_test}
    \vspace{-0.3cm}
    \begin{tabular}{llccc }
    \hline

         & Metric &w/o style & w/o cross &Ours\\
         & &encoder& attention &\\
        \hline
        Macro & Precision & \textbf{62.60} & 58.81 & 61.57\\
         & Recall &43.56 & 46.43& \textbf{46.63}\\
         & F1 Score &45.84 & 49.12& \textbf{49.25}\\
         \hline
        Weighted & Precision &\textbf{54.91}& 50.99 & 54.80\\
         & Recall &50.72 &50.76 &\textbf{53.09}\\
         & F1 Score & 49.86&49.80 &\textbf{52.31}\\
        \cline{2-5}
          & Accuracy & 50.73&50.76  &\textbf{53.09}\\
        \hline      
    \end{tabular}  
    \vspace{-0.4cm}
\end{table}
