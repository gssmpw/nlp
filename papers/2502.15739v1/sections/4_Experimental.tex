\section{Experimental Setup}

\subsection{Dataset}
\label{Sec:dataset}
Our dataset is a snapshot of the Google Play Store, which includes metadata and creatives for 1.3 million apps. This dataset was collected using a Python crawler from January 2023 to November 2023. We deployed an extremely slow crawling rate during this data collection.
For this work, we filtered out and used only the games category, which is more popular among children and as such, the correct content rating matters significantly. During our crawl, the crawler's geo-location was set as Australia (AU) to be consistent in obtaining content rating values of G, PG, M, MA15+, or R18+.

We sorted the selected gaming apps by rank, i.e., sorting in the descending order of number of downloads, star rating count and final star rating number following similar previous work~\cite{seneviratne2015early,rajasegaran2019multi}, and the first 20k games were selected as training and validation sets (80:20 random split) while the next 10k games were selected as the test set. We specifically did not mix the former due to the assumption that more popular apps are well monitored within the community and well maintained by the developers such that the metadata and content ratings information are less noisy than in the rest of the order. Due to the scarcity of games in categories of MA15+ and R18+ within the top 30k, we expanded our search space for them and appended them into train, validation and test sets. For analysis purposes, we created another dataset by including apps with the `Teacher Approved' tag~\cite{teacherAApps}. We report the distribution of apps by content rating across various datasets in Tab.~\ref{tab:dataset_distribution}.


\begin{table}[ht]
    \centering
    \caption{Dataset split and class distribution}
    % \vspace{-0.3cm}
    \label{tab:dataset_distribution}
    \begin{tabular}{l cccc}
    \hline
        & Train & Valid. & Test & Teacher Approved \\
        \hline
        G  & 4,544& 1,139&2,650& 2140\\
        PG & 4,540& 1,130&2,649& 30\\
        M  & 4,530& 1,120&2,648& 2\\
        MA15+ & 2,131& 547&1,796&- \\
        R18+ & 255& 62&255&- \\
        \hline
    \end{tabular}
    % \vspace{-0.3cm}
\end{table}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/malpractice_examples_v6.pdf}
    \caption{Examples belonging to 1) potential malpractices, and 2) potential disguises. For each app, the image on the left represents the app icon, and on the right is a screenshot. Red * represents app that removed from the Play store after the initial data crawl in 2023.}
    \label{fig:malpractices}
\end{figure*}

\subsection{Implementation Details}
\label{sec:ImplementationDetails}
We use the ViT-B/16 CLIP image encoder as our image backbone for both style and content branches and a frozen RoBERTa backbone in the text encoder branch. The model is pre-trained on eight NVIDIA V100 GPUs for 30 epochs with a minibatch size of 64. We use the Adam optimizer~\cite{kingma2014adam} with learning rate of $10^{-5}$, momentum of $0.9$, and weight decay of $0.02$. The learning rate follows a cosine decay schedule~\cite{loshchilov2016sgdr}, starting from 0 with $10$ warmup epochs and with a final value of $10^{-8}$. We perform a grid search to select the loss coefficients $\lambda$ in Eq.~\ref{eq:finallos} and set it to 5.

\subsection{Content Rating Predictions}
\label{subsec: content rating prediction}

We train a linear classification head to perform content rating predictions based on the previous outputs of image and text embeddings $z_i$ and $z_j$. That is, we propagate them via two separate MLP networks, concatenate the outputs, and then propagate again via another MLP network, followed by softmax classification to identify the prediction class. This stage is shown in Fig.~\ref{fig:model_architecture}(c). As we possess multiple images (app icon and screenshots) for a given app, we take the majority voting for the classification outputs for all of such image and text pairs. Note that we disable back-propagation in all the steps starting from $(i,j)$ up to obtaining $z_i,z_j$ during this classification stage. 

\subsubsection{Performance Metrics}

Due to the persistent class imbalance of our datasets, we report our model's performance in macro and weighted versions of \emph{precision}, \emph{recall} and \emph{F1 scores}. The overall \emph{accuracy} is calculated based on the elements of the principle diagonal of the confusion matrix. Predictions mapping to upper triangular or lower triangular portions of the confusion matrix are undesirable for app users and we later evaluate them in Sec.~\ref{subsec: potential malpractices} as \emph{potential malpractices} and in Sec.~\ref{subsec: potential disguises} as \emph{potential disguises}, respectively. 




