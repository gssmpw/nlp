\section{Introduction}
\label{Sec:introduction_2}

In recent years, our reliance on mobile services has surged, whether for entertainment, communication, shopping, travel, or finance. According to recent reports, 60.42\% of the world's population owns a smart device~\cite{ashtuener2024}. One implication of this trend is that children of all ages are using smart devices and apps more than ever.


In 2013, a survey revealed that over 75\% of children under 8 years old were using mobile phones~\cite{liu2016identifying} and in 2019, 69\% of teens owned a smartphone~\cite{2019smartphone}. This widespread dependence among young children poses a significant social challenge whether they are being provided with age-appropriate content, usually defined by content ratings. For example, the Google Play Store ratings in the US and Canada are maintained by the Entertainment Software Rating Board (ESRB), with rating categories \emph{Everyone, Everyone 10+, Teen, Mature} and \emph{Adult only}, whereas in Australia, games adhere to local content maturity ratings issued by the Australian Classification Board (ACB), including G, PG, M, MA15+, and R18+. 
Similarly, rating systems are employed in different regions, such as Pan-European Game Information (PEGI) in Europe. These guidelines use six different content descriptors: themes, violence, sex, language, drug use (substances) and nudity and asses the impact depends on the frequency, intensity.

To assist users, especially parents, in finding suitable apps, the Google Play Store enforces strict developer policies. As a result, each app page displays critical app information, such as download counts, requested permissions, content rating, developer names, and user reviews, allowing users/ parents to understand the app before downloading.
% Google Play has several developer policies related to app content~\cite{google_developer_policy}.
Additionally,  all apps undergo automated inspection and vetting procedures before being published. Furthermore, in 2020, Play Store introduced ``Teacher Approved'' apps, which are published after being rated by teachers and specialists. They take into consideration factors such as design, age appropriateness, and appeal when rating an app~\cite{teacherAApps}. Despite such initiatives, Luo et al.~\cite{luo2020automatic} identified that 40\% of apps contained inappropriate content among 70 children's apps in 2020. Moreover, children reportedly spent 27\% more screen time for online video platforms, 120\% more for TikTok in 2023 compared to 2022~\cite{anualdatareport2023} despite those apps being rated for ages 13 and up. Furthermore, by analysing 20,000 apps in Google Play Store, Sun et al.~\cite{sun2023not} claimed that 19.25\% of apps have inconsistent content ratings across different protection authorities around the world, thus making them un-generalisable.

One reason why such content rating violations and discrepancies are possible among apps, especially in the Google Play Store, is its loosely regulated nature. The Google Play Store relies on an app developer's completed questionnaire and self-reported information to automatically determine the content rating as disclosed by developers~\cite{content_ratin_q_geographical}. In a profit-driven app ecosystem with over 3.6 million apps in the Google Play Store~\cite{avada}, where app engagements matter significantly, especially those from young children, it can not always be expected that all developers will play fair. Furthermore, Google employs different content rating systems based on geographical locations~\cite{content_ratin_q_geographical}, and there are no clear boundaries among the categories, to the extent that an average smartphone user can easily get overwhelmed. On the other hand, the app vetting process by the Apple App Store is manual~\cite{app_review_apple}, and as a result, it is most likely to have correct content ratings for apps; however, the downside is that getting apps published in App Store takes time.

As such, there is a stronger need to develop methods to automatically assign correct content ratings for given apps. This requirement is further exacerbated by the fact that regulatory bodies, such as a country's e-safety commission, do not have the necessary means to identify apps that violate the country's content rating guidelines unless end users complain about specific apps. Currently, such studies by regulators are mostly carried out manually or semi-manually. For example, in 2012 the FTC reviewed 200 apps, mostly through manual processes~\cite{federal2012mobile}.

To this end, in this paper, we propose a vision-language approach based on self-supervised learning and supervised contrastive learning that allows us to identify content rating violations in app markets. Our approach is based on the intuition that multi-modality is important in this problem (i.e., considering both app descriptions and images such as icons and screenshots, commonly known as app creatives). Within creatives, it is crucial to consider both the content and style of these images. The style information is effective in identifying the target demographic of an app, as apps designed for children tend to have cartoon effects and tactile textures like glitter. More specifically, we make the following contributions:

%Content rating violations can be intentional or unintentional; for example, a developer can intentionally assign an incorrect content rating to an app to increase the app's engagement and revenue. Equally, a developer can unknowingly assign an incorrect rating, making an unsuitable app available for underage users. Either way, automatically verifying whether an app has the correct content rating is essential. 

\begin{itemize}
    \item We propose a vision-language approach using trained content, style, and text encoders, along with a cross-attention module, to predict mobile app content ratings from descriptions and creatives.
    \item  Using a real-world dataset, we show that our approach achieves 5.9\% and 5.8\% relative improvements in accuracy over state-of-the-art CLIP and CLIP fine-tuned models.
    \item Upon evaluating the test dataset, our model identified 71 apps ($\sim$17\% of the total verified) with potential content rating malpractices in the Google Play Store and 32 apps subtly attracting an unsuitable audience. This includes nine `Teacher Approved' apps, which Google Play claims to verify manually.

    \item We conduct extensive experiments on nearly 16,000 apps to validate the effectiveness of our model. The results show that 45.7\% of identified malpractices and 39.1\% of identified disguises were removed by the Play Store after nine months of our initial crawl.
\end{itemize}

