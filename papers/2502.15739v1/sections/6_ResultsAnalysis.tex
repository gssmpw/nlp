\section{Result Analysis}

In this section, we analyse the results and predictions of our method from the perspective of the mobile app ecosystem. First, we observe how deviated the text and visual data are compared to natural language and text. Next, we present interesting findings among the lower and upper triangular parts of the confusion matrix (i.e., apps having a lower or higher content rating than our predictions). 


\subsection{Image-text Cross Attention}
\begin{figure}[ht]  
    \centering
    \includegraphics[width=0.97\linewidth]{figures/vis_ca_module.pdf}
    \caption{Visualisation of image patches attending to text tokens in the custom cross-attention layer.}
    \label{fig:ca_visualized}
\end{figure}

In Sec.~\ref{ssec:cross attention}, we discussed how our image-text cross attention (CA) design allows every patch of the content image to attend over all tokens in the text input sequence. Also, for each image patch, there are 12 attention heads running in parallel. Therefore, to qualitatively measure how the CA happens, we select attention-heads of the first layer as lower layers are often associated with broader attention~\cite{vig_2019_multiscale}. Then, for each image patch, we select the tokens with the highest numerical attention values after excluding some stop-word and punctuation mark-related tokens. In Fig.~\ref{fig:ca_visualized}, we visualise the highly attended words in a given input image portion for some example images. An image portion is a collection of consecutive patches which we select as a region of interest; for example, the patches outlining the gun in Fig.~\ref{fig:ca_visualized} (d) or the heart in Fig.~\ref{fig:ca_visualized} (a). The results show that mobile app ecosystem-related tokens such as `simulator', `game', `upgrade', and `developers' are now attended by the image patches, even though those words cannot be identified by observing the image in a general context. Furthermore, the tokens representing target audiences such as `kids', `children' and `girls' are now attended. This further demonstrates how our model has been able to reduce the gap between visual data and textual data in the mobile app domain.

\subsection{Predictions in the Wild}

When our classifier is applied to the test set containing 10,000 gaming apps, two interesting cases emerge, i.e., apps with a higher or lower labelled rating than our model's predictions. These two scenarios lead to potential  \emph{``malpractices"} and \emph{``disguises"} that are important from a content safety point of view. 

\subsubsection{Potential malpractices}
\label{subsec: potential malpractices}

When our method predicts a class label that is higher than the developer-defined label, we characterise such examples as possible malpractices (i.e., having a lower content rating than what the app is supposed to have). Occurrences belonging to this category could be identified along the horizontal axis of the confusion matrix, on the right-hand side to the diagonal. This is more observed in the G category as any higher prediction, such as PG, M, MA15+, and R18+, raises a concern. As we go higher in prediction classes, the possibilities for malpractices decrease, and therefore, the R18+ category is not susceptible to malpractices. 

We manually evaluated 350 apps with predicted labels that were two classes or more higher than the true label (e.g., prediction [M, MA15+ or R18+] when the true label is [G]) and identified 62 (17.7\%) of them as potential malpractices (i.e., possible content rating violations). \textit{Also, we highlight that at the time of writing, 14 of them were removed from Play Store, and 20 of them were increased to a higher rating class compared to the time we crawled the dataset. While we can't be exactly sure why these 14 apps were removed from  Google Play Store, previous work has reported that Google take down apps violating their content policies~\cite{seneviratne2015early}.} 

We highlight ten exemplary instances that are potentially linked to malpractices in Fig.~\ref{fig:malpractices} (1). Examples (a) and (b) both represent gambling-related games rated [G] that would at least require a rating of [PG]. Example (c) depicts shooting and gun usage, again not suitable for a general audience. (e) and (j) contain images more suited for an [MA15+] audience, and (d) and (f) entertain visual and textual cues strongly related to illegal substances that require a rating higher than [PG]. (g) and (h) contain images with horror themes, blood and intense cartoon or fantasy violence which are more suited for [MA15+] audience. These examples suggest that our model can flag potential content malpractices in the app ecosystem.


\subsubsection{Potential disguises}
\label{subsec: potential disguises}

Apps belonging to higher content ratings are likely not to conduct malpractices, but alarmingly, they could be disguised as a lower-rated app and, as a consequence, could attract an unsuitable audience to the app.  
As an example, an R18+ app consisting of cartoonish images and not-so-alarming textual data could attract underage audiences due to their natural tendency for curiosity and their interest in cartoons. From the developer's perspective, they comply with the content policies and applicable laws. However, in such cases, we argue that at least the textual description must contain information such that someone (e.g., a parent or a guardian) who misses the content rating label should be able to figure out the app's purpose and functionality independently. We define this category as possible disguises. R18+ is more vulnerable to disguises that can be identified horizontally left to the diagonal of the confusion matrix. Being the lowest content rating, category [G] is not susceptible to disguises. We checked 131 samples that were predicted [G, PG, M] when the true class was [R18+], and 203 samples predicted [G] when the true class was [MA15+] and observed $\sim$9.3\% of them to be possible disguises.

We demonstrate some of such examples in Fig.~\ref{fig:malpractices}(2). App represented by (a) is an app rated for [PG] and our method predicts even lower as [G]. Observing the images, it is evident their content is sexual in nature despite the cartoonish theme. In our test dataset, the average rating for an app with such sexualised imagery is [MA15+]. 
Note that in this category, our model is likely to predict a lower rating as the images or text is not suggestive of requiring high ratings. Due to such examples being rare, our model does not know how to predict them correctly, yet we still can automatically identify them based on our off-to-left-diagonal results, as explained before. Apps (b, e, g) and (i), though rated for [MA15+] and higher based on a storyline related to a `fashion and makeup', is likely to attract a younger audience due to the visual appearance similar to the majority of [PG] rated \emph{Dress up} games. A similar interpretation can be given to [M] rated examples (c, h), which are too cartoonish yet contain images related to mature audiences, such as pregnancy. Furthermore, (f) and (j) are rated as [MA15+], which contains appealing games for young audience. Despite measures such as \emph{Not designed for children} tagging is available in Google Play~\cite{notdisgisechil.} to safeguard users, it doesn't appear to be used by many developers. 

\subsubsection{Unverifiable apps} 
\label{subsec: unverifiable_apps}

\begin{figure}[ht]  
    \centering
    \includegraphics[width=0.97\linewidth]{figures/unverifiables.pdf}
    \caption{Examples of unverifiable apps with developer defined content rating descriptors.}
    \label{fig:unverifiable_apps}
\end{figure}
We further highlight discrepancies in content rating declarations, as illustrated in Fig.~\ref{fig:unverifiable_apps}. These noisy instances represent cases where we could not manually verify the developer-assigned ratings based on the appâ€™s available metadata, visuals, or descriptions. The app in (a) is rated as [M] with descriptors indicating simulated gambling, online interactivity, and in-game purchases. However, compared to similar games in the dataset, it does not display any visual cues or textual descriptions to justify such a rating. Therefore, the absence of gambling graphics or explicit mention of gambling mechanics justifies the predicted rating of [G].
Similarly, app (b) and (c) fail to reflect sensitive content such as sexualised imagery and strong violence in app creatives. Hence, in all these cases, our model predicted a lower rating than the original rating. While it may not be explicitly illegal to omit sensitive descriptors from app screenshots or descriptions, missing descriptors in visuals hinder the user's ability to make informed decisions, leading to unwanted downloads or unexpected experiences and children and vulnerable users may unintentionally be exposed to harmful or age-inappropriate content.


\subsubsection{Teacher Approved (TA) Apps}

\begin{figure}[ht]  
    \centering
    \includegraphics[width=0.97\linewidth]{figures/teacher_approved_v2.pdf}
    \caption{Examples of teacher-approved apps with incorrect content ratings - A section of the app description is quoted and * indicates this app is no longer available.}
    \label{fig:teacher_apps}
\end{figure}


Google Play has deployed the `teacher approved' apps based on consultations with experts to determine the suitability for kids, and especially the age appropriateness~\cite{teacherAApps}. Hence, these apps are likely to be more content appropriate as per the rating labelling. However, after analysing 2,172 TA apps, we found that our model predictions deviate from the declared content rating classifications, with 10.4\% of them being flagged for potential malpractices. Among them, 92\% of the instances were flagged as requiring [PG] despite being declared as [G]. Further evaluating their continuity, within a time span of nine months, we observed that 34.5\% of apps that were classified as potential malpractices have been removed from the Play Store. On the contrary, only 27.4\% of correctly predicted apps were removed, which is lower compared to the deletion rate of apps identified with malpractices.
We further discuss why app removal rate can be a proxy measure of content policy violations in the next subsection. 


As depicted in Fig.~\ref{fig:teacher_apps} we manually verified a portion of apps flagged before as malpractices based on the available metadata and identified that nine apps are likely to be not suitable for children despite being tagged as TA. 
The presence of violence (Fig.~\ref{fig:teacher_apps}a), horror themes (Fig.~\ref{fig:teacher_apps}b) or online multiplayer interactions (Fig.~\ref{fig:teacher_apps}d) were the main reasons we identified behind these content rating discrepancies. 
The example in Fig.~\ref{fig:teacher_apps} c highlights a contradiction in the app's description, which mandates parental presence, despite the app being labeled as [G] in the Play Store.
\textit{Overall, the presence of these practices among `teacher approved' apps is alarming. It shows that even manually verified apps are not immune to content rating malpractices, and further rigour is required in app vetting.}


\begin{figure}[ht]    
    \centering
    \includegraphics[width=0.97\linewidth]{figures/app_deletion_rate_.pdf}
    \caption{App deletion rates w.r.t number of downloads.}
    \label{fig:app_deletion}
   % \vspace{-0.3cm}
\end{figure}


\subsubsection{App Deletion Rate}

An app can be discontinued in Google Play for two reasons: the developer could discontinue the app~\cite{denipitiyage2024detecting}, or Google could remove the app for violating its policies~\cite{2023gplypolicyviolation}. As a result, an app being removed from Google Play can be used as an indication of a possible violation of Google Play policies.

To this end, we used 15,985 apps that are gathered from the test set (10,000 apps: c.f. Sec.~\ref{Sec:dataset}) added with 5,985 apps with lower downloads (download count $<$ 100,000 - to account for a better distribution as our test set consists of top apps only). Next, we attempted to re-crawl these apps to check whether they were still there in Google Play. Overall, we found 45.7\% of apps identified as having malpractices, 39.1\% of apps that predicted to be disguises were removed within the time span of nine months. In comparison only  29.1\% of apps that correctly predicted were removed.


In Fig.~\ref{fig:app_deletion}, we show the percentage of apps that we found as deleted according to the download numbers and the predictions of our classifier. At all download ranges apart from `$<$ 100', we notice that apps we classified as potential malpractices have a higher deletion rate than apps we classified as correct. Similar values of `$<$~100' category can be explained by less attention and consequently fewer complaints received on those apps for Google to action.

On the other hand, apps classified as disguises are likely not to be removed as malpractices, as they are unlikely to be noticed or complained about by an average audience. Notably, apps flagged as disguises with more than 1M downloads are far less likely to be removed as the number of apps with a higher content rating label (e.g., MA15+, R18+) are not frequent among the apps. 


