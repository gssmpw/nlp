\section{Experiments}

\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{figs/expsetting.pdf}
  \caption{Comparison of three different experimental settings. In addition to common evaluation metrics on the test set, we also assess the reranker's ability to identify relevant documents from the training set.}
  \label{fig:expsetting}
\end{figure*}

\begin{table*}[!ht]
\centering
\small
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{ l || c c|| c c | c || c c c }
    \toprule
    & \multicolumn{2}{c||}{\textbf{Mining Setting}} & \multicolumn{3}{c||}{\textbf{Reranker Setting}} & \multicolumn{3}{c}{\textbf{Generator Setting}} \\  
    \cmidrule(r){2-3} \cmidrule(r){4-6} \cmidrule(r){7-9}
    \bf Training Methods & \textbf{Recall@5} & \textbf{NDCG@5} & \textbf{Recall@5} & \textbf{NDCG@5} & \textbf{MRR} & \textbf{EM} & \textbf{SubEM} & \textbf{F1} \\
    \addlinespace[0.2em]
    \hline
    \hline
    \addlinespace[0.4em]
    \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{Hotpotqa}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{RankT5}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & \underline{78.0} & \underline{80.5} & \underline{78.7} & \underline{80.6} & \bf 95.9 & \underline{60.8} & \underline{66.1} & \underline{75.8} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & 76.8 & 79.5 & 78.1 & 80.0 & \underline{95.7} & \underline{60.8} & 66.0 & \underline{75.8} \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & 71.7 & 74.7 & 72.5 & 74.9 & 93.0 & 60.0 & 65.1 & 75.0 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & 71.3 & 72.1 & 71.3 & 71.9 & 88.4 & 57.0 & 61.9 & 71.5 \\
    \rowcolor{gray!10} - G-Rerank & \bf 83.3 & \bf 84.7 & \bf 84.4 & \bf 84.9 & \bf 95.9 & \bf 61.1 & \bf 66.5 & \bf 76.3 \\
    % \midrule
    \addlinespace[0.4em]
    % \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{Hotpotqa}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{BGE-Base}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & \underline{81.1} & \underline{83.2} &\bf{81.8} & \bf 83.1 & \bf{96.3} & \underline{60.8} & \underline{66.0} & \bf 75.8 \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & 79.1 & 81.6 & 81.2 & 82.6 & \underline{96.2} & \bf 60.9 & \bf 66.1 & \underline{75.7} \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & 79.1 & 81.1 & 80.4 & 81.7 & 95.3 & 60.3 & 65.4 & 75.2 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & 77.7 & 79.5 & 78.1 & 79.5 & 93.7 & 59.8 & 65.0 & 74.7 \\
    \rowcolor{gray!10} - G-Rerank & \bf 81.6 & \bf 83.3 & \underline{81.1} & \underline{82.9} & 95.8 & \bf 60.9 & \bf 66.1 & \underline{75.7} \\
    \addlinespace[0.4em]
    \hline
    \hline
    \addlinespace[0.4em]
    \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{Musique}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{RankT5}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & 56.6 & \underline{65.8} & \underline{55.0} & \underline{58.1} & \bf 82.0 & \underline{39.6} & 42.1 & \underline{48.6} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & \underline{57.3} & 65.3 & 52.7 & 55.0 & 79.5 & \underline{39.6} & \underline{42.2} & 48.3 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & 56.3 & 64.9 & 53.3 & 55.6 & 79.6 & 39.2 & 41.7 & 48.0 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & 53.8 & 55.3 & 47.7 & 47.3 & 66.4 & 35.4 & 37.9 & 44.1 \\
    \rowcolor{gray!10} - G-Rerank & \bf 60.7 & \bf 67.8 & \bf 57.9 & \bf 59.7 & \underline{81.5} & \bf 40.0 & \bf 42.4 & \bf 49.1 \\
    % \midrule
    \addlinespace[0.4em]
    % \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{Musique}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{BGE-Base}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & 56.6 & 65.7 & 53.6 & 57.1 & \underline{81.5} & \underline{39.7} & \underline{42.4} & \underline{48.8} \\ 
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & \underline{60.3} & \underline{66.1} & \bf 58.2 & \underline{59.6} & 80.5 & 39.4 & 42.3 & 48.6 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & 58.7 & 65.6 & 57.2 & 59.3 & \bf 81.8 & \underline{39.7} & 42.2 & \underline{48.8} \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & 57.9 & 64.5 & 46.0 & 45.3 & 64.7 & 34.8 & 37.3 & 43.4 \\
    \rowcolor{gray!10} - G-Rerank & \bf 60.9 & \bf 66.6 & \underline{57.6} & \bf 59.7 & \underline{81.5} & \bf 39.9 & \bf 42.7 & \bf 49.1 \\
    \addlinespace[0.4em]
    \hline
    \hline
    \addlinespace[0.4em]
    \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{2wikihop}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{RankT5}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & 58.6 & 63.4 & 62.9 & 68.7 & 88.7 & 67.2 & 69.9 & 72.5 \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & 72.6 & 76.5 & 77.2 & 81.9 & 94.1 & 70.2 & 73.0 & 75.5 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & \underline{80.4} & \bf 87.1 & \underline{79.2} & \underline{85.4} & \underline{97.5} & \underline{71.6} & \underline{74.4} & \underline{76.9} \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & 74.7 & 79.2 & 72.4 & 76.6 & 90.1 & 64.1 & 66.5 & 69.6 \\
    \rowcolor{gray!10} - G-Rerank & \bf 80.8 & \underline{86.9} & \bf 82.7 & \bf 88.4 & \bf 97.8 & \bf 71.8 & \bf 74.7 & \bf 77.2 \\
    % \midrule
    \addlinespace[0.4em]
    % \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{2wikihop}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{BGE-Base}} & \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24} & 61.8 & 67.3 & 71.0 & 77.1 & 93.8 & 68.9 & 71.8 & 74.3 \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22} & 74.0 & 76.8 & 76.6 & 82.2 & 94.5 & 69.1 & 71.9 & 74.4 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23} & 77.3 & 85.0 & 76.0 & 83.3 & \bf 98.5 & \bf 71.2 & \bf 73.9 & \bf 76.3 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21} & \bf 81.4 & \bf 87.7 & \underline{80.5} & \underline{86.4} & 97.1 & 70.7 & 73.5 & 76.1 \\
    \rowcolor{gray!10} - G-Rerank & \underline{79.6} & \underline{86.4} & \bf 81.4 & \bf 86.5 & \underline{97.5} & \underline{70.9} & \underline{73.7} & \underline{76.2} \\
    \bottomrule
\end{tabular}
}
\caption{Experiments on 2WikiHop, Musique, and HotpotQA using FiD-Large as reader. We consider the settings illustrated in \autoref{fig:expsetting}. The best performance is highlighted in bold, while the second-best performance is underlined.}
\label{tbl:multi dataset experiment}
\end{table*}



In \autoref{sec: main exp}, we first validate the effectiveness of our approach under three different experimental settings. Then, in \autoref{sec: indirect exp}, we focus on whether the reranker can learn to prioritize \textit{indirect evidence} in multi-hop question answering. 
% These are documents that belong to the evidence chain but do not directly contain the final correct answer. Our results demonstrate that our method is significantly more effective at capturing the importance of such indirect evidence. 
Next, in \autoref{sec: gumbel exp}, we conduct an ablation study on the Gumbel trick and demonstrate its necessity. 
% Without the Gumbel trick, the model tends to learn a ``uniform'' soft attention mask. 
Finally, in \autoref{sec: learnable weight exp}, we remove the reranker and assign each document a learnable weight to further verify the efficacy of our training objective in capturing the relative importance of documents.



\subsection{Experimental Setup}

\paragraph{Language Models}
We experiment with two different language models as the generation module in our RAG system: Fusion-in-Decoder (FiD)~\cite{DBLP:conf/eacl/IzacardG21} and CEPE-Llama2-7B~\cite{DBLP:conf/acl/YenG024}.  FiD~\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20}, built upon the T5 architecture, is specifically designed for knowledge-intensive QA and is fine-tuned for each task. CEPE-Llama2-7B segments long documents with a lightweight encoder and employs cross-attention for effective context utilization, operating in a zero-shot manner.

\paragraph{Reranker}
We experiment with RankT5-Base~\cite{DBLP:conf/sigir/Zhuang0J0MLNWB23} and BGE-Base-Reranker~\cite{xiao2023bge} as the reranking module in the RAG system. RankT5-Base is fine-tuned in an encoder-decoder setup to perform reranking, while BGE-Base-Reranker is an encoder-only model based on BERT.


\paragraph{Datasets}
We evaluate on five QA datasets: multi-hop (2WikiHop~\cite{DBLP:conf/coling/HoNSA20}, HotpotQA~\cite{DBLP:conf/emnlp/Yang0ZBCSM18}, Musique~\cite{DBLP:journals/tacl/TrivediBKS22}) and single-hop (NQ~\cite{DBLP:journals/tacl/KwiatkowskiPRCP19}, TQA~\cite{DBLP:conf/acl/KimKK19}). Details are in \autoref{sec: appendix_datasets}. For NQ and TQA, we retrieve 20 candidate documents per query using DPR~\cite{DBLP:conf/emnlp/KarpukhinOMLWEC20}. For multi-hop datasets, we apply the distraction setting to ensure ground-truth documents are included, adding 10 random candidates in Musique to maintain 20 candidates per query.

\paragraph{Baselines}
We compare against four LLM-supervised reranker training methods: EMDR~\cite{DBLP:conf/nips/SachanRHDY21, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/iclr/Lin0CSL00KSLZY24}, PDist~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/GlassRCNCG22}, LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}, and ADist~\cite{DBLP:conf/iclr/IzacardG21}, which employ different LLM-supervised losses. Details about these baselines can be found in \autoref{sec: appendix_baseline}.
\begin{table*}[!th]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l | c c c | c c c | c c c | c c c}
    \toprule
    & \multicolumn{6}{c|}{\textbf{Reranker Setting}} & \multicolumn{6}{c}{\textbf{Mining Setting}} \\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13}
    & \multicolumn{3}{c|}{\textbf{FiD-Base}} & \multicolumn{3}{c|}{\textbf{FiD-Large}} & \multicolumn{3}{c|}{\textbf{FiD-Base}} & \multicolumn{3}{c}{\textbf{FiD-Large}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
    \textbf{Training Methods} & Recall & MRR & NDCG & Recall & MRR & NDCG & Recall & MRR & NDCG & Recall & MRR & NDCG \\
    \midrule
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24}  & \underline{63.0} & \underline{45.6} & \underline{45.2} & \underline{61.8} & \underline{45.2} & \underline{44.4} & \underline{60.3} & \underline{42.9} & \underline{42.3} & \underline{59.0} & \underline{42.7} & \underline{41.6} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22}  & 50.5 & 39.8 & 36.2 & 60.2 & 44.4 & 43.4 & 47.6 & 37.5 & 33.5 & 56.3 & 41.3 & 39.7 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}  & 53.1 & 40.7 & 38.1 & 52.5 & 40.2 & 37.3 & 49.6 & 38.2 & 34.9 & 49.8 & 37.6 & 34.5 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21}  & 55.2 & 43.4 & 40.8 & 56.3 & 44.5 & 41.9 & 52.8 & 41.5 & 38.5 & 54.2 & 42.3 & 39.5 \\
    \rowcolor{gray!10} - G-Rerank  & \bf 69.3 & \bf 48.2 & \bf 49.6 & \bf 72.2 & \bf 49.5 & \bf 51.5 & \bf 65.5 & \bf 45.0 & \bf 45.8 & \bf 68.4 & \bf 46.4 & \bf 47.8 \\
    \bottomrule
\end{tabular}
}
\caption{Results on HotpotQA using FiD as reader for identifying \textit{indirectly relevant documents}, which are part of the evidence chain but do not directly contain the answer. Details can be found in \autoref{sec: appendix_indirectly relevant documents setting}.}

\label{tbl:hotpotqa-indirect-combined}
\end{table*}

\begin{table}[!ht]
\centering
\small
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l | c c | c c}
    \toprule
    & \multicolumn{2}{c|}{\textbf{RankT5}} & \multicolumn{2}{c}{\textbf{BGE-Base}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    \textbf{Training Methods} & NQ & TQA & NQ & TQA \\
    \midrule
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24}  & 33.4 & \underline{62.4} & 33.7 & \underline{62.5} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22}  & 32.9 & 61.8 & \underline{33.9} & 61.7 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}  & \underline{33.7} & 62.1 & 33.5 & 62.2 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21}  & 33.1 & 61.6 & 33.2 & 62.0 \\
    \rowcolor{gray!10} - G-Rerank  & \bf 34.3 & \bf 62.8 & \bf 34.5 & \bf 63.1 \\
    \bottomrule
\end{tabular}
}
\caption{Experimental results on NQ and TQA datasets using CEPE-Llama2-7B as the reader. We employ SubEM as the evaluation metric.}
\label{tbl:cepe-nq-tqa-subem}
\end{table}

\subsection{Main Experiments}\label{sec: main exp}

\paragraph{Task Definition}
We consider the QA task where the model is trained on question-answer pairs along with retrieved documents, but at test time, it only receives the question and the retrieved documents. We define three evaluation settings, with their respective distinctions illustrated in \autoref{fig:expsetting}:

\begin{enumerate}
    \item \textbf{Mining Setting}: During training, given a \textit{question-answer pair}, can the reranker effectively identify relevant documents?
    \item \textbf{Reranker Setting}: At test time, given a \textit{question}, can the reranker effectively identify relevant documents?
    \item \textbf{Generator Setting}: At test time, given a \textit{question}, can the model generate correct answers?
\end{enumerate}

% \autoref{fig:expsetting} illustrates the three experimental settings. Since our primary focus in this work is on the training strategy for the reranker, we keep the parameters of the generation module fixed and train only the reranker in our comparative experiments.

\paragraph{Experimental Results}
\autoref{tbl:multi dataset experiment} presents the experimental results using FiD-Large as the generator model. Our method, G-Rerank, achieves the best or second-best performance across most datasets. In the Mining Setting, G-Rerank significantly improves the ability to identify relevant documents during training, given question-answer pairs. For instance, it achieves a 5.3\% improvement on the \texttt{HotpotQA} when using RankT5. In the Reranker Setting, G-Rerank demonstrates a notable improvement over other LLM-supervised loss-based training methods, with a 5.7\% Recall improvement on \texttt{HotpotQA} when using RankT5. Furthermore, in the Generator Setting, G-Rerank shows consistent performance gains in generation quality, as G-Rerank directly takes the  minimization of the final generation loss as the training objective.

\autoref{tbl:cepe-nq-tqa-subem} presents the SubEM results using CEPE-Llama2-7B as the generator model. We do not fine-tune CEPE-Llama2-7B on the downstream datasets; instead, we leverage its zero-shot capabilities. On both \texttt{NQ} and \texttt{TQA}, the G-Rerank training strategy leads to the best generation performance. Notably, these improvements are achieved solely by fine-tuning the retrieval module while keeping the language model parameters fixed.

% By performing end-to-end fine-tuning of the reranker with the objective of minimizing the final language loss in answer generation, G-Rerank not only improves the reranker's ability to distinguish relevant documents but also enhances the quality of the generator. This results in an overall performance boost in the RAG-based QA system.

\begin{figure*}[!ht]
\centering
\begin{minipage}{0.4\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figs/gumbel_masking.pdf}
  \caption{Comparison of Max Sampling Weight (indicating the reranker's ability to distinguish between candidate documents) with and without Gumbel noise on the NQ dataset.}
  \label{fig:gumbel_masking}
\end{minipage}
\begin{minipage}{0.04\linewidth}
\end{minipage}
\begin{minipage}{0.55\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figs/Performance_Comparison_DOC_Metrics.pdf}
  \caption{Performance comparison of different scalar metrics for assessing candidate document relevance in the Mining Setting. Our method is illustrated in \autoref{fig:learnable_sampling_weight method} and \hyperref[alg:masking_no_reranker]{Algorithm~\ref{alg:masking_no_reranker}}, while other baseline methods are described in detail in \autoref{sec: appendix_Learnable Sampling Weights}.}
  \label{fig:Performance_Comparison_DOC_Metrics}
\end{minipage}%
\end{figure*}


\subsection{Identifying Indirectly Relevant Documents}\label{sec: indirect exp}

In multi-hop question answering, a RAG system is required to retrieve a complete evidence chain comprising multiple documents to support its final answer. In such scenarios, the reranker should be able to identify \textit{indirectly relevant documents}, which are relevant to the query but do not directly contain the final answer. The challenge, however, lies in the fact that these documents often serve as `partial' evidence, and their relevance is not immediately apparent without being combined with other documents. Existing perplexity-based training methods commonly used in the literature distill independent relevance scores for each document, which fail to capture the inter-document dependencies that are essential for identifying indirectly relevant documents, as discussed in \autoref{sec: Challenges in Handling Indirectly Relevant Documents with EMDR/PDist}.

We evaluate various reranker training methods on \texttt{HotpotQA} to assess their ability to identify \textit{indirectly relevant documents}. To obtain the data, we employ a straightforward rule-based method to extract such documents: any document labeled as relevant in the dataset but not directly containing the final answer is considered an indirectly relevant document. Further discussion about this rule can be found in \autoref{sec: appendix_indirectly relevant documents setting}.

The experimental results are summarized in \autoref{tbl:hotpotqa-indirect-combined}. Our method, G-Rerank, demonstrates a significant improvement in identifying indirectly relevant documents. Specifically, when FiD-Large is used as the generator model, G-Rerank achieves a recall improvement of 10.4\%. These results suggest that our approach, which views reranking as a subset sampling problem, allows the model to better capture inter-document relationships and effectively recognize complete evidence chains.


\subsection{Necessity of Gumbel Trick}\label{sec: gumbel exp}

We leverage the Gumbel trick to transform the output weights of the reranker into an approximately discrete attention mask, where values tend to converge to either 0 or 1. A natural question arises: \textit{Is the introduction of Gumbel noise essential?} We conduct an ablation study by removing the Gumbel noise and directly utilizing the reranker's output weights as the attention mask while maintaining the same end-to-end optimization process.

Our experiments reveal a substantial drop in performance when Gumbel noise is omitted. Specifically, the EM metric on the \texttt{NQ} dataset decreases drastically from \( 46.2 \) (with Gumbel) to \( 12.7 \) (without Gumbel). To gain further insight, we visualize the reranker's output weights during training.

\autoref{fig:gumbel_masking} presents the average maximum normalized document weight assigned by the reranker. With the Gumbel noise applied, we observe a clear upward trend in the maximum document weight, indicating that the reranker progressively enhances the differentiation between candidate documents, which ultimately leads to convergence. In contrast, when Gumbel noise is removed, the maximum document weight decreases over time, eventually stabilizing at \( 0.05 \), signaling a diminished ability to distinguish between candidates. This degradation occurs because, in the absence of the discretization constraint introduced by the Gumbel trick, the model tends to preserve the original attention distribution, thus treating the removal of the attention mask as its objective. Consequently, the reranker learns to assign uniform soft mask across all candidates, i.e., \( \hat{\mathcal{M}}^{\mathcal{R}}_{i, \text{w/o Gumbel}}=\frac{1}{N}=0.05, \forall i \), thereby reverting the masked attention mechanism \autoref{eq: DMA} to its original form as defined in \autoref{eq: original attn}. These findings underscore the importance of the discretization constraint imposed by the Gumbel trick for learning an effective attention mask.

% \autoref{fig:gumbel_masking} illustrates the maximum normalized document weight assigned by the reranker. When Gumbel noise is applied, we observe a clear upward trend in the maximum document weight, indicating that the reranker progressively enhances the distinction between candidate documents, ultimately leading to convergence. This suggests that the model is continuously improving the probability of sampling the most relevant document. In contrast, without Gumbel noise, the maximum document weight decreases over time, converging towards \( 0.05 \), which indicates a reduced ability to distinguish between candidate documents. This degradation occurs because, in the absence of the discretization constraint introduced by the Gumbel trick, the model tends to preserve the original attention distribution, effectively treating the removal of the attention mask as its objective. Consequently, the model learns to assign uniform attention weights across all candidates, i.e., \( \hat{\mathcal{M}}^{\mathcal{R}}_{i, \text{w/o Gumbel}}=\frac{1}{N}=0.05, \forall i \), thereby reverting \autoref{eq: DMA} to \autoref{eq: original attn}. These results demonstrate that the discretization constraint introduced by the Gumbel trick is crucial for learning an effective attention mask.

\subsection{Learnable Sampling Weights}\label{sec: learnable weight exp}
The presence of the reranker can be viewed as incorporating text-based prior knowledge into the document relevance learning process. However, even in the absence of text-based priors, our training methodology can still effectively identify the relevant documents. To verify this, we focus on the Mining Setting and investigate whether the model is capable of learning meaningful document relevance scores \textit{without the use of a reranker}.

Our method is illustrated in \autoref{fig:learnable_sampling_weight method} and \hyperref[alg:masking_no_reranker]{Algorithm~\ref{alg:masking_no_reranker}}, while the experimental setup and baseline methods are explained in \autoref{sec: appendix_Learnable Sampling Weights}. Specifically, we remove the reranker component and instead assign each candidate document a learnable sampling weight, initializing all weights to zero. The results, presented in \autoref{fig:Performance_Comparison_DOC_Metrics}, show that even without the reranker (i.e., without prior knowledge of the text), our approach is still able to learn reliable relevance scores for each document. Moreover, it significantly outperforms other scalar metrics based on perplexity or attention scores, further confirming the effectiveness of our training objective.






