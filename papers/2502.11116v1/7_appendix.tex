\newpage
\begin{figure*}[!th]
  \includegraphics[width=\linewidth]{figs/all_tau_sampling_weight.pdf}
  \centering
\caption{The impact of different \(\tau\) values on the training process. We conduct with 20 candidate documents and RankT5 on the NQ dataset. The solid line in the figure represents the moving average. The differences in sampling weights indicate the Reranker's ability to distinguish between candidate documents.}
\label{fig:all_tau_sampling_weight}
\end{figure*}


\begin{figure*}[!th]
  \includegraphics[width=\linewidth]{figs/all_kappa_sampling_weight.pdf}
  \centering
\caption{The impact of different \(\kappa\) values on the training process. We conduct with 20 candidate documents and RankT5 on the NQ dataset. The solid line in the figure represents the moving average. The differences in sampling weights indicate the Reranker's ability to distinguish between candidate documents.}
\label{fig:all_kappa_sampling_weight}
\end{figure*}




\section{Effect of hyper-parameters on the Training Process}\label{sec: Effect of hyper-parameters on the Training Process}
The temperature parameter \(\tau\) controls the sharpness of the softmax distribution used in the selection process of documents. We conduct sampling weight learning for 20 candidate documents based on RankT5 on the NQ dataset, and tested the impact of different \(\tau\) values on the sampling weights during the training process. The experimental results are shown in \autoref{fig:all_tau_sampling_weight}. Specifically, as \(\tau\) approaches zero, the softmax distribution becomes increasingly sharp, leading to a \textit{hard} selection process where the model heavily favors the document with the highest score. This results in a deterministic decision-making process, where the model’s focus is on exploitation, quickly converging to a particular document. On the other hand, when \(\tau\) increases, the distribution becomes smoother, allowing for a more stochastic sampling process. This introduces more exploration, as the model is less likely to fixate on a single document, encouraging the exploration of other potential candidates. A larger \(\tau\) thus promotes diversity in the selection process, which can be beneficial for avoiding local optima and improving generalization during training.

The scaling factor \(\kappa\) plays a critical role in controlling the relative influence of the Reranker scores on the overall document selection process. We test the impact of different \(\kappa\) values on the sampling weights during the training process. The experimental results are shown in \autoref{fig:all_kappa_sampling_weight}. Specifically, \(\kappa\) modulates the contribution of the Reranker score \(w_i\) to the final selection probability. When \(\kappa\) is small, the contribution of the original Gumbel noise term \(G_i\) dominates the selection process. This introduces significant randomness, increasing the exploration rate during training. A small \(\kappa\) value results in noisy selection, encouraging the model to explore various documents and learn more diverse representations. Conversely, when \(\kappa\) is large, the Reranker score \(w_i\) has a stronger influence, and the model’s selection becomes more deterministic. In this case, the Reranker score dominates the sampling process, leading to faster convergence as the model focuses on selecting the most highly scored documents. However, an overly large \(\kappa\) may limit the exploration of alternative options, potentially leading to overfitting and reduced generalization.



\section{LLM-Supervised Baselines} \label{sec: appendix_baseline}
\paragraph{Baselines.}
We compare our approach against four LLM-supervised reranker training methods that leverage generative language model signals to supervise retriever learning without requiring additional document annotations. In particular, we consider the following methods:

\textbf{Attention Distillation (ADist)}~\cite{DBLP:conf/iclr/IzacardG21}: This method utilizes the cross-attention scores from the language model—augmented by the norms of the corresponding value vectors—to compute a target relevance distribution over retrieved documents. The reranker $\mathcal{R}$ is trained by minimizing the KL-divergence between its own distribution over the top-$k$ documents and the attention-based target distribution. The target distribution for the reranker is defined as:
\[
p_{\textsc{attn}}(\mathbf{p}_k) = \frac{\exp(\alpha_k \| \mathbf{v}_k \|_2)}{\sum_{i=1}^K \exp(\alpha_i \| \mathbf{v}_i \|_2)}
\]
where $\alpha_k$ is the attention score for document $\mathbf{p}_k$ and $\|\mathbf{v}_k\|_2$ is the L2 norm of the corresponding value vector. The loss function minimizes the KL-divergence between the reranker's distribution $p_{\mathcal{R}}$ and the target distribution $p_{\textsc{attn}}$:
\[
\textsc{KL}(p_{\textsc{attn}} \ \| \ p_{\mathcal{R}}) = \sum_{k=1}^K p_{\textsc{attn}} (\mathbf{p}_k) \log \frac{p_{\textsc{attn}} (\mathbf{p}_k)}{p_{\mathcal{R}} (\mathbf{p}_k)}
\]

\textbf{End-to-end Multi-Document Reader and Reranker (EMDR$^2$)}~\cite{DBLP:conf/nips/SachanRHDY21, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/iclr/Lin0CSL00KSLZY24}: EMDR$^2$ adopts an expectation-maximization approach, treating the retrieved documents as latent variables. Given a query $\mathbf{q}$ and a corresponding answer $\mathbf{a}$, along with the top-$k$ retrieved documents, the loss is designed to maximize the log-likelihood of the output given these documents. The objective function is:
\[
\mathcal{L}_{\text{EMDR}^2} = \log \left[ \sum_{k=1}^K p_{\textsc{lm}}(\mathbf{a} \ | \ \mathbf{q}, \mathbf{p}_k) p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q}) \right]
\]
where $p_{\textsc{lm}}(\mathbf{a} \ | \ \mathbf{q}, \mathbf{p}_k)$ is the language model’s probability of generating the answer $\mathbf{a}$ conditioned on the query $\mathbf{q}$ and document $\mathbf{p}_k$, and $p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q})$ is the reranker’s distribution over the top-$k$ documents.

\textbf{Perplexity Distillation (PDist)}~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/GlassRCNCG22}: In this approach, the reranker is trained to predict the improvement in the language model’s perplexity when each document is used to condition the model’s output. The KL divergence is minimized between the reranker's distribution over documents and the posterior distribution derived from the language model, which provides a direct measure of how much a document contributes to the model’s performance. The target distribution for the reranker is computed as:
\[
p_k = \frac{\exp(\log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathbf{p}_k, \mathbf{q}))}{\sum_{i=1}^K \exp ( \log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathbf{p}_i, \mathbf{q}))}
\]
The reranker is trained to minimize the KL-divergence between its predicted distribution over the documents and this target distribution:

\[
\mathcal{L}_{\text{PDist}} = \sum_{k=1}^K p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q}) \log \frac{p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q})}{p_k}
\]

Here, $p_k$ is the distribution over documents that the language model would prefer, and the reranker is trained to match this distribution to improve the language model's perplexity. The KL-divergence loss encourages the reranker to select documents that enhance the model's ability to generate the correct answer.

The objective function in EMDR$^2$ is based on maximizing the log-likelihood of the correct answer, given the query and documents. This method treats the documents as latent variables and aims to optimize the likelihood of generating the correct answer based on the combination of the language model and reranker’s distributions. On the other hand, PDist focuses on optimizing the reranker’s distribution by minimizing the KL-divergence between its predictions and the target distribution, which is derived from the language model’s perplexity.

\textbf{Leave-one-out Perplexity Distillation (LOOP)}~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}: LOOP refines the PDist approach by considering the impact of each document in the context of all other documents in the top-$k$ set. For each document, the log-likelihood of the output is computed by excluding the document from the retrieval set, and the negative of this value is used as a relevance score. The target distribution is:
\begin{equation*}
    \begin{aligned}
        &p_{\textsc{loop}}(\mathbf{p}_k) = \\
        &\frac{\exp(- \log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathcal{D}_K \setminus \{ \mathbf{p}_k \}, \mathbf{q}))}
        {\sum_{i=1}^K \exp (- \log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathcal{D}_K \setminus \{ \mathbf{p}_i \}, \mathbf{q}))}
    \end{aligned}
\end{equation*}
The reranker is trained to minimize the KL-divergence between this distribution and the one obtained from the reranker.


\section{More Details}
\subsection{Variants of FiD}\label{sec: appendix_fid}
Recent advancements in Open-Domain Question Answering have led to the development of several enhanced Fusion-in-Decoder models. KG-FiD~\cite{DBLP:conf/acl/Yu0F0WXRY022} enhances the traditional FiD framework by integrating knowledge graphs to establish structural relationships among retrieved passages. This integration employs graph neural networks to re-rank passages, selecting the most pertinent ones for answer generation, thereby improving both effectiveness and efficiency. FiDO~\cite{DBLP:conf/acl/JongZAFSSC23} addresses memory bandwidth constraints inherent in the FiD architecture by reallocating computational resources. This optimization results in a significant increase in inference speed without compromising performance, making it more suitable for real-time applications. FiD-Light~\cite{DBLP:conf/sigir/HofstatterC0Z23} focuses on efficient retrieval-augmented text generation by optimizing the balance between retrieval and generation components. This approach reduces computational overhead while maintaining answer accuracy, offering a more resource-efficient alternative. RFiD~\cite{DBLP:conf/acl/WangY023} introduces a multi-task learning approach to discern evidentiality, combining passage re-ranking with sentence classification. This method enhances the model's ability to identify causal relationships between questions and passages, leading to improved answer accuracy. Multi-Granularity Guided Fusion-in-Decoder (MG-FiD)~\cite{DBLP:conf/naacl/ChoiLL24} further refines the FiD approach by aggregating evidence across multiple levels of granularity. It harmonizes passage re-ranking with sentence-level classification, enhancing both accuracy and decoding efficiency.

In our experiments, since we are mainly focusing on reranker training strategies rather than reader, we utilize the classical Fusion-in-Decoder model architecture. Building upon this foundation, we compare our approach with various LLM-supervised reranker training strategies to assess their impact on ODQA performance.

\begin{table*}[!ht]
\centering
\small
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ l || c c|| c c | c || c c c }
    \toprule
    & \multicolumn{2}{c||}{\textbf{Mining Setting}} & \multicolumn{3}{c||}{\textbf{Reranker Setting}} & \multicolumn{3}{c}{\textbf{Generator Setting}} \\  
    \cmidrule(r){2-3} \cmidrule(r){4-6} \cmidrule(r){7-9}
    & \textbf{Recall@5} & \textbf{NDCG@5} & \textbf{Recall@5} & \textbf{NDCG@5} & \textbf{MRR} & \textbf{EM} & \textbf{SubEM} & \textbf{F1} \\
    \addlinespace[0.2em]
    \hline
    \hline
    \addlinespace[0.4em]
    \multicolumn{9}{c}{\textbf{Dataset: }\normalsize\texttt{Hotpotqa}} \\[-0.1em]
    \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{Rank-T5}}& \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24}           & \underline{78.8}  & \underline{81.1}  & \underline{79.5} & \underline{81.1} & \underline{95.8} & \underline{58.3} & \underline{64.6} & \underline{73.1} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22}         & 70.9  & 73.9  & 71.4 & 73.6 & 92.6 & 57.8 & 64.0 & 72.5 \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}           & 72.2  & 75.4  & 73.4 & 75.7 & 93.7 & 58.0 & 64.2 & 72.7 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21}& 72.2  & 74.1  & 72.5 & 73.8 & 90.5 & 56.5 & 62.6 & 71.1 \\
    \rowcolor{gray!10} - G-Rerank   & \bf 81.9  & \bf 83.7  & \bf 83.1 & \bf 84.0 & \bf 95.9 & \bf 58.8 & \bf 65.1 & \bf 73.5 \\
    \midrule
     \addlinespace[0.4em]
     \addlinespace[0.1em]\multicolumn{1}{l}{\bf{Reranker: }\normalsize\texttt{BGE-Base}}& \multicolumn{8}{l}{}\\[0.2em]
    - EMDR~\cite{DBLP:conf/iclr/Lin0CSL00KSLZY24}           & 78.4  & \underline{81.1}  & 78.8 & 80.7 & \underline{95.9} & 58.6 & 64.8 & \underline{73.4} \\
    - PDist~\cite{DBLP:conf/naacl/GlassRCNCG22}         & 76.7  & 79.8  & 78.5 & 80.7 & \bf 96.1 & \underline{58.7} & \underline{64.9} & \underline{73.4} \\
    - LOOP~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}           & 76.0  & 79.0  & 77.1 & 79.2 & 95.3 & 58.3 & 64.5 & 73.0 \\
    - ADist~\cite{DBLP:conf/iclr/IzacardG21}& \underline{78.5}  & 80.7  & \underline{79.3} & \underline{80.9} & 95.1 & 58.2 & 64.4 & 73.0 \\
    \rowcolor{gray!10} - G-Rerank   & \bf 81.6  & \bf 83.3  & \bf 82.6 & \bf 83.3 & 95.8 & \bf 58.8 & \bf 65.0 & \bf 73.5 \\
    \bottomrule
\end{tabular}
}
\caption{Experiments on HotpotQA using FiD-Base as reader. We consider the settings illustrated in \autoref{fig:expsetting}. The best performance is highlighted in bold, while the second-best performance is underlined.}
\label{tbl:multi dataset experiment FiD-Base}
\end{table*}

\subsection{Multi-Hop Question Answering}\label{sec: Multi-Hop QA}
Multi-hop Question Answering (QA) systems typically follow a two-phase process: first, retrieving relevant passages, and then using these passages to answer the question. 2WikiHop~\cite{DBLP:conf/coling/HoNSA20}, HotpotQA~\cite{DBLP:conf/emnlp/Yang0ZBCSM18}, Musique~\cite{DBLP:journals/tacl/TrivediBKS22}, and MultiHop-RAG \cite{DBLP:journals/corr/abs-2401-15391} are widely used benchmarks for evaluating and improving RAG systems in handling complex multi-hop reasoning tasks. The retrieval strategies can differ depending on the QA setting, which may either be open-domain or reading comprehension. 

In open-domain QA, the focus is on retrieving relevant passages from a large corpus. Methods like MDR \cite{DBLP:conf/iclr/XiongLIDLWMY0KO21} and BeamDR \cite{DBLP:conf/naacl/ZhaoXBD21} are commonly used in this context. In the case of reading comprehension, retrieval methods are generally categorized into one-step and two-step approaches. One-step methods, such as SAE \cite{DBLP:conf/aaai/TuHW0HZ20}, rank passages by concatenating the question with each candidate passage. Two-step methods, including S2G \cite{DBLP:journals/corr/abs-2107-11823} and FE2H \cite{DBLP:conf/icassp/LiLY23}, start by selecting an initial hop passage and then refine the search by pairing it with additional candidates. The R$^3$ model \cite{DBLP:conf/cncl/YinWHWYZCHQ23} enhances this approach by selecting multiple passages at the outset and combining them to find the correct answer. Beam Retrieval \cite{DBLP:conf/naacl/ZhangZ00H24} further extends the process by using a beam search, enabling it to handle more complex multi-hop retrieval tasks that go beyond just two hops.

Our work focuses on a different scenario: we analyze the limitations of the LLM-Supervised Reranker Training strategy in widely used RAG systems for multi-hop question answering tasks (as detailed in \autoref{sec: Challenges in Handling Indirectly Relevant Documents with EMDR/PDist}) and propose an end-to-end reranker training strategy based on Gumbel Subset Sampling, which is well-suited for multi-hop question answering tasks.

\subsection{Dataset Description}\label{sec: appendix_datasets}
The datasets in our study encompass a variety of challenges designed to assess different facets of question answering. \textbf{HotpotQA}~\cite{DBLP:conf/emnlp/Yang0ZBCSM18} is a multi-hop QA dataset that requires reasoning over multiple Wikipedia articles to derive answers, emphasizing both factual retrieval and reasoning capabilities. Similarly, \textbf{2WikiHop}~\cite{DBLP:conf/coling/HoNSA20} extends the complexity of multi-hop reasoning by introducing questions that necessitate navigating a knowledge graph, enhancing the evaluation of entity-based information retrieval. \textbf{Musique}~\cite{DBLP:journals/tacl/TrivediBKS22} is designed to assess compositional reasoning by decomposing complex questions into a sequence of simpler sub-questions, providing a structured approach to multi-step reasoning evaluation. Meanwhile, \textbf{Natural Questions (NQ)}~\cite{DBLP:journals/tacl/KwiatkowskiPRCP19} presents real-world search queries answered using long-form documents, challenging models to extract and summarize information from extensive contexts. Lastly, \textbf{TextbookQA (TQA)}~\cite{DBLP:conf/acl/KimKK19} focuses on domain-specific comprehension, where questions require understanding of textbook-style knowledge, integrating both textual and diagrammatic content for a holistic assessment of contextual understanding and inferential capabilities.

\subsection{Additional Results with FiD-Base}
We present additional results on the HotpotQA dataset using FiD-Base as the reader, as shown in \autoref{tbl:multi dataset experiment FiD-Base}. Our method outperforms others across most metrics, demonstrating its efficacy.

\section{Challenges in Handling Indirectly Relevant Documents with EMDR/PDist}\label{sec: Challenges in Handling Indirectly Relevant Documents with EMDR/PDist}

Both EMDR$^2$~\cite{DBLP:conf/nips/SachanRHDY21, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/iclr/Lin0CSL00KSLZY24} and PDist~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/GlassRCNCG22} are based on the premise of distilling the importance of individual documents in a multi-document retrieval and generation process. While effective for ranking directly relevant documents, both methods encounter challenges when dealing with \textit{indirectly relevant documents}, which provide context but do not directly contain the answer. 

In EMDR$^2$, the objective is to maximize the log-likelihood of generating the answer given the query and individual documents. The loss function is given by:
\[
\mathcal{L}_{\text{EMDR}^2} = \log \left[ \sum_{k=1}^K p_{\textsc{lm}}(\mathbf{a} \ | \ \mathbf{q}, \mathbf{p}_k) p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q}) \right]
\]
where \( p_{\textsc{lm}}(\mathbf{a} \ | \ \mathbf{q}, \mathbf{p}_k) \) represents the language model’s probability of generating the answer conditioned on the query and document, and \( p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q}) \) is the reranker's preference for the \(k\)-th document. However, this approach assumes the independence of documents when generating the answer. Indirectly relevant documents, while crucial in providing context, do not appear to contribute meaningfully when evaluated independently. The importance of such documents can only be assessed when they interact with other documents in the evidence chain, making their relevance difficult to capture in this formulation.

Similarly, PDist minimizes the KL divergence between the reranker's distribution \(p_{\mathcal{R}}(\mathbf{p}_k \ | \ \mathbf{q})\) and the distribution \( p_k \) derived from the language model's perplexity. \( p_k \) represents the distribution over documents that the language model prefers based on its perplexity improvement:
\[
p_k = \frac{\exp(\log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathbf{p}_k, \mathbf{q}))}{\sum_{i=1}^K \exp ( \log p_{\textsc{lm}} (\mathbf{a} \ | \ \mathbf{p}_i, \mathbf{q}))}
\]

In both methods, the language model \( p_{\textsc{lm}}(\mathbf{a} \ | \ \mathbf{q}, \mathbf{p}_k) \) computes the probability of generating the answer based on the query and a single document \( \mathbf{p}_k \), treating the document in isolation. This formulation assumes that each document, independently, provides enough information to determine the relevance to the query and the answer. However, in the case of indirectly relevant documents, this assumption breaks down. Indirectly relevant documents do not contain the answer directly but instead contribute to the reasoning process by supporting or contextualizing other documents. When evaluated alone, these documents may appear less relevant or even irrelevant, which undermines the effectiveness of both methods.

\begin{center}
\begin{algorithm*}
\caption{Learnable Sampling Weights Setting}
\label{alg:masking_no_reranker}
\begin{algorithmic}[1]
\Procedure{StochasticSubsetMask}{document weights $w_1, \dots, w_n$, temperature $\tau$, scale factor $\kappa$, subset size $k$}
    \For{$j = 1$ \textbf{to} $k$} 
        \State $\tilde{w}_i = -\log(-\log(u_i)) + \kappa \cdot w_i,\quad u_i \sim \mathcal{U}(0, 1) \quad \forall i \in [n]$
        \State $\hat{\mathcal{M}}^{j} = \max(\hat{\mathcal{M}}^{j-1}, \text{softmax}\left(\frac{(\tilde{w}_1, \dots, \tilde{w}_n)}{\tau}\right))$ \quad \textcolor{gray}{\# $\hat{\mathcal{M}}^{0} = [0, \dots, 0]$}
    \EndFor
    \State \Return $\hat{\mathcal{M}}^{k}$ \Comment{Return Relaxed top-$k$ Mask}
\EndProcedure
\State
\State Given a query $\mathbf{q}$, answer $\mathbf{a}$, and $n$ retrieved passages $\mathbf{p_1}, \dots, \mathbf{p_n}$
\State \textbf{Initialization:} Initialize learnable document weights $w_1 = 0, \, w_2 = 0, \, \dots, \, w_n = 0$
\For{each training step}
    \State $\hat{\mathcal{M}} = \operatorname{StochasticSubsetMask}(w_1, \dots, w_n, \tau, \kappa, k)$
    \State Apply $\mathcal{DMA}(\hat{\mathcal{M}})$ to obtain logits and language loss $\mathcal{L}_{LM}$  \Comment{\autoref{sec: DMA}}
    \State Update document weights $w_1, \dots, w_n$ with $\nabla_{w_1, \dots, w_n} \mathcal{L}_{LM}$ \Comment{Gradient-based update}
\EndFor
\end{algorithmic}
\end{algorithm*}
\end{center}


\begin{figure*}[!th]
  \includegraphics[width=0.9\linewidth]{figs/learnable_sampling_weight.pdf}
  \centering
\caption{Setting of Learnable Sampling Weight. Optimizing candidate document sampling weights directly without leveraging reranker's prior textual knowledge.}
\label{fig:learnable_sampling_weight method}
\end{figure*}


\section{Irrelevant Document Setting}\label{sec: appendix_indirectly relevant documents setting}
Multi-hop question answering in \texttt{HotpotQA} involves synthesizing information from multiple documents to resolve a single query. Although these questions can be categorized into four major reasoning types---such as bridging intermediate information, comparing entities, verifying multiple properties, or inferring properties through a bridge---they all share the common requirement of gathering evidence across several sources. In this setting, identifying and assessing \emph{indirectly relevant documents} can be instrumental for measuring how effectively a model captures the full chain of reasoning. Our approach defines an \emph{indirectly relevant document} as any document labeled as relevant in the dataset yet not explicitly containing the final answer. This rule is rational in that it highlights the documents that contribute background or bridging information. However, this simple rule sometimes blurs the distinction between direct and indirect relevance. For instance, when a document only partially contains the answer, or when multiple sources each provide different fragments of a single reasoning chain (especially in question types like comparing entities or verifying multiple properties), all supporting documents will be classified as indirectly relevant by this rule. The concept of “partial” evidence is inherently difficult to categorize as either direct or indirect, and our rule consequently treats such “partial” evidence as indirectly relevant documents.

After processing the dataset, we observe that in the training set, the ratio of total queries to data entries that do contain \emph{indirectly relevant documents} is 90,447 to 664,247. In the development set, this ratio is 7,405 to 5,966. The relatively large number of data entries that do contain indirectly relevant documents allows for a robust evaluation of a model’s ability to retrieve and utilize such supporting evidence. Thus, this formulation not only aligns well with the structure of \texttt{HotpotQA} but also provides a meaningful benchmark for analyzing the effectiveness of different methods in capturing multi-hop dependencies beyond direct answer retrieval.


\section{Learnable Sampling Weights Setting}\label{sec: appendix_Learnable Sampling Weights}
\subsection{Scalar Relevance Baselines}
In our thesis, we employ different scalar metrics to quantify the relevance of each candidate document in the RAG system. These metrics correspond to the different LLM-supervised reranker training methods, and they serve as proxies for the document's contribution to generating the correct answer. In particular, we consider the following three metrics:

\paragraph{Lowest Perplexity}
For methods such as EMDR$^2$ and Perplexity Distillation (PDist), each candidate document $\mathbf{p}_k$ is evaluated by combining it with the query $\mathbf{q}$ and computing the language model’s negative log-likelihood of generating the ground-truth answer $\mathbf{a}$. Formally, the scalar relevance score is defined as:
\[
s_{\text{perplexity}}(\mathbf{p}_k) = -\log p_{\textsc{lm}}(\mathbf{a} \mid \mathbf{q}, \mathbf{p}_k).
\]
In this setting, a lower perplexity (i.e., a higher value of $s_{\text{perplexity}}$) indicates that the document better facilitates the generation of the correct answer, and is therefore considered more relevant.

\paragraph{Highest Attention Score}
The Attention Distillation (ADist) method evaluates relevance by feeding all candidate documents to the language model simultaneously and aggregating the cross-attention scores. For each document $\mathbf{p}_k$, the relevance score is computed by weighting the attention score $\alpha_k$ with the L2 norm of its corresponding value vector $\mathbf{v}_k$:
\[
s_{\text{attn}}(\mathbf{p}_k) = \alpha_k \,\|\mathbf{v}_k\|_2.
\]
Here, a higher attention score signifies that the language model assigns more importance to the document during answer generation, thereby indicating greater relevance.

\paragraph{Highest Leave-one-out Perplexity}
The Leave-one-out Perplexity Distillation (LOOP) method assesses the impact of each candidate document by measuring the degradation in the language model’s performance when that document is excluded from the candidate set. For each document $\mathbf{p}_k$, the relevance score is defined as:
\[
s_{\text{loop}}(\mathbf{p}_k) = -\log p_{\textsc{lm}}\Big(\mathbf{a} \mid \mathcal{D}_K \setminus \{\mathbf{p}_k\}, \mathbf{q}\Big),
\]
where $\mathcal{D}_K$ denotes the set of top-$k$ candidate documents. A higher leave-one-out score implies that the removal of $\mathbf{p}_k$ leads to a significant deterioration in the language model’s ability to generate the answer, marking it as highly relevant.

\subsection{Our method}
In our proposed approach, we eliminate the dependency on a dedicated reranker by replacing it with a set of learnable scalar weights—one per document—that are initialized at zero and updated directly via gradients from the language modeling loss computed by the differentiable masked attention module. As detailed in Algorithm~\ref{alg:masking_no_reranker} and \autoref{fig:learnable_sampling_weight method}, the algorithm employs stochastic Gumbel noise to perform a relaxed top‑$k$ selection over these weights, ensuring that the entire process remains fully differentiable. This method iteratively refines the document weights over multiple steps on a single query–answer pair and its corresponding documents, thereby enabling the model to learn which documents are most informative for the downstream language modeling task.

\begin{table*}[!th]
\small
\centering
\begin{tabular}{p{5cm} p{5cm} p{4cm}}
\toprule
\textbf{Dataset} & \textbf{URL} & \textbf{License} \\
\midrule
\textbf{Multi-hop QA} & & \\
2WikiMultiHopQA~\cite{DBLP:conf/coling/HoNSA20} & \url{https://github.com/Alab-NII/2wikimultihop} & Apache License 2.0: \url{https://github.com/Alab-NII/2wikimultihop/blob/master/LICENSE} \\
HotpotQA~\cite{DBLP:conf/emnlp/Yang0ZBCSM18} & \url{https://hotpotqa.github.io/} & CC BY-SA 4.0: \url{https://hotpotqa.github.io/} \\
MuSiQue~\cite{DBLP:journals/tacl/TrivediBKS22} & \url{https://github.com/stonybrooknlp/musique} & CC BY 4.0: \url{https://github.com/stonybrooknlp/musique/blob/main/LICENSE} \\
\midrule
\textbf{Single-hop QA} & & \\
Natural Questions (NQ)~\cite{DBLP:journals/tacl/KwiatkowskiPRCP19} & \url{https://ai.google.com/research/NaturalQuestions} & CC BY-SA 3.0: \url{https://ai.google.com/research/NaturalQuestions/download} \\
Textbook Question Answering (TQA)~\cite{DBLP:conf/acl/KimKK19} & \url{https://prior.allenai.org/projects/tqa} & CC BY-NC 3.0: \url{https://prior.allenai.org/projects/tqa} \\
\bottomrule
\end{tabular}
\caption{Summary of URLs and Licenses for Datasets}
\label{tab:qa_datasets}
\end{table*}

\section{URLs and Licenses}
\autoref{tab:qa_datasets} provides license information for the datasets we utilize in our experiments. We employ all the above datasets solely for research purposes, in accordance with their designated uses.