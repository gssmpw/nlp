\section{Related Works}
% \paragraph{Training the Retrieval Module in RAG Systems}
% Retrieval-Augmented Generation (RAG) systems consist of three key components: the retrieval module, the reranking module, and the generation module (see a comprehensive RAG survey for an overview). Among these, the retrieval and reranking quality play a crucial role in determining the final answer quality of the RAG system**Vinyals et al., "Retrieval-Augmented Generation for Less-Common Queries"**.

% Traditionally, information retrieval systems relied on hand-crafted sparse representations such as TF-IDF and BM25**Salton et al., "A Vector Space Model for Automatic Indexing"**, but later **Robertson et al., "New Proportional Retrieval Measures Aimed Exclusively at Novelty Order"**. More recently, dense vector-based approaches leveraging deep learning have demonstrated superior performance**Bordes et al., "Learning Syntactic Structures from Unlabeled Text with Structured Inference Networks"**. However, most retrieval modules are fine-tuned on publicly available datasets, making them suboptimal when applied to specific RAG downstream tasks, often requiring further adaptation to improve effectiveness**Wang et al., "Revisiting Embedding Methods for Open-Domain Question Answering"**.

% Recent studies have increasingly focused on fine-tuning retrieval and reranking modules for task-specific downstream applications, such as open-domain question answering (ODQA). In these scenarios, queries often lack labeled relevant articles, necessitating alternative approaches for retrieval supervision**Lewis et al., "Retrieval-Augmented Generation of Conversational Dialogue"**. Current RAG systems typically employ a powerful large language model (LLM) for generation**Radford et al., "Improving Language Understanding by Generative Models"**, and a common strategy is to distill knowledge from the LLM into the retriever module. One effective approach is to concatenate each candidate document with the query and use the perplexity of the generated answer as a relevance signal**Henderson et al., "Using Deep Neural Networks for Information Retrieval in Question Answering Systems"**. 

% However, RAG systems typically retrieve multiple documents as input, and existing independent document-level distillation methods fail to capture inter-document relationships**Wang et al., "A Systematic Study on Multi-Hop Reasoning in Open-Domain QA"**, which is particularly problematic for multi-hop reasoning tasks, where intermediate inference steps are required to determine the relevance of candidates**Khashabi et al., "Multi-Hop Question Answering via Resource Efficient Attention Co-Pooling"**. Other methods have explored distillation using attention scores**Liu et al., "Improving Multi-Hop Reasoning with Dense Retrieve-Rerank"**, yet attention distributions tend to be unreliable when handling a large number of candidate documents**Chen et al., "Dense Retrieve-Rerank for Open-Domain Question Answering"**. Leave-one-out-based methods**Zhu et al., "A Study on Leave-One-Out Methods for Multi-Hop Question Answering"**, have also been proposed to evaluate document relevance, but they do not perform end-to-end fine-tuning and are not explicitly optimized to reduce the final language modeling loss of the RAG system. This misalignment creates a performance gap between retriever training and generation quality**Wang et al., "A Systematic Study on Multi-Hop Reasoning in Open-Domain QA"**.

% Therefore, an ideal retrieval fine-tuning approach should be end-to-end, explicitly optimizing for generation performance, and capable of modeling inter-document dependencies in multi-hop reasoning tasks.

\paragraph{Training the Reranking Module in RAG Systems}
The effectiveness of RAG systems relies heavily on the quality of retrieval and reranking **Salton et al., "A Vector Space Model for Automatic Indexing"**. Traditional retrieval methods are based on lexical similarity **Robertson et al., "New Proportional Retrieval Measures Aimed Exclusively at Novelty Order"**, while recent advances leverage dense vectors and transformer architectures**Bordes et al., "Learning Syntactic Structures from Unlabeled Text with Structured Inference Networks"**. However, retrieval modules fine-tuned on public datasets often require additional adaptation for specific downstream tasks**Wang et al., "Revisiting Embedding Methods for Open-Domain Question Answering"**.

To bridge this gap, recent works explore fine-tuning retrieval and reranking modules for tasks such as open-domain question answering (ODQA). One common strategy distills knowledge from LLMs into retrievers by ranking candidate documents based on generated answer perplexity**Henderson et al., "Using Deep Neural Networks for Information Retrieval in Question Answering Systems"**. However, such methods overlook inter-document dependencies, crucial for multi-hop reasoning tasks**Khashabi et al., "Multi-Hop Question Answering via Resource Efficient Attention Co-Pooling"**. Alternative approaches use attention scores**Liu et al., "Improving Multi-Hop Reasoning with Dense Retrieve-Rerank"**, or leave-one-out methods**Zhu et al., "A Study on Leave-One-Out Methods for Multi-Hop Question Answering"**, but these are not end-to-end optimized for generation quality, leading to a retriever-generation gap**Wang et al., "A Systematic Study on Multi-Hop Reasoning in Open-Domain QA"**.

% An ideal retrieval fine-tuning approach should be end-to-end, explicitly optimized for generation, and capable of capturing inter-document dependencies.


% \paragraph{Stochastic k-Subset Selection and Masked Attention}
% Stochastic $k$-subset selection and top-$k$ relaxation have been actively studied in neural network optimization. Several works**Maddison et al., "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"**, have explored differentiable sampling methods for subset selection from categorical distributions, extending or generalizing the Gumbel-Softmax trick**Jang et al., "Categorical Reparameterization with Gumbel-Softmax"**. 

% In NLP, stochastic subset selection methods have significant applications. For instance, in the model pruning domain, **Dong et al., "Learning Sparsely Optimized Networks (LSON) via Masked Autoencoders"**, employs learnable sparsity to end-to-end optimize an optimal $N$:$M$ sparsity scheme. Another line of research focuses on differentiable relaxations of the top-$k$ operator, such as **Chen et al., "Relaxed Greedy Search for Neural Architecture Optimization"**, which improves the differentiability of the top-$k$ operation through relaxation mechanisms. These methods have been instrumental in model interpretability, where instance-wise feature selection is achieved via learnable subset sampling**Liu et al., "Learned Subset Selection for Interpretable Model Pruning"**. 

% This observation motivates a key question: since reranking also involves instance-wise selection of the most relevant documents for a given query, can we model reranking using a similar subset sampling approach? However, in the context of reranking for RAG, we must also consider the structure of modern LLMs, where candidate documents influence generation through attention computation. Thus, an important challenge is how to introduce sparsity into the attention mechanism. 

% Common approaches involve inserting soft masks at different stages of attention computation, as explored in **Liu et al., "Sparse Transformer with Soft Masking for Efficient Attention"**. In this work, we adopt a similar soft-mask-based strategy to model the reranking process in RAG as a subset sampling problem, thereby enabling end-to-end training of the reranking module.


\paragraph{Stochastic k-Subset Selection and Masked Attention}
Top-$k$ relaxation has been widely studied for differentiable subset sampling**Maddison et al., "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"**, extending the Gumbel-Softmax trick**Jang et al., "Categorical Reparameterization with Gumbel-Softmax"**. This has important applications in semi-structured pruning**Dong et al., "Learning Sparsely Optimized Networks (LSON) via Masked Autoencoders"**, model interpretability**Liu et al., "Learned Subset Selection for Interpretable Model Pruning"**, and point clouds analysis**Wang et al., "Point Cloud Learning via Top-K Relaxation"**.

The reranking process can also be modeled as a subset sampling problem. However, since retrieved documents influence LLM outputs through attention, a key challenge lies in introducing sparsity into the attention computation. Existing approaches employ soft attention masks to model discrete selections**Liu et al., "Sparse Transformer with Soft Masking for Efficient Attention"**. Inspired by these methods, we model RAG reranking as a subset sampling process with soft masks, facilitating end-to-end optimization.



\begin{center}
\begin{algorithm*}
\caption{Gumbel Reranking: Training Reranker via Differentiable Masked Attention}
\label{alg:ranker_as_selector}
\begin{algorithmic}[1]
\Procedure{StochasticSubsetMask}{reranker $\mathcal{R}$, documents $\mathbf{d_1}, \dots, \mathbf{d_n}$, query $\mathbf{q}$, temperature $\tau$, scale factor $\kappa$, subset size $k$}
    \State $w_i = \mathcal{R}( \text{Concatenate}(\mathbf{q}, \mathbf{d}_i))\quad \forall i \in [n] \triangleq \{1, 2, \dots, n\}$  \Comment{Apply Reranker}
    \State // apply Gumbel softmax trick to $w_i$
    \State $\hat{p_i} = gumbelsoftmax(w_i, \tau)$
    \State // set the probability for each document
    \State $p_i = \text{masking}(gumbelsoftmax(w_i, \tau))$
    \State // select top-k documents based on p_i
    \State selected_docs = topk(p_i)
\end{algorithmic}
\end{center}