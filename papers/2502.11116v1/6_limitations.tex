\section{Limitations}

Our method imposes certain constraints on its applicability to existing decoder-only large language models (LLMs) due to its reliance on parallel encoding/decoding capabilities during the pre-filling stage. This requirement limits its direct adoption in conventional autoregressive LLMs. However, it is worth noting that many high-performance language models with parallel encoding/decoding capabilities have already become standard choices in various Retrieval-Augmented Generation (RAG) systems, such as FiD~\cite{DBLP:conf/eacl/IzacardG21}, CEPE~\cite{DBLP:conf/acl/YenG024}, and Parallel Windows~\cite{DBLP:conf/acl/RatnerLBRMAKSLS23}. Furthermore, our approach requires such models only during the reranker training phase; once trained, the reranker itself is independent of any specific LLM and can be flexibly adapted to other decoder-only models. Therefore, our method primarily serves as a general training framework rather than imposing architectural constraints on the final inference model. Additionally, our approach introduces extra hyperparameters in the Gumbel-Softmax process, including the temperature parameter $\tau$ and the scaling factor $\kappa$, which require tuning to achieve optimal performance. However, through empirical studies, we find that $\tau=0.5$ and $\kappa=1.0$ provide robust and stable performance across different model architectures and datasets. We provide a further discussion on the effect of $\tau$ and $\kappa$ in \autoref{sec: Effect of hyper-parameters on the Training Process}.

\section{Ethical Considerations}
While our method aims to improve the accuracy of the RAG system, it does not eliminate the inherent risks of biased data or model outputs, as the performance of RAG systems still heavily depends on the quality of training data and underlying models. The potential for bias in the training data, particularly for domain-specific queries, can lead to the amplification of these biases in the retrieved results, which can impact downstream applications.