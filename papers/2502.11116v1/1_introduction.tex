\section{Introduction}

\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{figs/introduction.pdf}
\caption{Vanilla reranker training methods for RAG systems typically rely on supervised learning of query-document pairs, which is limited by the scarcity of labeled data. To address this issue, existing methods leverage various LLM-supervised losses. However, this can lead to potential gaps between training and inference. In contrast, G-Rerank frames reranker training as learning a stochastic, document-wise top-\(k\) attention mask. This enables end-to-end optimization by minimizing language loss, ensuring better alignment between training and inference.}
  \label{fig:introduction}
\end{figure*}

Retrieval-Augmented Generation (RAG) has shown great potential in natural language processing tasks~\cite{DBLP:conf/nips/LewisPPPKGKLYR020, DBLP:conf/icml/GuuLTPC20, DBLP:conf/eacl/IzacardG21, DBLP:conf/icml/BorgeaudMHCRM0L22}. Despite their remarkable progress, retrieval models in RAG systems—comprising both the retriever and reranker—are typically trained on publicly available datasets and often struggle with long-tail queries requiring domain-specific knowledge.  As a result, they necessitate further fine-tuning for specific downstream tasks~\cite{DBLP:conf/naacl/GlassRCNCG22, DBLP:conf/naacl/ShiMYS0LZY24}. A key challenge in this context is the scarcity of labeled query-document pairs~\cite{DBLP:conf/acl/LeeCT19, DBLP:journals/tacl/SachanLYZPZ23}. Therefore, a critical research question is how to end-to-end optimize the retrieval models of RAG systems solely relying on the system’s final language modeling loss.

Recent efforts to improve retriever or reranker in RAG systems have explored distilling knowledge from LLMs into retrieval components. Techniques such as attention-based distillation~\cite{DBLP:conf/iclr/IzacardG21} and perplexity-based distillation~\cite{DBLP:conf/nips/SachanRHDY21, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/iclr/Lin0CSL00KSLZY24, DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/GlassRCNCG22} have yielded notable performance gains. However, these methods still exhibit critical limitations. First, although these methods claim to be end-to-end optimized, they focus on LLM-supervised losses like KL divergence~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/GlassRCNCG22} or marginalization~\cite{DBLP:conf/nips/SachanRHDY21, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/iclr/Lin0CSL00KSLZY24}, which do not directly minimize the RAG system’s final generation loss, leading to potential misalignment between training and evaluation objectives. Additionally, attention-based distillation suffers from the distraction problem, where accumulated attention scores do not always reflect document relevance~\cite{DBLP:conf/acl/KeK00MB24, DBLP:journals/corr/abs-2408-11745}. While perplexity-based distillation methods evaluate each candidate document in isolation, neglecting the interdependencies among retrieved documents. This oversight is particularly detrimental in multi-hop reasoning tasks requiring coherent logical relationships between documents~\cite{DBLP:journals/tacl/TrivediBKS22, DBLP:conf/coling/HoNSA20}.

In this work, we propose a novel end-to-end strategy for training rerankers in RAG systems. We reformulate the reranking task through the lens of attention masks, where selecting the top-$k$ subset from the retrieved candidate documents is viewed as the application of a document-wise top-$k$ attention mask during attention computation. This perspective leads to a shift in the problem formulation: instead of directly learning a more effective reranker, we focus on learning the optimal document-wise top-$k$ attention mask.

However, since the hard attention mask is discrete, it can not be directly optimized via gradient descent. To overcome this challenge, we introduce a solution based on the Gumbel Trick~\cite{DBLP:conf/iclr/JangGP17} and Relaxed Top-$k$ techniques~\cite{DBLP:conf/icml/ChenSWJ18}. This enables us to design a \textit{stochastic}, top-$k$ attention mask that is fully differentiable, allowing for end-to-end optimization. We note this approach as \textbf{D}ifferentiable \textbf{M}asked \textbf{A}ttention (DMA).
% , which makes the attention computation differentiable even with discrete document selection.

With DMA in place, we reformulate the reranking problem as learning the optimal sampling weight for the corresponding attention mask. This leads to our end-to-end training framework, which we refer to as \textbf{G}umbel \textbf{Rerank}ing (G-Rerank). Unlike previous methods that rely on LLM-supervised losses, G-Rerank directly optimizes the reranker by minimizing the overall language modeling loss of the RAG system, thereby ensuring that the training objective closely aligns with the inference process. Additionally, G-Rerank accounts for interdependencies between retrieved candidate documents, making it suitable for multi-hop QA tasks.

We evaluate our training approach across various architectures. Specifically, we conduct experiments using two language models—FiD~\cite{DBLP:conf/eacl/IzacardG21} and CEPE-Llama2-7B~\cite{DBLP:conf/acl/YenG024}—as well as two rerankers—BGE-Reranker-Base~\cite{xiao2023bge} and RankT5~\cite{DBLP:conf/sigir/Zhuang0J0MLNWB23}. Our method is tested on five benchmark datasets, covering both single-hop and multi-hop QA tasks. To comprehensively assess the effectiveness of our approach, we consider three different evaluation settings: \textit{mining}, \textit{reranking}, and \textit{generation}. Our proposed training strategy achieves consistent improvements across all these settings. Furthermore, compared to distillation-based methods, our training approach significantly improves the reranker's ability to distinguish \textit{indirectly relevant documents}, leading to a 10.4\% improvement in the Recall@5 metric on HotpotQA. Finally, we analyze the necessity of the Gumbel trick and the impact of prior knowledge in rerankers.

