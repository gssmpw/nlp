\section{Related Works}
% \paragraph{Training the Retrieval Module in RAG Systems}
% Retrieval-Augmented Generation (RAG) systems consist of three key components: the retrieval module, the reranking module, and the generation module (see a comprehensive RAG survey for an overview). Among these, the retrieval and reranking quality play a crucial role in determining the final answer quality of the RAG system~\cite{re2g, DBLP:journals/corr/abs-2405-18414}. 

% Traditionally, information retrieval systems relied on hand-crafted sparse representations such as TF-IDF and BM25~\cite{jones1972, robertson1995}. More recently, dense vector-based approaches leveraging deep learning have demonstrated superior performance~\cite{karpukhin2020dense, khattab2020colbert}. However, most retrieval modules are fine-tuned on publicly available datasets, making them suboptimal when applied to specific RAG downstream tasks, often requiring further adaptation to improve effectiveness~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, retrieval_quality_rag}.

% Recent studies have increasingly focused on fine-tuning retrieval and reranking modules for task-specific downstream applications, such as open-domain question answering (ODQA). In these scenarios, queries often lack labeled relevant articles, necessitating alternative approaches for retrieval supervision~\cite{dkr_qa}. Current RAG systems typically employ a powerful large language model (LLM) for generation~\cite{replug}, and a common strategy is to distill knowledge from the LLM into the retriever module. One effective approach is to concatenate each candidate document with the query and use the perplexity of the generated answer as a relevance signal~\cite{replug, re2g, ra-dit, end2end_reader_retriever_qa}. 

% However, RAG systems typically retrieve multiple documents as input, and existing independent document-level distillation methods fail to capture inter-document relationships. This limitation is particularly problematic for multi-hop reasoning tasks, where intermediate inference steps are required to determine the relevance of candidates~\cite{musique, 2wikihop}. Other methods have explored distillation using attention scores~\cite{dkr_qa}, yet attention distributions tend to be unreliable when handling a large number of candidate documents~\cite{focus_llm, focused_transformer}. Leave-one-out-based methods~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/Asai0H22} have also been proposed to evaluate document relevance, but they do not perform end-to-end fine-tuning and are not explicitly optimized to reduce the final language modeling loss of the RAG system. This misalignment creates a performance gap between retriever training and generation quality~\cite{DBLP:conf/acl/KeK00MB24}. 

% Therefore, an ideal retrieval fine-tuning approach should be end-to-end, explicitly optimizing for generation performance, and capable of modeling inter-document dependencies in multi-hop reasoning tasks.

\paragraph{Training the Reranking Module in RAG Systems}
The effectiveness of RAG systems relies heavily on the quality of retrieval and reranking ~\cite{DBLP:conf/naacl/GlassRCNCG22, DBLP:journals/corr/abs-2405-18414}. Traditional retrieval methods are based on lexical similarity ~\cite{DBLP:journals/ftir/RobertsonZ09}, while recent advances leverage dense vectors and transformer architectures~\cite{DBLP:conf/emnlp/KarpukhinOMLWEC20, DBLP:conf/sigir/KhattabZ20}. However, retrieval modules fine-tuned on public datasets often require additional adaptation for specific downstream tasks~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/sigir/SalemiZ24a}.

To bridge this gap, recent works explore fine-tuning retrieval and reranking modules for tasks such as open-domain question answering (ODQA). One common strategy distills knowledge from LLMs into retrievers by ranking candidate documents based on generated answer perplexity~\cite{DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/naacl/GlassRCNCG22, DBLP:conf/iclr/Lin0CSL00KSLZY24}. However, such methods overlook inter-document dependencies, crucial for multi-hop reasoning tasks~\cite{DBLP:journals/tacl/TrivediBKS22, DBLP:conf/coling/HoNSA20}. Alternative approaches use attention scores~\cite{DBLP:conf/iclr/IzacardG21} or leave-one-out methods~\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23, DBLP:conf/naacl/Asai0H22}, but these are not end-to-end optimized for generation quality, leading to a retriever-generation gap~\cite{DBLP:conf/acl/KeK00MB24}.

% An ideal retrieval fine-tuning approach should be end-to-end, explicitly optimized for generation, and capable of capturing inter-document dependencies.


% \paragraph{Stochastic k-Subset Selection and Masked Attention}
% Stochastic $k$-subset selection and top-$k$ relaxation have been actively studied in neural network optimization. Several works~\cite{ref26, ref5, ref39} have explored differentiable sampling methods for subset selection from categorical distributions, extending or generalizing the Gumbel-Softmax trick~\cite{ref23, ref15}. 

% In NLP, stochastic subset selection methods have significant applications. For instance, in the model pruning domain,~\cite{DBLP:conf/nips/FangYMHPK0W24} employs learnable sparsity to end-to-end optimize an optimal $N$:$M$ sparsity scheme. Another line of research focuses on differentiable relaxations of the top-$k$ operator, such as~\cite{DBLP:conf/nips/XieDCDZZ0P20}, which improves the differentiability of the top-$k$ operation through relaxation mechanisms. These methods have been instrumental in model interpretability, where instance-wise feature selection is achieved via learnable subset sampling~\cite{DBLP:conf/icml/ChenSWJ18}. 

% This observation motivates a key question: since reranking also involves instance-wise selection of the most relevant documents for a given query, can we model reranking using a similar subset sampling approach? However, in the context of reranking for RAG, we must also consider the structure of modern LLMs, where candidate documents influence generation through attention computation. Thus, an important challenge is how to introduce sparsity into the attention mechanism. 

% Common approaches involve inserting soft masks at different stages of attention computation, as explored in~\cite{DBLP:conf/naacl/FanGLWWJDZH21, DBLP:conf/cvpr/YangZNLLZT19}. In this work, we adopt a similar soft-mask-based strategy to model the reranking process in RAG as a subset sampling problem, thereby enabling end-to-end training of the reranking module.


\paragraph{Stochastic k-Subset Selection and Masked Attention}
Top-$k$ relaxation has been widely studied for differentiable subset sampling, extending the Gumbel-Softmax trick~\cite{DBLP:conf/iclr/JangGP17, DBLP:conf/ijcai/XieE19, DBLP:conf/nips/XieDCDZZ0P20}, with important applications in  semi-structured pruning~\cite{DBLP:conf/nips/FangYMHPK0W24}, model interpretability~\cite{DBLP:conf/icml/ChenSWJ18} and point clouds analysis~\cite{DBLP:conf/cvpr/YangZNLLZT19}.

The reranking process can also be modeled as a subset sampling problem. However, since retrieved documents influence LLM outputs through attention, a key challenge lies in introducing sparsity into the attention computation. Existing approaches employ soft attention masks to model discrete selections~\cite{DBLP:conf/naacl/FanGLWWJDZH21, DBLP:conf/cvpr/YangZNLLZT19}. Inspired by these methods, we model RAG reranking as a subset sampling process with soft masks, facilitating end-to-end optimization.



\begin{center}
\begin{algorithm*}
\caption{Gumbel Reranking: Training Reranker via Differentiable Masked Attention}
\label{alg:ranker_as_selector}
\begin{algorithmic}[1]
\Procedure{StochasticSubsetMask}{reranker $\mathcal{R}$, documents $\mathbf{d_1}, \dots, \mathbf{d_n}$, query $\mathbf{q}$, temperature $\tau$, scale factor $\kappa$, subset size $k$}
    \State $w_i = \mathcal{R}( \text{Concatenate}(\mathbf{q}, \mathbf{d}_i))\quad \forall i \in [n] \triangleq \{1, 2, \dots, n\}$  \Comment{Apply Reranker}

    \For{$j = 1$ \textbf{to} $k$} 
    \Comment{Stochastic Top-$k$ Sampling}
        \State $\tilde{w}_i = -\log(-\log(u_i)) + \kappa \cdot w_i,\quad u_i \sim \mathcal{U}(0, 1) \quad \forall i \in [n]$ 
        \State $\hat{\mathcal{M}}^{\mathcal{R},j} = \text{softmax}\left(\frac{\tilde{\mathbf{w}}}{\tau}\right), \quad \tilde{\mathbf{w}} = (\tilde{w}_1, \tilde{w}_2, \ldots, \tilde{w}_n)$
    \EndFor

    \State \Return $\max ( \hat{\mathcal{M}}^{\mathcal{R},1}, \dots, \hat{\mathcal{M}}^{\mathcal{R},k} )$ \Comment{Return Relaxed Top-$k$ Mask}
\EndProcedure
\State
\For{each $(\text{query } \mathbf{q}, \text{answer } \mathbf{a})$ in training data} \Comment{Training Loop}
    \State Retrieve $n$ documents $\mathbf{d_1}, \dots, \mathbf{d_n}$ using $\mathbf{q}$
    \State $\hat{\mathcal{M}}^\mathcal{R} = \operatorname{StochasticSubsetMask}(\mathcal{R}, \mathbf{d_1}, \dots, \mathbf{d_n}, \mathbf{q}, \tau, \kappa, k)$
    \State Apply $\mathcal{DMA}$($\hat{\mathcal{M}}^\mathcal{R}$) to obtain logits and language loss $\mathcal{L}_{LM}$  \Comment{\autoref{sec: DMA}}
    \State Update reranker $\mathcal{R}$ with $\nabla_{\mathcal{R}} \mathcal{L}_{LM}$ \Comment{Reranker Optimization}
\EndFor

\end{algorithmic}
\end{algorithm*}
\end{center}