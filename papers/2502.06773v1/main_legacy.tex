    %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}
% \usepackage{subfigure}

\usepackage{booktabs} % for professional tables
\usepackage{dsfont}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
% \usepackage{tcolorbox}
\usepackage{amsmath}
\usetikzlibrary{positioning} % for "below=..." and "right=..."

\usetikzlibrary{decorations.text}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}



% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025/icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{macro} % Import all the packages, enviornments and math commands here


\icmltitlerunning{On the Emergence of Thinking in LLMs I: Searching for the Right Intuition}

\begin{document}

\twocolumn[
\icmltitle{On the Emergence of Thinking in LLMs: I \\ {\em Searching for the Right Intuition}}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text. Commented for submision


\begin{abstract}
Recent advancements in AI, such as OpenAI's new o models, Google's Gemini Thinking model, and Deepseek R1, are transforming LLMs into LRMs (Large Reasoning Models). Unlike LLMs, LRMs perform {\em thinking or reasoning} during inference, taking additional time and compute to produce higher-quality outputs. 
This work aims to discover the algorithmic framework behind  training LRMs. 
Approaches based on self-consistency, process reward modeling, AlphaZero, highlight that reasoning is a form of guided search. Building on this principle, we ask: what is the simplest and most scalable way to implement search in the context of LLMs?  
%How does this differ from systems like Alpha-Go or Alpha-Proof?

Towards answering these questions, we propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). 
RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning {\em process}, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an {\em outcome verifier} to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.

%We provide a mathematical justification for RLSPâ€™s suitability over other approaches for implementing search in the context of LLMs. Testing on math and coding tasks, models trained with RLSP framework demonstrated improved reasoning abilities, with performance improvements of 25\% in math datasets and a pass rate increase from 10\% to 30\% in coding tasks. 
%RLSP enables LLMs to effectively search for problem-solving strategies, showing potential for complex reasoning when scaled appropriately.

We perform empirical studies of the RLSP framework in the math domain, and show that
the models trained with the RLSP framework demonstrated improved reasoning abilities.
On Llama-3.1-8B-Instruct model the RLSP framework can boost performance by 23\% in MATH-500 test set;  
On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10\% due to RLSP technique.

However, the more important finding of this work is that the models trained using RLSP technique, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification.
These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled appropriately.


Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs compared to previous approaches considered in the literature, inspired by a remarkable recent result that says that CoT {\em provably} increases computation power of LLMs, and hence reasoning, and these abilities grow as the number of steps in CoT \cite{li2024chain,merrill2023expresssive}. 

%First, we give a mathematical justification as to why RLSP search strategy is more suitable for LLMs than other approaches considered in the literature.
%Next, we perform empirical studies of the RLSP framework in math and coding domains, and show that
%the models trained with the RLSP framework demonstrated improved reasoning abilities.
%On Llama models the RLSP framework can boost performance by 25\% on the MATH dataset; 
%in the coding domain, the pass rate of our model increased from 10\% to 30\% on the CodeContests dataset. 
%Besides performance improvements, we study  emergent behaviors and transfer learning properties of the RLSP process.
%Our empirical findings on all these fronts show that the RLSP framework might be enough to enable complex reasoning abilities in LLMs when scaled appropriately.
%More importantly the RLSP framework enables the model to search for the right problem solving strategies.
\end{abstract}

\iffalse
\begin{figure}[htbp]
\vskip -0.1in
\begin{center}
 \centerline{\includegraphics[width=\columnwidth]{figs/reward_resp_len_acc2.pdf}}
\caption{Reward, response length and AIME24 accuracy during RL training with the PPO algorithm using the simplest creativity reward: reward thinking more. 
The increase in response length is not sufficient but necessary for search behavior and reasoning, even in a strict theoretical sense \cite{merrill2023expresssive}. The base model is Qwen2.5-32B-Instruct; No SFT or special prompting were employed during training and inference.}
\label{fig:qwen_32b_aime}
\end{center}
\vskip -0.2in
\end{figure}
\fi

\input{body}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\bibliography{ref}
\bibliographystyle{icml2025/icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
