\section{Related Work}
\label{sec:related-work}

In this section, we give a detailed literature survey.
%Not to be influenced by all the latest work that was made publicly available since 1-Jan-2025 on our methodology, we consider all those works as concurrent works, and we do not compare our results to those techniques. 

\paragraph{Large Reasoning Models}
OpenAI’s o1 series \cite{openai-o1-mini, openai-o1, jaech2024openai} and other reasoning models \cite{gemini-2,deepseek-r1, qwq} represent a significant leap in reasoning capabilities, excelling in structured reasoning, systematic problem decomposition, and reliable handling of complex tasks. 
%Their development relies on large-scale reinforcement learning with chain-of-thought (CoT) reasoning, coupled with optimized training and test-time computation strategies. 
Testing is conducted on high-stakes benchmarks, including mathematics \cite{hendrycks2021measuring,gsm8k,aime24,amc23,math500}, competitive programming \cite{jain2024livecodebench,humaneval,codeforces}, and scientific problem-solving \cite{hendrycks2020measuring,rein2023gpqa}, often achieving performance levels that surpass human experts.

Recently, numerous open-source frameworks strive to replicate o1’s reasoning capabilities through diverse methodologies. At the post-training stage, frameworks such as \cite{wang2024openr, zhang2024rest, zhang2024llama} utilize automated data augmentation with MCTS, while \cite{huang2024o1, huang2025o1replicationjourney, skyt1, min2024imitate} exploit reasoning traces in long-CoT data. Process reward models are integrated into the training process by \cite{cui2024process, guan2025rstar}, boosting self-exploration. 
\cite{kimi-k15} introduces an effective RL framework emphasizing long-context scaling and robust policy optimization and incorporates with techniques for distilling long CoT reasoning into more efficient short-CoT models.
Meanwhile, \cite{deepseek-r1} highlights the potential of reinforcement learning in driving reasoning improvements on very large base models such as Deepseek V3 without process reward models or MCTS.
During inference, \cite{wang2024openr} employs majority-vote and beam search strategies, while \cite{zhang2024rest, qin2024o1, guan2025rstar, jiang2024technical} incorporate tree search-based inference. Additionally, \cite{zhang2024llama} emphasizes pairwise preference ranking and advanced exploration techniques, further enhancing inference performance.



\paragraph{Scaling Test-Time Compute}
Scaling test-time compute enhances reasoning capabilities by allocating more computational resources during inference. The test-time scaling laws demonstrate that increased deliberate reasoning (e.g., through additional token generation or iterative steps) directly improves accuracy, especially in complex tasks like mathematics, coding, and planning \cite{snell2024scaling,wu2024empirical,brown2024large,beeching2024scalingtesttimecompute}.
Recent work on simple test-time scaling \cite{muennighoff2025s1} shows that even small models can achieve significant improvements in reasoning tasks by strategically allocating inference budgets. In addition, RL-based scaling approaches \cite{hou2025advancing} show that inference scaling trends becomes more evident as training scales, reinforcing the connection between RL and test-time compute.

Various test-time search methods exploit this principle \cite{kang2024mindstar,wang2024q}. Majority vote aggregates predictions from multiple inference traces to refine accuracy. Tree search methods such as \cite{yao2024tree,hao2023reasoning,zhang2024accessing,qi2024mutual} systematically explore reasoning paths. Beam search \cite{smoke1961program} leverages the PRM and retains top-K paths at each step to improve sequence generation. Lookahead search \cite{snell2024scaling} further enhances exploration depth by evaluating paths using simulations. While these methods improve reasoning accuracy, they increase computational demand, highlighting the trade-off between performance and resource efficiency.
% Todo: s1: simple test-time scaling (https://arxiv.org/abs/2501.19393)
% Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling (https://arxiv.org/pdf/2501.11651)


\paragraph{Math Data with Long Reasoning Traces}

Research indicates that combining System 2-inspired deliberate reasoning with System 1’s fast, heuristic-based thinking significantly enhances reasoning performance \cite{su2024dualformer}. 
To equip language models with System 2 reasoning techniques, recent advancements have explored approaches such as supervised fine-tuning (SFT) with extended chain-of-thought (CoT) reasoning \cite{bai2024longwriter,abdin2024phi,min2024imitate,huang2024o1,qin2024o1,wang2024enhancing,xu2024llava}. 
Training datasets with long reasoning traces, often distilled from advanced models like GPT \cite{li2024numinamath,luo2023wizardmath, yu2023metamath} and Qwen-qwq \cite{qwq-longcot}, are critical for fostering complex reasoning capabilities. 
To address the limitations of generator models, exploration-based techniques like MCTS \cite{guan2025rstar,zhang2024rest} and rejection sampling \cite{yuan2023scaling,brown2024large} systematically enhance reasoning by expanding decision paths. 
These methods enable language models to improve reasoning abilities and generate high-quality solutions to challenging problems, surpassing the constraints of their training data.

Recent studies further support the effectiveness of long CoT data. \cite{simplerl} demonstrates that reinforcement learning on long CoT-augmented data enables small models to exhibit emergent reasoning capabilities with significantly fewer examples. Similarly, \cite{open-r1} highlights that long CoT SFT leads to substantial improvements in problem-solving accuracy across mathematics and programming tasks. Furthermore, \cite{yeo2025demystifying} systematically investigates the factors driving long CoT emergence, revealing that while long CoT is not strictly necessary for reasoning ability, it substantially enhances training efficiency and stabilizes reinforcement learning by providing structured, verifiable reasoning trajectories. 
Meta-CoT \cite{xiang2025towards} provides a theoretical foundation for why long reasoning traces may enhance reasoning abilities. It argues that standard CoT fails to fully capture complex, non-linear thought processes and that explicitly modeling latent reasoning steps improves performance in high-difficulty tasks.



\paragraph{Self-Correction in Language Models}

Self-correction in LLMs has gained significant attention as a mechanism to enhance reasoning and problem-solving abilities. A range of techniques has been explored, spanning from fine-tuning methods to advanced reinforcement learning strategies. 
Fine-tuning approaches leverage curated data to train models for iterative corrections, improving their ability to refine responses \cite{min2024imitate, qu2024recursive, zhang2024small}. 
Prompt-based approaches focus on eliciting better outputs through iterative feedback loops \cite{huang2023large}, while inference strategies like MCTS are employed to refine reasoning by exploring diverse solution paths \cite{zhang2024accessing, yang2024qwen2, tian2024toward, wang2024towards}. 
Reinforcement learning methods such as Self-Correction via Reinforcement Learning (SCoRe) \cite{kumar2024training}, enhance self-correction through multi-turn RL using intrinsic rewards to guide learning.

While extrinsic feedback mechanisms remain effective, intrinsic self-correction—where models refine their outputs without external input—has proven challenging yet promising. Notable advancements include reward-based RL \cite{yang2024qwen2, choi2024self, kumar2024training} and curriculum preference learning \cite{wang2024towards}, which enable iterative refinement of reasoning steps. 
Unlike these approaches, we observe that self-correction behavior emerges naturally through unsupervised RL guided solely by a length penalty signals. This suggests that our method can be easily adapted to other domains without the need for domain-specific agent design or reward hacking.


\paragraph{Reinforcement Learning with Auxiliary rewards}

Reinforcement Learning (RL) with auxiliary rewards improves policy optimization by incorporating pseudo-reward signals that guide learning beyond task completion. 
The foundational work by Jaderberg et al. \cite{jaderberg2016reinforcement} uses auxiliary control and prediction tasks to accelerate learning. Recently, RL has been applied to enhance reasoning in large language models (LLMs). 
\cite{havrilla2024teaching} trained LLMs with outcome-based reward models (ORMs) and utilized a dense reward signal derived from comparing partial solutions to reference solutions.
\cite{setlur2024rewarding} introduced process advantage verifiers (PAVs), which assign stepwise rewards based on changes in correctness probability judged by a stronger prover policy.
\cite{luo2023wizardmath} employed process-supervised RL by distilling a process reward model (PRM) from GPT-4 annotations. In contrast, our work introduces a dense exploration reward that does not require a reference solution, making it more unsupervised and adaptable. Furthermore, rather than distilling PRM from GPT-4, we propose a more general approach to measuring creativity and reasoning effort, enhancing applicability across diverse domains.