\clearpage

\input{figs/main_examples}

\input{figs/exploration_example}



\clearpage

\section{Introduction}



With the release of o-series of models from OpenAI \cite{openai-o1-mini,openai-o1}, Gemini Thinking model from Google \cite{gemini-2}, and Deepseek R1 \cite{deepseek-r1}, LLMs are rapidly evolving into thinking machines,  now referred to as LRMs (Large Reasoning Models).
The key technical difference between LLMs and LRMs is the ability of LRMs to do {\em thinking} during the inference, which we define as the ability to take more time and compute during (inference) with the goal of producing a higher quality output to a given input, 
which is a fair definition of thinking process in all systems capable of reasoning both artificial and biological.

The main goal of this work is to discover the computational process behind reasoning or thinking in the context of LLMs.
There have been several attempts in the past towards understanding this process, ranging from techniques such as self-consistency \cite{wang2022self, liu2023pre}, (automated) process reward modeling (PRM) \cite{uesato2022solving,lightman2023let,snell2024scaling,beeching2024scalingtesttimecompute}, and adapting AlphaZero style search for LLMs \cite{feng2023alphazero,chen2024alphamath,trinh2024alphageometry,alphaproof}. 
All of these techniques have one principle in common: reasoning or thinking is {\em some form of search}.
We build on the principle of defining thinking as a form of guided search and ask the question:

\smallskip
{\em What is the simplest and most scalable framework for training LLMs that leads to the emergence of thinking or search behavior?}


\subsection{Our Contributions}

\paragraph{The RLSP Framework}
Towards answering these questions, we propose a {\em post-training} framework called Reinforcement Learning via Self-Play (RLSP)\footnote{RLSP name is inspired by the quote "Play is the highest form of research'' attributed to Albert Einstein and the self-play technique used in AlphaZero systems}.
Our framework is a natural extension and generalization of the RLHF \cite{ouyang2022training} and RL from Constitutional AI feedback \cite{bai2022constitutional} frameworks and consists of three simple steps: 

% 
\begin{itemize}

\item \textbf{Step 1:} If high-quality demonstrations of the {\em process of thinking} are available, either via human annotations or synthetic traces constructed via tree search procedures, do SFT (supervised fine-tuning) on the demonstration dataset.
% 

\item \textbf{Step 2:} Use an {\em exploration reward} that is {\em independent} of the correctness of the solution to
implicitly encourage {\em diverse} search behaviors such as  backtracking,   consideration of alternative possibilities, verification, etc. 
% 

\item \textbf{Step 3:} Do RL (reinforcement learning) with PPO (proximal policy optimization, \cite{schulman2017proximal}) as the training algorithm, using an {\em outcome verifier} that gives an unambiguous {\em binary signal of the correctness} of the solution.
% 
\end{itemize}

In this work, we focus on domains where outcome verification is possible (and easier) during training.
Our key insight is that any reward engineering in RL training should encourage the model to synthetically create {\em novel} CoT data that it can learn from during the PPO training; that is, it incentivizes self-play over new CoT reasoning traces.
A simple way to implement this is to decouple the exploration reward signal that encourages search behavior from the correctness of the solution, and incentivize it.
During PPO training, we do a careful weighing of the score from the outcome verifier and the score from exploration reward model for the optimal performance.
By design, both  components of the reward signal mitigate issues such as reward hacking and overfitting to the training data.
More importantly, both signals provide minimal but essential feedback for the model to learn better with scale consuming more compute and data.

In \autoref{sec:theory} we propose an argument supporting these intuitions and how we arrived at the RLSP framework guided by some remarkable recent results \cite{li2024chain,merrill2023expresssive}.


\paragraph{Empirical Performance Analysis: {\em RLSP search leads to better reasoning in LLMs.}} 

\begin{figure}[htbp]
% \vskip -0.1in
\begin{center}
 \centerline{\includegraphics[width=0.7\columnwidth]{figs/reward_resp_len_acc2.pdf}}
\caption{Reward, response length and AIME24 accuracy during RL training with the PPO algorithm using the simplest exploration reward: reward thinking more. 
The increase in response length is not sufficient but necessary for search behavior and better reasoning, even in a strict theoretical sense \cite{merrill2023expresssive}. The base model is Qwen2.5-32B-Instruct; no SFT or special prompting were employed during training and inference.}
\label{fig:qwen_32b_aime}
\end{center}
\end{figure}

We empirically evaluate the improvement in reasoning abilities of models trained using the RLSP framework in the math domain.

On Llama models, the RLSP framework can boost performance by 23\% on the MATH dataset \cite{hendrycks2021measuring}; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10\% due to RLSP technique as shown in Figure~\ref{fig:qwen_32b_aime}. 

Moreover, under the {\em same token budget}, the self-consistency (SC) accuracy of
Llama-3.1-8B-Instruct on MATH-500 is 61.8\%, whereas the same model trained via RLSP achieves 73.4\%.
%See \autoref{sec:tokenefficiency} for details.
Our empirical findings show that enabling guided search behavior leads to significant gains in the reasoning abilities of LLMs compared to CoT and SC.



\paragraph{Emergent Properties and Behavioral Analysis}
The most interesting contribution of our work is understanding the emergent properties of models trained via RLSP, and the various ablation studies we perform to delineate the role played by SFT, RL with or without exploration reward, and the impact of base models and the pretraining data.


We show that even with no SFT but equipped with minimal exploration reward, such as incentivizing longer generations that explicitly output all intermediate steps of a reasoning trajectory, {\em all} models in {\em both} coding and math domains learn several interesting search behaviors. 
They show various emergent properties such as exploring alternative possibilities, cross-verification, backtracking, and self-correction, which are some of the well-known meta search strategies.
See \autoref{fig:llama_emergence_example}, \autoref{fig:qwen_emergence_example}, and \autoref{fig:emergence_example}; \autoref{sec:app-rlsp-emergence} contains several more examples of emergent behaviors of models  just using the generation length reward.

On the other hand, if there is no exploration reward but pure PPO training based on binary reward from the verifier, search behavior {\em only} emerged in Qwen2.5-7B-Instruct model on math domain (and not in coding domain for the same base model).


These results indicate, not surprisingly, that emergent properties of the models trained via pure RL can vary widely based on the pretraining data and the choice of base models. 
In contrast, {\em RLSP enables emergent  search behaviors across multiple model families, sizes, and
domains.}
Thus, to train frontier models at scale, RLSP offers a smoother and more efficient framework to equip LLMs with sophisticated search behaviors that can lead to improved reasoning.
%See \autoref{fig:stagesofreasoing} for a highlight.


\paragraph{Remark 1} A word about our choice of words. We use the phrase "thinking behavior'' to mean the ability of LLMs to take more time and compute with the goal to produce a higher quality output to a given input, use it interchangeably to mean search behavior or reasoning. This is based on our hypothesis that search enables thinking which in turn should lead to better reasoning. However, technically speaking, search/thinking behavior is necessary for reasoning \cite{li2024chain,merrill2023expresssive} but not sufficient. 

\paragraph{Remark 2} We call a behavior emergent if the model does not exhibit that behavior in regular CoT without any special prompting techniques. 
However, it should be noted that the behaviors we call emergent in this paper such as backtracking or self-correction are implicitly present in pretraining data, although it is less frequent than standard step-by-step reasoning. 
See \autoref{sec:theory} and the discussion in \autoref{sec:method_ppo} for some mathematical justification of why standard CoT or pure RL may not always show behaviors such as backtracking or self-correction.
Finally, we also notice that emergent behaviors, although in principle should lead to better reasoning, may not always result in correct solutions; see \autoref{sec:app-rlsp-emergence} for an example.
We speculate that this could be due to small model sizes, data, and compute we use in our experiments.

\paragraph{Remark 3} We are aware of several other works that are published in the last 2-3 weeks, including the Deepseek r1 report, which have  findings that are related to our work.
This is a concurrent work (that is already under ICML submission) and hence we do not compare to those very recent papers.
However, we believe that our work is more principled and has many new findings.


\subsection{A Theory of RLSP} 
\label{sec:theory}
We propose a theory to explain how we arrived at the RLSP search strategy and how it can lead to continuous self-improvement. 

The guiding principle behind RLSP is that any RL training technique should incentivize the model to synthetically create {\em novel} CoT reasoning trajectories that are not already present in the training data, and learn from it.
Our intuition comes from a recent elegant mathematical result that states CoT can {\em provably} enhance the computational power of transformers \cite{li2024chain,merrill2023expresssive}.
Broadly speaking, \cite{merrill2023expresssive, li2024chain} argument says that the {\em length} of  chain-of-thought trace impacts its reasoning power, and more intermediate steps lead to more computational power under standard computational complexity assumptions.
Recall that CoT is an empirical implementation of 
$\arg \max P_\theta(\text{answer, rationale}~|~\text{problem})$ 
for an autoregressive LLM parameterized by $\theta$. 


Suppose we assume that as the difficulty of a reasoning problem increases, arriving at the {\em right rationale or intuition} necessary to solve the problem becomes harder.
In particular, commonly occurring ideas in the pretraining data, which is what standard CoT finds, fail to lead to the correct solution.
Then it is natural to train the model to {\em search} over the space of rationales thus maximizing $P_\theta(\text{correct rationale, trajectory over rationales}~|~\text{problem})$, which can be interpreted as CoT in the space of trajectories over rationales.
However, such trajectories may not be present in training data.
Here, it is beneficial to think of settings where the problems are so difficult that no human can solve it. 
Taking cue from \cite{merrill2023expresssive, li2024chain},  we can still design reward signals that encourage the model to use more intermediate steps as the problem difficulty increases and explore diverse rationales to solve the problem.
This was our motivation to design exploration reward signal in RLSP.
During the RL training most of the reasoning trajectories fail to lead to the correct answer; yet the model gets a small reward for the exploration. 
However, when it eventually finds the correct answer via a long reasoning trajectory, it gets the full reward.
Thus, exploration and response length reward signals in RLSP encourage the model to generate all intermediate steps (CoT) to arrive at a solution, thus synthetically creating the CoT trajectories to learn from during PPO.

In nutshell, {\em RLSP enables models to generate new CoT data via self-play.}
Since we already know that CoT improves reasoning abilities of LLMs, RLSP in principle can keep improving the reasoning abilities as long as there are sufficiently diverse new problems to solve!

These discussions should also give an intuition to the reader why an SFT dataset over the demonstrations of reasoning traces can only help the model during RL training, and in fact can be viewed as created by some search process either by humans or other techniques such as tree search.
Thus, SFT over reasoning traces and RL should be considered as working together to instill search behavior in LLMs.
However, in the limit, RL training with exploration and outcome reward signals enables the models to continuously self-improve by creating synthetic CoT traces.

Finally, we note that many of the emergent behaviors of models trained using RLSP give some validation to this theory. For example, in \autoref{fig:emergence_example} the model searchers over multiple rationales (and within each rationale it does CoT) before verifying that all of them lead to the same answer.
Similar behaviors are also seen in other examples given in \autoref{sec:app-rlsp-emergence}.






\section{Details of the RLSP Framework}

\begin{figure}[htbp]
\centering
\resizebox{0.5\columnwidth}{!}{%
    \input{figs/circle}
}
\caption{A possible training paradigm for the emergence of complex reasoning process. In this work, we propose RLSP framework to enable the thinking process.}
\label{fig:stagesofreasoing}
\end{figure}



In this section, we describe full RLSP framework that we used to train our best reasoning models.
In \autoref{sec:can_pure_rl_lead_to_thinking}, we do ablation studies to understand how individual steps of RLSP change the thinking behavior of models, and show why all 3 steps may be the smoothest way to empower search behavior in LLMs. However, SFT step may be optional with proper reward engineering during RL stage.

RLSP is a post-training framework for LLMs in similar spirit as RLHF. While RLHF encourages instruction following and teaches human preferences, RLSP encourages reasoning in model responses.
We give a meta hierarchy for model training in \autoref{fig:stagesofreasoing} which shows how each of these frameworks encourage different abilities in language models and what emergent abilities can future frameworks achieve.
Our framework consists of following steps.

\subsection{SFT of the Base Model}\label{sec:method_sft}
In the first step of RLSP framework, we perform a supervised fine-tuning using cross-entropy loss.
A high-quality SFT dataset should contain demonstrations of the {\em thinking process} that incorporates typical reasoning principles such as backtracking, abandoning a reasoning chain, self-verification, etc.
We note that SFT dataset need not be exhaustive in terms of all possible reasoning traces; 
In simpler terms, this step can be thought of as studying chapters in a textbook or attending lectures on a particular topic before asking the student to solve homework problems.
In fact, this step is not even necessary, but helps in  (RL) training models that exhibit better search behavior as we will see in coming sections.

SFT datasets can be constructed through various methods: 1) Human demonstrations. 2) Depth First Search (DFS) traversal of MCTS or other tree search techniques 3) Synthetic data via agentic workflows or using a thinking model that is already trained. 
Although the quality of data produced by each of these steps can vary, with proper curation one could hope to yield high-quality demonstration data.
Extensive research on the relative efficacy of these techniques is beyond the scope of this paper, and we leave it as an open problem.
In our experiments, we create an SFT dataset using a filtered version of publicly available reasoning traces QwQ-LongCoT \cite{qwq-longcot}.


\subsection{Reward Function}\label{sec:method_creativity_reward}

The most important component of RL training is establishing the reward function $\mathcal{R}$. Suppose for a given prompt $q$ the model outputs $o$. Suppose we have an outcome verifier $\mathrm{Ver}$ which objectively decides whether the model response $o$ given prompt $q$ is correct or incorrect (which can be done in math domain if we know the final answer for a problem and in coding domain by running the code on a few test cases). We will also utilize an \emph{exploration reward} $\mathcal{R}_{\mathrm{ex}}(q,o)$ which judges the effort and creativity shown by the response $o$ in answering the prompt $q$.
During training, the output reward signal $\mathcal{R}(q,o)$ consists of two components:
\begin{align}
    \mathcal{R}(q,o) = \alpha \cdot \mathds{1}\left[\mathrm{Ver}(q,o) =\mathrm{True}\right] + (1-\alpha) \cdot \mathcal{R}_{\mathrm{ex}}(q,o).
\label{eq:reward}
\end{align}
The key insight in our work compared to most previous approaches based on PRM is to give a reward signal independent of the output correctness that encourages desirable properties of the process of reasoning.
This is related to concept of auxiliary rewards in RL, we refer the reader to \cite{jaderberg2016reinforcement} and references therein.
Unlike PRM, the exploration reward does not {\em directly} measure the progress the policy model is making towards the outcome, but rather meta-properties of the reasoning trajectory that increases the success probability, as discussed in \autoref{sec:theory}.
Thus, we reward the process itself instead of process reward modeling (PRM). 
Moreover, in the beginning of RL training, when the reward signal from the outcome verifier is very sparse, the exploration reward serves as a dense signal guiding the model towards longer and better reasoning trajectories.

To prevent reward hacking of the exploration reward signal, we carefully balance the correctness signal from the outcome verifier and exploration reward.
In practice $\alpha$ is a hyperparameter that needs to be tuned  and decayed over time,  but we set it as 0.8 in our experiments for simplicity; 
that is bulk of the reward signal comes purely from the outcome verifier which makes the learning process more challenging but enables better generalization.

In our experiments, we do not train an exploration reward model; instead, we experiment with two simple ways to assign this exploration reward:

\begin{itemize}

\item {\em Based on length of response.} That is, $\mathcal{R}_{\mathrm{ex}}(q,o)=-\frac{C}{|o|}$ where $|o|$ is the number of tokens in the response and $C>0$ is some constant. So longer responses, which explicitly output all the intermediate steps taken to arrive at a solution, get a higher reward. This is based on the intuition that for difficult problems the reasoning effort is correlated with length of the model response, which is made precise by the work of \cite{merrill2023expresssive}.
During the discussion of PPO training (see Eq. \eqref{eq:whyexplore}), we will see another motivation for adding this reward explicitly.
Just as importantly, this is a simple and objective reward to implement.

As we saw earlier, even with this simple exploration reward, models show several emergent behaviors after PPO training \autoref{fig:emergence_example}, \autoref{fig:qwen_emergence_example}, \autoref{fig:llama_emergence_example}.
However, in many cases, this exploration reward signal is not enough as the models learn to hack the reward signal by repeating responses, necessitating a more nuanced reward signal.


\item {\em By using LLM-as-a-judge} to score $(q,o)$ based on the creativity, reasoning effort and other merits as judged by an independent LLM (while ignoring the correctness of the response). In our paper, we use GPT-4o model as the judge, see the prompt we use in \autoref{sec:prompts}.
We note that we use GPT-4o model as a judge as our base models are rather small; however, if the base model is a frontier model then the policy  and the judge models need not be different. 


\end{itemize}

One could use constitutional AI approach to train an exploration reward model using RL \cite{bai2022constitutional}, which can be more easily adapted to new reasoning domains. 
Here, one can formulate a constitution of what constitutes novel or creative reasoning process for that specific domain, and steer the model towards those desired behaviors.
We leave this as an interesting future research direction.


\subsection{RL Training with PPO}\label{sec:method_ppo}
We use the PPO algorithm \cite{schulman2017proximal} to train our policy model.
Our implementation of the PPO algorithm for training the policy is similar to the setup used in RLHF fine-tuning using PPO \cite{ouyang2022training}. Suppose the model response can be decomposed into tokens as $o=o_1,o_2,\dots,o_T$ with $o_T=\mathrm{EOS}$ being the end-of-string token.
The loss function in PPO is given by:
\[
L^{\text{PPO}}(\theta) = \mathbb{E}_t \left[ \min\left(\rho_t(\theta) \hat{A}_t, \text{clip}(\rho_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right) \right]
\]
where $\rho_t(\theta)=\frac{\pi_\theta(o_t|q,o_{<t})}{\pi_{\theta_{\mathrm{old}}}(o_t|q,o_{<t})}$ and $\hat{A}_t$ is the GAE estimate for the advantage function given by:
$\hat{A}_t=\delta_t+(\gamma \lambda)\delta_{t+1}+\dots+(\gamma\lambda)^{T-t} \delta_{T}$ where $\delta_t=r_{t} +\gamma V_\phi(q,o_{<(t+1)}) - V_\phi(q,o_{<t})$ and $r_t$ is the per-token reward at step $t$ and $V_\phi(q,o_{<t})$ is the value function predicted by critic model which is trained along with the policy model using squared loss with target given by reward-to-go $\hat{R}_t=r_t+\gamma r_{t+1}+\dots+\gamma^{T-t}r_T$. We use the implementation of PPO from OpenRLHF framework \cite{hu2024openrlhf} for our experiments and set $\gamma=1$ and $\lambda=0.95$. 
As in previous works, we also use a KL penalty term to stop the model from drifting too far from the base SFT model ($\pi_{\theta_{\mathrm{SFT}}}$). 
Therefore the per-token reward is given by:
$$r_t = \mathcal{R}(q, o)\cdot \mathds{1}(o_t=\mathrm{EOS}) - \beta \log\left(\frac{\pi_{\theta_{\mathrm{old}}}(o_t|q,o_{<t})}{\pi_{\theta_{\mathrm{SFT}}}(o_t|q,o_{<t})}\right).$$
Note that the expected total reward is 
\begin{align} 
&\mathbb{E}_{o \sim \pi_{\theta_{\text{old}}}(\cdot|q)}\left[\sum_{t=1}^T r_t\right] =  \mathbb{E}_o[\mathcal{R}(q,o)] 
 - \beta \sum_{t=1}^T D_{\mathrm{KL}}\left(\pi_{\theta_{\text{old}}}(\cdot|q,o_{<t}) \|\ \pi_{\theta_{\text{SFT}}}(\cdot|q,o_{<t})\right). \label{eq:whyexplore}
\end{align}
Since the KL divergence at each step is non-negative, the KL penalty term leads to lower reward for longer responses which could prevent the model from learning to produce long reasoning responses. {\em The exploration reward compensates this by rewarding longer responses.} In our experiments, adding the simple length based exploration reward consistently leads to longer responses with detailed CoT and better reasoning behavior in the model. But without the exploration reward, it works for some settings and doesn't work for some others.


\section{Can Pure RL Lead to Thinking Behavior?}
\label{sec:can_pure_rl_lead_to_thinking}
An immediate question that arises from the RLSP framework is: Are SFT and exploration reward signals necessary or can LLMs learn to search directly with pure PPO training on the binary output of a verifier?
Recall that \autoref{eq:whyexplore} sheds some light on this question in a theoretical sense.
To empirically answer these questions, we perform following set of experiments on Llama and Qwen models.
In these experiments we use response length as an objective metric to measure the search behavior.
We will not concern ourselves with accuracy in this section. 

\paragraph{Llama  Models}
Starting from Llama-3.1-8B-Instruct \cite{llama-31-8b} as the base model on which we perform no SFT, we do a) PPO training on the training dataset of MATH with no exploration reward of any kind; that is $\alpha = 1$ in Eq. \eqref{eq:reward}.
b) PPO training on the training dataset of MATH with the creativity reward proportional to length: $R_{cr}(q,o) \propto-\frac{1}{|o|}$.
Our findings are in \autoref{fig:llama_response_len}.

\input{figs/llama_plot}


Our results indicate that on Llama-3.1-8B-Instruct models pure RL with only outcome reward does not lead to search behavior. 
It is also reflected in the solutions produced by the model, which show no emergent search behavior, see \autoref{fig:emergence_example}.
On the other hand, even when we give a weak signal of exploration reward that is proportional to the response length, the model exhibits several interesting search behaviors; see \autoref{fig:llama_emergence_example},  \autoref{fig:qwen_emergence_example}, \autoref{fig:emergence_example} and \autoref{sec:app-rlsp-emergence}.
{\em We want to emphasize that the increase in response length in this case is expected but what is surprising is the emergence of new search behaviors such as backtracking and verification, which models do not seem to perform in standard CoT.}


\paragraph{Qwen Models}
We also carried out a similar experiment with Qwen2.5-7B-Instruct model in math domain.
The result is presented in \autoref{fig:qwen_result_math}.

\input{figs/qwen_plot}
Interestingly, we observe that Qwen2.5-7B-Instruct model does demonstrate an increase in the response length with pure RL training using only outcome reward.
Thus the base model and the pretraining data used for training them can have significant impact on the emergent behaviors.

We next ask if this behavior is specific to the math domain or the Qwen2.5-7B-Instruct model would also exhibit this behavior in other reasoning domains such as coding.
Our experiments in the coding domain are in \autoref{fig:qwen_result_code}.

\input{figs/qwen_plot2}


We note that as opposed to the math domain, {\em in coding domain Qwen2.5-7B-Instruct model does not lead to search behavior with pure RL training using only outcome reward.}
On the other hand, as expected, reward with the response length leads to an increase on the search behavior. 
Thus, these ablation studies indicate that some form of explicit reward to encourage the thinking process may be more efficient and scalable way to enable search behavior in LLMs overall.



\section{Empirical Evaluation of RLSP}
\label{sec:empirical}
Having established that RL training even with simple exploration reward signal can lead to emergent search behaviors, in this section we evaluate the impact of full RLSP training on model performance within the math domain.
Our main goal in this section is to establish that search behavior leads to improved problem solving abilities.

Our experiments focus on two different model types with varying sizes: an 8B parameter model and a larger 32B parameter model.
Each model is trained and evaluated on a corresponding dataset tailored to its size, enabling a comprehensive analysis of the effectiveness of RLSP across different model scales.
The results are presented in two subsections, each dedicated to a specific model and dataset configuration.
The general experimental settings are provided in \autoref{sec:exp-settings}, while the specific hyperparameters for each experiment will be detailed in their respective sections.

We remark that our goal is not to compare against the SOTA performance of models on these benchmarks but {\em relative} improvement due to RLSP. 
Therefore, in all our experiments we compare the performance of same model trained using various strategies.
We did not optimize prompts and various other factors that can influence the performance of models both in base models and in RLSP training.
All our experiments are conducted on a cluster with 6 nodes (48 H100 GPUs), though most of our experiments utilize 24 H100 GPUs. 
Despite being performed at a small scale with only a few thousand training samples and iterations, our empirical evaluation provides strong evidence that RLSP can yield substantial gains when scaled up.

\subsection{Performance Analysis: 8B Model}
\label{sec:empirical_8b}

\subsubsection{Setup}
For this experiment, we use Llama-3.1-8B-Instruct \cite{llama-31-8b} as our base model.
As observed in \autoref{sec:can_pure_rl_lead_to_thinking}, while pure RL with an exploration reward signal encourages thinking behavior, the overall performance remains suboptimal due to the model's moderate size and limited capabilities.
Therefore, we conduct additional SFT on a math-specific dataset that includes long Chain-of-Thought (CoT) reasoning traces \cite{qwq-longcot}, sourced from the QwQ-32B-preview model \cite{qwq-32B-preview}.
To ensure the integrity of our evaluation, we decontaminate the dataset to remove any overlap with a wide range of test datasets.
Further details on the dataset curation process are provided in \autoref{sec:app-dataset-curation}.
Hyperparameters for the fine-tuning is reported in \autoref{sec:appendix_empirical_8b_sft_hyperparams}.

During the RL stage, the SFT model is trained with PPO on the training set of the MATH dataset \cite{hendrycks2021measuring}, utilizing only the (problem, answer) pairs. 
Problems from the training set that are already correctly solved by the SFT model are excluded. 
Details on the hyperparameters used for RL training are provided in \autoref{sec:appendix_empirical_8b_rl_hyperparams}.

For RLSP training, we use structured prompts with the GPT-4o model \cite{gpt-4o} to generate exploration reward scores. 
Further details about the prompts can be found in \autoref{sec:prompts}.

For evaluation, we measure the model's accuracy on the 500 test splits of the MATH dataset (a.k.a. MATH-500). 


\subsubsection{Evaluation Results}
\autoref{tab:math_performance_8b} presents the results of this section.

% \vskip -0.1in

\begin{table}[ht]
\caption{Performance comparison of RLSP training for MATH dataset. Base model is Llama-3.1-8B-Instruct. SFT model is the fine-tuned version of the base model with long Chain-of-Thought (CoT) reasoning traces. ER-RLSP represents RL training of the SFT model with our exploration reward. All results are based on pass@1 accuracy. The results demonstrate that RLSP training significantly boosts the performance of the base model.}
\label{tab:math_performance_8b}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Model}     & \textbf{MATH-500 (\%)}  \\ 
\midrule
Base          &        50.6    \\
SFT       &        70.2   \\
SFT + ER-RLSP    & 73.4        \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

We now analyze these findings and highlight key observations.
Firstly, SFT training yields a significant improvement in the base model's performance. 
This result underscores the importance of high-quality supervised demonstrations in enhancing the model’s reasoning capabilities, especially for small models.
Additionally, PPO training with our exploration reward yields a further 3\% improvement in performance. 
While this gain is not as pronounced as that achieved through SFT, it is expected, as SFT has already contributed substantial improvements, leaving limited room for additional gains through RL on the dataset MATH-500. 
Further, recall our (theory) discussion in \autoref{sec:theory} that long CoT SFT dataset itself can be thought of as constructed via a search procedure as it includes trajectories over rationales.
Thus, we conclude that even for small models, long CoT trajectories that incorporate search behaviors can lead to improved reasoning.


\subsection{Performance Analysis: 32B Model}
\label{sec:empirical_32b}

\subsubsection{Setup}
In this setup, we use Qwen2.5-32B-Instruct \cite{qwen-25-32b} as the base model.
Given its larger size and enhanced capabilities, we omit the SFT stage and focus on directly improving performance through RL training within our RLSP framework.

During the RL stage, the base model is trained using PPO on AIME 918 problems from the years 1983 to 2003.
We transition to the AIME dataset since the model already performs exceptionally well on the MATH dataset, nearing saturation. 
Thus, we focus on a more challenging setting to further assess and improve its capabilities.
Details on the hyperparameters used for RL training are provided in \autoref{sec:appendix_empirical_32b_rl_hyperparams}.

For evaluation, we assess the model's accuracy on AIME problems from the year 2024, as well as on the 500 test splits of the MATH dataset (referred to as MATH-500).

\subsubsection{Evaluation Results}
\autoref{tab:math_performance_32b} presents the results of this section.



\begin{table}[h]
\caption{Performance comparison of RLSP training for AIME and MATH datasets. Base model is Qwen2.5-32B-Instruct. ER-RLSP represents RL training of the base model with our exploration reward. All results are based on pass@1 accuracy. The results demonstrate that even with fewer than 1000 challenging math problems, RL training with our exploration reward can lead to significant performance improvements.}
\label{tab:math_performance_32b}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Model}     & \textbf{MATH (\%)} & \textbf{AIME (\%)} \\ 
\midrule
Base          & 81.6      & 13.3      \\
Base + ER-RLSP       &    83.2  &  23.3   \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}

\end{table}

We highlight that even with a very small-scale set of challenging math problems as training data, our RL training approach can yield significant performance gains.
It is also noteworthy that while significant performance improvements are achieved on AIME 2024, we observe additional gains even on MATH-500, where the base model already demonstrates exceptional performance.

\subsection{Token Efficiency of RLSP vs Self-Consistency}
\label{sec:tokenefficiency}
A key intuition behind the RLSP framework is incentivizing the model to learn and apply search behavior during inference. 
We evaluate the token efficiency of the RLSP-trained model by asking: 
For a similar compute budget, what is the accuracy achieved by an RLSP-trained model versus a model using standard CoT with majority voting? 
We discuss the results in the following.

\autoref{tab:self_consistency_performance} presents the self-consistency performance of different models on the MATH-500 and AIME 2024 datasets, utilizing an average budget of 8192 tokens. 
Specifically, the Llama-3.1-8B-Instruct generates an average of 16 samples per test problem in MATH-500, while Qwen2.5-32B-Instruct produces an average of 8 samples per test problem in AIME 2024.
The self-consistency accuracy of Llama-3.1-8B-Instruct on MATH-500 is 61.8\%, while that of Qwen2.5-32B-Instruct on AIME24 is 20\%.
We note that these results fall short of our best performances, which reach 73.4\% (\autoref{tab:math_performance_8b}) and 23.3\% (\autoref{tab:math_performance_32b}), respectively.
Therefore, our study demonstrates that the RLSP framework enables better search and reasoning capabilities compared to majority voting or other independent sampling strategies.

\begin{table}[h]
\caption{Self-consistency performance metrics of Llama-3.1-8B-Instruct and Qwen2.5-32B-Instruct Models on MATH-500 and AIME 2024 with an average budget of 8192 tokens. Here \textbf{Avg Samples} = Average Samples per Problem, \textbf{SC Accuracy} = Self-Consistency Accuracy, and \textbf{Best Perf} = Best Performance.}
\label{tab:self_consistency_performance}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Llama-8B} & \textbf{Qwen-32B} \\
\midrule
\textbf{Benchmark} & MATH-500 & AIME 2024 \\
\textbf{Avg Samples} & 16 & 8 \\
\textbf{SC Accuracy} & 61.8\% & 20\% \\
\midrule
\textbf{SFT+ER-RLSP} & 73.4\% & 23.3\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


% RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Related Work}\label{sec:related-work}

In this section, we give a detailed literature survey.
%Not to be influenced by all the latest work that was made publicly available since 1-Jan-2025 on our methodology, we consider all those works as concurrent works, and we do not compare our results to those techniques. 

\paragraph{Large Reasoning Models}
OpenAI’s o1 series \cite{openai-o1-mini, openai-o1, jaech2024openai} and other reasoning models \cite{gemini-2,deepseek-r1, qwq} represent a significant leap in reasoning capabilities, excelling in structured reasoning, systematic problem decomposition, and reliable handling of complex tasks. 
%Their development relies on large-scale reinforcement learning with chain-of-thought (CoT) reasoning, coupled with optimized training and test-time computation strategies. 
Testing is conducted on high-stakes benchmarks, including mathematics \cite{hendrycks2021measuring,gsm8k,aime24,amc23,math500}, competitive programming \cite{jain2024livecodebench,humaneval,codeforces}, and scientific problem-solving \cite{hendrycks2020measuring,rein2023gpqa}, often achieving performance levels that surpass human experts.

Recently, numerous open-source frameworks strive to replicate o1’s reasoning capabilities through diverse methodologies. At the post-training stage, frameworks such as \cite{wang2024openr, zhang2024rest, zhang2024llama} utilize automated data augmentation with MCTS, while \cite{huang2024o1, huang2025o1replicationjourney, skyt1, min2024imitate} exploit reasoning traces in long-CoT data. Process reward models are integrated into the training process by \cite{cui2024process, guan2025rstar}, boosting self-exploration. 
\cite{kimi-k15} introduces an effective RL framework emphasizing long-context scaling and robust policy optimization and incorporates with techniques for distilling long CoT reasoning into more efficient short-CoT models.
Meanwhile, \cite{deepseek-r1} highlights the potential of reinforcement learning in driving reasoning improvements on very large base models such as Deepseek V3 without process reward models or MCTS.
During inference, \cite{wang2024openr} employs majority-vote and beam search strategies, while \cite{zhang2024rest, qin2024o1, guan2025rstar, jiang2024technical} incorporate tree search-based inference. Additionally, \cite{zhang2024llama} emphasizes pairwise preference ranking and advanced exploration techniques, further enhancing inference performance.



\paragraph{Scaling Test-Time Compute}
Scaling test-time compute enhances reasoning capabilities by allocating more computational resources during inference. The test-time scaling laws demonstrate that increased deliberate reasoning (e.g., through additional token generation or iterative steps) directly improves accuracy, especially in complex tasks like mathematics, coding, and planning \cite{snell2024scaling,wu2024empirical,brown2024large,beeching2024scalingtesttimecompute}.
Recent work on simple test-time scaling \cite{muennighoff2025s1} shows that even small models can achieve significant improvements in reasoning tasks by strategically allocating inference budgets. In addition, RL-based scaling approaches \cite{hou2025advancing} show that inference scaling trends becomes more evident as training scales, reinforcing the connection between RL and test-time compute.

Various test-time search methods exploit this principle \cite{kang2024mindstar,wang2024q}. Majority vote aggregates predictions from multiple inference traces to refine accuracy. Tree search methods such as \cite{yao2024tree,hao2023reasoning,zhang2024accessing,qi2024mutual} systematically explore reasoning paths. Beam search \cite{smoke1961program} leverages the PRM and retains top-K paths at each step to improve sequence generation. Lookahead search \cite{snell2024scaling} further enhances exploration depth by evaluating paths using simulations. While these methods improve reasoning accuracy, they increase computational demand, highlighting the trade-off between performance and resource efficiency.
% Todo: s1: simple test-time scaling (https://arxiv.org/abs/2501.19393)
% Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling (https://arxiv.org/pdf/2501.11651)


\paragraph{Math Data with Long Reasoning Traces}

Research indicates that combining System 2-inspired deliberate reasoning with System 1’s fast, heuristic-based thinking significantly enhances reasoning performance \cite{su2024dualformer}. 
To equip language models with System 2 reasoning techniques, recent advancements have explored approaches such as supervised fine-tuning (SFT) with extended chain-of-thought (CoT) reasoning \cite{bai2024longwriter,abdin2024phi,min2024imitate,huang2024o1,qin2024o1,wang2024enhancing,xu2024llava}. 
Training datasets with long reasoning traces, often distilled from advanced models like GPT \cite{li2024numinamath,luo2023wizardmath, yu2023metamath} and Qwen-qwq \cite{qwq-longcot}, are critical for fostering complex reasoning capabilities. 
To address the limitations of generator models, exploration-based techniques like MCTS \cite{guan2025rstar,zhang2024rest} and rejection sampling \cite{yuan2023scaling,brown2024large} systematically enhance reasoning by expanding decision paths. 
These methods enable language models to improve reasoning abilities and generate high-quality solutions to challenging problems, surpassing the constraints of their training data.

Recent studies further support the effectiveness of long CoT data. \cite{simplerl} demonstrates that reinforcement learning on long CoT-augmented data enables small models to exhibit emergent reasoning capabilities with significantly fewer examples. Similarly, \cite{open-r1} highlights that long CoT SFT leads to substantial improvements in problem-solving accuracy across mathematics and programming tasks. Furthermore, \cite{yeo2025demystifying} systematically investigates the factors driving long CoT emergence, revealing that while long CoT is not strictly necessary for reasoning ability, it substantially enhances training efficiency and stabilizes reinforcement learning by providing structured, verifiable reasoning trajectories. 
Meta-CoT \cite{xiang2025towards} provides a theoretical foundation for why long reasoning traces may enhance reasoning abilities. It argues that standard CoT fails to fully capture complex, non-linear thought processes and that explicitly modeling latent reasoning steps improves performance in high-difficulty tasks.



\paragraph{Self-Correction in Language Models}

Self-correction in LLMs has gained significant attention as a mechanism to enhance reasoning and problem-solving abilities. A range of techniques has been explored, spanning from fine-tuning methods to advanced reinforcement learning strategies. 
Fine-tuning approaches leverage curated data to train models for iterative corrections, improving their ability to refine responses \cite{min2024imitate, qu2024recursive, zhang2024small}. 
Prompt-based approaches focus on eliciting better outputs through iterative feedback loops \cite{huang2023large}, while inference strategies like MCTS are employed to refine reasoning by exploring diverse solution paths \cite{zhang2024accessing, yang2024qwen2, tian2024toward, wang2024towards}. 
Reinforcement learning methods such as Self-Correction via Reinforcement Learning (SCoRe) \cite{kumar2024training}, enhance self-correction through multi-turn RL using intrinsic rewards to guide learning.

While extrinsic feedback mechanisms remain effective, intrinsic self-correction—where models refine their outputs without external input—has proven challenging yet promising. Notable advancements include reward-based RL \cite{yang2024qwen2, choi2024self, kumar2024training} and curriculum preference learning \cite{wang2024towards}, which enable iterative refinement of reasoning steps. 
Unlike these approaches, we observe that self-correction behavior emerges naturally through unsupervised RL guided solely by a length penalty signals. This suggests that our method can be easily adapted to other domains without the need for domain-specific agent design or reward hacking.


\paragraph{Reinforcement Learning with Auxiliary rewards}

Reinforcement Learning (RL) with auxiliary rewards improves policy optimization by incorporating pseudo-reward signals that guide learning beyond task completion. 
The foundational work by Jaderberg et al. \cite{jaderberg2016reinforcement} uses auxiliary control and prediction tasks to accelerate learning. Recently, RL has been applied to enhance reasoning in large language models (LLMs). 
\cite{havrilla2024teaching} trained LLMs with outcome-based reward models (ORMs) and utilized a dense reward signal derived from comparing partial solutions to reference solutions.
\cite{setlur2024rewarding} introduced process advantage verifiers (PAVs), which assign stepwise rewards based on changes in correctness probability judged by a stronger prover policy.
\cite{luo2023wizardmath} employed process-supervised RL by distilling a process reward model (PRM) from GPT-4 annotations. In contrast, our work introduces a dense exploration reward that does not require a reference solution, making it more unsupervised and adaptable. Furthermore, rather than distilling PRM from GPT-4, we propose a more general approach to measuring creativity and reasoning effort, enhancing applicability across diverse domains. 




\section{Conclusions, Limitations, and Future Work}
In this work we proposed a post-training technique called RLSP to enable thinking behavior for LRMs, showed promising results both in terms of performance and emergent behaviors.
More large scale experiments and analysis are necessary to fully understand capabilities and limitations of our work.
Needless to say, our work is a small step towards complex reasoning in LLMs and opens up several fascinating research directions:
How do we enable finer-grained test time search in LLMs where search time can have a direct influence on the quality of the solution so that model learns to differentiate between 1+1 = ? and the Riemann hypothesis.
What is the impact of context length on reasoning?
Can pure RL with no exploration reward lead to thinking behavior at some model scale? and what is the precise influence of pretraining data?
While all our models show interesting search behaviors such as backtracking and verification, none of those search strategies are surprising to us, and indirectly present in the pretraining data as  humans use those strategies as well. 
Is there a truly emergent behavior akin to "move 37'' that surpasses human reasoning or at least unexpected?
Finally, looking back at \autoref{fig:stagesofreasoing}, what other training recipes are needed to unlock even higher forms of reasoning such as creating abstractions and theories, and solving open ended problems like climate change or grand unified theory of everything.

