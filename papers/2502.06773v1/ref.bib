@article{li2024chain,
  title={Chain of thought empowers transformers to solve inherently serial problems},
  author={Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  journal={arXiv preprint arXiv:2402.12875},
  year={2024}
}
@article{jaderberg2016reinforcement,
  title={Reinforcement learning with unsupervised auxiliary tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1611.05397},
  year={2016}
}

@article{wang2024chain,
  title={Chain-of-thought reasoning without prompting},
  author={Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.10200},
  year={2024}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{stechly2023gpt,
  title={Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems},
  author={Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2310.12397},
  year={2023}
}

@article{trinh2024alphageometry,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{chen2024alphamath,
  title={AlphaMath Almost Zero: process Supervision without process},
  author={Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai},
  journal={arXiv preprint arXiv:2405.03553},
  year={2024}
}

@article{havrilla2024teaching,
  title={Teaching large language models to reason with reinforcement learning},
  author={Havrilla, Alex and Du, Yuqing and Raparthy, Sharath Chandra and Nalmpantis, Christoforos and Dwivedi-Yu, Jane and Zhuravinskyi, Maksym and Hambro, Eric and Sukhbaatar, Sainbayar and Raileanu, Roberta},
  journal={arXiv preprint arXiv:2403.04642},
  year={2024}
}

@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}


@article{choi2024self,
  title={Self-Improving Robust Preference Optimization},
  author={Choi, Eugene and Ahmadian, Arash and Geist, Matthieu and Pietquin, Oilvier and Azar, Mohammad Gheshlaghi},
  journal={arXiv preprint arXiv:2406.01660},
  year={2024}
}

@article{zhang2024accessing,
  title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
  author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.07394},
  year={2024}
}

@article{valmeekam2024planbench,
  title={Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change},
  author={Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{valmeekam2024planning,
  title={Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1},
  author={Valmeekam, Karthik and Stechly, Kaya and Gundawar, Atharva and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2410.02162},
  year={2024}
}


@article{xiang2025towards,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@article{guan2025rstar,
  title={rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},
  author={Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2501.04519},
  year={2025}
}

@article{du2025virgo,
  title={Virgo: A Preliminary Exploration on Reproducing o1-like MLLM},
  author={Du, Yifan and Liu, Zikang and Li, Yifan and Zhao, Wayne Xin and Huo, Yuqi and Wang, Bingning and Chen, Weipeng and Liu, Zheng and Wang, Zhongyuan and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2501.01904},
  year={2025}
}

@article{zhang2025lessons,
  title={The Lessons of Developing Process Reward Models in Mathematical Reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{xie2024monte,
  title={Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},
  author={Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael},
  journal={arXiv preprint arXiv:2405.00451},
  year={2024}
}

@article{su2024dualformer,
  title={Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces},
  author={Su, DiJia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  journal={arXiv preprint arXiv:2410.09918},
  year={2024}
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}

@article{setlur2024rewarding,
  title={Rewarding progress: Scaling automated process verifiers for llm reasoning},
  author={Setlur, Amrith and Nagpal, Chirag and Fisch, Adam and Geng, Xinyang and Eisenstein, Jacob and Agarwal, Rishabh and Agarwal, Alekh and Berant, Jonathan and Kumar, Aviral},
  journal={arXiv preprint arXiv:2410.08146},
  year={2024}
}

@article{putta2024agent,
  title={Agent q: Advanced reasoning and learning for autonomous ai agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  journal={arXiv preprint arXiv:2408.07199},
  year={2024}
}

@article{gandhi2024stream,
  title={Stream of Search (SoS): Learning to Search in Language},
  author={Gandhi, Kanishk and Lee, Denise and Grand, Gabriel and Liu, Muxin and Cheng, Winson and Sharma, Archit and Goodman, Noah D},
  journal={arXiv preprint arXiv:2404.03683},
  year={2024}
}

@article{wang2024enhancing,
  title={Enhancing the reasoning ability of multimodal large language models via mixed preference optimization},
  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2411.10442},
  year={2024}
}

@article{zhang2024llama,
  title={Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning},
  author={Zhang, Di and Wu, Jianbo and Lei, Jingdi and Che, Tong and Li, Jiatong and Xie, Tong and Huang, Xiaoshui and Zhang, Shufei and Pavone, Marco and Li, Yuqiang and others},
  journal={arXiv preprint arXiv:2410.02884},
  year={2024}
}

@article{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@article{wang2024q,
  title={Q*: Improving multi-step reasoning for llms with deliberative planning},
  author={Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}

@article{qi2024mutual,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@article{wang2024multi,
  title={Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision},
  author={Wang, Zihan and Li, Yunxuan and Wu, Yuexin and Luo, Liangchen and Hou, Le and Yu, Hongkun and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.02658},
  year={2024}
}

@article{zhang2024rest,
  title={Rest-mcts*: Llm self-training via process reward guided tree search},
  author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{
  codecontest,
  author = {Yujia Li  and David Choi  and Junyoung Chung  and Nate Kushman  and Julian Schrittwieser  and R{\'e}mi Leblond  and Tom Eccles  and James Keeling  and Felix Gimeno  and Agustin Dal Lago  and Thomas Hubert  and Peter Choy  and Cyprien de Masson d’Autume  and Igor Babuschkin  and Xinyun Chen  and Po-Sen Huang  and Johannes Welbl  and Sven Gowal  and Alexey Cherepanov  and James Molloy  and Daniel J. Mankowitz  and Esme Sutherland Robson  and Pushmeet Kohli  and Nando de Freitas  and Koray Kavukcuoglu  and Oriol Vinyals },
  title = {Competition-level code generation with AlphaCode},
  journal = {Science},
  volume = {378},
  number = {6624},
  pages = {1092-1097},
  year = {2022},
  doi = {10.1126/science.abq1158},
  URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158}
}

@article{zhang2024generative,
  title={Generative verifiers: Reward modeling as next-token prediction},
  author={Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2408.15240},
  year={2024}
}

@article{chen2024not,
  title={Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{chow2024inference,
  title={Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models},
  author={Chow, Yinlam and Tennenholtz, Guy and Gur, Izzeddin and Zhuang, Vincent and Dai, Bo and Thiagarajan, Sridhar and Boutilier, Craig and Agarwal, Rishabh and Kumar, Aviral and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2412.15287},
  year={2024}
}

@article{renze2024self,
  title={Self-Reflection in LLM Agents: Effects on Problem-Solving Performance},
  author={Renze, Matthew and Guven, Erhan},
  journal={arXiv preprint arXiv:2405.06682},
  year={2024}
}

@article{zhang2024learn,
  title={Learn beyond the answer: Training language models with reflection for mathematical reasoning},
  author={Zhang, Zhihan and Ge, Tao and Liang, Zhenwen and Yu, Wenhao and Yu, Dian and Jia, Mengzhao and Yu, Dong and Jiang, Meng},
  journal={arXiv preprint arXiv:2406.12050},
  year={2024}
}

@article{wu2024empirical,
  title={An empirical analysis of compute-optimal inference for problem-solving with language models},
  journal = {Under review},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  year={2024}
}

@article{welleck2024decoding,
  title={From decoding to meta-generation: Inference-time algorithms for large language models},
  author={Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:2406.16838},
  year={2024}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}

@article{valmeekam2024planning,
  title={Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1},
  author={Valmeekam, Karthik and Stechly, Kaya and Gundawar, Atharva and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2410.02162},
  year={2024}
}

@article{li2024openai,
  title={OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?},
  author={Li, Leo and Luo, Ye and Pan, Tingyou},
  journal={arXiv preprint arXiv:2411.06198},
  year={2024}
}

@article{zheng2024processbench,
  title={Processbench: Identifying process errors in mathematical reasoning},
  author={Zheng, Chujie and Zhang, Zhenru and Zhang, Beichen and Lin, Runji and Lu, Keming and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2412.06559},
  year={2024}
}

@article{bansal2024smaller,
  title={Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling},
  author={Bansal, Hritik and Hosseini, Arian and Agarwal, Rishabh and Tran, Vinh Q and Kazemi, Mehran},
  journal={arXiv preprint arXiv:2408.16737},
  year={2024}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@article{min2024imitate,
  title={Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems},
  author={Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others},
  journal={arXiv preprint arXiv:2412.09413},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}

@article{guan2024search,
  title={Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering},
  author={Guan, Xinyan and Liu, Yanjiang and Lu, Xinyu and Cao, Boxi and He, Ben and Han, Xianpei and Sun, Le and Lou, Jie and Yu, Bowen and Lu, Yaojie and others},
  journal={arXiv preprint arXiv:2411.11504},
  year={2024}
}

@article{xi2024enhancing,
  title={Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision},
  author={Xi, Zhiheng and Yang, Dingwen and Huang, Jixuan and Tang, Jiafu and Li, Guanyu and Ding, Yiwen and He, Wei and Hong, Boyang and Do, Shihan and Zhan, Wenyu and others},
  journal={arXiv preprint arXiv:2411.16579},
  year={2024}
}

@article{gao2024designing,
  title={On designing effective rl reward at training time for llm reasoning},
  author={Gao, Jiaxuan and Xu, Shusheng and Ye, Wenjie and Liu, Weilin and He, Chuyi and Fu, Wei and Mei, Zhiyu and Wang, Guangju and Wu, Yi},
  journal={arXiv preprint arXiv:2410.15115},
  year={2024}
}

@article{kirchner2024prover,
  title={Prover-verifier games improve legibility of llm outputs},
  author={Kirchner, Jan Hendrik and Chen, Yining and Edwards, Harri and Leike, Jan and McAleese, Nat and Burda, Yuri},
  journal={arXiv preprint arXiv:2407.13692},
  year={2024}
}

@article{bai2024longwriter,
  title={Longwriter: Unleashing 10,000+ word generation from long context llms},
  author={Bai, Yushi and Zhang, Jiajie and Lv, Xin and Zheng, Linzhi and Zhu, Siqi and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi},
  journal={arXiv preprint arXiv:2408.07055},
  year={2024}
}

@article{wang2024planning,
  title={Planning in natural language improves llm search for code generation},
  author={Wang, Evan and Cassano, Federico and Wu, Catherine and Bai, Yunfeng and Song, Will and Nath, Vaskar and Han, Ziwen and Hendryx, Sean and Yue, Summer and Zhang, Hugh},
  journal={arXiv preprint arXiv:2409.03733},
  year={2024}
}

@article{wang2024mixture,
  title={Mixture-of-Agents Enhances Large Language Model Capabilities},
  author={Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  journal={arXiv preprint arXiv:2406.04692},
  year={2024}
}

@article{hu2024uncertainty,
  title={Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models},
  author={Hu, Zhiyuan and Liu, Chumin and Feng, Xidong and Zhao, Yilun and Ng, See-Kiong and Luu, Anh Tuan and He, Junxian and Koh, Pang Wei and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.03271},
  year={2024}
}

@article{yuan2024advancing,
  title={Advancing llm reasoning generalists with preference trees},
  author={Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2404.02078},
  year={2024}
}

@article{tian2024toward,
  title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2404.12253},
  year={2024}
}

@article{kang2024mindstar,
  title={Mindstar: Enhancing math reasoning in pre-trained llms at inference time},
  author={Kang, Jikun and Li, Xin Zhe and Chen, Xi and Kazemi, Amirreza and Sun, Qianyi and Chen, Boxing and Li, Dong and He, Xu and He, Quan and Wen, Feng and others},
  journal={arXiv preprint arXiv:2405.16265},
  year={2024}
}

@article{chen2024tree,
  title={When is tree search useful for llm planning? it depends on the discriminator},
  author={Chen, Ziru and White, Michael and Mooney, Raymond and Payani, Ali and Su, Yu and Sun, Huan},
  journal={arXiv preprint arXiv:2402.10890},
  year={2024}
}



@article{sprague2024cot,
  title={To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning},
  author={Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
  journal={arXiv preprint arXiv:2409.12183},
  year={2024}
}

@article{yang2024large,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author={Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2402.16837},
  year={2024}
}

@article{zhang2024chain,
  title={Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs},
  author={Zhang, Xuan and Du, Chao and Pang, Tianyu and Liu, Qian and Gao, Wei and Lin, Min},
  journal={arXiv preprint arXiv:2406.09136},
  year={2024}
}

@article{luong2024reft,
  title={Reft: Reasoning with reinforced fine-tuning},
  author={Luong, Trung Quoc and Zhang, Xinbo and Jie, Zhanming and Sun, Peng and Jin, Xiaoran and Li, Hang},
  journal={arXiv preprint arXiv:2401.08967},
  year={2024}
}

@article{ahmadian2024back,
  title={Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author={Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2402.14740},
  year={2024}
}

@article{kazemnejad2024vineppo,
  title={Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment},
  author={Kazemnejad, Amirhossein and Aghajohari, Milad and Portelance, Eva and Sordoni, Alessandro and Reddy, Siva and Courville, Aaron and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2410.01679},
  year={2024}
}

@article{mirzadeh2024gsm,
  title={Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{zhong2024evaluation,
  title={Evaluation of openai o1: Opportunities and challenges of agi},
  author={Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others},
  journal={arXiv preprint arXiv:2409.18486},
  year={2024}
}

@article{kamoi2024evaluating,
  title={Evaluating LLMs at Detecting Errors in LLM Responses},
  author={Kamoi, Ryo and Das, Sarkar Snigdha Sarathi and Lou, Renze and Ahn, Jihyun Janice and Zhao, Yilun and Lu, Xiaoxin and Zhang, Nan and Zhang, Yusen and Zhang, Ranran Haoran and Vummanthala, Sujeeth Reddy and others},
  journal={arXiv preprint arXiv:2404.03602},
  year={2024}
}

@article{wang2024planning,
  title={On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability},
  author={Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2409.19924},
  year={2024}
}

@article{hosseini2024not,
  title={Not All LLM Reasoners Are Created Equal},
  author={Hosseini, Arian and Sordoni, Alessandro and Toyama, Daniel and Courville, Aaron and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2410.01748},
  year={2024}
}

@article{valmeekam2024llms,
  title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench},
  author={Valmeekam, Karthik and Stechly, Kaya and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2409.13373},
  year={2024}
}

@article{wu2024comparative,
  title={A Comparative Study on Reasoning Patterns of OpenAI's o1 Model},
  author={Wu, Siwei and Peng, Zhongyuan and Du, Xinrun and Zheng, Tuney and Liu, Minghao and Wu, Jialong and Ma, Jiachen and Li, Yizhi and Yang, Jian and Zhou, Wangchunshu and others},
  journal={arXiv preprint arXiv:2410.13639},
  year={2024}
}

@article{wu2024thinking,
  title={Thinking LLMs: General Instruction Following with Thought Generation},
  author={Wu, Tianhao and Lan, Janice and Yuan, Weizhe and Jiao, Jiantao and Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2410.10630},
  year={2024}
}

@article{hosseini2024v,
  title={V-star: Training verifiers for self-taught reasoners},
  author={Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2402.06457},
  year={2024}
}

@article{wang2024cpl,
  title={CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning Tasks},
  author={Wang, Tianlong and Chen, Junzhe and Han, Xueting and Bai, Jing},
  journal={arXiv preprint arXiv:2409.08642},
  year={2024}
}

@article{gehring2024rlef,
  title={Rlef: Grounding code llms in execution feedback with reinforcement learning},
  author={Gehring, Jonas and Zheng, Kunhao and Copet, Jade and Mella, Vegard and Cohen, Taco and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2410.02089},
  year={2024}
}

@article{zhao2024probabilistic,
  title={Probabilistic inference in language models via twisted sequential monte carlo},
  author={Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger},
  journal={arXiv preprint arXiv:2404.17546},
  year={2024}
}

@article{jiang2024technical,
  title={Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search},
  author={Jiang, Jinhao and Chen, Zhipeng and Min, Yingqian and Chen, Jie and Cheng, Xiaoxue and Wang, Jiapeng and Tang, Yiru and Sun, Haoxiang and Deng, Jia and Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2411.11694},
  year={2024}
}

@article{mcaleese2024llm,
  title={Llm critics help catch llm bugs},
  author={McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
  journal={arXiv preprint arXiv:2407.00215},
  year={2024}
}

@article{liu2024enhancing,
  title={Enhancing multi-step reasoning abilities of language models through direct q-function optimization},
  author={Liu, Guanlin and Ji, Kaixuan and Zheng, Renjie and Wu, Zheng and Dun, Chen and Gu, Quanquan and Yan, Lin},
  journal={arXiv preprint arXiv:2410.09302},
  year={2024}
}

@article{wang2024offline,
  title={Offline Reinforcement Learning for LLM Multi-Step Reasoning},
  author={Wang, Huaijie and Hao, Shibo and Dong, Hanze and Zhang, Shenao and Bao, Yilin and Yang, Ziran and Wu, Yi},
  journal={arXiv preprint arXiv:2412.16145},
  year={2024}
}

@article{chen2024unlocking,
  title={Unlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought},
  author={Chen, Qiguang and Qin, Libo and Wang, Jiaqi and Zhou, Jinxuan and Che, Wanxiang},
  journal={arXiv preprint arXiv:2410.05695},
  year={2024}
}

@article{hoffman2024training,
  title={Training chain-of-thought via latent-variable inference},
  author={Hoffman, Matthew Douglas and Phan, Du and Dohan, David and Douglas, Sholto and Le, Tuan Anh and Parisi, Aaron and Sountsov, Pavel and Sutton, Charles and Vikram, Sharad and A Saurous, Rif},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{merrill2023expresssive,
  title={The expresssive power of transformers with chain of thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}

@article{feng2023alphazero,
  title={Alphazero-like tree-search can guide large language model decoding and training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}

@inproceedings{yu2024ovm,
  title={OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning},
  author={Yu, Fei and Gao, Anningzhe and Wang, Benyou},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={858--875},
  year={2024}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@inproceedings{liu2024don,
  title={Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding},
  author={Liu, Jiacheng and Cohen, Andrew and Pasunuru, Ramakanth and Choi, Yejin and Hajishirzi, Hannaneh and Celikyilmaz, Asli},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{poesia2023certified,
  title={Certified deductive reasoning with language models},
  author={Poesia, Gabriel and Gandhi, Kanishk and Zelikman, Eric and Goodman, Noah D},
  journal={arXiv preprint arXiv:2306.04031},
  year={2023}
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{zhou2023think,
  title={Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue},
  author={Zhou, Junkai and Pang, Liang and Shen, Huawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2311.07445},
  year={2023}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@article{xie2024self,
  title={Self-evaluation guided beam search for reasoning},
  author={Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yang2022chain,
  title={Chain of thought imitation with procedure cloning},
  author={Yang, Mengjiao Sherry and Schuurmans, Dale and Abbeel, Pieter and Nachum, Ofir},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36366--36381},
  year={2022}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}


@misc{alphaproof,
      title={AI achieves silver-medal standard solving International Mathematical Olympiad problems},
      author={Google Deepmind},
  year = {2024},
      url={https://deepmind.google/discover/blog/ai-solves-\\imo-problems-at-silver-medal-level/},
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@misc{beeching2024scalingtesttimecompute,
      title={Scaling test-time compute with open models},
      author={Edward Beeching and Lewis Tunstall and Sasha Rush},
  year = {2024},
      url={https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute},
}

@article{meta2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{jones2021scaling,
  title={Scaling scaling laws with board games},
  author={Jones, Andy L},
  journal={arXiv preprint arXiv:2104.03113},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{anthony2017thinking,
  title={Thinking fast and slow with deep learning and tree search},
  author={Anthony, Thomas and Tian, Zheng and Barber, David},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ferber2023surco,
  title={Surco: Learning linear surrogates for combinatorial nonlinear optimization problems},
  author={Ferber, Aaron M and Huang, Taoan and Zha, Daochen and Schubert, Martin and Steiner, Benoit and Dilkina, Bistra and Tian, Yuandong},
  booktitle={International Conference on Machine Learning},
  pages={10034--10052},
  year={2023},
  organization={PMLR}
}

@article{qu2024recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}

@article{zhang2024small,
  title={Small Language Models Need Strong Verifiers to Self-Correct Reasoning},
  author={Zhang, Yunxiang and Khalifa, Muhammad and Logeswaran, Lajanugen and Kim, Jaekyeom and Lee, Moontae and Lee, Honglak and Wang, Lu},
  journal={arXiv preprint arXiv:2404.17140},
  year={2024}
}

@article{stechly2024self,
  title={On the self-verification limitations of large language models on reasoning and planning tasks},
  author={Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2402.08115},
  year={2024}
}

@article{welleck2022generating,
  title={Generating sequences by learning to self-correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2211.00053},
  year={2022}
}

@article{choo2022simulation,
  title={Simulation-guided beam search for neural combinatorial optimization},
  author={Choo, Jinho and Kwon, Yeong-Dae and Kim, Jihoon and Jae, Jeongwoo and Hottung, Andr{\'e} and Tierney, Kevin and Gwon, Youngjune},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8760--8772},
  year={2022}
}

@article{liu2024skywork,
  title={Skywork-reward: Bag of tricks for reward modeling in llms},
  author={Liu, Chris Yuhao and Zeng, Liang and Liu, Jiacai and Yan, Rui and He, Jujie and Wang, Chaojie and Yan, Shuicheng and Liu, Yang and Zhou, Yahui},
  journal={arXiv preprint arXiv:2410.18451},
  year={2024}
}

@article{wang2024towards,
  title={Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning},
  author={Wang, Xiyao and Song, Linfeng and Tian, Ye and Yu, Dian and Peng, Baolin and Mi, Haitao and Huang, Furong and Yu, Dong},
  journal={arXiv preprint arXiv:2410.06508},
  year={2024}
}

@article{yu2024exact,
  title={ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning},
  author={Yu, Xiao and Peng, Baolin and Vajipey, Vineeth and Cheng, Hao and Galley, Michel and Gao, Jianfeng and Yu, Zhou},
  journal={arXiv preprint arXiv:2410.02052},
  year={2024}
}

@article{rafailov2024r,
  title={From r to Q*: Your Language Model is Secretly a Q-Function},
  author={Rafailov, Rafael and Hejna, Joey and Park, Ryan and Finn, Chelsea},
  journal={arXiv preprint arXiv:2404.12358},
  year={2024}
}

@inproceedings{agarwal2024policy,
  title={On-policy distillation of language models: Learning from self-generated mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Garea, Sabela Ramos and Geist, Matthieu and Bachem, Olivier},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}



@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{yu2023outcome,
  title={Outcome-supervised verifiers for planning in mathematical reasoning},
  author={Yu, Fei and Gao, Anningzhe and Wang, Benyou},
  journal={arXiv preprint arXiv:2311.09724},
  year={2023}
}

@article{yeo2025demystifying,
  title={Demystifying Long Chain-of-Thought Reasoning in LLMs},
  author={Yeo, Edward and Tong, Yuxuan and Niu, Morry and Neubig, Graham and Yue, Xiang},
  journal={arXiv preprint arXiv:2502.03373},
  year={2025}
}



@article{hou2025advancing,
  title={Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling},
  author={Hou, Zhenyu and Lv, Xin and Lu, Rui and Zhang, Jiajie and Li, Yujiang and Yao, Zijun and Li, Juanzi and Tang, Jie and Dong, Yuxiao},
  journal={arXiv preprint arXiv:2501.11651},
  year={2025}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@article{qin2024o1,
  title={O1 Replication Journey: A Strategic Progress Report--Part 1},
  author={Qin, Yiwei and Li, Xuefeng and Zou, Haoyang and Liu, Yixiu and Xia, Shijie and Huang, Zhen and Ye, Yixin and Yuan, Weizhe and Liu, Hector and Li, Yuanzhi and others},
  journal={arXiv preprint arXiv:2410.18982},
  year={2024}
}

@article{huang2024o1,
  title={O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?},
  author={Huang, Zhen and Zou, Haoyang and Li, Xuefeng and Liu, Yixiu and Zheng, Yuxiang and Chern, Ethan and Xia, Shijie and Qin, Yiwei and Yuan, Weizhe and Liu, Pengfei},
  journal={arXiv preprint arXiv:2411.16489},
  year={2024}
}

@article{huang2025o1replicationjourney,
      title={O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning}, 
      author={Zhongzhen Huang and Gui Geng and Shengyi Hua and Zhen Huang and Haoyang Zou and Shaoting Zhang and Pengfei Liu and Xiaofan Zhang},
      journal={arXiv preprint arXiv:2501.06458},
      year={2025}
}

@misc{skyt1,
  title = {Sky-T1: Train your own O1 preview model within \$450},
  author = {NovaSky},
  year = {2025},
  howpublished = {\url{https://novasky-ai.github.io/posts/sky-t1/}}
}

@misc{open-r1,
  title = {Open-R1: a fully open reproduction of DeepSeek-R1},
  author = {Bakouch, Elie and von Werra, Leandro and Tunstall, Lewis},
  year = {2025},
  howpublished = {\url{https://huggingface.co/blog/open-r1}}
}


@misc{simplerl,
  title = {7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient},
  author = {Zeng, Weihao and  Huang, Yuzhen and Liu, Wei and He, Keqing and Liu, Qian and Ma, Zejun and He, Junxian},
  year = {2025},
  howpublished = {\url{https://hkust-nlp.notion.site/simplerl-reason#18439bdc1c6b8083ba31f9cc912cf7f0}}
}


@misc{openai-o1,
  title = {Learning to reason with LLMs},
  author = {OpenAI},
  year = {2024},
  howpublished = {\url{https://openai.com/index/learning-to-\\
                       reason-with-llms/}}
}

@misc{openai-o1-mini,
  title = {OpenAI o1-mini Advancing Cost-efficient Reasoning.},
  author = {OpenAI},
  year = {2024},
  howpublished = {\url{https://openai.com/index/openai-o1-mini-advancing-cost-\\
                       efficient-reasoning/}}
}

@misc{gemini-2,
  title = {Introducing Gemini 2.0: our new AI model for the agentic era},
  author = {Google},
  year = {2024},
  howpublished = {\url{https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message}}
}

@misc{deepseek-r1,
  title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author = {DeepSeek-AI},
  year = {2025},
  howpublished = {\url{https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf}}
}

@misc{kimi-k15,
  title = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},
  author = {Kimi},
  year = {2025},
  howpublished = {\url{https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf}}
}


@misc{qwq,
  title = {Qwq: Reflect deeply on the boundaries of the unknown},
  author = {Qwen},
  year = {2024},
  howpublished = {\url{https://qwenlm.github.io/blog/qwq-32b-preview}}
}

@article{wang2024openr,
  title={Openr: An open source framework for advanced reasoning with large language models},
  author={Wang, Jun and Fang, Meng and Wan, Ziyu and Wen, Muning and Zhu, Jiachen and Liu, Anjie and Gong, Ziqin and Song, Yan and Chen, Lei and Ni, Lionel M and others},
  journal={arXiv preprint arXiv:2410.09671},
  year={2024}
}

@misc{cui2024process,
  title={Process Reinforcement through Implicit Rewards},
  author={Ganqu Cui and Lifan Yuan and Zefan Wang and Hanbin Wang and Wendi Li and Bingxiang He and Yuchen Fan and Tianyu Yu and Qixin Xu and Weize Chen and Jiarui Yuan and Huayu Chen and Kaiyan Zhang and Xingtai Lv and Shuo Wang and Yuan Yao and Hao Peng and Yu Cheng and Zhiyuan Liu and Maosong Sun and Bowen Zhou and Ning Ding},
  year={2024},
  howpublished={\url{https://curvy-check-498.notion.site/
  Process-Reinforcement-through-\\
                     Implicit-Rewards-15f4fcb9c42180f1b49\\
                     8cc9b2eaf896f}},
  note={Notion Blog}
}


@article{yuan2024implicitprm,
  title={Free Process Rewards without Process Labels},
  author={Lifan Yuan and Wendi Li and Huayu Chen and Ganqu Cui and Ning Ding and Kaiyan Zhang and Bowen Zhou and Zhiyuan Liu and Hao Peng},
  journal={arXiv preprint arXiv:2412.01981},
  year={2024}
}

@article{xu2025redstar,
  title={RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?},
  author={Xu, Haotian and Wu, Xing and Wang, Weinong and Li, Zhongzhi and Zheng, Da and Chen, Boyuan and Hu, Yi and Kang, Shijia and Ji, Jiaming and Zhang, Yingying and others},
  journal={arXiv preprint arXiv:2501.11284},
  year={2025}
}

@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Hu, Jian and Wu, Xibin and Wang, Weixun and Zhang, Dehao and Cao, Yu and others},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{gpt-4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@misc{gpt-4o-mini,
  title = {Gpt-4o mini},
  author = {OpenAI},
  year = {2024},
  howpublished = {\url{https://openai.com/index/gpt-4o-mini-advancing-cost-\\efficient-intelligence/}}
}


@article{abdin2024phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{xu2024llava,
  title={LLaVA-o1: Let Vision Language Models Reason Step-by-Step},
  author={Xu, Guowei and Jin, Peng and Hao, Li and Song, Yibing and Sun, Lichao and Yuan, Li},
  journal={arXiv preprint arXiv:2411.10440},
  year={2024}
}


@article{smoke1961program,
  title={A program for the machine translation of natural languages.},
  author={Smoke, W and Dubinsky, E},
  journal={Mech. Transl. Comput. Linguistics},
  volume={6},
  pages={2--10},
  year={1961}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@article{li2024numinamath,
  title={Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions},
  author={Li, Jia and Beeching, Edward and Tunstall, Lewis and Lipkin, Ben and Soletskyi, Roman and Huang, Shengyi and Rasul, Kashif and Yu, Longhui and Jiang, Albert Q and Shen, Ziju and others},
  journal={Hugging Face repository},
  volume={13},
  year={2024}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{yang2023large,
  title={Large language models as optimizers. 2023},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={Accessed on},
  volume={1},
  year={2023}
}

@misc{zhou2023teaching,
  title = {Teach Language Models to Reason},
  author = {Denny Zhou},
  year = {2023},
  howpublished = {\url{https://dennyzhou.github.io/LLMs-Reason-2023-Harvard-Yale.pdf}}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hugging Face Models and Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{aime24,
  title = {AIME 2024},
  author = {AI-MO},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/AI-MO/aimo-validation\\
    -aime}}
}


@misc{humaneval,
  title = {OpenAI HumanEval},
  author = {OpenAI},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/openai/openai_humaneval}}
}

@misc{math500,
  title = {Math-500},
  author = {OpenAI},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceH4/MATH-500}}
}

@misc{codeforces,
  title = {CodeForces Dataset},
  author = {CodeForces},
  year = {2024},
  howpublished = {\url{https://codeforces.com/blog/entry/136853}}
}


@misc{gsm8k,
  title = {GSM8K},
  author = {OpenAI},
  year = {2022},
  howpublished = {\url{https://huggingface.co/datasets/openai/gsm8k}}
}

@misc{amc23,
  title = {AMC 2023},
  author = {AI-MO},
  year = {2023},
  howpublished = {\url{https://huggingface.co/datasets/AI-MO/aimo-validation\\
    -amc}}
}

@misc{numinamath-cot,
  title = {NuminaMath-CoT},
  author = {AI-MO},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/AI-MO/NuminaMath-\\CoT}}
}


@misc{qwq-longcot,
  title = {QwQ-LongCoT-130K-cleaned},
  author = {Qwen},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/gghfez/QwQ-LongCoT-130K-cleaned}}
}


@misc{openo1-sft,
  title = {OpenO1-SFT},
  author = {OpenO1},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT}}
}


@misc{llama-31-8b,
  title = {Llama-3.1-8B},
  author = {Meta},
  year = {2024},
  howpublished = {\url{https://huggingface.co/meta-llama/Llama-3.1-8B}}
}

@misc{llama-31-8b-instruct,
  title = {Llama-3.1-8B-Instruct},
  author = {Meta},
  year = {2024},
  howpublished = {\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}
}

@misc{qwen-25-32b,
  title = {Qwen-2.5-32B},
  author = {Qwen},
  year = {2024},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen2.5-32B}}
}


@misc{qwen-25-32b-instruct,
  title = {Qwen2.5-32B-Instruct},
  author = {Qwen},
  year = {2024},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen2.5-32B-Instruct}}
}

@misc{qwq-32b-preview,
  title = {QwQ-32B-Preview},
  author = {Qwen},
  year = {2024},
  howpublished = {\url{https://huggingface.co/Qwen/QwQ-32B-Preview}}
}



@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX symposium on operating systems design and implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}