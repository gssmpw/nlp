Recent advancements in AI, such as OpenAI's new o models, Google's Gemini Thinking model, and Deepseek R1, are transforming LLMs into LRMs (Large Reasoning Models). Unlike LLMs, LRMs perform {\em thinking or reasoning} during inference, taking additional time and compute to produce higher-quality outputs. 
This work aims to discover the algorithmic framework behind  training LRMs. 
Approaches based on self-consistency, process reward modeling, AlphaZero, highlight that reasoning is a form of guided search. Building on this principle, we ask: what is the simplest and most scalable way to implement search in the context of LLMs?  

\smallskip

Towards answering these questions, we propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). 
RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning {\em process}, whenever possible (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an {\em outcome verifier} to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.

\smallskip

We perform empirical studies of the RLSP framework in the math domain, and show that
the models trained with the RLSP framework demonstrated improved reasoning abilities.
On Llama-3.1-8B-Instruct model the RLSP framework can boost performance by 23\% in MATH-500 test set;  
On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10\% due to RLSP technique.

\smallskip

The more important finding of this work is that the models trained using RLSP technique, even with the simplest exploration reward that encourages the model to take more intermediate steps before arriving at a solution, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. 
Furthermore, our framework enables such emergent behaviors across multiple model families, sizes, and domains.
These findings demonstrate that RLSP framework might be enough to enable the emergence of complex reasoning abilities in LLMs when scaled appropriately.

\smallskip

Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs compared to previous approaches considered in the literature, inspired by a remarkable recent result that says that CoT {\em provably} increases computation power of LLMs, and hence reasoning, and these abilities grow as the number of steps in CoT \cite{li2024chain,merrill2023expresssive}. Our code is available at: \url{https://github.com/GuanghaoYe/Emergence-of-Thinking}.
