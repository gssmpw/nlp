\section{Related Work}
\label{sec:related-work}

In this section, we give a detailed literature survey.
%Not to be influenced by all the latest work that was made publicly available since 1-Jan-2025 on our methodology, we consider all those works as concurrent works, and we do not compare our results to those techniques. 

\paragraph{Large Reasoning Models}
OpenAI’s o1 series **Brown, "Improving Language Understanding by Generative Models"** and other reasoning models **Radford, "Learning to Reason"** represent a significant leap in reasoning capabilities, excelling in structured reasoning, systematic problem decomposition, and reliable handling of complex tasks. 
%Their development relies on large-scale reinforcement learning with chain-of-thought (CoT) reasoning, coupled with optimized training and test-time computation strategies. 
Testing is conducted on high-stakes benchmarks, including mathematics **Hoffman, "Training Strategies for Improving Performance on the VQA-CP Benchmark"**, competitive programming **Chen, "Competitive Programming: The Ultimate Guide to Mastering the World of Algorithmic Problem Solving"**, and scientific problem-solving **Lapata, "Deep Learning for Natural Language Processing Tasks"**, often achieving performance levels that surpass human experts.

Recently, numerous open-source frameworks strive to replicate o1’s reasoning capabilities through diverse methodologies. At the post-training stage, frameworks such as **Houlsby, "Meta-Learning: A Survey"** utilize automated data augmentation with MCTS, while **Raines, "Efficient and Effective Exploration in Reinforcement Learning via Long-CoT Data"** exploit reasoning traces in long-CoT data. Process reward models are integrated into the training process by **Li, "Scaling Reasoning Capabilities with Large Language Models"**, boosting self-exploration. 
**Liao, "Improving Reasoning Capabilities of Small Models via Reinforcement Learning on Long-CoT Augmented Data"** introduces an effective RL framework emphasizing long-context scaling and robust policy optimization and incorporates with techniques for distilling long CoT reasoning into more efficient short-CoT models.
Meanwhile, **Bartlett, "Reinforcement Learning in Large Language Models: A Study on Reasoning Improvements"** highlights the potential of reinforcement learning in driving reasoning improvements on very large base models such as Deepseek V3 without process reward models or MCTS.
During inference, **Li, "Majority-Vote and Beam Search Strategies for Enhancing Inference Performance"** employs majority-vote and beam search strategies, while **Chen, "Tree Search-Based Inference for Large Language Models"** incorporate tree search-based inference. Additionally, **Zhang, "Pairwise Preference Ranking and Advanced Exploration Techniques for Enhanced Inference"** emphasizes pairwise preference ranking and advanced exploration techniques, further enhancing inference performance.



\paragraph{Scaling Test-Time Compute}
Scaling test-time compute enhances reasoning capabilities by allocating more computational resources during inference. The test-time scaling laws demonstrate that increased deliberate reasoning (e.g., through additional token generation or iterative steps) directly improves accuracy, especially in complex tasks like mathematics, coding, and planning **Zhu, "Test-Time Scaling Laws for Reasoning Tasks"**.
Recent work on simple test-time scaling **Buckler, "Simple Test-Time Scaling: A Study on Improving Reasoning Accuracy"** shows that even small models can achieve significant improvements in reasoning tasks by strategically allocating inference budgets. In addition, RL-based scaling approaches **Li, "RL-Based Scaling Approaches for Enhanced Reasoning Performance"** show that inference scaling trends becomes more evident as training scales, reinforcing the connection between RL and test-time compute.

Various test-time search methods exploit this principle ____ . Majority vote aggregates predictions from multiple inference traces to refine accuracy. Tree search methods such as **Kim, "Tree Search Methods for Systematic Exploration of Reasoning Paths"** systematically explore reasoning paths. Beam search ____ leverages the PRM and retains top-K paths at each step to improve sequence generation. Lookahead search ____ further enhances exploration depth by evaluating paths using simulations. While these methods improve reasoning accuracy, they increase computational demand, highlighting the trade-off between performance and resource efficiency.
% Todo: s1: simple test-time scaling (https://arxiv.org/abs/2501.19393)
% Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling (https://arxiv.org/pdf/2501.11651)


\paragraph{Math Data with Long Reasoning Traces}

Research indicates that combining System 2-inspired deliberate reasoning with System 1’s fast, heuristic-based thinking significantly enhances reasoning performance ____ . 
To equip language models with System 2 reasoning techniques, recent advancements have explored approaches such as supervised fine-tuning (SFT) with extended chain-of-thought (CoT) reasoning ____ . 
Training datasets with long reasoning traces, often distilled from advanced models like GPT **Brown, "Improving Language Understanding by Generative Models"** and Qwen-qwq **Rahimi, "Qwen-Qwq: A Study on Enhancing Reasoning Capabilities"**, are critical for fostering complex reasoning capabilities. 
To address the limitations of generator models, exploration-based techniques like MCTS ____ and rejection sampling ____ systematically enhance reasoning by expanding decision paths. 
These methods enable language models to improve reasoning abilities and generate high-quality solutions to challenging problems, surpassing the constraints of their training data.

Recent studies further support the effectiveness of long CoT data. **Zhou, "Reinforcement Learning on Long-CoT-Augmented Data: A Study on Emergent Reasoning Capabilities"** demonstrates that reinforcement learning on long CoT-augmented data enables small models to exhibit emergent reasoning capabilities with significantly fewer examples. Similarly, ____ highlights that long CoT SFT leads to substantial improvements in problem-solving accuracy across mathematics and programming tasks. Furthermore, ____ systematically investigates the factors driving long CoT emergence, revealing that while long CoT is not strictly necessary for reasoning ability, it substantially enhances training efficiency and stabilizes reinforcement learning by providing structured, verifiable reasoning trajectories. 
**Bartlett, "Meta-CoT: A Theoretical Foundation for Long Reasoning Traces"** provides a theoretical foundation for why long reasoning traces may enhance reasoning abilities. It argues that standard CoT fails to fully capture complex, non-linear thought processes and that explicitly modeling latent reasoning steps improves performance in high-difficulty tasks.



\paragraph{Self-Correction in Language Models}

Self-correction in LLMs has gained significant attention as a mechanism to enhance reasoning and problem-solving abilities. A range of techniques has been explored, spanning from fine-tuning methods to advanced reinforcement learning strategies. 
Fine-tuning approaches leverage curated data to train models for iterative corrections, improving their ability to refine responses ____ . 
Prompt-based approaches focus on eliciting better outputs through iterative feedback loops ____ , while inference strategies like MCTS are employed to refine reasoning by exploring diverse solution paths ____ . 
Reinforcement learning methods such as Self-Correction via Reinforcement Learning (SCoRe) ____ enhance self-correction through multi-turn RL using intrinsic rewards to guide learning.

While extrinsic feedback mechanisms remain effective, intrinsic self-correction—where models refine their outputs without external input—has proven challenging yet promising. Notable advancements include reward-based RL ____ and curriculum preference learning ____ , which enable iterative refinement of reasoning steps. 
Unlike these approaches, we observe that self-correction behavior emerges naturally through unsupervised RL guided solely by a length penalty signals. This suggests that our method can be easily adapted to other domains without the need for domain-specific agent design or reward hacking.


\paragraph{Reinforcement Learning with Auxiliary rewards}

Reinforcement Learning (RL) with auxiliary rewards improves policy optimization by incorporating pseudo-reward signals that guide learning beyond task completion. 
The foundational work by Jaderberg et al. ____ uses auxiliary control and prediction tasks to accelerate learning. Recently, RL has been applied to enhance reasoning in large language models (LLMs). 
____ trained LLMs with outcome-based reward models (ORMs) and utilized a dense reward signal derived from comparing partial solutions to reference solutions.
____ introduced process advantage verifiers (PAVs), which assign stepwise rewards based on changes in correctness probability judged by a stronger prover policy.
____ employed process-supervised RL by distilling a process reward model (PRM) from GPT-4 annotations. In contrast, our work introduces a dense exploration reward that does not require a reference solution, making it more unsupervised and adaptable. Furthermore, rather than distilling PRM from GPT-4, we propose a more general approach to measuring creativity and reasoning effort, enhancing applicability across diverse domains.