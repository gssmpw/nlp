@inproceedings{Alonso-jimenez2022,
author = {Alonso-jim{\'{e}}nez, Pablo},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/243.pdf:pdf},
title = {{MUSIC REPRESENTATION LEARNING BASED ON EDITORIAL METADATA FROM DISCOGS}},
year = {2022}
}
@article{Williamon2019KinematicApproaches,
abstract = {Established pedagogical theories for classical piano usually do not consider the essential relationship between the musical structure, whole body movements, and expression. Research focusing on musicians' expression has shown that body movements reflect the performer's understanding of the musical structure. However, most studies to date focus on the performance of a single piece at a time, leaving unanswered the question on how structural parameters of pieces with varied technical difficulties influence pianists' movements. In this study, 10 pianists performed three contrasting Romantic excerpts in terms of technical level and character, while motion data was collected with a passive infrared motion capture system. We observed how pianists modulate their performances for each of the three pieces and measured the absolute difference in percentage of duration and quantity of motion (QoM) between four expressive conditions (normal, deadpan, exaggerated, immobile). We analyzed common patterns within the time-series of position data to investigate whether pianists embody musical structure in similar ways. A survey was filled in by pianists to understand how they conceive the relationship between body movements and musical structure. Results show that the variation in duration between the exaggerated and deadpan conditions was significant in one measure for one of the excerpts, and that tempo was less affected by the QoM used than by the level of expression. By applying PCA on the pianists' position data, we found that the head QoM is an important parameter for communicating different expressions and structural features. Significant variations in head QoM were found in the immobile and deadpan conditions if compared to the normal condition, only in specific regions of the score. Recurrent head movements occurred along with certain structural parameters for two of the excerpts only. Altogether, these results indicate that the analysis of pianists' body movements and expressive intentions should be carried out in relation to the specific musical context, being dependent on the technical level of the pieces and the repertoire. These results, combined with piano teaching methods, may lead to the development of new approaches in instrumental lessons to help students make independent choices regarding body movements and expression.},
author = {Williamon, Aaron and Blom, Diana Mary and Massie-Laberge, Catherine and Cossette, Isabelle and Wanderley, Marcelo M},
doi = {10.3389/fpsyg.2018.02725},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Williamon et al. - 2019 - Kinematic Analysis of Pianists' Expressive Performances of Romantic Excerpts Applications for Enhanced Pedagog.pdf:pdf},
journal = {Frontiers in Psychology | www.frontiersin.org},
keywords = {body movement,expression,motion capture,motion recurrence,musical structure,piano performance},
pages = {2725},
title = {{Kinematic Analysis of Pianists' Expressive Performances of Romantic Excerpts: Applications for Enhanced Pedagogical Approaches}},
url = {www.frontiersin.org},
volume = {1},
year = {2019}
}
@article{Gao2022,
author = {Gao, Haiyan and Shi, Dibo and Jiang, Tianling and Li, Xin and Zhang, Zefan and Ji, Yi and Li, Ying and Liu, Chunping and Shi, D},
doi = {10.1016/j.websem.2022.100735},
file = {:Users/huanzhang/Downloads/1-s2.0-S1570826822000245-main.pdf:pdf},
issn = {1570-8268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
keywords = {metadata,music,semantic modelling},
pages = {100735},
publisher = {Elsevier B.V.},
title = {{Jou pro of}},
url = {https://doi.org/10.1016/j.websem.2022.100735},
year = {2022}
}
@article{Koelsch2006InvestigatingStudy,
abstract = {The present study used pleasant and unpleasant music to evoke emotion and functional magnetic resonance imaging (fMRI) to determine neural correlates of emotion processing. Unpleasant (permanently dissonant) music contrasted with pleasant (consonant) music showed activations of amyg-dala, hippocampus, parahippocampal gyrus, and temporal poles. These structures have previously been implicated in the emotional processing of stimuli with (negative) emotional valence; the present data show that a cerebral network comprising these structures can be activated during the perception of auditory (musical) information. Pleasant (contrasted to unpleasant) music showed activations of the inferior frontal gyrus (IFG, inferior Brodmann's area (BA) 44, BA 45, and BA 46), the anterior superior insula, the ventral striatum, Heschl's gyrus, and the Rolandic operculum. IFG activations appear to reflect processes of music-syntactic analysis and working memory operations. Activations of Rolandic opercular areas possibly reflect the activation of mirror-function mechanisms during the perception of the pleasant tunes. Rolandic operculum, anterior superior insula, and ventral striatum may form a motor-related circuitry that serves the formation of (premotor) representations for vocal sound production during the perception of pleasant auditory information. In all of the mentioned structures, except the hippocampus, activations increased over time during the presentation of the musical stimuli, indicating that the effects of emotion processing have temporal dynamics; the temporal dynamics of emotion have so far mainly been neglected in the functional imaging literature.},
author = {Koelsch, Stefan and Fritz, Thomas and {Yves Cramon}, D and M{\"{u}} ller, Karsten and Friederici, Angela D},
doi = {10.1002/hbm.20180},
file = {:Users/huanzhang/Downloads/hbm.20180.pdf:pdf},
journal = {Hum Brain Mapp},
keywords = {amygdala,emotion,fMRI,mirror function,music},
pages = {239--250},
title = {{Investigating Emotion With Music: An fMRI Study}},
volume = {27},
year = {2006}
}
@article{McAuley2004,
abstract = {Episodic recognition of novel and familiar melodies was examined by asking participants to make judgments about the recency and frequency of presentation of melodies over the course of two days of testing. For novel melodies, recency judgments were poor and participants often confused the number of presentations of a melody with its day of presentation; melodies heard frequently were judged as have been heard more recently than they actually were. For familiar melodies, recency judgments were much more accurate and the number of presentations of a melody helped rather than hindered performance. Frequency judgments were generally more accurate than recency judgments and did not demonstrate the same interaction with musical familiarity. Overall, these findings suggest that (1) episodic recognition of novel melodies is based more on a generalized "feeling of familiarity" than on a specific episodic memory, (2) frequency information contributes more strongly to this generalized memory than recency information, and (3) the formation of an episodic memory for a melody depends either on the overall familiarity of the stimulus or the availability of a verbal label. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
annote = {From Duplicate 1 (Play it again: Did this melody occur more frequently or was it heard more recently? The role of stimulus familiarity in episodic recognition of music - McAuley, J. Devin; Stevens, Catherine; Humphreys, Michael S.)

Week 6

recency vs. frequency},
author = {McAuley, J. Devin and Stevens, Catherine and Humphreys, Michael S.},
doi = {10.1016/j.actpsy.2004.02.001},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/McAuley, Stevens, Humphreys - 2004 - Play it again Did this melody occur more frequently or was it heard more recently The role of stimu.pdf:pdf},
issn = {00016918},
journal = {Acta Psychologica},
keywords = {Episodic memory,Melody recognition,Music},
month = {may},
number = {1},
pages = {93--108},
pmid = {15111232},
title = {{Play it again: Did this melody occur more frequently or was it heard more recently? The role of stimulus familiarity in episodic recognition of music}},
volume = {116},
year = {2004}
}
@inproceedings{Ciranni2025COCOLARepresentations,
abstract = {We present COCOLA (Coherence-Oriented Contrastive Learning for Audio), a contrastive learning method for musical audio representations that captures the harmonic and rhythmic coherence between samples. Our method operates at the level of stems (or their combinations) composing music tracks and allows the objective evaluation of compositional models for music in the task of accompaniment generation. We also introduce a new baseline for compositional music generation called CompoNet, based on ControlNet \cite{zhang2023adding}, generalizing the tasks of MSDM, and quantify it against the latter using COCOLA. We release all models trained on public datasets containing separate stems (MUSDB18-HQ, MoisesDB, Slakh2100, and CocoChorales).},
archivePrefix = {arXiv},
arxivId = {2404.16969},
author = {Ciranni, Ruben and Postolache, Emilian and Mariani, Giorgio and Mancusi, Michele and Cosmo, Luca and Rodol{\`{a}}, Emanuele},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {2404.16969},
file = {:Users/huanzhang/Downloads/2404.16969v1.pdf:pdf},
title = {{COCOLA: Coherence-Oriented Contrastive Learning of Musical Audio Representations}},
url = {http://arxiv.org/abs/2404.16969},
year = {2025}
}
@article{Learning2024,
author = {Of, Evaluation and Satellite, Passive and Sensing, Remote and Cloud, O F and Water, Liquid},
file = {:Users/huanzhang/Downloads/10110.pdf:pdf},
number = {February},
pages = {2--3},
title = {{for Review Only for Review Only for Review Only for Review Only}},
year = {2009}
}
@article{Li2020,
abstract = {Whether piano touch can influence piano timbre or not is a highly contested topic between acousticians and musicians. To gain insight into the ways in which pianists understand and use timbre, eight piano students were interviewed about their conceptualisation of timbre and ways of producing different timbres on the piano. Results indicate that pianists interpret timbre holistically as the overall sonic outcome and use embodied conceptualisations to produce timbre: expected/intended timbral effects are associated with corporeal experiences and the body and mind are strongly coupled in perception and production.},
author = {Li, Shen and Timmers, Renee},
doi = {10.1080/09298215.2020.1826532},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li, Timmers - 2020 - Exploring pianists' embodied concepts of piano timbre An interview study.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {Piano timbre,body/mind connection,embodied musical experience,sound conceptualisation,sound-producing/facilitating gestures,timbre metaphors,touch-tone relationship},
number = {5},
pages = {477--492},
title = {{Exploring pianists' embodied concepts of piano timbre: An interview study}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {49},
year = {2020}
}
@inproceedings{Hashida2018CrestMusePEDBInformation,
abstract = {Music performance databases that can be referred to as numerical values play important roles in the research of music interpretation, analysis of expressive performances , automatic transcription, and performance rendering technology. We are creating and will publicly release a new version of the CrestMuse Performance Expression Database (PEDB), which is a performance expression database of more than two hundred virtuoso classical piano performances of scores from the Baroque period through the early 20th century, including music by Bach, Mozart, Beethoven, and Chopin. The CrestMusePEDB has been used by more than 50 research institutions around the world. It has especially contributed to research on performance rendering systems as training data. Responding to the demand to increase the database, we started a three-year project in 2016 to develop a second edition of the CrestMusePEDB. In this second edition, 443 performances that contain quantitative data and phrase information about what the pianists had in mind while playing the performance are also included. We further report on the final stage of the project, which will end next winter.},
author = {Hashida, Mitsuyo and Nakamura, Eita and Katayose, Haruhiro},
booktitle = {Proceedings of the 15th Sound and Music Computing (SMC) Conference},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hashida, Nakamura, Katayose - Unknown - CrestMusePEDB 2nd EDITION MUSIC PERFORMANCE DATABASE WITH PHRASE INFORMATION.pdf:pdf},
title = {{{CrestMusePEDB} 2nd Edition: Music Performance Database with Phrase Information}},
year = {2018}
}
@phdthesis{Phillips2016,
author = {Phillips, Peter},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Phillips - 2016 - Piano Rolls and Contemporary Player Pianos The Catalogues, Technologies, Archiving and Accessibility.pdf:pdf},
school = {Sydney Conservatorium of Music},
title = {{Piano Rolls and Contemporary Player Pianos: The Catalogues, Technologies, Archiving and Accessibility}},
year = {2016}
}
@article{Widmer2004AutomaticMachine,
abstract = {The paper addresses the question whether it is possible for a machine to learn to distinguish and recognise famous musicians (concert pianists), based on their style of playing. We extract a number of low-level features related to expressive timing and dynamics from the original audio CD recordings by famous pianists, and apply various machine learning algorithms to the task of learning classifiers based on these features. Experiments show that the computer can learn to identify the performer in a new recording with a probability significantly higher than chance, despite the fact that the features only capture a very limited amount of information about a performance. An analysis of the learned classifiers reveals a number of performance features that seem particularly relevant to style differentiation , and an application of the classifiers to music of a very different style shows that the machine seems to have captured truly fundamental aspects of artistic style. One limitation of the current approach is that sequential information is totally ignored, and we briefly report on ongoing work that tries to address this problem via an interesting conversion of music performances to strings.},
author = {Widmer, Gerhard and Zanon, Patrick},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Widmer, Zanon - Unknown - Automatic Recognition of Famous Artists by Machine.pdf:pdf},
isbn = {9781586034528},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
pages = {1109--1110},
title = {{Automatic recognition of famous artists by machine}},
volume = {110},
year = {2004}
}
@article{Oord2017NeuralLearning,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {{Van Den Oord}, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1711.00937},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/van den Oord DeepMind, Vinyals DeepMind, Kavukcuoglu DeepMind - Unknown - Neural Discrete Representation Learning.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Neural discrete representation learning}},
year = {2017}
}
@article{Turetsky2003Ground-TruthSyntheses,
abstract = {Many modern polyphonic music transcription algorithms are presented\nin a statistical pattern recognition framework. But without a large\ncorpus of real-world music transcribed at the note level, these algorithms\nare unable to take advantage of supervised learning methods and also\nhave difficulty reporting a quantitative metric of their performance,\nsuch as a Note Error Rate.},
author = {Turetsky, R and Ellis, D},
file = {:Users/huanzhang/Downloads/ismir03-midi.pdf:pdf},
isbn = {2-9746194-0-1},
journal = {Proceedings of the 4th International Symposium on Music Information Retrieval},
keywords = {dynamic time warping},
pages = {135--141},
title = {{Ground-Truth Transcriptions of Real Music from Force-Aligned MIDI Syntheses}},
year = {2003}
}
@techreport{Madsen2006SeparatingMIDI,
abstract = {This paper presents an algorithm for converting midi events into logical voices. The algorithm is fundamentally based on the pitch proximity principle. New heuristics are introduced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input. {\textcopyright} 2006 University of Victoria.},
author = {Madsen, S{\o}ren Tjagvad and Widmer, Gerhard},
booktitle = {ISMIR 2006 - 7th International Conference on Music Information Retrieval},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Madsen, Widmer - Unknown - Separating voices in MIDI.pdf:pdf},
isbn = {9781550583496},
keywords = {Stream separation,Voice separation},
pages = {57--60},
title = {{Separating voices in MIDI}},
url = {www.bachcentral.com},
year = {2006}
}
@inproceedings{Chen2020ARepresentations,
abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learn-able nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outper-forming AlexNet with 100× fewer labels. 1},
archivePrefix = {arXiv},
arxivId = {2002.05709v3},
author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {2002.05709v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf:pdf},
title = {{A Simple Framework for Contrastive Learning of Visual Representations}},
url = {https://github.com/google-research/simclr.},
year = {2020}
}
@article{Dhariwal2021,
abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.},
archivePrefix = {arXiv},
arxivId = {2105.05233},
author = {Dhariwal, Prafulla and Nichol, Alex},
eprint = {2105.05233},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dhariwal, Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:pdf},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {8780--8794},
title = {{Diffusion Models Beat GANs on Image Synthesis}},
volume = {11},
year = {2021}
}
@article{Wang2023,
abstract = {Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demo.github.io/.},
archivePrefix = {arXiv},
arxivId = {2304.00830},
author = {Wang, Yuancheng and Ju, Zeqian and Tan, Xu and He, Lei and Wu, Zhizheng and Bian, Jiang and Zhao, Sheng},
eprint = {2304.00830},
file = {:Users/huanzhang/Downloads/2304.00830v2.pdf:pdf},
issn = {10495258},
pages = {1--17},
title = {{AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models}},
url = {http://arxiv.org/abs/2304.00830},
year = {2023}
}
@article{Vall2017,
abstract = {Automated music playlist generation is a specific form of music recommendation. Generally stated, the user receives a set of song suggestions defining a coherent listening session. We hypothesize that the best way to convey such playlist coherence to new recommendations is by learning it from actual curated examples, in contrast to imposing ad hoc constraints. Collaborative filtering methods can be used to capture underlying patterns in hand-curated playlists. However, the scarcity of thoroughly curated playlists and the bias towards popular songs result in the vast majority of songs occurring in very few playlists and thus being poorly recommended. To overcome this issue, we propose an alternative model based on a song-to-playlist classifier, which learns the underlying structure from actual playlists while leveraging song features derived from audio, social tags and independent listening logs. Experiments on two datasets of hand-curated playlists show competitive performance compared to collaborative filtering when sufficient training data is available and more robust performance when recommending rare and out-of-set songs. For example, both approaches achieve a recall@100 of roughly 35% for songs occurring in 5 or more training playists, whereas the proposed model achieves a recall@100 of roughly 15% for songs occurring in 4 or less training playlists, compared to the 3% achieved by collaborative filtering. CCS CONCEPTS • Information systems → Recommender systems; KEYWORDS automated playlist generation, cold-start problem, hybrid recom-mender systems, music information retrieval, neural networks},
author = {Vall, Andreu and Eghbal-Zadeh, Hamid and Dorfer, Matthias and Schedl, Markus and Widmer, Gerhard},
doi = {10.1145/3125486.3125494},
isbn = {9781450353533},
title = {{Como, ItalyCurated Examples and Song Features}},
url = {https://doi.org/10.1145/3125486.3125494},
volume = {9},
year = {2017}
}
@techreport{Marsden2007AutomaticAnalysis,
abstract = {This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable 'matrix' of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate 'goodness' metrics to be incorporated into the software and tested on actual music. {\textcopyright}2007 Austrian Computer Society (OCG).},
author = {Marsden, Alan},
booktitle = {Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Marsden - 2007 - AUTOMATIC DERIVATION OF MUSICAL STRUCTURE A TOOL FOR RESEARCH ON SCHENKERIAN ANALYSIS.pdf:pdf},
isbn = {9783854032182},
pages = {55--58},
title = {{Automatic derivation of musical structure: A tool for research on schenkerian analysis}},
url = {http://www.lancs.ac.uk/staff/marsdena/research/schenker},
year = {2007}
}
@article{Latif2023SparksOutlook,
abstract = {This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.},
archivePrefix = {arXiv},
arxivId = {2308.12792},
author = {Latif, Siddique and Shoukat, Moazzam and Shamshad, Fahad and Usama, Muhammad and Ren, Yi and Cuay{\'{a}}huitl, Heriberto and Wang, Wenwu and Zhang, Xulong and Togneri, Roberto and Cambria, Erik and Schuller, Bj{\"{o}}rn W.},
eprint = {2308.12792},
file = {:Users/huanzhang/Downloads/2308.12792.pdf:pdf},
pages = {1--32},
title = {{Sparks of Large Audio Models: A Survey and Outlook}},
url = {http://arxiv.org/abs/2308.12792},
year = {2023}
}
@inproceedings{Fradet2021MiditokTokenization,
abstract = {This article presents MidiTok, a Python package to encode MIDI files into sequences of tokens to be used with sequential Deep Learning models like Transformers or Recurrent Neural Networks. It allows researchers and developers to encode datasets with various strategies built around the idea that they share common parameters. This key idea makes it easy to :1) optimize the size of the vocabulary and the elements it can represent w.r.t. the MIDI specifications ; 2) compare tokenization methods to see which performs best in which case; 3) measure the relevance of additional information like chords or tempo changes. Code and documentation of MidiTok are on Github 1 .},
author = {Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien and {El Fallah Seghrouchni}, Amal and Gutowski, Nicolas},
booktitle = {International Society for Music Information Retrieval (ISMIR) Late Breaking Demo (LBD)},
file = {:Users/huanzhang/Downloads/000005.pdf:pdf},
isbn = {9783319701622},
title = {{Miditok: a Python Package for Midi File Tokenization}},
year = {2021}
}
@techreport{Portelas2020AutomaticSurvey,
abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
author = {Portelas, R{\'{e}}my and Colas, C{\'{e}}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short Survey.pdf:pdf},
keywords = {Agent-based and Multi-agent Systems: general,Machine Learning: general},
title = {{Automatic Curriculum Learning For Deep RL: A Short Survey}},
year = {2020}
}
@techreport{,
abstract = {In many musical styles, to perform a piece means to produce an 'interpretation' of a musical score. This interpretation involves performers adding nuances to musical parameters such as timing, dynamics, and pitch in order to communicate their artistic conception of the piece, often to an audience. Much previous research into musical interpretation has examined various aspects of expressive performance strategies. However, these studies have largely focussed on a single mode: audio recordings. Multimodal datasets enable more holistic study of the broader factors that can influence both performers' interpretative decision-making as well as audience perception. We summarise data modalities relevant to music performance interpretations and review extant multimodal datasets of expressive musical interpretations, assessing to what extent they cover the different possible modalities. Finally, we outline future research directions and the challenges of reporting and working with multimodal datasets, emphasising the need for more comprehensive datasets, inclusion of performer and (where possible) audience member data, standardisation of data reporting for dataset comparison and cross-disciplinary analysis, and reliable options for data storage and access. Word Count: 7,522},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Multimodal Datasets for Studying Performers' Expressive Interpretations of Musical Scores.pdf:pdf},
keywords = {expressive performance,interpretation,multimodal dataset,music performance},
title = {{Multimodal Datasets for Studying Performers' Expressive Interpretations of Musical Scores}}
}
@article{ChenIsolatingVAEs,
abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the $\beta$-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the $\beta$-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.},
archivePrefix = {arXiv},
arxivId = {1802.04942v5},
author = {Chen, Ricky T Q and Li, Xuechen and Grosse, Roger and Duvenaud, David},
eprint = {1802.04942v5},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Isolating Sources of Disentanglement in VAEs.pdf:pdf},
title = {{Isolating Sources of Disentanglement in VAEs}}
}
@article{Won2021MultimodalRetrieval,
abstract = {Tag-based music retrieval is crucial to browse large-scale music libraries efficiently. Hence, automatic music tagging has been actively explored, mostly as a classification task, which has an inherent limitation: a fixed vocabulary. On the other hand, metric learning enables flexible vocabularies by using pretrained word embeddings as side information. Also, metric learning has already proven its suitability for cross-modal retrieval tasks in other domains (e.g., text-to-image) by jointly learning a multimodal embedding space. In this paper, we investigate three ideas to successfully introduce multi-modal metric learning for tag-based music retrieval: elaborate triplet sampling, acoustic and cultural music information, and domain-specific word embeddings. Our experimental results show that the proposed ideas enhance the retrieval system quantitatively, and qualitatively. Furthermore, we release the MSD500, a subset of the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually annotated tag categories, and user taste profiles.},
archivePrefix = {arXiv},
arxivId = {2010.16030v1},
author = {Won, Minz and Oramas, Sergio and Nieto, Oriol and Gouyon, Fabien and Serra, Xavier},
eprint = {2010.16030v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Won et al. - Unknown - MULTIMODAL METRIC LEARNING FOR TAG-BASED MUSIC RETRIEVAL.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Index Terms-Metric learning,Music retrieval},
title = {{Multimodal Metric Learning for Tag-Based Music Retrieval}},
url = {https://github.com/minzwon/tag-based-music-retrieval},
year = {2021}
}
@inproceedings{Nakamura2017Merged-OutputSkips,
abstract = {A score-following algorithm for polyphonic MIDI performances is presented that can handle performance mistakes, ornaments, desynchronized voices, arbitrary repeats and skips. The algorithm is derived from a stochastic performance model based on hidden Markov model (HMM), and we review the recent development of model construction. In this paper, the model is further extended to capture the multi-voice structure, which is necessary to handle note re-orderings by desynchronized voices and widely stretched ornaments in polyphony. For this, we propose merged-output HMM, which describes performed notes as merged outputs from multiple HMMs, each corresponding to a voice part. It is confirmed that the model yields a score-following algorithm which is effective under frequent note reorder-ings across voices and complicated ornaments.},
author = {Nakamura, Eita and Ono, Nobutaka and Saito, Yasuyuki and Sagayama, Shigeki},
booktitle = {Proceedings of the 11st Sound and Music Computing Conference 2017, SMC},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nakamura et al. - Unknown - Merged-Output Hidden Markov Model for Score Following of MIDI Performance with Ornaments, Desynchronized Voi.pdf:pdf},
title = {{Merged-Output Hidden Markov Model for Score Following of MIDI Performance with Ornaments, Desynchronized Voices, Repeats and Skips}},
year = {2017}
}
@inproceedings{Huang2019TimbretronTransfer,
author = {Huang, Sicong and Li, Qiyang and Anil, Cem and Bao, Xuchan and Oore, Sageev and Grosse, Roger B.},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
file = {:Users/huanzhang/Downloads/1370_timbretron_a_wavenet_cyclegan_.pdf:pdf},
title = {{Timbretron: Awavenet(yclegan(Cqt(Audio))) Pipeline for Musical Timbre Transfer}},
year = {2019}
}
@inproceedings{Sapp2008HybridAnalysis,
abstract = {This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effectiveness compared to Pearson correlation. Starting with correlation at multiple timescales, other concepts such as a performance "noise-floor" are used to generate measurements which are more refined than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists.},
author = {Sapp, Craig Stuart},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sapp - Unknown - ISMIR 2008-Session 4b-Musical Expression and Meaning.pdf:pdf},
title = {{Hybrid Numeric/Rank Similarity Metrics for Musical Performance Analysis}},
year = {2008}
}
@article{Kreuk2022,
abstract = {We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://tinyurl.com/audiogen-text2audio},
archivePrefix = {arXiv},
arxivId = {2209.15352},
author = {Kreuk, Felix and Synnaeve, Gabriel and Polyak, Adam and Singer, Uriel and D{\'{e}}fossez, Alexandre and Copet, Jade and Parikh, Devi and Taigman, Yaniv and Adi, Yossi},
eprint = {2209.15352},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kreuk et al. - Unknown - AUDIOGEN TEXTUALLY GUIDED AUDIO GENERA-TION.pdf:pdf},
title = {{AudioGen: Textually Guided Audio Generation}},
url = {http://arxiv.org/abs/2209.15352},
year = {2022}
}
@techreport{HerremansTensionTension,
abstract = {Tension is a complex multidimensional concept that is not easily quantified. This research proposes three methods for quantifying aspects of tonal tension based on the spiral array, a model for tonality. The cloud diameter measures the dispersion of clusters of notes in tonal space; the cloud momentum measures the movement of pitch sets in the spiral array; finally, tensile strain measures the distance between the local and global tonal context. The three methods are implemented in a system that displays the results as tension ribbons over the music score to allow for ease of interpretation. All three methods are extensively tested on data ranging from small snippets to phrases with the Tris-tan chord and larger sections from Beethoven and Schubert piano sonatas. They are further compared to results from an existing empirical experiment.},
author = {Herremans, Dorien and Chew, Elaine},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Herremans, Chew - Unknown - Tension ribbons Quantifying and visualising tonal tension.pdf:pdf},
title = {{Tension ribbons: Quantifying and visualising tonal tension}}
}
@phdthesis{CancinoChacon2018ComputationalModels,
abstract = {This thesis gives a comprehensive overview of the Basis Function Models (BMs), a family of computational models of expressive music performance. These models have been developed over the past years, and have been steadily growing in complexity. The motivation for this work is to model the complex relationship between properties and structure of a given composition, and musically plausible ways of playing the piece expressively and in this way also to learn more about this complex art. The focus is on Western classical music – mostly on the piano, but also with recent extensions towards complex orchestral pieces.},
author = {Cancino-Chac{\'{o}}n, Carlos Eduardo},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cancino Chac{\'{o}}n - 2018 - Computational Modeling of Expressive Music Performance with Linear and Non-linear Basis Function Models.pdf:pdf},
pages = {247},
school = {Johannes Kepler University Linz},
title = {{Computational Modeling of Expressive Music Performance with Linear and Non-linear Basis Function Models}},
year = {2018}
}
@article{Song2020Score-BasedEquations,
abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 × 1024 images for the first time from a score-based generative model.},
archivePrefix = {arXiv},
arxivId = {2011.13456},
author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
eprint = {2011.13456},
file = {:Users/huanzhang/Downloads/2011.13456.pdf:pdf},
journal = {ICLR 2021 - 9th International Conference on Learning Representations},
pages = {1--36},
title = {{Score-Based Generative Modeling Through Stochastic Differential Equations}},
url = {http://arxiv.org/abs/2011.13456},
year = {2021}
}
@techreport{Giraud2013SubjectFugues,
abstract = {Fugue analysis is a challenging problem. We propose an algorithm that detects subjects and counter-subjects in a symbolic score where all the voices are separated, determining the precise ends and the occurrence positions of these patterns. The algorithm is based on a diatonic similarity between pitch intervals combined with a strict length matching for all notes, except for the first and the last one. On the 24 fugues of the first book of Bach's Well-Tempered Clavier, the algorithm predicts 66% of the subjects with a musically relevant end, and finally retrieves 85% of the subject occurrences, with almost no false positive. {\textcopyright} 2013 Springer-Verlag.},
author = {Giraud, Mathieu and Groult, Richard and Lev{\'{e}}, Florence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-41248-6{\_}24},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Giraud, Groult, Lev{\'{e}} - 2012 - Subject and counter-subject detection for analysis of the Well-Tempered Clavier fugues.pdf:pdf},
isbn = {9783642412479},
issn = {03029743},
keywords = {contrapuntal music,fugue analysis,repeating patterns,symbolic music analysis},
pages = {422--438},
title = {{Subject and counter-subject detection for analysis of the well-tempered Clavier fugues}},
url = {https://hal.archives-ouvertes.fr/hal-00712554},
volume = {7900 LNCS},
year = {2013}
}
@techreport{Huanga,
abstract = {We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story-they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models. Generated examples: https://google-research.github.io/noise2music},
archivePrefix = {arXiv},
arxivId = {2302.03917v2},
author = {Huang, Qingqing and Park, Daniel S and Wang, Tao and Denk, Timo I and Ly, Andy and Chen, Nanxin and Zhang, Zhengdong and Zhang, Zhishuai and Yu, Jiahui and Frank, Christian and Engel, Jesse and Le, Quoc V and Chan, William and Chen, Zhifeng and Han, Wei},
eprint = {2302.03917v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - Unknown - Noise2Music Text-conditioned Music Generation with Diffusion Models.pdf:pdf},
title = {{Noise2Music: Text-conditioned Music Generation with Diffusion Models}},
url = {https://google-research.github.io/noise2music}
}
@article{Ning2019,
author = {Ning, Zhang; Tao Jiang; Feng Deng; Yan Li},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ning - 2019 - AUTOMATIC SINGING EVALUATION WITHOUT REFERENCE MELODY USING BI-DENSE NEURAL NETWORK.pdf:pdf},
isbn = {9781538646588},
pages = {466--470},
title = {{AUTOMATIC SINGING EVALUATION WITHOUT REFERENCE MELODY USING BI-DENSE NEURAL NETWORK}},
year = {2019}
}
@article{Wang,
abstract = {BERT has proven to be a powerful language model in natural language processing and established an effective pre-training & fine-tuning methodology. We see that music , as a special form of language, can benefit from such methodology if we carefully handle its highly-structured and polyphonic properties. To this end, we propose Muse-BERT and show that: 1) MuseBERT has detailed specification of note attributes and explicit encoding of music relations, without presuming any pre-defined sequential event order, 2) the pre-trained MuseBERT is not merely a language model, but also a controllable music generator, and 3) MuseBERT gives birth to various downstream music generation and analysis tasks with practical value. Experiment shows that the pre-trained model outperforms the baselines in terms of reconstruction likelihood and generation quality. We also demonstrate downstream applications including chord analysis, chord-conditioned texture generation , and accompaniment refinement.},
author = {Wang, Ziyu and Lab, Music X and Shanghai, Nyu and Xia, Gus},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - MUSEBERT PRE-TRAINING OF MUSIC REPRESENTATION FOR MUSIC UNDERSTANDING AND CONTROLLABLE GENERATION.pdf:pdf},
title = {{MUSEBERT: PRE-TRAINING OF MUSIC REPRESENTATION FOR MUSIC UNDERSTANDING AND CONTROLLABLE GENERATION}},
url = {https://github.com/}
}
@article{Tsay2013SightPerformance,
author = {Tsay, Chia-Jung},
file = {:Users/huanzhang/Downloads/pnas201221454 14580..14585 _ Enhanced Reader.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
title = {{Sight Over Sound in the Judgment of Music Performance}},
year = {2013}
}
@article{Mcadams1995PerceptualClasses,
abstract = {To study the perceptual structure of musical timbre and the effects of musical training, timbral dis-similarities of synthesized instrument sounds were rated by professional musicians, amateur musicians, and nonmusicians. The data were analyzed with an extended version of the multidimensional scaling algorithm CLASCAL (Winsberg & De Soete, 1993), which estimates the number of latent classes of subjects, the coordinates of each timbre on common Euclidean dimensions , a specificity value of unique attributes for each timbre, and a separate weight for each latent class on each of the common dimensions and the set of specificities. Five latent classes were found for a three-dimensional spatial model with specificities. Common dimensions were quantified psychophysically in terms of log-rise time, spectral centroid, and degree of spectral variation. The results further suggest that musical timbres possess specific attributes not accounted for by these shared perceptual dimensions. Weight patterns indicate that perceptual salience of dimensions and specificities varied across classes. A comparison of class structure with biographical factors associated with},
annote = {From Duplicate 1 (Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes - Mcadams, Stephen; Winsberg, Suzanne; Donnadieu, Sophie; De Soete, Geert; Krimphoff, Jochen; Mcadams, S; Donnadieu, S; De Soete, G)

Week 3},
author = {Mcadams, Stephen and Winsberg, Suzanne and Donnadieu, Sophie and {De Soete}, Geert and Krimphoff, Jochen and Mcadams, Stephen and Donnadieu, Sophie and {De Soete}, Geert},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mcadams et al. - 1995 - Perceptual scaling of synthesized musical timbres Common dimensions, specificities, and latent subject classes.pdf:pdf},
journal = {Psychol Res},
publisher = {Springer-Verlag},
title = {{Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes}},
volume = {58},
year = {1995}
}
@phdthesis{Chowdhury2022Thesis,
author = {Chowdhury, Shreyan},
file = {:Users/huanzhang/Downloads/Modelling Emotional Expression in Music Using Interpretable and Transferable Perceptual Features.pdf:pdf},
school = {Johannes Kepler University Linz},
title = {{Modelling Emotional Expression in Music Using Interpretable and Transferable Perceptual Features}},
year = {2022}
}
@inproceedings{Li2024Diff-BGMGeneration,
abstract = {When editing a video, a piece of attractive background music is indispensable. However, video background music generation tasks face several challenges, for example, the lack of suitable training datasets, and the difficulties in flexibly controlling the music generation process and sequentially aligning the video and music. In this work, we first propose a high-quality music-video dataset BGM909 with detailed annotation and shot detection to provide multi-modal information about the video and music. We then present evaluation metrics to assess music quality, including music diversity and alignment between music and video with retrieval precision metrics. Finally, we propose the Diff-BGM framework to automatically generate the background music for a given video, which uses different signals to control different aspects of the music during the generation process, i.e., uses dynamic video features to control music rhythm and semantic features to control the melody and atmosphere. We propose to align the video and music sequentially by introducing a segment-aware cross-attention layer. Experiments verify the effectiveness of our proposed method. The code and models are available at https://github.com/sizhelee/Diff-BGM.},
archivePrefix = {arXiv},
arxivId = {2405.11913},
author = {Li, Sizhe and Qin, Yiming and Zheng, Minghang and Jin, Xin and Liu, Yang},
booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition},
eprint = {2405.11913},
file = {:Users/huanzhang/Downloads/Li_Diff-BGM_A_Diffusion_Model_for_Video_Background_Music_Generation_CVPR_2024_paper.pdf:pdf},
title = {{Diff-BGM: A Diffusion Model for Video Background Music Generation}},
url = {http://arxiv.org/abs/2405.11913},
year = {2024}
}
@article{Maslen2013,
abstract = {Studies of classical music have typically neglected the physical production of musical sound, in line with an ideal that a musician's performance appears effortless. However, the costs of elite classical musician performance are increasingly becoming evident, as studies capture the very high incidence of injury and doping. This paper takes a preliminary step towards critically analysing elite classical musician performance enhancement measures and health implications by examining the contexts of music performance. Taking the case of pianists, I address the question of 'enhancement to what end?' arguing that what counts as optimum performance, and the methods by which it is achieved, is situated historically and socioculturally, and can be specific to factors such as gender. I suggest that studies of elite classical musician performance enhancement and health need to take account of this context, and contribute to this research approach through an examination of the cases of the 'accomplished' pianist, the classical pianist, and the female classical pianist. {\textcopyright} 2013 Elsevier Ltd.},
author = {Maslen, Sarah},
doi = {10.1016/j.peh.2013.01.001},
file = {:Users/huanzhang/Downloads/1-s2.0-S2211266913000029-main.pdf:pdf},
issn = {22112669},
journal = {Performance Enhancement and Health},
keywords = {Gender,Musicians,Performance,Piano,Sociology,Technique},
number = {1},
pages = {3--7},
publisher = {Elsevier Ltd},
title = {{'Playing like a girl': Practices and performance ideals at the piano}},
url = {http://dx.doi.org/10.1016/j.peh.2013.01.001},
volume = {2},
year = {2013}
}
@article{Benetos2019,
abstract = {The capability of transcribing music audio into music notation is a fascinating example of human intelligence. It involves perception (analyzing complex auditory scenes), cognition (recognizing musical objects), knowledge representation (forming musical structures), and inference (testing alternative hypotheses). Automatic music transcription (AMT), i.e., the design of computational algorithms to convert acoustic music signals into some form of music notation, is a challenging task in signal processing and artificial intelligence. It comprises several subtasks, including multipitch estimation (MPE), onset and offset detection, instrument recognition, beat and rhythm tracking, interpretation of expressive timing and dynamics, and score typesetting.},
annote = {From Duplicate 1 (Automatic Music Transcription: An Overview - Benetos, Emmanouil; Dixon, Simon; Duan, Zhiyao; Ewert, Sebastian)

This is a review that Benetos wrote, maybe for reference.},
author = {Benetos, Emmanouil and Dixon, Simon and Duan, Zhiyao and Ewert, Sebastian},
doi = {10.1109/MSP.2018.2869928},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Benetos et al. - 2019 - Automatic Music Transcription An Overview.pdf:pdf},
issn = {15580792},
journal = {IEEE Signal Processing Magazine},
number = {1},
pages = {20--30},
publisher = {IEEE},
title = {{Automatic Music Transcription: An Overview}},
volume = {36},
year = {2019}
}
@inproceedings{Flossmann2011TowardsChopin,
author = {Flossmann, S and Widmer, G},
booktitle = {International Symposium on Performance Science},
file = {:Users/huanzhang/Downloads/Flossmann_Widmer_ISPS_2011_errormodel.pdf:pdf},
isbn = {9789490306021},
keywords = {both when practicing,chopin corpus,due to the lack,er-,error catalogue,in private and when,of precisely meas-,perennial issue for musicians,performance errors,performance errors are a,performing in public,piano performance,ror model},
title = {{Toward a Model of Performance Errors: A Qualitative Review of {Magaloff's Chopin}}},
year = {2011}
}
@inproceedings{Wu2019DisentanglingDistillation,
abstract = {It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation differently and unpredictably. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This constraint encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256 × 256 resolution that are clearly disentangled in content and style.},
archivePrefix = {arXiv},
arxivId = {1905.04538},
author = {Wu, Wayne and Cao, Kaidi and Li, Cheng and Qian, Chen and Loy, Chen Change},
booktitle = {International Conference on Learning Representations Workshop Proceedings},
eprint = {1905.04538},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - DISENTANGLING CONTENT AND STYLE VIA UNSUPERVISED GEOMETRY DISTILLATION.pdf:pdf},
title = {{Disentangling content and style via unsupervised geometry distillation}},
year = {2019}
}
@article{Korzeniowski2021,
abstract = {Artist similarity plays an important role in organizing, understanding, and subsequently, facilitating discovery in large collections of music. In this paper, we present a hybrid approach to computing similarity between artists using graph neural networks trained with triplet loss. The novelty of using a graph neural network architecture is to combine the topology of a graph of artist connections with content features to embed artists into a vector space that encodes similarity. To evaluate the proposed method, we compile the new OLGA dataset, which contains artist similarities from AllMusic, together with content features from AcousticBrainz. With 17,673 artists, this is the largest academic artist similarity dataset that includes content-based features to date. Moreover, we also showcase the scalability of our approach by experimenting with a much larger proprietary dataset. Results show the superiority of the proposed approach over current state-of-the-art methods for music similarity. Finally, we hope that the OLGA dataset will facilitate research on data-driven models for artist similarity.},
archivePrefix = {arXiv},
arxivId = {2107.14541},
author = {Korzeniowski, Filip and Oramas, Sergio and Gouyon, Fabien},
eprint = {2107.14541},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Korzeniowski, Oramas, Gouyon - Unknown - ARTIST SIMILARITY WITH GRAPH NEURAL NETWORKS.pdf:pdf},
title = {{Artist Similarity with Graph Neural Networks}},
url = {http://arxiv.org/abs/2107.14541},
year = {2021}
}
@article{Dixon2006,
abstract = {We report three experiments examining the perception of tempo in expressively performed classical piano music. Each experiment investigates beat and tempo perception in a different way: rating the correspondence of a click track to a musical excerpt with which it was simultaneously presented; graphically marking the positions of the beats using an interactive computer program; and tapping in time with the musical excerpts. We examine the relationship between the timing of individual tones, that is, the directly measurable temporal information, and the timing of beats as perceived by listeners. Many computational models of beat tracking assume that beats correspond with the onset of musical tones. We introduce a model, supported by the experimental results, in which the beat times are given by a curve calculated from the tone onset times that is smoother (less irregular) than the tempo curve of the onsets. Tempo and beat are well-defined concepts in the abstract setting of a musical score, but not in the context of analysis of expressive musical performance. That is, the regular pulse, which is the basis of rhythmic notation in common music notation, is anything but regular when the timing of performed notes is measured. These deviations from mechanical timing are an important part of musical expression, although they remain, for the most part, poorly understood. In this study we report on three experiments using one set of musical excerpts, which investigate the characteristics of the relationship between performed timing and perceived local tempo. The experiments address this relationship via the following tasks: rating the correspondence of a click track to a musical excerpt with which it was},
author = {Dixon, Simon and Goebl, Werner and Cambouropoulos, Emilios},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dixon, Goebl, Cambouropoulos - 2006 - Perceptual Smoothness of Tempo in Expressively Performed Music.pdf:pdf},
journal = {Music Perception},
number = {3},
pages = {195--214},
title = {{Perceptual Smoothness of Tempo in Expressively Performed Music}},
volume = {23},
year = {2006}
}
@article{Marchini2014TheQuartets,
abstract = {Computational approaches for modelling expressive music performance have produced systems that emulate music expression, but few steps have been taken in the domain of ensemble performance. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of interdependence among musicians we propose the introduction of inter-voice contextual attributes. We evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. We used machine-learning algorithms to produce computational models for predicting intensity, timing deviations, vibrato extent , and bowing speed of each note. As a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. Furthermore, results on attribute selection show that the models trained on ensemble recordings took more advantage of inter-voice contextual attributes than those trained on solo recordings.},
author = {Marchini, Marco and Ramirez, Rafael and Papiotis, Panos and Maestre, Esteban},
doi = {10.1080/09298215.2014.922999},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Marchini et al. - 2014 - Journal of New Music Research The Sense of Ensemble a Machine Learning Approach to Expressive Performance Model.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
keywords = {ensemble music,expressive performance,machine learning,music analysis,performance modelling},
number = {3},
pages = {303--317},
title = {{The Sense of Ensemble: a Machine Learning Approach to Expressive Performance Modelling in String Quartets}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20http://dx.doi.org/10.1080/09298215.2014.922999},
volume = {43},
year = {2014}
}
@inproceedings{Zhao2021ViolinistFeatures,
abstract = {Identifying performers from polyphonic music is a challenging task in music information retrieval. As a ubiquitous expressive element in violin music, vibrato contains important information about the performers' interpretation. This paper proposes to use vibrato features for identifying violinists from commercial orchestral recordings. We present and compare two systems, which take the same note-level melodies as input while using different vibrato feature extractors and classification schemes. One system calculates vibrato features according to vibrato definition, models the feature distribution using histograms, and classifies performers based on the distribution similarity. The other system uses the adaptive wavelet scattering which contains vibrato information and identifies violinists with a machine learning classifier. We report accuracy improvement of 19.8% and 17.8%, respectively, over a random baseline on piece-level evaluation. This suggests that vibrato notes in polyphonic music are useful for master violinist identification.},
author = {Zhao, Yudong and Wang, Changhong and Fazekas, Gy{\"{o}}rgy and Benetos, Emmanouil and Sandler, Mark},
booktitle = {European Signal Processing Conference},
doi = {10.23919/EUSIPCO54536.2021.9616197},
file = {:Users/huanzhang/Downloads/Violinist_identification_based_on_vibrato_features.pdf:pdf},
isbn = {9789082797060},
issn = {22195491},
keywords = {Music signal processing,Statistical modelling,Vibrato feature,Violinist identification,Wavelet scattering},
title = {{Violinist identification based on vibrato features}},
year = {2021}
}
@inproceedings{Wu2020PekingNetwork,
abstract = {Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network (DurIAN) framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness.},
archivePrefix = {arXiv},
arxivId = {2008.03029},
author = {Wu, Yusong and Li, Shengchen and Yu, Chengzhu and Lu, Heng and Weng, Chao and Zhang, Liqiang and Yu, Dong},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
doi = {10.21437/Interspeech.2020-1724},
eprint = {2008.03029},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2020 - Peking opera synthesis via duration informed attention network.pdf:pdf},
issn = {19909772},
keywords = {Deep learning,Expressive singing synthesis,Lagrange multiplier,Machine learning,Singing synthesis},
pages = {1226--1230},
title = {{Peking opera synthesis via duration informed attention network}},
year = {2020}
}
@inproceedings{Hsu2017UnsupervisedData,
abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1709.07902},
author = {Hsu, Wei Ning and Zhang, Yu and Glass, James},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1709.07902},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hsu, Zhang, Glass - Unknown - Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.pdf:pdf},
issn = {10495258},
title = {{Unsupervised learning of disentangled and interpretable representations from sequential data}},
url = {https://youtu.be/naJZITvCfI4.},
year = {2017}
}
@article{Repp2010RateSubdivision,
annote = {From Duplicate 2 (Rate Limits in Sensorimotor Synchronization The Synchronization Threshold and the With Auditory and Visual Sequences: Benefits and Costs of Interval Subdivision - Repp, Bruno H)

Week 4},
author = {Repp, Bruno H},
doi = {10.1080/00222890309603156},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - 2010 - Interval Subdivision.pdf:pdf},
issn = {1940-1027},
journal = {Journal of Motor Behavior},
number = {4},
pages = {355--370},
title = {{Rate Limits in Sensorimotor Synchronization The Synchronization Threshold and the With Auditory and Visual Sequences: Benefits and Costs of Interval Subdivision}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=vjmb20},
volume = {35},
year = {2010}
}
@inproceedings{Seshadri2021ImprovingLearning,
abstract = {Several automatic approaches for objective music performance assessment (MPA) have been proposed in the past, however, existing systems are not yet capable of reliably predicting ratings with the same accuracy as professional judges. This study investigates contrastive learning as a potential method to improve existing MPA systems. Con-trastive learning is a widely used technique in representation learning to learn a structured latent space capable of separately clustering multiple classes. It has been shown to produce state of the art results for image-based classification problems. We introduce a weighted contrastive loss suitable for regression tasks applied to a convolutional neural network and show that contrastive loss results in performance gains in regression tasks for MPA. Our results show that contrastive-based methods are able to match and exceed SoTA performance for MPA regression tasks by creating better class clusters within the latent space of the neural networks.},
author = {Seshadri, Pavan and Lerch, Alexander},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Seshadri, Lerch - Unknown - IMPROVING MUSIC PERFORMANCE ASSESSMENT WITH CONTRASTIVE LEARNING(2).pdf:pdf},
title = {{Improving Music Performance Assessment with Contrastive Learning}},
url = {https://github.com/pseshadri9/contrastive-},
year = {2021}
}
@article{Hernandez-Olivan,
abstract = {In this article, we present musicaiz, an object-oriented library for analyzing, generating and evaluating symbolic music. The submodules of the package allow the user to create symbolic music data from scratch, build algorithms to analyze symbolic music, encode MIDI data as tokens to train deep learning sequence models, modify existing music data and evaluate music generation systems. The evaluation submodule builds on previous work to objectively measure music generation systems and to be able to reproduce the results of music generation models. The library is publicly available online. We encourage the community to contribute and provide feedback.},
archivePrefix = {arXiv},
arxivId = {2209.07974v1},
author = {Hernandez-Olivan, Carlos and Beltran, Jose R},
eprint = {2209.07974v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hernandez-Olivan, Beltran - Unknown - musicaiz A Python Library for Symbolic Music Generation, Analysis and Visualization.pdf:pdf},
keywords = {deep learning,machine learning,music generation,music information retrieval},
title = {{musicaiz: A Python Library for Symbolic Music Generation, Analysis and Visualization}},
url = {https://carlosholivan.github.io/}
}
@article{LocatelloChallengingRepresentations,
abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12 000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties "encouraged" by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"{a}}tsch, Gunnar and Gelly, Sylvain and Sch{\"{o}}lkopf, Bernhard and Bachem, Olivier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Locatello et al. - Unknown - Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.pdf:pdf},
title = {{Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations}},
url = {https://github.com/google-research/}
}
@article{Dalmazzo2017AirRecognition,
abstract = {We train and evaluate two machine learning models for predicting fingering in violin performances using motion and EMG sensors integrated in the Myo device. Our aim is twofold: first, provide a fingering recognition model in the context of a gamification virtual violin application where we measure both right hand (i.e. bow) and left hand (i.e. fingering) gestures, and second, implement a tracking system for a computer assisted pedagogical tool for self-regulated learners in high-level music education. Our approach is based on the principle of mapping-by-demonstration in which the model is trained by the performer. We evaluated a model based on Decision Trees and compared it with a Hidden Markovian Model.},
author = {Dalmazzo, David and Ramirez, Rafael},
doi = {10.1145/3139513.3139526},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dalmazzo, Ramirez - 2017 - Air violin A machine learning approach to fingering gesture recognition.pdf:pdf},
isbn = {9781450355575},
journal = {MIE 2017 - Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education, Co-located with ICMI 2017},
keywords = {Gamification,Gestures,HMM,Hand tracking,Machine learning,Music education,Violin},
pages = {63--66},
title = {{Air violin: A machine learning approach to fingering gesture recognition}},
volume = {2017-Novem},
year = {2017}
}
@article{Goebl2001MelodyArtifact,
abstract = {As reported in the recent literature on piano performance, an emphasized voice the melody tends to be played not only louder than the other voices, but also about 30 ms earlier melody lead. It remains unclear whether pianists deliberately apply melody lead to separate different voices, or whether it occurs because the melody is played louder velocity artifact. The velocity artifact explanation implies that pianists initially strike the keys simultaneously; it is only different velocities that make the hammers arrive at different points in time. The measured note onsets in these studies, mostly derived from computer-monitored pianos, represent the hammer-string impact times. In the present study, the finger-key contact times are calculated and analyzed as well. If the velocity artifact hypothesis is correct, the melody lead phenomenon should disappear at the finger-key level. Chopin's Ballade op. 38 45 measures and Etude op. 10/3 21 measures were performed on a B{\"{o}} sendorfer computer-monitored grand piano by 22 skilled pianists. The hammer-string asynchronies among voices closely resemble the results reported in the literature. However, the melody lead decreases almost to zero at the finger-key level, which supports the velocity artifact hypothesis. In addition to this, expected onset asynchronies are predicted from differences in hammer velocity, if finger-key asynchronies are assumed to be zero. They correlate highly with the observed melody lead.},
author = {Goebl, Werner},
doi = {10.1121/1.1376133},
isbn = {10.1121/1.415889},
journal = {The Journal of the Acoustical Society of America},
keywords = {4375Mn RDA,4375St,PACS numbers},
pages = {641},
title = {{Melody lead in piano performance: Expressive device or artifact?}},
url = {https://doi.org/10.1121/1.417245https://doi.org/10.1121/1.1944648https://doi.org/10.1121/1.399933},
volume = {110},
year = {2001}
}
@article{Mehr2019,
abstract = {What is universal about music, and what varies? We built a corpus of ethnographic text on musical behavior from a representative sample of the world's societies, as well as a discography of audio recordings. The ethnographic corpus reveals that music (including songs with words) appears in every society observed; that music varies along three dimensions (formality, arousal, religiosity), more within societies than across them; and that music is associated with certain behavioral contexts such as infant care, healing, dance, and love. The discography—analyzed through machine summaries, amateur and expert listener ratings, and manual transcriptions—reveals that acoustic features of songs predict their primary behavioral context; that tonality is widespread, perhaps universal; that music varies in rhythmic and melodic complexity; and that elements of melodies and rhythms found worldwide follow power laws.},
annote = {From Duplicate 1 (Universality and diversity in human song - Mehr, Samuel A.; Singh, Manvir; Knox, Dean; Ketter, Daniel M.; Pickens-Jones, Daniel; Atwood, S.; Lucas, Christopher; Jacoby, Nori; Egner, Alena A.; Hopkins, Erin J.; Howard, Rhea M.; Hartshorne, Joshua K.; Jennings, Mariela V.; Simson, Jan; Bainbridge, Constance M.; Pinker, Steven; O'Donnell, Timothy J.; Krasnow, Max M.; Glowacki, Luke)

eHRAF World Cultures database},
author = {Mehr, Samuel A. and Singh, Manvir and Knox, Dean and Ketter, Daniel M. and Pickens-Jones, Daniel and Atwood, S. and Lucas, Christopher and Jacoby, Nori and Egner, Alena A. and Hopkins, Erin J. and Howard, Rhea M. and Hartshorne, Joshua K. and Jennings, Mariela V. and Simson, Jan and Bainbridge, Constance M. and Pinker, Steven and O'Donnell, Timothy J. and Krasnow, Max M. and Glowacki, Luke},
doi = {10.1126/science.aax0868},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mehr et al. - 2019 - Universality and diversity in human song.pdf:pdf},
issn = {10959203},
journal = {Science},
number = {6468},
pmid = {31753969},
title = {{Universality and diversity in human song}},
volume = {366},
year = {2019}
}
@inproceedings{Deshmukh2023PengiTasks,
abstract = {In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 21 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding.},
archivePrefix = {arXiv},
arxivId = {2305.11834},
author = {Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
booktitle = {In Proceedings of the Conference on Neural Information Processing Systems},
eprint = {2305.11834},
file = {:Users/huanzhang/Downloads/2305.11834v2.pdf:pdf},
issn = {10495258},
title = {{Pengi: An Audio Language Model for Audio Tasks}},
year = {2023}
}
@article{Ramirez2010AutomaticPerformances,
abstract = {We present a pattern recognition approach to the task of identifying performers from their interpretative styles. We investigate how professional musicians express their view of the musical content of musical pieces and how to use this information in order to automatically identify performers. We apply sound analysis techniques based on spectral models for extracting deviation patterns of parameters such as pitch, timing, amplitude and timbre characterising both the internal structure of notes and the musical context in which they appear. We describe successful performer identification case studies involving monophonic audio recordings of both score-guided and commercial improvised performances. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Ramirez, Rafael and Maestre, Esteban and Serra, Xavier},
doi = {10.1016/j.patrec.2009.12.032},
file = {:Users/huanzhang/Downloads/prl2010.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classification techniques,Expressive performance,Information retrieval,Music,Signal processing},
number = {12},
pages = {1514--1523},
publisher = {Elsevier B.V.},
title = {{Automatic performer identification in commercial monophonic Jazz performances}},
volume = {31},
year = {2010}
}
@article{Kim2022a,
abstract = {We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.},
archivePrefix = {arXiv},
arxivId = {2207.02505},
author = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
eprint = {2207.02505},
file = {:Users/huanzhang/Downloads/2207.02505.pdf:pdf},
number = {NeurIPS},
pages = {1--28},
title = {{Pure Transformers are Powerful Graph Learners}},
url = {http://arxiv.org/abs/2207.02505},
year = {2022}
}
@inproceedings{Flossmann2010MagaloffStudy,
author = {Flossmann, Sebastian and Goebl, Werner and Widmer, Gerhard},
booktitle = {Proceedings of the 11th International Conference on Music Perception and Cognition (ICMPC)},
file = {:Users/huanzhang/Downloads/Flossmann_etal_ICMPC_2010.pdf:pdf},
keywords = {[Electronic Manuscript]},
title = {{The {Magaloff} Corpus: An Empirical Error Study}},
year = {2010}
}
@article{Oramas2016,
abstract = {In this paper we present a gold standard dataset for Entity Linking (EL) in the Music Domain. It contains thousands of musical named entities such as Artist, Song or Record Label, which have been automatically annotated on a set of artist biographies coming from the Music website and social network Last.fm. The annotation process relies on the analysis of the hyperlinks present in the source texts and in a voting-based algorithm for EL, which considers, for each entity mention in text, the degree of agreement across three state-of-the-art EL systems. Manual evaluation shows that EL Precision is at least 94%, and due to its tunable nature, it is possible to derive annotations favouring higher Precision or Recall, at will. We make available the annotated dataset along with evaluation data and the code.},
author = {Oramas, Sergio and Espinosa-Anke, Luis and Sordo, Mohamed and Saggion, Horacio and Serra, Xavier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Oramas et al. - Unknown - ELMD An Automatically Generated Entity Linking Gold Standard Dataset in the Music Domain.pdf:pdf},
isbn = {9782951740891},
journal = {Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016},
keywords = {Entity Linking,Language resources,Music information retrieval},
pages = {3312--3317},
title = {{ELMD: An automatically generated Entity Linking gold standard dataset in the Music Domain}},
url = {https://github.com/dbpedia-spotlight},
year = {2016}
}
@phdthesis{Heeringa2005,
author = {Heeringa, Wilbert},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Heeringa - Unknown - Measuring Dialect Pronunciation Differences using Levenshtein Distance.pdf:pdf},
school = {University of Groningen},
title = {{Measuring Dialect Pronunciation Differences using {Levenshtein} Distance}},
year = {2005}
}
@article{WanQuanWangAlanPapirIgnacioLopezMorenoGeneralizedVerification,
abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation-training a more accurate model that supports multiple keywords (i.e., "OK Google" and "Hey Google") as well as multiple dialects.},
archivePrefix = {arXiv},
arxivId = {1710.10467v5},
author = {{Wan Quan Wang Alan Papir Ignacio Lopez Moreno}, Li},
eprint = {1710.10467v5},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wan Quan Wang Alan Papir Ignacio Lopez Moreno - Unknown - GENERALIZED END-TO-END LOSS FOR SPEAKER VERIFICATION(2).pdf:pdf},
keywords = {Index Terms-Speaker verification,Multi-Reader,end-to-end loss,keyword detection},
title = {{Generalized End-to-end Loss for Speaker Verification}},
url = {https://google.}
}
@inproceedings{Ens2021,
abstract = {We introduce the MetaMIDI Dataset (MMD), a large scale collection of 436,631 MIDI files and metadata. MMD contains artist and title metadata for 221,504 MIDI files, and genre metadata for 143,868 MIDI files, collected during the web-scraping process. MIDI files in MMD were matched against a collection of 32,000,000 30-second audio clips retrieved from Spotify, resulting in over 10,796,557 audio-MIDI matches. In addition, we linked 600,142 Spotify tracks with 1,094,901 MusicBrainz recordings to produce a set of 168,032 MIDI files that are matched to the MusicBrainz database. We also provide a set of 53,496 MIDI files using audio-MIDI matches where the derived metadata on Spotify is a fuzzy match to the web-scraped metadata. These links augment many files in the dataset with the extensive metadata available via the Spotify API and the MusicBrainz database. We anticipate that this collection of data will be of great use to MIR researchers addressing a variety of research topics.},
author = {Ens, Jeff and Pasquier, Philippe},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ens, Pasquier - Unknown - BUILDING THE METAMIDI DATASET LINKING SYMBOLIC AND AUDIO MUSICAL DATA.pdf:pdf},
title = {{Building the MetaMidi Dataset: Linking Symbolic and Audio Musical Data}},
url = {https://github.com/craffel/midi-dataset},
year = {2021}
}
@inproceedings{Kingma2015AdamOptimization,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1412.6980},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Lei Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Zatorre1988,
annote = {From Duplicate 2 (Pitch Perception of Complex Tones and Human Temporal-lobe Function - Zatorre, Robert J)

Week 1 reading},
author = {Zatorre, Robert J},
doi = {10.1121/1.396834},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zatorre - 1988 - A Duplex Theory of Pitch Perception The.pdf:pdf},
journal = {Citation: The Journal of the Acoustical Society of America},
pages = {1811},
title = {{Pitch Perception of Complex Tones and Human Temporal-lobe Function}},
url = {https://doi.org/10.1121/1.396834},
volume = {84},
year = {1988}
}
@article{Chen2022,
abstract = {Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large GPU memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model's scalability in audio tasks. To combat these problems, we introduce HTS-AT: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate HTS-AT on three datasets of audio classification where it achieves new state-of-the-art (SOTA) results on AudioSet and ESC-50, and equals the SOTA on Speech Command V2. It also achieves better performance in event localization than the previous CNN-based models. Moreover, HTS-AT requires only 35% model parameters and 15% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of HTS-AT.},
archivePrefix = {arXiv},
arxivId = {2202.00874},
author = {Chen, Ke and Du, Xingjian and Zhu, Bilei and Ma, Zejun and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
doi = {10.1109/ICASSP43922.2022.9746312},
eprint = {2202.00874},
file = {:Users/huanzhang/Downloads/2202.00874.pdf:pdf},
isbn = {9781665405409},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Audio Classification,Sound Event Detection,Token-Semantic Module,Transformer},
pages = {646--650},
title = {{Hts-At: a Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection}},
volume = {2022-May},
year = {2022}
}
@article{,
file = {:Users/huanzhang/Downloads/Repp--1995--Acoustics-Perception-Legato-Articulation.pdf:pdf},
isbn = {9786021018187},
issn = {2252-3405},
keywords = {Decision Sciences,Education,Operations Research & Management},
number = {August},
pages = {128},
title = {{Acoustics, perception, and production of legato articulation on a digital piano}},
volume = {6},
year = {2016}
}
@inproceedings{Chowdhury2021OnFeatures,
abstract = {Despite recent advances in audio content-based music emotion recognition, a question that remains to be explored is whether an algorithm can reliably discern emotional or expressive qualities between different performances of the same piece. In the present work, we analyze several sets of features on their effectiveness in predicting arousal and va-lence of six different performances (by six famous pianists) of Bach's Well-Tempered Clavier Book 1. These features include low-level acoustic features, score-based features, features extracted using a pre-trained emotion model, and Mid-level perceptual features. We compare their predictive power by evaluating them on several experiments designed to test performance-wise or piece-wise variations of emotion. We find that Mid-level features show significant contribution in performance-wise variation of both arousal and valence-even better than the pre-trained emotion model. Our findings add to the evidence of Mid-level perceptual features being an important representation of musical attributes for several tasks-specifically, in this case, for capturing the expressive aspects of music that manifest as perceived emotion of a musical performance.},
address = {Online},
author = {Chowdhury, Shreyan and Widmer, Gerhard},
booktitle = {Proceeding of the 22nd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chowdhury, Widmer - Unknown - ON PERCEIVED EMOTION IN EXPRESSIVE PIANO PERFORMANCE FURTHER EXPERIMENTAL EVIDENCE FOR THE RELEVANCE OF MI.pdf:pdf},
title = {{ON PERCEIVED EMOTION IN EXPRESSIVE PIANO PERFORMANCE: FURTHER EXPERIMENTAL EVIDENCE FOR THE RELEVANCE OF MID-LEVEL PERCEPTUAL FEATURES}},
year = {2021}
}
@article{Szelogowski2022,
author = {Szelogowski, Daniel and Whitcomb, Benjamin},
file = {:Users/huanzhang/Downloads/152.pdf:pdf},
title = {{A NOVEL DATASET AND DEEP LEARNING BENCHMARK FOR CLASSICAL MUSIC FORM RECOGNITION AND ANALYSIS}},
year = {2022}
}
@inproceedings{Velarde2016ComposerPiano-rolls,
abstract = {We propose a method for music classification based on the use of convolutional models on symbolic pitch–time representations (i.e. piano-rolls) which we apply to composer recognition. An excerpt of a piece to be classified is first sampled to a 2D pitch–time representation which is then subjected to various transformations, including convolution with predefined filters (Morlet or Gaussian) and classified by means of support vector machines. We combine classifiers based on different pitch representations (MIDI and morphetic pitch) and different filter types and configurations. The method does not require parsing of the music into separate voices, or extraction of any other predefined features prior to processing; instead it is based on the analysis of texture in a 2D pitch–time representation. We show that filtering significantly improves recognition and that the method proves robust to encoding, transposition and amount of information. On discriminating between Haydn and Mozart string quartet movements, our best classifier reaches state-of-the-art performance in leave-one-out cross validation.},
author = {Velarde, Gissel and Weyde, Tillman and Cancino-Chac{\'{o}}n, Carlos E. and Meredith, David and Grachten, Maarten},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Velarde et al. - Unknown - COMPOSER RECOGNITION BASED ON 2D-FILTERED PIANO-ROLLS.pdf:pdf},
isbn = {9780692755068},
title = {{Composer recognition based on 2D-filtered piano-rolls}},
url = {https://www.semanticscholar.org/paper/Composer-Recognition-Based-on-2D-Filtered-Velarde-Weyde/2ee8df37e3f5363c573b2aeed2243034ea638f71},
year = {2016}
}
@article{Yang2024,
author = {Yang, Yuxin and Zhu, Pengfei and Qi, Mengshi},
file = {:Users/huanzhang/Downloads/8347.pdf:pdf},
pages = {1--5},
title = {{FOR REVIEW ONLY FOLLOWING IN THE FOOTSTEPS : FOR REVIEW ONLY FOR REVIEW ONLY FOR REVIEW ONLY}},
year = {2024}
}
@inproceedings{Katakis2021,
abstract = {In this paper, we present a Deep Learning (DL) approach to tackle a real-world, large-scale music entity matching task. The quality of data, the lack of necessary information, and the absence of unique identifiers affect the effectiveness of entity matching and pose many challenges to the matching process. We propose an efficient matching method for linking recordings to their compositions through metadata using pre-trained language models. We represent each entity as a vector and estimate the similarity between vectors for a pair of entities. Our experiments show that an application of language models such as BERT, DistilBERT or ALBERT to large text corpora significantly improves the matching quality at an industrial level. We created a human-annotated dataset with sound recordings and composition pairs obtained from music usage logs and publishers, respectively. The proposed language model achieves 95% precision and reaches 96.5% recall which is a high performance on this challenging task.},
author = {Katakis, Nikiforos and Vikatos, Pantelis},
booktitle = {Proceedings of the 17th International Conference on Web Information Systems and Technologies - Volume 1: WEBIST},
doi = {10.5220/0010713900003058},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Katakis, Vikatos - Unknown - Entity Linking of Sound Recordings and Compositions with Pre-trained Language Models.pdf:pdf},
isbn = {9789897585364},
keywords = {Deep Learning,Industrial Entity Matching,Language Models},
pages = {474--481},
title = {{Entity Linking of Sound Recordings and Compositions with Pre-trained Language Models}},
year = {2021}
}
@inproceedings{Zeng2021MusicBERTPre-Training,
abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.},
archivePrefix = {arXiv},
arxivId = {2106.05630},
author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie Yan},
booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP},
doi = {10.18653/v1/2021.findings-acl.70},
eprint = {2106.05630},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - Unknown - MusicBERT Symbolic Music Understanding with Large-Scale Pre-Training.pdf:pdf},
isbn = {9781954085541},
title = {{{MusicBERT}: Symbolic Music Understanding with Large-Scale Pre-Training}},
year = {2021}
}
@article{Bhatt2024CharacterizingAnalysis,
abstract = {Audio analysis is useful in many application scenarios. The state-of-the-art audio analysis approaches assume the data distribution at training and deployment time will be the same. However, due to various real-life challenges, the data may encounter drift in its distribution or can encounter new classes in the late future. Thus, a one-time trained model might not perform adequately. Continual learning (CL) approaches are devised to handle such changes in data distribution. There have been a few attempts to use CL approaches for audio analysis. Yet, there is a lack of a systematic evaluation framework. In this paper, we create a comprehensive CL dataset and characterize CL approaches for audio-based monitoring tasks. We have investigated the following CL and non-CL approaches: EWC, LwF, SI, GEM, A-GEM, GDumb, Replay, Naive, Cumulative, and Joint training. The study is very beneficial for researchers and practitioners working in the area of audio analysis for developing adaptive models. We observed that Replay achieved better results than other methods in the DCASE challenge data. It achieved an accuracy of 70.12% for the domain incremental scenario and an accuracy of 96.98% for the class incremental scenario.},
archivePrefix = {arXiv},
arxivId = {2407.00465},
author = {Bhatt, Ruchi and Kumari, Pratibha and Mahapatra, Dwarikanath and Saddik, Abdulmotaleb El and Saini, Mukesh},
eprint = {2407.00465},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bhatt et al. - 2024 - Characterizing Continual Learning Scenarios and Strategies for Audio Analysis.pdf:pdf},
journal = {Arxiv preprint arXiv:2407.00465},
month = {jun},
title = {{Characterizing Continual Learning Scenarios and Strategies for Audio Analysis}},
url = {http://arxiv.org/abs/2407.00465},
year = {2024}
}
@article{Nakamura2016a,
abstract = {This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.07748v1},
author = {Nakamura, Tomohiko and Nakamura, Eita and Sagayama, Shigeki},
doi = {10.1109/TASLP.2015.2507862},
eprint = {arXiv:1512.07748v1},
file = {:Users/huanzhang/Downloads/1512.07748.pdf:pdf},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Arbitrary repeats and skips,Audio-to-score alignment,Fast viterbi algorithm,Hidden markov model,Music signal processing,Score following},
number = {2},
pages = {329--339},
title = {{Real-time audio-to-score alignment of music performances containing errors and arbitrary repeats and skips}},
volume = {24},
year = {2016}
}
@book{Sandor1981OnExpression,
annote = {From Duplicate 1 (On Piano Playing: Motion, Emotion and Expression - Sandor, Georgy)

Page 14: the ability to vary tone color},
author = {Sandor, Georgy},
file = {:Users/huanzhang/04MusicalResources/03 Books and Papers/gyorgy-gyorgy-sandor-sandor-on-piano-playing-motion-1981.pdf:pdf},
title = {{On Piano Playing: Motion, Emotion and Expression}},
year = {1981}
}
@article{Chen2019Conduct:Control,
abstract = {Recent research in music-gesture relationship has paid more attention on the sound variations and its corresponding gesture expressiveness. In this study we are interested by gestures performed by orchestral conductors, with a focus on the expressive gestures made by the non dominant hand. We make the assumption that these gestures convey some meaning shared by most of conductors, and that they implicitly correspond to sound effects which can be encoded in musical scores. Following this hypothesis, we defined a collection of gestures for musical direction. These gestures are designed to correspond to well known functional effect on sounds, and they can be modulated to vary this effect by simply modifying one of their structural component (hand movement or hand shape). This paper presents the design of the gesture and sound sets and the protocol that has led to the database construction. The relevant musical excerpts and the related expressive gestures have been first defined by one expert musician. The gestures were then recorded through motion capture by two non experts who performed them along with recorded music. This database will serve as a basis for training gesture recognition system for live sound control and modulation.},
author = {Chen, Lei and Gibet, Sylvie and Marteau, Camille},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Gibet, Marteau - 2019 - Conduct An expressive conducting gesture dataset for sound control.pdf:pdf},
isbn = {9791095546009},
journal = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
keywords = {Conducting,Corpus,Expressive gesture,Sound-control gestures},
pages = {1719--1725},
title = {{Conduct: An expressive conducting gesture dataset for sound control}},
year = {2019}
}
@inproceedings{Anderson2016SPICEevaluation,
abstract = {There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?.},
archivePrefix = {arXiv},
arxivId = {1607.08822},
author = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
booktitle = {In Proceedings of the European Conference on Computer Vision},
doi = {10.1007/978-3-319-46454-1_24},
eprint = {1607.08822},
file = {:Users/huanzhang/Downloads/1607.08822v1.pdf:pdf},
isbn = {9783319464534},
issn = {16113349},
title = {{SPICE: Semantic propositional image caption evaluation}},
year = {2016}
}
@article{Arzt2014TheCompanion,
abstract = {We present a system that we call 'The Piano Music Companion' and that is able to follow and understand (at least to some extent) a live piano performance. Within a few seconds this system can identify the piece that is being played, and the position within the piece. It then tracks the progress of the performer over time via a robust score following algorithm. The companion is useful in multiple ways, e.g., it can be used for piece identification, music visualisation, during piano rehearsal and for automatic page turning.},
author = {Arzt, Andreas and B{\"{o}}ck, Sebastian and Flossmann, Sebastian and Frostel, Harald and Gasser, Martin and Liem, Cynthia C.S. S and Widmer, Gerhard},
doi = {10.3233/978-1-61499-419-0-1221},
file = {:Users/huanzhang/Downloads/FAIA263-1221.pdf:pdf},
isbn = {9781614994183},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
pages = {1221--1222},
title = {{The piano music companion}},
volume = {263},
year = {2014}
}
@inproceedings{Cosenza2023GraphGeneration,
abstract = {Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.},
archivePrefix = {arXiv},
arxivId = {2307.14928},
author = {Cosenza, Emanuele and Valenti, Andrea and Bacciu, Davide},
booktitle = {In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
eprint = {2307.14928},
file = {:Users/huanzhang/Downloads/2307.14928.pdf:pdf},
title = {{Graph-based Polyphonic Multitrack Music Generation}},
url = {http://arxiv.org/abs/2307.14928},
year = {2023}
}
@inproceedings{Kosta2022,
annote = {interpretability: RELEIF feature ranking},
author = {Kosta, Katerina and Lu, Wei Tsung and Medeot, Gabriele and Chanquion, Pierre},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/196.pdf:pdf},
title = {{A Deep Learning Method for Melody Extraction from a Polyphonic Symbolic Music Representation}},
year = {2022}
}
@article{Girin2021,
abstract = {Variational autoencoders (VAEs) are powerful deep gen-erative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which 2 encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimple-mented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier and Alameda, Xavier},
doi = {10.1561/2200000089},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Girin et al. - 2021 - Dynamical Variational Autoencoders A Compre-hensive Review.pdf:pdf},
journal = {Foundations and Trends {\textregistered} in Machine Learning},
number = {2},
pages = {1--175},
title = {{Dynamical Variational Autoencoders: A Compre-hensive Review}},
volume = {15},
year = {2021}
}
@phdthesis{Prang2021RepresentationMusic,
author = {Prang, Mathieu},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Prang - 2021 - Representation learning for symbolic music(3).pdf:pdf},
school = {IRCAM},
title = {{Representation learning for symbolic music}},
url = {https://hal.archives-ouvertes.fr/tel-03329980},
year = {2021}
}
@article{Gardner2022,
abstract = {Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information. Further, many AMT datasets are “low-resource”, as even expert musicians find music transcription difficult and time-consuming. Thus, prior work has focused on task-specific architectures, tailored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT.},
archivePrefix = {arXiv},
arxivId = {2111.03017},
author = {Gardner, Josh and Simon, Ian and Manilow, Ethan and Hawthorne, Curtis and Engel, Jesse},
eprint = {2111.03017},
file = {:Users/huanzhang/Downloads/2111.03017v4.pdf:pdf},
journal = {ICLR 2022 - 10th International Conference on Learning Representations},
pages = {1--21},
title = {{Mt3: Multi-Task Multitrack Music Transcription}},
year = {2022}
}
@article{Wang2024,
author = {Of, Evaluation and Satellite, Passive and Sensing, Remote and Cloud, O F and Water, Liquid},
file = {:Users/huanzhang/Downloads/7471.pdf:pdf},
number = {February},
pages = {2--3},
title = {{for Review Only for Review Only for Review Only for Review Only}},
year = {2009}
}
@article{Burns2020ComputerRetrieval,
author = {Burns, Anne-marie and Wanderley, Marcelo M and Vision, Computer and For, Method and Burns, Anne-marie and Wanderley, Marcelo M},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Burns et al. - 2020 - COMPUTER VISION METHOD FOR GUITARIST FINGERING RETRIEVAL Anne-Marie Burns , Marcelo Wanderley To cite this version.pdf:pdf},
title = {{Computer Vision Method for Guitarist Fingering Retrieval}},
year = {2020}
}
@article{Khulusi2020AData,
abstract = {Digital methods are increasingly applied to store, structure and analyse vast amounts of musical data. In this context, visualization plays a crucial role, as it assists musicologists and non-expert users in data analysis and in gaining new knowledge. This survey focuses on this unique link between musicology and visualization. We classify 129 related works according to the visualized data types, and we analyse which visualization techniques were applied for certain research inquiries and to fulfill specific tasks. Next to scientific references, we take commercial music software and public websites into account, that contribute novel concepts of visualizing musicological data. We encounter different aspects of uncertainty as major problems when dealing with musicological data and show how occurring inconsistencies are processed and visually communicated. Drawing from our overview in the field, we identify open challenges for research on the interface of musicology and visualization to be tackled in the future.},
author = {Khulusi, R. and Kusnick, J. and Meinecke, C. and Gillmann, C. and Focht, J. and J{\"{a}}nicke, S.},
doi = {10.1111/cgf.13905},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Khulusi et al. - 2020 - A Survey on Visualizations for Musical Data.pdf:pdf},
issn = {0167-7055},
journal = {Computer Graphics Forum},
keywords = {information visualization,visualization},
month = {mar},
pages = {cgf.13905},
publisher = {Blackwell Publishing Ltd},
title = {{A Survey on Visualizations for Musical Data}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13905},
year = {2020}
}
@inproceedings{Huang2020PopCompositions,
abstract = {A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.},
archivePrefix = {arXiv},
arxivId = {2002.00212},
author = {Huang, Yu Siang and Yang, Yi Hsuan},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
doi = {10.1145/3394171.3413671},
eprint = {2002.00212},
isbn = {9781450379885},
keywords = {automatic music composition,neural sequence model,transformer},
title = {{Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions}},
year = {2020}
}
@article{Proutskova2022,
annote = {Jazz Ontology},
author = {Proutskova, Polina and Wolff, Daniel and Fazekas, Gy{\"{o}}rgy and Frieler, Klaus and H{\"{o}}ger, Frank and Velichkina, Olga and Solis, Gabriel and Weyde, Tillman and Pfleiderer, Martin},
doi = {10.1016/j.websem.2022.100735},
file = {:Users/huanzhang/Downloads/1-s2.0-S1570826822000245-main (1).pdf:pdf},
issn = {1570-8268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
pages = {100735},
publisher = {Elsevier B.V.},
title = {{The Jazz Ontology : A semantic model and large-scale RDF repositories for jazz ✩}},
url = {https://doi.org/10.1016/j.websem.2022.100735},
volume = {74},
year = {2022}
}
@inproceedings{Huron1989CharacterizingTextures,
author = {Huron, David},
booktitle = {International Conference on Mathematics and Computing},
file = {:Users/huanzhang/Downloads/characterizing-musical-textures.pdf:pdf},
title = {characterizing-musical-textures.pdf},
year = {1989}
}
@article{Zhang2024DExterModels,
abstract = {In the pursuit of developing expressive music performance models using artificial intelligence, this paper introduces DExter, a new approach leveraging diffusion probabilistic models to render Western classical piano performances. The main challenge faced in performance rendering tasks is the continuous and sequential modeling of expressive timing and dynamics over time, which is critical for capturing the evolving nuances that characterize live musical performances. In this approach, performance parameters are represented in a continuous expression space, and a diffusion model is trained to predict these continuous parameters while being conditioned on a musical score. Furthermore, DExter also enables the generation of interpretations (expressive variations of a performance) guided by perceptually meaningful features by being jointly conditioned on score and perceptual-feature representations. Consequently, we find that our model is useful for learning expressive performance, generating perceptually steered performances, and transferring performance styles. We assess the model through quantitative and qualitative analyses, focusing on specific performance metrics regarding dimensions like asynchrony and articulation, as well as through listening tests that compare generated performances with different human interpretations. The results show that DExter is able to capture the time-varying correlation of the expressive parameters, and it compares well to existing rendering models in subjectively evaluated ratings. The perceptual-feature-conditioned generation and transferring capabilities of DExter are verified via a proxy model predicting perceptual characteristics of differently steered performances.},
author = {Zhang, Huan and Chowdhury, Shreyan and Cancino-Chac{\'{o}}n, Carlos Eduardo and Liang, Jinhua and Dixon, Simon and Widmer, Gerhard},
doi = {10.3390/app14156543},
issn = {2076-3417},
journal = {Applied Sciences},
number = {15},
title = {{DExter: Learning and Controlling Performance Expression with Diffusion Models}},
url = {https://www.mdpi.com/2076-3417/14/15/6543},
volume = {14},
year = {2024}
}
@article{Music,
author = {Music, Synthesizing},
file = {:Users/huanzhang/Downloads/JASM-D-23-00060_reviewer.pdf:pdf},
title = {{EURASIP Journal on Audio , Speech , and Music Processing Synthesizing Music and Speech as MIDI Files Through Machine Learning Synthesizing Music and Speech as MIDI Files Through Machine Learning}}
}
@inproceedings{Spijkervet2021ContrastiveRepresentations,
abstract = {While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTa-gATune and Million Song datasets and present an abla-tion study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster repro-ducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.},
archivePrefix = {arXiv},
arxivId = {2103.09410v2},
author = {Spijkervet, Janne and Burgoyne, John Ashley},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2103.09410v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Spijkervet, Burgoyne - Unknown - CONTRASTIVE LEARNING OF MUSICAL REPRESENTATIONS.pdf:pdf},
title = {{Contrastive Learning of Musical Representations}},
url = {https://spijkervet.github.io/CLMR},
year = {2021}
}
@article{Wiggins1941EvolutionaryComposition,
abstract = {We discuss the use of genetic algorithms (GAs) for the generation of music. We explain the structure of a typical GA, and outline existing work on the use of GAs in computer music. We propose that the addition ofdomain-specific knowledge can enhance the quality and speed of production of GA results, and describe two systems which exemplify this. However, we conclude that GAs are not ideal for the simulation ofhuman musical thought (notwithstanding their ability to produce good results) because their operation in no way simulates human behaviour. Keywords:},
author = {Wiggins, Geriant and Papadopoulos, George},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wiggins, Papadopoulos - 1941 - Evolutionary Methods for Musical Composition.pdf:pdf},
keywords = {Genetic Algorithms,Music Generation,Search Space},
number = {1934},
pages = {535--546},
title = {{Evolutionary Methods for Musical Composition}},
volume = {74},
year = {1941}
}
@article{Lai2019,
abstract = {With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence. However, opposite results are also observed in other domains, where standard recurrent networks often outper-form stochastic models. To better understand this discrepancy, we reexamine the roles of latent variables in stochastic recurrent models for speech density estimation. Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic variants were implicitly leverag-ing intra-step correlation but the deterministic recurrent baselines were prohibited to do so, resulting in an unfair comparison. To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure. Over a diverse set of univariate and multivariate sequential data, including human speech, MIDI music, handwriting trajectory and frame-permuted speech, our results show that stochas-tic recurrent models fail to deliver the performance advantage claimed in previous work. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, dramatically advancing the state-of-the-art results on three speech datasets.},
archivePrefix = {arXiv},
arxivId = {1902.01388v2},
author = {Lai, Guokun and Dai, Zihang and Yang, Yiming and Yoo, Shinjae},
eprint = {1902.01388v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lai et al. - 2019 - Re-examination of the Role of Latent Variables in Sequence Modeling.pdf:pdf},
title = {{Re-examination of the Role of Latent Variables in Sequence Modeling}},
year = {2019}
}
@article{PearceMusicCognition,
author = {Pearce, Marcus},
title = {{Music Perception {\&} Cognition}}
}
@techreport{Harry2018,
abstract = {Musical influence is a topic of interest and debate among critics, historians, and general listeners alike, yet to date there has been limited work done to tackle the subject in a quantitative way. In this thesis, we address the problem of modeling musical influence using a dataset of 143,625 audio files and a ground truth expert-curated network graph of artist-to-artist influence consisting of 16,704 artists scraped from AllMusic.com. We explore two audio content-based approaches to modeling influence: first, we take a topic modeling approach, specifically using the Document Influence Model (DIM) to infer artist-level influence on the evolution of musical topics. We find the artist influence measure derived from this model to correlate with the ground truth graph of artist influence. Second, we propose an approach for classifying artist-to-artist influence using siamese convolutional neural networks trained on mel-spectrogram representations of song audio. We find that this approach is promising, achieving an accuracy of 0.7 on a validation set, and we propose an algorithm using our trained siamese network model to rank influences. iii},
annote = {From Duplicate 1 (Modeling Musical Influence Through Data - Harry, Xue)

Binary artist-to-artist influence realation:
- in degree and out degree
- page rank

network formation for cover songs:
- next chronological neighbor
- first artist to each successor
- each artist to every possible successor

conclusion: using the metadata (networks provided by platform) is not very convincing, so it focused on content

Document Influence Model: extend topic modeling allows for evolution over time},
author = {Harry, Xue},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Modeling Musical Influence Through Data.pdf:pdf},
title = {{Modeling Musical Influence Through Data}},
url = {http://nrs.harvard.edu/urn-3:HUL.InstRepos:38811527},
year = {2018}
}
@inproceedings{Zhang2018MixupMinimization,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1710.09412},
file = {:Users/huanzhang/Downloads/1710.09412.pdf:pdf},
title = {{{MixUp}: Beyond empirical risk minimization}},
year = {2018}
}
@inproceedings{Jeong2019VirtuosoNetPerformance,
abstract = {In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models.We also share the dataset we used for the experiment.},
address = {Delft, Netherlands},
author = {Jeong, Dasaem and Kwon, Taegyun and Kim, Yoojin and Lee, Kyogu and Nam, Juhan},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jeong et al. - Unknown - VirtuosoNet A HIERARCHICAL RNN-BASED SYSTEM FOR MODELING EXPRESSIVE PIANO PERFORMANCE.pdf:pdf},
isbn = {9781732729919},
title = {{{VirtuosoNet}: A Hierarchical {RNN}-based System for Modeling Expressive Piano Performance}},
year = {2019}
}
@inproceedings{Vaswani2017AttentionNeed,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
eprint = {1706.03762},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
issn = {10495258},
title = {{Attention is all you need}},
year = {2017}
}
@article{Processing2024,
author = {Processing, Language},
file = {:Users/huanzhang/Downloads/T-ASL-11167-2024_Proof_hi.pdf:pdf},
title = {{Can Audio Reveal Music Performance Difficulty ? Insights from the Piano Syllabus Dataset Author ' s Responses to Custom Submission Questions}},
year = {2024}
}
@inproceedings{Giraud2014TowardsData,
abstract = {Studying texture is a part of many musicological analy-ses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature commonly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to formalize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an algorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evaluation of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn.},
author = {Giraud, Mathieu and Lev{\'{e}}, Florence and Mercier, Florent and Rigaudi{\`{e}}re, Marc and Thorez, Donatien},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Downloads/2014-ismir-texture (1).pdf:pdf},
title = {{Towards Modeling Texture in Symbolic Data}},
year = {2014}
}
@inproceedings{Chen2020MusicRhythm,
abstract = {Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process.},
archivePrefix = {arXiv},
arxivId = {2008.01291},
author = {Chen, Ke and Wang, Cheng I. and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference, ISMIR 2020},
eprint = {2008.01291},
file = {:Users/huanzhang/Downloads/2008.01291v1.pdf:pdf},
isbn = {9780981353708},
pages = {77--84},
title = {{Music Sketchnet: Controllable Music Generation Via Factorized Representations of Pitch and Rhythm}},
year = {2020}
}
@article{Peter2023AutomaticDataset,
author = {Peter, Silvan David and Cancino-chac{\'{o}}n, Carlos Eduardo and Foscarin, Francesco and Henkel, Florian and Widmer, Gerhard},
doi = {10.5334/tismir.149},
file = {:Users/huanzhang/Downloads/6499a04b4a67d.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval (TISMIR)},
keywords = {alignment,expression,following,score,symbolic music,time warping},
title = {{Automatic Note-Level Alignments in the {ASAP} Dataset}},
year = {2023}
}
@article{EstradaBascunana2017NewOp.43,
author = {{Estrada Bascu{\~{n}}ana}, Carolina},
doi = {10.5070/d82135899},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Estrada Bascu{\~{n}}ana - 2017 - New Approaches to Enrique Granados' Pedagogical Methods and Pianistic Tradition A Case Study of Valses po.pdf:pdf},
issn = {2470-4199},
journal = {Diagonal: An Ibero-American Music Review},
number = {1},
title = {{New Approaches to Enrique Granados' Pedagogical Methods and Pianistic Tradition: A Case Study of Valses po{\'{e}}ticos op.43}},
volume = {2},
year = {2017}
}
@article{Mcdonald2019,
abstract = {For concertgoers, musical interpretation is the most important factor in determining whether or not we enjoy a classical performance. Every performance includes mistakes-intonation issues , a lost note, an unpleasant sound-but these are all easily forgotten (or unnoticed) when a performer engages her audience, imbuing a piece with novel emotional content beyond the vague instructions inscribed on the printed page. While music teachers use imagery or heuristic guidelines to motivate interpretive decisions, combining these vague instructions to create a convincing performance remains the domain of the performer, subject to the whims of the moment, technical fluency, and taste. In this research, we use data from the CHARM Mazurka Project-forty-six professional recordings of Chopin's Mazurka Op. 63 No. 3 by consumate artists-with the goal of elucidating musically interpretable performance decisions. Using information on the inter-onset intervals of the note attacks in the recordings, we apply functional data analysis techniques enriched with prior information gained from music theory to discover relevant features and perform hierarchical clustering. The resulting clusters suggest methods for informing music instruction, discovering listening preferences, and analyzing performances.},
archivePrefix = {arXiv},
arxivId = {1907.06244v1},
author = {Mcdonald, Daniel J and Mcbride, Michael and Gu, Yupeng and Raphael, Christopher},
eprint = {1907.06244v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mcdonald et al. - 2019 - Markov-switching State Space Models for Uncovering Musical Interpretation.pdf:pdf},
keywords = {Kalman filter,classification and clustering,hidden Markov model},
title = {{Markov-switching State Space Models for Uncovering Musical Interpretation}},
year = {2019}
}
@article{Georges2017WesternEvolution,
abstract = {This paper proposes a statistical analysis that captures similarities and differences between classical music composers with the eventual aim to understand why particular composers ‘sound' different even if their ‘lineages' (influences network) are similar or why they ‘sound' alike if their ‘lineages' are different. In order to do this we use statistical methods and measures of association or similarity (based on presence/absence of traits such as specific ‘ecological' characteristics and personal musical influences) that have been developed in biosystematics, scientometrics, and bibliographic coupling. This paper also represents a first step towards a more ambitious goal of developing an evolutionary model of Western classical music.},
author = {Georges, Patrick},
doi = {10.1007/s11192-017-2387-x},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Georges - Unknown - Western classical music development a statistical analysis of composers similarity, differentiation and evolution.pdf:pdf},
issn = {15882861},
journal = {Scientometrics},
keywords = {Classical composers,Differentiation,Evolution,Imitation,Influences network,Similarity indices},
number = {1},
pages = {21--53},
title = {{Western classical music development: a statistical analysis of composers similarity, differentiation and evolution}},
volume = {112},
year = {2017}
}
@article{Gillick2021DrumrollGrids,
annote = {From Duplicate 2 (Drumroll Please: Modeling Multi-Scale Rhythmic Gestures with Flexible Grids - Gillick, Jon; Yang, Joshua; Cella, Carmine-Emanuele; Bamman, David)

Problem: Grid based and Event-based representations

Flexible Grid: 
Primary Events + Secondary Events

Secondary: multiple attacks of the same channel on one timestep (flam, roll, double stroke)},
author = {Gillick, Jon and Yang, Joshua and Cella, Carmine-Emanuele and Bamman, David},
doi = {10.5334/tismir.98},
file = {:Users/huanzhang/Downloads/98-2895-1-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {computational creativity,human computer interaction,machine learning,music,music generation,music representations,production},
number = {1},
pages = {156--166},
title = {{Drumroll Please: Modeling Multi-Scale Rhythmic Gestures with Flexible Grids}},
volume = {4},
year = {2021}
}
@inproceedings{Wang2020PlayingScattering,
author = {Wang, Changhong and Lostanlen, Vincent and Benetos, Emmanouil and Chew, Elaine},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Downloads/icassp2020-changhong_wang.pdf:pdf},
title = {{Playing Technique Recognition by Joint Time Frequency Scattering}},
year = {2020}
}
@article{Of2009a,
author = {Of, Evaluation and Satellite, Passive and Sensing, Remote and Cloud, O F and Water, Liquid},
file = {:Users/huanzhang/Downloads/7959.pdf:pdf},
number = {February},
pages = {2--3},
title = {{for Review Only for Review Only for Review Only for Review Only}},
year = {2009}
}
@article{Lenz,
archivePrefix = {arXiv},
arxivId = {arXiv:2410.02060v1},
author = {Lenz, Julian},
eprint = {arXiv:2410.02060v1},
file = {:Users/huanzhang/Downloads/2410.02060v1.pdf:pdf},
title = {{PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and Variations}}
}
@article{Iversen2008,
abstract = {Many aspects of perception are known to be shaped by experience, but others are thought to be innate universal properties of the brain. A specific example comes from rhythm perception, where one of the fundamental perceptual operations is the grouping of successive events into higher-level patterns, an operation critical to the perception of language and music. Grouping has long been thought to be governed by innate perceptual principles established a century ago. The current work demonstrates instead that grouping can be strongly dependent on culture. Native English and Japanese speakers were tested for their perception of grouping of simple rhythmic sequences of tones. Members of the two cultures showed different patterns of perceptual grouping, demonstrating that these basic auditory processes are not universal but are shaped by experience. It is suggested that the observed perceptual differences reflect the rhythms of the two languages, and that native language can exert an influence on general auditory perception at a basic level.},
author = {Iversen, John R. and Patel, Aniruddh D. and Ohgushi, Kengo},
doi = {10.1121/1.2973189},
file = {:Users/huanzhang/Downloads/Iversen_Patel_Ohgushi_JASA_2008.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {2263--2271},
pmid = {19062864},
title = {{Perception of rhythmic grouping depends on auditory experience}},
volume = {124},
year = {2008}
}
@article{Hartea,
abstract = {In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, context-independent syntax for representing chord symbols in text, supported with a new database of annotations using this system.},
author = {Harte, Christopher and Sandler, Mark and Abdallah, Samer and G{\'{o}}mez, Emilia},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Harte et al. - Unknown - SYMBOLIC REPRESENTATION OF MUSICAL CHORDS A PROPOSED SYNTAX FOR TEXT ANNOTATIONS.pdf:pdf},
keywords = {Anno-tation,Chords,Harmony,Music,Notation},
title = {{SYMBOLIC REPRESENTATION OF MUSICAL CHORDS: A PROPOSED SYNTAX FOR TEXT ANNOTATIONS}}
}
@article{DallaBella2001,
abstract = {Do children use the same properties as adults in determining whether music sounds happy or sad? We addressed this question with a set of 32 excerpts (16 happy and 16 sad) taken from pre-existing music. The tempo (i.e. the number of beats per minute) and the mode (i.e. the specific subset of pitches used to write a given musical excerpt) of these excerpts were modified independently and jointly in order to measure their effects on happy-sad judgments. Adults and children from 3 to 8 years old were required to judge whether the excerpts were happy or sad. The results show that as adults, 6-8-year-old children are affected by mode and tempo manipulations. In contrast, 5-year-olds' responses are only affected by a change of tempo. The youngest children (3-4-year-olds) failed to distinguish the happy from the sad tone of the music above chance. The results indicate that tempo is mastered earlier than mode to infer the emotional tone conveyed by music. Copyright {\textcopyright} 2001 Elsevier Science B.V.},
author = {{Dalla Bella}, Simone and Peretz, Isabelle and Rousseau, Luc and Gosselin, Nathalie},
doi = {10.1016/S0010-0277(00)00136-0},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dalla Bella et al. - 2001 - A developmental study of the affective value of tempo and mode in music.pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
keywords = {Developmental study,Mode,Tempo},
month = {jul},
number = {3},
pages = {B1--B10},
publisher = {Elsevier},
title = {{A developmental study of the affective value of tempo and mode in music}},
volume = {80},
year = {2001}
}
@inproceedings{Hu2022MaskedListen,
archivePrefix = {arXiv},
arxivId = {arXiv:2207.06405v3},
author = {Hu, Po-yao Huang and Juncheng, Xu and Feichtenhofer, Christoph and Ai, Meta},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:2207.06405v3},
file = {:Users/huanzhang/Downloads/2207.06405.pdf:pdf},
title = {{Masked Autoencoders that Listen}},
year = {2022}
}
@article{Jiang2020RL-DuetLearning,
abstract = {This paper presents a deep reinforcement learning algorithm for online accompaniment generation, with potential for real-time interactive human-machine duet improvisation. Different from offline music generation and harmonization, online music accompaniment requires the algorithm to respond to human input and generate the machine counterpart in a sequential order. We cast this as a reinforcement learning problem, where the generation agent learns a policy to generate a musical note (action) based on previously generated context (state). The key of this algorithm is the well-functioning reward model. Instead of defining it using music composition rules, we learn this model from monophonic and polyphonic training data. This model considers the compatibility of the machine-generated note with both the machine-generated context and the human-generated context. Experiments show that this algorithm is able to respond to the human part and generate a melodic, harmonic and diverse machine part. Subjective evaluations on preferences show that the proposed algorithm generates music pieces of higher quality than the baseline method.},
archivePrefix = {arXiv},
arxivId = {2002.03082},
author = {Jiang, Nan and Jin, Sheng and Duan, Zhiyao and Zhang, Changshui},
doi = {10.1609/aaai.v34i01.5413},
eprint = {2002.03082},
file = {:Users/huanzhang/Downloads/2002.03082v1.pdf:pdf},
isbn = {9781577358350},
issn = {2159-5399},
journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
pages = {710--718},
title = {{RL-Duet: Online music accompaniment generation using deep reinforcement learning}},
year = {2020}
}
@inproceedings{Wang2020PianotreeMusic,
abstract = {The dominant approach for music representation learning involves the deep unsupervised model family variational autoencoder (VAE). However, most, if not all, viable attempts on this problem have largely been limited to mono-phonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the Pi-anoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning. The experiments prove the validity of the PianoTree VAE via (i)-semantically meaningful latent code for polyphonic segments ; (ii)-more satisfiable reconstruction aside of decent geometry learned in the latent space; (iii)-this model's benefits to the variety of the downstream music generation. 1},
archivePrefix = {arXiv},
arxivId = {2008.07118v1},
author = {Wang, Ziyu and Zhang, Yiyi and Zhang, Yixiao and Jiang, Junyan and Yang, Ruihan and Zhao, Junbo and Xia, Gus},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference, ISMIR},
eprint = {2008.07118v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - PIANOTREE VAE STRUCTURED REPRESENTATION LEARNING FOR POLYPHONIC MUSIC.pdf:pdf},
title = {{Pianotree {VAE}: Structured Representation Learning for Polyphonic Music}},
url = {https://github.com/},
year = {2020}
}
@article{Wang2020Pop909Generation,
abstract = {Music arrangement generation is a subtask of automatic music generation, which involves reconstructing and re-conceptualizing a piece with new compositional techniques. Such a generation process inevitably requires reference from the original melody, chord progression, or other structural information. Despite some promising models for arrangement, they lack more refined data to achieve better evaluations and more practical results. In this paper, we propose POP909, a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, we provide the annotations of tempo, beat, key, and chords, where the tempo curves are hand-labeled and others are done by MIR algorithms. Finally, we conduct several baseline experiments with this dataset using standard deep music generation algorithms.},
archivePrefix = {arXiv},
arxivId = {2008.07142v1},
author = {Wang, Ziyu and Chen, Ke and Jiang, Junyan and Zhang, Yiyi and Xu, Maoran and Dai, Shuqi and Gu, Xianbin and Xia, Gus},
eprint = {2008.07142v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - POP909 A POP-SONG DATASET FOR MUSIC ARRANGEMENT GENERATION.pdf:pdf},
isbn = {2008.07142v1},
journal = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
title = {{POP909: A Pop-song Dataset for Music Arrangement Generation}},
url = {https://github.com/music-x-lab/POP909-},
year = {2020}
}
@article{Zhang,
archivePrefix = {arXiv},
arxivId = {arXiv:2303.09618v1},
author = {Zhang, Shu and Yang, Xinyi and Feng, Yihao and Qin, Can and Chen, Chia-chih and Yu, Ning and Chen, Zeyuan and Wang, Huan and Savarese, Silvio and Ermon, Stefano and Xiong, Caiming and Xu, Ran},
eprint = {arXiv:2303.09618v1},
file = {:Users/huanzhang/Downloads/2303.09618.pdf:pdf},
title = {{HIVE : Harnessing Human Feedback for Instructional Visual Editing}}
}
@article{Katayose2012OnExperience,
abstract = {Research into music generation and into emulating human musical competence has attracted much attention in the field of computer science. In general, the results of academic research should be verified by assessing 'objective effectiveness', which is often represented by a 'recognition ratio'. Although 'objective effectiveness' is also a requirement for research in music generation, it is meaningless unless subjective requisites are also satisfied. However, it is not easy for researchers to execute subjective evaluations within their individual endeavours. To address this difficulty within the research area of computer systems for generating expressive music performances, the Performance Rendering Contest (Re-ncon) was created. This is an international competition in which entrants present computer systems and the performances generated are graded, and has been held in conjunction with related international conferences. This paper presents an overview of Rencon history, highlighting the evaluative motivation of each contest. In addition, we discuss the possibilities of a new scientific research field in which future Rencons may play a role.},
author = {Katayose, Haruhiro and Hashida, Mitsuyo and {De Poli}, Giovanni and Hirata, Keiji},
doi = {10.1080/09298215.2012.745579},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Katayose et al. - 2012 - On Evaluating Systems for Generating Expressive Music Performance the Rencon Experience.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
number = {4},
pages = {299--310},
title = {{On Evaluating Systems for Generating Expressive Music Performance: the Rencon Experience}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {41},
year = {2012}
}
@misc{Luis,
booktitle = {Music Business Association},
file = {:Users/huanzhang/Downloads/Classical_MetadataStyleGuide_v1.pdf:pdf},
isbn = {9788490225370},
title = {{Classical Music Metadata Style Guide}}
}
@article{Goebl2001MelodyArtifact,
abstract = {As reported in the recent literature on piano performance, an emphasized voice the melody tends to be played not only louder than the other voices, but also about 30 ms earlier melody lead. It remains unclear whether pianists deliberately apply melody lead to separate different voices, or whether it occurs because the melody is played louder velocity artifact. The velocity artifact explanation implies that pianists initially strike the keys simultaneously; it is only different velocities that make the hammers arrive at different points in time. The measured note onsets in these studies, mostly derived from computer-monitored pianos, represent the hammer-string impact times. In the present study, the finger-key contact times are calculated and analyzed as well. If the velocity artifact hypothesis is correct, the melody lead phenomenon should disappear at the finger-key level. Chopin's Ballade op. 38 45 measures and Etude op. 10/3 21 measures were performed on a B{\"{o}} sendorfer computer-monitored grand piano by 22 skilled pianists. The hammer-string asynchronies among voices closely resemble the results reported in the literature. However, the melody lead decreases almost to zero at the finger-key level, which supports the velocity artifact hypothesis. In addition to this, expected onset asynchronies are predicted from differences in hammer velocity, if finger-key asynchronies are assumed to be zero. They correlate highly with the observed melody lead.},
annote = {velocity artifact hypothesis

hammer-string contact points
finger-key contact points

melody lagg / bass lead (more prominent, usually between hands, 50ms+)

key motion: 25 - 160ms},
author = {Goebl, Werner},
doi = {10.1121/1.1376133},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl - 2001 - Melody lead in piano performance Expressive device or artifact.pdf:pdf},
isbn = {10.1121/1.415889},
journal = {The Journal of the Acoustical Society of America},
keywords = {4375Mn RDA,4375St,PACS numbers},
pages = {641},
title = {{Melody lead in piano performance: Expressive device or artifact?}},
url = {https://asa.scitation.org/doi/10.1121/1.1376133},
volume = {110},
year = {2001}
}
@article{Lu,
abstract = {The subjective evaluation of music generation techniques has been mostly done with questionnaire-based listening tests while ignoring the perspectives from music composition, arrangement, and soundtrack editing. In this paper, we propose an editing test to evaluate users' editing experience of music generation models in a systematic way. To do this, we design a new music style transfer model combining the non-chronological inference architecture, autoregressive models and the Transformer, which serves as an improvement from the baseline model on the same style transfer task. Then, we compare the performance of the two models with a conventional listening test and the proposed editing test, in which the quality of generated samples is assessed by the amount of effort (e.g., the number of required keyboard and mouse actions) spent by users to polish a music clip. Results on two target styles indicate that the improvement over the baseline model can be reflected by the editing test quantitatively. Also, the editing test provides profound insights which are not accessible from usual listening tests. The major contribution of this paper is the systematic presentation of the editing test and the corresponding insights, while the proposed music style transfer model based on state-of-the-art neural networks represents another contribution. CCS CONCEPTS • Applied computing → Sound and music computing; • Human-centered computing → Empirical studies in HCI.},
archivePrefix = {arXiv},
arxivId = {2110.12855v1},
author = {Lu, Wei-Tsung and {Meng-Hsuan Wu}, Taiwan and {Yuh-Ming Chiu}, Taiwan and Su, Li and Wu, Meng-Hsuan and Chiu, Yuh-Ming},
doi = {10.1145/3474085.3475529},
eprint = {2110.12855v1},
isbn = {9781450386517},
keywords = {Human-centered computing,artificial intelligence,music generation,neural networks,style transfer},
publisher = {Virtual Event},
title = {{Actions Speak Louder than Listening: Evaluating Music Style Transfer based on Editing Experience; Actions Speak Louder than Listening: Evaluating Music Style Transfer based on Editing Experience}},
url = {https://doi.org/10.1145/3474085.3475529}
}
@article{Huang,
abstract = {Questions about the ethical dimensions of artificial intelligence (AI) become more pressing as its applications multiply. While there is a growing literature calling attention to the ethics of AI in general, sector-specific and culturally sensitive approaches remain under-explored. We thus initiate an effort to establish a framework of ethical guidelines for music AI in the context of East Asia, a region whose rapid technological advances are playing a leading role in contemporary geopolitical competition. We draw a connection between technological ethics and non-Western philosophies such as Confucianism, Buddhism, Shintoism, and Daoism. We emphasize interrelations between AI and traditional cultural heritage and values. Drawing on the IEEE Principles of Ethically Aligned Design, we map its proposed ethical principles to East Asian contexts and their respective music ecosystem. In this process of establishing a culturally situated understanding of AI ethics, we see that the seemingly universal concepts of "human rights", "well-being", and potential "misuse" are ultimately fluid and need to be examined in specific cultural contexts.},
author = {Huang, Rujing and Sturm, Bob L T and Holzapfel, Andre},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Sturm, Holzapfel - Unknown - DE-CENTERING THE WEST EAST ASIAN PHILOSOPHIES AND THE ETHICS OF APPLYING ARTIFICIAL INTELLIGENCE TO.pdf:pdf},
title = {{De-centering the West: East Asian Philosophies and the Ethics of Applying Artificial Intelligence to Music}},
url = {https://facctconference.org}
}
@article{Kuang,
abstract = {In this paper, we consider a novel research problem, music-to-text synaesthesia. Different from the classical music tagging problem that classifies a music recording into pre-defined categories, the music-to-text synaesthesia aims to generate descriptive texts from music recordings for further understanding. Although this is a new and interesting application to the machine learning community, to our best knowledge, the existing music-related datasets do not contain the semantic descriptions on music recordings and cannot serve the music-to-text synaesthesia task. In light of this, we collect a new dataset that contains 1,955 aligned pairs of classical music recordings and text descriptions. Based on this, we build a computational model to generate sentences that can describe the content of the music recording. To tackle the highly non-discriminative classical music, we design a group topology-preservation loss in our computational model, which considers more samples as a group reference and preserves the relative topology among different samples. Extensive experimental results qualitatively and quantitatively demonstrate the effectiveness of our proposed model over five heuristics or pre-trained competitive methods and their variants on our collected dataset.},
archivePrefix = {arXiv},
arxivId = {2210.00434},
author = {Kuang, Zhihuan and Zong, Shi and Zhang, Jianbing and Chen, Jiajun and Liu, Hongfu},
eprint = {2210.00434},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kuang et al. - Unknown - MUSIC-TO-TEXT SYNAESTHESIA GENERATING DESCRIPTIVE TEXT FROM MUSIC RECORDINGS(2).pdf:pdf},
title = {{Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings}},
url = {http://arxiv.org/abs/2210.00434},
year = {2022}
}
@article{Author2023a,
author = {Author, Anonymous and Address, Affiliation},
file = {:Users/huanzhang/Downloads/470_robopianist_dexterous_piano_pl.pdf:pdf},
keywords = {bi-manual dexterity,high-dimensional control},
number = {CoRL},
title = {{R OBO P IANIST : Dexterous Piano Playing with Deep Reinforcement Learning}},
year = {2023}
}
@article{Bernardes2020,
abstract = {Since the heydays of music informatics, around the 1950s, the modeling and prediction of musical structures manifested as symbolic representations have been continuously pursued. The operational property of such methods is to provide the conditional distribution over an alphabet – i.e., the entire col- lection of unique musical events in a composition or corpus – given a context – i.e., a preceding sequence. This distribution unpacks temporal morphologies that support multiple appli- cations for predictive and assisted creative tasks, such as the generation of new musical sequences that retain a structural resemblance to a modeled source. Despite their longstanding tradition, state-of-the-art methodologies for symbolic music modeling are yet to reach the music community. Naive mod- els such as Markov chains, which are known to neglect the fundamental hierarchical nature of musical structure, remain common practice. In this paper, we extensively review existing methodologies for symbolic music representation and modeling, as the first steps towards a study on the resulting balance across familiarity and novelty in generative music applications},
author = {Bernardes, Gilberto and {Nadia Carvalho}},
file = {:Users/huanzhang/Downloads/ICCC-paper.pdf:pdf},
isbn = {9789895416028},
journal = {11th International Conference on Computational Creativity},
number = {September},
pages = {236 -- 242},
title = {{Towards balanced tunes : A review of symbolic music representations and their hierarchical modeling Towards balanced tunes : A review of symbolic music representations and their hierarchical modeling}},
year = {2020}
}
@techreport{Guiomard-Kagan2016ImprovingContigs,
abstract = {Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005 [2], is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two questions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by considering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously proposed policies [2, 9]. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection.},
author = {Guiomard-Kagan, Nicolas and Giraud, Mathieu and Groult, Richard and Lev{\'{e}}, Florence},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Guiomard-Kagan et al. - Unknown - IMPROVING VOICE SEPARATION BY BETTER CONNECTING CONTIGS.pdf:pdf},
isbn = {9780692755068},
pages = {164--170},
title = {{Improving voice separation by better connecting contigs}},
year = {2016}
}
@article{Dowling1973,
abstract = {If the notes of two melodies whose pitch ranges do not overlap are interleaved in time so that successive tones come from the different melodies, the resulting sequence of tones is perceptually divided into groups that correspond to the two melodies. Such "melodic fission" demonstrates perceptual grouping based on pitch alone, and has been used extensively in music. Experiment I showed that the identification of interleaved pairs of familiar melodies is possible if their pitch ranges do not overlap, but difficult otherwise. A short-term recognition-memory paradigm (Expt II) showed that interleaving a "background" melody with an unfamiliar melody interferes with same-different judgments regardless of the separation of their pitch ranges, but that range separation attenuates the interference effect. When pitch ranges overlap, listeners can overcome the interference effect and recognize a familiar target melody if the target is prespecified, thereby permitting them to search actively for it (Expt III). But familiarity or prespecification of the interleaved background melody appears not to reduce its interfering effects on same-different judgments concerning unfamiliar target melodies (Expt IV). {\textcopyright} 1973.},
annote = {From Duplicate 1 (The perception of interleaved melodies - Dowling, W. J.)

Week 5},
author = {Dowling, W. J.},
doi = {10.1016/0010-0285(73)90040-6},
file = {:Users/huanzhang/Downloads/1-s2.0-0010028573900406-main.pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
number = {3},
pages = {322--337},
title = {{The perception of interleaved melodies}},
volume = {5},
year = {1973}
}
@inproceedings{Gong2023JointUnderstanding,
abstract = {Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.},
archivePrefix = {arXiv},
arxivId = {2309.14405},
author = {Gong, Yuan and Liu, Alexander H and Luo, Hongyin and Karlinsky, Leonid and Glass, James},
booktitle = {2023 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU},
doi = {10.1109/ASRU57964.2023.10389742},
eprint = {2309.14405},
file = {:Users/huanzhang/Downloads/2309.14405v3.pdf:pdf},
isbn = {9798350306897},
title = {{Joint Audio and Speech Understanding}},
year = {2023}
}
@article{Li2015,
author = {Li, Bochen and Duan, Zhiyao},
file = {:Users/huanzhang/Downloads/LiDuan_ScoreFollowing_ISMIR15.pdf:pdf},
journal = {Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015},
title = {{Score Following for Piano Performances With Sustain-Pedal Effects}},
year = {2015}
}
@article{Schlichtkrull2018,
abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.},
archivePrefix = {arXiv},
arxivId = {1703.06103},
author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
doi = {10.1007/978-3-319-93417-4_38},
eprint = {1703.06103},
file = {:Users/huanzhang/Downloads/1703.06103.pdf:pdf},
isbn = {9783319934167},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {1},
pages = {593--607},
title = {{Modeling Relational Data with Graph Convolutional Networks}},
volume = {10843 LNCS},
year = {2018}
}
@article{Liua,
archivePrefix = {arXiv},
arxivId = {arXiv:2106.01035v1},
author = {Liu, Daochang and Li, Qiyue and Jiang, Tingting and Wang, Yizhou and Miao, Rulin and Shan, Fei and Li, Ziyu},
eprint = {arXiv:2106.01035v1},
file = {:Users/huanzhang/Downloads/2106.01035v1.pdf:pdf},
title = {{Towards Unified Surgical Skill Assessment}}
}
@article{Urata2016,
abstract = {The opportunity to read music articles and blogs on the Web to get music information is more and more increasing. However, hyperlinks to artist information do not often exist in such articles, and it is a troublesome task for the reader to look it up online. In this paper, in order to make it easy to look up artist information in music articles, we propose a method to extract entities such as artist names in music articles in Japanese, and to perform entity linking which links from artist entities to artist information automatically. The method consists of two phases. First, we extract artist names in music articles. An artist name is a named entity, and it is necessary to distinguish artist names from other named entities, such as personal names of non-artists, place names, organization names, etc. In order to achieve it, we prepare training data of music articles in which artist names are manually tagged, and extract artist names using Support Vector Machine (SVM). Next, we choose a web page of artist information for each extracted artist name. We use Wikipedia as the source of artist information to verify the usefulness of the proposed method, we conducted evaluation experiments using cross-validation. For the experiments, we used 35 Japanese music articles and extracted artist names manually from the articles, and used them for cross-validation. We achieved the recall of 0.2788, precision of 0.7530, and F-measure of 0.4070. We also conducted an experiment to find correct artist information from Wikipedia using edit distance between extracted artist names from ten music articles and Wikipedia titles. We achieved the correct rate of 0.8740 in linking correct Wikipedia articles for artist names.},
author = {Urata, Tomoaki and Maeda, Akira},
doi = {10.1109/IIAI-AAI.2016.130},
file = {:Users/huanzhang/Downloads/Entity_Linking_of_Artists_Names_in_Japanese_Music_Articles.pdf:pdf},
isbn = {9781467389853},
journal = {Proceedings - 2016 5th IIAI International Congress on Advanced Applied Informatics, IIAI-AAI 2016},
keywords = {Machine learning,Named entity extraction},
pages = {179--184},
publisher = {IEEE},
title = {{Entity linking of artists names in Japanese music articles}},
year = {2016}
}
@techreport{Tsushima2017,
abstract = {This paper presents an automatic harmonization method that, for a given melody (sequence of musical notes), generates a sequence of chord symbols in the style of existing data. A typical way is to use hidden Markov models (HMMs) that represent chord transitions on a regular grid (e.g., bar or beat grid). This approach, however, cannot explicitly describe the rhythms, harmonic functions (e.g., tonic, dominant, and subdominant), and the hierarchical structure of chords, which are supposedly important in traditional harmony theories. To solve this, we formulate a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chords incorporating their syntactic functions, (2) a metrical Markov model describing chord rhythms, and (3) a Markov model generating melodies conditionally on a chord sequence. To estimate a variable-length chord sequence for a given melody, we iteratively refine the latent tree structure and the chord symbols and rhythms using a Metropolis-Hastings sampler with split-merge operations. Experimental results show that the proposed method outperformed the HMM-based method in terms of predictive abilities.},
author = {Tsushima, Hiroaki and Nakamura, Eita and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tsushima et al. - 2017 - Function- And rhythm-aware melody harmonization based on tree-structured parsing and split-merge sampling of ch.pdf:pdf},
isbn = {9789811151798},
pages = {502--508},
title = {{Function- And rhythm-aware melody harmonization based on tree-structured parsing and split-merge sampling of chord sequences}},
year = {2017}
}
@inproceedings{Keller2021WhatLearn,
abstract = {Since their conception for NLP tasks in 2017, Transformer neural networks have been increasingly used with compelling results for a variety of symbolic MIR tasks including music analysis, classification and generation. Although the concept of self-attention between words in text can intuitively be transposed as a relation between musical objects such as notes or chords in a score, it remains relatively unknown what kind of musical relations precisely tend to be captured by self attention mechanisms when applied to musical data. Moreover, the principle of self-attention has been elaborated in NLP to help model the "meaning" of a sentence while in the musical domain this concept appears to be more subjective. In this explorative work, we open the music transformer black box looking to identify which aspects of music are actually learnt by the self-attention mechanism. We apply this approach to two MIR probing tasks : composer classification and cadence identification.},
author = {Keller, Mikaela and Loiseau, Gabriel and Bigo, Louis},
booktitle = {Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Loiseau, Keller, Bigo - 2021 - What Musical Knowledge Does Self-Attention Learn.pdf:pdf},
pages = {6--10},
title = {{What Musical Knowledge Does Self-Attention Learn?}},
url = {https://aclanthology.org/2021.nlp4musa-1.2},
year = {2021}
}
@inproceedings{Zhang2022ATEPPPerformance,
address = {Bengaluru, India},
author = {Zhang, Huan and Tang, Jingjing and Rafee, Syed and Dixon, Simon and Fazekas, George},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/02Career/Kristin33.github.io/assets/ATEPP__a_dataset_of_automatically_transcribed_expressive_piano_performance.pdf:pdf},
title = {{{ATEPP}: A Dataset of Automatically Transcribed Expressive Piano Performance}},
year = {2022}
}
@article{Kelz2018,
abstract = {Rethinking how to model polyphonic transcription formally, we frame it as a reinforcement learning task. Such a task formulation encompasses the notion of a musical agent and an environment containing an instrument as well as the sound source to be transcribed. Within this conceptual framework, the transcription process can be described as the agent interacting with the instrument in the environment, and obtaining reward by playing along with what it hears. Choosing from a discrete set of actions - the notes to play on its instrument - the amount of reward the agent experiences depends on which notes it plays and when. This process resembles how a human musician might approach the task of transcription, and the satisfaction she achieves by closely mimicking the sound source to transcribe on her instrument. Following a discussion of the theoretical framework and the benefits of modelling the problem in this way, we focus our attention on several practical considerations and address the difficulties in training an agent to acceptable performance on a set of tasks with increasing difficulty. We demonstrate promising results in partially constrained environments.},
archivePrefix = {arXiv},
arxivId = {1805.11526},
author = {Kelz, Rainer and Widmer, Gerhard},
eprint = {1805.11526},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kelz, Widmer - Unknown - LEARNING TO TRANSCRIBE BY EAR.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kelz, Widmer - Unknown - LEARNING TO TRANSCRIBE BY EAR(2).pdf:pdf},
title = {{Learning to Transcribe by Ear}},
url = {http://available.upon.publicat.ion http://arxiv.org/abs/1805.11526},
year = {2018}
}
@inproceedings{Kim2023DiffVelModel,
address = {Tokyo, Japan},
author = {Kim, Hyon and Serra, Xavier},
booktitle = {16th International Symposium on Computer Music Multidisciplinary Research (CMMR)},
file = {:Users/huanzhang/Downloads/Kim_cmm_diff.pdf:pdf},
keywords = {conditioned deep neu-,diffusion model,film conditioning,midi velocity estimation,ral network},
title = {{{DiffVel} : Note-Level MIDI Velocity Estimation for Piano Performance by A Double Conditioned Diffusion Model}},
year = {2023}
}
@inproceedings{Gemmeke2017AudioEvents,
abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets-principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
author = {Gemmeke, Jort F and Ellis, Daniel P W and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and {Channing Moore}, R and Plakal, Manoj and Ritter, Marvin},
booktitle = {Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gemmeke et al. - Unknown - AUDIO SET AN ONTOLOGY AND HUMAN-LABELED DATASET FOR AUDIO EVENTS.pdf:pdf},
keywords = {Index Terms-Audio event detection,audio databases,data collection,sound ontology},
title = {{Audio Set: An Ontology and Human-Labeled Dataset for Audio Events}},
year = {2017}
}
@inproceedings{Plaja-roglans2022,
author = {Plaja-roglans, Gen{\'{i}}s},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/262.pdf:pdf},
title = {{A DIFFUSION-INSPIRED TRAINING STRATEGY FOR SINGING VOICE EXTRACTION IN THE WAVEFORM DOMAIN}},
year = {2022}
}
@article{Grachten2009Phase-planeTiming,
abstract = {For the past few decades there has been considerable scientific interest in expression in music performance (Gabrielsson, 2003). A particularly relevant aspect of music performance is expressive timing, that is, the intentional fluctuations of tempo during a performance. Accordingly, expressive timing has been one of the major topics in music performance research. As an expressive parameter, timing is used to clarify the musical structure of the piece (Clarke, 1988), among other things. The problem of explaining expressive timing in music performances can be regarded as a special case of a very wide range of problems where we want to learn experimentally about the temporal behaviour of some dynamical system based on limited observation. A common way of studying data in dynamical systems theory is by phase-plane representation. In this paper we argue that phase-plane representations of expressive timing provide a useful way of visualizing data, and furthermore, we show that such representations are promising in the context of performer characterization and identification.},
author = {Grachten, Maarten and Goebl, Werner and Flossmann, Sebastian and Widmer, Gerhard},
doi = {10.1080/09298210903171160},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Grachten et al. - 2009 - Phase-plane Representation and Visualization of Gestural Structure in Expressive Timing.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {jun},
number = {2},
pages = {183--195},
publisher = {Informa UK Limited},
title = {{Phase-plane Representation and Visualization of Gestural Structure in Expressive Timing}},
volume = {38},
year = {2009}
}
@article{Kim2018DisentanglingFactorising,
abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
annote = {From Duplicate 2 (Disentangling by factorising - Kim, Hyunjik; Mnih, Andriy)

FactorVAE paper},
archivePrefix = {arXiv},
arxivId = {1802.05983v3},
author = {Kim, Hyunjik and Mnih, Andriy},
eprint = {1802.05983v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Mnih - 2018 - Disentangling by Factorising.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning (ICML)},
pages = {4153--4171},
title = {{Disentangling by Factorising}},
volume = {6},
year = {2018}
}
@article{Palmer2005,
abstract = {Pianists of varying levels of experience performed polyphonic excerpts from the classical repertoire on a computer-monitored (MIDI interface) synthesizer. Preliminary findings showed consistent and reliable temporal patterns of articulation. Both legato and staccato patterns were accurately predicted by a combination of the durations of successive notes in the musical score. Onset times of the individual voices notated as simultaneous differed such that the melody preceded the other voices, similar to Rasch's [J. Acoust. Soc. Am. 43, 121–131 (1979)] finding with ensemble players. Rubato patterns (alterations in tempo from mechanical regularity) were also consistent within an excerpt, demonstrating predictable changes within phrases and at cadences. Patterns of rubato in these polyphonic performances were similar to Bengtsson and Gabrielsson's [Proc. R. Swed. Acad. Music 39, 27–60 (1983)] profiles of monophonic performances by pianists. Each of these patterns of articulation was strongest in experienced pianists, and existed to varying degrees in student musicians. When experts were asked to perform nonmusically, the articulation patterns were still present but dampened in degree. Ongoing research is comparing the influence of amplitude (using a velocity-sensitive weighted keyboard) on pianists' articulation patterns. [Work supported by NSF and NIMH.]},
author = {Palmer, Caroline},
doi = {10.1121/1.2023377},
file = {:Users/huanzhang/Downloads/s75_3_online.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {S1},
pages = {S75--S75},
title = {{Methods of articulation in piano performance}},
volume = {79},
year = {1986}
}
@article{Libraries2016,
author = {Libraries, Music and Centres, Documentation},
file = {:Users/huanzhang/Downloads/44709384.pdf:pdf},
number = {4},
pages = {285--298},
title = {{KNOWLEDGE IS OUT THERE : A NEW STEP IN THE EVOLUTION OF MUSIC DIGITAL LIBRARIES Author ( s ): Sergio Oramas and Mohamed Sordo Published by : International Association of Music Libraries , Archives , and Documentation Centres ( IAML ) Stable URL : https://}},
volume = {63},
year = {2016}
}
@article{Slade2020a,
abstract = {Body Mapping is becoming increasingly popular among musicians as an educational approach to improve bodily movement and thereby the audible quality of music performances. This study used MIDI data to quantitatively measure changes in scale and arpeggio piano performance one day before and one day after a Body Mapping workshop. While there were subtle changes in the MIDI data, these changes were generally neither statistically significant, nor a magnitude that would be audible. Based on these findings, we theorise that reports of immediate improvements to music performance originate in visual dominance: audience members observe changes in bodily movement and perceive this as improved sound quality.},
author = {Slade, Teri and Comeau, Gilles and Russell, Donald},
doi = {10.1080/09298215.2020.1784958},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {Body Mapping,MIDI data,music perception,piano performance,somatic method},
pages = {362--372},
title = {{Measurable changes in piano performance of scales and arpeggios following a Body Mapping workshop}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2020}
}
@article{Bernays2011VerbalDescriptors,
abstract = {High-level pianists refer to and can identify nuances in timbre by way of a wide and rich vocabulary, whose abstract, imaged, and metaphoric terms acutely designate a variety of sounds. This timbre-describing lexicon is hereby studied quantitatively. The semantic proximity between pairs taken among 14 common piano timbre descriptors was evaluated in questionnaires distributed to 17 pianists. Ratings were analyzed with multidimensional scaling algorithms, yielding a four-dimensional space representing the semantic proximity between descriptors. Using cluster analyses, five main subsets were identified, within which the most familiar terms were selected. We thus obtained five descriptors which optimally describe the whole semantic space for the group of pianists taking part in this study: bright, dry, dark, round, and velvety.},
author = {Bernays, Michel and Traube, Caroline},
file = {:Users/huanzhang/Downloads/fl0026743.pdf:pdf},
isbn = {9789490306021},
journal = {International Symposium on Performance Science},
keywords = {43: Sound sources -- Keyboard,general,instruments--keyboard (piano family) -- piano pla,instruments--keyboard (piano family) -- piano play,language -- verbal descriptors -- timbre -- piano,timbre -- piano playing -- verbal descriptors},
number = {August 2011},
pages = {299--304},
title = {{Verbal expression of piano timbre: Multidimensional semantic space of adjectival descriptors}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=rih&AN=2011-24330%5Cnhttp://www.legacyweb.rcm.ac.uk/cache/fl0026743.pdf},
year = {2011}
}
@article{Wang2021SurveyLearning,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer ++ Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
doi = {10.1109/TPAMI.2021.3069908},
eprint = {2010.13166},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Chen, Zhu - 2020 - A Survey on Curriculum Learning.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Curriculum learning,example reweighting,machine learning,self-paced learning,training strategy},
month = {oct},
number = {9},
pmid = {33788677},
title = {{A Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
volume = {44},
year = {2022}
}
@article{Gong2021,
abstract = {In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for endto- end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.},
archivePrefix = {arXiv},
arxivId = {2104.01778},
author = {Gong, Yuan and Chung, Yu An and Glass, James},
doi = {10.21437/Interspeech.2021-698},
eprint = {2104.01778},
file = {:Users/huanzhang/Downloads/2104.01778.pdf:pdf},
isbn = {9781713836902},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Audio classification,Self-attention,Transformer},
pages = {56--60},
title = {{Ast: Audio spectrogram transformer}},
volume = {1},
year = {2021}
}
@article{Stamenovic2020,
abstract = {A cover song, by definition, is a new performance or recording of a previously recorded, commercially released song. It may be by the original artist themselves or a different artist altogether and can vary from the original in unpredictable ways including key, arrangement, instrumentation, timbre and more. In this work we propose a novel approach to learning audio representations for the task of cover song detection. We train a neural architecture on tens of thousands of cover-song audio clips and test it on a held out set. We obtain a mean precision@1 of 65% over mini-batches, ten times better than random guessing. Our results indicate that Siamese network configurations show promise for approaching the cover song identification problem.},
archivePrefix = {arXiv},
arxivId = {2005.10294},
author = {Stamenovic, Marko},
eprint = {2005.10294},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Stamenovic - 2020 - Towards Cover Song Detection with Siamese Convolutional Neural Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--4},
title = {{Towards Cover Song Detection with Siamese Convolutional Neural Networks}},
year = {2020}
}
@techreport{Emberley2013RobertStructure,
author = {Emberley, Stephanie Abigail},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Emberley - 2013 - Robert Schumann's Piano Sonata no. 1 in f-sharp minor, op. 11–style and structure.pdf:pdf},
title = {{Robert Schumann's Piano Sonata no. 1 in f-sharp minor, op. 11: style and structure}},
url = {https://commons.lib.jmu.edu/diss201019/169},
year = {2013}
}
@inproceedings{Huang2019AutomaticExercises,
annote = {From Duplicate 1 (Automatic Assessment of Sight-Reading Exercises - Huang, Jiawen; Alexander Lerch)

jDTW},
author = {Huang, Jiawen and Lerch, Alexander},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Huang - 2001 - Automatic Assessment of Sight-Reading Exercises.pdf:pdf},
title = {{Automatic Assessment of Sight-Reading Exercises}},
year = {2019}
}
@article{Bigand2005MultidimensionalExcerpts,
author = {Bigand, E and Vieillard, S and Madurell, F and Marozeau, J and Dacquet, A},
doi = {10.1080/02699930500204250},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bigand et al. - 2010 - Multidimensional scaling of emotional responses to music The effect of musical expertise and of the duration of t.pdf:pdf},
title = {{Multidimensional scaling of emotional responses to music: The effect of musical expertise and of the duration of the excerpts}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=pcem20},
year = {2005}
}
@article{Peter2024,
abstract = {Note alignment refers to the task of matching individual notes of two versions of the same symbolically encoded piece. Methods addressing this task commonly rely on sequence alignment algorithms such as Hidden Markov Models or Dynamic Time Warping (DTW) applied directly to note or onset sequences. While successful in many cases, such methods struggle with large mismatches between the versions. In this work, we learn note-wise representations from data augmented with various complex mismatch cases, e.g. repeats, skips, block insertions, and long trills. At the heart of our approach lies a transformer encoder network - TheGlueNote - which predicts pairwise note similarities for two 512 note subsequences. We postprocess the predicted similarities using flavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches for two sequences of arbitrary length. Our approach performs on par with the state of the art in terms of note alignment accuracy, is considerably more robust to version mismatches, and works directly on any pair of MIDI files.},
archivePrefix = {arXiv},
arxivId = {2408.04309},
author = {Peter, Silvan David and Widmer, Gerhard},
eprint = {2408.04309},
file = {:Users/huanzhang/Downloads/2408.04309v1.pdf:pdf},
title = {{TheGlueNote: Learned Representations for Robust and Flexible Note Alignment}},
url = {http://arxiv.org/abs/2408.04309},
year = {2024}
}
@inproceedings{Min2023PolyffusionControls,
abstract = {We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.},
address = {Milan, Italy},
archivePrefix = {arXiv},
arxivId = {2307.10304},
author = {Min, Lejun and Jiang, Junyan and Xia, Gus and Zhao, Jingwei},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
eprint = {2307.10304},
file = {:Users/huanzhang/Downloads/2307.10304.pdf:pdf},
title = {{Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls}},
url = {http://arxiv.org/abs/2307.10304},
year = {2023}
}
@article{Preniqi2022,
abstract = {This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song's overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression approaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores ($.20 \leq r \leq .30$) than values of empathy and equality ($.08 \leq r \leq .11$), while basic demographic variables only account for a small part in the models' explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.},
archivePrefix = {arXiv},
arxivId = {2209.01169},
author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
eprint = {2209.01169},
file = {:Users/huanzhang/Downloads/281.pdf:pdf},
title = {{"More Than Words": Linking Music Preferences and Moral Values Through Lyrics}},
url = {http://arxiv.org/abs/2209.01169},
year = {2022}
}
@article{Feng2011HarmonizingFigure,
abstract = {In this paper, a meta-structure of piano accompaniment figure (meta-structure for short) is proposed to harmonize a melodic piece of music so as to construct a multi-voice music. Here we approach melody harmonization with piano accompaniment as a machine learning task in a probabilistic framework. A series of piano accompaniment figures are collected from the massive existing sample scores and converted into a set of meta-structure. After the procedure of samples training, a model is formulated to generate a proper piano accompaniment figure for a harmonizing unit in the context. This model is flexible in harmonizing a melody with piano accompaniment. The experimental results are evaluated and discussed. {\textcopyright} 2011 Springer Science+Business Media, LLC & Science Press, China.},
author = {Feng, Yin and Chen, Kui and Liu, Xiang Bin},
doi = {10.1007/s11390-011-1200-1},
file = {:Users/huanzhang/Downloads/pdf.pdf:pdf},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Algorithmic composition,Automatic harmonization,Computer music,Meta-learning},
number = {6},
pages = {1041--1060},
title = {{Harmonizing melody with meta-structure of piano accompaniment figure}},
volume = {26},
year = {2011}
}
@article{Pretet2020LearningLoss,
abstract = {Most music streaming services rely on automatic recommendation algorithms to exploit their large music catalogs. These algorithms aim at retrieving a ranked list of music tracks based on their similarity with a target music track. In this work, we propose a method for direct recommendation based on the audio content without explicitly tagging the music tracks. To that aim, we propose several strategies to perform triplet mining from ranked lists. We train a Convolutional Neural Network to learn the similarity via triplet loss. These different strategies are compared and validated on a large-scale experiment against an auto-tagging based approach. The results obtained highlight the efficiency of our system, especially when associated with an Auto-pooling layer.},
author = {Pretet, Laure and Richard, Gael and Peeters, Geoffroy},
doi = {10.1109/ICASSP40776.2020.9053135},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pretet, Richard, Peeters - 2020 - Learning to Rank Music Tracks Using Triplet Loss(2).pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {audio music similarity,deep learning,triplet loss,triplet mining},
pages = {511--515},
title = {{Learning to Rank Music Tracks Using Triplet Loss}},
volume = {2020-May},
year = {2020}
}
@article{Evans2024,
abstract = {Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.},
archivePrefix = {arXiv},
arxivId = {2404.10301},
author = {Evans, Zach and Parker, Julian D. and Carr, CJ and Zukowski, Zack and Taylor, Josiah and Pons, Jordi},
eprint = {2404.10301},
file = {:Users/huanzhang/Downloads/2404.10301.pdf:pdf},
title = {{Long-form music generation with latent diffusion}},
url = {http://arxiv.org/abs/2404.10301},
year = {2024}
}
@article{Dieleman2018ChallengeScale,
abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
archivePrefix = {arXiv},
arxivId = {1806.10474},
author = {Dieleman, Sander and {Van Den Oord}, A{\"{a}}ron and Simonyan, Karen},
eprint = {1806.10474},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dieleman, Van Den Oord, Simonyan - Unknown - The challenge of realistic music generation modelling raw audio at scale.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NeurIPS)},
title = {{The challenge of realistic music generation: Modelling raw audio at scale}},
year = {2018}
}
@inproceedings{maman2022UnalignedWild,
abstract = {Multi-instrument Automatic Music Transcription (AMT), or the decoding of a musical recording into semantic musical content, is one of the holy grails of Music Information Retrieval. Current AMT approaches are restricted to piano and (some) guitar recordings, due to difficult data collection. In order to overcome data collection barriers, previous AMT approaches attempt to employ musical scores in the form of a digitized version of the same song or piece. The scores are typically aligned using audio features and strenuous human intervention to generate training labels. We introduce NoteEM, a method for simultaneously training a transcriber and aligning the scores to their corresponding performances, in a fully-automated process. Using this unaligned supervision scheme, complemented by pseudo-labels and pitch shift augmentation, our method enables training on in-the-wild recordings with unprecedented accuracy and instrumental variety. Using only synthetic data and unaligned supervision, we report SOTA note-level accuracy of the MAPS dataset, and large favorable margins on cross-dataset evaluations. We also demonstrate robustness and ease of use; we report comparable results when training on a small, easily obtainable, self-collected dataset, and we propose alternative labeling to the MusicNet dataset, which we show to be more accurate. Our project page is available at https://benadar293.github.io.},
archivePrefix = {arXiv},
arxivId = {2204.13668},
author = {Maman, Ben and Bermano, Amit H.},
booktitle = {Proceedings of Machine Learning Research},
eprint = {2204.13668},
file = {:Users/huanzhang/Downloads/2204.13668v1.pdf:pdf},
issn = {26403498},
title = {{Unaligned Supervision for Automatic Music Transcription in-the-Wild}},
volume = {162},
year = {2022}
}
@article{Jiang2020,
abstract = {Structure awareness and interpretability are two of the most desired properties of music generation algorithms. Structure-aware models generate more natural and coherent music with long-term dependencies, while interpretable models are more friendly for human-computer interaction and co-creation. To achieve these two goals simultaneously, we designed the Transformer Variational AutoEncoder, a hierarchical model that unifies the efforts of two recent breakthroughs in deep music generation: 1) the Music Transformer and 2) Deep Music Analogy. The former learns long-term dependencies using attention mechanism, and the latter learns interpretable latent representations using a disentangled conditional-VAE. We showed that Transformer VAE is essentially capable of learning a context-sensitive hierarchical representation, regarding local representations as the context and the dependencies among the local representations as the global structure. By interacting with the model, we can achieve context transfer, realizing the imaginary situation of “what if" a piece is developed following the music flow of another piece.},
author = {Jiang, Junyan and Xia, Gus G. and Carlton, Dave B. and Anderson, Chris N. and Miyakawa, Ryan H.},
doi = {10.1109/ICASSP40776.2020.9054554},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2020 - Transformer VAE A hierarchical model for structure-aware and interpretable music representation learning(2).pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Music structure,Representation learning,Transformer,VAE},
pages = {516--520},
publisher = {IEEE},
title = {{Transformer VAE: A hierarchical model for structure-aware and interpretable music representation learning}},
volume = {2020-May},
year = {2020}
}
@article{Chowdhury,
abstract = {Expert musicians can mould a musical piece to convey specific emotions that they intend to communicate. In this paper, we place a mid-level features based music emotion model in this performer-to-listener communication scenario, and demonstrate via a small visualisation music emotion decoding in real time. We also extend the existing set of mid-level features using analogues of perceptual speed and perceived dynamics.},
archivePrefix = {arXiv},
arxivId = {2303.01875v1},
author = {Chowdhury, Shreyan and Widmer, Gerhard},
eprint = {2303.01875v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chowdhury, Widmer - Unknown - DECODING AND VISUALISING INTENDED EMOTION IN AN EXPRESSIVE PIANO PERFORMANCE.pdf:pdf},
title = {{DECODING AND VISUALISING INTENDED EMOTION IN AN EXPRESSIVE PIANO PERFORMANCE}},
url = {https://youtu.be/}
}
@inproceedings{Hawthorne2018OnsetsTranscription,
abstract = {We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.},
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
doi = {10.5281/zenodo.1492341},
eprint = {1710.11153},
file = {:Users/huanzhang/Downloads/1710.11153.pdf:pdf},
isbn = {9782954035123},
pages = {50--57},
title = {{Onsets and frames: Dual-objective piano transcription}},
year = {2018}
}
@article{Wu2024TowardsOverview,
abstract = {Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.},
archivePrefix = {arXiv},
arxivId = {2402.13236},
author = {Wu, Haibin and Chen, Xuanjun and Lin, Yi-Cheng and Chang, Kai-wei and Chung, Ho-Lam and Liu, Alexander H. and Lee, Hung-yi},
eprint = {2402.13236},
file = {:Users/huanzhang/Downloads/2402.13236.pdf:pdf},
pages = {4--8},
title = {{Towards audio language modeling - an overview}},
url = {http://arxiv.org/abs/2402.13236},
year = {2024}
}
@article{Weck2024,
abstract = {Multimodal models that jointly process audio and language hold great promise in audio understanding and are increasingly being adopted in the music domain. By allowing users to query via text and obtain information about a given audio input, these models have the potential to enable a variety of music understanding tasks via language-based interfaces. However, their evaluation poses considerable challenges, and it remains unclear how to effectively assess their ability to correctly interpret music-related inputs with current methods. Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music understanding in multimodal language models focused on audio. MuChoMusic comprises 1,187 multiple-choice questions, all validated by human annotators, on 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Through the holistic analysis afforded by the benchmark, we evaluate five open-source models and identify several pitfalls, including an over-reliance on the language modality, pointing to a need for better multimodal integration. Data and code are open-sourced.},
archivePrefix = {arXiv},
arxivId = {2408.01337},
author = {Weck, Benno and Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, George and Bogdanov, Dmitry},
eprint = {2408.01337},
file = {:Users/huanzhang/Downloads/2408.01337v1.pdf:pdf},
title = {{MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models}},
url = {http://arxiv.org/abs/2408.01337},
year = {2024}
}
@article{Luo2022,
abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
archivePrefix = {arXiv},
arxivId = {2208.11970},
author = {Luo, Calvin},
eprint = {2208.11970},
file = {:Users/huanzhang/Downloads/2208.11970.pdf:pdf},
pages = {1--23},
title = {{Understanding Diffusion Models: A Unified Perspective}},
url = {http://arxiv.org/abs/2208.11970},
year = {2022}
}
@article{doi:10.1076/jnmr.30.1.39.7119,
author = {Dixon, Simon},
doi = {10.1076/jnmr.30.1.39.7119},
journal = {Journal of New Music Research},
number = {1},
pages = {39--58},
publisher = {Routledge},
title = {{Automatic Extraction of Tempo and Beat From Expressive Performances}},
url = {https://www.tandfonline.com/doi/abs/10.1076/jnmr.30.1.39.7119},
volume = {30},
year = {2001}
}
@article{Repp1994AcousticsPiano,
abstract = {This study investigated the perception and production of legato ("connected") articulation in repeatedly ascending and descending tone sequences on a digital piano (Roland RD-250s). Initial measurements of the synthetic tones revealed substantial decay times following key release. High tones decayed faster than low tones, as they did prior to key release, and long tones decayed sooner than short tones because of their more extensive pre-release decay. Musically trained subjects (including pianists) were asked to adjust the key overlap times (KOTs) of successive piano tones so that they sounded optimally, minimally , or maximally legato. The results supported two predictions based on the acoustic measurements: KOTs for successive tones judged to be optimally or maximally legato were greater for high than for low tones, and greater for long than for short tones, so that auditory overlap presumably remained more nearly constant. For minimal legato adjustments the effect of tone duration was reversed, however. Adjusted KOTs were also longer for relatively consonant tones (3 semitones separation) than for dissonant tones (1 semitone separation). Subsequently, KOTs were measured in skilled pianists' legato productions of tone sequences similar to those in the perceptual experiment. KOTs clearly increased with tone duration, an effect that was probably mototic in origin. There was no effect of tone height, suggesting that the pianists did not immediately adjust to differences in acoustic overlap. KOTs were slightly shorter for dissonant than for conson.ant tones.},
author = {Repp, Bruno H},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - 1994 - Acoustics, Perception, and Production of Legato Articulation on a Digital Piano.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
title = {{Acoustics, Perception, and Production of Legato Articulation on a Digital Piano}},
volume = {97},
year = {1995}
}
@article{Salami1989,
abstract = {Immediate memory for visually presented verbal material is disrupted by concurrent speech, even when the speech is unattended and in a foreign language. Unattended noise does not produce a reliable decrement. These results have been interpreted in terms of a phonological short-term store that excludes non-speechlike sounds. The characteristics of this exclusion process were explored by studying the effects of music on the serial recall of sequences of nine digits presented visually. Experiment 1 compared the effects of unattended vocal or instrumental music with quiet and showed that both types of music disrupted STM performance, with vocal music being more disruptive than instrumental music. Experiment 2 attempted to replicate this result using more highly trained subjects. Vocal music caused significantly more disruption than instrumental music, which was not significantly worse than the silent control condition. Experiment 3 compared instrumental music with unattended speech and with noise modulated in amplitude, the degree of modulation being the same as in speech. The results showed that the noise condition did not differ from silence; both of these proved less disruptive than instrumental music, which was in turn less disruptive than the unattended speech condition. Theoretical interpretation of these results and their potential practical implications for the disruption of cognitive performance by background music are discussed. There is abundant evidence that the immediate serial recall of visually presented verbal material is impaired when presentation is accompanied by speech, even when this speech is in a foreign language and the subject is Requests for reprints should be sent to Pierre Salame,},
annote = {From Duplicate 2 (Effects of Background Music on Phonological Short-term Memory - Salami, Pierre; Baddeley, Alan)

compulsory},
author = {Salami, Pierre and Baddeley, Alan},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Salami, Baddeley - 1989 - 41A ( I ).pdf:pdf},
journal = {THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY},
pages = {107--122},
title = {{Effects of Background Music on Phonological Short-term Memory}},
year = {1989}
}
@article{Honing2012,
abstract = {It was recently shown that rhythmic entrainment, long considered a human-specific mechanism, can be demonstrated in a selected group of bird species, and, somewhat surprisingly, not in more closely related species such as nonhuman primates. This observation supports the vocal learning hypothesis that suggests rhythmic entrainment to be a by-product of the vocal learning mechanisms that are shared by several bird and mammal species, including humans, but that are only weakly developed, or missing entirely, in nonhuman primates. To test this hypothesis we measured auditory event-related potentials (ERPs) in two rhesus monkeys (Macaca mulatta), probing a well-documented component in humans, the mismatch negativity (MMN) to study rhythmic expectation. We demonstrate for the first time in rhesus monkeys that, in response to infrequent deviants in pitch that were presented in a continuous sound stream using an oddball paradigm, a comparable ERP component can be detected with negative deflections in early latencies (Experiment 1). Subsequently we tested whether rhesus monkeys can detect gaps (omissions at random positions in the sound stream; Experiment 2) and, using more complex stimuli, also the beat (omissions at the first position of a musical unit, i.e. the 'downbeat'; Experiment 3). In contrast to what has been shown in human adults and newborns (using identical stimuli and experimental paradigm), the results suggest that rhesus monkeys are not able to detect the beat in music. These findings are in support of the hypothesis that beat induction (the cognitive mechanism that supports the perception of a regular pulse from a varying rhythm) is species-specific and absent in nonhuman primates. In addition, the findings support the auditory timing dissociation hypothesis, with rhesus monkeys being sensitive to rhythmic grouping (detecting the start of a rhythmic group), but not to the induced beat (detecting a regularity from a varying rhythm).},
author = {Honing, Henkjan and Merchant, Hugo and bor H{\'{a}} den, G{\'{a}} P and Prado, Luis and Bartolo, Ram{\'{o}}},
doi = {10.1371/journal.pone.0051369},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Honing et al. - 2012 - Rhesus Monkeys (Macaca mulatta) Detect Rhythmic Groups in Music, but Not the Beat.pdf:pdf},
title = {{Rhesus Monkeys (Macaca mulatta) Detect Rhythmic Groups in Music, but Not the Beat}},
url = {www.plosone.org},
year = {2012}
}
@article{Choi2022,
abstract = {Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.},
archivePrefix = {arXiv},
arxivId = {2211.07131},
author = {Choi, Eunjin and Chung, Yoonjin and Lee, Seolhee and Jeon, JongIk and Kwon, Taegyun and Nam, Juhan},
eprint = {2211.07131},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Choi et al. - Unknown - YM2413-MDB A MULTI-INSTRUMENTAL FM VIDEO GAME MUSIC DATASET WITH EMOTION ANNOTATIONS.pdf:pdf},
title = {{YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations}},
url = {http://arxiv.org/abs/2211.07131},
year = {2022}
}
@article{Hiraga2004,
abstract = {Rencon is an annual international event that started in 2002. It has roles of (1) pursuing evaluation methods for systems whose output includes subjective issues, and (2) providing a forum for researches of several fields related to musical expression. In the past, Rencon was held as a workshop as-sociated with a musical contest that provided a forum for presenting and discussing the latest research in automatic performance rendering. This year we introduce new evalua-tion methods of performance expression to Rencon: a Turing Test and a Gnirut Test, which is a reverse Turing Test, for performance expression. We have opened a section of the contests to any instruments and genre of music, including synthesized human voices.},
author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hiraga et al. - Unknown - Rencon 2004 Turing Test for Musical Expression.pdf:pdf},
journal = {Proceedings of the 2004 Conference on New Interface for Musical Expression (NIME04)},
keywords = {musical expression,performance ren,rencon,turing test},
pages = {120--123},
title = {{Rencon 2004 : Tuaring Test for Musical Expression}},
url = {http://portal.acm.org/citation.cfm?id=1085911},
year = {2004}
}
@article{Liang2024a,
abstract = {We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing. Specifically, WavCraft describes the content of raw sound materials in natural language and prompts the LLM conditioned on audio descriptions and users' requests. WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with audio expert modules. Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating users' control. In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands. Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips. Moreover, WavCraft can follow complex instructions to edit and even create audio content on the top of input recordings, facilitating audio producers in a broader range of applications. Our implementation and demos are available at https://github.com/JinhuaLiang/WavCraft.},
archivePrefix = {arXiv},
arxivId = {2403.09527},
author = {Liang, Jinhua and Zhang, Huan and Liu, Haohe and Cao, Yin and Kong, Qiuqiang and Liu, Xubo and Wang, Wenwu and Plumbley, Mark D. and Phan, Huy and Benetos, Emmanouil},
eprint = {2403.09527},
file = {:Users/huanzhang/Downloads/2403.09527.pdf:pdf},
pages = {1--11},
title = {{WavCraft: Audio Editing and Generation with Natural Language Prompts}},
url = {http://arxiv.org/abs/2403.09527},
year = {2024}
}
@inproceedings{Yang2019,
abstract = {Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general , an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships , from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitly-constrained variational autoencoder (EC 2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of "what if " a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study.},
author = {Yang, Ruihan and Wang, Dingsu and Wang, Ziyu and Chen, Tianyao and Jiang, Junyan and Xia, Gus},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - Unknown - DEEP MUSIC ANALOGY VIA LATENT REPRESENTATION DISENTANGLEMENT.pdf:pdf},
title = {{Deep Music Analogy via Latent Representation Disentanglement}},
year = {2019}
}
@techreport{Goebl2001ComputerResearch,
author = {Goebl, Werner and Bresin, Roberto},
booktitle = {MOSART workshop},
file = {:Users/huanzhang/Downloads/p35.pdf:pdf},
pages = {1--6},
title = {{Are Computer-Controlled Pianos a Reliable Tool in Music Performance Research?}},
year = {2001}
}
@article{Of2009,
author = {Of, Evaluation and Satellite, Passive and Sensing, Remote and Cloud, O F and Water, Liquid},
file = {:Users/huanzhang/Downloads/1740.pdf:pdf},
number = {February},
pages = {2--3},
title = {{for Review Only for Review Only for Review Only for Review Only}},
year = {2009}
}
@article{Park2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7580v3},
author = {Park, Jisoo and Kim, Jongho and Park, Jeong Mi and Choi, Ahyeon and Li, Wen-syan and Park, Jonghwa and Hwang, Seung-won},
eprint = {arXiv:1412.7580v3},
file = {:Users/huanzhang/Downloads/s41598-024-73810-0.pdf:pdf},
number = {Section 6},
pages = {1--17},
title = {{Piano Performance Evaluation Dataset With Multi-Level Perceptual Features}},
year = {2015}
}
@article{Ji2024,
abstract = {In recent years, large language models have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serves as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) most codec models are trained on only 1,000 hours of data, whereas most speech language models are trained on 60,000 hours; 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models; 3) The initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with improved Fourier transform structures and larger training datasets to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .},
archivePrefix = {arXiv},
arxivId = {2402.12208},
author = {Ji, Shengpeng and Fang, Minghui and Jiang, Ziyue and Huang, Rongjie and Zuo, Jialung and Wang, Shulei and Zhao, Zhou},
eprint = {2402.12208},
file = {:Users/huanzhang/Downloads/2402.12208.pdf:pdf},
title = {{Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models}},
url = {http://arxiv.org/abs/2402.12208},
year = {2024}
}
@article{Schubert2017,
abstract = {Can a computer play a music score, e.g. via a Disklavier, in a way that cannot be distinguished from a human performance of the same music? One hundred and seventy-two participants with a wide range of music playing backgrounds rated sound recordings of 7 performances of piano music by Kuhlau, one played by a human, and six generated by algorithms, including a ‘mechanical' and an ‘unmusical' rendering. Participants rated the extent to which each performance was by a human and explained their answers. The mechanical performance had the lowest mean rating, but the human performance was rated as statistically identical to the other stimuli. There were no differences between ratings made by classical piano experts and lay listeners, but despite this, the musicians were more confident with their ratings. Qualitative analysis revealed five broad themes that contribute to judging whether a piece appears to be human. The themes were labelled (in descending order of frequency) intuitive, expressive, imperfections, halo (global preference) and empathy. This paper presents new evidence systematically demonstrating that algorithm generated performances of piano music can be indistinguishable from human performances, suggesting some parallels with the 1990s victory of the Deep Blue computer of the world champion (human) chess player.},
author = {Schubert, Emery and Canazza, Sergio and {De Poli}, Giovanni and Rod{\`{a}}, Antonio},
doi = {10.1080/09298215.2016.1264976},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Schubert et al. - 2017 - Journal of New Music Research Algorithms can Mimic Human Piano Performance The Deep Blues of Music Algorithms c.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {artificial intelligence,automated piano performance,expert-novice,music competitions,music judgement,musical Turing test},
number = {2},
pages = {175--186},
title = {{Algorithms can Mimic Human Piano Performance: The Deep Blues of Music}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {46},
year = {2017}
}
@inproceedings{Jeong2019GraphPerformance,
abstract = {Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its own duration. In this paper, we represent the unique form of musical score using graph neu-ral network and apply it for rendering expressive piano performance from the music score. Specifically , we design the model using note-level gated graph neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of performance for a given input score, we employ a varia-tional auto-encoder. The result of the listening test shows that our proposed model generated more human-like performances compared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.},
author = {Jeong, Dasaem and Kwon, Taegyun and Kim, Yoojin and Nam, Juhan},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jeong et al. - Unknown - Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance.pdf:pdf},
title = {{Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance}},
year = {2019}
}
@techreport{Kilian2002VoiceApproach,
abstract = {Voice separation, along with tempo detection and quantisation, is one of the basic problems of computer-based transcription of music. An adequate separation of notes into different voices is crucial for obtaining readable and usable scores from performances of polyphonic music recorded on keyboard (or other polyphonic) instruments; for improving quantisation results within a transcription system; and in the context of music retrieval systems that primarily support monophonic queries. In this paper we propose a new voice separation algorithm based on a stochastic local search method. Different from many previous approaches, our algorithm allows chords in the individual voices; its behaviour is controlled by a small number of intuitive and musically motivated parameters; and it is fast enough to allow interactive optimisation of the result by adjusting the parameters in real-time. We demonstrate that compared to existing approaches, our new algorithm generates better solutions for a number of typical voice separation problems. We also show how by changing its parameters it is possible to create score output suitable for different needs, , piano-style orchestral scores.},
author = {Kilian, J and Hoos, HH H},
booktitle = {Proceedings of the Third Annual International Symposium on Music Information Retrieval},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Urgen Kilian, Hoos - Unknown - Voice Separation-A Local Optimisation Approach.pdf:pdf},
pages = {39--46},
title = {{Voice separation—a local optimisation approach}},
url = {http://www-devel.cs.ubc.ca/$\sim$hoos/Publ/KilHoo02.pdf},
year = {2002}
}
@inproceedings{Lee2019,
abstract = {Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and query-by-singer in both the in-domain and cross-domain tasks.},
archivePrefix = {arXiv},
arxivId = {1906.11139},
author = {Lee, Kyungyun and Nam, Juhan},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019},
eprint = {1906.11139},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Nam - 2019 - Learning a joint embedding space of monophonic and mixed music signals for singing voice.pdf:pdf},
isbn = {9781732729919},
pages = {295--302},
title = {{Learning a joint embedding space of monophonic and mixed music signals for singing voice}},
year = {2019}
}
@article{Weigl2020,
abstract = {The Web and other digital technologies have democratised music creation, reception, and analysis, putting music in the hands, ears, and minds of billions of users. Music digital libraries typically focus on an essential subset of this deluge-commercial and academic publications, and historical materials-but neglect to incorporate contributions by scholars, performers, and enthusiasts, such as annotations or performed interpretations of these artifacts, despite their potential utility for many types of users. In this paper we consider means by which digital libraries for mu-sicology may incorporate such contributions into their collections, adhering to principles of FAIR data management and respecting contributor rights as outlined in the EU's General Data Protection Regulation. We present an overview of centralised and decentralised approaches to this problem, and propose hybrid solutions in which contributions reside in a) user-controlled personal online datastores, b) decentralised file storage, and c) are published and aggregated into digital library collections. We outline the implementation of these ideas using Solid, a Web decentralisation project building on W3C standard technologies to facilitate publication and control over Linked Data. We demonstrate the feasibility of this approach by implementing prototypes supporting two types of contribution: Web Annotations describing or analysing musical elements in score encodings and music recordings; and, music performances and associated metadata supporting performance analyses across many renditions of a given piece. Finally, we situate these ideas within a wider conception of enriched, decentralised, and interconnected online music repositories.},
author = {Weigl, David M and Goebl, Werner and Hofmann, Alex and Crawford, Tim and Zubani, Federico and {S Liem}, Cynthia C and Porter, Alastair and {Pompeu Fabra Barcelona}, Universitat},
doi = {10.1145/3424911.3425519},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Weigl et al. - 2020 - ReadWrite Digital Libraries for Musicology.pdf:pdf},
isbn = {9781450387606},
keywords = {Applied computing → Digital libraries and archives,CCS CONCEPTS,In-formation systems → Multimedia content creation},
publisher = {ACM},
title = {{Read/Write Digital Libraries for Musicology}},
url = {https://doi.org/10.1145/3424911.3425519},
year = {2020}
}
@article{Pantev2001,
abstract = {Neural imaging studies have shown that the brains of skilled musicians respond differently to musical stimuli than do the brains of non-musicians, particularly for musicians who commenced practice at an early age. Whether brain attributes related to musical skill are attributable to musical practice or are hereditary traits that influence the decision to train musically is a subject of controversy, owing to its pedagogic implications. Here we report that auditory cortical representations measured neuromagnetically for tones of different timbre (violin and trumpet) are enhanced compared to sine tones in violinists and trumpeters, preferentially for timbres of the instrument of training. Timbre specificity is predicted by a principle of use-dependent plasticity and imposes new requirements on nativistic accounts of brain attributes associated with musical skill. {\textcopyright} 2001 Lippincott Williams & Wilkins.},
author = {Pantev, Christo and Roberts, Larry E. and Schulz, Matthias and Engelien, Almut and Ross, Bernhard},
doi = {10.1097/00001756-200101220-00041},
file = {:Users/huanzhang/Downloads/Timbre_specific_enhancement_of_auditory_cortical.41.pdf:pdf},
issn = {09594965},
journal = {NeuroReport},
keywords = {Auditory cortex,Magnetoencephalography (MEG),Musical skill,Neural plasticity,Timbre specificity},
number = {1},
pages = {169--174},
pmid = {11201080},
title = {{Timbre-specific enhancement of auditory cortical representations in musicians}},
volume = {12},
year = {2001}
}
@techreport{Lee,
abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore , mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
author = {Lee, Jaehoon and Lechao, Xiao ⇤ and Schoenholz, Samuel S and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey and Brain, Google},
title = {{Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent}},
url = {https://g.co/airesidency}
}
@article{Pati2018AssessmentNetworks,
abstract = {Music performance assessment is a highly subjective task often relying on experts to gauge both the technical and aesthetic aspects of the performance from the audio signal. This article explores the task of building computational models for music performance assessment, i.e., analyzing an audio recording of a performance and rating it along several criteria such as musicality, note accuracy, etc. Much of the earlier work in this area has been centered around using hand-crafted features intended to capture relevant aspects of a performance. However, such features are based on our limited understanding of music perception and may not be optimal. In this article, we propose using Deep Neural Networks (DNNs) for the task and compare their performance against a baseline model using standard and hand-crafted features. We show that, using input representations at different levels of abstraction, DNNs can outperform the baseline models across all assessment criteria. In addition, we use model analysis techniques to further explain the model predictions in an attempt to gain useful insights into the assessment process. The results demonstrate the potential of using supervised feature learning techniques to better characterize music performances.},
annote = {From Duplicate 1 (Assessment of student music performances using Deep Neural Networks - Pati, Kumar Ashis; Gururani, Siddharth; Lerch, Alexander)

Main points: understanding different music performance assessment facet


MFCC

DNN:
- objective: predicting human expert rating
- representation: Pitch Contour and Mel Spectralgram},
author = {Pati, Kumar Ashis and Gururani, Siddharth and Lerch, Alexander},
doi = {10.3390/app8040507},
file = {:Users/huanzhang/Downloads/applsci-08-00507.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {DNN,Deep learning,Deep neural networks,MIR,Music education,Music informatics,Music information retrieval,Music learning,Music performance assessment},
number = {4},
title = {{Assessment of student music performances using Deep Neural Networks}},
volume = {8},
year = {2018}
}
@inproceedings{Korzeniowski2017EndNetwork,
abstract = {We present an end-to-end system for musical key estimation, based on a convolutional neural network. The proposed system not only out-performs existing key estimation methods proposed in the academic literature; it is also capable of learning a unified model for diverse musical genres that performs comparably to existing systems specialised for specific genres. Our experiments confirm that different genres do differ in their interpretation of tonality, and thus a system tuned e.g. for pop music performs subpar on pieces of electronic music. They also reveal that such cross-genre setups evoke specific types of error (predicting the relative or parallel minor). However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.},
archivePrefix = {arXiv},
arxivId = {1706.02921},
author = {Korzeniowski, Filip and Widmer, Gerhard},
booktitle = {25th European Signal Processing Conference, EUSIPCO},
doi = {10.23919/EUSIPCO.2017.8081351},
eprint = {1706.02921},
file = {:Users/huanzhang/Downloads/1706.02921.pdf:pdf},
isbn = {9780992862671},
title = {{End-to-end musical key estimation using a convolutional neural network}},
year = {2017}
}
@techreport{Mehta,
abstract = {We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.},
archivePrefix = {arXiv},
arxivId = {2309.03199v2},
author = {Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\'{e}}kely, {\'{E}}va and Henter, Gustav Eje},
eprint = {2309.03199v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mehta et al. - Unknown - MATCHA-TTS A FAST TTS ARCHITECTURE WITH CONDITIONAL FLOW MATCHING.pdf:pdf},
isbn = {2309.03199v2},
keywords = {Index Terms-Diffusion models,acoustic modelling,flow matching,speech syn-thesis,text-to-speech},
title = {{MATCHA-TTS: A FAST TTS ARCHITECTURE WITH CONDITIONAL FLOW MATCHING}},
url = {https://github.com/jik876/hifi-gan/}
}
@article{2008AUTOMATEDPERFORMANCES,
file = {:Users/huanzhang/Downloads/2008-MusicPerception.pdf:pdf},
keywords = {automated analysis,bodily movements,emotional,emotional expression,piano performance},
pages = {103--120},
title = {{AUTOMATED ANALYSIS OF BODY MOVEMENT IN EMOTIONALLY EXPRESSIVE PIANO PERFORMANCES}},
year = {2008}
}
@inproceedings{Copet2023SimpleGeneration,
abstract = {We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft},
archivePrefix = {arXiv},
arxivId = {2306.05284},
author = {Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'{e}}fossez, Alexandre},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
eprint = {2306.05284},
file = {:Users/huanzhang/Downloads/2306.05284.pdf:pdf},
issn = {10495258},
title = {{Simple and Controllable Music Generation}},
url = {http://arxiv.org/abs/2306.05284},
year = {2023}
}
@article{Sarkar,
abstract = {Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articula-tion mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.},
author = {Sarkar, Saurjya and Benetos, Emmanouil and Sandler, Mark},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sarkar, Benetos, Sandler - Unknown - ENSEMBLESET A NEW HIGH QUALITY SYNTHESISED DATASET FOR CHAMBER ENSEMBLE SEPARATION.pdf:pdf},
title = {{ENSEMBLESET: A NEW HIGH QUALITY SYNTHESISED DATASET FOR CHAMBER ENSEMBLE SEPARATION}}
}
@article{Widmer2009YQXChopin,
abstract = {This article is about AI research in the context of a complex artistic behavior: expressive music performance. A computer program is presented that learns to play piano with "expression" and that even won an international computer piano performance contest. A superficial analysis of an expressive performance generated by the system seems to suggest creative musical abilities. After a critical discussion of the processes underlying this behavior, we abandon the question of whether the system is really creative and turn to the true motivation that drives this research: to use AI methods to investigate and better understand music performance as a human creative behavior. A number of recent and current results from our research are briefly presented that indicate that machines can give us interesting insights into such a complex creative behavior, even if they may not be creative themselves. Copyright {\textcopyright} 2009, Association for the Advancement of Artificial Intelligence. All rights reserved.},
author = {Widmer, Gerhard and Flossmann, Sebastian and Grachten, Maarten},
doi = {10.1609/aimag.v30i3.2249},
file = {:Users/huanzhang/Downloads/2249-Article Text-3075-1-10-20091020.pdf:pdf},
issn = {07384602},
journal = {AI Magazine},
number = {3},
pages = {35--48},
title = {{YQX plays Chopin}},
volume = {30},
year = {2009}
}
@misc{.pdf,
annote = {From Duplicate 1 (弗朗科利《理解后调性音乐》.pdf - )

音高中心性Pitch centricity: 围绕一个或多个音高中心的音高结构组织（不需要围绕一个特定主音，也不需要功能分级）。确立是靠音乐的前后关系
持续音
固定音型ostinato
对称轴
曲式：
分层stratification / layering
音程循环 interval cycle
八声音阶：半音全音交替排列，只有三种
六声音阶：半音加小三度，只有四种

音高级集合
集合中的音程是无序的
音级集合pitch set
标准序normal order：首尾音之间拥有最短间距
移位等同transpositional equivalence
倒影等同inversional equivalence
基本型prime form：同一个集合的等同形式的组合
- 标准序将第一个音标记为0，然后也许倒影把小的音程放在左边
音程级矢量interval-class vector：6个整数，数集合中6类音程各自有几个
Z关联集合z-related sets：有相同音程级矢量的不等同集合
不变音（移位中的和倒影中的）
抽象补集：具象补集的基本型

我们永远都不可能确切知道一位作者的所思所为。

超现代运动ultra-modern movement
作曲家联盟：Aaron Copland, Walter Piston, Roger Sessions
泛美作曲家协会：Ives, Ruth Crawford Seeger, Edgar Varese, Henry Cowell
Ives：Musical borrowing十四种类型
Crawford：纽姆neume

自由无调性：每部作品有专门的组织原则，更依赖作曲家的音乐直觉
序列：有序音级组合ordered pitch-class collection
音列的四种基本形式：原型。倒影，逆行，逆行倒影
berg - 全音程音列
webern - 派生音列derived row
音列集row class
十二音矩阵，可以得到四种变换形式的所有形态。
不变性
对称音列：从前向后或从后向前都可以保持其音程
配套combinatoriality：通过转换使集合的前六个音与原集的前六个音完全不同，形成互补

序列主义
二战后的二十几年是音乐史上风格最多样的时期之一
斯特拉文斯基：序列作品，轮转rotation技术
布列兹：整体序列主义，不仅仅是音高；乘法
巴比特：三音集合阵列

拓展音乐时间性的限制
节拍置换
polymeter
polyrhythm
梅西安：有限移位调式
塔拉：印度节奏模式
卡特Elliott Carter
节拍转换
施托克豪森
音乐的非线性：听不到作为之前音乐事件结果的音乐事件。
瞬间时间，纵向时间

偶然音乐，音响集群及其他
音响集群sound-mass composition (织体作曲法texture composition): 织体，密度，音区，力度取代了节奏，节拍，线条，和弦
偶然对位

后现代主义
音乐拼贴

二十一世纪},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 弗朗科利《理解后调性音乐》.pdf.pdf:pdf},
title = {弗朗科利《理解后调性音乐》.pdf}
}
@inproceedings{Kumar2023High-FidelityRVQGAN,
abstract = {Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves $\sim$90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.},
archivePrefix = {arXiv},
arxivId = {2306.06546},
author = {Kumar, Rithesh and Seetharaman, Prem and Luebs, Alejandro and Kumar, Ishaan and Kumar, Kundan},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (Neurips)},
eprint = {2306.06546},
file = {:Users/huanzhang/Downloads/2306.06546.pdf:pdf},
title = {{High-Fidelity Audio Compression with Improved {RVQGAN}}},
url = {http://arxiv.org/abs/2306.06546},
year = {2023}
}
@article{Boom2020RhythmNetworks,
abstract = {Music that is generated by recurrent neural networks often lacks a sense of direction and coherence. We therefore propose a two-stage LSTM-based model for lead sheet generation, in which the harmonic and rhythmic templates of the song are produced first, after which, in a second stage, a sequence of melody notes is generated conditioned on these templates. A subjective listening test shows that our approach outperforms the baselines and increases perceived musical coherence.},
archivePrefix = {arXiv},
arxivId = {2002.10266},
author = {{De Boom}, Cedric and {Van Laere}, Stephanie and Verbelen, Tim and Dhoedt, Bart},
doi = {10.1007/978-3-030-43887-6_38},
eprint = {2002.10266},
file = {:Users/huanzhang/Downloads/2002.10266v1.pdf:pdf},
isbn = {9783030438869},
issn = {18650937},
journal = {Communications in Computer and Information Science},
keywords = {Lead sheets,Music generation,Neural networks},
pages = {454--461},
title = {{Rhythm, Chord and Melody Generation for Lead Sheets Using Recurrent Neural Networks}},
volume = {1168 CCIS},
year = {2020}
}
@article{Xiao2023,
author = {Xiao, Zhe and Chen, Xin and Zhou, Li},
doi = {10.1016/j.sigpro.2023.109134},
file = {:Users/huanzhang/Downloads/1-s2.0-S0165168423002086-main.pdf:pdf},
issn = {0165-1684},
journal = {Signal Processing},
keywords = {"Automatic Music Transcription","Chord structure modeling","Deep Learning","Graph Convolutional Network","Multi-Label classification"},
pages = {109134},
publisher = {Elsevier B.V.},
title = {{Polyphonic Piano Transcription Based on Graph Convolutional Network}},
url = {https://doi.org/10.1016/j.sigpro.2023.109134},
year = {2023}
}
@article{Bregman1978AuditoryTimbre,
abstract = {In a natural environment, the auditory system must analyse an incoming wave train to determine two things: (a) which series of frequency components arose over time from the same source and should be integrated into a sequential stream, and (b) which set of simultaneous components arose from one source and should be fused into a timbre structure. A set of experiments was performed in which subject judged the stream organization and the timbre of a repeating cycle formed by a pair of more or less synchronous pure tones, B and C, and a preceding pure tone, A, whose frequency was varied in its proximity to that of the upper tone of the BC pair. These experiments demonstrated that fusion and sequential organization of streams are carried out using two sorts of information which compete to determine the best perceptual description of the input. Proxi-mal frequencies between sequential components promotes a sequential organization and the simultaneity of onset of frequency components promotes perceptual fusion.},
annote = {From Duplicate 1 (Auditory Streaming and the Building of Timbre - Bregman, Albert S; Pinker, Steven)

Week 5},
author = {Bregman, Albert S and Pinker, Steven},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bregman, Pinker - Unknown - Auditory Streaming and the Building of Timbre.pdf:pdf},
title = {{Auditory Streaming and the Building of Timbre}},
year = {1978}
}
@inproceedings{Dixon2002PerformanceAnimation,
author = {Dixon, Simon and {Werner Goebl} and {Gerhard Widmer}},
booktitle = {Proceedings of the International Computer Music Conference (ICMC)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dixon, Werner Goebl, Gerhard Widmer - 2002 - The Performance Worm Real Time Visualisation of Expression based on Langner's Tempo-Loudnes.pdf:pdf},
title = {{The Performance Worm: Real Time Visualisation of Expression based on Langner's Tempo-Loudness Animation}},
year = {2002}
}
@inproceedings{Hu2020UnsupervisedSynthesis,
abstract = {We present a method to generate speech from input text and a style vector that is extracted from a reference speech signal in an unsupervised manner, i.e., no style annotation, such as speaker information, is required. Existing unsupervised methods, during training, generate speech by computing style from the corresponding ground truth sample and use a decoder to combine the style vector with the input text. Training the model in such a way leaks content information into the style vector. The decoder can use the leaked content and ignore some of the input text to minimize the reconstruction loss. At inference time, when the reference speech does not match the content input, the output may not contain all of the content of the input text. We refer to this problem as “content leakage”, which we address by explicitly estimating and minimizing the mutual information between the style and the content through an adversarial training formulation. We call our method MIST - Mutual Information based Style Content Separation. The main goal of the method is to preserve the input content in the synthesized speech signal, which we measure by the word error rate (WER) and show substantial improvements over state-of-the-art unsupervised speech synthesis methods.},
annote = {MIST - mutal information based style-content separation

GST - global style token
- additional style encoder, jointly with TTS

"content leaking"

pre-training: single style

Why do we need to maximize T?},
archivePrefix = {arXiv},
arxivId = {2003.06227},
author = {Hu, Ting Yao and Shrivastava, Ashish and Tuzel, Oncel and Dhir, Chandra},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP40776.2020.9054591},
eprint = {2003.06227},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - Unknown - UNSUPERVISED STYLE AND CONTENT SEPARATION BY MINIMIZING MUTUAL INFORMATION FOR SPEECH SYNTHESIS.pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
keywords = {Controllable speech synthesis,Mutual information estimation,Unsupervised style-content separation},
title = {{Unsupervised style and content separation by minimizing mutual information for speech synthesis}},
year = {2020}
}
@article{Goebl2010InvestigationsChopin,
abstract = {This article presents research towards automated computational analysis of large corpora of music performance data. In particular, we focus on between-hand asynchronies in piano performances-an expressive device in which the performer's timing deviates from the nominally synchronized timing of the score. Between-hand asynchronies play an important role, particularly in Romantic music, but they have not been assessed quantitatively in any substantial way. We give a first report on a computational approach to analyzing a unique corpus of historic performance data: basically the complete works of Chopin, performed by the Russian-Georgian pianist Nikita Magaloff. Corpora of that size-hundreds of thousands of played notes with substantial expressive (and other) deviations from the written score-require a level of automation of analysis that has not been attained so far. We describe the required processing steps, from converting scanned scores into symbolic notation, to score-performance matching, definition, and automatic measurement of between-hand asynchronies, and a computational visualization tool for exploring and understanding the extracted information. Temporal asynchronies between the members of musical ensembles have been found to exhibit specific regularities: The principal instruments in classical wind and string trios tend to be 30-50 msec ahead of the others (Rasch 1979); soloists in jazz},
annote = {bass anticipation: bass tone precedes others by 70 msec

salience voice 30 msec earlier

hand info: score clef -> hand parts},
author = {Goebl, Werner and Flossmann, Sebastian and Widmer, Gerhard},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl, Flossmann, Widmer - 2010 - Investigations of Between-Hand Synchronization in Magaloff's Chopin.pdf:pdf},
journal = {Computer Music Journal},
number = {3},
pages = {35--44},
title = {{Investigations of Between-Hand Synchronization in Magaloff's Chopin}},
volume = {34},
year = {2010}
}
@article{Yin2023DeepGeneration,
abstract = {Deep learning methods are recognised as state-of-the-art for many applications of machine learning. Recently, deep learning methods have emerged as a solution to the task of automatic music generation (AMG) using symbolic tokens in a target style, but their superiority over non-deep learning methods has not been demonstrated. Here, we conduct a listening study to comparatively evaluate several music generation systems along six musical dimensions: stylistic success, aesthetic pleasure, repetition or self-reference, melody, harmony , and rhythm. A range of models, both deep learning algorithms and other methods, are used to generate 30-s excerpts in the style of Classical string quartets and classical piano improvisations. Fifty participants with relatively high musical knowledge rate unla-belled samples of computer-generated and human-composed excerpts for the six musical dimensions. We use non-parametric Bayesian hypothesis testing to interpret the results, allowing the possibility of finding meaningful non-differences between systems' performance. We find that the strongest deep learning method, a reimplemented version of Music Transformer, has equivalent performance to a non-deep learning method, MAIA Markov, demonstrating that to date, deep learning does not outperform other methods for AMG. We also find there still remains a significant gap between any algorithmic method and human-composed excerpts.},
author = {Yin, Zongyu and Reuben, Federico and Stepney, Susan and Collins, Tom and Learning, Machine},
doi = {10.1007/s10994-023-06309-w},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yin et al. - 123 - Deep learning's shallow gains a comparative evaluation of algorithms for automatic music generation.pdf:pdf},
isbn = {0123456789},
journal = {Machine Learning},
keywords = {Comparative evaluation,Deep learning,Listening study,Markov model,Music generation,Non-parametric Bayesian hypothesis testing},
title = {{Deep learning's shallow gains: a comparative evaluation of algorithms for automatic music generation}},
url = {https://doi.org/10.1007/s10994-023-06309-w},
year = {2023}
}
@article{Cifka2019SupervisedData,
abstract = {Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the 'style' of a piece while keeping its original 'content'. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of 'aligned' data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings.},
annote = {Style profile: note onsets and interval diff, in histogram},
archivePrefix = {arXiv},
arxivId = {1907.02265},
author = {C{\'{i}}fka, Ond\v{r}ej and \c{S}im\c{s}ekli, Umut and Richard, Ga{\"{e}}l},
doi = {10.5281/zenodo.3527878},
eprint = {1907.02265},
file = {:Users/huanzhang/Downloads/1907.02265.pdf:pdf},
isbn = {9781732729919},
journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {588--595},
title = {{Supervised symbolic music style translation using synthetic data}},
year = {2019}
}
@misc{Plack2005ASystem,
annote = {From Duplicate 2 (A Journey Through the Auditory System - Plack, C)

Week 1 reading},
author = {Plack, C},
booktitle = {A Sense of Hearing},
file = {:Users/huanzhang/Downloads/Plack2005_04.pdf:pdf},
pages = {67--87},
title = {{A Journey Through the Auditory System}},
year = {2005}
}
@article{Masada2019ChordMusic,
abstract = {We present a new approach to harmonic analysis that is trained to segment music into a sequence of chord spans tagged with chord labels. Formulated as a semi-Markov Conditional Random Field (semi-CRF), this joint segmentation and labeling approach enables the use of a rich set of segment-level features, such as segment purity and chord coverage, that capture the extent to which the events in an entire segment of music are compatible with a candidate chord label. The new chord recognition model is evaluated extensively on three corpora of classical music and a newly created corpus of rock music. Experimental results show that the semi-CRF model performs substantially better than previous approaches when trained on a sufficient number of labeled examples and remains competitive when the amount of training data is limited.},
archivePrefix = {arXiv},
arxivId = {1810.10002},
author = {Masada, Kristen and Bunescu, Razvan},
doi = {10.5334/tismir.18},
eprint = {1810.10002},
file = {:Users/huanzhang/Downloads/18-448-2-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {1,chord recognition,harmonic analysis,harmonic analysis is an,high-level,high-level representations of tonal,important step towards creating,introduction and motivation,music,segmental crf,semi-crf,symbolic music},
number = {1},
pages = {1},
title = {{Chord Recognition in Symbolic Music: A Segmental CRF Model, Segment-Level Features, and Comparative Evaluations on Classical and Popular Music}},
volume = {2},
year = {2019}
}
@book{Viberg2023,
author = {Viberg, Olga and Jivet, Ioana and Mu{\~{n}}oz-merino, Pedro J and Perifanou, Maria and Papathoma, Tina and Goos, Gerhard},
doi = {10.1007/978-3-031-42682-7},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Viberg et al. - 2023 - Responsive and Sustainable Founding Editors.pdf:pdf},
isbn = {9783031426810},
title = {{Responsive and Sustainable Founding Editors}},
year = {2023}
}
@article{Dannenberg2020,
abstract = {Computational Creativity is a multidisciplinary field that tries to obtain creative behaviors from computers. One of its most prolific subfields is that of Music Generation (also called Algorithmic Composition or Musical Metacreation), that uses computational means to compose music. Due to the multidisciplinary nature of this research field, it is sometimes hard to define precise goals and to keep track of what problems can be considered solved by state-of-the-art systems and what instead needs further developments. With this survey, we try to give a complete introduction to those who wish to explore Computational Creativity and Music Generation. To do so, we first give a picture of the research on the definition and the evaluation of creativity, both human and computational, needed to understand how computational means can be used to obtain creative behaviors and its importance within Artificial Intelligence studies. We then review the state of the art of Music Generation Systems, by citing examples for all the main approaches to music generation, and by listing the open challenges that were identified by previous reviews on the subject. For each of these challenges, we cite works that have proposed solutions, describing what still needs to be done and some possible directions for further research.},
author = {Dannenberg, Roger B and {Aurelio D'asaro}, Fabio and Goyal, Rinkaj and Carnovalini, Filippo and Rod{\`{a}}, Antonio},
doi = {10.3389/frai.2020.00014},
journal = {Frontiers in Artificial Intelligence | www.frontiersin.org},
keywords = {algorithmic composition,automatic composition,computational creativity,computer music,meta-review,music generation,musical metacreation,survey},
pages = {14},
title = {{Computational Creativity and Music Generation Systems: An Introduction to the State of the Art}},
url = {www.frontiersin.org},
volume = {1},
year = {2020}
}
@article{Sturm,
abstract = {Validity is the truth of an inference made from evidence, such as data collected in an experiment, and is central to working scientifically. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Considering validity in one's work can improve its scientific and engineering value. Puzzling MIR phenomena like adversarial attacks and performance glass ceilings become less mysterious through the lens of validity. In this article, we review the subject of validity in general, considering the four major types of validity from a key reference: Shadish et al. [2002]. We ground our discussion of these types with a prototypical MIR experiment: music classification using machine learning. Through this MIR experimentalists can be guided to make valid inferences from data collected from their experiments.},
archivePrefix = {arXiv},
arxivId = {2301.01578v1},
author = {Sturm, Bob L T and Flexer, Arthur},
eprint = {2301.01578v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sturm, Flexer - Unknown - Validity in Music Information Research Experiments.pdf:pdf},
title = {{Validity in Music Information Research Experiments}},
url = {https://ismir.net/conferences/}
}
@article{Repetto2017QantitativeScores,
abstract = {When lyrics of tonal languages are set to music, the pitch contour of the tones has to agree to a certain extent with the melodic contour to assure intelligibility. The relationship between the linguistic tones of the complex dialectal construct used in jingju (commonly known as Beijing or Peking opera) and its melody has been largely studied, but not definite consensus has been achieved among scholars. After reviewing the related literature, we present a first approach for the quantitative analysis of the relationship between linguistic tones and melody in jingju using a collection of machine readable music scores with tone category annotations for 7,283 syllables. We describe two statistical analyses performed in this collection regarding the melodic contour for each syllable and the pitch height relationship in 5,494 pairs of consecutive syllables. We argue that the obtained results contribute to supporting claims from the literature and complementing others, although some limitations of the approach might nuance the confidence of their validity.},
author = {Repetto, Rafael Caro and Zhang, Shuo and Serra, Xavier},
doi = {10.1145/3144749.3144758},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repetto, Zhang, Serra - 2017 - Qantitative analysis of the relationship between linguistic tones and melody in jingju using music scores.pdf:pdf},
isbn = {9781450353472},
journal = {ACM International Conference Proceeding Series},
keywords = {Chinese linguistic tones,Corpusdriven research,Jingju music,Music scores analysis},
pages = {41--44},
title = {{Qantitative analysis of the relationship between linguistic tones and melody in jingju using music scores}},
year = {2017}
}
@article{Collins2013SIARCT-CFP:Representations,
abstract = {The geometric approach to intra-opus pattern discovery (in which notes are represented as points in pitch-time space in order to discover repeated patterns within a piece of music) shows promise particularly for polyphonic music, but has attracted some criticism because: (1) the approach extends to a limited number of inexact repetition types only; (2) typically geometric pattern discovery algorithms have poor precision, returning many false positives. This paper describes and evaluates a solution to the inexactness problem where algorithms for pattern discovery and inexact pattern matching are integrated for the first time. Two complementary solutions are proposed and assessed for the precision problem, one involving categorisation (hence reduction) of output patterns, and the second involving a new algorithm that calculates the difference between consecutive point pairs, rather than all point pairs.},
author = {Collins, Tom and Arzt, Andreas and Flossmann, Sebastian and Widmer, Gerhard},
file = {:Users/huanzhang/Downloads/collinsEtAlISMIR2013.pdf:pdf},
isbn = {9780615900650},
journal = {Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013},
number = {Ismir},
pages = {549--554},
title = {{SIARCT-CFP: Improving precision and the discovery of inexact musical patterns in point-set representations}},
year = {2013}
}
@article{Rubenstein2023AudioPaLMListen,
abstract = {We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples},
archivePrefix = {arXiv},
arxivId = {2306.12925},
author = {Rubenstein, Paul K. and Asawaroengchai, Chulayuth and Nguyen, Duc Dung and Bapna, Ankur and Borsos, Zal{\'{a}}n and Quitry, F{\'{e}}lix de Chaumont and Chen, Peter and Badawy, Dalia El and Han, Wei and Kharitonov, Eugene and Muckenhirn, Hannah and Padfield, Dirk and Qin, James and Rozenberg, Danny and Sainath, Tara and Schalkwyk, Johan and Sharifi, Matt and Ramanovich, Michelle Tadmor and Tagliasacchi, Marco and Tudor, Alexandru and Velimirovi{\'{c}}, Mihajlo and Vincent, Damien and Yu, Jiahui and Wang, Yongqiang and Zayats, Vicky and Zeghidour, Neil and Zhang, Yu and Zhang, Zhishuai and Zilka, Lukas and Frank, Christian},
eprint = {2306.12925},
file = {:Users/huanzhang/Downloads/2306.12925v1.pdf:pdf},
title = {{AudioPaLM: A Large Language Model That Can Speak and Listen}},
url = {http://arxiv.org/abs/2306.12925},
year = {2023}
}
@techreport{Ewert2011Score-informedRecordings,
abstract = {The decomposition of a monaural audio recording into musicallymeaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for realworld non-synthetic piano recordings. {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Ewert, Sebastian and M{\"{u}}ller, Meinard},
booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ewert - 2011 - SCORE-INFORMED VOICE SEPARATION FOR PIANO RECORDINGS.pdf:pdf},
isbn = {9780615548654},
pages = {245--250},
title = {{Score-informed voice separation for piano recordings}},
year = {2011}
}
@inproceedings{Plasser2023DiscreteGeneration,
abstract = {Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in generating high-quality samples in both discrete and continuous domains. However, Discrete DDPMs (D3PMs) have yet to be applied to the domain of Symbolic Music. This work presents the direct generation of Polyphonic Symbolic Music using D3PMs. Our model exhibits state-of-the-art sample quality, according to current quantitative evaluation metrics, and allows for flexible infilling at the note level. We further show, that our models are accessible to post-hoc classifier guidance, widening the scope of possible applications. However, we also cast a critical view on quantitative evaluation of music sample quality via statistical metrics, and present a simple algorithm that can confound our metrics with completely spurious, non-musical samples.},
address = {Macau, China},
archivePrefix = {arXiv},
arxivId = {2305.09489},
author = {Plasser, Matthias and Peter, Silvan and Widmer, Gerhard},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2023/648},
eprint = {2305.09489},
file = {:Users/huanzhang/Downloads/2305.09489.pdf:pdf},
isbn = {9781956792034},
issn = {10450823},
title = {{Discrete Diffusion Probabilistic Models for Symbolic Music Generation}},
url = {http://arxiv.org/abs/2305.09489},
year = {2023}
}
@inproceedings{Melechovsky2024MustangoGeneration,
abstract = {The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models such as MusicGen and AudioLDM2.},
archivePrefix = {arXiv},
arxivId = {2311.08355},
author = {Melechovsky, Jan and Guo, Zixun and Ghosal, Deepanway and Majumder, Navonil and Herremans, Dorien and Poria, Soujanya},
booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL},
doi = {10.18653/v1/2024.naacl-long.459},
eprint = {2311.08355},
file = {:Users/huanzhang/Downloads/2311.08355.pdf:pdf},
isbn = {9798891761148},
title = {{Mustango: Toward Controllable Text-to-Music Generation}},
volume = {1},
year = {2024}
}
@article{Rowe2005RealPerformance,
abstract = {''Real time'' indicates that the actions of a computer system take place at the same time as events in the environment to which the system is responding. The expressivity of an interactive multimedia system is directly related to its real-time operation: the aesthetic qualities of ensemble performance or a call-and-response interplay between human and machine forces are changed dramatically when there is a significant delay to the computer's output. At the same time that some artistic opportunities are closed off, however, new ones emerge. In particular, this article will explore the spaces made available through unreal time-those latencies between the immediacy of ensemble interaction and delays so long as to be outside of any kind of ''real-time'' performance-the range between about 20 and 2,000 milliseconds. A discussion of these latencies, as encountered in a composition staged in distributed performance over Internet2, closes the article.},
author = {Rowe, Robert},
doi = {10.1080/1080/09298210500124034},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rowe - 2005 - Real Time and Unreal Time Expression in Distributed Performance.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
number = {1},
pages = {87--95},
title = {{Real Time and Unreal Time: Expression in Distributed Performance}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {34},
year = {2005}
}
@article{Slevc2009MakingSyntax,
abstract = {Linguistic processing, especially syntactic processing, is often considered a hallmark of human cognition; thus, the domain specificity or domain generality of syntactic processing has attracted considerable debate. The present experiments address this issue by simultaneously manipulating syntactic processing demands in language and music. Participants performed self-paced reading of garden path sentences, in which structurally unexpected words cause temporary syntactic processing difficulty. A musical chord accompanied each sentence segment, with the resulting sequence forming a coherent chord progression. When structurally unexpected words were paired with harmonically unexpected chords, participants showed substantially enhanced garden path effects. No such interaction was observed when the critical words violated semantic expectancy or when the critical chords violated timbral expectancy. These results support a prediction of the shared syntactic integration resource hypothesis (Patel, 2003), which suggests that music and language draw on a common pool of limited processing resources for integrating incoming elements into syntactic structures. Notations of the stimuli from this study may be downloaded from pbr.psychonomic-journals.org/content/supplemental. {\textcopyright} 2009 The Psychonomic Society, Inc.},
author = {Slevc, L. Robert and Rosenberg, Jason C. and Patel, Aniruddh D.},
doi = {10.3758/16.2.374},
file = {:Users/huanzhang/Downloads/Slevc2009_Article_MakingPsycholinguisticsMusical.pdf:pdf},
issn = {10699384},
journal = {Psychonomic Bulletin and Review},
number = {2},
pages = {374--381},
pmid = {19293110},
title = {{Making psycholinguistics musical: Self-paced reading time evidence for shared processing of linguistic and musical syntax}},
volume = {16},
year = {2009}
}
@techreport{Nakano2006AnFeatures,
abstract = {This paper presents a method of evaluating singing skills that does not require score information of the sung melody. This requires an approach that is different from existing systems, such as those currently used for Karaoke systems. Previous research on singing evaluation has focused on analyzing the characteristics of singing voice, but were not aimed at developing an automatic evaluation method. The approach presented in this study uses pitch interval accuracy and vibrato as acoustic features which are independent from specific characteristics of the singer or melody. The approach was tested by a 2-class (good/poor) classification test with 600 song sequences, and achieved an average classification rate of 83.5%.},
author = {Nakano, Tomoyasu and Goto, Masataka and Hiraga, Yuzuru},
booktitle = {INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nakano, Goto, Hiraga - Unknown - An Automatic Singing Skill Evaluation Method for Unknown Melodies Using Pitch Interval Accuracy and Vib.pdf:pdf},
isbn = {9781604234497},
keywords = {Automatic evaluation,Singing skill,Unknown melodies},
pages = {1706--1709},
title = {{An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features}},
volume = {4},
year = {2006}
}
@article{Kong2020,
abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset-tagging-cnn.},
archivePrefix = {arXiv},
arxivId = {1912.10211},
author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
doi = {10.1109/TASLP.2020.3030497},
eprint = {1912.10211},
file = {:Users/huanzhang/Downloads/1912.10211.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Audio tagging,pretrained audio neural networks,transfer learning},
number = {1},
pages = {2880--2894},
title = {{PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition}},
volume = {28},
year = {2020}
}
@article{Jacobs1998,
author = {Jacobs, J Pieter and Bullock, Daniel},
file = {:Users/huanzhang/Downloads/Jacobs-TwoProcessModelControl-1998.pdf:pdf},
number = {2},
pages = {169--199},
title = {{A Two-Process Model for Control of Legato Articulation across a Wide Range of Tempos during Piano Performance Author ( s ): J . Pieter Jacobs and Daniel Bullock Source : Music Perception : An Interdisciplinary Journal , Winter , 1998 , Vol . 16 , No . 2 P}},
volume = {16},
year = {1998}
}
@article{Iqbal2021ARCA23K:Noise,
abstract = {The availability of audio data on sound sharing platforms such as Freesound gives users access to large amounts of annotated audio. Utilising such data for training is becoming increasingly popular, but the problem of label noise that is often prevalent in such datasets requires further investigation. This paper introduces ARCA23K, an Automatically Retrieved and Curated Audio dataset comprised of over 23000 labelled Freesound clips. Unlike past datasets such as FSDKaggle2018 and FSDnoisy18K, ARCA23K facilitates the study of label noise in a more controlled manner. We describe the entire process of creating the dataset such that it is fully reproducible, meaning researchers can extend our work with little effort. We show that the majority of labelling errors in ARCA23K are due to out-of-vocabulary audio clips, and we refer to this type of label noise as open-set label noise. Experiments are carried out in which we study the impact of label noise in terms of classification performance and representation learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:2109.09227v1},
author = {Iqbal, Turab and Cao, Yin and Bailey, Andrew and Plumbley, Mark D and Wang, Wenwu},
eprint = {arXiv:2109.09227v1},
file = {:Users/huanzhang/Downloads/Iqbal et al_2021_ARCA23K.pdf:pdf},
journal = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)},
number = {November},
pages = {201--205},
title = {{ARCA23K: An Audio Dataset for Investigating Open-Set Label Noise}},
year = {2021}
}
@article{Bernays2013ExpressivePerformance,
abstract = {Timbre is an essential expressive parameter in piano per-formance. Advanced-level pianists have integrated the pal-ette of timbres at their artistic disposal as abstract concepts and multimodal images. A correspondingly imaged vocab-ulary composed of various adjectival descriptors is used in discussing and designating precise timbral nuances. How-ever, the actual means of production and control of timbral nuances at the piano are not always explicitly expressed. This study explores the precise performance parameters used in producing different timbral nuances. For this aim, four short pieces were composed. Each was performed by four pianists, who highlighted five timbral nuances most representative of the piano timbre-describing vocabulary: dry, bright, round, velvety and dark. The performances were recorded with the B{\"{o}}sendorfer CEUS system, a high-quality piano equipped with high-accuracy sensors and an embedded computer. Fine-grained performance features were extracted from the data collected. The features that significantly differed between different-timbre performances were identified. The performance space resulting from a principal component analysis revealed an average organi-zation of timbral nuances along a circular arc. Thirteen es-sential, timbre-discriminating performance features were selected. Detailed descriptions were thus obtained for each timbral nuance, according to the fine characteristics of their production and control in piano performance.},
author = {Bernays, Michel and Traube, Caroline},
file = {:Users/huanzhang/Downloads/smc_2013_040.pdf:pdf;:Users/huanzhang/Downloads/Bernays_Traube_SMAC2013_sub3.pdf:pdf},
isbn = {978-3-8325-3472-1},
journal = {Proceedings of the Sound and Music Computing Conference},
number = {August 2013},
pages = {341--346},
title = {{Expressive Production of Piano Timbre: Touch and Playing Techniques for Timbre Control in Piano Performance}},
url = {http://smcnetwork.org/system/files/EXPRESSIVE PRODUCTION OF PIANO TIMBRE - TOUCH AND PLAYING TECHNIQUES FOR CONTROL IN PERFORMANCE.pdf http://smcnetwork.org/system/files/EXPRESSIVE PRODUCTION OF PIANO TIMBRE - TOUCH AND PLAYING TECHNIQUES FOR TIMBRE CONTR},
year = {2013}
}
@article{Pasini2022,
abstract = {Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.},
archivePrefix = {arXiv},
arxivId = {2208.08706},
author = {Pasini, Marco and Schl{\"{u}}ter, Jan},
eprint = {2208.08706},
file = {:Users/huanzhang/Downloads/74.pdf:pdf},
title = {{Musika! Fast Infinite Waveform Music Generation}},
url = {http://arxiv.org/abs/2208.08706},
year = {2022}
}
@techreport{McFee2018AdaptiveDetection,
abstract = {Sound event detection (SED) methods are tasked with labeling segments of audio recordings by the presence of active sound sources. SED is typically posed as a supervised machine learning problem, requiring strong annotations for the presence or absence of each sound source at every time instant within the recording. However, strong annotations of this type are both labor- and cost-intensive for human annotators to produce, which limits the practical scalability of SED methods. In this paper, we treat SED as a multiple instance learning (MIL) problem, where training labels are static over a short excerpt, indicating the presence or absence of sound sources but not their temporal locality. The models, however, must still produce temporally dynamic predictions, which must be aggregated (pooled) when comparing against static labels during training. To facilitate this aggregation, we develop a family of adaptive pooling operators - referred to as autopool - which smoothly interpolate between common pooling operators, such as min-, max-, or average-pooling, and automatically adapt to the characteristics of the sound sources in question. We evaluate the proposed pooling operators on three datasets, and demonstrate that in each case, the proposed methods outperform nonadaptive pooling operators for static prediction, and nearly match the performance of models trained with strong, dynamic annotations. The proposed method is evaluated in conjunction with convolutional neural networks, but can be readily applied to any differentiable model for time-series label prediction. While this paper focuses on SED applications, the proposed methods are general, and could be applied widely to MIL problems in any domain.},
archivePrefix = {arXiv},
arxivId = {1804.10070v2},
author = {McFee, Brian and Salamon, Justin and Bello, Juan Pablo},
booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},
doi = {10.1109/TASLP.2018.2858559},
eprint = {1804.10070v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mcfee, Salamon, Pablo Bello - Unknown - IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, IN PRESS, 2018 Adaptive pooling oper.pdf:pdf},
issn = {23299290},
keywords = {Sound event detection,deep learning,machine learning,multiple instance learning},
number = {11},
pages = {2180--2193},
title = {{Adaptive Pooling Operators for Weakly Labeled Sound Event Detection}},
volume = {26},
year = {2018}
}
@article{Raphael2011,
abstract = {A system for musical accompaniment is presented in which a computer-driven orchestra follows and learns from a soloist in a concerto-like setting. The system is decomposed into three modules: The first computes a real-time score match using a hidden Markov model; the second generates the output audio by phase-vocoding a preexisting audio recording; the third provides a link between these two, by predicting future timing evolution using a Kalman Filter-like model. Several examples are presented showing the system in action in diverse musical settings. Connections with machine learning are highlighted, showing current weaknesses and new possible directions. {\textcopyright} 2011 ACM.},
annote = {From Duplicate 1 (The informatics philharmonic - Raphael, Christopher)

HMM score match
Phase Vocoding
predicting future timing evolution by Kalman Filter},
author = {Raphael, Christopher},
doi = {10.1145/1897852.1897875},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Raphael - 2011 - The informatics philharmonic.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {87--93},
title = {{The informatics philharmonic}},
volume = {54},
year = {2011}
}
@article{Yarbrough2018PerceptionPianists,
author = {Yarbrough, Cornelia and Speer, D. and Parker, S.},
file = {:Users/huanzhang/Downloads/Yarbrough-PerceptionPerformanceDynamics-1993.pdf:pdf},
journal = {Bulletin of the Council for Research in Music Education},
number = {118},
pages = {33--43},
title = {{Perception and performance of dynamics and articulation among young pianists}},
url = {https://www.jstor.org/stable/40318591},
volume = {118},
year = {1993}
}
@article{Qian2022ContentVecSpeakers,
abstract = {Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations.},
archivePrefix = {arXiv},
arxivId = {2204.09224},
author = {Qian, Kaizhi and Zhang, Yang and Gao, Heting and Ni, Junrui and Lai, Cheng-I and Cox, David and Hasegawa-Johnson, Mark and Chang, Shiyu},
eprint = {2204.09224},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Qian et al. - Unknown - CONTENTVEC An Improved Self-Supervised Speech Representation by Disentangling Speakers.pdf:pdf},
journal = {Proceedings of the 39th International Conference on Machine Learning (PMLR)},
title = {{ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers}},
url = {http://arxiv.org/abs/2204.09224},
year = {2022}
}
@inproceedings{Harris1991RepresentingSymbolically,
author = {Harris, Mitch and Smaill, Alan and Wiggins, Geraint},
booktitle = {IX Colloquio di Informatica Musicale (Venice)},
file = {:Users/huanzhang/Downloads/download (3).pdf:pdf},
title = {{Representing Music Symbolically}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.473},
year = {1991}
}
@inproceedings{Masaki2011,
abstract = {This study assesses the validity of a new self-report measure of piano performance quality, the Quality Assessment in Music Performance Inventory (QAMPI), and the adaptability of sport video analysis methods to piano performance evaluation. Piano students from Brandon Univer-sity's School of Music (n=21) volunteered to have real rehearsal and concert performances video recorded. Students employed QAMPI before and while watching the video recording to assess the perceived quality of their concert performance compared with their individual performance potential established in the rehearsal. An expert also employed QAMPI to evaluate each student's concert piano performance quality after reviewing the rehearsal and concert video recordings. Initial results indicate that piano students' assessment of their performance quality measured by QAMPI before and while watching the video recording differ substantially and that the students' self-evaluation while watching the performance video recording is closer to that of expert assessment. Cronbach's alpha demonstrated good internal consistency. These initial results indicate that QAMPI provides a consistent measure of music performance elements using video feedback and insight into pianists' self-perception of performance quality.},
author = {Masaki, Megumi and Hechler, Peter and Gadbois, Shannon and Waddell, George},
booktitle = {Proceedings of the International Symposium on Performance Science 2011},
file = {:Users/huanzhang/Downloads/086Masaki.pdf:pdf},
isbn = {9789490306021},
keywords = {and understanding,feedback for training,music performance anxiety,music performance assessment,performance analysis,performance perception,qampi,sport studies have established,the efficacy of video,video feedback,wang and parameswaran 2004},
pages = {503--508},
title = {{Piano performance assessment: Video feedback and the Quality Assessment in Music Performance Inventory (QAMPI)}},
year = {2011}
}
@article{Take2024,
abstract = {Recent MIDI-to-audio synthesis methods have employed deep neural networks to successfully generate high-quality and expressive instrumental tracks. However, these methods require MIDI annotations for supervised training, limiting the diversity of the output audio in terms of instrument timbres, and expression styles. We propose CoSaRef, a MIDI-to-audio synthesis method that can be developed without MIDI-audio paired datasets. CoSaRef first performs concatenative synthesis based on MIDI inputs and then refines the resulting audio into realistic tracks using a diffusion-based deep generative model trained on audio-only datasets. This approach enhances the diversity of audio timbres and expression styles. It also allows for control over the output timbre based on audio sample selection, similar to traditional functions in digital audio workstations. Experiments show that while inherently capable of generating general tracks with high control over timbre, CoSaRef can also perform comparably to conventional methods in generating realistic audio.},
archivePrefix = {arXiv},
arxivId = {2410.16785},
author = {Take, Osamu and Akama, Taketo},
eprint = {2410.16785},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Take, Akama - 2024 - Annotation-Free MIDI-to-Audio Synthesis via Concatenative Synthesis and Generative Refinement.pdf:pdf},
month = {oct},
title = {{Annotation-Free MIDI-to-Audio Synthesis via Concatenative Synthesis and Generative Refinement}},
url = {http://arxiv.org/abs/2410.16785},
year = {2024}
}
@article{Ramoneda2024CanDataset,
abstract = {Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music Information Retrieval (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription. This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset as well as the developed code and trained models are publicly shared to promote further research in the field.},
archivePrefix = {arXiv},
arxivId = {2403.03947},
author = {Ramoneda, Pedro and Lee, Minhee and Jeong, Dasaem and Valero-Mas, J. J. and Serra, Xavier},
eprint = {2403.03947},
file = {:Users/huanzhang/Downloads/2403.03947.pdf:pdf},
pages = {1--13},
title = {{Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset}},
url = {http://arxiv.org/abs/2403.03947},
year = {2024}
}
@article{CarvalhoExploitingIdentification,
abstract = {This paper addresses the problem of cross-modal musical piece identification and retrieval: finding the appropriate recording(s) from a database given a sheet music query, and vice versa, working directly with audio and scanned sheet music images. The fundamental approach to this [1] is to learn a cross-modal embedding space with a suitable similarity structure for audio and sheet image snippets, using a deep neural network, and identifying candidate pieces by cross-modal near neighbour search in this space. However, this method is oblivious of temporal aspects of music. In this paper, we introduce two strategies that address this shortcoming. First, we present a strategy that aligns sequences of embeddings learned from sheet music scans and audio snippets. A series of experiments on whole piece and fragment-level retrieval on 24 hours worth of classical piano recordings demonstrates significant improvement. Second, we show that the retrieval can be further improved by introducing an attention mechanism to the embedding learning model that reduces the effects of tempo variations in music. To conclude, we assess the scalability of our method and discuss potential measures to make it suitable for truly large-scale applications.},
archivePrefix = {arXiv},
arxivId = {2105.12536v1},
author = {Carvalho, Lu{\'{i}}s and Widmer, Gerhard},
eprint = {2105.12536v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Carvalho, Widmer - Unknown - Exploiting Temporal Dependencies for Cross-modal Music Piece Identification.pdf:pdf},
keywords = {Index Terms-alignment,cross-modal,embedding learning,piece identification,sheet music},
title = {{Exploiting Temporal Dependencies for Cross-modal Music Piece Identification}},
url = {https://imslp.org}
}
@phdthesis{Heeringa2005MeasuringDistance,
author = {Heeringa, Wilbert},
school = {University of Groningen},
title = {{Measuring Dialect Pronunciation Differences using Levenshtein Distance}},
year = {2005}
}
@inproceedings{Koepke2020SightTranscription,
abstract = {Automatic music transcription has primarily focused on transcribing audio to a symbolic music representation (e.g. MIDI or sheet music). However, audio-only approaches often struggle with polyphonic instruments and background noise. In contrast, visual information (e.g. a video of an instrument being played) does not have such ambiguities. In this work, we address the problem of transcribing piano music from visual data alone. We propose an end-to-end deep learning framework that learns to automatically predict note onset events given a video of a person playing the piano. From this, we are able to transcribe the played music in the form of MIDI data. We find that our approach is surprisingly effective in a variety of complex situations, particularly those in which music transcription from audio alone is impossible. We also show that combining audio and video data can improve the transcription obtained from each modality alone.},
author = {Koepke, A Sophia and Wiles, Olivia and Moses, Yael and Zisserman, Andrew and Vgg, †},
booktitle = {Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2020)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Koepke et al. - Unknown - SIGHT TO SOUND AN END-TO-END APPROACH FOR VISUAL PIANO TRANSCRIPTION.pdf:pdf},
keywords = {Index Terms-visual music transcription,automatic music transcription,deep learning,music information retrieval},
title = {{Sight to Sound: An End-to-end Approach for Visual Piano Transcription}},
year = {2020}
}
@book{NeuhausBook,
author = {Neuhaus, Heinrich},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Neuhaus - Unknown - The Art of Piano Playing.pdf:pdf},
title = {{The Art of Piano Playing}}
}
@article{Morsi2024SimulatingLearning,
abstract = {The development of machine-learning based technologies to support music instrument learning needs large-scale datasets that capture the different stages of learning in a manner that is both realistic and computation-friendly. We are interested in modeling the mistakes of beginner-intermediate piano performances in practice or work-in-progress settings. In the absence of large-scale data representing our target case, our approach is to start by understanding such mistakes from real data and then provide a methodology for their simulation, thus creating synthetic data to support the training of performance assessment models. The main goals of this paper are: a) to propose a taxon-omy of performance mistakes, specifically apt for simulating or reproducing/recreating them on mistake-free MIDI performances, and b) to provide a pipeline for creating synthetic datasets based on the former. We incorporate prior research in related contexts to facilitate the understanding of common mistake behaviours. Then, we design a hierarchical mistake taxonomy to categorize two real-world datasets capturing relevant piano performance contexts. Finally, we discuss our approach with 3 music teachers through a listening test and subsequent discussions.},
author = {Morsi, Alia and Zhang, Huan and Maezawa, Akira and Dixon, Simon and Serra, Xavier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/MORSIaliamorsi et al. - 2024 - SIMULATING PIANO PERFORMANCE MISTAKES FOR MUSIC LEARNING.pdf:pdf},
journal = {In Proceedings of the Sound and Music Computing Conference (SMC)},
title = {{Simulating Piano Performance Mistakes for Music Learning}},
url = {https://github.com/Alia-morsi/piano-synmist},
year = {2024}
}
@inproceedings{Garcia2023VampNetModeling,
abstract = {We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.},
archivePrefix = {arXiv},
arxivId = {2307.04686},
author = {Garcia, Hugo Flores and Seetharaman, Prem and Kumar, Rithesh and Pardo, Bryan},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
eprint = {2307.04686},
file = {:Users/huanzhang/Downloads/2307.04686.pdf:pdf},
title = {{{VampNet}: Music Generation via Masked Acoustic Token Modeling}},
url = {http://arxiv.org/abs/2307.04686},
year = {2023}
}
@inproceedings{Li2018DisentangledAutoencoder,
abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
annote = {unsupervised!

beta-vae: objective function
infogan: network architecture

restrict the dimentionality of local variable z, small enough to only obtain information on single timestep 

f: time-invariant
z: time-variant

q: 
- content features independent from motion 
- content features dependent},
archivePrefix = {arXiv},
arxivId = {1803.02991},
author = {Li, Yingzhen and Mandt, Stephan},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1803.02991},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li, Mandt - 2018 - Disentangled Sequential Autoencoder.pdf:pdf},
isbn = {9781510867963},
title = {{Disentangled sequential autoencoder}},
year = {2018}
}
@article{Desain2003ThePriming,
abstract = {Two experiments on categorical rhythm perception are reported, the object of which was to investigate how listeners perceive discrete rhythmic categories while listening to rhythms performed on a continuous time scale. This is studied by considering the space of all temporal patterns (all possible rhythms made up of three intervals) and how they, in perception, are partitioned into categories, ie where the boundaries of these categories are located. This process of categorisation is formalised as the mapping from the continuous space of a series of time intervals to a discrete, symbolic domain of integer-ratio sequences. The methodological frame-work uses concepts from mathematics and psychology (eg convexity and entropy) that allow precise characterisations of the empirical results. In the first experiment, twenty-nine participants performed an identification task with 66 rhythmic stimuli (a systematic sampling of the performance space). The results show that listeners do not just perceive the time intervals between onsets of sounds as placed in a homogeneous continuum. Instead, they can reliably identify rhythmic categories, as a chronotopic time clumping map reveals. In a second experiment, the effect of metric priming was studied by presenting the same stimuli but preceded with a duple or triple metre subdivision. It is shown that presenting patterns in the context of a metre has a large effect on rhythmic categorisation: the presence of a specific musical metre primes the perception of specific rhythmic patterns.},
annote = {From Duplicate 2 (The formation of rhythmic categories and metric priming - Desain, Peter; Honing, Henkjan)

Week 4},
author = {Desain, Peter and Honing, Henkjan},
doi = {10.1068/p3370},
file = {:Users/huanzhang/Downloads/p3370.pdf:pdf},
issn = {03010066},
journal = {Perception},
number = {3},
pages = {341--365},
pmid = {12729384},
title = {{The formation of rhythmic categories and metric priming}},
volume = {32},
year = {2003}
}
@article{Goel2024,
abstract = {Existing datasets for audio understanding primarily focus on single-turn interactions (i.e. audio captioning, audio question answering) for describing audio in natural language, thus limiting understanding audio via interactive dialogue. To address this gap, we introduce Audio Dialogues: a multi-turn dialogue dataset containing 163.8k samples for general audio sounds and music. In addition to dialogues, Audio Dialogues also has question-answer pairs to understand and compare multiple input audios together. Audio Dialogues leverages a prompting-based approach and caption annotations from existing datasets to generate multi-turn dialogues using a Large Language Model (LLM). We evaluate existing audio-augmented large language models on our proposed dataset to demonstrate the complexity and applicability of Audio Dialogues. Our code for generating the dataset will be made publicly available. Detailed prompts and generated dialogues can be found on the demo website https://audiodialogues.github.io/.},
archivePrefix = {arXiv},
arxivId = {2404.07616},
author = {Goel, Arushi and Kong, Zhifeng and Valle, Rafael and Catanzaro, Bryan},
eprint = {2404.07616},
file = {:Users/huanzhang/Downloads/2404.07616.pdf:pdf},
title = {{Audio Dialogues: Dialogues dataset for audio and music understanding}},
url = {http://arxiv.org/abs/2404.07616},
year = {2024}
}
@article{Drake1993AccentPerformance,
abstract = {Perceptual studies suggest that the segmentation of a musical sequence is influenced by three accent structures: rhythmic grouping, melodic, and metric accent structures. We investigate whether performers emphasize these types of accents with systematic performance variations (intensity, interonset timing, and articulation). In three experiments, skilled pianists performed sequences of various musical complexities: simple sequences containing only one accent structure (Experiment 1), more complex sequences containing coinciding or conflicting accent structures (Experiment 2), and a concert pianist's performance of a sonata containing coinciding and conflicting accent structures (Experiment 3). In all three musical contexts, similar systematic performance variations were observed in relation to each type of accent. Variations corresponding to rhythmic grouping accents were most consistent across musical contexts and dominated when the accent structures conflicted. These findings suggest perceptual correlates for the accent structures in music performance that may facilitate listeners' segmentation of musical sequences. {\textcopyright} 1993, The Regents of the University of California. All rights reserved.},
author = {Drake, Carolyn and Palmer, Caroline},
doi = {10.2307/40285574},
file = {:Users/huanzhang/Downloads/Drake-AccentStructuresMusic-1993.pdf:pdf},
issn = {15338312},
journal = {Music Perception},
number = {3},
pages = {343--378},
title = {{Accent Structures in Music Performance}},
volume = {10},
year = {1993}
}
@article{Johnson,
abstract = {Synthesis using physical modeling has a long history. As computational costs for physical modeling synthesis are often much greater than for conventional synthesis methods, most techniques currently rely on simplifying assumptions. These include digital waveguides, as well as modal synthesis methods. Although such methods are efficient, it can be difficult to approach some of the more detailed behavior of musical instruments in this way, including strongly nonlinear interactions. Mainstream time-stepping simulation methods, despite being computationally costly, allow for such detailed modeling. In this article, the results of a five-year research project, Next Generation Sound Synthesis, are presented, with regard to algorithm design for a variety of sound-producing systems, including brass and bowed-string instruments, guitars, and large-scale environments for physical modeling synthesis. In addition, 3-D wave-based modeling of large acoustic spaces is discussed, as well as the embedding of percussion instruments within such spaces for full spatialization. This article concludes with a discussion of some of the basics of such time-stepping methods, as well as their application in audio synthesis.},
author = {Johnson, David and Damian, Daniela and Tzanetakis, George},
doi = {10.1162/COMJ},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Damian, Tzanetakis - Unknown - Detecting Hand Posture in Piano Playing Using Depth Data.pdf:pdf},
journal = {Computer Music Journal},
number = {12},
pages = {1--8},
title = {{Detecting Hand Postrue in Piano Playing Using Depth Data}},
url = {http://direct.mit.edu/comj/article-pdf/43/1/59/2005111/comj_a_00500.pdf},
volume = {52},
year = {2013}
}
@article{Zacharakis2014,
abstract = {A STUDY OF MUSICAL TIMBRE SEMANTICS WAS conducted with listeners from two different linguistic groups. In two separate experiments, native Greek and English speaking participants were asked to describe 23 musical instrument tones of variable pitch using a predefined vocabulary of 30 adjectives. The common experimental protocol facilitated the investigation of the influence of language on musical timbre semantics by allowing for direct comparisons between linguistic groups. Data reduction techniques applied to the data of each group revealed three salient semantic dimensions that shared common conceptual properties between linguistic groups namely: luminance, texture, and mass. The results supported universality of timbre semantics. A correlation analysis between physical characteristics and semantic dimensions associated: i) texture with the energy distribution of harmonic partials, ii) thickness (a term related to either mass or luminance) and brilliance with inharmonicity and spectral centroid variation, and iii) F0 with mass or luminance depending on the linguistic group. {\textcopyright} 2014 By The Regents of The University of California.},
annote = {Week 3},
author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
doi = {10.1525/MP.2014.31.4.339},
issn = {15338312},
journal = {Music Perception},
keywords = {Acoustic correlates,CATPCA,Musical timbre semantics,Timbre spaces,Verbal attribute magnitude estimation},
number = {4},
pages = {339--358},
title = {{An interlanguage study of musical timbre semantic dimensions and their acoustic correlates}},
url = {http://online.ucpress.edu/mp/article-pdf/31/4/339/191000/mp_2014_31_4_339.pdf},
volume = {31},
year = {2014}
}
@inproceedings{Lin2024ContentModelling,
abstract = {Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with low-resource semi-supervised learning. We fine-tune the model with less than 4% of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online 1 2 .},
archivePrefix = {arXiv},
arxivId = {2310.17162v3},
author = {Lin, Liwei and Xia, Gus and Jiang, Junyan and Zhang, Yixiao},
booktitle = {Proceeding of the 25t International Society on Music Information Retrieval (ISMIR)},
eprint = {2310.17162v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - Unknown - CONTENT-BASED CONTROLS FOR MUSIC LARGE LANGUAGE MODELING.pdf:pdf},
title = {{Content-based Controls for Music Large Language Modeling}},
url = {https://kikyo-16.github.io/coco-mulla/.},
year = {2024}
}
@book{Kirke2013GuidePerformance,
abstract = {SPRINGER COMPUTER SALE: PRICE SHOWN IS 50% OFF This book discusses all aspects of computing for expressive performance, from the history of CSEMPs to the very latest research, in addition to discussing the fundamental ideas, and key issues and directions for future research. Topics and features: includes review questions at the end of each chapter; presents a survey of systems for real-time interactive control of automatic expressive music performance, including simulated conducting systems; examines two systems in detail, YQX and IMAP, each providing an example of a very different approach; introduces techniques for synthesizing expressiv},
author = {Kirke, Alexis and Miranda, Eduardo},
booktitle = {Guide to Computing for Expressive Music Performance},
doi = {10.1007/978-1-4471-4123-5},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Guide to Computing for Expressive Music Performance.pdf:pdf},
title = {{Guide to Computing for Expressive Music Performance}},
year = {2013}
}
@article{Lee2018,
abstract = {Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large-with many complex patterns-and noisy which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate "attention" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction, etc.). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.},
archivePrefix = {arXiv},
arxivId = {1807.07984v1},
author = {Lee, John Boaz and Rossi, Ryan A and Kim, Sungchul and Ahmed, Nesreen K and Koh, Eunyee},
eprint = {1807.07984v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2018 - Attention Models in Graphs A Survey(2).pdf:pdf},
journal = {ACM Trans. Knowl. Discov. Data},
keywords = {Additional Key Words and Phrases: Attention mechan,CCS Concepts: • Computing methodologies → Artifici,Combinatorics,Graph theory,Logical and relational learning,Machine learning,Streaming, sublinear and near linear time algorith,• Information systems → Data mining,• Mathematics of computing → Graph algorithms,• Theory of computation → Graph algorithms analysi},
pages = {18},
title = {{Attention Models in Graphs: A Survey}},
url = {https://doi.org/},
volume = {0},
year = {2018}
}
@article{Battcock2022IndividualizedClavier,
abstract = {Audiences, juries, and critics continually evaluate performers based on their interpretations of familiar classics. Yet formally assessing the perceptual consequences of interpretive decisions is challenging particularly with respect to how they shape emotional messages. Here, we explore the issue through comparison of emotion ratings (using scales of arousal and valence) for excerpts of all 48 pieces from Bach's Well-Tempered Clavier. In this series of studies, participants evaluated one of seven interpretations by highly regarded pianists. This work offers the novel ability to simultaneously explore (1) how different interpretations by expert pianists shape emotional messages, (2) the degree to which structural and interpretative elements shape the clarity of emotional messages, and (3) how interpretative differences affect the strength of specific features or cues to convey musical emotion. ARTICLE HISTORY},
annote = {I think this work confuses performances with compositions...},
author = {Battcock, Aimee and Schutz, Michael},
doi = {10.1080/09298215.2021.1979050},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Battcock, Schutz - 2022 - Well Tempered Clavier.pdf:pdf},
journal = {Journal of New Music Research},
keywords = {Emotion,interpretation,musical expression,perception,performance},
number = {5},
pages = {447--468},
title = {{Individualized interpretation: Exploring structural and interpretive effects on evaluations of emotional content in Bach's Well Tempered Clavier}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {50},
year = {2022}
}
@inproceedings{Hawthorne2018EnablingDataset,
abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (∼0.1 ms to ∼100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (≈3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
archivePrefix = {arXiv},
arxivId = {1810.12247},
author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {1810.12247},
file = {:Users/huanzhang/Downloads/1810.12247.pdf:pdf},
pages = {1--12},
title = {{Enabling Factorized Piano Music Modeling and Generation with the {Maestro} Dataset}},
year = {2019}
}
@inproceedings{Pati2021IsGeneration,
abstract = {Improving controllability or the ability to manipulate one or more attributes of the generated data has become a topic of interest in the context of deep generative models of music. Recent attempts in this direction have relied on learning disentangled representations from data such that the underlying factors of variation are well separated. In this paper, we focus on the relationship between disentanglement and controllability by conducting a systematic study using different supervised disentanglement learning algorithms based on the Variational Auto-Encoder (VAE) architecture. Our experiments show that a high degree of disentanglement can be achieved by using different forms of supervision to train a strong discriminative encoder. However, in the absence of a strong generative decoder, disentanglement does not necessarily imply controllability. The structure of the latent space with respect to the VAE-decoder plays an important role in boosting the ability of a generative model to manipulate different attributes. To this end, we also propose methods and metrics to help evaluate the quality of a latent space with respect to the afforded degree of controllability.},
archivePrefix = {arXiv},
arxivId = {2108.01450},
author = {Pati, Ashis and Lerch, Alexander},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2108.01450},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pati, Lerch - Unknown - IS DISENTANGLEMENT ENOUGH ON LATENT REPRESENTATIONS FOR CONTROLLABLE MUSIC GENERATION.pdf:pdf},
title = {{Is Disentanglement enough? {O}n Latent Representations for Controllable Music Generation}},
url = {http://arxiv.org/abs/2108.01450},
year = {2021}
}
@article{Miller1950,
annote = {From Duplicate 2 (The Trill Threshold - Miller, George A; Heise, George A)

Week 5},
author = {Miller, George A and Heise, George A},
doi = {10.1121/1.1906663},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Miller, Heise - 1950 - The Trill Threshold.pdf:pdf},
journal = {Citation: The Journal of the Acoustical Society of America},
pages = {637},
title = {{The Trill Threshold}},
url = {https://doi.org/10.1121/1.1906663},
volume = {22},
year = {1950}
}
@article{Alonso-Jimenez2024,
abstract = {We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve most of the performance achieved with the autoencoder embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier.},
archivePrefix = {arXiv},
arxivId = {2402.09318},
author = {Alonso-Jim{\'{e}}nez, Pablo and Pepino, Leonardo and Batlle-Roca, Roser and Zinemanas, Pablo and Bogdanov, Dmitry and Serra, Xavier and Rocamora, Mart{\'{i}}n},
eprint = {2402.09318},
file = {:Users/huanzhang/Downloads/2402.09318.pdf:pdf},
pages = {1--5},
title = {{Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio}},
url = {http://arxiv.org/abs/2402.09318},
year = {2024}
}
@inproceedings{Gardner2024LLarkMusic,
abstract = {Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .},
archivePrefix = {arXiv},
arxivId = {2310.07160},
author = {Gardner, Josh and Durand, Simon and Stoller, Daniel and Bittner, Rachel M.},
booktitle = {In Proceedings of International Conference on Machine Learning (ICML)},
eprint = {2310.07160},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gardner et al. - 2023 - LLark A Multimodal Foundation Model for Music.pdf:pdf},
month = {oct},
title = {{LLark: A Multimodal Foundation Model for Music}},
url = {https://arxiv.org/abs/2310.07160v3 http://arxiv.org/abs/2310.07160},
year = {2024}
}
@article{Draft--,
author = {Draft--, Manuscript},
file = {:Users/huanzhang/Downloads/JASM-D-23-00084.pdf:pdf},
keywords = {music rearrangement,symbolic music processing,token representation},
title = {{Piano score rearrangement into multiple difficulty levels via notation-to-notation approach}}
}
@article{Tillmann2004,
abstract = {The present study investigated the influence of acoustical characteristics on the implicit learning of statistical regularities (transition probabilities) in sequences of musical timbres. The sequences were constructed in such a way that the acoustical dissimilarities between timbres potentially created segmentations that either supported (S1) or contradicted (S2) the statistical regularities or were neutral (S3). In the learning group, participants first listened to the continuous timbre sequence and then had to distinguish statistical units from new units. In comparison to a control group without the exposition phase, no interaction between sequence type and amount of learning was observed: Performance increased by the same amount for the three sequences. In addition, performance reflected an overall preference for acoustically similar timbre units. The present outcome extends previous data from the domain of implicit learning to complex nonverbal auditory material. It further suggests that listeners become sensitive to statistical regularities despite acoustical characteristics in the material that potentially affect grouping.},
annote = {From Duplicate 1 (Implicit learning of musical timbre sequences: Statistical regularities confronted with acoustical (dis)similarities - Tillmann, Barbara; McAdams, Stephen)

Week 5},
author = {Tillmann, Barbara and McAdams, Stephen},
doi = {10.1037/0278-7393.30.5.1131},
file = {:Users/huanzhang/Downloads/2004_TiMcA_IL.pdf:pdf},
issn = {02787393},
journal = {Journal of Experimental Psychology: Learning Memory and Cognition},
number = {5},
pages = {1131--1142},
pmid = {15355141},
title = {{Implicit learning of musical timbre sequences: Statistical regularities confronted with acoustical (dis)similarities}},
volume = {30},
year = {2004}
}
@article{Pearce2006,
abstract = {THE IMPLICATION-REALIZATION (IR) theory (Narmour, 1990) posits two cognitive systems involved in the generation of melodic expectations: The first consists of a limited number of symbolic rules that are held to be innate and universal; the second reflects the top-down influences of acquired stylistic knowledge. Aspects of both systems have been implemented as quantitative models in research which has yielded empirical support for both components of the theory (Cuddy & Lunny, 1995; Krumhansl, 1995a, 1995b; Schellenberg, 1996, 1997). However, there is also evidence that the implemented bottom-up rules constitute too inflexible a model to account for the influence of the musical experience of the listener and the melodic context in which expectations are elicited. A theory is presented, according to which both bottom-up and top-down descriptions of observed patterns of melodic expectation may be accounted for in terms of the induction of statistical regularities in existing musical repertoires. A computational model that embodies this theory is developed and used to reanalyze existing experimental data on melodic expectancy. The results of three experiments with increasingly com- plex melodic stimuli demonstrate that this model is capa- ble of accounting for listeners' expectations as well as or better than the two-factor model of Schellenberg (1997).},
author = {Pearce, Marcus Thomas},
file = {:Users/huanzhang/Downloads/mp_2006_23_5_377.pdf:pdf},
pages = {377--405},
title = {{Expectation in Melody: The Influence of Context and Learning}},
year = {2006}
}
@article{Zhu,
abstract = {We propose a sequential variational autoencoder to learn disentangled representations of sequential data (e.g., videos and audios) under self-supervision. Specifically, we exploit the benefits of some readily accessible supervisory signals from input data itself or some off-the-shelf functional models and accordingly design auxiliary tasks for our model to utilize these signals. With the supervision of the signals, our model can easily disentangle the representation of an input sequence into static factors and dynamic factors (i.e., time-invariant and time-varying parts). Comprehensive experiments across videos and audios verify the effectiveness of our model on representation disentangle-ment and generation of sequential data, and demonstrate that, our model with self-supervision performs comparable to, if not better than, the fully-supervised model with ground truth labels, and outperforms state-of-the-art unsupervised models by a large margin.},
archivePrefix = {arXiv},
arxivId = {2005.11437v1},
author = {Zhu, Yizhe and {Renqiang Min}, Martin and Kadav, Asim and {Peter Graf}, Hans},
eprint = {2005.11437v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - Unknown - S3VAE Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation.pdf:pdf},
title = {{S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation}}
}
@article{Steinbeis2006,
abstract = {The purpose of the present study was to investigate the effect of harmonic expectancy violations on emotions. Subjective response measures for tension and emotionality, as well as electrodermal activity (EDA) and heart rate (HR), were recorded from 24 subjects (12 musicians and 12 nonmusicians) to observe the effect of expectancy violations on subjective and physiological measures of emotions. In addition, an electroencephalogram was recorded to observe the neural correlates for detecting these violations. Stimuli consisted of three matched versions of six Bach chorales, which differed only in terms of one chord (harmonically either expected, unexpected or very unexpected). Musicians' and nonmusicians' responses were also compared. Tension, overall subjective emotionality, and EDA increased with an increase in harmonic unexpectedness. Analysis of the event-related potentials revealed an early negativity (EN) for both the unexpected and the very unexpected harmonies, taken to reflect the detection of the unexpected event. The EN in response to very unexpected chords was significantly larger in amplitude than the EN in response to merely unexpected harmonic events. The ENs did not differ in amplitude between the two groups but peaked earlier for musicians than for nonmusicians. Both groups also showed a P3 component in response to the very unexpected harmonies, which was considerably larger for musicians and may reflect the processing of stylistic violations of Western classical music. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Steinbeis, Nikolaus and Koelsch, Stefan and Sloboda, John A.},
doi = {10.1162/jocn.2006.18.8.1380},
file = {:Users/huanzhang/Downloads/download (2).pdf:pdf},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
number = {8},
pages = {1380--1393},
pmid = {16859422},
title = {{The role of harmonic expectancy violations in musical emotions: Evidence from subjective, physiological, and neural responses}},
volume = {18},
year = {2006}
}
@article{Cook2013,
abstract = {Is the ability to entrain motor activity to a rhythmic auditory stimulus, that is "keep a beat," dependent on neural adaptations supporting vocal mimicry? That is the premise of the vocal learning and synchronization hypothesis, recently advanced to explain the basis of this behavior (A. Patel, 2006, Musical Rhythm, Linguistic Rhythm, and Human Evolution, Music Perception, 24, 99-104). Prior to the current study, only vocal mimics, including humans, cockatoos, and budgerigars, have been shown to be capable of motoric entrainment. Here we demonstrate that a less vocally flexible animal, a California sea lion (Zalophus californianus), can learn to entrain head bobbing to an auditory rhythm meeting three criteria: a behavioral response that does not reproduce the stimulus; performance transfer to a range of novel tempos; and entrainment to complex, musical stimuli. These findings show that the capacity for entrainment of movement to rhythmic sounds does not depend on a capacity for vocal mimicry, and may be more widespread in the animal kingdom than previously hypothesized. {\textcopyright} 2013 American Psychological Association.},
author = {Cook, Peter and Rouse, Andrew and Wilson, Margaret and Reichmuth, Colleen},
doi = {10.1037/a0032345},
file = {:Users/huanzhang/Downloads/ContentServer.pdf:pdf},
issn = {07357036},
journal = {Journal of Comparative Psychology},
keywords = {Music cognition,Rhythmic entrainment,Sea lions,Sensorimotor synchronization},
number = {4},
pages = {412--427},
pmid = {23544769},
title = {{A California sea lion (Zalophus californianus) can keep the beat: Motor entrainment to rhythmic auditory stimuli in a non vocal mimic}},
volume = {127},
year = {2013}
}
@inproceedings{Shi2021ComputationalMazurkas,
abstract = {Performers' distortion of notated rhythms in a musical score is a significant factor in the production of convincingly expressive music interpretations. Sometimes exaggerated, and sometimes subtle, these distortions are driven by a variety of factors, including schematic features (both structural such as phrase boundaries and surface events such as recurrent rhythmic patterns), as well as relatively rare veridical events that characterize the individuality and uniqueness of a particular piece. Performers tend to adopt similar pervasive approaches to interpreting schemas, resulting in common performance practices, while often formulating less common approaches to the interpretation of veridical events. Furthermore, some performers choose anomalous interpretations of schemas. We present a machine learning model of expressive performance of Chopin Mazurkas and a critical analysis of the output based upon statistical analyses of the musical scores and of recorded performances. We compare the timings of recorded human performances of selected Mazurkas by Fr{\'{e}}d{\'{e}}ric Chopin with performances of the same works generated by a neural network trained with recorded human performances of the entire corpus. This paper demonstrates that while machine learning succeeds, to some degree, in expressive interpretation of schemata, convincingly capturing performance characteristics remains very much a work in progress.},
author = {Shi, Zhengshan},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Downloads/Shi-0826.pdf:pdf},
title = {{Computational Analysis and Modeling of Expressive Timing in Chopin Mazurkas}},
year = {2021}
}
@article{Vasilache2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7580v3},
eprint = {arXiv:1412.7580v3},
file = {:Users/huanzhang/Downloads/5327_piano_performance_evaluation_d.pdf:pdf},
number = {Section 6},
pages = {1--17},
title = {{PIANO PERFORMANCE EVALUATION DATASET WITH MULTI-LEVEL PERCEPTUAL FEATURES}},
year = {2015}
}
@inproceedings{He2016DeepRecognition,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@article{Vyas2023AudioboxPrompts,
abstract = {Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like "a person speaking" and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/},
archivePrefix = {arXiv},
arxivId = {2312.15821},
author = {AudioBox team},
eprint = {2312.15821},
file = {:Users/huanzhang/Downloads/416201240_1109648396840016_4084243924939353841_n.pdf:pdf},
journal = {Arxiv preprint arXiv:2312.15821},
title = {{Audiobox: Unified Audio Generation with Natural Language Prompts}},
url = {http://arxiv.org/abs/2312.15821},
year = {2023}
}
@techreport{GehringConvolutionalLearning,
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. 1 Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
archivePrefix = {arXiv},
arxivId = {1705.03122v3},
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
eprint = {1705.03122v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gehring et al. - Unknown - Convolutional Sequence to Sequence Learning.pdf:pdf},
title = {{Convolutional Sequence to Sequence Learning}}
}
@article{Panteli2017AMusic,
author = {Panteli, Maria and Benetos, Emmanouil and Dixon, Simon},
file = {:Users/huanzhang/Downloads/A computational study on outliers in world music.pdf:pdf},
isbn = {1111111111},
pages = {1--28},
title = {{A computational study on outliers in world music}},
year = {2017}
}
@article{Palmer1997MusicPerformance,
abstract = {Music performance provides a rich domain for study of both cognitive and motor skills. Empirical research in music performance is summarized, with particular emphasis on factors that contribute to the formation of conceptual interpretations , retrieval from memory of musical structures, and transformation into appropriate motor actions. For example, structural and emotional factors that contribute to performers' conceptual interpretations are considered. Research on the planning of musical sequences for production is reviewed, including hierarchical and associative retrieval influences, style-specific syntactic influences , and constraints on the range of planning. The fine motor control evidenced in music performance is discussed in terms of internal timekeeper models, motor programs, and kinematic models. The perceptual consequences of music performance are highlighted, including the successful communication of interpretations , resolution of structural ambiguities, and concordance with listeners' expectations. Parallels with other domains support the conclusion that music performance is not unique in its underlying cognitive mechanisms. CONTENTS},
author = {Palmer, Caroline},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Palmer - 1997 - MUSIC PERFORMANCE.pdf:pdf},
journal = {Annual Review of Psychology},
keywords = {motor skills,music perception,musical memory,sequence production,skilled performance},
title = {{Music Performance}},
url = {www.annualreviews.org},
volume = {48},
year = {1997}
}
@inproceedings{Koops2013,
abstract = {Melody harmonisation is a centuries-old problem of long tradition, and a core aspect of composition in Western tonal music. In this work we describe FHARM, an automated system for melody harmonisation based on a functional model of harmony. Our system first generates multiple harmonically well-formed chord sequences for a given melody. From the generated sequences, the best one is chosen, by picking the one with the smallest deviation from the harmony model. Unlike all existing systems, FHARM guarantees that the generated chord sequences follow the basic rules of tonal harmony. We carry out two experiments to evaluate the quality of our harmonisations. In one experiment, a panel of harmony experts is asked to give its professional opinion and rate the generated chord sequences for selected melodies. In another experiment, we generate a chord sequence for a selected melody, and compare the result to the original harmonisation given by a harmony scholar. Our experiments confirm that FHARM generates realistic chords for each melody note. However, we also conclude that harmonising a melody with individually well-formed chord sequences from a harmony model does not guarantee a well-sounding coherence between the chords and the melody. We reflect on the experience gained with our experiment, and propose future improvements to refine the quality of the harmonisation.},
annote = {From Duplicate 2 (A functional approach to automatic melody harmonisation - Koops, Hendrik Vincent; Magalh{\~{a}}es, Jos{\'{e}} Pedro; De Haas, W. Bas)

Context Gree Grammar},
author = {Koops, Hendrik Vincent and Magalh{\~{a}}es, Jos{\'{e}} Pedro and {De Haas}, W. Bas},
booktitle = {Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP},
doi = {10.1145/2505341.2505343},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Koops, Magalh{\~{a}}es, De Haas - 2013 - A functional approach to automatic melody harmonisation.pdf:pdf},
isbn = {9781450323864},
keywords = {Automatic harmonisation,FHarm,HarmTrace,Harmony,Haskell,Haskore},
pages = {47--58},
title = {{A functional approach to automatic melody harmonisation}},
year = {2013}
}
@article{Blood1999,
abstract = {Neural correlates of the often-powerful emotional responses to music are poorly understood. Here we used positron emission tomography to examine cerebral blood flow (CBF) changes related to affective responses to music. Ten volunteers were scanned while listening to six versions of a novel musical passage varying systematically in degree of dissonance. Reciprocal CBF covariations were observed in several distinct paralimbic and neocortical regions as a function of dissonance and of perceived pleasantness/unpleasantness. The findings suggest that music may recruit neural mechanisms similar to those previously associated with pleasant/unpleasant emotional states, but different from those underlying other components of music perception, and other emotions such as fear.},
author = {Blood, Anne J and Zatorre, Robert J and Bermudez, Patrick and Evans, Alan C},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Blood et al. - 1999 - Emotional responses to pleasant and unpleasant music correlate with activity in paralimbic brain regions.pdf:pdf},
title = {{Emotional responses to pleasant and unpleasant music correlate with activity in paralimbic brain regions}},
url = {http://neurosci.nature.com},
year = {1999}
}
@article{Lerch2020AnAnalysis,
abstract = {A musical performance renders an acoustic realization of a musical score or other representation of a composition. Different performances of the same composition may vary in terms of performance parameters such as timing or dynamics, and these variations may have a major impact on how a listener perceives the music. The analysis of music performance has traditionally been a peripheral topic for the MIR research community, where often a single audio recording is used as representative of a musical work. This paper surveys the field of Music Performance Analysis (MPA) from several perspectives including the measurement of performance parameters, the relation of those parameters to the actions and intentions of a performer or perceptual effects on a listener, and finally the assessment of musical performance. This paper also discusses MPA as it relates to MIR, pointing out opportunities for collaboration and future research in both areas.},
archivePrefix = {arXiv},
arxivId = {2104.09018},
author = {Lerch, Alexander and Arthur, Claire and Pati, Ashis and Gururani, Siddharth},
doi = {10.5334/tismir.53},
eprint = {2104.09018},
file = {:Users/huanzhang/Downloads/53-1660-1-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {Music Performance Analysis,Survey},
number = {1},
pages = {221--245},
publisher = {DOI},
title = {{An Interdisciplinary Review of Music Performance Analysis}},
url = {https://doi.org/10.5334/tismir.53},
volume = {3},
year = {2020}
}
@article{Patel2009,
abstract = {The tendency to move in rhythmic synchrony with a musical beat (e.g., via head bobbing, foot tapping, or dance) is a human universal [1] yet is not commonly observed in other species [2]. Does this ability reflect a brain specialization for music cognition, or does it build on neural circuitry that ordinarily serves other functions? According to the "vocal learning and rhythmic synchronization" hypothesis [3], entrainment to a musical beat relies on the neural circuitry for complex vocal learning, an ability that requires a tight link between auditory and motor circuits in the brain [4, 5]. This hypothesis predicts that only vocal learning species (such as humans and some birds, cetaceans, and pinnipeds, but not nonhuman primates) are capable of synchronizing movements to a musical beat. Here we report experimental evidence for synchronization to a beat in a sulphur-crested cockatoo (Cacatua galerita eleonora). By manipulating the tempo of a musical excerpt across a wide range, we show that the animal spontaneously adjusts the tempo of its rhythmic movements to stay synchronized with the beat. These findings indicate that synchronization to a musical beat is not uniquely human and suggest that animal models can provide insights into the neurobiology and evolution of human music [6]. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Patel, Aniruddh D. and Iversen, John R. and Bregman, Micah R. and Schulz, Irena},
doi = {10.1016/J.CUB.2009.03.038},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Patel et al. - 2009 - Experimental Evidence for Synchronization to a Musical Beat in a Nonhuman Animal.pdf:pdf},
issn = {0960-9822},
journal = {Current Biology},
keywords = {SYSNEURO},
month = {may},
number = {10},
pages = {827--830},
pmid = {19409790},
publisher = {Cell Press},
title = {{Experimental Evidence for Synchronization to a Musical Beat in a Nonhuman Animal}},
volume = {19},
year = {2009}
}
@article{Weigl,
abstract = {EXTENDED ABSTRACT We present a visualisation interface for real-time performance-to-score alignment based on the Music Encoding and Linked Data (MELD) framework for semantic digital notation [6], employing the Matcher for Alignment of Performance and Score (MAPS), an HMM-based polyphonic score-following system for symbolic (MIDI) piano performances. Performance metadata and alignment information is presented to the pianist during the performance in real-time through a web application. This application, implemented using the MELD-clients-core 1 Javascript libraries, presents digital score notation (rendered from MEI encodings using the Verovio 2 engraving tool) augmented with real-time note highlighting corresponding to current performance position, coloured according to attack velocity (Figure 1). Prior score-following systems have typically associated the performance timeline with pixel positions on score images (e.g., [2]) or semantically sparse MIDI representations (e.g., [3]). More recent approaches have offered support for digital music scores encoded according to the Music Encoding Initiative's machine-readable MEI XML schema 3 (e.g., ScoreTube [4]). Going beyond these approaches, MAPS offers native MEI support for synchronisation of digital score en-codings with piano performances in real-time, enabling the capture of alignment information at the note level that is musically meaningful to both human performers and software agents. The captured performance MIDI stream-characterising note events according to their pitch, duration, and attack velocity-is aligned with the MEI-encoded score using Linked Data (RDF) structures according to the MELD semantic framework. Thus exposed, the alignment information is made available for immediate reuse, e.g., to review one's own performance, or to audit and compare different interpreters' renditions of the same score. By anchoring within the comprehensive musical model of the MEI schema, but lifting the alignment structures themselves into the higher layers of abstraction offered by Linked Data, the score-synchronised performances become available for annotation and interlinking within a wider Web of data. This provides a foundation for the enrichment of public-domain music materials available on the Web, both by software processes , and by musicians, music scholars, and music enthusiasts, as envisioned by the TROMPA project [5] 4. MAPS is implemented within the context of development of the ACCompanion, an artificial accompaniment system [1] for four-handed piano performance (one human performer with machine accompaniment). Aside from providing feedback to the performer and to interested listeners, the visualisation of performance-to-score alignment is thus also useful as a development tool, enabling alignment issues to be identified and addressed in order to improve the matching performance of the accompaniment system.},
author = {Weigl, David M and Cancino-Chac{\'{o}}n, Carlos and Bonev, Martin and Goebl, Werner},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Weigl et al. - Unknown - ISMIR Late-BreakingDemo Unrefereed LINKING AND VISUALISING PERFORMANCE DATA AND SEMANTIC MUSIC ENCODINGS IN REA.pdf:pdf},
title = {{ISMIR Late-Breaking/Demo [Unrefereed] LINKING AND VISUALISING PERFORMANCE DATA AND SEMANTIC MUSIC ENCODINGS IN REAL-TIME}},
url = {https://verovio.org}
}
@techreport{WeiKoh2017UnderstandingFunctions,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions a classic technique from robust statistics to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neu-ral networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730v3},
author = {{Wei Koh}, Pang and Liang, Percy},
eprint = {1703.04730v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wei Koh, Liang - 2017 - Understanding Black-box Predictions via Influence Functions.pdf:pdf},
isbn = {1703.04730v3},
title = {{Understanding Black-box Predictions via Influence Functions}},
year = {2017}
}
@article{Bregman1971,
abstract = {A recent finding of the inability of listeners to judge the order of three or four nonspeech sounds presented in a repetitive cycle is explained by the concept of stream segregation. Two experiments showed that at high presention rates of a short cycle of six tones (three high and three low), 5s invariably segregated the tone sequences into streams based on frequency and could perceive only those patterns relating elements of the same subjective stream.},
annote = {From Duplicate 2 (Primary Auditory Stream Segregation and Perception of Order in Rapid Sequences of Tones - Bregman, Albert S; Campbell, Jeffrey)

Week 5},
author = {Bregman, Albert S and Campbell, Jeffrey},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bregman, Campbell - 1971 - PRIMARY AUDITORY STREAM SEGREGATION AND PERCEPTION OF ORDER IN RAPID SEQUENCES OF TONES l f.pdf:pdf},
journal = {Journal of Experimental Psychology},
number = {2},
pages = {244--249},
title = {{Primary Auditory Stream Segregation and Perception of Order in Rapid Sequences of Tones}},
volume = {89},
year = {1971}
}
@article{Haseeb2024,
author = {Haseeb, Muhammad Taimoor and Hammoudeh, Ahmad and Xia, Gus},
doi = {10.1109/ICASSP48485.2024.10447950},
file = {:Users/huanzhang/Downloads/GPT-4_Driven_Cinematic_Music_Generation_Through_Text_Processing.pdf:pdf},
isbn = {9798350344851},
journal = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {6995--6999},
publisher = {IEEE},
title = {{GPT-4 DRIVEN CINEMATIC MUSIC GENERATION THROUGH TEXT PROCESSING}},
year = {2024}
}
@article{Guan2024,
abstract = {Recently, the application of diffusion models has facilitated the significant development of speech and audio generation. Nevertheless, the quality of samples generated by diffusion models still needs improvement. And the effectiveness of the method is accompanied by the extensive number of sampling steps, leading to an extended synthesis time necessary for generating high-quality audio. Previous Text-to-Audio (TTA) methods mostly used diffusion models in the latent space for audio generation. In this paper, we explore the integration of the Flow Matching (FM) model into the audio latent space for audio generation. The FM is an alternative simulation-free method that trains continuous normalization flows (CNF) based on regressing vector fields. We demonstrate that our model significantly enhances the quality of generated audio samples, achieving better performance than prior models. Moreover, it reduces the number of inference steps to ten steps almost without sacrificing performance.},
archivePrefix = {arXiv},
arxivId = {2406.08203},
author = {Guan, Wenhao and Wang, Kaidi and Zhou, Wangjin and Wang, Yang and Deng, Feng and Wang, Hui and Li, Lin and Hong, Qingyang and Qin, Yong},
eprint = {2406.08203},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Guan et al. - 2024 - LAFMA A Latent Flow Matching Model for Text-to-Audio Generation.pdf:pdf},
month = {jun},
title = {{LAFMA: A Latent Flow Matching Model for Text-to-Audio Generation}},
url = {http://arxiv.org/abs/2406.08203},
year = {2024}
}
@article{Kim2018a,
abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
archivePrefix = {arXiv},
arxivId = {1802.06182},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
doi = {10.1109/ICASSP.2018.8461329},
eprint = {1802.06182},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2018 - Crepe A Convolutional Representation for Pitch Estimation.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural network,Pitch estimation},
pages = {161--165},
title = {{Crepe: A Convolutional Representation for Pitch Estimation}},
volume = {2018-April},
year = {2018}
}
@article{Jaegle,
abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however , cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
archivePrefix = {arXiv},
arxivId = {2107.14795v3},
author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'{e}}naff, Olivier and Botvinick, Matthew M and Zisserman, Andrew and Vinyals, Oriol and {Carreira Deepmind}, Jo{\~{a}}o},
eprint = {2107.14795v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jaegle et al. - Unknown - PERCEIVER IO A GENERAL ARCHITECTURE FOR STRUCTURED INPUTS & OUTPUTS(2).pdf:pdf},
title = {{PERCEIVER IO: A GENERAL ARCHITECTURE FOR STRUCTURED INPUTS & OUTPUTS}}
}
@article{Chen2014AnNotation,
abstract = {This paper presents an optical music recognition (OMR) system to process the handwritten musical scores of Kunqu Opera written in Gong-Che Notation (GCN). First, it introduces the background of Kunqu Opera and GCN. Kunqu Opera is one of the oldest forms of musical activity, spanning the sixteenth to eighteenth centuries, and GCN has been the most popular notation for recording musical works in China since the seventh century. Many Kunqu Operas that use GCN are available as original manuscripts or photocopies, and transforming these versions into a machine-readable format is a pressing need. The OMR system comprises six stages: image pre-processing, segmentation, feature extraction, symbol recognition, musical semantics, and musical instrument digital interface (MIDI) representation. This paper focuses on the symbol recognition stage and obtains the musical information with Bayesian, genetic algorithm, and K-nearest neighbor classifiers. The experimental results indicate that symbol recognition for Kunqu Opera's handwritten musical scores is effective. This work will help to preserve and popularize Chinese cultural heritage and to store Kunqu Opera scores in a machine-readable format, thereby ensuring the possibility of spreading and performing original Kunqu Opera musical scores. {\textcopyright} 2014 Chen and Sheu; licensee Springer.},
author = {Chen, Gen Fang and Sheu, Jia Shing},
doi = {10.1186/1687-4722-2014-7},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Sheu - 2014 - An optical music recognition system for traditional Chinese Kunqu Opera scores written in Gong-Che Notation.pdf:pdf},
issn = {16874722},
journal = {Eurasip Journal on Audio, Speech, and Music Processing},
keywords = {Classifiers,Gong-Che Notation,Kunqu Opera,MIDI,Optical music recognition},
pages = {1--12},
title = {{An optical music recognition system for traditional Chinese Kunqu Opera scores written in Gong-Che Notation}},
volume = {2014},
year = {2014}
}
@inproceedings{Manilow2022SourceModels,
abstract = {We showcase a method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's JUKEBOX as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.},
archivePrefix = {arXiv},
arxivId = {arXiv:2110.13071v1},
author = {Manilow, Ethan and O'Reilly, Patrick and Seetharaman, Prem and Pardo, Bryan},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP43922.2022.9747909},
eprint = {arXiv:2110.13071v1},
file = {:Users/huanzhang/Downloads/2110.13071.pdf:pdf},
isbn = {9781665405409},
issn = {15206149},
keywords = {automatic music tagging,generative music models,gradient ascent,music source separation},
pages = {126--130},
title = {{Source Separation By Steering Pretrained Music Models}},
volume = {2022-May},
year = {2022}
}
@article{Hadjeres2020Anticipation-RNNGeneration,
abstract = {Recurrent neural networks (RNNs) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This article introduces a novel architecture called anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined unary constraints. We demonstrate its efficiency on the task of generating melodies satisfying unary constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.},
author = {Hadjeres, Ga{\"{e}}tan and Nielsen, Frank},
doi = {10.1007/s00521-018-3868-4},
file = {:Users/huanzhang/Downloads/s00521-018-3868-4.pdf:pdf},
isbn = {0123456789},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Automatic symbolic music generation,Interactive models,Recurrent neural networks,Unary constraints},
number = {4},
pages = {995--1005},
title = {{Anticipation-RNN: enforcing unary constraints in sequence generation, with application to interactive music generation}},
volume = {32},
year = {2020}
}
@article{Rusconi2006,
abstract = {Through the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The finding that our cognitive system maps pitch height onto an internal representation of space, which in turn affects motor performance even when this perceptual attribute is irrelevant to the task, extends previous studies on auditory perception and suggests an interesting analogy between music perception and mathematical cognition. Both the basic elements of mathematical cognition (i.e. numbers) and the basic elements of musical cognition (i.e. pitches), appear to be mapped onto a mental spatial representation in a way that affects motor performance. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
annote = {From Duplicate 1 (Spatial representation of pitch height: The SMARC effect - Rusconi, Elena; Kwan, Bonnie; Giordano, Bruno L.; Umilt{\`{a}}, Carlo; Butterworth, Brian)

Week 2

SRC: spatial response code},
author = {Rusconi, Elena and Kwan, Bonnie and Giordano, Bruno L. and Umilt{\`{a}}, Carlo and Butterworth, Brian},
doi = {10.1016/j.cognition.2005.01.004},
file = {:Users/huanzhang/Downloads/1-s2.0-S0010027705000260-main.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Pitch height,SMARC effect,SNARC effect,Spatial representation,Stimulus-Response Compatability},
number = {2},
pages = {113--129},
pmid = {15925355},
title = {{Spatial representation of pitch height: The SMARC effect}},
volume = {99},
year = {2006}
}
@techreport{TsushimaINTERACTIVEMODEL,
abstract = {We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols , (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodiza-tion, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system.},
author = {Tsushima, Hiroaki and Nakamura, Eita and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tsushima et al. - Unknown - INTERACTIVE ARRANGEMENT OF CHORDS AND MELODIES BASED ON A TREE-STRUCTURED GENERATIVE MODEL.pdf:pdf},
title = {{INTERACTIVE ARRANGEMENT OF CHORDS AND MELODIES BASED ON A TREE-STRUCTURED GENERATIVE MODEL}}
}
@article{Wu2023MusicGeneration,
abstract = {Text-to-music generation models are now capable of generating high-quality music audio in broad styles. However, text control is primarily suitable for the manipulation of global musical attributes like genre, mood, and tempo, and is less suitable for precise control over time-varying attributes such as the positions of beats in time or the changing dynamics of the music. We propose Music ControlNet, a diffusion-based music generation model that offers multiple precise, time-varying controls over generated audio. To imbue text-to-music models with time-varying control, we propose an approach analogous to pixel-wise control of the image-domain ControlNet method. Specifically, we extract controls from training audio yielding paired data, and fine-tune a diffusion-based conditional generative model over audio spectrograms given melody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet method already allows generation with any subset of controls, we devise a new masking strategy to allow creators to input controls that are only partially specified in time. We evaluate both on controls extracted from audio and controls we expect creators to provide, demonstrating that we can generate realistic music that corresponds to control inputs in both settings. While few comparable music generation models exist, we benchmark against MusicGen, a recent model that accepts text and melody input, and show that our model generates music that is 49% more faithful to input melodies despite having 35x fewer parameters, training on 11x less data, and enabling two additional forms of time-varying control. Sound examples can be found at https://musiccontrolnet.github.io/web/.},
archivePrefix = {arXiv},
arxivId = {2311.07069},
author = {Wu, Shih Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J.},
doi = {10.1109/TASLP.2024.3399026},
eprint = {2311.07069},
file = {:Users/huanzhang/Downloads/2311.07069.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Music generation,controllable generative modeling,diffusion models},
title = {{Music ControlNet: Multiple Time-Varying Controls for Music Generation}},
url = {http://arxiv.org/abs/2311.07069},
year = {2024}
}
@article{Tan2021,
abstract = {This paper presents a novel design of neural network system for fine-grained style modeling, transfer and prediction in expressive text-to-speech (TTS) synthesis. Fine-grained modeling is realized by extracting style embeddings from the mel-spectrograms of phone-level speech segments. Collaborative learning and adversarial learning strategies are applied in order to achieve effective disentanglement of content and style factors in speech and alleviate the “content leakage” problem in style modeling. The proposed system can be used for varying-content speech style transfer in the single-speaker scenario. The results of objective and subjective evaluation show that our system performs better than other fine-grained speech style transfer models, especially in the aspect of content preservation. By incorporating a style predictor, the proposed system can also be used for text-to-speech synthesis. Audio samples are provided for system demonstration.},
archivePrefix = {arXiv},
arxivId = {2011.03943},
author = {Tan, Daxin and Lee, Tan},
doi = {10.21437/Interspeech.2021-1129},
eprint = {2011.03943},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Lee - 2021 - Fine-grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disenta.pdf:pdf},
isbn = {9781713836902},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
keywords = {Prosody,Speech synthesis,Style transfer},
pages = {3421--3425},
title = {{Fine-grained style modeling, transfer and prediction in text-to-speech synthesis via phone-level content-style disentanglement}},
url = {http://dx.doi.org/10.21437/Interspeech.2021-1129},
volume = {5},
year = {2021}
}
@article{Berndt,
abstract = {MEI, the established representation format for digital music editions , barely finds consideration in other music-related communities. Reasons are the format's complexity and ambiguity that make processing expensive and laborious. On the other hand, digital music editions are an invaluable source of symbolic music data and further accompanying information far beyond the typical meta-data found in other formats. With meico, we provide a novel tool that makes access, processing and use of MEI encoded music more convenient and appealing for other application scenarios. Meico is a converter framework that translates MEI data to a series of formats relevant to many other applications. With ScoreTube we demonstrate this in an audio-to-score alignment scenario.},
author = {Berndt, Axel and Waloschek, Simon and Hadjakos, Aristotelis},
doi = {10.1145/3243274.3243282},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Berndt, Waloschek, Hadjakos - Unknown - Meico A Converter Framework for Bridging the Gap between Digital Music Editions and its Applicat.pdf:pdf},
isbn = {9781450366090},
keywords = {CCS CONCEPTS • Applied computing → Sound and music,For-mat and notation,• Information systems → Information re-trieval},
publisher = {ACM},
title = {{Meico: A Converter Framework for Bridging the Gap between Digital Music Editions and its Applications Music Encoding Initiative, Digital Music Editions, Format Converter}},
url = {https://doi.org/10.1145/3243274.3243282},
volume = {18}
}
@article{Goebl,
abstract = {Both timbre and dynamics of isolated piano tones are determined exclusively by the speed with which the hammer hits the strings. This physical view has been challenged by pianists who emphasize the importance of the way the keyboard is touched. This article presents empirical evidence from two perception experiments showing that touch-dependent sound components make sounds with identical hammer velocities but produced with different touch forms clearly distinguishable. The first experiment focused on finger-key sounds: musicians could identify pressed and struck touches. When the finger-key sounds were removed from the sounds, the effect vanished, suggesting that these sounds were the primary identification cue. The second experiment looked at key-keyframe sounds that occur when the key reaches key-bottom. Key-bottom impact was identified from key motion measured by a computer-controlled piano. Musicians were able to discriminate between piano tones that contain a key-bottom sound from those that do not. However, this effect might be attributable to sounds associated with the mechanical components of the piano action. In addition to the demonstrated acoustical effects of different touch forms, visual and tactile modalities may play important roles during piano performance that influence the production and perception of musical expression on the piano. V},
author = {Goebl, Werner and Bresin, Roberto and Fujinaga, Ichiro},
doi = {10.1121/1.4896461},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl, Bresin, Fujinaga - Unknown - Perception of touch quality in piano tones a).pdf:pdf},
isbn = {201421:04:03},
keywords = {4366Jh [DD] Pages: 2839-2850,4375St,number(s): 4375Mn},
title = {{Perception of touch quality in piano tones a)}},
url = {http://dx.doi.org/10.1121/1.4896461]}
}
@techreport{Finlay2020HowRegularization,
abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
archivePrefix = {arXiv},
arxivId = {2002.02798v3},
author = {Finlay, Chris and Jacobsen, J{\"{o}}rn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
eprint = {2002.02798v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Finlay et al. - 2020 - How to Train Your Neural ODE the World of Jacobian and Kinetic Regularization.pdf:pdf},
title = {{How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization}},
year = {2020}
}
@article{Zhang2022,
archivePrefix = {arXiv},
arxivId = {arXiv:2403.10131v1},
author = {Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E and Mar, C L},
eprint = {arXiv:2403.10131v1},
file = {:Users/huanzhang/Downloads/2403.10131.pdf:pdf},
title = {{RAFT : Adapting Language Model to Domain Specific RAG}},
year = {2022}
}
@article{Bigand2003SensoryPriming,
abstract = {This study investigated the strength of sensory and cognitive components involved in musical priming. In Experiment 1, the harmonic function of the target chord and the number of pitch classes shared by the prime sequence and the target chord were manipulated. In Experiment 2, the temporal course of sensory and cognitive priming was investigated. For both musician and nonmusician listeners, cognitive priming systematically overruled sensory priming even at fast and very fast tempi (300 ms and 150 ms per chord). Cognitive priming continued to challenge sensory priming processes at extremely fast tempo (75 ms per chord) but only for participants who began the experimental session with slower tempi. This outcome suggests that the cognitive component is a fast-acting component that competes with sensory priming.},
author = {Bigand, Emmanuel and Poulin, B{\'{e}}n{\'{e}}dicte and Tillmann, Barbara and Madurell, Fran{\c{c}}ois and D'Adamo, Daniel A.},
doi = {10.1037/0096-1523.29.1.159},
file = {:Users/huanzhang/Downloads/2003-04481-012.pdf:pdf},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {1},
pages = {159--171},
pmid = {12669755},
title = {{Sensory Versus Cognitive Components in Harmonic Priming}},
volume = {29},
year = {2003}
}
@article{Yang2022,
abstract = {In the stereo-to-multichannel upmixing problem for music, one of the main tasks is to set the directionality of the instrument sources in the multichannel rendering results. In this paper, we propose a modified variational autoencoder model that learns a latent space to describe the spatial images in multichannel music. We seek to disentangle the spatial images and music content, so the learned latent variables are invariant to the music. At test time, we use the latent variables to control the panning of sources. We propose two upmixing use cases: transferring the spatial images from one song to another and blind panning based on the generative model. We report objective and subjective evaluation results to empirically show that our model captures spatial images separately from music content and achieves transfer-based interactive panning.},
archivePrefix = {arXiv},
arxivId = {2203.12053},
author = {Yang, Haici and Wager, Sanna and Russell, Spencer and Luo, Mike and Kim, Minje and Kim, Wontak},
doi = {10.1109/ICASSP43922.2022.9746978},
eprint = {2203.12053},
file = {:Users/huanzhang/Downloads/Upmixing_Via_Style_Transfer_A_Variational_Autoencoder_for_Disentangling_Spatial_Images_And_Musical_Content.pdf:pdf},
isbn = {9781665405409},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Stereo-to-multichannel upmixing,information disentanglement,panning,variational autoencoders},
pages = {426--430},
publisher = {IEEE},
title = {{Upmixing Via Style Transfer: a Variational Autoencoder for Disentangling Spatial Images and Musical Content}},
volume = {2022-May},
year = {2022}
}
@article{Kim2022OverviewContext,
abstract = {Piano is one of the most popular instruments among music learners. Technologies to evaluate piano performances have been researched and developed in recent years rapidly, including data driven methods using machine learning. Despite the demand from people and speed of the development, there are still gaps between the methods and the pedagogical setup for real use case scenarios due to lack of accuracy of methods, insufficient amount of training data or the biases in training machine learning models, ignoring actual use case of the technology and such. In this paper, we first propose a feedback approach in piano performance education and review methods for Automated Piano Performance Assessment (APPA). After that, we discuss about gaps between a feedback approach and current methods, emphasizing their music education application. As a future work we propose a potential approach to overcome the gaps.},
author = {Kim, Hyon and Ramoneda, Pedro and Miron, Marius and Serra, Xavier},
doi = {10.5220/0011137600003182},
file = {:Users/huanzhang/Downloads/Kim_csm_over.pdf:pdf},
isbn = {9789897585623},
issn = {21845026},
journal = {International Conference on Computer Supported Education, CSEDU - Proceedings},
keywords = {Music Education,Music Information Retrieval,Pedagogy,Piano Performance Assessment},
title = {{An Overview of Automatic Piano Performance Assessment within the Music Education Context}},
volume = {1},
year = {2022}
}
@article{Gui2023,
abstract = {The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.},
archivePrefix = {arXiv},
arxivId = {2311.01616},
author = {Gui, Azalea and Gamper, Hannes and Braun, Sebastian and Emmanouilidou, Dimitra},
eprint = {2311.01616},
file = {:Users/huanzhang/Downloads/2311.01616.pdf:pdf},
title = {{Adapting Frechet Audio Distance for Generative Music Evaluation}},
url = {http://arxiv.org/abs/2311.01616},
year = {2023}
}
@article{Wang2020GeneralizingLearning,
abstract = {Machine learning has been highly successful in data-intensive applications, but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research. 1},
archivePrefix = {arXiv},
arxivId = {1904.05046v3},
author = {Wang, Yaqing and Kwok, James T and Ni, Lionel M and Kong, Hong},
doi = {10.1145/3386252},
eprint = {1904.05046v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-Shot Learning.pdf:pdf},
keywords = {CCS Concepts: • Computing methodologies → Artifici,Learning paradigms Additional Key Words and Phras,Learning paradigms Additional Key Words and Phrase,Low-Shot Learning,Machine learning,Meta-Learning,One-Shot Learning,Prior Knowledge,Small Sample Learning},
title = {{Generalizing from a Few Examples: A Survey on Few-Shot Learning}},
url = {https://doi.org/10.1145/3386252},
year = {2020}
}
@techreport{Lamb2020GraphPerspective,
abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explain-ability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper , we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.},
author = {Lamb, Lu{\'{i}}s C and {D'avila Garcez}, Artur and Gori, Marco and Prates, Marcelo O R and Avelar, Pedro H C and Vardi, Moshe Y},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lamb et al. - 2020 - Graph Neural Networks Meet Neural-Symbolic Computing A Survey and Perspective.pdf:pdf},
keywords = {Constraints and Satisfiability: general,Knowledge Representation and Reasoning: general,Machine Learning: general,Safe & Explainable & Trustworthy AI: general,Safe {\&} Explainable {\&} Trustworthy AI: genera},
title = {{Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective}},
year = {2020}
}
@article{Palmer1990,
abstract = {Investigations of the psychological representation for musical meter provided evidence for an internalized hierarchy from 3 sources: frequency distributions in musical compositions, goodness-of-fit judgments of temporal patterns in metrical contexts, and memory confusions in discrimination judgments. The frequency with which musical events occurred in different temporal locations differentiates one meter from another and coincides with music-theoretic predictions of accent placement. Goodness-of-fit judgments for events presented in metrical contexts indicated a multileveled hierarchy of relative accent strength, with finer differentiation among hierarchical levels by musically experienced than inexperienced listeners. Memory confusions of temporal patterns in a discrimination task were characterized by the same hierarchy of inferred accent strength. These findings suggest mental representations for structural regularities underlying musical meter that influence perceiving, remembering, and composing music. Perception of music, speech, and other complex human behaviors requires the processing of structured information over time. Psychological theories of serially ordered behaviors often reveal hierarchical principles of mental processing and organization that express relations among nonadjacent as well as adjacent events. Mental representations for these behaviors suggest that the complex information is recoded or organized in a form more efficient for abstract operations. A primary assumption is that the observed behavior involves complex mental processes that transform early sensory information, compare it to detailed memories, and apply decision rules to the transformed internal codes. This theoretical framework suggests that music perception involves the recoding and organizing of musical material through reference to a more abstract system of knowledge about musical structure. This abstract knowledge often represents the underlying regularities found in one's own musical culture, such as a particular tonal system or common metrical properties. These mental structures may facilitate comprehension of global aspects of musical structure and lead to expectations about future events. Thus, tonality can provide a (pitch-based) framework for melodic expectations, and meter may provide a (time-based) framework from which temporal expectations are formed. The research described later focuses on the nature of mental representation of one important aspect of musical structure: meter. We present evidence indicating that abstract knowledge of meter affects comprehension , memory, and composition of Western tonal music.},
annote = {From Duplicate 1 (Mental Representations for Musical Meter - Palmer, Caroline; Krumhansl, Carol L)

Week 4},
author = {Palmer, Caroline and Krumhansl, Carol L},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Palmer, Krumhansl - 1990 - Human Perception and Performance 0096-152390$00.pdf:pdf},
number = {4},
pages = {728--741},
title = {{Mental Representations for Musical Meter}},
volume = {75},
year = {1990}
}
@inproceedings{Tang2024SALMONNModels,
author = {Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
doi = {10.1111/j.1474-919x.1940.tb00022.x},
file = {:Users/huanzhang/Downloads/5325_salmonn_towards_generic_hearin.pdf:pdf},
issn = {0019-1019},
number = {2},
title = {{SALMONN: Towards Generic Hearing Abilities for Large Language Models}},
volume = {82},
year = {2024}
}
@article{Nakamura2016,
abstract = {We study indeterminacies in realization of ornaments and how they can be incorporated in a stochastic performance model applicable for music information processing such as score-performance matching. We point out the importance of temporal information , and propose a hidden Markov model which describes it explicitly and represents ornaments with several state types. Following a review of the indeterminacies, they are carefully incorporated into the model through its topology and parameters, and the state construction for quite general polyphonic scores is explained in detail. By analyzing piano performance data, we find significant overlaps in inter-onset-interval distributions of chordal notes, ornaments, and inter-chord events, and the data is used to determine details of the model. The model is applied for score following and offline score-performance matching, yielding highly accurate matching for performances with many ornaments and relatively frequent errors, repeats, and skips.},
archivePrefix = {arXiv},
arxivId = {1404.2314v2},
author = {Nakamura, Eita and Ono, Nobutaka and Sagayama, Shigeki and Watanabe, Kenji},
eprint = {1404.2314v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nakamura et al. - 2016 - A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments.pdf:pdf},
keywords = {* Electronic address: eita,hidden Markov model,ornaments,performance analysis,score following,score-performance matching,stochastic performance model},
title = {{A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments}},
year = {2016}
}
@article{Repp1995PatternsPerformance,
author = {Repp, Bruno H},
file = {:Users/huanzhang/Downloads/3917_1_online.pdf:pdf},
isbn = {0470866942},
journal = {The Journal of Finance},
number = {2},
pages = {679--698},
title = {{Patterns of note onset asynchronies in expressive piano performance}},
volume = {50},
year = {1995}
}
@inproceedings{Kim2022Guided-TTSGuidance,
abstract = {We propose Guided-TTS, a high-quality text-to-speech (TTS) model that does not require any transcript of target speaker using classifier guidance. Guided-TTS combines an unconditional diffusion probabilistic model with a separately trained phoneme classifier for classifier guidance. Our unconditional diffusion model learns to generate speech without any context from untranscribed speech data. For TTS synthesis, we guide the generative process of the diffusion model with a phoneme classifier trained on a large-scale speech recognition dataset. We present a norm-based scaling method that reduces the pronunciation errors of classifier guidance in Guided-TTS. We show that Guided-TTS achieves a performance comparable to that of the state-of-the-art TTS model, Grad-TTS, without any transcript for LJSpeech. We further demonstrate that Guided-TTS performs well on diverse datasets including a long-form untranscribed dataset.},
address = {Baltimore, USA},
archivePrefix = {arXiv},
arxivId = {2111.11755},
author = {Kim, Heeseung and Kim, Sungwon and Yoon, Sungroh},
booktitle = {Proceedings of Machine Learning Research},
eprint = {2111.11755},
file = {:Users/huanzhang/Downloads/kim22d.pdf:pdf},
issn = {26403498},
title = {{{Guided-TTS}: A Diffusion Model for Text-to-Speech via Classifier Guidance}},
year = {2022}
}
@techreport{Gururani2016AnalysisAssessment,
abstract = {The assessment of musical performances in, e.g., student competitions or auditions, is a largely subjective evaluation of a performer's technical skills and expressivity. Objective descriptors extracted from the audio signal have been proposed for automatic performance assessment in such a context. Such descriptors represent different aspects of pitch, dynamics and timing of a performance and have been shown to be reasonably successful in modeling human assessments of student performances through regression. This study aims to identify the influence of individual descriptors on models of human assessment in 4 categories: musicality, note accuracy, rhythmic accuracy, and tone quality. To evaluate the influence of the individual descriptors, the descriptors highly correlated with the human assessments are identified. Subsequently, various subsets are chosen using different selection criteria and the adjusted R-squared metric is computed to evaluate the degree to which these subsets explain the variance in the assessments. In addition, sequential forward selection is performed to identify the most meaningful descriptors. The goal of this study is to gain insights into which objective descriptors contribute most to the human assessments as well as to identify a subset of well-performing descriptors. The results indicate that a small subset of the designed descriptors can perform at a similar accuracy as the full set of descriptors. Sequential forward selection shows how around 33% of the descriptors do not add new information to the linear regression models, pointing towards redundancy in the descriptors.},
author = {Gururani, Siddharth and {Ashis Pati}, Kumar and Wu, Chih-Wei and Lerch, Alexander},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gururani et al. - Unknown - Analysis of Objective Descriptors for Music Performance Assessment.pdf:pdf},
title = {{Analysis of Objective Descriptors for Music Performance Assessment}},
url = {https://get.yousician.com},
year = {2016}
}
@inproceedings{Lee2020a,
abstract = {Music similarity search is useful for a variety of creative tasks such as replacing one music recording with another recording with a similar "feel", a common task in video editing. For this task, it is typically necessary to define a similarity metric to compare one recording to another. Music similarity, however, is hard to define and depends on multiple simultaneous notions of similarity (i.e. genre, mood, instrument, tempo). While prior work ignore this issue, we embrace this idea and introduce the concept of multidimensional similarity and unify both global and specialized similarity metrics into a single, semantically disentangled multidimensional similarity metric. To do so, we adapt a variant of deep metric learning called conditional similarity networks to the audio domain and extend it using track-based information to control the specificity of our model. We evaluate our method and show that our single, multidimensional model outperforms both specialized similarity spaces and alternative baselines. We also run a user-study and show that our approach is favored by human annotators as well.},
archivePrefix = {arXiv},
arxivId = {2008.03720v2},
author = {Lee, Jongpil and Bryan, Nicholas J and Salamon, Justin and Jin, Zeyu and Nam, Juhan},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {2008.03720v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - Unknown - DISENTANGLED MULTIDIMENSIONAL METRIC LEARNING FOR MUSIC SIMILARITY.pdf:pdf},
keywords = {Index Terms-multidimensional music similarity,disentangled representation,met-ric learning,query-by-example},
title = {{Disentangled Multidimentional Metric Learning for Music Similarity}},
year = {2020}
}
@inproceedings{FloresGarcia2021LeveragingRecognition,
abstract = {Deep learning work on musical instrument recognition has generally focused on instrument classes for which we have abundant data. In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments , given a few examples at inference. We apply a hierarchical loss function to the training of prototypical networks, combined with a method to aggregate prototypes hierarchically, mirroring the structure of a predefined musical instrument hierarchy. These extensions require no changes to the network architecture and new levels can be easily added or removed. Compared to a non-hierarchical few-shot baseline, our method leads to a significant increase in classification accuracy and significant decrease in mistake severity on instrument classes unseen in training.},
author = {{Flores Garcia}, Hugo and Aguilar, Aldo and Manilow, Ethan and Pardo, Bryan},
booktitle = {Proceedings of the 22th International Society for Music Information Retrieval Conference, ISMIR 2021},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Flores Garcia et al. - Unknown - LEVERAGING HIERARCHICAL STRUCTURES FOR FEW-SHOT MUSICAL INSTRUMENT RECOGNITION.pdf:pdf},
title = {{Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition}},
year = {2021}
}
@article{RutGudmundsdottir2010,
abstract = {This study analyzed the music reading performances of 6—13-year-old piano students (N = 35) in their second year of piano study. The stimuli consisted of three piano pieces, systematically constructed to vary in terms of left-hand complexity and input simultaneity. The music reading performances were recorded digitally and a code of error analysis was constructed from the data. The effect of age on the types of errors made was investigated. The age differences found were in terms of error frequency, performance continuity, contour preservation, and stimulus complexity. The study sheds light on what may be typical music reading errors of piano students in their second year of study and suggests some trends of age-related development in music reading among piano students. {\textcopyright} 2010, SAGE Publications. All rights reserved.},
author = {{Rut Gudmundsdottir}, Helga},
doi = {10.1177/0255761409351342},
file = {:Users/huanzhang/Downloads/IJME_2010_Pitchreadingpaper_HRG.pdf:pdf},
isbn = {0255761409351},
issn = {02557614},
journal = {International Journal of Music Education},
keywords = {error analysis,music literacy,music reading,music reading development,piano students},
number = {1},
pages = {61--70},
title = {{Pitch error analysis of young piano students' music reading performances}},
volume = {28},
year = {2010}
}
@article{palmer1989mappingperformance,
abstract = {Expressive timing methods are described that map pianists' musical thoughts to sounded performance. In Experiment 1, 6 pianists performed the same musical excerpt on a computer-monitored keyboard. Each performance contained 3 expressive timing patterns: chord asynchronies, rubato patterns, and overlaps (staccato and legato). Each pattern was strongest in experienced pianists' performances and decreased when pianists attempted to play unmusically. In Experiment 2 pianists performed another musical excerpt and notated their musical intentions on an unedited score. The notated interpretations correlated with the presence of the 3 methods: The notated melody preceded other events in chords (chord asynchrony); events notated as phrase boundaries showed greatest tempo changes (rubato); and the notated melody showed most consistent amount of overlap between adjacent events (staccato and legato). These results suggest that the mapping of musical thought to musical action is rule-governed, and the same rules produce different interpretations.},
author = {Palmer, Caroline},
doi = {10.1037/0096-1523.15.2.331},
file = {:Users/huanzhang/Downloads/palmer1989jep.pdf:pdf},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {2},
pages = {331--346},
pmid = {2525602},
title = {{Mapping Musical Thought to Musical Performance}},
volume = {15},
year = {1989}
}
@phdthesis{Pearce2005TheComposition,
author = {Pearce, Marcus Thomas},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pearce - 2005 - THE CONSTRUCTION AND EVALUATION OF STATISTICAL MODELS OF MELODIC STRUCTURE IN MUSIC PERCEPTION AND COMPOSITION.pdf:pdf},
title = {{The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition}},
year = {2005}
}
@article{PatwariSEMANTICALLYEXPANSION,
abstract = {Audio embeddings of musical similarity are often used for music recommendations and autoplay discovery. These embeddings are typically learned using co-listen data to train a deep neural network, to provide consistent triplet-loss distances. Instead of directly using these co-listen-based embeddings, we explore making recommendations based on a second, smaller embedding space of human-intelligible musical attributes. To do this, we use the co-listen-based audio embeddings as inputs to small attribute classifiers, trained on a small hand-labeled dataset. These classifiers map from the original embedding space to a new interpretable attribute coordinate system that provides a more useful distance measure for downstream applications. The attributes and attribute embeddings allow us to provide a search interface and more intelligible recommendations for music curators. We examine the relative performance of these two embedding spaces (the co-listen-audio embedding and the attribute embedding) for the mathematical separation of thematic playlists. We also report on the usefulness of recommendations from the attribute-embedding space to human curators for automatically extending thematic playlists.},
author = {Patwari, Ayush and Kong, Nicholas and Wang, Jun and Gargi, Ullas and Covell, Michele and Jansen, Aren},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Patwari et al. - Unknown - SEMANTICALLY MEANINGFUL ATTRIBUTES FROM CO-LISTEN EMBEDDINGS FOR PLAYLIST EXPLORATION AND EXPANSION.pdf:pdf},
title = {{SEMANTICALLY MEANINGFUL ATTRIBUTES FROM CO-LISTEN EMBEDDINGS FOR PLAYLIST EXPLORATION AND EXPANSION}}
}
@article{Xia2018ShIFT:Tutoring,
abstract = {Traditional instrument learning is time-consuming; it begins with learning music notation and necessitates layers of sophistication and abstraction. Haptic interfaces open another door to the music world for the vast majority of beginners when traditional training methods are not effective. However, existing haptic interfaces can only deal with specially designed pieces with great restrictions on performance duration and pitch range due to the fact that not all performance motions could be guided haptically for most instruments. Our ShIFT system breaks such restrictions using a semi-haptic interface. For the first time, the pitch range of the haptically learned pieces goes beyond an octave (with the fingering motion covers most of the possible choices) and the duration of learned pieces cover a whole phrase. This significant change leads to a more realistic instrument learning process. Experiments show that our semi-haptic interface is effective as long as learners are not “tone deaf.” Using our prototype device, the learning rate is about 30% faster compared to learning from videos.},
author = {Xia, Gus G. and Jacobsen, Carter O. and Chen, Qiawen and Yang, Xing Dong and Dannenberg, Roger B.},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Xia et al. - 2018 - ShIFT A semi-haptic interface for flute tutoring.pdf:pdf},
journal = {arXiv},
keywords = {Computer-aided learning,Flute,Haptic interface,Music Tutoring},
title = {{ShIFT: A semi-haptic interface for flute tutoring}},
year = {2018}
}
@inproceedings{Liu2022SymphonyModel,
abstract = {In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.},
archivePrefix = {arXiv},
arxivId = {2205.05448},
author = {Liu, Jiafeng and Dong, Yuanliang and Cheng, Zehua and Zhang, Xinran and Li, Xiaobing and Yu, Feng and Sun, Maosong},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2205.05448},
file = {:Users/huanzhang/Downloads/49.pdf:pdf},
title = {{Symphony Generation with Permutation Invariant Language Model}},
url = {http://arxiv.org/abs/2205.05448},
year = {2022}
}
@inproceedings{Lerch2019MusicSurvey,
abstract = {Music Information Retrieval (MIR) tends to focus on the analysis of audio signals. Often, a single music recording is used as representative of a “song” even though different performances of the same song may reveal different properties. A performance is distinct in many ways from a (arguably more abstract) representation of a “song,” “piece,” or musical score. The characteristics of the (recorded) performance -as opposed to the score or musical idea- can have a major impact on how a listener perceives music. The analysis of music performance, however, has been traditionally only a peripheral topic for the MIR research community. This paper surveys the field of Music Performance Analysis (MPA) from various perspectives, discusses its significance to the field of MIR, and points out opportunities for future research in this field.},
annote = {From Duplicate 1 (Music performance analysis: A survey - Lerch, Alexander; Arthur, Claire; Pati, Ashis; Gururani, Siddharth)

Survey of MPA:
- description and visualization of the performance itself
- most statistical methods, few ML
- finding commonalities or differences
- learn performance rules
- performance parameters relating to performer and listener
(more perception focused)
- measuring performance parameter (perceptually relevant)
- performance assessment
- feature design to feature learning
-},
author = {Lerch, Alexander and Arthur, Claire and Pati, Ashis and Gururani, Siddharth},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lerch et al. - Unknown - MUSIC PERFORMANCE ANALYSIS A SURVEY.pdf:pdf},
title = {{Music performance analysis: A survey}},
year = {2019}
}
@inproceedings{Doughty2017,
abstract = {This paper presents a method for assessing skill of performance from video, for a variety of tasks, ranging from drawing to surgery and rolling dough. We formulate the problem as pairwise and overall ranking of video collections, and propose a supervised deep ranking model to learn discriminative features between pairs of videos exhibiting different amounts of skill. We utilise a two-stream Temporal Segment Network to capture both the type and quality of motions and the evolving task state. Results demonstrate our method is applicable to a variety of tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 82% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated and objective organisation of how-to videos and overall, generic skill determination in video.},
archivePrefix = {arXiv},
arxivId = {1703.09913},
author = {Doughty, Hazel and Damen, Dima and Mayol-Cuevas, Walterio},
eprint = {1703.09913},
file = {:Users/huanzhang/Downloads/Doughty_Whos_Better_Whos_CVPR_2018_paper.pdf:pdf},
title = {{Who's Better, Who's Best: Skill Determination in Video using Deep Ranking}},
url = {https://arxiv.org/pdf/1703.09913.pdf},
year = {2017}
}
@article{Winkler2009,
abstract = {To shed light on how humans can learn to understand music, we need to discover what the perceptual capabilities with which infants are born. Beat induction, the detection of a regular pulse in an auditory signal, is considered a fundamental human trait that, arguably, played a decisive role in the origin of music. Theorists are divided on the issue whether this ability is innate or learned. We show that newborn infants develop expectation for the onset of rhythmic cycles (the downbeat), even when it is not marked by stress or other distinguishing spectral features. Omitting the downbeat elicits brain activity associated with violating sensory expectations. Thus, our results strongly support the view that beat perception is innate. event-related brain potentials (ERP) neonates rhythm},
author = {Winkler, Istv{\'{a}} and bor H{\'{a}} den, G{\'{a}} P and Ladinig, Olivia and Sziller, Istv{\'{a}} and Honing, Henkjan},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Winkler et al. - 2009 - Newborn infants detect the beat in music.pdf:pdf},
title = {{Newborn infants detect the beat in music}},
url = {www.pnas.org/cgi/content/full/},
year = {2009}
}
@article{Stamatatos2005AutomaticEnsembles,
abstract = {This article addresses the problem of identifying the most likely music performer, given a set of performances of the same piece by a number of skilled candidate pianists. We propose a set of very simple features for representing stylistic characteristics of a music performer, introducing 'norm-based' features that relate to a kind of 'average' performance. A database of piano performances of 22 pianists playing two pieces by Fr{\'{e}}d{\'{e}}ric Chopin is used in the presented experiments. Due to the limitations of the training set size and the characteristics of the input features we propose an ensemble of simple classifiers derived by both subsampling the training set and subsampling the input features. Experiments show that the proposed features are able to quantify the differences between music performers. The proposed ensemble can efficiently cope with multi-class music performer recognition under inter-piece conditions, a difficult musical task, displaying a level of accuracy unlikely to be matched by human listeners (under similar conditions).},
annote = {average performance

OTD as articulation?

Score deviation & Norm deviation},
author = {Stamatatos, Efstathios and Widmer, Gerhard},
doi = {10.1016/j.artint.2005.01.007},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Stamatatos, Widmer - 2005 - Automatic identification of music performers with learning ensembles(2).pdf:pdf},
journal = {Artificial Intelligence},
keywords = {Classification,Ensemble learning,Machine learning,Music},
pages = {37--56},
title = {{Automatic identification of music performers with learning ensembles}},
url = {www.elsevier.com/locate/artint},
volume = {165},
year = {2005}
}
@article{Gan2024,
abstract = {Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://agnjason.github.io/PianoMotion-page.},
archivePrefix = {arXiv},
arxivId = {2406.09326},
author = {Gan, Qijun and Wang, Song and Wu, Shengtao and Zhu, Jianke},
eprint = {2406.09326},
file = {:Users/huanzhang/Downloads/2406.09326v1.pdf:pdf},
title = {{PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance}},
url = {http://arxiv.org/abs/2406.09326},
year = {2024}
}
@misc{ThompsonMusic2,
author = {Thompson, William Forde},
file = {:Users/huanzhang/Downloads/Thompson2014Chapter2.pdf:pdf},
title = {{Music, thought and feeling: Understanding the psychology of music [CH 2]}}
}
@article{Cook2007PerformanceMazurkas,
abstract = {• ABSTRACT Reporting on work carried out in conjunction with Andrew Earis and Craig Sapp, this paper introduces recently developed approaches to the analysis of recorded music, illustrating them in terms of selected Chopin mazurkas. Topics covered include the stylistic characterisation and aesthetic values of Paderewski's playing of Op. 17 No.4, contrasted with performances from the last quarter of the twentieth century, as well as relationships between different pianists' interpretations of Op. 68 No.3. A possible performance genealogy of performances of the latter is proposed, in which recordings by Rubinstein and Cortot playa key role, while clustering based on Pearson correlation of tempo data yields relationships supported in one instance by documented teacher/pupil relationships. Representing the early outcomes of a more extended research project, these findings are encouraging in that it appearspossibleto draw meaningful conclusionsfrom the consideration only of tempo data. The current phase of the project is also working with rhythmic and dynamic data, which should significantly enhance the potential for objective modelling of musically meaningful relationships.},
author = {Cook, Nicholas},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cook - 2007 - Performance analysis and Chopin's mazurkas.pdf:pdf},
journal = {Musicae Scientiae},
pages = {183--207},
title = {{Performance analysis and Chopin's mazurkas}},
volume = {2},
year = {2007}
}
@article{Oka2013,
abstract = {Piano fingering is one of the important skills for piano performance, especially for beginners. Consequently, technology for recognizing a player's fingering is required in order to develop an automated piano lesson system. The term "piano fingering" refers to which fingers are used for pressing piano keys. In this paper, we propose a method for recognizing piano fingering by analyzing motion of multiple fingers of a piano player through the use of depth images continuously acquired with a depth sensor. Our method makes it possible to develop a practical system that does not require the use of any special markers such as color labels on fingers. First, a dictionary data set for various fingering patterns is registered. Each data element consists of a depth image, the name of a pressed key, correct information for its fingering, and the wrist positions of the player in an image. Next, a fingering pattern for unknown depth images is identified by matching acquired images to those in the dictionary data set. To reduce the search space size, the wrist position detected from an input image and a note name signal obtained from a MIDI keyboard are effectively used. The Nearest Neighbor search algorithm is utilized to search for solutions. Experimental results obtained using actual piano pieces for beginners demonstrate that the system achieves an 91.6% recognition rate and that its processing time is less than 120 msec per note. {\textcopyright} 2013 IEEE.},
author = {Oka, Akiya and Hashimoto, Manabu},
doi = {10.1109/FCV.2013.6485443},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Oka, Hashimoto - 2013 - Marker-less piano fingering recognition using sequential depth images.pdf:pdf},
isbn = {9781467356206},
journal = {FCV 2013 - Proceedings of the 19th Korea-Japan Joint Workshop on Frontiers of Computer Vision},
pages = {1--4},
publisher = {IEEE},
title = {{Marker-less piano fingering recognition using sequential depth images}},
year = {2013}
}
@article{Repetto,
annote = {From Duplicate 1 (CREATING A CORPUS OF JINGJU ( BEIJING OPERA ) MUSIC AND POSSIBILITIES FOR MELODIC ANALYSIS Rafael Caro Repetto Music Technology Group , - Repetto, Rafael Caro; Serra, Xavier)

Alignment: melody and lyrics
musically informed segmentation
Melody and Tonal Analysis
banshi identification},
author = {Repetto, Rafael Caro and Serra, Xavier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repetto, Serra - Unknown - CREATING A CORPUS OF JINGJU ( BEIJING OPERA ) MUSIC AND POSSIBILITIES FOR MELODIC ANALYSIS Rafael Caro Repett.pdf:pdf},
title = {{CREATING A CORPUS OF JINGJU ( BEIJING OPERA ) MUSIC AND POSSIBILITIES FOR MELODIC ANALYSIS Rafael Caro Repetto Music Technology Group ,}}
}
@article{MirandaArtificialApproach,
author = {Miranda, Eduardo R and Kirke, Alexis and Zhang, Qijun},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Miranda, Kirke, Zhang - Unknown - Artificial Evolution of Expressive Performance of Music An Imitative Multi-Agent Systems Approach.pdf:pdf},
journal = {Computer Music Journal},
title = {{Artificial Evolution of Expressive Performance of Music: An Imitative Multi-Agent Systems Approach}},
url = {http://direct.mit.edu/comj/article-pdf/34/1/80/1855482/comj.2010.34.1.80.pdf}
}
@inproceedings{Riley2024GAPSModel,
abstract = {We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of classical guitar performances, and a benchmark guitar transcription model that achieves state-of-the-art performance on GuitarSet in both supervised and zero-shot settings. GAPS is the largest dataset of real guitar audio, containing 14 hours of freely available audio-score aligned pairs, recorded in diverse conditions by over 200 performers, together with high-resolution note-level MIDI alignments and performance videos. These enable us to train a state-of-the-art model for automatic transcription of solo guitar recordings which can generalise well to real world audio that is unseen during training.},
archivePrefix = {arXiv},
arxivId = {2408.08653},
author = {Riley, Xavier and Guo, Zixun and Edwards, Drew and Dixon, Simon},
booktitle = {Proceeding of the 25th International Society on Music Information Retrieval (ISMIR)},
eprint = {2408.08653},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Riley et al. - 2024 - GAPS A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model.pdf:pdf},
month = {aug},
title = {{GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark Transcription Model}},
url = {http://arxiv.org/abs/2408.08653},
year = {2024}
}
@techreport{NasrullahMusicNetworks,
abstract = {Previous attempts at music artist classification use frame level audio features which summarize frequency content within short intervals of time. Comparatively, more recent music information retrieval tasks take advantage of temporal structure in audio spectrograms using deep convolutional and recurrent models. This paper revisits artist classification with this new framework and empirically explores the impacts of incorporating temporal structure in the feature representation. To this end, an established classification architecture, a Convolutional Recurrent Neural Network (CRNN), is applied to the artist20 music artist identification dataset under a comprehensive set of conditions. These include audio clip length, which is a novel contribution in this work, and previously identified considerations such as dataset split and feature level. Our results improve upon baseline works, verify the influence of the producer effect on classification performance and demonstrate the trade-offs between audio length and training set size. The best performing model achieves an average F1 score of 0.937 across three independent trials which is a substantial improvement over the corresponding baseline under similar conditions. Additionally, to showcase the effectiveness of the CRNN's feature extraction capabilities, we visualize audio samples at the model's bottleneck layer demonstrating that learned representations segment into clusters belonging to their respective artists.},
archivePrefix = {arXiv},
arxivId = {1901.04555v2},
author = {Nasrullah, Zain and Zhao, Yue},
eprint = {1901.04555v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nasrullah, Zhao - Unknown - Music Artist Classification with Convolutional Recurrent Neural Networks.pdf:pdf},
isbn = {1901.04555v2},
keywords = {Index Terms-artist classification,convolutional recurrent neural network,deep learning,information retrieval,music},
title = {{Music Artist Classification with Convolutional Recurrent Neural Networks}},
url = {https://github.com/ZainNasrullah/music-artist-classification-crnn}
}
@inproceedings{Liang2019TransferDetection,
abstract = {Detecting piano pedalling techniques in polyphonic music remains a challenging task in music information retrieval. While other piano-related tasks, such as pitch estimation and onset detection, have seen improvement through applying deep learning methods, little work has been done to develop deep learning models to detect playing techniques. In this paper, we propose a transfer learning approach for the detection of sustain-pedal techniques, which are commonly used by pianists to enrich the sound. In the source task, a convolutional neural network (CNN) is trained for learning spectral and temporal contexts when the sustain pedal is pressed using a large dataset generated by a physical modelling virtual instrument. The CNN is designed and experimented through exploiting the knowledge of piano acoustics and physics. This can achieve an accuracy score of 0.98 in the validation results. In the target task, the knowledge learned from the synthesised data can be transferred to detect the sustain pedal in acoustic piano recordings. A concatenated feature vector using the activations of the trained convolutional layers is extracted from the recordings and classified into frame-wise pedal press or release. We demonstrate the effectiveness of our method in acoustic piano recordings of Chopin's music. From the cross-validation results, the proposed transfer learning method achieves an average F-measure of 0.89 and an overall performance of 0.84 obtained using the micro-averaged F-measure. These results outperform applying the pre-trained CNN model directly or the model with a fine-tuned last layer.},
archivePrefix = {arXiv},
arxivId = {2103.13219},
author = {Liang, Beici and Fazekas, Gyorgy and Sandler, Mark},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2019.8851724},
eprint = {2103.13219},
file = {:Users/huanzhang/Downloads/Transfer_Learning_for_Piano_Sustain-Pedal_Detection.pdf:pdf},
isbn = {9781728119854},
title = {{Transfer Learning for Piano Sustain-Pedal Detection}},
year = {2019}
}
@inproceedings{Fukuda2015Score-informedSimplification,
abstract = {This paper presents a novel piano tutoring system that encourages a user to practice playing a piano by simplifying difficult parts of a musical score according to the playing skill of the user. To identify the difficult parts to be simplified, the system is capable of accurately detecting mistakes of a user's performance by referring to the musical score. More specifically, the audio recording of the user's performance is transcribed by using supervised non-negative matrix factorization (NMF) whose basis spectra are trained from isolated sounds of the same piano in advance. Then the audio recording is synchronized with the musical score using dynamic time warping (DTW). The user's mistakes are then detected by comparing those two kinds of data. Finally, the detected parts are simplified according to three kinds of rules: removing some musical notes from a complicated chord, thinning out some notes from a fast passage, and removing octave jumps. The experimental results showed that the first rule can simplify musical scores naturally. The second rule, however, often simplified the scores awkwardly when the passage formed a melody line.},
author = {Fukuda, Tsubasa and Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
booktitle = {Proceedings of the 12th International Conference in Sound and Music Computing (SMC)},
file = {:Users/huanzhang/Downloads/FUK-SMC15.pdf:pdf},
isbn = {9780992746629},
title = {{A Score-Informed Piano Tutoring System with Mistake Detection and Score Simplification}},
year = {2015}
}
@article{Gupta2022,
abstract = {Standard evaluation metrics such as the Inception score and Fr\'echet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics to control-parameter variations in audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.},
archivePrefix = {arXiv},
arxivId = {2208.10743},
author = {Gupta, Chitralekha and Wei, Yize and Gong, Zequn and Kamath, Purnima and Li, Zhuoyao and Wyse, Lonce},
eprint = {2208.10743},
file = {:Users/huanzhang/Downloads/107.pdf:pdf},
title = {{Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures}},
url = {http://arxiv.org/abs/2208.10743},
year = {2022}
}
@techreport{McLeod2016HMM-BasedPerformance,
abstract = {Voice separation is an important component of Music Information Retrieval (MIR). In this paper, we present an HMM which can be used to separate music performance data in the form of MIDI into monophonic voices. It works on two basic principles: that consecutive notes within a single voice will tend to occur on similar pitches, and that there are short (if any) temporal gaps between them. We also present an incremental algorithm which can perform inference on the model efficiently. We show that our approach achieves a significant improvement over existing approaches, when run on a corpus of 78 compositions by J.S. Bach, each of which has been separated into the gold standard voices suggested by the original score. We also show that it can be used to perform voice separation on live MIDI data without an appreciable loss in accuracy. The code for the model described here is available at https://github.com/apmcleod/voice-splitting.},
author = {McLeod, Andrew and Steedman, Mark},
booktitle = {Journal of New Music Research},
doi = {10.1080/09298215.2015.1136650},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mcleod, Steedman - Unknown - HMM-Based Voice Separation of MIDI Performance.pdf:pdf},
issn = {17445027},
keywords = {music analysis,perception,software,voice},
number = {1},
pages = {17--26},
title = {{HMM-Based Voice Separation of MIDI Performance}},
url = {https://github.com/apmcleod/voice-},
volume = {45},
year = {2016}
}
@article{Cribier-Delande,
abstract = {Learning & predicting sequence dynamics remain an outstanding research issue. Many application domains ranging from sales forecast to smart cities face challenging problems. Most of the approaches in the literature have weaknesses regarding the modelling of the time series context. We choose to tackle long-term forecasting with a focus on learning disentangled representations of contextual factors. We turned the forecasting problem into a generative problem : given spatial, temporal and other factors, we are able to generate the corresponding time series. Not only are we able to predict the sequence dynamics for a specific day or location, but we also show that our architecture is very robust to the introduction of outliers in the training set. We propose different variants of our approach, including an encoder-decoder architecture that enables us to extrapolate all contextual combinations corresponding to a new factor observed only once. This ambitious formulation raises theoretical questions about how to aggregate information associated with the different factors. This article examines several options and highlights the value of introducing an attention mechanism to improve the effectiveness of this critical step.},
author = {Cribier-Delande, Perrine and Puget, Raphael and Guigue, Vincent and Denoyer, Ludovic},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cribier-Delande et al. - Unknown - Attention mechanism on disentangled contextual factor representations for time series generation.pdf:pdf},
keywords = {Mots-clef},
title = {{Attention mechanism on disentangled contextual factor representations for time series generation}}
}
@inproceedings{Zhang2024FromPiano,
abstract = {Our study investigates an approach for understanding musical performances through the lens of audio encoding models, focusing on the domain of solo Western classical piano music. Compared to composition-level attribute understanding such as key or genre, we identify a knowledge gap in performance-level music understanding, and address three critical tasks: expertise ranking, difficulty estimation, and piano technique detection, introducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose. We leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT, and DAC, demonstrating varied capabilities in tackling downstream tasks, to explore whether domain-specific fine-tuning enhances capability in capturing performance nuances. Our best approach achieved 93.6\% accuracy in expertise ranking, 33.7\% in difficulty estimation, and 46.7\% in technique detection, with Audio-MAE as the overall most effective encoder. Finally, we conducted a case study on Chopin Piano Competition data using trained models for expertise ranking, which highlights the challenge of accurately assessing top-tier performances.},
archivePrefix = {arXiv},
arxivId = {2407.04518},
author = {Zhang, Huan and Liang, Jinhua and Dixon, Simon},
booktitle = {Proceeding of the 25t International Society on Music Information Retrieval (ISMIR)},
eprint = {2407.04518},
file = {:Users/huanzhang/Downloads/From_Audio_Encoders_to_Piano_Judges__Benchmarking_Performance_Understanding_for_Solo_Piano (2).pdf:pdf},
isbn = {2407.04518v1},
title = {{From Audio Encoders to Piano Judges: Benchmarking Performance Understanding for Solo Piano}},
url = {https://arxiv.org/abs/2407.04518v1},
year = {2024}
}
@article{Dou2020,
abstract = {Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews ("spams") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies. However, the game has no closed-form solution and is not differentiable to admit the typical gradient-based algorithms. We turn the game into two dependent Markov Decision Processes (MDPs) to allow efficient stochastic optimization based on multi-armed bandit and policy gradient. We experiment on three large review datasets using various state-of-the-art spamming and detection strategies and show that the optimization algorithm can reliably find an equilibrial detector that can robustly and effectively prevent spammers with any mixed spamming strategies from attaining their practical goal. Our code is available at https://github.com/YingtongDou/Nash-Detect.},
annote = {From Duplicate 2 (Robust Spammer Detection by Nash Reinforcement Learning - Dou, Yingtong; Ma, Guixiang; Yu, Philip S.; Xie, Sihong)

Bipartite graph
U: accounts
V: products
s(v): reputation

attack strategies a = [a1, a2..]
mixed strategy
detectors d = [d1..], importance q = [q1, q2...]},
archivePrefix = {arXiv},
arxivId = {2006.06069},
author = {Dou, Yingtong and Ma, Guixiang and Yu, Philip S. and Xie, Sihong},
doi = {10.1145/3394486.3403135},
eprint = {2006.06069},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dou et al. - 2020 - Robust Spammer Detection by Nash Reinforcement Learning(4).pdf:pdf},
isbn = {9781450379984},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {adversarial learning,reinforcement learning,spam detection},
pages = {924--933},
title = {{Robust Spammer Detection by Nash Reinforcement Learning}},
year = {2020}
}
@article{Wang2021VQMIVCConversion,
abstract = {One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement. Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance. To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner. Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics. In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.},
archivePrefix = {arXiv},
arxivId = {2106.10132},
author = {Wang, Disong and Deng, Liqun and Yeung, Yu Ting and Chen, Xiao and Liu, Xunying and Meng, Helen},
doi = {10.21437/Interspeech.2021-283},
eprint = {2106.10132},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - VQMIVC Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for O.pdf:pdf},
isbn = {9781713836902},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
keywords = {Mutual information,One-shot,Unsupervised disentanglement,Vector quantization,Voice conversion},
title = {{VQMIVC: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion}},
url = {https://github.com/Wendison/VQMIVC.},
year = {2021}
}
@article{Sap2018ATOMIC:Reasoning,
abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., “if X pays Y a compliment, then Y will likely return the compliment”). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
author = {Sap, Maarten and {Le Bras}, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
file = {:Users/huanzhang/Downloads/4160-Article Text-7214-1-10-20190705.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Description Logics,OBDA,Tractable Reasoning},
title = {{ATOMIC: An atlas of machine commonsense for if-then reasoning}},
year = {2018}
}
@article{Li2023MutimodalAssistants,
abstract = {This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.},
archivePrefix = {arXiv},
arxivId = {2309.10020},
author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
eprint = {2309.10020},
file = {:Users/huanzhang/Downloads/2309.10020.pdf:pdf},
journal = {arXiv preprint arXiv:2309.10020},
title = {{Multimodal Foundation Models: From Specialists to General-Purpose Assistants}},
url = {http://arxiv.org/abs/2309.10020},
year = {2023}
}
@inproceedings{Wu2018LearnedPerformances,
abstract = {The automatic assessment of (student) music performance involves the characterization of the audio recordings and the modeling of human judgments. To build a computational model that provides a reliable assessment, the system must take into account various aspects of a performance including technical correctness and aesthetic standards. While some progress has been made in recent years, the search for an effective feature representation remains open-ended. In this study, we explore the possibility of using learned features from sparse coding. Specifically, we investigate three sets of features, namely a baseline set, a set of designed features, and a feature set learned with sparse coding. In addition, we compare the impact of two different input representations on the effectiveness of the learned features. The evaluation is performed on a dataset of annotated recordings of students playing snare exercises. The results imply the general viability of feature learning in the context of automatic assessment of music performances.},
author = {Wu, Chih Wei and Lerch, Alexander},
booktitle = {Proceedings - 12th IEEE International Conference on Semantic Computing, ICSC 2018},
doi = {10.1109/ICSC.2018.00022},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Lerch - Unknown - Learned Features for the Assessment of Percussive Music Performances.pdf:pdf},
isbn = {9781538644072},
keywords = {Feature Learning,Music Information Retrieval,Music Performance Assessment,Sparse Coding},
pages = {93--99},
title = {{Learned Features for the Assessment of Percussive Music Performances}},
url = {http://www.smartmusic.com},
volume = {2018-Janua},
year = {2018}
}
@techreport{HaoTan2020GenerativePerformance,
abstract = {We present a controllable neural audio synthesizer based on Gaussian Mixture Variational Au-toencoders (GM-VAE), which can generate realistic piano performances in the audio domain that closely follows temporal conditions of two essential style features for piano performances: artic-ulation and dynamics. We demonstrate how the model is able to apply fine-grained style morphing over the course of synthesizing the audio. This is based on conditions which are latent variables that can be sampled from the prior or inferred from other pieces. One of the envisioned use cases is to inspire creative and brand new interpretations for existing pieces of piano music.},
archivePrefix = {arXiv},
arxivId = {2006.09833v2},
author = {{Hao Tan}, Hao and Luo, Yin-Jyun and Herremans, Dorien},
eprint = {2006.09833v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hao Tan, Luo, Herremans - 2020 - Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance.pdf:pdf},
title = {{Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance}},
url = {https://github.com/gudgud96/piano-synthesis},
year = {2020}
}
@article{Kong2022GiantMIDI-PianoMusic,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.07061v1},
author = {Kong, Qiuqiang and Li, Bochen and Chen, Jitong and Wang, Yuxuan},
eprint = {arXiv:2010.07061v1},
file = {:Users/huanzhang/Downloads/2010.07061 (1).pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {dataset,giantmidi-piano,piano transcription},
number = {1},
pages = {87--98},
title = {{{GiantMIDI-Piano} : A large-scale MIDI dataset for classical piano music}},
volume = {5},
year = {2022}
}
@article{,
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - (No Title)(2).pdf:pdf},
title = {{(No Title)}}
}
@article{Dong2022,
abstract = {Music performance synthesis aims to synthesize a musical score into a natural performance. In this paper, we borrow recent advances in text-to-speech synthesis and present the Deep Performer -- a novel system for score-to-audio music performance synthesis. Unlike speech, music often contains polyphony and long notes. Hence, we propose two new techniques for handling polyphonic inputs and providing a fine-grained conditioning in a transformer encoder-decoder model. To train our proposed system, we present a new violin dataset consisting of paired recordings and scores along with estimated alignments between them. We show that our proposed model can synthesize music with clear polyphony and harmonic structures. In a listening test, we achieve competitive quality against the baseline model, a conditional generative audio model, in terms of pitch accuracy, timbre and noise level. Moreover, our proposed model significantly outperforms the baseline on an existing piano dataset in overall quality.},
archivePrefix = {arXiv},
arxivId = {2202.06034},
author = {Dong, Hao-Wen and Zhou, Cong and Berg-Kirkpatrick, Taylor and McAuley, Julian},
eprint = {2202.06034},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2022 - Deep Performer Score-to-Audio Music Performance Synthesis.pdf:pdf},
month = {feb},
title = {{Deep Performer: Score-to-Audio Music Performance Synthesis}},
url = {http://arxiv.org/abs/2202.06034},
year = {2022}
}
@techreport{Collins2010,
abstract = {Are there new insights through computational methods to the thorny problem of plotting the flow of musical influence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the investigator. Web scraping and web services provide one angle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of influence are constructed in GraphViz combining artist similarity and dates. Content based music similarity is the second approach, based around a core collection of synth pop albums. The prospect for new musical analyses are discussed with respect to these techniques. {\textcopyright} 2010 International Society for Music Information Retrieval.},
annote = {From Duplicate 2 (Computational analysis of musical influence: A musicological case study using MIR tools - Collins, Nick)

This one very early

two approaches:
- web metadata for the influence
- also content based: MFCC, etc},
author = {Collins, Nick},
booktitle = {Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Collins - Unknown - COMPUTATIONAL ANALYSIS OF MUSICAL INFLUENCE A MUSICOLOGICAL CASE STUDY USING MIR TOOLS.pdf:pdf},
isbn = {9789039353813},
pages = {177--182},
title = {{Computational analysis of musical influence: A musicological case study using MIR tools}},
url = {http://people.wku.edu/charles.smith/music/index2.htm},
year = {2010}
}
@article{Feisthauer2019ModelingForms,
abstract = {Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts. We introduce high-level features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks.},
author = {Feisthauer, Laurent and Bigo, Louis and Giraud, Mathieu},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Feisthauer, Bigo, Giraud - 2019 - Modeling and learning structural breaks in sonata forms.pdf:pdf},
journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR 2019)},
number = {July},
pages = {398--404},
title = {{Modeling and learning structural breaks in sonata forms}},
url = {https://hal.archives-ouvertes.fr/hal-02162936},
year = {2019}
}
@article{Wang2021SupervisedFeatures,
abstract = {Music structure analysis (MSA) methods traditionally search for musically meaningful patterns in audio: homo-geneity, repetition, novelty, and segment-length regularity. Hand-crafted audio features such as MFCCs or chroma-grams are often used to elicit these patterns. However, with more annotations of section labels (e.g., verse, chorus, bridge) becoming available, one can use supervised feature learning to make these patterns even clearer and improve MSA performance. To this end, we take a supervised metric learning approach: we train a deep neural network to output embeddings that are near each other for two spec-trogram inputs if both have the same section type (accord-ing to an annotation), and otherwise far apart. We propose a batch sampling scheme to ensure the labels in a training pair are interpreted meaningfully. The trained model extracts features that can be used in existing MSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and RWC), we demonstrate that using the proposed features can improve a traditional MSA algorithm significantly in both intra-and cross-dataset scenarios.},
author = {Wang, Ju-Chiang and Smith, Jordan B L and Lu, Wei-Tsung and Song, Xuchen},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - SUPERVISED METRIC LEARNING FOR MUSIC STRUCTURE FEATURES.pdf:pdf},
journal = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Supervised Metric Learning for Music Structure Features}},
url = {https://github.com/KevinMusgrave/pytorch-metric-learning},
year = {2021}
}
@inproceedings{McCallum2022SupervisedUnderstanding,
abstract = {In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks. We show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs. Within the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning -- and in some cases, supervised learning -- for music understanding. We also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.},
archivePrefix = {arXiv},
arxivId = {2210.03799},
author = {McCallum, Matthew C. and Korzeniowski, Filip and Oramas, Sergio and Gouyon, Fabien and Ehmann, Andreas F.},
booktitle = {Proceeding of the 21t International Society on Music Information Retrieval (ISMIR)},
eprint = {2210.03799},
file = {:Users/huanzhang/Downloads/2210.03799.pdf:pdf},
title = {{Supervised and Unsupervised Learning of Audio Representations for Music Understanding}},
url = {http://arxiv.org/abs/2210.03799},
year = {2022}
}
@article{Mathieu,
abstract = {We develop a generalisation of disentanglement in variational autoencoders (VAEs)-decomposition of the latent representation-characterising it as the fulfilment of two factors: a) the latent encod-ings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disen-tanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering , independent subspaces, or even intricate hierarchical dependency relationships. We show that the $\beta$-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N and Teh, Yee Whye},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mathieu et al. - Unknown - Disentangling Disentanglement in Variational Autoencoders.pdf:pdf},
title = {{Disentangling Disentanglement in Variational Autoencoders}},
url = {http://github.com/iffsid/disentangling-disentanglement.}
}
@inproceedings{Foscarin2022ConceptClassifier,
abstract = {Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.},
annote = {CAV: test if the concept is relevant to the system

Piano roll; resnet, accuracy 0.93

Concept dataset: 30 exceprts of $\sim$25s

Layer outputs -> separated 

S: how much output change if we perturb layer activations

NTD: obtain a set of C-CAVs},
archivePrefix = {arXiv},
arxivId = {2208.12485},
author = {Foscarin, Francesco and Hoedt, Katharina and Praher, Verena and Flexer, Arthur and Widmer, Gerhard},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2208.12485},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Foscarin et al. - Unknown - CONCEPT-BASED TECHNIQUES FOR MUSICOLOGIST-FRIENDLY EXPLANATIONS IN A DEEP MUSIC CLASSIFIER.pdf:pdf},
title = {{Concept-Based Techniques for "Musicologist-friendly" Explanations in a Deep Music Classifier}},
url = {http://arxiv.org/abs/2208.12485},
year = {2022}
}
@article{,
file = {:Users/huanzhang/Downloads/子平真诠-沈孝瞻原著.pdf:pdf},
pages = {1--27},
title = {子平真诠 - 沈孝瞻原著}
}
@article{BeiciLiangGyorgyFazekasandMarkSandlerCentreforDigitalMusicQueenMaryUniversityofLondon2019,
author = {{Beici Liang , Gy ¨ orgy Fazekas and Mark Sandler Centre for Digital Music , Queen Mary University of London}, United Kingdom},
file = {:Users/huanzhang/Downloads/Piano_Sustain-pedal_Detection_Using_Convolutional_Neural_Networks.pdf:pdf},
isbn = {9781538646588},
pages = {241--245},
title = {{Piano Sustain Pedal Detection Using Convolutional Neural Networks}},
year = {2019}
}
@article{Al-Tahan2021CLAR:Representations,
abstract = {Learning rich visual representations using contrastive self-supervised learning has been extremely successful. However, it is still a major question whether we could use a similar approach to learn superior auditory representations. In this paper, we expand on prior work (SimCLR) to learn better auditory representations. We (1) introduce various data augmentations suitable for auditory data and evaluate their impact on pre-dictive performance, (2) show that training with time-frequency audio features substantially improves the quality of the learned representations compared to raw signals, and (3) demonstrate that training with both supervised and contrastive losses simultaneously improves the learned representations compared to self-supervised pre-training followed by supervised fine-tuning. We illustrate that by combining all these methods and with substantially less labeled data, our framework (CLAR) achieves significant improvement on prediction performance compared to supervised approach. Moreover, compared to self-supervised approach, our framework converges faster with significantly better representations 1 .},
author = {Al-Tahan, Haider and Mohsenzadeh, Yalda},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Al-Tahan, Mohsenzadeh - 2021 - CLAR Contrastive Learning of Auditory Representations.pdf:pdf},
journal = {Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)},
title = {{CLAR: Contrastive Learning of Auditory Representations}},
url = {https://github.com/},
year = {2021}
}
@article{MacRitchie2013PlayingDevice,
abstract = {Pianists often use asynchrony with the purpose of making multiple voices more transparent or providing extra emphasis at particular locations. This study evaluates the perceptual effects of such asynchronies in per- formances of three pieces with varying textures: a Bach Fugue, a Brahms Intermezzo, and a Chopin Prelude. By varying onset times (+55 ms, 0, -55 ms) and key velocity values (0, +10 units) of the bass and melody voices of a pre-recorded professional performance on a Yamaha Disk- lavier, 11 different performances were synthesized. 21 participants were presented these performances in random order and asked three ques- tions in consecutive sessions: How rich is the timbre? How transparent are the different voices? How expressive do you think this performance is and is it appropriate for the piece? Strong agreement is found for ratings of richness and transparency, but few significant differences appear be- tween different levels of asynchrony and key velocity. ANOVAs revealed effects of asynchrony in Bach, and pianists differed from non-pianists in their ratings of transparency and expressiveness for Chopin and Brahms.},
author = {MacRitchie, Jennifer and Eiholzer, Hubert},
file = {:Users/huanzhang/Downloads/ISPS13MacRitchieEiholzer.pdf:pdf},
isbn = {9782960137804},
journal = {International Symposium on Performance Science},
keywords = {asynchrony,expressive performance,melody lead,perception,voice separation},
number = {August},
title = {{Playing hands together: Exploring the use of asynchrony as an expressive device}},
year = {2013}
}
@article{Kim2021QuantitativeHands,
abstract = {Quantitative evaluation of piano performance is of interests in many fields, including music education and computational performance rendering. Previous studies utilized features extracted from audio or musical instrument digital interface (MIDI) files but did not address the difference between hands (DBH), which might be an important aspect of high-quality performance. Therefore, we investigated DBH as an important factor determining performance proficiency. To this end, 34 experts and 34 amateurs were recruited to play two excerpts on a Yamaha Disklavier. Each performance was recorded in MIDI, and handcrafted features were extracted separately for the right hand (RH) and left hand (LH). These were conventional MIDI features representing temporal and dynamic attributes of each note and computed as absolute values (e. g., MIDI velocity) or ratios between performance and corresponding scores (e. g., ratio of duration or inter-onset interval (IOI)). These note-based features were rearranged into additional features representing DBH by simple subtraction between features of both hands. Statistical analyses showed that DBH was more significant in experts than in amateurs across features. Regarding temporal features, experts pressed keys longer and faster with the RH than did amateurs. Regarding dynamic features, RH exhibited both greater values and a smoother change along melodic intonations in experts that in amateurs. Further experiments using principal component analysis (PCA) and support vector machine (SVM) verified that hand-difference features can successfully differentiate experts from amateurs according to performance proficiency. Moreover, existing notebased raw feature values (Basic features) and DBH features were tested repeatedly via 10- fold cross-validation, suggesting that adding DBH features to Basic features improved F1 scores to 93.6% (by 3.5%) over Basic features. Our results suggest that differently controlling both hands simultaneously is an important skill for pianists; therefore, DBH features should be considered in the quantitative evaluation of piano performance.},
author = {Kim, Sarah and Park, Jeong Mi and Rhyu, Seungyeon and Nam, Juhan and Lee, Kyogu},
doi = {10.1371/journal.pone.0250299},
file = {:Users/huanzhang/Downloads/journal.pone.0250299.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {5 May},
pages = {1--28},
pmid = {34010289},
title = {{Quantitative analysis of piano performance proficiency focusing on difference between hands}},
url = {http://dx.doi.org/10.1371/journal.pone.0250299},
volume = {16},
year = {2021}
}
@inproceedings{Doh2023LP-MusicCapsCaptioning,
abstract = {Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.},
archivePrefix = {arXiv},
arxivId = {2307.16372},
author = {Doh, SeungHeon and Choi, Keunwoo and Lee, Jongpil and Nam, Juhan},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
eprint = {2307.16372},
file = {:Users/huanzhang/Downloads/2307.16372.pdf:pdf},
title = {{{LP-MusicCaps}: {LLM}-Based Pseudo Music Captioning}},
url = {http://arxiv.org/abs/2307.16372},
year = {2023}
}
@article{Johnson2020,
abstract = {Synthesis using physical modeling has a long history. As computational costs for physical modeling synthesis are often much greater than for conventional synthesis methods, most techniques currently rely on simplifying assumptions. These include digital waveguides, as well as modal synthesis methods. Although such methods are efficient, it can be difficult to approach some of the more detailed behavior of musical instruments in this way, including strongly nonlinear interactions. Mainstream time-stepping simulation methods, despite being computationally costly, allow for such detailed modeling. In this article, the results of a five-year research project, Next Generation Sound Synthesis, are presented, with regard to algorithm design for a variety of sound-producing systems, including brass and bowed-string instruments, guitars, and large-scale environments for physical modeling synthesis. In addition, 3-D wave-based modeling of large acoustic spaces is discussed, as well as the embedding of percussion instruments within such spaces for full spatialization. This article concludes with a discussion of some of the basics of such time-stepping methods, as well as their application in audio synthesis.},
author = {Johnson, David and Damian, Daniela and Tzanetakis, George},
doi = {10.1162/COMJ},
file = {:Users/huanzhang/Downloads/Detecting_Hand_Posture_in_Piano_Playing_Using_Dept.pdf:pdf},
journal = {Computer Music Journal},
number = {1},
pages = {59--78},
title = {{Detecting Hand Posture in Piano Playing Using Depth Data}},
volume = {43},
year = {2020}
}
@article{Ying,
abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GNNEXPLAINER, the first general, model-agnostic approach for providing inter-pretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNEXPLAINER identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNEXPLAINER can generate consistent and concise explanations for an entire class of instances. We formulate GNNEXPLAINER as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GNNEXPLAINER provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
archivePrefix = {arXiv},
arxivId = {1903.03894v4},
author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
eprint = {1903.03894v4},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ying et al. - Unknown - GNNExplainer Generating Explanations for Graph Neural Networks(2).pdf:pdf},
isbn = {1903.03894v4},
title = {{GNNExplainer: Generating Explanations for Graph Neural Networks}}
}
@techreport{Alban,
abstract = {In our project, we address the overarching question: can we use audio data to identify and characterize musical influence relationships? Using human-annotated data about popular musicians from allmusic.com, we construct a musical influence graph wherein a directed edge between two artist nodes signifies a musical influence relationship. We perform link prediction on this graph by training a classifier with input features based on graph structure as well as song audio content. The results demonstrate that features based on song audio content have predictive power, and our highest-scoring feature combination achieves an area under the receiver operating characteristic (ROC) curve of 0.867. These findings show that audio features of songs can, to some extent, identify musical influence relationships between artists, and this finding affirms the promise of data-driven analyses of the progression of musical creativity.},
annote = {From Duplicate 1 (CS224W Final Report: Influence Networks in Popular Music - Alban, Marco; Choksi, Vivek; Tsai, Stephanie)

form a graph by annotated data from allmuic.com
and do link prediction},
author = {Alban, Marco and Choksi, Vivek and Tsai, Stephanie},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Alban, Choksi, Tsai - Unknown - CS224W Final Report Influence Networks in Popular Music.pdf:pdf},
title = {{CS224W Final Report: Influence Networks in Popular Music}}
}
@article{McCallum2024,
abstract = {Audio embeddings enable large scale comparisons of the similarity of audio files for applications such as search and recommendation. Due to the subjectivity of audio similarity, it can be desirable to design systems that answer not only whether audio is similar, but similar in what way (e.g., wrt. tempo, mood or genre). Previous works have proposed disentangled embedding spaces where subspaces representing specific, yet possibly correlated, attributes can be weighted to emphasize those attributes in downstream tasks. However, no research has been conducted into the independence of these subspaces, nor their manipulation, in order to retrieve tracks that are similar but different in a specific way. Here, we explore the manipulation of tempo in embedding spaces as a case-study towards this goal. We propose tempo translation functions that allow for efficient manipulation of tempo within a pre-existing embedding space whilst maintaining other properties such as genre. As this translation is specific to tempo it enables retrieval of tracks that are similar but have specifically different tempi. We show that such a function can be used as an efficient data augmentation strategy for both training of downstream tempo predictors, and improved nearest neighbor retrieval of properties largely independent of tempo.},
archivePrefix = {arXiv},
arxivId = {2401.08902},
author = {McCallum, Matthew C. and Henkel, Florian and Kim, Jaehun and Sandberg, Samuel E. and Davies, Matthew E. P.},
eprint = {2401.08902},
file = {:Users/huanzhang/Downloads/2401.08902v1.pdf:pdf},
title = {{Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for Tempo Prediction and Search}},
url = {http://arxiv.org/abs/2401.08902},
year = {2024}
}
@article{Bresin2008ArtificialScores,
author = {Bresin, Roberto},
doi = {10.1080/09298219808570748},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bresin - 2008 - Journal of New Music Research Artificial neural networks based models for automatic performance of musical scores.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
title = {{Artificial neural networks based models for automatic performance of musical scores}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2008}
}
@techreport{Lim2017ChordNetworks,
abstract = {Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners.},
author = {Lim, Hyungui and Rhyu, Seungyeon and Lee, Kyogu},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lim, Rhyu, Lee - 2017 - Chord generation from symbolic melody using blstm networks.pdf:pdf},
isbn = {9789811151798},
pages = {621--627},
title = {{Chord generation from symbolic melody using blstm networks}},
year = {2017}
}
@article{Ramoneda2023CombiningClassification,
abstract = {Predicting the difficulty of playing a musical score is essential for structuring and exploring score collections. Despite its importance for music education, the automatic difficulty classification of piano scores is not yet solved, mainly due to the lack of annotated data and the subjectiveness of the annotations. This paper aims to advance the state-of-the-art in score difficulty classification with two major contributions. To address the lack of data, we present Can I Play It? (CIPI) dataset, a machine-readable piano score dataset with difficulty annotations obtained from the renowned classical music publisher Henle Verlag. The dataset is created by matching public domain scores with difficulty labels from Henle Verlag, then reviewed and corrected by an expert pianist. As a second contribution, we explore various input representations from score information to pre-trained ML models for piano fingering and expressiveness inspired by the musicology definition of performance. We show that combining the outputs of multiple classifiers performs better than the classifiers on their own, pointing to the fact that the representations capture different aspects of difficulty. In addition, we conduct numerous experiments that lay a foundation for score difficulty classification and create a basis for future research. Our best-performing model reports a 39.5% balanced accuracy and 1.1 median square error across the nine difficulty levels proposed in this study. Code, dataset, and models are made available for reproducibility.},
archivePrefix = {arXiv},
arxivId = {2306.08480},
author = {Ramoneda, Pedro and Jeong, Dasaem and Eremenko, Vsevolod and Tamer, Nazif Can and Miron, Marius and Serra, Xavier},
doi = {10.1016/j.eswa.2023.121776},
eprint = {2306.08480},
file = {:Users/huanzhang/Downloads/2306.08480.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Can I play it,Difficulty analysis,Education technology,Expressive piano performance,Music complexity,Music difficulty,Music information retrieval,Music playability,Performance difficulty,Performance difficulty prediction,Piano fingering},
title = {{Combining piano performance dimensions for score difficulty classification}},
url = {http://arxiv.org/abs/2306.08480},
year = {2024}
}
@article{Vieillard2008,
abstract = {Three experiments were conducted in order to validate 56 musical excerpts that conveyed four intended emotions (happiness, sadness, threat and peacefulness). In Experiment 1, the musical clips were rated in terms of how clearly the intended emotion was portrayed, and for valence and arousal. In Experiment 2, a gating paradigm was used to evaluate the course for emotion recognition. In Experiment 3, a dissimilarity judgement task and multidimensional scaling analysis were used to probe emotional content with no emotional labels. The results showed that emotions are easily recognised and discriminated on the basis of valence and arousal and with relative immediacy. Happy and sad excerpts were identified after the presentation of fewer than three musical events. With no labelling, emotion discrimination remained highly accurate and could be mapped on energetic and tense dimensions. The present study provides suitable musical material for research on emotions.},
author = {Vieillard, Sandrine and Peretz, Isabelle and Gosselin, Nathalie and Khalfa, Stephanie and Gagnon, Lise and Bouchard, Bernard},
doi = {10.1080/02699930701503567},
file = {:Users/huanzhang/Downloads/Vieillardetal2008_CognitionEmotion.pdf:pdf},
issn = {14640600},
journal = {Cognition and Emotion},
number = {4},
pages = {720--752},
title = {{Happy, sad, scary and peaceful musical excerpts for research on emotions}},
volume = {22},
year = {2008}
}
@inproceedings{Walder2016ModellingRoll,
abstract = {In this paper, we consider the problem of probabilistically modelling symbolic music data. We introduce a representation which reduces polyphonic music to a univariate categorical sequence. In this way, we are able to apply state of the art natural language processing techniques, namely the long short-term memory sequence model. The representation we employ permits arbitrary rhythmic structure, which we assume to be given. We show that our model is effective on four out of four piano roll based benchmark datasets. We further improve our model by augmenting our training data set with transpositions of the original pieces through all musical keys, thereby convincingly advancing the state of the art on these benchmark problems. We also fit models to music which is unconstrained in its rhythmic structure, discuss the properties of the model, and provide musical samples. We also describe and provide with full (non piano roll) timing information our new carefully preprocessed dataset of 19700 classical midi music files - significantly more than previously available.},
archivePrefix = {arXiv},
arxivId = {1606.01368},
author = {Walder, Christian},
booktitle = {Journal of Machine Learning Research},
eprint = {1606.01368},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Walder - 2016 - Modelling Symbolic Music Beyond the Piano Roll.pdf:pdf},
issn = {15337928},
pages = {174--189},
title = {{Modelling symbolic music: Beyond the piano roll}},
volume = {63},
year = {2016}
}
@inproceedings{Dong2018MuseGANAccompaniment,
abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/.},
archivePrefix = {arXiv},
arxivId = {1709.06298},
author = {Dong, Hao Wen and Hsiao, Wen Yi and Yang, Li Chia and Yang, Yi Hsuan},
booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v32i1.11312},
eprint = {1709.06298},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - Unknown - MuseGAN Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - Unknown - MuseGAN Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompanimen(2).pdf:pdf},
isbn = {9781577358008},
issn = {2159-5399},
title = {{{MuseGAN}: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment}},
url = {https://salu133445.github.io/musegan/},
year = {2018}
}
@article{Lu2021,
abstract = {The subjective evaluation of music generation techniques has been mostly done with questionnaire-based listening tests while ignoring the perspectives from music composition, arrangement, and soundtrack editing. In this paper, we propose an editing test to evaluate users' editing experience of music generation models in a systematic way. To do this, we design a new music style transfer model combining the non-chronological inference architecture, autoregressive models and the Transformer, which serves as an improvement from the baseline model on the same style transfer task. Then, we compare the performance of the two models with a conventional listening test and the proposed editing test, in which the quality of generated samples is assessed by the amount of effort (e.g., the number of required keyboard and mouse actions) spent by users to polish a music clip. Results on two target styles indicate that the improvement over the baseline model can be reflected by the editing test quantitatively. Also, the editing test provides profound insights which are not accessible from usual listening tests. The major contribution of this paper is the systematic presentation of the editing test and the corresponding insights, while the proposed music style transfer model based on state-of-the-art neural networks represents another contribution.},
archivePrefix = {arXiv},
arxivId = {arXiv:2110.12855v1},
author = {Lu, Wei Tsung and Wu, Meng Hsuan and Chiu, Yuh Ming and Su, Li},
doi = {10.1145/3474085.3475529},
eprint = {arXiv:2110.12855v1},
file = {:Users/huanzhang/Downloads/2110.12855.pdf:pdf},
isbn = {9781450386517},
journal = {MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
keywords = {artificial intelligence,human-centered computing,music generation,neural networks,style transfer},
pages = {3936--3944},
title = {{Actions Speak Louder than Listening: Evaluating Music Style Transfer based on Editing Experience}},
year = {2021}
}
@article{Zhu2022,
abstract = {Graphs are ubiquitous in encoding relational information of real-world objects in many domains. Graph generation, whose purpose is to generate new graphs from a distribution similar to the observed graphs, has received increasing attention thanks to the recent advances of deep learning models. In this paper, we conduct a comprehensive review on the existing literature of deep graph generation from a variety of emerging methods to its wide application areas. Specifically, we first formulate the problem of deep graph generation and discuss its difference with several related graph learning tasks. Secondly, we divide the state-of-the-art methods into three categories based on model architectures and summarize their generation strategies. Thirdly, we introduce three key application areas of deep graph generation. Lastly, we highlight challenges and opportunities in the future study of deep graph generation. We hope that our survey will be useful for researchers and practitioners who are interested in this exciting and rapidly-developing field.},
archivePrefix = {arXiv},
arxivId = {2203.06714},
author = {Zhu, Yanqiao and Du, Yuanqi and Wang, Yinkai and Xu, Yichen and Zhang, Jieyu and Liu, Qiang and Wu, Shu},
eprint = {2203.06714},
file = {:Users/huanzhang/Downloads/2203.06714.pdf:pdf},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
number = {LoG},
title = {{A Survey on Deep Graph Generation: Methods and Applications}},
volume = {198},
year = {2022}
}
@article{Ryu2022,
author = {Ryu, Jesung and Rhyu, Seungyeon and Yoon, Hong-gyu and Kim, Eunchong and Yang, Ju Young and Kim, Taehyun},
file = {:Users/huanzhang/Downloads/27774-Article Text-31828-1-2-20240324.pdf:pdf},
keywords = {Application Domains (APP): APP: Other Applications,Machine Learning (ML): ML: Applications},
pages = {222--230},
title = {{MID-FiLD : MIDI Dataset for Fine-Level Dynamics}},
year = {2022}
}
@inproceedings{Foscarin2020ASAPTranscription,
abstract = {In this paper we present Aligned Scores and Performances (ASAP): a new dataset of 222 digital musical scores aligned with 1068 performances (more than 92 hours) of Western classical piano music. The scores are provided as paired MusicXML files and quantized MIDI files, and the performances as paired MIDI files and partially as audio recordings. Scores and performances are aligned with downbeat, beat, time signature, and key signature annotations. ASAP has been obtained thanks to a new annotation workflow that combines score analysis and alignment algorithms, with the goal of reducing the time for manual annotation. The dataset itself is, to our knowledge, the largest that includes an alignment of music scores to MIDI and audio performance data. As such, it is a useful resource for a wide variety of MIR applications, from those that target the complete audio-to-score Automatic Music Transcription task, to others that target more specific aspects (e.g., key signature estimation and beat or downbeat tracking from both MIDI and audio representations).},
author = {Foscarin, Francesco and Mcleod, Andrew and Rigaux, Philippe and Jacquemard, Florent and Sakai, Masahiko},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Downloads/127.pdf:pdf},
title = {{{ASAP} : A Dataset of Aligned Scores and Performances for Piano Transcription}},
year = {2020}
}
@article{Zhang2023,
author = {Zhang, Yixiao and Mary, Queen and Dixon, Simon and Mary, Queen},
file = {:Users/huanzhang/Downloads/Loop_Copilot__arxiv_.pdf:pdf},
number = {October},
title = {{Loop Copilot : Conducting AI Ensembles for Music Generation and Iterative Editing Loop Copilot : Conducting AI Ensembles for Music Generation and Iterative Editing}},
year = {2023}
}
@article{GrahnRhythmBrain,
abstract = {& When we listen to rhythm, we often move spontaneously to the beat. This movement may result from processing of the beat by motor areas. Previous studies have shown that several motor areas respond when attending to rhythms. Here we investigate whether specific motor regions respond to beat in rhythm. We predicted that the basal ganglia and supplementary motor area (SMA) would respond in the presence of a regular beat. To establish what rhythm properties induce a beat, we asked subjects to reproduce different types of rhythmic sequences. Improved reproduction was observed for one rhythm type, which had integer ratio relationships between its intervals and regular perceptual accents. A subsequent functional magnetic resonance imaging study found that these rhythms also elicited higher activity in the basal ganglia and SMA. This finding was consistent across different levels of musical training, although musicians showed activation increases unrelated to rhythm type in the premotor cortex, cerebellum, and SMAs (pre-SMA and SMA). We conclude that, in addition to their role in movement production, the basal ganglia and SMAs may mediate beat perception. &},
annote = {From Duplicate 2 (Rhythm and Beat Perception in Motor Areas of the Brain - Grahn, Jessica A; Brett, Matthew)

Week 4},
author = {Grahn, Jessica A and Brett, Matthew},
doi = {10.1162/jocn.2007.19.5.893},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Grahn, Brett - Unknown - Rhythm and Beat Perception in Motor Areas of the Brain.pdf:pdf},
title = {{Rhythm and Beat Perception in Motor Areas of the Brain}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/jocn.2007.19.5.893}
}
@article{Yichi2017,
author = {Yichi, Zhang and Zhiyao, Duan},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yichi, Zhiyao - 2017 - IMINET CONVOLUTIONAL SEMI-SIAMESE NETWORKS FOR SOUND SEARCH BY VOCAL IMITATION.pdf:pdf},
isbn = {9781538616314},
pages = {304--308},
title = {{Iminet: Convolutional Semi-Siamese Networks for Sound Search by Vocal Imitation}},
year = {2017}
}
@article{Hepper1991,
author = {Hepper, Peter G.},
doi = {10.1080/03033910.1991.10557830},
file = {:Users/huanzhang/Downloads/Hepper1991.pdf:pdf},
issn = {21580812},
journal = {The Irish Journal of Psychology},
number = {2},
pages = {95--107},
title = {{An Examination of Fetal Learning Before and After Birth}},
volume = {12},
year = {1991}
}
@article{Rizo2010SymbolicStructures,
author = {Rizo, David},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rizo - 2010 - Symbolic Music Comparison with Tree Data Structures.pdf:pdf},
keywords = {Reconocimiento de patrones. Similitud musical. Rep},
title = {{Symbolic Music Comparison with Tree Data Structures}},
year = {2010}
}
@inproceedings{Gilbert2018DisentanglingCompletion,
abstract = {Content-aware image completion or in-painting is a fundamental tool for the correction of defects or removal of objects in images. We propose a non-parametric in-painting algorithm that enforces both structural and aesthetic (style) consistency within the resulting image. Our contributions are twofold: 1) we explicitly disentangle image structure and style during patch search and selection to ensure a visually consistent look and feel within the target image. 2) we perform adaptive stylization of patches to conform the aesthetics of selected patches to the target image, so harmonizing the integration of selected patches into the final composition. We show that explicit consideration of visual style during in-painting delivers excellent qualitative and quantitative results across the varied image styles and content, over the Places2 scene photographic dataset and a challenging new in-painting dataset of artwork derived from BAM!},
author = {Gilbert, Andrew and Collomosse, John and Jin, Hailin and Price, Brian},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gilbert et al. - Unknown - Disentangling Structure and Aesthetics for Style-aware Image Completion.pdf:pdf},
title = {{Disentangling Structure and Aesthetics for Style-aware Image Completion}},
year = {2018}
}
@article{Hadjeres2017DeepBachGeneration,
abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
archivePrefix = {arXiv},
arxivId = {1612.01010},
author = {Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois and Nielsen, Frank},
eprint = {1612.01010},
file = {:Users/huanzhang/Downloads/1612.01010v2.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2187--2196},
title = {{DeepBach: A steerable model for bach chorales generation}},
volume = {3},
year = {2017}
}
@article{Shen2023,
abstract = {Pre-trained language models have become the prevailing approach for handling natural language processing tasks in recent years. Given the similarities in sequential features between symbolic music and natural language text, it is fairly logical to adopt pre-training methods to symbolic music data. However, the disparity between music and natural language text makes it difficult to comprehensively model the unique features of music through traditional text-based pre-training strategies alone. To address this challenge, in this paper, we design the quad-attribute masking (QM) strategy and propose the key prediction (KP) task to improve the extraction of generic knowledge from symbolic music. We evaluate the impact of various pre-training strategies on several public symbolic music datasets, and the results of our experiments reveal that the proposed multi-task pre-training model can effectively capture music domain knowledge from symbolic music data and significantly improve performance on downstream tasks. CCS CONCEPTS • Applied computing → Sound and music computing; • Computing methodologies → Natural language processing.},
author = {Shen, Zhexu and Yang, Liang and Yang, Zhihan and Lin, Hongfei},
doi = {10.1145/3591106.3592237},
isbn = {9798400701788},
keywords = {deep learning,masking strategies,multi-task pre-training,pre-trained models,symbolic music},
number = {23},
publisher = {ACM},
title = {{More Than Simply Masking: Exploring Pre-training Strategies for Symbolic Music Understanding}},
url = {https://doi.org/10.1145/3591106.3592237},
volume = {5},
year = {2023}
}
@article{Konz2011SaarlandData,
author = {Konz, Verena and Bogler, Wolfgang and Arifi-M, Vlora},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Konz, Bogler, Arifi-M - Unknown - SAARLAND MUSIC DATA (SMD)(2).pdf:pdf},
journal = {Late-Breaking and Demo Session of the International Society on Music Information Retrieval (ISMIR)},
title = {{Saarland Music Data}},
year = {2011}
}
@inproceedings{Tai2023,
abstract = {Recent literature has shown that denoising diffusion probabilistic models (DDPMs) can be used to synthesize high-fidelity samples with a competitive (or sometimes better) quality than previous state-of-the-art approaches. However, few attempts have been made to apply DDPM for the speech enhancement task. The reported performance of the existing works is relatively poor and significantly inferior to other generative methods. In this work, we first reveal the difficulties in applying existing diffusion models to the field of speech enhancement. Then we introduce DR-DiffuSE, a simple and effective framework for speech enhancement using conditional diffusion models. We present three strategies (two in diffusion training and one in reverse sampling) to tackle the condition collapse and guarantee the sufficient use of condition information. For efficiency, we introduce the fast sampling technique to reduce the sampling process into several steps and exploit a refinement network to calibrate the defective speech. Our proposed method achieves state-of-the-art performance to the GAN-based model and shows a significant improvement over existing DDPM-based algorithms.},
author = {Tai, Wenxin and Zhou, Fan and Trajcevski, Goce and Zhong, Ting},
booktitle = {Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023},
doi = {10.1609/aaai.v37i11.26597},
file = {:Users/huanzhang/Downloads/26597-Article Text-30660-1-2-20230626.pdf:pdf},
isbn = {9781577358800},
issn = {2159-5399},
keywords = {Speech & Natural Language Processing (SNLP): SNLP:},
pages = {13627--13635},
title = {{Revisiting Denoising Diffusion Probabilistic Models for Speech Enhancement: Condition Collapse, Efficiency and Refinement}},
volume = {37},
year = {2023}
}
@article{Bernays2014InvistigatingPedaling,
abstract = {, each with five different timbral intentions (bright, dark, dry, round, and velvety). The performances were recorded with the high-accuracy B{\"{o}}sendorfer CEUS system. Fine-grained performance features of dynamics, touch, articulation and pedaling were extracted. Reduced PCA performance spaces and descriptive performance portraits confirmed that pianists exhibited unique, specific profiles for different timbral intentions, derived from underlying traits of general individuality, while sharing some broad commonalities of dynamics and articulation for each timbral intention. These results confirm that pianists' abstract notions of timbre correspond to reliable patterns of performance technique. Furthermore, these effects suggest that pianists can express individual styles while complying with specific timbral intentions.},
author = {Bernays, Michel and Traube, Caroline and Gingras, Bruno and Goebl, Werner and Spcl, Idmil /},
doi = {10.3389/fpsyg.2014.00157},
file = {:Users/huanzhang/Downloads/fpsyg-05-00157.pdf:pdf},
keywords = {B{\"{o}}sendorfer CEUS,articulation,expression,individuality,performance,piano,timbre,touch},
title = {{Investigating pianists' individuality in the performance of five timbral nuances through patterns of articulation, touch, dynamics, and pedaling}},
url = {www.frontiersin.org},
year = {2014}
}
@article{Li2022b,
abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
archivePrefix = {arXiv},
arxivId = {2201.12086},
author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
eprint = {2201.12086},
file = {:Users/huanzhang/Downloads/2201.12086.pdf:pdf},
number = {2},
title = {{BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}},
url = {http://arxiv.org/abs/2201.12086},
year = {2022}
}
@article{Brunner2018,
abstract = {We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.},
archivePrefix = {arXiv},
arxivId = {1809.07600v1},
author = {Brunner, Gino and Konrad, Andres and Wang, Yuyi and Wattenhofer, Roger},
eprint = {1809.07600v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Brunner et al. - Unknown - MIDI-VAE MODELING DYNAMICS AND INSTRUMENTATION OF MUSIC WITH APPLICATIONS TO STYLE TRANSFER.pdf:pdf},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
title = {{MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer}},
url = {https://junyanz.github.io/CycleGAN/},
year = {2018}
}
@article{Serra2009CrossIdentification,
abstract = {There is growing evidence that nonlinear time series analysis techniques can be used to successfully characterize, classify, or process signals derived from real-world dynamics even though these are not necessarily deterministic and stationary. In the present study, we proceed in this direction by addressing an important problem our modern society is facing, the automatic classification of digital information. In particular, we address the automatic identification of cover songs, i.e. alternative renditions of a previously recorded musical piece. For this purpose, we here propose a recurrence quantification analysis measure that allows the tracking of potentially curved and disrupted traces in cross recurrence plots (CRPs). We apply this measure to CRPs constructed from the state space representation of musical descriptor time series extracted from the raw audio signal. We show that our method identifies cover songs with a higher accuracy as compared to previously published techniques. Beyond the particular application proposed here, we discuss how our approach can be useful for the characterization of a variety of signals from different scientific disciplines. We study coupled R{\"{o}}ssler dynamics with stochastically modulated mean frequencies as one concrete example to illustrate this point.},
author = {Serr{\`{a}}, Joan and Serra, Xavier and Andrzejak, Ralph G},
doi = {10.1088/1367-2630/11/9/093017},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2009 - Cross recurrence quantification for cover song identification-Hamiltonian tomography for quantum many-body systems w.pdf:pdf},
journal = {New Journal of Physics},
title = {{Cross recurrence quantification for cover song identification}},
volume = {11},
year = {2009}
}
@techreport{Kosta2018,
author = {Kosta, Katerina},
file = {:Users/huanzhang/Downloads/TENOR2018-presentation-kosta.pdf:pdf},
title = {{MazurkaBL Presentation Slides}},
year = {2018}
}
@article{Cooper,
abstract = {Speech synthesis and music audio generation from symbolic input differ in many aspects but share some similarities. In this study, we investigate how text-to-speech synthesis techniques can be used for piano MIDI-to-audio synthesis tasks. Our investigation includes Tacotron and neural source-filter waveform models as the basic components, with which we build MIDI-to-audio synthesis systems in similar ways to TTS frameworks. We also include reference systems using conventional sound modeling techniques such as sample-based and physical-modeling-based methods. The subjective experimental results demonstrate that the investigated TTS components can be applied to piano MIDI-to-audio synthesis with minor modifications. The results also reveal the performance bottleneck -- while the waveform model can synthesize high quality piano sound given natural acoustic features, the conversion from MIDI to acoustic features is challenging. The full MIDI-to-audio synthesis system is still inferior to the sample-based or physical-modeling-based approaches, but we encourage TTS researchers to test their TTS models for this new task and improve the performance.},
archivePrefix = {arXiv},
arxivId = {2104.12292},
author = {Cooper, Erica and Wang, Xin and Yamagishi, Junichi},
doi = {10.21437/ssw.2021-23},
eprint = {2104.12292},
file = {:Users/huanzhang/Downloads/2104.12292.pdf:pdf},
pages = {130--135},
title = {{Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis}},
year = {2021}
}
@inproceedings{Cifka2021Self-SupervisedTransfer,
author = {C{\'{i}}fka, Ond\v{r}ej and Ozerov, Alexy and \c{S}im\c{s}ekli, Umut and Richard, Gael},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Downloads/Self-Supervised_VQ-VAE_for_One-Shot_Music_Style_Transfer.pdf:pdf},
isbn = {9781728176055},
title = {{Self-Supervised {VQ-VAE} for One Shot Music Style Transfer}},
year = {2021}
}
@techreport{Liuc,
abstract = {Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement , separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.},
author = {Liu, Alexander H and Le, Matt and Vyas, Apoorv and Shi, Bowen and Tjandra, Andros and Hsu, Wei-Ning and Csail, Mit and Ai, Meta},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - GENERATIVE PRE-TRAINING FOR SPEECH WITH FLOW MATCHING.pdf:pdf},
title = {{GENERATIVE PRE-TRAINING FOR SPEECH WITH FLOW MATCHING}}
}
@inproceedings{Sebastien2012ScoreLearning,
abstract = {Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often rely on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Components Analysis and compared to human estimations. Lastly we discuss the integration of this work to @-MUSE, a collaborative score annotation platform based on multimedia contents indexation.},
annote = {Also feature-based learning

PCA on the features to see how they are linked. 

Clustering on the first few principal components - interpretation of the classess},
author = {S{\'{e}}bastien, V{\'{e}}ronique and Ralambondrainy, Henri and S{\'{e}}bastien, Olivier and Conruyt, No{\"{e}}l},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/S{\'{e}}bastien et al. - 2012 - Score Analyzer automatically determining scores difficulty level for instrumental e-Learning.pdf:pdf},
title = {{Score Analyzer: automatically determining scores difficulty level for instrumental e-Learning}},
year = {2012}
}
@phdthesis{Scheirer2014ExtractingMusic,
abstract = {A computer system is described which performs polyphonic transcription of known solo piano music by using high-level musical information to guide a signal-processing system. This process, which we term expressive performance extraction, maps a digital audio rep- resentation of a musical performance to a MIDI representation of the same performance using the score of the music as a guide. Analysis of the accuracy of the system is presented, and its usefulness both as a tool for music-psychology researchers and as an example of a musical-knowledge-based signal-processing system is discussed.},
author = {Scheirer, Eric David},
file = {:Users/huanzhang/Downloads/Extracting_Expressive_Performance_Information_from.pdf:pdf},
number = {October},
school = {Cornell University},
title = {{Extracting Expressive Performance Information from Recorded Music}},
year = {2014}
}
@article{Maman,
abstract = {Multi-instrument Automatic Music Transcription (AMT), or the decoding of a musical recording into semantic musical content, is one of the holy grails of Music Information Retrieval. Current AMT approaches are restricted to piano and (some) guitar recordings, due to difficult data collection. In order to overcome data collection barriers, previous AMT approaches attempt to employ musical scores in the form of a digitized version of the same song or piece. The scores are typically aligned using audio features and strenuous human intervention to generate training labels. We introduce Note EM , a method for simultaneously training a transcriber and aligning the scores to their corresponding performances, in a fully-automated process. Using this unaligned supervision scheme, complemented by pseudo-labels and pitch-shift augmentation, our method enables training on in-the-wild recordings with unprecedented accuracy and instrumental variety. Using only synthetic data and unaligned supervision, we report SOTA note-level accuracy of the MAPS dataset, and large favorable margins on cross-dataset evaluations. We also demonstrate robustness and ease of use; we report comparable results when training on a small, easily obtainable, self-collected dataset, and we propose alternative labeling to the MusicNet dataset, which we show to be more accurate. Our project page is available at https://benadar293.github.io.},
archivePrefix = {arXiv},
arxivId = {2204.13668v1},
author = {Maman, Ben and Bermano, Amit H},
eprint = {2204.13668v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Maman, Bermano - Unknown - Unaligned Supervision for Automatic Music Transcription In-the-Wild.pdf:pdf},
title = {{Unaligned Supervision for Automatic Music Transcription In-the-Wild}},
url = {https://benadar293.github.io.}
}
@article{Worrall2024,
abstract = {There have been many attempts to model the ability of human musicians to take a score and perform or render it expressively, by adding tempo, timing, loudness and articulation changes to non-expressive music data. While expressive rendering models exist in academic research, most of these are not open source or accessible, meaning they are difficult to evaluate empirically and have not been widely adopted in professional music software. Systematic comparative evaluation of such algorithms stopped after the last Performance Rendering Contest (RENCON) in 2013, making it difficult to compare newer models to existing work in a fair and valid way. In this paper, we introduce the first transformer-based model for expressive rendering, Cue-Free Express + Pedal (CFE+P), which predicts expressive attributes such as note-wise dynamics and micro-timing adjustments, and beat-wise tempo and sustain pedal use based only on the start and end times and pitches of notes (e.g., inexpressive MIDI input). We perform two comparative evaluations on our model against a non-machine learning baseline taken from professional music software and two open-source algorithms &#x2013; a feedforward neural network (FFNN) and hierarchical recurrent neural network (HRNN). The results of two listening studies indicate that our model renders passages that outperform what can be done in professional music software such as Logic Pro and Ableton Live.},
author = {Worrall, Kyle and Yin, Zongyu and Collins, Tom},
doi = {10.1109/TAI.2024.3408717},
file = {:Users/huanzhang/Downloads/Comparative_evaluation_in_the_wild_Systems_for_the_expressive_rendering_of_music.pdf:pdf},
issn = {26914581},
journal = {IEEE Transactions on Artificial Intelligence},
keywords = {Annotations,Artificial Intelligence in art and music,Computational modeling,Computer Generated Music,Deep Learning,Music,Music Information Retrieval,Neural Networks,Neural networks,Rendering (computer graphics),Timing,Transformers},
number = {0},
pages = {1--15},
publisher = {IEEE},
title = {{Comparative evaluation in the wild: Systems for the expressive rendering of music}},
volume = {PP},
year = {2024}
}
@article{Christen2017,
abstract = {Cybersecurity is of capital importance in a world where economic and social processes increasingly rely on digital technology. Although the primary ethical motivation of cybersecurity is prevention of informational or physical harm, its enforcement can also entail conflicts with other moral values. This contribution provides an outline of value conflicts in cybersecurity based on a quantitative literature analysis and qualitative case studies. The aim is to demonstrate that the security-privacy-dichotomy—that still seems to dominate the ethics discourse based on our bibliometric analysis—is insufficient when discussing the ethical challenges of cybersecurity. Furthermore, we want to sketch how the notion of contextual integrity could help to better understand and mitigate such value conflicts.},
author = {Christen, Markus and Gordijn, Bert and Weber, Karsten and van de Poel, Ibo and Yaghmaei, Emad},
doi = {10.29297/orbit.v1i1.28},
file = {:Users/huanzhang/Downloads/7-document.pdf:pdf},
issn = {25158562},
journal = {The ORBIT Journal},
keywords = {contextual integrity,cybersecurity,moral values,privacy,value conflicts},
number = {1},
pages = {1--19},
title = {{A Review of Value-Conflicts in Cybersecurity}},
volume = {1},
year = {2017}
}
@article{Couturier2022,
abstract = {Symbolic texture describes how sounding components are organized in a musical score. Along with other high-level musical components such as melody, harmony or rhythm, symbolic texture has a significant impact on the structure and the style of a musical piece. In this article, we present a syntax to describe compositional texture in the specific case of Western classical piano music. The syntax is expressive and flexible, unifying into a single text label information about density, diversity, musical function and note relationships in distinct textural units. The formal definition of the syntax enables its parsing and computational processing, opening promising perspectives in computer-aided music analysis and composition. We provide an implementation to parse and manipulate textural labels as well as a bestiary of annotated examples of textural configurations .},
author = {Couturier, Louis and Bigo, Louis and Lev{\'{e}}, Florence},
file = {:Users/huanzhang/Downloads/main.pdf:pdf},
title = {{Annotating Symbolic Texture in Piano Music: a Formal Syntax}},
url = {https://hal.archives-ouvertes.fr/hal-03631151},
year = {2022}
}
@inproceedings{Pampalk2004AAudio,
abstract = {A Matlab toolbox implementing music similarity measures for audio is presented. The implemented measures focus on aspects related to timbre and periodicities in the signal. This paper gives an overview of the implemented functions. In particular, the basics of the similarity measures are reviewed and some visualizations are discussed.},
author = {Pampalk, Elias},
booktitle = {Proceedings of the 5th International Conference on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pampalk - Unknown - A MATLAB TOOLBOX TO COMPUTE MUSIC SIMILARITY FROM AUDIO.pdf:pdf},
title = {{A Matlab Toolbox to Compute Music Similarity from Audio}},
url = {http://www.ncrg.aston.ac.uk/netlab},
year = {2004}
}
@inproceedings{Hawthorne2022GeneralAR,
abstract = {Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens , enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 × 64 ImageNet images and PG-19 books.},
archivePrefix = {arXiv},
arxivId = {2202.07765v2},
author = {Hawthorne, Curtis and Jaegle, Andrew and {Catalina Cangea} and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and Sheahan, Hannah and Zeghidour, Neil and Alayrac, Jean-Baptiste and Carreira, Jo{\~{a}}o and Engel, Jesse},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
eprint = {2202.07765v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hawthorne et al. - Unknown - General-purpose, long-context autoregressive modeling with Perceiver AR.pdf:pdf},
title = {{General-purpose, long-context autoregressive modeling with Perceiver AR}},
url = {https://github.com/},
year = {2022}
}
@article{Liu2023a,
abstract = {Video-based lessons are becoming a popular way for distance piano education. However, limited by the fixed camera angle, a video is difficult to tell precise 3D hand posture, which is one of the most essential factors for learning piano. This paper presents a visualization system providing the intuitive discrepancy of hand postures in two piano performance videos. Through a motion capture system, the estimated 3D postures are visualized and discrepancies based on distinct metrics are displayed, integrated with modular functions assisting skill acquisition. A pilot study proves that the proposed visualization can be a supplementary means for only video-based lessons in terms of correcting hand postures and fingering.},
author = {Liu, Ruofan and Wu, Erwin and Liao, Chen Chieh and Nishioka, Hayato and Furuya, Shinichi and Koike, Hideki},
doi = {10.1145/3544549.3585705},
file = {:Users/huanzhang/Downloads/3544549.3585705.pdf:pdf},
isbn = {9781450394222},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Motion Analysis,Piano Training,Virtual Visualization},
title = {{PianoHandSync: An Alignment-based Hand Pose Discrepancy Visualization System for Piano Learning}},
year = {2023}
}
@inproceedings{Winters2016AutomaticStudy,
abstract = {Musicians spend countless hours practicing their instruments. To document and organize this time, musicians commonly use practice charts to log their practice. However, manual techniques require time, dedication, and experience to master, are prone to fallacy and omission, and ultimately can not describe the subtle variations in each repetition. This paper presents an alternative: by analyzing and classifying the audio recorded while practicing, logging could occur automatically, with levels of detail, accuracy, and ease that would not be possible otherwise. Towards this goal, we introduce the problem of Automatic Practice Logging (APL), including a discussion of the benefits and unique challenges it raises. We then describe a new dataset of over 600 annotated recordings of solo piano practice, which can be used to design and evaluate APL systems. After framing our approach to the problem, we present an algorithm designed to align short segments of practice audio with reference recordings using pitch chroma and dynamic time warping.},
author = {{Michael Winters}, R. and Gururani, Siddharth and Lerch, Alexander},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, (ISMIR)},
file = {:Users/huanzhang/Downloads/000181.pdf:pdf},
isbn = {9780692755068},
title = {{Automatic Practice Logging: Introduction, Dataset and Preliminary Study}},
year = {2016}
}
@article{Reiner2017,
abstract = {This study investigated melody recognition with a modification of the standard-comparison paradigm. Subjects listened to an original melody, were exposed to a silent retention interval, and then were presented with a target and a distractor. There were two types of trials. On target-same trials, listeners heard the target (the original melody) played in the same key exactly as it was previously heard and a distractor (a novel melody) played in a key either a major second or a perfect fourth from the original. On target-different trials, targets were heard in keys either a major second or a perfect fourth from the original, while distractors were played in the same key as the original. The contours of targets and distractors were also examined. Retention intervals varied from 0.5 s to 15 s. The results indicate that contour complexity and key-distance interact in the recognition of short melodies.},
author = {Reiner, Thomas W},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Reiner - 2017 - Melody Recognition after Short Delays Effects of Contour Complexity and Key-Distance.pdf:pdf},
journal = {Journal of Scientific Psychology},
title = {{Melody Recognition after Short Delays: Effects of Contour Complexity and Key-Distance}},
year = {2017}
}
@article{Vyas2023,
abstract = {Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like "a person speaking" and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/},
archivePrefix = {arXiv},
arxivId = {2312.15821},
author = {Vyas, Apoorv and Shi, Bowen and Le, Matthew and Tjandra, Andros and Wu, Yi-Chiao and Guo, Baishan and Zhang, Jiemin and Zhang, Xinyue and Adkins, Robert and Ngan, William and Wang, Jeff and Cruz, Ivan and Akula, Bapi and Akinyemi, Akinniyi and Ellis, Brian and Moritz, Rashel and Yungster, Yael and Rakotoarison, Alice and Tan, Liang and Summers, Chris and Wood, Carleigh and Lane, Joshua and Williamson, Mary and Hsu, Wei-Ning},
eprint = {2312.15821},
file = {:Users/huanzhang/Downloads/416201240_1109648396840016_4084243924939353841_n.pdf:pdf},
pages = {1--41},
title = {{Audiobox: Unified Audio Generation with Natural Language Prompts}},
url = {http://arxiv.org/abs/2312.15821},
year = {2023}
}
@incollection{MacKenzie1990,
abstract = {the present study investigated rhythmic precision in the performance of piano scales  we had two related goals: first, to provide motor psychophysical functions for scales performance, and second, to make inferences about motor programming  we asked how tempo and unimanual or bimanual playing conditions would affect piano performance (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
address = {Hillsdale,  NJ,  US},
author = {MacKenzie, Christine L and {Van Eerd}, D L},
booktitle = {Attention and performance 13:  Motor representation and control.},
isbn = {0-8058-0565-6 (Hardcover)},
keywords = {*Motor Performance,*Musical Ability,Psychophysics},
pages = {375--408},
publisher = {Lawrence Erlbaum Associates, Inc},
title = {{Rhythmic precision in the performance of piano scales: Motor psychophysics and motor programming.}},
year = {1990}
}
@article{Cheuk2021,
abstract = {Most of the current supervised automatic music transcription (AMT) models lack the ability to generalize. This means that they have trouble transcribing real-world music recordings from diverse musical genres that are not presented in the labelled training data. In this paper, we propose a semi-supervised framework, ReconVAT, which solves this issue by leveraging the huge amount of available unlabelled music recordings. The proposed ReconVAT uses reconstruction loss and virtual adversarial training. When combined with existing U-net models for AMT, ReconVAT achieves competitive results on common benchmark datasets such as MAPS and MusicNet. For example, in the few-shot setting for the string part version of MusicNet, ReconVAT achieves F1-scores of 61.0% and 41.6% for the note-wise and note-with-offset-wise metrics respectively, which translates into an improvement of 22.2% and 62.5% compared to the supervised baseline model. Our proposed framework also demonstrates the potential of continual learning on new data, which could be useful in real-world applications whereby new data is constantly available. CCS CONCEPTS • Applied computing → Sound and music computing; • Computing methodologies → Semi-supervised learning settings; Neural networks.},
author = {Cheuk, Kin Wai and Herremans, Dorien and Su, Li},
doi = {10.1145/3474085.3475405},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cheuk, Herremans, Su - 2021 - ReconVAT A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data.pdf:pdf},
isbn = {9781450386517},
keywords = {audio processing,automatic music transcription,music information retrieval,semi-supervised training,virtual adversarial training},
publisher = {Virtual Event},
title = {{ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data}},
url = {https://doi.org/10.1145/3474085.3475405},
year = {2021}
}
@inproceedings{Tamer2022ViolinAnalysis,
author = {{Can Tamer}, Nazif and Ramoneda, Pedro and Serra, Xavier},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/247.pdf:pdf},
title = {{Violin Etudes: A Comprehensive Dataset for F0 Estimation and Performance Analysis}},
year = {2022}
}
@article{Huang2024,
author = {Huang, Yu-fen and Moran, Nikki and Coleman, Simon and Kelly, Jon and Wei, Shun-hwa and Chen, Po-yin and Huang, Yun-hsin and Chen, Tsung-ping and Kuo, Yu-chia and Wei, Yu-chi and Li, Chih-hsuan and Huang, Da-yu and Kao, Hsuan-kai and Su, Li},
doi = {10.1109/TASLP.2024.3407529},
file = {:Users/huanzhang/Downloads/MOSA_Music_Motion_with_Semantic_Annotation_Dataset_for_Cross-Modal_Music_Processing.pdf:pdf},
number = {8},
title = {{MOSA : Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing}},
volume = {14},
year = {2024}
}
@article{Karystinaios2023MusicalProblem,
abstract = {This paper targets the perceptual task of separating the different interacting voices, i.e., monophonic melodic streams, in a polyphonic musical piece. We target symbolic music, where notes are explicitly encoded, and model this task as a Multi-Trajectory Tracking (MTT) problem from discrete observations, i.e., notes in a pitch-time space. Our approach builds a graph from a musical piece, by creating one node for every note, and separates the melodic trajectories by predicting a link between two notes if they are consecutive in the same voice/stream. This kind of local, greedy prediction is made possible by node embeddings created by a heterogeneous graph neural network that can capture inter- and intratrajectory information. Furthermore, we propose a new regularization loss that encourages the output to respect the MTT premise of at most one incoming and one outgoing link for every node, favouring monophonic (voice) trajectories; this loss function might also be useful in other general MTT scenarios. Our approach does not use domain-specific heuristics, is scalable to longer sequences and a higher number of voices, and can handle complex cases such as voice inversions and overlaps. We reach new state-of-the-art results for the voice separation task in classical music of different styles. All code, data, and pretrained models are available on https://github.com/manoskary/vocsepijcai2023.},
archivePrefix = {arXiv},
arxivId = {2304.14848},
author = {Karystinaios, Emmanouil and Foscarin, Francesco and Widmer, Gerhard},
doi = {10.24963/ijcai.2023/430},
eprint = {2304.14848},
file = {:Users/huanzhang/Downloads/2304.14848.pdf:pdf},
isbn = {9781956792034},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {3866--3874},
title = {{Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem}},
year = {2023}
}
@article{Darwin1972,
abstract = {Three experiments are reported on the partial report of material presented auditorily over three spatially different channels. When partial report was required by spatial location, it was superior to whole report if the cue came less than four seconds after the end of the stimuli (Exp. I). When partial report was required by semantic category (letters/digits) the relation between it and whole report depended on whether the S was asked also to attribute each item to its correct spatial location. When location was required partial report was lower than whole report and showed no significant decay with delay of the partial report indicator (Exp. II), but when location was not required, partial report was superior to whole report for indicator delays of less than two seconds (Exp. III). This superiority was, however, much less than that found in Exp. I when partial report was required by spatial location. These results are compatible with a store which has a useful life of around two seconds and from which material may be retrieved more easily by spatial location than by semantic category. The concept of brief sensory storage has played a central role in recent discourse on the nature of human information processing (e.g., Neisser, 1967; Haber, 1969; Hunt, 1971). The proposition is that sensory data is initially represented in a literal, labile form for a brief duration during the course of conversion into a relatively more persistent, categorized form. The sensory store which has received the most attention and which},
annote = {From Duplicate 2 (An Auditory Analogue of the Sperling Partial Report Procedure: Evidence for Brief Auditory Storage1 - Darwin, Christopher J; Turvey, Michael T; Crowder, Robert G)

Week 6},
author = {Darwin, Christopher J and Turvey, Michael T and Crowder, Robert G},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Darwin, Turvey, Crowder - 1972 - An Auditory Analogue of the Sperling Partial Report Procedure Evidence for Brief Auditory Storage1.pdf:pdf},
journal = {COGNITIVE PSYCHOLOGY},
pages = {255--267},
title = {{An Auditory Analogue of the Sperling Partial Report Procedure: Evidence for Brief Auditory Storage1}},
volume = {3},
year = {1972}
}
@article{Shi2018,
author = {Shi, Zhengshan and Nakano, Tomoyasu and Goto, Masataka},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2018 - INSTLISTENER AN EXPRESSIVE PARAMETER ESTIMATION SYSTEM IMITATING HUMAN PERFORMANCES OF MONOPHONIC MUSICAL INSTRUMENTS.pdf:pdf},
isbn = {9781538646588},
pages = {581--585},
title = {{Instlistener: An Expressive Parameter Estimation System Imitating Human Performances of Monophonic Musical Instruments}},
volume = {c},
year = {2018}
}
@book{Benade1977FundamentalsAcoustics,
author = {Rowan, Larry and Benade, Arthur H.},
booktitle = {Notes},
doi = {10.2307/897685},
file = {:Users/huanzhang/Downloads/Fundamentals of Musical Acoustics (Arthur H. Benade) (Z-Library).epub:epub},
issn = {00274380},
number = {4},
pages = {837},
title = {{Fundamentals of Musical Acoustics}},
volume = {33},
year = {1977}
}
@inproceedings{Liub,
abstract = {Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training. Audio samples can be found at https://voicebox.metademolab.com/speechflow.html.},
archivePrefix = {arXiv},
arxivId = {2310.16338},
author = {Liu, Alexander H and Le, Matt and Vyas, Apoorv and Shi, Bowen and Tjandra, Andros and Hsu, Wei Ning},
booktitle = {12th International Conference on Learning Representations, ICLR 2024},
eprint = {2310.16338},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - GENERATIVE PRE-TRAINING FOR SPEECH WITH FLOW MATCHING(2).pdf:pdf},
title = {{GENERATIVE PRE-TRAINING FOR SPEECH WITH FLOW MATCHING}},
url = {https://voicebox.metademolab.com/speechflow.html.},
year = {2024}
}
@article{Won2020Data-DrivenLearning,
abstract = {We introduce a trainable front-end module for audio representation learning that exploits the inherent harmonic structure of audio signals. The proposed architecture, composed of a set of filters, compels the subsequent network to capture harmonic relations while preserving spectro-temporal locality. Since the harmonic structure is known to have a key role in human auditory perception, one can expect these harmonic filters to yield more efficient audio representations. Experimental results show that a simple convolutional neural network back-end with the proposed front-end outperforms state-of-the-art baseline methods in automatic music tagging, keyword spotting, and sound event tagging tasks.},
author = {Won, Minz and Chun, Sanghyuk and Nieto, Oriol and Serrc, Xavier},
doi = {10.1109/ICASSP40776.2020.9053669},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Won et al. - 2020 - Data-Driven Harmonic Filters for Audio Representation Learning.pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Harmonic filters,audio representation learning,deep learning},
pages = {536--540},
title = {{Data-Driven Harmonic Filters for Audio Representation Learning}},
volume = {2020-May},
year = {2020}
}
@article{Dowling1991,
abstract = {In a continuous-ronning-memory task, subjects heard novel seven-note melodies that were tested after delays of 11 sec (empty) or 39 sec (filled). Test items were transposed to new pitch levels (to moderately distant keys in the musical sense) and included exact transpositions (targets), same-contour lures with altered pitch intervals, and new-contour lures. Melodies differed in tonal strength (degree of conformity to a musical key) and were tonally strong, tonally weak, or atonal. False alarms to same-contour lures decreased over the longer delay period, but only for tonal stimuli. In agreement with previous studies, discrimination of detailed changes in pitch intervals improved with increased delay, whereas discrimination of more global contour information declined, again only for tonal stimuli. These results suggest that poor short-delay performance in rejecting same-contour lures arises from confusion that is based on the similarity of tonality between standard stimuli and lures. If a test item has the same contour and a similar tonality to a just-presented item, subjects tend to accept it. After a delay filled with melodies in other tonalities, the salience of key information recedes, and subjects base their judgments on more detailed pattern information (namely, exact pitch intervals). The fact that tonality affects judgments of melodic contour indicates that contour is not an entirely separable feature of melodies but rather that a melody with its contour constitutes an integrated perceptual whole. Listeners are often confused in melody recognition between transpositions of novel melodies and lures that copy the contour (the ups and downs) of the target but not its interval sizes. For example, immediately after hearing the melody in Figure lA, listeners tend to judge the melody in Figure lB to be an exact transposition of Figure lA, even though two pitches (and three intervals) have been altered in Figure lB from those of an exact transposition (as indicated by the bracket). This confusion is particularly strong when the transposition is to a closely related key in the musical sense; that is, when the musical scale underlying the transposition is very similar to the scale underlying the original melody (Bartlett & Dowling, 1980). The confusion of melodies that are different in in-tervallic detail but are similar in contour and scale suggests that contour and scale are important features in memory for melodies (Dowling, 1978). Figure 2 elaborates on the notion of scale and contour as important features of the pitch material of melodies. In the figure, parallel sets of features go from local characteristics of single melodies (in this case, the theme from Bach's "Little" G-minor Fugue, BWV 578) to global in-variants that hold across sets of melodies, for example, the set of transpositions and contour-preserving imitations I thank Se-Yeul Kwak for assistance in generating the stimuli and Melinda Andrews and James C. Bartlett for helpful comments on the design and the manuscript. Requests for reprints should be addressed to W. Jay Dowling, Program in Applied Cognition and Neuroscience, University of Texas at Dallas, Richardson, TX 75083-0688. of the fugue theme found in Bach's piece. (An "imita-tion" preserves pitch contour but not the exact interval sizes of a transposition.) The left side of Figure 2 shows successive abstractions from the interval pattern of Bach's melody: the contour (the pattern of ups and downs), the slope of the contour (the succession of melodic trajecto-ries), and the set of contour inflections (or changes in slope). The right side of Figure 2 shows successive abstractions from the pitch pattern of Bach's melody: the key (the hierarchically ordered set of pitches), the tuning system (the unordered pitch set of the key) with its associated modes (or alternate hierarchical orderings), and the tonal scheme (the abstracted pitch pattern of the tuning system, which is not tied to anyone pitch class). As Deutsch (1969) suggested, there is evidence for the cog-nitive relevance and separateness of each of these parallel schemes of invariants based on pitch and interval. As an example of the effects of features toward the bottom of the two columns, Cuddy, Cohen, and Mewhort (1981) demonstrated that both contour complexity (in the sense of number of contour inflections) and harmonic complexity (in the sense of key-region relationships within a melody) were important determinants of transposition recognition. The confusion of transpositions with same-eontour lures arises from the listener's reliance on contour and scale features and lies at the heart of two effects in recognition memory for melodies: the key-distance effect (mentioned above) and the shift of importance of contour and intervals in recognition with the passage of time. Key distance refers to the similarity of scales underlying a pair of melo-305},
annote = {From Duplicate 2 (Tonal strength and melody recognition after long and short delays - Dowling, W Jay)

Week 6

7 note melodies
exact transpositions
same contour, altered pitch intervals
new contour 

tonal strength: strong, weak, atonal

tonality affects jugments of melodic contour},
author = {Dowling, W Jay},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dowling - 1991 - Tonal strength and melody recognition after long and short delays.pdf:pdf},
journal = {Perception &: Psychophysics},
number = {4},
pages = {305--313},
title = {{Tonal strength and melody recognition after long and short delays}},
volume = {50},
year = {1991}
}
@article{Chew2022,
abstract = {This position paper makes the case for an innovative, multi-disciplinary methodological approach to advance knowledge on the nature and work of music performance, driven by a novel experiential perspective, that also benefits analysis of electrocardiographic sequences. Music performance is considered by many to be one of the most breathtaking feats of human intelligence. It is well accepted that music performance is a creative act, but the nature of its work remains elusive. Taking the view of performance as an act of creative problem solving, ideas in citizen science and data science, optimization, and computational thinking provide means through which to deconstruct the process of music performance in scalable ways. The method tackles music expression's lack of notation-based data by leveraging listeners' perception and experience of the structures elicited by the performer, with implications for data collection and processing. The tools offer ways to parse a musical sequence into coherent structures, to design a performance, and to explore the space of possible interpretations of the musical sequence. These ideas and tools can be applied to other music-like sequences such as electrocardiographic recordings of arrhythmias (abnormal heart rhythms). Leveraging musical thinking and computational approaches to performance analysis, variations in expressions of cardiac arrhythmias can be more finely characterized, with implications for tailoring therapies and stratifying heart rhythm disorders.},
author = {Chew, Elaine},
doi = {10.3389/fpsyg.2022.527539},
file = {:Users/huanzhang/Downloads/fpsyg-13-527539.pdf:pdf},
journal = {Frontiers in Psychology},
keywords = {cardiac,citizen science,computational modeling,music performance,music performance, computational modeling, technol,musical structure,technology},
number = {May},
title = {{COSMOS: Computational Shaping and Modeling of Musical Structures}},
volume = {13},
year = {2022}
}
@inproceedings{Hiraga2002,
author = {Hiraga, Rumi and Mitsuyo, Hashida and Keiji, Hirata and Katayose, Haruhiro and Noike, Kenzi},
booktitle = {Proceedings of the 28th International Computer Music Conference (ICMC)},
file = {:Users/huanzhang/Downloads/bbp2372.2002.072.pdf:pdf},
title = {{RENCON: toward a new evaluation method for performance rendering systems}},
year = {2002}
}
@article{Team2024,
author = {Team, Llama and Meta, A I},
file = {:Users/huanzhang/Downloads/452387774_1036916434819166_4173978747091533306_n.pdf:pdf},
title = {{The Llama 3 Herd of Models}},
year = {2024}
}
@article{Duan2019AudiovisualField,
abstract = {In the physical sciences and engineering domains, music has traditionally been considered an acoustic phenomenon. From a perceptual viewpoint, music is naturally associated with hearing, i.e., the audio modality. Moreover, for a long time, the majority of music recordings were distributed through audio-only media, such as vinyl records, cassettes, compact discs, and mp3 files. As a consequence, existing automated music analysis approaches predominantly focus on audio signals that represent information from the acoustic rendering of music.},
author = {Duan, Zhiyao and Essid, Slim and Liem, Cynthia C.S. S and Richard, Gael and Sharma, Gaurav},
doi = {10.1109/MSP.2018.2875511},
file = {:Users/huanzhang/Downloads/Audiovisual_Analysis_of_Music_Performances_Overview_of_an_Emerging_Field.pdf:pdf},
issn = {15580792},
journal = {IEEE Signal Processing Magazine},
number = {1},
pages = {63--73},
title = {{Audiovisual Analysis of Music Performances: Overview of an Emerging Field}},
volume = {36},
year = {2019}
}
@misc{,
file = {:Users/huanzhang/Downloads/average_pedal.png:png},
title = {average_pedal}
}
@article{2022CouturierDatasetSonatas,
author = {Couturier, Louis and Bigo, Louis and Lev{\'{e}}, Florence and Picardie, Universit{\'{e}} De and Verne, Jules and Cnrs, U M R and Lille, Universit{\'{e}} De},
file = {:Users/huanzhang/Downloads/236.pdf:pdf},
journal = {Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)},
title = {{A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas}},
year = {2022}
}
@article{Mazzola2002,
abstract = {Performance is a substantial component of the musical work, adding the physical reality to “fictitious” score symbols and the power of sensorial evidence to the interpretative reflection of analysis. We first discuss the theory of musical interpretation and its performance. Performance transformations from symbolic to physical reality are described by performance vector fields, a generalization of tempo curves. Such fields are shaped by specific performance operators the arguments of which are weight functions issued from music analyses. The action of performance fields is stratified according to hierarchies of musical parameters, such as onset, duration, pitch, loudness, or more complex parameters of instrument and gesture. Performance also unfolds according to the genealogical refinement while rehearsing, and is realized in the mathematical stemma model and implemented in the performance software RUBATO {\textregistered}. We also present inverse performance theory, which deals with the reconstruction of system parameters that lead to a given performance. More generally speaking, performance may be viewed as a transformation of any symbolic data into its sensorial representation in the ordinary human spacetime. We discuss this extension in the case of “multimedia performance of knowledge” as it occurs in the visualization and auralization of musical and other databases, such as scores or libraries. This subject is a central feature in the ongoing implementation of the Distributed RUBATO {\textregistered} software for music representation, analysis, composition, and performance. {\textcopyright} 2002, Copyright Taylor & Francis Group, LLC.},
author = {Mazzola, Guerino and G{\"{o}}ller, Stefan},
doi = {10.1076/jnmr.31.3.221.14190},
file = {:Users/huanzhang/Downloads/Performance and Interpretation.pdf:pdf},
issn = {15497879},
journal = {International Journal of Phytoremediation},
number = {1},
pages = {221--232},
title = {{Performance and interpretation}},
volume = {21},
year = {2002}
}
@article{Giraud2020TowardsSpaces,
author = {Giraud, Mathieu},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Giraud - 2020 - Towards Custom Dilated Convolutions on Pitch Spaces.pdf:pdf},
pages = {4--6},
title = {{Towards Custom Dilated Convolutions on Pitch Spaces}},
year = {2020}
}
@article{Dhariwal2020JukeboxMusic,
abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
archivePrefix = {arXiv},
arxivId = {2005.00341},
author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
eprint = {2005.00341},
file = {:Users/huanzhang/Downloads/2005.00341.pdf:pdf},
journal = {Computing Research Repository (CoRR)},
title = {{Jukebox: A Generative Model for Music}},
url = {http://arxiv.org/abs/2005.00341},
year = {2020}
}
@article{Kwon2019,
abstract = {We propose a framework for audio-to-score alignment on piano performance that employs automatic music transcription (AMT) using neural networks. Even though the AMT result may contain some errors, the note prediction output can be regarded as a learned feature representation that is directly comparable to MIDI note or chroma representation. To this end, we employ two recurrent neural networks that work as the AMT-based feature extractors to the alignment algorithm. One predicts the presence of 88 notes or 12 chroma in frame-level and the other detects note onsets in 12 chroma. We combine the two types of learned features for the audio-to-score alignment. For comparability, we apply dynamic time warping as an alignment algorithm without any additional post-processing. We evaluate the proposed framework on the MAPS dataset and compare it to previous work. The result shows that the alignment framework with the learned features significantly improves the accuracy, achieving less than 10 ms in mean onset error.},
annote = {From Duplicate 1 (Audio-to-score alignment of piano music using RNN-based automatic music transcription - Kwon, Taegyun; Jeong, Dasaem; Nam, Juhan)

not real time 
DTW of [audio chroma onset] and [midi chroma onset]},
archivePrefix = {arXiv},
arxivId = {1711.04480},
author = {Kwon, Taegyun and Jeong, Dasaem and Nam, Juhan},
eprint = {1711.04480},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kwon, Jeong, Nam - 2019 - Audio-to-score alignment of piano music using RNN-based automatic music transcription.pdf:pdf},
isbn = {9789526037295},
journal = {Proceedings of the 14th Sound and Music Computing Conference 2017, SMC 2017},
pages = {380--385},
title = {{Audio-to-score alignment of piano music using RNN-based automatic music transcription}},
year = {2019}
}
@article{Xie2024,
abstract = {Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.},
archivePrefix = {arXiv},
arxivId = {2402.15116},
author = {Xie, Junlin and Chen, Zhihong and Zhang, Ruifei and Wan, Xiang and Li, Guanbin},
eprint = {2402.15116},
file = {:Users/huanzhang/Downloads/2402.15116.pdf:pdf},
title = {{Large Multimodal Agents: A Survey}},
url = {http://arxiv.org/abs/2402.15116},
year = {2024}
}
@article{Cancino-Chacon2017AnMusic,
abstract = {Expressive interpretation forms an important but complex aspect of music, particularly in Western classical music. Modeling the relation between musical expression and structural aspects of the score being performed is an ongoing line of research. Prior work has shown that some simple numerical descriptors of the score (capturing dynamics annotations and pitch) are effective for predicting expressive dynamics in classical piano performances. Nevertheless, the features have only been tested in a very simple linear regression model. In this work, we explore the potential of non-linear and temporal modeling of expressive dynamics. Using a set of descriptors that capture different types of structure in the musical score, we compare linear and different non-linear models in a large-scale evaluation on three different corpora, involving both piano and orchestral music. To the best of our knowledge, this is the first study where models of musical expression are evaluated on both types of music. We show that, in addition to being more accurate, non-linear models describe interactions between numerical descriptors that linear models do not.},
author = {Cancino-Chac{\'{o}}n, Carlos Eduardo and Gadermaier, Thassilo and Widmer, Gerhard and Grachten, Maarten},
doi = {10.1007/s10994-017-5631-y},
file = {:Users/huanzhang/Downloads/s10994-017-5631-y.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
number = {6},
title = {{An evaluation of linear and non-linear models of expressive dynamics in classical piano and symphonic music}},
volume = {106},
year = {2017}
}
@article{Draft--a,
author = {Draft--, Manuscript},
file = {:Users/huanzhang/Downloads/JASM-D-23-00084_R1.pdf:pdf},
keywords = {music rearrangement,symbolic music processing,token representation},
title = {{EURASIP Journal on Audio , Speech , and Music Processing Piano score rearrangement into multiple difficulty levels via notation-to-notation approach}}
}
@article{Chen2022a,
abstract = {The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.},
archivePrefix = {arXiv},
arxivId = {2212.09058},
author = {Chen, Sanyuan and Wu, Yu and Wang, Chengyi and Liu, Shujie and Tompkins, Daniel and Chen, Zhuo and Wei, Furu},
eprint = {2212.09058},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2022 - BEATs Audio Pre-Training with Acoustic Tokenizers.pdf:pdf},
month = {dec},
title = {{BEATs: Audio Pre-Training with Acoustic Tokenizers}},
url = {http://arxiv.org/abs/2212.09058},
year = {2022}
}
@article{Mirzadeh2019ImprovedAssistant,
abstract = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
archivePrefix = {arXiv},
arxivId = {1902.03393},
author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
doi = {10.1609/aaai.v34i04.5963},
eprint = {1902.03393},
file = {:Users/huanzhang/Downloads/5963-Article Text-9188-1-10-20200513.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Machine Learning},
title = {{Improved knowledge distillation via teacher assistant}},
year = {2019}
}
@article{Real2018RegularizedSearch,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier-AmoebaNet-A-that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
file = {:Users/huanzhang/Downloads/4405-Article Text-7444-1-10-20190706.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {evolutionary algorithms,neural networks,neuro-ev},
title = {{Regularized evolution for image classifier architecture search}},
year = {2018}
}
@inproceedings{Fradet2023ByteMusic,
abstract = {The symbolic music modality is nowadays mostly represented as discrete and used with sequential models such as Transformers, for deep learning tasks. Recent research put efforts on the tokenization, i.e. the conversion of data into sequences of integers intelligible to such models. This can be achieved by many ways as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations are based on small vocabularies describing the note attributes and time events, resulting in fairly long token sequences. In this paper, we show how Byte Pair Encoding (BPE) can improve the results of deep learning models while improving its performances. We experiment on music generation and composer classification, and study the impact of BPE on how models learn the embeddings, and show that it can help to increase their isotropy, i.e., the uniformity of the variance of their positions in the space.},
annote = {isotropy: uniformity of the variance of their positions in the space

Embedding pooling: merging the embeddings of several distinct tokens with pooling (concat+projection)

BPE: gradually shrink the recurring subsequences into new tokens

Experiment:
model: transformer
tokenization: REMI + TSD
task: generation (continuation of 1k tokens) and composer recognition},
archivePrefix = {arXiv},
arxivId = {2301.11975},
author = {Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien and Seghrouchni, Amal El Fallah and Gutowski, Nicolas},
eprint = {2301.11975},
file = {:Users/huanzhang/Downloads/2301.11975.pdf:pdf},
title = {{Byte Pair Encoding for Symbolic Music}},
url = {http://arxiv.org/abs/2301.11975},
year = {2023}
}
@article{Miyazaki1988,
annote = {From Duplicate 2 (Musical pitch identification by absolute pitch possessors - Miyazaki, Ichi)

Week 2

tone chroma is considered equivalent to tonality, the musical property specific to each tone in a musical context},
author = {Miyazaki, Ichi},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Miyazaki - 1988 - Musical pitch identification by absolute pitch possessors.pdf:pdf},
journal = {Perceptton& Psychophystcs},
number = {6},
pages = {501--512},
title = {{Musical pitch identification by absolute pitch possessors}},
volume = {44},
year = {1988}
}
@article{Jonason2023a,
abstract = {We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.},
archivePrefix = {arXiv},
arxivId = {2311.10384},
author = {Jonason, Nicolas and Casini, Luca and Thom{\'{e}}, Carl and Sturm, Bob L. T.},
eprint = {2311.10384},
file = {:Users/huanzhang/Downloads/2311.10384.pdf:pdf},
pages = {2--4},
title = {{Retrieval Augmented Generation of Symbolic Music with LLMs}},
url = {http://arxiv.org/abs/2311.10384},
year = {2023}
}
@article{Ghosh2024,
abstract = {Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning and instruction following capabilities.},
archivePrefix = {arXiv},
arxivId = {2406.11768},
author = {Ghosh, Sreyan and Kumar, Sonal and Seth, Ashish and Evuru, Chandra Kiran Reddy and Tyagi, Utkarsh and Sakshi, S and Nieto, Oriol and Duraiswami, Ramani and Manocha, Dinesh},
eprint = {2406.11768},
file = {:Users/huanzhang/Downloads/2406.11768v1.pdf:pdf},
title = {{GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities}},
url = {http://arxiv.org/abs/2406.11768},
year = {2024}
}
@article{Interiano2018,
abstract = {We analyse more than 500 000 songs released in the UK between 1985 and 2015 to understand the dynamics of success (defined as ‘making it' into the top charts), correlate success with acoustic features and explore the predictability of success. Several multi-decadal trends have been uncovered. For example, there is a clear downward trend in ‘happiness' and ‘brightness', as well as a slight upward trend in ‘sadness'. Furthermore, songs are becoming less ‘male'. Interestingly, successful songs exhibit their own distinct dynamics. In particular, they tend to be ‘happier', more ‘party-like', less ‘relaxed' and more ‘female' than most. The difference between successful and average songs is not straightforward. In the context of some features, successful songs pre-empt the dynamics of all songs, and in others they tend to reflect the past. We used random forests to predict the success of songs, first based on their acoustic features, and then adding the ‘superstar' variable (informing us whether the song's artist had appeared in the top charts in the near past). This allowed quantification of the contribution of purely musical characteristics in the songs' success, and suggested the time scale of fashion dynamics in popular music.},
author = {Interiano, Myra and Kazemi, Kamyar and Wang, Lijia and Yang, Jienian and Yu, Zhaoxia and Komarova, Natalia L.},
doi = {10.1098/rsos.171274},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Complex social dynamics,Music evolution,Temporal trends},
number = {5},
title = {{Musical trends and predictability of success in contemporary songs in and out of the top charts}},
url = {http://dx.doi.org/10.1098/rsos.171274},
volume = {5},
year = {2018}
}
@article{Oh2023,
archivePrefix = {arXiv},
arxivId = {arXiv:2307.04292v1},
author = {Oh, Sangshin and Kang, Minsung and Moon, Hyeongi and Choi, Keunwoo and Chon, Ben Sangbae},
eprint = {arXiv:2307.04292v1},
file = {:Users/huanzhang/Downloads/2307.04292.pdf:pdf},
number = {Icml},
title = {{A Demand-Driven Perspective on Generative Audio AI}},
year = {2023}
}
@article{Chawin2021Sliding-WindowForm,
author = {Chawin, Dror and Rom, Uri B.},
doi = {10.5334/tismir.83},
file = {:Users/huanzhang/Downloads/83-2945-1-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {beethoven,mozart,musical form,pitch-class histograms,sliding window},
number = {1},
pages = {223--235},
title = {{Sliding-Window Pitch-Class Histograms as a Means of Modeling Musical Form}},
volume = {4},
year = {2021}
}
@article{Togami2020KULLBACK-LEIBLERJapan,
author = {Togami, Masahito and Masuyama, Yoshiki and Komatsu, Tatsuya and Nakagome, Yu},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Togami et al. - 2020 - KULLBACK-LEIBLER DIVERGENCE BASED PROBABILISTIC LOSS FUNCTION LINE Corporation , Tokyo , Japan.pdf:pdf},
isbn = {9781509066315},
journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {56--60},
publisher = {IEEE},
title = {{KULLBACK-LEIBLER DIVERGENCE BASED PROBABILISTIC LOSS FUNCTION LINE Corporation , Tokyo , Japan}},
year = {2020}
}
@article{Raffel2020,
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. 1},
archivePrefix = {arXiv},
arxivId = {1910.10683v3},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
eprint = {1910.10683v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
pages = {1--67},
title = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
url = {http://jmlr.org/papers/v21/20-074.html.},
volume = {21},
year = {2020}
}
@article{Bassan,
abstract = {Symbolic music segmentation is the process of dividing symbolic melodies into smaller meaningful groups, such as melodic phrases. We proposed an unsupervised method for segmenting symbolic music. The proposed model is based on an ensemble of temporal prediction error models. During training, each model predicts the next token to identify musical phrase changes. While at test time, we perform a peak detection algorithm to select segment candidates. Finally, we aggregate the predictions of each of the models participating in the ensemble to predict the final segmentation. Results suggest the proposed method reaches state-of-the-art performance on the Essen Folksong dataset under the unsupervised setting when considering F-Score and R-value. We additionally provide an ablation study to better assess the contribution of each of the model components to the final results. As expected, the proposed method is inferior to the supervised setting, which leaves room for improvement in future research considering closing the gap between unsupervised and supervised methods.},
archivePrefix = {arXiv},
arxivId = {2207.00760v1},
author = {Bassan, Shahaf and Adi, Yossi and Rosenschein, Jeffrey S},
eprint = {2207.00760v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bassan, Adi, Rosenschein - Unknown - Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors.pdf:pdf},
keywords = {Index Terms: symbolic music,sequence segmentation,tempo-ral prediction},
title = {{Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors}}
}
@article{Repp1996ArtHear,
author = {Repp, Bruno H.},
file = {:Users/huanzhang/Downloads/40285716.pdf:pdf},
journal = {Music Perception: An Interdisciplinary Journal},
number = {2},
pages = {161--183},
title = {{The Art of Inaccuracy: Why Pianists' Errors Are Difficult to Hear}},
volume = {14},
year = {1996}
}
@inproceedings{Sapp2007ComparativePerformances,
abstract = {A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible influence between performances.},
author = {Sapp, Craig Stuart},
booktitle = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sapp, Holloway - Unknown - COMPARATIVE ANALYSIS OF MULTIPLE MUSICAL PERFORMANCES.pdf:pdf},
title = {{Comparative Analysis of Multiple Performances}},
year = {2007}
}
@techreport{Guo2020BreakingSearch,
abstract = {Neural architecture search (NAS) has become an important approach to automatically find effective architectures. To cover all possible good archi-tectures, we need to search in an extremely large search space with billions of candidate architec-tures. More critically, given a large search space, we may face a very challenging issue of space explosion. However, due to the limitation of computational resources, we can only sample a very small proportion of the architectures, which provides insufficient information for the training. As a result, existing methods may often produce sub-optimal architectures. To alleviate this issue, we propose a curriculum search method that starts from a small search space and gradually incorporates the learned knowledge to guide the search in a large space. With the proposed search strategy , our Curriculum Neural Architecture Search (CNAS) method significantly improves the search efficiency and finds better architectures than existing NAS methods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {2007.07197v2},
author = {Guo, Yong and Chen, Yaofo and Zheng, Yin and Zhao, Peilin and Chen, Jian and Huang, Junzhou and Tan, Mingkui},
eprint = {2007.07197v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2020 - Breaking the Curse of Space Explosion Towards Efficient NAS with Curriculum Search.pdf:pdf},
title = {{Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search}},
year = {2020}
}
@article{Rifat2020,
author = {Rifat, Syed and Rafee, Mahmud},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rifat, Rafee - 2020 - No Title.pdf:pdf},
title = {{No Title}},
year = {2020}
}
@inproceedings{Jiang2023ExpertFeedback,
author = {Jiang, Yucong},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/ISMIR2023_Jiang_final.pdf:pdf},
title = {{Expert and Novice Evaluations of Piano Performances : Criteria for Computer-Aided Feedback}},
year = {2023}
}
@article{Jonason,
abstract = {We present work in progress on TimbreCLIP, an audio-text cross modal embedding trained on single instrument notes. We evaluate the models with a cross-modal retrieval task on synth patches. Finally, we demonstrate the application of TimbreCLIP on two tasks: text-driven audio equalization and timbre to image generation.},
archivePrefix = {arXiv},
arxivId = {2211.11225v1},
author = {Jonason, Nicolas and Sturm, Bob L T},
eprint = {2211.11225v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jonason, Sturm - Unknown - TimbreCLIP Connecting Timbre to Text and Images(2).pdf:pdf},
title = {{TimbreCLIP: Connecting Timbre to Text and Images}},
url = {https://www.arturia.com/products/analog-classics/v-}
}
@article{Yuan2023,
abstract = {Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks.},
archivePrefix = {arXiv},
arxivId = {2309.08051},
author = {Yuan, Yi and Liu, Haohe and Liu, Xubo and Huang, Qiushi and Plumbley, Mark D. and Wang, Wenwu},
eprint = {2309.08051},
file = {:Users/huanzhang/Downloads/2309.08051.pdf:pdf},
title = {{Retrieval-Augmented Text-to-Audio Generation}},
url = {http://arxiv.org/abs/2309.08051},
year = {2023}
}
@article{Sundberg2010AttemptsRules,
abstract = {The Director Musices generative grammar of music performance is a system of context dependent rules that automatically introduces expressive deviation in performances of input score files. A number of these rule concern timing. In this investigation the ability of such rules to reproduce a professional pianist's timing deviations from nominal note inter-onset-intervals is examined. Rules affecting tone inter-onset-intervals were first tested one by one for the various sections of the excerpt, and then in combinations. Results were evaluated in terms of the correlation between the deviations made by the pianist and by the rule system. It is found that rules reflecting the phrase structure produced high correlations in some sections. On the other hand, some rules failed to produce significant correlation with the pianist's deviations, and thus seemed irrelevant to the particular performance analysed. It is concluded that phrasing was a prominent principle in this performance and that rule combinations have to change between sections in order to match this pianist's deviations.},
author = {Sundberg, Johan and Friberg, Anders and Bresin, Roberto},
doi = {10.1076/jnmr.32.3.317.16867},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sundberg, Friberg, Bresin - 2010 - Journal of New Music Research Attempts to Reproduce a Pianist's Expressive Timing with Director Music.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
title = {{Attempts to Reproduce a Pianist's Expressive Timing with Director Musices Performance Rules}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2010}
}
@techreport{Ventura2018VoiceApproach,
abstract = {Voice Separation is a delicate stage in a music information retrieval process intended to be used in the automated music analysis processes through textual segmentation or for the indexation of a music score. This article presents a method that is capable of separating polyphonic music, considered in its symbolic aspect, into its individual parts (or voices). This method considers every single note as an individual entity and assigns it to the part (or voice) where the information content that it assumes in relation to the already-existing notes of the same score is maximum. The algorithm may separate the voices identifying them even in the points that intersect. The algorithm was tested against a handful of musical works that were carefully selected from the repertoire of Bach and of Mendelssohn.},
author = {Ventura, Michele Della},
booktitle = {IFIP Advances in Information and Communication Technology},
doi = {10.1007/978-3-319-92007-8{\_}54},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jordanous - Unknown - VOICE SEPARATION IN POLYPHONIC MUSIC A DATA-DRIVEN APPROACH.pdf:pdf},
isbn = {9783319920061},
issn = {18684238},
keywords = {Entropy,Information,Music segmentation,Overlapping musical notes,Voice separation},
pages = {638--646},
title = {{Voice separation in polyphonic music: Information theory approach}},
url = {http://kern.humdrum.net},
volume = {519},
year = {2018}
}
@article{Hawthrone2018TransformerPerformances,
abstract = {Self-attention based Transformers are compelling sequence models because they can capture relatively long-term dependencies by having direct access to the past. However, their memory requirements grow quadratically with sequence length, making it prohibitive to model long sequences with global attention. In contrast to previous representations of music that "flatten" a note's performance attributes such as velocity, note on, and note off into a single sequence, we reduce sequence length by proposing a new representation, NoteTuple, which groups a note's attributes as one event. This makes it natural to factorize a musical performance into a sequence of notes and model a note as a NADE on its attributes. The resulting models require fewer parameters and have faster generation. NoteTuples is a promising extension to Music Transformer (Huang et al., 2018b), enabling rich downstream tasks such as infilling of piano performances, e.g., Ippolito et al. (2018). Samples can be heard at https://goo.gl/magenta/notetuple-examples.},
archivePrefix = {arXiv},
arxivId = {1809.04281},
author = {Hawthorne, Curtis and Huang, Anna and Ippolito, Daphne and Eck, Douglas},
eprint = {1809.04281},
file = {:Users/huanzhang/Downloads/Transformer_NADE.pdf:pdf},
journal = {32nd Conference on Neural Information Processing Systems (NIPS 2018)},
number = {Nips},
title = {{Transformer-NADE for Piano Performances}},
url = {https://goo.gl/magenta/notetuple-examples.},
year = {2018}
}
@article{Won2020EvaluationModels,
abstract = {Recent advances in deep learning accelerated the development of content-based automatic music tagging systems. Music information retrieval (MIR) researchers proposed various architecture designs, mainly based on convolutional neural networks (CNNs), that achieve state-of-the-art results in this multi-label binary classification task. However, due to the differences in experimental setups followed by researchers, such as using different dataset splits and software versions for evaluation, it is difficult to compare the proposed architectures directly with each other. To facilitate further research, in this paper we conduct a consistent evaluation of different music tagging models on three datasets (MagnaTagATune, Million Song Dataset, and MTG-Jamendo) and provide reference results using common evaluation metrics (ROC-AUC and PR-AUC). Furthermore, all the models are evaluated with perturbed inputs to investigate the generalization capabilities concerning time stretch, pitch shift, dynamic range compression, and addition of white noise. For reproducibility, we provide the PyTorch implementations with the pre-trained models.},
archivePrefix = {arXiv},
arxivId = {2006.00751},
author = {Won, Minz and Ferraro, Andres and Bogdanov, Dmitry and Serra, Xavier},
eprint = {2006.00751},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Won et al. - 2020 - Evaluation of CNN-based Automatic Music Tagging Models.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Evaluation of CNN-based Automatic Music Tagging Models}},
year = {2020}
}
@inproceedings{Cheuk2023DiffrollCapability,
abstract = {In this paper we propose a novel generative approach, DiffRoll, to tackle automatic music transcription (AMT). Instead of treating AMT as a discriminative task in which the model is trained to convert spectrograms into piano rolls, we think of it as a conditional gen-erative task where we train our model to generate realistic looking piano rolls from pure Gaussian noise conditioned on spectrograms. This new AMT formulation enables DiffRoll to transcribe, generate and even inpaint music. Due to the classifier-free nature, DiffRoll is also able to be trained on unpaired datasets where only piano rolls are available. Our experiments show that DiffRoll outperforms its discriminative counterpart by 19 percentage points (ppt.) and our ablation studies also indicate that it outperforms similar existing methods by 4.8 ppt.Source code and demonstration are available at https://sony.github.io/DiffRoll/.},
address = {Rhodes Island, Greece},
author = {Cheuk, Kin Wai and Sawata, Ryosuke and Uesaka, Toshimitsu and Murata, Naoki and Takahashi, Naoya and Takahashi, Shusuke and Herremans, Dorien and Mitsufuji, Yuki},
booktitle = {Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cheuk et al. - Unknown - DIFFROLL DIFFUSION-BASED GENERATIVE MUSIC TRANSCRIPTION WITH UNSUPERVISED PRETRAINING CAPABILITY.pdf:pdf},
keywords = {Diffusion Model,Generative Model,Index Terms-Automatic Music Transcription,Music Infor-mation Retrieval,Signal Processing,Unsupervised Pretraining},
title = {{{DiffRoll}: DIFFUSION-BASED GENERATIVE MUSIC TRANSCRIPTION WITH UNSUPERVISED PRETRAINING CAPABILITY}},
url = {https://sony.github.io/DiffRoll/.},
year = {2023}
}
@article{Cancino-Chacon,
abstract = {In this work, we present some preliminary results about quantifying differences in expressive piano performances in the context of computational generative models.},
author = {Cancino-Chac{\'{o}}n, Carlos and Peter, Silvan and Karystinaios, Emmanouil and Widmer, Gerhard},
doi = {10.3389/fdigh.2018.00025},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cancino-Chac{\'{o}}n et al. - Unknown - Towards Quantifying Differences in Expressive Piano Performances Are Euclidean-like Distance Measures.pdf:pdf},
title = {{Towards Quantifying Differences in Expressive Piano Performances: Are Euclidean-like Distance Measures Enough? *}},
url = {https://www.frontiersin.org/article/10.3389/fdigh.2018.00025}
}
@techreport{Jure2012PitchAnalysis,
abstract = {This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is briefly reviewed. Its application to music analysis is exemplified with two pieces of non-notated music: a field recording of a folkloric form of polyphonic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting complex pitch evolution, difficult to analyze and notate with precision using Western common music notation. By using novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difficult or impossible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional mu-sicological analysis. Two software tools are released that allow the practical use of the described methods. {\textcopyright} 2012 International Society for Music Information Retrieval.},
author = {Jure, Luis and Lopez, Ernesto and Rocamora, Mart{\'{i}}n and Cancela, Pablo and Sponton, Haldo and Irigaray, Ignacio},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jure et al. - Unknown - PITCH CONTENT VISUALIZATION TOOLS FOR MUSIC PERFORMANCE ANALYSIS.pdf:pdf},
isbn = {9789727521449},
pages = {493--498},
title = {{Pitch content visualization tools for music performance analysis}},
url = {http://www.sonicvisualiser.org/},
year = {2012}
}
@article{John2020,
abstract = {This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches1.},
archivePrefix = {arXiv},
arxivId = {1808.04339},
author = {John, Vineet and Mou, Lili and Bahuleyan, Hareesh and Vechtomova, Olga},
doi = {10.18653/v1/p19-1041},
eprint = {1808.04339},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/John et al. - Unknown - Disentangled Representation Learning for Non-Parallel Text Style Transfer.pdf:pdf},
isbn = {9781950737482},
journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
pages = {424--434},
title = {{Disentangled representation learning for non-parallel text style transfer}},
url = {https://sites.google.com/view/},
year = {2020}
}
@article{AsahiTowardBeginners,
abstract = {In piano learning, it is difficult especially for beginners to judge by themselves whether their musical performances are appropriate in terms of rhythm and melody. Therefore, we have been developing a piano practice support system, which enables piano beginners to conduct independent practice without their instructors. In this paper, we propose the system with the aid of a deep learning technique: Long Short-Term Memory (LSTM). Our system accepts raw piano sounds, extracting performance information. From these information, we evaluate performance. We evaluated the scheme using actual beginners' performances, and found the proposed system achieved better than previous conventional methods. This paper also presents an application employing our methods. Through subjective evaluation experiments for the proposed application, it turns out almost the all beginners found reflection points, and they maintained their motivation for independent practice.},
author = {Asahi, Shota and Tamura, Satoshi and Sugiyama, Yuko and Hayamizu, Satoru},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Asahi et al. - Unknown - Toward a High Performance Piano Practice Support System for Beginners.pdf:pdf},
isbn = {9789881476852},
title = {{Toward a High Performance Piano Practice Support System for Beginners}}
}
@inproceedings{Guo2023DomainModeling,
abstract = {Following the success of the transformer architecture in the natural language domain, transformer-like architectures have been widely applied to the domain of symbolic music recently. Symbolic music and text, however, are two different modalities. Symbolic music contains multiple attributes, both absolute attributes (e.g., pitch) and relative attributes (e.g., pitch interval). These relative attributes shape human perception of musical motifs. These important relative attributes, however, are mostly ignored in existing symbolic music modeling methods with the main reason being the lack of a musically-meaningful embedding space where both the absolute and relative embeddings of the symbolic music tokens can be efficiently represented. In this paper, we propose the Fundamental Music Embedding (FME) for symbolic music based on a bias-adjusted sinusoidal encoding within which both the absolute and the relative attributes can be embedded and the fundamental musical properties (e.g., translational invariance) are explicitly preserved. Taking advantage of the proposed FME, we further propose a novel attention mechanism based on the relative index, pitch and onset embeddings (RIPO attention) such that the musical domain knowledge can be fully utilized for symbolic music modeling. Experiment results show that our proposed model: RIPO transformer which utilizes FME and RIPO attention outperforms the state-of-the-art transformers (i.e., music transformer, linear transformer) in a melody completion task. Moreover, using the RIPO transformer in a downstream music generation task, we notice that the notorious degeneration phenomenon no longer exists and the music generated by the RIPO transformer outperforms the music generated by state-of-the-art transformer models in both subjective and objective evaluations.},
archivePrefix = {arXiv},
arxivId = {2212.00973},
author = {Guo, Z. and Kang, J. and Herremans, D.},
booktitle = {Proceedings of the 37th AAAI Conference on Artificial Intelligence},
eprint = {2212.00973},
file = {:Users/huanzhang/Downloads/2212.00973.pdf:pdf},
title = {{A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling}},
url = {http://arxiv.org/abs/2212.00973},
year = {2023}
}
@article{Rifat,
author = {Rifat, Syed and Rafee, Mahmud},
file = {:Users/huanzhang/Downloads/icassp.pdf:pdf},
title = {{HIPI : A HIERARCHICAL PERFORMER IDENTIFICATION MODEL BASED ON SYMBOLIC REPRESENTATION OF MUSIC}}
}
@article{Sako2014Ryry:Jumps,
abstract = {In this work, we propose an automatic accompaniment playback system called Ryry, which follows human performance and plays a corresponding accompaniment automatically, in an attempt to realize human-computer concerts. Recognizing and anticipating the score position in real-time, known as score following, by a computer is difficult. The proposed system is based on a robust on-line algorithm for real-time audio-to-score alignment. The algorithm is devised using a delayed-decision and anticipation framework by modeling real-time music performance that includes uncertainties such as tempo fluctuation and mistakes. We developed an automatic accompaniment system that is capable of generating polyphonic music signals. {\textcopyright} 2014 Springer International Publishing.},
author = {Sako, Shinji and Yamamoto, Ryuichi and Kitamura, Tadashi},
doi = {10.1007/978-3-319-09912-5{\_}12},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sako, Yamamoto, Kitamura - 2014 - Ryry A real-time score-following automatic accompaniment playback system capable of real performances.pdf:pdf},
isbn = {9783319099118},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automatic accompaniment,Chord transition,Linear Dynamic System (LDS),Score following,Segmental Conditional Random Fields (SCRFs)},
number = {C},
pages = {134--145},
title = {{Ryry: A real-time score-following automatic accompaniment playback system capable of real performances with errors, repeats and jumps}},
volume = {8610 LNCS},
year = {2014}
}
@techreport{LiuLeadNetwork,
abstract = {Research on automatic music generation has seen great progress due to the development of deep neural networks. However, the generation of multi-instrument music of arbitrary genres still remains a challenge. Existing research either works on lead sheets or multi-track piano-rolls found in MIDIs, but both musical notations have their limits. In this work, we propose a new task called lead sheet arrangement to avoid such limits. A new recurrent convolutional generative model for the task is proposed, along with three new symbolic-domain harmonic features to facilitate learning from unpaired lead sheets and MIDIs. Our model can generate lead sheets and their arrangements of eight-bar long. Audio samples of the generated result can be found at https://drive.google.com/open? id=1c0FfODTpudmLvuKBbc23VBCgQizY6-Rk Index Terms-Lead sheet arrangement, multi-track polyphonic music generation, conditional generative adversarial network},
archivePrefix = {arXiv},
arxivId = {1807.11161v1},
author = {Liu, Hao-Min and Yang, Yi-Hsuan},
eprint = {1807.11161v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Yang - Unknown - Lead Sheet Generation and Arrangement by Conditional Generative Adversarial Network.pdf:pdf},
title = {{Lead Sheet Generation and Arrangement by Conditional Generative Adversarial Network}},
url = {https://drive.google.com/open?}
}
@article{Chiu2012StudyMusic,
abstract = {Looking for a piano sheet music with proper difficulty for a piano learner is always an important work to his/her teacher. In the paper, we study on a new and challenging issue of recognizing the difficulty level of piano sheet music. To analyze the semantic content of music, we focus on symbolic music, i.e., sheet music or score. Specifically, difficulty level recognition is formulated as a regression problem to predict the difficulty level of piano sheet music. Since the existing symbolic music features are not able to capture the characteristics of difficulty, we propose a set of new features. To improve the performance, a feature selection approach, RReliefF, is used to select relevant features. An extensive performance study is conducted over two real datasets with different characteristics to evaluate the accuracy of the regression approach for predicting difficulty level. The best performance evaluated in terms of the R2 statistics over two datasets reaches 39.9% and 38.8%, respectively. {\textcopyright} 2012 IEEE.},
annote = {Datasets: 
PS: piano street, 
EN: not necessarily classical 
their difficulty levels are not matched... 

The features they designed are not best cp

MIDI may be performance midis and were quantized},
author = {Chiu, Shih Chuan and Chen, Min Syan},
doi = {10.1109/ISM.2012.11},
file = {:Users/huanzhang/Downloads/A_Study_on_Difficulty_Level_Recognition_of_Piano_Sheet_Music.pdf:pdf},
isbn = {9780769548753},
journal = {Proceedings - 2012 IEEE International Symposium on Multimedia, ISM 2012},
keywords = {Classification,Computer music,Difficulty recognition,Sheet music analysis},
pages = {17--23},
publisher = {IEEE},
title = {{A study on difficulty level recognition of piano sheet music}},
year = {2012}
}
@inproceedings{Ewert2016Score-InformedRecordings,
abstract = {A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education.},
author = {Ewert, Sebastian and Wang, Siying and Sandler, Mark},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ewert, Wang, Sandler - Unknown - SCORE-INFORMED IDENTIFICATION OF MISSING AND EXTRA NOTES IN PIANO RECORDINGS.pdf:pdf},
title = {{Score-Informed Identification of Missing and Extra Notes in Piano Recordings}},
year = {2016}
}
@article{Colombo2018BachPropStyles,
abstract = {Hand in hand with deep learning advancements, algorithms of music composition increase in performance. However, most of the successful models are designed for specific musical structures. Here, we present BachProp, an algorithmic composer that can generate music scores in any style given sufficient training data. To adapt BachProp to a broad range of musical styles, we propose a novel normalized representation of music and train a deep network to predict the note transition probabilities of a given music corpus. In this paper, new music scores sampled by BachProp are compared with the original corpora via crowdsourcing. This evaluation indicates that the music scores generated by BachProp are not less preferred than the original music corpus the algorithm was provided with.},
archivePrefix = {arXiv},
arxivId = {1802.05162},
author = {Colombo, Florian and Gerstner, Wulfram},
eprint = {1802.05162},
file = {:Users/huanzhang/Downloads/BachProp_Learning_to_Compose_Music_in_Multiple_Sty.pdf:pdf},
journal = {Computing Research Repository (CoRR)},
number = {April},
title = {{BachProp: Learning to Compose Music in Multiple Styles}},
url = {http://arxiv.org/abs/1802.05162},
year = {2018}
}
@article{Dai2020,
abstract = {Repetition is a basic indicator of musical structure. This study introduces new algorithms for identifying musical phrases based on repetition. Phrases combine to form sections yielding a two-level hierarchical structure. Automatically detected hierarchical repetition structures reveal significant interactions between structure and chord progressions , melody and rhythm. Different levels of hierarchy interact differently , providing evidence that structural hierarchy plays an important role in music beyond simple notions of repetition or similarity. Our work suggests new applications for music generation and music evaluation.},
archivePrefix = {arXiv},
arxivId = {2010.07518v1},
author = {Dai, Shuqi and Zhang, Huan and Dannenberg, Roger B},
eprint = {2010.07518v1},
isbn = {9789151955605},
keywords = {Multi-level Hierarchy,Music Segmentation,Music Similarity,Music Structure,Music Understanding,Pattern Detection,Repeti-tion,Structure Analysis},
title = {{Automatic Analysis and Influence of Hierarchical Structure on Melody, Rhythm and Harmony in Popular Music}},
year = {2020}
}
@article{Bonnici2018ExpressiveFilter,
abstract = {In this paper, we present an algorithm that uses the Kalman filter to combine simple phrase structure models with observed differences in pitch within the phrase to refine the phrase model and hence adjust the loudness level and tempo of qualities of the melody line. We show how similar adjustments may be made to the accompaniment to introduce expressive attributes to a midi file representation of a score. In the paper, we show that the subjects had some difficulty in distinguishing between the resulting expressive renderings and human performances of the same score.},
author = {Bonnici, Alexandra and Mifsud, Maria and Camilleri, Kenneth P.},
doi = {10.1007/978-3-319-77583-8{\_}6},
file = {:Users/huanzhang/Downloads/expressive-piano-music.pdf:pdf},
isbn = {9783319775821},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {January},
pages = {78--94},
title = {{Expressive piano music playing using a kalman filter}},
volume = {10783 LNCS},
year = {2018}
}
@techreport{SorensenAReport,
abstract = {In this article we report on progress at the Australian CRC for Interaction Design investigating the computational generation of orchestral music based in the Germanic Symphonic tradition. We present an introduction to the project including a brief overview of our intended methods and some guiding principles for the project. We then outline the current state of the project and introduce our initial algorithmic system with a special emphasis on an implementation of Paul Hin-demith's harmonic system. We conclude with some initial findings and future goals. We provide an extensive selection of audio examples online that accompany, verify and enhance information provided in this report.},
author = {Sorensen, Andrew and Brown, Andrew R},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sorensen, Brown - Unknown - A Computational Model For The Generation Of Orches-tral Music In The Germanic Symphonic Tradition A pro-gres.pdf:pdf},
title = {{A Computational Model For The Generation Of Orches-tral Music In The Germanic Symphonic Tradition: A pro-gress report}}
}
@article{Zhang2023a,
abstract = {Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.},
archivePrefix = {arXiv},
arxivId = {2303.13336},
author = {Zhang, Chenshuang and Zhang, Chaoning and Zheng, Sheng and Zhang, Mengchun and Qamar, Maryam and Bae, Sung-Ho and Kweon, In So},
eprint = {2303.13336},
file = {:Users/huanzhang/Downloads/2303.13336.pdf:pdf},
keywords = {Survey, Generative AI, AIGC, Diffusion model, Text},
number = {1},
publisher = {Association for Computing Machinery},
title = {{A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI}},
url = {http://arxiv.org/abs/2303.13336},
volume = {1},
year = {2023}
}
@article{Xu2021,
abstract = {We propose a way of learning disentangled content-style representation of image, allowing us to extrapolate images to any style as well as interpolate between any pair of styles. By augmenting data set in a supervised setting and imposing triplet loss, we ensure the separation of information encoded by content and style representation. We also make use of cycle-consistency loss to guarantee that images could be reconstructed faithfully by their representation.},
archivePrefix = {arXiv},
arxivId = {2111.15624},
author = {Xu, Sailun and Zhang, Jiazhi and Liu, Jiamei},
eprint = {2111.15624},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Xu, Zhang, Liu - Unknown - Image Style Transfer and Content-Style Disentanglement.pdf:pdf},
title = {{Image Style Transfer and Content-Style Disentanglement}},
url = {http://arxiv.org/abs/2111.15624},
year = {2021}
}
@article{Illescas2007HarmonicAnalysis,
abstract = {This work 1 is an effort towards the development of a system for the automation of traditional harmonic analysis of polyphonic scores in symbolic format. A number of stages have been designed in this procedure: melodic analysis of harmonic and non-harmonic tones, vertical harmonic analysis, tonality, and tonal functions. All these informations are represented as a weighted directed acyclic graph. The best possible analysis is the path that maximizes the sum of weights in the graph, obtained through a dynamic programming algorithm. The feasibility of this approach has been tested on six J.S. Bach's harmonized chorales.},
author = {Illescas, Pl{\'{a}}cido R. and Rizo, David and I{\~{n}}esta, Jos{\'{e}} M.},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Illescas, Rizo, I{\~{n}}esta - 2007 - Harmonic, melodic, and functional automatic analysis.pdf:pdf},
journal = {International Computer Music Conference, ICMC 2007},
pages = {165--168},
title = {{Harmonic, melodic, and functional automatic analysis}},
year = {2007}
}
@inproceedings{Renault2022DifferentiableSynthesis,
abstract = {Recent neural-based synthesis models have achieved impressive results for musical instrument sound generation. In particular, the Differentiable Digital Signal Processing (DDSP) framework enables the usage of spectral modeling analysis and synthesis techniques in fully differentiable architectures. Yet currently, it has only been used for modeling monophonic instruments. Leveraging the interpretability and modularity of this framework, the present work introduces a polyphonic differentiable model for piano sound synthesis, conditioned on Musical Instrument Digital Interface (MIDI) inputs. The model architecture is motivated by high-level acoustic modeling knowledge of the instrument which, in tandem with the sound structure priors inherent to the DDSP components, makes for a lightweight, interpretable and realistic sounding piano model. The proposed model has been evaluated in a listening test, demonstrating improved sound quality compared to a benchmark neural-based piano model, with significantly less parameters and even with reduced training data. The same listening test indicates that physical-modeling-based models still achieve better quality, but the differentiability of our lightened approach encourages its usage in other musical tasks dealing with polyphonic audio and symbolic data.},
author = {Renault, Lenny and Mignot, R{\'{e}}mi and Roebel, Axel},
booktitle = {Proceedings of the International Conference on Digital Audio Effects, DAFx},
file = {:Users/huanzhang/Downloads/DAFx20in22_paper_48.pdf:pdf},
isbn = {9783200085992},
issn = {24136689},
pages = {232--239},
title = {{Differentiable Piano Model for MIDI-to-Audio Performance Synthesis}},
volume = {3},
year = {2022}
}
@inproceedings{Giraud2012,
abstract = {Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modula-tory sections called episodes. The episodes play an important role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an algorithm to retrieve episodes in the fugues of the first book of Bach's Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject occurrences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering "quantized partially overlapping intervals" {\textcopyright} 2012 International Society for Music Information Retrieval. [14] and a strict length matching for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue. {\textcopyright} 2012 International Society for Music Information Retrieval.},
author = {Giraud, Mathieu and Groult, Richard and Lev{\'{e}}, Florence},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - (No Title).pdf:pdf},
isbn = {9789727521449},
pages = {457--462},
title = {{Detecting episodes with harmonic sequences for fugue analysis}},
year = {2012}
}
@article{Borsos2023AudioLMGeneration,
abstract = {We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
archivePrefix = {arXiv},
arxivId = {2209.03143},
author = {Borsos, Zalan and Marinier, Raphael and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
doi = {10.1109/TASLP.2023.3288409},
eprint = {2209.03143},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Borsos et al. - Unknown - AudioLM a Language Modeling Approach to Audio Generation.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Computer generated music,speech synthesis},
title = {{AudioLM: A Language Modeling Approach to Audio Generation}},
url = {https://google-research.github.io/seanet/audiolm/examples},
year = {2023}
}
@article{Deutsch1975Two-channelScales,
annote = {From Duplicate 2 (Two-channel listening to musical scales - Deutsch, Diana)

Week 5},
author = {Deutsch, Diana},
doi = {10.1121/1.380573},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Deutsch - 1975 - Two-channel listening to musical scales.pdf:pdf},
journal = {Citation: The Journal of the Acoustical Society of America},
pages = {1156},
title = {{Two-channel listening to musical scales}},
url = {https://doi.org/10.1121/1.380573},
volume = {57},
year = {1975}
}
@article{Barrington2010ModelingTexture,
abstract = {We consider representing a short temporal fragment of musical audio as a dynamic texture, a model of both the timbral and rhythmical qualities of sound, two of the important aspects required for automatic music analysis. The dynamic texture model treats a sequence of audio feature vectors as a sample from a linear dynamical system. We apply this new representation to the task of automatic song segmentation. In particular, we cluster audio fragments , extracted from a song, as samples from a dynamic texture mixture (DTM) model. We show that the DTM model can both accurately cluster coherent segments in music and detect transition boundaries. Moreover, the generative character of the proposed model of music makes it amenable for a wide range of applications besides segmentation. As examples, we use DTM models of songs to suggest possible improvements in other music information retrieval applications such as music annotation and similarity.},
author = {Barrington, Luke and Chan, Antoni B and Lanckriet, Gert},
doi = {10.1109/TASL.2009.2036306},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Barrington, Chan, Lanckriet - 2010 - Modeling Music as a Dynamic Texture.pdf:pdf},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms-Automatic segmentation,dynamic texture model (DTM),music modeling,music similarity},
number = {3},
title = {{Modeling Music as a Dynamic Texture}},
url = {http://ieeexplore.ieee.org.},
volume = {18},
year = {2010}
}
@inproceedings{Huang2018MusicTransformer,
abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
archivePrefix = {arXiv},
arxivId = {1809.04281},
author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
booktitle = {Proceedings of the International Conference of Learning Representations (ICLR)},
eprint = {1809.04281},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Anna Huang, Vaswani Jakob Uszkoreit Noam Shazeer Ian Simon Curtis Hawthorne Andrew Dai Matthew D Hoffman Monica Dinculescu Douglas Ec(2).pdf:pdf},
title = {{Music Transformer}},
url = {http://arxiv.org/abs/1809.04281},
year = {2019}
}
@techreport{LeeSamuelSchoenholzFiniteStudy,
abstract = {We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neu-ral network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; reg-ularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.},
author = {{Lee Samuel Schoenholz}, Jaehoon S and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee Samuel Schoenholz et al. - Unknown - Finite Versus Infinite Neural Networks an Empirical Study.pdf:pdf},
title = {{Finite Versus Infinite Neural Networks: an Empirical Study}},
url = {https://g.co/airesidency}
}
@article{Goto2010,
abstract = {In this paper, we propose a novel area of research referred to as singing information processing. To shape the concept of this area, we first introduce singing understanding systems for synchronizing between vocal melody and corresponding lyrics, identifying the singer name, evaluating singing skills, creating hyperlinks between phrases in the lyrics of songs, and detecting breath sounds. We then introduce music information retrieval systems based on similarity of vocal melody timbre and vocal percussion, and singing synthesis systems. Common signal processing techniques for modeling singing voices that are used in these systems, such as techniques for extracting the vocal melody from polyphonic music recordings and modeling the lyrics by using phoneme HMMs for singing voices, are discussed. {\textcopyright}2010 IEEE.},
author = {Goto, Masataka and Saitou, Takeshi and Nakano, Tomoyasu and Fujihara, Hiromasa},
doi = {10.1109/ICASSP.2010.5495212},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goto et al. - 2010 - Singing information processing based on singing voice modeling.pdf:pdf},
isbn = {9781424442966},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Music,Singing information processing,Singing voice modeling,Vocal melody},
pages = {5506--5509},
publisher = {IEEE},
title = {{Singing information processing based on singing voice modeling}},
year = {2010}
}
@article{Widmer2003DiscoveringDiscoveries,
abstract = {This article presents a new rule discovery algorithm named PLCG that can find simple, robust partial rule models (sets of classification rules) in complex data where it is difficult or impossible to find models that completely account for all the phenomena of interest. Technically speaking, PLCG is an ensemble learning method that learns multiple models via some standard rule learning algorithm, and then combines these into one final rule set via clustering, generalization, and heuristic rule selection. The algorithm was developed in the context of an interdisciplinary research project that aims at discovering fundamental principles of expressive music performance from large amounts of complex real-world data (specifically, measurements of actual performances by concert pianists). It will be shown that PLCG succeeds in finding some surprisingly simple and robust performance principles, some of which represent truly novel and musically meaningful discoveries. A set of more systematic experiments shows that PLCG usually discovers significantly simpler theories than more direct approaches to rule learning (including the state-of-the-art learning algorithm RIPPER), while striking a compromise between coverage and precision. The experiments also show how easy it is to use PLCG as a meta-learning strategy to explore different parts of the space of rule models. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Widmer, Gerhard},
doi = {10.1016/S0004-3702(03)00016-X},
file = {:Users/huanzhang/Downloads/Discovering_simple_rules_in_complex_data_A_meta-le.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Data mining,Ensemble methods,Expressive music performance,Machine learning,Meta-learning,Partial models,Rule discovery},
number = {2},
pages = {129--148},
title = {{Discovering simple rules in complex data: A meta-learning algorithm and some surprising musical discoveries}},
volume = {146},
year = {2003}
}
@article{McAuley2006,
abstract = {Life span developmental profiles were constructed for 305 participants (ages 4-95) for a battery of paced and unpaced perceptual-motor timing tasks that included synchronize-continue tapping at a wide range of target event rates. Two life span hypotheses, derived from an entrainment theory of timing and event tracking, were tested. A preferred period hypothesis predicted a monotonic slowing of a preferred rate (tempo) of event tracking across the life span. An entrainment region hypothesis predicted a quadratic profile in the range of event rates that produced effective timing across the life span; specifically, age-specific entrainment regions should be narrower in childhood and late adulthood than in midlife. Findings across tasks provide converging support for both hypotheses. Implications of these findings are discussed for understanding critical periods in development and age-related slowing of event timing. Copyright 2006 by the American Psychological Association.},
annote = {From Duplicate 1 (The time of our lives: Life span development of timing and event tracking - McAuley, J. Devin; Jones, Mari Riess; Holub, Shayla; Johnston, Heather M.; Miller, Nathaniel S.)

Week 4},
author = {McAuley, J. Devin and Jones, Mari Riess and Holub, Shayla and Johnston, Heather M. and Miller, Nathaniel S.},
doi = {10.1037/0096-3445.135.3.348},
file = {:Users/huanzhang/Downloads/mcauley_etAl_2006.pdf:pdf},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Entrainment,Life span development,Preferred tempo,Rhythm perception and production,Timing},
number = {3},
pages = {348--367},
pmid = {16846269},
title = {{The time of our lives: Life span development of timing and event tracking}},
volume = {135},
year = {2006}
}
@article{Molina-Solana2010EvidenceNocturnes,
abstract = {The performance of music usually involves a great deal of interpretation by the musician. In classical music, the final ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what degree individual performance style has an effect on the timing of final ritardandi. The particular approach taken here uses Friberg and Sundberg's kinematic rubato model in order to characterize performed ritardandi. Using a machine- learning classifier, we carry out a pianist identification task to assess the suitability of the data for characterizing the in- dividual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-specific aspects, pianists can often be identified with accuracy above baseline. This fact suggests the existence of a performer-specific style of playing ritardandi. {\textcopyright} 2010 International Society for Music Information Retrieval.},
annote = {Ritartando - does pianist have an individual rubato style that's invariant to the piece?

kinetic ritartando model with w, q

starting position determined by annotation

identifying performer with compostion component removed:
- performance norm of the given ritartando
- norm: mean of all w and q},
author = {Molina-Solana, Miguel and Grachten, Maarten and Widmer, Gerhard},
file = {:Users/huanzhang/Downloads/molina-ismir10.pdf:pdf},
isbn = {9789039353813},
journal = {Proceedings of the 11st International Society for Music Information Retrieval Conference, ISMIR 2010},
pages = {225--230},
title = {{Evidence for pianist-specific rubato style in chopin nocturnes}},
url = {https://www.researchgate.net/publication/220723460_Evidence_for_Pianist-specific_Rubato_Style_in_Chopin_Nocturnes},
year = {2010}
}
@misc{DuanAudio-VisualPerformances,
author = {Duan, Zhiyao and Liem, Cynthia C. S. and {Richard Ga{\"{e}}l} and Sharma, Gaurav},
file = {:Users/huanzhang/Downloads/duan2018audiovisual.pdf:pdf},
title = {{Audio-Visual Analysis of Music Performances}}
}
@inproceedings{Hawthorne2022Multi-instrumentDiffusion,
abstract = {An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr\'echet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.},
address = {Bengaluru, India},
annote = {Spectrogram generation (decoder)
- GAN
- DDPM 

5 seconds, with providing previous segment

1000 steps 
transformer decoder: 
time step position encoding + 
FiLM layer + 
self attention + 
cross attention 

inversion: MelGAN

Q: 
how is the classifier-free technique being implemented?
Do you think diffusion model can learn the long-time dependency of music structure?},
archivePrefix = {arXiv},
arxivId = {2206.05408},
author = {Hawthorne, Curtis and Simon, Ian and Roberts, Adam and Zeghidour, Neil and Gardner, Josh and Manilow, Ethan and Engel, Jesse},
booktitle = {Proceeding of the International Society on Music Information Retrieval (ISMIR)},
eprint = {2206.05408},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hawthorne et al. - Unknown - MULTI-INSTRUMENT MUSIC SYNTHESIS WITH SPECTROGRAM DIFFUSION(2).pdf:pdf},
title = {{Multi-instrument Music Synthesis with Spectrogram Diffusion}},
url = {http://arxiv.org/abs/2206.05408},
year = {2022}
}
@article{Marozeau2007TheTimbre,
annote = {From Duplicate 1 (The effect of fundamental frequency on the brightness dimension of timbre - Marozeau, Jeremy; De Cheveign{\'{e}}, Alain)

Week 3},
author = {Marozeau, Jeremy and {De Cheveign{\'{e}}}, Alain},
doi = {10.1121/1.2384910},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Marozeau, De Cheveign{\'{e}} - 2007 - The effect of fundamental frequency on the brightness dimension of timbre.pdf:pdf},
journal = {Citation: The Journal of the Acoustical Society of America},
pages = {383},
title = {{The effect of fundamental frequency on the brightness dimension of timbre}},
url = {https://doi.org/10.1121/1.2384910},
volume = {121},
year = {2007}
}
@inproceedings{Zhang2024LLaQoAssessment,
abstract = {Research in music understanding has extensively explored composition-level attributes such as key, genre, and instrumentation through advanced representations, leading to cross-modal applications using large language models. However, aspects of musical performance such as stylistic expression and technique remain underexplored, along with the potential of using large language models to enhance educational outcomes with customized feedback. To bridge this gap, we introduce LLaQo, a Large Language Query-based music coach that leverages audio language modeling to provide detailed and formative assessments of music performances. We also introduce instruction-tuned query-response datasets that cover a variety of performance dimensions from pitch accuracy to articulation, as well as contextual performance understanding (such as difficulty and performance techniques). Utilizing AudioMAE encoder and Vicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in predicting teachers' performance ratings, as well as in identifying piece difficulty and playing techniques. Textual responses from LLaQo was moreover rated significantly higher compared to other baseline models in a user study using audio-text matching. Our proposed model can thus provide informative answers to open-ended questions related to musical performance from audio data.},
archivePrefix = {arXiv},
arxivId = {2409.08795},
author = {Zhang, Huan and Cheung, Vincent and Nishioka, Hayato and Dixon, Simon and Furuya, Shinichi},
booktitle = {In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {2409.08795},
file = {:Users/huanzhang/Downloads/2409.08795v2.pdf:pdf},
title = {{{LLaQo}: Towards a Query-Based Coach in Expressive Music Performance Assessment}},
url = {http://arxiv.org/abs/2409.08795},
year = {2025}
}
@article{Cheng2009,
abstract = {This paper proposes a Local Maximum Phrase Detection (LMPD) method for the analysis of phrasing strategies in expressive performances. The LMPD method systematically extracts a quantitative representation of phrasing strategy by equating the occurrence of a local maximum in the loudness curve with the occurrence of a phrase or sub-phrase. We further define mathematical descriptors for phrase strength and volatility, and phrase typicality, for comparing phrasing strategies among performances. Phrase strength measures the prominence or clarity of a phrase, and the volatility is defined as the standard deviation of the phrase strengths within a performance. Phrase typicality quantifies the degree to which a phrase loudness peak location is characteristic among the performances polled. The ideas behind these descriptors extend to phrase information derived from tempo variation. We illustrate the LMPD method using preliminary results from its application to eleven commercially available audio recordings of a solo violin Bach Sonata. {\textcopyright} 2009 Springer-Verlag.},
author = {Cheng, Eric and Chew, Elaine},
doi = {10.1007/978-3-642-04579-0_34},
file = {:Users/huanzhang/Downloads/A_Local_Maximum_Phrase_Detection_Method_for_Analyz.pdf:pdf},
isbn = {3642045782},
issn = {18650929},
journal = {Communications in Computer and Information Science},
number = {June},
pages = {347--353},
title = {{A local maximum phrase detection method for analyzing phrasing strategies in expressive performances}},
volume = {37 CCIS},
year = {2009}
}
@article{ParkANPLAYLISTS,
abstract = {Collaborative music consumption behavior has morphed drastically with the availability of customized recommendations and online platforms for co-creating playlists. In this pilot study, we find that not only have the practices of collaborative curation changed, but the emotions associated with the songs and playlists have also been affected. Considering users' innate desires with respect to social factors and implications is crucial to developing music technologies for today. Further investigation is needed to gain more nuanced understandings of the habits and emotions of today's collaborative music curators.},
author = {Park, So Yeon and Kaneshiro, Blair},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Park, Kaneshiro - Unknown - AN ANALYSIS OF USER BEHAVIOR IN CO-CURATION OF MUSIC THROUGH COLLABORATIVE PLAYLISTS.pdf:pdf},
title = {{AN ANALYSIS OF USER BEHAVIOR IN CO-CURATION OF MUSIC THROUGH COLLABORATIVE PLAYLISTS}}
}
@article{RodaTowardPerformance,
abstract = {This paper reports a "musical Turing test" conducted at a live concert of algorithm-generated performances, where one group of participants were invited to rank the most human-like performance while knowing that one of the performances was by a human, and another group of participant were asked to do the same, but without knowing that there was a human performer on the program. The program consisted of five pieces from the classical/romantic period, played on a Disklavier. High quality music-expression algorithms were used to generate the algorithmic renditions. Regardless of the group, musical experience and a number of other factors, the subjects were unable to identify the human performer out of the five performances. The group that did not know there was a human performer had a wider range of votes compared to the group that did know. Furthermore, subjects were less confident of their answers when they knew that they were comparing human and computer-generated performances. On the contrary, if subjects believed they were only comparing computer-generated performances the task may have been less demanding. Findings suggest that computer algorithms are able to substitute for human performance, but the role of the physical presence of the performer (who was absent in this study) could be an area for further investigation.},
author = {Rod{\`{a}}, Antonio and Schubert, Emery and {De Poli}, Giovanni and Canazza, Sergio},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rod{\`{a}} et al. - Unknown - Toward a musical Turing test for automatic music performance.pdf:pdf},
keywords = {algorithm-generated performance,expressive music performance,turing test},
title = {{Toward a musical Turing test for automatic music performance}},
url = {http://renconmusic.org}
}
@article{Gupta2018a,
abstract = {A perceptually valid automatic singing evaluation score could serve as a complement to singing lessons, and make singing training more reachable to the masses. In this study, we adopt the idea behind PESQ (Perceptual Evaluation of Speech Quality) scoring metrics, and propose various perceptually relevant features to evaluate singing quality. We correlate the obtained singing quality score, which we term as Perceptual Evaluation of Singing Quality (PESnQ) score, with that given by music-expert human judges, and compare the results with the known baseline systems. It is shown that the proposed PESnQ has a correlation of 0.59 with human ratings, which is an improvement of 96% over baseline systems.},
author = {Gupta, Chitralekha and Li, Haizhou and Wang, Ye},
doi = {10.1109/APSIPA.2017.8282110},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Li, Wang - 2018 - Perceptual evaluation of singing quality.pdf:pdf},
isbn = {9781538615423},
journal = {Proceedings - 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2017},
number = {December},
pages = {577--586},
title = {{Perceptual evaluation of singing quality}},
volume = {2018-Febru},
year = {2018}
}
@article{Donahue2022,
author = {Donahue, Chris and Thickstun, John and Liang, Percy},
file = {:Users/huanzhang/Downloads/300.pdf:pdf},
title = {{MELODY TRANSCRIPTION VIA GENERATIVE PRE-TRAINING}},
year = {2022}
}
@techreport{Phan2021MULTI-VIEWCLASSIFICATION,
abstract = {We propose in this work a multi-view learning approach for audio and music classification. Considering four typical low-level representations (i.e. different views) commonly used for audio and music recognition tasks, the proposed multi-view network consists of four subnetworks, each handling one input types. The learned embedding in the subnetworks are then concatenated to form the multi-view embedding for classification similar to a simple concatenation network. However, apart from the joint classification branch, the network also maintains four classification branches on the single-view embedding of the subnetworks. A novel method is then proposed to keep track of the learning behavior on the classification branches and adapt their weights to proportionally blend their gradients for network training. The weights are adapted in such a way that learning on a branch that is generalizing well will be encouraged whereas learning on a branch that is overfitting will be slowed down. Experiments on three different audio and music classification tasks show that the proposed multi-view network not only outperforms the single-view baselines but also is superior to the multi-view baselines based on concatena-tion and late fusion.},
archivePrefix = {arXiv},
arxivId = {2103.02420v1},
author = {Phan, Huy and {Le Nguyen}, Huy and Ch{\'{e}}n, Oliver Y and Pham, Lam and Koch, Philipp and Mcloughlin, Ian and Mertins, Alfred},
eprint = {2103.02420v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Phan et al. - 2021 - MULTI-VIEW AUDIO AND MUSIC CLASSIFICATION.pdf:pdf},
keywords = {Index Terms-multi-view learning,audio clas-sification,deep learning,gradient blending,music classification},
title = {{MULTI-VIEW AUDIO AND MUSIC CLASSIFICATION}},
year = {2021}
}
@inproceedings{Flexer2017CloserClassification,
abstract = {Musical genre classification is the automatic classification of audio signals into user defined labels describing pieces of music. A problem inherent to genre classification experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to overoptimistic accuracy results but also selectively favours particular classification approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist filter. The same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files. {\textcopyright}2007 Austrian Computer Society (OCG).},
author = {Flexer, Arthur},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Downloads/A_Closer_Look_on_Artist_Filters_for_Musical_Genre_.pdf:pdf},
isbn = {9783854032182},
title = {{A closer look on artist filters for musical genre classification}},
year = {2007}
}
@techreport{Windsor1997ExpressiveTool,
author = {Windsor, W Luke and Clarke, Eric F},
booktitle = {An Interdisciplinary Journal},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Windsor, Clarke - 1997 - Expressive Timing and Dynamics in Real and Artificial Musical Performances Using an Algorithm as an Analytical.pdf:pdf},
number = {2},
pages = {127--152},
publisher = {Winter},
title = {{Expressive Timing and Dynamics in Real and Artificial Musical Performances: Using an Algorithm as an Analytical Tool}},
volume = {15},
year = {1997}
}
@techreport{Linb,
abstract = {Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autore-gressive generation limits their utility in music editing tasks. To address this gap, we propose a novel approach leveraging a parameter-efficient heterogeneous adapter combined with a masking training scheme. This approach enables autoregres-sive language models to seamlessly address music inpainting tasks. Additionally, our method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autore-gressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. The source codes and a demo page showcasing our work are available at https://kikyo-16.github.io/AIR.},
archivePrefix = {arXiv},
arxivId = {2402.09508v2},
author = {Lin, Liwei and Xia, Gus and Zhang, Yixiao and Jiang, Junyan},
eprint = {2402.09508v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - Unknown - Arrange, Inpaint, and Refine Steerable Long-term Music Audio Generation and Editing via Content-based Controls.pdf:pdf},
title = {{Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls}},
url = {https://kikyo-16.github.io/AIR.}
}
@article{Flossmann2010MagaloffReport,
abstract = {One of the main difficulties in studying expression in musical performance is the acquisition of data. While audio recordings abound, automatically extracting precise information related to timing, dynamics, and articulation is still not possible at the level of precision required for large-scale music performance studies. In 1989, the Russian pianist Nikita Magaloff performed essentially the entire works for solo piano by Fr{\'{e}}d́ric Chopin on a B{\"{o}} sendorfer SE, a computer-controlled grand piano that precisely measures every key and pedal action by the performer. In this paper, we describe the process and the tools for the preparation of this collection, which comprises hundreds of thousands of notes. We then move on to presenting the results of initial exploratory studies of the expressive content of the data, specifically effects of performer age, performance errors, between-hand asynchronies, and tempo rubato. We also report preliminary results of a systematic study of the shaping of particular rhythmic passages, using the notion of phase-plane trajectories. Finally, we briefly describe how the Magaloff data were used to train a performance rendering system that won the 2008 Rencon International Performance Rendering Contest. {\textcopyright} 2010 Taylor & Francis.},
author = {Flossmann, Sebastian and Goebl, Werner and Grachten, Maarten and Niedermayer, Bernhard and Widmer, Gerhard},
doi = {10.1080/09298215.2010.523469},
file = {:Users/huanzhang/Downloads/FlossmannGoeblGrachtenNiedermayerWidmer2010-MagaloffReport.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
number = {4},
pages = {363--377},
title = {{The {Magaloff} Project: An Interim Report}},
volume = {39},
year = {2010}
}
@article{Pati2021,
abstract = {Selective manipulation of data attributes using deep generative models is an active area of research. In this paper, we present a novel method to structure the latent space of a variational auto-encoder to encode different continuous-valued attributes explicitly. This is accomplished by using an attribute regularization loss which enforces a monotonic relationship between the attribute values and the latent code of the dimension along which the attribute is to be encoded. Consequently, post training, the model can be used to manipulate the attribute by simply changing the latent code of the corresponding regularized dimension. The results obtained from several quantitative and qualitative experiments show that the proposed method leads to disentangled and interpretable latent spaces which can be used to effectively manipulate a wide range of data attributes spanning image and symbolic music domains.},
archivePrefix = {arXiv},
arxivId = {2004.05485},
author = {Pati, Ashis and Lerch, Alexander},
doi = {10.1007/s00521-020-05270-2},
eprint = {2004.05485},
file = {:Users/huanzhang/Downloads/2004.05485.pdf:pdf},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Generative modeling,Latent space disentanglement,Latent space regularization,Representation learning},
number = {9},
pages = {4429--4444},
title = {{Attribute-based regularization of latent spaces for variational auto-encoders}},
url = {https://arxiv.org/pdf/2004.05485.pdf},
volume = {33},
year = {2021}
}
@article{Oore2018ThisPerformance,
abstract = {Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the significance and qualities of the dataset needed for this. Having identified both a problem domain and characteristics of an appropriate dataset, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.},
archivePrefix = {arXiv},
arxivId = {1808.03715},
author = {Oore, Sageev and Simon, Ian and Dieleman, Sander and Eck, Douglas and Simonyan, Karen},
doi = {10.1007/s00521-018-3758-9},
eprint = {1808.03715},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Oore et al. - 2018 - This Time with Feeling Learning Expressive Musical Performance PreambleRequest.pdf:pdf},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Artificial intelligence,Deep learning,Music generation,Recurrent neural networks},
number = {4},
pages = {955--967},
title = {{This time with feeling: learning expressive musical performance}},
volume = {32},
year = {2018}
}
@article{Kuan2024,
abstract = {Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.},
archivePrefix = {arXiv},
arxivId = {2406.08402},
author = {Kuan, Chun-Yi and Huang, Wei-Ping and Lee, Hung-yi},
eprint = {2406.08402},
file = {:Users/huanzhang/Downloads/2406.08402v1.pdf:pdf},
title = {{Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models}},
url = {http://arxiv.org/abs/2406.08402},
year = {2024}
}
@article{Thompson2001,
abstract = {The "Mozart effect" refers to claims that people perform better on tests of spatial abilities after listening to music composed by Mozart. We examined whether the Mozart effect is a consequence of between-condition differences in arousal and mood. Participants completed a test of spatial abilities after listening to music or sitting in silence. The music was a Mozart sonata (a pleasant and energetic piece) for some participants and an Albinoni adagio (a slow, sad piece) for others. We also measured enjoyment, arousal, and mood. Performance on the spatial task was better following the music than the silence condition, but only for participants who heard Mozart. The two music selections also induced differential responding on the enjoyment , arousal, and mood measures. Moreover, when such differences were held constant by statistical means, the Mozart effect disappeared. These findings provide compelling evidence that the Mozart effect is an artifact of arousal and mood.},
author = {Thompson, William Forde and {Glenn Schellenberg}, E and Husain, Gabriela},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Thompson, Glenn Schellenberg, Husain - 2001 - AROUSAL, MOOD, AND THE MOZART EFFECT.pdf:pdf},
number = {3},
title = {{Arousal, Mood, and the Mozart Effect}},
volume = {12},
year = {2001}
}
@article{SimonettaAudio-to-ScoreTranscription,
abstract = {This PDF is an updated version of the paper published at the IEEE MMSP 2021. It contains some erratum highlighted in red, especially in sections V, VI, and VII. Abstract Audio-to-score alignment (A2SA) is a multimodal task consisting in the alignment of audio signals to music scores. Recent literature confirms the benefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In this work, we aim to elaborate on the exploitation of AMT Deep Learning (DL) models for achieving alignment at the note-level. We propose a method which benefits from HMM-based score-to-score alignment and AMT, showing a remarkable advancement beyond the state-of-the-art. We design a systematic procedure to take advantage of large datasets which do not offer an aligned score. Finally, we perform a thorough comparison and extensive tests on multiple datasets.},
archivePrefix = {arXiv},
arxivId = {2107.12854v3},
author = {Simonetta, Federico and Ntalampiras, Stavros and Avanzini, Federico},
eprint = {2107.12854v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Simonetta, Ntalampiras, Avanzini - Unknown - Audio-to-Score Alignment Using Deep Automatic Music Transcription.pdf:pdf},
keywords = {Index Terms audio-to-score alignment,automatic-music-transcription,music information retrieval},
title = {{Audio-to-Score Alignment Using Deep Automatic Music Transcription}},
url = {https://github.com/LIMUNIMI/MMSP2021-Audio2ScoreAlignment}
}
@techreport{Santurkar,
abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Mit, Aleksander M ˛ Adry},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Santurkar et al. - Unknown - How Does Batch Normalization Help Optimization.pdf:pdf},
title = {{How Does Batch Normalization Help Optimization?}}
}
@article{Processing2023,
author = {Processing, Language},
file = {:Users/huanzhang/Downloads/T-ASL-09749-2022_Proof_hi.pdf:pdf},
title = {{Score-Informed Music Interpretation Analysis and Transcription}},
year = {2023}
}
@article{Kim2020,
abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g., music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights into how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
archivePrefix = {arXiv},
arxivId = {1802.04051},
author = {Kim, Jaehun and Urbano, Juli{\'{a}}n and Liem, Cynthia C.S. S and Hanjalic, Alan},
doi = {10.1007/s00521-019-04076-1},
eprint = {1802.04051},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - Unknown - DEEP LEARNING FOR MUSIC AND AUDIO One deep music representation to rule them all A comparative analysis of differ.pdf:pdf},
isbn = {0123456789},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Multitask learning,Music Information Retrieval,Representation learning},
number = {4},
pages = {1067--1093},
title = {{One deep music representation to rule them all? A comparative analysis of different representation learning strategies}},
url = {https://doi.org/10.1007/s00521-019-04076-1},
volume = {32},
year = {2020}
}
@inproceedings{Peter2023SoundingPerformance,
address = {Milan, Italy},
annote = {- human failed to discern timing and velocity, but can discern articulation and tempo.},
author = {Peter, Silvan David and Cancino-chac{\'{o}}n, Carlos Eduardo and Widmer, Gerhard},
booktitle = {Proceedings of the Digital Libraries for Musicology (DLfM)},
doi = {10.1145/3625135.3625141},
file = {:Users/huanzhang/Downloads/3625135.3625141.pdf:pdf},
isbn = {9798400708336},
keywords = {acm reference format,carlos eduardo cancino-chac{\'{o}}n,emmanouil karysti-,evaluation,expression,listening study,performance,silvan david peter,validity},
title = {{Sounding Out Reconstruction Error-Based Evaluation of Generative Models of Expressive Performance}},
year = {2023}
}
@inproceedings{Cheng2015ModellingSounds,
author = {Cheng, Tian and Dixon, Simon and Mauch, Matthias},
booktitle = {Proceedings of International Conference on Acoustic, Speech and Signal Processsing (ICASSP)},
file = {:Users/huanzhang/Downloads/Modelling_the_decay_of_piano_sounds.pdf:pdf},
isbn = {9780992862633},
publisher = {IEEE},
title = {{Modelling the Decay of Piano Sounds}},
year = {2015}
}
@article{Evans2024StableOpen,
abstract = {Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.},
archivePrefix = {arXiv},
arxivId = {2407.14358},
author = {Evans, Zach and Parker, Julian D. and Carr, CJ and Zukowski, Zack and Taylor, Josiah and Pons, Jordi},
eprint = {2407.14358},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Evans et al. - 2024 - Stable Audio Open.pdf:pdf},
journal = {Arxiv preprint arXiv:2407.14358},
month = {jul},
title = {{Stable Audio Open}},
url = {http://arxiv.org/abs/2407.14358},
year = {2024}
}
@article{Li2019CreatingApplications,
abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches to address the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multimodal tasks. We benchmark two existing MIR tasks (multipitch analysis and score-informed source separation) on the dataset and compare them with other existing music audio datasets. In addition, we consider two novel multimodal MIR tasks (visually informed multipitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measurements and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.},
archivePrefix = {arXiv},
arxivId = {1612.08727},
author = {Li, Bochen and Liu, Xinzhao and Dinesh, Karthik and Duan, Zhiyao and Sharma, Gaurav},
doi = {10.1109/TMM.2018.2856090},
eprint = {1612.08727},
file = {:Users/huanzhang/Downloads/Creating_a_Multitrack_Classical_Music_Performance_Dataset_for_Multimodal_Music_Analysis_Challenges_Insights_and_Applications.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Multimodal music dataset,audio-visual analysis,music performance,synchronization},
number = {2},
pages = {522--535},
publisher = {IEEE},
title = {{Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications}},
volume = {21},
year = {2019}
}
@article{Cook2015BetweenScience,
author = {Cook, Nicholas},
doi = {10.5871/bacad/9780197265864.003.0001},
file = {:Users/huanzhang/Downloads/01_Cook_N_1803-2.pdf:pdf},
journal = {British Academy Lectures 2013-14},
keywords = {digital,humanities,mazurka,modernism,music,performance,science,text},
number = {March},
pages = {1--25},
title = {{Between art and science}},
year = {2015}
}
@inproceedings{Renault2023,
abstract = {Recent advances in data-driven expressive performance rendering have enabled automatic models to reproduce the characteristics and the variability of human performances of musical compositions. However, these models need to be trained with aligned pairs of scores and performances and they rely notably on score-specific markings, which limits their scope of application. This work tackles the piano performance rendering task in a low-informed setting by only considering the score note information and without aligned data. The proposed model relies on an adversarial training where the basic score notes properties are modified in order to reproduce the expressive qualities contained in a dataset of real performances. First results for unaligned score-to-performance rendering are presented through a conducted listening test. While the interpretation quality is not on par with highly-supervised methods and human renditions, our method shows promising results for transferring realistic expressivity into scores.},
author = {Renault, Lenny and Mignot, R{\'{e}}mi and Roebel, Axel},
booktitle = {Proceedings of the International Conference on Digital Audio Effects, DAFx},
file = {:Users/huanzhang/Downloads/Perf_Render_DAFx23_LBR.pdf:pdf},
issn = {24136689},
pages = {355--358},
title = {{EXPRESSIVE PIANO PERFORMANCE RENDERING FROM UNPAIRED DATA}},
year = {2023}
}
@inproceedings{Cancino-Chacon2022PartituraProcessing,
abstract = {Partitura is a lightweight Python package for handling symbolic musical information. It provides easy access to features commonly used in music information retrieval tasks, like note arrays (lists of timed pitched events) and 2D piano roll matrices, as well as other score elements such as time and key signatures, performance directives, and repeat structures. Partitura can load musical scores (in MEI, MusicXML, Kern, and MIDI formats), MIDI performances, and score-to-performance alignments. The package includes some tools for music analysis, such as automatic pitch spelling, key signature identification, and voice separation. Partitura is an open-source project and is available at https://github.com/CPJKU/partitura/.},
address = {Halifax, Canada},
archivePrefix = {arXiv},
arxivId = {2206.01071},
author = {Cancino-Chac{\'{o}}n, Carlos and Peter, Silvan David and Karystinaios, Emmanouil and Foscarin, Francesco and Grachten, Maarten and Widmer, Gerhard},
booktitle = {Proceedings of the Music Encoding Conference (MEC)},
eprint = {2206.01071},
file = {:Users/huanzhang/Downloads/2206.01071v1.pdf:pdf},
title = {{Partitura: A Python Package for Symbolic Music Processing}},
year = {2022}
}
@techreport{Shi2017ModelingRolls,
abstract = {Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ performance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early digital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the raw image data into the MIDI file format by applying histogram-based image processing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expressions when compared with original playback recordings.},
author = {Shi, Zhengshan and Arul, Kumaran and Smith, Julius O.},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Shi, Arul, Smith - Unknown - MODELING AND DIGITIZING REPRODUCING PIANO ROLLS.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Shi, Arul, Smith - Unknown - MODELING AND DIGITIZING REPRODUCING PIANO ROLLS(2).pdf:pdf},
isbn = {9789811151798},
pages = {197--203},
title = {{Modeling and digitizing reproducing piano rolls}},
year = {2017}
}
@article{Guzhov2021AudioCLIP:,
abstract = {In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models. In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bi-modal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion. AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07 % on the UrbanSound8K and 97.15 % on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets (68.78 % and 69.40 %, respectively). Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.},
archivePrefix = {arXiv},
arxivId = {2106.13043v1},
author = {Guzhov, Andrey and Raue, Federico and Hees, J{\"{o}}rn and Dengel, Andreas},
eprint = {2106.13043v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Guzhov et al. - 2021 - AudioCLIP Extending CLIP to Image, Text and Audio ⋆.pdf:pdf},
keywords = {Audio classification {\textperiodcentered},Multimodal learning {\textperiodcentered},Zero-shot in-ference},
title = {{AudioCLIP: Extending CLIP to Image, Text and Audio ⋆}},
year = {2021}
}
@inproceedings{Tobudic2005LearningPianists,
abstract = {An application of relational instance-based learning to the complex task of expressive music performance is presented. We investigate to what extent a machine can automatically build 'expressive profiles' of famous pianists using only minimal performance information extracted from audio CD recordings by pianists and the printed score of the played music. It turns out that the machine-generated expressive performances on unseen pieces are substantially closer to the real performances of the 'trainer' pianist than those of all others. Two other interesting applications of the work are discussed: recognizing pianists from their style of playing, and automatic style replication.},
author = {Tobudic, Asmir and Widmer, Gerhard},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
file = {:Users/huanzhang/Downloads/0448.pdf:pdf},
issn = {10450823},
title = {{Learning to play like the great pianists}},
year = {2005}
}
@article{Repp1995QuantitativeEvidence,
abstract = {This study examines whether global tempo and expressive timing microstructure are independent in the aesthetic judgment of music performance. Measurements of tone interonset intervals in pianists' performances of pieces by Schumann (“Tr{\"{a}}umerei”) and Debussy (“La fille aux cheveux de lin”）at three different tempi show a tendency toward reduced relative variation in expressive timing at both faster and slower tempi, relative to the pianist's original tempo. However, this could reflect merely the pianists' discomfort when playing at an unfamiliar tempo. Therefore, a perceptual approach was taken here. Experimental stimuli were created artificially by independently manipulating global tempo (three levels) and “relative modulation depth” of expressive timing (RMD, five levels) in MIDI-recorded complete performances of the Schumann and Debussy pieces. Skilled pianists rated the quality of the resulting two sets of 15 manees on a 10-point scale. The question was whether the same would receive the highest rating at all three tempi, or whether an interaction would emerge, such that different RMDs are preferred at different tempi. A small but significant interaction was obtained for both pieces, indicating that the listeners preferred a reduced RMD when the tempo was increased, but the same or a larger RMD when the tempo was decreased. Thus, they associated an increase in tempo with a decrease in (relative) expressive timing variation, which, in general agreement with the performance data, suggests that the two temporal dimensions are not independent. {\textcopyright} 1995, by The Regents of the University of California.},
author = {Repp, Bruno H},
doi = {10.2307/40285684},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - 1995 - Quantitative Effects of Global Tempo on Expressive Timing in Music Performance Some Perceptual Evidence.pdf:pdf},
issn = {15338312},
journal = {Music Perception},
number = {1},
pages = {39--57},
title = {{Quantitative Effects of Global Tempo on Expressive Timing in Music Performance: Some Perceptual Evidence}},
volume = {13},
year = {1995}
}
@article{Todd1992DynamicsExpression,
abstract = {A computational model of musical dynamics is proposed that complements an earlier model of expressive timing. The model, implemented in the artificial intelligence language LISP, is based on the observation that a musical phrase is often indicated by a crescendo/decrescendo shape. The functional form of this shape is derived by making two main assumptions. First, that musical dynamics and tempo are coupled, that is, “the faster the louder, the slower the softer.” This tempo/dynamics coupling, it is suggested, may be a characteristic of some classical and romantic styles perhaps exemplified by performances of Chopin. Second, that the tempo change is governed by analogy to physical movement. The allusion of musical expression to physical motion is further extended by the introduction of the concepts of energy and mass. The utility of the model, in addition to giving an insight into the nature of musical expression, is that it provides a basis for a method of performance style analysis. {\textcopyright} 1992, Acoustical Society of America. All rights reserved.},
author = {Todd, Neil P. McAngus},
doi = {10.1121/1.402843},
file = {:Users/huanzhang/Downloads/3540_1_online.pdf:pdf},
isbn = {0635401150},
issn = {NA},
journal = {Journal of the Acoustical Society of America},
number = {6},
pages = {3540--3550},
title = {{The dynamics of dynamics: A model of musical expression}},
volume = {91},
year = {1992}
}
@inproceedings{Sundberg2003Musicians1-20,
author = {Sundberg, J and Friberg, A and Bresin, R},
booktitle = {Speech, Music and Hearing Quarterly Progress and Status Report},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Sundberg, Friberg, Bresin - Unknown - Musician's and computer's tone inter-onset-interval in Mozart's Piano Sonata K 332, 2nd mvt, bar 1.pdf:pdf},
title = {{Musician's and computer's tone inter-onset-interval in Mozart's Piano Sonata K 332, 2nd mvt, bar 1-20}},
url = {http://www.speech.kth.se/qpsr},
year = {2003}
}
@inproceedings{Won2024FoundationalInformatics,
author = {Won, Minz and Hung, Yun-ning and Le, Duc},
booktitle = {2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP48485.2024.10448314},
file = {:Users/huanzhang/Downloads/A_Foundation_Model_for_Music_Informatics.pdf:pdf},
isbn = {9798350344851},
title = {{A Foundation Model for Music Informatics}},
year = {2024}
}
@article{Cifka,
author = {C{\'{i}}fka, Ondřej},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/C{\'{i}}fka - Unknown - Deep learning methods for music style transfer.pdf:pdf},
title = {{Deep learning methods for music style transfer}},
url = {https://tel.archives-ouvertes.fr/tel-03499991}
}
@article{Tsai2015,
abstract = {This work aims to develop an automatic singing evaluation system for general public. Given a CD/mp3 song recording as the reference basis, the proposed system rates a user's singing performance by comparing it with the vocal in the song recording. This modality allows users to not only enjoy listening to and singing with CD/mp3 songs but also know how well or bad they sing. However, as a majority of songs contain background accompaniments during most or all vocal passages, directly comparing a user's singing performance with the signals in a song recording does not make sense. To tackle this problem, we propose methods to extract pitch-, volume-, and rhythm-based features of the original singer in the accompanied vocals. Our experiment shows that the results of automatic singing evaluation are close to the human rating, where the Pearson product-moment correlation coefficient between them is 0.8. The results are also comparable to those in a previous work using Karaoke music as reference bases, where the latter's task is considered to be easier than that of this work.},
annote = {From Duplicate 1 (Automatic singing performance evaluation using accompanied vocals as reference bases - Tsai, Wei Ho; Ma, Cin Hao; Hsu, Yi Po)

pitch-based rating
DTW distance between midi sequence 
volume-based rating
rhythm-based rating},
author = {Tsai, Wei Ho and Ma, Cin Hao and Hsu, Yi Po},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tsai, Ma, Hsu - 2015 - Automatic singing performance evaluation using accompanied vocals as reference bases.pdf:pdf},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {Accompanied vocal,Pitch,Rhythm,Singing evaluation,Volume},
number = {3},
pages = {821--838},
title = {{Automatic singing performance evaluation using accompanied vocals as reference bases}},
volume = {31},
year = {2015}
}
@article{Geva2021,
abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
archivePrefix = {arXiv},
arxivId = {2012.14913},
author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
doi = {10.18653/v1/2021.emnlp-main.446},
eprint = {2012.14913},
file = {:Users/huanzhang/Downloads/Geva et al_2021_Transformer Feed-Forward Layers Are Key-Value Memories.pdf:pdf},
isbn = {9781955917094},
journal = {EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings},
pages = {5484--5495},
title = {{Transformer Feed-Forward Layers Are Key-Value Memories}},
year = {2021}
}
@article{Borovik2023,
author = {Borovik, Ilya and Viro, Vladimir},
file = {:Users/huanzhang/Downloads/nime2023_91.pdf:pdf},
title = {{Real-Time Co-Creation of Expressive Music Performances Using Speech and Gestures}},
year = {2023}
}
@inproceedings{Benetos2012Score-informedTutoring,
abstract = {In this paper, a score-informed transcription method for automatic piano tutoring is proposed. The method takes as input a recording made by a student which may contain mistakes, along with a reference score. The recording and the aligned synthesized score are automatically transcribed using the non-negative matrix factorization algorithm for multi-pitch estimation and hidden Markov models for note tracking. By comparing the two transcribed recordings, common errors occurring in transcription algorithms such as extra octave notes can be suppressed. The result is a piano-roll description which shows the mistakes made by the student along with the correctly played notes. Evaluation was performed on six pieces recorded using a Disklavier piano, using both manually-aligned and automatically-aligned scores as an input. Results comparing the system output with ground-truth annotation of the original recording reach a weighted F-measure of 93%, indicating that the proposed method can successfully analyze the student's performance. {\textcopyright} 2012 EURASIP.},
author = {Benetos, Emmanouil and Klapuri, Anssi and Dixon, Simon},
booktitle = {European Signal Processing Conference (EUSIPCO)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Benetos, Klapuri, Dixon - Unknown - SCORE-INFORMED TRANSCRIPTION FOR AUTOMATIC PIANO TUTORING.pdf:pdf},
isbn = {9781467310680},
issn = {22195491},
keywords = {HMMs,Music signal analysis,NMF,score-informed transcription},
title = {{Score-informed transcription for automatic piano tutoring}},
year = {2012}
}
@inproceedings{Grohganz2014EstimatingFiles,
abstract = {Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musical time information as specified by sheet music as well as physical time information that reflects performance aspects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this paper , we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert performed MIDI files into semantically enriched score-like MIDI files.},
author = {Grohganz, Harald and Clausen, Michael},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Grohganz, Clausen - Unknown - ESTIMATING MUSICAL TIME INFORMATION FROM PERFORMED MIDI FILES.pdf:pdf},
title = {{Estimating Musical Time Information from Performed MIDI Files}},
year = {2014}
}
@inproceedings{Zhou2019BandnetMachine,
abstract = {In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness. The generated music samples can be downloaded at https://goo.gl/uaLXoB.},
archivePrefix = {arXiv},
arxivId = {1812.07126},
author = {Zhou, Yichao and Chu, Wei and Young, Sam and Chen, Xin},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR},
eprint = {1812.07126},
file = {:Users/huanzhang/Downloads/1812.07126v1.pdf:pdf},
isbn = {9781732729919},
title = {{Bandnet: A neural network-based, multi-instrument beatles-style midi music composition machine}},
year = {2019}
}
@article{Hentschel2021TheCadence,
abstract = {This article describes a new expert-labelled dataset featuring harmonic, phrase, and cadence analyses of all piano sonatas by W.A. Mozart. The dataset draws on the DCML standard for harmonic annotation and is being published adopting the FAIR principles of Open Science. The annotations have been verified using a data triangulation procedure which is presented as an alternative approach to handling annotator subjectivity. This procedure is suited for ensuring consistency, within the dataset and beyond, despite the high level of analytical detail afforded by the employed harmonic annotation syntax. The harmony labels also encode contextual information and are therefore suited for investigating music theoretical questions related to tonal harmony and the harmonic makeup of cadences in the classical style. Apart from providing basic statistical analyses characterizing the dataset, its music theoretical potential is illustrated by two preliminary experiments, one on the terminal harmonies of cadences and the other on the relation between performance durations and harmonic density. Furthermore, particular features can be selected to produce more coarse-grained training data, for example for chord detection algorithms that require less analytical detail. Facilitating the dataset's reusability, it comes with a Python script that allows researchers to easily access various representations of the data tailored to their particular needs.},
author = {Hentschel, Johannes and Neuwirth, Markus and Rohrmeier, Martin},
doi = {10.5334/tismir.63},
file = {:Users/huanzhang/Downloads/63-2532-4-PB (1).pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {cadence,classical style,expert-annotated dataset,piano music,tonal harmony},
number = {1},
pages = {67--80},
title = {{The Annotated Mozart Sonatas: Score, Harmony, and Cadence}},
volume = {4},
year = {2021}
}
@article{Dai2020a,
abstract = {Repetition is a basic indicator of musical structure. This study introduces new algorithms for identifying musical phrases based on repetition. Phrases combine to form sections yielding a two-level hierarchical structure. Automatically detected hierarchical repetition structures reveal significant interactions between structure and chord progressions, melody and rhythm. Different levels of hierarchy interact differently, providing evidence that structural hierarchy plays an important role in music beyond simple notions of repetition or similarity. Our work suggests new applications for music generation and music evaluation.},
archivePrefix = {arXiv},
arxivId = {2010.07518},
author = {Dai, Shuqi and Zhang, Huan and Dannenberg, Roger B.},
eprint = {2010.07518},
file = {:Users/huanzhang/Downloads/2010.07518.pdf:pdf},
isbn = {9789151955605},
keywords = {multi-level hierarchy,music segmentation,music similarity,music structure,music understanding,pattern detection,repeti-,structure analysis,tion},
title = {{Automatic Analysis and Influence of Hierarchical Structure on Melody, Rhythm and Harmony in Popular Music}},
url = {http://arxiv.org/abs/2010.07518},
year = {2020}
}
@article{Article2024,
author = {Article, Overview},
file = {:Users/huanzhang/Downloads/tismir-review-assignment-240-Manuscript-7013.pdf:pdf},
keywords = {authorship attribution,composer classification,composer identification,music information processing,music scores,symbolic},
title = {{A systematic survey on music composer style identification , attribution , and classification}},
year = {2024}
}
@article{Ghatas2022HybridMusic,
abstract = {Musical difficulty estimation is an essential part of musical learning. Without a precise estimate, a music learner cannot choose a piece to play according to their current level. This problem is highly complicated for its subjectivity and data scarcity. In this study, we investigate deep learning approaches to solve this problem. Our pipeline can be summarized as follows: firstly, we convert the symbolic music MIDI file of a piano performance to piano roll representation. Secondly, the piano roll is divided into smaller parts. Finally, a model is trained on parts accompanied by the corresponding difficulty labels. We test our models on both complete and partial track difficulty classification problems. Multiple deep convolutional neural networks are proposed and evaluated. Accompanied with handcrafted features, the proposed hybrid deep model yields a relative F1 score improvement of more than 10% compared to previous studies, achieving a state-of-the-art F1 score of 76.26%. Besides the direct application of the work to classify musical pieces, the promising results can be a starting point for more complicated applications, including automated difficulty-controlled music generation.},
author = {Ghatas, Youssef and Fayek, Magda and Hadhoud, Mayada},
doi = {10.1016/j.aej.2022.03.060},
file = {:Users/huanzhang/Downloads/1-s2.0-S1110016822002356-main.pdf:pdf},
issn = {11100168},
journal = {Alexandria Engineering Journal},
keywords = {Classification,Computer music,Deep learning,Machine learning,Neural networks},
number = {12},
pages = {10183--10196},
title = {{A hybrid deep learning approach for musical difficulty estimation of piano symbolic music}},
volume = {61},
year = {2022}
}
@article{ParkTUNESPLAYLISTS,
abstract = {Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP). We conducted an exploratory study of CP users and non-users (N = 65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes-Practical, Cognitive, and Social-and two connotations-Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music's role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups.},
author = {Park, So Yeon and Laplante, Audrey and Lee, Jin Ha and Kaneshiro, Blair},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - Unknown - TUNES TOGETHER PERCEPTION AND EXPERIENCE OF COLLABORATIVE PLAYLISTS.pdf:pdf},
title = {{TUNES TOGETHER: PERCEPTION AND EXPERIENCE OF COLLABORATIVE PLAYLISTS}}
}
@article{Xiang2019,
abstract = {Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. Training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1905.10742},
author = {Xiang, Sitao and Li, Hao},
eprint = {1905.10742},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Xiang, Li - Unknown - Disentangling Style and Content in Anime Illustrations.pdf:pdf},
title = {{Disentangling Style and Content in Anime Illustrations}},
url = {http://arxiv.org/abs/1905.10742},
year = {2019}
}
@article{Eremenko2020PerformanceLearning,
abstract = {Recent technological developments are having a significant impact on musical instruments and singing voice learning. A proof is the number of successful software applications that are being used by aspiring musicians in their regular practice. These practicing apps offer many useful functionalities to support learning, including performance assessment technologies that analyze the sound produced by the student while playing, identifying performance errors and giving useful feedback. However, despite the advancements in these sound analysis technologies, they are still not reliable and effective enough to support the strict requirements of a professional music education context. In this article we first introduce the topic and context, reviewing some of the work done in the practice of music assessment, then going over the current state of the art in performance assessment technologies, and presenting, as a proof of concept, a complete assessment system that we have developed for supporting guitar exercises. We conclude by identifying the challenges that should be addressed in order to further advance these assessment technologies and their useful integration into professional learning contexts.},
annote = {formative purposes / summative purposes.

Rubrics: leve + descriptions},
author = {Eremenko, Vsevolod and Morsi, Alia and Narang, Jyoti and Serra, Xavier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Eremenko et al. - Unknown - Performance Assessment Technologies for the Support of Musical Instrument Learning.pdf:pdf},
journal = {Proceedings of the 12th International Conference on Computer Supported Education (CSME)},
keywords = {Audio Signal Processing,Machine Learning,Music Assessment,Music Education,Music Information Retrieval,Music Performance Analysis},
title = {{Performance Assessment Technologies for the Support of Musical Instrument Learning}},
url = {https://orcid.org/0000-0003-1395-2345},
year = {2020}
}
@article{Mycka2023,
abstract = {This paper considers the problem of automatic music generation in the form of chord voicings, which is an important part of melody harmonization, with the use of Evolutionary Algorithms (EA), based on the rules derived from music theory and practice. The rules, whose role is to ensure the fulfillment of both the formal requirements of chord voicings and its less-formalized aesthetic requirements are encoded in the fitness function of EA. The fitness function is composed of several modules, each of which consists of smaller parts corresponding to the implemented music rules. The above modular design allows for flexible modification and extension of this function. The way the fitness function is constructed and tuned for better chord voicings quality is discussed in the context of music theory and technical EA implementation. In particular, we show how could generated voices be modeled by means of adjusting the relevance of particular fitness function components or extended by adding new components to the fitness function. The proposed algorithm is tested on two types of music: tonal and modal. Although tonal and modal chord voicings are significantly different, the achieved results (assessed by a human expert) indicate that the obtained solutions are both technically and aesthetically correct (i.e. they adhere to the theoretical rules and are nice to listen to). This study extends our previously published conference paper (Mycka et al., 2022).},
author = {Mycka, Jan and {\.{Z}}ychowski, Adam and Ma{\'{n}}dziuk, Jacek},
doi = {10.1016/j.jocs.2023.101963},
file = {:Users/huanzhang/Downloads/1-s2.0-S1877750323000236-main.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Chord voicings,Evolutionary algorithm,Harmonization,Music generation},
number = {October 2022},
pages = {101963},
publisher = {Elsevier B.V.},
title = {{Toward human-level tonal and modal melody harmonizations}},
url = {https://doi.org/10.1016/j.jocs.2023.101963},
volume = {67},
year = {2023}
}
@inproceedings{Knight2011TheQuality,
abstract = {The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (good and bad) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (good, medium, and bad), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%). {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Knight, Trevor and Upham, Finn and Fujinaga, Ichiro},
booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Knight, Upham - 2011 - THE POTENTIAL FOR AUTOMATIC ASSESSMENT OF TRUMPET TONE QUALITY.pdf:pdf},
isbn = {9780615548654},
pages = {573--578},
title = {{The potential for automatic assessment of trumpet tone quality}},
year = {2011}
}
@inproceedings{Wang2020LearningGeneration,
abstract = {While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.},
archivePrefix = {arXiv},
arxivId = {2008.07122},
author = {Wang, Ziyu and Wang, Dingsu and Zhang, Yixiao and Xia, Gus},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2008.07122},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - LEARNING INTERPRETABLE REPRESENTATION FOR CONTROLLABLE POLYPHONIC MUSIC GENERATION.pdf:pdf},
title = {{Learning Interpretable Representation for Controllable Polyphonic Music Generation}},
url = {http://arxiv.org/abs/2008.07122},
year = {2020}
}
@article{Liu2023,
abstract = {Diffusion models, as a novel generative paradigm, have achieved remarkable success in various image generation tasks such as image inpainting, image-to-text translation, and video generation. Graph generation is a crucial computational task on graphs with numerous real-world applications. It aims to learn the distribution of given graphs and then generate new graphs. Given the great success of diffusion models in image generation, increasing efforts have been made to leverage these techniques to advance graph generation in recent years. In this paper, we first provide a comprehensive overview of generative diffusion models on graphs, In particular, we review representative algorithms for three variants of graph diffusion models, i.e., Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Model (DDPM), and Score-based Generative Model (SGM). Then, we summarize the major applications of generative diffusion models on graphs with a specific focus on molecule and protein modeling. Finally, we discuss promising directions in generative diffusion models on graph-structured data.},
archivePrefix = {arXiv},
arxivId = {2302.02591},
author = {Liu, Chengyi and Fan, Wenqi and Liu, Yunqing and Li, Jiatong and Li, Hang and Liu, Hui and Tang, Jiliang and Li, Qing},
doi = {10.24963/ijcai.2023/751},
eprint = {2302.02591},
file = {:Users/huanzhang/Downloads/2302.02591.pdf:pdf},
isbn = {9781956792034},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {6702--6711},
title = {{Generative Diffusion Models on Graphs: Methods and Applications}},
volume = {2023-Augus},
year = {2023}
}
@article{Lin,
archivePrefix = {arXiv},
arxivId = {arXiv:2402.09508v1},
author = {Lin, Liwei and Xia, Gus and Zhang, Yixiao and Jiang, Junyan},
eprint = {arXiv:2402.09508v1},
file = {:Users/huanzhang/Downloads/2402.09508.pdf:pdf},
title = {{Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls}}
}
@inproceedings{Ho2022Classifier-FreeGuidance,
abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
address = {Online},
archivePrefix = {arXiv},
arxivId = {2207.12598},
author = {Ho, Jonathan and Salimans, Tim},
booktitle = {NeurIPS Workshop on Deep Generative Models and Downstream Applications},
eprint = {2207.12598},
file = {:Users/huanzhang/Downloads/2207.12598.pdf:pdf},
title = {{Classifier-Free Diffusion Guidance}},
url = {http://arxiv.org/abs/2207.12598},
year = {2021}
}
@article{SnellPrototypicalLearning,
abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
archivePrefix = {arXiv},
arxivId = {1703.05175v2},
author = {Snell, Jake and Twitter, Kevin Swersky and Zemel, Richard S},
eprint = {1703.05175v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Snell, Twitter, Zemel - Unknown - Prototypical Networks for Few-shot Learning.pdf:pdf},
title = {{Prototypical Networks for Few-shot Learning}}
}
@techreport{Real2020AutoML-Zero:Scratch,
abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research , known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks , where it has relied on sophisticated expert-designed layers as building blocks-or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neu-ral networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions , normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
archivePrefix = {arXiv},
arxivId = {2003.03384v2},
author = {Real, Esteban and Liang, Chen and So, David R and Le, Quoc V},
eprint = {2003.03384v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Real et al. - 2020 - AutoML-Zero Evolving Machine Learning Algorithms From Scratch.pdf:pdf},
title = {{AutoML-Zero: Evolving Machine Learning Algorithms From Scratch}},
url = {https://github.},
year = {2020}
}
@article{Foster2021,
abstract = {The Filosax dataset is a large collection of specially commissioned recordings of jazz saxophonists playing with commercially available backing tracks. Five participants each recorded themselves playing the melody, interpreting a transcribed solo and improvising on 48 tracks, giving a total of around 24 hours of audio data. The solos are annotated both as individual note events with physical timing , and as sheet music with a metrical interpretation of the timing. In this paper, we outline the criteria used for choosing and sourcing the repertoire, the recording process and the semi-automatic transcription pipeline. We demonstrate the use of the dataset to analyse musical phenomena such as swing timing and dynamics of typical musical figures , as well as for training a source activity detection system and predicting expressive characteristics. Other potential applications include the modelling of jazz improvisation , performer identification, automatic music transcription , source separation and music generation.},
author = {Foster, Dave and Dixon, Simon},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Foster, Dixon - Unknown - FILOSAX A DATASET OF ANNOTATED JAZZ SAXOPHONE RECORDINGS.pdf:pdf},
journal = {In Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Filosax: A Dataset of Annotated Jazz Saxophone Recordings}},
url = {https://www.halleonard.com/series/OMNIBK?},
year = {2021}
}
@article{Saeed,
abstract = {We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. Our approach is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning to design a lightweight, easy-to-implement self-supervised model of audio. We pre-train em-beddings on the large-scale Audioset database and transfer these representations to 9 diverse classification tasks, including speech, music, animal sounds, and acoustic scenes. We show that despite its simplicity, our method significantly out-performs previous self-supervised systems. We furthermore conduct ablation studies to identify key design choices and release a library 1 to pre-train and fine-tune COLA models.},
archivePrefix = {arXiv},
arxivId = {2010.10915v1},
author = {Saeed, Aaqib and Grangier, David and Zeghidour, Neil},
eprint = {2010.10915v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Saeed, Grangier, Zeghidour - Unknown - CONTRASTIVE LEARNING OF GENERAL-PURPOSE AUDIO REPRESENTATIONS.pdf:pdf},
keywords = {Index Terms-self-supervised learning,audio,sound},
title = {{CONTRASTIVE LEARNING OF GENERAL-PURPOSE AUDIO REPRESENTATIONS}},
url = {https://github.com/google-research/google-research/}
}
@inproceedings{Hashida2008Rencon:Systems,
address = {Sapporo},
author = {Hashida, M and Nakra, M and Katayose, H and Murao, T and Hirata, K and Suzuki, K and Kitahara, T},
booktitle = {Proceedings of the 10th International Conference on Music Perception and Cognition (ICMPC).},
file = {:Users/huanzhang/Downloads/ICMPC2008.pdf:pdf},
title = {{Rencon: Performance Rendering Contest for Automated Music Systems}},
year = {2008}
}
@inproceedings{Liang2017AutomaticLSTM,
author = {Liang, Feynman and Gotham, Mark and Johnson, Matthew and Shotton, Jamie},
booktitle = {Proceeding of the 18th International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/000156.pdf:pdf},
title = {{Automatic Stylistic Composition of Bach Chorales with Deep LSTM}},
year = {2017}
}
@article{Shirian2022,
abstract = {Large-scale databases with high-quality manual labels are scarce in audio domain. We thus explore a self-supervised graph approach to learning audio representations from highly limited labelled data. Considering each audio sample as a graph node, we propose a subgraph-based framework with novel self-supervision tasks to learn effective audio representations. During training, subgraphs are constructed by sampling the entire pool of available training data to exploit the relationship between the labelled and unlabeled audio samples. During inference, we use random edges to alleviate the overhead of graph construction. We evaluate our model on three benchmark audio datasets spanning two tasks: acoustic event classification and speech emotion recognition. We show that our semi-supervised model performs better or on par with fully supervised models and outperforms several competitive existing models. Our model is compact and can produce generalized audio representations robust to different types of signal noise.},
archivePrefix = {arXiv},
arxivId = {2202.00097},
author = {Shirian, Amir and Somandepalli, Krishna and Guha, Tanaya},
doi = {10.1109/JSTSP.2022.3190083},
eprint = {2202.00097},
file = {:Users/huanzhang/Downloads/6801.pdf:pdf},
issn = {19410484},
journal = {IEEE Journal on Selected Topics in Signal Processing},
keywords = {Acoustic event classification,graph neural network,self-supervised learning,semi-supervised learning,speech emotion recognition,sub-graph construction},
number = {6},
pages = {1391--1401},
publisher = {IEEE},
title = {{Self-Supervised Graphs for Audio Representation Learning With Limited Labeled Data}},
volume = {16},
year = {2022}
}
@article{Deutsch2006AbsoluteSpeech-r,
abstract = {Absolute pitch is extremely rare in the U.S. and Europe; this rarity has so far been unexplained. This paper reports a substantial difference in the prevalence of absolute pitch in two normal populations, in a large-scale study employing an on-site test, without self-selection from within the target populations. Music conservatory students in the U.S. and China were tested. The Chinese subjects spoke the tone language Mandarin, in which pitch is involved in conveying the meaning of words. The American subjects were nontone language speakers. The earlier the age of onset of musical training, the greater the prevalence of absolute pitch; however, its prevalence was far greater among the Chinese than the U.S. students for each level of age of onset of musical training. The findings suggest that the potential for acquiring absolute pitch may be universal, and may be realized by enabling infants to associate pitches with verbal labels during the critical period for acquisition of features of their native language.},
annote = {From Duplicate 2 (Absolute pitch among American and Chinese conservatory students: Prevalence differences, and evidence for a speech-related critical period Absolute pitch among American and Chinese conservatory students: Prevalence differences, and evidence for a speech-r - Deutsch, Diana; Henthorn, Trevor; Marvin, Elizabeth; Xu, Hongshuai)

Week 2},
author = {Deutsch, Diana and Henthorn, Trevor and Marvin, Elizabeth and Xu, Hongshuai},
doi = {10.1121/1.2151799},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Deutsch et al. - 2006 - Absolute pitch among American and Chinese conservatory students Prevalence differences, and evidence for a speec.pdf:pdf},
journal = {Absolute Pitch The Journal of the Acoustical Society of America},
pages = {1180},
title = {{Absolute pitch among American and Chinese conservatory students: Prevalence differences, and evidence for a speech-related critical period Absolute pitch among American and Chinese conservatory students: Prevalence differences, and evidence for a speech-r}},
url = {https://doi.org/10.1121/1.1908155},
volume = {119},
year = {2006}
}
@article{Wu2022,
abstract = {Even with strong sequence models like Transformers, generating expressive piano performances with long-range musical structures remains challenging. Meanwhile, methods to compose well-structured melodies or lead sheets (melody + chords), i.e., simpler forms of music, gained more success. Observing the above, we devise a two-stage Transformer-based framework that Composes a lead sheet first, and then Embellishes it with accompaniment and expressive touches. Such a factorization also enables pretraining on non-piano data. Our objective and subjective experiments show that Compose & Embellish shrinks the gap in structureness between a current state of the art and real performances by half, and improves other musical aspects such as richness and coherence as well.},
archivePrefix = {arXiv},
arxivId = {2209.08212},
author = {Wu, Shih-Lun and Yang, Yi-Hsuan},
eprint = {2209.08212},
file = {:Users/huanzhang/Downloads/2209.08212.pdf:pdf},
title = {{Compose & Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach}},
url = {http://arxiv.org/abs/2209.08212},
year = {2022}
}
@inproceedings{Ramoneda2022ScoreFingering,
abstract = {In this paper, we introduce score difficulty classification as a sub-task of music information retrieval (MIR), which may be used in music education technologies, for personalised curriculum generation , and score retrieval. We introduce a novel dataset for our task, Mikrokosmos-difficulty, containing 147 piano pieces in symbolic representation and the corresponding difficulty labels derived by its composer B{\'{e}}la Bart{\'{o}}k and the publishers. As part of our methodology, we propose piano technique feature representations based on different piano fingering algorithms. We use these features as input for two classifiers: a Gated Recurrent Unit neural network (GRU) with attention mechanism and gradient-boosted trees trained on score segments. We show that for our dataset fingering based features perform better than a simple baseline considering solely the notes in the score. Furthermore, the GRU with attention mechanism classifier surpasses the gradient-boosted trees. Our proposed models are interpretable and are capable of generating difficulty feedback both locally, on short term segments, and globally, for whole pieces. Code, datasets, models, and an online demo are made available for reproducibility.},
archivePrefix = {arXiv},
arxivId = {2203.13010v1},
author = {Ramoneda, Pedro and {Can Tamer}, Nazif and Eremenko, Vsevolod and Serra, Xavier and Miron, Marius},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.5281/zenodo.6092709},
eprint = {2203.13010v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ramoneda et al. - Unknown - Score Difficulty Analysis for Piano.pdf:pdf},
title = {{Score Difficulty Analysis for Piano Performance Education based on Fingering}},
url = {https://arxiv.org/abs/2203.13010},
year = {2022}
}
@techreport{Lerch2009Software-BasedPerformances,
abstract = {Different music performances of the same score may significantly differ from each other. It is obvious that not only the composer's work, the score, defines the listener's music experience, but that the music performance itself is an integral part of this experience. Music performers use the information contained in the score, but interpret, transform or add to this information.\n\nFour parameter classes can be used to describe a performance objectively: tempo and timing, loudness, timbre and pitch. Each class contains a multitude of individual parameters that are at the performers' disposal to generate a unique physical rendition of musical ideas.\n\nThe extraction of such objective parameters is one of the difficulties in music performance research. This work presents an approach to the software-based extraction of tempo and timing, loudness and timbre parameters from audio files to provide a tool for the automatic parameter extraction from music performances.\n\nThe system is applied to extract data from 21 string quartet performances and a detailed analysis of the extracted data is presented. The main contributions of this thesis are the adaptation and development of signal processing approaches to performance parameter extraction and the presentation and discussion of string quartet performances of a movement of Beethoven's late String Quartet op. 130.},
author = {Lerch, Alexander},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Fakult{\"{a}}t, Dipl, Lerch - Unknown - Software-Based Extraction of Objective Parameters from Music Performances.pdf:pdf},
isbn = {978-3640294961},
title = {{Software-Based Extraction of Objective Parameters from Music Performances}},
url = {http://opus.kobv.de/tuberlin/volltexte/2008/2067/pdf/lerch_alexander.pdf},
year = {2009}
}
@article{Rusch,
abstract = {We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to keyword spotting and language model-ing, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.},
author = {Rusch, T Konstantin and Z{\"{u}}rich, Eth and Mishra, Siddhartha and Erichson, N Benjamin and Mahoney, Michael W},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rusch et al. - Unknown - LONG EXPRESSIVE MEMORY FOR SEQUENCE MODELING.pdf:pdf},
title = {{Long Expressive Memory for Sequence Modeling}}
}
@inproceedings{Zbontar2021BarlowReduction,
archivePrefix = {arXiv},
arxivId = {2103.03230v3},
author = {Zbontar, Jure and Jing, Li and Misra, Ishan and Lecun, Yann and Deny, St{\'{e}}phane},
booktitle = {In proceedings of the International Conference on Machine Learning (ICML) 2021},
eprint = {2103.03230v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redundancy Reduction.pdf:pdf},
title = {{Barlow Twins: Self-Supervised Learning via Redundancy Reduction}},
url = {https://github.com/facebookresearch/barlowtwins},
year = {2021}
}
@article{Kanani2023GraphGeneration,
abstract = {Our study aims to compare the effects of direct mutation and graph-based mutation on representations of music domain. We focus on short tunes from the Irish folk tradition, represented as integer sequences, and use a graph-based representation based on Pathway Assembly (a directed acyclic graph) and the Sequitur algorithm. We define multiple mutation operators to work directly on the sequences or on the graphs, hypothesizing that graph-based mutations will tend to preserve the pattern used per tune, while direct mutation of sequences will tend to destroy patterns, resulting in new generated tunes that are more complex. We perform experiments on a corpus of tunes and apply the mutation operators many times consecutively to analyze their effects.},
author = {Kanani, Maziar and O'Leary, Se{\'{a}}n and McDermott, James},
doi = {10.1145/3583133.3596318},
file = {:Users/huanzhang/Downloads/Graph-Based Mutations for Music Generation.pdf:pdf},
isbn = {9798400701207},
journal = {GECCO 2023 Companion - Proceedings of the 2023 Genetic and Evolutionary Computation Conference Companion},
keywords = {Pathway Assembly,Sequitur,genetic algorithm,genetic programming,graph-based mutation,music generation},
number = {18},
title = {{Graph-Based Mutations for Music Generation}},
year = {2023}
}
@inproceedings{Kosta2018MazurkaBLRecordings,
author = {Kosta, Katerina and Bandtlow, Oscar F and Chew, Elaine},
booktitle = {Proceedings of the International Conference on Technologies for Music Notation and Representation -- TENOR'18},
file = {:Users/huanzhang/Downloads/tenor2018kosta.pdf:pdf},
isbn = {978-1-5251-0551-7},
pages = {85--94},
title = {{{MazurkaBL}: Score-aligned Loudness, Beat, and Expressive Markings Data for 2000 Chopin Mazurka Recordings}},
year = {2018}
}
@inproceedings{Kong2021DiffwaveSynthesis,
abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audio in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
address = {Vienna, Austria},
archivePrefix = {arXiv},
arxivId = {2009.09761},
author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
booktitle = {ICLR 2021 - 9th International Conference on Learning Representations},
eprint = {2009.09761},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kong et al. - Unknown - DIFFWAVE A VERSATILE DIFFUSION MODEL FOR AUDIO SYNTHESIS.pdf:pdf},
title = {{{DiffWave}: a Versatile Diffusion Model for Audio Synthesis}},
url = {https://diffwave-demo.github.io/},
year = {2021}
}
@article{Bohm2017,
abstract = {The quality of the singing voice is an important aspect of subjective, aesthetic perception of music. In this contribution, we propose a method to automatically assess perceived singing quality. We classify monophonic vocal recordings without accompaniment into one of three classes of singing quality. Unprocessed private and non-commercial recordings from a social media website are utilised. In addition to the user ratings given on the website, we let both subjects with and without a musical background annotate the samples. Building on musicological foundations, we define and extract acoustic parameters describing the quality of the sound, musical expression and intonation of the singing. Besides features which are already established in the field of Music Information Retrieval, such as loudness and mel-frequency cepstral coefficients, we propose and employ new types of features which are specific to intonation. For automatic classification by supervised machine learning methods, models predicting the subjective ratings and the user ratings on the social media website are learnt. We perform an exhaustive evaluation of both different classifiers and combinations of features. We show that the performance of automatic classification is close to that of human evaluators. Utilising support vector machines, an accuracy of classification of 55.4 %, based on the subjective ratings, and of 84.7 %, based on the user ratings of the social media website, are achieved.},
author = {Bohm, Johanna and Eyben, Florian and Schmitt, Maximilian and Kosch, Harald and Schuller, Bjorn},
doi = {10.1109/IJCNN.2017.7966037},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bohm et al. - 2017 - Seeking the SuperStar Automatic assessment of perceived singing quality.pdf:pdf},
isbn = {9781509061815},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {1560--1569},
title = {{Seeking the SuperStar: Automatic assessment of perceived singing quality}},
volume = {2017-May},
year = {2017}
}
@inproceedings{AnnaHuang2017CounterpointConvolution,
abstract = {Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE [36], which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality , which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from [40] yields better samples than ancestral sampling, based on both log-likelihood and human evaluation.},
author = {{Anna Huang}, Cheng-Zhi and Cooijmans, Tim and {Roberts Aaron Courville}, Adam and {Eck Equal contributions}, Douglas},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Anna Huang et al. - Unknown - COUNTERPOINT BY CONVOLUTION.pdf:pdf},
title = {{Counterpoint by Convolution}},
url = {https://coconets.github.io/},
year = {2017}
}
@inproceedings{Xu2022,
abstract = {Long-term action quality assessment is a task of evaluating how well an action is performed, namely, estimating a quality score from a long video. Intuitively, long-term actions generally involve parts exhibiting different levels of skill, and we call the levels of skill as performance grades. For example, technical highlights and faults may appear in the same long-term action. Hence, the final score should be determined by the comprehensive effect of different grades exhibited in the video. To explore this latent relationship, we design a novel Likert scoring paradigm in-spired by the Likert scale in psychometrics, in which we quantify the grades explicitly and generate the final quality score by combining the quantitative values and the corresponding responses estimated from the video, instead of performing direct regression. Moreover, we extract grade-specific features, which will be used to estimate the responses of each grade, through a Transformer decoder architecture with diverse learnable queries. The whole model is named as Grade-decoupling Likert Transformer (GDLT), and we achieve state-of-the-art results on two long-term action assessment datasets.11Project page https://isee-ai.cn/-angchi/CVPR22_GDLT.html},
author = {Xu, Angchi and Zeng, Ling An and Zheng, Wei Shi},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR52688.2022.00323},
file = {:Users/huanzhang/Downloads/Xu_Likert_Scoring_With_Grade_Decoupling_for_Long-Term_Action_Assessment_CVPR_2022_paper.pdf:pdf},
isbn = {9781665469463},
issn = {10636919},
keywords = {Video analysis and understanding},
pages = {3222--3231},
title = {{Likert Scoring with Grade Decoupling for Long-term Action Assessment}},
volume = {2022-June},
year = {2022}
}
@book{Duffy2018,
abstract = {Clapping Music is a minimalist work by Steve Reich based on twelve phased variations of a rhythmic pattern. It has been reimagined as a game-based mobile application, designed with a dual purpose. First, to introduce new audiences to the Minimalist genre through interaction with the piece presented as an engaging game. Second, to use large-scale data collection within the app to address research questions about the factors determining rhythm production performance. The twelve patterns can be differentiated using existing theories of rhythmic complexity. Using performance indicators from the game such as tap accuracy we can determine which patterns players found most challenging and so assess hypotheses from theoretical models with empirical evidence. The app has been downloaded over 140,000 times since the launch in July 2015, and over 46 million rows of gameplay data have been collected, requiring a big data approach to analysis. The results shed light on the rhythmic factors contributing to performance difficulty and show that the effect of making a transition from one pattern to the next is as significant, in terms of pattern difficulty, as the inherent complexity of the pattern itself. Challenges that arose in applying this novel approach are discussed.},
author = {Duffy, Sam and Pearce, Marcus},
booktitle = {PLoS ONE},
doi = {10.1371/journal.pone.0205847},
file = {:Users/huanzhang/Downloads/file (1).pdf:pdf},
isbn = {1111111111},
issn = {19326203},
number = {10},
pmid = {30335798},
title = {{What makes rhythms hard to perform? An investigation using Steve Reich's Clapping Music}},
volume = {13},
year = {2018}
}
@inproceedings{Ramesh2021Zero-ShotGeneration,
abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
address = {Online},
archivePrefix = {arXiv},
arxivId = {2102.12092},
author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
eprint = {2102.12092},
file = {:Users/huanzhang/Downloads/2102.12092.pdf:pdf},
isbn = {9781713845065},
issn = {26403498},
title = {{Zero-Shot Text-to-Image Generation}},
year = {2021}
}
@article{HuangLegendOpera,
author = {Huang, Lingdong and Jiang, Zheng and Sun, Syuan-cheng and Bai, Tong and Kang, Eunsu and Poczos, Barnabas},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - Unknown - Legend of Wrong Mountain AI Generated Opera.pdf:pdf},
keywords = {according to the script,an emperor,by machine intelligence,generated,generated by machine,his lords,is a story about,legend of wrong mountain,our result,the first kunqu opera},
title = {{Legend of Wrong Mountain : AI Generated Opera}}
}
@inproceedings{Lesort2019TrainingOnes,
abstract = {Generative models are known to be difficult to assess. Recent works, especially on generative adversarial networks (GANs), produce good visual samples of varied categories of images. However, the validation of their quality is still difficult to define and there is no existing agreement on the best evaluation process. This paper aims at making a step toward an objective evaluation process for generative models. It presents a new method to assess a trained generative model by evaluating the test accuracy of a classifier trained with generated data. The test set is composed of real images. Therefore, The classifier accuracy is used as a proxy to evaluate if the generative model fit the true data distribution. By comparing results with different generated datasets we are able to classify and compare generative models. The motivation of this approach is also to evaluate if generative models can help discriminative neural networks to learn, i.e., measure if training on generated data is able to make a model successful at testing on real settings. Our experiments compare different generators from the Variational Auto-Encoders (VAE) and Generative Adversarial Network (GAN) frameworks on MNIST and fashion MNIST datasets. Our results show that none of the gen-erative models is able to replace completely true data to train a discriminative model. But they also show that the initial GAN and WGAN are the best choices to generate on MNIST database (Modified National Institute of Standards and Technology database) and fashion MNIST database.},
archivePrefix = {arXiv},
arxivId = {1806.10840v2},
author = {Lesort, Timothee and Stoian, Andrei and Goudou, Jean-Francois and Filliat, David},
booktitle = {Proceedings of the International Conference on Artificial Neural Networks},
eprint = {1806.10840v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Training Discriminative Models to Evaluate Generative Ones.pdf:pdf},
isbn = {1806.10840v2},
title = {{Training Discriminative Models to Evaluate Generative Ones}},
year = {2019}
}
@article{Furuya2023,
abstract = {Sensory afferent information, such as auditory and somatosensory feedback while moving, plays a crucial role in both control and learning of motor performance across the lifespan. Music performance requires skillful integration of multimodal sensory information for the production of dexterous movements. However, it has not been understood what roles somatosensory afferent information plays in the acquisition and sophistication of specialized motor skills of musicians across different stages of development. In the present preliminary study, we addressed this issue by using a novel technique with a hand exoskeleton robot that can externally move the fingers of pianists. Short-term exposure to fast and complex finger movements generated by the exoskeleton (i.e., passive movements) increased the maximum rate of repetitive piano keystrokes by the pianists. This indicates that somatosensory inputs derived from the externally generated motions enhanced the quickness of the sequential finger movements in piano performance, even though the pianists did not voluntarily move the fingers. The enhancement of motor skill through passive somatosensory training using the exoskeleton was more pronounced in adolescent pianists than adult pianists. These preliminary results implicate a sensitive period of neuroplasticity of the somatosensory-motor system of trained pianists, which emphasizes the importance of somatosensory-motor training in professional music education during adolescence.},
author = {Furuya, Shinichi and Tanibuchi, Ryuya and Nishioka, Hayato and Kimoto, Yudai and Hirano, Masato and Oku, Takanori},
doi = {10.1111/nyas.14939},
file = {:Users/huanzhang/Downloads/Annals of the New York Academy of Sciences - 2022 - Furuya - Passive somatosensory training enhances piano skill in.pdf:pdf},
issn = {17496632},
journal = {Annals of the New York Academy of Sciences},
keywords = {exoskeleton,experts,plasticity,sensorimotor learning},
number = {1},
pages = {167--172},
pmid = {36398868},
title = {{Passive somatosensory training enhances piano skill in adolescent and adult pianists: A preliminary study}},
volume = {1519},
year = {2023}
}
@article{Poli2004,
abstract = {Expression is an important aspect of music performance. It is the added value of a performance, and is part of the reason that music is interesting to listen to and sounds alive. Moreover, understanding and modelling expressive content communication is important in many engineering applications. In human musical performance, acoustical or perceptual changes in sound are organized in a complex way by the performer in order to communicate musical content to the listener. The same piece of music can be performed trying to convey a specific interpretation of the score by adding mutable expressive intentions. The analysis of these systematic deviations has led to the formulation of several models that try to describe their structures, with the aim of explaining where, how and why a performer modifies, sometime in an unconscious way, what is indicated by the notation of the score. Modelling paradigms and problems are reviewed and issues for future research efforts are discussed. {\textcopyright} 2004, Taylor & Francis Group, LLC.},
author = {Poli, Giovanni De},
doi = {10.1080/0929821042000317796},
file = {:Users/huanzhang/Downloads/Methodologies for Expressiveness Modelling of and for Music Performance.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
number = {3},
pages = {189--202},
title = {{Methodologies for Expressiveness Modelling of and for Music Performance}},
volume = {33},
year = {2004}
}
@article{Yeon2021SocialPlaylists,
abstract = {Social interactions, such as sharing songs and listening together, are fundamental to the experience of music. Yet our understanding of these interactions and how they influence social dynamics with today's streaming platforms is lacking. To better understand successful instances of social music practice, we conducted a two-part study to investigate real-world usage of collaborative playlists (CPs). Using an exploratory survey, we queried CP users on characteristics-Who, What, When, Where, Why, and How-and practices around favorite CPs, which serve as concrete examples of successful social music curation on streaming platforms. We found these playlists to vary in group sizes, purposes, listening contexts, engagement behaviors, and content attributes. We also observed significant cross-category interactions; for one, group size led to differences in perceived roles and frequency of actions within users' favorite CPs. Subsequent interviews confirmed favorite CPs as being exemplary of success, and users further elucidated factors that engender and hinder CP success. Together, our results underscore the importance of social motivations for engaging in CPs and of building greater understanding around these experiences. To these point we derived six design implications to inform development of CP platforms and online music platforms at large.},
author = {Yeon, S O and Kaneshiro, Blair},
doi = {10.1145/3449191},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yeon, Kaneshiro - 2021 - Social Music Curation That Works Insights from Successful Collaborative Playlists Social Music Curation That Wo.pdf:pdf},
keywords = {Collaborative playlist,Interview,Mixed methods,Music playlist,Music streaming,Online collaboration,Social music,Spotify,Successful curation,Survey,User behavior},
title = {{Social Music Curation That Works: Insights from Successful Collaborative Playlists; Social Music Curation That Works: Insights from Successful Collaborative Playlists}},
url = {https://doi.org/10.1145/3449191},
year = {2021}
}
@techreport{VanDenOordWAVENET:AUDIO,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predic-tive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499v2},
author = {{Van Den Oord}, A{\"{a}}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Van Den Oord et al. - Unknown - WAVENET A GENERATIVE MODEL FOR RAW AUDIO.pdf:pdf},
title = {{WAVENET: A GENERATIVE MODEL FOR RAW AUDIO}}
}
@inproceedings{Srivatsan2022ChecklistPrediction,
author = {Srivatsan, Nikita and Berg-kirkpatrick, Taylor},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/278.pdf:pdf},
title = {{CHECKLIST MODELS FOR IMPROVED OUTPUT FLUENCY IN PIANO FINGERING PREDICTION}},
year = {2022}
}
@article{Dannenberg2008HandbookAcoustics,
abstract = {applicability for this approach.},
author = {Dannenberg, Roger B and Goto, Masataka},
doi = {10.1007/978-0-387-30441-0},
file = {:Users/huanzhang/Downloads/Music_Structure_Analysis_from_Acoustic_Signals.pdf:pdf},
isbn = {9780387304410},
journal = {Handbook of Signal Processing in Acoustics},
number = {May 2014},
pages = {0--19},
title = {{Handbook of Signal Processing in Acoustics}},
year = {2008}
}
@article{CheukTheAccuracy,
abstract = {Most of the state-of-the-art automatic music transcription (AMT) models break down the main transcription task into sub-tasks such as onset prediction and offset prediction and train them with onset and offset labels. These predictions are then concatenated together and used as the input to train another model with the pitch labels to obtain the final transcription. We attempt to use only the pitch labels (together with spectrogram reconstruction loss) and explore how far this model can go without introducing supervised sub-tasks. In this paper, we do not aim at achieving state-of-the-art transcription accuracy, instead, we explore the effect that spectrogram reconstruction has on our AMT model. Our proposed model consists of two U-nets: the first U-net transcribes the spectrogram into a posteriorgram, and a second U-net transforms the posteriorgram back into a spectrogram. A reconstruction loss is applied between the original spectrogram and the reconstructed spectrogram to constrain the second U-net to focus only on reconstruction. We train our model on three different datasets: MAPS, MAESTRO, and MusicNet. Our experiments show that adding the reconstruction loss can generally improve the note-level transcription accuracy when compared to the same model without the reconstruction part. Moreover, it can also boost the frame-level precision to be higher than the state-of-the-art models. The feature maps learned by our U-net contain gridlike structures (not present in the baseline model) which implies that with the presence of the reconstruction loss, the model is probably trying to count along both the time and frequency axis, resulting in a higher note-level transcription accuracy.},
archivePrefix = {arXiv},
arxivId = {2010.09969v1},
author = {Cheuk, Kin Wai and Luo, Yin-Jyun and Benetos, Emmanouil and Herremans, Dorien},
eprint = {2010.09969v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cheuk et al. - Unknown - The Effect of Spectrogram Reconstruction on Automatic Music Transcription An Alternative Approach to Improve Tr.pdf:pdf},
title = {{The Effect of Spectrogram Reconstruction on Automatic Music Transcription: An Alternative Approach to Improve Transcription Accuracy}},
url = {https://github.com/KinWaiCheuk/ICPR2020}
}
@article{Balkwill2004,
abstract = {Japanese listeners rated the expression of joy, anger and sadness in Japanese, Western, and Hindustani music. Excerpts were also rated for tempo, loudness, and complexity. Listeners were sensitive to the intended emotion in music from all three cultures, and judgments of emotion were related to judgments of acoustic cues. High ratings of joy were associated with music judged to be fast in tempo and melodically simple. High ratings of sadness were associated with music judged to be slow in tempo and melodically complex. High ratings of anger were associated with music judged to be louder and more complex. The findings suggest that listeners are sensitive to emotion in familiar and unfamiliar music, and this sensitivity is associated with the perception of acoustic cues that transcend cultural boundaries.},
author = {Balkwill, Laura Lee and Thompson, William Forde and Matsunaga, Rie},
doi = {10.1111/j.1468-5584.2004.00265.x},
file = {:Users/huanzhang/Downloads/Recognition_of_emotion_in_Japanese_Western_and_Hin.pdf:pdf},
issn = {00215368},
journal = {Japanese Psychological Research},
keywords = {Cross-culture,Music,Recognition of emotion},
number = {4},
pages = {337--349},
title = {{Recognition of emotion in Japanese, Western, and Hindustani music by Japanese listeners}},
volume = {46},
year = {2004}
}
@article{Liang2024,
author = {Liang, Yun and Lin, Hai and Qiu, Shaojian and Zhang, Yihang},
file = {:Users/huanzhang/Downloads/8961.pdf:pdf},
pages = {1--5},
title = {{FOR REVIEW ONLY AAT : ADAPTING AUDIO TRANSFORMER FOR VARIOUS}},
year = {2024}
}
@article{Liu,
abstract = {This study investigated whether congenital amusia, a neuro-developmental disorder of musical perception, also has implications for speech intonation processing. In total, 16 British amusics and 16 matched controls completed five intonation perception tasks and two pitch threshold tasks. Compared with controls, amusics showed impaired performance on discrimination, identification and imitation of statements and questions that were characterized primarily by pitch direction differences in the final word. This intonation-processing deficit in amusia was largely associated with a psychophysical pitch direction discrimination deficit. These findings suggest that amusia impacts upon one's language abilities in subtle ways, and support previous evidence that pitch processing in language and music involves shared mechanisms.},
annote = {Week 2},
author = {Liu, Fang and Patel, Aniruddh D and Fourcin, Adrian and Stewart, Lauren},
doi = {10.1093/brain/awq089},
journal = {A JOURNAL OF NEUROLOGY},
keywords = {congenital amusia,intonation processing,pitch change/direction,pitch threshold,statement-question discrimination/identification/i},
title = {{Intonation processing in congenital amusia: discrimination, identification and imitation Abbreviations: F 0 = fundamental frequency; MBEA = Montreal Battery of Evaluation of Amusia; NART = National Adult Reading Test}},
url = {http://www.delosis.com/listening/home.html}
}
@article{Anglada-Tort2023,
abstract = {Article Large-scale iterated singing experiments reveal oral transmission mechanisms underlying music evolution Highlights d Online singing experiments enable large-scale music evolution studies d Vocal, cognitive, and cultural factors cause oral transmission biases d Transmission biases cause the emergence of diverse musical structures d Social factors amplify or attenuate transmission biases},
author = {Anglada-Tort, Manuel and Harrison, Peter M C and Lee, Harin and {Jacoby Correspondence}, Nori},
doi = {10.1016/j.cub.2023.02.070},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Anglada-Tort et al. - 2023 - Large-scale iterated singing experiments reveal oral transmission mechanisms underlying music evolution.pdf:pdf},
journal = {Current Biology},
keywords = {cultural evolution,cultural transmission,iterated learning,melody,music evolution,online experiments,oral transmission,pitch perception,singing,social learning},
title = {{Large-scale iterated singing experiments reveal oral transmission mechanisms underlying music evolution}},
url = {https://doi.org/10.1016/j.cub.2023.02.070},
year = {2023}
}
@article{LocatelloDisentanglingLabels,
abstract = {Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52 000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01-0.5% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.},
archivePrefix = {arXiv},
arxivId = {1905.01258v2},
author = {Locatello, Francesco and Tschannen, Michael and Bauer, Stefan and R{\"{a}}tsch, Gunnar and Sch{\"{o}}lkopf, Bernhard and Bachem, Olivier},
eprint = {1905.01258v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Locatello et al. - Unknown - DISENTANGLING FACTORS OF VARIATION USING FEW LABELS.pdf:pdf},
journal = {ICLR 2020},
title = {{Disentangling Factors of Variation Using Few Labels}}
}
@inproceedings{Tal2024JointGeneration,
abstract = {We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method. This allows music generation controlled both locally (e.g., chords) and globally (text description). Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This allows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence, using both objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/JASCO.},
archivePrefix = {arXiv},
arxivId = {2406.10970},
author = {Tal, Or and Ziv, Alon and Gat, Itai and Kreuk, Felix and Adi, Yossi},
booktitle = {Proceeding of the 25t International Society on Music Information Retrieval (ISMIR)},
eprint = {2406.10970},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tal et al. - 2024 - Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation.pdf:pdf},
month = {jun},
title = {{Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation}},
url = {http://arxiv.org/abs/2406.10970},
year = {2024}
}
@inproceedings{Puvvada2024DiscreteRecognition,
abstract = {Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.},
archivePrefix = {arXiv},
arxivId = {2309.10922},
author = {Puvvada, Krishna C. and Koluguri, Nithin Rao and Dhawan, Kunal and Balam, Jagadeesh and Ginsburg, Boris},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {2309.10922},
file = {:Users/huanzhang/Downloads/2309.10922.pdf:pdf},
title = {{Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition}},
url = {http://arxiv.org/abs/2309.10922},
year = {2024}
}
@inproceedings{Kotovenko2019ContentTransfer,
abstract = {Artists rarely paint in a single style throughout their career. More often they change styles or develop variations of it. In addition, artworks in different styles and even within one style depict real content differently: While Picasso's Blue Period displays a vase in a blueish tone but as a whole, his Cubist works deconstruct the object. To produce artistically convincing stylizations, style transfer models must be able to reflect these changes and variations. Recently many works have aimed to improve the style transfer task, but neglected to address the described observations. We present a novel approach which captures particularities of style and the variations within and separates style and content. This is achieved by introducing two novel losses: A fixpoint triplet style loss to learn subtle variations within one style or between different styles and a disentanglement loss to ensure that the stylization is not conditioned on the real input photo. In addition the paper proposes various evaluation methods to measure the importance of both losses on the validity, quality and variability of final stylizations. We provide qualitative results to demonstrate the performance of our approach.},
author = {Kotovenko, Dmytro and Sanakoyeu, Artsiom and Lang, Sabine and Ommer, Bj{\"{o}}rn},
booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2019.00452},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kotovenko et al. - Unknown - Content and Style Disentanglement for Artistic Style Transfer.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kotovenko et al. - Unknown - Content and Style Disentanglement for Artistic Style Transfer(2).pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
title = {{Content and Style Disentanglement for Artistic Style Transfer}},
year = {2019}
}
@article{Castellon2021CodifiedRetrieval,
abstract = {We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically , we explore representations from Jukebox [1]: a music generation system containing a language model trained on codified audio from 1M songs. To determine if Juke-box's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, key detection, and emotion recognition. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging , suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.},
archivePrefix = {arXiv},
arxivId = {2107.05677v1},
author = {Castellon, Rodrigo and Donahue, Chris and Liang, Percy},
eprint = {2107.05677v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Castellon, Donahue, Liang - Unknown - CODIFIED AUDIO LANGUAGE MODELING LEARNS USEFUL REPRESENTATIONS FOR MUSIC INFORMATION RETRIEVAL.pdf:pdf},
journal = {Proceedings of the 22th International Society for Music Information Retrieval Conference, ISMIR 2021},
title = {{Codified Audio Language Modeling Learns Useful Representations for Music Information Retrieval}},
url = {https://worksheets.codalab.org/worksheets/},
year = {2021}
}
@article{Liang2023AcousticCapabilities,
abstract = {The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce Acoustic Prompt Turning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at https://github.com/JinhuaLiang/APT.},
archivePrefix = {arXiv},
arxivId = {2312.00249},
author = {Liang, Jinhua and Liu, Xubo and Wang, Wenwu and Plumbley, Mark D. and Phan, Huy and Benetos, Emmanouil},
eprint = {2312.00249},
file = {:Users/huanzhang/Downloads/2312.00249v1.pdf:pdf},
journal = {arXiv: 2312.00249},
title = {{Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities}},
url = {http://arxiv.org/abs/2312.00249},
year = {2023}
}
@article{Egermann2015,
abstract = {Subjective and psychophysiological emotional responses to music from two different cultures were compared within these two cultures. Two identical experiments were conducted: the first in the Congolese rainforest with an isolated population of Mbenz{\'{e}}l{\'{e}} Pygmies without any exposure to Western music and culture, the second with a group of Western music listeners, with no experience with Congolese music. Forty Pygmies and 40 Canadians listened in pairs to 19 music excerpts of 29 to 99 seconds in duration in random order (8 from the Pygmy population and 11 Western instrumental excerpts). For both groups, emotion components were continuously measured: subjective feeling (using a two- dimensional valence and arousal rating interface), peripheral physiological activation, and facial expression. While Pygmy music was rated as positive and arousing by Pygmies, ratings of Western music by Westerners covered the range from arousing to calming and from positive to negative. Comparing psychophysiological responses to emotional qualities of Pygmy music across participant groups showed no similarities. However, Western stimuli, rated as high and low arousing by Canadians, created similar responses in both participant groups (with high arousal associated with increases in subjective and physiological activation). Several low-level acoustical features of the music presented (tempo, pitch, and timbre) were shown to affect subjective and physiological arousal similarly in both cultures. Results suggest that while the subjective dimension of emotional valence might be mediated by cultural learning, changes in arousal might involve a more basic, universal response to low-level acoustical characteristics of music.},
author = {Egermann, Hauke and Fernando, Nathalie and Chuen, Lorraine and McAdams, Stephen},
doi = {10.3389/fpsyg.2014.01341},
file = {:Users/huanzhang/Downloads/fpsyg-05-01341.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {affect,cross-cultural,emotion,music,psychophysiology,universa,universal},
number = {January},
pages = {1--9},
title = {{Music induces universal emotion-related psychophysiological responses: comparing Canadian listeners to Congolese Pygmies}},
volume = {5},
year = {2015}
}
@inproceedings{Ozer2022,
author = {{\"{O}}zer, Yigitcan},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/187.pdf:pdf},
title = {{Source Separation of Piano Concertos with Test-time Adaptation}},
year = {2022}
}
@inproceedings{Shi2019SupraArchive,
abstract = {This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stan-ford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a "raw" MIDI file of hole data, an "expressive" MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, "SUPRA-RW" from a selection of "red Welte"-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time.},
author = {Shi, Zhengshan and Sapp, Craig Stuart and Arul, Kumaran and McBride, Jerry and Smith, Julius O},
booktitle = {Proceeding of the 20th International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Shi et al. - Unknown - SUPRA DIGITIZING THE STANFORD UNIVERSITY PIANO ROLL ARCHIVE.pdf:pdf},
title = {{{SUPRA}: Digitaizing the Stanford University Piano Roll Archive}},
year = {2019}
}
@article{Blood2001IntenselyEmotion,
abstract = {We used positron emission tomography to study neural mechanisms underlying intensely pleasant emotional responses to music. Cerebral blood flow changes were measured in response to subject-selected music that elicited the highly pleasurable experience of "shivers-down-the-spine" or "chills." Subjective reports of chills were accompanied by changes in heart rate, electromyogram, and respiration. As intensity of these chills increased, cerebral blood flow increases and decreases were observed in brain regions thought to be involved in reward/motivation, emotion, and arousal, including ventral striatum, midbrain, amygdala, orbitofrontal cortex, and ventral medial prefrontal cortex. These brain structures are known to be active in response to other euphoriainducing stimuli, such as food, sex, and drugs of abuse. This finding links music with biologically relevant, survival-related stimuli via their common recruitment of brain circuitry involved in pleasure and reward.},
author = {Blood, Anne J. and Zatorre, Robert J.},
doi = {10.1073/pnas.191355898},
file = {:Users/huanzhang/Downloads/11818.full.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {20},
pages = {11818--11823},
pmid = {11573015},
title = {{Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion}},
volume = {98},
year = {2001}
}
@inproceedings{Hsiao2021CompoundHypergraphs,
abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties , existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5-10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.},
archivePrefix = {arXiv},
arxivId = {2101.02402v1},
author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
booktitle = {Proceedings of the 35th AAAI Conference on Artificial Intelligence},
eprint = {2101.02402v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.pdf:pdf},
title = {{Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs}},
year = {2021}
}
@article{Author2023,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Author, Author, Author - 2023 - A STUDY ON SYNTHESIZING EXPRESSIVE VIOLIN PERFORMANCES.pdf:pdf},
title = {{A STUDY ON SYNTHESIZING EXPRESSIVE VIOLIN PERFORMANCES :}},
year = {2023}
}
@inproceedings{Hawthorne2021Sequence-to-SequenceTransformers,
abstract = {Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoder-decoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design.},
archivePrefix = {arXiv},
arxivId = {2107.09142},
author = {Hawthorne, Curtis and Simon, Ian and Swavely, Rigel and Manilow, Ethan and Engel, Jesse},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2107.09142},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hawthorne et al. - Unknown - SEQUENCE-TO-SEQUENCE PIANO TRANSCRIPTION WITH TRANSFORMERS.pdf:pdf},
title = {{Sequence-to-Sequence Piano Transcription with Transformers}},
year = {2021}
}
@article{Palmer1993,
abstract = {The units of knowledge that form cognitive plans for music performance were examined in production errors. In Experiment 1, pianists performed multivoice homophonic music (containing strong across-voice associations) and polyphonic music (containing strong within-voice associations). Pitch errors reflected more chordal (across-voice) units in homophonic performances and more single-note units in polyphonic performances. Error instructions were harmonically and diatonically related to their intended pitches more often than chance, which demonstrates retrieval-based influences on planning. In Experiment 2, pianists conceptualized one of several voices as melody. Both the melody and the voice controlled by outer right-hand fingers (a common location of melody) contained fewer errors, which implies that there are conceptual, retrieval-based, and articulatory influences on units of knowledge that contribute to planning music performance.},
author = {Palmer, Caroline and van de Sande, Carla},
doi = {10.1037/0278-7393.19.2.457},
file = {:Users/huanzhang/Downloads/Units_of_Knowledge_in_Music_Performance.pdf:pdf},
issn = {02787393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {2},
pages = {457--470},
pmid = {8454966},
title = {{Units of Knowledge in Music Performance}},
volume = {19},
year = {1993}
}
@article{Honing,
abstract = {Rhythm, as it is performed and perceived, is only sparingly addressed in music theory. Existing theories of rhythmic structure are often restricted to music as notated in a score, and as a result are bound to refrain from making statements about music as it is perceived and appreciated by listeners. This paper outlines a cognitive approach to the study of rhythm and timing that allows for making scientific observations and statements about 'sounding' music, music as it is performed and listened to. This approach was developed over the last few years in the context of the Music, Mind, Machine project at the Nijmegen Institute for Cognition of Information. 1 In addition, the notion of rhythm space (the set of all possible performed rhythms) is elaborated into a systematic method for the investigation of the relation between rhythmic structure, expressive timing and tempo. As such the paper presents a research program that aims to develop a theory of music incorporating both the structural and perceptual aspects of musical time.},
annote = {From Duplicate 1 (Structure and interpretation of rhythm and timing - Honing, Henkjan)

Week 4},
author = {Honing, Henkjan},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Honing - Unknown - Structure and interpretation of rhythm and timing Structure and Interpretation of Rhythm and Timing.pdf:pdf},
title = {{Structure and interpretation of rhythm and timing}},
url = {https://www.researchgate.net/publication/239062094}
}
@inproceedings{Sarasua2019DatasetGestures,
abstract = {In this paper we present two datasets of instrumental gestures performed with expressive variations: five violinists performing standard pedagogical phrases with variation in dynamics and tempo; and two pianists performing a repertoire piece with variations in tempo, dynamics and articulation. We show the utility of these datasets by highlighting the different movement qualities embedded in both datasets. In addition, for the violin dataset, we report on gesture recognition tests using two state-of-The-Art realtime gesture recognizers. We believe that these resources create opportunities for further research on the understanding of complex human movements through computational methods.},
author = {Sarasu{\'{a}}, Aĺvaro and Caramiaux, Baptiste and Tanaka, Atau and Ortiz, Miguel},
booktitle = {In Proceedings of the 4th International Conference on Movement Computing (MOCO)},
doi = {10.1145/3077981.3078032},
file = {:Users/huanzhang/Downloads/sarasua2017datasets.pdf:pdf},
isbn = {1595930361},
keywords = {Database,EMG,Gesture recognition,Hidden Markov models,Machine learning,Motion capture,Myo,Particle filtering},
title = {{Datasets for the analysis of expressive musical gestures}},
year = {2017}
}
@article{HoningIntroductionMusicality,
abstract = {One contribution of 12 to a theme issue 'Biology, cognition and origins of musicality'.},
author = {Honing, Henkjan and ten Cate, Carel and Peretz, Isabelle and Trehub, Sandra E},
doi = {10.1098/rstb.2014.0088},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Honing et al. - Unknown - Introduction Without it no music cognition, biology and evolution of musicality.pdf:pdf},
keywords = {Subject Areas: behaviour,cognition,evolution,evolution of music,genetics Keywords: musicality,multicomponent view,music cognition,music perception,neuroscience,theoretical biology},
title = {{Introduction Without it no music: cognition, biology and evolution of musicality}},
url = {http://dx.doi.org/10.1098/rstb.2014.0088}
}
@techreport{Langner2002RepresentingSpace,
abstract = {Background A large number of very detailed studies analyse various parameters of expressive music performance (e.g., timing, dynamics, articulation. For an overview see Palmer, 1997; Gabrielsson, 1999). They often fall short by addressing each expressive parameter separately. In our every day experience, we perceive these parameters simultaneously and the impression of a performance always results from an integrated perception of all performance parameters. Given that these parameters generally interact with each other, an integrated analysis technique of more than one parameter at a time may provide new insight into an individual performer's expressive strategies, and into the phenomenon of musical expression generally.},
author = {Langner, J{\"{o}}rg},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Langner, Goebl - 2002 - Representing expressive performance in tempo-loudness space.pdf:pdf},
title = {{Representing expressive performance in tempo-loudness space}},
year = {2002}
}
@article{Hamiltona,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216v4},
author = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216v4},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hamilton, Ying, Leskovec - Unknown - Inductive Representation Learning on Large Graphs(2).pdf:pdf},
title = {{Inductive Representation Learning on Large Graphs}}
}
@article{ParekhMotionSeparation,
abstract = {In this paper we tackle the problem of single channel audio source separation driven by descriptors of the sounding object's motion. As opposed to previous approaches, motion is included as a soft-coupling constraint within the nonnegative matrix factorization framework. The proposed method is applied to a multimodal dataset of instruments in string quartet performance recordings where bow motion information is used for separation of string instruments. We show that the approach offers better source separation result than an audio-based baseline and the state-of-the-art multimodal-based approaches on these very challenging music mixtures.},
author = {Parekh, Sanjeel and Essid, Slim and Ozerov, Alexey and Duong, Ngoc Q K and P{\'{e}}rez, Patrick and Richard, Gael Ga{\"{e}}l and Duong, Ngoc Q K and Richard, Gael Ga{\"{e}}l},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Parekh et al. - Unknown - Motion informed audio source separation.pdf:pdf},
keywords = {Index Terms-audio source separation,motion,multimodal analysis,nonnegative matrix factorization},
title = {{Motion informed audio source separation}},
url = {https://hal.archives-ouvertes.fr/hal-01447977}
}
@article{Zhang2024,
author = {Zhang, Ying and Zhang, Yan and Xu, Wei and Wang, Zhifeng and Sun, Jianwen},
file = {:Users/huanzhang/Downloads/2024.EDM-short-papers.30.pdf:pdf},
keywords = {knowledge tracing,music performance,sight-singing dataset},
number = {July},
pages = {332--340},
title = {{SingPAD : A Knowledge Tracing Dataset Based on Music Performance Assessment}},
year = {2024}
}
@article{Simonetta2018SymbolicRepresentation,
abstract = {In this work, a novel representation system for symbolic music is described. The proposed representation system is graph-based and could theoretically represent music both from a horizontal (contrapuntal) and from a vertical (harmonic) point of view, by keeping into account contextual and harmonic information. It could also include relationships between internal variations of motifs and themes. This is achieved by gradually simplifying the melodies and generating layers of reductions that include only the most important notes from a structural and harmonic viewpoint. This representation system has been tested in a music information retrieval task, namely melodic similarity, and compared to another system that performs the same task but does not consider any contextual or harmonic information, showing how the structural information is needed in order to find certain relations between musical pieces. Moreover, a new dataset consisting of more than 5000 leadsheets is presented, with additional meta-musical information taken from different web databases, including author, year of first performance, lyrics, genre and stylistic tags.},
author = {Simonetta, Federico and Orio, Nicola and Carnovalini, Filippo and Rod{\`{a}}, Antonio},
doi = {10.1145/3243274.3243301},
file = {:Users/huanzhang/Downloads/3243274.3243301.pdf:pdf},
isbn = {9781450366090},
journal = {ACM International Conference Proceeding Series},
keywords = {Melodic similarity,Music reduction,Symbolic music},
title = {{Symbolic music similarity through a graph-based representation}},
year = {2018}
}
@article{Akbari2015,
abstract = {One important problem in musical information retrieval is automatic music transcription, which is an automated conversion process from played music to a symbolic notation such as MIDI file. Since the accuracy of previous audio-based transcription systems is not satisfactory, we propose an innovative computer vision-based automatic music transcription system named claVision to perform piano music transcription. Instead of processing the music audio, the system performs the transcription only from the video performance captured by a camera mounted over the piano keyboard. In this paper, we describe the architecture and the algorithms used in claVision. The claVision system has a high accuracy (F1 score over 0.95) and a very low latency (about 7.0 ms) in real-time music transcription, even under different illumination conditions. This technology can also be used for other musical keyboard instruments.},
annote = {From Duplicate 2 (Real-Time Piano Music Transcription Based on Computer Vision - Akbari, Mohammad; Cheng, Howard)

Keyboard Registration
Hough line transform
Background Update
Keys Detection


Positive Difference 
Negative DIfference},
author = {Akbari, Mohammad and Cheng, Howard},
doi = {10.1109/TMM.2015.2473702},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Akbari, Cheng - 2015 - Real-Time Piano Music Transcription Based on Computer Vision.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Automatic music transcription,claVision,computer vision,multipitch estimation,piano},
number = {12},
pages = {2113--2121},
publisher = {IEEE},
title = {{Real-Time Piano Music Transcription Based on Computer Vision}},
volume = {17},
year = {2015}
}
@techreport{GoeblChapterModels,
abstract = {This chapter gives an introduction into basic strands of current research in expressive music performance. A special focus is given on the various methods to acquire performance data either during a performance (e.g., through computer-monitored instruments) or from audio recordings. We then overview the different computational approaches to formalise and model the various aspects in expressive music performance. Future challenges and open problems are tackled briefly at the end of this chapter.},
author = {Goebl, Werner and Dixon, Simon and {De Poli}, Giovanni and Friberg, Anders and Bresin, Roberto and Widmer, Gerhard},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl et al. - Unknown - Chapter 1 'Sense' in Expressive Music Performance Data Acquisition, Computational Studies, and Models.pdf:pdf},
title = {{Chapter 1 'Sense' in Expressive Music Performance: Data Acquisition, Computational Studies, and Models}}
}
@inproceedings{Le2023VoiceboxScale,
abstract = {Large-scale generative models such as GPT and DALL-E have revolutionized the research community.These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught.In contrast, speech generative models are still primitive in terms of scale and task generalization.In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale.Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced.Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context.Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation.In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster.Audio samples can be found in https://voicebox.metademolab.com.},
archivePrefix = {arXiv},
arxivId = {2306.15687},
author = {Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and Hsu, Wei Ning},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {2306.15687},
file = {:Users/huanzhang/Downloads/2306.15687v2.pdf:pdf},
issn = {10495258},
title = {{Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale}},
year = {2023}
}
@inproceedings{Mcleod,
abstract = {Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a time-frequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation-for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H. 1},
author = {Mcleod, Andrew and Steedman, Mark},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mcleod, Steedman - Unknown - EVALUATING AUTOMATIC POLYPHONIC MUSIC TRANSCRIPTION(2).pdf:pdf},
title = {{Evaluating Automatic Polyphonic Music Transcription}},
url = {https://www.github.com/apmcleod/MV2H.},
year = {2018}
}
@article{Author,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Author, Author, Author - Unknown - A HYBRID MODEL FOR VIOLIN FINGERING GENERATION USING.pdf:pdf},
title = {{A HYBRID MODEL FOR VIOLIN FINGERING GENERATION USING}}
}
@article{Miguel2021,
abstract = {This work proposes modeling the beat percept as a 2d probability distribution and its inference from musical stimulus as a new MIR task. We present a methodology for collecting a 2d beat distribution of period and phase from free beat-tapping data from multiple participants. The methodology allows capturing beat-tapping variability both within (e.g.: mid-track beat change) and between annotators (e.g.: participants tap at different phases). The data analysis methodology was tested with simulated beat tracks assessing robustness to tapping variability, mid-tapping beat change and disagreement between annotators. It was also tested on experimental tapping data where the entropy of the estimated beat distributions correlated with tapping difficulty reported by the participants. For the MIR task, we propose using optimal transport as an evaluation criterion for models that estimate the beat distribution from musical stimuli. This criterion provides better scores to beat estimations closer in phase or period to distributions obtained from data. Finally, we present baseline models for the task of estimating the beat distribution. The methodology is presented with aims to enhance the exploration of ambiguity in the beat percept. For example, it exposes if beat uncertainty is related to a pulse that is hard to produce or conflicting interpretations of the beat.},
author = {Miguel, Martin A and Slezak, Diego Fern{\'{a}}ndez},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Miguel, Slezak - Unknown - MODELING BEAT UNCERTAINTY AS A 2D DISTRIBUTION OF PERIOD AND PHASE A MIR TASK PROPOSAL.pdf:pdf},
journal = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Modeling Beat Uncertainty as a 2D Distribution of Period and Phrase: A MIR Task Proposal}},
year = {2021}
}
@article{Hall2011CalibratingStudents,
abstract = {Playing a string instrument, such as the violin, requires solving many problems in controlling the bow. For example, the player may find that ‘saving bow' is crucial to getting through a passage, yet not know why. Computer models may be able to help musicians to understand such problems better and to find solutions. We have developed a simple bowing model that analyses a piece of music and calculates predicted bow speeds and positions. The model takes account of notes, slurs and ties, tempo, and dynamics. It has been implemented as a computer program that reads a score, analyses it and outputs an annotated score that indicates detailed information about the predicted bow positions and speeds. To support practical applications, as well as further research, the model needs to be validated and calibrated. In this article, we compare the predictions of the model with the performances of one of us, a violinist with a professional violin performance education. The results indicate that the model is accurate enough to predict places where practical difficulties, such as running out of bow, are likely to occur.},
author = {Hall, Cordelia V and O'Donnell, John T},
doi = {https://doi.org/10.1386/jmte.3.2-3.125_1},
issn = {1752-7074},
journal = {Journal of Music, Technology & Education},
keywords = {model,music education,violin bowing},
number = {2-3},
pages = {125--139},
publisher = {Intellect},
title = {{Calibrating a bowing checker for violin students}},
type = {Journal Article},
url = {https://intellectdiscover.com/content/journals/10.1386/jmte.3.2-3.125_1},
volume = {3},
year = {2011}
}
@article{WuMIDI-DDSP:MODELING,
abstract = {Musical expression requires control of both what notes are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls , but at the cost of realism. Black-box neural audio synthesis and concatena-tive samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an in-terpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience 1 .},
archivePrefix = {arXiv},
arxivId = {2112.09312v1},
author = {Wu, Yusong and Manilow, Ethan and Deng, Yi and Swavely, Rigel and Kastner, Kyle and Cooijmans, Tim and Courville, Aaron and {Anna Huang}, Cheng-Zhi and Engel, Jesse},
eprint = {2112.09312v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - MIDI-DDSP DETAILED CONTROL OF MUSICAL PERFORMANCE VIA HIERARCHICAL MODELING.pdf:pdf},
title = {{{MIDI-DDSP}: DETAILED CONTROL OF MUSICAL PERFORMANCE VIA HIERARCHICAL MODELING}},
url = {https://midi-ddsp.github.io/}
}
@article{Kong2021High-ResolutionTimes,
abstract = {Automatic music transcription (AMT) is the task of transcribing audio recordings into symbolic representations. Recently, neural network-based methods have been applied to AMT, and have achieved state-of-the-art results. However, many previous systems only detect the onset and offset of notes frame-wise, so the transcription resolution is limited to the frame hop size. There is a lack of research on using different strategies to encode onset and offset targets for training. In addition, previous AMT systems are sensitive to the misaligned onset and offset labels of audio recordings. Furthermore, there are limited researches on sustain pedal transcription on large-scale datasets. In this article, we propose a high-resolution AMT system trained by regressing precise onset and offset times of piano notes. At inference, we propose an algorithm to analytically calculate the precise onset and offset times of piano notes and pedal events. We show that our AMT system is robust to the misaligned onset and offset labels compared to previous systems. Our proposed system achieves an onset F1 of 96.72% on the MAESTRO dataset, outperforming previous onsets and frames system of 94.80%. Our system achieves a pedal onset F1 score of 91.86%, which is the first benchmark result on the MAESTRO dataset. We have released the source code and checkpoints of our work at https://github.com/bytedance/piano_transcription},
archivePrefix = {arXiv},
arxivId = {2010.01815},
author = {Kong, Qiuqiang and Li, Bochen and Song, Xuchen and Wan, Yuan and Wang, Yuxuan},
doi = {10.1109/TASLP.2021.3121991},
eprint = {2010.01815},
file = {:Users/huanzhang/Downloads/High-Resolution_Piano_Transcription_With_Pedals_by_Regressing_Onset_and_Offset_Times (1).pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Piano transcription,high-resolution,pedal transcription},
pages = {3707--3717},
publisher = {IEEE},
title = {{High-Resolution Piano Transcription with Pedals by Regressing Onset and Offset Times}},
volume = {29},
year = {2021}
}
@inproceedings{Dong2022DeepSynthesis,
abstract = {Music performance synthesis aims to synthesize a musical score into a natural performance. In this paper, we borrow recent advances in text-to-speech synthesis and present the Deep Performer-a novel system for score-to-audio music performance synthesis. Unlike speech, music often contains polyphony and long notes. Hence, we propose two new techniques for handling polyphonic inputs and providing a fine-grained conditioning in a transformer encoder-decoder model. To train our proposed system, we present a new violin dataset consisting of paired recordings and scores along with estimated alignments between them. We show that our proposed model can synthesize music with clear polyphony and harmonic structures. In a listening test, we achieve competitive quality against the baseline model, a conditional generative audio model, in terms of pitch accuracy, timbre and noise level. Moreover, our proposed model significantly outperforms the baseline on an existing piano dataset in overall quality.},
author = {Dong, Hao-Wen and Zhou, Cong and Berg-Kirkpatrick, Taylor and Mcauley, Julian},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - Unknown - Deep Performer Score-to-audio Music Performance Synthesis.pdf:pdf},
keywords = {Audio synthesis,computer music,machine learning,music information retrieval,neural network},
title = {{Deep Performer: Score-to-audio Music Performance Synthesis}},
year = {2022}
}
@inproceedings{Perez2018FiLMlayer,
abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
address = {New Orleans, USA},
archivePrefix = {arXiv},
arxivId = {1709.07871},
author = {Perez, Ethan and Strub, Florian and {De Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
doi = {10.1609/aaai.v32i1.11671},
eprint = {1709.07871},
file = {:Users/huanzhang/Downloads/1709.07871.pdf:pdf},
isbn = {9781577358008},
issn = {2159-5399},
title = {{{FiLM}: Visual reasoning with a general conditioning layer}},
year = {2018}
}
@inproceedings{Matsubara2021CROCUSCritiques,
author = {Matsubara, Masaki and Kagawa, Rina and Hirano, Takeshi and Tsuji, Isao},
booktitle = {In Proceedings of the International Symposium on Computer Music Multidisciplinary Research (CMMR)},
file = {:Users/huanzhang/Downloads/CMMR2021_7-4.pdf:pdf},
keywords = {database,music education,verbal information},
title = {{{CROCUS}: Dataset of Musical Performance Critiques}},
year = {2021}
}
@article{Bresin2002DirectorSystem,
abstract = {Director Musices is a program that transforms notated scores into musical performances. It implements the performance rules emerging from research projects at the Royal Institute of Technology (KTH). Rules in the program model performance aspects such as phrasing, articulation, and intonation, and they operate on performance variables such as tone, inter-onset duration, amplitude, and pitch. By manipulating rule parameters, the user can act as a metaperformer controlling different feature of the performance, leaving the technical execution to the computer. Different interpretations of the same piece can easily be obtained. Features of Director Musices include MIDI file input and output, rule palettes, graphical display of all performance variables (along with the notation), and user- defined performance rules. The program is implemented in Common Lisp and is available free as a stand-alone application both for Macintosh and Windows platforms. Further information, including music examples, publications, and the program itself, is located online at http://www.speech.kth.se/music/performance. This paper is a revised and updated version of a previous paper published in the Computer Music Journal in year 2000 that was mainly written by Anders Friberg (Friberg, Colombo, Fryd{\'{e}}n and Sundberg, 2000).},
author = {Bresin, Roberto and Friberg, Anders and Sundberg, Johan},
file = {:Users/huanzhang/Downloads/Director_musices_The_KTH_performance_rules_system.pdf:pdf},
journal = {Special Interest Group on Music and Computer(SIGMUS) - 46 Kyoto},
pages = {43--48},
title = {{Director Musices : The {KTH} Performance Rules System}},
url = {http://www.speech.kth.se/prod/publications/files/873.pdf},
year = {2002}
}
@techreport{DevlinRobustFill:I/O,
abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.},
archivePrefix = {arXiv},
arxivId = {1703.07469v1},
author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-Rahman and Kohli, Pushmeet},
eprint = {1703.07469v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - Unknown - RobustFill Neural Program Learning under Noisy IO.pdf:pdf},
title = {{RobustFill: Neural Program Learning under Noisy I/O}}
}
@article{Hannon2005,
abstract = {Intrinsic perceptual biases for simple duration ratios are thought to constrain the organization of rhythmic patterns in music. We tested that hypothesis by exposing listeners to folk melodies differing in metrical structure (simple or complex duration ratios), then testing them on alterations that preserved or violated the original metrical structure. Simple meters predominate in North American music, but complex meters are common in many other musical cultures. In Experiment 1, North American adults rated structure-violating alterations as less similar to the original version than structure-preserving alterations for simple-meter patterns but not for complex-meter patterns. In Experiment 2, adults of Bulgarian or Mace-donian origin provided differential ratings to structure-violating and structure-preserving alterations in complex as well as simple-meter contexts. In Experiment 3, 6-month-old infants responded differentially to structure-violating and structure-preserving alterations in both metrical contexts. These findings imply that the metrical biases of North American adults reflect enculturation processes rather than processing predispositions for simple meters.},
annote = {From Duplicate 2 (Metrical Categories in Infancy and Adulthood - Hannon, Erin E; Trehub, Sandra E)

Week 4},
author = {Hannon, Erin E and Trehub, Sandra E},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hannon, Trehub - 2005 - Metrical Categories in Infancy and Adulthood.pdf:pdf},
title = {{Metrical Categories in Infancy and Adulthood}},
year = {2005}
}
@article{Wang2016,
abstract = {Chinese traditional opera plays an important role in Chinese traditional culture, it reflects the customs and value tendency of different areas. Though researchers have already gained some achievements, studies on this field are scarce and the existing achievements still need to be improved. This paper proposes a system based on multi-feature fusion and extreme learning machine (ELM) to classify Chinese traditional opera genre. Inspired by music genre classification, each aria is split into multiple segments. 19 features are then extracted and fused to generate the fusion feature. Finally, we use ELM and majority voting methods to determine the genre of the whole aria. The research data are 800 arias of 8 typical genres collected from Internet. This system achieves a mean classification accuracy of 92% among 8 famous Chinese traditional opera genres. The experimental results demonstrated that multi-feature fusion improves classification accuracy of Chinese traditional opera genres. Feature fusion is more effective than decision fusion in dealing with this problem.},
author = {Wang, Jianrong and Wang, Chenliang and Wei, Jianguo and Dang, Jianwu},
doi = {10.1109/APSIPA.2015.7415449},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - Chinese opera genre classification based on multi-feature fusion and extreme learning machine.pdf:pdf},
isbn = {9789881476807},
journal = {2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2015},
number = {December},
pages = {93--100},
title = {{Chinese opera genre classification based on multi-feature fusion and extreme learning machine}},
year = {2016}
}
@article{Lu2023,
abstract = {Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly extracted from music sequences, making the model training self-supervised. In the text-to-attribute understanding stage, the text is synthesized and refined by ChatGPT based on the defined attribute templates. Secondly, the system can achieve precise control with specific attributes in text descriptions and offers multiple control options through attribute-conditioned or text-conditioned approaches. MuseCoco outperforms baseline systems in terms of musicality, controllability, and overall score by at least 1.27, 1.08, and 1.32 respectively. Besides, there is a notable enhancement of about 20% in objective control accuracy. In addition, we have developed a robust large-scale model with 1.2 billion parameters, showcasing exceptional controllability and musicality.},
archivePrefix = {arXiv},
arxivId = {2306.00110},
author = {Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
eprint = {2306.00110},
file = {:Users/huanzhang/Downloads/2306.00110.pdf:pdf},
pages = {1--18},
title = {{MuseCoco: Generating Symbolic Music from Text}},
url = {http://arxiv.org/abs/2306.00110},
year = {2023}
}
@article{Foscarin2022MatchPerformances,
abstract = {This paper presents the specifications of match: a file format that extends a MIDI human performance with note-, beat-, and downbeat-level alignments to a corresponding musical score. This enables advanced analyses of the performance that are relevant for various tasks, such as expressive performance modeling, score following, music transcription, and performer classification. The match file includes a set of score-related descriptors that makes it usable also as a bare-bones score representation. For applications that require the use of structural score elements (e.g., voices, parts, beams, slurs), the match file can be easily combined with the symbolic score. To support the practical application of our work, we release a corrected and upgraded version of the Vienna4x22 dataset of scores and performances aligned with match files.},
annote = {P: Performance S: Score},
archivePrefix = {arXiv},
arxivId = {2206.01104},
author = {Foscarin, Francesco and Karystinaios, Emmanouil and Peter, Silvan David and Cancino-Chac{\'{o}}n, Carlos and Grachten, Maarten and Widmer, Gerhard},
eprint = {2206.01104},
file = {:Users/huanzhang/Downloads/2206.01104.pdf:pdf},
pages = {1--9},
title = {{The match file format: Encoding Alignments between Scores and Performances}},
url = {http://arxiv.org/abs/2206.01104},
year = {2022}
}
@article{Gong2017IdentificationTraining,
abstract = {Music Information Retrieval (MIR) technologies have been proven useful in assisting western classical singing training. Jingju (also known as Beijing or Peking opera) singing is different from western singing in terms of most of the perceptual dimensions, and the trainees are taught by using mouth/heart method. In this paper, we first present the training method used in the professional jingju training classroom scenario and show the potential benefits of introducing the MIR technologies into the training process. The main part of this paper dedicates to identify the potential MIR technologies for jingju singing training. To this intent, we answer the question: how the jingju singing tutors and trainees value the importance of each jingju musical dimension-intonation, rhythm, loudness, tone quality and pronunciation? This is done by (i) classifying the classroom singing practices, tutor's verbal feedbacks into these 5 dimensions, (ii) surveying the trainees. Then, with the help of the music signal analysis, a finer inspection on the classroom practice recording examples reveals the detailed elements in the training process. Finally, based on the above analysis, several potential MIR technologies are identified and would be useful for the jingju singing training.},
author = {Gong, Rong and Serra, Xavier},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gong, Serra - 2017 - Identification of potential music information retrieval technologies for computer-aided jingju singing training.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Computer-aided singing training,Jingju,Music information retrieval},
pages = {1--16},
title = {{Identification of potential music information retrieval technologies for computer-aided jingju singing training}},
year = {2017}
}
@article{Schubert2014MusicSame,
abstract = {This paper investigates three individual differences with respect to ratings of the same piece of classical piano music that has undergone different expressive performance treatments. The individual difference variables investigated were music systemising (those interested in the structural and organizational aspects of music), music empathizing (those interested in the emotional/human aspects of music) and musical experience (years of playing). Five pieces, based on stimuli used in Rencon-GATM were rated according to expressiveness and execution , each being related to musical expression, but the former suggesting an empathizing processing style and the latter a systemizing processing style. Ratings made by 45 participants did not show any clear differences that could be attributed to a cognitive style. One explanation for this finding was that cognitive music styles are more likely to influence justifications of ratings, rather than ratings magnitude. High music systemisers reported having higher concentration than other participants.},
author = {Schubert, Emery and {De Poli}, Giovanni and Roda, Antonio and Canazza, Sergio},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Schubert et al. - 2014 - Music Systemisers and Music Empathisers-Do they rate expressiveness of computer generated performances the same.pdf:pdf},
title = {{Music Systemisers and Music Empathisers-Do they rate expressiveness of computer generated performances the same?}},
url = {https://www.worldapp.com/surveys/overview.html},
year = {2014}
}
@article{Ge2023,
abstract = {Visual Programming (VP) has emerged as a powerful framework for Visual Question Answering (VQA). By generating and executing bespoke code for each question, these methods demonstrate impressive compositional and reasoning capabilities, especially in few-shot and zero-shot scenarios. However, existing VP methods generate all code in a single function, resulting in code that is suboptimal in terms of both accuracy and interpretability. Inspired by human coding practices, we propose Recursive Visual Programming (RVP), which simplifies generated routines, provides more efficient problem solving, and can manage more complex data structures. RVP is inspired by human coding practices and approaches VQA tasks with an iterative recursive code generation approach, allowing decomposition of complicated problems into smaller parts. Notably, RVP is capable of dynamic type assignment, i.e., as the system recursively generates a new piece of code, it autonomously determines the appropriate return type and crafts the requisite code to generate that output. We show RVP's efficacy through extensive experiments on benchmarks including VSR, COVR, GQA, and NextQA, underscoring the value of adopting human-like recursive and modular programming techniques for solving VQA tasks through coding.},
archivePrefix = {arXiv},
arxivId = {2312.02249},
author = {Ge, Jiaxin and Subramanian, Sanjay and Shi, Baifeng and Herzig, Roei and Darrell, Trevor},
eprint = {2312.02249},
file = {:Users/huanzhang/Downloads/2312.02249.pdf:pdf},
title = {{Recursive Visual Programming}},
url = {http://arxiv.org/abs/2312.02249},
year = {2023}
}
@article{Ramoneda2023a,
abstract = {Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the Music Information Retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing an encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets -- more than 7500 scores with up to 9 difficulty levels -- , two of them particularly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.34\% and a mean square error of 1.33. Finally, we provide access to our code, data, and models for transparency and reproducibility.},
archivePrefix = {arXiv},
arxivId = {2309.16287},
author = {Ramoneda, Pedro and Valero-Mas, Jose J. and Jeong, Dasaem and Serra, Xavier},
eprint = {2309.16287},
file = {:Users/huanzhang/Downloads/2309.16287.pdf:pdf},
pages = {1--8},
title = {{Predicting performance difficulty from piano sheet music images}},
url = {http://arxiv.org/abs/2309.16287},
year = {2023}
}
@article{Knees2009AUGMENTINGSIMILARITY,
abstract = {We investigate an approach to a music search engine that indexes music pieces based on related Web documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process-either by directly modifying the retrieval process or by performing post-hoc audio-based re-ranking of the search results. The aim of this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections , one large real-world collection containing about 35,000 tracks and on the CAL500 set.},
author = {Knees, P and Pohle, T and Schedl, M and Schnitzer, D and Seyerlehner, K and Widmer, G},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Knees et al. - 2009 - AUGMENTING TEXT-BASED MUSIC RETRIEVAL WITH AUDIO SIMILARITY.pdf:pdf},
title = {{AUGMENTING TEXT-BASED MUSIC RETRIEVAL WITH AUDIO SIMILARITY}},
url = {http://lucene.apache.org/nutch},
year = {2009}
}
@article{Garoufis2020,
author = {Garoufis, Christos and Zlatintsi, Athanasia and Maragos, Petros},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Garoufis, Zlatintsi, Maragos - 2020 - AN LSTM-BASED DYNAMIC CHORD PROGRESSION GENERATION SYSTEM FOR INTERACTIVE MUSIC PERFORMANCE Sch(2).pdf:pdf},
isbn = {9781509066315},
journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {4497--4501},
publisher = {IEEE},
title = {{AN LSTM-BASED DYNAMIC CHORD PROGRESSION GENERATION SYSTEM FOR INTERACTIVE MUSIC PERFORMANCE School of ECE , National Technical University of Athens , Zografou 15773 , Greece Robot Perception and Interaction Unit , Athena Research Center , 15125 Maroussi ,}},
year = {2020}
}
@article{Hsiao2021LearningData,
abstract = {Voice segregation, melody line identification and other tasks of identifying the horizontal elements of music have been developed independently, although their purposes are similar. In this paper, we propose a unified framework to solve the voice segregation and melody line identification tasks of symbolic music data. To achieve this, a neural network model is trained to learn note-to-note affinity values directly from their contextual notes, in order to represent a music piece as a weighted undirected graph, with the affinity values being the edge weights. Individual voices or streams are then obtained with spectral clustering over the learned graph. Conditioned on minimal prior knowledge, the framework can achieve state-of-the-art performance on both tasks, and further demonstrates strong advantages on simulated real-world symbolic music data with missing notes and asynchronous chord notes.},
annote = {For each note the feature is a window - (2M-1) * 3 matrix

each pair of notes are looked at, and predict the affinity

but it is bascially predicting the edge (affinity), instead of using edge as information to input},
author = {Hsiao, Yo-Wei and Su, Li},
file = {:Users/huanzhang/Downloads/000035.pdf:pdf},
journal = {International Society for Music Information Retrieval (ISMIR)},
pages = {285--292},
title = {{Learning note-to-note affinity for voice segregation and melody line identification of symbolic music data}},
year = {2021}
}
@article{Won,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.04972v1},
author = {Won, Minz and Chun, Sanghyuk and Serra, Xavier and Group, Music Technology and Fabra, Universitat Pompeu},
eprint = {arXiv:1906.04972v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Won et al. - Unknown - Toward Interpretable Music Tagging with Self-Attention 1.pdf:pdf},
number = {Section 2},
title = {{Toward Interpretable Music Tagging with Self-Attention 1}}
}
@inproceedings{Chuan2018ModelingRepresentation,
abstract = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks (CNN, for learning an efficient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells (LSTM, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three configurations of the network: LSTM without CNN, LSTM with CNN (pre-trained vs. not pre-trained). Visualizations of the feature maps and filters in the CNN are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a finding that is directly useful for tackling current challenges in music and AI such as smart music generation.},
author = {Chuan, Ching Hua and Herremans, Dorien},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
doi = {10.1609/aaai.v32i1.11880},
file = {:Users/huanzhang/Downloads/11880-Article Text-15408-1-2-20201228.pdf:pdf},
isbn = {9781577358008},
issn = {2159-5399},
keywords = {Machine Learning Applications Track},
pages = {2159--2166},
title = {{Modeling temporal tonal relations in polyphonic music through deep networks with a novel image-based representation}},
year = {2018}
}
@article{Lv2023,
abstract = {Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrumental tracks from scratch, or based on user-provided source tracks. Considering the diverse and flexible combination between source and target tracks, a unified model capable of generating any arbitrary tracks is of crucial necessity. Previous works fail to address this need due to inherent constraints in music representations and model architectures. To address this need, we propose a unified representation and diffusion framework named GETMusic (`GET' stands for GEnerate music Tracks), which includes a novel music representation named GETScore, and a diffusion model named GETDiff. GETScore represents notes as tokens and organizes them in a 2D structure, with tracks stacked vertically and progressing horizontally over time. During training, tracks are randomly selected as either the target or source. In the forward process, target tracks are corrupted by masking their tokens, while source tracks remain as ground truth. In the denoising process, GETDiff learns to predict the masked target tokens, conditioning on the source tracks. With separate tracks in GETScore and the non-autoregressive behavior of the model, GETMusic can explicitly control the generation of any target tracks from scratch or conditioning on source tracks. We conduct experiments on music generation involving six instrumental tracks, resulting in a total of 665 combinations. GETMusic provides high-quality results across diverse combinations and surpasses prior works proposed for some specific combinations.},
archivePrefix = {arXiv},
arxivId = {2305.10841},
author = {Lv, Ang and Tan, Xu and Lu, Peiling and Ye, Wei and Zhang, Shikun and Bian, Jiang and Yan, Rui},
eprint = {2305.10841},
file = {:Users/huanzhang/Downloads/2305.10841.pdf:pdf},
title = {{GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework}},
url = {http://arxiv.org/abs/2305.10841},
year = {2023}
}
@article{McAdams2008,
abstract = {This article discusses musical-timbre perception. Musical timbre is a combination of continuous perceptual dimensions and discrete features to which listeners are differentially sensitive. The continuous dimensions often have quantifiable acoustic correlates. The timbre-space representation is a powerful psychological model that allows predictions to be made about timbre perception in situations beyond those used to derive the model in the first place. Timbre can play a role in larger-scale movements of tension and relaxation and thus contribute to the expression inherent in musical form. Under conditions of high blend among instruments composing a vertical sonority, timbral roughness is a major component of musical tension. However, it strongly depends on the way auditory grouping processes have parsed the incoming acoustic information into events and streams.},
annote = {From Duplicate 1 (The perception of musical timbre - McAdams, Stephen; Giordano, Bruno L.)

Week 3 compulsory},
author = {McAdams, Stephen and Giordano, Bruno L.},
doi = {10.1093/oxfordhb/9780199298457.013.0007},
file = {:Users/huanzhang/Downloads/SMC_BLG_2009_Timbre.pdf:pdf},
isbn = {9780191743931},
journal = {The Oxford Handbook of Music Psychology},
keywords = {Musical expression,Musical tension,Relaxation,Tension,Timbre perception,Timbre-space representation},
number = {January},
title = {{The perception of musical timbre}},
year = {2008}
}
@article{Repp1996PedalInvestigation,
abstract = {The timing of pedal depressions and releases was measured relative to key depressions and releases in two pianists' performances of Robert Schumann's "Traumerei" on an electronic instrument. Each pianist provided 9 complete performances, 3 at each of 3 tempi, which were analyzed by examining in detail those positions in the music in which the pedal was used consistently. The principal questions were whether and how pedal timing adjusts to changes in global tempo (across performances) and in local tempo (within performances): Do pedal release times, onset times, or change (onset minus release) times exhibit absolute or relative invariance across either or both of these tempo changes? The results do not suggest any simple answer, since neither type of invariance was observed consistently. Pedal timing emerges as having a complex pattern that is sensitive to local and global tempo changes in varying degrees, yet exhibits consistency across repeated performances by the same pianist. There were striking differences in pedal timing between the two pianists, who differed in level of skill.},
author = {Repp, Bruno H},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - Unknown - Haskins Iilboratories Status Report on Speech Research Pedal Timing and Tempo in Expressive Piano Performance A Prelimi.pdf:pdf},
journal = {Psychology of Music},
number = {2},
pages = {199--221},
title = {{Pedal Timing and Tempo in Expressive Piano Performance: A Preliminary Investigation}},
volume = {24},
year = {1996}
}
@article{Shu,
abstract = {A common test for whether a generative model learns disentangled representations is its ability to learn style and content as independent factors of variation on digit datasets. To achieve such disentanglement with variational autoencoders, the label information is often provided in either a fully-supervised or semi-supervised fashion. We show, however, that the variational objective is insufficient in explaining the observed style and content disentanglement. Furthermore, we present an empirical framework to systematically evaluate the disentanglement behavior of our models. We show that the encoder and decoder independently favor disentangled representations and that this tendency depends on the implicit regularization by stochastic gradient descent.},
author = {Shu, Rui and Zhao, Shengjia and Kochenderfer, Mykel J.},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Shu, Zhao, Kochenderfer - Unknown - Workshop track-ICLR 2018 RETHINKING STYLE AND CONTENT DISENTANGLE-MENT IN VARIATIONAL AUTOENCODERS.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
title = {{Rethinking style and content disentanglement in variational autoencoders}},
year = {2018}
}
@article{Kosta2018DynamicsScore,
abstract = {Knowing how notated dynamic markings map to/from performed loudness have practical implications for music transcription and expression synthesis. Analysing 2000 recordings of 44 Chopin Mazurkas, we show empirically that the meaning of a score dynamic marking changes based on the notated and performed dynamic levels of the surrounding musical context, and the position of the marking relative to the previous one. Transitions between two markings are more consistent when moving from louder to softer dynamics, and between markings of high intensity or of high contrast. Pianists who diverge from predominant dynamic transition behaviours tend to do so in similar ways.},
annote = {conclusion: meaning of dynamic marking change with surrounding musical context

1. overall loudness level of a marking throughout a piece
2. loudness change - consecutive 

tau rank correlation coefficient 

Op.67 No.2 doesn't adhere to OLS in most cases

Transition is more consistent if pianist move from louder to softer marking

ANOVA for the same dynamic markings throughout the piece, as well as 2-way ANOVA across pianist

Three-beat window:},
author = {Kosta, Katerina and Bandtlow, Oscar F. and Chew, Elaine},
doi = {10.1080/09298215.2018.1486430},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kosta, Bandtlow, Chew - 2018 - Journal of New Music Research Dynamics and relativity Practical implications of dynamic markings in the s.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {Music analysis,expression,information retrieval,notation,performance,recording},
number = {5},
pages = {438--461},
title = {{Dynamics and relativity: Practical implications of dynamic markings in the score}},
url = {https://www.tandfonline.com/doi/abs/10.1080/09298215.2018.1486430?journalCode=nnmr20},
volume = {47},
year = {2018}
}
@article{Nakamura2018StatisticalDifficulty,
abstract = {We present a statistical-modeling method for piano reduction, i.e. converting an ensemble score into piano scores, that can control performance difficulty. While previous studies have focused on describing the condition for playable piano scores, it depends on player's skill and can change continuously with the tempo. We thus computationally quantify performance difficulty as well as musical fidelity to the original score, and formulate the problem as optimization of musical fidelity under constraints on difficulty values. First, performance difficulty measures are developed by means of probabilistic generative models for piano scores and the relation to the rate of performance errors is studied. Second, to describe musical fidelity, we construct a probabilistic model integrating a prior piano-score model and a model representing how ensemble scores are likely to be edited. An iterative optimization algorithm for piano reduction is developed based on statistical inference of the model. We confirm the effect of the iterative procedure; we find that subjective difficulty and musical fidelity monotonically increase with controlled difficulty values; and we show that incorporating sequential dependence of pitches and fingering motion in the piano-score model improves the quality of reduction scores in high-difficulty cases.},
archivePrefix = {arXiv},
arxivId = {1808.05006},
author = {Nakamura, Eita and Yoshii, Kazuyoshi},
doi = {10.1017/ATSIP.2018.18},
eprint = {1808.05006},
file = {:Users/huanzhang/Downloads/div-class-title-statistical-piano-reduction-controlling-performance-difficulty-div.pdf:pdf},
issn = {20487703},
journal = {APSIPA Transactions on Signal and Information Processing},
keywords = {Automatic Music Arrangement,Statistical Modelling,Symbolic Music Processing},
title = {{Statistical piano reduction controlling performance difficulty}},
volume = {7},
year = {2018}
}
@article{Tsai2012,
abstract = {This study aims to develop an automatic singing evaluation system for Karaoke performances. Many Karaoke systems in the market today come with a scoring function. The addition of the feature enhances the entertainment appeal of the system due to the competitive nature of humans. The automatic Karaoke scoring mechanism to date, however, is still rudimentary, often giving inconsistent results with scoring by human raters.A cause of blunder arises from the fact that often only the singing volume is used as the evaluation criteria. To improve on the singing evaluation capabilities on Karaoke machines, this study exploits various acoustic features, including pitch, volume, and rhythm to assess a singing performance. We invited a number of singers having different levels of singing capabilities to record for Karaoke solo vocal samples. The performances were rated independently by four musicians, and then used in conjunction with additional Karaoke Video Compact Disk music for the training of our proposed system. Our experiment shows that the results of automatic singing evaluation are close to the human rating, where the Pearson product-moment correlation coefficient between them is 0.82. {\textcopyright} 2011 IEEE.},
author = {Tsai, Wei Ho and Lee, Hsin Chieh},
doi = {10.1109/TASL.2011.2174224},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tsai, Lee - 2012 - Automatic evaluation of Karaoke singing based on pitch, volume, and rhythm features.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Accompaniment,Karaoke,Singing evaluation,Solo vocal},
number = {4},
pages = {1233--1243},
publisher = {IEEE},
title = {{Automatic evaluation of Karaoke singing based on pitch, volume, and rhythm features}},
volume = {20},
year = {2012}
}
@techreport{Schellenberg1999NameExcerpts,
abstract = {Wetested listeners' ability to identify brief excerpts from popular recordings. Listeners were required to match 200-or 100-msec excerpts with the song titles and artists. Performance was well above chance levels for 200-msec excerpts and poorer but still better than chance for 100-msec excerpts. Performance fell to chance levels when dynamic (time-varying) information was disrupted by playing the 100-msec excerpts backward and when high-frequency information was omitted from the 100-msec excerpts; performance was unaffected by the removal of low-frequency information. In sum, successful identification required the presence of dynamic, high-frequency spectral information.},
annote = {From Duplicate 2 (Name that tune: Identifying popular recordings from brief excerpts - Schellenberg, E Glenn; Iverson, Paul; Mckinnon, Margaret C)

Week 3},
author = {Schellenberg, E Glenn and Iverson, Paul and Mckinnon, Margaret C},
booktitle = {Psychonomic Bulletin {\&} Review},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Schellenberg, Iverson, Mckinnon - 1999 - Name that tune Identifying popular recordings from brief excerpts.pdf:pdf},
number = {4},
pages = {641--646},
title = {{Name that tune: Identifying popular recordings from brief excerpts}},
volume = {6},
year = {1999}
}
@inproceedings{Go2023CombinatorialAnalysis,
abstract = {In this work, we propose a symbolic music generation model with the song structure graph analysis network. We construct a graph that uses information such as note sequence and instrument as node features, while the correlation between note sequences acts as the edge feature. We trained a Graph Neural Network to obtain node representation in the graph, then we use node representation as input of Unet to generate CONLON pianoroll image latent. The outcomes of our experimental results show that the proposed model can generate a comprehensive form of music. Our approach represents a promising and innovative method for symbolic music generation and holds potential applications in various fields in Music Information Retreival, including music composition, music classification, and music inpainting systems.},
archivePrefix = {arXiv},
arxivId = {2312.15400},
author = {Go, Seonghyeon and Lee, Kyogu},
booktitle = {Arxiv preprint arXiv:2105.06337},
eprint = {2312.15400},
file = {:Users/huanzhang/Downloads/2312.15400v1.pdf:pdf},
title = {{Combinatorial music generation model with song structure graph analysis}},
url = {http://arxiv.org/abs/2312.15400},
year = {2023}
}
@article{Vahidi2023,
author = {Vahidi, Cyrus and Singh, Shubhr and Benetos, Emmanouil and Phan, Huy and Stowell, Dan and Vahidi, Cyrus and Singh, Shubhr and Benetos, Emmanouil and Phan, Huy and Stowell, Dan},
file = {:Users/huanzhang/Downloads/WASPAA_2023__Perceptual_Musical_Similarity_Metric_Learning_with_Graph_Neural_Networks.pdf:pdf},
isbn = {9798350323726},
title = {{Perceptual musical similarity metric learning with graph neural networks To cite this version : HAL Id : hal-04178191}},
year = {2023}
}
@inproceedings{Ho2020Denoisingmodels,
abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
address = {Online},
annote = {beta: variance schedule 

N(x_t; mean, variance)},
archivePrefix = {arXiv},
arxivId = {2006.11239},
author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {2006.11239},
file = {:Users/huanzhang/Downloads/2006.11239.pdf:pdf},
issn = {10495258},
title = {{Denoising diffusion probabilistic models}},
year = {2020}
}
@article{Authora,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Downloads/Submission 178/178\\Submission 178.pdf:pdf},
title = {{[ ANONYMIZED SOFTWARE NAME ]: ADVANCING PERFORMANCE ANALYSIS BY INTEGRATING AUDIO-TO-SCORE ALIGNMENT}}
}
@article{Nistal2024DiffModels,
abstract = {Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce "Diff-A-Riff," a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website: sonycslparis.github.io/diffariff-companion/},
archivePrefix = {arXiv},
arxivId = {2406.08384},
author = {Nistal, Javier and Pasini, Marco and Aouameur, Cyran and Grachten, Maarten and Lattner, Stefan},
eprint = {2406.08384},
file = {:Users/huanzhang/Downloads/2406.08384v2.pdf:pdf},
journal = {Proceeding of the 25th International Society on Music Information Retrieval (ISMIR)},
title = {{Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models}},
url = {http://arxiv.org/abs/2406.08384},
year = {2024}
}
@techreport{XiaSPECTRALPERFORMANCE,
abstract = {We apply machine learning to a database of recorded ensemble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers' musical expression as co-evolving time series and learn their interactive relationship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspondence not only between different performers but also between the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to generate a more human-like interaction.},
author = {Xia, Guangyu and Wang, Yun and Dannenberg, Roger and Gordon, Geoffrey},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Xia et al. - Unknown - SPECTRAL LEARNING FOR EXPRESSIVE INTERACTIVE ENSEMBLE MUSIC PERFORMANCE.pdf:pdf},
title = {{SPECTRAL LEARNING FOR EXPRESSIVE INTERACTIVE ENSEMBLE MUSIC PERFORMANCE}}
}
@article{Goebl2000SkilledDifferentiation,
author = {Goebl, Werner},
file = {:Users/huanzhang/Downloads/oefai-tr-2000-11.pdf:pdf},
journal = {Proc. of the 6th Int. Conf. on Music Perception and Cognition},
number = {1932},
pages = {1--11},
title = {{Skilled piano performance: Melody lead caused by dynamic differentiation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.176%5C&rep=rep1%5C&type=pdf},
year = {2000}
}
@inproceedings{Li2019DeepGCNsCNNs,
abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.},
archivePrefix = {arXiv},
arxivId = {1904.03751},
author = {Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2019.00936},
eprint = {1904.03751},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - DeepGCNs Can GCNs Go as Deep as CNNs(2).pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
title = {{{DeepGCNs}: Can {GCNs} go as deep as {CNNs}?}},
url = {https://sites.google.com/view/deep-gcns},
year = {2019}
}
@techreport{Harte,
abstract = {We propose a novel method for detecting changes in the harmonic content of musical audio signals. Our method uses a new model for Equal Tempered Pitch Class Space. This model maps 12-bin chroma vectors to the interior space of a 6-D polytope; pitch classes are mapped onto the vertices of this polytope. Close harmonic relations such as fifths and thirds appear as small Euclidian distances. We calculate the Euclidian distance between analysis frames n + 1 and n − 1 to develop a harmonic change measure for frame n. A peak in the detection function denotes a transition from one harmonically stable region to another. Initial experiments show that the algorithm can successfully detect harmonic changes such as chord boundaries in polyphonic audio recordings.},
author = {Harte, Christopher and Sandler, Mark and Gasser, Martin},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Harte, Sandler, Gasser - Unknown - Detecting Harmonic Change In Musical Audio.pdf:pdf},
keywords = {Audio,Harmonic,J0 [Computer Applications]: General General Terms,Music,Segmentation,Theory Keywords Pitch Space},
title = {{Detecting Harmonic Change In Musical Audio}}
}
@article{Lian2022RobustConversion,
abstract = {Traditional studies on voice conversion (VC) have made progress with parallel training data and known speakers. Good voice conversion quality is obtained by exploring better alignment modules or expressive mapping functions. In this study, we investigate zero-shot VC from a novel perspective of self-supervised disentangled speech representation learning. Specifically, we achieve the disentanglement by balancing the information flow between global speaker representation and time-varying content representation in a sequential variational autoencoder (VAE). A zero-shot voice conversion is performed by feeding an arbitrary speaker embedding and content embeddings to the VAE decoder. Besides that, an on-the-fly data augmentation training strategy is applied to make the learned representation noise invariant. On TIMIT and VCTK datasets, we achieve state-of-the-art performance on both objective evaluation, i.e., speaker verification (SV) on speaker embedding and content embedding, and subjective evaluation, i.e., voice naturalness and similarity, and remains to be robust even with noisy source/target utterances.},
archivePrefix = {arXiv},
arxivId = {2203.16705},
author = {Lian, Jiachen and Zhang, Chunlei and Yu, Dong},
doi = {10.1109/icassp43922.2022.9747272},
eprint = {2203.16705},
file = {:Users/huanzhang/Downloads/Robust_Disentangled_Variational_Speech_Representation_Learning_for_Zero-Shot_Voice_Conversion.pdf:pdf},
isbn = {9781665405409},
journal = {Proceedings of International Conference on Acoustic, Speech and Signal Processsing (ICASSP)},
publisher = {IEEE},
title = {{Robust Disentangled Variational Speech Representation Learning for Zero-Shot Voice Conversion}},
year = {2022}
}
@inproceedings{Liu2024MusicCaptioning,
abstract = {Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.},
archivePrefix = {arXiv},
arxivId = {2308.11276},
author = {Liu, Shansong and Hussain, Atin Sakkeer and Sun, Chenshuo and Shan, Ying},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
eprint = {2308.11276},
file = {:Users/huanzhang/Downloads/2308.11276.pdf:pdf},
title = {{Music Understanding {LLaMA}: Advancing Text-to-Music Generation with Question Answering and Captioning}},
url = {http://arxiv.org/abs/2308.11276},
year = {2024}
}
@inproceedings{Liu2023AudioLDMModels,
abstract = {Text-to-audio (TTA) systems have recently gained attention for their ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining (CLAP) embeddings. The pretrained CLAP models enable us to train LDMs with audio embeddings while providing text embeddings as the condition during sampling. By learning the latent representations of audio signals without modelling the cross-modal relationship, AudioLDM improves both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance compared to other open-sourced systems, measured by both objective and subjective metrics. AudioLDM is also the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
address = {Hawaii, USA},
archivePrefix = {arXiv},
arxivId = {2301.12503},
author = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
eprint = {2301.12503},
file = {:Users/huanzhang/Downloads/2301.12503.pdf:pdf},
issn = {26403498},
title = {{{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models}},
url = {http://arxiv.org/abs/2301.12503},
year = {2023}
}
@article{Demorest2008,
abstract = {The purpose of this study was to test the cross-cultural musical understanding of trained and untrained listeners from two distinct musical cultures by exploring the influence of enculturation on musical memory performance. Trained and untrained participants (N = 150) from the United States and Turkey listened to a series of novel musical excerpts from both familiar and unfamiliar cultures and then completed a recognition memory task for each set of examples. All participants were significantly better at remembering novel music from their native culture and there were no performance differences based on musical expertise. In addition, Turkish participants were better at remembering Western music, a familiar but nonnative musical culture, than Chinese music. The results suggest that our cognitive schemata for musical information are culturally derived and that enculturation influences musical memory at a structural level. {\textcopyright} 2008 By the Regents of the University of California.},
author = {Demorest, Steven M. and Morrison, Steven J. and Beken, M{\"{u}}nir N. and Jungbluth, Denise},
doi = {10.1525/mp.2008.25.3.213},
file = {:Users/huanzhang/Downloads/Lost_in_Translation_An_Enculturation_Effect_in_Mus.pdf:pdf},
issn = {07307829},
journal = {Music Perception},
keywords = {Cultural differences,Enculturation,Familiarity,Music expertise,Music memory},
number = {3},
pages = {213--223},
title = {{Lost in translation: An enculturation effect in music memory performance}},
volume = {25},
year = {2008}
}
@techreport{SerraTowardsSeries,
abstract = {We study the use of a time series encoder to learn representations that are useful on data set types with which it has not been trained on. The encoder is formed of a convolutional neural network whose temporal output is summarized by a con-volutional attention mechanism. This way, we obtain a compact, fixed-length representation from longer, variable-length time series. We evaluate the performance of the proposed approach on a well-known time series classification benchmark, considering full adaptation, partial adaptation, and no adaptation of the encoder to the new data type. Results show that such strategies are competitive with the state-of-the-art, often outperforming conceptually-matching approaches. Besides accuracy scores, the facility of adaptation and the efficiency of pre-trained encoders make them an appealing option for the processing of scarcely-or non-labeled time series.},
archivePrefix = {arXiv},
arxivId = {1805.03908v1},
author = {Serr{\`{a}}, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
eprint = {1805.03908v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Serr{\`{a}}, Pascual, Karatzoglou - Unknown - Towards a Universal Neural Network Encoder for Time Series.pdf:pdf},
keywords = {Neural networks,classification,generalization,multi-task learning,representation learning,time series,transfer learning},
title = {{Towards a Universal Neural Network Encoder for Time Series}}
}
@article{Chin-Shyurng2019Real-timeCamera,
abstract = {Gesture recognition is a human-computer interaction method, which is widely used for educational, medical, and entertainment purposes. Humans also use gestures to communicate with each other, and musical conducting uses gestures in this way. In musical conducting, conductors wave their hands to control the speed and strength of the music played. However, beginners may have a limited comprehension of the gestures and might not be able to properly follow the ensembles. Therefore, this paper proposes a real-time musical conducting gesture recognition system to help music players improve their performance. We used a single-depth camera to capture image inputs and establish a real-time dynamic gesture recognition system. The Kinect software development kit created a skeleton model by capturing the palm position. Different palm gestures were collected to develop training templates for musical conducting. The dynamic time warping algorithm was applied to recognize the different conducting gestures at various conducting speeds, thereby achieving real-time dynamic musical conducting gesture recognition. In the experiment, we used 5600 examples of three basic types of musical conducting gestures, including seven capturing angles and five performing speeds for evaluation. The experimental result showed that the average accuracy was 89.17% in 30 frames per second.},
author = {Chin-Shyurng, Fahn and Lee, Shih En and Wu, Meng Luen},
doi = {10.3390/app9030528},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chin-Shyurng, Lee, Wu - 2019 - Real-time musical conducting gesture recognition based on a dynamic time warping classifier using a singl.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Depth camera,Dynamic gesture recognition,Dynamic time warping,Human-computer interaction,Musical conductor,Musical gesture,Palm tracking},
number = {3},
title = {{Real-time musical conducting gesture recognition based on a dynamic time warping classifier using a single-depth camera}},
volume = {9},
year = {2019}
}
@article{Miron2014Audio-to-scoreRecordings,
abstract = {In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise alignment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and offsets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best combination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets.},
author = {Miron, Marius and Carabias-Orti, Julio Jos{\'{e}} and Janer, Jordi},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Miron, Carabias-Orti, Janer - 2014 - Audio-to-score alignment at note level for orchestral recordings.pdf:pdf},
journal = {Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014},
pages = {125--130},
title = {{Audio-to-score alignment at note level for orchestral recordings}},
year = {2014}
}
@article{DiGiorgi2022,
abstract = {Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve a high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. We propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, using a novel harmonic error metric.},
archivePrefix = {arXiv},
arxivId = {2208.12782},
author = {{Di Giorgi}, Bruno and Levy, Mark and Sharp, Richard},
eprint = {2208.12782},
file = {:Users/huanzhang/Downloads/188.pdf:pdf},
title = {{Mel Spectrogram Inversion with Stable Pitch}},
url = {http://arxiv.org/abs/2208.12782},
year = {2022}
}
@article{Tsai2020,
abstract = {This paper investigates the problem of matching a MIDI file against a large database of piano sheet music images. Previous sheet-audio and sheet-MIDI alignment approaches have primarily focused on a 1-to-1 alignment task, which is not a scalable solution for retrieval from large databases. We propose a method for scalable cross-modal retrieval that might be used to link the Lakh MIDI dataset with IM-SLP sheet music data. Our approach is to modify a previously proposed feature representation called a symbolic bootleg score to be suitable for hashing. On a database of 5,000 piano scores containing 55,000 individual sheet music images, our system achieves a mean reciprocal rank of 0.84 and an average retrieval time of 25.4 seconds.},
author = {Tsai, T. J.},
doi = {10.1109/ICASSP40776.2020.9053815},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tsai - 2020 - Towards Linking the Lakh and IMSLP Datasets(2).pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {MIDI,cross-modal,retrieval,search,sheet music},
pages = {546--550},
publisher = {IEEE},
title = {{Towards Linking the Lakh and IMSLP Datasets}},
volume = {2020-May},
year = {2020}
}
@article{Authore,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Downloads/387 Submission.pdf:pdf},
title = {{EXPRESSIONNET : A LARGE-SCALE DATASET FOR EXPRESSION-CONDITIONED SYMBOLIC MUSIC}}
}
@article{Bresin2000Articulation545,
abstract = {Articulation strategies applied by pianists in expressive performances of the same score are analysed. Measurements of key overlap time and its relation to the inter-onset-interval are collected for notes marked legato and staccato in the first sixteen bars of the Andante movement of W. A. Mozart's Piano Sonata in G major, K 545. Five pianists played the piece nine times. First, they played in a way that they considered (optimal). In the remaining eight performances they were asked to represent different expressive characters, as specified in terms of different adjectives. Legato, staccato, and repeated notes articulation applied by the right hand were examined by means of statistical analysis. Although the results varied considerably between pianists, some trends could be observed. The pianists generally used similar strategies in the renderings intended to represent different expressive characters. Legato was played with a key overlap ratio that depended on the inter-onset-interval (IOI). Staccato tones had approximate duration of 40% of the IOI. Repeated notes were played with a duration of about 60% of the IOI. The results seem useful as a basis for articulation rules in grammars for automatic piano performance.},
annote = {key overlap time (KOT)
Staccato has 40% of the IOI 

KOT is proportional to the IOI
KOT dependent of the interval size (maybe because of voices???)


Exp:
- first have a natural version * 1s
- comparison between different descriptors * 8

repeated notes are rendered differently from staccato notes.},
author = {Bresin, Roberto and {Umberto Battel}, Giovanni},
doi = {10.1076/jnmr.29.3.211.3092},
file = {:Users/huanzhang/Downloads/2000_JNMR_BresinBattel.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {3},
pages = {211--224},
title = {{Articulation Strategies in Expressive Piano Performance Analysis of Legato, Staccato, and Repeated Notes in Performances of the {Andante} Movement of {Mozart}'s Sonata in G Major (K.545)}},
url = {https://www.researchgate.net/publication/233013901_Articulation_Strategies_in_Expressive_Piano_Performance_Analysis_of_Legato_Staccato_and_Repeated_Notes_in_Performances_of_the_Andante_Movement_of_Mozart's_Sonata_in_G_Major_K_545},
volume = {29},
year = {2000}
}
@inproceedings{Rafee2021PerformerModels,
abstract = {Music Performers have their own idiosyncratic way of interpreting a musical piece. A group of skilled performers playing the same piece of music would likely to inject their unique artistic styles in their performances. The variations of the tempo, timing, dynamics, articulation etc. from the actual notated music are what make the performers unique in their performances. This study presents a dataset consisting of four movements of Schubert's ``Sonata in B-flat major, D.960" performed by nine virtuoso pianists individually. We proposed and extracted a set of expressive features that are able to capture the characteristics of an individual performer's style. We then present a performer identification method based on the similarity of feature distribution, given a set of piano performances. The identification is done considering each feature individually as well as a fusion of the features. Results show that the proposed method achieved a precision of 0.903 using fusion features. Moreover, the onset time deviation feature shows promising result when considered individually.},
archivePrefix = {arXiv},
arxivId = {2108.02576},
author = {Rafee, Syed Rifat Mahmud and Fazekas, Gyorgy and Wiggins, Geraint A.},
booktitle = {Proceedings of the International Computer Music Conference (ICMC)},
eprint = {2108.02576},
file = {:Users/huanzhang/Downloads/icmc_latest.pdf:pdf},
title = {{Performer Identification From Symbolic Representation of Music Using Statistical Models}},
url = {https://arxiv.org/pdf/2108.02576.pdf},
year = {2021}
}
@techreport{Trainor2004Long-termTimbre,
abstract = {We show that infants' long-term memory representations for melodies are not just reduced to the structural features of relative pitches and durations, but contain surface or performance tempo-and timbre-specific information. Using a head turn preference procedure, we found that after a one week exposure to an old English folk song, infants preferred to listen to a novel folk song, indicating that they remembered the familiarized melody. However, if the tempo (25% faster or slower) or instrument timbre (harp vs. piano) of the familiarized melody was changed at test, infants showed no preference, indicating that they remembered the specific tempo and timbre of the melodies. The results are consistent with an exemplar-based model of memory in infancy rather than one in which structural features are extracted and performance features forgotten.},
annote = {From Duplicate 1 (Long-term memory for music: infants remember tempo and timbre - Trainor, Laurel J; Wu, Luann; Tsang, Christine D)

Week 3},
author = {Trainor, Laurel J and Wu, Luann and Tsang, Christine D},
booktitle = {Developmental Science},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Trainor, Wu, Tsang - 2004 - Long-term memory for music infants remember tempo and timbre.pdf:pdf},
pages = {289--296},
title = {{Long-term memory for music: infants remember tempo and timbre}},
volume = {7},
year = {2004}
}
@article{Lee2020,
abstract = {Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning , classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.},
archivePrefix = {arXiv},
arxivId = {2008.03729v2},
author = {Lee, Jongpil and Bryan, Nicholas J and Salamon, Justin and Jin, Zeyu and Nam, Juhan},
eprint = {2008.03729v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - Unknown - METRIC LEARNING VS CLASSIFICATION FOR DISENTANGLED MUSIC REPRESENTATION LEARNING.pdf:pdf},
journal = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Metric Learning vs Classification for Disentangled Music Representation Learning}},
year = {2020}
}
@article{Dean2014,
abstract = {We assessed the hypothesis elaborated by Pressing and by the first author that improvisation is normally based on the generation of segmental contrasts in musical features. Nine experienced improvisers performed a series of free- and 3-section referent-based pieces, each of about three minutes. Each player undertook eight improvisations with 'referent' instructions, preceded and followed by a free improvisation. A MIDI-equipped grand piano was used, and audio and MIDI were recorded. Computational analyses of the MIDI data assessed whether performers realized the referents successfully, and then determined the large-scale segmentation of the free improvisations. The performers almost always fulfilled the referents (68/72 improvisations, p < 0.0001), and their free improvisations were also susceptible to large-scale segmentation (p < 0.005). Since performers were only told of the referent structures as the session proceeded, the similar segmentation of both the first and second free improvisation suggests that such segmentation is common in solo free improvisation, in accord with the hypothesis. {\textcopyright} 2014 Taylor & Francis.},
author = {Dean, Roger T. and Bailes, Freya and Drummond, Jon},
doi = {10.1080/09298215.2013.859710},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dean, Bailes, Drummond - 2014 - Generative Structures in Improvisation Computational Segmentation of Keyboard Performances.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {music analysis,performance,representation,tonality},
number = {2},
pages = {224--236},
publisher = {Taylor and Francis Ltd.},
title = {{Generative Structures in Improvisation: Computational Segmentation of Keyboard Performances}},
volume = {43},
year = {2014}
}
@inproceedings{Hamilton2017InductiveGraphs,
author = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
file = {:Users/huanzhang/Downloads/graphsage-nips17.pdf:pdf},
issn = {03706699},
title = {{Inductive Representation Learning on Large Graphs}},
year = {2017}
}
@article{Ramoneda2022PianoLearning,
abstract = {Hand and finger movements are a mainstay of piano technique. Automatic Fingering from symbolic music data allows us to simulate finger and hand movements. Previous proposals achieve automatic piano fingering based on knowledge-driven or data-driven techniques. We combine both approaches with deep reinforcement learning techniques to derive piano fingering. Finally, we explore how to incorporate past experience into reinforcement learning-based piano fingering in further work.},
archivePrefix = {arXiv},
arxivId = {2111.08009},
author = {Ramoneda, Pedro and Miron, Marius and Serra, Xavier},
eprint = {2111.08009},
file = {:Users/huanzhang/Downloads/Piano_Fingering_with_Reinforcement_Learning.pdf:pdf},
title = {{Piano Fingering with Reinforcement Learning}},
url = {http://arxiv.org/abs/2111.08009},
year = {2021}
}
@article{DING2024121902,
abstract = {Acoustic scene classification (ASC) has gained significant interest recently due to its diverse applications. Various audio signal processing and machine learning methods have been proposed for ASC. The volume and scope of ASC publications covering theories, algorithms, and applications have also been expanded. However, no recent comprehensive surveys exist to collect and organize the knowledge, impeding the ability of researchers and its applications. To fill this gap, we present an up-to-date overview of ASC methods, covering earlier works and recent advances. In this work, we first define a general framework for ASC, starting with a historical review of previous research in the ASC field. Then, we review core techniques for ASC that have achieved good performance. Focus on machine learning based ASC systems, this work summarizes and groups the existing techniques in terms of data processing, feature acquisition, and modeling. Furthermore, we provide a summary of the available resources for ASC research and analyze ASC tasks in Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. Finally, we discuss limitations of the current ASC algorithms and open challenges to possible future developments toward practical applications of ASC systems.},
author = {Ding, Biyun and Zhang, Tao and Wang, Chao and Liu, Ganjun and Liang, Jinhua and Hu, Ruimin and Wu, Yulin and Guo, Difei},
doi = {https://doi.org/10.1016/j.eswa.2023.121902},
issn = {0957-4174},
journal = {Expert Systems with Applications},
keywords = {Acoustic scene classification,DCASE,Data augmentation,Datasets,Deep learning,Environmental sound,Feature extraction,Modeling},
pages = {121902},
title = {{Acoustic scene classification: A comprehensive survey}},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024041},
volume = {238},
year = {2024}
}
@article{Rapoport2010Schoenberg-HartlebensPerformance,
abstract = {This work presents detailed examination of the various aspects of the vocal part of Pierrot Lunaire: literary, linguistic , melodic, acoustic, and vocal performance, and their interrelations. An examination of Hartleben's German translation reveals the decisive role of characteristic linguistic features in the German language in shaping rhythms in Schoenberg's "Sprechmelodies". Vocal analysis of speech intonation contours in the spoken texts, as read aloud by two persons of German native language, brings experimental evidence and elucidates the origin of Schoenberg's "Sprechmelodies" in intonation in German speech. Intonation patterns in excerpts of Schoenberg's own speech, recorded in 1931 and 1936, also subjected to such analysis, reveal typical intonation patterns in German speech, of relevance to his "Sprechmelodies" in Pierrot Lunaire. Singing-reciting of four poems by five artists: Erika Stiedry Wagner (recorded 1941), Jan de Gaetani (rec., in Schoenberg's Pierrot Lunaire is analyzed and compared, by analysis of their Fast Fourier Transform (FFT) spectrograms. These performances, spanning almost sixty years, reflect a variety of styles and approaches to this extraordinary work. A large variety of types and (temporal) structures of vocal tones were encountered-reflecting the vocal means the artists designed (mentally) in order to capture and express the unique atmosphere of the texts, and the music, in Schoenberg's unique style of Sprechgesang. These vocal tones are decomposed into elementary units and a special notation was devised for their description and classification. This forms the basis of a detailed analysis on the micro-level of a single tone and a single syllable. Analysis of the melodic phrases of Schoenberg's "Sprechmelodien" (speech melodies) in terms of melodic segment contours by means of this notation is also described. Thus, this special notation enables a symbolic representation, or transcription, of speech intonation, singing, and "Sprechgesang" melodic contours.},
author = {Rapoport, Eliezer},
doi = {10.1076/jnmr.33.1.71.35393},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2010 - Journal of New Music Research Schoenberg-Hartleben's Pierrot Lunaire Speech-Poem-Melody-Vocal Performance Eliezer Rapop.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
keywords = {FFT analysis,Pierrot Lunaire,Schoenberg,Speech Intonation,Sprechgesang,Vocal Performance,Voice},
title = {{Schoenberg-Hartleben's Pierrot Lunaire: Speech-Poem-Melody-Vocal Performance}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2010}
}
@article{Yeh2020,
abstract = {Several prior works have proposed various methods for the task of automatic melody harmonization, in which a model aims to generate a sequence of chords to serve as the harmonic accompaniment of a given multiple-bar melody sequence. In this paper, we present a comparative study evaluating and comparing the performance of a set of canonical approaches to this task, including a template matching based model, a hidden Markov based model, a genetic algorithm based model, and two deep learning based models. The evaluation is conducted on a dataset of 9,226 melody/chord pairs we newly collect for this study, considering up to 48 triad chords, using a standardized training/test split. We report the result of an objective evaluation using six different metrics and a subjective study with 202 participants.},
annote = {From Duplicate 1 (Automatic Melody Harmonization with Triad Chords: A Comparative Study - Yeh, Yin-Cheng; Hsiao, Wen-Yi; Fukayama, Satoru; Kitahara, Tetsuro; Genchel, Benjamin; Liu, Hao-Min; Dong, Hao-Wen; Chen, Yian; Leong, Terence; Yang, Yi-Hsuan)

- Each model predicts a chord label for every half bar, M = 2T
- compute a sequence of features for melody


- HMM solved by Viterbi
- Genetic algorithm: only needs to know how to evaluate a solution},
archivePrefix = {arXiv},
arxivId = {2001.02360},
author = {Yeh, Yin-Cheng and Hsiao, Wen-Yi and Fukayama, Satoru and Kitahara, Tetsuro and Genchel, Benjamin and Liu, Hao-Min and Dong, Hao-Wen and Chen, Yian and Leong, Terence and Yang, Yi-Hsuan},
eprint = {2001.02360},
file = {:Users/huanzhang/Downloads/Automatic Melody Harmonization with Triad Chords- A Comparative Study.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yeh et al. - Unknown - ARTICLE TEMPLATE Automatic Melody Harmonization with Triad Chords A Comparative Study(2).pdf:pdf},
keywords = {Symbolic music generation,automatic melody harmonization,functional harmony,symbolic music generation},
month = {jan},
title = {{Automatic Melody Harmonization with Triad Chords: A Comparative Study}},
url = {http://arxiv.org/abs/2001.02360},
year = {2020}
}
@inproceedings{Parmar2021PianoAssessment,
abstract = {Can a computer determine a piano player's skill level? Is it preferable to base this assessment on visual analysis of the player's performance or should we trust our ears over our eyes? Since current CNNs have difficulty processing long video videos, how can shorter clips be sampled to best reflect the players skill level? In this work, we collect and release a first-of-its-kind dataset for multimodal skill assessment focusing on assessing piano player's skill level, answer the asked questions, initiate work in automated evaluation of piano playing skills and provide baselines for future work.},
archivePrefix = {arXiv},
arxivId = {2101.04884v2},
author = {Parmar, Paritosh and Reddy, Jaiden and Morris, Brendan},
booktitle = {IEEE 23th International Workshop on Multimedia Signal Processing (MMSP)},
eprint = {2101.04884v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Parmar, Reddy, Morris - Unknown - PIANO SKILLS ASSESSMENT.pdf:pdf},
keywords = {Ac-tion Quality Assessment,Index Terms-Automated Piano Skills Assessment},
title = {{Piano Skills Assessment}},
url = {https://arxiv.org/abs/2101.04884},
year = {2021}
}
@article{Miller,
archivePrefix = {arXiv},
arxivId = {arXiv:2407.05427v1},
author = {Miller, Matthias and F{\"{u}}rst, Daniel and Fischer, Maximilian T and Hauptmann, Hanna and Keim, Daniel and El-assady, Mennatallah and Jul, H C},
eprint = {arXiv:2407.05427v1},
file = {:Users/huanzhang/Downloads/2407.05427v1.pdf:pdf},
number = {1},
title = {{M ELODY V IS : Visual Analytics for Melodic Patterns in Sheet Music}}
}
@article{Kroger2008Rameau:Analysis,
abstract = {Automated harmonic analysis is an important and interesting music research topic. Although many researchers have studied solutions to this problem, there is no comprehensive and systematic comparison of the many techniques proposed. In this paper we present Rameau, a framework for automatic harmonic analysis we are developing. With Rameau we are able to reimplement and analyze previous techniques, and develop new ones as well. We present a performance evaluation of ten algorithms on a corpus of 140 Bach chorales. We also evaluate four of them using precision and recall and discuss possible improvements. We also present a numeric codification for tonal music with interesting properties, such as easy transposition, preservation of enharmonic information and easy conversion to standard pitch-class notation.},
author = {Kr{\"{o}}ger, Pedro and Passos, Alexandre and Sampaio, Marcos and {De Cidra}, Givaldo},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kr{\"{o}}ger et al. - 2008 - Rameau A system for automatic harmonic analysis.pdf:pdf},
journal = {International Computer Music Conference, ICMC 2008},
title = {{Rameau: A system for automatic harmonic analysis}},
year = {2008}
}
@inproceedings{Rhyu2022SketchingLearning,
abstract = {We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other.},
address = {Bengaluru, India},
archivePrefix = {arXiv},
arxivId = {2208.14867},
author = {Rhyu, Seungyeon and Kim, Sarah and Lee, Kyogu},
booktitle = {Proceeding of the International Society on Music Information Retrieval (ISMIR)},
eprint = {2208.14867},
file = {:Users/huanzhang/Downloads/151.pdf:pdf},
title = {{Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning}},
url = {http://arxiv.org/abs/2208.14867},
year = {2022}
}
@techreport{AdebayoSanityMaps,
abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings 2 .},
author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been and Brain, Google},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Adebayo et al. - Unknown - Sanity Checks for Saliency Maps.pdf:pdf},
title = {{Sanity Checks for Saliency Maps}},
url = {https://goo.gl/hBmhDt}
}
@article{Luo2022TowardsAudio,
abstract = {Disentangled sequential autoencoders (DSAEs) represent a class of probabilistic graphical models that describes an observed sequence with dynamic latent variables and a static latent variable. The former encode information at a frame rate identical to the observation, while the latter globally governs the entire sequence. This introduces an inductive bias and facilitates unsupervised disentanglement of the underlying local and global factors. In this paper, we show that the vanilla DSAE suffers from being sensitive to the choice of model architecture and capacity of the dynamic latent variables, and is prone to collapse the static latent variable. As a countermeasure, we propose TS-DSAE, a two-stage training framework that first learns sequence-level prior distributions, which are subsequently employed to regularise the model and facilitate auxiliary objectives to promote disentanglement. The proposed framework is fully unsupervised and robust against the global factor collapse problem across a wide range of model configurations. It also avoids typical solutions such as adversarial training which usually involves laborious parameter tuning, and domain-specific data augmentation. We conduct quantitative and qualitative evaluations to demonstrate its robustness in terms of disentanglement on both artificial and real-world music audio datasets.},
archivePrefix = {arXiv},
arxivId = {2205.05871},
author = {Luo, Yin-Jyun and Ewert, Sebastian and Dixon, Simon},
doi = {10.24963/ijcai.2022/458},
eprint = {2205.05871},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Luo, Ewert, Dixon - Unknown - Towards Robust Unsupervised Disentanglement of Sequential Data-A Case Study Using Music Audio.pdf:pdf},
isbn = {9781956792003},
issn = {10450823},
journal = {Proceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI)},
title = {{Towards Robust Unsupervised Disentanglement of Sequential Data: A Case Study Using Music Audio}},
year = {2022}
}
@inproceedings{Cancino-Chacon2020OnGame,
abstract = {A piece of music can be expressively performed, or interpreted, in a variety of ways. With the help of an online questionnaire, the Con Espressione Game, we collected some 1,500 descriptions of expressive character relating to 45 performances of 9 excerpts from classical piano pieces, played by different famous pianists. More specifically, listeners were asked to describe, using freely chosen words (preferably: adjectives), how they perceive the expressive character of the different performances. In this paper, we offer a first account of this new data resource for expressive performance research, and provide an exploratory analysis, addressing three main questions: (1) how similarly do different listeners describe a performance of a piece? (2) what are the main dimensions (or axes) for expressive character emerging from this?; and (3) how do measurable parameters of a performance (e.g., tempo, dynamics) and mid- and high-level features that can be predicted by machine learning models (e.g., articulation, arousal) relate to these expressive dimensions? The dataset that we publish along with this paper was enriched by adding hand-corrected score-to-performance alignments, as well as descriptive audio features such as tempo and dynamics curves.},
archivePrefix = {arXiv},
arxivId = {2008.02194},
author = {Cancino-Chac{\'{o}}n, Carlos and Peter, Silvan and Chowdhury, Shreyan and Aljanaki, Anna and Widmer, Gerhard},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2008.02194},
file = {:Users/huanzhang/Downloads/279.pdf:pdf},
title = {{On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game}},
url = {http://arxiv.org/abs/2008.02194},
year = {2020}
}
@techreport{Guo,
abstract = {Deep clustering utilizes deep neural networks to learn feature representation that is suitable for clustering tasks. Though demonstrating promising performance in various applications, we observe that existing deep clustering algorithms either do not well take advantage of convolutional neural networks or do not considerably preserve the local structure of data generating distribution in the learned feature space. To address this issue, we propose a deep convolutional embedded clustering algorithm in this paper. Specifically, we develop a convolutional autoencoders structure to learn embedded features in an end-to-end way. Then, a clustering oriented loss is directly built on embedded features to jointly perform feature refinement and cluster assignment. To avoid feature space being distorted by the clustering loss, we keep the decoder remained which can preserve local structure of data in feature space. In sum, we simultaneously minimize the reconstruction loss of convolutional autoencoders and the clustering loss. The resultant optimization problem can be effectively solved by mini-batch stochastic gradient descent and back-propagation. Experiments on benchmark datasets empirically validate the power of convolutional autoencoders for feature learning and the effectiveness of local structure preservation.},
author = {Guo, Xifeng and Liu, Xinwang and Zhu, En and Yin, Jianping},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - Unknown - Deep Clustering with Convolutional Autoencoders.pdf:pdf},
keywords = {Convolution-al Neural Networks,Convolutional Autoencoders,Deep Clustering,Unsupervised Learning},
title = {{Deep Clustering with Convolutional Autoencoders}}
}
@article{Huang2022,
abstract = {Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.},
archivePrefix = {arXiv},
arxivId = {2208.12415},
author = {Huang, Qingqing and Jansen, Aren and Lee, Joonseok and Ganti, Ravi and Li, Judith Yue and Ellis, Daniel P. W.},
eprint = {2208.12415},
file = {:Users/huanzhang/Downloads/2208.12415.pdf:pdf},
title = {{MuLan: A Joint Embedding of Music Audio and Natural Language}},
url = {http://arxiv.org/abs/2208.12415},
year = {2022}
}
@inproceedings{Li2023SiameseAssessment,
author = {Li, Xiaoquan and Weiss, Stephan and Yan, Yijun and Li, Yinhe and Ren, Jinchang and Soraghan, John and Gong, Ming},
booktitle = {Proceeding of the 31st European Signal Processing Conference},
doi = {10.23919/eusipco58844.2023.10289901},
file = {:Users/huanzhang/Downloads/Li_etal_EUSIPCO_2023_Siamese_residual_neural_network_for_musical_shape_evaluation_in_piano_performance_assessment.pdf:pdf},
isbn = {9789464593600},
issn = {22195491},
keywords = {audio,classification,musical shape evaluation,piano performance assessment,siamese network},
pages = {1--5},
title = {{Siamese Residual Neural Network for Musical Shape Evaluation in Piano Performance Assessment}},
year = {2023}
}
@article{Liu,
author = {Liu, Haoxuan and Wang, Zihao and Hong, Haorong and Feng, Youwei},
file = {:Users/huanzhang/Downloads/1929 Submission.pdf:pdf},
isbn = {3200102306},
title = {{MetaBGM : Dynamic Soundtrack Transformation For Continuous Multi-Scene Experiences With Ambient Awareness And Personalization}}
}
@article{Widmer2003InFactor,
abstract = {The article introduces the reader to a large interdisciplinary research project whose goal is to use AI to gain new insight into a complex artistic phenomenon. We study fundamental principles of expressive music performance by measuring performance aspects in large numbers of recordings by highly skilled musicians (concert pianists) and analyzing the data with state-of-the-art methods from areas such as machine learning, data mining, and data visualization. The article first introduces the general research questions that guide the project and then summarizes some of the most important results achieved to date, with an emphasis on the most recent and still rather speculative work. A broad view of the discovery process is given, from data acquisition through data visualization to inductive model building and pattern discovery, and it turns out that AI plays an important role in all stages of such an ambitious enterprise. Our current results show that it is possible for machines to make novel and interesting discoveries even in a domain such as music and that even if we might never find the "Horowitz Factor," AI can give us completely new insights into complex artistic behavior.},
annote = {- formalizing significant patterns and regularities in the artists' musical behavior
- extends the frontiers of AI-based scientific discovery},
author = {Widmer, Gerhard and Dixon, Simon and Goebl, Werner and Pampalk, Elias and Tobudic, Asmir},
file = {:Users/huanzhang/Downloads/In_Search_of_the_Horowitz_Factor.pdf:pdf},
issn = {07384602},
journal = {AI Magazine},
keywords = {Copyright {\textcopyright} 2003 American Association for Artifici},
number = {3},
pages = {111--130},
title = {{In Search of the {Horowitz} Factor}},
volume = {24},
year = {2003}
}
@article{ChenInfoGAN:Nets,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Gener-ative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, pres-ence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1606.03657.}
}
@article{Cideron2024,
archivePrefix = {arXiv},
arxivId = {arXiv:2402.04229v1},
author = {Cideron, Geoffrey and Girgin, Sertan and Verzetti, Mauro and Vincent, Damien and Kastelic, Matej and Borsos, Zal{\'{a}}n},
eprint = {arXiv:2402.04229v1},
file = {:Users/huanzhang/Downloads/2402.04229.pdf:pdf},
pages = {1--21},
title = {{MusicRL : Aligning Music Generation to Human Preferences}},
year = {2024}
}
@article{Buerhop2023,
author = {Buerhop, Claudia and Pickel, Tobias and Hauch, Jens and Peters, Ian Marius},
doi = {10.1002/pip.3649},
file = {:Users/huanzhang/Downloads/Progress in Photovoltaics - 2022 - Buerhop - Assessment of string performance using self‐referencing method in comparison.pdf:pdf},
keywords = {monitoring data,pv power station,underperformance},
number = {July 2022},
pages = {1329--1335},
title = {{Assessment of string performance using self-referencing method in comparison to performance ratio}},
year = {2023}
}
@techreport{Atherton2016,
abstract = {We present an analysis of musical influence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed networks. We form networks of lyrical influence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected components suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling confirm network centrality and provide insight into the most influential genres at the heart of the network. Next, we present metrics for influence and self-referential behavior, examining their interactions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters' genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, influence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical influence networks in music information retrieval research. The networks constructed in this study are made publicly available for research purposes.},
annote = {From Duplicate 2 (I said it first: Topological analysis of lyrical influence networks - Atherton, Jack; Kaneshiro, Blair)

networks: lyrics of three-word phrases, weighted by tf-idf
metrics for influence and examination of network

network formation:
each song (from different years) sharing the same phrase are connected by edge, and edge weight is determined by the product of both tf-idf score

techniques:
edge reduction

metrics for influence and self-referential behavior},
author = {Atherton, Jack and Kaneshiro, Blair},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Atherton, Kaneshiro - Unknown - I SAID IT FIRST TOPOLOGICAL ANALYSIS OF LYRICAL INFLUENCE NETWORKS.pdf:pdf},
isbn = {9780692755068},
pages = {654--660},
title = {{I said it first: Topological analysis of lyrical influence networks}},
url = {https://graph-tool.skewed.de/},
year = {2016}
}
@inproceedings{Wu2016,
author = {Wu, Chih-Wei and Gururani, Siddharth and Laguna, Christophoer and Pati, Ashis and Vidwans, Amruta and Lerch, Alexander},
booktitle = {Proceedings of International Conference on Music Perception and Cognition (ICMPC)},
file = {:Users/huanzhang/Downloads/Wuetal_2016_TowardstheObjectiveAssessmentofMusicPerformances.pdf:pdf},
isbn = {1876346655},
number = {July},
pages = {99--102},
title = {{Towards the Objective Assessment of Music Performances}},
year = {2016}
}
@inproceedings{Li2023MERTTraining,
abstract = {Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.},
archivePrefix = {arXiv},
arxivId = {2306.00107},
author = {Li, Yizhi and Yuan, Ruibin and Zhang, Ge and Ma, Yinghao and Chen, Xingran and Yin, Hanzhi and Xiao, Chenghao and Lin, Chenghua and Ragni, Anton and Benetos, Emmanouil and Gyenge, Norbert and Dannenberg, Roger and Liu, Ruibo and Chen, Wenhu and Xia, Gus and Shi, Yemin and Huang, Wenhao and Wang, Zili and Guo, Yike and Fu, Jie},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {2306.00107},
file = {:Users/huanzhang/Downloads/2306.00107.pdf:pdf},
title = {{{MERT}: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}},
year = {2024}
}
@inproceedings{Zhang2023AddingModels,
abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions"(zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
archivePrefix = {arXiv},
arxivId = {2302.05543},
author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV51070.2023.00355},
eprint = {2302.05543},
file = {:Users/huanzhang/Downloads/2302.05543v3.pdf:pdf},
isbn = {9798350307184},
issn = {15505499},
title = {{Adding Conditional Control to Text-to-Image Diffusion Models}},
year = {2023}
}
@phdthesis{Bernays2013ThePratice,
abstract = {This dissertation presents an interdisciplinary, systematic study of the expression of piano timbre by advanced-level pianists in the context of musical performance and prac- tice. To begin, general issues and aims are introduced, as well as differing definitions and perspectives on piano timbre from scientific and musical points of view. After the conception of piano timbre is presented as documented by pianists in pedagogical trea- tises, the perception and verbalisation of piano timbre is investigated with experimental and quantitative scientific methods. The words that pianists use to describe and talk about different timbral nuances are studied quantitatively, according to their semantic relationships, and a semantic map of common piano timbre descriptors is drawn out. In two separate studies, the perception of piano timbre by highly skilled pianists is investi- gated. Results suggest that advanced pianists can identify and label performer-controlled timbral nuances in audio recordings with consistency and agreement from production to perception. Finally, the production and gestural control of piano timbre in musical per- formance is explored using the B{\"{o}}sendorfer CEUS piano performance recording system.The PianoTouch toolbox, specifically developed inMATLABfor extracting performance features from high-resolution keyboard and pedalling data, is presented and used to study the expressive production of piano timbre through touch and gesture in CEUS-recorded performances by four pianists in five timbral nuances. Gestural spaces and portraits of the timbral nuances are obtained with differing patterns in intensity, attack, balance be- tween hands, articulation and pedalling. The data represents common strategies used for the expression of each timbral nuance in piano performance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bernays, Michel},
eprint = {arXiv:1011.1669v3},
file = {:Users/huanzhang/Downloads/bernays_michel_2013_these.pdf:pdf;:Users/huanzhang/Downloads/bernays_michel_2013_these.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {B{\"{o}}sendorfer CEUS,expression,gesture,perception,performance,piano,timbre,touch,verbal description},
pmid = {25246403},
school = {University of Montreal},
title = {{The expression and production of piano timbre: Gestural control and technique, perception and verbalisation in the context of piano performance and pratice}},
year = {2013}
}
@article{palmer1996anatomyexpression,
abstract = {Research on music performance often assumes that a performer's intention to emphasize musical structure as specified in a score accounts for most musical expression. Relatively unstudied sources of expression in performance include notational variants of compositional scores, performer-specific aspects, and the cultural norms of a particular idiom, including both stylistic patterns that exist across musical works and expectations that arise from a particular musical context. A case study of an expert performance of a Mozart piano sonata is presented in which influences of historical interpretations of scores, performer-specific treatments of ornamentation and pedaling, and music-theoretic notions of melodic expectancy and tension-relaxation are revealed. Patterns of organization internal to the performance expression transcended the coarse-grained information given in scores, suggesting that performance is a better starting point than a musical score for testing theories of many musical behaviors. {\textcopyright} 1996 by the Regents of the University of California.},
author = {Palmer, Caroline},
doi = {10.2307/40286178},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Palmer - 1996 - Anatomy of a performance Sources of musical expression.pdf:pdf},
issn = {07307829},
journal = {Music Perception},
number = {3},
pages = {433--453},
publisher = {University of California Press},
title = {{Anatomy of a performance: Sources of musical expression}},
volume = {13},
year = {1996}
}
@inproceedings{Alain2017UnderstandingProbes,
abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
archivePrefix = {arXiv},
arxivId = {1610.01644},
author = {Alain, Guillaume and Bengio, Yoshua},
booktitle = {Proceedings of ICLR Workshop},
eprint = {1610.01644},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Alain, Bengio Mila - Unknown - Understanding intermediate layers using linear classifier probes.pdf:pdf},
title = {{Understanding intermediate layers using linear classifier probes}},
url = {http://arxiv.org/abs/1610.01644},
year = {2017}
}
@article{Arzt2008,
abstract = {We present a system that automatically turns the pages of the music score for musicians during a performance. It is based on a new algorithm for following an incoming audio stream in real time and aligning it to a music score (in the form of a synthesised audio file). Precision and robustness of the algorithm are quantified in systematic experiments, and a demonstration using an actual page turning machine built by an Austrian company is described.},
annote = {From Duplicate 1 (Automatic page turning for musicians via real-time machine listening - Arzt, Andreas; Widmer, Gerhard; Dixon, Simon)

DTW of two audios
fft, and compute 84 pitch vectors, cost is euclidean distance},
author = {Arzt, Andreas and Widmer, Gerhard and Dixon, Simon},
doi = {10.3233/978-1-58603-891-5-241},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Arzt, Widmer, Dixon - 2008 - Automatic page turning for musicians via real-time machine listening.pdf:pdf},
isbn = {978158603891},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
number = {1},
pages = {241--245},
title = {{Automatic page turning for musicians via real-time machine listening}},
volume = {178},
year = {2008}
}
@inproceedings{Nakamura2014Merged-outputHands,
abstract = {This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of fingering for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the fingering model is also proposed and yields reasonable results.},
author = {Nakamura, Eita and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nakamura, Ono, Sagayama - Unknown - MERGED-OUTPUT HMM FOR PIANO FINGERING OF BOTH HANDS.pdf:pdf},
title = {{Merged-output HMM for Piano Fingering of Both Hands}},
year = {2014}
}
@article{Agostinelli2023MusicLMText,
abstract = {We introduce MusicLM, a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.},
archivePrefix = {arXiv},
arxivId = {2301.11325},
author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zal{\'{a}}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
eprint = {2301.11325},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Agostinelli et al. - Unknown - MusicLM Generating Music From Text.pdf:pdf},
journal = {arXiv preprint arXiv:2301.11325},
title = {{MusicLM: Generating Music From Text}},
url = {http://arxiv.org/abs/2301.11325},
year = {2023}
}
@article{Author2024,
author = {Author, First},
file = {:Users/huanzhang/Downloads/tismir-review-assignment-185-Manuscript-6063.pdf:pdf},
keywords = {composer classification,dataset,piano,style recognition},
pages = {1--13},
title = {{PBSCSR : The Piano Bootleg Score Composer Style Recognition Dataset}},
year = {2024}
}
@techreport{Luo2015DetectionPlaying,
abstract = {Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed.},
author = {Luo, Yin Jyun and Su, Li and Yang, Yi Hsuan and Chi, Tai Shih},
booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - DETECTION OF COMMON MISTAKES IN NOVICE VIOLIN PLAYING.pdf:pdf},
isbn = {9788460688532},
pages = {316--322},
title = {{Detection of common mistakes in novice violin playing}},
url = {http://perception.cm.nctu.edu.tw/sound-demo/.},
year = {2015}
}
@article{LiVideo-BasedMusic,
abstract = {In music performance, vibrato is an important artistic effect , where slight variations in pitch are introduced to add expressiveness and warmth. Automatic vibrato detection and analysis, although well studied for monophonic music , has rarely been explored for polyphonic music, because of the challenge in multi-pitch analysis. We propose a video-based approach for detecting and analyzing vibrato in polyphonic string music. Specifically, we capture the fine motion of the left hand of string players through optical flow analysis of video frames. We explore two methods. The first uses a feature extraction and SVM classification pipeline, and the second is an unsupervised technique based on autocorrelation analysis of the principal motion component. The proposed methods are compared with audio-only methods applied to individual instrument tracks separated from original audio mixture using the score. Experiments show that the proposed video-based methods achieve a significantly higher vibrato detection accuracy than the audio-based methods especially in high polyphony cases. Further experiments also demonstrate the utility of the approach in vibrato rate and extent analysis.},
author = {Li, Bochen and Dinesh, Karthik and Sharma, Gaurav and Duan, Zhiyao},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - VIDEO-BASED VIBRATO DETECTION AND ANALYSIS FOR POLYPHONIC STRING MUSIC.pdf:pdf},
title = {{Video-Based Vibrato Detectionand Analysis for Polyphonic String Music}}
}
@article{Repp1994DynamicsRevisited,
abstract = {Ten graduate.student pianists were recorded playing Robert Schumann's "Traumerei" three times on a Yamaha Disklavier. Their expressive dynamics were analyzed at the level of hammer (MIDI) velocities. Individual dynamic profiles were similar across repeated performances, more so for the right hand (soprano and alto voices) than for the left har.::i (tenor and bass voices). As expected, the soprano voice, which usually had the principal melody, was played with greater force than the other voices, which gained prominence only when they carried temporarily important melodic fragments. Independent of this voice differentiation, there was a tendency for velocity to increase with pitch, at least in the soprano and alto voices. While there was an overall tendency for velocities to increase with local tempo, there were salient local departures from this coupling. Individual differences in expressive dynamics were not striking and were only weakly related to individual differences in expressive timing.},
author = {Repp, Bruno H},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - 1994 - The Dynamics of Expressive Piano Performance Schumann's Traumerei Revisited.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
pages = {641--50},
title = {{The Dynamics of Expressive Piano Performance: Schumann's {"Traumerei"} Revisited}},
year = {1996}
}
@article{Steele2006,
abstract = {TIMBRE INVARIANCE REFERS to the ability to determine whether two notes at different pitches were played or sung by the same instrument or voice. Handel and Erickson (2001) reported that nonmusician listeners heard pairs of notes as coming uniformly from different instruments when the pitches were separated by an octave or more; they concluded that the bandwidth of timbre invariance was only one octave. Here we repli-cate that study with methodological refinements and include musicians as well as nonmusicians. We presented listeners with pairs of notes from two instruments (horn and bassoon) spanning a 2.5 octave range, and listeners judged whether two notes were the same pitch and produced by the same instrument. Nonmusicians replicated Handel and Erickson's result of a decline in timbre invariance beyond 1.0 octave, whereas musicians' performance declined less-to about 80% correct at 2.5 octave. Pitch judgments did not vary across the range and were more accurate for musicians than for nonmusicians. The difference between musicians and nonmusicians in timbre judgments suggests caution in stating a range for the operation of timbre invariance. T IMBRE is defined as the quality by which two notes may be judged to be dissimilar when pitch and loudness have been equated (ANSI, 1973). This definition tells little about the nature of timbre itself, only that it is not subsumed by pitch or loudness. Hadja, Kendall, Carterette, and Harshberger (1997) reviewed timbre research methodology and concluded that there is still a great deal of mystery about timbre as a musical variable. Psychologically, it is unclear whether timbre should be measured on a categorical or continuous scale. The physical correlates of timbre are incompletely defined, although it is clear that they consist of more than steady-state harmonics alone (Iverson & Krumhansel, 1993; Risset & Wessel, 1999). People treat timbre as a categorical variable in daily use, identifying timbre with the source instrument. Three different notes on the piano keyboard have individual timbres, since each played note produces a unique combination of transients and harmonics. However, people will treat them as having the common quality of being piano notes, distinct from, for example, clarinet notes. Handel and Erickson (2001) used the phrase timbre invariance to describe the quality that is assumed to be preserved across the notes of an instrument. Handel and Erickson asked the experimental question of the range, or bandwidth, of timbre invari-ance by investigating a listener's ability to determine whether two notes at different pitches came from the same instrument or singer. The first experiment of Handel and Erickson (2001) investigated how well listeners identify whether two notes at different pitches were played by the same instrument or not. Their participants were musically naive undergraduates. The note range was from G3 to C6 and the notes came from six wind instruments, recorded on the McGill University Master Samples Compact Disks (Opolko & Wapnick, 1987, 1989). Each note consisted of the first 1.5 to 2.5 s of the original recordings, with a smoothed offset. Notes were normalized to the same amplitude. On every trial, two notes were presented in an ABABA sequence. The lowest pitch was presented first, and there was a 0.5 s silence between each note. The participants responded on a four-point scale ranging from "(1) very certain identical instrument" to "(4) very certain different instruments. " Handel and Erickson reported that average judgments could be collapsed into two categories, note pairs with less than one octave separation and note pairs with more than one octave separation. Note pairs separated by more than one octave were judged as coming from different instruments uniformly. Handel and Erickson concluded that one octave of separation of notes constitutes a border, beyond which timbre invariance breaks},
annote = {From Duplicate 2 (Is the Bandwidth for Timbre Invariance Only One Octave? - Steele, Kenneth M; Williams, Amber K)

Week 3},
author = {Steele, Kenneth M and Williams, Amber K},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Steele, Williams - 2006 - Is the Bandwidth for Timbre Invariance Only One Octave IS THE BANDWIDTH FOR TIMBRE INVARIANCE ONLY ONE OCTAVE.pdf:pdf},
issn = {1533-8312},
title = {{Is the Bandwidth for Timbre Invariance Only One Octave?}},
url = {http://theremin.music.},
year = {2006}
}
@article{Zhu2018XiaoIceBand,
abstract = {With the development of knowledge of music composition and the recent increase in demand, an increasing number of companies and research institutes have begun to study the automatic generation of music. However, previous models have limitations when applying to song generation, which requires both the melody and arrangement. Besides, many critical factors related to the quality of a song such as chord progression and rhythm patterns are not well addressed. In particular, the problem of how to ensure the harmony of multi-track music is still underexplored. To this end, we present a focused study on pop music generation, in which we take both chord and rhythm influence of melody generation and the harmony of music arrangement into consideration. We propose an end-to-end melody and arrangement generation framework , called XiaoIce Band, which generates a melody track with several accompany tracks played by several types of instruments. Specifically, we devise a Chord based Rhythm and Melody Cross-Generation Model (CRMCG) to generate melody with chord progressions. Then, we propose a Multi-Instrument Co-Arrangement Model (MICA) using multi-task learning for multi-track music arrangement. Finally, we conduct extensive experiments on a real-world dataset, where the results demonstrate the effectiveness of XiaoIce Band.},
author = {Zhu, Hongyuan and Liu, Qi and Yuan, Nicholas Jing and Qin, Chuan and Li, Jiawei and Zhang, Kun and Zhou, Guang and Wei, Furu and Xu, Yuanchun and Chen, Enhong},
doi = {10.1145/3219819.3220105},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2018 - XiaoIce Band.pdf:pdf},
isbn = {9781450355520},
keywords = {acm reference format,harmony evaluation,joint learning,melody and arrangement generation,multi-task,music generation},
pages = {2837--2846},
title = {{XiaoIce Band}},
year = {2018}
}
@inproceedings{Zhang2024MusicMagusModels,
abstract = {Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, the task of editing these generated music remains a significant challenge. This paper introduces a novel approach to edit music generated by such models, enabling the modification of specific attributes, such as genre, mood, and instrument, while maintaining other aspects unchanged. Our method transforms text editing to the latent space manipulation, and adds an additional constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. We also show the practical applicability of our approach in real-world music editing scenarios.},
archivePrefix = {arXiv},
arxivId = {2402.06178},
author = {Zhang, Yixiao and Ikemiya, Yukara and Xia, Gus and Murata, Naoki and Mart{\'{i}}nez-Ram{\'{i}}rez, Marco A. and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon},
booktitle = {Proceeding of the International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2024/864},
eprint = {2402.06178},
file = {:Users/huanzhang/Downloads/2402.06178v3.pdf:pdf},
isbn = {9781956792041},
issn = {10450823},
title = {{MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models}},
year = {2024}
}
@article{Yu2022,
abstract = {Singing melody extraction from polyphonic music is a critical and challenging task in music information retrieval (MIR). However, due to the interfere of the accompaniment and the background noise, it is key and challenging to obtain a global semantic representation that discriminates the singing melody line. To address this issue, we consider the two aspects that regards to obtaining the global semantic representation: the global relationships in the spectrum and the relationships between channels. In this paper, we propose a novel hierarchical graph-based network for singing melody extraction. In particular, according to its characteristics of the spectrum, we first model the spectrum into graph structure, a two-layer graph convolution network is used to obtain the global semantic representation in the spectrum. Then to capture the relationships between channels, channel-wise graph convolution module is devised to capture and reasoning the relationship between channels. The conducted experiments demonstrate the effectiveness of the proposed network.},
author = {Yu, Shuai and Chen, Xi and Li, Wei},
doi = {10.1109/ICASSP43922.2022.9747629},
file = {:Users/huanzhang/Downloads/Hierarchical_Graph-Based_Neural_Network_for_Singing_Melody_Extraction.pdf:pdf},
isbn = {9781665405409},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Graph-based Neural Network,Music Information Retrieval,Singing Melody Extraction},
pages = {626--630},
publisher = {IEEE},
title = {{Hierarchical Graph-Based Neural Network for Singing Melody Extraction}},
volume = {2022-May},
year = {2022}
}
@article{Maezawa2019RenderingRNN,
address = {Delft, Netherlands},
author = {Maezawa, Akira and Yamamoto, Kazuhiko and Fujishima, Takuya},
file = {:Users/huanzhang/Downloads/000105 (1).pdf:pdf},
isbn = {9781732729919},
journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Rendering music performance with interpretation variations using conditional variational {RNN}}},
year = {2019}
}
@article{Porcaro,
abstract = {Recognizing Musical Entities is important for Music Information Retrieval (MIR) since it can improve the performance of several tasks such as music recommendation, genre classification or artist similarity. However, most entity recognition systems in the music domain have concentrated on formal texts (e.g. artists biographies, encyclope-dic articles, etc.), ignoring rich and noisy user-generated content. In this work, we present a novel method to recognize musical entities in Twit-ter content generated by users following a classical music radio channel. Our approach takes advantage of both formal radio schedule and users tweets to improve entity recognition. We instantiate several machine learning algorithms to perform entity recognition combining task-specific and corpus-based features. We also show how to improve recognition results by jointly considering formal and user-generated content.},
archivePrefix = {arXiv},
arxivId = {1904.00648v1},
author = {Porcaro, Lorenzo and Saggion, Horacio},
eprint = {1904.00648v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Porcaro, Saggion - Unknown - Recognizing Musical Entities in User-generated Content.pdf:pdf},
keywords = {Content,Entity,Information,Music,Named,Recognition {\textperiodcentered},Retrieval {\textperiodcentered},User-generated},
title = {{Recognizing Musical Entities in User-generated Content}},
url = {http://www.cp.jku.at/datasets/MMTD}
}
@inproceedings{Qu2022,
abstract = {The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of "equal-loudness contour" can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre have rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines.},
annote = {psychoacoustic loudness estimation of natural piano tone

- psychoacoustic 
- ML

performance control transfer: adjust MIDI velocities on different pianos},
archivePrefix = {arXiv},
arxivId = {2209.10674},
author = {Qu, Yang and Qin, Yutian and Chao, Lecheng and Qian, Hangkai and Wang, Ziyu and Xia, Gus},
booktitle = {Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2209.10674},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Qu et al. - Unknown - MODELING PERCEPTUAL LOUDNESS OF PIANO TONE THEORY AND APPLICATIONS.pdf:pdf},
title = {{Modeling Perceptual Loudness of Piano Tone: Theory and Applications}},
url = {http://arxiv.org/abs/2209.10674},
year = {2022}
}
@article{Gupta2023,
abstract = {We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it uses the incontext learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing subroutines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VIsPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like Visprog are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.},
archivePrefix = {arXiv},
arxivId = {2211.11559},
author = {Gupta, Tanmay and Kembhavi, Aniruddha},
doi = {10.1109/CVPR52729.2023.01436},
eprint = {2211.11559},
file = {:Users/huanzhang/Downloads/2211.11559.pdf:pdf},
isbn = {9798350301298},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Vision,and reasoning,language},
pages = {14953--14962},
title = {{Visual Programming: Compositional visual reasoning without training}},
volume = {2023-June},
year = {2023}
}
@inproceedings{Maman2023PerformanceSynthesis,
abstract = {Generating multi-instrument music from symbolic music representations is an important task in Music Information Retrieval (MIR). A central but still largely unsolved problem in this context is musically and acoustically informed control in the generation process. As the main contribution of this work, we propose enhancing control of multi-instrument synthesis by conditioning a generative model on a specific performance and recording environment, thus allowing for better guidance of timbre and style. Building on state-of-the-art diffusion-based music generative models, we introduce performance conditioning - a simple tool indicating the generative model to synthesize music with style and timbre of specific instruments taken from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. Our project page, including samples and demonstrations, is available at benadar293.github.io/midipm},
archivePrefix = {arXiv},
arxivId = {2309.12283},
author = {Maman, Ben and Zeitler, Johannes and M{\"{u}}ller, Meinard and Bermano, Amit H.},
eprint = {2309.12283},
file = {:Users/huanzhang/Downloads/2309.12283.pdf:pdf},
title = {{Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis}},
url = {http://arxiv.org/abs/2309.12283},
year = {2023}
}
@article{Jacoby2015AnHarmony,
abstract = {We present new tools for categorizing chords based on corpus data, applicable to a variety of representations from Roman numerals to MIDI notes. Using methods from information theory, we propose that harmonic theories should be evaluated by at least two criteria, accuracy (how well the theory describes the musical surface) and complexity (the efficiency of the theory according to Occam's razor). We use our methods to consider a range of approaches in music theory, including function theory, root functionality, and the figured-bass tradition. Using new corpus data as well as eleven datasets from five published works, we argue that our framework produces results consistent both with musical intuition and previous work, primarily by recovering the tonic/subdominant/dominant categorization central to traditional music theory. By showing that functional harmony can be analysed as a clustering problem, we link machine learning, information theory, corpus analysis and music theory.},
author = {Jacoby, Nori and Tishby, Naftali and Tymoczko, Dmitri},
doi = {10.1080/09298215.2015.1036888},
file = {:Users/huanzhang/Downloads/JacobyTishbyTymockzo.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {cluster analysis,corpus analysis,functional harmony,information bottleneck,information theory},
number = {3},
pages = {219--244},
title = {{An Information Theoretic Approach to Chord Categorization and Functional Harmony}},
volume = {44},
year = {2015}
}
@phdthesis{Li2016ExpressiveSelection,
author = {Li, Shengchen},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li - 2016 - Expressive timing analysis in classical piano performance by mathematical model selection.pdf:pdf},
school = {Queen Mary University of London},
title = {{Expressive timing analysis in classical piano performance by mathematical model selection}},
year = {2016}
}
@article{Fritz2009,
abstract = {It has long been debated which aspects of music perception are universal and which are developed only after exposure to a specific musical culture [1-5]. Here, we report a crosscultural study with participants from a native African population (Mafa) and Western participants, with both groups being naive to the music of the other respective culture. Experiment 1 investigated the ability to recognize three basic emotions (happy, sad, scared/fearful) expressed in Western music. Results show that the Mafas recognized happy, sad, and scared/fearful Western music excerpts above chance, indicating that the expression of these basic emotions in Western music can be recognized universally. Experiment 2 examined how a spectral manipulation of original, naturalistic music affects the perceived pleasantness of music in Western as well as in Mafa listeners. The spectral manipulation modified, among other factors, the sensory dissonance of the music. The data show that both groups preferred original Western music and also original Mafa music over their spectrally manipulated versions. It is likely that the sensory dissonance produced by the spectral manipulation was at least partly responsible for this effect, suggesting that consonance and permanent sensory dissonance universally influence the perceived pleasantness of music. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Fritz, Thomas and Jentschke, Sebastian and Gosselin, Nathalie and Sammler, Daniela and Peretz, Isabelle and Turner, Robert and Friederici, Angela D. and Koelsch, Stefan},
doi = {10.1016/J.CUB.2009.02.058},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Fritz et al. - 2009 - Universal Recognition of Three Basic Emotions in Music.pdf:pdf},
issn = {0960-9822},
journal = {Current Biology},
keywords = {SYSNEURO},
month = {apr},
number = {7},
pages = {573--576},
publisher = {Cell Press},
title = {{Universal Recognition of Three Basic Emotions in Music}},
volume = {19},
year = {2009}
}
@article{Nichol2021,
abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
archivePrefix = {arXiv},
arxivId = {2112.10741},
author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
eprint = {2112.10741},
file = {:Users/huanzhang/Downloads/2112.10741.pdf:pdf},
title = {{GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models}},
url = {http://arxiv.org/abs/2112.10741},
year = {2021}
}
@inproceedings{Dixon2002b,
abstract = {Title from title screen. Proceedings of the 7th International Conference on Music Perecption & Cognition, held July 17-21, 2002 in Sydney, Australia. Proceedings are on searchable PDF documents.},
author = {Dixon, Simon and Goebl, Werner},
booktitle = {Proceedings of the 7th International Conference on Music Perception and Cognition, Sydney, 2002},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Stevens, Australian Music & Psychology Society. - 2002 - Pinpointing the Beat Tapping to Expressive Performances.pdf:pdf},
isbn = {1876346396},
title = {{Pinpointing the Beat: Tapping to Expressive Performances}},
year = {2002}
}
@article{Authorb,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Downloads/260 Submission.pdf:pdf},
title = {{DEEP-LEARNING BASED MUSIC PERFORMANCE ASSESSMENT USING PAIRWISE COMPARISONS AND REFERENCE SEQUENCES}}
}
@inproceedings{Nakamura2017PerformanceAlignment,
abstract = {This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals , to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results.},
author = {Nakamura, Eita and Yoshii, Kazuyoshi and Katayose, Haruhiro},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Nakamura, Yoshii, Katayose - Unknown - PERFORMANCE ERROR DETECTION AND POST-PROCESSING FOR FAST AND ACCURATE SYMBOLIC MUSIC ALIGNMENT.pdf:pdf},
title = {{Performance Error Detection and Post-Processing for Fast and Accurate Symbolic Music Alignment}},
year = {2017}
}
@article{LeeUNDERSTANDINGIMPLICATIONS,
abstract = {Most of the previous literature on music users' needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users' personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement , especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control.},
author = {Lee, Jin Ha and Price, Rachel},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Price - Unknown - UNDERSTANDING USERS OF COMMERCIAL MUSIC SERVICES THROUGH PERSONAS DESIGN IMPLICATIONS.pdf:pdf},
title = {{UNDERSTANDING USERS OF COMMERCIAL MUSIC SERVICES THROUGH PERSONAS: DESIGN IMPLICATIONS}}
}
@inproceedings{Zhang2023DisentanglingPerformance,
address = {Rhodes Island, Greece},
author = {Zhang, Huan and Dixon, Simon},
booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/icassp49357.2023.10095009},
file = {:Users/huanzhang/Downloads/Disentangling_the_Horowitz_Factor_Learning_Content_and_Style_From_Expressive_Piano_Performance.pdf:pdf},
isbn = {9781728163277},
issn = {15206149},
title = {{Disentangling the Horowitz Factor: Learning Content and Style From Expressive Piano Performance}},
year = {2023}
}
@inproceedings{Kong2020Large-scaleComposer,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.14805v1},
author = {Kong, Qiuqiang and Choi, Keunwoo and Wang, Yuxuan},
booktitle = {arXiv},
eprint = {arXiv:2010.14805v1},
file = {:Users/huanzhang/Downloads/2010.14805.pdf:pdf},
title = {{Large-Scale MIDI-Based Composer Classification}},
year = {2020}
}
@techreport{Marsden2010RecognitionReduction,
abstract = {Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a given theme are reported, using a test corpus derived from ten of Mozart's sets of variations for piano. Methods which examine the notes of the surface are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of matching based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are discussed. Possibilities for improved recognition of matching using reduction are outlined. {\textcopyright} 2010 International Society for Music Information Retrieval.},
author = {Marsden, Alan},
booktitle = {Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Marsden - 2010 - G5 C5 C3 C4 G5 C5 C3 C4 G5 C4 C5 C3 G5 C4 A5 C4 G5 C4 RECOGNITION OF VARIATIONS USING AUTOMATIC SCHENKERIAN REDUCTION.pdf:pdf},
isbn = {9789039353813},
pages = {501--506},
title = {{Recognition of variations using automatic schenkerian reduction}},
year = {2010}
}
@article{Yuan2024ChatMusicianLLM,
abstract = {While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.},
archivePrefix = {arXiv},
arxivId = {2402.16153},
author = {Yuan, Ruibin and Lin, Hanfeng and Wang, Yi and Tian, Zeyue and Wu, Shangda and Shen, Tianhao and Zhang, Ge and Wu, Yuhang and Liu, Cong and Zhou, Ziya and Ma, Ziyang and Xue, Liumeng and Wang, Ziyu and Liu, Qin and Zheng, Tianyu and Li, Yizhi and Ma, Yinghao and Liang, Yiming and Chi, Xiaowei and Liu, Ruibo and Wang, Zili and Li, Pengfei and Wu, Jingcheng and Lin, Chenghua and Liu, Qifeng and Jiang, Tao and Huang, Wenhao and Chen, Wenhu and Benetos, Emmanouil and Fu, Jie and Xia, Gus and Dannenberg, Roger and Xue, Wei and Kang, Shiyin and Guo, Yike},
eprint = {2402.16153},
file = {:Users/huanzhang/Downloads/2402.16153.pdf:pdf},
title = {{ChatMusician: Understanding and Generating Music Intrinsically with LLM}},
url = {http://arxiv.org/abs/2402.16153},
year = {2024}
}
@article{Zhao2022,
author = {Zhao, Xintao and Liu, Feng and Song, Changhe and Wu, Zhiyong and Kang, Shiyin and Tuo, Deyi and Meng, Helen and International, Shenzhen},
file = {:Users/huanzhang/Downloads/Disentangling_Content_and_Fine-Grained_Prosody_Information_Via_Hybrid_ASR_Bottleneck_Features_for_Voice_Conversion.pdf:pdf},
isbn = {9781665405409},
journal = {Proceedings of International Conference on Acoustic, Speech and Signal Processsing (ICASSP)},
pages = {7022--7026},
publisher = {IEEE},
title = {{Disentangling Content and Fine-Grained Prosody Information via Hybrid {ASR} Bottleneck Features for Voice Conversion}},
year = {2022}
}
@article{Theusch2021,
annote = {From Duplicate 2 (Absolute Pitch Twin Study and Segregation Analysis - Theusch, Elizabeth; Gitschier, Jane)

Week 2},
author = {Theusch, Elizabeth and Gitschier, Jane},
doi = {10.1375/twin.14.2.173},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/aap - 2021 - TRHG_14.2_finaltext.x_TRHG_14.2_finaltext.x.pdf:pdf},
title = {{Absolute Pitch Twin Study and Segregation Analysis}},
url = {https://doi.org/10.1375/twin.14.2.173},
year = {2021}
}
@article{Dai2024InterconnectionsPredictivity,
abstract = {Repetition and structure have a significant place in music theory, but the structure hierarchy and its influences are often ignored in both music analysis and music generation. In this article, we first describe novel algorithms based on repetition to extract music structure hierarchy from a MIDI data set of popular music and show its effectiveness through evaluation. Then, we introduce new data-driven approaches to estimate and validate structural influences in music. Results show that the automatically detected hierarchical repetition structures reveal significant interactions between structure and harmony, melody, rhythm, and predictivity. Different levels of hierarchy interact differently, providing evidence that structural hierarchy plays an important role in our popular music data set beyond simple notions of repetition or similarity. We further study how musical structure has evolved over decades of popular music writing. Finally, we discuss the importance of this work in highlighting roles that structure can play in music analysis, music similarity, music generation, music evaluation, and other music information retrieval tasks.},
author = {Dai, Shuqi and Zhang, Huan and Dannenberg, Roger B.},
doi = {10.1177/20592043241234758},
file = {:Users/huanzhang/Downloads/dai-et-al-2024-the-interconnections-of-music-structure-harmony-melody-rhythm-and-predictivity.pdf:pdf},
issn = {20592043},
journal = {Music and Science},
keywords = {Multilevel hierarchy,music prediction,music segmentation,music similarity,music structure,music understanding,pattern detection,repetition,structure analysis},
title = {{The Interconnections of Music Structure, Harmony, Melody, Rhythm, and Predictivity}},
volume = {7},
year = {2024}
}
@article{Kilgour,
annote = {target: clean, studio recorded 

In embedding lever, how close is the generated audio is towards the background clean music},
archivePrefix = {arXiv},
arxivId = {arXiv:1812.08466v4},
author = {Kilgour, Kevin and Zuluaga, Mauricio and Roblek, Dominik and Sharifi, Matthew},
eprint = {arXiv:1812.08466v4},
file = {:Users/huanzhang/Downloads/1812.08466.pdf:pdf},
pages = {1--20},
title = {{Frechet Audio Distance : A Metric for Evaluating Music Enhancement Algorithms}}
}
@article{Wang2018BeijingLearning,
abstract = {Speech synthesis is an important research content in the field of human-computer interaction and has a wide range of applications. As one of its branches, singing synthesis plays an important role. Beijing Opera is a famous traditional Chinese opera, and it is called Chinese quintessence. The singing of Beijing Opera carries some features of speech but it has its own unique pronunciation rules and rhythms which differ from ordinary speech and singing. In this paper, we propose three models for the synthesis of Beijing Opera. Firstly, the speech signals of the source speaker and the target speaker are extracted by using the straight algorithm. And then through the training of GMM, we complete the voice control model to input the voice to be converted and output the voice after the voice conversion. Finally, by modeling the fundamental frequency, duration, and frequency separately, a melodic control model is constructed using GAN to realize the synthesis of the Beijing Opera fragment. We connect the fragments and superimpose the background music to achieve the synthesis of Beijing Opera. The experimental results show that the synthesized Beijing Opera has some audibility and can basically complete the composition of Beijing Opera. We also extend our models to human-AI cooperative music generation: given a target voice of human, we can generate a Beijing Opera which is sung by a new target voice.},
author = {Wang, Xueting and Jin, Cong and Zhao, Wei},
doi = {10.1155/2018/5158164},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Jin, Zhao - 2018 - Beijing opera synthesis based on straight algorithm and deep learning.pdf:pdf},
issn = {16875699},
journal = {Advances in Multimedia},
title = {{Beijing opera synthesis based on straight algorithm and deep learning}},
volume = {2018},
year = {2018}
}
@article{Mauch2015,
author = {Mauch, M and Maccallum, R M and Levy, M and Leroi, A M},
doi = {10.1098/rsos.150081},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mauch et al. - 2015 - The evolution of popular music USA 1960-2010.pdf:pdf},
journal = {R. Soc. open sci},
keywords = {Subject Category: Biology (whole organism) Subject,cultural evolution,diversity,stylistic revolutions},
pages = {150081},
title = {{The evolution of popular music: USA 1960-2010}},
url = {http://dx.doi.org/10.1098/rsos.150081http://dx.doi.org/10.1098/rsos.150081orviahttp://rsos.royalsocietypublishing.org.},
volume = {2},
year = {2015}
}
@article{Authorc,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Downloads/375 Submission.pdf:pdf},
title = {{HOW OBJECTIVE MUSICAL FEATURES RELATE TO SUBJECTIVE RATINGS : A SYSTEMATIC EVALUATION OF CLASSICAL PIANO PERFORMANCE CONTINUATIONS}}
}
@inproceedings{Chen2021WavegradGeneration,
abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
address = {Vienna, Austria},
archivePrefix = {arXiv},
arxivId = {2009.00713},
author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
booktitle = {ICLR 2021 - 9th International Conference on Learning Representations},
eprint = {2009.00713},
file = {:Users/huanzhang/Downloads/2009.00713.pdf:pdf},
title = {{{WaveGrad}: Estimating Gradients for Waveform Generation}},
year = {2021}
}
@article{Liu2021LearningDomain,
abstract = {Disentangled representation learning has been proposed as an approach to learning general representations even in the absence of, or with limited, supervision. A good general representation can be fine-tuned for new target tasks using modest amounts of data, or used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for applications in computer vision and healthcare. In this tutorial paper, we motivate the need for disentangled representations, present key theory, and detail practical building blocks and criteria for learning such representations. We discuss applications in medical imaging and computer vision emphasising choices made in exemplar key works. We conclude by presenting remaining challenges and opportunities.},
archivePrefix = {arXiv},
arxivId = {2108.12043v4},
author = {Liu, Xiao and Sanchez, Pedro and Thermos, Spyridon and O'neil, Alison Q and Tsaftaris, Sotirios A},
eprint = {2108.12043v4},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2021 - Learning Disentangled Representations in the Imaging Domain.pdf:pdf},
journal = {Medical Image Analysis},
keywords = {applications,computer vision,content-style,disentangled representation,med-ical imaging,tutorial},
title = {{Learning Disentangled Representations in the Imaging Domain}},
year = {2021}
}
@techreport{PathakCuriosity-drivenPrediction,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
archivePrefix = {arXiv},
arxivId = {1705.05363v1},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
eprint = {1705.05363v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pathak et al. - Unknown - Curiosity-driven Exploration by Self-supervised Prediction.pdf:pdf},
title = {{Curiosity-driven Exploration by Self-supervised Prediction}},
url = {http://pathak22.}
}
@inproceedings{Mittal2021SymbolicModels,
abstract = {Score-based generative models and diffusion probabilis-tic models have been successful at generating high-quality samples in a variety of continuous domains. However, due to their Langevin-inspired sampling mechanisms, their application to discrete symbolic music data has been limited. In this work, we present a technique for training diffusion models on symbolic music data by parameteriz-ing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous em-beddings.},
address = {Online},
author = {Mittal, Gautam and Engel, Jesse and Hawthorne, Curtis and Simon, Ian},
booktitle = {Proceeding of the 22nd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mittal et al. - Unknown - SYMBOLIC MUSIC GENERATION WITH DIFFUSION MODELS(3).pdf:pdf},
title = {{Symbolic Music Generation with Diffusion Models}},
url = {https://goo.gl/magenta/},
year = {2021}
}
@inproceedings{Benetos2012Score-InformedTutoring,
abstract = {In this paper, a score-informed transcription method for automatic piano tutoring is proposed. The method takes as input a recording made by a student which may contain mistakes, along with a reference score. The recording and the aligned synthesized score are automatically transcribed using the non-negative matrix factorization algorithm for multi-pitch estimation and hidden Markov models for note tracking. By comparing the two transcribed recordings, common errors occurring in transcription algorithms such as extra octave notes can be suppressed. The result is a piano-roll description which shows the mistakes made by the student along with the correctly played notes. Evaluation was performed on six pieces recorded using a Disklavier piano, using both manually-aligned and automatically-aligned scores as an input. Results comparing the system output with ground-truth annotation of the original recording reach a weighted F-measure of 93%, indicating that the proposed method can successfully analyze the student's performance. {\textcopyright} 2012 EURASIP.},
author = {Benetos, Emmanouil and Klapuri, Anssi and Dixon, Simon},
booktitle = {European Signal Processing Conference},
file = {:Users/huanzhang/Downloads/benetosklapuridixon_eusipco12.pdf:pdf},
isbn = {9781467310680},
issn = {22195491},
keywords = {HMMs,Music signal analysis,NMF,score-informed transcription},
title = {{Score-informed Transcription for Automatic Piano Tutoring}},
year = {2012}
}
@article{Bengio2013RepresentationPerspectives,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538v3},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
eprint = {1206.5538v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - Unknown - Representation Learning A Review and New Perspectives.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis {\&} Machine Intelligence},
keywords = {Boltzmann Machine,Index Terms-Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
title = {{Representation Learning: A Review and New Perspectives}},
url = {http://www.image-net.org/challenges/LSVRC/2012/results.html},
year = {2013}
}
@article{Weck2023,
abstract = {Multi-modal deep learning techniques for matching free-form text with music have shown promising results in the field of Music Information Retrieval (MIR). Prior work is often based on large proprietary data while publicly available datasets are few and small in size. In this study, we present WikiMuTe, a new and open dataset containing rich semantic descriptions of music. The data is sourced from Wikipedia's rich catalogue of articles covering musical works. Using a dedicated text-mining pipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo. To show the use of this data, we train a model that jointly learns text and audio representations and performs cross-modal retrieval. The model is evaluated on two tasks: tag-based music retrieval and music auto-tagging. The results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training.},
archivePrefix = {arXiv},
arxivId = {2312.09207},
author = {Weck, Benno and Kirchhoff, Holger and Grosche, Peter and Serra, Xavier},
eprint = {2312.09207},
file = {:Users/huanzhang/Downloads/2312.09207.pdf:pdf},
keywords = {cross-modal,music information retrieval,text-mining},
pages = {1--14},
title = {{WikiMuTe: A web-sourced dataset of semantic descriptions for music audio}},
url = {http://arxiv.org/abs/2312.09207},
year = {2023}
}
@article{Traube2012PianoData,
abstract = {We hereby present the analytic tools developed as a MATLAB toolbox for exploring the most subtle features of piano touch and gesture. From high-sample-rate, high- precision precision key, hammer and pedal tracking data about a performance gathered thanks to the B{\"{o}}sendorfer grand piano-embedded CEUS digital recording system, the toolbox main functions can extract exhaustive features detailing the pianist's touch as a thorough account of nu- ances in articulation, timing, dynamics, attack and ped- alling. Each performance and gestural control thereof is thus described over each of its notes and chords. By com- paring several performances, it is possible to characterize the gestural control of expression in piano performance, as the correlations among piano touch features towards one examined expressive parameter. The analytic functions in the toolbox — with piano touch feature visualization and comparison, chords and notes selection tools, score- performance matching and advanced, automated statisti- cal analyses and visualization thereof — allow for rig- orous, quantitative exploration of expressive performance and its gestural control, which here is especially applied towards investigating the use of timbre as expressive de- vice in piano performance. With these tools, we thus in- tend to build a gestural mapping of piano timbre. 1.},
author = {Traube, Caroline and Bernays, Michel},
file = {:Users/huanzhang/Downloads/jim2012_10_p_bernays.pdf:pdf},
journal = {Journ{\'{e}}es d'Informatique Musicale (JIM)},
number = {JIM2012},
pages = {55--64},
title = {{Piano Touch Analysis: a Matlab Toolbox for Extracting Performance Descriptors from High Resolution Keyboard and Pedalling Data}},
url = {http://s3.amazonaws.com/academia.edu.documents/31049074/jim2012_10_p_bernays.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1490991762&Signature=fAbVC3trfxW2sa5c0Q5P%2BIkaPSg%3D&response-content-disposition=inline%3B filename%3DPIANO_TOUCH_ANALYSIS_A_MAT},
year = {2012}
}
@inproceedings{Venkatesh2022,
author = {Venkatesh, Anil},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/133.pdf:pdf},
title = {{DETECTING SYMMETRIES OF ALL CARDINALITIES WITH APPLICATION TO MUSICAL 12-TONE ROWS}},
year = {2022}
}
@article{Authord,
author = {Author, First and Author, Second and Author, Third},
file = {:Users/huanzhang/Downloads/105 Submission.pdf:pdf},
title = {{TRAINING BETTER EMBEDDING WITH PERTURBED DATA AUGMENTATION FOR AUTOMATIC SINGING QUALITY ASSESSMENT}}
}
@article{Cserna2018a,
abstract = {Many systems, such as mobile robots, need to be controlled in real time. Real-time heuristic search is a popular on-line planning paradigm that supports concurrent planning and execution. However, existing methods do not incorporate a notion of safety and we show that they can perform poorly in domains that contain dead-end states from which a goal cannot be reached. We introduce new real-time heuristic search methods that can guarantee safety if the domain obeys certain properties. We test these new methods on two different simulated domains that contain dead ends, one that obeys the properties and one that does not. We find that empirically the new methods provide good performance. We hope this work encourages further efforts to widen the applicability of real-time planning.},
author = {Cserna, Bence and Doyle, William J. and Ramsdell, Jordan S. and Ruml, Wheeler},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Cserna et al. - 2018 - Avoiding dead ends in real-time heuristic search.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
keywords = {Heuristic Search and Optimization Track},
pages = {1306--1313},
title = {{Avoiding dead ends in real-time heuristic search}},
year = {2018}
}
@article{Plantinga2005,
abstract = {Pitch perception is fundamental to melody in music and prosody in speech. Unlike many animals, the vast majority of human adults store melodic information primarily in terms of relative not absolute pitch, and readily recognize a melody whether rendered in a high or a low pitch range. We show that at 6 months infants are also primarily relative pitch processors. Infants familiarized with a melody for 7 days preferred, on the eighth day, to listen to a novel melody in comparison to the familiarized one, regardless of whether the melodies at test were presented at the same pitch as during familiarization or transposed up or down by a perfect fifth (7/12th of an octave) or a tritone (1/2 octave). On the other hand, infants showed no preference for a transposed over original-pitch version of the familiarized melody, indicating that either they did not remember the absolute pitch, or it was not as salient to them as the relative pitch.},
annote = {From Duplicate 1 (Memory for melody: Infants use a relative pitch code - Plantinga, Judy; Trainor, Laurel J.)

Week 2},
author = {Plantinga, Judy and Trainor, Laurel J.},
doi = {10.1016/j.cognition.2004.09.008},
file = {:Users/huanzhang/Downloads/1-s2.0-S0010027704001878-main.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Auditory perception,Development,Infants,Pitch},
number = {1},
pages = {1--11},
pmid = {16297673},
title = {{Memory for melody: Infants use a relative pitch code}},
volume = {98},
year = {2005}
}
@article{Pearce,
author = {Pearce, Marcus},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pearce - Unknown - Music Perception & Cognition.pdf:pdf},
title = {{Music Perception & Cognition}}
}
@inproceedings{Rutte2022FigaroControl,
abstract = {Generating music with deep neural networks has been an area of active research in recent years. While the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. We propose the self-supervised description-to-sequence task, which allows for fine-grained controllable generation on a global level. We do so by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding high-level description in a sequence-to-sequence modelling setup. We train FIGARO (FIne-grained music Generation via Attention-based, RObust control) by applying description-to-sequence modelling to symbolic music. By combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation and generalizes well beyond the training distribution.},
archivePrefix = {arXiv},
arxivId = {2201.10936},
author = {von R{\"{u}}tte, Dimitri and Biggio, Luca and Kilcher, Yannic and Hofmann, Thomas},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {2201.10936},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Von R ¨ Utte et al. - 2022 - FIGARO Generating Symbolic Music with Fine-Grained Artistic Control(2).pdf:pdf},
title = {{FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control}},
url = {http://arxiv.org/abs/2201.10936},
year = {2023}
}
@inproceedings{deValk2018DeepRepresentations,
abstract = {In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach's The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art.},
author = {de Valk, Reinier and Weyde, Tillman},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/De Valk, Weyde - Unknown - DEEP NEURAL NETWORKS WITH VOICE ENTRY ESTIMATION HEURISTICS FOR VOICE SEPARATION IN SYMBOLIC MUSIC REPRESENTA.pdf:pdf},
isbn = {9782954035123},
pages = {281--288},
title = {{Deep neural networks with voice entry estimation heuristics for voice separation in symbolic music representations}},
url = {https://www.github.com/reinierdevalk/voice_},
year = {2018}
}
@article{Xiangjian,
author = {Xiangjian, Zeng and Le, Ren and Zebin, Huang and Ruoxuan, Liang and Qingqiang, Wu},
file = {:Users/huanzhang/Downloads/7955 Submission.pdf:pdf},
title = {{A three-stage Automatic Intonation Evaluation Method for Perceived Singing Quality}}
}
@article{Park2018RepresentationLabels,
abstract = {In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models.},
archivePrefix = {arXiv},
arxivId = {1710.06648v2},
author = {Park, Jangyeon Jiyoung and Lee, Jongpil and Park, Jangyeon Jiyoung and Ha, Jung-Woo and Nam, Juhan},
eprint = {1710.06648v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - Unknown - REPRESENTATION LEARNING OF MUSIC USING ARTIST LABELS.pdf:pdf},
isbn = {1710.06648v2},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
title = {{Representation Learning of Music Using Artist Labels}},
url = {https://github.com/jiyoungpark527/},
year = {2018}
}
@article{Pons2018End-to-endScale,
abstract = {The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios.},
archivePrefix = {arXiv},
arxivId = {1711.02520},
author = {Pons, Jordi and Nieto, Oriol and Prockup, Matthew and Schmidt, Erik and Ehmann, Andreas and Serra, Xavier},
doi = {10.5281/zenodo.1492497},
eprint = {1711.02520},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pons et al. - 2018 - End-to-end learning for music audio tagging at scale.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {637--644},
title = {{End-to-end learning for music audio tagging at scale}},
year = {2018}
}
@article{Wang2021Audio-basedMechanism,
abstract = {In this paper, we propose two different audio-based piano performance evaluation systems for beginners. The first is a sequential and modularized system, including three steps: Convolutional Neural Network (CNN)-based acoustic feature extraction, matching via dynamic time warping (DTW), and performance score regression. The second system is an end-to-end system with CNNs and the attention mechanism. It takes two acoustic feature sequences as input and directly predicts a performance score. We evaluate two proposed methods with our new open-access Yingcai Piano Performance Evaluation Phase III Dataset (YCU-PPE-III) that contains more than 2000 piano audio pieces recorded in multiple real test sessions. Experimental results show that the modularized system achieves a mean absolute error (MAE) of 3.79 in a 0-100-point range. Another end-to-end system also achieves an MAE of 4.40, which shows that it is possible to train a robust end-to-end piano performance evaluation system with only two thousand audio pieces.},
author = {Wang, Weiqing and Pan, Jin and Yi, Hua and Song, Zhanmei and Li, Ming},
doi = {10.1109/TASLP.2021.3061267},
file = {:Users/huanzhang/Downloads/Audio-Based_Piano_Performance_Evaluation_for_Beginners_With_Convolutional_Neural_Network_and_Attention_Mechanism.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Attention,computer assisted piano learning,convolutional neural network,dynamic time warping,piano performance evaluation},
pages = {1119--1133},
title = {{Audio-Based Piano Performance Evaluation for Beginners with Convolutional Neural Network and Attention Mechanism}},
url = {https://ieeexplore.ieee.org/document/9361233},
volume = {29},
year = {2021}
}
@article{Conklin1995MultiplePrediction,
abstract = {This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short-term and long-term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are used for analysis and prediction, and we conjecture that highly predictive theories will also generate original, acceptable, works. Although the quality of the works generated is hard to quantify objectively, the predictive power of models can be measured by the notion of entropy, or unpredictability. Highly predictive theories will produce low-entropy estimates of a musical language. The methods developed are applied to the Bach chorale melodies. Multiple-viewpoint systems are learned from a sample of 95 chorales, estimates of entropy are produced, and a predictive theory is used to generate new, unseen pieces. {\textcopyright} 1995, Taylor & Francis Group, LLC. All rights reserved.},
author = {Conklin, Darrell and Witten, Ian H.},
doi = {10.1080/09298219508570672},
file = {:Users/huanzhang/Downloads/jnmr95.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
number = {1},
pages = {51--73},
title = {{Multiple Viewpoint Systems for Music Prediction}},
volume = {24},
year = {1995}
}
@inproceedings{Gover2022MusicTranslation,
author = {Gover, Matan and Zwei, Oded},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/37.pdf:pdf},
title = {{Music Translation: Generating Piano Arrangements in Different Playing Levels}},
year = {2022}
}
@inproceedings{Chen2020ContinuousConditions,
abstract = {Automatic music generation is an interdisciplinary research topic that combines computational creativity and semantic analysis of music to create automatic machine improvisations. An important property of such a system is allowing the user to specify conditions and desired properties of the generated music. In this paper we designed a model for composing melodies given a user specified symbolic scenario combined with a previous music context. We add manual labeled vectors denoting external music quality in terms of chord function that provides a low dimensional representation of the harmonic tension and resolution. Our model is capable of generating long melodies by regarding 8-beat note sequences as basic units, and shares consistent rhythm pattern structure with another specific song. The model contains two stages and requires separate training where the first stage adopts a Conditional Variational Autoencoder (C-VAE) to build a bijection between note sequences and their latent representations, and the second stage adopts long short-term memory networks (LSTM) with structural conditions to continue writing future melodies. We further exploit the disentanglement technique via C-VAE to allow melody generation based on pitch contour information separately from conditioning on rhythm patterns. Finally, we evaluate the proposed model using quantitative analysis of rhythm and the subjective listening study. Results show that the music generated by our model tends to have salient repetition structures, rich motives, and stable rhythm patterns. The ability to generate longer and more structural phrases from disentangled representations combined with semantic scenario specification conditions shows a broad application of our model.},
archivePrefix = {arXiv},
arxivId = {2002.02393v1},
author = {Chen, Ke and Xia, Gus and Dubnov, Shlomo},
booktitle = {IEEE 14th International Conference on Semantic Computing (ICSC)},
eprint = {2002.02393v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Xia, Dubnov - Unknown - Continuous Melody Generation via Disentangled Short-Term Representations and Structural Conditions.pdf:pdf},
title = {{Continuous Melody Generation via Disentangled Short-Term Representations and Structural Conditions}},
url = {https://github.com/RetroCirce/Auto-mask-Music-},
year = {2020}
}
@article{Chou2021MidiBERT-PianoUnderstanding,
abstract = {This paper presents an attempt to employ the mask language modeling approach of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of polyphonic piano MIDI files for tackling a number of symbolic-domain discriminative music understanding tasks. These include two note-level classification tasks, i.e., melody extraction and velocity prediction, as well as two sequence-level classification tasks, i.e., composer classification and emotion classification. We find that, given a pre-trained Transformer, our models outperform recurrent neural network based baselines with less than 10 epochs of fine-tuning. Ablation studies show that the pre-training remains effective even if none of the MIDI data of the downstream tasks are seen at the pre-training stage, and that freezing the self-attention layers of the Transformer at the fine-tuning stage slightly degrades performance. All the five datasets employed in this work are publicly available, as well as checkpoints of our pre-trained and fine-tuned models. As such, our research can be taken as a benchmark for symbolic-domain music understanding.},
archivePrefix = {arXiv},
arxivId = {2107.05223},
author = {Chou, Yi-Hui and Chen, I-Chun and Chang, Chin-Jui and Ching, Joann and Yang, Yi-Hsuan},
eprint = {2107.05223},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Chou et al. - Unknown - MidiBERT-Piano Large-scale Pre-training for Symbolic Music Understanding.pdf:pdf},
keywords = {Index Terms-Large-scale pre-trained model,Transformer,composer classification,emotion classification,melody recognition,symbolic-domain music understanding,ve-locity prediction},
title = {{MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding}},
url = {http://arxiv.org/abs/2107.05223},
year = {2021}
}
@techreport{Hamanaka2014,
abstract = {This paper, we present the publication of our analysis data and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical pieces with standard MIDI files and annotated data are key to advancements in the field of music information technology. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical structure analysis, we are now publicizing 300 pieces of analysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations.},
author = {Hamanaka, Masatoshi and Hirata, Keiji and Tojo, Satoshi},
booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hamanaka, Hirata, Tojo - Unknown - MUSICAL STRUCTURAL ANALYSIS DATABASE BASED ON GTTM.pdf:pdf},
pages = {325--330},
title = {{Musical structural analysis database based on GTTM}},
year = {2014}
}
@article{Butler1988,
author = {Butler, Judith and Butler, Judith},
file = {:Users/huanzhang/Downloads/17a.Butler,performative[1].pdf:pdf},
number = {4},
pages = {519--531},
title = {{Performative Acts and Gender Constitution: An Essay in Phenomenology and Feminist Theory}},
volume = {40},
year = {1988}
}
@article{Allegraud2019,
author = {Allegraud, Pierre and Bigo, Louis and Feisthauer, Laurent and Giraud, Mathieu and Groult, Richard and Leguy, Emmanuel and Lev{\'{e}}, Florence},
doi = {10.5334/tismir.27},
file = {:Users/huanzhang/Downloads/27-816-1-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {computational music analysis,music structure,musical form,sonata form},
number = {1},
pages = {82--96},
title = {{Learning Sonata Form Structure on Mozart's String Quartets}},
volume = {2},
year = {2019}
}
@article{Doh2024,
author = {Doh, Seungheon and Lee, Minhee and Jeong, Dasaem and Nam, Juhan},
doi = {10.1109/ICASSP48485.2024.10446380},
file = {:Users/huanzhang/Downloads/Enriching_Music_Descriptions_with_A_Finetuned-LLM_and_Metadata_for_Text-to-Music_Retrieval.pdf:pdf},
isbn = {9798350344851},
journal = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {826--830},
publisher = {IEEE},
title = {{ENRICHING MUSIC DESCRIPTIONS WITH A FINETUNED-LLM AND METADATA FOR TEXT-TO-MUSIC RETRIEVAL}},
year = {2024}
}
@inproceedings{Borovik2023ScorePerformerControl,
address = {Milan, Italy},
author = {Borovik, Ilya and Viro, Vladimir},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/000069.pdf:pdf},
title = {{{ScorePerformer} : Expressive Piano Performance Rendering with Fine-grained Control}},
year = {2023}
}
@article{Liebman2012,
abstract = {This paper presents a novel algorithmic approach to music performance analysis. Previous attempts to use algorithmic tools in this field focused typically on tempo and dynamics alone. We base our analysis on ten different performance categories (such as bowing, vibrato and durations). We adapt phylogenetic analysis tools to resolve the inherent inconsistencies between these categories , and describe the relationships between performances. Taking samples from 29 different performances of two pieces from Bach's sonatas for solo violin, we construct a 'phylogenetic' tree, representing the relationship between those performances. The tree supports several interesting relations previously conjectured by the musicology community, such as the importance of date of birth and recording period in determining interpreta-tive style. Our work also highlights some unexpected inter-connections between performers, and challenges previous assumptions regarding the significance of educational background and affiliation to the historically informed performance (HIP) style.},
author = {Liebman, Elad and Ornoy, Eitan and Chor, Benny},
doi = {10.1080/09298215.2012.668194},
issn = {1744-5027},
journal = {Journal of New Music Research},
number = {2},
pages = {195--222},
title = {{A Phylogenetic Approach to Music Performance Analysis}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
volume = {41},
year = {2012}
}
@article{Lia,
author = {Li, Ruiqi and Hong, Zhiqing and Wang, Yongqi and Zhang, Lichao and Zheng, Siqi and Zhao, Zhou},
file = {:Users/huanzhang/Downloads/6099 Submission.pdf:pdf},
pages = {3--7},
title = {{Text-driven Cascaded Song Generation}}
}
@article{Juslin2008,
abstract = {The Experience Sampling Method was used to explore emotions to music as they naturally occurred in everyday life, with a focus on the prevalence of different musical emotions and how such emotions are related to various factors in the listener, the music, and the situation. Thirty-two college students, 20 to 31 years old, carried a palmtop that emitted a sound signal seven times per day at random intervals for 2 weeks. When signaled, participants were required to complete a questionnaire on the palmtop. Results showed that music occurred in 37% of the episodes, and in 64% of the music episodes, the participants reported that the music affected how they felt. Comparisons showed that happiness-elation and nostalgia-longing were more frequent in episodes with musical emotions, whereas anger-irritation, boredom-indifference, and anxiety-fear were more frequent in episodes with nonmusical emotions. The prevalence of specific musical emotions correlated with personality measures and also varied depending on the situation (e.g., current activity, other people present), thus highlighting the need to use representative samples of situations to obtain valid estimates of prevalence. {\textcopyright} 2008 American Psychological Association.},
author = {Juslin, Patrik N. and Liljestr{\"{o}}m, Simon and V{\"{a}}stfj{\"{a}}ll, Daniel and Barradas, Gon{\c{c}}alo and Silva, Ana},
doi = {10.1037/a0013505},
file = {:Users/huanzhang/Downloads/2008-13989-009.pdf:pdf},
issn = {15283542},
journal = {Emotion},
keywords = {emotion,everyday life,experience sampling method,music listening,personality},
number = {5},
pages = {668--683},
pmid = {18837617},
title = {{An Experience Sampling Study of Emotional Reactions to Music: Listener, Music, and Situation}},
volume = {8},
year = {2008}
}
@inproceedings{Cancino-Chacon2023ACCompanionAccompanist,
abstract = {This paper introduces the ACCompanion, an expressive accompaniment system. Similarly to a musician who accompanies a soloist playing a given musical piece, our system can produce a human-like rendition of the accompaniment part that follows the soloist's choices in terms of tempo, dynamics, and articulation. The ACCompanion works in the symbolic domain, i.e., it needs a musical instrument capable of producing and playing MIDI data, with explicitly encoded onset, offset, and pitch for each played note. We describe the components that go into such a system, from real-time score following and prediction to expressive performance generation and online adaptation to the expressive choices of the human player. Based on our experience with repeated live demonstrations in front of various audiences, we offer an analysis of the challenges of combining these components into a system that is highly reactive and precise, while still a reliable musical partner, robust to possible performance errors and responsive to expressive variations.},
address = {Macau, China},
archivePrefix = {arXiv},
arxivId = {2304.12939},
author = {Cancino-Chac{\'{o}}n, Carlos and Peter, Silvan and Hu, Patricia and Karystinaios, Emmanouil and Henkel, Florian and Foscarin, Francesco and Widmer, Gerhard},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2023/641},
eprint = {2304.12939},
file = {:Users/huanzhang/Downloads/2304.12939.pdf:pdf},
isbn = {9781956792034},
issn = {10450823},
title = {{The {ACCompanion}: Combining Reactivity, Robustness, and Musical Expressivity in an Automatic Piano Accompanist}},
year = {2023}
}
@article{Chen2018FusingIdentification,
abstract = {Cover Song Identification (CSI) technique, refers to the process of identifying an alternative version, performance, rendition, or recording of a previously recorded musical composition by measuring and modeling the musical similarity between them quantitatively and objectively. However, it is not possible to describe the similarity between tracks comprehensively and reliably with only one similarity function. In this paper, the Similarity Network Fusion (SNF) technique, which was originally proposed for combining different kernels for predicting drug-target interactions, is adopted to fuse different similarities based on the same descriptor and different similarity functions. First, the Harmonic Pitch Class Profile (HPCP) is extracted from each track. Next, the similarities, in terms of Qmax and Dmax measures, between the HPCP descriptors of any two tracks are calculated, respectively. Then, the track-by-track similarity networks based on Qmax and on Dmax similarity are constructed separately and then fused into one network by SNF. Finally, the fused similarities obtained from the fused similarity network are adopted to train a classifier, which can then be used to identify whether the input two tracks belong to reference/cover or reference/non-cover pair. Experimental results on Covers80 (http://labrosa.ee.columbia.edu/projects/coversongs/covers80/), subset of SecondHandSongs (SHS) (http://labrosa.ee.columbia.edu/millionsong/secondhand), and the Mixed Collection and Mazurka Cover Collection provided by MIREX (http://www.music-ir.org/mirex/wiki/2016:Audio_Cover_Song_Identification) demonstrate that the proposed scheme performs comparably with or even better than state-of-the-art CSI schemes.},
author = {Chen, Ning and Li, Wei and Xiao, Haidong},
doi = {10.1007/s11042-017-4456-9},
file = {:Users/huanzhang/Downloads/Chen2018_Article_FusingSimilarityFunctionsForCo.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Cover song identification (CSI),Dmax,Qmax,Similarity network fusion (SNF)},
number = {2},
pages = {2629--2652},
publisher = {Multimedia Tools and Applications},
title = {{Fusing similarity functions for cover song identification}},
volume = {77},
year = {2018}
}
@article{Pons2017Score-informedNetworks,
abstract = {This paper introduces a new score-informed method for the segmentation of jingju a cappella singing phrase into syllables. The proposed method estimates the most likely sequence of syllable boundaries given the estimated syllable onset detection function (ODF) and its score. Throughout the paper, we first examine the jingju syllables structure and propose a definition of the term "syllable onset". Then, we identify which are the challenges that jingju a cappella singing poses. Further, we investigate how to improve the syllable ODF estimation with convolutional neural networks (CNNs). We propose a novel CNN architecture that allows to efficiently capture different timefrequency scales for estimating syllable onsets. Besides, we propose using a score-informed Viterbi algorithm - instead of thresholding the onset function-, because the available musical knowledge we have (the score) can be used to inform the Viterbi algorithm to overcome the identified challenges. The proposed method outperforms the state-of-the-art in syllable segmentation for jingju a cappella singing. We further provide an analysis of the segmentation errors which points possible research directions.},
archivePrefix = {arXiv},
arxivId = {1707.03544},
author = {Pons, Jordi and Gong, Rong and Serra, Xavier},
eprint = {1707.03544},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Pons, Gong, Serra - 2017 - Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks.pdf:pdf},
isbn = {9789811151798},
journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
pages = {383--389},
title = {{Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks}},
year = {2017}
}
@article{Gupta2020a,
abstract = {Automatic evaluation of singing quality can be done with the help of a reference singing or the digital sheet music of the song. However, such a standard reference is not always available. In this article, we propose a framework to rank a large pool of singers according to their singing quality without any standard reference. We define musically motivated absolute measures based on pitch histogram, and relative measures based on inter-singer statistics to evaluate the quality of singing attributes such as intonation, and rhythm. The absolute measures evaluate the goodness of pitch histogram specific to a singer, while the relative measures use the similarity between singers in terms of pitch, rhythm, and timbre as an indicator of singing quality. With the relative measures, we formulate the concept of veracity or truth-finding for the ranking of singing quality. We successfully validate a self-organizing approach to rank-ordering a large pool of singers. The fusion of absolute and relative measures results in an average Spearman's rank correlation of 0.71 with human judgments in a 10-fold cross-validation experiment, which is close to the inter-judge correlation.},
annote = {From Duplicate 3 (Automatic Evaluation of singing quality without a standard reference - Gupta, Chitralekha; Li, Haizhou; Wang, Ye)

GMM:
multiple peaks
- PeakBW: spread around the peaks indicates the consistency of hitting the same notes
- PeakConc: 

kMeans
- find the average distance of each pitch value to it's corresponding cluster centroid},
author = {Gupta, Chitralekha and Li, Haizhou and Wang, Ye},
doi = {10.1109/TASLP.2019.2947737},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Li, Wang - 2020 - Automatic leaderboard Evaluation of singing quality without a standard reference.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Li, Wang - 2020 - Automatic leaderboard Evaluation of singing quality without a standard reference(2).pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Evaluation by ranking,evaluation of singing quality,inter-singer measures,music-theory motivated measures},
pages = {13--26},
publisher = {IEEE},
title = {{Automatic leaderboard: Evaluation of singing quality without a standard reference}},
volume = {28},
year = {2020}
}
@article{Kong2024,
archivePrefix = {arXiv},
arxivId = {arXiv:2402.01831v1},
author = {Kong, Zhifeng and Goel, Arushi and Badlani, Rohan and Ping, Wei and Valle, Rafael and Catanzaro, Bryan},
eprint = {arXiv:2402.01831v1},
file = {:Users/huanzhang/Downloads/2402.01831.pdf:pdf},
title = {{Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities}},
year = {2024}
}
@inproceedings{Kato2023,
abstract = {We developed a computational evaluation method for piano performance with the goal of building a practice support system for beginners. We recorded students' performances as audio data and applied several recent methods for audio-to-MIDI transcription based on deep neural networks to extract the pitch, onset time, and offset time of musical notes. To determine the correctness of the performance, we aligned the extracted MIDI data with the musical score using a hidden Markov model (HMM). We compared the audio-to-MIDI transcription methods and optimized the weight on different types of performance errors to conform to teacher's assessment. Our experiments showed a strong correlation between the rate of performance errors obtained from the alignment and the evaluation by a teacher who listened to the performance. The results that indicate performance errors and tempo stability can be used in a practice support system that provides feedback to learners.},
author = {Kato, Norihiro and Nakamura, Eita and Mine, Kyoko and Doeda, Orie and Yamada, Masanao},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-031-42682-7_46},
file = {:Users/huanzhang/Downloads/Kato_PianoPerformanceAutomaticEvaluation_ ECTEL2023 (1).pdf:pdf},
isbn = {9783031426810},
issn = {16113349},
keywords = {Audio-to-MIDI transcription,Computational performance evaluation,Hidden Markov model},
title = {{Computational Analysis of Audio Recordings of Piano Performance for Automatic Evaluation}},
volume = {14200 LNCS},
year = {2023}
}
@article{Friberg1999MusicRunners,
abstract = {This investigation explores the common assumption that music and motion are closely related by comparing the stopping of running and the termination of a piece of music. Video recordings were made of professional dancers' stopping from running under different deceleration conditions, and instant values of body velocity, step frequency, and step length were estimated. In decelerations that were highly rated for aesthetic quality by a panel of choreographers, the mean body velocity could be approximated by a square- root function of time, which is equivalent to a cubic-root function of position. This implies a linear relationship between kinetic energy and time, i.e., a constant braking power. The mean body velocity showed a striking similarity with the mean tempo pattern of final ritardandi in music performances. The constant braking power was used as the basis for a model describing both the changes of tempo in final ritardandi and the changes of velocity in runners' decelerations. The translation of physical motion to musical tempo was realized by assuming that velocity and musical tempo are equivalent. Two parameters were added to the model to account for the variation observed in individual ritardandi and in individual decelerations: (1) the parameter q controlling the curvature, q = 3 corresponding to the runners' deceleration, and (2) the parameter v(end) for the final velocity and tempo value, respectively. A listening experiment was carried out presenting music examples with final ritardandi according to the model with different q values or to an alternative function. Highest ratings were obtained for the model with q = 2 and q = 3. Out of three functions, the model produced the best fit to individual measured ritardandi as well as to individual decelerations. A function previously used for modeling phrase- related tempo variations (interonset duration as a quadratic function of score position) produced the lowest ratings and the poorest fits to individual ritardandi. The results thus seem to substantiate the commonly assumed analogies between motion and music.},
author = {Friberg, Anders and Sundberg, Johan},
doi = {10.1121/1.426687},
file = {:Users/huanzhang/Downloads/590.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1469--1484},
title = {{Does music performance allude to locomotion? A model of final ritardandi derived from measurements of stopping runners}},
volume = {105},
year = {1999}
}
@article{Dannenberg2003,
abstract = {Getting computers to understand and process audio recordings in terms of their musical content is a difficult challenge. We describe a method in which general, polyphonic audio recordings of music can be aligned to symbolic score information in standard MIDI files. Because of the difficulties of polyphonic transcription, we perform matching directly on acoustic features that we extract from MIDI and audio. Polyphonic audio matching can be used for polyphonic score following, building intelligent editors that understand the content of recorded audio, and the analysis of expressive performance.},
annote = {From Duplicate 1 (Polyphonic Audio Matching for Score Following and Intelligent Audio Editors - Dannenberg, R B; Hu, N)

Get accoustic features from MIDI
DTW},
author = {Dannenberg, R B and Hu, N},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dannenberg, Hu - 2003 - Polyphonic Audio Matching for Score Following and Intelligent Audio Editors.pdf:pdf},
journal = {Proc 2003 ICMC ICMA},
pages = {27--33},
title = {{Polyphonic Audio Matching for Score Following and Intelligent Audio Editors}},
url = {https://www.cs.cmu.edu/$\sim$rbd/papers/icmc03matchingaudio.pdf},
year = {2003}
}
@phdthesis{Gomez2006TonalSignals,
author = {Gomez, Emilia},
file = {:Users/huanzhang/Downloads/tegg.pdf:pdf},
isbn = {9788469078228},
school = {University of Pompeu Fabra},
title = {{Tonal Description of Music Audio Signals}},
year = {2006}
}
@inproceedings{Zhang2024Instruct-Musicgen,
abstract = {Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.},
archivePrefix = {arXiv},
arxivId = {2405.18386},
author = {Zhang, Yixiao and Ikemiya, Yukara and Choi, Woosung and Murata, Naoki and Mart{\'{i}}nez-Ram{\'{i}}rez, Marco A. and Lin, Liwei and Xia, Gus and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon},
booktitle = {Proceeding of the International Joint Conference on Artificial Intelligence (IJCAI)},
eprint = {2405.18386},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2024 - Instruct-MusicGen Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning.pdf:pdf},
title = {{Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning}},
url = {http://arxiv.org/abs/2405.18386},
year = {2024}
}
@article{Qian2021,
abstract = {Prosody plays an important role in characterizing the style of a speaker or an emotion, but most non-parallel voice or emotion style transfer algorithms do not convert any prosody information. Two major components of prosody are pitch and rhythm. Disentangling the prosody information , particularly the rhythm component, from the speech is challenging because it involves breaking the synchrony between the input speech and the disentangled speech representation. As a result , most existing prosody style transfer algorithms would need to rely on some form of text transcriptions to identify the content information, which confines their application to high-resource languages only. Recently, SPEECHSPLIT (Qian et al., 2020b) has made sizeable progress towards unsupervised prosody style transfer, but it is unable to extract high-level global prosody style in an unsupervised manner. In this paper, we propose AUTOPST, which can disentangle global prosody style from speech without relying on any text transcriptions. AUTOPST is an Autoencoder-based Prosody Style Transfer framework with a thorough rhythm removal module guided by self-expressive representation learning. Experiments on different style transfer tasks show that AU-TOPST can effectively convert prosody that correctly reflects the styles of the target domains.},
author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Xiong, Jinjun and Gan, Chuang and Cox, David and Hasegawa-Johnson, Mark},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Qian et al. - 2021 - Global Rhythm Style Transfer Without Text Transcriptions.pdf:pdf},
title = {{Global Rhythm Style Transfer Without Text Transcriptions}},
year = {2021}
}
@article{Jin2023OrderPerformance,
abstract = {Computational aesthetics evaluation has made great achievements in the field of visual arts, but the research work on music still needs to be explored. Although the existing work of music generation is very substantial, the quality of music score generated by AI is relatively poor compared with that created by human composers. The music scores created by AI are usually monotonous and devoid of emotion. Based on Birkhoff's aesthetic measure, this paper proposes an objective quantitative evaluation method for homophony music score aesthetic quality assessment. The main contributions of our work are as follows: first, we put forward a homophony music score aesthetic model to objectively evaluate the quality of music score as a baseline model; second, we put forward eight basic music features and four music aesthetic features.},
archivePrefix = {arXiv},
arxivId = {2301.05908},
author = {Jin, Xin and Zhou, Wu and Wang, Jinyu and Xu, Duo and Rong, Yiqing and Cui, Shuai},
eprint = {2301.05908},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - Unknown - AN ORDER-COMPLEXITY MODEL FOR AESTHETIC QUALITY ASSESSMENT OF HOMOPHONY MUSIC PERFORMANCE.pdf:pdf},
keywords = {Birkhoff's measure,Index Terms-Computational aesthetics,Music features,Music perfor-mance evaluation},
title = {{An Order-Complexity Model for Aesthetic Quality Assessment of Homophony Music Performance}},
url = {http://arxiv.org/abs/2301.05908},
year = {2023}
}
@article{KulkarniDeepNetwork,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.},
author = {Kulkarni, Tejas D and Whitney, William F and Kohli, Pushmeet and Tenenbaum, Joshua B},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kulkarni et al. - Unknown - Deep Convolutional Inverse Graphics Network.pdf:pdf},
title = {{Deep Convolutional Inverse Graphics Network}}
}
@article{Hu2020,
author = {Hu, Zhejing and Liu, Yan and Chen, Gong and Ma, Xiao and Zhong, Shenghua and Luo, Qianwen},
file = {:Users/huanzhang/Downloads/27807-Article Text-31861-1-2-20240324.pdf:pdf},
keywords = {Cognitive Modeling & Cognitive Systems (CMS): CMS:},
pages = {521--529},
title = {{Responding to the Call : Exploring Automatic Music Composition Using a Knowledge-Enhanced Model Call-Response Dataset}},
year = {2020}
}
@article{Cancino-Chacon2018ComputationalReview,
abstract = {Expressive performance is an indispensable part of music making. When playing a piece, expert performers shape various parameters (tempo, timing, dynamics, intonation, articulation, etc.) in ways that are not prescribed by the notated score, in this way producing an expressive rendition that brings out dramatic, affective, and emotional qualities that may engage and affect the listeners. Given the central importance of this skill for many kinds of music, expressive performance has become an important research topic for disciplines like musicology, music psychology, etc. This paper focuses on a specific thread of research: work on computational music performance models. Computational models are attempts at codifying hypotheses about expressive performance in terms of mathematical formulas or computer programs, so that they can be evaluated in systematic and quantitative ways. Such models can serve at least two main purposes: they permit us to systematically study certain hypotheses regarding performance; and they can be used as tools to generate automated or semi-automated performances, in artistic or educational contexts. The present article presents an up-to-date overview of the state of the art in this domain. We explore recent trends in the field, such as a strong focus on data-driven (machine learning); a growing interest in interactive expressive systems, such as conductor simulators and automatic accompaniment systems; and an increased interest in exploring cognitively plausible features and models. We provide an in-depth discussion of several important design choices in such computer models, and discuss a crucial (and still largely unsolved) problem that is hindering systematic progress: the question of how to evaluate such models in scientifically and musically meaningful ways. From all this, we finally derive some research directions that should be pursued with priority, in order to advance the field and our understanding of expressive music performance.},
annote = {P: Performance S: Score},
author = {Cancino-Chac{\'{o}}n, Carlos E. and Grachten, Maarten and Goebl, Werner and Widmer, Gerhard},
doi = {10.3389/fdigh.2018.00025},
file = {:Users/huanzhang/Downloads/fdigh-05-00025.pdf:pdf},
issn = {2297-2668},
journal = {Frontiers in Digital Humanities},
keywords = {computational modeling,generative,machine learning,music expression,music performance},
number = {October},
pages = {1--23},
title = {{Computational Models of Expressive Music Performance: A Comprehensive and Critical Review}},
volume = {5},
year = {2018}
}
@inproceedings{Mescheder2018WhichConverge,
abstract = {GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss reg- ularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero- centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high- resolution generative image models for a variety of datasets with little hyperparameter tuning.},
archivePrefix = {arXiv},
arxivId = {1801.04406},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
eprint = {1801.04406},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mescheder, Geiger, Nowozin - 2018 - Which Training Methods for GANs do actually Converge.pdf:pdf},
isbn = {9781510867963},
title = {{Which training methods for {GAN}s do actually converge?}},
year = {2018}
}
@inproceedings{Huang2020Score-informedAssessment,
abstract = {The assessment of music performances in most cases takes into account the underlying musical score being performed. While there have been several automatic approaches for objective music performance assessment (MPA) based on extracted features from both the performance audio and the score, deep neural network-based methods incorporating score information into MPA models have not yet been investigated. In this paper, we introduce three different models capable of score-informed performance assessment. These are (i) a convolutional neural network that utilizes a simple time-series input comprising of aligned pitch contours and score, (ii) a joint embedding model which learns a joint latent space for pitch contours and scores, and (iii) a distance matrix-based convolutional neural network which utilizes patterns in the distance matrix between pitch contours and musical score to predict assessment ratings. Our results provide insights into the suitability of different architectures and input representations and demonstrate the benefits of score-informed models as compared to score-independent models.},
annote = {From Duplicate 2 (Score-informed Networks for Music Performance Assessment - Huang, Jiawen; Hung, Yun-Ning; Pati, Ashis; Gururani, Siddharth Kumar; Lerch, Alexander)

Score: MIDI pitch Sequence
Perf: Pitch Contour},
archivePrefix = {arXiv},
arxivId = {2008.00203},
author = {Huang, Jiawen and Hung, Yun-Ning and Pati, Ashis and Gururani, Siddharth Kumar and Lerch, Alexander},
booktitle = {Proceedings of the 21st International Society for Music Information Retrieval Conference (ISMIR)},
eprint = {2008.00203},
file = {:Users/huanzhang/Downloads/307.pdf:pdf},
title = {{Score-informed Networks for Music Performance Assessment}},
url = {http://arxiv.org/abs/2008.00203},
year = {2020}
}
@techreport{Xie2016UnsupervisedAnalysis,
abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1511.06335v2},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
eprint = {1511.06335v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Girshick, Farhadi - 2016 - Unsupervised Deep Embedding for Clustering Analysis(3).pdf:pdf},
title = {{Unsupervised Deep Embedding for Clustering Analysis}},
url = {https://github.com/piiswrong/dec.},
year = {2016}
}
@article{Slade2020,
author = {Slade, Teri and Comeau, Gilles and Russell, Donald},
doi = {10.1080/09298215.2020.1784958},
title = {{Measurable changes in piano performance of scales and arpeggios following a Body Mapping workshop}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2020}
}
@article{Velickovic2023EverythingNetworks,
abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
annote = {locality constraint: 
- local function: node feature <--> neightbor's node features

Transformer: fully connected graph!! Everyone is everyone's neighbor, we combine them with attention},
archivePrefix = {arXiv},
arxivId = {2301.08210},
author = {Veli{\v{c}}kovi{\'{c}}, Petar},
doi = {10.1016/j.sbi.2023.102538},
eprint = {2301.08210},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Veli{\v{c}}kovi´veli{\v{c}}kovi´c - Unknown - Everything is Connected Graph Neural Networks.pdf:pdf},
issn = {1879033X},
journal = {Artificial Intelligence (AI) Methodology in Structural Biology},
title = {{Everything is Connected: Graph Neural Networks}},
url = {http://arxiv.org/abs/2301.08210},
year = {2023}
}
@techreport{JiangMelodyFiles,
abstract = {Melody identification is an important early step in music analysis. This paper presents a tool to identify the melody in each measure of a Standard MIDI File. We also share an open dataset of manually labeled music for researchers. We use a Bayesian maximum-likelihood approach and dynamic programming as the basis of our work. We have trained parameters on data sampled from the million song dataset [1, 2] and tested on a dataset including 1703 measures of music from different genres. Our algorithm achieves an overall accuracy of 89% in the test dataset. We compare our results to previous work.},
author = {Jiang, Zheng and Dannenberg, Roger B},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jiang, Dannenberg - Unknown - Melody Identification in Standard MIDI Files.pdf:pdf},
title = {{Melody Identification in Standard MIDI Files}},
url = {https://en.wikipedia.org/wiki/Viterbi}
}
@article{Ren2020,
abstract = {In pop music, accompaniments are usually played by multiple instruments (tracks) such as drum, bass, string and guitar, and can make a song more expressive and contagious by arranging together with its melody. Previous works usually generate multiple tracks separately and the music notes from different tracks not explicitly depend on each other, which hurts the harmony modeling. To improve harmony, in this paper1, we propose a novel MUlti-track MIDI representation (MuMIDI), which enables simultaneous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks. While this greatly improves harmony, unfortunately, it enlarges the sequence length and brings the new challenge of long-term music modeling. We further introduce two new techniques to address this challenge: 1) We model multiple note attributes (e.g., pitch, duration, velocity) of a musical note in one step instead of multiple steps, which can shorten the length of a MuMIDI sequence. 2) We introduce extra long-context as memory to capture long-term dependency in music. We call our system for pop music accompaniment generation as PopMAG. We evaluate PopMAG on multiple datasets (LMD, FreeMidi and CPMD, a private dataset of Chinese pop songs) with both subjective and objective metrics. The results demonstrate the effectiveness of PopMAG for multi-track harmony modeling and long-term context modeling. Specifically, PopMAG wins 42%/38%/40% votes when comparing with ground truth musical pieces on LMD, FreeMidi and CPMD datasets respectively and largely outperforms other state-of-the-art music accompaniment generation models and multi-track MIDI representations in terms of subjective and objective metrics.},
archivePrefix = {arXiv},
arxivId = {2008.07703},
author = {Ren, Yi and He, Jinzheng and Tan, Xu and Qin, Tao and Zhao, Zhou and Liu, Tie Yan},
eprint = {2008.07703},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2020 - PopMAG Pop Music Accompaniment Generation.pdf:pdf},
journal = {arXiv},
keywords = {Accompaniment generation,Music generation,Music representation,Pop music,Sequence-to-sequence model},
number = {1},
title = {{PopMAG: Pop Music Accompaniment Generation}},
year = {2020}
}
@techreport{Bowling1978,
abstract = {This article develops a two-component model of how melodies are stored in long-and short-term memory. The first component is the overlearned perceptual-motor schema of the musical scale. Evidence is presented supporting the lifetime stability of scales and the fact that they seem to have a basically logarithmic form cross-culturally. The second component, melodic contour, is shown to function independently of pitch interval sequence in memory. A new experiment is reported , using a recognition memory paradigm in which tonal standard stimuli are confused with same-contour comparisons, whether they are exact transpositions or tonal answers, but not with atonal comparison stimuli. This result is contrasted with earlier work using atonal melodies and shows the interdependence of the two components, scale and contour.},
annote = {From Duplicate 1 (Scale and Contour: Two Components of a Theory of Memory for Melodies - Bowling, W Jay)

Week 6

Tonal standard stimuli are confused with same-contour comparisons, atonal comparison does not

Helmhotz: Discrete pitch problem. continuous is difficult for people to decide how much changes. 

memory for the contour function separately from exact interval sizes},
author = {Bowling, W Jay},
booktitle = {Psychological Review},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bowling - 1978 - Scale and Contour Two Components of a Theory of Memory for Melodies.pdf:pdf},
number = {4},
pages = {341--354},
title = {{Scale and Contour: Two Components of a Theory of Memory for Melodies}},
volume = {85},
year = {1978}
}
@article{Elizalde2023CLAPSupervision,
abstract = {Mainstream machine listening models are trained to learn audio concepts under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which connects language and audio by using two encoders and a contrastive learning objective, bringing audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 7 domains, such as classification of sound events, scenes, music, and speech. CLAP establishes state-of-the-art (SoTA) in Zero-Shot performance. Also, we evaluated CLAP's audio encoder in a supervised learning setup and achieved SoTA in 5 tasks. The Zero-Shot capability removes the need of training with class labeled audio, enables flexible class prediction at inference time, and generalizes well in multiple downstream tasks. Code is available at: https://github.com/microsoft/CLAP.},
archivePrefix = {arXiv},
arxivId = {2206.04769},
author = {Elizalde, Benjamin and Deshmukh, Soham and Ismail, Mahmoud Al and Wang, Huaming},
doi = {10.1109/ICASSP49357.2023.10095889},
eprint = {2206.04769},
file = {:Users/huanzhang/Downloads/2206.04769v1.pdf:pdf},
isbn = {9781728163277},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {contrastive learning,general purpose audio representation,sound event classification,zero-shot},
title = {{CLAP Learning Audio Concepts from Natural Language Supervision}},
volume = {2023-June},
year = {2023}
}
@article{Faber2023MNISTLearning,
abstract = {Continual learning (CL) is one of the most promising trends in recent machine learning research. Its goal is to go beyond classical assumptions in machine learning and develop models and learning strategies that present high robustness in dynamic environments. This goal is realized by designing strategies that simultaneously foster the incorporation of new knowledge while avoiding forgetting past knowledge. The landscape of CL research is fragmented into several learning evaluation protocols, comprising different learning tasks, datasets, and evaluation metrics. Additionally, the benchmarks adopted so far are still distant from the complexity of real-world scenarios, and are usually tailored to highlight capabilities specific to certain strategies. In such a landscape, it is hard to clearly and objectively assess models and strategies. In this work, we fill this gap for CL on image data by introducing two novel CL benchmarks that involve multiple heterogeneous tasks from six image datasets, with varying levels of complexity and quality. Our aim is to fairly evaluate current state-of-the-art CL strategies on a common ground that is closer to complex real-world scenarios. We additionally structure our benchmarks so that tasks are presented in increasing and decreasing order of complexity—according to a curriculum—in order to evaluate if current CL models are able to exploit structure across tasks. We devote particular emphasis to providing the CL community with a rigorous and reproducible evaluation protocol for measuring the ability of a model to generalize and not to forget while learning. Furthermore, we provide an extensive experimental evaluation showing that popular CL strategies, when challenged with our proposed benchmarks, yield sub-par performance, high levels of forgetting, and present a limited ability to effectively leverage curriculum task ordering. We believe that these results highlight the need for rigorous comparisons in future CL works as well as pave the way to design new CL strategies that are able to deal with more complex scenarios.},
archivePrefix = {arXiv},
arxivId = {2303.11076},
author = {Faber, Kamil and Zurek, Dominik and Pietron, Marcin and Japkowicz, Nathalie and Vergari, Antonio and Corizzo, Roberto},
doi = {10.1007/s10994-024-06524-z},
eprint = {2303.11076},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Faber et al. - 2023 - From MNIST to ImageNet and Back Benchmarking Continual Curriculum Learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Computer vision,Continual learning,Curriculum learning,Image classification,Lifelong learning,Neural networks},
month = {mar},
number = {10},
title = {{From MNIST to ImageNet and back: benchmarking continual curriculum learning}},
url = {http://arxiv.org/abs/2303.11076},
volume = {113},
year = {2024}
}
@inproceedings{Karystinaios2022CadenceNetworks,
abstract = {Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, or music segmentation. However, automatic cadence detection remains challenging mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are roughly on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution can learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here.},
annote = {135 features per note},
archivePrefix = {arXiv},
arxivId = {2208.14819},
author = {Karystinaios, Emmanouil and Widmer, Gerhard},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
eprint = {2208.14819},
file = {:Users/huanzhang/Downloads/Cadence_Detection_in_Symbolic_Classical_Music_usin.pdf:pdf},
title = {{Cadence Detection in Symbolic Classical Music using Graph Neural Networks}},
url = {http://arxiv.org/abs/2208.14819},
year = {2022}
}
@article{Juslin2000,
author = {Juslin, Patrik N.},
file = {:Users/huanzhang/Downloads/10.1.1.578.6120.pdf:pdf},
journal = {Journal of experimental psychology: Human perception and performance},
number = {6},
pages = {1797--1813},
title = {{Cue Utilization in Communication of Emotion in Music Performance}},
volume = {26},
year = {2000}
}
@article{Takahashi2021,
abstract = {Conventional singing voice conversion (SVC) methods often suffer from operating in high-resolution audio owing to a high dimensionality of data. In this paper, we propose a hierarchical representation learning that enables the learning of disentangled representations with multiple resolutions independently. With the learned disentangled representations, the proposed method progressively performs SVC from low to high resolutions. Experimental results show that the proposed method outperforms baselines that operate with a single resolution in terms of mean opinion score (MOS), similarity score, and pitch accuracy.},
archivePrefix = {arXiv},
arxivId = {2101.06842},
author = {Takahashi, Naoya and Singh, Mayank Kumar and Mitsufuji, Yuki},
doi = {10.1109/IJCNN52387.2021.9533583},
eprint = {2101.06842},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Takahashi, Singh, Mitsufuji - Unknown - Hierarchical disentangled representation learning for singing voice conversion.pdf:pdf},
isbn = {9780738133669},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Singing voice conversion,VQVAE,high-resolution},
title = {{Hierarchical disentangled representation learning for singing voice conversion}},
volume = {2021-July},
year = {2021}
}
@article{GuModelingFilter,
abstract = {An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a "smoothed" version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method.},
author = {Gu, Yupeng and Raphael, Christopher},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gu, Raphael - Unknown - MODELING PIANO INTERPRETATION USING SWITCHING KALMAN FILTER.pdf:pdf},
title = {{Modeling Piano Interpretation Using Switching Kalman Filter}}
}
@article{Lucic2018AreStudy,
abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].},
archivePrefix = {arXiv},
arxivId = {1711.10337},
author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain},
eprint = {1711.10337},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lucic et al. - Unknown - Are GANs Created Equal A Large-Scale Study.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Are {GAN}s created equal? {A} large-scale study}},
year = {2018}
}
@techreport{Shalit2013,
abstract = {The role of musical influence has long been debated by scholars and critics in the humanities, but never in a data-driven way. In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970's and the mid 1990's. Copyright 2013 by the author(s).},
annote = {From Duplicate 2 (Modeling musical influence with topic models - Shalit, Uri; Weinshall, Daphna; Chechik, Gal)

DTM: Dynamic Topic Model
DIM: Document Influence Model

Topic = Features? genres?},
author = {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},
booktitle = {30th International Conference on Machine Learning, ICML 2013},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Shalit - 2010 - Modeling Musical Influence with Topic Models.pdf:pdf},
keywords = {computer audition,digital humanities,music information retrieval,topic models},
number = {PART 2},
pages = {903--911},
title = {{Modeling musical influence with topic models}},
year = {2013}
}
@inproceedings{Gong2023ListenUnderstand,
abstract = {The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.},
archivePrefix = {arXiv},
arxivId = {2305.10790},
author = {Gong, Yuan and Luo, Hongyin and Liu, Alexander H. and Karlinsky, Leonid and Glass, James},
booktitle = {In proceedings of the International Conference on Learning Representations (ICLR)},
eprint = {2305.10790},
file = {:Users/huanzhang/Downloads/2305.10790.pdf:pdf},
title = {{Listen, Think, and Understand}},
url = {http://arxiv.org/abs/2305.10790},
year = {2024}
}
@techreport{Eftekhari2020TrainingResults,
abstract = {Linear networks provide valuable insights into the workings of neural networks in general. This paper identifies conditions under which the gradient flow provably trains a linear network, in spite of the non-strict saddle points present in the optimization landscape. This paper also provides the computational complexity of training linear networks with gradient flow. To achieve these results , this work develops a machinery to provably identify the stable set of gradient flow, which then enables us to improve over the state of the art in the literature of linear networks (Bah et al., 2019; Arora et al., 2018a). Crucially, our results appear to be the first to break away from the lazy training regime which has dominated the literature of neural networks. This work requires the network to have a layer with one neuron, which subsumes the networks with a scalar output, but extending the results of this theoretical work to all linear networks remains a challenging open problem.},
archivePrefix = {arXiv},
arxivId = {2002.09852v3},
author = {Eftekhari, Armin},
eprint = {2002.09852v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Eftekhari - 2020 - Training Linear Neural Networks Non-Local Convergence and Complexity Results.pdf:pdf},
title = {{Training Linear Neural Networks: Non-Local Convergence and Complexity Results}},
year = {2020}
}
@article{Siupsinskiene2003QuantitativeVoices.,
abstract = {The aim of this study was to compare healthy trained and untrained voices as well as healthy and dysphonic trained voices in adults using combined voice range profile and aerodynamic tests, to define the normal range limiting values of quantitative voice parameters and to select the most informative quantitative voice parameters for separation between healthy and dysphonic trained voices. Three groups of persons were evaluated. One hundred eighty six healthy volunteers were divided into two groups according to voice training: non-professional speakers group consisted of 106 untrained voices persons (36 males and 70 females) and professional speakers group--of 80 trained voices persons (21 males and 59 females). Clinical group consisted of 103 dysphonic professional speakers (23 males and 80 females) with various voice disorders. Eighteen quantitative voice parameters from combined voice range profile (VRP) test were analyzed: 8 of voice range profile, 8 of speaking voice, overall vocal dysfunction degree and coefficient of sound, and aerodynamic maximum phonation time. Analysis showed that healthy professional speakers demonstrated expanded vocal abilities in comparison to healthy non-professional speakers. Quantitative voice range profile parameters- pitch range, high frequency limit, area of high frequencies and coefficient of sound differed significantly between healthy professional and non-professional voices, and were more informative than speaking voice or aerodynamic parameters in showing the voice training. Logistic stepwise regression revealed that VRP area in high frequencies was sufficient to discriminate between healthy and dysphonic professional speakers for male subjects (overall discrimination accuracy--81.8%) and combination of three quantitative parameters (VRP high frequency limit, maximum voice intensity and slope of speaking curve) for female subjects (overall model discrimination accuracy--75.4%). We concluded that quantitative voice assessment with selected parameters might be useful for evaluation of voice education for healthy professional speakers as well as for detection of vocal dysfunction and evaluation of rehabilitation effect in dysphonic professionals.},
author = {Siupsinskiene, Nora},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Siupsinskiene - 2003 - Quantitative analysis of professionally trained versus untrained voices.pdf:pdf},
issn = {1010660X},
journal = {Medicina (Kaunas, Lithuania)},
keywords = {as healthy and dysphonic,coefficient of,healthy trained and untrained,profile and aero-,quantitative voice assessment,sound,speaking voice,study was to compare,summary,the aim of this,trained voices in adults,using combined voice range,voice range profile,voice training,voices as well},
number = {1},
pages = {36--46},
pmid = {12576764},
title = {{Quantitative analysis of professionally trained versus untrained voices.}},
volume = {39},
year = {2003}
}
@article{Haas2011Harmtrace:Analysis,
abstract = {Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences. {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Haas, W. Bas De and Magalh{\~{a}}es, Jos{\'{e}} Pedro and Veltkamp, Remco C. and Wiering, Frans},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Haas et al. - 2011 - Harmtrace Improving harmonic similarity estimation using functional harmony analysis.pdf:pdf},
isbn = {9780615548654},
journal = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
pages = {67--72},
title = {{Harmtrace: Improving harmonic similarity estimation using functional harmony analysis}},
year = {2011}
}
@article{Nie,
author = {Nie, Ke and Diego, San},
file = {:Users/huanzhang/Downloads/148.pdf:pdf},
title = {{INACCURATE PREDICTION OR GENRE EVOLUTION ?}}
}
@article{EgermannProbabilisticMusic,
abstract = {We present the results of a study testing the often-theorized role of musical expectations in inducing listeners' emotions in a live flute concert experiment with 50 participants. Using an audience response system developed for this purpose, we measured subjective experience and peripheral psychophys-iological changes continuously. To confirm the existence of the link between expectation and emotion, we used a threefold approach. (1) On the basis of an information-theoretic cognitive model, melodic pitch expectations were predicted by analyzing the musical stimuli used (six pieces of solo flute music). (2) A continuous rating scale was used by half of the audience to measure their experience of unexpectedness toward the music heard. (3) Emotional reactions were measured using a multicomponent approach: subjective feeling (valence and arousal rated continuously by the other half of the audience members), expressive behavior (facial EMG), and peripheral arousal (the latter two being measured in all 50 participants). Results confirmed the predicted relationship between high-information-content musical events, the violation of musical expectations (in corresponding ratings), and emotional reactions (psychologically and physiologically). Musical structures leading to expectation reactions were manifested in emotional reactions at different emotion component levels (increases in subjective arousal and autonomic nervous system activations). These results emphasize the role of musical structure in emotion induction, leading to a further understanding of the frequently experienced emotional effects of music.},
author = {Egermann, Hauke and Pearce, Marcus T and Wiggins, Geraint A and Mcadams, Stephen},
doi = {10.3758/s13415-013-0161-y},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Egermann et al. - Unknown - Probabilistic models of expectation violation predict psychophysiological emotional responses to live concer.pdf:pdf},
keywords = {Computational modeling,Emotion,Expectation,Music,Psychophysiology,Statistical learning},
title = {{Probabilistic models of expectation violation predict psychophysiological emotional responses to live concert music}}
}
@article{Touvron2023LLaMAModels,
abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
archivePrefix = {arXiv},
arxivId = {2302.13971},
author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-anne and Lacroix, Timoth{\'{e}}e and Rozi{\`{e}}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
eprint = {2302.13971},
file = {:Users/huanzhang/Downloads/2302.13971v1.pdf:pdf},
journal = {arXiv:2302.13971},
title = {{LLaMA: Open and Efficient Foundation Language Models}},
url = {http://arxiv.org/abs/2302.13971},
year = {2023}
}
@article{Parncutt2003AccentPerformance,
abstract = {Computer simulations of expressive piano performance are becoming increasingly sophisticated. But they do not yet produce consistently satisfying results. In this regard, musical expression is lagging behind timbral synthesis; most instrumental timbres can now be convincingly synthesized by physical models (Smith, 1996). What is the problem? Most recent computer-based attempts to create expressive musical performances (e.g., Clynes, 1985; Friberg, 1991; Honing, 1990; Mazzola & Zahorka, 1994; Sundberg, 1988; Todd, 1989) have focused on the role of musical structure (as defined for example by Clarke, 1988). It has been clear since (at the latest) Riemann (1884) that musicians' manipulations of performance parameters often clarify the musical structure for the listener. But two major problems arise in a computer model of music performance that is based purely on structure. First, there are many possible ways of representing musical structure in a computer model, and of expressing a given structure using timing, dynamics, amd so on; which, if any, of these models comes closer to the "right" representation? Second, musical structure is evidently not the only thing upon which interpretation depends.},
author = {Parncutt, Richard},
file = {:Users/huanzhang/Downloads/Pa03_AccentExpression.pdf:pdf},
journal = {Bach},
keywords = {Music,Performance,Piano,Psychology,expressive},
pages = {163--185},
title = {{Accents and expression in piano performance}},
url = {http://www.uni-graz.at/richard.parncutt/publications/Pa03_AccentExpression.pdf},
year = {2003}
}
@techreport{Schellenberg2003,
abstract = {Here we show that good pitch memory is widespread among adults with no musical training. We tested unselected college students on their memory for the pitch level of instrumental soundtracks from familiar television programs. Participants heard 5-s excerpts either at the original pitch level or shifted upward or downward by 1 or 2 semitones. They successfully identified the original pitch levels. Other participants who heard comparable excerpts from unfamiliar recordings could not do so. These findings reveal that ordinary listeners retain fine-grained information about pitch level over extended periods. Adults' reportedly poor memory for pitch is likely to be a by-product of their inability to name isolated pitches.},
annote = {From Duplicate 2 (Good Pitch Memory is Widespread - Schellenberg, E Glenn; Trehub, Sandra E)

Week 2},
author = {Schellenberg, E Glenn and Trehub, Sandra E},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Schellenberg, Trehub - 2003 - GOOD PITCH MEMORY IS WIDESPREAD.pdf:pdf},
number = {3},
title = {{Good Pitch Memory is Widespread}},
url = {http://www.digidesign.},
volume = {14},
year = {2003}
}
@article{Prinz2021OnRetrieval,
author = {Prinz, Katharina and Flexer, Arthur and Widmer, Gerhard},
doi = {10.5334/tismir.85},
file = {:Users/huanzhang/Downloads/85-2680-1-PB.pdf:pdf},
journal = {Transactions of the International Society for Music Information Retrieval},
keywords = {adversarial attack,instrument classification,music recommendation,validity},
number = {1},
pages = {93},
title = {{On End-to-End White-Box Adversarial Attacks in Music Information Retrieval}},
volume = {4},
year = {2021}
}
@inproceedings{Micchi2018NeuralClassification,
author = {Micchi, Gianluca},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR) Late-Breading Demo (LBD)},
file = {:Users/huanzhang/Downloads/2018-ismir-lb-composer.pdf:pdf},
title = {{A neural network for composer classification}},
year = {2018}
}
@techreport{Paulus2010Audio-basedAnalysis,
abstract = {Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-theart methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneitybased approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field. {\textcopyright} 2010 International Society for Music Information Retrieval.},
author = {Paulus, Jouni and M{\"{u}}ller, Meinard and Klapuri, Anssi},
booktitle = {Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Paulus, Klapuri - Unknown - AUDIO-BASED MUSIC STRUCTURE ANALYSIS.pdf:pdf;:Users/huanzhang/Downloads/Audio-based_music_structure_analysis.pdf:pdf},
isbn = {9789039353813},
number = {January},
pages = {625--636},
title = {{Audio-based music structure analysis}},
year = {2010}
}
@inproceedings{Zhou2024Non-compositionalLearning,
abstract = {Non-compositional expressions, by virtue of their non-compositionality, are a classic 'pain in the neck' for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we propose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model's performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https://github.com/zhjjn/CL2Gen.git.},
author = {Zhou, Jianing and Zeng, Ziheng and Gong, Hongyu and Bhat, Suma},
booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
doi = {10.18653/v1/2023.findings-emnlp.286},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - Unknown - Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning.pdf:pdf},
isbn = {9798891760615},
issn = {0736587X},
title = {{Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning}},
year = {2023}
}
@article{Acl,
author = {Acl, Anonymous},
file = {:Users/huanzhang/Downloads/4729_the_music_maestro_or_the_music.pdf:pdf},
title = {{The Music Maestro or The Musically Challenged , A Massive Music Evaluation Benchmark for Large Language Models}}
}
@techreport{LuoDEEPTOGETHER,
abstract = {Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation , presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components.},
author = {Luo, Yi and Chen, Zhuo and Hershey, John R and {Le Roux}, Jonathan and Mesgarani, Nima},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Luo et al. - Unknown - DEEP CLUSTERING AND CONVENTIONAL NETWORKS FOR MUSIC SEPARATION STRONGER TOGETHER.pdf:pdf},
keywords = {Deep learning,Index Terms-Deep clustering,Mu-sic separation,Singing voice separation},
title = {{DEEP CLUSTERING AND CONVENTIONAL NETWORKS FOR MUSIC SEPARATION: STRONGER TOGETHER}}
}
@inproceedings{Zhang2021LearnAssessment,
author = {Zhang, Huan and Jiang, Yiliang and Jiang, Tao and Hu, Peng},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Learn by Referencing: Towards Deep Metric Learning for Singing Assessment}},
year = {2021}
}
@article{Zeghidour,
abstract = {We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, SoundStream at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.},
archivePrefix = {arXiv},
arxivId = {2107.03312v1},
author = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco},
eprint = {2107.03312v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zeghidour et al. - Unknown - SoundStream An End-to-End Neural Audio Codec.pdf:pdf},
title = {{SoundStream: An End-to-End Neural Audio Codec}}
}
@article{Zou2022MelonsGraph,
abstract = {The creation of long melody sequences requires effective expression of coherent musical structure. However, there is no clear representation of musical structure. Recent works on music generation have suggested various approaches to deal with the structural information of music, but generating a full-song melody with clear long-term structure remains a challenge. In this paper, we propose MELONS, a melody generation framework based on a graph representation of music structure which consists of eight types of bar-level relations. MELONS adopts a multi-step generation method with transformer-based networks by factoring melody generation into two sub-problems: structure generation and structure conditional melody generation. Experimental results show that MELONS can produce structured melodies with high quality and rich contents.},
archivePrefix = {arXiv},
arxivId = {2110.05020},
author = {Zou, Yi and Zou, Pei and Zhao, Yi and Zhang, Kaixiang and Zhang, Ran and Wang, Xiaorui},
doi = {10.1109/ICASSP43922.2022.9747802},
eprint = {2110.05020},
file = {:Users/huanzhang/Downloads/2110.05020v3.pdf:pdf},
isbn = {9781665405409},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Music structure,melody generation,structure graph,transformer},
title = {{Melons: Generating Melody With Long-Term Structure Using Transformers and Structure Graph}},
year = {2022}
}
@techreport{MauchThe1960-2010,
abstract = {In modern societies, cultural change seems ceaseless. The flux of fashion is especially obvious for popular music. While much has been written about the origin and evolution of pop, most claims about its history are anecdotal rather than scientific in nature. To rectify this we investigate the US Billboard Hot 100 between 1960 and 2010. Using Music Information Retrieval (MIR) and text-mining tools we analyse the musical properties of ∼17,000 recordings that appeared in the charts and demonstrate quantitative trends in their harmonic and timbral properties. We then use these properties to produce an audio-based classification of musical styles and study the evolution of musical diversity and disparity, testing, and rejecting, several classical theories of cultural change. Finally, we investigate whether pop musical evolution has been gradual or punctuated. We show that, although pop music has evolved continuously, it did so with particular rapidity during three stylistic "revolutions" around 1964, 1983 and 1991. We conclude by discussing how our study points the way to a quantitative science of cultural change.},
archivePrefix = {arXiv},
arxivId = {1502.05417v1},
author = {Mauch, Matthias and Maccallum, Robert M and Levy, Mark and Leroi, Armand M},
eprint = {1502.05417v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Mauch et al. - Unknown - The Evolution of Popular Music USA 1960-2010.pdf:pdf},
title = {{The Evolution of Popular Music: USA 1960-2010}}
}
@inproceedings{Zhang2022Inversion-BasedModels,
abstract = {The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the attributes of a particular painting. The uniqueness of an artwork lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.},
address = {Vancouver, Canada},
archivePrefix = {arXiv},
arxivId = {2211.13203},
author = {Zhang, Yuxin and Huang, Nisha and Tang, Fan and Huang, Haibin and Ma, Chongyang and Dong, Weiming and Xu, Changsheng},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR52729.2023.00978},
eprint = {2211.13203},
file = {:Users/huanzhang/Downloads/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf:pdf},
isbn = {9798350301298},
issn = {10636919},
keywords = {Image and video synthesis and generation},
title = {{Inversion-based Style Transfer with Diffusion Models}},
url = {http://arxiv.org/abs/2211.13203},
year = {2023}
}
@inproceedings{Lesota2022,
author = {Lesota, Oleg and Parada-cabaleiro, Emilia and Brandl, Stefan and Lex, Elisabeth and Rekabsaz, Navid and Schedl, Markus},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/165.pdf:pdf},
title = {{TRACES OF GLOBALIZATION IN ONLINE MUSIC CONSUMPTION PATTERNS AND RESULTS OF RECOMMENDATION ALGORITHMS}},
year = {2022}
}
@article{Fazekas2012,
abstract = {In order for audio applications to interoperate, some agreement on how information is structured and encoded has to be in place within developer and user communities. This agreement can take the form of an industry standard or widely adapted open framework consisting of conceptual data models expressed using formal description languages. There are several viable approaches to conceptualise audio related metadata, and several ways to describe conceptual models, as well as encode and exchange information. While emerging standards have already been proven invaluable in audio information management, it remains dicult to design or choose the model that is most appropriate for an application. This paper facilitates this process by providing an overview, focussing on di↵erences in conceptual models underlying audio metadata schemata.},
author = {Fazekas, Gy{\"{o}}rgy and Sandler, Mark B},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Fazekas, Sandler - 2012 - Knowledge representation issues in audio related metadata model design.pdf:pdf},
title = {{Knowledge representation issues in audio related metadata model design}},
url = {http://www.merriam-webster.com/dictionary/ontology},
year = {2012}
}
@techreport{Cancino-Chacon2019ISMIRSlides,
author = {Cancino-Chac{\'{o}}n, Carlos Eduardo and Kosta, Katerina},
file = {:Users/huanzhang/Downloads/ismir2019_expressive_performance_tutorial-master/ismir2019_T4_slides.pdf:pdf},
title = {{ISMIR Performance Tutorial Slides}},
year = {2019}
}
@article{Crowder1989,
abstract = {Subjects made same/different judgments concerning the pitches of two successive tones. In Experiment 1, the two tones were played on either the same instrument (guitar, flute, trumpet) or on different instruments. When the two pitches were indeed the same, people were faster to respond "same" when the instrumental timbres also matched than when two different instruments played the tones. In Experiment 2, the first tone was always a sine wave and the second was one of the same three instrumental tones. Following the sine wave, but before presentation of the second tone, people were asked to form an image of what an assigned instrument would have sounded like playing that pitch. A match between this imagined timbre of the first tone and the timbre of the second tone produced faster reaction times to identical pitches than a mismatch.},
annote = {From Duplicate 1 (Imagery for Musical Timbre - Crowder, Robert G.)

Week 3},
author = {Crowder, Robert G.},
doi = {10.1037/0096-1523.15.3.472},
file = {:Users/huanzhang/Downloads/1990-00317-001.pdf:pdf},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {3},
pages = {472--478},
title = {{Imagery for Musical Timbre}},
volume = {15},
year = {1989}
}
@inproceedings{Roberts2018HierarchicalMusic,
abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online.2.},
address = {Stockholm, Sweden},
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1803.05428},
file = {:Users/huanzhang/Downloads/1803.05428.pdf:pdf},
isbn = {9781510867963},
title = {{A hierarchical latent vector model for learning long-term structure in music}},
year = {2018}
}
@article{Nakamura2020StatisticalFingering,
abstract = {Automatic estimation of piano fingering is important for understanding the computational process of music performance and applicable to performance assistance and education systems. While a natural way to formulate the quality of fingerings is to construct models of the constraints/costs of performance, it is generally difficult to find appropriate parameter values for these models. Here we study an alternative data-driven approach based on statistical modeling in which the appropriateness of a given fingering is described by probabilities. Specifically, we construct two types of hidden Markov models (HMMs) and their higher-order extensions. We also study deep neural network (DNN)-based methods for comparison. Using a newly released dataset of fingering annotations, we conduct systematic evaluations of these models as well as a representative constraint-based method. We find that the methods based on high-order HMMs outperform the other methods in terms of estimation accuracies. We also quantitatively study individual difference of fingering and propose evaluation measures that can be used with multiple ground truth data. We conclude that the HMM-based methods are currently state of the art and generate acceptable fingerings in most parts and that they have certain limitations such as ignorance of phrase boundaries and interdependence of the two hands.},
archivePrefix = {arXiv},
arxivId = {1904.10237},
author = {Nakamura, Eita and Saito, Yasuyuki and Yoshii, Kazuyoshi},
doi = {10.1016/j.ins.2019.12.068},
eprint = {1904.10237},
file = {:Users/huanzhang/Downloads/1-s2.0-S0020025519311879-main.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Deep learning,Hidden Markov models,Piano fingering dataset,Piano fingering model,Statistical learning,Symbolic music processing},
pages = {68--85},
publisher = {Elsevier Inc.},
title = {{Statistical learning and estimation of piano fingering}},
volume = {517},
year = {2020}
}
@inproceedings{Berndt2021,
abstract = {Music Performance Markup (MPM) is a new XML format that offers a model-based, systematic approach to describing and analysing musical performances. Its foundation is a set of mathematical models that capture the characteristics of performance features such as tempo, rubato, dynamics , articulations, and metrical accentuations. After a brief introduction to MPM, this paper will put the focus on the infrastructure of documentations, software tools and ongoing development activities around the format.},
author = {Berndt, Axel},
booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference (ISMIR 2021)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Berndt - Unknown - THE MUSIC PERFORMANCE MARKUP FORMAT AND ECOSYSTEM.pdf:pdf},
title = {{The Music Performance Markup Format and Ecosystem}},
year = {2021}
}
@article{Istvanek2023,
author = {Istvanek, Matej and Miklanek, Stepan},
file = {:Users/huanzhang/Downloads/applsci-13-03603-v2.pdf:pdf},
keywords = {citation,classification,classification of,interpretation,istvanek,l,m,machine learning,miklanek,music analysis,music information,origin,retrieval,s,spurny,string quartet,synchronization},
title = {{Classification of Interpretation Differences in String Quartets Based on the Origin of Performers}},
year = {2023}
}
@article{Deutsch20136Music,
annote = {From Duplicate 2 (6 Grouping Mechanisms in Music - Deutsch, Diana)

Week 5},
author = {Deutsch, Diana},
doi = {10.1016/B978-0-12-381460-9.00006-7},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Deutsch - 2013 - 6 Grouping Mechanisms in Music.pdf:pdf},
isbn = {9780123814609},
title = {{6 Grouping Mechanisms in Music}},
url = {http://dx.doi.org/10.1016/B978-0-12-381460-9.00006-7},
year = {2013}
}
@article{Furuya2011HandPlaying,
abstract = {Dexterous use of the hand represents a sophisticated sensorimotor function. In behaviors such as playing the piano, it can involve strong temporal and spatial constraints. The purpose of this study was to determine fundamental patterns of covariation of motion across joints and digits of the human hand. Joint motion was recorded while 5 expert pianists played 30 excerpts from musical pieces, which featured $\sim$50 different tone sequences and fingering. Principal component analysis and cluster analysis using an expectation-maximization algorithm revealed that joint velocities could be categorized into several patterns, which help to simplify the description of the movements of the multiple degrees of freedom of the hand. For the thumb keystroke, two distinct patterns of joint movement covariation emerged and they depended on the spatiotemporal patterns of the task. For example, the thumb-under maneuver was clearly separated into two clusters based on the direction of hand translation along the keyboard. While the pattern of the thumb joint velocities differed between these clusters, the motions at the metacarpo-phalangeal and proximal-phalangeal joints of the four fingers were more consistent. For a keystroke executed with one of the fingers, there were three distinct patterns of joint rotations, across which motion at the striking finger was fairly consistent, but motion of the other fingers was more variable. Furthermore, the amount of movement spillover of the striking finger to the adjacent fingers was small irrespective of the finger used for the keystroke. These findings describe an unparalleled amount of independent motion of the fingers. {\textcopyright} 2011 the American Physiological Society.},
author = {Furuya, Shinichi and Flanders, Martha and Soechting, John F.},
doi = {10.1152/jn.00378.2011},
file = {:Users/huanzhang/Downloads/furuya-et-al-2011-hand-kinematics-of-piano-playing.pdf:pdf},
issn = {00223077},
journal = {Journal of Neurophysiology},
keywords = {Cluster analysis,Individuated finger movements,Multijoint movements,Principal component analysis,Synergy},
number = {6},
pages = {2849--2864},
pmid = {21880938},
title = {{Hand kinematics of piano playing}},
volume = {106},
year = {2011}
}
@inproceedings{Zheng2023JudgingArena,
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
archivePrefix = {arXiv},
arxivId = {2306.05685},
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
booktitle = {NeurIPS 2023 Datasets and Benchmarks Poster},
eprint = {2306.05685},
file = {:Users/huanzhang/Downloads/2306.05685v4.pdf:pdf},
title = {{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}},
url = {http://arxiv.org/abs/2306.05685}
}
@inproceedings{Chowdhury2019,
abstract = {Emotional aspects play an important part in our interaction with music. However, modelling these aspects in MIR systems have been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its predictions, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features and observe that the loss in predictive performance of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the midlevel features is justified by the gain in explainability of the predictions.},
address = {Delft, Netherlands},
archivePrefix = {arXiv},
arxivId = {1907.03572},
author = {Chowdhury, Shreyan and Vall, Andreu and Haunschmid, Verena and Widmer, Gerhard},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019},
eprint = {1907.03572},
file = {:Users/huanzhang/Downloads/1907.03572.pdf:pdf},
isbn = {9781732729919},
title = {{Towards explainable music emotion recognition: The route via Mid-level features}},
year = {2019}
}
@article{BalkeLEARNINGRETRIEVAL,
abstract = {Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.},
archivePrefix = {arXiv},
arxivId = {1906.10996v1},
author = {Balke, Stefan and Dorfer, Matthias and Carvalho, Luis and Arzt, Andreas and Widmer, Gerhard},
eprint = {1906.10996v1},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Balke et al. - Unknown - LEARNING SOFT-ATTENTION MODELS FOR TEMPO-INVARIANT AUDIO-SHEET MUSIC RETRIEVAL.pdf:pdf},
title = {{LEARNING SOFT-ATTENTION MODELS FOR TEMPO-INVARIANT AUDIO-SHEET MUSIC RETRIEVAL}},
url = {http://www.fluidsynth.org}
}
@inproceedings{Li2022,
author = {Li, Yuqiang and Li, Shengchen and Fazekas, George},
booktitle = {Proceeding of the 23rd International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/90.pdf:pdf},
title = {{HOW MUSIC FEATURES AND MUSICAL DATA REPRESENTATIONS AFFECT OBJECTIVE EVALUATION OF MUSIC COMPOSITION : A REVIEW OF THE CSMT DATA CHALLENGE 2020}},
year = {2022}
}
@article{Ji2024a,
abstract = {Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/},
archivePrefix = {arXiv},
arxivId = {2308.14430},
author = {Ji, Shengpeng and Zuo, Jialong and Fang, Minghui and Jiang, Ziyue and Chen, Feiyang and Duan, Xinyu and Huai, Baoxing and Zhao, Zhou},
doi = {10.1109/icassp48485.2024.10445879},
eprint = {2308.14430},
file = {:Users/huanzhang/Downloads/TextrolSpeech_A_Text_Style_Control_Speech_Corpus_with_Codec_Language_Text-to-Speech_Models.pdf:pdf},
isbn = {9798350344851},
journal = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {10301--10305},
publisher = {IEEE},
title = {{TextrolSpeech: A Text Style Control Speech Corpus with Codec Language Text-to-Speech Models}},
year = {2024}
}
@inproceedings{Robertson2012DecodingAnnotations,
abstract = {This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However , whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing.},
author = {Robertson, Andrew},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR 2012)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Robertson - 2012 - Decoding Tempo and Timing Variations in Music Recordings from Beat Annotations.pdf:pdf},
title = {{Decoding Tempo and Timing Variations in Music Recordings from Beat Annotations}},
year = {2012}
}
@article{Cook2010TheRecordings,
abstract = {This article introduces the other contributions to this second issue of Musicae Scientiae devoted to the work of the AHRC Research Centre for the History and Analysis of Recorded Music (CHARM), and sets them into the larger context of musicological research into recorded musical performance. There is consideration of musicology's historically odd relationship to performance, including the historically informed performance movement and what is referred to as the ‘page-to-stage' approach of recent music theory: CHARM's analytical projects focussed on aspects overlooked by the score-based approach, on the potential for bottom-up methods, and on the nature of performance style and the extent to which it can be meaningfully analysed by empirical methods. Another strand of CHARM's research investigated the extent to which the commercial practices of the record industry have helped to shape twentieth-century performance. The author includes brief accounts of his own projects with CHARM so as to provide an overview of the Centre's work as a whole. {\textcopyright} 2010, European Society for the Cognitive Sciences of Music. All rights reserved.},
author = {Cook, Nicholas},
doi = {10.1177/102986491001400201},
file = {:Users/huanzhang/Downloads/102986491001400201.pdf:pdf},
issn = {10298649},
journal = {Musicae Scientiae},
number = {2},
pages = {3--21},
title = {{The ghost in the machine: Towards a musicology of recordings}},
volume = {14},
year = {2010}
}
@inproceedings{Morsi2023SoundsPerformances,
author = {Morsi, Alia and Tatsumi, Kana and Maezawa, Akira and Fujishima, Takuya and Serra, Xavier},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
file = {:Users/huanzhang/Downloads/Morsi_ism_soun.pdf:pdf},
title = {{Sounds {Out} of {Pl{\"{a}}ce}? {S}core-Independent Detection of Conspicuous Mistakes in Piano Performances}},
year = {2023}
}
@article{Muhamed2021SymbolicGANs,
abstract = {Autoregressive models using Transformers have emerged as the dominant approach for music generation with the goal of synthesizing minute-long compositions that exhibit large-scale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. Unfortunately, the quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In addition, we break the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.},
author = {Muhamed, Aashiq and Li, Liang and Shi, Xingjian and Yaddanapudi, Suri and Chi, Wayne and Jackson, Dylan and Suresh, Rahul and Lipton, Zachary C. and Smola, Alexander J.},
doi = {10.1609/aaai.v35i1.16117},
file = {:Users/huanzhang/Downloads/16117-Article Text-19611-1-2-20210518 (1).pdf:pdf},
isbn = {9781713835974},
issn = {2159-5399},
journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
keywords = {Application Domains: Art/Music/Creativity,Machine Learning: (Deep) Neural Network Algorithms,Machine Learning: Adversarial Learning & Robustnes,Machine Learning: Neural Generative Models & Autoe},
pages = {408--417},
title = {{Symbolic Music Generation with Transformer-GANs}},
volume = {1},
year = {2021}
}
@article{Cheng,
author = {Cheng, Haonan and Zhang, Junwei and Zou, Yi and Ye, Long},
file = {:Users/huanzhang/Downloads/3920 Submission.pdf:pdf},
title = {{ES-Midiformer based Symbolic Music Understanding Model for High-accuracy Detection}}
}
@article{Simonetta2019,
abstract = {In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score - i.e. without listening to the music performed - can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications. In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input.},
archivePrefix = {arXiv},
arxivId = {1906.10547},
author = {Simonetta, Federico and Cancino-Chac{\'{o}}n, Carlos and Ntalampiras, Stavros and Widmer, Gerhard},
eprint = {1906.10547},
file = {:Users/huanzhang/Downloads/1906.10547v1.pdf:pdf},
isbn = {9781732729919},
journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019},
pages = {924--931},
title = {{A convolutional approach to melody line identification in symbolic scores}},
year = {2019}
}
@article{Prince2009PitchCombine,
abstract = {The authors examined how the structural attributes of tonality and meter influence musical pitch-time relations. Listeners heard a musical context followed by probe events that varied in pitch class and temporal position. Tonal and metric hierarchies contributed additively to the goodness-of-fit of probes, with pitch class exerting a stronger influence than temporal position (Experiment 1), even when listeners attempted to ignore pitch (Experiment 2). Speeded classification tasks confirmed this asymmetry. Temporal classification was biased by tonal stability (Experiment 3), but pitch classification was unaffected by temporal position (Experiment 4). Experiments 5 and 6 ruled out explanations based on the presence of pitch classes and temporal positions in the context, unequal stimulus quantity, and discrim-inability. The authors discuss how typical Western music biases attention toward pitch and distinguish between dimensional discriminability and salience. A central goal of cognitive psychology is to understand the process by which listeners apprehend the complex visual and auditory stimuli with which they come into contact and how they subsequently represent this information internally. This process must act on the multidimensional nature of objects and exhibit stability in the face of complex and often subtle variations that frequently occur along any number of physical dimensions simultaneously. Therefore, one of the critical questions in understanding perceptual and cognitive organization is how changes along various stimulus dimensions lead to varying percepts and correspondingly different cognitive structures. Equally important, however, is determining the interrelations between the dimensions, and how processing of change in the attributes in one perceptual dimension may or may not be influenced by change in the attributes of a different dimension. Accordingly, a core question of perceptual and cognitive processing concerns how stimulus dimensions combine in perception.},
author = {Prince, Jon B and Thompson, William F and Schmuckler, Mark A},
doi = {10.1037/a0016456},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Prince, Thompson, Schmuckler - 1970 - Human Perception and Performance.pdf:pdf},
keywords = {dimensional interactions,music,pitch,time},
number = {5},
pages = {1598--1617},
publisher = {Marks},
title = {{Pitch and Time, Tonality and Meter: How do Musical Dimensions Combine?}},
volume = {35},
year = {2009}
}
@article{Tjandra2021UnsupervisedRepresentation,
abstract = {Speech is influenced by a number of underlying factors, which can be broadly categorized into linguistic contents and speaking styles. However, collecting the labeled data that annotates both content and style is an expensive and time-consuming task. Here, we present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate a useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.},
archivePrefix = {arXiv},
arxivId = {2010.12973v2},
author = {Tjandra, Andros and Pang, Ruoming and Zhang, Yu and Karita, Shigeki},
doi = {10.21437/Interspeech.2021-1936},
eprint = {2010.12973v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tjandra et al. - Unknown - Unsupervised Learning of Disentangled Speech Content and Style Representation.pdf:pdf;:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tjandra et al. - 2021 - Unsupervised Learning of Disentangled Speech Content and Style Representation.pdf:pdf},
isbn = {9781713836902},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)},
keywords = {Disentanglement,Index Terms: Unsupervised,Speech,Unsupervised,VQ-VAE,VQVAE},
title = {{Unsupervised Learning of Disentangled Speech Content and Style Representation}},
url = {http://dx.doi.org/10.21437/Interspeech.2021-1936 https://unsup-content-style-demo-v1.netlify.app.},
year = {2021}
}
@inproceedings{Li2024MusicModels,
archivePrefix = {arXiv},
arxivId = {arXiv:2406.15885v1},
author = {Li, Jiajia and Yang, Lu and Tang, Mingni and Chen, Cong and Li, Zuchao and Wang, Ping and Zhao, Hai},
booktitle = {In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
eprint = {arXiv:2406.15885v1},
file = {:Users/huanzhang/Downloads/2406.15885v1.pdf:pdf},
title = {{The Music Maestro or The Musically Challenged , A Massive Music Evaluation Benchmark for Large Language Models}},
year = {2024}
}
@techreport{Li2015AnalysisFeatures,
abstract = {The manipulation of different interpretational factors, including dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specifically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music information retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expressive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction.},
author = {Li, Pei Ching and Su, Li and Yang, Yi Hsuan and Su, Alvin W.Y. Y},
booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - ANALYSIS OF EXPRESSIVE MUSICAL TERMS IN VIOLIN USING SCORE-INFORMED AND EXPRESSION-BASED AUDIO FEATURES.pdf:pdf},
isbn = {9788460688532},
pages = {809--815},
title = {{Analysis of expressive musical terms in violin using score-informed and expression-based audio features}},
url = {http://www.},
year = {2015}
}
@article{Jiang2022,
abstract = {This study investigated the perceptions of piano performance programs in higher education from current collegiate students and faculty members' perspectives in China and the United States. Participants were from representative university-level institutions in the U.S. (n = 41) and China (n = 130). They were sent a questionnaire concerning (a) the factors that motivate students to pursue a piano performance degree, (b) the most important educational practices in their programs, (c) the most challenging tasks the students encounter, (d) students' career goal, and (e) faculty members' suggestions for prospective students and opinions on improving the piano performance programs. A summary of students and faculty members' perceptions were outlined and the comparison between the two countries were explored. It is encouraging that not only students gave careful attention toward the applied lessons and performance opportunities in their studies, but also that a large percentage of the students believed they received excellent advice regarding practice strategies and artistry in their applied lessons in both countries. Most of the faculty participants in both countries expressed positive attitudes regarding the piano performance programs in their universities. By providing statistically significant data, this study provides a comprehensive vision for institutions to continue establishing piano programs.},
author = {Jiang, Yuan},
doi = {10.1177/02557614211065760},
file = {:Users/huanzhang/Downloads/jiang-2021-a-comparative-study-of-piano-performance-programs-at-university-level-institutions-in-china-and-the-united.pdf:pdf},
issn = {1744795X},
journal = {International Journal of Music Education},
keywords = {Faculty members,higher education,piano performance programs,students},
number = {3},
pages = {378--391},
title = {{A comparative study of piano performance programs at university-level institutions in China and the United States: From current faculty members' and students' perspectives}},
volume = {40},
year = {2022}
}
@article{Higgins,
abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce $\beta$-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter $\beta$ that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that $\beta$-VAE with appropriately tuned $\beta$ > 1 qualitatively outperforms VAE ($\beta$ = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, $\beta$-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter $\beta$, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
title = {{Β-VAE: Learning basic visual concepts with a constrained variational framework}},
year = {2017}
}
@article{Bj,
author = {Bj, Otso},
file = {:Users/huanzhang/Downloads/61.pdf:pdf},
title = {{SIATEC-C : COMPUTATIONALLY EFFICIENT REPEATED PATTERN DISCOVERY IN POLYPHONIC MUSIC}}
}
@misc{Forte2018TheMusic:,
abstract = {Describes and cites examples of pitch-class sets and relations in atonal music. Preface -- Part 1. Pitch-class sets and relations. Pitch combinations -- Pitch-class sets -- Normal order ; the prime forms -- Transpositionally equivalent pc sets -- Inversionally equivalent pc sets -- The list of prime forms; set names -- Intervals of a pc set ; the interval content -- Non-equivalent pc sets with identical vectors -- The subsets of a pc set -- Invariant subsets under transposition -- Invariant subsets under inversion -- Similarity relations -- Order relations -- The complement of a pc set -- Segmentation -- Part 2. Pitch-class Sct complexes. Introduction -- The set complex K -- The subcomplex Kh -- Set-complex sizes -- The closure property -- Invariance within the set complex -- Similarity relations within the set complex -- Set-complex structures of small scale -- Set-complex structures of larger scale -- Appendix 1. Prime forms and vectors of pitch-class sets -- Appendix 2. Similarity relations -- Appendix 3. The subcomplexes Kh -- Glossary of technical terms -- References -- Index 1. Musical examples -- Index 2. Pc sets in musical examples -- Index 3. General index.},
author = {Forte, Allen},
booktitle = {Music Analysis in Theory and Practice},
doi = {10.2307/j.ctt1xp3t38.20},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Forte - 2018 - The Structure of Atonal Music.pdf:pdf},
pages = {173--185},
title = {{The Structure of Atonal Music:}},
year = {2018}
}
@article{Wu2023,
abstract = {We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets.},
archivePrefix = {arXiv},
arxivId = {2304.11029},
author = {Wu, Shangda and Yu, Dingyao and Tan, Xu and Sun, Maosong},
eprint = {2304.11029},
file = {:Users/huanzhang/Downloads/2304.11029.pdf:pdf},
title = {{CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval}},
url = {http://arxiv.org/abs/2304.11029},
year = {2023}
}
@book{Briot2017DeepSurvey,
abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
archivePrefix = {arXiv},
arxivId = {1709.01620},
author = {Briot, Jean-Pierre and Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois-David},
eprint = {1709.01620},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Briot, Hadjeres, Pachet - 2017 - Deep Learning Techniques for Music Generation -- A Survey.pdf:pdf},
isbn = {9783319701622},
title = {{Deep Learning Techniques for Music Generation -- A Survey}},
url = {http://arxiv.org/abs/1709.01620},
year = {2017}
}
@techreport{Li,
abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimiz-ers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
title = {{Visualizing the Loss Landscape of Neural Nets}},
url = {https://github.com/tomgoldstein/loss-landscape}
}
@article{Duan2011,
abstract = {We present a novel online audio-score alignment approach for multi-instrument polyphonic music. This approach uses a 2-dimensional state vector to model the underlying score position and tempo of each time frame of the audio performance. The process model is defined by dynamic equations to transition between states. Two representations of the observed audio frame are proposed, resulting in two observation models: a multi-pitch-based and a chroma-based. Particle filtering is used to infer the hidden states from observations. Experiments on 150 music pieces with polyphony from one to four show the proposed approach outperforms an existing offline global string alignment-based score alignment approach. Results also show that the multi-pitch-based observation model works better than the chroma-based one. {\textcopyright} 2011 IEEE.},
annote = {From Duplicate 1 (A state space model for online polyphonic audio-score alignment - Duan, Zhiyao; Pardo, Bryan)

the probabiliy of observing this audio frame under (x_n, v_n) given the set of pitches indicated by the score at position x_n},
author = {Duan, Zhiyao and Pardo, Bryan},
doi = {10.1109/ICASSP.2011.5946374},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Duan, Pardo - 2011 - A state space model for online polyphonic audio-score alignment.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Score following,audio-score alignment,hidden Markov model,online algorithm,realtime},
number = {November 2015},
pages = {197--200},
title = {{A state space model for online polyphonic audio-score alignment}},
year = {2011}
}
@article{Lin2023,
abstract = {Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and training on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls, and we illustrate the control power via chords and rhythms, two of the most salient features of music audio. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and style transfer. Our source codes and demos are available online.},
archivePrefix = {arXiv},
arxivId = {2310.17162},
author = {Lin, Liwei and Xia, Gus and Jiang, Junyan and Zhang, Yixiao},
eprint = {2310.17162},
file = {:Users/huanzhang/Downloads/2310.17162.pdf:pdf},
pages = {1--10},
title = {{Content-based Controls For Music Large Language Modeling}},
url = {http://arxiv.org/abs/2310.17162},
year = {2023}
}
@article{,
file = {:Users/huanzhang/Downloads/153 Submission.pdf:pdf},
title = {{ADVERSARIAL-MIDIBERT : SYMBOLIC MUSIC UNDERSTANDING MODEL BASED ON UNBIAS PRE-TRAINING AND MASK FINE-TUNING Zijian Zhao School of Computer Science and Engineering , Sun Yat-sen University , Guangzhou , China}}
}
@article{Wang2018a,
author = {Wang, Cheng-i and Tzanetakis, George},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2018 - SINGING STYLE INVESTIGATION BY RESIDUAL SIAMESE CONVOLUTIONAL NEURAL NETWORKS Cheng-i Wang University of California , S.pdf:pdf},
isbn = {9781538646588},
pages = {116--120},
title = {{SINGING STYLE INVESTIGATION BY RESIDUAL SIAMESE CONVOLUTIONAL NEURAL NETWORKS}},
year = {2018}
}
@article{Thompson2003EvaluatingTool,
abstract = {Much applied research into musical performance requires a method of quantifying differences and changes between performances; for this pur- pose, researchers have commonly used performance assessment schemes taken from educational contexts. This article considers some conceptual and practical problems with using judgments of performance quality as a research tool. To illustrate some of these, data are reported from a study in which three experienced evaluators watched performances given by students at the Royal College of Music, London, and assessed them according to a marking scheme based on that of the Associated Board of the Royal Schools of Music. Correlations between evaluators were only moderate, and some evidence of bias according to the evaluators' own instrumental experience was found. Strong positive correlations were found between items on the assessment scheme, indicating an extremely limited range of discrimination between categories. Implications for the use of similar assessment systems as dependent measures in empirical work are discussed, and suggestions are made for developing scales with greater utility in such work.},
author = {Thompson, Sam and Williamon, Aaron},
file = {:Users/huanzhang/Downloads/mp.2003.21.1.21.pdf:pdf},
journal = {Music Perception: An Interdisciplinary Journal},
number = {1},
pages = {21--41},
title = {{Evaluating Evaluation: Musical Performance Assessment as a Research Tool}},
volume = {21},
year = {2003}
}
@article{Hamilton,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216v4},
author = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216v4},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hamilton, Ying, Leskovec - Unknown - Inductive Representation Learning on Large Graphs.pdf:pdf},
title = {{Inductive Representation Learning on Large Graphs}}
}
@article{Kim2018Sample-LevelWaveforms,
abstract = {Recent work has shown that the end-to-end approach using convolutional neural network (CNN) is effective in various types of machine learning tasks. For audio signals, the approach takes raw waveforms as input using an 1-D convolution layer. In this paper, we improve the 1-D CNN architecture for music auto-tagging by adopting building blocks from state-of-the-art image classification models, ResNets and SENets, and adding multi-level feature aggregation to it. We compare different combinations of the modules in building CNN architectures. The results show that they achieve significant improvements over previous state-of-the-art models on the MagnaTagATune dataset and comparable results on Million Song Dataset. Furthermore, we analyze and visualize our model to show how the 1-D CNN operates.},
archivePrefix = {arXiv},
arxivId = {1710.10451},
author = {Kim, Taejun and Lee, Jongpil and Nam, Juhan},
doi = {10.1109/ICASSP.2018.8462046},
eprint = {1710.10451},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Lee, Nam - 2018 - Sample-Level CNN Architectures for Music Auto-Tagging Using Raw Waveforms.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Convolutional neural networks,Multi-level learning,Music auto-tagging,Raw waveforms},
pages = {366--370},
title = {{Sample-Level CNN Architectures for Music Auto-Tagging Using Raw Waveforms}},
volume = {2018-April},
year = {2018}
}
@inproceedings{VanHerwaarden2014PredictingNetworks,
abstract = {This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these features are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a B{\"{o}}sendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by different pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach differs from earlier work in a number of ways: (1) an additional input representation based on a local history of velocity values is used, (2) the RBMs are trained to result in a network with sparse activations, (3) network connectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance.},
author = {van Herwaarden, Sam and Grachten, Maarten and de Haas, W and {Bas de Haas}, W.},
booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Van Herwaarden, Grachten, Bas De Haas - Unknown - PREDICTING EXPRESSIVE DYNAMICS IN PIANO PERFORMANCES USING NEURAL NETWORKS.pdf:pdf},
title = {{Predicting expressive dynamics in piano performances using neural networks}},
year = {2014}
}
@inproceedings{Aljanaki2018Data-drivenModeling,
address = {Paris, France},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.04903v1},
author = {Aljanaki, Anna},
booktitle = {Proceedings of the 19th International Society for Music Information Retrieval Conference, (ISMIR)},
eprint = {arXiv:1806.04903v1},
file = {:Users/huanzhang/Downloads/1806.04903.pdf:pdf},
title = {{A data-driven approach to mid-level perceptual musical feature modeling}},
year = {2018}
}
@article{Waddell2018,
author = {Waddell, George and Perkins, Rosie and Williamon, Aaron},
file = {:Users/huanzhang/Downloads/Making_an_Impression_Error_Location_and_Repertoire.pdf:pdf},
keywords = {continuous mea-,errors,evaluation,familiarity,performance,sures},
pages = {60--76},
title = {{MAKING AN IMPRESSION:ERROR LOCATION AND REPERTOIRE FEATURES AFFECT PERFORMANCE QUALITY RATING PROCESSES}},
year = {2018}
}
@article{Morijiri2022,
abstract = {In common with other professional musicians, self-evaluation of practise and performance is an integral part of a pianist's professional life. They will also have opportunities to listen to and evaluate the performances of others based on their own criteria. These self-constructed perspectives towards to a piano performance will have an influence on both self-evaluation and external evaluation, but whether differently or similarly is not known. Consequently, this research study aimed to explore how judgements on the perceived quality of a performance are undertaken by professional standard pianists and what criteria are applied, both with regards their own performances as well as the performance of others. Participants were six professional pianists (3 men, 3 women) who were based in the United Kingdom (Mean age = 31.5 years old. SD = 5.1). They were asked to play individually six trials of a piece of R. Schumann's “Tr{\"{a}}umerei” Op. 15 No. 7 in a hired hall for recordings. Then, within 2 months, each participant was asked to come to a self-evaluation session to listen to and evaluate their own six recordings, using a Triadic method as a Repertory Grid. For the external evaluation focused session, the participants were asked to return again to evaluate a further six recordings made up of ‘best' recordings as selected by each participant from their own individual self-evaluations. Analyses of the resultant data suggest that there was no significant difference between the participants in their overall ratings in the external phase, but that self-evaluation showed significant individual differences amongst several participants. The performance criteria in both self-evaluation and external evaluation predominately overlapped with each other in terms of musical factors, such as tone quality, phrasing, and pedalling. The ranking of the performances was highly correlated with perceptions of overall flow, tone quality and pedalling. It appears that pianists apply similar criteria to decide performance quality when evaluating their own performances as well as others.},
author = {Morijiri, Yuki and Welch, Graham F.},
doi = {10.3389/fpsyg.2022.954261},
file = {:Users/huanzhang/Downloads/fpsyg-13-954261.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {external-evaluation,performance criteria,pianists,piano performance,self-evaluation},
number = {November},
title = {{Decisions on the quality of piano performance: Evaluation of self and others}},
volume = {13},
year = {2022}
}
@techreport{Bryan2011,
abstract = {Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results. {\textcopyright} 2011 International Society for Music Information Retrieval.},
annote = {From Duplicate 1 (Musical influence network analysis and rank of sample-based music - Bryan, Nicholas J.; Wang, Ge)

Whosampled dataset, artist-to-artist relationships, specifying origins of borrowed sample material

Network analysis
- degree distributions
- influence measures
- higher level: collapse-and-sum influence rank},
author = {Bryan, Nicholas J. and Wang, Ge},
booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bryan, Wang - 2011 - MUSICAL INFLUENCE NETWORK ANALYSIS AND RANK OF SAMPLE-BASED MUSIC.pdf:pdf},
isbn = {9780615548654},
pages = {329--334},
title = {{Musical influence network analysis and rank of sample-based music}},
year = {2011}
}
@article{David2016ComputationalSeparation,
abstract = {This book provides an in-depth introduction and overview of current research in computational music analysis. Its seventeen chapters, written by leading researchers, collectively represent the diversity as well as the technical and philosophical sophistication of the work being done today in this intensely interdisciplinary field. A broad range of approaches are presented, employing techniques originating in disciplines such as linguistics, information theory, information retrieval, pattern recognition, machine learning, topology, algebra and signal processing. Many of the methods described draw on well-established theories in music theory and analysis, such as Forte's pitch-class set theory, Schenkerian analysis, the methods of semiotic analysis developed by Ruwet and Nattiez, and Lerdahl and Jackendoff's Generative Theory of Tonal Music. The book is divided into six parts, covering methodological issues, harmonic and pitch-class set analysis, form and voice-separation, grammars and hierarchical reduction, motivic analysis and pattern discovery and, finally, classification and the discovery of distinctive patterns. As a detailed and up-to-date picture of current research in computational music analysis, the book provides an invaluable resource for researchers, teachers and students in music theory and analysis, computer science, music information retrieval and related disciplines. It also provides a state-of-the-art reference for practitioners in the music technology industry.},
author = {David, Meredith},
doi = {10.1007/978-3-319-25931-4},
file = {:Users/huanzhang/Downloads/WeydeDeValk.pdf:pdf},
isbn = {9783319259314},
journal = {Computational Music Analysis},
number = {October},
pages = {1--480},
title = {{Computational music analysis - Chord and Note-Based Approaches to Voice Separation}},
year = {2016}
}
@article{Zhang2023b,
abstract = {Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%.},
archivePrefix = {arXiv},
arxivId = {2310.14044},
author = {Zhang, Jincheng and Tang, Jingjing and Saitis, Charalampos and Fazekas, Gy{\"{o}}rgy},
eprint = {2310.14044},
file = {:Users/huanzhang/Downloads/2310.14044.pdf:pdf},
keywords = {Index Terms-Deep learning,composer style,diffusion models,sym-bolic music generation},
title = {{Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models}},
url = {https://arxiv.org/abs/2310.14044v1},
year = {2023}
}
@techreport{Grachten2019PartituraData,
author = {Grachten, Maarten and Cancino-Chac{\'{o}}n, Carlos Eduardo and Gadermaier, Thassilo},
booktitle = {ISMIR 2019 Late-Breaking Demo},
file = {:Users/huanzhang/Downloads/000013.pdf:pdf},
pages = {4--5},
title = {{Partitura: A Python Package for Handling Symbolic Music Data}},
year = {2019}
}
@article{Yi2024,
abstract = {AI-based music generation has progressed significantly in recent years. However, creating symbolic music that is both long-structured and expressive remains a considerable challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving musical diversity. The proposed model has been evaluated using the Maestro dataset and has demonstrated improvements in generating music of conventional length with expressive nuances. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io},
archivePrefix = {arXiv},
arxivId = {2411.08307},
author = {Yi, Yungang and Li, Weihua and Kuo, Matthew and Bai, Quan},
eprint = {2411.08307},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Yi et al. - 2024 - PerceiverS A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation.pdf:pdf},
month = {nov},
title = {{PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation}},
url = {http://arxiv.org/abs/2411.08307},
year = {2024}
}
@inproceedings{Hung2019MusicalRepresentations,
abstract = {Music creation involves not only composing the different parts (e.g., melody, chords) of a musical work but also arranging/selecting the instruments to play the different parts. While the former has received increasing attention, the latter has not been much investigated. This paper presents, to the best of our knowledge, the first deep learning models for rearranging music of arbitrary genres. Specifically, we build encoders and decoders that take a piece of polyphonic musical audio as input, and predict as output its musical score. We investigate disentanglement techniques such as adversarial training to separate latent factors that are related to the musical content (pitch) of different parts of the piece, and that are related to the instrumentation (timbre) of the parts per short-time segment. By disentangling pitch and timbre, our models have an idea of how each piece was composed and arranged. Moreover, the models can realize “composition style transfer” by rearranging a musical piece without much affecting its pitch content. We validate the effectiveness of the models by experiments on instrument activity detection and composition style transfer. To facilitate follow-up research, we open source our code at https://github.com/biboamy/instrument-disentangle.},
archivePrefix = {arXiv},
arxivId = {1905.13567},
author = {Hung, Yun Ning and Chiang, I. Tung and Chen, Yi An and Yang, Yi Hsuan},
booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.24963/ijcai.2019/652},
eprint = {1905.13567},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hung et al. - 2019 - Musical Composition Style Transfer via Disentangled Timbre Representations.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
keywords = {Machine Learning: Adversarial Machine Learning,Machine Learning: Deep Learning,Machine Learning: Transfer & Adaptation & Multi-ta,Multidisciplinary Topics and Applications: Art and,Multidisciplinary Topics and Applications: Informa},
title = {{Musical composition style transfer via disentangled timbre representations}},
url = {https://www.youtube.com/watch?v=buXqNqBFd6E},
year = {2019}
}
@article{Lisena2022,
abstract = {An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale. Recently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good comparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music, avoiding feature engineering and producing highly scalable and reusable models with low dimensionality. Our proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.},
author = {Lisena, Pasquale and Mero{\~{n}}o-Pe{\~{n}}uela, Albert and Troncy, Rapha{\"{e}}l},
doi = {10.3233/SW-210446},
issn = {22104968},
journal = {Semantic Web},
keywords = {Music,graph embeddings,metadata,metadata prediction,neural networks},
number = {3},
pages = {357--377},
title = {{MIDI2vec: Learning MIDI embeddings for reliable prediction of symbolic music metadata}},
volume = {13},
year = {2022}
}
@article{Sales2023InteractiveEngagement,
abstract = {Music scholars and enthusiasts have long been engaged with both performance recordings and musical scores, but inconveniently, these two closely connected mediums are usually stored separately. Currently, digital music libraries tend to have fairly traditional user interfaces for browsing music recordings, and more importantly, performance recordings are organized separately from their musical scores. In recent years, however, the same technological advances that have made vast troves of sound recordings and musical scores more widely available have also created tremendous potential for innovative new interfaces that can facilitate enhanced engagement with the music. In this paper, we present a web-based prototype tool that allows users to navigate classical piano recordings interactively in conjunction with their scores. We describe the technologies involved, and provide access to the actual website. Our pilot testing results are very positive, confirming the usefulness and potential of such a tool, especially in the areas of music education and scholarly research. We also discuss future development of this prototype.},
author = {Sales, Caitlin and Wang, Peiyi and Jiang, Yucong},
doi = {10.1145/3616195.3616212},
file = {:Users/huanzhang/Downloads/3616195.3616212.pdf:pdf},
isbn = {9798400708183},
journal = {ACM International Conference Proceeding Series},
keywords = {audio-to-score alignment,digital music libraries,piano music performance,web-based interaction design},
pages = {30--33},
title = {{An Interactive Tool for Exploring Score-Aligned Performances: Opportunities for Enhanced Music Engagement}},
year = {2023}
}
@article{Gupta2018AQuality,
abstract = {Human experts evaluate singing quality based on many perceptual parameters such as intonation, rhythm, and vibrato, with reference to music theory. We proposed previously the Perceptual Evaluation of Singing Quality (PESnQ) framework that incorporated acoustic features related to these perceptual parameters in combination with the cognitive modeling concept of the telecommunication standard Perceptual Evaluation of Speech Quality to evaluate singing quality. In this study, we present further the study of the PESnQ framework to approximate the human judgments. First, we find that a linear combination of the individual perceptual parameter human scores can predict their overall singing quality judgment. This provides us with a human parametric judgment equation. Next, the prediction of the individual perceptual parameter scores from the PESnQ acoustic features show a high correlation with the respective human scores, which means more meaningful feedback to learners. Finally, we compare the performance of early fusion and late fusion of the acoustic features in predicting the overall human scores. We find that the late fusion method is superior to that of the early fusion method. This work underlines the importance of modeling human perception in automatic singing quality assessment.},
author = {Gupta, Chitralekha and Li, Haizhou and Wang, Ye},
doi = {10.1017/ATSIP.2018.10},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Li, Wang - 2018 - A technical framework for automatic perceptual evaluation of singing quality.pdf:pdf},
issn = {20487703},
journal = {APSIPA Transactions on Signal and Information Processing},
keywords = {Automatic Evaluation,Human Perception,Perceptual Evaluation of Singing Quality,Singing Vocal},
number = {2018},
title = {{A technical framework for automatic perceptual evaluation of singing quality}},
volume = {7},
year = {2018}
}
@article{Kazemi2019StyleNetworks,
abstract = {Disentangling factors of variation within data has become a very challenging problem for image generation tasks. Current frameworks for training a Generative Adversarial Network (GAN), learn to disentangle the representations of the data in an unsupervised fashion and capture the most significant factors of the data variations. However, these approaches ignore the principle of content and style disentanglement in image generation, which means their learned latent code may alter the content and style of the generated images at the same time. This paper describes the Style and Content Disentangled GAN (SC-GAN), a new unsupervised algorithm for training GANs that learns disentangled style and content representations of the data. We assume that the representation of an image can be decomposed into a content code that represents the geometrical information of the data, and a style code that captures textural properties. Consequently, by fixing the style portion of the latent representation, we can generate diverse images in a particular style. Reversely, we can set the content code and generate a specific scene in a variety of styles. The proposed SC-GAN has two components: a content code which is the input to the generator, and a style code which modifies the scene style through modification of the Adaptive Instance Normalization (AdaIN) layers' parameters. We evaluate the proposed SC-GAN framework on a set of baseline datasets.},
archivePrefix = {arXiv},
arxivId = {1811.05621},
author = {Kazemi, Hadi and Iranmanesh, Seyed Mehdi and Nasrabadi, Nasser M.},
doi = {10.1109/WACV.2019.00095},
eprint = {1811.05621},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kazemi, Iranmanesh, Nasrabadi - Unknown - Style and Content Disentanglement in Generative Adversarial Networks.pdf:pdf},
isbn = {9781728119755},
journal = {Proceedings of the 2019 IEEE Winter Conference on Applications of Computer Vision, WACV},
title = {{Style and content disentanglement in generative adversarial networks}},
year = {2019}
}
@article{Grindlay2006ModelingModels,
abstract = {Trained musicians intuitively produce expressive variations that add to their audi-ence's enjoyment. However, there is little quantitative information about the kinds of strategies used in different musical contexts. Since the literal synthesis of notes from a score is bland and unappealing, there is an opportunity for learning systems that can automatically produce compelling expressive variations. The ESP (Expressive Synthetic Performance) system generates expressive renditions using hierarchical hidden Markov models trained on the stylistic variations employed by human performers. Furthermore, the generative models learned by the ESP system provide insight into a number of musicological issues related to expressive performance.},
author = {Grindlay, Graham and Helmbold, David},
doi = {10.1007/s10994-006-8751-3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Grindlay, Helmbold - 2006 - Modeling, analyzing, and synthesizing expressive piano performance with graphical models.pdf:pdf},
journal = {Mach Learn},
keywords = {Graphical models,Hierarchical hidden,Markov models,Music performance,Musical information retrieval},
pages = {361--387},
title = {{Modeling, analyzing, and synthesizing expressive piano performance with graphical models}},
volume = {65},
year = {2006}
}
@article{Li2023,
abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
archivePrefix = {arXiv},
arxivId = {2301.12597},
author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
eprint = {2301.12597},
file = {:Users/huanzhang/Downloads/2301.12597.pdf:pdf},
title = {{BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}},
url = {http://arxiv.org/abs/2301.12597},
year = {2023}
}
@article{Friberg2008,
author = {Friberg, Anders and Bresin, Roberto and Fryd{\'{e}}n, Lars and Sundberg, Johan},
doi = {10.1080/09298219808570749},
file = {:Users/huanzhang/Downloads/Musical punctuation on the microlevel Automatic identification and performance of small melodic units.pdf:pdf},
issn = {1744-5027},
journal = {Journal of New Music Research},
title = {{Musical punctuation on the microlevel: Automatic identification and performance of small melodic units}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=nnmr20},
year = {2008}
}
@article{Tobudic2006RelationalMusic,
abstract = {It is well known that many hard tasks considered in machine learning and data mining can be solved in a rather simple and robust way with an instance- and distance-based approach. In this work we present another difficult task: learning, from large numbers of complex performances by concert pianists, to play music expressively. We model the problem as a multi-level decomposition and prediction task. We show that this is a fundamentally relational learning problem and propose a new similarity measure for structured objects, which is built into a relational instance-based learning algorithm named DISTALL. Experiments with data derived from a substantial number of Mozart piano sonata recordings by a skilled concert pianist demonstrate that the approach is viable. We show that the instance-based learner operating on structured, relational data outperforms a propositional k-NN algorithm. In qualitative terms, some of the piano performances produced by DISTALL after learning from the human artist are of substantial musical quality; one even won a prize in an international 'computer music performance' contest. The experiments thus provide evidence of the capabilities of ILP in a highly complex domain such as music.},
author = {Tobudic, Asmir and Widmer, Gerhard},
doi = {10.1007/s10994-006-8260-4},
file = {:Users/huanzhang/Downloads/Tobudic-Widmer2006_Article_RelationalIBLInClassicalMusic.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Music,Relational instance-based learning},
number = {1-3},
pages = {5--24},
title = {{Relational {IBL} in classical music}},
volume = {64},
year = {2006}
}
@inproceedings{Kim2020DeepRepresentation,
abstract = {In this study, we train deep neural networks to classify composer on a symbolic domain. The model takes a two-channel two-dimensional input, i.e., onset and note activations of time-pitch representation, which is converted from MIDI recordings and performs a single-label classification. On the experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for the classification of 13$\sim$classical composers.},
archivePrefix = {arXiv},
arxivId = {2010.00823},
author = {Kim, Sunghyeon and Lee, Hyeyoon and Park, Sunjong and Lee, Jinho and Choi, Keunwoo},
booktitle = {International Society for Music Information Retrieval (ISMIR) Late Breaking Demo (LBD)},
eprint = {2010.00823},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - Unknown - Deep Composer Classification Using Symbolic Representation.pdf:pdf},
title = {{Deep Composer Classification Using Symbolic Representation}},
url = {http://arxiv.org/abs/2010.00823},
year = {2020}
}
@phdthesis{Goebl2003,
abstract = {Goebl, W. (2003). The role of timing and intensity in the production and perception of melody in expressive piano performance (Doctoral thesis). Karl-Franzens-Universit{\"{a}}t Graz, Graz, Austria. Retrieved from http://iwk.mdw.ac.at/ goebl},
author = {Goebl, Werner},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl - 2003 - The Role of Timing and Intensity in the Production and Perception of Melody in Expressive Piano Performance.pdf:pdf},
title = {{The Role of Timing and Intensity in the Production and Perception of Melody in Expressive Piano Performance}},
year = {2003}
}
@article{Ese2020THTranscription,
author = {Ese, T H and Doctorat, D E},
file = {:Users/huanzhang/Downloads/96680_FOSCARIN_2020_archivage.pdf:pdf},
title = {{TH ESE The Musical Score : a challenging goal for automatic music transcription}},
year = {2020}
}
@article{Palmer1996a,
author = {Palmer, Caroline},
file = {:Users/huanzhang/Downloads/40285708 (1).pdf:pdf},
number = {1},
pages = {23--56},
title = {{On the Assignment of Structure in Music Performance}},
volume = {14},
year = {1996}
}
@inproceedings{Wang2019ContinualReplay,
abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4% of the size of all previous training data matches the performance of refining the classifier keeping 20% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
archivePrefix = {arXiv},
arxivId = {1906.00654},
author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2019.8937236},
eprint = {1906.00654},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2019 - Continual Learning of New Sound Classes using Generative Replay.pdf:pdf},
isbn = {9781728111230},
issn = {19471629},
keywords = {Sound classification,continual learning,generative replay,neural networks},
month = {jun},
title = {{Continual learning of new sound classes using generative replay}},
url = {http://arxiv.org/abs/1906.00654},
volume = {2019-Octob},
year = {2019}
}
@article{Oramas2016a,
abstract = {The rate at which information about music is being created and shared on the web is growing exponentially. However, the challenge of making sense of all this data remains an open problem. In this paper, we present and evaluate an Information Extraction pipeline aimed at the construction of a Music Knowledge Base. Our approach starts off by collecting thousands of stories about songs from the songfacts.com website. Then, we combine a state-of-the-art Entity Linking tool and a linguistically motivated rule-based algorithm to extract semantic relations between entity pairs. Next, relations with similar semantics are grouped into clusters by exploiting syntactic dependencies. These relations are ranked thanks to a novel confidence measure based on statistical and linguistic evidence. Evaluation is carried out intrinsically, by assessing each component of the pipeline, as well as in an extrinsic task, in which we evaluate the contribution of natural language explanations in music recommendation. We demonstrate that our method is able to discover novel facts with high precision, which are missing in current generic as well as music-specific knowledge repositories.},
author = {Oramas, Sergio and Espinosa-Anke, Luis and Sordo, Mohamed and Saggion, Horacio and Serra, Xavier},
doi = {10.1016/J.DATAK.2016.06.001},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Oramas et al. - 2016 - Information extraction for knowledge base construction in the music domain.pdf:pdf},
issn = {0169-023X},
journal = {Data & Knowledge Engineering},
keywords = {Entity linking,Knowledge base construction,Music recommendation,Relation extraction,Semantic web},
month = {nov},
pages = {70--83},
publisher = {North-Holland},
title = {{Information extraction for knowledge base construction in the music domain}},
volume = {106},
year = {2016}
}
@article{Zakka,
author = {Zakka, Kevin and Smith, Laura and Gileadi, Nimrod and Howell, Taylor and Bin, Xue and Singh, Sumeet and Tassa, Yuval and Florence, Pete and Zeng, Andy and Abbeel, Pieter and Berkeley, U C},
file = {:Users/huanzhang/Downloads/robopianist.pdf:pdf},
number = {ii},
title = {{ROBOPIANIST : A Benchmark for High-Dimensional Robot Control}}
}
@inproceedings{Goebl2001AnalysisSonatas,
abstract = {This preliminary study investigates the relationship between tempo indications in a score and the tempo performed by musicians. As several 18 th century theorists point out, the chosen tempo should depend not only on the tempo indication, but also on other factors such as the time signature and the fastest note values to be played. It is examined whether the four main tempo indications (Adagio, Andante, Allegro, and Presto) imply specific tempo classes which can be found in professional performances or whether other factors influence the choice of performed tempo. In Experiment I, 34 movements of Mozart sonatas performed by one professional pianist are analysed. The mode of the inter-beat interval distribution of a performance is considered to be a representation of the performed tempo. The tempo values depend on what is taken as the beat level; performed tempo did not group according to tempo indications. Event density (score events per second) is found to separate the data into just two clusters, namely slow and fast movements. In experiment II, the tempo values of 12 movements of the first experiment were derived from commercial recordings (Barenboim, Pires, Schiff, Uchida) with the help of an interactive beat tracking system. The pianists' tempos are surprisingly homogenous; they deviate from each other more in slower movements than in the faster ones.},
author = {Goebl, Werner and Dixon, Simon},
booktitle = {Proceedings of the VII International Symposium on Systematic and Comparative Musicology and III International Conference on Cognitive Musicology},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Goebl, Dixon - 2001 - Analysis of tempo classes in performances of Mozart sonatas Achieving Togetherness in Ensemble Performance View pr.pdf:pdf},
pages = {65--76},
title = {{Analysis of tempo classes in performances of Mozart sonatas}},
year = {2001}
}
@article{Brunner2018SymbolicCycleGAN,
abstract = {Deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have recently been applied to style and domain transfer for images, and in the case of VAEs, music. GAN-based models employing several generators and some form of cycle consistency loss have been among the most successful for image domain transfer. In this paper we apply such a model to symbolic music and show the feasibility of our approach for music genre transfer. Evaluations using separate genre classifiers show that the style transfer works well. In order to improve the fidelity of the transformed music, we add additional discriminators that cause the generators to keep the structure of the original music mostly intact, while still achieving strong genre transfer. Visual and audible results further show the potential of our approach. To the best of our knowledge, this paper represents the first application of GANs to symbolic music domain transfer.},
archivePrefix = {arXiv},
arxivId = {1809.07575},
author = {Brunner, Gino and Wang, Yuyi and Wattenhofer, Roger and Zhao, Sumu},
doi = {10.1109/ICTAI.2018.00123},
eprint = {1809.07575},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Brunner et al. - Unknown - Symbolic Music Genre Transfer with CycleGAN.pdf:pdf},
isbn = {9781538674499},
issn = {10823409},
journal = {Proceedings of International Conference on Tools with Artificial Intelligence (ICTAI)},
keywords = {Cnn,Cyclegan,Deep learning,Domain,Gan,Genre,Music,Neural networks,Style,Transfer,midi},
title = {{Symbolic music genre transfer with {CycleGAN}}},
url = {https://github.com/sumuzhao/CycleGAN-Music-Style-Transfer},
year = {2018}
}
@article{Povel1985,
abstract = {Studied the perception and reproduction of tone sequences in which only the tone-onset intervals were varied. A theory of the processing of such sequences, partly implemented as a computer program, is presented. A basic assumption of the theory is that perceivers try to generate an internal clock while listening to a temporal pattern. The distribution of accented events perceived in the sequence is supposed to determine whether a clock can be generated internally. It is assumed that if a clock is induced in the perceiver, it will be used as a measuring device to specify the temporal structure of the pattern. The nature of this specification is formalized in a tentative coding model. In Exp I, 24 college students reproduced various temporal patterns that differed structurally in order to test the hypothesis that patterns more readily inducing an internal clock will give rise to more accurate percepts. In Exp II, conducted with the same Ss, clock induction was manipulated to test the clock motion directly. Exp III, conducted with 25 college students, tested the coding portion of the model by correlating theoretical complexity of temporal patterns based on the coding model with complexity judgments. Results support the model.},
annote = {From Duplicate 1 (Perception of Temporal Patterns - Povel, Dirk-Jan; Essens, Peter J)

Week 4},
author = {Povel, Dirk-Jan and Essens, Peter J},
file = {:Users/huanzhang/Downloads/PovelEssens85.pdf:pdf},
issn = {0031-5117},
journal = {Perception},
number = {4},
pages = {411--440},
pmid = {3991313},
title = {{Perception of Temporal Patterns}},
url = {http://www.jstor.org/pss/40285311},
volume = {2},
year = {1985}
}
@book{Piao2024,
author = {Piao, Ziyue and Wanderley, Marcelo M and Verdugo, Felipe},
doi = {10.1007/978-3-031-55312-7},
file = {:Users/huanzhang/Downloads/978-3-031-55312-7_24.pdf:pdf},
isbn = {9783031553127},
keywords = {Digital Musical Interfaces,Electromyography,Multimodal Interaction,Music Pedagogy,STEAM},
number = {March},
publisher = {Springer Nature Switzerland},
title = {{MappEMG : Enhancing Music Pedagogy by Mapping Electromyography to Multimodal Feedback MappEMG : Enhancing Music Pedagogy by Mapping Electromyography to Multimodal Feedback}},
url = {http://dx.doi.org/10.1007/978-3-031-55312-7_24},
year = {2024}
}
@article{Jonason2023,
abstract = {We explore the use of neural synthesis for acoustic guitar from string-wise MIDI input. We propose four different systems and compare them with both objective metrics and subjective evaluation against natural audio and a sample-based baseline. We iteratively develop these four systems by making various considerations on the architecture and intermediate tasks, such as predicting pitch and loudness control features. We find that formulating the control feature prediction task as a classification task rather than a regression task yields better results. Furthermore, we find that our simplest proposed system, which directly predicts synthesis parameters from MIDI input performs the best out of the four proposed systems. Audio examples are available at https://erl-j.github.io/neural-guitar-web-supplement.},
archivePrefix = {arXiv},
arxivId = {2309.07658},
author = {Jonason, Nicolas and Wang, Xin and Cooper, Erica and Juvela, Lauri and Sturm, Bob L. T. and Yamagishi, Junichi},
eprint = {2309.07658},
file = {:Users/huanzhang/Downloads/2309.07658.pdf:pdf},
pages = {1--5},
title = {{DDSP-based Neural Waveform Synthesis of Polyphonic Guitar Performance from String-wise MIDI Input}},
url = {http://arxiv.org/abs/2309.07658},
year = {2023}
}
@article{Rosner2018AutomaticSeparation,
abstract = {The aim of this article is to investigate whether separating music tracks at the pre-processing phase and extending feature vector by parameters related to the specific musical instruments that are characteristic for the given musical genre allow for efficient automatic musical genre classification in case of database containing thousands of music excerpts and a dozen of genres. Results of extensive experiments show that the approach proposed for music genre classification is promising. Overall, conglomerating parameters derived from both an original audio and a mixture of separated tracks improve classification effectiveness measures, demonstrating that the proposed feature vector and the Support Vector Machine (SVM) with Co-training mechanism are applicable to a large dataset.},
author = {Rosner, Aldona and Kostek, Bozena},
doi = {10.1007/s10844-017-0464-5},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Rosner, Kostek - 2018 - Automatic music genre classification based on musical instrument track separation.pdf:pdf},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Automatic music genre classification,Automatic separation of music tracks,Music information retrieval (MIR),Support vector machine (SVM)},
number = {2},
pages = {363--384},
title = {{Automatic music genre classification based on musical instrument track separation}},
volume = {50},
year = {2018}
}
@article{Goebl2010,
author = {Goebl, Werner and Flossmann, Sebastian and Widmer, Gerhard},
file = {:Users/huanzhang/Downloads/comj_a_00002.pdf:pdf},
journal = {Computer Music Journal},
number = {3},
pages = {35--44},
title = {{Investigations of Between-Hand Synchronization in Magaloffs Chopin}},
url = {https://dl.acm.org/doi/10.1162/COMJ_a_00002},
volume = {34},
year = {2010}
}
@techreport{KimLearningNetworks,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
archivePrefix = {arXiv},
arxivId = {1703.05192v2},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192v2},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - Unknown - Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.pdf:pdf},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}}
}
@inproceedings{Radford2023RobustSupervision,
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
archivePrefix = {arXiv},
arxivId = {2212.04356},
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
booktitle = {Proceedings of Machine Learning Research},
eprint = {2212.04356},
file = {:Users/huanzhang/Downloads/2212.04356v1.pdf:pdf},
issn = {26403498},
pages = {28492--28518},
title = {{Robust Speech Recognition via Large-Scale Weak Supervision}},
volume = {202},
year = {2023}
}
@article{Repp1994RelationalStudy,
abstract = {This study addressed the question of whether the expressive microstructure of a music performance remains relationally invariant across moderate (musically acceptable) changes in tempo. Two pianists played Schumann's "Tr{\"{a}}umerei" three times at each of three tempi on a digital piano, and the performance data were recorded in MIDI format. In a perceptual test, musically trained listeners attempted to distinguish the original performances from performances that had been artificially speeded up or slowed down to the same overall duration. Accuracy in this task was barely above chance, suggesting that relational invariance was largely preserved. Subsequent analysis of the MIDI data confirmed that each pianist's characteristic timing patterns were highly similar across the three tempi, although there were statistically significant deviations from perfect relational invariance. The timing of (relatively slow) grace notes seemed relationally invariant, but selective examination of other detailed temporal features (chord asynchrony, tone overlap, pedal timing) revealed no systematic scaling with tempo. Finally, although the intensity profile seemed unaffected by tempo, a slight overall increase in intensity with tempo was observed. Effects of musical structure on expressive microstructure were large and pervasive at all levels, as were individual differences between the two pianists. For the specific composition and range of tempi considered here, these results suggest that major (cognitively controlled) temporal and dynamic features of a performance change roughly in proportion with tempo, whereas minor features tend to be governed by tempo-independent motoric constraints. {\textcopyright} 1994 Springer-Verlag.},
author = {Repp, Bruno H.},
doi = {10.1007/BF00419657},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Repp - 1994 - Relational invariance of expressive microstructure across global tempo changes in music performance An exploratory study.pdf:pdf},
issn = {03400727},
journal = {Psychological Research},
number = {4},
pages = {269--284},
pmid = {8090862},
title = {{Relational invariance of expressive microstructure across global tempo changes in music performance: An exploratory study}},
volume = {56},
year = {1994}
}
@techreport{VanDenOordDeepRecommendation,
abstract = {Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative filtering. However, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the Million Song Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep con-volutional neural networks significantly outperforming the traditional approach.},
author = {{Van Den Oord}, A{\"{a}}ron and Dieleman, Sander and Schrauwen, Benjamin},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Van Den Oord, Dieleman, Schrauwen - Unknown - Deep content-based music recommendation.pdf:pdf},
title = {{Deep content-based music recommendation}}
}
@inproceedings{Zhang2023SymbolicEvaluation,
abstract = {Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language like fashion. However, symbolic music is neither an image nor a sentence, and research in the symbolic domain lacks a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation. Our results suggest that the graph representation, as the newest and least explored among the three approaches, exhibits promising performance, while being more light-weight in training.},
address = {Milan, Italy},
archivePrefix = {arXiv},
arxivId = {2309.02567},
author = {Zhang, Huan and Karystinaios, Emmanouil and Dixon, Simon and Widmer, Gerhard and Cancino-Chac{\'{o}}n, Carlos Eduardo},
booktitle = {Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)},
eprint = {2309.02567},
file = {:Users/huanzhang/01Acdemics/PhD/Research/Reports&Papers/Symbolic_Music_Representations_for_Classification_Tasks__A_Systematic_Evaluation.pdf:pdf},
title = {{Symbolic Music Representations for Classification Tasks: A Systematic Evaluation}},
url = {http://arxiv.org/abs/2309.02567},
year = {2023}
}
@inproceedings{Oramas2017,
abstract = {In this work, we present MEL, the first Music Entity Link-ing system. MEL is able to identify mentions of musi-cal entities (e.g., album, songs, and artists) in free text, and disambiguate them to a music knowledge base, i.e., MusicBrainz. MEL combines different state-of-the-art li-braries and SimpleBrainz, an RDF knowledge base created from MusicBrainz after a simplification process. MEL is released as a REST API and as an online demo Web.},
author = {Oramas, Sergio and Ferraro, Andr{\'{e}}s and Correya, Albin and Serra, Xavier},
booktitle = {Late-Breaking and Demo of 18th International Society for Music Information Retrieval Conference (ISMIR), Suzhou, China,},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Oramas et al. - Unknown - {MEL} A Music Entity Linking System.pdf:pdf},
title = {{{MEL}: a Music Entity Linking System}},
url = {https://ismir2017.smcnus.org/lbds/Oramas2017.pdf},
year = {2017}
}
@techreport{LiuBadThem,
abstract = {Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize explicitly, then SGD can be easily made to converge to poorly-generalizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels. In contrast, we find that in the presence of explicit regularization, pretraining with random labels has no detrimental effect on SGD. We believe that our results give evidence that explicit regularization plays a far more important role in the success of overparameterized neural networks than what has been understood until now. Specifically, by penalizing complicated models independently of their fit to the data, regularization affects training dynamics also far away from optima, making simple models that fit the data well discoverable by local methods, such as SGD.},
author = {Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Papailiopoulos, Achlioptas - Unknown - Bad Global Minima Exist and SGD Can Reach Them.pdf:pdf},
title = {{Bad Global Minima Exist and SGD Can Reach Them}},
url = {https://github.com/chao1224/BadGlobalMinima.}
}
@article{Wang2024ComprehensiveApplication,
abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
archivePrefix = {arXiv},
arxivId = {2302.00487},
author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
doi = {10.1109/TPAMI.2024.3367329},
eprint = {2302.00487},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2023 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Continual learning,catastrophic forgetting,incremental learning,lifelong learning},
month = {jan},
number = {8},
pmid = {38407999},
title = {{A Comprehensive Survey of Continual Learning: Theory, Method and Application}},
url = {http://arxiv.org/abs/2302.00487},
volume = {46},
year = {2024}
}
@article{Belghazi2018MutualEstimation,
abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Belghazi et al. - 2018 - Mutual Information Neural Estimation.pdf:pdf},
isbn = {9781510867963},
journal = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
keywords = {Deep Learning,GAN,Generative Adversarial Networks,ICML,Information Theory,Machine Learning,Neural Networks},
title = {{Mutual information neural estimation}},
year = {2018}
}
@article{Li2018SkeletonPerformance,
abstract = {Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces.},
author = {Li, Bochen and Maezawa, Akira and Duan, Zhiyao},
file = {:Users/huanzhang/Downloads/109_Paper.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
pages = {218--224},
title = {{Skeleton plays piano: Online generation of pianist body movements from MIDI performance}},
year = {2018}
}
@article{Ma2024FoundationSurvey,
abstract = {In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.},
archivePrefix = {arXiv},
arxivId = {2408.14340},
author = {Ma, Yinghao and {\O}land, Anders and Ragni, Anton and {Del Sette}, Bleiz MacSen and Saitis, Charalampos and Donahue, Chris and Lin, Chenghua and Plachouras, Christos and Benetos, Emmanouil and Quinton, Elio and Shatri, Elona and Morreale, Fabio and Zhang, Ge and Fazekas, Gy{\"{o}}rgy and Xia, Gus and Zhang, Huan and Manco, Ilaria and Huang, Jiawen and Guinot, Julien and Lin, Liwei and Marinelli, Luca and Lam, Max W. Y. and Sharma, Megha and Kong, Qiuqiang and Dannenberg, Roger B. and Yuan, Ruibin and Wu, Shangda and Wu, Shih-Lun and Dai, Shuqi and Lei, Shun and Kang, Shiyin and Dixon, Simon and Chen, Wenhu and Huang, Wehhao and Du, Xingjian and Qu, Xingwei and Tan, Xu and Li, Yizhi and Tian, Zeyue and Wu, Zhiyong and Wu, Zhizheng and Ma, Ziyang and Wang, Ziyu},
eprint = {2408.14340},
file = {:Users/huanzhang/Downloads/2408.14340v1.pdf:pdf},
journal = {arXiv:2408.14340},
title = {{Foundation Models for Music: A Survey}},
url = {http://arxiv.org/abs/2408.14340},
year = {2024}
}
@misc{Sloboda1991MusicFindings,
author = {Sloboda, John A.},
file = {:Users/huanzhang/Downloads/Emotion_Sloboda.pdf:pdf},
title = {{Music Structure and Emotional Response: Some Empirical Findings}},
year = {1991}
}
@inproceedings{Wu2023LargeAugmentation,
abstract = {Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K1 and the proposed model 2 are both available to the public.},
archivePrefix = {arXiv},
arxivId = {2211.06687},
author = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP49357.2023.10095969},
eprint = {2211.06687},
file = {:Users/huanzhang/Downloads/2211.06687v4.pdf:pdf},
isbn = {9781728163277},
issn = {15206149},
keywords = {Audio Classification,Audio Dataset,Contrastive Learning,Representation Learning,Text-to-Audio Retrieval},
title = {{Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation}},
year = {2023}
}
@inproceedings{Cramer2019LookEmbeddings,
abstract = {A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L3-Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L3-Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3-Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3-Net embedding model as well as pre-trained models are made freely available online.},
author = {Cramer, Jason and Wu, Ho Hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {In Proceedings of the ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal (ICASSP)},
file = {:Users/huanzhang/Downloads/cramer_looklistenlearnmore_icassp_2019.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
keywords = {Audio classification,deep audio embeddings,deep learning,machine listening,transfer learning},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings,}},
year = {2019}
}
