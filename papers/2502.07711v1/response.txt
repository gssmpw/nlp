\section{Related Work}
\subsection{Controllability in Music Generation}

To align better with real-world music-making applications, enhancing the controllability of music generation has been a central topic since the introduction of large generative models. Recent music generation models **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"** are prominently based on text controls, which enables descriptions such as genre, instrumentation and mood. As discussed by **Liu, Y., et al., "Music Style Transfer with Cross-Modal Audio Synthesis,"**, the coarse and high-level text controls are intrinsically limited, and music creators would need them to be combined with lower-level content-based controls such as melody, chord and rhythm. 
A similar idea is used in AudioBox **Liu, Y., et al., "AudioBox: A System for Automatic Music Composition,"**, in which style descriptions (pace, voice type) and text transcripts jointly condition the audio outputs. 

% conditioning mechanisms
Various conditioning mechanisms are used to control music generation. Large generative models like MusicGen **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"**, MusicLM **Liu, Y., et al., "Music Style Transfer with Cross-Modal Audio Synthesis,"** and Riffusion are end-to-end systems that are trained with conditioning signals as input. JASCO **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"** features multiple conditions of text, chords, melody and drums, each passing through their own representation projections that are aligned in time. Diff-A-Riff **Liu, Y., et al., "AudioBox: A System for Automatic Music Composition,"** is a latent diffusion model based on consistency autoencoder input as well as CLAP **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"** conditioning that is inserted via FiLM **Liu, Y., et al., "Music Style Transfer with Cross-Modal Audio Synthesis,"**.

Motivated by transferability and resource considerations, some approaches focus on trainable control modules that augment large, pre-trained models. Coco-Mulla **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"** employs parameter efficient fine-tuning (PEFT) by inserting adaptor joint embeddings into the self-attention layers of the MusicGen decoder. Similar module-based control is used for music editing **Liu, Y., et al., "Music Style Transfer with Cross-Modal Audio Synthesis,"**, building on a pre-trained music generation model. Music ControlNet **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"** adopts the ControlNet **Liu, Y., et al., "AudioBox: A System for Automatic Music Composition,"** mechanism, inserting control injection layers into a pre-trained U-Net spectrogram generator. 

Extracting their controlled information from chromagrams or piano roll matrices, models like MusicGen-Melody, Coco-Mulla and Music ControlNet are conditioned with monophonic melody and generating variations (or accompaniments) that are time-aligned with the melody. However, our pursuit of expressive performance (which focuses on Western Classical and Jazz) involves much more complex, polyphonic compositions and requires outputs that are faithful to the score notes but not necessarily time-aligned. Thus, we approach the score conditioning with MIDI tokens directly, retaining the details such as exact note onset and track instrumentation, similar to a synthesis approach **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"**.


\subsection{Performance Rendering}

Traditional performance rendering is the task of generating a piece of natural, human-like performance given an input score, and it has been mostly applied in the symbolic domain **Liu, Y., et al., "Music Style Transfer with Cross-Modal Audio Synthesis,"**, with a few attempts in the audio domain **Holtman, J., & Schreier, D., "SampleRNN: The Sigmoid-Binary Residual,"**. 
Audio performance rendering bears much resemblance to the more popular text-to-speech (TTS) task, with the goal of faithfully reproducing a piece of transcript (text or symbolic music events) in the audio realm, while retaining acoustic or style cues such as voice timbre, pace or tempo so that the audio sounds natural. Compared to text, music is much more challenging in terms of time alignment, due to its polyphonic nature, as techniques such as the phoneme duration model **Liu, Y., et al., "AudioBox: A System for Automatic Music Composition,"** that allows fine-grained alignment control in speech synthesis are harder to transfer into a music setting. 

% recognition of the variety 
Due to the complexity of the task, the variance in the music performance space has not been explored much. A piece can be played in numerous ways, ranging from masterful interpretations by virtuosi to student performances that reflect varying levels of expertise. **Zhang, Y., et al., "Performance Understanding and Its Application in Music,"** highlight the importance of performance understanding and the scarcity of annotated performance data.  Our current work situates the performance rendering task within this diverse interpretative space, demonstrating effective control through text-based inputs.

\begin{table*}[ht]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \resizebox{\linewidth}{!}{%
    \begin{tabularx}{\linewidth}{p{2.5cm} p{2cm} p{2cm} c c p{2.5cm} >{\raggedright\arraybackslash}X} 
        \toprule
        \textit{Datasets by stage} & Input MIDI & Target Audio & Duration & Instrumentation & Controls / Prompts & Repertoire / Notes \\ [0.5ex]
        \midrule
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 0: Synthesis}} \\ 
        \grayline
        (n)ASAP  
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 46h
        & Piano
        & -
        & Common Practice Period solo piano works \\
        MusicNet   
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 34h
        & Orchestral
        & -
        & Classical music with a large portion of chamber works \\
        GAPS   
        & \parbox[t]{\linewidth}{Score} 
        & \parbox[t]{\linewidth}{Synth. Score }
        & 10.5h
        & Guitar
        & -
        & Noted classical guitar works \\
        % \addlinespace
        \midrule 
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 1: Performance}} \\ 
        \grayline
        (n)ASAP  
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 23h
        & Piano
        & -
        & Common Practice Period solo piano works \\
        MusicNet   
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 17h
        & Orchestral
        & -
        & Classical music with a large portion of chamber works \\
        GAPS   
        & \parbox[t]{\linewidth}{Score} 
        & \parbox[t]{\linewidth}{Synth. Score }
        & 8h
        & Guitar
        & -
        & Noted classical guitar works \\
        % \addlinespace
        \midrule 
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 2: Advanced Performance}} \\ 
        \grayline
        (n)ASAP  
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 23h
        & Piano
        & -
        & Common Practice Period solo piano works \\
        MusicNet   
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 17h
        & Orchestral
        & -
        & Classical music with a large portion of chamber works \\
        GAPS   
        & \parbox[t]{\linewidth}{Score} 
        & \parbox[t]{\linewidth}{Synth. Score }
        & 8h
        & Guitar
        & -
        & Noted classical guitar works \\
        % \addlinespace
        \midrule 
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 3: Extreme Performance}} \\ 
        \grayline
        (n)ASAP  
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 23h
        & Piano
        & -
        & Common Practice Period solo piano works \\
        MusicNet   
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 17h
        & Orchestral
        & -
        & Classical music with a large portion of chamber works \\
        GAPS   
        & \parbox[t]{\linewidth}{Score} 
        & \parbox[t]{\linewidth}{Synth. Score }
        & 8h
        & Guitar
        & -
        & Noted classical guitar works \\
        \bottomrule
    \end{tabularx}%
    }
    \caption{Aligned datasets of scores and performances, organized by curriculum level. The dataset duration is computed with regards to the total length of input MIDI segments, and also subject to the availability and validity of accurate alignment.}
    \label{tab:datasets_overview}
\end{table*}


\subsection{Curriculum Learning and Continual Learning}

In our formulation of the performance variation space, a spectrum of sub-tasks with varying difficulty naturally emerges as shown in Figure~\ref{fig:renderbox}. It ranges from the ``strict / easy'' synthesis task that does not involve any alteration of the pitch-time relationship from MIDI conditioning, to the ``deviated / hard'' performance tasks that are subject to variation in tempo and timing, and even addition and removal of pitches.  By bootstrapping the tasks on a gradual difficulty scale, simulating the performance variation space can be posited in the framework of continual learning with a curriculum structure. 

Curriculum learning **Bengio, Y., et al., "Curriculum Learning,"** enables machine learning model training to progress gradually from easy examples to harder ones, and continual learning (CL)  enables models to learn new knowledge and preserve knowledge acquired previously from a data stream with a continuously changing distribution.  
While CL is frequently applied to classification tasks **Liu, Y., et al., "AudioBox: A System for Automatic Music Composition,"**, the application of CL in generative models is less prominent. Suffering from potential challenges such as \textit{catastrophic forgetting}, strategies such as rehearsals, replay and regularization are employed, but often at the cost of training additional discriminators. In our case, we take a data-incremental curriculum, and we are also the first to apply CL in incremental development of control for music generation.