\section{Related Work}
\subsection{Controllability in Music Generation}

To align better with real-world music-making applications, enhancing the controllability of music generation has been a central topic since the introduction of large generative models. Recent music generation models ____ are prominently based on text controls, which enables descriptions such as genre, instrumentation and mood. As discussed by ____, the coarse and high-level text controls are intrinsically limited, and music creators would need them to be combined with lower-level content-based controls such as melody, chord and rhythm. 
A similar idea is used in AudioBox ____, in which style descriptions (pace, voice type) and text transcripts jointly condition the audio outputs. 

% conditioning mechanisms
Various conditioning mechanisms are used to control music generation. Large generative models like MusicGen ____, MusicLM ____ and Riffusion\footnote{https://www.riffusion.com/} are end-to-end systems that are trained with conditioning signals as input. JASCO ____ features multiple conditions of text, chords, melody and drums, each passing through their own representation projections that are aligned in time. Diff-A-Riff ____ is a latent diffusion model based on consistency autoencoder input as well as CLAP ____ conditioning that is inserted via FiLM ____.

Motivated by transferability and resource considerations, some approaches focus on trainable control modules that augment large, pre-trained models. Coco-Mulla ____ employs parameter efficient fine-tuning (PEFT) by inserting adaptor joint embeddings into the self-attention layers of the MusicGen decoder. Similar module-based control is used for music editing ____, building on a pre-trained music generation model. Music ControlNet ____ adopts the ControlNet ____ mechanism, inserting control injection layers into a pre-trained U-Net spectrogram generator. 

Extracting their controlled information from chromagrams or piano roll matrices, models like MusicGen-Melody, Coco-Mulla and Music ControlNet are conditioned with monophonic melody and generating variations (or accompaniments) that are time-aligned with the melody. However, our pursuit of expressive performance (which focuses on Western Classical and Jazz) involves much more complex, polyphonic compositions and requires outputs that are faithful to the score notes but not necessarily time-aligned. Thus, we approach the score conditioning with MIDI tokens directly, retaining the details such as exact note onset and track instrumentation, similar to a synthesis approach ____.


\subsection{Performance Rendering}

Traditional performance rendering is the task of generating a piece of natural, human-like performance given an input score, and it has been mostly applied in the symbolic domain ____, with a few attempts in the audio domain ____. 
Audio performance rendering bears much resemblance to the more popular text-to-speech (TTS) task, with the goal of faithfully reproducing a piece of transcript (text or symbolic music events) in the audio realm, while retaining acoustic or style cues such as voice timbre, pace or tempo so that the audio sounds natural. Compared to text, music is much more challenging in terms of time alignment, due to its polyphonic nature, as techniques such as the phoneme duration model ____ that allows fine-grained alignment control in speech synthesis are harder to transfer into a music setting. 

% recognition of the variety 
Due to the complexity of the task, the variance in the music performance space has not been explored much. A piece can be played in numerous ways, ranging from masterful interpretations by virtuosi to student performances that reflect varying levels of expertise. ____ \shortcite{Zhang2024LLaQoAssessment,Zhang2024FromPiano} highlight the importance of performance understanding and the scarcity of annotated performance data.  Our current work situates the performance rendering task within this diverse interpretative space, demonstrating effective control through text-based inputs.

\begin{table*}[ht]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \resizebox{\linewidth}{!}{%
    \begin{tabularx}{\linewidth}{p{2.5cm} p{2cm} p{2cm} c c p{2.5cm} >{\raggedright\arraybackslash}X} 
        \toprule
        \textit{Datasets by stage} & Input MIDI & Target Audio & Duration & Instrumentation & Controls / Prompts & Repertoire / Notes \\ [0.5ex]
        \midrule
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 0: Synthesis}} \\ 
        \grayline
        (n)ASAP  
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 46h
        & Piano
        & -
        & Common Practice Period solo piano works \\
        MusicNet   
        & \parbox[t]{\linewidth}{Score \& \\ Performance} 
        & \parbox[t]{\linewidth}{Synth. Score \& \\ Perf. Audio}
        & 34h
        & Orchestral
        & -
        & Classical music with a large portion of chamber works \\
        GAPS   
        & \parbox[t]{\linewidth}{Score} 
        & \parbox[t]{\linewidth}{Synth. Score }
        & 10.5h
        & Guitar
        & -
        & Noted classical guitar works \\
        % \addlinespace
        \midrule 
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 1: Speed-controlled synthesis}} \\ 
        \grayline     
        (n)ASAP-aug   
        & Score
        & Synth. Score
        & 68.3h 
        & Piano
        & Tempo 
        & Time-varied score synthesis \\
        GAPS-aug  
        & Score
        & Synth. Score
        & 25h 
        & Guitar
        & Tempo 
        & Time-varied score synthesis \\
        MusicNet-aug  
        & Score 
        & Synth. Score
        & 51h 
        & Orchestral
        & Tempo 
        & Time-varied score synthesis \\
        %\addlinespace
        \midrule 
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 2: Expressive Performance}} \\ 
        \grayline        
        (n)ASAP  
        & Score
        & Perf. Audio
        & 48.1h
        & Piano
        & Expressive 
        & Aligned perf. segments \\
        GAPS  
        & Score
        & Perf. Audio
        & 5.3h
        & Guitar
        & Expressive 
        & Aligned perf. segments \\
        In-house-Sax  
        & Score
        & Perf. Audio
        & 2.13h
        & Saxophone
        & Expressive or not %n-expressive playing
        & Jazz standards (2 versions) \\ %with two versions of playing \\
        MusicNet   
        & Score
        & Perf. Audio
        & 30.4h
        & Orchestral
        & Expressive 
        & Aligned perf. segments \\
        BachViolin  
        & Score
        & Perf. Audio
        & 4.4h 
        & Violin
        & Expressive
        & Bach violin sonata  \\
        \midrule
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 3: Mistake-corrupted performance}} \\ 
        \grayline        
        (n)ASAP-synmis  
        & Score
        & Synth. Perf.
        & 36h 
        & Piano
        & Mistakes (no annotation)
        & Synthesized from perf.\ MIDI which is corrupted with artificial mistakes \\
        \midrule
        \rowcolor[gray]{.9}
        \multicolumn{7}{l}{\textit{Stage 4: Style-directioned Performance}} \\ 
        \grayline
        ATEPP-subset  
        & Score
        & Perf. Audio
        & 273h
        & Piano
        & Performer labels 
        & Standard piano repertoire played by 40+ pianists \\
        Con Espressione  
        & Score
        & Perf. Audio
        & 0.5h
        & Piano
        & Human-annotated expression labels 
        & 45 perf.\ of 9 excerpts %, with perceptual 
        annot.\ by 179 participants  \\
        Vienna 4x22  
        & Score
        & Perf. Audio
        & 1.2h 
        & Piano
        & Performer labels 
        & 4 short pieces played by 22 pianists each \\
        \bottomrule
    \end{tabularx}%
    }
    \caption{Aligned datasets of scores and performances, organized by curriculum level. The dataset duration is computed with regards to the total length of input MIDI segments, and also subject to the availability and validity of accurate alignment.}
    \label{tab:datasets_overview}
\end{table*}


\subsection{Curriculum Learning and Continual Learning}

In our formulation of the performance variation space, a spectrum of sub-tasks with varying difficulty naturally emerges as shown in Figure~\ref{fig:renderbox}. It ranges from the ``strict / easy'' synthesis task that does not involve any alteration of the pitch-time relationship from MIDI conditioning, to the ``deviated / hard'' performance tasks that are subject to variation in tempo and timing, and even addition and removal of pitches.  By bootstrapping the tasks on a gradual difficulty scale, simulating the performance variation space can be posited in the framework of continual learning with a curriculum structure. 

Curriculum learning ____ enables machine learning model training to progress gradually from easy examples to harder ones, and continual learning (CL) 
% ____ 
enables models to learn new knowledge and preserve knowledge acquired previously from a data stream with a continuously changing distribution.  
While CL is frequently applied to classification tasks ____, the application of CL in generative models 
% ____
is less prominent. Suffering from potential challenges such as \textit{catastrophic forgetting}, strategies such as rehearsals, replay and regularization are employed, but often at the cost of training additional discriminators. In our case, we take a data-incremental curriculum, and we are also the first to apply CL in incremental development of control for music generation.