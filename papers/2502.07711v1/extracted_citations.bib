@article{Agostinelli2023MusicLMText,
abstract = {We introduce MusicLM, a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.},
archivePrefix = {arXiv},
arxivId = {2301.11325},
author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zal{\'{a}}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
eprint = {2301.11325},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Agostinelli et al. - Unknown - MusicLM Generating Music From Text.pdf:pdf},
journal = {arXiv preprint arXiv:2301.11325},
title = {{MusicLM: Generating Music From Text}},
url = {http://arxiv.org/abs/2301.11325},
year = {2023}
}

@article{Bhatt2024CharacterizingAnalysis,
abstract = {Audio analysis is useful in many application scenarios. The state-of-the-art audio analysis approaches assume the data distribution at training and deployment time will be the same. However, due to various real-life challenges, the data may encounter drift in its distribution or can encounter new classes in the late future. Thus, a one-time trained model might not perform adequately. Continual learning (CL) approaches are devised to handle such changes in data distribution. There have been a few attempts to use CL approaches for audio analysis. Yet, there is a lack of a systematic evaluation framework. In this paper, we create a comprehensive CL dataset and characterize CL approaches for audio-based monitoring tasks. We have investigated the following CL and non-CL approaches: EWC, LwF, SI, GEM, A-GEM, GDumb, Replay, Naive, Cumulative, and Joint training. The study is very beneficial for researchers and practitioners working in the area of audio analysis for developing adaptive models. We observed that Replay achieved better results than other methods in the DCASE challenge data. It achieved an accuracy of 70.12% for the domain incremental scenario and an accuracy of 96.98% for the class incremental scenario.},
archivePrefix = {arXiv},
arxivId = {2407.00465},
author = {Bhatt, Ruchi and Kumari, Pratibha and Mahapatra, Dwarikanath and Saddik, Abdulmotaleb El and Saini, Mukesh},
eprint = {2407.00465},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Bhatt et al. - 2024 - Characterizing Continual Learning Scenarios and Strategies for Audio Analysis.pdf:pdf},
journal = {Arxiv preprint arXiv:2407.00465},
month = {jun},
title = {{Characterizing Continual Learning Scenarios and Strategies for Audio Analysis}},
url = {http://arxiv.org/abs/2407.00465},
year = {2024}
}

@article{Bresin2002DirectorSystem,
abstract = {Director Musices is a program that transforms notated scores into musical performances. It implements the performance rules emerging from research projects at the Royal Institute of Technology (KTH). Rules in the program model performance aspects such as phrasing, articulation, and intonation, and they operate on performance variables such as tone, inter-onset duration, amplitude, and pitch. By manipulating rule parameters, the user can act as a metaperformer controlling different feature of the performance, leaving the technical execution to the computer. Different interpretations of the same piece can easily be obtained. Features of Director Musices include MIDI file input and output, rule palettes, graphical display of all performance variables (along with the notation), and user- defined performance rules. The program is implemented in Common Lisp and is available free as a stand-alone application both for Macintosh and Windows platforms. Further information, including music examples, publications, and the program itself, is located online at http://www.speech.kth.se/music/performance. This paper is a revised and updated version of a previous paper published in the Computer Music Journal in year 2000 that was mainly written by Anders Friberg (Friberg, Colombo, Fryd{\'{e}}n and Sundberg, 2000).},
author = {Bresin, Roberto and Friberg, Anders and Sundberg, Johan},
file = {:Users/huanzhang/Downloads/Director_musices_The_KTH_performance_rules_system.pdf:pdf},
journal = {Special Interest Group on Music and Computer(SIGMUS) - 46 Kyoto},
pages = {43--48},
title = {{Director Musices : The {KTH} Performance Rules System}},
url = {http://www.speech.kth.se/prod/publications/files/873.pdf},
year = {2002}
}

@inproceedings{Copet2023SimpleGeneration,
abstract = {We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft},
archivePrefix = {arXiv},
arxivId = {2306.05284},
author = {Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'{e}}fossez, Alexandre},
booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
eprint = {2306.05284},
file = {:Users/huanzhang/Downloads/2306.05284.pdf:pdf},
issn = {10495258},
title = {{Simple and Controllable Music Generation}},
url = {http://arxiv.org/abs/2306.05284},
year = {2023}
}

@inproceedings{Dong2022DeepSynthesis,
abstract = {Music performance synthesis aims to synthesize a musical score into a natural performance. In this paper, we borrow recent advances in text-to-speech synthesis and present the Deep Performer-a novel system for score-to-audio music performance synthesis. Unlike speech, music often contains polyphony and long notes. Hence, we propose two new techniques for handling polyphonic inputs and providing a fine-grained conditioning in a transformer encoder-decoder model. To train our proposed system, we present a new violin dataset consisting of paired recordings and scores along with estimated alignments between them. We show that our proposed model can synthesize music with clear polyphony and harmonic structures. In a listening test, we achieve competitive quality against the baseline model, a conditional generative audio model, in terms of pitch accuracy, timbre and noise level. Moreover, our proposed model significantly outperforms the baseline on an existing piano dataset in overall quality.},
author = {Dong, Hao-Wen and Zhou, Cong and Berg-Kirkpatrick, Taylor and Mcauley, Julian},
booktitle = {Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - Unknown - Deep Performer Score-to-audio Music Performance Synthesis.pdf:pdf},
keywords = {Audio synthesis,computer music,machine learning,music information retrieval,neural network},
title = {{Deep Performer: Score-to-audio Music Performance Synthesis}},
year = {2022}
}

@article{Faber2023MNISTLearning,
abstract = {Continual learning (CL) is one of the most promising trends in recent machine learning research. Its goal is to go beyond classical assumptions in machine learning and develop models and learning strategies that present high robustness in dynamic environments. This goal is realized by designing strategies that simultaneously foster the incorporation of new knowledge while avoiding forgetting past knowledge. The landscape of CL research is fragmented into several learning evaluation protocols, comprising different learning tasks, datasets, and evaluation metrics. Additionally, the benchmarks adopted so far are still distant from the complexity of real-world scenarios, and are usually tailored to highlight capabilities specific to certain strategies. In such a landscape, it is hard to clearly and objectively assess models and strategies. In this work, we fill this gap for CL on image data by introducing two novel CL benchmarks that involve multiple heterogeneous tasks from six image datasets, with varying levels of complexity and quality. Our aim is to fairly evaluate current state-of-the-art CL strategies on a common ground that is closer to complex real-world scenarios. We additionally structure our benchmarks so that tasks are presented in increasing and decreasing order of complexity—according to a curriculum—in order to evaluate if current CL models are able to exploit structure across tasks. We devote particular emphasis to providing the CL community with a rigorous and reproducible evaluation protocol for measuring the ability of a model to generalize and not to forget while learning. Furthermore, we provide an extensive experimental evaluation showing that popular CL strategies, when challenged with our proposed benchmarks, yield sub-par performance, high levels of forgetting, and present a limited ability to effectively leverage curriculum task ordering. We believe that these results highlight the need for rigorous comparisons in future CL works as well as pave the way to design new CL strategies that are able to deal with more complex scenarios.},
archivePrefix = {arXiv},
arxivId = {2303.11076},
author = {Faber, Kamil and Zurek, Dominik and Pietron, Marcin and Japkowicz, Nathalie and Vergari, Antonio and Corizzo, Roberto},
doi = {10.1007/s10994-024-06524-z},
eprint = {2303.11076},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Faber et al. - 2023 - From MNIST to ImageNet and Back Benchmarking Continual Curriculum Learning.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Computer vision,Continual learning,Curriculum learning,Image classification,Lifelong learning,Neural networks},
month = {mar},
number = {10},
title = {{From MNIST to ImageNet and back: benchmarking continual curriculum learning}},
url = {http://arxiv.org/abs/2303.11076},
volume = {113},
year = {2024}
}

@inproceedings{Hawthorne2022Multi-instrumentDiffusion,
abstract = {An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr\'echet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.},
address = {Bengaluru, India},
annote = {Spectrogram generation (decoder)
- GAN
- DDPM 

5 seconds, with providing previous segment

1000 steps 
transformer decoder: 
time step position encoding + 
FiLM layer + 
self attention + 
cross attention 

inversion: MelGAN

Q: 
how is the classifier-free technique being implemented?
Do you think diffusion model can learn the long-time dependency of music structure?},
archivePrefix = {arXiv},
arxivId = {2206.05408},
author = {Hawthorne, Curtis and Simon, Ian and Roberts, Adam and Zeghidour, Neil and Gardner, Josh and Manilow, Ethan and Engel, Jesse},
booktitle = {Proceeding of the International Society on Music Information Retrieval (ISMIR)},
eprint = {2206.05408},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Hawthorne et al. - Unknown - MULTI-INSTRUMENT MUSIC SYNTHESIS WITH SPECTROGRAM DIFFUSION(2).pdf:pdf},
title = {{Multi-instrument Music Synthesis with Spectrogram Diffusion}},
url = {http://arxiv.org/abs/2206.05408},
year = {2022}
}

@inproceedings{Jeong2019VirtuosoNetPerformance,
abstract = {In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models.We also share the dataset we used for the experiment.},
address = {Delft, Netherlands},
author = {Jeong, Dasaem and Kwon, Taegyun and Kim, Yoojin and Lee, Kyogu and Nam, Juhan},
booktitle = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Jeong et al. - Unknown - VirtuosoNet A HIERARCHICAL RNN-BASED SYSTEM FOR MODELING EXPRESSIVE PIANO PERFORMANCE.pdf:pdf},
isbn = {9781732729919},
title = {{{VirtuosoNet}: A Hierarchical {RNN}-based System for Modeling Expressive Piano Performance}},
year = {2019}
}

@inproceedings{Le2023VoiceboxScale,
abstract = {Large-scale generative models such as GPT and DALL-E have revolutionized the research community.These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught.In contrast, speech generative models are still primitive in terms of scale and task generalization.In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale.Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced.Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context.Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation.In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster.Audio samples can be found in https://voicebox.metademolab.com.},
archivePrefix = {arXiv},
arxivId = {2306.15687},
author = {Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and Hsu, Wei Ning},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {2306.15687},
file = {:Users/huanzhang/Downloads/2306.15687v2.pdf:pdf},
issn = {10495258},
title = {{Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale}},
year = {2023}
}

@inproceedings{Lin2024ContentModelling,
abstract = {Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieves high-quality music generation with low-resource semi-supervised learning. We fine-tune the model with less than 4% of the orignal parameters on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls. We illustrate its controllability via chord and rhythm conditions, two of the most salient features of pop music. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and arrangement. Our source codes and demos are available online 1 2 .},
archivePrefix = {arXiv},
arxivId = {2310.17162v3},
author = {Lin, Liwei and Xia, Gus and Jiang, Junyan and Zhang, Yixiao},
booktitle = {Proceeding of the 25t International Society on Music Information Retrieval (ISMIR)},
eprint = {2310.17162v3},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - Unknown - CONTENT-BASED CONTROLS FOR MUSIC LARGE LANGUAGE MODELING.pdf:pdf},
title = {{Content-based Controls for Music Large Language Modeling}},
url = {https://kikyo-16.github.io/coco-mulla/.},
year = {2024}
}

@article{Maezawa2019RenderingRNN,
address = {Delft, Netherlands},
author = {Maezawa, Akira and Yamamoto, Kazuhiko and Fujishima, Takuya},
file = {:Users/huanzhang/Downloads/000105 (1).pdf:pdf},
isbn = {9781732729919},
journal = {Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Rendering music performance with interpretation variations using conditional variational {RNN}}},
year = {2019}
}

@inproceedings{Melechovsky2024MustangoGeneration,
abstract = {The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models such as MusicGen and AudioLDM2.},
archivePrefix = {arXiv},
arxivId = {2311.08355},
author = {Melechovsky, Jan and Guo, Zixun and Ghosal, Deepanway and Majumder, Navonil and Herremans, Dorien and Poria, Soujanya},
booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL},
doi = {10.18653/v1/2024.naacl-long.459},
eprint = {2311.08355},
file = {:Users/huanzhang/Downloads/2311.08355.pdf:pdf},
isbn = {9798891761148},
title = {{Mustango: Toward Controllable Text-to-Music Generation}},
volume = {1},
year = {2024}
}

@article{Nistal2024DiffModels,
abstract = {Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce "Diff-A-Riff," a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website: sonycslparis.github.io/diffariff-companion/},
archivePrefix = {arXiv},
arxivId = {2406.08384},
author = {Nistal, Javier and Pasini, Marco and Aouameur, Cyran and Grachten, Maarten and Lattner, Stefan},
eprint = {2406.08384},
file = {:Users/huanzhang/Downloads/2406.08384v2.pdf:pdf},
journal = {Proceeding of the 25th International Society on Music Information Retrieval (ISMIR)},
title = {{Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models}},
url = {http://arxiv.org/abs/2406.08384},
year = {2024}
}

@inproceedings{Perez2018FiLMlayer,
abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
address = {New Orleans, USA},
archivePrefix = {arXiv},
arxivId = {1709.07871},
author = {Perez, Ethan and Strub, Florian and {De Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
doi = {10.1609/aaai.v32i1.11671},
eprint = {1709.07871},
file = {:Users/huanzhang/Downloads/1709.07871.pdf:pdf},
isbn = {9781577358008},
issn = {2159-5399},
title = {{{FiLM}: Visual reasoning with a general conditioning layer}},
year = {2018}
}

@inproceedings{Renault2022DifferentiableSynthesis,
abstract = {Recent neural-based synthesis models have achieved impressive results for musical instrument sound generation. In particular, the Differentiable Digital Signal Processing (DDSP) framework enables the usage of spectral modeling analysis and synthesis techniques in fully differentiable architectures. Yet currently, it has only been used for modeling monophonic instruments. Leveraging the interpretability and modularity of this framework, the present work introduces a polyphonic differentiable model for piano sound synthesis, conditioned on Musical Instrument Digital Interface (MIDI) inputs. The model architecture is motivated by high-level acoustic modeling knowledge of the instrument which, in tandem with the sound structure priors inherent to the DDSP components, makes for a lightweight, interpretable and realistic sounding piano model. The proposed model has been evaluated in a listening test, demonstrating improved sound quality compared to a benchmark neural-based piano model, with significantly less parameters and even with reduced training data. The same listening test indicates that physical-modeling-based models still achieve better quality, but the differentiability of our lightened approach encourages its usage in other musical tasks dealing with polyphonic audio and symbolic data.},
author = {Renault, Lenny and Mignot, R{\'{e}}mi and Roebel, Axel},
booktitle = {Proceedings of the International Conference on Digital Audio Effects, DAFx},
file = {:Users/huanzhang/Downloads/DAFx20in22_paper_48.pdf:pdf},
isbn = {9783200085992},
issn = {24136689},
pages = {232--239},
title = {{Differentiable Piano Model for MIDI-to-Audio Performance Synthesis}},
volume = {3},
year = {2022}
}

@inproceedings{Tal2024JointGeneration,
abstract = {We present JASCO, a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. JASCO can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. JASCO is based on the Flow Matching modeling paradigm together with a novel conditioning method. This allows music generation controlled both locally (e.g., chords) and globally (text description). Specifically, we apply information bottleneck layers in conjunction with temporal blurring to extract relevant information with respect to specific controls. This allows the incorporation of both symbolic and audio-based conditions in the same text-to-music model. We experiment with various symbolic control signals (e.g., chords, melody), as well as with audio representations (e.g., separated drum tracks, full-mix). We evaluate JASCO considering both generation quality and condition adherence, using both objective metrics and human studies. Results suggest that JASCO is comparable to the evaluated baselines considering generation quality while allowing significantly better and more versatile controls over the generated music. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/JASCO.},
archivePrefix = {arXiv},
arxivId = {2406.10970},
author = {Tal, Or and Ziv, Alon and Gat, Itai and Kreuk, Felix and Adi, Yossi},
booktitle = {Proceeding of the 25t International Society on Music Information Retrieval (ISMIR)},
eprint = {2406.10970},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Tal et al. - 2024 - Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation.pdf:pdf},
month = {jun},
title = {{Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation}},
url = {http://arxiv.org/abs/2406.10970},
year = {2024}
}

@article{Vyas2023AudioboxPrompts,
abstract = {Audio is an essential part of our life, but creating it often requires expertise and is time-consuming. Research communities have made great progress over the past year advancing the performance of large scale audio generative models for a single modality (speech, sound, or music) through adopting more powerful generative models and scaling data. However, these models lack controllability in several aspects: speech generation models cannot synthesize novel styles based on text description and are limited on domain coverage such as outdoor environments; sound generation models only provide coarse-grained control based on descriptions like "a person speaking" and would only generate mumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that is capable of generating various audio modalities. We design description-based and example-based prompting to enhance controllability and unify speech and sound generation paradigms. We allow transcript, vocal, and other audio styles to be controlled independently when generating speech. To improve model generalization with limited labels, we adapt a self-supervised infilling objective to pre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and sound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles. We further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the default ODE solver for flow-matching, without loss of performance on several tasks. Our demo is available at https://audiobox.metademolab.com/},
archivePrefix = {arXiv},
arxivId = {2312.15821},
author = {AudioBox team},
eprint = {2312.15821},
file = {:Users/huanzhang/Downloads/416201240_1109648396840016_4084243924939353841_n.pdf:pdf},
journal = {Arxiv preprint arXiv:2312.15821},
title = {{Audiobox: Unified Audio Generation with Natural Language Prompts}},
url = {http://arxiv.org/abs/2312.15821},
year = {2023}
}

@inproceedings{Wang2019ContinualReplay,
abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4% of the size of all previous training data matches the performance of refining the classifier keeping 20% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
archivePrefix = {arXiv},
arxivId = {1906.00654},
author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2019.8937236},
eprint = {1906.00654},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2019 - Continual Learning of New Sound Classes using Generative Replay.pdf:pdf},
isbn = {9781728111230},
issn = {19471629},
keywords = {Sound classification,continual learning,generative replay,neural networks},
month = {jun},
title = {{Continual learning of new sound classes using generative replay}},
url = {http://arxiv.org/abs/1906.00654},
volume = {2019-Octob},
year = {2019}
}

@article{Wang2021SurveyLearning,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer ++ Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
doi = {10.1109/TPAMI.2021.3069908},
eprint = {2010.13166},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Chen, Zhu - 2020 - A Survey on Curriculum Learning.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Curriculum learning,example reweighting,machine learning,self-paced learning,training strategy},
month = {oct},
number = {9},
pmid = {33788677},
title = {{A Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
volume = {44},
year = {2022}
}

@article{Wang2024ComprehensiveApplication,
abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
archivePrefix = {arXiv},
arxivId = {2302.00487},
author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
doi = {10.1109/TPAMI.2024.3367329},
eprint = {2302.00487},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2023 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Continual learning,catastrophic forgetting,incremental learning,lifelong learning},
month = {jan},
number = {8},
pmid = {38407999},
title = {{A Comprehensive Survey of Continual Learning: Theory, Method and Application}},
url = {http://arxiv.org/abs/2302.00487},
volume = {46},
year = {2024}
}

@inproceedings{Wu2023LargeAugmentation,
abstract = {Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K1 and the proposed model 2 are both available to the public.},
archivePrefix = {arXiv},
arxivId = {2211.06687},
author = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP49357.2023.10095969},
eprint = {2211.06687},
file = {:Users/huanzhang/Downloads/2211.06687v4.pdf:pdf},
isbn = {9781728163277},
issn = {15206149},
keywords = {Audio Classification,Audio Dataset,Contrastive Learning,Representation Learning,Text-to-Audio Retrieval},
title = {{Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation}},
year = {2023}
}

@article{Wu2023MusicGeneration,
abstract = {Text-to-music generation models are now capable of generating high-quality music audio in broad styles. However, text control is primarily suitable for the manipulation of global musical attributes like genre, mood, and tempo, and is less suitable for precise control over time-varying attributes such as the positions of beats in time or the changing dynamics of the music. We propose Music ControlNet, a diffusion-based music generation model that offers multiple precise, time-varying controls over generated audio. To imbue text-to-music models with time-varying control, we propose an approach analogous to pixel-wise control of the image-domain ControlNet method. Specifically, we extract controls from training audio yielding paired data, and fine-tune a diffusion-based conditional generative model over audio spectrograms given melody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet method already allows generation with any subset of controls, we devise a new masking strategy to allow creators to input controls that are only partially specified in time. We evaluate both on controls extracted from audio and controls we expect creators to provide, demonstrating that we can generate realistic music that corresponds to control inputs in both settings. While few comparable music generation models exist, we benchmark against MusicGen, a recent model that accepts text and melody input, and show that our model generates music that is 49% more faithful to input melodies despite having 35x fewer parameters, training on 11x less data, and enabling two additional forms of time-varying control. Sound examples can be found at https://musiccontrolnet.github.io/web/.},
archivePrefix = {arXiv},
arxivId = {2311.07069},
author = {Wu, Shih Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J.},
doi = {10.1109/TASLP.2024.3399026},
eprint = {2311.07069},
file = {:Users/huanzhang/Downloads/2311.07069.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Music generation,controllable generative modeling,diffusion models},
title = {{Music ControlNet: Multiple Time-Varying Controls for Music Generation}},
url = {http://arxiv.org/abs/2311.07069},
year = {2024}
}

@inproceedings{Zhang2023AddingModels,
abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions"(zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
archivePrefix = {arXiv},
arxivId = {2302.05543},
author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV51070.2023.00355},
eprint = {2302.05543},
file = {:Users/huanzhang/Downloads/2302.05543v3.pdf:pdf},
isbn = {9798350307184},
issn = {15505499},
title = {{Adding Conditional Control to Text-to-Image Diffusion Models}},
year = {2023}
}

@article{Zhang2024DExterModels,
abstract = {In the pursuit of developing expressive music performance models using artificial intelligence, this paper introduces DExter, a new approach leveraging diffusion probabilistic models to render Western classical piano performances. The main challenge faced in performance rendering tasks is the continuous and sequential modeling of expressive timing and dynamics over time, which is critical for capturing the evolving nuances that characterize live musical performances. In this approach, performance parameters are represented in a continuous expression space, and a diffusion model is trained to predict these continuous parameters while being conditioned on a musical score. Furthermore, DExter also enables the generation of interpretations (expressive variations of a performance) guided by perceptually meaningful features by being jointly conditioned on score and perceptual-feature representations. Consequently, we find that our model is useful for learning expressive performance, generating perceptually steered performances, and transferring performance styles. We assess the model through quantitative and qualitative analyses, focusing on specific performance metrics regarding dimensions like asynchrony and articulation, as well as through listening tests that compare generated performances with different human interpretations. The results show that DExter is able to capture the time-varying correlation of the expressive parameters, and it compares well to existing rendering models in subjectively evaluated ratings. The perceptual-feature-conditioned generation and transferring capabilities of DExter are verified via a proxy model predicting perceptual characteristics of differently steered performances.},
author = {Zhang, Huan and Chowdhury, Shreyan and Cancino-Chac{\'{o}}n, Carlos Eduardo and Liang, Jinhua and Dixon, Simon and Widmer, Gerhard},
doi = {10.3390/app14156543},
issn = {2076-3417},
journal = {Applied Sciences},
number = {15},
title = {{DExter: Learning and Controlling Performance Expression with Diffusion Models}},
url = {https://www.mdpi.com/2076-3417/14/15/6543},
volume = {14},
year = {2024}
}

@inproceedings{Zhang2024Instruct-Musicgen,
abstract = {Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.},
archivePrefix = {arXiv},
arxivId = {2405.18386},
author = {Zhang, Yixiao and Ikemiya, Yukara and Choi, Woosung and Murata, Naoki and Mart{\'{i}}nez-Ram{\'{i}}rez, Marco A. and Lin, Liwei and Xia, Gus and Liao, Wei-Hsiang and Mitsufuji, Yuki and Dixon, Simon},
booktitle = {Proceeding of the International Joint Conference on Artificial Intelligence (IJCAI)},
eprint = {2405.18386},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2024 - Instruct-MusicGen Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning.pdf:pdf},
title = {{Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning}},
url = {http://arxiv.org/abs/2405.18386},
year = {2024}
}

@inproceedings{Zhou2024Non-compositionalLearning,
abstract = {Non-compositional expressions, by virtue of their non-compositionality, are a classic 'pain in the neck' for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we propose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step, but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model's performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https://github.com/zhjjn/CL2Gen.git.},
author = {Zhou, Jianing and Zeng, Ziheng and Gong, Hongyu and Bhat, Suma},
booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
doi = {10.18653/v1/2023.findings-emnlp.286},
file = {:Users/huanzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - Unknown - Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning.pdf:pdf},
isbn = {9798891760615},
issn = {0736587X},
title = {{Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning}},
year = {2023}
}

