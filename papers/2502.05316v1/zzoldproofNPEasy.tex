\leonard{Just adding some results I am using later}

\begin{lemma} \label{lm:mpd_ptime}
    Given a simple quantitative MDP $\MDProc$, the quantities $\max_\sigma \pexp_\sigma[\mu]$ and $\sup_\sigma \oexp_\sigma[\mu]$ can be computed in polynomial time.
    As a consequence, the pessimistic and optimistic expectation of a simple quantitative payoff function can also be computed in polynomial time in a Markov chain.
\end{lemma}

\begin{proof}
    \leonard{todo}
\end{proof}



\subsection{Examples}

\leonard{Add here some examples to explain the concept of anchoring players.}


\subsection{Lower bound}


%    \leonard{Thejaswini, this is for you.}
\input{5NPhardness}



\subsection{Upper bound in the general case}
We first prove a small combinatorial lemma that helps bound our memory. 
\input{5NPComplete-Combinatorial}






\begin{lemma}\label{lm:reach_safe_positional}
    Let $\Game$ be a game.
    Let $W$ be a set of vertices, and let $X, Y \subseteq \Rb^\Pi$ be two sets of payoff vectors with $X \cap Y = \emptyset$.
    Then, there exists a positional strategy profile $\bsigma$ such that:
    \begin{itemize}
        \item the elements of $W$ are visited with probability $0$;
        \item some payoff vector of $X$ is achieved with nonzero probability;
        \item the payoff vectors of $Y$ are achieved with probability zero;
    \end{itemize}
    when $\bsigma$ is followed, from any vertex from which some strategy satisfying those constraints exists.
\end{lemma}

\begin{proof}
    Let us define dynamically the edges that must never be taken, and those that can.
    At first, let us assume that all edges are marked as white.
    Let $W'$ be the set of vertices from which, for every chosen strategy profile, some element of $W$ has nonzero probability to be visited, or some payoff vector of $Y$ has nonzero probability to be achieved (in particular, we have $W \subseteq W'$).
    Then, mark as red all the edges $uv$ with $u \not\in W'$ and $v \in W$.
    Note that any $u \not\in W$ has, still, one outgoing edge that remains white, and all outgoing edges that remains white if it is a stochastic node.

    Then, the game $\Game$ without the red edges can be seen as an MDP, where the objective is to reach one of the terminals associated to a payoff vector belonging to $X$ (if $\bar{0} \not\in X$) or to avoid the terminals associated to a payoff that does not belong to $X$ (if $\bar{0} \in X$).
    It is, then, a reachability or a safety objective, for which, by TODO, there exists a positional uniformly optimal strategy (profile) $\bsigma$, which can also be interpreted as a strategy profile in the game $\Game$ (which never takes the red edges), which satisfies our requirements.
\end{proof}

\begin{lemma} \label{lm:rse_finite_memory}
    Let $\Game_{\|v_0}$ be a game with $n$ vertices and $p$ players.
    Let $\bsigma$ be a $\brho$-RSE in $\Game_{\|v_0}$.
    Then, there exists a $\brho$-RSE $\bsigma^\star$ in $\Game_{\|v_0}$ such that $\re_{\bsigma^\star}[\mu] = \re_{\bsigma}[\mu]$ and that has at most $4np -2n + p + 1$ memory states.
\end{lemma}

\begin{proof}
    \leonard{I think I'm done with this proof, but maybe another brain should check it}

    Let $\bsigma$ be a $\brho$-RSE in $\Game_{\|v_0}$.
    Let $\bz = \re_\bsigma[\mu]$.
    Let us first define, for each history compatible with $\bsigma$, which players are currently being anchored.
    From that labelling, we will be able to define a strategy profile $\bsigma^\star$.
    Finally, we will prove that $\bsigma^\star$ is a $\brho$-RSE with $\re_\bsigma[\mu] = \re_{\bsigma^\star}[\mu]$.

\paragraph*{Definition of the labelling $\Lambda$}
    \begin{proposition}[A finite abstraction $\Lambda$ of a strategy]
        There is a labelling $\Lambda: \Comp(\bsigma) \to 2^\Pi$ such that for every history $hv$, where $h\in V^*$, and $v\in V$, and for every player $i\in \Lambda(hv)$ \theju{Check if compliant players are defined before}
        \begin{enumerate}
        \item $\Lambda(h) = \bigcup_{v \in \Supp (\bsigma(h))} \Lambda(hv)$;~\label{itm:partitionanchor}
        \item if $i$ is an optimist, then  $\re_{\bsigma_{\|hv}}[\mu_i] = z_i$;~\label{itm:optimistanchor}

        \item if $i$ is a pessimist, then for every strategy $\tau_i$ for player $i$, we have $\re_{(\bsigma_{-i\|hv}, \tau_i)}[\mu_i] \leq z_i$.~\label{itm:pessimistanchor}
    \end{enumerate}
    \end{proposition}
    \begin{claimproof}
        We define inductively a labelling based on the length of histories $\Lambda: \Comp(\bsigma) \to 2^\Pi$ that satisfies the conditions from the proposition. 
    % , with the following induction hypothesis, for every history $hv$ and for every player $i \in \Lambda(hv)$:
    % \begin{itemize}
    %     \item if $i$ is an optimist, then we have $\re_{\bsigma_{\|hv}}[\mu_i] = z_i$;

    %     \item if $i$ is a pessimist, then for every strategy $\tau_i$, we have $\re_{(\bsigma_{-i\|hv}, \tau_i)}[\mu_i] \leq z_i$.
    %     \item $\Lambda(h) = \bigcup_{v \in \Supp (\bsigma(h))} \Lambda(hv)$.
    % \end{itemize}
    
    First, we define $\Lambda$ for the trivial history $v_0$, the value $\Lambda(v_0) = \Pi$, that is, all players must be anchored.
    Let us notice that the history $v_0$ satisfies the induction hypothesis, which simply states that the pessimists have no profitable deviation, and that the optimists get the optimistic expectation they are supposed to get.
    
    Suppose $\Lambda(hv)$ has already been defined, where $hv$ is a history compatible with $\bsigma$. 
    Let $w_1, \dots, w_k$ be the successors of $v$ that are chosen by the strategy $\bsigma(hv)$ with non-zero probability, that is, the support of $\bsigma(hv)$.
    If $k=1$, then $\Lambda(hvw_1) = \Lambda(hv)$. 
    If not, and $k>1$, we define each $\Lambda(hvw_\l)$  by first declaring it to be the empty set and successively adding vertices from $\Lambda(hv)$ to $\Lambda(hvw_\l)$.
    \begin{claim}\label{claim:successorAnchor}
    If $i \in \Lambda(hv)$ is a pessimistic player who does not control $v$, then   there is a successor $w_\ell$ of $v$, chosen with non-zero probability by $\sigma$ such that for every  strategy $\tau_i^\ell$, we have $\re_{(\bsigma_{-i\|hvw_\ell}, \tau_i)}[\mu_i] \leq z_i$.     
    Instead if $i$ is an optimist, and $\re_{\bsigma_{\|hv}}[\mu_i] = z_i$, then  there is at least one successor $w_\l$ such that $\re_{\bsigma_{\|hvw_\l}}[\mu_i] = z_i$.
    \end{claim}
    \begin{claimproof}
            We prove the above claim using contradiction for pessimists. Suppose not, and for each $w_\l$, there exists a strategy $\tau_i^\l$ such that $\re_{\bsigma_{-i\|hvw_\l}, \tau^\l_i}[\mu_i] > z_i$. Then, the strategy $\tau_i$ defined by $\tau_i(w_\l h') = \tau^\l(h')$ for each $\l$ is such that $\re_{\bsigma_{-i\|hv}, \tau_j}[\mu_i] > z_i$, which is impossible since $\Lambda(hv)$ satisfies the conditions in the proposition.\theju{Contradiction is not necessary here. We can instead prove it directly, from definition.}

            It follows from the definition of $\re$ for an optimistic player. \theju{Is this enough?}
    \end{claimproof}
    \subparagraph*{Vertex $v$ is controlled by a pessimistic player.}
    If the vertex $v$ is controlled by some pessimistic player $i$ who is already anchored at history $hv$, that is, $i \in \Lambda(hv)$, then we add $i$ to  $\Lambda(hvw_\l)$ for all $\ell\in[k]$.
    
    However, if $i \in \Lambda(hv)$ is a pessimistic player who does not control $v$, then from \cref{claim:successorAnchor} that there is a successor $w_\ell$ of $v$  such that for every  strategy $\tau_i^\ell$, we have $\re_{(\bsigma_{-i\|hvw_\ell}, \tau_i)}[\mu_i] \leq z_i$.     
    We pick one such vertex $w_\ell$, and add $i$ to the set $\Lambda(hvw_\l)$.

    Therefore, there exists at least one $w_\l$ such that $\re_{\bsigma_{-i\|hvw_\l}, \tau_i}[\mu_i] \leq z_i$ for all $\tau_i$. 
    Note that this satisfies the Condition~\ref{itm:pessimistanchor}, the induction hypothesis is satisfied.
    \subparagraph*{Vertex $v$ is controlled by an optimistic player.}Now, let us consider an optimistic player $i \in \Lambda(hv)$.
    By the inductive construction of $\Lambda$, we have $\re_{\bsigma_{\|hv}}[\mu_i] = z_i$. 
    Therefore, from \cref{claim:successorAnchor}, there is at least one successor $w_\l$ such that $\re_{\bsigma_{\|hvw_\l}}[\mu_i] = z_i$.
    We then add player $i$ to the set $\Lambda(hvw_\l)$. Note that adding such an $i$ to $\Lambda(hvw_\l)$ ensures Condition~\ref{itm:optimistanchor} is satisfied.

    In both of the above cases for player $i$ being optimistic or pessimistic, if there is more than one $w_\l$ that satisfies the condition in \cref{claim:successorAnchor}, then we chose a $w_\l$ such that the set $\Lambda(hvw_\l)$ is empty.
    
    This way, for all $i \in \Lambda(hv)$, we ensure that we define $\Lambda(hvw_\l)$ that satisfying the three conditions in the proposition for histories $hvw_\ell$.
    \end{claimproof}

\paragraph*{Definition and counting of the memory states}
\thejaswini{Defining and counting needs to be separated. It is too much information at the same time. Counting can be postponed to the end.}
    We define the memory states for a strategy $\bsigma^\star$ that we construct as follows
    \begin{itemize}    
        \item for each player $i$, the state $\punish_i$;

        \item for each vertex $v$ and each subset $P \subseteq \Pi$ of players such that there exists $h$ with $P = \Lambda(h)$, the state $\anchor_{Pv}$;

        \item the state $\anchor_{\Pi\bot}$
    \end{itemize}

    Let us now bound the number of such states.
    There are, obviously, exactly $p$ states of the form $\punish_i$.
    Let us now prove that there are at most $4np-2n$ states of the form $\anchor_{Pv}$ with $v \in V$.
    
    We say that a subset $P$ of player is $\Lambda(h)$-compatible for a given function $\Lambda$ if there is some history $h'$ such that $P = \lambda(hh')$.
    % We further say that a subset $P$ is $\Lambda(h)$-compatible if $P\subseteq \Lambda(h)$, $P$ is $\Lambda$-compatible, and there is an extension of the history $h$ to $hh'$ such that $P = \Lambda(hh')$.

    Let $f(h)$ be the number of $\Lambda(h)$-compatible subsets.
    Clearly, if $|\Lambda(h)| = m$, then $f(h) \leq 2^m$ because every subset such that $P = \lambda(hh')$ is a subset of $\Lambda(h)$.
We show tighter bounds for $\Lambda(h)$.
    \begin{claim}\label{claim:LambdaCompatible}
         If $|\Lambda(h)| = m > 0$ then the number of $\Lambda(h)$-compatible sets is at most $4m-2$, that is, we have $f(h) \leq 4m-2$.
    \end{claim}
    \begin{claimproof}[Proof of \cref{claim:LambdaCompatible}]
        We construct a (directed acyclic) graph where the states of this graph are exactly $\Lambda$-compatible sets. We later show that the such a DAG is indeed a split-graph for the subset $\Lambda(h)$. From \cref{lemma:counting_subset_dag}, our claim follows since the number of vertices in a split-graph of a set $S$ is at most $4|S|-2$.
    \end{claimproof}

    
    \begin{claimproof}
    We 
    \subparagraph*{Strategy $\bsigma$ extends history $h$ uniquely.}If the strategy $\sigma$ after the history $h$ is has a unique extension in history, that is, the strategy is pure and no confirming play visits a stochastic vertex, then 
    for all $h'$, we have $\Lambda(hh') = \Lambda(h)$. Therefore, only the set $\Lambda(h)$ is $\Lambda(h)$-compatible, and therefore, since $m>2$, 
    in such cases we have $f(h) = 1<3m-3$. 
    that is, in any extension $h'$ of the history $h$, then $\Lambda(hh') = \Lambda(h)$.
%    Therefore, if $\bsigma_{\|h}$ is fully deterministic (pure and no confirming play visits a stochastic vertex), then 

    \subparagraph*{Strategy $\bsigma$ on history $h$ has more than one extension.} If the strategy $\bsigma$ after history $h$ does not have a unique extension, then we say that $\Lambda(h)$ \emph{splits}. That is, $\Lambda(h)$ splits if there is an extension $h'v$ such that $v$ is a stochastic node, or $v$ is controlled player $i$ and strategy $\sigma_i(hh'v)$ is a randomized action.

    Let $h'v$ be the shortest history such that $hh'v$ is compatible with $\bsigma$, and that $\bsigma(hh'v)$ is a randomised action (either $v$ is stochastic or $\bsigma(hh'v)$ is a randomised action), then we then have $\Lambda(hh'v) = \Lambda(h)$.
    Let $\{w_1, \dots, w_k\} = \Supp (\bsigma(hh'v))$. For brevity, let $\Bar{h}=hh'v$. 
    Then, either the sets $\Lambda(\Bar{h}w_1), \dots, \Lambda(\Bar{h}w_k)$ 
    \begin{itemize}
        \item  form a partition of $\Lambda(h)$, or
        \item $v$ is controlled by $i\in \Lambda(\Bar{h})$, and therefore $i \in \Lambda(\Bar{h}w_\l)$ for all $\l\in[k]$. 
    \end{itemize}
    It the latter case, if we consider two sets $\Lambda(\Bar{h}w_\l)$ and $\Lambda(\Bar{h}w_{\l'})$ with $\l \neq \l'$, they have exactly one element in common.
    That implies that any set of the form $\Lambda(\Bar{h}w_\l h'')$ that has cardinality at least $2$ is necessarily different from every set of the form $\Lambda(\Bar{h}w_{\l'} h'')$ with $\l' \neq \l$.
    And, therefore, that we have $f(h) = \sum_\l f(\Bar{h}w_\l)$.

% \thejaswini{I think this proof has too many components of reasoning about the sizes of the sets, and also about the shape of the sets simultaneously. We need to distil the combinatorics from the strategy shape. I would recommend some lemma/proposition/claim as above~\cref{lemma:counting_subset_dag}, and then just showing that the $\Lambda(v)$-compatible sets have a DAG structure described before.}
    
    % Splits occur when a stochastic node is visited, or when some player proceeds to a randomized action: if $\bsigma_{\|h}$ is fully deterministic (pure and visiting no stochastic vertex), then only the set $\Lambda(h)$ has the desired form, hence we have $f(h) = 1 < 3m-3$ and the proof is done.
    % Otherwise, let $h'$ be the shortest history such that $hh'$ is compatible with $\bsigma$, and that $\bsigma(hh')$ is a random transition: we then have $\Lambda(hh') = \Lambda(h)$.
    % Let $\{w_1, \dots, w_k\} = \Supp (\bsigma(hh'))$.    
    % Then, the sets $\Lambda(hh'w_1), \dots, \Lambda(hh'w_k)$ form a partition of $\Lambda(h)$, except for the case of the player controlling the last vertex of the history $hh'$, if there is any and if they belong to $\Lambda(h)$: that player would then be in each of those sets.
    % It is then still true that, if we consider two sets $\Lambda(hh'w_\l)$ and $\Lambda(hh'w_{\l'})$ with $\l \neq \l'$, they have at most one element in common.
    % That implies that any set of the form $\Lambda(hh'w_\l h'')$ that has cardinality at least $2$ is necessarily different from every set of the form $\Lambda(hh'w_{\l'} h'')$ with $\l' \neq \l$.
    % And, therefore, that we have $f(h) = \sum_\l f(hh'w_\l)$.
    
    If there is $\l$ such that $\Lambda(hh'w_\l) = \Lambda(hh') = \Lambda(h)$, then for every $\l' \neq \l$, the set $\Lambda(hh'w_{\l'})$ contains at most one element, hence $f(hh'w_{\l'}) = 0<3m-3$.
    But in such a case, we can extend $h'$ with $w_\l$ and the sequel, until we find a history $h'$ such that $hh'$ is compatible with $\bsigma$, that $\bsigma(hh')$ is a random transition, that $\Lambda(hh') = \Lambda(h)$ and that there is no $w_\l$ such that $\Lambda(hh'w_\l) = \Lambda(h)$ (if this process does not terminate, it means that $\Lambda(h)$ is actually never split, i.e. $f(h) = 1$, and the proof is done).
    Then, we can apply our induction hypothesis, and we obtain the inequality:
    $$f(h) \leq \sum_\l (3 |\Lambda(hh'w_\l)| - 3) = 3 \sum_\l |\Lambda(hh'w_\l)| - 3k.$$
    Let us now note that from the observation we made above about the sets $\Lambda(hh'w_\l)$, we can deduce the inequality $\sum_\l |\Lambda(hh'w_\l)| \leq m - 1 + k$ (each of the $m$ elements of $\Lambda(h)$ occurs once in these sets, except maybe one that may occur $k$ times).
    Hence:
    $$f(h) \leq 3 (m - 1 + k) - 3k = 3m - 3,$$
    as desired.
    \end{claimproof}
%    To do so, for each history $h$, let $f(h)$ be the number of sets of the form $\Lambda(hh')$ that has cardinality at least $2$, where $h'$ is a history; and let us prove that if $|\Lambda(h)| = m > 0$ then we have $f(h) \leq 3m-3$.

    Thus, if we take $h = v_0$, we find that there exist at most $3p-3$ sets of the form $\Lambda(v_0 h')$ with cardinality at least $2$.
    If we add all the singletons and the empty set, there are, then, at most $4p-2$ sets of the form $\Lambda(v_0 h')$, and therefore $4np-2n$ states of the form $\anchor_{Pv}$.
    If we add the states of the form $\punish_i$ and the state $\anchor_{\Pi\bot}$, we have in total at most $4np-2n+p+1$ states, as desired.


    \paragraph*{Definition of the strategy profile $\bsigma^\star$}

    We now have the states of our Mealy machine, let us define the transitions.\theju{what Mealy machine? Need to say that strategies are in the form of a Mealy machine, maybe?}
    First, let us define what $\sigma^\star$ does when in state $\punish_i$, for some player $i$: those states will correspond to the \emph{punishing strategies}, followed when player $i$ deviates.
    By Lemma~\ref{lm:reach_safe_positional}, there is a positional strategy profile $\btau^{\dag i}_{-i}$ that minimizes, from every vertex of the game, the payoff that player $i$ can enforce.
    Let us pick an arbitrary positional strategy $\sigma^{\dag i}_i$.
    Then, when the strategy profile $\bsigma^\star$ is in memory state $\punish_i$ and reads the vertex $v$, it outputs the action $\btau(v)$ and stays in state $\punish_i$.
    
    Let us now define what happens in state $\anchor_{\emptyset v}$.
    Consider the Boolean objective of achieving a payoff vector that has nonzero probability to be achieved in $\bsigma$, and avoiding every vertex that has probability zero of being visited in $\bsigma$.
    By Lemma~\ref{lm:reach_safe_positional}, there exists a positional strategy profile that is uniformly optimal for that objective: let us call it $\btau^{\anch \emptyset}$.
    Then, when in memory state $\anchor_{\emptyset v}$ and reading the vertex $w$, we distinguish two cases:
    \begin{itemize}
        \item if $vw$ is an edge that is compatible with the strategy profile $\btau^{\anch \emptyset}$, then the strategy profile $\bsigma^\star$ outputs the action $\btau^{\dag i}$, where $i$ is the player controlling $v$, and shifts to the memory states $\punish_i$;

        \item otherwise, it outputs the action $\btau^{\anch \emptyset}(w)$ and moves to the memory state $\anchor_{\emptyset w}$.
    \end{itemize}

    We can now move to singletons, and define what happens in the states of the form $\anchor_{\{i\} v}$.
    In that state, our objective is to give player $i$ exactly the risk entropy $z_i$.
    More precisely, consider the objective where player $i$ receives payoff $z_i$ with non-zero probability and ensuring that no vertex that has zero probability of being visited in $\bsigma$ is visited.\theju{should we say something about strategy restricted to the history?}    
%    of giving, with nonzero probability, the payoff $z_i$ to player $i$, and maintaining to $0$ the probability of achieving a payoff vector (including $0$ for every player) that has probability zero to be achieved in $\bsigma$, or of visiting a vertex that has probability zero to be visited in $\bsigma$. 
    By Lemma~\ref{lm:reach_safe_positional}, there exists a positional strategy profile $\btau^{\anch \{i\}}$ that satisfies that objective from every vertex from which it is possible.
    Note that that objective is in particular satisfiable, and therefore satisfied by $\btau^{\anch \{i\}}$, from every vertex $v$ such that there exists a history $hv$ compatible with $\bsigma$ such that $i \in \Lambda(hv)$.
    Similar to the previous step, we define the strategy profile $\bsigma^\star$ in the states of the form $\anchor_{\{i\} v}$ so that it follows the strategy profile $\btau^{\anch \{i\}}$, remembers the last vertex that was visited and use that memory to switch to the corresponding punishing state when some player $j$ deviates.

    Now, let us consider the states of the form $\anchor_{P v}$, where $P$ has cardinality at least $2$.
    The existence of $\anchor_{P v}$ implies that there is a history $h$ such that $\Lambda(h) = P$.
    Moreover, since each randomization splits the label of histories in sets that have at most one element in common, there is only one side of each split that can contain $P$, which implies that there is such a history that is minimal for the prefix relation --- we therefore choose that one.
    After $h$, the histories labelled by $P$ form a (possibly empty) sequence $hv_1, hv_1v_2, \dots$ which may be infinite, or ends with a new split.
    

    Let us first consider the finite case: there is, then, a longest history $hv_1 \dots v_q$ with $\Lambda(hv_1 \dots v_q) = P$.
    Then, there are at least two vertices $w_1, \dots, w_k \in \Supp(\bsigma(hv_1 \dots v_q))$ such that $\Lambda(hv_1 \dots v_q w_\l) \neq \emptyset$ for each $\l$.
    Therefore, the strategy profile $\bsigma_{\|h}$ guarantees, from the vertex $\last(h)$, that the vertex $v_q$ is reached with nonzero probability.
    By Lemma~\ref{lm:reach_safe_positional} (and by replacing $v_q$ by a terminal), there exists then a positional strategy profile that satisfies that requirement: let us write it $\btau^{\anch P}$.
    Let then $h'v_q$ be a simple history from $\last(h)$ to $v_q$ that is compatible with the strategy profile $\btau^{\anch P}$.
    We define the strategy profile $\bsigma^\star$ in each state $\anchor_{Pv}$ so that it follows the strategy profile $\btau^{\anch P}$ and remembers the last vertex visited, and switches to the state $\punish_i$ and follows the strategy profile $\btau^{\dag i}$ when a given player $i$ deviates and takes an edge that they are not supposed to take.
    Moreover, when an edge is taken that does belong to the history $h'v_q$, but not because a player deviated (it is then necessarily because of a stochastic node), the strategy profile switches to the corresponding state $\anchor_{\emptyset v}$ and immediately follows the corresponding strategy.
    Finally, when the vertex $v_q$ is reached when in state $\anchor_{P \last(h')}$, the strategy profile $\bsigma^\star$ chooses randomly between the edges $v_q w_1$, \dots, and $v_q w_k$.



    Let us finally consider the infinite case: then, the set $P$ is never split, except possibly into $\emptyset$ or some singleton $\{i\}$ and the set $P$ itself.
    There is, therefore, a play $\pi$ such that for every $k \geq 0$, we have $\Lambda(h\pi_{< k}) = P$.
    And, by the induction hypothesis maintained during the construction of $\Lambda$, for each optimistic $i \in P$, we have $\re_{\bsigma_{\|h\pi_{<k}}}[\mu_i] = z_i$; and for each pessimistic $i \in P$, the strategy profile $\bsigma_{-i\|h\pi_{<k}}$ is such that whatever player $i$ plays, they get a risk entropy at most $z_i$\dots which means that in the strategy profile $\bsigma_{\|h\pi_{<k}}$ itself, such a pessimist $i$ gets at most the risk entropy $z_i$; and in fact, exactly $z_i$, otherwise their risk entropy in $\bsigma$ would be lesser than $z_i$.
    We can then show that for each $i \in P$, we actually have $\mu_i(h\pi) = z_i$ and $\prob_{\bsigma_{\|h}}(\pi) \neq 0$\footnote{Note that this result is actually almost immediate when $\pi$ reaches a terminal; the arguments we present here are designed to include also the case where it does not, and therefore where we actually have $z_i = 0$ for all $i$.}.
    Indeed, let us recall that in the construction of $\Lambda$, when there were several possibilities for the set to which a given player could be added, we always favoured the empty ones: that means that, for each $k$, the vertex $\pi_{k+1}$ was actually the only element of $\Supp(\bsigma(h\pi_{<k}))$ satisfying the required properties.
    Consider then the two cases:

    \begin{itemize}
        \item If $i$ is an optimist: then, by the induction hypothesis maintained during the construction of $\Lambda$, we have $\re_{\bsigma_{\|h}}[\mu_i] = z_i$, and therefore $\prob_{\bsigma_{\|h}}(\mu_i = z_i) > 0$, i.e.:
        $$\sum_k \sum_{w \in E(\pi_k) \setminus \{\pi_{k+1}\}} \prob_{\bsigma_{\|h}}(\pi_{\leq k} w) \prob_{\bsigma_{\|h}}(\mu_i = z_i \mid \pi_{\leq k}w) + \prob_{\bsigma_{\|h}}(\pi) \prob_{\bsigma_{\|h}}(\mu_i \mid \pi) > 0.$$
        But by the remark made above, all the terms of the left sum are zero, hence we actually have $\prob_{\bsigma_{\|h}}(\pi) \prob_{\bsigma_{\|h}}(\mu_i \mid \pi) > 0$, i.e. $\prob_{\bsigma_{\|h}}(\pi) > 0$ and $\mu_i(\pi) = z_i$.

        \item If $i$ is a pessimist, then because of that same remark, for every $k \geq 0$ and each $w \in \Supp\left(\bsigma\left(h\pi_{\leq k}\right)\right)$, there exists a strategy $\tau_i^{kw}$ such that $\re_{\bsigma_{-i\|h\pi_{\leq k}w}, \tau_i^{kw}}[\mu_i] > z_i$.
        By composing all those strategies, we obtain a deviation $\tau_i$ of the strategy $\sigma_{i\|h}$; which, by the induction hypothesis maintained during the construction of $\Lambda$, satisfies the inequality $\re_{\bsigma_{-i\|h}, \tau_i}[\mu_i] \leq z_i$, i.e. either $\prob_{\bsigma_{-i\|h}, \tau_i}(\pi) = \prob_{\bsigma_{\|h}}(\pi) \neq 0$ and $\mu_i(\pi) \leq z_i$ (and then actually $\mu_i(\pi) = z_i$), or:
        $$\min_k \min_{w \in \Supp\left(\bsigma\left(h\pi_{\leq k}\right)\right) \setminus \{\pi_{k+1}\}} \re_{\bsigma_{-i\|h\pi_{\leq k}w}, \tau^{kw}_i}[\mu_i] \leq z_i,$$
        which is impossible by definition of the strategies $\tau_i^{kw}$.
    \end{itemize}

    Then, since $\pi$ has nonzero probability to be achieved, it traverses finitely many stochastic nodes.
    We can therefore define a lasso $\pi^\star$ with $\pi^\star_0 = \pi_0$, $\Occ(\pi^\star) = \Occ(\pi)$ and $\mu(\pi^\star) = \mu(\pi)$, which also traverses finitely many stochastic nodes.
    Then, we can define $\sigma^\star$ in the states of the form $\anchor_{P v}$ as following the positional strategy profile $\btau^P$, which follows deterministically the play $\pi^\star$, and remembering the last vertex seen.
    Then, when a player $i$ deviates and takes an edge that should not be taken, it switches to the strategy profile $\btau^{\dag i}$ and the state $\punish_i$.
    Finally, when an edge is taken that does not belong to $\pi^\star$ but does not correspond to a deviation either, we switch to the state $\anchor_{\emptyset w}$, with the last visited vertex $w$, and to the corresponding strategy profile.

    Let us end by the initialization: the strategy profile $\bsigma^\star$ initializes in the state $\anchor_{\Pi\bot}$.
    In this state, it behaves exactly as in any state of the form $\anchor_{\Pi v}$, but without having memorized a last visited vertex $v$, since there is no such vertex.
    From that state therefore, it necessarily reads the vertex $v_0$, proceeds to the action $\btau^{\anch \Pi}(v_0)$, and switches to the memory state $\anchor_{\Pi v_0}$, unless $v_0$ is already the vertex that the strategy profile $\btau^{\anch \Pi}$ aims at reaching, in which case it proceeds immediately to the corresponding split.


    % \paragraph*{The strategy profile $\bsigma^\star$ has the desired risk entropies.}
\begin{proposition}
    Let $\bsigma^\star$ be a strategy as constructed above from a risk sensitive equilibrium $\bsigma$. Then for each player $i$, $\re_{\bsigma^\star}[\mu_i] = z_i$.
\end{proposition}
\begin{claimproof}
    
    Let $i$ be a player: we want to prove that $\re_{\bsigma^\star}[\mu_i] = z_i$.
    Let us first see how $z_i$ is achieved in the strategy profile $\bsigma$: let $P \subseteq \Pi$ be the smallest set containing $i$ such that there exists a history $h$ with $\Lambda(h) = P$.
    Then, by construction of $\Lambda$, there exists a finite sequence of sets $\Pi = P_0, P_1, \dots, P_m = P$ and of histories $h_1 v_1 w_1, \dots, h_m v_m w_m$ where for each $k$, the history $h_{k+1}$ starts from $w_k$, the history $h_1 v_1 \dots h_k v_k w_k$ is compatible with $\bsigma$, and we have $\Lambda(h_1 v_1 \dots h_k v_k) = P_{k-1}$ and $\Lambda(h_1 v_1 \dots h_k v_k w_k) = P_k$.
    The strategy profile $\bsigma^\star$ follows initially the positional strategy profile $\btau^{\anch \Pi}$, from the vertex $v_0$: with non-zero probability, that strategy profile generates a history $h'_1 v_1$ from $v_0$ to $v_1$.
    From that vertex, it proceeds to a randomized action and, with nonzero probability, moves to the vertex $w_1$ and switches to the positional strategy profile $\btau^{\anch P_1}$, and so on: there is, therefore, a history $h'_1 v_1 h'_2 v_2 \dots h'_m v_m w_m$ that is compatible with the strategy profile $\bsigma^\star$ and after which $\bsigma^\star$ is in the state $\anchor_{P v_m}$, and plays accordingly.

    Then, we are in the case where the set $P$ is no longer split: there is an infinite play $\pi$ from $w_m$ such that $\Lambda(h_1 v_1 \dots h_m v_m \pi_{\leq k}) = P$ for every $k$.
    Then, in the construction of the strategy profile $\bsigma^\star$ we have distinguished two cases: the one where $P$ was a singleton, and the one where it had at least two elements (the empty case is excluded, since $P$ contains the player $i$).
    But in both cases, the strategy profile $\bsigma^\star$ follows the strategy profile $\btau^{\anch P}$ which, with nonzero probability, achieves the payoff $z_i$.
    
    Therefore, the strategy profile $\bsigma^\star$ is such that, with some nonzero probability, player $i$ gets the payoff $z_i$.
    We still have to prove that that player has zero probability of getting a lower payoff (if they are a pessimist) or a higher payoff (if they are an optimist).
    In both cases, that will be the case if we prove that any payoff vector $\bz'$ that has nonzero probability to be achieved in $\bsigma^\star$ had also nonzero probability to be achieved in $\bsigma$.
    Let therefore $\bz'$ be such a payoff vector\footnote{Again, the arguments that follow are actually necessary only because of the case in which $\bz'$ is the zero vector, obtained when no terminal is reached: the case where it corresponds to a terminal could simply be handled by noticing that $\bsigma^\star$ visits only vertices, including the terminals, that have nonzero probability to be visited in $\bsigma$.}.
    Then there is a history $hw$ compatible with $\bsigma^\star$ and a set $P \subseteq \Pi$ such that, after the history $hw$, the strategy profile $\bsigma^\star$ is in state $\anchor_{P \last(h)}$, and from that point, has non-zero probability of achieving payoff vector $\bz'$ while staying in states of the form $\anchor_{P v}$.
    If $P = \emptyset$, then the strategy profile $\btau^{\anch \emptyset}$ has been defined as a uniformly optimal strategy profile for the objective of realizing a payoff vector that has nonzero probability to be achieved in $\bsigma$.
    That objective is achievable, and therefore achieved by $\btau^{\anch \emptyset}$, from the vertex $w$, since that vertex is reached with nonzero probability in the strategy profile $\bsigma$.
    Therefore, the payoff vector $\bz'$ is also achieved with nonzero probability in $\bsigma$.
    If $P$ is a singleton $P = \{j\}$, then similarly, the strategy profile $\btau^{\anch \{j\}}$ has been defined as a strategy profile that is uniformly optimal\theju{what does uniformly optimal mean? I think I missed it} for an objective satisfiable from $w$ that includes avoiding the payoff vectors that have probability $0$ to be achieved in $\bsigma$: therefore, again, the payoff vector $\bz'$ is also achieved with nonzero probability in $\bsigma$.
    If now $P$ has at least two elements, then the strategy profile $\bsigma^\star$ stays in states of the form $\anchor_{P v}$ only along one play, which generates a payoff vector that was also associated with a play that has nonzero probability to be generated in $\bsigma$, hence the same conclusion.
\end{claimproof}

\begin{proposition}
    The strategy profile $\bsigma^\star$ constructed as above is a risk sensitive equilibrium
\end{proposition}
\begin{claimproof}
    We show that no player has an incentive to deviate, and deal with the cases 
    of a player being an optimist or a pessimist. 
    \paragraph*{No optimist has a profitable deviation in $\bsigma^\star$.}

    Let $i$ be an optimist, and let us assume that player $i$ has a profitable deviation from $\bsigma^\star$, i.e. that there is a strategy $\sigma'_i$ of player $i$ such that the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$ achieves with nonzero probability a payoff strictly greater than $z_i$.

    Let us notice that along every play compatible with $\bsigma^\star_{-i}$, the strategy profile $\bsigma^\star$ can have transitions among states of the form $\anchor_{P v}$ for a fixed $P$, from a state of the form $\anchor_{P v}$ to a state of the form $\anchor_{Q w}$ with $Q \subset P$, from a state of the form $\anchor_{P v}$ to the state $\punish_i$, or from $\punish_i$ to itself.
    Therefore, any such play stabilizes either in the state $\punish_i$, or among the states of the form $\anchor_{P v}$ for a fixed set $P$.
    Consequently, if in the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$ player $i$ gets a payoff greater than $z_i$ with nonzero probability, then we can also say that either:
    \begin{itemize}
        \item with nonzero probability, player $i$ gets such a payoff \emph{and} the state $\punish_i$ is reached;

        \item or with nonzero probability, player $i$ gets such a payoff and there exists $P \subseteq \Pi$ such that the strategy profile $\bsigma^\star_{-i}$ remains in states of the form $\anchor_{P v}$.
    \end{itemize}

    The first case is impossible: for any history $hv$ compatible with $\bsigma^\star_{-i}$ such that that strategy profile is in an anchoring state after $h$ and in state $\punish_i$ after $hv$, we have that the vertex $\last(h)$ is also visited with nonzero probability in the strategy profile $\bsigma$.
    Then, if by moving to $v$ player $i$ can get with nonzero probability a payoff greater than $z_i$ against the strategy profile $\bsigma^\star_{-i\|hv} = \btau^{\dag i}_{-i}$, then they can do so against every strategy profile, which induces a profitable deviation in the $\brho$-RSE $\bsigma$.

    As for the second case, let us notice that the strategy profiles of the form $\btau^{\anch P}$ are pure, and therefore that any deviation of player $i$ is immediately detected and leads to a switch to state $\punish_i$.
    Therefore, if $\bsigma^\star$ remains in states of the form $\anchor_{P v}$, it means that player $i$ is actually following the strategy $\sigma^\star_i$, which cannot be a profitable deviation.


    \paragraph*{No pessimist has a profitable deviation.}

    Let $i$ be a pessimist, and let us assume that $i$ has a profitable deviation, i.e. that there exists a strategy $\sigma'_i$ such that in the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$, it is almost sure that player $i$ will get no payoff lesser than or equal to $z_i$.

    First, let us notice that for each history $hv$ compatible with $\bsigma^\star$ such that, after $hv$, the strategy profile $\bsigma^\star$ is in state $\anchor_{P \last(h)}$ with $i \in P$, the vertex $v$ is such that there also exists a history $h'v$ compatible with $\bsigma$ and such that $\Lambda(h'v) = P$, and by the induction hypothesis maintained during the construction of $\Lambda$, we have $\re_{\bsigma_{-i\|h'v}, \tau_i}[\mu_i] \leq z_i$ for every $\tau_i$.
    Therefore, if, from $v$, player $i$ takes an edge that makes $\bsigma^\star_{-i}$ switch to the state $\punish_i$, then with nonzero probability player $i$ gets a payoff lesser than or equal to $z_i$.
    If $\sigma'_i$ is a profitable deviation, such an action is therefore never performed.

    Then, since for every history $hv$ such that the strategy profile $\bsigma^\star$ is in state $\anchor_{P \last(h)}$ and $\bsigma^\star(h)$ is a randomized action, there is at least one $w \in \Supp(\bsigma^\star(hv))$ such that $\bsigma^\star$ is in state $\anchor_{Q v}$ with $i \in Q$ after $hvw$; and that is the case of every $w \in \Supp(\bsigma^\star(hv))$ if $v \in V_i$.
    Thus, if the strategy $\sigma'_i$ observes the constraint described above, there still exists a set $P$ with $i \in P$ such that, with nonzero probability, when following the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$, the strategy profile $\bsigma^\star_{-i}$ stabilizes among memory states of the form $\anchor_{P v}$, and therefore follows the strategy profile $\btau^{\anch P}_{-i}$.
    Moreover, since the strategy profile $\btau^{\anch P}$ is positional, player $i$ must follow the strategy $\tau^{\anch P}_i$, otherwise any other action would lead to a switch to the state $\punish_i$.
    But then, by definition of the strategy profile $\btau^{\anch P}$, player $i$ gets the payoff $z_i$ with nonzero probability, and the deviation is not profitable.
    \end{claimproof}
    The strategy profile $\bsigma^\star$ is a $\brho$-RSE, satisfies the equality $\re_{\bsigma^\star}[\mu] = \re_{\bsigma}[\mu]$, and uses the desired number of memory states.
\end{proof}



\begin{theorem}
    The constrained existence problem of $\brho$-RSEs is $\NP$-complete.
\end{theorem}

\begin{proof}
    The lower bound is given by~\cref{lm:infinite_rho_np_hardness}.

    Now, let $\Game_{\|v_0}$ be a simple quantitative stochastic game, let $\brho$ be a risk-sensitivity vector with only infinite values, and let $\bx$ and $\by$ be threshold vectors.
    By \cref{lm:rse_finite_memory}, if there exists a $\brho$-RSE with $\bx \leq \re_\bsigma[\mu] \leq \by$, there exists one with at most $4np-2n+p+1$ memory states, where $p$ is the number of players and $n$ is the number of vertices.
    Such a strategy profile can be guessed in polynomial time.
    
    We now need to show that it can be checked in polynomial time.
    That can be done as follows:
    \begin{itemize}
        \item First, compute the vector $\re_\bsigma[\mu]$: by \cref{lm:mpd_ptime}, that can be done in polynomial time, by computing the corresponding risk entropies in the Markov chain induced by $\bsigma$ (which has polynomial size).

        \item Second, check that $\bx \leq \re_\bsigma[\mu] \leq \by$.
        That can clearly be done in polynomial time.

        \item Third, check for each player $i$ that player $i$ has no profitable deviation: by ~\cref{lm:mpd_ptime}, that can be done in polynomial time by computing the best risk entropy player $i$ can get in the MDP induced by $\bsigma_{-i}$ (which has polynomial size).
    \end{itemize}
\end{proof}

\leonard{I no longer believe in FPTness: we also need to find the vertices in which the splits occur, and that requires a time exponential in the number of vertices...}

\begin{theorem}\label{thm:infinite_rho_restricted_strategy_np_easy}
    The constrained existence problem of $\brho$-RSEs is $\NP$-complete
    \theju{, and fixed-parameter tractable?} 
    when the number of players is fixed, even when the players are restricted to positional, memoryless,  or pure strategies. 
\end{theorem}
\begin{proof}
    The lowerbound for all three of the above cases follow from \cref{cor:np_hardness_restricted_strat}. 
        \thejaswini{This is not a proof. A proof sketch only.}
    For the upper bound, we show that we can still guess a strategy and verify in polynomial time if it is indeed a strategy. For the cases of positional and memoryless strategies, guessing a strategy is straightforward. Whereas for pure strategies, this requires some work. 
    \paragraph*{Positional strategies and memoryless strategies} The size of such a strategy $\sigma$ that is an RSE can be represented using polynomially many bits, since one only needs to guess the set of edges from each vertex that are being used with non-zero probability. From \cref{lm:mpd_ptime}, we can then verify if the given $\sigma$ indeed gives a simple quantitative payoff within the constraints, and also if it is indeed an RSE in polynomial time. 

    \paragraph*{Pure strategies}
    For pure strategies, the strategies might require more memory to represent. We argue therefore that if there is a winning strategy, there is one that requires only 
\end{proof}