
\subsection{Proof of \cref{thm:memorysmall}}\label{app:memorysmall}

% We first show some simple lemmas that we use during the proof of  \cref{thm:memorysmall}.\theju{to check if this is also needed for later on in PTIME case}
% \begin{lemma} \label{lm:mpd_ptime}
%     Given a simple quantitative MDP $\MDProc$, the quantities $\max_\sigma \pexp_\sigma$ and $\sup_\sigma \oexp_\sigma$ can be computed in polynomial time.
%     As a consequence, the pessimistic and optimistic expectation of a simple quantitative payoff function can also be computed in polynomial time in a Markov chain.
% \end{lemma}

% \begin{proof}
%     \leonard{todo}
% \end{proof}


\memorysmall*

\begin{proof}[Proof of \cref{thm:memorysmall}]
We first define the anchoring set of players $\Lambda$ given a strategy profile~$\bsigma$.
\subparagraph*{Definition of $\Lambda$.}

\finiteMemAbstraction*

\begin{claimproof}
    We define the labelling $\Lambda$ inductively. 

    \subparagraph*{Base case.}
    First, on the one-vertex history $v_0$, we define $\Lambda(v_0) = \Pi$: at the start, all players must be anchored.
    Let us notice that the history $v_0$ satisfies Property~\ref{itm:optimistanchor}, which states that the optimists get the optimistic expectation they are supposed to get, and Property~\ref{itm:pessimistanchor}, which states that the pessimists have no profitable deviations.
    The other properties will be checked in the inductive case.

    \subparagraph*{Inductive case.}
    Suppose $\Lambda(hv)$ has already been defined, where $hv$ is a history compatible with the strategy profile $\bsigma$, and that the five properties are satisfied by $\Lambda$ on all histories on which it is already defined.
    Let $w_1, \dots, w_k$ be the successors of $v$ that are chosen by the strategy $\bsigma(hv)$ with non-zero probability, that is, the support of $\bsigma(hv)$.
    If $k=1$, then we define $\Lambda(hvw_1) = \Lambda(hv)$.
    Note that Properties~\ref{itm:splitsetsanchorwithouti},~\ref{itm:splitsetsanchorwithi}, and~\ref{itm:nosplit} are immediately satisfied, and that Properties~\ref{itm:optimistanchor} and~\ref{itm:pessimistanchor} are satisfied by induction hypothesis.
    
    If $k>1$, we need to partition the set $\Lambda(h)$ between the $k$ successors.
    To do so, we will use the following claim.
    
    \begin{claim}\label{claim:successorAnchor}
    For each player $i \in \Lambda(hv)$ that does not control the vertex $v$, the follwing holds.
    \begin{itemize}
        \item If player $i$ is an optimist, and $\X_i(\bsigma_{\|hv}) = z_i$, then there is at least one successor $w_\l$ such that $\X_i(\bsigma_{\|hvw_\l}) = z_i$.
    
        \item If player $i$ is a pessimist, then there is a successor $w_\ell \in \Supp(\bsigma(hv))$, such that for every  strategy $\tau_i^\ell$ from $w_\l$, we have $\X_i(\bsigma_{-i\|hvw_\ell}, \tau_i) \leq z_i$.
    \end{itemize}
    \end{claim}
    
    \begin{claimproof}
        The first case follows from Property~\ref{itm:optimistanchor} in the induction hypothesis.
    
        As for the second case, we proceed by contradiction.
        Let us assume that for each $w_\l$, there exists a strategy $\tau_i^\l$ such that $\X_i(\bsigma_{-i\|hvw_\l}, \tau^\l_i) > z_i$.
        Then, the strategy $\tau_i$ defined by $\tau_{i\|vw_\l} = \tau^\l$ for each $\l$ is such that $\X_i(\bsigma_{-i\|hv}, \tau_j) > z_i$, which is impossible since $\Lambda(hv)$ is assumed to satisfy Property~\ref{itm:pessimistanchor}.
    \end{claimproof}

    We define each set $\Lambda(hvw_\l)$ by iterating through each element of $\Lambda(hv)$ as follows:
    \begin{itemize}
        \item \textbf{Initialisation.} For all $w_\l$, declare $\Lambda(hvw) = \emptyset$.
        \item \textbf{Iteration over players.} Consider each player $i\in \Lambda(hv)$ sequentially and proceed as follows:
        If $i$ controls the vertex $v$, then add $i$to every set $\Lambda(hvw)$.
        If $i$ does not control $v$, then, by the claim stated earlier, there exists a successor $w_\l$. In this case, add ii to $\Lambda(hvw_\l)$ corresponding to this specific $w_\l)$
    \end{itemize}
    
    Not that the first four properties are thus guaranteed to be satisfied.

    Moreover, when there are several successors $w_\l$ possible, we always favour those such that, at the moment where the decision is taken, the sets $\Lambda(hvw_\l)$ are the smallest.
    This suffices to guarantee Property~\ref{itm:nosplit}.
    \end{claimproof}




\paragraph*{Construction of the strategy profile $\bsigma^\star$}
Based on $\Lambda$ as in \cref{lm:Lambda}, we construct a strategy profile that finite memory states. 
Formally, the definition of the finite-memory strategy $\bsigma^\star$ will be done by defining its memory structure.

The memory states are the following:
    \begin{itemize}    
        \item for each player $i$, the state $\punish_i$;

        \item for each vertex $v$ and each subset $A \subseteq \Pi$ of players such that there exists $h$ with $A = \Lambda(h)$, the state $\anchor_{Av}$;

        \item the state $\anchor_{\Pi\bot}$.
    \end{itemize}
Observe that there are at most $2^p + \Oh(p)$ such memory states.
We now define the transitions from each of those states from each vertex. Observe that long as the memory state does not change, strategy profile corresponds to a positional strategy profile. So, we describe such memoryless strategy profiles and also describe when the memory state changes. 
For each set $A \subseteq \Pi$, we write $W_A$ for the set of vertices that $\bsigma$ may visit while anchoring the set $A$, i.e., the set of vertices $v$ such that there exists a history $hv$ with $\Lambda(hv) = A$.

    \subparagraph*{Punishing memory states $\punish_i$.}
    First, let us define what $\bsigma^\star$ does when in state $\punish_i$, for some player $i$. Those memory states will correspond to the \emph{punishing strategies}, followed when player $i$ deviates from the assigned strategy with the other memory states. 
    By \cref{lm:secretlemma}, there is a positional strategy profile $\btau^{\dag i}_{-i}$ that minimises, from every vertex of the game, the payoff that player $i$ can enforce.
    In addition, we pick an arbitrary positional strategy $\sigma^{\dag i}_i$.
    Then, when the strategy profile $\bsigma^\star$ is in the memory state $\punish_i$ on reads a vertex $v$, the memory update function outputs the vertex $\btau^{\dag i}(v)$ and the same memory state $\punish_i$.

    \subparagraph*{Anchoring states with no player to anchor.}
    Let us now define what happens in memory state $\anchor_{\emptyset v}$.
    Consider the objective of achieving a payoff vector that has positive probability to be achieved in $\bsigma$, while visiting only vertices of $W_\emptyset$.
    Using classical attractor-based proofs (or \cref{lm:secretlemma}), there exists a positional strategy profile that achieves that objective with probability $1$ from every vertex from which that is possible: let us call it $\btau^{\anch \emptyset}$.
    Then, when in memory state $\anchor_{\emptyset v}$ and reading the vertex $w$, we distinguish two cases.
    \begin{itemize}
        \item If $vw$ is an edge that is compatible with the strategy profile $\btau^{\anch \emptyset}$, then the strategy profile $\bsigma^\star$ outputs the vertex $\btau^{\dag i}(w)$, where $i$ is the player controlling $v$, and shifts to the memory state $\punish_i$.

        \item Otherwise, it outputs the vertex $\btau^{\anch \emptyset}(w)$ and moves to the memory state $\anchor_{\emptyset w}$.
    \end{itemize}

    \subparagraph*{Anchoring state with one player to anchor.}
    We can now move to singletons, and define what happens in the states of the form $\anchor_{\{i\} v}$.
    In such a state, we define the strategy based on the that gives player $i$ exactly the extreme risk measure $z_i$.
    More precisely, we want player $i$ to receive payoff $z_i$ with positive probability, and never leave the set of vertices $W_{\{i\}}$ with probability~$1$.
%    of giving, with positive probability, the payoff $z_i$ to player $i$, and maintaining to $0$ the probability of achieving a payoff vector (including $0$ for every player) that has probability zero to be achieved in $\bsigma$, or of visiting a vertex that has probability zero to be visited in $\bsigma$. 
    Using \cref{lm:secretlemma}, there exists a positional strategy profile $\btau^{\anch i}$ that satisfies that property from every vertex from which it is possible.
    Note that that objective is in particular satisfiable, and therefore satisfied by $\btau^{\anch i}$, from every vertex $v \in W_{\{i\}}$.
    Similar to the previous step, we define the strategy profile $\bsigma^\star$ in the states of the form $\anchor_{\{i\} v}$ so that it follows the strategy profile $\btau^{\anch i}$, remembers the last vertex that was visited and uses that memory to switch to the corresponding punishing state when some player $j$ deviates.

        \subparagraph*{Anchoring states with two or more players to anchor.}
    Now, let us consider the states of the form $\anchor_{A v}$, where $A$ has cardinality at least $2$.
    The existence of $\anchor_{A v}$ implies that there is a history $h$ such that $\Lambda(h) = A$.
    Moreover, since each randomisation splits the label of histories in sets that have at most one element in common (Properties~\ref{itm:splitsetsanchorwithouti} and~\ref{itm:splitsetsanchorwithi}), there is only one side of each split that can contain $A$, which implies that among such histories $h$, we can choose one that is a prefix of all others.
    After history $h$, the histories labelled by $A$ form a sequence $h, hv_1, hv_1v_2, \dots$ which may be infinite, end in a terminal vertex, or end with a new split.
    

    \textbf{If that sequence end with a split,} then there is a longest history $hv_1 \dots v_q$ with $\Lambda(hv_1 \dots v_q) = A$ and $k \geq 2$ vertices $w_1, \dots, w_k \in \Supp(\bsigma(hv_1 \dots v_q))$ such that we have $\Lambda(hv_1 \dots v_q w_\l) \neq \emptyset, A$ for each $\l$.
    We can then define a simple history $h'v_q$ that also goes from the vertex $\last(h)$ to the vertex $v_q$, with $\Occ(h'v_q) \subseteq \Occ(\last(h) v_1 \dots v_q)$.
    We then define the strategy profile $\bsigma^\star$ in each state $\anchor_{Av}$ so that it follows the history $h' v_q$ and remembers the last vertex visited, and switches to the state $\punish_i$ and follows the strategy profile $\btau^{\dag i}$ when a given player $i$ deviates and takes an edge that they are not supposed to take.
    Moreover, when an edge is taken that does belong to the history $h'v_q$, but not because a player deviated (it is then necessarily because of a stochastic vertex), the memory switches to the state $\anchor_{\emptyset v}$ (where $v$ is the last vertex seen) and immediately follows the corresponding strategy.
    Finally, when the vertex $v_q$ is reached and the memory is in state $\anchor_{A \last(h')}$, the strategy profile $\bsigma^\star$ chooses randomly between the edges $v_q w_1$, \dots, and $v_q w_k$, all with positive probability.
    Such action will often be referred to as a \emph{split}.


        \textbf{If that sequence is infinite or end in a terminal vertex,} then $\pi^A = v_1 v_2 \dots$ is a play, and satisfies $\Lambda(h\pi^A_{< k}) = A$ for each $k$.
        We can then consider a play $\pi^{A\star}$ with $\Occ(\pi^{A\star}) \subseteq \Occ(\pi^A)$ and $\Inf(\pi^{A\star}) \subseteq \Inf(\pi^A)$ that is either a simple path from $\pi^A_0$ to the terminal reached by $\pi^A$, or, if $\pi^A$ is infinite, a simple lasso (i.e., a play of the form $h'c^\omega$, where the history $h'c$ is simple).
        We can moreover choose $\pi^{A\star}$ so that the set of vertices visited infinitely often (if there are any) in $\pi^{A\star}$ is included in the set of vertices visited infinitely often in $\pi^A$.
Then, we can define $\bsigma^\star$ in the states of the form $\anchor_{A v}$ as following the play $\pi^{A\star}$, and remembering the last vertex seen.
    When a player $i$ deviates and takes an edge that should not be taken, the memory switches to the state $\punish_i$ and follows the strategy profile $\btau^{\dag i}$.
    Finally, when an edge is taken that does not belong to $\pi^{A\star}$ but does not correspond to a deviation either, we switch to the state $\anchor_{\emptyset w}$ where $w$ is the last vertex seen, and to the corresponding strategy profile.



\subparagraph*{Initialisation.}
    The strategy profile $\bsigma^\star$  has the state $\anchor_{\Pi\bot}$ as the initial memory state.
    In this state, it behaves exactly as in any state of the form $\anchor_{\Pi v}$, but without having memorised a last visited vertex $v$, since there is no such vertex.
    From that memory state therefore, it necessarily reads the vertex $v_0$, and starts acting as described in the previous case.


\subparagraph*{The pure case.}
In this construction, the vertices on which the strategy profile $\bsigma^\star$ proceeds to an actual randomisation (i.e., the vertices $v$ such that there exists a history $hv$ such that the support of the distribution $\bsigma^\star(hv)$ contains more than one element) are vertices on which $\bsigma$ also proceeds to such a randomisation.
Therefore, if $\bsigma$ is pure (i.e., if randomisations occur only on stochastic vertices), so is $\bsigma^\star$.





\paragraph*{A combinatorial break: counting states}

    Now that the strategy $\bsigma^\star$ is defined, let us bound the memory it uses.
    There are, obviously, exactly $p$ states of the form $\punish_i$, and one state $\anchor_{\Pi\bot}$.
    To prove that there are at most $3np-2n$ states of the form $\anchor_{Av}$, we need to prove that there are at most $3p-2$ sets $A$ such that there is a history $h$ with $\Lambda(h) = A$.

    Let us call \emph{$\Lambda$-anchored} all such sets $A$.
    By analogy with strategies, we write $\Lambda_{\|hv}$ for the labelling that maps each history $h' \in \Hist\Game_{\|v}$ compatible with $\bsigma_{\|hv}$ to the set $\Lambda(hh')$, and we will also use the notion of anchoredness for each of those labellings $\Lambda_{\|hv}$.
    We proceed by proving the following stronger result.
    
    \begin{proposition}\label{prop:combinatorial}
        For every history $h$ compatible with $\bsigma$, if $\Lambda(h)$ contains at least two elements, then there are at most $3|\Lambda(h)|-2$ sets that are $\Lambda_{\|h}$-anchored.
    \end{proposition}
    


\begin{claimproof}
    For each history $h$, we write $f(h)$ for the number of $\Lambda_{\|h}$-anchored sets that have cardinal at least $2$.
    There are $|\Lambda(h)|+1$ subsets of $\Lambda(h)$ that have cardinality $0$ or $1$: the result will therefore be proved if we prove $f(h) \leq 2|\Lambda(h)| - 3$.
    The proof goes by induction on $m = \Lambda(h) \geq 2$.

    \subparagraph*{Base case.}
    If $m = 2$, the set $\Lambda(h)$ is a pair $\Lambda(h) = \{i, j\}$.
    Then, since the $\Lambda_{\|h}$-anchored sets are all subsets of $\Lambda(h)$, the only set of cardinality at most $2$ that is $\Lambda_{\|h}$-anchored is the pair $\{i, j\}$ itself, hence we have $f(h) = 2 \times 2 - 3 = 1$, as desired.

    \subparagraph*{Inductive case.}
    If $m > 2$, and if we assume that the result is true for every history $h'$ with $2 \leq |\Lambda(h')| \leq m-1$, then let $\{v_1, \dots, v_k\} \subseteq \Supp(\bsigma(h))$ be the set of possible next vertices $v$ such that $|\Lambda(hv)| \geq 2$.

    If $k = 1$, i.e., if $\Lambda(hv_1) = \Lambda(h)$, then we have $f(h) = f(hv_\l)$ and the result for $h$ will be proved if we prove it for $hv_1$.
    Following that reasoning, we can extend the history $h$ until we are not in that case: if we always are, then the only $\Lambda_{\|h}$-anchored sets are $\Lambda(h)$ itself, and possibly the empty sets and some singletons, hence $f(h) = 1$ and the result is immediate.
    We can therefore assume that $k > 1$.

    Let $i$ be the player controlling the vertex $\last(h)$; we set $i = \bot$ if $\last(h)$ is a stochastic node.
    Then, by Properties~\ref{itm:splitsetsanchorwithouti} and~\ref{itm:splitsetsanchorwithi} of \cref{lm:Lambda}  guaranteed during the construction of $\Lambda$, the sets $\Lambda(hv_1) \setminus \{i\}, \dots, \Lambda(hv_k) \setminus \{i\}$ form a partition of $\Lambda(h) \setminus \{i\}$.
    Therefore, no set of cardinality at least $2$ can be simultaneously $\Lambda(hv_\l)$-anchored and $\Lambda(hv_{\l'})$-anchored for $\l \neq \l'$, hence the equality $f(h) = 1+\sum_\l f(hv_\l)$.
    Now, since each set $\Lambda(hv_\l)$ has at least $2$ and less than $m$ elements, we can apply the induction hypothesis to deduce:
    $$f(h) \leq 1 + \sum_{\l=1}^k (2|\Lambda(hv_\l)| - 3).$$
    Moreover, we have $\sum_\l |\Lambda(hv_\l)| \leq m + k - 1$ (each element of $\Lambda(h)$ occurs in one of the sets $\Lambda(hv_\l)$, except possibly one that would occur in all of them), hence the inequality above becomes:
    $$f(h) \leq 1 + 2(m+k-1) -3k$$
    $$= 2m - 1 - k$$
    and since we have assumed $k \geq 2$, we obtain $f(h) \leq 2m-3$.
\end{claimproof}    

Let us recall that $\Lambda(v_0) = \Pi$.
As a particular case of this claim, we obtain, if $p \geq 2$, that there are at most $3p-2$ sets that are $\Lambda$-anchored, as desired.
In the case $p = 1$, the game $\Game_{\|v_0}$ is an MDP, and using \cref{lm:secretlemma}, we can immediately construct $\bsigma^\star$ as a positional strategy (which has therefore $1 \leq 3n \times 1 - 2n + 1 + 1$ memory states) with the same risk measure as $\bsigma$.




    \paragraph*{The strategy profile $\bsigma^\star$ has the desired extreme risk measures.}

We now show that $\X(\bsigma^\star) = \X(\bsigma)$.
Let us recall that we defined $\bz = \X(\bsigma)$.
    
\begin{proposition}\label{prop:ActualPayoff}
    The strategy profile $\bsigma^\star$ satisfies the equality $\X(\bsigma^\star) = \bz$.
\end{proposition}

\begin{claimproof}
    Let $i$ be a player: we want to prove that $\X(\bsigma^\star) = z_i$.
    Let us first see how $z_i$ has positive probability of being obtained in the strategy profile $\bsigma$, and we will then show that no larger (respectively smaller) if $i$ is optimistic (respectively pessimistic) has a positive probability by the same strategy.

   

    \subparagraph*{Player $i$ gets payoff $z_i$ with positive probability.}
    Let $A \subseteq \Pi$ be one of the smallest sets (for the inclusion relation) containing $i$ such that there exists a history $hu$ with $\Lambda(hu) = A$.
    Then, by construction of $\Lambda$, there exists a finite sequence of sets $\Pi = A_0, A_1, \dots, A_m = A$ and of histories $h_1 v_1 w_1, \dots, h_m v_m w_m$ where for each $k$, the history $h_{k+1}$ starts from $w_k$, the history $h_1 v_1 \dots h_k v_k w_k$ is compatible with $\bsigma$, and we have $\Lambda(h_1 v_1 \dots h_k v_k) = A_{k-1}$ and $\Lambda(h_1 v_1 \dots h_k v_k w_k) = A_k$.
    We can then write $hu = h_1 v_1 \dots h_m v_m w_m$.
    
    Consider the strategy profile $\bsigma^\star$, which initially follows  the positional strategy profile $\btau^{\anch \Pi}$. This strategy profile generates, with nonzero probability, a history $h'_1 v_1$ starting from vertex $v_0$ to vertex $v_1$, based on our construction. 
    From that vertex $v_1$, it proceeds to a randomised action and, with positive probability, moves to the vertex $w_1$ and switches to the positional strategy profile $\btau^{\anch A_1}$, and so on: there is, therefore, a history $h'_1 v_1 h'_2 v_2 \dots h'_m v_m w_m$ that is compatible with the strategy profile $\bsigma^\star$ and after which the collective memory is in the state $\anchor_{A v_m}$, and plays accordingly.

    Since $A_m= A$ is the  subset of $\Pi$ where $i\in A = \Lambda(hu)$, we are in the case where the set $A$ is no longer split further by our labelling. That is, there is a play $\pi$ from $w_m$ such that $\Lambda(h \pi_{\leq k}) = A$ for every $k$ such that that is defined.
    Then, in the construction of the strategy profile $\bsigma^\star$ we have distinguished two cases: the one where $A$ was a singleton, and the one where it had at least two elements (the empty case is excluded, since $A$ contains the player $i$).

    \textbf{If $A$ is a singleton,} then after the history $hu$, without any player deviating, all players are following the strategy profile $\btau^{\anch i}$.
    By its definition, that strategy profile achieves the payoff $z_i$ for player $i$ with positive probability.

    \textbf{If $A$ has at least two elements,} then after that same history, all players are following the play $\pi^{A\star}$, which yields the same payoffs as $\pi^A$.
    However, we must still prove that player $i$ actually gets the payoff $z_i$ in $\pi^A$, and that the play $\pi^{A\star}$ is generated with positive probability (i.e. that it does not cross infinitely many stochastic vertices---which we must first show for $\pi^A$).
We do so in the following claim, which we will use again later.

\begin{claim}\label{claim:piA}
    The play $\pi^A$ is (eventually) generated with positive probability when the players follow the strategy profile $\bsigma$.
    Similarly, the play $\pi^{A\star}$ is generated with positive probability when they follow $\bsigma^\star$.
    Both plays yield to each player $j \in A$ the payoff $z_j$.
\end{claim}

\begin{claimproof}
    % Let us first not that using Property~\ref{itm:nosplit} of \cref{lm:Lambda}, we have that for each $k$, the vertex $\pi^A_{k+1}$ was actually the only element of $\Supp(\bsigma(h\pi^A_{<k}))$ satisfying the required properties in the construction of $\Lambda$.
    Let $j \in A$.
    Let us proceed by case disjunction according to the risk measure used by player $j$.

\emph{If player $j$ is an optimist,} then, by Property~\ref{itm:optimistanchor} of \cref{lm:Lambda}, we have $\X_j(\bsigma_{\|h}) = z_j$, and therefore $\prob_{\bsigma_{\|h}}(\mu_j = z_j) > 0$, i.e., by the law of total probability:
        $$\prob_{\bsigma_{\|hu}}(\pi^A) \prob_{\bsigma_{\|h}}(\mu_j = z_j \mid \pi^A) + \sum_k \sum_{w \in E(\pi_k) \setminus \{\pi_{k+1}\}} \prob_{\bsigma_{\|hu}}(\pi^A_{\leq k} w) \prob_{\bsigma_{\|hu}}(\mu_j = z_j \mid \pi^A_{\leq k}w) > 0.$$
        But using Property~\ref{itm:nosplit}, all the terms of the summation on the right are zero, hence the product $\prob_{\bsigma_{\|h}}(\pi^A) \prob_{\bsigma_{\|h}}(\mu_j = z_j \mid \pi^A) > 0$ is positive, i.e. the play $\pi^A$ has  a positive probability of being generated and $\mu_j(\pi^A) = z_j$.

   \emph{If player $j$ is a pessimist,} then because of Property~\ref{itm:nosplit} again, for every $k \geq 0$ and each $w \in \Supp\left(\bsigma\left(h\pi^A_{\leq k}\right)\right)$, there exists a strategy $\tau_j^{kw}$ such that $\X_j(\bsigma_{-j\|h\pi^A_{\leq k}w}, \tau_j^{kw}) > z_j$.
        By composing all those strategies, we obtain a deviation $\tau_j$ of the strategy $\sigma_{j\|h}$; which, by Property~\ref{itm:pessimistanchor}, satisfies the inequality $\X_j(\bsigma_{-j\|h}, \tau_j) \leq z_j$.
        Therefore, either:
        \begin{itemize}
            \item we have:
            $$\min_k \min_{w \in \Supp\left(\bsigma\left(h\pi^A_{\leq k}\right)\right) \setminus \{\pi^A_{k+1}\}} \X_j(\bsigma_{-j\|h\pi_{\leq k}w}, \tau^{kw}_j) \leq z_j,$$
            which is impossible by definition of the strategies $\tau_j^{kw}$;

            \item or we have $\prob_{\bsigma_{-j\|h}, \tau_j}(\pi^A) = \prob_{\bsigma_{\|h}}(\pi^A) \neq 0$ and $\mu_j(\pi^A) \leq z_j$, and then actually $\mu_j(\pi^A) = z_j$.
        \end{itemize}
        
         


We have thus proven that player $j$ gets the payoff $z_j$ in $\pi^A$, and that the play $\pi^A$ is generated with positive probability in $\bsigma$.
The analogous results about $\pi^{A\star}$ follow using the equalities $\Occ(\pi^{A\star}) = \Occ(\pi^A)$ and $\Inf(\pi^{A\star}) = \Inf(\pi^A)$.
\end{claimproof}

    In those two cases (if $A$ is a singleton or has several elements), we obtain that the strategy profile $\bsigma^\star$ is such that, with some positive probability, player $i$ gets the payoff $z_i$.


    \subparagraph*{Player $i$ gets risk measure $z_i$.}
    We still have to prove that player $i$ has zero probability of getting a lower payoff (if they are a pessimist) or a higher payoff (if they are an optimist).
    To show both cases, we prove the following claim:

    \begin{claim}
        Every payoff vector that has a positive probability of being achieved in the strategy profile $\bsigma^\star$ also has a positive probability of being achieved in the strategy profile $\bsigma$.
    \end{claim}

\begin{claimproof}
    Let $\bz'$ be such a payoff vector.
    Then, there is a history $hw$ compatible with $\bsigma^\star$ and a set $A \subseteq \Pi$ such that, after the history $hw$, the strategy profile $\bsigma^\star$ is in state $\anchor_{A \last(h)}$, and from that point it has a nonzero probability of achieving the payoff vector $\bz'$ while staying in states of the form $\anchor_{A v}$.
    
    \emph{If $A$ is empty}, then the strategy profile $\btau^{\anch \emptyset}$ has been defined as a strategy profile that almost surely generates a payoff vector that is generated with positive probability by $\bsigma$, from every vertex from which that is possible.
    That requirement is satisfiable, and therefore satisfied by $\btau^{\anch \emptyset}$, from the vertex $w$, since that vertex is itself reached with positive probability in the strategy profile $\bsigma$.
    Therefore, the payoff vector $\bz'$ is also achieved with positive probability in $\bsigma$.
    
    \emph{If $A$ is a singleton,} say $A = \{j\}$, then the strategy profile $\btau^{\anch j}$ has been defined so that from every vertex from which that is possible, on the one hand, it generates the payoff $z_j$ with positive probability, and on the other hand, it is almost sure that the payoff vector that will be generated has also positive probability to be generated in $\bsigma$.
    Similarly as above, that requirement is satisfiable from the vertex $w$, since $\bsigma_{\|hw}$ satisfies it.
    Therefore, again, the payoff vector $\bz'$ is also achieved with positive probability in $\bsigma$.
    
    \emph{If $A$ has at least two elements}, then the strategy profile $\bsigma^\star$ stays in states of the form $\anchor_{A v}$ only along one play, namely $\pi^{A\star}$, and that play generates a payoff vector that was also associated with the play $\pi^A$ that by \cref{claim:piA}, has positive probability to be generated in $\bsigma$, hence the same conclusion.
\end{claimproof}

This proves the equality $\X_i(\bsigma^\star) = z_i$.
\end{claimproof}


\paragraph*{The strategy profile $\bsigma^\star$ is an XRSE.}

We have now constructed the finite-memory strategy profile $\bsigma^\star$, showed that it had the expected number of memory states, and that it generates the expected risk measures.
We must now give the final argument for our construction: that strategy profile is also an extreme risk-sensitive equilibrium.
We will prove that result by showing separately that optimists have no profitable deviations, and then that neither do pessimists.

\begin{proposition}\label{prop:NodeviationOpt}
    No optimist has a profitable deviation in $\bsigma^\star$.
\end{proposition}

\begin{claimproof}
    Let $i$ be an optimist, and let us consider a deviation $\sigma'_i$ of that player from $\bsigma^\star$. Let us write $z'$ for the risk measure $z' = \X_i(\bsigma^\star_{-i}, \sigma'_i)$.

    Let us notice that along every play compatible with $\bsigma^\star_{-i}$, the transitions that are possible in the memory structure of the strategy profile $\bsigma^\star$ can be classified as follows:
    \begin{itemize}
        \item transitions among states of the form $\anchor_{A v}$ for a fixed $A$;
        
        \item transitions from a state of the form $\anchor_{A v}$ to a state of the form $\anchor_{B w}$ with $B \subset A$;

        \item transitions from a state of the form $\anchor_{A v}$ to the state $\punish_i$;

        \item and transitions from $\punish_i$ to itself
    \end{itemize}
    Therefore, any such play stabilises either in the state $\punish_i$, or among the states of the form $\anchor_{A v}$ for a fixed set $A$.
    Consequently, if in the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$ player $i$ gets the payoff $z'$ with positive probability, then we can also say that either:
    \begin{itemize}
        \item with positive probability, player $i$ gets the payoff $z'$ \emph{and} the state $\punish_i$ is reached;

        \item or there exists a set $A \subseteq \Pi$ such that with positive probability, player $i$ gets the payoff $z'$, and the collective memory remains in states of the form $\anchor_{A v}$.
    \end{itemize}

    \emph{In the first case,} let us consider a history $hv$ compatible with $\bsigma^\star_{-i}$ such that the collective memory is in an anchoring state after $h$ and in state $\punish_i$ after $hv$.
    If player $i$ can obtain the risk measure $z'$ by going to $v$ from that vertex against $\bsigma^\star_{-i\|h}$, and therefore, against the punishing strategy profile $\btau^{\dag i}_{-i}$, it means that they can enforce that risk measure against every possible strategy profile from $\last(h)$.
    On the other hand, if the collective memory is in an anchoring state after $h$, it means that the vertex $\last(h)$ is also visited with positive probability in the strategy profile $\bsigma$ (otherwise we would have switched to a punishing state earlier).
    There is therefore a history $h'$ compatible with $\bsigma$ such that $\last(h) = \last(h')$; and after that history, against the strategy profile $\bsigma_{\|h'}$, player $i$ also has the possibility of getting with positive probability the payoff $z'$.
    Since $\bsigma$ is an XRSE, that implies $z' \leq z_i$.

    \emph{In the second case,} let us notice that the strategy profiles of the form $\btau^{\anch A}$ are pure, and therefore that any deviation of player $i$ is immediately detected and leads to a switch to state $\punish_i$.
    Therefore, if the collective memory remains in states of the form $\anchor_{A v}$, it means that player $i$ is actually following the strategy $\sigma^\star_i$.
    Thus, we also have $z' \leq z_i$.
    
    The strategy $\sigma'_i$ is not a profitable deviation from $\bsigma^\star$.
\end{claimproof}

We can now end the proof with the dual proposition.

\begin{proposition}
    No pessimist has a profitable deviation in $\bsigma^\star$.
\end{proposition}

\begin{claimproof}
    Let $i$ be a pessimist, and consider a deviation $\sigma'_i$ of that player from $\bsigma^\star$.
    We intend to prove that the deviation $\sigma'_i$ is not profitable, that is, when following the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$, there is still a positive probability that player $i$ receives a payoff smaller than or equal to $z_i$.
    Using \cref{lm:secretlemma}, we can assume without loss of generality that $\sigma'_i$ is pure.

    First, we observe that for each history $hv$ compatible with $\bsigma^\star$ such that, after $hv$, the collective memory is in state $\anchor_{A \last(h)}$ with $i \in A$, the vertex $v$ is such that there also exists a history $h'v$ compatible with $\bsigma$ with $\Lambda(h'v) = A$.
    By Property~\ref{itm:pessimistanchor} of \cref{lm:Lambda}, we have $\X_i(\bsigma_{-i\|h'v}, \tau_i) \leq z_i$ for every $\tau_i$.
    Therefore, if player $i$ accepts to follow the history $hv$ and, then, deviates and takes an edge that makes the collective memory switch to the state $\punish_i$, then with positive probability player $i$ gets a payoff lesser than or equal to $z_i$.
    If such an action is ever performed, then the deviation $\sigma'_i$ is not profitable.

    Let us now assume that $\sigma'_i$ performs no such action: after every history $hv \in \Hist_i \Game_{\|v_0}$, if the collective memory is in a state of the form $\anchor_{A \last(h)}$ with $i \in A$, the vertex $\sigma'_i(hv)$ belongs to the set $\Supp(\sigma^\star_i(hv))$.
    Then, by Property~\ref{itm:splitsetsanchorwithi} of \cref{lm:Lambda}, we also have $i \in \Lambda(hv\sigma'_i(hv))$.
    Thus, there still exists a set $A$ with $i \in A$ such that, with positive probability, when following the strategy profile $(\bsigma^\star_{-i}, \sigma'_i)$, the strategy profile $\bsigma^\star_{-i}$ stabilises among memory states of the form $\anchor_{A v}$; and then, the strategy profile $\bsigma^\star$ only proceeds to pure actions, hence the strategy $\sigma'_i$ is actually following $\sigma^\star_i$.

    Using the same arguments as in the proof of \cref{prop:ActualPayoff} (definition of $\btau^{\anch i}$ in case $A = \{i\}$, and \cref{claim:piA} in case $A$ has more elements), we can then conclude that the player $i$ gets the payoff $z_i$ with positive probability and therefore the deviation $\sigma'_i$ is not profitable.
    \end{claimproof}

    The strategy profile $\bsigma^\star$ is an XRSE, satisfies the equality $\X(\bsigma^\star) = \X(\bsigma)$, and uses the desired number of memory states.
    Furthermore, if $\bsigma$ is pure, so is $\bsigma^\star$.
\end{proof}
\input{8App3NPhard}