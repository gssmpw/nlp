We assume that the reader is familiar with the basics of probability and graph theory. However, we define some concepts for establishing notation. 

%\thejaswini{We need a good prelims set for probability here.}
%G
\subparagraph*{Probabilities.} Given a (finite or infinite) set of outcomes $\Omega$ and a probability measure $\prob$ over $\Omega$, let $X$ be a random variable over $\Omega$, that is, a mapping $X: \Omega \to \Rb$. We then write $\Eb_\prob[\X]$, or simply $\Eb[X]$, for the expectation of $X$, when it is defined.
%For a random variable $X$, we write $\Eb_M[X]$ to denote the expectation of $X$ to represent the exception of the variable $X$ when the probability measure or distribution over the set of outcomes is $M$. Mostly, when $M$ is known from context, we omit such a measure $M$.
Given a finite set $S$, a \emph{probability distribution} over $S$ is a mapping $d: S \to [0,1]$ that satisfies the equality $\sum_{x \in S} d(x) = 1$.
We write $\Supp (d)$ for the \emph{support} of the distribution $d$, that is, the set of elements $x \in S$ such that $d(x) > 0$.

\subparagraph{Risk measures.}
Given a set $\Omega$ of outcomes, a \emph{risk measure} over $\Omega$ is a mapping $M$ which maps a probability measure $\prob$ over $\Omega$ and a random variable $X$ to a real value $M^\prob[X]$.

Sometimes, in the literature, risk measures are expected to have the following three properties: (1) they are \emph{normalised}, i.e., we have $M^\prob[0] = 0$; (2) they are  \emph{monotone}, i.e., the pointwise inequality $X \leq Y$ implies $M^\prob[X] \leq M^\prob[Y]$; and (3) they are \emph{translative}, i.e., $M^\prob[X + c] = M^\prob[X] + c$ for every constant $c$.
In particular, the expectation of a random variable $\Eb$ is a risk measure.
We do not refer to the above properties again and only state them here to remark that all the risk measures we will consider satisfy the above properties.
We remark that the definition of translative sometimes instead refers to satisfying the opposite of the property we define as translative, $M^\prob[X + c] = M^\prob[X] - c$ for every constant $c$. This is a matter of whether we use our risk measure or its negation.
%This property will not be satisfied by the risk measures we consider, as we want our risk measures to capture the perceived payoff, and not its opposite.
%However, we choose to not use this definition of the translative, as we also want our risk measure to capture perceived reward. % so that they correspond to not inherent measure of risk, but rat  

%as an output such that it is normalised ($\mathbb{RM}(0) = 0$), translative ($\mathbb{RM}(X + c) = \mathbb{RM}(X) + c$) and monotone $X\leq Y\implies \mathbb{RM}(X)\leq \mathbb{RM}(X')$.
%The \emph{perceived reward} on a game $\Game$ for a fixed strategy $\bsigma$ of a risk-measure $\mathbb{RM}$, represented by  $\pReward_{\bsigma}^i$ is $\mathbb{RM}(\prob_{\bsigma\mu_i})$. This value is exactly  the risk measure of the random variable that takes the value equal to player $i$'s outcomes based on the probability distribution by the strategy profile $\bsigma$.


\subparagraph*{Graph, paths, games.}A directed graph $(V,E)$ consists of a set of \emph{vertices} $V$ and a set of ordered pair of vertices, called \emph{edges}, $E$. 
In a directed graph $(V, E)$, for each vertex $u$, we write $E(u)$ to represent the set $E \cap (\{u\} \times V)$.
For simplicity, we often write $uv$ for an edge $(u, v)\in E$.
A \emph{path} in the directed graph $(V, E)$ is a (finite or infinite) word $\pi = \pi_0 \pi_1 \dots$ over the alphabet $V$ such that $\pi_n\pi_{n+1} \in E$ for every $n$ such that $\pi_n$ and $\pi_{n+1}$ exist.
We write $\Occ(\pi)$ for the set of vertices occurring along $\pi$, and $\Inf(\pi)$ for those that occur infinitely often, if there are any.
The prefix $\pi_0 \dots \pi_n$ is written as $\pi_{\leq n}$ or $\pi_{< n+1}$, and the suffix $\pi_n \pi_{n+1} \dots$ is written as $\pi_{\geq n}$ or $\pi_{>n-1}$.
A finite path $\pi = \pi_0 \dots \pi_n$ is \emph{simple} if every vertex occurs at most once along $\pi$.
It is a \emph{cycle} if its last vertex $\pi_n$ is such that $\pi_n\pi_0 \in E$.
%\theju{deleted some definitions about $\Paths_{<\infty}$ here @L\'eonard, can you check? if you ever use it?}
%Given a graph $(V, E)$, we will write $\Paths_{<\infty}(V, E)$ (resp. $\Paths_\infty(V, E)$) the set of finite (resp. infinite) paths in $(V, E)$, and $\Paths_{<\infty}(V, E)_{\|v_0}$ (resp. $\Paths_\infty(V, E)_{\|v_0}$) for the set of finite (resp. infinite) paths in $(V, E)$ that start in the vertex $v_0 \in V$.


\begin{definition}[Game]
    A \emph{game} is a tuple $\Game = (V, E, \Pi, (V_i)_{i \in \Pi}, \p, \mu)$, where we have:
    \begin{itemize}
        \item a directed graph $(V, E)$, called the \emph{underlying} graph of $\Game$;

        \item a finite set $\Pi$ of \emph{players};

        \item a partition $(V_i)_{i \in \Pi \cup \{?\}}$ of the set $V$, where $V_i$ denotes the set of vertices \emph{controlled} by player $i$, and the vertices in $V_?$ are called \emph{stochastic vertices};

        \item a \emph{probability function} $\p: E(V_?) \to [0, 1]$, such that for each stochastic vertex $u$, the restriction of $\p$ to $E(u)$ is a probability distribution;

        \item a mapping $\mu: T \to \Rb^\Pi$ called \emph{payoff function}, where $T$ is the set of \emph{terminal vertices}, i.e. vertices of the graph $(V, E)$ that have no outgoing edges.
        We also write $\mu_i$, for each player $i$, for the function that maps a terminal vertex $t$ to the $i^\text{th}$ coordinate of the tuple $\mu(t)$.
    \end{itemize}
\end{definition}
%Any finite path that ends in a vertex in $T$ is said to give payoff $\mu_i$ for player $i$, and infinite paths and paths that do not end in $T$ give payoff $0$ to player $i$. We therefore often say payoff of a path for player $i$ to refer to the above.

In a more general framework, payoffs can be assigned to all infinite paths.
Here, we only focus on what is usually called \emph{simple quantitative games}, i.e. games in which the underlying graph contains terminal vertices and the payoffs depend only on which terminal vertex is eventually reached.
We thus extend the mapping $\mu$ to the set $(V \setminus T)^\omega \cup (V \setminus T)^* T$ by defining $\mu(v_1 \dots v_k t) = \mu(t)$, and $\mu(v_1 v_2 \dots) = (0)_{i \in \Pi}$ (if no terminal vertex is reached, everyone gets the payoff $0$).
A game is \emph{Boolean} if all payoffs belong to the set $\{0, 1\}$.

An \emph{initialised game} is a tuple $(\Game, v_0)$, usually written $\Game_{\|v_0}$, where $v_0 \in V$ is an \emph{initial vertex}.
In what follows, when the context is clear, we use the word \emph{game} also for an initialised game.
We often assume that we are given a game $\Game_{\|v_0}$ and implicitly use the same notations as in the definition above.

\subparagraph*{Histories and plays.} We call \emph{play} a path in the underlying graph that is infinite, or whose last vertex is terminal.
Other paths are called \emph{histories}.
We will then use the notations $\Hist(\Game)$ to denote finite paths in the graph of the game, and $\Plays(\Game)$ to denote both finite and infinite paths. For a history $h = h_0 \dots h_n$, we write $\last(h) = h_n$.
We will also write $\Hist_i(\Game)$ for the set of histories whose last vertex is controlled by player $i$.
%If the game $\Gc$ is clear from context, we omit it.
A history or play in an initialised game $\Game_{\|v_0}$ is a history or play in $\Game$ whose first vertex is $v_0$.




\begin{definition}[Markov decision process, Markov chain]
    A (initialised or not) \emph{Markov decision process} is a game with one player.
    A \emph{Markov chain} is a game with zero player.
\end{definition}




\paragraph*{Strategies, and strategy profiles}

% \subparagraph*{Stable set.} A subset of vertices $W$ is a \emph{stable} vertex set in $\Game_{\|v_0}$ if\theju{What about the word closed instead of stable? Can this be moved to the appendix, since we only need it for the proofs?}
% \begin{itemize}
%     \item every $v \in W$ is accessible from $v_0$ using only vertices of $W$, 
%     \item for every $v \in W \setminus V_?$, we have $E(v) \cap W \neq \emptyset$, and 
%     \item for every $v \in W \cap V_?$, we have $E(v) \subseteq W$.
% \end{itemize}

    In a game $\Game_{\|v_0}$, a \emph{strategy} for player $i$ is a mapping $\sigma_i$ that maps each history $hu \in \Hist_i(\Game_{\|v_0})$ to a probability distribution over $E(u)$.
The set of possible strategies for player $i$ in $\Game_{\|v_0}$ is written as $\Strat_i(\Game_{\|v_0})$.
A path $\pi_0 \pi_1 \dots$ (be it a history or a play) is \emph{compatible} with the strategy $\sigma_i$ if for each $k$ such that $\pi_k \in V_i$, we have the probability that the strategy $\sigma_i$ proposes $h_{k+1}$ after  history $h_{\leq k}$ is positive, that is, $\sigma_i(h_{\leq k})(h_{k+1}) > 0$.
A \emph{strategy profile} for a subset $P \subseteq \Pi$ is a tuple $(\sigma_i)_{i \in P}$.
A strategy profile for the set $P$ of players is written $\bsigma_P$, or simply $\bsigma$ when $P = \Pi$. We also write $\bsigma_{-i}$ for $\bsigma_P$ where $P = \Pi \setminus \{i\}$.
Similarly, we use $(\sigma_{-i}, \sigma'_i)$ to denote the strategy profile $\btau$ defined by $\tau_i = \sigma'_i$ and $\tau_j = \sigma_j$ for $j \neq i$. 
We sometimes write $\bsigma(hv)$ to mean $\sigma_i(hv)$ where $i$ is the player controlling $v$, or $\p(v)$ when $v \in V_?$.
We sometimes also write $V_{-i}$ instead of $\bigcup_{j \neq i} V_j$. %, or $\Hist_{-i}$ for $\bigcup_{j \neq i} \Hist_j$\leon{Do we?}.

For some history $h$, and a strategy $\sigma_i$, we define the strategy truncated to a history $h$, written $\sigma_{i\| hv}$, as the strategy $\sigma'_i: h' = \sigma_i(hh')$ in the game $\Game_{\|v}$.

A strategy profile $\bsigma_{-i}$ in the game $\Game_{\|v_0}$ defines an initialised Markov decision process $\Game_{\|v_0}(\bsigma_{-i})$, where the vertices of the (infinite) underlying graph are the histories of $\Game_{\|v_0}$ and the edges are added from $hu$ to each the history $huv$ iff  $uv \in E$.
Similarly, a strategy profile $\bsigma$ for $\Pi$ defines an initialised Markov chain $\Game_{\|v_0}(\bsigma)$.
Thus, it also defines a probability measure $\prob_\bsigma$ over plays --- which turns the payoff functions $\mu_i$ into random variables.
%We omit the game $\Gc$ and $v_0$ if that is clear from context. Alternatively, if we consider a strategy truncated to a history $h$, then we instead write $\prob_{\sigma_{\| h}}$.


    
\subparagraph{Pure, stationary, and positional strategies.}
We say that a strategy $\sigma_i$ is \emph{pure} when for each history $hu$, there is a vertex $v$ such that $\sigma_i(hu)(v) = 1$; then we often just write $\sigma_i(hu) = v$. 
%Consider the equivalence relation over histories, defined by $h \approx_{\sigma_i} h'$ if and only if the two strategies $\tau_i: h'' \mapsto \sigma_i(hh'')$ and $\tau_i': h'' \mapsto \sigma_i(h'h'')$ are identical. The number of such classes will be called the \emph{memory} of $\sigma_i$. 
%A strategy $\sigma_i$ is \emph{finite-memory} if the equivalence relation $\approx_{\sigma_i}$ has finitely many classes. 
We say that $\sigma_i$ is \emph{stationary} when for every two histories $hu, h'u \in \Hist_i(\Game_{\|v_0})$, we have $\sigma_i(hu) = \sigma_i(h'u)$.
In that case, we sometimes assume that strategy $\sigma_i$ is defined in every game $\Game_{\|u}$ and simply write $\sigma_i(u)$ for $\sigma_i(hu)$.
Finally, the strategy $\sigma_i$ is \emph{positional} when it is pure and stationary.
Those concepts are naturally generalised to strategy profiles.

\subparagraph*{Memory structures.}
%Observe that we can represent a stationary strategy as a function that maps every vertex to a distribution, and similarly, a positional strategy as a function that maps every vertex to another vertex.
% We represent the set of all stationary strategies for player $i$ using $\Mless_i(\Game)$ and the set of all positional strategies of player $i$ as $\Pos_i(\Game)$.
A \emph{memory structure} for player $i$ is a tuple $(S_i, s_0, \delta_i, \nu_i)$, where $S_i$ is a finite set of \emph{memory states}, where $s_0 \in S_i$ is an \emph{initial state}, where $\delta_i$ is a \emph{memory-update mapping} that maps each pair $(s, v) \in S_i \times V$ to a memory state $s'$, and where $\nu_i$ is an \emph{output mapping} that maps each pair $(s, v) \in S_i \times V_i$ to a distribution $d$ over $E(v)$.
The memory-update mapping can be extended to a mapping $\delta_i^*: \Hist(\Game_{\|v_0}) \to S_i$ with $\delta_i^*(\epsilon) = m_0$ and $\delta_i^*(hu) = \delta_i(\delta_i^*(h), u)$ for each history $hu$.
The memory structure then induces a strategy $\sigma_i$ defined by $\sigma_i(hu) = \nu_i(\delta^*_i(h), u)$ for each history $hu \in \Hist_i\Game_{\|v_0}$.
A strategy induced by a memory structure is called \emph{finite-memory strategy}.
Note that stationary strategies are exactly the strategies that are induced by a memory structure with $|S_i| = 1$.

%We analogously define memory structures for all players. 
A tuple of finite-memory strategy profiles for each player is a \emph{finite-memory strategy profile}  is given by a similar object $(S, s_0, \delta, \nu)$ to memory structure, where by replacing $\nu$ by its restriction to $S \times V_i$, we obtain a memory structure that induces the strategy $\sigma_i$.%\footnote{Here, it might be useful to point out that our model of memory structure is deterministic, even though the outputs are probability distributions: after a given history $h$, the memory state $\delta^*(h)$ is deterministically defined.
%This prevents the players from behaviours in which they would, for example, toss a common coin at the beginning of the game, and all play according to the outcome: such behaviours do not fit in our formalism of strategy profiles, which assumes implicitely that they cannot communicate during the game, since each action can be based only on the vertices that have been visited at a given point of time.}.



% Given any finite set $M_i$, and a memory update map $\nu_i$ that is a function $M_i\times V\to M_i\times \Dist(V)$, and an initial memory state $m_\init^i$, then we can construct a finite memory strategy $\sigma_i$
% which we define recursively. For the empty history $\epsilon$, the \emph{current memory state} is $m_\init^i$, 
% for a history $hv$, where the memory state $m$ of history $h$, the memory state of the history $hv$ is defined as $m'$.
% where 
% $(\Dc,m') = \nu_i(m,v)$. 
% We define the strategy $\sigma_i(h)$ for player $i$ as 
% and the distribution $\Dc$ obtained above if $v\in V_i$. 
% %Note that if $v\notin V_i$, then the distribution is the empty-distribution, otherwise, this does not represent a valid strategy.  %\theju{empty distribution to be defined}

% We therefore call such a tuple $\Mc_i = (M_i,\nu_i,m_\init^i)$ as a memory structure for player $i$, and $M_i$ memory states of the strategy $\sigma_i$. A memory structure can be smaller than the number of equivalence classes in $\approx_{\sigma_i}$ since all elements of an equivalence class must correspond to histories that have the same last vertex, whereas the same is not true for elements of $M_i$ in a memory structure.  

% For a finite set $M$ and a memory update function $\nu$ such that $M\times V\to M\times \Dist(V)$, along with  $p$ initial memory functions $\Bar{m_\init} =(m_\init^1,\dots,m_\init^p)$, we can also describe $p$ different memory-structures for $p$ players $\Mc_i = (M, \nu, m_\init^i)$ for each $i\in \{1,\dots,p\}$.
% We refer to such an $M$ as the memory states of a strategy of a \emph{coalition} $\{1,\dots,p\}$ of players. We define coalition and strategies profiles in more detail below. 

% Sometimes, we deal with finite memory strategies of specific types, composed of finitely many positional or stationary strategies. In such cases we call such objects memory structure.
% If $M = \Mc_i\times V$, such that $\mu_i(m, v) = $
% A memory structure $\Mc_i$ is defined 

% A specific kind of 
% finite memory strategies are those that with a memory-structure $M$ consists of a finite set $M\times \Mless(\Gc)$ along with transitions from $M\times $



% Suppose each player has a finite memory strategy with a memory structure and a memory-update function $(M_i,\nu_i)$, then this gives a memory structure $(\Bar{M},\Bar{\nu})$, where $M = \prod_i M_i$ and $\nu$ takes vertex $v$ and based on weather 
% To represent a strategy profile in which each player's strategy is finite memory, 
% \begin{itemize}
%     \item the vertices of the (infinite) underlying graph are the histories of $\Game_{\|v_0}$;

%     \item there is an edge from each vertex $hu$ to each vertex $huv$ such that $uv \in E$, and only there;

%     \item the initial vertex is $v_0$;

%     \item the unique player $i$ controls the set $\Hist_i\Game_{\|v_0}$;

%     \item the probability function $\p_{\sigma_i}$ is defined by $\p_{\sigma_i}(hu, huv) = \p(uv)$ if $u \in V_?$, and $\p_{\sigma_i}(hu, huv) = \bsigma_{-i}(hu)(v)$ if $u \in V_{-i}$;

%     \item the payoff function is $\mu_i$.
% \end{itemize}





\subparagraph{Risk-sensitive equilibria, constrained existence problem.}
In multiplayer stochastic games, we wish to study generalisations of the classical Nash equilibria where the expectation is replaced by other risk measures.
Such generalisations are called \emph{risk-sensitive equilibria}~\cite{Now05}. We define this for games played over graphs
%A strategy profile is a risk-sensitive equilibrium if no player can reduce their risk measure by deviating from the strategy.
%The concept is similar to Nash equilibria when the payoffs are instead replaced with the player's perceived risk.  

When $M$ is a risk measure and $\bsigma$ is a strategy profile, we write $M(\bsigma)$ for $M^{\prob_\bsigma}$.

\begin{definition}[Risk-sensitive equilibrium]
    Let $\Game_{\|v_0}$ be a game, and let $\bM = (M_i)_{i \in \Pi}$ be a tuple of risk measures.
    Let $\bsigma$ be a strategy profile in $\Game_{\|v_0}$, let $i$ be a player, and let $\sigma'_i$ be a strategy for player $i$, called \emph{deviation} of player $i$ from $\bsigma$.
    The deviation $\sigma'_i$ is \emph{profitable} with regards to the risk measure $M_i$ if we have $M_i(\bsigma_{-i}, \sigma'_i)[\mu_i] > M_i(\bsigma)[\mu_i]$
    The strategy profile $\bsigma$ is a $\bM$-\emph{risk-sensitive equilibrium}, or $\bM$-RSE, if no player $i$ has a profitable deviation from $\bsigma$ with regards to $M_i$.
\end{definition}


% We sometimes say that 
% in an MDP $\MDProc$, a strategy $\sigma_i$ is \emph{risk-optimal} for player $i$ if for every strategy $\sigma'_i$, we have $\M(\sigma'_i)[\mu_i] \leq \M(\sigma_i)[\mu_i]$. Notice above that in a risk-sensitive equilibria $\bsigma$ in a game, for every player, the MDP obtained by considering the game with the strategy $\sigma$ when viewed as an MDP against the strategies of other players, \leon{Do we use this?}
% \theju{I think this paragraph is not used here at all. To verify maybe(?). I never use it}\theju{@Leonard, check if this is needed anywhere and remove?}

The following problem is the main focus throughout our paper.

\begin{question}[Constrained existence of risk-sensitive equilibria]
    Given a game $\Game_{\|v_0}$, a tuple of risk measures $\bM$, and two payoff vectors $\bx, \by \in \Qb^\Pi$, does there exist a $\bM$-RSE $\bsigma$ in $\Game_{\|v_0}$ such that for each $i \in \Pi$, we have $x_i \leq M_i(\bsigma)[\mu_i] \leq y_i$?
\end{question}

To turn this problem into an algorithmic decision problem, we need to restrict it to some specific sets of risk measures that can be finitely encoded. 
That is what we do in the sequel of this paper, with the \emph{entropic risk measure}, and later with the \emph{extreme risk measure}.

%\thejaswini{Write a note on randomness used.}




