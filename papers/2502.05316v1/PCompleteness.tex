% \begin{theorem}
%     The constrained existence problem of $\Bar{\rho}$-RSEs in $\PTIME$-complete if all the players are optimists, i.e, $\brho = \tpl{-\infty}^{\Pi}$.
% \end{theorem}

% \subsection{Upper bound}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \thejaswini{working on this}
    \begin{itemize}
        \item \emph{Definition of the algorithm}

  

        The algorithm will proceed by constructing a decreasing sequence $E_0, E_1, \dots$ of subsets of $E$.
        At each step, we will always leave at least one outgoing edge from each vertex, and we will never remove outgoing edges of stochastic vertices.
        In the cycle-averse case, we will keep the induction hypothesis that for each $k \geq 1$, there is never a vertex $v$ that is accessible from $v_0$ in $(V, E_k)$ and such that no terminal vertex is accessible from $v$.
        
        Moreover, a preprocessing computes for each vertex $v$ the adversarial optimistic risk measure the player who controls the vertex $v$ can ensure against every strategy profile from $v$. That is, we compute $\val(v) = \inf_{\btau_{-i}} \sup_{\tau_i} \oexp^\btau)[\mu_i]$ where player $i$ controls $v$. 
        All those values can be computed in time $O(pn)$ by TODO.

        Let $E_0 = E$.
        Step $k$ starts when $E_k$ is defined.

        At each step, we then define the strategy profile $\bsigma^k$ as follows:
        \begin{itemize}
            \item In the cycle-averse case: from each vertex $v$, the strategy profile randomizes uniformly between all the edges $vw \in E_k$.
            Note that, when $k \geq 1$, the induction hypothesis implies that from every vertex $v$ accessible from $v_0$, when following $\bsigma^n$, there is a positive probability $q$, independent of the past history, to reach some terminal vertex, and therefore that the probability of visiting $v$ infinitely often is $\lim_\l (1-q)^\l = 0$.
            Consequently, when the strategy $\bsigma^k$ is followed from $v_0$, it is almost sure that some terminal vertex will eventually be reached.
            
            \item In the cycle-friendly case: from each vertex $v$, when $v$ is seen for the first time, the strategy profile randomizes uniformly between all the edges $vw \in E_k$, and then, when $v$ is visited again, it always replicates the same choice.
            Equivalently, each player initially chooses, at random, a positional strategy, and then follows it\footnote{More generally, our model of randomness can be interpreted by considering that the outcome of each randomized action is simply \emph{revealed} when that action is made, but was already decided (randomly) in the past.
            That approach will often be convenient for those strategy profiles $\bsigma^k$.
            But, in the general case, it should be avoided because it might be counter-intuitive: even if our model allows to consider that the outcome of a randomized action has been decided beforehand, it does not allow the players to \emph{communicate} it beforehand to anyone else.
            But in many cases, they might have an incentive to communicate them: it is therefore more intuitive to consider that those choices are made when they are revealed.}.
            \item  In both cases, if some player $i$ deviates and takes an edge that they are not supposed to take (be it an edge that does not belong to $E_k$ or, in the cycle-friendly case, an edge whose origin has already been seen in the past, and from which a different choice has been made), then all the players switch to the positional strategy profile $\btau^{\dag i}$, where $\btau^{\dag i}_{-i}$ is a positional strategy profile that minimizes the quantity $\sup_{\tau_i} \X({\btau^{\dag i}_{-i}, \tau_i})[\mu_i]$, and $\tau^{\dag i}_i$ is some positional strategy.
        \end{itemize}
       
                
        At step $0$, we define $V_{\frownie}^0$ as the set of terminals that give to some player $i$ a payoff greater than $y_i$, and compute it in time $O(n)$.
        At step $k \geq 1$, we define $V_\bad^k$ as follows: first, we compute the perceived reward $z^k_i$ of each player $i$ in $\bsigma^k$, which can be done in time $O(n^2)$, by computing the set of terminal vertices that are reached with nonzero probability, i.e., that are accessible from $v_0$ in $(V, E_k)$, and by deciding whether there exists a positive probability of reaching no terminal---that is the case if and only if we are in the cycle-friendly case (using the induction hypothesis, since we are at step $k \geq 1$), and there exists a positional strategy profile that uses only vertices of $E_k$ such that there is a positive probability of reaching no terminal.
        Then, the set $V_\bad^k$ is the set of vertices $v$ such that there exists a player $i$ and a vertex $v$ such that $\val(v) > z^k_i$.
        
        In both cases, we compute the set $\Attr(V_{\frownie}^k, E_k)$ in time $O(n+m)$.
        If $v_0 \in \Attr(V_{\frownie}^k, E_k)$, we stop there and answer $\No$.
        Otherwise, we define $F_k$ as being the set $E_k$ in which we have removed every edge $uv$ with $u \not\in \Attr(V_\bad^k, E_k)$ and $v \in \Attr(V_{\frownie}^k, E_k)$ (\emph{first removal} of step $k$).
        Note that every stochastic vertex still has all of its outgoing edges in $F_k$, since if it had an outgoing edge leading to a vertex of $\Attr(V_\bad^k, E_k)$, it would itself belong to $\Attr(V_\bad^k, E_k)$; and that every non-stochastic vertex still has at least one outgoing edge in $F_k$, since if it had all of its outgoing edges leading to a vertex of $\Attr(V_\bad^k, E_k)$, it would itself belong to $\Attr(V_\bad^k, E_k)$.
        
        If we are in the cycle-friendly case, we define $E_{k+1} = F_k$.
        If we are in the cycle-averse case, we compute the set $W_\bad^k$ of vertices $v$ from which there is a positive probability of reaching no terminal, following any strategy profile $\btau$ that uses only edges of $F_k$.\theju{Isn't this $\Attr$ in graph with edges in $F_k$?}\leon{It's the same idea, but then it's the attractor of nothing, like an anti-attractor.} This can be computed in time $O(n+m)$. 
        Then, again, if $v_0 \in W_\bad^k$, we stop there and answer $\No$.
        Otherwise, we define $E_{k+1}$ as the set $F_k$ in which we have removed every edge $uv$ with $u \not\in W_{\frownie^k}$ and $v \in W_{\frownie}^k$ (\emph{second removal} of step $k$).
        With the same argument as above, every stochastic vertex still has all of its outgoing edges, and every non-stochastic vertex still has at least one.
        The invariant is then satisfied: for every vertex $v$ from which no terminal vertex is accessible in $(V, E_{k+1})$, we can construct a strategy profile that guarantees positive probability of reaching no terminal vertex from $v$ by using only edges of $F_k$, and deduce $v \in W_\bad^k$: that vertex is therefore no longer accessible from $v_0$ in $(V, E_{k+1})$.
        
        If $k \geq 1$ and if no edge has been removed during the whole step $k$, then we stop the algorithm there, and answer $\Yes$ if $z^k_i \geq x_i$ for each $i$, and $\No$ otherwise.
        Since the sequence $E_0, E_1, \dots$ is strictly decreasing (except at the last, and possibly the first, steps), the algorithm terminates and takes at most time $O(n^4)$.

        If the algorithm answers $\Yes$ at step $k$, its functional version, in the cycle-friendly case, outputs the set $E_{k+1} = E_k$ as a succinct representation of the strategy profile $\bsigma^{k+1} = \bsigma^k$.
        In the cycle-averse case, it defines the set $E_{k+1}$ (which might be equal to $E_k$) from $E_k$ by removing, one by one, edges $uv$ such that, in the current graph:
        \begin{itemize}
            \item the vertex $u$ is not stochastic and has several outgoing edges;
            \item all the terminal vertices accessible from $v$ are also accessible from $v_0$ without using $uv$;
            \item at least one terminal vertex is accessible from $u$ without using $uv$;
        \end{itemize}
        until there is no such edge.
        This optional step can be done in time $O(n^4)$, and therefore does not increase the complexity given above: we call it the \emph{finishing touches step}.
        Note that the set $E_{k+1}$ still satisfies the induction hypothesis, since no edge $uv$ is ever removed if it leaves no terminal vertex accessible from $u$.
        
        Then, the algorithm outputs the set $E_{k+1}$, as a succinct representation of $\bsigma^{k+1}$.
        

        \item \emph{This algorithm recognizes only positive instances and outputs the representation of an RSE.}

        Let us assume that the algorithm answers $\Yes$ at step $k$: let us show that the strategy profile $\bsigma^{k+1}$ is an RSE that satisfies the desired constraints.
        Note that the algorithm does never answer $\Yes$ at step $0$, hence we necessarily have $k \geq 1$.

        \begin{itemize}
            \item \emph{The strategy profile $\bsigma^{k+1}$ satisfies $\X({\bsigma^{k+1}})[\mu_i] \geq x_i$ for each $i$.}

            Indeed, the algorithm answers $\Yes$ at step $k$ only if the strategy profile $\bsigma^k$ satisfies that constraint.
        In the cycle-friendly case, we have $\bsigma^{k+1} = \bsigma^k$, hence the result is immediate.
        
        In the cycle-averse case, the finishing touches step does not affect the risk entropies: the terminal vertices accessible from $v_0$ in $(V, E_k)$ are still accessible in $(V, E_{k+1})$, and by the induction hypothesis, both $\bsigma^k$ and $\bsigma^{k+1}$ are such that some terminal is almost surely eventually reached.

        
            \item \emph{The strategy profile $\bsigma^{k+1}$ satisfies $\X({\bsigma^{k+1}})[\mu_i] \leq y_i$ for each $i$.}
        
        The set $E_1$ has been defined so that the set $\Attr(V_{\frownie}^0, E)$, and therefore the set $V_{\frownie}^0$, is not accessible from $v_0$ in the graph $(V, E_1)$, and therefore in the graph $(V, E_{k+1})$: thus, it is almost sure in $\bsigma^{k+1}$ that no vertex of $V_{\frownie}^0$ will ever be reached.
        Thus, no terminal that gives to some player $i$ a payoff greater than $y_i$ can be reached.
        Moreover, in the cycle-averse case, i.e. when $y_i \geq 0$ for each $i$, by the induction hypothesis, the probability of never reaching a terminal vertex, which would give each player the payoff $0$, is zero.
        
        % Moreover, in the cycle-averse case, it is almost sure that some terminal vertex is reached, because if there was a positive probability to reach no terminal, there would be a vertex $v$ that would be visited infinitely often with positive probability, and therefore from which there exists a strategy profile of the form $\bsigma^{k+1}_{\|hv}$ such that $v$ is almost surely visited infinitely often (otherwise along every play visiting $v$ infinitely often, at each visit of $v$, there would be a positive probability, greater than or equal to some constant $q > 0$ since $\bsigma^k$ is finite-memory, of not visiting $v$ again, which would imply that the probability of visiting $v$ infinitely often when playing that strategy profile is actually smaller than or equal to $\lim_m (1-q)^m = 0$).
        % In other words, no terminal vertex would be accessible from $v$ in $(V, E_{k+1})$; and since no edge is removed during the finishing touches step that would leave a vertex without any terminal vertex accessible from it, it means that it would also have been also the case in the graph $(V, E_k)$.
        % On the other hand, the second removal of step $k-1$ have made unaccessible all the vertices of the set $W_\bad^{k-1}$: we therefore have $v \not\in W_\bad^{k-1}$, which means that there is a strategy profile $\btau$ that, played from $v$, guarantees that a terminal vertex will be reached almost surely.
        % Such a strategy profile must necessarily guarantee that the set $W_\bad^{k-1}$ is never visited, and therefore makes no use of the edge that are removed during the second removal, and uses only edges that still belong to the set $E_k$.
        % But then, that implies that at least one terminal vertex is accessible from $v$ in the graph $(V, E_k)$, which is a contradiction.


            \item \emph{The strategy profile $\bsigma^{k+1}$ is a $\brho$-RSE.}
        
        Let us assume that some player $i$ has a profitable deviation $\sigma'_i$.
        We can assume without loss of generality that $\sigma'_i$ is pure.        
        Let $z' = \X({\bsigma^{k+1}_{-i}, \sigma'_i})[\mu_i]$.
        We first prove that obtaining the payoff $z'$ is impossible by using only edges of $E_{k+1}$.
        Indeed, since in the strategy profile $\bsigma^{k+1}$, player $i$ gets percieved reward $z^k_i < z'$ (we have proved above that the finishing touches do not change the risk entropies), all the terminal vertices that are accessible from $v_0$ in $(V, E_{k+1})$ give player $i$ a reward that is smaller than $z'$.
        Now, if $z' = 0$ and if in $(\bsigma^{k+1}_{-i}, \sigma'_i)$ there is a positive probability of reaching no terminal, then we must distinguish the cycle-friendly and the cycle-averse case.
        In both cases, let us assume toward contradiction that $\sigma'_i$ uses only edges of $E_{k+1}$.
        
        In the cycle-averse case, since $\bsigma^{k+1}$ randomizes between all the edges of $E_{k+1}$ in a memoryless way, we can assume that $\sigma'_i$ is actually positional, and with the same reasoning as above, if there is a positive probability of reaching no terminal vertex in that memoryless strategy profile, it means that there is a vertex $v$ that has positive probability to be reached and from which it is almost sure that no terminal vertex will be reached, and that $v$ will be visited infinitely often.
        But, since all vertices of $W_\bad^{k-1}$ have been made unaccessible during the second removal of step $k-1$, there must be a terminal vertex $t_1$ that is accessible from $v$ in the graph $(V, E_k)$, and therefore in the graph $(V, E_{k+1})$.
        Let then $h = h_0 \dots h_\l$ be a simple path from $v = h_0$ to some terminal $h_\l = t_1$, and let $h_{\leq m}$ be the longest prefix of $h$ that is compatible with $(\bsigma^{k+1}_{-i}, \sigma'_i)$.
        Then, since that path is compatible with the memoryless strategy profile $\bsigma^{k+1}$, the vertex $h_m$ is necessarily controlled by player $i$ and has another outgoing edge $h_mw$, such that from $w$, when following $(\bsigma^{k+1}, \sigma'_i)$, it is almost sure that $v$ will be visited again --- which implies that in particular $v$ is accessible from $w$.
        But then, since there is a terminal vertex (namely $t_1$) that is accessible from $h_m$ without using the edge $h_m w$, the only reason why that edge would not have been removed during the finishing touches step is that there is a vertex $t_2$, accessible from $w$ in $(V, E_{k+1})$, that is not accessible from $v_0$ in $(V, E_{k+1} \setminus \{h_mw\})$.
        By iterating that process, we can create an infinite sequence of different terminal vertices $t_1, t_2, \dots$, which is of course impossible.
        
        In the cycle-friendly case, since $\bsigma^{k+1}$ can be understood as a strategy profile in which each player chooses at random, initially, one positional strategy and then follows it, saying that there is a positive probability of reaching no terminal vertex in $(\bsigma^{k+1}_{-i}, \sigma'_i)$ is equivalent to saying that there is one positional strategy profile $\btau_{-i}$ such that with positive probability, no terminal vertex is reached \emph{and} the strategy profile $\bsigma^k_{-i}$ is following $\btau_{-i}$.
        Then, the strategy $\sigma'_i$ is such that there is a positive probability of reaching no terminal when following $(\btau_{-i}, \sigma'_i)$, and we can actually also assume without loss of generality that $\sigma'_i$ is positional.
        But then, there is also a positive probability that $\sigma^k_i$ actually follows $\sigma'_i$, hence there already is a positive probability to reach no terminal in $\bsigma^k$, hence $z^k_i \geq 0$, which contradicts the inequality $z^k_i < z'$.
        
        Therefore, the strategy $\sigma'_i$ cannot use only the edges of $E_{k+1}$, and the percieved reward $z'$ is actually obtained after having taken an edge $uv \not\in E_k$ (i.e. there exists $hu$ compatible with $\bsigma^k$ such that $\sigma'_i(hu) = v$ and $\prob_{\bsigma^k_{-i\|hu}, \sigma'_{i\|hu}}(\mu_i = z') > 0$, where $u \in V_i$ is accessible from $v_0$ in $(V, E_k)$).
        But, then, after that edge, the strategy profile $\bsigma^k_{-i}$ is actually following the positional strategy profile $\btau^{\dag i}_{-i}$, which prevents player $i$ from increasing the percieved reward to a value larger than $\val(u)$: therefore, we have $z' \leq \val(u)$.
        But, on the other hand, if the algorithm has answered $\Yes$, it means that we had $\val(u) \leq z^k_i$, which contradicts, again, the inequality $z^k_i < z'$.
        The strategy profile $\bsigma^k$ is therefore an RSE, which satisfies the desired constraints.
        \end{itemize}


\item \emph{This algorithm recognizes all positive instances.}

        Let us assume that we have a positive instance, i.e. that there exists an RSE $\bsigma$ with $\bx \leq \X(\bsigma)[\mu] \leq \by$.
        Let us show that the algorithm will never answer $\No$; and therefore, since it is guaranteed to terminate, that it answers $\Yes$.
             
        To do so, we first prove, by induction, that at the beginning of each step $k \geq 0$, every edge that has positive probability to be eventually taken in $\bsigma$ remains in $E_k$, and that $z^k_i \geq \X(\bsigma)[\mu_i]$ for each $i$.
        (Note that if the algorithm gives an answer at step $k$, then there is no step $k+1$, hence that result may not apply to the strategy profile $\bsigma^{k+1}$ which is output when the answer is $\Yes$ --- in other words, the finishing touches step is the only step that might falsify this result.)
        At step $0$, the result is immediate, since $E_0 = E$.

        % For each step $k$, let $uv$ be a vertex that is not accessible from $v_0$ in the graph $(V, E_k)$.
        % Then, it means that every path from $v_0$ to $u$ in $(V, E)$ uses an edge that has been removed during some step $k' < k$, i.e. traverses a vertex that belongs to some set $\Attr(V_\bad^{k'})$ or (in the cycle-averse case) or $W_\bad^{k'}$.
        % But, in the strategy profile $\bsigma$, all those sets have probability $0$ to be visited.

        At step $1$, if the strategy profile $\bsigma$ uses eventually, with positive probability, an edge that does not belong to $E_1$, i.e. goes with positive probability to a vertex $v \in \Attr(V^0_\bad, E_0)$, then with positive probability, a terminal vertex will be reached that gives to some player $i$ a payoff greater than $y_i$, which is impossible.
        
        Now, if $\bsigma$ uses with positive probability an edge that is removed during the first removal of step $k$, then it visits with positive probability a vertex $v \in \Attr(V^k_\bad, E_k)$, and therefore, with positive probability, a vertex $w$ with $\val(w) > z^k_i$, where $i$ is the player controlling $w$.
        But then, in the strategy profile $\bsigma$, player $i$ has a deviation that gives them a percieved reward greater than $z^k_i$.
        Since $\bsigma$ is an RSE, that deviation cannot be profitable, hence we must have $\X(\bsigma)[\mu_i] > z^{k'}_i$.
        Let us show that this is not possible, by showing that every payoff vector that has positive probability to be generated in $\bsigma$ also have positive probability to be generated in $\bsigma^k$.
        By induction hypothesis, all the terminal vertices that are reached with positive probability in $\bsigma$ are also reached with positive probability in $\bsigma^k$.
        If, now, there is a positive probability of getting the zero payoff vector by reaching no terminal in $\bsigma$, it will also be the case in $\bsigma^k$.
        Indeed, if that is the case, we are necessarily in the cycle-friendly case (otherwise $\bsigma$ would not match the constraints given in the instance).
        Then, if $u$ is a vertex that has positive probability to be visited infinitely often in $\bsigma$, then there is a strategy profile (namely $\bsigma$) that uses only vertices that have positive probability to be reached in $\bsigma$, and therefore only edges of $E_k$ by induction hypothesis, that guarantees that with positive probability, the vertex $u$ is visited infinitely often; therefore, there also exists a positional strategy profile giving the same guarantees, and there is a positive probability that $\bsigma^k$ follows that positional strategy profile, hence there is a positive probability of reaching no terminal vertex in $\bsigma^k$.
        This proves that all the payoff vectors that have positive probability to be achieved in $\bsigma$ also have positive probability to be achieved in $\bsigma^k$, and therefore that $\X(\bsigma)[\mu_i] \leq \X({\bsigma^k})[\mu_i] = z^k_i$, which is a contradiction.

        Finally, if we are in the cycle-averse case and there is a positive probability that the strategy profile $\bsigma$ uses an edge that has been removed during the \emph{second} removal of step $k$, and therefore to reach a vertex of $W^k_\bad$: then, since we have already proven that $\bsigma$ uses only edges of the set $F_k$, we deduce that $\bsigma$ is such that with positive probability, no terminal vertex is reached, which is impossible in the cycle-averse case.


        To conclude, the answer $\No$ can be given only in the two following cases:
            \begin{itemize}
                \item \emph{If at step $k$, we have $v_0 \in \Attr(V^k_\bad, E_k)$, or $v_0 \in W_0^k$ in the cycle-averse case.}

                Then, the strategy profile $\bsigma$ visits that set with positive probability, and we have already shown that it was impossible.

                \item \emph{If during step $k$, no edge is removed, but we have $z^k_i < x_i$ for some player $i$.}

                We have shown above that $\X(\bsigma)[\mu_i] \leq \X({\bsigma^k})[\mu_i] = z^k_i$: that case is therefore also impossible by definition of $\bsigma$.
            \end{itemize}

            None of those cases is possible, hence our algorithm will eventually answer $\Yes$.
    \end{itemize} 
\end{proof}

% \begin{proof}
%     \leonard{Old inefficient proof 2}

%     In all this proof, when $W$ is a set of vertices, we write $\Attr(W)$ the \emph{probabilistic attractor} of $W$, i.e. the set of vertices $v$ from which all the players together cannot avoid reaching $W$, i.e. such that following any strategy profile $\bsigma$, there is a positive probability of reaching $W$.
%     By TODO, given $W$, the set $\Attr(W)$ can be computed in time $O(n)$.
    
%     \begin{itemize}
%         \item \emph{Definition of the algorithm}

%         Before starting the algorithm, we need two distinguish two cases: the \emph{cycle-averse} case, in which there exists a player $i$ such that $y_i < 0$ (and therefore in which the requirement $\bx \leq \X(\bsigma)[\mu] \leq \by$ implies that $\bsigma$ must almost surely reach a terminal), and the \emph{cycle-friendly} case, in which there is no such player.
%         The algorithm will work slightly differently in those two cases.

%         The algorithm with proceed by constructing a decreasing sequence $E_0, E_1, \dots$ of subsets of $E$.
%         At each step, we will always leave at least one outgoing edge from each vertex, and we will never remove outgoing edges of stochastic vertices.
%         Moreover, a preprocessing computes for each vertex $v$ the maximal risk entropy $\val(v) = \inf_{\btau_{-i}} \sup_{\tau_i} \X(\btau)[\mu_i]$ that player $i$, the player who controls the vertex $v$ can ensure against every strategy profile from $v$.
%         All those values can be computed in time $O(pn)$ by TODO.

%         Let $E_0 = E$.
%         Step $k$ starts when $E_k$ is defined.
                
%         At step $1$, let $V_{\frownie}^1$ be the set of terminals that give to some player $i$ a payoff greater than $y_i$.
%         In the cycle-averse case, we add to $V_\bad^1$ the set of vertices from which no terminal is accessible in $(V, E)$.
%         Then, we compute the set $\Attr(V_{\frownie}^1)$ in time $O(n)$.
%         If $v_0 \in \Attr(V_{\frownie}^1)$, then we stop the algorithm there and answer $\No$.
%         Otherwise, let us define $E_2$ by removing all the edges that go from a vertex that does not belong to $\Attr(V_{\frownie}^1)$ to a vertex that does.

%         Step $k \geq 2$ start once the set $E_k$ is defined.
%         % We then define the strategy profile $\bsigma^k$ as follows:
%         % \begin{itemize}
%         %     \item in the cycle-averse case: from each vertex $v$, the strategy profile randomizes uniformly between all the edges $vw \in E_k$\footnote{Actually, we could have defined $\bsigma^k$ in the cycle-averse case in the same fashion as in the cycle-friendly case, since edges have been removed at step $1$ to avoid cycling where that was possible.
%         %     But this distinction enables us to have a memoryless strategy profile, and it will therefore enable us to show that, in the cycle-averse case, if an };
%         %     \item in the cycle-friendly case: from each vertex $v$, when $v$ is seen for the first time, the strategy profile randomizes uniformly between all the edges $vw \in E_k$, and then, when $v$ is visited again, it always replicates the same choice.
%         %     Equivalently, each player initially chooses, at random, a positional strategy, and then follows it\footnote{The different positional strategies may not have the same probability to be followed, but they all have a positive probability.
%         %     A rigorous proof of the equivalence can be infered from TODO.
%         %     We will however avoid that approach here, since assuming that the players choose from the start which positional strategy they are going to follow could easily lead to believe that they can communicate that choice between them, so that the players can act differently depending on what the other players plan to do in the future.
%         %     But, in our model, each player's actions can depend only on the actual history that has been realized.}.
%         % \end{itemize}
%         % In both cases, if a player $i$ deviates and takes an edge that they are not supposed to take (be it an edge that does not belong to $E_k$ or, in the cycle-friendly case, an edge whose origin has already been seen in the past, and from which a different choice has been made), then all the players switch to the positional strategy profile $\btau^{\dag i}$, where $\btau^{\dag i}_{-i}$ is a positional strategy profile that minimizes the quantity $\sup_{\tau_i} \X({\btau^{\dag i}_{-i}, \tau_i})[\mu_i]$, and $\tau^{\dag i}_i$ is some positional strategy.
%         We then define the strategy profile $\bsigma^k$ as follows: from each vertex $v$, when $v$ is seen for the first time, the strategy profile randomizes uniformly between all the edges $vw \in E_k$, and then, when $v$ is visited again, it always replicates the same choice.
%         Equivalently, each player initially chooses, at random, a positional strategy, and then follows it\footnote{The different positional strategies may not have the same probability to be followed, but they all have a positive probability.
%         A rigorous proof of the equivalence can be infered from TODO.
%         We will however avoid that approach here, since assuming that the players choose from the start which positional strategy they are going to follow could easily lead to believe that they can communicate that choice between them, so that the players can act differently depending on what the other players plan to do in the future.
%         But, in our model, each player's actions can depend only on the actual history that has been realized.}.        
%         If a player $i$ deviates and takes an edge that they are not supposed to take (be it an edge that does not belong to $E_k$, or an edge whose origin has already been seen in the past, and from which a different choice has been made), then all the players switch to the positional strategy profile $\btau^{\dag i}$, where $\btau^{\dag i}_{-i}$ is a positional strategy profile that minimizes the quantity $\sup_{\tau_i} \X({\btau^{\dag i}_{-i}, \tau_i})[\mu_i]$, and $\tau^{\dag i}_i$ is some positional strategy.

%         Computing the risk entropy $z^k_i$ of each player $i$ in $\bsigma^k$ can be done in time $O(n)$, by computing the set of terminal vertices that are reached with nonzero probability, i.e. that are accessible from $v_0$ in $(V, E_k)$, and by deciding whether there exists a positive probability of reaching no terminal --- which is the case if and only if there exists a positional strategy profile that uses only vertices of $E_k$ such that there is a positive probability of reaching no terminal.
%         Then, we can also check in time $O(n)$ whether there exists a player $i$ and a vertex $v$ such that $\val(v) > z_i$.
%         If that is not the case, then we stop the algorithm there, and answer $\Yes$ if $z^k_i \geq x_i$ for each $i$, and $\No$ otherwise.
%         Otherwise, we compute in time $O(n)$ the set $V_{\frownie}^k = \{v \in V_i ~|~ \val(v) > z^k_i\} \neq \emptyset$, and the set $\Attr(V_{\frownie}^k$.
%         If $v_0 \in \Attr(V_{\frownie}^k)$, we stop there and answer $\No$.
%         Otherwise, we define $E_{k+1}$ as the set $E_k$ in which we have removed every edge $uv$ with $u \not\in V_{\frownie^k}$ and $v \in V_{\frownie}^k$.

%         The algorithm does necessarily terminate, since the sequence $E_0, E_1, \dots$ is strictly decreasing.


%         \item \emph{This algorithm recognizes only positive instances.}

%         If the algorithm answers $\Yes$ at step $k$, then the strategy profile $\bsigma^k$ satisfies $\X({\bsigma^k})[\mu_i] \geq x_i$ for each $i$.
%         Moreover, we necessarily have $k \geq 2$, and the set $E_2$ has been defined so that the set $\Attr(V_{\frownie}^1)$, and therefore the set $V_{\frownie}^1$, is not accessible from $v_0$ in the graph $(V, E_2)$, and therefore in the graph $(V, E_k)$: therefore, it is almost sure in $\bsigma^k$ that no vertex of $V_{\frownie}^1$ will be reached.
%         Thus, no terminal that gives to some player $i$ a payoff greater than $y_i$ can be reached.
%         Moreover, in the cycle-averse case, it is almost sure that some terminal vertex is reached, because if there was a positive probability to reach no terminal, there would be a vertex $v$ that would be visited infinitely often with positive probability, and therefore from which there exists a strategy profile of the form $\bsigma^k_{\|hv}$ such that $v$ is almost surely visited infinitely often (otherwise along every play visiting $v$ infinitely often, at each visit of $v$, there would be a positive probability, greater than or equal to some constant $q > 0$ since $\bsigma^k$ is finite-memory, of not visiting $v$ again, which would imply that the probability of visiting $v$ infinitely often is actually smaller than or equal to $\lim_m (1-q)^m = 0$).
%         But then, by the property of $E_1$ in the cycle-averse case that we have proven when constructing $E_1$, the vertex $v$ is such that no terminal is accessible from $v$ in $(V, E)$, and therefore that $v \in V_\bad^1$, which is a contradiction.        
%         Thus, it is almost sure, when following $\bsigma^k$, that no player $i$ will get a payoff greater than $y_i$.
%         Consequently, we also have $\X({\bsigma^k})[\mu_i] \leq y_i$ for each $i$.
        
%         Finally, let us assume that some player $i$ has a profitable deviation $\sigma'_i$.
%         We assume without loss of generality that $\sigma'_i$ is pure.
%         Let $z' = \X({\bsigma^k_{-i}, \sigma'_i})[\mu_i]$.
%         Obtaining the payoff $z'$ is impossible by using only edges of $E_k$.
%         Indeed, since in the strategy profile $\bsigma^k$, player $i$ gets risk entropy $z^k_i < z'$, all the terminal vertices that are accessible from $v_0$ in $(V, E_k)$ give player $i$ a reward that is smaller than $z'$.
%         Moreover, if $z' = 0$ and if in $(\bsigma^k_{-i}, \sigma'_i)$ there is a positive probability of reaching no terminal, then there is in particular one positional strategy profile $\btau_{-i}$ such that with positive probability, no terminal vertex is reached \emph{and} the strategy profile $\bsigma^k_{-i}$ is following $\btau_{-i}$.
%         Then, the strategy $\sigma'_i$ is such that there is a positive probability of reaching no terminal when following $(\btau_{-i}, \sigma'_i)$, and we can actually assume without loss of generality that $\sigma'_i$ is positional.
%         But then, there is also a positive probability that $\sigma^k_i$ actually follows $\sigma'_i$, and that event is independent of $\bsigma^k_{-i}$, hence there already is positive probability to reach no terminal in $\bsigma^k$, which is a contradiction.
        
%         Therefore, the risk entropy $z'$ is actually obtained after having taken an edge $uv \not\in E_k$ (i.e. there exists $hu$ compatible with $\bsigma^k$ such that $\sigma'_i(hu) = v$ and $\prob_{\bsigma^k_{-i\|hu}, \sigma'_{i\|hu}}(\mu_i = z') > 0$, where $u \in V_i$ is accessible from $v_0$ in $(V, E_k)$).
%         But, then, after that edge, the strategy profile $\bsigma^k_{-i}$ is actually following the positional strategy profile $\btau^{\dag i}_{-i}$, which prevents player $i$ to get a risk entropy better than $\val(u)$: therefore, we have $z' \leq \val(u)$.
%         But, on the other hand, if the algorithm has answered $\Yes$, it means that we had $\val(u) \leq z^k_i$, which, again, contradicts the inequality $z^k_i < z'$.
%         The strategy profile $\bsigma^k$ is therefore an RSE, which satisfies the desired constraints.


%         \item \emph{This algorithm recognizes all positive instances.}

%         Let us assume that we have a positive instance, i.e. that there exists an RSE $\bsigma$ with $\bx \leq \X(\bsigma)[\mu] \leq \by$.
%         Let us prove by induction that for every $k \geq 1$, if a vertex is almost surely never visited in $\bsigma^k$ (i.e. if it is not accessible from $v_0$ in $(V, E_k)$), then it also is in $\bsigma$, and that $z^k_i \geq \X(\bsigma)[\mu_i]$ for each $i$.

%         For $k = 1$, be it in the cycle-friendly or the cycle-averse case, the vertices accessible form $v_0$ in $(V, E_1)$ are the same as in $(V, E)$, since an edge $uv$ is removed only if $u$ is accessible from $v$ and from $v_0$.
        
        
%         Let $uv$ be an edge that is removed at step $k \geq 1$, i.e. such that $uv \in E_k \setminus E_{k+1}$.
%         Then, it is almost sure that the vertex $v$ is never visited in $\bsigma$.
%         Indeed, if the edge $uv$ has been removed at step $k$, then it means that we had $v \in \Attr(V_\bad^k)$.
%         Therefore, if we assume that the strategy profile $\bsigma$ visits $v$ with positive probability, then there is also a positive probability that it visits $V_\bad^k$.
%         If $k = 1$, then from each vertex of $V_\bad^1$, there is a positive probability that some player $i$ gets a payoff that is greater than $y_i$, which is impossible.
%         If now $k \geq 2$, then $V^k_\bad$ was defined as the set of vertices $w$ with $\val(w) > z^k_i$, where $i$ is the player controlling $w$: since the sequence $(z^k_i)_k$ is non-increasing (the set of terminal vertices that are accessible from $v_0$ in $(V, E_\l)$ is non-increasing, and if there is a positive probability to reach no terminal in $\bsigma^{\l+1}$, then it was also the case in $\bsigma^\l$).
%         Thus 
%     \end{itemize}
% \end{proof}







% \begin{proof}
%     \leonard{Old inefficient proof 1}

%     \begin{itemize}
%         \item \emph{Definition of $\bsigma^{Wp}$}

%         Let $p \in [0,1]$, and let $W$ be a stable vertex set in $\Game_{\|v_0}$.
%         Let $v_\circlearrowleft$ be a vertex.
%         We define the finite-memory strategy profile $\bsigma^{Wp}$ as follows:

%         \begin{itemize}\item At first (when reading the vertex $v_0$), each player's memory initializes in the state $\cycle$ if $p=0$, and in the state $\terminals$ otherwise.

%         \item When the players' memories are in the state $\cycle$, the strategy profile $\bsigma$ follows some pure memoryless strategy $\btau_{\circlearrowleft}$ that goes only to vertices that belong to $W$, and such that, if possible, the probability of never reaching a terminal vertex is nonzero.
%         Then, the players' memories stay in the state $\cycle$, unless some player $i$ deviates and uses an edge that they are not supposed to use: then, they switch to the state $\punish_i$. 

%         \item When the players' memories are in the state $\terminals$, if $p = 1$, the strategy profile $\bsigma$ follows the memoryless strategy profile $\btau_{\curlywedgedownarrow}$, which consists in, from every vertex $v \in W$, taking each of the edges $vw \in W \times W$ with some nonzero probability.
%         Note that since $v_0 \in W$, that is always possible until some player $i$ deviates and takes an edge to a vertex $w \not\in W$.
%         Then, the collective memory switches to the state $\punish_i$.

%         \item When the players' memories are in the state $\terminals$, if $p < 1$, the strategy profile $\bsigma$ follows the memoryless strategy profile $\btau_{\curlywedgedownarrow \circlearrowleft}$, which consists in, from every vertex $v \in W$, taking each of the edges $vw \in W \times W$ with some nonzero probability.
%         Then, when the vertex $v_\circlearrowleft$ is visited, all the players switch their memories to the state $\terminals$.
%         When some player $i$ deviates and moves to a vertex that does not belong to $W$, all the players switch their memories to the state $\punish_i$.

%         \item When the players' memories are in the state $\terminals_{v_0}$, they behave exactly as when they are in the state $\terminals$, except for the fact that if the vertex $v_\circlearrowleft$ is visited once again, they switch to the state $\cycle$ (which is therefore reached when the vertex $v_\circlearrowleft$ is visited twice).

%         \item When the players' memories are in the state $\punish_i$ for some player $i$, the strategy profile $\bsigma$ follows some memoryless pure strategy profile $\btau_{\dag_i}$, where $\btau_{\dag_i -i}$ minimizes the risk entropy $\max_{\tau_i} \X({\btau_{\dag_i -i}, \tau_i})[\mu_i]$ that player $i$ can get (such a pure memoryless strategy profile exists by Lemma~\ref{lm:reach_safe_positional}), and where $\sigma_{\dag_i i}$ is chosen arbitrarily.
%     \end{itemize}


%     \item \emph{If there is a $\brho$-RSE $\bsigma$ in $\Game_{\|v_0}$ that satisfies $\bx \leq \X(\bsigma)[\mu] \leq \by$, there is one of the form $\bsigma^{Wp}$.}

%     Let us assume the existence of such $\bsigma$.
%     Let $W$ be the set of vertices that have nonzero probability of being reached in $\bsigma$, and let $p$ be the probability of reaching a terminal vertex when following $\bsigma$.
%     If $p < 1$, i.e. if there is a nonzero probability of reaching no terminal, then we can choose a vertex $v_\circlearrowleft$ that has nonzero probability to be visited infinitely often.
%     Let us show that the corresponding strategy profile $\bsigma^{Wp}$ is also a $\brho$-RSE satisfying the desired constraints.

%     \begin{itemize}
%         \item \emph{It generates the same risk entropies as $\bsigma$.}

%         Indeed, the payoff vectors that have nonzero probability to be obtained in $\bsigma$ and in $\bsigma^{Wp}$ are the same.
%         That result is immediate for payoff vectors that are obtained by reaching a terminal, since the vertices, including the terminal vertices, that have nonzero probability to be reached in $\bsigma^{Wp}$ are exactly the vertices of $W$, i.e. the same as in $\bsigma$.

%         Now, let us show that if there is a nonzero probability of reaching no terminal in $\bsigma$, i.e. if $p < 1$, then that is also the case in $\bsigma^{Wp}$.
%         If $p = 0$, then the collective memory initializes in state $\cycle$, and since there is a strategy profile that avoids terminal vertices with nonzero probability, that is also what the strategy profile $\btau_{\circlearrowleft}$ does, and therefore so does $\bsigma^{Wp}$.
%         If $0 < p < 1$, then $\bsigma^{Wp}$ starts in the memory states $\terminals$.
%         Since the vertex $v_\circlearrowleft$ has nonzero probability to be visited infinitely often in $\bsigma$ there is a path from $v_0$ to $v_\circlearrowleft$, and then from $v_\circlearrowleft$ to itself, that traverses only vertices of $W$.
%         Therefore, with nonzero probability, the strategy profile $\bsigma^{Wp}$ visits twice that vertex and, then, switches to the state $\cycle$.
%         Then, for the same reason as above, with nonzero probability, no terminal is reached.

%         Conversely, if in $\bsigma^{Wp}$, with nonzero probability, no terminal vertex is reached, then it can be because the collective memory has switched to the state $\cycle$, and that is possible only if $p < 1$, or because the subgraph of $(V, E)$ induced by $W$ contains a strongly connected component that is not a terminal vertex without any outgoing edge leading to a vertex in $W$.
%         If that is the case, it means that a given vertex $v$ of that strongly connected component has nonzero probability to be visited in $\bsigma$, but that no terminal vertex can be reached with nonzero probability after $v$ has been visited.
%         In that case too, we have $p < 1$, i.e. in $\bsigma$ there is a nonzero probability that no terminal vertex is reached.


%         \item \emph{No optimist has a profitable deviation.}

%         \leonard{Oops: what about undetectable deviations that cycle?}
%     \end{itemize}
%     \end{itemize}
% \end{proof}



% \subsection{Upper bound}

% \begin{lemma}
%     The constrained existence problem of $\Bar{\rho}$-RSE is in $\PTIME$ if all the players are optimists. 
% \end{lemma}