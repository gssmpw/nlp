We now study the computational complexity of the constrained existence problem of XRSEs.
The main result of this section is the following theorem, which proves that, contrary to the same problem with ERSEs, it is a decidable fragment of the constrained existence of RSEs.

\begin{theorem}\label{thm:NPcomplete}
    The constrained existence problem for XRSEs is $\NP$-complete and is $\NP$-hard even when all players are pessimistic and all rewards are non-negative.
\end{theorem}

First, we prove \cref{thm:memorysmall}, which shows that if there is an XRSE then there is one that uses finite memory. We show that this finite-memory strategy profile can be described only using polynomial size, which in turn proves $\NP$ membership (\cref{lemma:np_easy}).

Later, we consider the problem of XRSEs when the players are restricted to pure, stationary or positional strategies. We show that in all the above cases, the problem remains $\NP$-complete. The upperbound is similar to the general case, but the lower bound is shown in \cref{lemma:np_hardness} by showing a reduction from $\THREESAT$ to the constrained existence problem. 
\begin{restatable}{theorem}{restrictedStrategies}\label{thm:infinite_rho_restricted_strategy_np_easy}
    The constrained existence problem of XRSEs is also $\NP$-complete when the players are restricted to positional, stationary,  or pure strategies. 
\end{restatable}

Finally, we show in the following theorem that when all players are optimistic, the problem becomes $\PTIME$-complete. 
\begin{restatable}{theorem}{PTIMEcompleteThm}\label{thm:PTIMEcomplete}
    The constrained existence problem of XRSE is $\PTIME$-complete when all players are optimists, that is, when $P=\emptyset$.
\end{restatable} %, by giving an algorithm that incrementally removes edges of the underl until an XRSE is found
We dedicate the rest of the section to proving these three results. 
\subsection{Membership in $\NP$}
%\thejaswini{We need to discuss some examples to make the proof less opaque. To discuss and put good examples.}
%\paragraph*{Intuition}
$\NP$-membership is a consequence of the fact that when an XRSE exists, there also exists one with the same extreme risk measures that uses finite memory, with a number of states that is polynomial in the size of the game.
Let us therefore illustrate, with examples, how and why memory is required in such XRSEs.
We consider the following constrained existence question and analyse the same question on three example graphs. 
\begin{quote}$(*)$
   Is there an XRSE in the game in which both players have a risk measure $1$?
\end{quote}

\subparagraph*{Game in \cref{fig:ex_extreme1}.}Let us consider the game in \cref{fig:ex_extreme1} again.
The answer to Question~$(*)$ here is \emph{no}.
Intuitively, such an XRSE would require at least two plays of positive probability: one that ends in $t_1$, and one that ends in $t_2$.
%We say that the first one \emph{anchors} player $\Circle$'s payoff, and the second one anchors player $\Square$'s one.
For those two plays to occur with positive probability, the strategy profile must proceed to a randomised action at vertex $a$ or $b$: i.e., one of the players, at some point of time, must toss a coin to give the payoff $1$ to player $\Circle$ in one case and to $\Square$ in the other.
But then, since that player is the only one that can see that coin, they have a profitable deviation by lying about the outcome, and always choose the option that gives them the best payoff.
More randomisation will not help: as long as one of the players randomises, be it once, several times, or infinitely often, they have an incentive to deviate and stay in the cycle and wait until the other player leaves.

\subparagraph*{Game in \cref{fig:ex_extreme2}.} Consider now a slight modification, as shown in~\cref{fig:ex_extreme2}.
There, the first player that plays is determined at random by the edge that is taken from an initial stochastic vertex.
The answer to Question $(*)$ for this game is \emph{yes}. 
The random choice on which player gets payoff $1$ is decided by the stochastic vertex. Since both players can see which edge is taken from there, this serves as a source of unbiased randomness based on which they act. % their future action on the outcome of that random experiment.
For example, it can be decided that if the play visits the vertex $a$ immediately after $c$, then player $\Circle$ must visit the terminal $t_1$, and similarly, if it visits the vertex $b$, then player $\Square$ must visit the terminal $t_2$. If the edge $ab$ is taken, player $\Square$ punishes player $\Circle$ by always going back to $a$, and vice versa.
In other words, the stochastic vertex provides the players with a common coin.
%Note that we could also have replaced $c$ by a vertex controlled by a third player, for whom $t_1$ and $t_2$ would be equivalent.

\subparagraph*{Game in \cref{fig:ex_extreme3}.} Finally, consider the game depicted in~\cref{fig:ex_extreme3}.
Here, both players $\Circle$ and $\Square$ have the possibility of deviating to a terminal with payoff $2$ in one play.
The stationary strategy profile in which from vertex $d$, player $\Circle$ goes from $d$ to $a$ and then to $t_1$, and in which player $\Square$ goes from $e$ to $b$ and then to $t_2$, is therefore not an XRSE: both players have a profitable deviation that goes to terminal $t_3$.
But the answer to Question~$(*)$ still remains \emph{yes}!
If, from vertex $d$, player $\Circle$ goes from vertex $d$ to $a$ and then to $b$, from which player $\Square$ leaves to $t_2$, and symmetrically, from vertex $e$, player $\Square$ goes to $a$ through $b$ from which player $\Circle$ goes to $t_1$, then that strategy profile is an XRSE, in which everyone gets the risk measure $1$. 
This is because a player has a profitable deviation only if they can play in a way that guarantees them a risk measure better than $1$, i.e., that guarantees them \emph{almost surely} a payoff greater than~$1$ by deviating.
If there remains a play that occurs with nonzero probability and offers a lower reward, then the player does not increase their risk measure.  
Therefore, an XRSE where player $\Circle$ gets the extreme risk measure $1$ only needs to have one play with positive probability in which she gets the payoff $1$, \emph{and} in which she cannot increase her payoff by deviating. We say that such a play \emph{anchors} that player. In our example, the play $cebat_1$ anchors player $\Circle$. 

We see in this last example that memory is required to remember either the subset of players that are being anchored, or if a player has deviated from the strategy and must be punished. 
Given one or more players that are being anchored, the memory state of any of the players does not change unless either a player deviates or, more importantly, randomisation occurs. When randomisation occurs, the set of players that are anchored in each of the plays is a subset of the set of players anchored before this play \emph{split}.
In our examples, the set of players that are anchored at $c$ is both $\Circle$ and $\Square$, and it immediately splits.
After the splits, when we have only one player to anchor, the players can follow a positional strategy profile; and similarly when one player deviates and must be punished the players can follow a positional strategy profile.
%\theju{to do: i will add some lines about anchoring needing only positional strategies. Currently, anchoring is not a objective. it is a property of a play}%Since only a positional strategy is required to ensure a positive probability of a specific payoff, 
%In general, to ensure that a player receives a specific payoff is achieved using a positional strategy profile.%, therefore if the memory-state is anchoring, then the players may follow a positional strategy-profile until the memory state %Anchoring a subset of players or punishing a subset of players requires only a positional strategy.  

% Moreover, the same idea can be used to prove that such a limited amount of memory is also sufficient when we restrict our work to pure strategy profiles: in that case, the players are no longer allowed to proceed themselves to a randomisation that would induce a split between several anchoring plays, and can only use stochastic vertices to do so, as it was actually the case in our examples.

We prove a theorem that bounds the amount of memory required by a strategy to a polynomial in the number of players and vertices in the game. %We further show that the same idea can be used to prove that such a limited amount of memory is also sufficient when we restrict our work to pure strategy profiles.
\begin{restatable}[App.~\ref{app:memorysmall}]{theorem}{memorysmall}\label{thm:memorysmall}
    Let $\bsigma$ be an XRSE in the game $\Game_{\|v_0}$ with $n$ vertices and $p$ players,  and a partition $(P, O)$ of player $\Pi$.
    Then, there exists a finite-memory XRSE $\bsigma^\star$ with at most $3np-2n+p+1$ many memory states, and such that $\X(\bsigma^\star) = \X(\bsigma)$. Furthermore, if $\bsigma$ is pure, then there is such a strategy profile $\bsigma^\star$ that is pure.
\end{restatable}

\begin{proof}[Proof sketch]
We prove this theorem by formalising the idea of \emph{anchoring plays}.
To do so, we define a labelling function $\Lambda$, that maps each history $h$ compatible with $\bsigma$ to the set of players that is, after the history $h$, currently being \emph{anchored}. In \cref{lm:Lambda}, we show the existence of such a labelling, with some properties. 
In the sequel, we write $\bz$ to represented the risk measure of each player in the strategy profile $\bsigma$, that is, $\bz = (z_i)_i = \X(\bsigma)$.

     \begin{restatable}[The labelling $\Lambda$, App.~\ref{app:memorysmall}]{lemma}{finiteMemAbstraction}\label{lm:Lambda}
        There exists a labelling $\Lambda$ that maps each history $h \in \Hist\Game_{\|v_0}$ compatible with $\bsigma$ to a set $\Lambda(h) \subseteq \Pi$, such that for each such $h$, if we write $\{v_1, \dots, v_k\} = \Supp(\bsigma(h))$, the labelling $\Lambda$ satisfies the following properties.
        \begin{enumerate}
            \item\label{itm:splitsetsanchorwithouti} If the vertex $\last(h)$ is stochastic, or belongs to some player $i \not\in \Lambda(h)$, then the sets $\Lambda(hv_1), \dots, \Lambda(hv_k)$ form a partition of $\Lambda(h)$.

            \item\label{itm:splitsetsanchorwithi} If the vertex $\last(h)$ belongs to some player $i \in \Lambda(h)$, then the sets $\Lambda(hv_1) \setminus \{i\}, \dots, \Lambda(hv_k) \setminus \{i\}$ along with $\{i\}$ form a partition of $\Lambda(h) \setminus \{i\}$, and $i$ belongs to all sets $\Lambda(hv_1), \dots, \Lambda(hv_k)$.

            \item\label{itm:optimistanchor} For each optimistic player $i \in \Lambda(h)$, we have $\X_i(\bsigma_{\|h}) = z_i$.
            
            \item\label{itm:pessimistanchor} For each pessimistic $i \in \Lambda(h)$, for all strategies $\tau_i$ of player $i$, we have $\X_i(\bsigma_{-i\|h}, \tau_i) \leq z_i$.

            \item\label{itm:nosplit} If there is a successor $v_\l$ such that $\Lambda(hv_\l) = \Lambda(h)$, then all other successors $v_{\l'}$ are such that $\X_i(\bsigma_{\|hv_{\l'}}) < z_i$ for each optimist $i \in \Lambda(h)$, and there exists $\tau_i$ with $\X_i(\bsigma_{-i\|hv_{\l'}}, \tau_i) > z_i$ for each pessimist $i \in \Lambda(h)$.
        \end{enumerate}
    \end{restatable}

        
    %     \begin{enumerate}
    %     \item $\Lambda(h) = \bigcup_{v \in \Supp (\bsigma(h))} \Lambda(hv)$;~\label{itm:partitionanchor}
    %     \item if there is more than one vertex $v\in \Supp (\bsigma(h)) = \set{v_1,\dots,v_\l} = \Supp(\bsigma(h))$
    %     %and $\Lambda(h)$ that has more than two vertices,
    %     and if  player $i$ controls the vertex $\last(h)$
    %     \begin{itemize}
    %         \item where $i\in \Lambda(h)$, then player $i$ is in all sets $\Lambda(hv_1),\Lambda(hv_u), \dots,\Lambda(hv_\l)$, and the pairwise intersection of any two of the sets is exactly $\{i\}$; 
    %         \item  where $i\notin \Lambda(h)$ (or if it is a stochastic vertex) , then the sets $\Lambda(hv_1),\Lambda(hv_u), \dots,\Lambda(hv_\l)$ form a partition of $\Lambda(h)$. 
    %     \end{itemize}\label{itm:splitsetsanchor}
    %     \item if $i$ is an optimist, then  $\xr(\bsigma_{\|hv})[\mu_i] = z_i$;~\label{itm:optimistanchor}\theju{check if this notation is defined}

    %     \item if $i$ is a pessimist, then for every strategy $\tau_i$ for player $i$, we have $\xr(\bsigma_{-i\|hv}, \tau_i)[\mu_i] \leq z_i$.~\label{itm:pessimistanchor}
    % \end{enumerate}
    % \end{restatable}
%     \begin{proof}[Proof sketch]
%     \leonard{Do we still need this proof sketch? Maybe the examples above are sufficient now?}
%     Let $z_i$ represent the perceived reward $\xr(\bsigma)[\mu_i]$. 
%     Once we extract such a $\Lambda$ from the proposition above, we say that a subset $P$ of players is $\Lambda$-compatible if there is a history $h$ with $P = \Lambda(h)$. 
    
%     We define a strategy profile $\bsigma^\star$ by providing memory structure $\Mc = (M, \nu, \Bar{m}_\init)$.

%     We define here set $M$ to consist exactly of the state $\punish_i$ for each player $i$, for each vertex $v$ and each subset $P \subseteq \Pi$ that is $\Lambda$-compatible, the state $\anchor_{Pv}$, and also the state $\anchor_{\emptyset v}$ and finally the state $\anchor_{\Pi\bot}$. 

%     The initial memory states are exactly those of the form $\anchor_{\Pi v_0}$. 
    
%     When in the memory state of the form $\anchor_{i}v$ for singleton sets $\{i\}$ is intuitively a positional strategy that ensures player $i$ exactly the risk measure $z_i$. 
%     Similarly, when the memory state is of the form $\punish_i$, then the memory structure is the strategies of all players, that ensures player $i$ receives the worst value of perceived reward from every vertex. We remark that these punishing strategy idea was first defined first for repeated games~\cite{Aum85}.
%     We say \emph{$P$ splits at $v$} if there is a history  $hv$, we had $\Lambda(hv) = P$, and there is some successor $w_i$ such that $\Lambda(hvw_i) = P_i\neq P$. This implies that the original strategy $\sigma$ proposed a probability distribution over vertices $w_1,\dots,w_\ell$, that is,  $\Supp(\sigma(hv)) = w_1,\dots,w_\ell$. If $P$ splits at $v$, then we describe the intuition behind the memory state  $\anchor_{P v}$. Here, the strategy  is a positional strategy that reaches vertex $v$ and then randomises at $v$ between the states $w_1,\dots,w_\ell$, and switches to the memory state to a different memory state $\anchor_{P_iu}$, where $w_i$ is the result of the randomisation and $P_i$ splits at vertex $u$. 
%     If not, then we follow a positional strategy that gives all the players $i$ in $P$ the reward $z_i$, and we know one such exists from the definition of $\Lambda$.

%     Finally, we show that the strategy profile $\bsigma^\star$ constructed has the same perceived reward $\bsigma$ for each player as in the strategy profile in \cref{prop:ActualPayoff}. We also show that $\bsigma^\star$ is indeed a risk-sensitive equilibrium, where no player has an incentive to deviate in \cref{prop:Nodeviation}. 

%     To show that the strategy profile is small, we argue that the partial order structure induced by the subset of players that is $\Lambda$-compatible form the nodes of a split-DAG, and we prove a small combinatorial lemma that bounds the size of a split-DAG 
%     \end{proof}
% \begin{restatable}[A split directed acyclic graph]{definition}{splitDAG}
%     A graph $\Dc$ is a \emph{split directed acyclic graph} or a split-DAG of a finite set $S$, if the vertices of the DAG are substes of $S$, that is, $V(\Dc)\subseteq 2^S$, and
%     \begin{enumerate}
%         \item    the only source vertex of $\Dc$ is $S$, and all vertices of the DAG are reachable from $S$;\label{itm:splitDAGreachable}
%         \item    for any edge $(X, Y)$, then $X\supseteq Y$;\label{itm:splitDAGsubset}
%         % \item   if there are two distinct edges $(X,Y_1)$ and $(X, Y_2)$, then $X\supsetneq Y_1$ or $X\supsetneq Y_2$. 
%         \item   the union of all subsets outgoing from $X$  is $X$, that is, $\cup_{(X,Y_i)}{Y_i} = X$ \label{itm:splitDAGunion}
%         \item   for each vertex labelled by $X$, either $Y_i$s form a partition of $X$ or there is one element $s\in X$ such that for any pair of edges $(X,Y_i)$, $(X,Y_j)$ from $X$, we have $Y_i\cap Y_j = \{s\}$.\label{itm:splitDAGpartition} 
%     \end{enumerate}
% \end{restatable}

% \leonard{This notion and this lemma seem very abstract and technical to me.
% Since $\Lambda$ is already defined here, can't we simply count the number of sets $P$ such that there exist $h$ with $\Lambda(h) = P$?}

% We can prove this lemma  by a simple induction on the size of $S$.
% \begin{restatable}{lemma}{countingDAG}\label{lemma:counting_subset_dag}
%         Let $S$ be a finite set of size $n$. A split-DAG of $S$ has at most $4n-2$ many vertices.
% \end{restatable}

% The rest of the proof of \cref{thm:memorysmall} proceeds by reconstructing the strategy profile $\bsigma^\star$ from $\Lambda$, with memory states that remember which players are currently being anchored, and what was the last vertex seen, so that we can detect deviations.
% When a deviation is detected, the players switch to a punishing memory state, and follow a stationary strategy profile to punish the deviator.
With such a labelling $\Lambda$, we later show that there are at most $3p-2$ subsets $A$ such that $\lambda(h) = A$ for some history $h$ by an inductive argument (\cref{prop:combinatorial} in \cref{app:memorysmall}). 
We use subsets in the range of the function $\Lambda$  to create $3p-2$ memory states for each of the $n$ vertices to remember the anchoring plays at that vertex of the play, including one extra memory-state for the empty subset. In addition, the memory states also include $p$ punishing  strategies, one for each player, adding up to the number $3np-2n+p+1$. 
We construct a strategy $\bsigma^\star$ from $\Lambda$ that uses only these memory states defined above.
\end{proof}

Finally, using \cref{thm:memorysmall}, we can show the following lemma.

\begin{lemma}\label{lemma:np_easy}
    The constrained existence problem of XRSEs is in $\NP$. The same problem when players are restricted to pure strategies is still in $\NP$.
\end{lemma}

\begin{proof}
    Let $\Game_{\|v_0}$ be a simple quantitative stochastic game.
    Let $(P,O)$ be a partition of $\Pi$, and let $\bx$ and $\by$ be threshold vectors.
    By \cref{thm:memorysmall}, if there exists a (pure) XRSE with $\bx \leq \X(\bsigma) \leq \by$, then there exists one with at most $3np-2n+p+1$ memory states, where $p$ is the number of players and $n$ is the number of vertices.
    Such a strategy profile can be guessed in polynomial time.
    
    We now show that, once such a finite-memory strategy profile $\bsigma$ is guessed, one can check in polynomial time whether it is an XRSE, and satisfies the constraint $\bx \leq \X(\bsigma) \leq \by$.
    
    \begin{itemize}
        \item First, given $\bsigma$, for each player $i$, the quantity $\xr_i(\bsigma)$ can be computed in polynomial time, since it reduces to computing player $i$'s risk measure in the Markov chain induced by $\bsigma$ (which has polynomial size) (\cref{lm:secretlemma} in App.~\ref{appendix:secretlemma}).

        \item Second, checking that $\bx \leq \xr(\bsigma) \leq \by$ can be done in polynomial time.

        \item Third, for each player $i$, one must check that player $i$ has no profitable deviation.
        This can also be done in polynomial time (\cref{lm:secretlemma} in App.~\ref{appendix:secretlemma}) by computing the best risk measure player $i$ can get in the MDP induced by $\bsigma_{-i}$ (which has polynomial size).\qedhere \qedhere
    \end{itemize}
\end{proof}

\subsection{Restrictions on strategies}
We now consider subcases where the space of a strategies is restricted. 
%We will prove hardness for all those results later.
We show in \cref{thm:infinite_rho_restricted_strategy_np_easy} that restricting the memory or amount of randomness of the strategy still renders the problem only in $\NP$.
Later in this section, we prove that all these problems, including the general problem, are $\NP$-hard. This subsection therefore completes the proof of \cref{thm:NPcomplete,thm:infinite_rho_restricted_strategy_np_easy}.

We restrict the set of strategies of each player to stationary, positional or pure. 
We show that the problem is in $\NP$ for each of these cases.
%We show that even in such cases finding an RSE is $\NP$-complete. 
\begin{lemma}\label{lm:restrictionsNPeasy}
    The constrained existence problem, when all the players are restricted to positional, stationary, or pure strategies, is in $\NP$. 
\end{lemma}
\begin{proof}
    We show that we can still guess a strategy profile, and verify in polynomial time if it is indeed an XRSE.
    For the cases of positional and stationary strategies, guessing a strategy profile is straightforward, since such a strategy profile $\bsigma$ can be represented using polynomially many bits.%, since one only needs to guess the set of edges from each vertex that are being used with nonzero probability.
    We can  then verify that a given strategy profile $\bsigma$ gives risk measures within the constraints, and also is an XRSE in polynomial time (\cref{lm:secretlemma} in \cref{appendix:secretlemma}). 

    However, for pure strategies, memory might be required. But we showed with \cref{thm:memorysmall} that if there is a pure strategy profile, then there is one that requires polynomial memory, and therefore our results follow.  
\end{proof}
We now prove $\NP$-hardness of the constrained existence problem for the general setting as well as the cases where the players are restricted. 

\begin{restatable}[App.~\ref{app:np_hardness}]{lemma}{NPHard}\label{lemma:np_hardness}
    The constrained existence problem of XRSEs
    is $\NP$-hard, even when all players are pessimists and all rewards are non-negative.
    It remains $\NP$-hard when the strategies are reduced to stationary, pure, or positional ones.
\end{restatable}

\begin{proof}[Proof sketch]
     We reduce instances of $\THREESAT$ to a an instance of the problem. From a given formula $\Phi$, we construct  a game $\Game_\Phi$
    with no optimist and $4n+m+1$ pessimists, where $n$ is the number of literals and $m$ the number of clauses in $\Phi$.
    That game will contain an XRSE where a witness player gets risk measure $2$ if and only if $\Phi$ is satisfiable.
\end{proof}
%    We note that in our reduction, we can assume that all players are pessimistic.
    % , and ask if there is a constraint existence problem that asks if there is an XRSE where player $\diamond$'s perceived risk is exactly $2$. 
 
%     % Each satisfying assignment can be converted into an RSE by taking the positional stationary strategy where for a satisfied literal $\ell$, the vertex owned by $\Square\ell$ is never visited. 
% \end{proof}

This lemma, along with \cref{lemma:np_easy}, proves \cref{thm:NPcomplete}; and along with \cref{lm:restrictionsNPeasy}, it proves \cref{thm:infinite_rho_restricted_strategy_np_easy}.




\subsection{Things get easier when everyone is optimistic}
Since our $\NP$-hardness results involved only pessimistic players, we now show that
the constrained existence problem of XRSEs becomes $\PTIME$-complete when the perceived reward of each player is computed based on the risk measure $\oexp$, thus proving \cref{thm:PTIMEcomplete}.
%In this scenario, we show that the problem is $\PTIME$-complete. 
We first show an upperbound by giving a polynomial-time algorithm.  

\begin{restatable}[App.~\ref{app:ptimeupperbound}]{lemma}{ptimeupperbound}\label{lm:ptimeupperbound}
    If all players are optimists, then the constrained existence problem for XRSE is in $\PTIME$, and there is an algorithm for the decision problem, which runs in time $\Oh(pm^2)$, where $m$ is the number of edges in $\Game$ and $p$ the number of players.
    Moreover, the algorithm can be modified to output an XRSE that satisfies the constraints, if one exists in time $\Oh(pm^2 + m^3)$.  Moreover, there is an algorithm that runs in time $\Oh(pm^2)$ if the upper bounds $y_i \geq 0$ for all players $i$.
\end{restatable}

\begin{proof}[Proof Sketch.]
    We want to decide whether there exists an XRSE $\bsigma$ satisfying the constraints $\Bar{x}\leq \X(\bsigma) \leq \Bar{y}$.
    The algorithm considers and deals with two cases, that we call \emph{cycle-friendly} and \emph{cycle-averse} cases, separately.
    In the cycle-friendly case, we have $y_i \geq 0$ for all players~$i$.
    Then, an XRSE could have positive probability of reaching no terminal vertex.
    However, in the cycle-averse case, that is impossible, since there is a player $i$ such that $y_i < 0$.
    In this proof sketch, we describe only the algorithm in the cycle-friendly case.

    The algorithm constructs a decreasing sequence of sets of edges $E_0, E_1, \dots$ until it reaches a fixed point.
    For each set $E_k$, it considers the strategy profile $\bsigma^{E_k}$, defined as follows: from each non-stochastic vertex $v$, when $v$ is seen for the first time, it randomises uniformly between all edges $vw \in E_k$.
    Later, if $v$ is visited again, it always repeats the same choice.
    If some player $i$ deviates and takes an edge that they are not supposed to take, then all the players switch to a positional strategy profile designed to minimise their risk measure.
    Such a strategy profile is finite-memory, but requires $2^{|V|}|V| + p$ memory states to be represented as a memory structure: we therefore use the set $E_k$ as a succinct representation.

    At each iteration $k$, the algorithm identifies new sets of vertices $V_\bad^k$ that must be avoided. This includes the terminals that give some player $i$ a payoff that is larger than $y_i$, or vertices from which player $i$ can deviate and obtain a higher value than the value offered by the strategy profile $\X_i(\bsigma^{E_k})$. 
    If it is not possible to avoid reaching the set $V_\bad^k$, the answer $\No$ is returned.
    Otherwise, the set $E_{k+1}$ is defined from $E_k$ by removing 
    edges that ensure that $V_\bad^k$ is not reached with positive probability.
    The algorithm stops when there are no more edges to remove and answers $\Yes$ and if we have $\X_i(\bsigma^{E_k}) \geq x_i$ for each $i$, and $\No$ otherwise.

    Each iteration requires time $\Oh(mp)$ to identify and remove edges.
    Since there are $\Oh(m)$ many edges, the algorithm terminates in time $\Oh(pm^2)$.
    % Each step $k$ consists in identifying a new set of vertices $V_\bad^k$ that must be avoided.
    % At step $k=0$, it is the set of terminal vertices that give to some player $i$ a payoff that is larger than $y_i$, which would then make them have an off-constraints risk measure.
    % At step $k \geq 1$, it is the set of vertices $v$ whose adversarial value $\val(v)$ is greater than the risk measure $\X_i(\bsigma^{E_k})$, where $i$ is the player controlling $v$.
    % In other words, the vertices from which that player can have a profitable deviation.
    % Then, the algorithm computes the set $A_k$ of vertices from which, whatever the players play (using only edges of $E_k$), they have a positive probability of visiting $V_\bad^k$.
    % If $k \geq 1$ and $v_0 \in A_k$, i.e., if it is not possible to avoid reaching the set $V_\bad^k$, the answer $\No$ is returned.
    % Otherwise, the set $E_{k+1}$ is defined from $E_k$ by removing all the edges that lead from a vertex that does not belong to $A_k$ to a vertex that does, thus making sure that $V_\bad^k$ will never be reached.
    % The algorithm stops when there is no more edge to remove.
    % Then, the algorithm answers $\Yes$ and outputs the set $E_k$, as a succinct representation of the strategy profile $\bsigma^{k+1}$, if we have $\X_i(\bsigma^{E_k}) \geq x_i$ for each $i$, and answers $\No$ otherwise.
\end{proof}

Finally, we show that the problem is $\PTIME$-hard, even when there are only two players.

\begin{restatable}[App.~\ref{app:ptimelowerbound}]{lemma}{ptimelowerbound}\label{lm:ptimelowerbound}
    The constrained existence problem of XRSEs with optimistic players is $\PTIME$-hard even with only two players.
\end{restatable}

\begin{proof}[Proof sketch]
    We give a log-space reduction from the problem of deciding two-player zero-sum reachability games, which is known to be $\PTIME$-complete~\cite[Proposition~6]{Imm81}.
\end{proof}
