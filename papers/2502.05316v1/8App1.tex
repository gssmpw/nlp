% \ERzeroExp*
% \begin{proof}[Proof of \cref{lemma:ERzeroExp}]
% We have:
%         $$\re_{\beta,\rho}= -\frac{1}{\rho} \log_\beta \left( \int_{x \in \Rb} \beta^{-\rho x} \d \prob(X = x) \right).$$
%         When $\rho$ tends to $0$, the function $x \mapsto \beta^{-\rho x}$, for $x$ ranging in $\Rb$ and $\rho$ in any neighborhood of $0$, converges dominatedly to $x \mapsto 1$.
%         Therefore, the argument of the logarithm above converges to $1$, and according to the usual equivalence $\ln(1+t) \sim_{t \to 0} t$, that risk entropy has the same limit, if it has one, as:
%         $$-\frac{1}{\rho\ln(\beta)} \left( \int_{x \in \Rb} \beta^{-\rho x} \d \prob(X = x) - 1 \right) = \int_{x \in \Rb} \frac{1 - \beta^{-\rho x}}{\rho\ln(\beta)}  \d \prob(X = x).$$
%         Now, when $\rho$ tends to $0$, the function $x \mapsto \frac{1 - \beta^{-\rho x}}{\rho\ln(\beta)}$ also converges dominatedly to $x \mapsto x$, using the usual equivalence $e^t - 1 \sim_{t \to 0} t$.
%         Hence the risk entropy converges to the quantity:
%         $$\int_{x \in \Rb} x  \d \prob(X = x) = \Eb(X).$$
% \end{proof}

\subsection{Proof of \cref{lemma:RSEtoQSSG}}\label{app:RSEtoQSSG}

\RSEtoQSSG*
\begin{proof}[Proof of \cref{lemma:RSEtoQSSG}]
    Consider the simple stochastic game $\Game_{\|v_0} = \tpl{V,E,\Pi,(V_i)_{i\in \Pi},\p ,\mu}$. We will define a payoff  function $\mu'$ over the same set of terminals for game $\Game'_{v_0}$ such that $\Game'_{\|v_0} = \tpl{V,E,\Pi,(V_i)_{i\in \Pi},\p ,\mu'}$ has a Nash equilibrium if and only if $\Game$ has a $(\beta,\brho)$-ERSE.

    For a terminal vertex $t$, we simply define $\mu_i'(t) = 1-(\beta^{-\rho_i\mu_i(t)})$ if $\rho>0$, and $\mu_i'(t) = (\beta^{-\rho_i\mu_i(t)})-1$ if $\rho<0$.
    
    Consider the function $\modifiedreward{\beta}{\rho}\colon x\mapsto 1-(\beta^{-\rho x})$ if 
    $\rho>0$ and $x\mapsto (\beta^{-\rho x})-1$ if $\rho<0$
    as the modified reward function.  This function is similar to the negative utility function defined in the work of Baier et al.,~\cite{BCMP24}, where they replace terminal rewards with the negative value of $(\beta^{-\rho\mu_i(t)})$ (as they assume $\rho>0$), in order to compute the winner in a two-player zero-sum game with risk-averse players. We additionally add or subtract $1$ from their value  to ensure that besides monotonicity, this function also maps the play that does not reach a terminal in the original game to the payoff $0$, and therefore in the modified game to preserve that such plays are still mapped to $0$. 
    
    Observe that for any random variable $X$ and constant $r$, for a value $\rho>0$, we have that 
    \begin{equation}\label{inequality:REvsExp}    
    \re_{\beta,\rho}\left[X\right] \geq r\text{ if and only if }\Eb\left[\modifiedreward{\beta}{\rho}(X)\right] \geq 1-\beta^{-\rho r}\end{equation}
    since $\re_{\beta,\rho}\left[X\right] = \frac{-1}{\rho}\log_\beta\tpl{\Eb[\beta^{-\rho X}]}$.
    
    Therefore, any Nash equilibrium in the game $\Game'_{\|v_0}$ implies that there is a strategy profile $\bsigma$ such that, for all players $i\in\Pi$, in the MDP induced by $\bsigma_{-i}$, the strategy $\sigma_i$ of player $i$ is an optimal strategy. 

    We first consider the case of player $i$ where $\rho_i>0$. The case of $\rho_i<0$ is analogous, so we omit it. 
    For every strategy $\tau_i$ of player $i$, where $\rho_i>0$, if we write $\btau = (\bsigma_{-i}, \tau_i)$, we have 
    $\Eb(\bsigma)[\mu_i']\geq \Eb(\btau)[\mu_i']$, since $\bsigma$ is a Nash equilibrium. 
    Since the payoffs of $\Gc'$ at a terminal $t$ is just $\modifiedreward{\beta}{\rho}(\mu_i(v))$, we therefore have $\Eb(\bsigma)[\modifiedreward{\beta}{\rho_i}(\mu_i)]\geq \Eb(\btau)[\modifiedreward{\beta}{\rho_i}(\mu_i)]$. From  \cref{inequality:REvsExp}, we have 
    \begin{align*}
         \Eb(\bsigma)\left[\modifiedreward{\beta}{\rho_i}(\mu_i)\right]\geq \Eb(\btau)\left[\modifiedreward{\beta}{\rho_i}(\mu_i)\right]\text{ if and only if }\\
          \Eb(\bsigma)\left[1-\beta^{-\rho_i \mu_i}\right]\geq \Eb(\btau)\left[1-\beta^{\rho_i \mu_i}\right] \\
           \iff\Eb(\bsigma)\left[-\beta^{-\rho_i \mu_i}\right]\geq \Eb(\btau)\left[-\beta^{-\rho_i \mu_i}\right]
    \end{align*}
Taking $\frac{1}{\rho}\log_\beta$ on both sides,   we get the above is true iff
    \begin{align*}
         \re_{\beta,\rho_i}(\bsigma)[\mu_i] \geq -\frac{1}{\rho_i}\log_\beta\tpl{-\Eb\left[\modifiedreward{\beta}{\rho_i}(\btau)[\mu_i]\right] }\\ \text{if and only if }\re_{\beta,\rho_i}[\bsigma](\mu_i) \geq \re_{\beta,\rho_i}[\btau](\mu_i)     \end{align*}
      Therefore the strategy  $\bsigma$ is at least as good as (any strategy where one player deviates) $\btau$ for the player $i$ that deviates, when their rewards are the risk-entropy measure. Thus, the strategy profile $\bsigma$ is an ERSE.
    
%     then consider the probability distribution $\prob_{\btau}$ over plays. Define a random variable $X'$ where the value of the play in $\Game'$ sampled according to the probability distribution $\prob_{\btau}$. \theju{Need to check this again.}
%     Similarly, let the random variable $Y'$ be the value of a play in game $\Game'$ where the play is sampled according to the probability distribution $\prob_{\bsigma}$. Since $\bsigma$ is a Nash equilibrium, we have that 
%      $\Eb\left[Y'\right]\geq \Eb\left[X'\right]$.
%      Since the payoffs in $\Game'$ is just $\modifiedreward{\beta}{\rho}(\mu_i(v))$, this implies that $\Eb\left[\modifiedreward{\beta}{\rho}(Y)\right]\geq \Eb\left[\modifiedreward{\beta}{\rho}(X)\right]$, where 
%      $X$ and $Y$ are random variables where $X$ and $Y$ are both the value of the play in the original game $\Game$, however the plays sampled for  $X$, and $Y$ are according to the probability distribution $\prob_{\btau}$ and $\prob_{\bsigma}$, respectively. 
%      From the statement (\ref{inequality:REvsExp}), we have     
%      \begin{align*}
%          \Eb\left[\modifiedreward{\beta}{\rho}(Y)\right]\geq \Eb\left[\modifiedreward{\beta}{\rho}(X)\right]\text{ if and only if }\\
%           \Eb\left[1-\beta^{\rho Y}\right]\geq \Eb\left[1-\beta^{\rho X}\right] \\
%            \iff\Eb\left[-\beta^{\rho Y}\right]\geq \Eb\left[-\beta^{\rho X}\right]
%     \end{align*}
% Taking $\log_\beta$ on both sides and multiplying with $-1/\rho$, we get the above is true iff
%     \begin{align*}
%          \re_{\beta,\rho}[Y'] \geq -\frac{1}{\rho}\log_\beta\tpl{-\Eb\left[\modifiedreward{\beta}{\rho}(X)\right] }\\ \text{if and only if }\re_{\beta,\rho}[Y'] \geq \re_{\beta,\rho}[X']
%      \end{align*}
%       Therefore the strategy  $\bsigma$ is at least as good as (any strategy where one player deviates) $\bsigma'$ for the player $i$ that deviates, when their rewards are the risk-entropy measure. Thus, this makes $\sigma$ an risk-sensitive equilibrium. %\theju{badly written, need to revise. Will do it tomorrow.}
\end{proof}


\subsection{Proof of \cref{thm:ERRSErestricted}}\label{app:ERRSErestricted}


\stationaryRSE*
\begin{proof}[Proof of \cref{thm:ERRSErestricted}]
For the proof of \cref{thm:ERRSErestricted}, we first focus on \cref{itm:ERRSEitmundec}.
\begin{proposition}
    The constrained existence problem of $(\beta,\brho)$-ERSEs in quantitative stochastic games where players are restricted to pure strategies is undecidable.
\end{proposition}
\begin{claimproof}
The undecidability of the case where pure strategies are considered is inherited from Nash equilibria~\cite[Theorem~4.9]{UW11}, since the reduction for undecidability uses only pure strategies. Therefore, our current undecidability follows from \cref{proposition:Undecidable,lemma:RSEtoQSSG}.  
%We remark that the case of two player zero-sum games in the work of Baier et al.~\cite{BCMP24} is a specific sub-case of the multi-player setting with positional strategies we consider. Since deciding such a game is the same as finding $\tpl{\beta,\brho}$-ERSE, where the underlying game is a two player game, and the players have zero-sum rewards and $\brho = (\rho,-\rho)$.
\end{claimproof}


\paragraph*{Decidable subcases.} Now, we turn our attention to \cref{itm:ERRSEdecidable} to show decidability and conditional decidability results. 
\subparagraph*{Decidability lowerbounds.} The problem is $\NP$-hard in general for even two players, which follows from  the work of Ummels and Wojtczak.
Since Nash equilibria are a specific instance of the setting of ERSEs, where the risk parameters of each player is $0$, $\NP$-hardness follows~\cite[Theorem 4.4]{UW11}. 
Similarly, for stationary strategy profiles, they show $\SQRTSUM$-hardness~\cite[Theorem 4.6]{UW11} for Nash equilibria which also shows a lower bound for our case. 

\subparagraph*{Decidability upperbounds.} For decidability, we show in \cref{prop:ETRformula} that we can write a formula in the existential theory of reals (with exponentiation if $\beta=e$)
which is satisfied if and only if the constrained existence problem is satisfied. This gives the $\PSPACE$ upper bound when $\beta$ is algebraic since the existential theory of reals is in $\PSPACE$~\cite{Can88}, and decidability subject to Shanuel's conjecture when the base is $\beta = e$, since the existential theory of reals with exponentiation is decidable assuming Shanuel's conjecture~\cite{MW95}.
Our proof is similar to the one by Ummels and Wojtczak~\cite[Theorem 4.5]{UW11}, however, we need to do slightly more work to encode the payoff expressed by the entropic risk measure. %We provide the proof here for completeness. 



We only have to show that we can encode the constrained existence problem using the existential theory of reals. 
First, observe that it is enough to verify if there is a memoryless Nash equilibrium in the modified game obtained where all the terminal rewards $\mu_i(v)$ are replaced instead with $1-\tpl{\beta^{\rho\mu_i(v)}}$. This follows from \cref{lemma:RSEtoQSSG}. 

Since the players are restricted to strategies that are memoryless, we give a non-deterministic algorithm that uses the solution to sentences in $\exists\Rb$ if the values of $\beta$ and $\rho$s can be expressed in $\Qb$.  Since $\NPSPACE = \PSPACE$, and a non-deterministic procedure is still a deterministic procedure, this does not change the complexity. %If $\beta = e$, then we compute the satisfiability of such existential first order formulas expressed in $\exp\text{-}\exists\Rb$ (the existential theory of reals with exponentiation), subjec. We use solutions to such solutions as a black-box. %For the rational case, since $\exists\Rb$ is contained in $\PSPACE$ and further $\NPSPACE = \PSPACE$, this gives us a $\PSPACE$ algorithm~\cite{Can88} for the case where $\beta$ and $\rho$ are rationals. 
     
     For a game $\Game_{\|v_0} = \tpl{V,E,\Pi,(v_i)_{i\in \Pi}, \p,\mu}$, where $\mu$ is the payoff function from a terminal set of nodes $T$ to values in $\Qb$, and constraints $\Bar{x}$ and $\Bar{y}$ for each of the $n$ players in $\Pi$, our algorithm guesses, first, the support $S \subseteq E$ of the strategies that will be considered; that is, the set of edges that will be used with positive probability. %Henceforth, we assume that the mixed strategy profile therefore uses all outoging edges with positive probability. 

         %\subparagraph*{Writing values of terminals efficiently}
\begin{claim}
            For any $z$, which requires $\ell$ bits to encode, there is a formula in $\exists\Rb$ that uses only polynomially many variables in $\ell$ to encode $\modifiedreward{\beta}{\rho}(z) = 1-\beta^{-\rho z}$, where $\beta$ and $\rho$ can also be represented in $\exists \Rb$ using a polynomial formula.  If $\beta = e$, then $\modifiedreward{e}{\rho}(z) = 1-\beta^{-\rho z}$ can be expressed using the existential theory of reals with exponentiation using a formula of most polynomial length.
\end{claim}
\begin{claimproof}
            We assume without loss of generality that $\beta$ is a natural number. If $\beta$ is rational instead, and is represented by a value $a/b$, then individually find $a_1  = a^{\rho z}$ and $b_1 = b^{\rho z}$, and just find $b_1/a_1$, which is just written in $\exists\Rb$ by stating that $\exists  t_r \;\exists a_1'\colon a_1'\times a_1  = 1\land t_r = a_1'\times b_1$.
        Now that we assume that $\beta$ is a natural number, we deal with fixed finite exponentiation with rational values. Similarly, we can assume without loss of generality that $\rho z$ is a natural number. Indeed, any value $r^{a/b} = r^a\times z^{1/b}$ and $z^{1/b}$ can be written as $\exists y \colon y^b = z$.

        It suffices to show therefore that for two values $b,a$, both natural numbers, $b^a$ can be expressed in $\exists\Rb$ succinctly, using only a formula that has length that is not more a poly-log of $b$ or $a$. 
        Let $a = \sum_{i=0}^{\log_2{a}}a_i 2^i$, where $a_i\in\{0,1\}$. 
        This follows from the following observations. 
        \begin{itemize}
            \item $b^a = \prod_{i=1}^{\log_2{a}}\tpl{b^{a_i}b^{2^i}}$
            \item  $2^i$ can be expressed in a formula with at most $i+1$ many variables
            \item $b^{2^i}$ further requires at most $i$ many variables to express, because if $b_i$ represents $b^{2^{i}}$, then we have $b^{2^{i+1}} = b^{2^i}\times b^{2^i}$.
            \item Finally, using a similar trick, $b$ itself can be represented using at most $\log_2{b}$-many variables. 
        \end{itemize}
        If $\beta = e$, it naturally follows that $e^{-\rho z}$ is expressed using exponentiation with $e$. 
\end{claimproof}
        % For each value $z = \mu_i(t)$, which requires $\ell$ bits to encode, we ensured that there is a formula that uses only polynomially many variables in $\ell$ to encode $\modifiedreward{\beta}{\rho}(z) = -\beta^{-\rho z}$. 

    The above claim ensures we can efficiently represent the variables used for the payoffs of the modified game, we now can write an equation assuming that all terminal rewards are available to us as constants. 
    This will write the equation in three parts. Since we have guessed the support, we first ensure that, in fact, there are variables corresponding to the probabilities of the strategy that only take positive values on the edges corresponding to the support set that we guessed.  
    Then, we write equations using variables that compute the values of the induced Markov chain from this strategies. Finally, we also have a formula whose solution corresponds to the values of the MDP obtained for each player when playing against the strategies of all other players. Then we compare if the value of the MDP is at least as large as the underlying Markov chain for each player, to ensure that it is indeed an equilibrium. To write all of this in $\exists\Rb$, we introduce the following variables. 
     \begin{itemize}
         \item one variable  $p_{vw}$ for each pair of vertices $vw$, which corresponds to the probabilities corresponding to the strategy;
         \item a variable $r^i_v$ which corresponds to the entropic risk measure of player $i$ from vertex $v$ if they followed the strategy defined by the probabilities above; 
         \item a variable $m^i_v$ which corresponds to the value obtained by player $i$ if the game is treated as an MDP against other players.
     \end{itemize}
 
     \begin{proposition}\label{prop:ETRformula}
        For the constrained existance of ERSE in a game $\Game_{\|v_0}$ with constraints $\Bar{x},\Bar{y}$, and a subset $S$ of edges of the game, 
         \begin{itemize}
             \item there is a formula  in $\exists\Rb$ that is satisfied if and only if there is a stationary strategy that uses exactly edges in $S$, when $\beta$ and $\rho_i$ are rational values.
             \item there is a formula $\Gamma_S'$ in $\exists\Rb\text{-}\exp$ that is satisfied if and only if there is a stationary strategy that uses exactly the edges in $S$ when $\beta = e$ and $\rho_i$ are rational values.
         \end{itemize}
    \end{proposition}
    \begin{claimproof}
    The following part of the proof is similar to the one found in Ummels and Wojtczak~\cite[Theorem 4.5]{UW11}, but we provide it to suit our setting, for the sake of completeness. 
    First, we have a formula that states that the values $p_{vw}$ indeed describe a strategy. We further ensure that for stochastic vertices, the value $p_{vw}$ encodes exactly the value dictated by the probability function $\p$ by the stochastic vertex:
    \begin{align*}
    \Phi_S(\Bar{p})\:= \bigwedge_{v,w\in V} \left( p_{vw}\geq 0\right)\land \bigwedge_{v,w\in V} \left( p_{vw} \leq 1\right)
    \land \bigwedge_{i\in \Pi}\bigwedge_{v\in V_i} \tpl{\sum_{w\in E(v)}\p_{vw}=1}\land \\
    \bigwedge_{v\in V_?} \left( p_{vw} =  \p(vw)\right) \land \bigwedge_{vw\in S} \left(p_{vw}> 0 \right)
    \end{align*}
    For a fixed support $S$ of a strategy $\bsigma$, it is possible to compute the terminals $T_S$ that have non-zero probability of being reached in the underlying Markov chain that is formed, and the vertices $V_S$ from which such terminals can be reached with non-zero probability. We assign value $\mu_i'(v)$ as the reward for the terminal vertices for player $i$, and the reward $0$ for all vertices that cannot reach any terminal with positive probability. % (since $\modifiedreward{\beta}{\rho}(0) = 1$).
    \[\Omega_S^i(\Bar{p},\Bar{r}^i)\:= \bigwedge_{t\in T_S} \left(r^i_t =\mu_i'(t)\right) \land
                 \bigwedge_{v\notin V_S} \left( r^i_v = 1\right) \land
                 \bigwedge_{v\in V_S\setminus T_S} \tpl{r^i_v = \sum_{w\in E(v)}p_{vw} r^i_v}
                 \]     
    Finally, for computing the values of the MDP, we construct a similar FO statement
    \[\Psi_S^i(\Bar{p},\Bar{m}^i) \:= \bigwedge_{t\in T} \left( m^i =\mu_i'(t) \right) \land
                 \bigwedge_{v\in V_i, w\in E(v)} \left(m_v^i\geq m_w^i\right) \land
                 \bigwedge_{v\notin V\setminus V_i} \tpl{m^i_w = \sum_{w\in E(v)} p_{vw} m^i_v}\]
Finally our statement would be $$\exists \Bar{p}\:\exists\Bar{r}\:\exists\Bar{m}\colon \Phi(\Bar{p})\land \bigwedge_{i\in\Pi}\left(\tpl{x^i_{v_0}\leq r^i_{v_0}}\land \tpl{r^i_{v_0}\leq y^i_{v_0}}\land\Omega_S^i(\Bar{p},\Bar{r^i})\land \Psi_S^i(\Bar{p},\Bar{m^i})\land \left( m_{v_0}^i\leq r_{v_0}^i \right) \right)$$
For rewards that are represented by algebraic numbers that also can be expressed succinctly via $\exists\Rb$, we observe that our above reduction extends naturally. 
For $\beta = e$, we remark that the same formula is expressible using $\exists\Rb\text{-}\exp$.
    \end{claimproof}

\end{proof}
