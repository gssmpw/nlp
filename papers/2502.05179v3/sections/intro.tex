\section{Introduction}

In recent years, text-to-video (T2V) generation has achieved remarkable progress, driven by advances in diffusion probabilistic modeling~\citep{diffusion, ho2020denoising, rectifiedflow, fm}, cutting-edge architectures~\citep{unet, dit}, and the integration of extensive model parameters and large-scale datasets~\citep{lvdm, cogvideo, videocrafter1, videocrafter2, videopoet, opensora, cogvideox, sora}. Among these, DiT-based models~\citep{dit} stand out for their excellent scalability in accommodating larger model capacities and datasets. 

In video DiTs, the key operator is the 3D full attention mechanism across time ($T$), height ($H$), and width ($W$), which effectively models visual relations in scenarios with large object motions and 3D consistency. The computational complexity scales as $\mathcal{O}(T^2 H^2 W^2 \cdot C \cdot N)$, where $C$ represents the feature dimension (linked to model size) and $N$ is the number of denoising steps (function evaluation). State-of-the-art methods~\citep{moviegen, kong2024hunyuanvideo, cogvideox} typically require large model capacities (\textit{e.g.}, 12 billion parameters), high-resolution modeling (\textit{e.g.}, 1080p), and up to 50 denoising steps, for high-quality outputs. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/teaserv3.png}
\end{center}
\caption{ \textbf{Comparison between FlashVideo and other text-to-video generation paradigms.} 
(a) Single Stage DiT suffers from an explosive increase in computation cost when generating at large resolutions, rising from 30s to 2150s (\textcolor{blue}{circle} in (d)) when increasing the resolution from 270p to 1080p. (b) Though the vanilla cascade can reduce the model size in the high resolution, its second stage still samples from Gaussian noise and only uses the first-stage results as a condition. This approach cannot effectively reduce the number of function evaluations at high resolution and still costs 571.5s ({\textcolor{earthyellow}{square} in (d)}) to generate a 1080p video. (c) In contrast, FlashVideo not only decreases the model size in the second stage but also starts sampling from the first-stage results,  requiring only 4 function evaluations at high resolution while integrating a wealth of visually pleasant details, which can generate 1080P video with only 102.3s (\textcolor{green}{triangle} in (d)). Details on obtaining these statistics are provided in our \supp. }
\end{figure}\label{fig:teaser}

These requirements arise from the need to tackle key challenges in video generation, particularly ensuring high prompt fidelity and visual quality. First, achieving fidelity in both content and motion demands the model to encode extensive world knowledge. Research has shown significant improvements when increasing model parameters ($C$) from 2 billion to 12 billion~\citep{cogvideox, kong2024hunyuanvideo}. Additionally, an adequate number of denoising steps ($N$)~\citep{moviegen, kong2024hunyuanvideo, cogvideox} is essential for generating high-quality videos. While some efforts to reduce the number of steps have shown promising progress~\citep{ding2024dollar}, they are limited to lower resolutions and simpler motions. Moreover, visual quality has been proven to be tightly tied to resolution in text-to-image generation  ($H \times W$)~\citep{blattmann2023align, chen2025pixart, ren2024ultrapixel}, and for T2V tasks, the integrity of motion ($T$) must also be maintained. However, the combination of these challenges—large parameters, sufficient denoising steps, and high resolution—significantly increases the computational cost. For instance, a 5-billion-parameter model takes 2150s to generate 1080p videos, up from just 30s at the 270p resolution (Figure~\ref{fig:teaser}~(d)). 

To overcome these challenges, we introduce FlashVideo, a two-stage framework designed to separately optimize prompt fidelity and visual quality, as illustrated in Figure~\ref{fig:teaser}~(c). In the first stage, we focus on generating video content and motion that closely aligns with the user prompt. By operating at a lower resolution (\textit{e.g.}, 270p), even though we utilize a large model with 5 billion parameters with 50 evaluation steps, the model still remains efficient, requiring only 30 seconds function evaluation times (as shown in Figure~\ref{fig:teaser}~(d)). And as demonstrated in our experiments (Section.~\ref{exp:low_res}), this approach preserves semantic fidelity and motion smoothness. In the second stage, we enhance the generated video at 1080p, focusing on fine-grained detail enhancement while minimizing computational overhead. This is achieved using a lighter 2-billion-parameter model and an efficient flow-matching process with fewer evaluation steps. The two-stage framework effectively balances computational efficiency with high-quality results. 

While previous two-stage frameworks~\citep{upscaleavideo, lavie, venhancer} treat the first-stage low-resolution output as a condition and begin the second stage from Gaussian noise (Figure~\ref{fig:teaser}~(c)), this design requires 30–50 evaluation steps and still incurs significant computational cost (\textit{e.g.}, 571 seconds for 1080p generation). In contrast, FlashVideo uses flow matching to directly traverse ODE trajectories from first stage low-quality video to the final high-quality videos, eliminating the need to start from Gaussian noise. The flow matching target also tries to constrain the ODE trajectories to be straight. This design efficiently reduces the number of function evaluations to just 4 steps. As a result, FlashVideo reduces the function evaluation time for 1080p videos to just 102s, nearly $\sfrac{1}{20}$ of the time required by a single-stage model (Figure~\ref{fig:teaser}~(a)), and 5 times faster than vanilla cascade frameworks (Figure~\ref{fig:teaser}~(b)).

In summary, our contributions are: 

\begin{itemize}[leftmargin=*] 
\item We propose FlashVideo, a method that decouples video generation into two objectives: prompt fidelity and visual quality. By tailoring model sizes, resolutions, and optimization strategies in two stages, our approach achieves superior effectiveness and efficiency compared to existing methods.
\item  Innovatively, we construct nearly straight ODE trajectories starting from low-quality videos to high-quality videos through flow matching, which enables ample detail to be integrated into the video within only 4 function evaluations.
\item Our method achieves top-tier performance on VBench-Long (83.29 score) while achieving impressive function evaluation time. The two-stage design allows users to preview initial output before full-resolution generation, curtailing computational costs and wait times.
\end{itemize}

