

\section{Method}
\label{exp:method}

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/methodv3.png}
\end{center}
\caption{ \textbf{The overall pipeline of FlashVideo}. FlashVideo adopts a cascade paradigm comprised of a 5-billion-parameter DiT at the low resolution (\textit{i.e.}, Stage \Romannum{1}) and a 2-billion-parameter DiT at a higher resolution (\textit{i.e.}, Stage \Romannum{2}). The 3D RoPE is employed at both stages to model the global and relative spatiotemporal distances efficiently.  We construct training data pairs for Stage \Romannum{1} by randomly sampling Gaussian noise and low-resolution video latent. For Stage \Romannum{2}, we apply both pixel and latent degradation to high-quality videos to obtain low-quality latent values. These are then paired with high-quality latents to serve as training data. During inference, we retain a sufficient $NFE=50$ at a low resolution of 270p  for Stage \Romannum{1}. The generated videos retains high fidelity and seamless motion, albeit with detail loss. These videos are then upscaled to a higher resolution of 1080p and processed by latent degradation. With only 4 steps, our Stage \Romannum{2} regenerates accurate structures and rich high-frequency details.
}
\label{fig:method}
\end{figure}




\subsection{Overview}



In the FlashVideo framework, video pixels $x \in \mathbb{R}^{H \times W \times T}$ are first compressed into latent features $f \in \mathbb{Q}^{h \times w \times t}$ using a 3D causal VAE~\citep{cogvideox}, where $h = \sfrac{H}{8}$, $w = \sfrac{W}{8}$, and $t = \sfrac{(T - 1)}{4} + 1$. The model is designed to generate 6-second videos (with 8 frames per second, so \( T = 49 \)) at 1080p resolution. As shown in Figure~\ref{fig:method}, we then employ a two-stage, low-to-high-resolution generation pipeline, where each stage is optimized with tailored model sizes and training strategies to ensure computational efficiency. The following subsections provide a detailed description of each stage.


% \subsection{Stage-\Romannum{1}}
\subsection{Low-Resolution Stage \Romannum{1}}



In the first stage, the goal is to generate videos with well-aligned content and motion corresponding to the input prompt. To achieve this, we initialize with a large-capacity model, CogVideoX-5B~\citep{cogvideox}, which contains 5 billion parameters. For improved computational efficiency, we perform parameter-efficient fine-tuning (PEFT) to adapt the model to a lower resolution of 270p. We find that adjusting the target resolution of the MMDiT architecture~\citep{sd3} is straightforward, which is achieved by applying LoRA~\citep{lora} with rank 128 to all attention~\citep{attention}, FFN, and adaptive layer normalization~\citep{adaln} layers. Compared to full-parameter tuning, PEFT demonstrates greater robustness, especially when fine-tuned with a small batch size of 32. In contrast, full-parameter tuning with such a small batch size significantly degrades generation quality. All other configuration settings, including the denoising scheduler and  prediction target, are kept consistent with CogVideoX-5B.


\subsection{High-Resolution Stage \Romannum{2}}
\label{exp:stage2}



\paragraph{Model architecture.} For fine-grained detail enhancement, we employ another model that adheres to the block design specified in CogvideoX-2B~\citep{cogvideox}. But, we replace the original position frequency embedding with 3D RoPE~\citep{rope}, as it offers better scalability for higher resolutions during inference (see Figure~\ref{fig:rope}).  Unlike the approach in~\citep{venhancer}, which uses spatial-temporal decomposition and time-slicing attention, we find that utilizing full 3D attention is crucial for maintaining consistency of enhanced visual details in videos with significant motion and scale variance, as shown in Figure~\ref{fig:comp_consis} and discussed in Section~\ref{sec:3d_attn}. As illustrated in Figure~\ref{fig:method}, the language embedding from the first stage is directly utilized in this stage.

\paragraph{Low-cost resolution transport.} Applying the conventional diffusion process at the high-resolution stage—starting from Gaussian noise and conditioned on low-resolution video—demands substantial computational resources. To improve efficiency while maintaining high-quality detail generation, we adopt flow matching~\citep{rectifiedflow, fm} to map the low-resolution latent representation, $\mathbf{Z}_{LR}$, to the high-resolution latent representation, $\mathbf{Z}_{HR}$. Intermediate points are computed through linear interpolation between $\mathbf{Z}_{LR}$ and $\mathbf{Z}_{HR}$, as outlined in Algorithm~\ref{alg:train}. This approach eliminates redundant sampling steps at the initialization phase and avoids reliance on additional control parameters, such as those proposed in~\citep{controlnet, supir, venhancer}. Furthermore, the $t$-independent target $\mathbf{Z}_{HR} - \mathbf{Z}_{LR}$ results in straighter ODE trajectories, enabling few-step generation. During training, $\mathbf{Z}_{LR}$ is simulated, as discussed later. In the testing phase, noise-augmented videos generated in the first stage serve as the starting point, and a commonly used Euler solver with $S=4$ steps, as outlined in Algorithm~\ref{alg:inf}, is employed. Other higher-order solvers can also be used for practical applications.



\begin{center}

\begin{minipage}[t]{0.5\linewidth}

  \centering
  \scalebox{0.86}
  {
  \begin{algorithm}[H]
    \caption{\small{~Training Stage}} \label{alg:train}
    \small{
    \textbf{Input: } High quality video dataset $D_{HR}$, model $F_\theta$ with parameters $\theta$, VAE encoder $\mathcal{E}$ \\
    \textbf{Procedure: }\\
    $\:\:$ \textbf{Repeat} \\
    $\:\:\:\: \mathbf{X}_{HR} \sim \mathbf{D}_{HR}$ \\
    $\:\:\:\: \mathbf{Z}_{HR} = \mathcal{E}(\mathbf{X}_{HR})$ \\
    $\:\:\:\: \mathbf{Z}_{LR} = DEG_{latent}(\mathcal{E} (DEG_{pixel}(\mathbf{X}_{HR})))$
    
    $\:\:\:\: Target =  \mathbf{Z}_{HR} - \mathbf{Z}_{LR}$ \\
    $\:\:\:\: t \sim  Uniform([0,1])$ \\
    $\:\:\:\: \mathbf{Z}_{t} = (1 - t) \cdot \mathbf{Z}_{LR} + t \cdot \mathbf{Z}_{HR}$ \\
    $\:\:\:\:$ Take gradient descent step on \\
    $\:\:\:\:\:\:\:\: \nabla_\theta \left\| Target - F_\theta\left( \mathbf{Z}_{t}, t \right) \right\|^2 $

    $\:\:$ \textbf{Until} Converged \\
    \textbf{Return:} Model $F_\theta$
    }
  \end{algorithm}
  }
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
  % \vspace{0pt}
  \centering
  \scalebox{0.81}
  {
  \begin{algorithm}[H]
    \caption{\small{~Inference Stage}} \label{alg:inf}
    \small{
    \textbf{Inputs: } The video sample $\mathbf{X}_{LR}$ generated during the first stage, model $F_\theta$ with parameters $\theta$, VAE encoder $\mathcal{E}$ and VAE decoder $\mathcal{D}$, step number $S$  \\
    \textbf{Procedure: }\\

    $\:\: \mathbf{Z}_{LR} = DEG_{latent}(\mathcal{E}(\mathbf{X}_{LR})))$ \\
    $\:\: \Delta_{t} = \sfrac{1}{S} $ \\
    $\:\:  Z =  \mathbf{Z}_{LQ} $ \\
    $\:\:  t = 0 $ \\
    $\:\:$ \For {$step~in~[0, 1, \cdots, S-1 ]$}
    {
        $\Delta_z = F_\theta\left(Z, t\right) * \Delta_{t}$ \\
        $Z = Z + \Delta_z$ \\
        $t = t + \Delta_t$ \\
    }
     $\:\: \mathbf{Z}_{HR} = Z$ \\
     $\:\: \mathbf{X}_{HR} = \mathcal{D}(\mathbf{Z}_{HR})$ \\
    \textbf{Return: } High quality video $\mathbf{X}_{HR}$
    }
  \end{algorithm}
  }
\end{minipage}
\end{center}






\label{exp:simulation} \paragraph{Low quality video simulation.} 


To train the second-stage model, we establish paired low-resolution and high-resolution latent representations, $\mathbf{Z}_{LR}$ and $\mathbf{Z}_{HR}$. Starting from a high-quality video $\mathbf{X}_{HR}$, we apply a sequence of blur and resize operations with randomized strengths in the pixel space (details provided in the \supp), yielding the low-resolution video. This process, denoted as $DEG_{pixel}$, is outlined in Algorithm~\ref{alg:train}. Training on this simulated data enables the model to enhance images with high-frequency details, improving overall clarity, as demonstrated in Figure~\ref{fig:deg_ab}.

However, simulating low-resolution data solely through $DEG_{pixel}$ retains strong fidelity between low- and high-resolution videos, which limits the model's ability to regenerate accurate structures for small objects at high resolutions—especially when artifacts are present in the first-stage output. This limitation often manifests when there are poor structural representations for small objects, such as blurry tree branches in Figure~\ref{fig:deg_ab} or distorted eye features in Figure~\ref{fig:stage2} (e). To address this issue, we introduce latent degradation, $DEG_{latent}$, which perturbs the latent representation with Gaussian noise. This approach allows the model to diverge from the input and generate more reasonable structures for small objects. As shown in Figure~\ref{fig:deg_ab}, compared to $DEG_{pixel}$, the combination of $DEG_{latent}$ enables the model to produce sharper and more detailed tree branches and tiny background objects, significantly enhancing visual quality.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/deg_compressed.pdf}
\end{center}

\caption{\textbf{Visual showcase of $DEG_{pixel}$ and $DEG_{latent}$ impact on quality enhancement.}  From left to right, the first is the $input$, generated by the first-stage model. The term $DEG_{pixel}$ stands for the improved result yielded from the model trained only with pixel-space degradation, which adds high-frequency details to the $input$. Further, $DEG_{pixel}$ \& $DEG_{latent}$ refers to the enhanced result with model trained under both types of degradation, which further  improves small structures, such as generating branches for small trees. The improvement is significantly apparent when compared to pixel degradation only.}


\label{fig:deg_ab}
\end{figure}
 
The overall simulation process during training can be described as follows: First, pixel-space degradation is applied to the high-quality video, yielding a degraded version. This is then encoded into the latent space, represented as:
\begin{equation}
Z = \mathcal{E} \left( DEG_{pixel} \left (\mathbf{X}_{HR} \right) \right) \,.
\end{equation}
Next, the latent representation is blended with Gaussian noise $ n \sim N(0,1) $ to simulate low-quality latents, defined as:
\begin{equation}\label{eq:lq-formulation} 
Z_{LR} = DEG_{latent}(Z) = \alpha_{step} \cdot Z + \beta_{step} \cdot n \,, \quad \text{where } \alpha_{step}^2 + \beta_{step}^2 = 1 \,.
\end{equation}
The parameter $step$ determines the strength of noise augmentation. To ensure the model can perceive the noise strength in the latent space, we introduce a noise strength embedding, which is added to the time embedding. At the inference stage, only $DEG_{latent}$ is applied to the first-stage output.  In order to determine the suitable  strength of $DEG_{\text{latent}}$, we start with a wide noise step range (600-900) during the initial training. We then assess the model results under different noise steps (as shown in Figure~\ref{fig:ab_inf}~(c) and Table~\ref{table:latent_deg}). Guided by these results, we restrict the noise range to 650-750 in following training stages.



\paragraph{Coarse-to-fine training.}

Training directly on high resolution requires substantial computational costs. The use of 3D RoPE~\citep{rope, cogvideox}, a relative spatiotemporal encoding, offers good resolution scalability for our model (Section~\ref{sec:pos_emb}). As a result, we first conduct large-scale pre-training on low-resolution images and videos ($540 \times 960$) before extending to the target resolution of 1080p ($1080 \times 1920$). Observing obvious performance fluctuations in the later stages, we further fine-tune the model with a small set of high-quality samples aligned to human preferences. This low-cost additional fine-tuning stage greatly improves the model's performance.


