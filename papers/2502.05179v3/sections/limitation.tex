\section{Discussion and Limitation}

\subsection{Discussion} 

In this section, we share some insights from our exploration to help readers gain a clearer understanding of the design principles and positioning of our work, as well as to provide guidance for potential future improvements.


\paragraph{Principles of adjusting latent degradation strength.} Selecting an appropriate latent degradation strength is crucial for training the Stage \Romannum{2} model. Achieving the balance between minimizing artifacts and preserving the integrity of the original content is key. We recommend adjusting the latent degradation strength based on the Signal-to-Noise Ratio (SNR), meaning that the noise step should be increased when either the resolution or the number of video frames increases. Notably, the number of frames has a greater impact than resolution, as visual content across multiple frames exhibits stronger correlations that are harder to disrupt. For example, in preliminary experiments with 17 video frames, we find that artifacts in the input could be corrected with a noise step of 500, which is significantly lower than the optimal noise range of 650 to 750 observed when the frame count is increased to 49.


\paragraph{Fidelity vs. visual quality improvement.} A delicate balance exists between maintaining fidelity and enhancing visual quality. Unlike real-world video enhancement, where input videos purely lack high-frequency details, the first-stage generated video often contains subtle structural flaws or artifacts that require refinement. Traditional super-resolution methods, which focus on maintaining high fidelity, are unable to address these issues effectively. Conversely, regenerating new content by treating the first-stage output as a rough guide also falls short, as it conflicts with our design philosophy. We view the first-stage output as a low-cost preview, and it must align closely with the final result. To achieve this balance, we carefully adjust the strength of both strategies, ensuring that visual quality is enhanced without compromising the integrity of the original content.


\paragraph{Can Stage \Romannum{2} be a general video enhancement model?} It is noteworthy that the current training setup is specifically tailored for 1080p  and is not suitable as a general enhancement method for videos with varying resolutions or frame counts. However, we believe that with further refinement, such as incorporating additional input information regarding resolution and frame number, the model could be adapted to handle a wider range of scenarios. We aim to explore this direction in future work.
\paragraph{Challenges with increased video length.} Video enhancement is more challenging than single-image processing, as it requires ensuring the consistency of newly added details across the entire video sequence. This task calls for a model that not only improves visual quality but also manages the intricate visual relationships and motion across frames. In Stage \Romannum{2}, we address these challenges by employing 3D full attention and adjusting the degradation strength. However, as the video length increases, the computational demand of 3D full attention escalates quadratically. Moreover, if the degradation strength is not carefully adjusted, the model may resort to recovering details by directly referencing multiple frames, which can compromise its generative capacity during inference.
\paragraph{Sparse attention in Stage \Romannum{2}.} We visualize the attention maps in Stage \Romannum{2} and observe significant sparsity, particularly in space compared to time. We attribute this phenomenon to the moderate motion intensity in the current first-stage output. To reduce the computational cost of Stage \Romannum{2}, we apply FlexAttention~\citep{flexattention} to implement window-based spatial-temporal attention with $H=11, W=11, T=7$. As a result, the method performs well with significantly improved efficiency when the first-stage output contains low motion. However, we observe inconsistencies and blurred patterns in the regenerated visual details when motion is large. We propose that dynamically adjusting the window size based on motion intensity could be a promising solution in future work.




\paragraph{Resolutions of two stages.} Given sufficient computational resources, higher resolutions in both stages could be pursued. Our choice of 270p for the first stage is driven by its ability to produce preliminary results in only 30 seconds, allowing users to quickly assess whether further computation in Stage \Romannum{2} is necessary. This provides a clear advantage over contemporary methods.


\subsection{Limitation}



\paragraph{Time-Consuming VAE decoding for high-resolution videos.} Due to GPU memory constraints, decoding 1080p videos requires spatial and temporal slicing, a process that is time-consuming. Engineering advances in parallel processing and more efficient VAE architectures are essential for enabling faster generation of high-resolution videos.

\paragraph{Long Prompt for inference.} The text descriptions adopted during training are typically long and highly detailed. This may increase complexity when users provide prompts in inference. Future research could employ joint training with short prompts or engage language models designed for prompt rewriting~\citep{ji2024prompt}. This advancement can significantly enhance the user experience.

\paragraph{Challenges with fast motion.} Due to constraints in data quantity, quality, and diversity, Stage \Romannum{2} may fail when processing videos with extreme and fast motion. Potential solutions include incorporating more training data with large motion and scaling up the model capacity.


\section{Conclusions}


We introduce FlashVideo, a novel two-stage framework that separately optimizes prompt fidelity and visual quality. This decoupling allows for strategic allocation of both model capacity and the number of function evaluations (NFEs) across two resolutions, greatly enhancing computational efficiency. In the first stage, FlashVideo prioritizes fidelity at a low resolution, utilizing large parameters and sufficient NFEs. The second stage performs flow matching between low and high resolutions, efficiently generating fine details with fewer NFEs. Extensive experiments and ablation studies demonstrate the effectiveness of our approach. Moreover, FlashVideo delivers preliminary results at a very low cost, enabling users to decide whether to proceed to the enhancement stage. This decision-making capability can significantly reduce costs for both users and service providers, offering substantial commercial value.
