
\section{Experiments}

\subsection{Data Collection}


We construct a high-quality dataset by first collecting a large corpus of 1080p videos, followed by aesthetic and motion-based filtering, resulting in 2 million high-quality samples. Motion filtering is performed using RAFT~\citep{teed2020raft} to compute the average optical flow, discarding clips with low motion scores \((<1.1)\). To ensure the second-stage model learns diverse texture details, we further collect 1.5 million high-quality images at a resolution of $2048 \times 2048$. All videos and images are annotated with detailed captions generated by an internal captioning model. For human preference alignment, we manually curate a subset of 50,000 videos exhibiting high aesthetic quality, rich textures, and significant motion diversity.

\subsection{Training Setup}
\label{sec:training}


For training the first-stage model, we use only video data, which are resized to the 270p resolution. The model is trained for 50,000 iterations with a batch size of 32 and a base learning rate of $4 \times 10^{-5}$. We employ the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, a weight decay of $1 \times 10^{-4}$, and gradient clipping set to 0.1.

\label{exp:optimizer}


The second-stage model, which includes both pre-training and human preference alignment, is trained with a batch size of 64, while other hyperparameters remain consistent with those used in the first stage. The pre-training is structured into three phases: (1) training for 25,000 iterations on $540 \times 960$ image patches cropped from $2048 \times 2048$ high-resolution images, (2) 30,000 iterations on a mixed dataset of $540 \times 960$ image patches and videos at a 1:2 ratio, and (3) training on full-resolution  $1080 \times 1920$ videos for 5000 iterations. Finally, we perform (4) fine-tuning on the human preference alignment dataset for 700 iterations. For latent degradation, we initially apply noise within the step range of 600–900 for phases (1), (2), and the first 1000 iterations of (3). Based on the findings in Table~\ref{table:latent_deg}, we then narrow the noise range to 650–750 for the remaining training in (3) and (4).





\subsection{Qualitative Results}
In this section, we present visualizations of the two-stage video generation results based on various user prompts. The first-stage output prioritizes high fidelity in both content and motion, while the second stage further refines details and mitigates generation artifacts, thereby enhancing overall visual quality.


\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/demo.pdf}
\end{center}
\caption{\textbf{Generated videos of FlashVideo.} The results in the top and bottom rows are from Stage \Romannum{1} and Stage \Romannum{2}, respectively. Stage \Romannum{1} generates videos with natural motion and high prompt fidelity, as evident from the visual elements (\textbf{bold} in prompts). However, they lack detailed structures for small objects and high-frequency textures (see the \textcolor{red}{red} box). In Stage \Romannum{2}, details are significantly enriched (see the \textcolor{green}{green} box), while content remains highly consistent with the original. Visualization results are compressed. More uncompressed cases can be found on our \href{https://jshilong.github.io/flashvideo-page/}{project page}.}\label{fig:demo}
\end{figure}

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/stage2v2.pdf}
\end{center}
\caption{\textbf{Quality improvements in Stage \Romannum{2} }. We mark regions with artifacts and lacking detail in the first-stage videos using \textcolor{red}{red} boxes, while improvements from the second stage are highlighted in \textcolor{green}{green}. Zoom in for a better view. Our Stage \Romannum{2} significantly elevates visual quality across diverse content—enhancing oil painting–style sunflowers in (a), refining wrinkles and hair in (b), enriching texture structures of animals and plants in (c) and (d), and mitigating facial and object artifacts in (e).
}
\label{fig:stage2}
\end{figure}


\paragraph{Two-stage generation results.}
As shown in Figure~\ref{fig:demo}, the first-stage outputs (top rows) exhibit strong prompt fidelity with smooth motion. The key visual elements specified in the prompt, highlighted in \textbf{bold}, are accurately generated. However, artifacts and insufficient texture details, marked by the red bounding box, may still be present. In contrast, the second-stage outputs (bottom rows) significantly improve visual quality by refining small objects with plausible structures and enhancing texture richness. Notable improvements include the refined depiction of human faces (a, d), the detailed rendering of animal fur (b, c), the intricate structures of plants (a, b), and the enhanced fabric textures (d), as highlighted in the green bounding box of the second row. Moreover, despite substantial motion, high-frequency details remain temporally consistent, owing to the full attention mechanism integrated into the second stage. More uncompressed cases can be found on our \href{https://jshilong.github.io/flashvideo-page/}{project page}.



\paragraph{Artifact correction and detail enhancement in Stage \Romannum{2}.}
To further demonstrate the effectiveness of the second-stage refinement, we provide additional examples of key frames in Figure~\ref{fig:stage2}. Compared to the first-stage outputs (marked in red), the second-stage results (marked in green) exhibit significant improvements by suppressing artifacts and enriching fine details. These enhancements are evident in the more coherent depiction of oil painting-style sunflowers in (a), the refined rendering of wrinkles and hair in (b), the improved texture structures of animals and plants in (c) and (d), and the correction of facial and object artifacts in (e).



\subsection{Quantitative Results}

We first evaluate our model on the VBench-Long~\citep{huang2024vbench} benchmark utilizing  its long prompt. Subsequently, we assess the visual quality improvements achieved in Stage \Romannum{2} by employing several widely used non-reference image and video quality assessment metrics.



\begin{table*}[t]
\centering
%\scriptsize
\tiny
\setlength{\tabcolsep}{1.5pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|l|ll|llllllllllllllll}
     Method  & 
    \makecell[bc]{\rotatebox{75}{Total Score}} & 
    \makecell[bc]{\rotatebox{75}{Quality Score}} & 
    \makecell[bc]{\rotatebox{75}{Semantic Score}} & 
    \makecell[bc]{\rotatebox{75}{subject consistency}} & 
    \makecell[bc]{\rotatebox{75}{background consistency}} & 
    \makecell[bc]{\rotatebox{75}{temporal flickering}} & 
    \makecell[bc]{\rotatebox{75}{motion smoothness}} & 
    \makecell[bc]{\rotatebox{75}{dynamic degree}} & 
    \makecell[bc]{\rotatebox{75}{aesthetic quality}} & 
    \makecell[bc]{\rotatebox{75}{imaging quality}} & 
    \makecell[bc]{\rotatebox{75}{object class}} & 
    \makecell[bc]{\rotatebox{75}{multiple objects}} & 
    \makecell[bc]{\rotatebox{75}{human action}} & 
    \makecell[bc]{\rotatebox{75}{color}} & 
    \makecell[bc]{\rotatebox{75}{spatial relationship}} & 
    \makecell[bc]{\rotatebox{75}{scene}} & 
    \makecell[bc]{\rotatebox{75}{appearance style}} & 
    \makecell[bc]{\rotatebox{75}{temporal style}} & 
    \makecell[bc]{\rotatebox{75}{overall consistency}} \\
    \toprule

HunyuanVideo & 83.24 & 85.09 & 75.82 &97.37 & 97.76 &99.44 & 98.99 & 70.83 & 60.36 & 67.56 & 86.10 & 68.55 &  94.40 & 91.60 & 68.68 & 53.88 & 19.80 & 23.89 & 26.44  \\
Vchitect(VEnhancer) & 82.24 & 83.54 & 77.06 & 96.83 & 96.66 & 98.57 & 98.98 & 63.89 & 60.41 & 65.35 & 86.61 & 68.84 & 97.20 & 87.04 & 57.55 & 56.57 & 23.73 & 25.01 & 27.57 \\
CogVideoX-1.5 & 82.17 & 82.78 & 79.76 & 96.87 & 97.35 & 98.88 & 98.31 & 50.93 & 62.79 & 65.02 & 87.47 & 69.65 & 97.20& 87.55& 80.25& 52.91& 24.89& 25.19& 27.30 \\
CogVideoX-5B & 81.61 & 82.75 & 77.04 & 96.23 & 96.52& 98.66& 96.92& 70.97 & 61.98 & 62.90& 85.23& 62.11& 99.40& 82.81& 66.35& 53.20& 24.91 & 25.38 & 27.59 \\
CogVideoX-2B & 80.91 & 82.18 & 75.83 & 96.78 & 96.63 & 98.89 & 99.02 & 59.86 & 60.82 & 61.68 & 83.37 & 62.63 & 98.00 & 79.41 &69.90& 51.14 & 24.80 & 24.36 & 26.66 \\
Mochi-1 & 80.13 &82.64 &70.08 & 96.99 & 97.28 & 99.40 & 99.02 & 61.85 & 56.94 & 60.64 & 86.51 & 50.47 & 94.60 & 79.73 & 69.24 & 36.99 & 20.33& 23.65 & 25.15 \\
LTX-Video & 80.00 & 82.30 & 70.79 & 96.56 & 97.20 & 99.34 & 98.96 & 54.35 & 59.81 & 60.28 & 83.45 & 45.43 & 92.80 & 81.45 & 65.43 & 51.07 & 21.47 & 22.62 & 25.19 \\
OpenSora-1.2 & 79.76 & 81.35 & 73.39 & 96.75 & 97.61 & 99.53 & 98.50 & 42.39 & 56.85 & 63.34 & 82.22 &51.83&91.20& 90.08 &68.56  &42.44  &23.95  &24.54  &26.85  \\
OpenSoraPlan-V1.1 &  78.00 & 80.91 & 66.38 & 95.73 & 96.73 & 99.03 & 98.28 & 47.72 & 56.85 & 62.28 & 76.30 & 40.35 & 86.80 & 89.19 & 53.11 & 27.17 & 22.90 & 23.87 & 26.52 \\
\midrule




FlashVideo$_{\scalebox{0.75}{$\scriptscriptstyle 8fps$}}$ & 82.80 & 82.99 & 82.03 & 96.91 & 96.77 & 98.56 & 96.84 & 63.47 & 62.55 & 66.96 & 90.02 & 81.47 & 99.00 & 85.71 & 83.20 & 55.34 & 24.64 & 25.23 & 27.65 \\
FlashVideo$_{\scalebox{0.75}{$\scriptscriptstyle 24fps$}}$ & 83.29 & 83.72 & 81.60 & 97.14 & 97.07 & 98.57 & 98.83 & 59.86 & 62.41 & 66.12  & 88.45 & 80.27 & 99.00 & 84.14 & 82.27 & 56.71  & 24.60  & 25.23 & 27.60 \\



\bottomrule 
\end{tabular}
\caption{\textbf{Comparison with state-of-the-art open-source models on VBench-Long benchmark~\citep{huang2024vbench}.}  This includes the recent HunyuanVideo~\citep{kong2024hunyuanvideo},   Vchitect-2.0 incorporated with VEnhancer~\citep{venhancer}, varying versions of CogVideoX~\citep{cogvideox}, Mochi-1~\citep{genmo2024mochi}, LTX-Video~\citep{HaCohen2024LTXVideo}, OpenSora~\citep{opensora} and OpenSoraPlan~\citep{lin2024open}. FlashVideo employs a cascade paradigm to deliver top-tier semantic fidelity and quality.}
\label{tab:vbench}
\end{table*}


\paragraph{VBench-Long benchmark.} We follow the standard evaluation protocol of VBench-Long, generating five videos per prompt. Noting that VBench metrics tend to favor higher frame rates, we apply a real-time video frame interpolation method~\citep{huang2022rife} to upscale the frame rate from 8 fps to 24 fps. This interpolation incurs negligible post-processing time (within 4 seconds), ensuring fair comparisons with high-frame-rate methods. A more detailed discussion on VBench’s frame rate preference is provided in the \supp.


\label{exp:low_res} As shown in Table~\ref{tab:vbench}, 
both our 8fps and 24fps models achieve high semantic scores exceeding 81. However, relying solely on the first-stage model results in aesthetic and imaging quality scores below top-tier methods, with 60.74 and 61.87 for 270p. After applying the second stage, both quality scores improve significantly, reaching state-of-the-art levels of approximately 62.55 and 66.96, respectively, as reported in Table~\ref{tab:vbench}. These results validate our approach of initially reducing the resolution in Stage \Romannum{1} to ensure high prompt fidelity at a lower computational cost, followed by quality enhancement in Stage \Romannum{2}. On the other hand, our entire functional evaluation only takes about 2 minutes, significantly outperforming other methods in terms of efficiency. For example, a concurrent work, Hunyuan Video~\citep{kong2024hunyuanvideo}, which achieves a total score of 83.24 using a larger 13B single-stage model, requires 1742 seconds for  function evaluation  to generate 720p $(720\times 1280)$ results. In contrast, our method not only demonstrates superior efficiency but also generates outputs at higher resolution. Furthermore, users can obtain preliminary previews in just 30 seconds for 270p, allowing them to decide whether to proceed with the second stage or refine the input prompt. This flexibility significantly enhances the user experience.



\begin{table*}[!t]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{4.5pt}
    \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{lcccccccc}
    \toprule
    &  & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\

    \cmidrule(lr){3-6}
\cmidrule(lr){7-8}
    
    & \#NFE / Time &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)   &  Technical($\uparrow$)   & Aesthetic($\uparrow$)    \\
\midrule




Stage \Romannum{1} ~(270p) &  50 / 30.1s  & 24.54   & 0.226 & 0.334 & 11.77 & 7.280 & 96.15  \\
Stage \Romannum{2}~(1080p) & 4 / 72.2s & \textbf{53.46}  & 
\textbf{0.302} & \textbf{0.436} & \textbf{5.380} & \textbf{11.68}  & \textbf{97.87}  \\

\bottomrule
\end{tabular}
\caption{Comparison of frame quality and video quality between two stages with Vbench-Long prompts.The best results are emphasized in \textbf{bold}.}
\label{table:two_stage}
\end{table*}


\begin{table*}[!t]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccccccccc}
\toprule
&  & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){3-6}
\cmidrule(lr){7-8}
    & \#NFE / Time &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)   &  Technical($\uparrow$)   & Aesthetic($\uparrow$)    \\
\midrule
RealbasicVSR & 1 / 71.5s & \underline{54.26}  & 0.272 &\underline{0.418} &\underline{5.281}&  10.71 & \textbf{99.42}  \\
Upscale-A-Video & 30 / 376.6s & 23.67 &0.201 & 0.285 & 12.02 & 7.690 & 97.61   \\
VEnhancer & 30 / 549.2s & 51.69  & \underline{0.280}& 0.385 &  5.330 & \underline{11.63} & 98.39   \\

FlashVideo (Ours) &  4 / 72.2s  & \textbf{58.69}    &   \textbf{0.296}   & \textbf{0.439}   &\textbf{4.501}  & \textbf{11.86}  & \underline{98.92} 


\\
\bottomrule
\end{tabular}
\caption{ Frame and video quality across various video enhancement methods.  The best results are highlighted in \textbf{bold} and the second-best in \underline{underline}. }
\label{table:com_other}
\end{table*}



\paragraph{Frame and video quality assessment.} As shown in Table~\ref{table:two_stage}, we present a comprehensive comparison of visual quality between the two stages with all VBench-Long prompts. We utilize widely recognized image quality assessment metrics, including MUSIQ ($\uparrow$)~\citep{musiq}, MANIQA ($\uparrow$)~\cite{maniqa}, CLIPIQA ($\uparrow$)~\citep{clipiqa}, and NIQE ($\downarrow$)~\citep{niqe}, along with the video metric DOVER~\citep{dover}, to assess the perception of distortions (Technical $\uparrow$) and content preference and recommendation (Aesthetic $\uparrow$). It is evident that all metrics show significant improvements following the application of Stage \Romannum{2}. We argue that increasing the resolution in the second stage (Section~\ref{sec:pos_emb}), ultimately producing higher outputs (\textit{e.g.}, 2K), would further enhance visual quality, and this will be explored in future work.





\subsection{Comparison with Video Enhancement Methods}




To comprehensively evaluate the effectiveness of our tailored Stage \Romannum{2}, we compare it against several state-of-the-art video enhancement methods, including VEnhancer~\citep{venhancer}, Upscale-a-Video~\citep{upscaleavideo}, and RealBasicVSR~\citep{RealBasicVSR}. Our evaluation comprises both quantitative and qualitative analyses based on the first-stage outputs. Specifically, we construct a curated test set of 100 text prompts with detailed descriptions and generate the corresponding low-resolution 6-second 49-frame videos using Stage \Romannum{1}, incorporating diverse visual elements such as characters, animals, fabrics, and landscapes. We refer to this test set as Texture100. The following ablation study is also conducted on this test set.

The frame and video quality metrics are reported in Table~\ref{table:com_other}, where FlashVideo consistently surpasses competing methods by a substantial margin while maintaining superior efficiency. Notably, although the GAN-based RealBasicVSR achieves competitive scores on some metrics, its outputs frequently exhibit excessive smoothing, indicating a misalignment between these metrics and human perceptual preferences. Consequently, we recommend interpreting quantitative evaluations as supplementary references while prioritizing qualitative assessments. On the other hand, the diffusion-based VEnhancer demonstrates stronger generative capabilities. However, its outputs often undergo significant deviations from the input, contradicting our core design principle of enhancing visual quality while preserving fidelity. Furthermore, VEnhancer employs separate spatial-temporal modules and time slicing instead of 3D full attention, leading to reduced content consistency across extended video sequences—an issue we will explore in subsequent discussions. Additionally, its high NFE results in increased computational overhead, making high-resolution generation time-intensive. In contrast, our model achieves nearly a sevenfold speedup over VEnhancer while producing sharper high-frequency details, as evidenced in Table~\ref{table:com_other}.

Figure~\ref{fig:comp_frame} (a) illustrates a case where the woman's face contains noticeable artifacts, and the background appears blurry. Our method effectively reconstructs intricate facial details while enriching the background with high-frequency textures, maintaining both structural integrity and fidelity. 
In comparison, although VEnhancer yields a relatively clear face, it also significantly alters the background, losing fidelity entirely. Essential visual elements like ``standing water'' on the ground and the overall dim tones are completely lost. This result is contrary to our intent of using the first-stage results for preview. Other methods, such as Upscale-a-Video and RealBasicVSR, fail to correct facial artifacts and instead generate excessively smoothed patterns, further reducing realism. A similar trend is observed in Figure~\ref{fig:comp_frame} (b), where our approach delivers richer textures—such as distinct individual hairs on the cat’s body—while preserving consistency with the original input.
\label{sec:3d_attn}As discussed earlier, the full attention mechanism in our model plays a crucial role in maintaining content consistency, outperforming VEnhancer in this regard. Figure~\ref{fig:comp_consis} presents a sequence of three frames featuring substantial motion, where the camera transitions from a distant to a close-up view, leading to significant scale variations in the subject’s appearance. While both FlashVideo and VEnhancer exhibit clear improvements over the initial input, VEnhancer struggles to preserve facial identity across the key frames and introduces inconsistencies in fine details such as jacket textures and background elements. In contrast, our method effectively mitigates these issues, ensuring stable and coherent visual quality throughout the sequence.






\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/cmp_compress.pdf}
\end{center}
\caption{\textbf{Visual comparison with various video enhancement methods}. We present our results alongside enhanced versions, derived from the first-stage outputs, of four video enhancement methods.}

\label{fig:comp_frame}
\end{figure}







\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{figure/cmp2v2_compressed.pdf}
\end{center}
\caption{\textbf{Comparison of long-range detail consistency in large-motion videos. }We select a first-stage generated video with significant motion and sample three key frames. The girl in this video undergoes substantial scale variation from distant to close-up views. VEhancer~\cite{venhancer}, with spatial-temporal module and time slicing, fails to preserve identity and detail consistency. In contrast, FlashVideo leverages 3D full attention to maintain consistent facial identity and texture details. 
}


\label{fig:comp_consis}
\end{figure}




