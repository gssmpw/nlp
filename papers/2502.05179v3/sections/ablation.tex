\section{Ablation}


In this section, we conduct a series of ablation studies to evaluate the key designs of our approach. First, we examine the advantage of LoRA fine-tuning compared to full fine-tuning for adapting Stage \Romannum{1} to a new resolution. We then assess the effectiveness of RoPE in Stage \Romannum{2}. Next, we detail the low-quality video simulation strategy employed for training the Stage \Romannum{2} model. Additionally, we explore the importance of aligning the model's output with human preferences. Finally, we analyze the influence of various inference hyperparameters on the final performance.

\subsection{LoRA v.s. Full Parameter Fine-Tuning in Stage \Romannum{1}}

In the setup with a batch size of 32, we compare LoRA fine-tuning with full parameter fine-tuning for training the first-stage model at 270p resolution over the same number of iterations. The frame and video quality are evaluated on Texture100, and the semantics-related scores are assessed on VBench-Long, as shown in Table~\ref{table:lora}. In this configuration, full parameter fine-tuning tends to produce more artifacts, resulting in a degradation of both visual quality and semantic fidelity. In contrast, LoRA fine-tuning preserves the generative capabilities of the original model while efficiently adapting it to a lower resolution. Based on efficiency and performance, we opt for the LoRA strategy.


\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccccccc}
\toprule
& \multicolumn{2}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}} & \multicolumn{2}{c}{\textbf{Sematics}} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
\cmidrule(lr){6-7}
 & MUSIQ($\uparrow$) & CLIPIQA($\uparrow$) & Technical($\uparrow$)   & Aesthetic($\uparrow$)  & Object Class($\uparrow$) & Overall Consistency($\uparrow$)   \\
\midrule
 Full Fine-Tuning & 20.53 & 0.273 & 8.531 & 97.64 & 85.6 & 26.1 \\
LoRA & \textbf{23.93} & \textbf{0.286} & \textbf{8.569} & \textbf{97.87} & 
\textbf{90.3} & \textbf{27.9}  \\
\bottomrule
\end{tabular}
\caption{Comparison of LoRA and full parameter fine-tuning in Stage \Romannum{1}. Best results  are in \textbf{bold}.}
\label{table:lora}
\end{table*}





\subsection{Position Embedding in Stage \Romannum{2}}
\label{sec:pos_emb}


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.85\linewidth]{figure/rope.pdf}
\end{center}
\caption{\textbf{Results of resolution extrapolation using absolute sinusoidal and RoPE position embeddings.} Both settings perform well at the training resolution. However, while RoPE preserves detail enhancement at higher resolutions, absolute position embedding introduces noticeable artifacts beyond the training range.
}
\label{fig:rope}
\end{figure}



\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccccccccc}
\toprule
&  & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){3-6}
\cmidrule(lr){7-8}


    & \#NFE / Time &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)   &  Technical($\uparrow$)   & Aesthetic($\uparrow$)    \\
\midrule

FlashVideo-1080p & 4 / 72.2s &  58.69 & 0.296 & 0.439 & 4.501 & 11.86 & 98.92   \\
FlashVideo-2K  &  4 / 209.8s   & \textbf{62.40}  & \textbf{0.354} & \textbf{0.497} & \textbf{4.463} & \textbf{12.25} & \textbf{99.20}  \\


\bottomrule
\end{tabular}
\caption{Inference resolution scaling results of FlashVideo with RoPE. Best results  are in \textbf{bold}.}
\label{table:rope_extr}
\end{table*}





To achieve high training efficiency, we first train the Stage \Romannum{2} model at low resolution and then apply fine-tuning at higher resolutions, as detailed in Sec.\ref{sec:training}. Additionally, we aim for our model to generate high-quality videos at resolutions that exceed those used during training. To enable effective resolution generalization, we explore the use of representative position embeddings. Specifically, we compare the default absolute position embeddings~\citep{attention} from the 2-billion DiT model~\citep{cogvideox} with the rotary position embedding (RoPE)~\citep{rope}, and find that RoPE offers superior performance in such a video enhancement task.

We train the model using both position embeddings at a $540 \times 960$ resolution and test it across three settings: $540 \times 960$, $1080 \times 1920$, and $1440 \times 2560$. For the larger resolutions, we employ position embedding extrapolation. As shown in Figure~\ref{fig:rope}, while both position embeddings yield satisfactory results at the training resolution, RoPE consistently enhances details when inferring at larger scales. In contrast, absolute position embeddings exhibit clear artifacts beyond the trainining resolution. Based on these findings, we incorporate RoPE for training the second-stage model.

After training the model with RoPE at the 1080p ($1080 \times 1920$) resolution, we further extend the inference resolution to 2K ($1440 \times 2560$) using RoPE-based extrapolation. As shown in Table~\ref{table:rope_extr}, our model demonstrates improved visual quality at 2K resolution, as observed from the visual comparisons. However, the inference time increases significantly, from 74.4 seconds to 209.8 seconds. We hypothesize that larger resolutions better stimulate the detail-generation capabilities of our model, aligning with the inference scaling law~\citep{snell2024scaling} observed in large language models.

\subsection{Low-Quality Video Simulation in Stage \Romannum{2}}



\begin{table*}[!t]
\centering
\scriptsize
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lccccccccc}
\toprule
  \multicolumn{2}{c}{\textbf{Degradation}} & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){1-2} \cmidrule(lr){3-6} \cmidrule(lr){7-8}
 $DEG_{pixel}$ & $DEG_{latent}$ & MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$) & Technical($\uparrow$) & Aesthetic($\uparrow$) & \\
\midrule
  &  & 23.61   & 0.200   & 0.286 & 12.02  & 6.43 & 97.32 \\
 \checkmark &  & 49.12   & 0.253 & 0.364 & 4.95  & 7.12 & 99.02\\

 \checkmark & \checkmark & \textbf{55.45} & \textbf{0.273} & \textbf{0.409} & \textbf{4.69} & \textbf{9.09} & \textbf{98.96}  \\

\bottomrule
\end{tabular}
\caption{Comparison of frame quality and video quality when applying different degradations. Best results are in \textbf{bold}.}
\label{table:deg}
\end{table*}




\label{sec:deg_discuss} As discussed in Section~\ref{exp:stage2}, we visually demonstrate (see Figure~\ref{fig:deg_ab}) the significance of incorporating latent and pixel degradation for simulating low-quality videos during the training of Stage \Romannum{2}. In this section, we provide a more detailed quantitative evaluation. For computational efficiency, we conduct the experiment using 5-frame 1080p video inputs. We train two models for 10,000 iterations: one with only pixel degradation applied, and the other with both pixel and latent degradation. As shown in Table~\ref{table:deg}, the baseline represents the results from Stage \Romannum{1}. When the Stage \Romannum{2} model is applied with pixel degradation ($DEG_{pixel}$), the first-stage output is significantly improved, with high-frequency textures being added and overall visual quality boosted. Furthermore, incorporating latent degradation ($DEG_{latent}$) leads to even further enhancement, producing clearer and more realistic structures for small objects and background details.

\subsection{Human Preference Alignment in Stage \Romannum{2}}
\begin{table*}[!t]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{lcccccc}
\toprule
  & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){2-5}
\cmidrule(lr){6-7}
   &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)  & Technical($\uparrow$) & Aesthetic($\uparrow$)  \\
\midrule
Before   & 55.61  & 0.278 & 0.427 & 4.667 & 11.76 & 98.90   \\
After   &  \textbf{58.69} & \textbf{0.296} & \textbf{0.439} & \textbf{4.501} & \textbf{11.86} & \textbf{98.92}  \\

\bottomrule
\end{tabular}
\caption{Performance comparison of FlashVideo before and after human preference alignment.  Best results  are in \textbf{bold}.}\label{table:human_stage}
\end{table*}




In our experiments, training at 1080p resolution reveals instability, characterized by performance fluctuations across different checkpoints (every 500 iterations). We attribute this inconsistency to the varying quality of the training samples. To address this issue, we manually curate a high-quality dataset of 50,000 samples, specifically selected based on strong human preference. Our model undergoes a quick fine-tuning process on this refined dataset to stabilize training and improve performance, and then is evaluated on the Texture100 benchmark, as presented in Table~\ref{table:human_stage}. Despite the relatively small size of the selected dataset, we observe substantial improvements in both aesthetic quality and the richness of fine details. These results highlight the effectiveness of incorporating human preference into the fine-tuning process.







\subsection{Inference Hyperparameters}

\begin{table*}[h]
\tiny
\setlength{\tabcolsep}{0.6pt}
\centering
\begin{minipage}{0.45\textwidth}
\centering
\renewcommand{\arraystretch}{1.0}

\scalebox{1}{
\begin{tabular}{lccccccc}
\toprule
& \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){2-5}\cmidrule(lr){6-7}
  NFE &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)  & Tech($\uparrow$) & Aesth($\uparrow$) \\
\midrule

1 & 48.60 & 0.253 & 0.307 & 5.148 & 8.643 & 98.03  \\
2 & 55.10 & 0.287 & 0.390 & 4.730 & 10.57 & 98.38  \\
3 & 57.59 & 0.290 & 0.418 & 4.543  & 11.39 &  98.62  \\
\rowcolor{gray!40} 4 &  58.69 & 0.296 & 0.439 & 4.501 & 11.86 & 98.92  \\
\rowcolor{gray!40} 5 & 59.24 & 0.299 & 0.441 & 4.492 & 12.15 & 99.05  \\
\rowcolor{gray!40} 6 & 59.17 & 0.295 & 0.440 & 4.521 & 12.48 & 99.05 \\
7 & 59.48 & 0.298 & 0.445 & 4.578 & 12.20 & 99.01  \\
8 & 59.64 & 0.298 & 0.451 & 4.554 & 12.05  & 99.16  \\

\bottomrule
\end{tabular}
}
\caption{Results of FlashVideo under different numbers of function evaluations (NFEs). The recommended range is highlighted in gray.}\label{table:nfe}
\end{minipage}%
\hspace{0.4cm}
\begin{minipage}{0.45\textwidth}
\centering
\renewcommand{\arraystretch}{1.0}
\scalebox{1}{
\begin{tabular}{lcccccccc}
\toprule
& \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){2-5}\cmidrule(lr){6-7}
CFG &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)  & Tech($\uparrow$) & Aesth($\uparrow$) \\  


\midrule

1 & 45.01 & 0.253 & 0.359 & 5.395 & 10.75& 98.98 \\
4 & 50.92 & 0.278 & 0.397 & 5.102 &  11.71 & 99.16  \\
7 & 54.26 & 0.287 & 0.418 & 4.905 & 11.97 & 99.10   \\
 \rowcolor{gray!40}{10} & 57.37 & 0.298 & 0.441 & 4.692 & 12.15 & 99.12  \\ 
\rowcolor{gray!40}{13} &  58.69 & 0.296 & 0.439 & 4.501 & 11.86 & 98.92  \\
16 & 58.42 & 0.285 & 0.416 & 4.353 & 11.54 & 98.57  \\
19 & 57.66 & 0.277 & 0.397 & 4.143 & 11.32 & 97.84 \\
22 & 57.48 & 0.270 & 0.379 & 3.982 & 10.94  & 97.76  \\

\bottomrule
\end{tabular}
}
\caption{Results of FlashVideo under different classifier-free guidance (CFG) scales. The recommended range is highlighted in gray.  }\label{table:cfg}
\end{minipage}

\end{table*}
\begin{table*}[h]
\tiny
\setlength{\tabcolsep}{6pt}
\centering
\begin{tabular}{lccccccc}
\toprule
 & & \multicolumn{4}{c}{\textbf{Frame Quality}} & \multicolumn{2}{c}{\textbf{Video  Quality}}  \\
\cmidrule(lr){3-6}\cmidrule(lr){7-8}
Training Noise Step & Inf Noise &  MUSIQ($\uparrow$) & MANIQA($\uparrow$) & CLIPIQA($\uparrow$) & NIQE($\downarrow$)  & Tech($\uparrow$) & Aesth($\uparrow$) \\
\midrule
 \multirow{6}{*}{600-900}& 600 & 53.62 & 0.269 & 0.403 & 4.911 & 11.85 & 99.03 \\
 &  650 & 53.98 & 0.269 & 0.399 & 4.832 & 11.77 & 99.06  \\
 &  700  & 53.82 & 0.274 & 0.399 & 4.763 & 11.93 & 99.02 \\
 &  750 & 54.06 & 0.279 & 0.400 & 4.785 & 11.96 & 98.92  \\
& 800 & 53.50 & 0.276 & 0.403 & 4.663 & 11.72 & 98.91 \\
& 850 & 51.39 & 0.279 & 0.391 & 4.787 & 11.26 & 98.72  \\
\midrule
 \multirow{5}{*}{650-750}& 650 & 58.49 & 0.294 & 0.431 & 4.583 & 11.96 & 98.84  \\
& 675  & 58.69 & 0.296 & 0.439 & 4.501 & 11.86 & 98.92\\
&700 & 57.80 & 0.290 & 0.418 & 4.531 & 12.01 & 98.78  \\
&725 & 57.97 & 0.295 & 0.426 & 4.462 & 11.98 & 98.83 \\
&750 & 57.62 & 0.294 & 0.422 & 4.437 & 12.10 & 98.72 \\
\bottomrule
\end{tabular}
\centering
\caption{Results of FlashVideo under different latent degradation strengths. During initial training, a noise step range of 600–900 is applied, with model performance evaluated across different steps. The range of 650–750 consistently yields satisfactory results (see upper half of Table). This refined range is then adopted for subsequent training, with final performance presented in the lower half of Table.}\label{table:latent_deg}
\end{table*}

During the testing phase, users can flexibly adjust several hyperparameters—namely the number of function evaluations (NFEs), classifier-free guidance (CFG), and latent degradation strength (noise strength)—to suit their specific needs. We provide a detailed analysis of how these hyperparameters affect performance in Figure~\ref{fig:ab_inf}, with corresponding quality scores reported in Tables~\ref{table:nfe},~\ref{table:cfg}, and~\ref{table:latent_deg}. Unless otherwise specified, the default values for these hyperparameters are set to NFE=4, CFG=13, and NOISE=675.

\noindent \textbf{Number of Function Evaluations.} As depicted in Figure~\ref{fig:ab_inf}~(a), the processed video exhibits slight haziness and blurriness when NFE=1. Increasing the NFE improves visual quality, with more defined facial details, \textit{e.g.}, teeth and hair, and sharper textures on elements such as leaves and sweaters observed at NFE=4. Beyond NFE=4, increasing the value further (\textit{i.e.}, to NFE=5 or higher) does not result in significant visual enhancement in most cases. The qualitative results on some metrics reported in Table~\ref{table:nfe} confirm this trend, aligning with the visual observations. We recommend users to adjust the NFE to between 4 and 6 during actual use.

\noindent \textbf{Classifier-free Guidance.} The impact of the CFG scale is illustrated in Figure~\ref{fig:ab_inf}~(b). At CFG=1, the result remains blurry, with insufficient details. As the CFG value increases, the video content becomes clearer and more defined, with finer details such as earrings becoming more distinctly visible. Specifically, CFG values between 10 and 13 yield satisfactory results, striking a balance between sharpness and details. However, further  increasing CFG beyond 13 results in excessive sharpness, leading to unnaturally textured visuals. As shown in Table~\ref{table:cfg}, both image and video quality scores improve as CFG increases from 1 to 13, but several metric scores degrade when CFG exceeds 13.

\noindent \textbf{Latent degradation strength.} 
The latent degradation strength, represented by the NOISE step in equation~\ref{eq:lq-formulation}, quantifies the degree of degradation applied to the first stage  video latent. As shown in Figure~\ref{fig:ab_inf}~(c), at lower degradation levels, the enhanced video retains higher fidelity to the original input. This preservation of fidelity, while beneficial for maintaining overall content integrity, can impede the repair of artifacts and restrict the generation of finer details, such as those seen in fingers, guitar strings, and surface textures. On the other hand, increasing the noise strength promotes the generation of additional visual details. Yet, if the noise is excessive, it can distort structures or introduce blurriness, due to the inherent limitations of Stage \Romannum{2}'s generative capacity.  During the initial training phase, a broad noise step range of 600-900 is utilized. From this, we evaluate the model performance under various noise steps (as shown in the upper  part of  Table~\ref{table:latent_deg}). It is identified that the range of 650-750 yields satisfactory results consistent with visual observation. Consequently, in the following stages of the training process, a narrower range is employed and final performance is shown in the  lower part of Table~\ref{table:latent_deg}.
 
Based on the analysis above, we recommend setting NFE within the range of 4 to 6, CFG between 10 and 13, and NOISE in the range of 650 to 750. These settings should be adjusted according to the video quality produced in the first stage and the user's specific preferences.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.9\linewidth]{figure/cfg_step.png}
\end{center}
\caption{Results of stage \Romannum{2} under different inference hyper-parameters. }
\label{fig:ab_inf}
\end{figure}









