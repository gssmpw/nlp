\section{Related Work}


\paragraph{Video generation models.} Recent advancements in text-to-video (T2V) generation have been remarkable~\citep{yan2021videogpt, cogvideo, videopoet, ho2022video, blattmann2023align, blattmann2023stable, sora, kling, vidu, luma2024dream, moviegen, pyramid_flow}. Key breakthroughs have been driven by the introduction of video diffusion and flow-matching algorithms~\citep{diffusion, ho2020denoising, rectifiedflow, fm}, alongside scaled text-video datasets and DiT parameters~\citep{peebles2023scalable}. Despite impressive generation quality, a major challenge remains the high computational cost, particularly for generating high-resolution videos.




\paragraph{Cascade diffusion models.} Numerous attempts have been made to explore cascade architectures in the text-to-image and text-to-video domains~\citep{saharia2022image, gu2023matryoshka, ho2022cascaded, stablecascade, upscaleavideo, supir, lavie, venhancer}. Researchers are motivated by the challenge that generating high-resolution images/videos in a single stage is both difficult and resource-intensive. In a cascade design, generation starts with a low-resolution sample, followed by an upsampling model to enhance visual appeal at higher resolutions. However, most methods perform the second-stage upsampling from pure noise, conditioning it on the low-resolution input, which requires a large number of function evaluations. While~\citep{zheng2024cogview3, teng2023relay, i2vgen, xing2024simda} have attempted to start from the first-stage distribution, their theories and implementations are complex, resulting in a high number of inference steps. Moreover,~\citep{boosting} proposes a pure super-resolution method for T2I using flow matching, but the limited generative priors in the second-stage model hinder substantial visual improvements. In this paper, we adhere to the principle of retaining only the most effective designs, developing FlashVideo, an efficient yet simple two-stage framework that achieves high-quality, high-resolution video generation with excellent computational efficiency.



\paragraph{Diffusion speeding up.} The generation process in diffusion models can be viewed as solving ordinary differential equations. To reduce the number of function evaluations, researchers have developed advanced samplers~\citep{song2020denoising, lu2022dpm, zhang2022fast}. Additionally, techniques for distilling pre-trained diffusion models into fewer steps have shown success~\citep{salimans2022progressive, meng2023distillation, yin2024one, nguyen2024swiftbrush, berthelot2023tract}. Adversarial training has also been employed to create few-step generators~\citep{xu2024ufogen, sauer2025adversarial, lin2024sdxl}. Recently, rectified flow~\citep{rectifiedflow} with straight ODE trajectories has been introduced, further refined by subsequent works~\citep{liu2023instaflow, yan_perflow_2024}, to enable faster sampling in T2I. However, few attempts have been made in the T2V field, where the added time dimension complicates the trajectories and increases computational demands. While some efforts to reduce the number of steps in T2V have shown promise~\citep{ding2024dollar}, they remain limited to low resolutions and simple motion. In this work, we propose an efficient flow matching pipeline that enables high-resolution video generation. Notably, the acceleration techniques discussed above are compatible with our framework, allowing for further speed improvements in both stages.