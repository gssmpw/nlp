\section{Related Work}
For the purpose of our exposition, the existing works on Vision Language Navigation can be partitioned into end-to-end and modular approaches. The end-to-end methods take the natural language instructions, visual observations, and actions and train a multi-modal sequence-to-sequence model, and in the inference stage, given the instruction and initial view, the model generates the sequence of actions while ingesting additional views.  
The modular approaches integrate LLMs, VLMs, or both with more traditional map-based representations along with a common robotics navigation stack comprised of basic navigation skills that are not learned.  \\
\noindent
\textbf{End-to-end approaches.} These methods typically adopt a sequence-to-sequence model, taking as an input the language instruction and visual information and outputs the sequence of low-level navigation actions (move, turn left/right) or local waypoints. During the forward pass, the entire instruction is processed by the Language Encoder (e.g., LSTM/transformer). The aggregation of the context vectors, plus the encoded current view of the agent, is then fed to the Action Decoder (e.g., LSTM/transformer) that generates the next \texttt{action}. The decoder continues to predict actions until it generates the \texttt{STOP} action. 
% \begin{figure}[!h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/PrimitiveVLN_Proposed_Figure_v2_Final.pdf}
%   \caption{Bird's Eye View visualization of a sample VLN episode from R2R dataset}
%   \label{fig:vlnsampleepisode}
% \end{figure}
The mixture of Reinforcement Learning (RL) and Imitation Learning (IL) has been commonly used for training these 
models~\cite{tan2019envdrop}. 
% The evaluation metrics for VLN tasks are success rate (SR) which counts the number of episodes where the agent stops within the 3-meter radius of the goal; trajectory length (TL) in meters; success rate weighted by the path length (SPL); navigation error as the distance between the goal and the agent's stop location (NE); and normalized dynamic-time warping (nDTW) for computing the similarity between the agent's path and the ground-truth path.
The existing approaches proposed different variations of model architectures, training strategies and choice of representations~\cite{vln, tan2019envdrop, fried2018speaker, wang2019reinforcedxmm, vlnce, hong2021vlnrecbert, moudgil2021soat, chen2021historyhamt, georgakis2022crosscm2}  typically using the Room-to-Room (R2R)~\cite{r2r} and Room-Across-Room (RxR)~\cite{rxr} benchmarks for training and evaluation. The natural language instructions in these benchmarks are quite complex, with an average length of $\sim$ 26 words. 
% and is usually fed to the model in the beginning.
These approaches have made substantial improvements in past years, mostly thanks to increasing the number of training episodes and auxiliary tasks that support grounding \cite{wang2022lessismore} and instruction generation \cite{fried2018speaker, kamath2023marval}. It has been shown \cite{zhu2021diagnosing} that the performance of the existing methods continues to be severely compromised by the inability to ground landmarks, understand spatial relationships, as well as grounding of action phrases. The ability to ground landmarks is more critical for indoor environments, while in outdoor settings, the grounding of actions in navigation instructions is more critical.
Furthermore, RL \& IL require a large number of high-quality training episodes, in addition to the extra computational complexity of RL due to the online interaction of the agent with the simulator/environment that makes it more difficult to scale the training \cite{kamath2023marval}.\\
%
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \includegraphics[scale=0.4]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \caption{Pipeline Overview}
%   \label{fig:vlnpipelineoverview}
% \end{figure*}
\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \caption{Topological Map Construction}
  \label{fig:vlntopo}
\end{figure*}
%
\noindent
\textbf{LLM and VLM based modular approaches.} Language Models were used in the past as zero-shot planners, where  \cite{huang2022lmzeroshotplanners} introduced the idea of utilizing the knowledge learned by LLMs, like OpenAI GPT-3~\cite{brown2020gpt3} and Codex~\cite{chen2021codex}, for decomposing high-level tasks (e.g. "make breakfast") to sequences of lower level skills executable by the agent. For navigation tasks, CLIP-Nav~\cite{clip-nav} utilized CLIP VLMs~\cite{clip} for grounding instruction phrases and GPT-3~\cite{brown2020gpt3} for decomposition of complex natural language instructions into phrases. In CLIP-Nav, the language instruction is decomposed using GPT-3~\cite{brown2020gpt3}, and then each sub-instruction, along with a panorama comprised of four egocentric views, is ranked by CLIP~\cite{clip} to determine the closest heading direction. The major limitations of CLIP-Nav are the dependency on the existence of a navigable graph of the environment and the poor ability of CLIP to associate landmarks with images. 
Another decomposition of the navigation task was adopted by the VLMaps~\cite{vlmaps} approach, which first builds a global joint vision-language semantic occupancy map by exploring the environment.
% and generating all the views (using RGBD sensory information collected). 
The cells of the map are populated by LSeg/CLIP embeddings~\cite{lseg, clip}, projected onto the grid from images. 
The navigation instructions are simpler, often resorting to point and object goal navigation, which are further translated into robotic navigation skills in the form of executable code. 

\noindent Lang2LTL \cite{liu2023lang2ltl} represents another line of work that has been proposed to use LLMs to translate free-form natural language instructions into linear temporal logic (LTL). Lang2LTL is advantageous because it disambiguates the goal specification and facilitates incorporating temporal constraints. The limitations of Lang2LTL are the need for a parallel dataset of natural language instructions and their corresponding fixed set of LTL formulas for fine-tuning the LLMs for the translation stage and the limited level of complexity of the instructions, compared to R2R \cite{r2r} and RxR \cite{rxr} benchmarks. Authors in LM-Nav~\cite{shah2023lmnav} propose a zero-shot approach for outdoor instruction following. They utilize a visual navigation system called ViNG~\cite{shah2021ving}, to construct a topological map $G$ from a set of observations, followed by extraction of landmarks $L$ from the free-form navigation instruction using GPT-3. 
CLIP is then used to infer a joint probability distribution over the nodes in $G$ and landmarks in $L$, followed by a graph search algorithm to find the optimal path that is executed by local navigation policy. The approach in LM-Nav can only navigate to a sequence of unique landmarks by design, discarding complexities like spatial clauses and fine-grained grounding
of landmark and action phrases. 

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \caption{Topological Map Construction}
%   \label{fig:vlntopo}
% \end{figure*}