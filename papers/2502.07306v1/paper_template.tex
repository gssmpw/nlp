\documentclass[conference]{IEEEtran}
\usepackage{times}
\pagestyle{plain}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\usepackage{graphicx}                    %   for imported graphics
% \usepackage{amsmath}                     %%
% \usepackage{amsfonts}                    %%  for AMS mathematics
% \usepackage{amssymb}                     %%
% \usepackage{amsthm}                      %%
% \usepackage[normalem]{ulem}              %   a nice standard underline package
% \usepackage[noadjust,verbose,sort]{cite} %   arranges reference citations neatly
% \usepackage{setspace}                    %   for line spacing commands

\usepackage{booktabs}
% % % \usepackage{hyperref}
% % \usepackage{times}
% \usepackage{latexsym}
% \usepackage{colortbl}
% \usepackage{graphicx}
% % \usepackage{hyperref}
% \usepackage{enumitem}
\usepackage{xcolor}
\definecolor{bggray}{RGB}{235, 235, 233}
\definecolor{prompt}{RGB}{245, 85, 85}
\definecolor{sent}{RGB}{83, 142, 33}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{microtype}
% \usepackage{inconsolata}
% \usepackage{todonotes}

% \usepackage{color, colortbl}
% \definecolor{Gray}{gray}{0.9}

% \usepackage{amssymb}
% \usepackage{pifont}
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%
% \usepackage{subcaption}
% \usepackage{caption}

% \usepackage{multicol}

% \usepackage{url}

% \usepackage[table]{xcolor}
\definecolor{verylightgray}{rgb}{0.95, 0.95, 0.95}
\definecolor{verylightgreen}{rgb}{0.9, 1, 0.9}
\definecolor{verylightred}{rgb}{1, 0.9, 0.9}

% \usepackage{arydshln}

% \usepackage{tabularx}
% \usepackage{listings}
% \usepackage{xcolor}


% % QGroundCAM
% \usepackage{multirow}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{adjustbox}
% \usepackage{wrapfig}
% \usepackage{multicol}
% \usepackage{lipsum}
% \usepackage{stfloats}
% \usepackage[accsupp]{axessibility}

% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{paralist}
% \usepackage{textcomp}
% \usepackage{placeins}

\usepackage{xcolor}
\definecolor{mylightgray}{gray}{0.5}
\definecolor{mydarkgreen}{RGB}{0, 150, 0}
\definecolor{mydarkred}{RGB}{200, 0, 0}
\definecolor{myblue}{RGB}{0, 0, 255}
\definecolor{myorange}{RGB}{255, 100, 0}

% \usepackage[dvipsnames]{xcolor}

% \usepackage{tabularx}
% \usepackage{booktabs, multirow} % for borders and merged ranges
% \usepackage{graphicx}
% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% % \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors

% \usepackage{tabularx}
% \usepackage{booktabs, multirow} % for borders and merged ranges
% \usepackage{soul}% for underlines

\usepackage{xcolor}
\usepackage{color, colortbl}
\definecolor{verylightgreen}{rgb}{0.9, 1, 0.9}
\definecolor{verylightred}{rgb}{1, 0.9, 0.9}
\definecolor{verylightblue}{rgb}{0.8, 0.9, 1.0}
% \definecolor{verylightorange}{rgb}{1.0, 0.9, 0.8}
\definecolor{verylightorange}{rgb}{1.0, 0.8, 0.6}
\definecolor{verylightgray}{rgb}{0.95, 0.95, 0.95}

% % \usepackage{wrapfig}
% \usepackage{wrapfig,lipsum,booktabs}
% \usepackage{subcaption}

\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}

\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\comment}[1]{}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
% \title{Template paper for the \\Robotics: Science and Systems Conference}

% \title{TRAVEL: Training-Free Retrieval and Alignment using VLMs and LLMs for the Extraction of Landmarks for Vision-and-Language Navigation}

\title{TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation}

% You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Author Names Omitted for Anonymous Review. Paper-ID 692}

\author{
\authorblockN{Navid Rajabi}
\authorblockA{Department of Computer Science\\
George Mason University\\
% Atlanta, Georgia 30332--0250\\
\texttt{nrajabi@gmu.edu}}
\and
% \authorblockN{Homer Simpson}
% \authorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
\authorblockN{Jana Ko{\v{s}}eck{\'a}}
\authorblockA{Department of Computer Science\\
George Mason University\\
\texttt{kosecka@gmu.edu}}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle


\begin{abstract}
In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM.  Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps \cite{vlmaps} on the complex R2R-Habitat \cite{r2r} instruction dataset and quantify in detail the effect of visual grounding on navigation performance.

% We first demonstrate that our landmark retrieval approach outperforms the other approach, which builds a joint semantic map, by a large margin. Second, we showed the effectiveness of our path generation \& re-ranking method achieves an average nDTW of \textbf{--} and an average accuracy of \textbf{--} when testing on 105 episodes across five different Matterport3D environments of the R2R dataset.
\end{abstract}

% \begin{abstract}
% In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art LLMs and Vision-Language Models (VLMs) in a zero-shot setting. 
% Given navigation instruction in natural language, we first use the pre-trained LLMs to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations where of the last landmark and generate top-k path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment we build. Each path hypothesis is represented by a sequence of panoramas.  
% % After that, we compute the shortest path from the starting pose to the top-k goal hypothesis 
% We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names. 
% % The path that is best aligned with the sequence of instructions is computed using dynamic programming algorithm to quantify the alignment between the sequence of panoramas/nodes and the sequence of landmarks that should be visited in the same order for the current episode,  
% Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score, and the ground-truth path on the topological map to evaluate the entire path fidelity, not only reaching the last goal.
% % We demonstrate superior performance compared to other approaches that use semantic maps on the complex R2R (JK?)instruction dataset and quantify in detail the effect of visual grounding on navigation performance. 
% First, we demonstrate that our landmark retrieval approach outperforms the other approach, which builds a joint semantic map, by a large margin. Second, we showed the effectiveness of our path generation \& re-ranking method achieves an average nDTW of \textbf{--} and an average accuracy of \textbf{--} when testing on 105 episodes across five different Matterport3D environments of the R2R dataset.
% \end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Vision-and-Language Navigation (VLN) task 
% lies at the intersection of computer vision, natural language processing (NLP), and robotics navigation. It 
involves controlling an agent, either in simulation or in the physical world, to navigate through an environment by following natural language instructions. Consider an example in Fig. 1 where agent is required to 
follow the instructions in a specific environment. This task requires parsing the language input (e.g., “Turn left in the hallway, go to the kitchen, and stop by the sink”), grounding the phrases to visual concepts such as scenes, landmarks, and actions (e.g., turn left) as well temporal cues (e.g., turn before). 


\begin{figure}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/intro_fig.pdf}
  \includegraphics[width=\linewidth]{figures/Agent_PrimitiveVLN.pdf}
  \caption{Bird's Eye View visualization of a sample VLN episode from R2R dataset~\cite{r2r}.}
  \label{fig:vlnsampleepisode}
\end{figure}

% The early explorations of the instruction following task largely focused on small, synthetic domains or adventure games where the world was abstracted to discrete states or simple topologies. For instance, \cite{mooneychen2011learning} demonstrated how to map instructions to action sequences in textual  or grid-based worlds. Despite these promising beginnings, these early works primarily dealt with symbolic representations, lacking the complexity of rich natural language inputs. 

% JK With the advances in deep learning emerged approaches that tackled realistic 3D scenes by integrating Convolutional Neural Networks (CNNs) for the vision side and Recurrent Neural Networks (RNNs) for processing the natural language side to build end-to-end models to solve this task. Further VLN research has been marked by steady progress in dataset development, model architectures, and training methodologies. From RNN-based language encoders and CNN visual features to transformer-based multi-modal fusion and large-scale data augmentation strategies, VLN systems have steadily improved instruction grounding, success rate, path efficiency, etc.

One class of approaches formulates the Vision-Language Navigation task as a supervised multi-modal sequence-to-sequence learning task, where the learner is given episodes of natural language instructions, along with visual observations and navigation actions. These approaches were supported by large-scale datasets of navigation instructions, e.g., Room-2-Room (R2R) \cite{r2r}, in Matterport3D \cite{chang2017matterport3d} indoor environments, providing the agent with panoramic images from different locations. The sequence-to-sequence methods varied in their multi-modal language and vision architectures, training techniques, and choices of representations, gradually improving the benchmark performance.  Despite these improvements, non-negligible gaps still exist between machines' and human performance on existing benchmarks, the performance suffers in novel environments and in the presence of more complex variation of instructions. 

% JK A significant leap forward in VLN was fueled by creating large-scale datasets like Room-2-Room (R2R) \cite{r2r} for indoor environments. The R2R dataset was built on top of the Matterport3D \cite{chang2017matterport3d} environments, providing an agent with panoramic images corresponding to various viewpoints in house-like settings. 

% JK uman-annotated instructions were collected for short navigation paths (around 5–8 steps each). This dataset enables to measure success rate, path fidelity, and other performance metrics, inspiring numerous follow-up works on imitation learning, data augmentation, and cross-modal alignment strategies.

% JK As the field progressed, the research community recognized that navigation instructions often revolved around objects or landmarks. For example, “Go to the couch in the corner of the room” demands an agent to localize and understand the referenced object within the visual field. The REVERIE dataset \cite{qi2020reverie}  and the SOON dataset \cite{zhu2021soon} were introduced to test an agent’s ability to navigate to specific objects. Tasks in these datasets emphasize grounding objects mentioned in language instructions to visual detections and underscore the importance of fine-grained alignment between text and scene content. Beyond these specific datasets, several simulation platforms, such as Habitat \cite{savva2019habitat} and AI2-THOR \cite{kolve2017ai2thor}, offer frameworks for large-scale embodied AI research. These platforms allow for continuous 3D navigation rather than just panoramic viewpoint hops, bringing agents closer to real-world conditions.


% \textbf{Sequence-to-Sequence Learning.}

% \textbf{Policy Learning for action predictions}: Once the visual perception and language instructions are processed through the multimodal fusions, a policy module is needed to learn to classify the next action, either the panorama waypoint (in \textit{discrete setting}) or a motion primitive out of (in \textit{continuous setting}). On the other hand, Imitation Learning (IL) is an alternative approach where the agent mimics expert demonstrations from the ground-truth paths during training. Purely IL-based methods often struggle with compounding errors when faced with novel viewpoints or environment variations at inference time. Consequently, many methods adopt Reinforcement Learning (RL) fine-tuning, using reward functions tied to the distance to the goal or the success of completing the instruction.

Another class of methods pursued a more modular approach, using 
or learning separate modules for processing natural language inputs, using semantic segmentation or detection to ground noun phrases images and in the map and integrating these with more traditional map-based navigation. These methods, however, use simple natural language instructions and are typically evaluated on small-scale datasets
\cite{liu2023lang2ltl, vlmaps}.

\noindent
{\bf Contributions.}
In the presented work, we pursue a modular approach, 
where we exploit zero-shot capabilities of the state-of-the-art LLMs for understanding and parsing navigation instructions and VLMs for grounding landmark names in the visual observations. The navigation component is 
carried out by finding a path in the topological map of the environment that is best 
aligned with the navigation instructions.
The map is acquired using the training 
episodes from R2R dataset~\cite{r2r}, and the alignment score is computed using dynamic programming, where the costs of individual 
steps are obtained from the state-of-the-art Vision-Language Model. The presented modular approach demonstrates superior performance over occupancy map-based approaches and reveals current strengths and weaknesses of the state-of-the-art LLMs and VLMs for vision-language instruction following.

\comment{
JK-maybe use some this later.  
\textbf{Data Augmentation.} Data scarcity remains a concern in VLN since human-annotated instructions are expensive to collect. A popular solution introduced by \cite{fried2018speaker} is the “speaker-follower” framework. The “speaker” model synthesizes new instructions given a path, augmenting the training set for the “follower” agent. Techniques such as back-translation and environmental dropout \cite{tan2019envdrop} further enrich the training corpus, ensuring the agent sees varied descriptions and is less prone to overfitting to specific wordings.
% \textbf{Multi-step Reasoning and Complex Instructions.} Many instructions have multiple clauses sequentially describing sub-goals like “Go straight to the living room, then turn right at the table, and go to the window.” Handling such hierarchically structured instructions demands advanced memory mechanisms or hierarchical policies. Graph-based planning has also been explored, where the agent builds a topological graph of the environment and attends to relevant nodes based on the instruction’s content. These methods aim to capture long-range dependencies and reduce the risk of the agent’s partial observability leading to disorientation.
%
% \textbf{Generalization to Unseen Environments.} A lingering question is how well VLN methods transfer to unseen environments, an essential requirement for realistic deployment. Domain randomization, large-scale data augmentation, and robust training objectives have been explored to address domain shifts. The push toward pre-trained multimodal encoders (e.g., CLIP-like models) also helps the agent generalize by harnessing prior knowledge of object categories, semantics, and contexts gleaned from large image-text corpora.
%
\textbf{Open Problems in VLN.} Despite these advancements, several open challenges remain. A persistent challenge in VLN arises because both Reinforcement Learning (RL) and Imitation Learning (IL) hinge on having abundant, high-quality data. This issue is particularly acute in tasks like VLN, where human-annotated instructions are labor-intensive to obtain. Gathering a diverse set of instructions for large, photorealistic environments is significantly more cumbersome than for simpler or synthetic tasks, leading to bottlenecks in training. Recent studies such as \cite{kamath2023new} highlight how data scarcity and the requirement for extensive, high-quality demonstrations amplify the difficulty of scaling up end-to-end multimodal policies.
%
Another layer of complexity arises from online RL in 3D environments. Training an embodied agent interactively—where each episode requires an agent to step through a photorealistic or physics-driven simulator—tends to be computationally expensive and slow. This overhead constraints experimentation and hyperparameter tuning, often becoming a significant bottleneck \cite{kamath2023new}. As a result, many recent efforts favor offline IL for large-scale training.
%
A natural attempt to offset the data scarcity in VLN is to leverage large-scale image-text pre-training from the web. However, efforts to directly transfer these internet-scale representations into downstream VLN tasks have shown surprisingly limited gains. VLN-BERT \cite{majumdar2020vlnbert} demonstrated that while pre-training on large image-text corpora does provide some improvements, the transfer to navigation-specific subtasks is less than expected. This outcome indicates that the linguistic and visual grounding learned from broad web data does not seamlessly translate to the intricate, step-by-step reasoning and spatial understanding required in VLN.
%
As mentioned earlier, data augmentation has been a popular approach for alleviating the paucity of human instructions. However, generating high-quality synthetic instructions remains non-trivial. For instance, despite the value of models like Speaker-Follower \cite{fried2018speaker}, subsequent evaluations revealed issues of instruction naturalness, coherence, and alignment with true human expression \cite{kamath2023new}. Poorly generated instructions, if used naively in training, may introduce detrimental noise and degrade an agent’s ability to interpret true human-authored instructions, underscoring the importance of careful curation and filtering of synthetic data.
%
Another key limitation in state-of-the-art VLN models is their often shallow compositional understanding of navigation instructions. Real-world instructions can reference attributes and spatial cues (“Stop in front of the grey couch”), contain long referring expressions (“Walk into the first open door in the hall that leads to a bedroom with photo art on the wall near the entrance”), or incorporate temporal constraints (“With the sink on your left go around the counter”). Handling these nuances demands a holistic approach that can parse language into discrete elements—objects, locations, and actions—and then maintain a coherent representation of the environment’s evolving state. Whether it is pre-condition constraints (“With the sink on your left…”) or post-condition constraints (“Walk until you are in the next room”), the agent must interpret each clause correctly while retaining context across multiple steps. Additional complexity arises when imperatives and negations come into play, such as “Do not enter the bathroom but wait just outside,” requiring the agent to maintain an internal model of forbidden and required actions.
%
A major concern in VLN pertains to the gap between performance in \underline{\textit{seen}} vs. \underline{\textit{unseen}} environments—an indicator of weak generalizability. In many cases, an agent overfits to the training scenes, failing to robustly adapt to new layouts or unfamiliar visual styles. This can be traced to several factors, including insufficient landmark grounding, the relative simplicity of most benchmark environments, and the data-hungry nature of RL-based end-to-end approaches. Moreover, while Large Language Models (LLMs) have showcased impressive zero-shot abilities in purely linguistic tasks, leveraging their language-only knowledge for VLN remains under-explored. If harnessed effectively, LLMs could distill high-level semantic and syntactic insights into navigation policies, reducing the need for massive environment-specific annotations or lengthy RL training cycles. Bridging this gap requires rethinking how to incorporate language priors from LLMs into multimodal pipelines and how to endow agents with sufficient visual grounding and navigational awareness.
%
A promising direction to address the bottlenecks in data collection, training speed, and instruction complexity is the emergence of powerful zero-shot or few-shot pipelines grounded in Vision-Language Models (VLMs), Large Language Models (LLMs), and increasingly Multimodal Large Language Models (MLLMs). These models, often pre-trained on massive web-scale datasets, come equipped with a wealth of world knowledge and linguistic flexibility that can be transferred to VLN tasks without extensive domain-specific fine-tuning. For instance, a large MLLM that already understands a wide range of referring expressions and spatial descriptions could more effectively parse previously unseen instructions by leveraging its pre-trained capabilities. Additionally, these models can operate in a \textit{zero-shot} mode, where minimal or no in-domain data is necessary to produce reasonable navigation policies. This approach can alleviate the need for expensive online RL interactions and reduce the complexity of training pipelines. Zero-shot pipelines help address issues such as compositional instruction understanding, landmark grounding, and language ambiguity by embedding high-level semantic and syntactic knowledge directly into navigation policies. Moreover, the ease of adaptation from broad real-world data can improve an agent’s ability to tackle novel objects, layouts, and linguistic variations—ultimately narrowing the performance gap between seen and unseen environments.
}
% \section{Section}

\section{Related Work}
For the purpose of our exposition, the existing works on Vision Language Navigation can be partitioned into end-to-end and modular approaches. The end-to-end methods take the natural language instructions, visual observations, and actions and train a multi-modal sequence-to-sequence model, and in the inference stage, given the instruction and initial view, the model generates the sequence of actions while ingesting additional views.  
The modular approaches integrate LLMs, VLMs, or both with more traditional map-based representations along with a common robotics navigation stack comprised of basic navigation skills that are not learned.  \\
\noindent
\textbf{End-to-end approaches.} These methods typically adopt a sequence-to-sequence model, taking as an input the language instruction and visual information and outputs the sequence of low-level navigation actions (move, turn left/right) or local waypoints. During the forward pass, the entire instruction is processed by the Language Encoder (e.g., LSTM/transformer). The aggregation of the context vectors, plus the encoded current view of the agent, is then fed to the Action Decoder (e.g., LSTM/transformer) that generates the next \texttt{action}. The decoder continues to predict actions until it generates the \texttt{STOP} action. 
% \begin{figure}[!h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/PrimitiveVLN_Proposed_Figure_v2_Final.pdf}
%   \caption{Bird's Eye View visualization of a sample VLN episode from R2R dataset}
%   \label{fig:vlnsampleepisode}
% \end{figure}
The mixture of Reinforcement Learning (RL) and Imitation Learning (IL) has been commonly used for training these 
models~\cite{tan2019envdrop}. 
% The evaluation metrics for VLN tasks are success rate (SR) which counts the number of episodes where the agent stops within the 3-meter radius of the goal; trajectory length (TL) in meters; success rate weighted by the path length (SPL); navigation error as the distance between the goal and the agent's stop location (NE); and normalized dynamic-time warping (nDTW) for computing the similarity between the agent's path and the ground-truth path.
The existing approaches proposed different variations of model architectures, training strategies and choice of representations~\cite{vln, tan2019envdrop, fried2018speaker, wang2019reinforcedxmm, vlnce, hong2021vlnrecbert, moudgil2021soat, chen2021historyhamt, georgakis2022crosscm2}  typically using the Room-to-Room (R2R)~\cite{r2r} and Room-Across-Room (RxR)~\cite{rxr} benchmarks for training and evaluation. The natural language instructions in these benchmarks are quite complex, with an average length of $\sim$ 26 words. 
% and is usually fed to the model in the beginning.
These approaches have made substantial improvements in past years, mostly thanks to increasing the number of training episodes and auxiliary tasks that support grounding \cite{wang2022lessismore} and instruction generation \cite{fried2018speaker, kamath2023marval}. It has been shown \cite{zhu2021diagnosing} that the performance of the existing methods continues to be severely compromised by the inability to ground landmarks, understand spatial relationships, as well as grounding of action phrases. The ability to ground landmarks is more critical for indoor environments, while in outdoor settings, the grounding of actions in navigation instructions is more critical.
Furthermore, RL \& IL require a large number of high-quality training episodes, in addition to the extra computational complexity of RL due to the online interaction of the agent with the simulator/environment that makes it more difficult to scale the training \cite{kamath2023marval}.\\
%
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \includegraphics[scale=0.4]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \caption{Pipeline Overview}
%   \label{fig:vlnpipelineoverview}
% \end{figure*}
\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \caption{Topological Map Construction}
  \label{fig:vlntopo}
\end{figure*}
%
\noindent
\textbf{LLM and VLM based modular approaches.} Language Models were used in the past as zero-shot planners, where  \cite{huang2022lmzeroshotplanners} introduced the idea of utilizing the knowledge learned by LLMs, like OpenAI GPT-3~\cite{brown2020gpt3} and Codex~\cite{chen2021codex}, for decomposing high-level tasks (e.g. "make breakfast") to sequences of lower level skills executable by the agent. For navigation tasks, CLIP-Nav~\cite{clip-nav} utilized CLIP VLMs~\cite{clip} for grounding instruction phrases and GPT-3~\cite{brown2020gpt3} for decomposition of complex natural language instructions into phrases. In CLIP-Nav, the language instruction is decomposed using GPT-3~\cite{brown2020gpt3}, and then each sub-instruction, along with a panorama comprised of four egocentric views, is ranked by CLIP~\cite{clip} to determine the closest heading direction. The major limitations of CLIP-Nav are the dependency on the existence of a navigable graph of the environment and the poor ability of CLIP to associate landmarks with images. 
Another decomposition of the navigation task was adopted by the VLMaps~\cite{vlmaps} approach, which first builds a global joint vision-language semantic occupancy map by exploring the environment.
% and generating all the views (using RGBD sensory information collected). 
The cells of the map are populated by LSeg/CLIP embeddings~\cite{lseg, clip}, projected onto the grid from images. 
The navigation instructions are simpler, often resorting to point and object goal navigation, which are further translated into robotic navigation skills in the form of executable code. 

\noindent Lang2LTL \cite{liu2023lang2ltl} represents another line of work that has been proposed to use LLMs to translate free-form natural language instructions into linear temporal logic (LTL). Lang2LTL is advantageous because it disambiguates the goal specification and facilitates incorporating temporal constraints. The limitations of Lang2LTL are the need for a parallel dataset of natural language instructions and their corresponding fixed set of LTL formulas for fine-tuning the LLMs for the translation stage and the limited level of complexity of the instructions, compared to R2R \cite{r2r} and RxR \cite{rxr} benchmarks. Authors in LM-Nav~\cite{shah2023lmnav} propose a zero-shot approach for outdoor instruction following. They utilize a visual navigation system called ViNG~\cite{shah2021ving}, to construct a topological map $G$ from a set of observations, followed by extraction of landmarks $L$ from the free-form navigation instruction using GPT-3. 
CLIP is then used to infer a joint probability distribution over the nodes in $G$ and landmarks in $L$, followed by a graph search algorithm to find the optimal path that is executed by local navigation policy. The approach in LM-Nav can only navigate to a sequence of unique landmarks by design, discarding complexities like spatial clauses and fine-grained grounding
of landmark and action phrases. 

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \caption{Topological Map Construction}
%   \label{fig:vlntopo}
% \end{figure*}

\section{Our Approach}
% In order to improve the compositionality and resource efficiency of the end-to-end approaches to some extent, 
We introduce a modular approach for solving the VLN task using the pre-trained state-of-the-art language and vision and language models in a zero-shot setting, focusing on complex instructions in { R2R-Habitat dataset}. Our approach consists of eight main steps.
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \includegraphics[scale=0.4]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \caption{Pipeline Overview}
%   \label{fig:vlnpipelineoverview}
% \end{figure*}

In \textbf{\textsc{Step 1}}, the agent first builds a topological map of the environment using the train split episodes of the dataset. We used all the available unique waypoints and trajectories of the environment to build the graph $G$, where each node $v$ is represented by a 360\textdegree{} RGB panorama and each edge $e$ has a weight of 1, representing the connectivity between each pair of nodes, as shown in Figure \ref{fig:vlntopo}. In this way, we ensure consistency in our evaluation process as every node of the ground-truth waypoints from the training episodes has a corresponding node in the topological map.
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \caption{Topological Map Construction}
%   \label{fig:vlntopo}
% \end{figure*}

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \caption{Topological Map Construction}
%   \label{fig:vlntopo}
% \end{figure*}

% \subsection{Landmark Sequence Extraction from Instruction}


% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/Llama3_Landmark_Extraction_Prompt_Viz_Final.pdf}
%   % \includegraphics[scale=0.45]{figures/Llama3_Landmark_Extraction_Prompt_Viz_Final.pdf}
%   \includegraphics[width=\linewidth]{figures/Llama3_Landmark_Extraction_Prompt_Viz_Final.pdf}
%   \caption{LLama-3.1-8B Prompting for Landmark Sequence Extraction}
%   \label{fig:llama3_for_landmarks}
% \end{figure*}


In \textbf{\textsc{Step 2}}, we extract the sequence of landmarks from the natural language instruction using a pre-trained LLM, \texttt{LLama-3.1-8B-Instruct} in our case. We identify the last landmark phrase and search panoramas for the top-k most likely goal nodes. Suppose that the last landmark is \textit{bedroom}, we can locate the goals by recognizing whether the \textit{bedroom} can be found in the panoramic images associated with the graph nodes. In this way, we will narrow down the set of possible paths that lead to the goal locations. 

In \textbf{\textsc{Step 3}}, we use the state-of-the-art vision language model SigLIP \cite{siglip} for goal/final landmark recognition, as shown in Figure \ref{fig:siglipVSvlmaps}. SigLIP training is similar to the CLIP model, replacing the contrastive loss with sigmoid binary prediction. The recognition is carried out by computing cosine similarity between panorama images and the textual description of the landmark. 
In order to compare the effectiveness of this choice with an open-vocabulary semantic map such as VLMaps \cite{vlmaps} that endows the occupancy map with CLIP embeddings, 
we ran the landmark localization experiment on all 127 landmarks and reported the mean Precision@10 in Table \ref{tab:vlmapsSiglipRetrievalResults}. The superiority of our approach stems from recognizing the landmarks in the panoramic views and replacing CLIP \cite{clip} with SigLIP \cite{siglip}, instead of using open-vocabulary semantic occupancy maps.
% \begin{table*}[!h]
% \centering
% {\footnotesize
% \begin{tabular}{lccc}
% \toprule
% Model & R2R-Habitat-MP3D & \# Landmarks & Precision@10 ($\%$)\\ \midrule
% VLMaps ~\cite{vlmaps} w/ CLIP ~\cite{clip} & 8WUmhLawc2A & 127 & 34.4 \\ \midrule
% \rowcolor{verylightgreen} \textbf{Ours} w/ SigLIP ~\cite{siglip} & 8WUmhLawc2A & 127 & 70.0 \\
% % \rowcolor{white}  & \texttt{Env2} & \textbf{-} \\
% % \rowcolor{white} \multirow{-2}{*}{{(Ours)}} & \texttt{Env2} & \textbf{-}\\ \midrule
% % \rowcolor{green}  & \texttt{Env1} & \textbf{-} \\
% % \rowcolor{lime} \multirow{-2}{*}{{(Ours)}} & \texttt{Env2} & \textbf{-}\\
% \bottomrule \\
% \end{tabular}}
% \caption{SigLIP vs. VLMaps Quantitative Results for Last Landmark Indexing}
% \label{tab:vlmapsSiglipRetrievalResults}
% \vspace*{-1em}
% \end{table*}

\begin{table}[!h]
\centering
{\footnotesize
\begin{tabular}{lcc}
\toprule
Model & \# Landmarks & Precision@10 ($\%$)\\ \midrule
VLMaps \cite{vlmaps} w/ CLIP \cite{clip} &  127 & 34.4 \\ \midrule
\rowcolor{verylightgreen} \textbf{Ours} w/ SigLIP \cite{siglip} & 127 & \textbf{70.0} \\
% \rowcolor{white}  & \texttt{Env2} & \textbf{-} \\
% \rowcolor{white} \multirow{-2}{*}{{(Ours)}} & \texttt{Env2} & \textbf{-}\\ \midrule
% \rowcolor{green}  & \texttt{Env1} & \textbf{-} \\
% \rowcolor{lime} \multirow{-2}{*}{{(Ours)}} & \texttt{Env2} & \textbf{-}\\
\bottomrule \\
\end{tabular}}
\caption{SigLIP vs. VLMaps Quantitative Results for Last Landmark Indexing}
\label{tab:vlmapsSiglipRetrievalResults}
\vspace*{-1em}
\end{table}

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/SigLIP_vs_VLMaps_Small_Landmark_Bedroom_Viz.pdf}
%   \includegraphics[scale=0.34]{figures/SigLIP_vs_VLMaps_Small_Landmark_Bedroom_Viz.pdf}
%   \caption{SigLIP vs. VLMaps Query Result for Bedroom Landmark}
%   \label{fig:siglip_vs_vlmaps_large_bedroom}
% \end{figure*}
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/SigLIP_vs_VLMaps_Small_Landmark_AirVent_Viz.pdf}
%   \includegraphics[scale=0.34]{figures/SigLIP_vs_VLMaps_Small_Landmark_AirVent_Viz.pdf}
%   \caption{SigLIP vs. VLMaps Query Result for Air Vent Landmark}
%   \label{fig:siglip_vs_vlmaps_small_airvent}
% \end{figure*}

\begin{figure*}[!h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SigLIP_vs_VLMaps_Small_Landmark_Bedroom_Viz.pdf}
        \caption{Bedroom}
        \label{fig:sub1}
    \end{subfigure}
    \hfill % This ensures minimal space between the subfigures
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/SigLIP_vs_VLMaps_Small_Landmark_AirVent_Viz.pdf}
        \caption{Air Vent}
        \label{fig:sub2}
    \end{subfigure}
    \caption{SigLIP vs. VLMaps Query Result for Last Landmark Indexing}
    \label{fig:siglipVSvlmaps}
\end{figure*}


In \textbf{\textsc{Step 4}}, given the top-k goal locations, we compute the BFS shortest path from the starting pose to the goal nodes, obtaining k paths hypotheses. In the next stage, we quantify the alignment of the instruction with each of the paths and select the one with the highest alignment score. We introduce two approaches for path-instruction alignment and ranking. In \textsc{Approach I}, which is described as \textbf{\textsc{Step 5}}, we formulate this problem as a sequence-to-sequence alignment, where the sequence of panoramas is  $X = \left[X_{0}, X_{1}, ..., X_{p}\right]$, and the sequence of landmark phrases $Y = \left[Y_{0}, Y_{1}, ..., Y_{l}\right]$, as shown in Figure \ref{fig:seqalignment}. Considering $X \times Y$ as a matrix $A$, where $A_{ij}$ is the binary grounding scores
of landmark being present in the panorama associated with the waypoint. We use the state-of-the-art VLM, GPT-4o, in our case, as shown in Figure \ref{fig:gpt4o_grounding_score} to obtain these scores.
\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/GPT4o_Grounding_Score_Prompting_Viz_Final.pdf}
  % \includegraphics[scale=0.45]{figures/GPT4o_Grounding_Score_Prompting_Viz_Final.pdf}
  \includegraphics[width=\linewidth]{figures/GPT4o_Grounding_Score_Prompting_Viz_Final.pdf}
  \caption{GPT-4o Landmark Grounding Score Extraction}
  \label{fig:gpt4o_grounding_score}
\end{figure*}
We first discard the path hypotheses where the number of nodes is smaller than the number of landmarks. Then, given the $A$ matrix 
% for each path where the number of panoramas exceeds the landmark numbers, 
we compute for each path the normalized alignment score using Dynamic Programming (DP) formulation similar to the Longest Common Subsequence (LCS) problem, named Pano2Land described in Algorithm \ref{alg:algo}. 
% We perform Dynamic Programming (DP) on the matrix $A$, specifying which alignment of each landmark phrase with a set of waypoints ~\cite{wang2022lessismore}.
%
\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/sample_alignment_scoring_final.png}
  % \includegraphics[scale=0.38]{figures/DP_LCS_Scores_Examples.pdf}
  \includegraphics[width=\linewidth]{figures/DP_LCS_Scores_Examples.pdf}
  \caption{Sequence Alignment for Path Ranking (Pano2Land)}
  \label{fig:seqalignment}
\end{figure*}

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/sample_alignment_scoring_final.png}
%   \includegraphics[scale=0.38]{figures/sample_alignment_scoring_final.png}
%   \caption{Sequence Alignment for Path Ranking}
%   \label{fig:seqalignment}
% \end{figure*}
Figure \ref{fig:seqalignment} shows the alignment matrix $A$  for three path hypotheses, comprised of 8, 7, and 6 nodes. The left example yields a score of 5/8, corresponding to 5 of the landmark names being successfully grounded in the right order in 8 consecutive panoramas. The middle example yields a score of 5 by grounding landmarks in panoramas 2, 3, 4, 5, and 7, where the final score would be $5/7 = 0.71$. The right example demonstrates the perfect way of aligning all the panoramas to the corresponding landmarks without skipping, yielding the top score of $6/6 = 1$.

Alternatively, we introduce \textsc{Approach II} for path ranking by prompting GPT-4o to rate each path on a scale of 1 to 5 given the sequence of panoramas in order, original natural language instruction, and the extracted sequence of landmark phrases, as shown in Figure \ref{fig:gpt4o_full_prompting}. This approach bypasses the individual landmark grounding stage and alignment score computation done by \textsc{Pano2Land} algorithm. 
% although we believe its r
The performance of this approach is slightly worse than \textsc{Approach I}. Furthermore, the results are less interpretable since the internal ranking mechanism of GPT-4o is unknown. 

\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/GPT4o_Full_Prompting_Viz_Final.pdf}
  % \includegraphics[scale=0.44]{figures/GPT4o_Full_Prompting_Viz_Final.pdf}
  \includegraphics[width=\linewidth]{figures/GPT4o_Full_Prompting_Viz_Final.pdf}
  \caption{GPT-4o Full Prompting for Entire Sequence Scoring}
  \label{fig:gpt4o_full_prompting}
\end{figure*}





\begin{table*}[!h]
\centering
{%\footnotesize
\begin{tabular}{lc|c|c|cc|ccc}
\toprule
& \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textsc{Num}} & \multicolumn{1}{c}{\textsc{Hypo Path Gen}} & \multicolumn{2}{c}{\textsc{Approach \textbf{I}}} & \multicolumn{2}{c}{\textsc{Approach \textbf{II}}} \\
& & Episodes & Accuracy ($\%$) & nDTW ($\%$)  & Accuracy ($\%$)  & nDTW ($\%$) & Accuracy ($\%$) \\ \midrule
% Random Chance & - & - & - & - \\
% \rowcolor{verylightblue} 8WUmhLawc2A & 21 & 67.0 & 0.88 & 57.1 & 0.86 & 52.4 \\
\texttt{8WUmhLawc2A} & & 21 & 66.7 & 88.68$\pm$0.0 & 57.10$\pm$0.0 & 87.34$\pm$0.52 & 52.38$\pm$6.73 \\
\texttt{JeFG25nYj2p} & & 21 & 61.9 & 87.51$\pm$0.13 & 52.38$\pm$0.0 & 88.92$\pm$0.64 & 49.20$\pm$5.93\\
\texttt{mJXqzFtmKg4} & & 21 & 66.7 & 91.21$\pm$0.07 & 66.70$\pm$0.0 & 90.08$\pm$0.21 & 57.14$\pm$0.0 \\
\texttt{r1Q1Z4BcV1o} & & 21 & 57.1 & 87.96$\pm$0.30 & 49.20$\pm$2.24  & 88.69$\pm$0.42 & 39.68$\pm$2.24 \\
\texttt{sT4fr6TAbpF} & & 21 & 76.2 & 89.25$\pm$1.05 & 61.90$\pm$3.88   & 86.68$\pm$0.34 & 52.38$\pm$6.73\\
\midrule
\textbf{\texttt{Average}} & & 105 & \textbf{65.72}$\pm$6.33 & \textbf{88.92}$\pm$1.28 & \textbf{57.45}$\pm$6.31 & 88.34$\pm$1.20  & 50.15$\pm$5.81 \\
\bottomrule \\
\end{tabular}}
\caption{Full Pipeline Quantitative Results}
\label{tab:vln_final_results}
\end{table*}

Finally, for each approach's output, we compute the normalized dynamic-time warping (nDTW) metric between the ground truth and the best-aligned path to measure the path fidelity; nDTW is more aligned with our task objective compared to the Success Rate (SR), which only considers an episode to be successful if the agent's last position is within 3 meters of the ground-truth goal and it does not explicitly consider the intermediate alignments with the landmarks that were supposed to be visited in order by the agent~\cite{vlnpathfidelity}.


\begin{algorithm}
\small
\caption{- \textsc{Pano2Land} algorithm for calculating path alignment using grounding scores, similar to DP/LCS.}
\begin{algorithmic}[1]
\Require Binary grounding matrix $\mathbf{M} \in \{0,1\}^{R \times C}$
\Ensure Alignment score $S$

\State $R \gets$ number of rows (landmarks) in $\mathbf{M}$
\State $C \gets$ number of columns (panoramas) in $\mathbf{M}$
\State Initialize matrix $\mathbf{dp} \in {N}_0^{(R+1) \times (C+1)}$ with zeros

\For{$r \gets 1$ to $R+1$}
    \For{$c \gets 1$ to $C+1$}
        \If{$M_{r,c} == 1$}
            \State $\mathbf{dp}[r][c] \gets (\mathbf{dp}[r-1][c-1]) + 1$
        \Else
            \State $\mathbf{dp}[r][c] \gets \max\left(\mathbf{dp}[r-1][c],\; \mathbf{dp}[r][c-1] \right)$
        \EndIf
    \EndFor
\EndFor

\State $S \gets \mathbf{dp}[R][C]$
\State \Return $S$
\label{alg:algo}
\end{algorithmic}
\end{algorithm}





% \begin{algorithm}
% \small
% \caption{Alignment Computation using CTC Greedy Decoding for Probabilistic Grounding Scores}
% \begin{algorithmic}[1]
% \Require Alignment probabilities matrix $\mathbf{A} \in {R}^{(M+1) \times N}$
% \Ensure Alignment sequence $\mathbf{y}$

% \State $M \gets$ number of landmarks (excluding BLANK)
% \State $N \gets$ number of panoramas
% \State $\text{BLANK} \gets M$ 
% \State $\mathbf{y} \gets$ empty list
% \State $\text{prevLabel} \gets -1$

% \For{$i \gets 1$ to $N$}
%     \State $\mathbf{p}_i \gets \mathbf{A}_{:, i}$
%     \State $\mathcal{L}_i \gets \{ j \mid j \geq \text{prevLabel} \} \cup \{ \text{BLANK} \}$
%     \ForAll{$j \in \{0, 1, \dots, M\}$}
%         \If{$j \notin \mathcal{L}_i$}
%             \State $p_{j,i} \gets -\infty$
%         \EndIf
%     \EndFor
%     \State $\hat{y}_i \gets \arg\max_{j} \, p_{j,i}$
%     \If{$\hat{y}_i == \text{BLANK}$}
%         % \State Append $\text{BLANK}$ to $\mathbf{y}$
%         \State $\mathbf{y}$ += $\text{BLANK}$
%     \Else
%         % \If{$\hat{y}_i \neq \text{prevLabel}$}
%         %     % \State Append $\hat{y}_i$ to $\mathbf{y}$
%         %     \State $\mathbf{y}$ += $\hat{y}_i$
%         %     \State $\text{prevLabel} \gets \hat{y}_i$
%         % \Else
%         %     % \State Append $\text{BLANK}$ to $\mathbf{y}$
%         %     \State $\mathbf{y}$ += $\text{BLANK}$
%         % \EndIf

%        \State $\mathbf{y}$ += $\hat{y}_i$
%        \State $\text{prevLabel} \gets \hat{y}_i$
       
%     \EndIf
% \EndFor
% \State \Return $\mathbf{y}$
% \end{algorithmic}
% \end{algorithm}


In Table \ref{tab:vln_final_results}, 
% after the initial hypothesis filtering to a minimum of two and maximum of three paths, the ground-truth path, or a slightly modified one with a maximum of 2 nearby node variations, was among the hypothesis paths for 14 out of 21 episodes, where the 
\textsc{Hypo Path Gen} accuracy indicates the fraction of episodes where the ground-truth path or a highly similar one is among the selected path hypotheses. There might be multiple reasons why the correct path couldn't be retrieved, including but not limited to (1) not being able to ground the last landmark, (2) encountering a dramatically different landmark that has been part of the train samples, (3) a highly-frequent last landmark which exists in multiple locations (e.g., door) where the ground-truth landmark location may not fall into the top-3 retrieved ones, etc.

The \textsc{Approach I} nDTW shows the average nDTW score for all 21 episodes in each environment.
% where we empirically found out that if nDTW is above 0.87, the path is highly aligned with the ground-truth. Therefore, we've set 
If the nDTW is above 87\%, we consider the path successful, where this threshold is based on our empirical analysis. We applied the same evaluation metrics to  \textsc{Approach II} 
% results of GPT-4o prompting and reported the number accordingly. As 
Both numbers are higher in the \textsc{Approach I}. While in \textsc{Approach II} the full natural language instruction, including the action phrases (e.g. turn left) is given to GPT-4o, we hypothesize that the model has trouble grounding actions and/or landmarks and \textsc{Approach I} benefits from explicit decomposition of different stages. Since there were cases in which multiple top grounding scores or GPT-4o rating scores would exist, we repeated the process of picking the path with the highest score randomly up to 3 times and reported the mean and standard deviation.

% \begin{table*}[!h]
% \centering
% {\footnotesize
% \begin{tabular}{lc|c|c|cc|ccc}
% \toprule
% & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textsc{Num}} & \multicolumn{1}{c}{\textsc{Hypo Path Gen}} & \multicolumn{2}{c}{\textsc{Approach \textbf{I}}} & \multicolumn{2}{c}{\textsc{Approach \textbf{II}}} \\
% & & Episodes & Accuracy ($\%$) & nDTW ($\%$)  & Accuracy ($\%$)  & nDTW ($\%$) & Accuracy ($\%$) \\ \midrule
% % Random Chance & - & - & - & - \\
% % \rowcolor{verylightblue} 8WUmhLawc2A & 21 & 67.0 & 0.88 & 57.1 & 0.86 & 52.4 \\
% \texttt{8WUmhLawc2A} & & 21 & 66.7 & 88.68$\pm$0.0 & 57.10$\pm$0.0 & 87.34$\pm$0.52 & 52.38$\pm$6.73 \\
% \texttt{JeFG25nYj2p} & & 21 & 61.9 & 87.51$\pm$0.13 & 52.38$\pm$0.0 & 88.92$\pm$0.64 & 49.20$\pm$5.93\\
% \texttt{mJXqzFtmKg4} & & 21 & 66.7 & 91.21$\pm$0.07 & 66.70$\pm$0.0 & 90.08$\pm$0.21 & 57.14$\pm$0.0 \\
% \texttt{r1Q1Z4BcV1o} & & 21 & 57.1 & 87.96$\pm$0.30 & 49.20$\pm$2.24  & 88.69$\pm$0.42 & 39.68$\pm$2.24 \\
% \texttt{sT4fr6TAbpF} & & 21 & 76.2 & 89.25$\pm$1.05 & 61.90$\pm$3.88   & 86.68$\pm$0.34 & 52.38$\pm$6.73\\
% \midrule
% \textbf{\texttt{Average}} & & 105 & 65.72$\pm$6.33 & 88.78 & -- & 88.07  & -- \\
% \bottomrule \\
% \end{tabular}}
% \caption{Full Pipeline Quantitative Results}
% \label{tab:vln_final_results}
% \end{table*}


% \begin{table*}[!h]
% \centering
% {\footnotesize
% \begin{tabular}{lc|c|cc|ccc}
% \toprule
% & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\textsc{Hypo Path Gen}} & \multicolumn{2}{c}{\textsc{Approach \textbf{I}}} & \multicolumn{2}{c}{\textsc{Approach \textbf{II}}} \\
% & \# Episodes & Accuracy & nDTW  & Accuracy  & nDTW  & Accuracy  \\ \midrule
% % Random Chance & - & - & - & - \\
% % \rowcolor{verylightblue} 8WUmhLawc2A & 21 & 67.0 & 0.88 & 57.1 & 0.86 & 52.4 \\
% 8WUmhLawc2A & 21 & 67.0 & 0.88 & 57.1 & 0.86 & 52.4 \\
% JeFG25nYj2p & 21 & -- & 0.88 & 57.1 & 0.86 & 52.4 \\
% mJXqzFtmKg4 & 21 & -- & 0.88 & 57.1 & 0.86 & 52.4 \\
% R & 21 & -- & 0.88 & 57.1 & 0.86 & 52.4 \\
% S & 21 & -- & 0.88 & 57.1 & 0.86 & 52.4 \\
% \bottomrule \\
% \end{tabular}}
% \caption{Full Pipeline Quantitative Results}
% \label{tab:vln_final_results}
% \end{table*}

\section{Limitations}
There are specific limitations to our approach that we'd like to elaborate on. Firstly, our approach only works in the previously explored environments, given the topological map. Secondly, it only works in cases where the natural language instruction is landmarks-based and is not heavily based on spatial and temporal phrases, action phrases, and absolute metric distances. Since our pipeline is modular and not trained end-to-end, drawbacks of each module, especially the early stages of the LLM landmark extraction and VLM retrieval, propagate the errors to later stages of \textsc{Pano2Land} alignment or GPT-4o ranking. The quality of the path hypotheses eventually determines the upper bound on the ranking computed by GPT-4o or any other VLM being used.


\section{Conclusion}
In this work, we introduced a modular approach for the vision-and-language navigation (VLN) task based on the R2R-Matterport3D dataset \cite{r2r, chang2017matterport3d} within the Meta Habitat Simulator \cite{habitat19iccv, Yadav2022HabitatMatterport3S}. Our approach assumes that the agent has built a topological map in the exploration stage. We then use LLM to extract the sequence of landmarks the agent needs to visit, retrieve the top-k goal locations, and rank the path hypotheses to select the one with the highest alignment with the natural language instructions as the final answer. For the task, the approach demonstrates the superiority of the topological map with per-node panoramas to an open-vocabulary semantic occupancy map for land-mark grounding and goal retrieval. The overall performance on this benchmark is mainly affected by the zero-shot capabilities of VLM's to ground special landmark names in the panoramas. Future improvements can be 
achieved by fine-tuning the existing VLMs on navigation tasks and deploying the agent in previously unseen environments by seamlessly integrating the exploration and navigation part. 




% \subsection{Subsection Heading Here}
% Subsection text here.

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.


% \section{RSS citations}

% Please make sure to include \verb!natbib.sty! and to use the
% \verb!plainnat.bst! bibliography style. \verb!natbib! provides additional
% citation commands, most usefully \verb!\citet!. For example, rather than the
% awkward construction 

% {\small
% \begin{verbatim}
% \cite{kalman1960new} demonstrated...
% \end{verbatim}
% }

% \noindent
% rendered as ``\cite{kalman1960new} demonstrated...,''
% or the
% inconvenient 

% {\small
% \begin{verbatim}
% Kalman \cite{kalman1960new} 
% demonstrated...
% \end{verbatim}
% }

% \noindent
% rendered as 
% ``Kalman \cite{kalman1960new} demonstrated...'', 
% one can
% write 

% {\small
% \begin{verbatim}
% \citet{kalman1960new} demonstrated... 
% \end{verbatim}
% }
% \noindent
% which renders as ``\citet{kalman1960new} demonstrated...'' and is 
% both easy to write and much easier to read.
  
% \subsection{RSS Hyperlinks}

% This year, we would like to use the ability of PDF viewers to interpret
% hyperlinks, specifically to allow each reference in the bibliography to be a
% link to an online version of the reference. 
% As an example, if you were to cite ``Passive Dynamic Walking''
% \cite{McGeer01041990}, the entry in the bibtex would read:

% {\small
% \begin{verbatim}
% @article{McGeer01041990,
%   author = {McGeer, Tad}, 
%   title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
%   volume = {9}, 
%   number = {2}, 
%   pages = {62-82}, 
%   year = {1990}, 
%   doi = {10.1177/027836499000900206}, 
%   URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
%   eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
%   journal = {The International Journal of Robotics Research}
% }
% \end{verbatim}
% }
% \noindent
% and the entry in the compiled PDF would look like:

% \def\tmplabel#1{[#1]}

% \begin{enumerate}
% \item[\tmplabel{1}] Tad McGeer. \href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic
% Walking}. {\em The International Journal of Robotics Research}, 9(2):62--82,
% 1990.
% \end{enumerate}
% %
% where the title of the article is a link that takes you to the article on IJRR's website. 


% Linking cited articles will not always be possible, especially for
% older articles. There are also often several versions of papers
% online: authors are free to decide what to use as the link destination
% yet we strongly encourage to link to archival or publisher sites
% (such as IEEE Xplore or Sage Journals).  We encourage all authors to use this feature to
% the extent possible.

% \section{Conclusion} 
% \label{sec:conclusion}

% The conclusion goes here.

% \section*{Acknowledgments}

% %% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\newpage
\vspace{1cm}
% \hspace{1cm}\textbf{\Large Supplementary Material}
\appendix
\subsection{Environments Visualizations}
The bird's-eye-view visualizations of the last four environments, which are not shown in the main paper, are provided in Figures \ref{fig:env_4}, \ref{fig:env_3}, \ref{fig:env_5}, and \ref{fig:env_2}.


\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{figures/env_4.png}
  \caption{mJXqzFtmKg4}
  \label{fig:env_4}
\end{figure*}


\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{figures/env_3.png}
  \caption{r1Q1Z4BcV1o}
  \label{fig:env_3}
\end{figure*}


\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/env_5.png}
  \includegraphics[scale=0.3]{figures/env_5.png}
  \caption{JeFG25nYj2p}
  \label{fig:env_5}
\end{figure*}


\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/env_2.png}
  \includegraphics[scale=0.4]{figures/env_2.png}
  \caption{sT4fr6TAbpF}
  \label{fig:env_2}
\end{figure*}

\end{document}


