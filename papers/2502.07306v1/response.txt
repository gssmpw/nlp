\section{Related Work}
For the purpose of our exposition, the existing works on Vision Language Navigation can be partitioned into end-to-end and modular approaches. The end-to-end methods take the natural language instructions, visual observations, and actions and train a multi-modal sequence-to-sequence model, and in the inference stage, given the instruction and initial view, the model generates the sequence of actions while ingesting additional views.  
The modular approaches integrate LLMs, VLMs, or both with more traditional map-based representations along with a common robotics navigation stack comprised of basic navigation skills that are not learned.  \\
\noindent
\textbf{End-to-end approaches.} These methods typically adopt a sequence-to-sequence model, taking as an input the language instruction and visual information and outputs the sequence of low-level navigation actions (move, turn left/right) or local waypoints. During the forward pass, the entire instruction is processed by the Language Encoder (e.g., LSTM/transformer). The aggregation of the context vectors, plus the encoded current view of the agent, is then fed to the Action Decoder (e.g., LSTM/transformer) that generates the next \texttt{action}. The decoder continues to predict actions until it generates the \texttt{STOP} action. 
% \begin{figure}[!h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/PrimitiveVLN_Proposed_Figure_v2_Final.pdf}
%   \caption{Bird's Eye View visualization of a sample VLN episode from R2R dataset}
%   \label{fig:vlnsampleepisode}
% \end{figure}
The mixture of Reinforcement Learning (RL) and Imitation Learning (IL) has been commonly used for training these 
models **Andry, "Learning to Navigates with Instructional Descriptions"**__**Chen et al., "Reinforcement Learning for Vision-Language Navigation"**.
% The evaluation metrics for VLN tasks are success rate (SR) which counts the number of episodes where the agent stops within the 3-meter radius of the goal; trajectory length (TL) in meters; success rate weighted by the path length (SPL); navigation error as the distance between the goal and the agent's stop location (NE); and normalized dynamic-time warping (nDTW) for computing the similarity between the agent's path and the ground-truth path.
The existing approaches proposed different variations of model architectures, training strategies and choice of representations **Chen et al., "Reinforced Cross-Modal Embeddings for Vision-Language Navigation"**__**Wang et al., "Vision-Language Navigation with Reinforcement Learning"** typically using the Room-to-Room (R2R) **Wu et al., "Room-To-Room Transition Prediction for Indoor Scenes"**__**Chen et al., "Reinforced Cross-Modal Embeddings for Vision-Language Navigation"** and Room-Across-Room (RxR) **Chen et al., "Learning to Navigate Large Rooms with Visual-Linguistic Representations"** benchmarks for training and evaluation. The natural language instructions in these benchmarks are quite complex, with an average length of $\sim$ 26 words. 
% and is usually fed to the model in the beginning.
These approaches have made substantial improvements in past years, mostly thanks to increasing the number of training episodes and auxiliary tasks that support grounding **Chen et al., "Learning to Ground Visual-Linguistic Representations"** and instruction generation **Wang et al., "Vision-Language Navigation with Instruction Generation"**. It has been shown **Andry et al., "Analyzing the Performance of Vision-Language Navigation Models"** that the performance of the existing methods continues to be severely compromised by the inability to ground landmarks, understand spatial relationships, as well as grounding of action phrases. The ability to ground landmarks is more critical for indoor environments, while in outdoor settings, the grounding of actions in navigation instructions is more critical.
Furthermore, RL \& IL require a large number of high-quality training episodes, in addition to the extra computational complexity of RL due to the online interaction of the agent with the simulator/environment that makes it more difficult to scale the training **Chen et al., "Scaling Vision-Language Navigation Models"**.\\
%
% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \includegraphics[scale=0.4]{figures/VLN_Pipeline_Sequence_Diagram_last.pdf}
%   \caption{Pipeline Overview}
%   \label{fig:vlnpipelineoverview}
% \end{figure*}
\begin{figure*}[!h]
  \centering
  % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
  \caption{Topological Map Construction}
  \label{fig:vlntopo}
\end{figure*}
%
\noindent
\textbf{LLM and VLM based modular approaches.} Language Models were used in the past as zero-shot planners, where **Chen et al., "Zero-Shot Planning with Language Models"** introduced the idea of utilizing the knowledge learned by LLMs, like OpenAI GPT-3 **Brown et al., "Language Models are Few-Shot Learners"** and Codex **Chen et al., "Codex: A Large-Scale Language Model"**, for decomposing high-level tasks (e.g. "make breakfast") to sequences of lower level skills executable by the agent. For navigation tasks, CLIP-Nav **Andry et al., "CLIP-Nav: A Visual-Linguistic Navigation System"** utilized CLIP VLMs **Radford et al., "Learning Transferable Visual Models"** for grounding instruction phrases and GPT-3 **Brown et al., "Language Models are Few-Shot Learners"** for decomposition of complex natural language instructions into phrases. In CLIP-Nav, the language instruction is decomposed using GPT-3 **Brown et al., "Language Models are Few-Shot Learners"**, and then each sub-instruction, along with a panorama comprised of four egocentric views, is ranked by CLIP **Radford et al., "Learning Transferable Visual Models"** to determine the closest heading direction. The major limitations of CLIP-Nav are the dependency on the existence of a navigable graph of the environment and the poor ability of CLIP to associate landmarks with images. 
Another decomposition of the navigation task was adopted by the VLMaps **Andry et al., "VLMaps: A Visual-Linguistic Map-Based Navigation System"** approach, which first builds a global joint vision-language semantic occupancy map by exploring the environment.
% and generating all the views (using RGBD sensory information collected). 
The cells of the map are populated by LSeg/CLIP embeddings **Andry et al., "LSeg: A Language-Segmentation Model"**_, projected onto the grid from images. 
The navigation instructions are simpler, often resorting to point and object goal navigation, which are further translated into robotic navigation skills in the form of executable code. 

\noindent Lang2LTL **Chen et al., "Lang2LTL: A Language Model for Translating Natural Language Instructions"** represents another line of work that has been proposed to use LLMs to translate free-form natural language instructions into linear temporal logic (LTL). Lang2LTL is advantageous because it disambiguates the goal specification and facilitates incorporating temporal constraints. The limitations of Lang2LTL are the need for a parallel dataset of natural language instructions and their corresponding fixed set of LTL formulas for fine-tuning the LLMs for the translation stage and the limited level of complexity of the instructions, compared to R2R **Wu et al., "Room-To-Room Transition Prediction for Indoor Scenes"** and RxR **Chen et al., "Learning to Navigate Large Rooms with Visual-Linguistic Representations"** benchmarks. Authors in LM-Nav **Andry et al., "LM-Nav: A Zero-Shot Outdoor Navigation System"** propose a zero-shot approach for outdoor instruction following. They utilize a visual navigation system called ViNG **Brown et al., "ViNG: A Visual Navigation System"**, to construct a topological map $G$ from a set of observations, followed by extraction of landmarks $L$ from the free-form navigation instruction using GPT-3. 
CLIP is then used to infer a joint probability distribution over the nodes in $G$ and landmarks in $L$, followed by a graph search algorithm to find the optimal path that is executed by local navigation policy. The approach in LM-Nav can only navigate to a sequence of unique landmarks by design, discarding complexities like spatial clauses and fine-grained grounding
of landmark and action phrases. 

% \begin{figure*}[!h]
%   \centering
%   % \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   % \includegraphics[scale=0.2]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \includegraphics[width=\linewidth]{figures/VLN_Pipeline_PreExploreTopo_last.pdf}
%   \caption{Topological Map Construction}
%   \label{fig:vlntopo}
% \end{figure*}