\input{fig/fig-taxonomy}


\section{LLM Methods for Protein Engineering, Generation and Translation}
\label{sec:llm_generation}


Protein engineering and generation aims to design protein sequences with desired attributes (e.g. structures and properties). Given the desired attributes \(T\) and reference protein sequence \(\mathcal{S}\) (optional), the model is expected to output a protein sequence \(\mathcal{S}'\) with desired attributes. Key tasks include:

\noindent \textit{I. Protein Engineering:} 
\( f_\theta: (\mathcal{S}, T) \rightarrow \mathcal{S}' \) modifies protein \(\mathcal{S}\) toward the desired attributes \(T\), yielding the engineered protein \(\mathcal{S}'\).

\noindent \textit{II. Protein Generation:} 
\( f_\theta: (T, R) \rightarrow \mathcal{P} \) generates proteins with attributes \(T\) by sampling from the protein space using random seeds \(R\).



\noindent \textit{III. Protein Translation:} \( f_\theta: (\mathcal{P},T) \rightarrow \mathcal{P'} \) translates a protein \( \mathcal{P} \) into an alternative representation \( \mathcal{P}' \)  based on the target translation parameters \( T \). 



\subsection{Protein Engineering Models}

ProteinDT~\cite{liu2023text} is a multimodal protein design framework that robustly integrates textual protein knowledge with sequence-based generative modeling. ProteinDT employs contrastive alignment and a facilitator module, enabling zero-shot text-to-protein generation and editing. %For editing tasks, it takes an initial protein sequence and a descriptive text prompt as inputs, and outputs a modified protein sequence that aligns with the target property without any task-specific training. 
Meanwhile, PLMeAE~\cite{plmeae} is a closed-loop protein engineering framework that integrates protein language models with an automated biofoundry within a Design-Build-Test-Learn cycle.
Furthermore, Toursynbio~\cite{shen2024toursynbio} introduces an agent that is capable of facilitating the modification and engineering of wet lab proteins.


\subsection{Protein Generation Models}

Protein generation models are designed to create novel protein sequences for specific engineering applications, often leveraging large-scale datasets of existing proteins with known amino acid sequences and properties. These models typically employ decoder-based architectures to generate functional protein sequences conditioned on various biological annotations. For example, ProGen~\cite{madani2023large} is a GPT-based generative protein engineering model that treats protein engineering as an unsupervised sequence generation process, and generates functional protein sequences conditioned on annotations like molecular function or taxonomy. The model is trained on diverse, non-redundant protein sequences from databases such as UniProt and Pfam, utilizing associated tags for conditional generation. ProtGPT2~\cite{ferruz2022protgpt2} is another model that generates de novo protein sequences with natural amino acid compositions using autoregressive modeling. In particular, they noticed that the generated sequences could explore a few uncharted areas of the protein sequence space. 
ProGen2~\cite{nijkamp2023progen2} is an extended version of ProGen, featuring a larger model size and a more extensive training dataset to enhance sequence diversity.  
Notably, ProGen2 can predict protein fitness without requiring additional fine-tuning. Recently, ProLLaMA~\citep{lv2024prollama} proposed a multi-task protein language model to handle both protein sequence generation and protein understanding tasks. Built on LLaMA2, ProLLaMA introduces a two-stage training framework: (1) continued pre-training on protein sequences, and (2) instruction tuning with a 13-million-sample dataset for multitasking capabilities.

Beyond conventional decoder-based approaches, Ankh~\citep{elnaggar2023ankhoptimizedproteinlanguage} employs an encoder-decoder architecture that optimizes efficiency by reducing parameters while maintaining high-quality protein generation. PAAG~\citep{yuan2024annotation} is another encoder-decoder architecture which focuses on the alignment between textual annotations and protein sequences at multiple levels before generating new sequences.  
Pinal~\citep{dai2024toward} does not directly generate protein sequences from text. Instead, it first constrains the protein design space by generating structure tokens, then predicts sequences based on those constraints to improve foldability and function alignment.


While many of these models are designed for general protein generation, some focus on specialized applications such as antibody design. IgLM~\cite{shuai2023iglm} employs autoregressive sequence generation conditioned on an antibody's sequence chain type and species of origin. %A key feature is its ability to generate infilled residue spans located at indicated positions within the entire antibody sequence. 
As a further step, PALM-H3~\cite{he2024novo} specifically targets SARS-CoV-2 antibody generation, highlighting how protein generation language models can be tailored for highly specific protein design tasks.


\subsection{Protein Translation Models}


Protein translation models are specifically developed to handle tasks that require translating between different protein representations, which could be helpful in protein design. 

ProstT5~\cite{heinzinger2023bilingual} addresses the task of simultaneously modeling the dual nature of proteins — their linear one-dimensional (1D) sequences and three-dimensional (3D) structures — using a bilingual language model based on T5~\cite{raffel2020exploring} and ProtT5~\cite{pokharel2022improving}. It extracts features and patterns from both the sequence and the structure data %, enabling improved protein design, remote homology detection, and structure-sequence translation. 
Fold2Seq~\cite{cao2021fold2seq} is another model that learns structure-sequence relationships of proteins. %Using a multi-step training framework that involves fold-to-sequence reconstruction, fold classification, and cross-domain losses, 
The model could guide designs of protein sequences conditioned on desired structural folds. Recently, ProtAgents \cite{ghafarollahi2024protagents}, a multiagent framework, has been proposed to handle 1D sequence generation and 3D fold generation simultaneously. LM-DESIGN~\citep{10.5555/3618408.3620189} is a method for reprogramming protein language models (pLMs) to design protein sequences for given structural folds.

