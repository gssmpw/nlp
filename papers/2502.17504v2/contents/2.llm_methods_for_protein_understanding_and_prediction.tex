\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{fig/Tasks.pdf}
    \caption{An Overview of Tasks in Protein Large Language Models.}
    \label{fig:methods}
\end{figure*}


\section{LLM Methods for Protein Understanding and Prediction}


\label{sec:llm_understanding}

\subsection{Problem Definition}

A protein, composed of amino acids (residues), can be represented as a sequence \( [x_1, \dots, x_L] \) in the residue token space \( \mathcal{P} \), where \(L\) denotes its length. According to Anfinsen's dogma, a protein’s primary sequence determines its structure and function. General problems in protein understanding and prediction are as follows:


\noindent \textit{I. Sequence-to-Property Prediction:} \( f_\theta: \mathcal{P} \rightarrow \mathcal{R}^+ \) mapping sequences to numerical properties, such as stability or fluorescence intensity.

\noindent \textit{II. Sequence-to-Label Prediction:} \( f_\theta: \mathcal{P} \rightarrow \mathcal{L} \) mapping sequences to categorical labels, including secondary structure types, contact maps, or functional annotations.

\noindent \textit{III. Sequence-to-Structure Prediction} \( f_\theta: \mathcal{P} \rightarrow \mathcal{S} \) mapping sequences to the 3D folding structures (i.e. tertiary structures). 

\noindent \textit{IV. Sequence-to-Text Understanding:} \( f_\theta: \mathcal{P} \rightarrow \mathcal{T} \), where \( \mathcal{T} \) represents generated textual descriptions of protein sequences.


\subsection{Protein Sequence Models}


\noindent \textbf{Individual Protein Sequences Models.}
Protein language models process amino acid sequences into meaningful representations for downstream tasks including structure and function prediction. Like NLP models, they are usually first pretrained on large sequence datasets with masked language modeling (MLM) objective; and then the protein sequences' embeddings are adapted for downstream tasks.
Initially, researchers leveraged long short-term memory (LSTM) architectures to learn representation of proteins \citep{alley2019unified,bepler2019learning,zhou2020mutation}. 
Following the breakthrough of transformer architectures \citep{vaswani2017attention} in NLP, transformer-based protein language models emerged as the new paradigm. Large-scale transformer models, scaling up to billions of parameters and trained on millions of protein sequences, have demonstrated remarkable effectiveness for protein understanding and prediction tasks \citep{rao2019evaluating,elnaggar2021prottrans, xiao2021modeling,hu2022exploring}, and 3D structure folding \citep{chowdhury2022single,fang2022helixfold,chen2024xtrimopglm}.
The interpretability of these \proteinllms has also been explored, with \citep{vig2020bertology} analyzing learned representations through the lens of attention.
Beyond general-purpose protein language models, several works have focused on domain-specific applications. For instance, \citet{hie2021learning} applied BiLSTM to model viral escape patterns; TCR-BERT \citep{wu2024tcr} specialized in T-cell receptor (TCR) analysis for improved TCR-antigen binding prediction; PeptideBERT \citep{guntuboina2023peptidebert} focused on predicting key properties of peptides; \citet{kroll2023turnover,yu2023enzyme} adapted ESM-1b for enzymatic function prediction.
 

\noindent \textbf{Multiple Sequence Alignments (MSA) Models.} MSA aligns homologous proteins within sequence space by mapping their residues to the coordinate framework of a designated seed sequence. MSA reveals evolutionary relationships between proteins and thus serves as a cornerstone of computational biology, particularly for mutation effects prediction \citep{ram2022few,hawkins2021msa}. The MSA Transformer \citep{rao2021msa} processed MSAs instead of single sequences. It used a modified axial attention mechanism \citep{ho2019axial,child2019generating} to model both intra- and inter-sequence relationships. In contrast, Tranception \citep{notin2022tranception}, was trained on individual non-aligned sequences but could leverage aligned sequences during inference. It extracted patterns from contiguous protein subsequences and improves fitness prediction by integrating MSAs retrieved at inference time. In specific subdomains, \citet{lin2023deep} developed a transfer learning framework that utilized ESM-MSA-1b for transmembrane protein complexes. Additionally, vcMSA~\citep{mcwhite2023leveraging} and Poet~\citep{truong2023poet} leveraged protein LLMs to identify MSAs or homologous sequences.

\noindent \textbf{Evolutionary Scale Modeling (ESM) Series.} ESM is a family of transformer models for protein modeling. 
ESM-1b~\cite{rives2021biological}, the first model in the series with up to 669.2 million parameters, was trained on 250 million protein sequences using a masked language modeling (MLM) objective and contains up to 669.2 million parameters. 
Building on this,  ESM-1v~\citep{meier2021language} focused on predicting the effects of mutations in zero-shot setting, while incorporating the MSA Transformer \citep{rao2021msa} for few-shot mutation prediction. 
Thanks to the success of AlphaFold2 \citep{jumper2021highly}, ESM-IF \citep{hsu2022learning} utilized predicted structures to train large models combining Geometric Vector Perceptron \citep{jing2020learning} with GNN or transformer on the inverse folding task that predicts protein strings from the 3D structures. The new general-purpose language protein model ESM-2 \citep{lin2023evolutionary} further scaled up the model size to 15 billion parameters and incorporated a folding head to create an end-to-end single-sequence structure prediction model ESMFold. The latest model ESM-3~\citep{hayes2025simulating} is a multimodal generative model with 98 billion parameters that could reason over protein sequences, structures, and functions. Using a chain-of-thought approach, it successfully designed a novel fluorescent protein far from any known fluorescent proteins.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{fig/Methods.pdf}
    \caption{An Overview of Methods of Protein Large Language Models.}
    \label{fig:methods}
\end{figure*}

\subsection{Structure-Integrated and Knowledge-Enhanced Models}

Beyond residue sequences, many models integrate additional information, such as structure data or external knowledge, to enhance protein understanding and prediction ability.

\noindent \textbf{Structure-Integrated Models}: Structural information plays an important role in protein understanding, as a protein's functions are determined by its structures. Therefore, many works have incorporated structural information to enhance protein modeling ability. 
Some works utilized structure information as additional inputs \citep{chen2024endowing,tan2024simple}. For instance, \citet{zhang2023systematic} fused global structure information captured by structure encoder (GVP, GearNet \cite{zhang2022protein}, or CDConv \citep{fan2022continuous}) into representations of ESM-2; SaProt \citep{su2023saprot} incorporated local structural information for each amino acid, derived from Foldseek \citep{van2022foldseek}, to generate structure-aware tokens.
Alternatively, other works injected the structure information only in the training stage by either additional training tasks \citet{wang2022multi,sun2024structure,zhang2024structure} or contrastive learning \citep{wang2025s}.
Some studies have also leveraged pretrained protein language models to improve structure models~\citep{wu2023integration, zheng2024ccpl}.


\noindent \textbf{Knowledge-Enhanced Models}: Beyond large protein sequence datasets, information in other formats can further enhance a model’s understanding of proteins in the training stage. 
OntoProtein \citep{zhang2022ontoprotein} and KeAP \citep{zhou2023protein} incorporated knowledge graphs data during training by additional MLM objectives and/or contrastive learning to inject factual biological knowledge into the pre-trained \proteinllms.
ProteinBERT \citep{brandes2022proteinbert} performed dual-task learning during pretraining to learn both protein sequence modeling and Gene Ontology (GO) annotation prediction. It utilized a specialized BERT architecture with parallel input pathways for sequences and annotations.
To leverage the rich information in textual descriptions or other modalities, ProteinCLIP \citep{wu2024proteinclip} and MolBind \citep{xiao2024molbind} applied contrastive learning between protein sequences and textual descriptions and/or molecular to learn improved embeddings.


\subsection{Protein Description and Annotation Models}


The previously mentioned models have primarily focused on learning protein representations and utilizing them for classification, regression, or 3D structure folding tasks. To enhance expressiveness and understanding, more recent models have been trained on both protein sequences and textual data, allowing them to integrate NLP capabilities with protein representation learning \citep{wang2023instructprotein, liu2024prott3, zhuo2024protllm,jin2024prollm}.
\citet{xu2022protranslator} proposed ProTranslator, a bilingual translation framework between protein sequences and GO functions with textual descriptions. ProTranslator encoded and aligned the textual definitions of GO functions and protein sequences within the same low-dimensional space, facilitating the annotation of novel GO functions and the generation of textual descriptions for proteins. BioTranslator \citep{xu2023multilingual} further improved ProTranslator by extending the bilingual framework to a multilingual translation framework, embedding text and multiple biomedical modalities into a shared space.
ProtST~\cite{xu2023protst} was a framework designed to jointly learn from protein sequences and their associated biomedical text descriptions. It integrated protein language models (e.g., ESM or ProtBERT) with biomedical language models (e.g., PubMedBERT) to fuse sequence and text information through pre-training tasks. Prot2Text~\citep{abdine2024prot2text} combined ESM-2 with a structure encoder (RGCN) and extended function prediction from categorical classification to free-text descriptions. BioT5 and BioT5+~\citep{pei2023biot5,pei2024biot5+} further unified molecular information within a more comprehensive training framework.

There have also been several interactive LLMs for protein understanding. These models enhanced pretrained LLMs with protein comprehension by integrating a protein processing module~\citep{wu2024structure, wang2024protchatgpt,wang2024long}. For instance, ProteinChat~\citep{guo2023proteinchat} allowed users to input protein structures and query them using texts. ProteinGPT~\citep{xiao2024proteingpt} extended this capability by supporting both protein sequences and structures as inputs. In these models, protein data were processed through \proteinllms to generate embeddings, which were then projected to the natural language embedding space. The backbone LLMs integrated these adapted embeddings with user’s queries to produce meaningful answers.