\section{Conclusion and Future Work}
\label{sec:conclusion}




This survey provides a comprehensive overview of Protein Large Language Models, highlighting their architectures, datasets, evaluation, and applications. These works represent significant advancements in protein science and offer innovative approaches to protein analysis and design. In addition to these advancements, several challenges remain to be solved in the future.

\smallskip \noindent \textbf{Protein Dynamics.} AlphaFold~\cite{jumper2021highly} has been shown to provide accurate static 3D structures. However, proteins are naturally dynamic molecules with various conformations~\cite{ohnuki2024integration}. Although several works incorporate 3D structures into LLMs, the conformational dynamics of proteins have not yet been considered. Since conformational dynamics are highly related to the transporter functions of proteins, it would benefit the model to include protein dynamics.

\smallskip \noindent \textbf{Combination with Single-cell Data.} Recently, single-cell proteomics sequencing technology~\cite{li2024scprotein,liu2024geneverse,bennett2023single} has attracted extensive attention in the field of biology, which can help us understand the pathways in specific cells. Since LLMs have shown effectiveness in understanding both proteins and single-cell data, they can be extended to learn from single-cell proteomics data in the future.


\smallskip \noindent \textbf{Towards Biological Applications.} Although several biological applications have been studied in recent works, a range of detailed and complex problems remain unsolved, including protein-ligand interaction learning~\cite{koh2024physicochemical}, cryptic pocket identification~\cite{ge2024exploring}, and rational ligand generation~\cite{li2024deep}. These applications require extensive and diverse domain knowledge of proteins and their related fields. We believe LLMs have the potential to incorporate and utilize more domain knowledge to solve these problems.


\smallskip \noindent \textbf{Interpretability.} In addition to effectiveness, interpretability is also of strong significance for trustworthy models~\cite{huang2024trustllm}. Previous language models for proteins~\cite{gu2023hierarchical,vecchietti2024recent} have provided extensive case studies, such as key residue analysis, which could be challenging for large-scale and closed-source models. To improve interpretability, InterPLM~\cite{simon2024interplm} employs sparse autoencoders to extract biologically meaningful features from \proteinllms, revealing their alignment with known biological concepts. Inspired by this, we should design prompts to enhance the interpretability of \proteinllms for reliable outputs.

\newpage
\section*{Limitations}

This survey primarily focuses on \proteinllms. We acknowledge that the study of protein interactions with other molecules (e.g., DNA, RNA) in the inter-molecular domain is a broad and valuable field worth reviewing. Given its vast scope, we do not extensively cover it in this survey, and instead focus on \proteinllms centered on proteins themselves. In the future, we may either expand our review to include these areas or write a separate survey specifically dedicated to this domain, providing more comprehensive coverage for researchers.
