\section{Introduction}

``\textit{Proteins are the machinery of life, and understanding their language unlocks the secrets of biology.}''
\rightline{--- David Baker (Nobel Prize laureate 2024)}
\\

Proteins are essential biological molecules, driving functions such as catalyzing biochemical reactions, maintaining cell structure, and enabling cellular communication. Understanding their sequence-structure-function relationships is central to biological research. However, traditional experimental methods, including X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy, are time-consuming and labor-intensive, posing bottlenecks for large-scale applications.

Recent advancements in language modeling have revolutionized computational biology, offering powerful tools for protein analysis. Protein large language models (\textbf{\proteinllms}) share several foundational similarities with LLMs: 1) \emph{Training objectives and learning paradigms}, both LLMs and \proteinllms are trained in a self-supervised manner on large-scale datasets using objectives such as masked language modeling~\cite{devlin2019bert},  auto-regressive modeling~\cite{luo2022biogpt}, or sentence permutation~\cite{lewis2019bart, yuan2022biobart}, learning to predict missing or next elements in sequences from the vocabulary. While LLMs predict missing words or phrases within textual data~\cite{reimers2019sentence, liu2019roberta, touvron2023llama}, \proteinllms predict amino acids or subsequences within protein sequences. 2) \emph{Pretraining data.} \proteinllms adopt a data-driven paradigm to learn directly from large-scale protein datasets~\cite{liu2024timemattersexaminetemporal, jones2024examiningimbalanceeffectsperformance}. The datasets for training \proteinllms consist of vast collections of protein sequences, analogous to the textual corpora used for LLMs. This eliminates the need for explicit feature engineering, allowing \proteinllms to learn intricate patterns, such as structural motifs, evolutionary relationships, and functional insights, similar to how LLMs capture semantic and syntactic structures in language.

This paradigm shift has led to the emergence of highly effective models that can predict protein folding, annotate biological functions, and even design novel proteins with desired characteristics. Beyond their predictive capabilities, \proteinllms also provide interactive interfaces that allow users to upload protein sequences or structural files (e.g., PDB format), pose questions, and interact with the model in a conversational manner~\cite{liu2024prott3,xiao2024proteingpt}, proving deeper insights into protein structure, function, and design.

We present the first dedicated survey of \proteinllms, analyzing their unique architectures, training methodologies, and practical applications in protein research. While previous studies have explored the applications of various computational methods for protein research~\cite{xinhui2024generative, wu2022survey} or discussed the role of language models in general scientific domains such as biomedicine~\cite{wang2023pre} and chemistry~\cite{liao2024words}, this survey focuses specifically on \proteinllms--a rapidly evolving area at the intersection of computational biology and NLP.  

The key contributions are as follows:


\begin{itemize}[leftmargin=1em, noitemsep, topsep=0pt]
    \item \textbf{Architectural Overview.} A structured taxonomy of state-of-the-art \proteinllms (Figure \ref{fig:taxonomy}) detailing their unique architectures for protein understanding (\S\ref{sec:llm_understanding}) and generation (\S\ref{sec:llm_generation}), highlighting how these models surpass traditional experimental methods in both efficiency and accuracy (Appendix \S\ref{sec:experiment}). 
    \item \textbf{Data Insights.} A comprehensive summary of datasets for pretraining, fine-tuning, and benchmarking \proteinllms, providing critical insights into data curation strategies and their impact on model performance (\S\ref{sec:dataset}).
    \item \textbf{Evaluation Protocols.} A thorough discussion of methodologies for assessing the performance and impact of \proteinllms, including comprehensive new benchmarking strategies (\S\ref{sec:eval} and Appendix \S\ref{sec:app_eval}).
    \item \textbf{Applications.} A detailed exploration of practical applications in protein prediction, annotation, and design, remarkably highlighting recent innovative advancements and showcasing the transformative potential of \proteinllms in advancing biomedical research.
\end{itemize}
