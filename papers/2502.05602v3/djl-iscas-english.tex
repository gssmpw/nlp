\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[a4paper, total={184mm,239mm}]{geometry}
\usepackage{float}
\usepackage[colorlinks]{hyperref}

\usepackage{colortbl}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\placeh{UbiMoE}
\setlength{\textfloatsep}{5pt}
\begin{document}

\title{UbiMoE: A Ubiquitous Mixture-of-Experts Vision Transformer Accelerator With Hybrid Computation Pattern on FPGA}
\author{\IEEEauthorblockN{Jiale Dong, Wenqi Lou$^{\dagger}$, Zhendong Zheng, Yunji Qin, Lei Gong, Chao Wang$^{\dagger}$, Xuehai Zhou}
\IEEEauthorblockA{University of Science and Technology of China, Hefei, China \\
Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China \\
\{louwenqi, cswang\}@ustc.edu.cn }
\vspace{-3em}
}

\maketitle

\begin{abstract}
Compared to traditional Vision Transformers (ViT), Mixture-of-Experts Vision Transformers (MoE-ViT) are introduced to scale model size without a proportional increase in computational complexity, making them a new research focus. 
Given the high performance and reconfigurability, FPGA-based accelerators for MoE-ViT emerge, delivering substantial gains over general-purpose processors. However, existing accelerators often fall short of fully exploring the design space, leading to suboptimal trade-offs between resource utilization and performance.
To overcome this problem, we introduce \placeh, a novel \textit{end-to-end} FPGA accelerator tailored for MoE-ViT. Leveraging the unique computational and memory access patterns of MoE-ViTs, we develop a latency-optimized streaming attention kernel and a resource-efficient reusable linear kernel, effectively balancing performance and resource consumption. To further enhance design efficiency, we propose a two-stage heuristic search algorithm that optimally tunes hardware parameters for various FPGA resource constraints.
Compared to state-of-the-art (SOTA) FPGA designs, \placeh~achieves 1.34× and 3.35× throughput improvements for MoE-ViT on Xilinx ZCU102 and Alveo U280 platforms, respectively, while enhancing energy efficiency by 1.75× and 1.54×. Our implementation is available at https://github.com/DJ000011/UbiMoE.
% Due to FPGA's high performance and reconfigurability, the FPGA-based MoE-ViT accelerator has been proposed and demonstrates superior performance over general-purpose processors. However, the design space of accelerators in previous works has not been fully explored, making it challenging to achieve an optimal balance between resource usage and performance. 
% Moreover, FPGA's ever-expanding resources and diversity further complicate this issue.
% To overcome this problem, we present a novel \textit{end-to-end} FPGA accelerator for MoE-ViT. First, based on MoE-ViT's computational and memory access characteristics, we design a latency-oriented streaming attention kernel and a resource-oriented reusable linear kernel, striking a balance between performance and resource utilization. Additionally, we propose an efficient two-stage search algorithm to find optimized solutions under different FPGA resource constraints.
% Compared to state-of-the-art (SOTA) designs, \placeh~achieves 1.76× and 3.35× throughput improvements for MoE-ViT on Xilinx ZCU102 and Alveo U280 platforms, respectively, and improves energy efficiency by 1.74× and 1.54×. The code will be open-sourced at Github.
% Compared to the traditional Vision Transformer (ViT), Mixture-of-Experts ViT (MoE ViT) significantly boosts model parameters without a corresponding increase in computational complexity, making it a new focal point for research. The previous work, Edge-MoE, explored how to deploy it on resource-constrained FPGAs. However, because the design relies heavily on reusable modules and off-chip storage, directly deploying it to modern FPGA devices with more on-chip resources could lead to unnecessary memory access and inefficient resource utilization, ultimately reducing portability. To this end, this paper presents~\placeh, a flexible and efficient \textit{end-to-end} FPGA accelerator for Mixture of Experts ViT. At the module level, we used parameterized streaming attention kernels and reusable linear kernels optimized for memory access and computation to achieve portability across different platforms. Also, a two-stage search algorithm is introduced to facilitate parameter exploration across various design scenarios. 
% Compared to state-of-the-art mtl vit work $\text{M}^3$ViT, \placeh~improves throughput by 1.76x and 3.35x on ZCU102 and U280, respectively, and enhances energy efficiency by 1.74x and 1.54x. Additionally, our design also achieves better efficiency than previous ViT accelerators.
%Additionally, For multi-die platforms, we present a highly parallel deployment strategy that operates across multiple SLRs.  
\end{abstract}

%\begin{IEEEkeywords}
 
%\end{IEEEkeywords}

\section{Introduction}
The Vision Transformer (ViT) has garnered widespread attention for its excellent performance in computer vision tasks~\cite{dosovitskiy2020image,chen2022dearkd,yun2024shvit}. Building on the Mixture-of-Experts (MoE) architecture, the MoE-ViT extends ViT by scaling model size without a corresponding increase in computational complexity, achieving improved multitasking capabilities and becoming a new research focus~\cite{riquelme2021scaling,aaai22_moevit,chen2023adamv,liu2021swin,iscas24_swat,kim2024monde}.

Although MoE-ViT and ViT share similar computational paradigms, numerous expert layers and sparse activation in MoE-ViT significantly increase memory access requirements and computational complexity. To address this, $\text{M}^3$ViT~\cite{fan2022m3vit} proposes an expert-by-expert computation mode, effectively reducing memory demands and achieving SOTA performance. However, traditional ViT accelerators~\cite{dong2023heatvit,ye2023accelerating,glvlsi24_qin,liu2023efficientvit}, which operate in a patch-by-patch manner, fail to benefit from this optimization due to the frequent swapping of expert weights.
To address this challenge, Edge-MoE~\cite{sarkar2023edge} developed a specialized accelerator for $\text{M}^3$ViT, optimizing memory access in the expert-by-expert computation mode and achieving leading performance on the ZCU102 platform. However, it focuses on resource-constrained optimization for embedded platforms limits comprehensive hardware design space exploration, struggling to balance performance and resource utilization effectively.
Specifically: 1) the hardware design only emphasizes reusable computational kernels, overlooking latency optimization for critical bottlenecks~\cite{tc_octcnn,asplos23_flat}; and 2) the deployment strategy lacks efficient hardware design space exploration, limiting the ability to find optimal solutions across different FPGA platforms~\cite{ICCAD20_exp,hwang2024pre}.
As FPGA platforms continue to evolve and offer richer resources (\textit{e.g.}, Alveo U250/U280 with multi-chip architectures), efficiently utilizing computational resources becomes crucial. 
% To address this, we propose \textbf{\placeh}, a highly efficient accelerator design for MoE-ViT. It integrates highly optimized attention and linear kernels and employs heuristic algorithms to optimize hardware design parameters, achieving globally optimal solutions across different FPGA resources. 
To this end, we propose \textbf{\placeh}, an efficient FPGA-based accelerator for MoE-ViTs. The design integrates highly optimized kernels with distinct computation patterns while employing heuristic algorithms to explore deployment strategies, achieving optimized solutions across different FPGA resources.

Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a heterogeneous architecture featuring a hybrid computation pattern, containing a fully streaming attention kernel optimized for latency and a reusable linear kernel optimized for resource efficiency. Both kernels exhibit excellent scalability, enabling flexible trade-offs between performance and resource utilization.
    \item We design a two-stage model-accelerator deployment strategy based on a genetic algorithm, which effectively balances the latency of both blocks under varying FPGA resource constraints to achieve an optimal overall solution.
    \item We validate \placeh~by deploying $\text{M}^3$ViT on both edge (ZCU102) and cloud (U280) platforms. Experimental results show that \placeh~improves energy efficiency by 7.84× over the NVIDIA V100S and 1.74× over the previous SOTA FPGA accelerator. Furthermore, our design approach effectively accelerates traditional transformer models as well.
\end{itemize}
% Despite the computational similarities between MoE-ViT and ViT, the high memory demands and sparse, dynamically activated experts in MoE-ViTrender existing ViT accelerators inefficient. Specifically, traditional ViT accelerators operate by processing patches sequentially~\cite{dong2023heatvit,ye2023accelerating,glvlsi24_qin}, while MoE ViT’s near-random expert activation results in frequent expert weight exchanges across different patches. This increases memory access overhead and introduces computational delays.
% To address this issue, Edge-MoE~\cite{sarkar2023edge} proposed an expert-by-expert computation model, optimizing the frequent memory access problem in MoE ViT. It successfully deployed the state-of-the-art (SOTA) MoE-ViTmodel, M3ViT, on the ZCU102 platform. However, Edge-MoE focuses primarily on resource-constrained platform deployment, with several limitations: 1) In terms of \textit{architecture design}, it concentrates only on reusable compute cores without fully exploring the trade-offs between computation, memory access, and resource utilization. 2) Regarding \textit{deployment strategy}, it does not adequately account for the hardware parameters of different FPGAs and the corresponding design space exploration, limiting the effective use of resources.

% In response to these challenges, this paper proposes a flexible and efficient accelerator, \placeh, for MoE-ViTdeployment across different FPGA platforms. Our main contributions are summarized as follows:
% %
% \begin{itemize}   
% \item To the best of our knowledge, \placeh~is the first end-to-end accelerator designed for the MoE ViT, capable of deployment on different FPGA platforms, with on-FPGA implementation, verified functionality, and open-sourced hardware design.
% %
% \item We designed parameterized compute units, which include fully streaming attention cores and reusable linear cores, allowing for a trade-off between performance and resources. Combining a two-stage deployment strategy, \placeh~can flexibly adapt to different deployment scenarios, enhancing computational efficiency.
% %
% \item We deployed $\text{M}^3$ViT on Xilinx ZCU102 (edge platform) and U280 (cloud platform). Experimental results demonstrate that \placeh~achieves an energy efficiency improvement of 7.84 times compared to the NVIDIA V100S and 1.74 times compared to previous state-of-the-art FPGA accelerators. Additionally, our design approach excels in accelerating traditional transformers as well.
% \end{itemize}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{moe.pdf}   
    \caption{The structure of the MoE Vision Transformer.}
    \vspace{-0.5em}
    \label{fig:moe}
    
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{overall.pdf} 
    \caption{The overall architecture of \placeh~(except the QKV Generate and Norm kernels for simplicity).    }
    \label{fig:overall}
    \vspace{-1.8em}
\end{figure*}
\section{Mixture-of-Experts Vision Transformer} \label{review}
% $M^{3}$ViT demonstrates 
% Fig.~\ref{fig:moe} illustrates the architecture of the Mixture of Experts Vision Transformer (MoE-ViT). Unlike the traditional ViT, this architecture replaces the feed-forward component in every alternate encoder layer with a Mixture of Experts (MoE) layer while keeping the original Multi-head Self Attention (MSA) layer intact. In practical implementation, each expert functions as a smaller MLP layer and processes different input patches. A gate network is used for determining which experts to activate based on the given input.
Fig.~\ref{fig:moe} presents the architecture of the MoE-ViT. In contrast to the traditional ViT, MoE-ViT replaces the feed-forward part in every alternate encoder with a MoE block, while preserving the original Multi-head Self-Attention (MSA) block. Each expert in the MoE block acts as a smaller MLP, processing distinct input patches\cite{jaszczur2021sparse}. A gate network determines which experts to activate based on the input data.

Since the number of experts is uncertain, it is impractical to load all of them at once. Therefore, $\text{M}^{3}$ViT introduced the widely adopted method of loading the weights for each expert individually, and partial results are then computed for all tokens that utilize that expert.
  
\section{Method}
\subsection{Overall Architecture}
Fig.~\ref{fig:overall} presents the architecture of \placeh, where the hardware is divided into independent blocks based on the MoE-ViT model: the MSA block, the MoE block, and optional non-encoder components such as patch embedding. Each block contains several computational kernels. In the MSA block, the streaming attention kernel handles the attention mechanism, processing QKV inputs and generating output for the projection module. The reusable linear kernel in the MoE block performs expert computations and can also be employed for other linear tasks, such as QKV generation and linear projection, either in a pipelined mode or by reusing the same kernel. 

% In terms of memory management, the streaming attention kernel employs an on-chip cache, while the reusable linear kernel accesses off-chip memory for data. 

During execution, each block works independently, and the host CPU manages the data transfer through DDR (Fig.~\ref{CPU}). To avoid conflicts during read/write operations, we designate \(Buf_0\) for MSA outputs and \(Buf_1\) for MoE inputs. Upon completion of both processes, the buffers are swapped. In this scenario, the overall latency depends on the maximum value of the two components, as illustrated in Fig.~\ref{fig:Parallel}. This concept is further explored and elaborated in Section~\ref{DSE}.

%Before the computation starts, the hardware parameters of the computational kernels are configured by the Host side.
%GELU is used as activation in MoE/FFN 
%the Attention core, the MoE core, and the left modules at the beginning and ending of ViT encoders. 
%The design space for the linear kernel is quite extensive, as it is not only utilized in the MoE layer but also plays a crucial role in \( QKV \) generating and linear projection in the attention layer.
\begin{figure}[tbp]
    \centering
    \subfloat[Host Data Transfer.]
    {
    \label{CPU}
    \includegraphics[scale = 0.49]{DMA.pdf}
    }
    \subfloat[Timeline of the first MoE-ViT layer.]{
    \label{fig:Parallel}
    \includegraphics[scale = 0.49]{multi-die.pdf} 
    }
    \caption{Processing flow with double buffering.}
    \vspace{-0.8em}
    \label{fig:multidie}
\end{figure} 

Additionally, for multi-die FPGAs, different blocks are allocated to distinct SLRs to minimize SLR-crossing and maintain load balancing. The placement of the MoE block, which frequently loads weights, is guided by strategies from prior work (\textit{e.g.}, AutoBridge~\cite{guo2021autobridge}), where memory-accessing logic is positioned near the corresponding memory devices. On the U280 FPGA, which features three SLRs and HBM subsystems attached only to the bottom SLR (SLR0), we position the MoE module at the bottom and distribute the expert weights across various HBM channels to optimize throughput.



%We also implement the placement strategy used in previous works (e.g., AutoBridge~\cite{guo2021autobridge}), which considers both circuitry and memory by positioning memory-accessing logic close to the respective memory devices.  
%Since the MoE kernel requires frequent loading of expert weights, we prefer to store the weights separately in different HBM channels to improve throughput. In practice, the U280 FPGA comprises three SLRs with the HBM subsystem only attached to the bottom SLR, SLR0. Thus, we place the MoE module at the bottom to minimize data transfer latency. 
%The Attention module is positioned on the middle SLR, because of the data transfer between layers.
 
 
% Our design is a versatile multi-task learning framework that encompasses two levels of optimization:
%\subsubsection{internal module design}
% we shifted our focus to optimizing for multi-die architectures. 

%\textbf{Internal Module Optimization}: Our focus is on designing the attention and linear modules within the encoder section of the MTL ViT. By optimizing and parameterizing their computational processes, we improve efficiency while addressing diverse computational demands. Additionally, by standardizing the I/O of our modules, the design supports both parallelism and resource-sharing patterns, further enhancing reconfigurability.

%\textbf{Inter-Module Optimization}: By analyzing the data flow patterns between modules and leveraging the characteristics of FPGAs, we enable parallel computation across different modes for multiple modules, providing greater flexibility in the design.


\subsection{Fully Streaming Attention Kernel}
The attention kernel primarily involves operations such as the QK dot product, softmax, and weighted summation. These computations are complex and significantly contribute to latency. Streaming the attention kernel can effectively mitigate runtime delays. However, as shown in Equation~\ref{3-pass}, the commonly used safe softmax algorithm, while preventing overflow, introduces a data dependency that hinders parallel computation. %Additionally, for efficient processing, both \(x_i\) and \(\exp(x_i - m(x))\) must be stored on-chip.
%Due to competition, simply computing in parallel requires doubling the buffer.
%, becoming a bottleneck for streaming. To illustrate this intuitively, we represent the computation process as follows:
\begin{equation}\label{3-pass}
m(x) \coloneqq \max_{i} x_i \quad    \textit{l}(x) \coloneqq \sum_{i} e^{(x_i - m(x))}\quad 
\textit{s}(x_i)\coloneqq \frac{e^{(x_i - m(x))}}{\textit{l}(x)}  
\end{equation}

To address this, we have integrated the separate attention mechanism into a streaming, fused module, which simultaneously reduces latency and also conserves on-chip resources.
%In traditional solutions, the entire row of the attention score \((S = Q \times K)\) must be retained for the softmax reduction to compute all \(x_i\), which requires storing the entire input on-chip and results in high BRAM consumption. To circumvent buffering the entire input, we have integrated the complete attention mechanism into a streaming, tiled module, which simultaneously reduces latency and conserves on-chip resources.
%By recording additional statistics \((m(x),\textit{l}(x))\), we can perform the softmax reduction incrementally.
\subsubsection{Patch Reorder in QK Dot}
The naive blockwise approach is for each processing element (PE) to compute \( K_j \) sequentially, then pass the result to the specific softmax module to get final scores. 
As shown in Fig.~\ref{fig:naive}, in each running cycle, every PE must reload \( K \) patches, which is not memory-efficient. 
%Additionally, performing the max operation within the PE is challenging because results from parallel computations across PEs cannot be directly compared.
%A natural idea is to perform the max operation while computing \( Q_i K_j \), thereby reducing time consumption.

%Additionally, performing the max operation within the PE is challenging because results from parallel computations across PEs cannot be directly compared.

Therefore, instead of using different \( K \) values, we assign different \( Q \) values to the \( N_{\text{a}} \) corresponding PEs (Fig.~\ref{fig:seperate}), and the same \( K \) is broadcast to all PEs, reducing the bandwidth pressure. Also, each \( Q_i \) remains within the same PE for the entire computation, allowing compute the max value directly. 
\begin{figure}[htbp]
    \centering
    \subfloat[Traditonal Single-q  computation.]{
    \label{fig:naive}
    \includegraphics[width=0.23\textwidth]{reorder.pdf}   
    } 
    \subfloat[Computation after reordering.]{  
    \label{fig:seperate}
    \includegraphics[width=0.23\textwidth]{reorder-1.pdf}   
    } 
    \caption{Running process before and after optimization. Blue q blocks are fixed to specific PEs, while the color of k blocks changes during kernel running.}
    \label{fig:reorder}
    
\end{figure}
\subsubsection{Fused Softmax Kernel}
The softmax operation is divided into two parts: the first part handles the max operation, while the second part performs the exp computation and summation. During execution, we ensure consistency between the two modules, enabling the transfer of intermediate variables using the streaming pattern.
%The two parts are executed in parallel, and 

As mentioned above, the \(x_i\) can be directly used for computing after reordering. Thus, we equip each head with corresponding max registers \(m(x)\) to store the respective maximum values. As both computations run simultaneously, the runtime latency remains unchanged compared to the former QK dot.
% During execution, the generation and consumption rates of the two modules are the same, with the size of the FIFO fixed at N.

%, before the first maximum value is passed to the next processing module for the exp operation, the FIFO channel of the stream needs to store the N values computed. After this, 
% Due to the properties of the safe softmax, the values of \(x_i - m(x)\) are all less than or equal to 1. Thus, when directly multiplying the numerator \(\exp(x_i - m(x))\) of the safe softmax with \(V\), overflow will not occur.
Meanwhile, we combine the computation of the numerator \(\exp(x_i - m(x))\) and the denominator \(l(x)\), executing them in parallel. The numerator is directly computed and multiplied with \(V\), thereby avoiding using large blocks of cache. Since the denominator is the same for calculations within a single head, after obtaining the sum of the results, only one division operation is needed to produce the final result, reducing the number of computations required.
%Similarly, after receiving multiple parallel \( Q_i \) results, we compute the numerator and denominator for the softmax function and rearrange them to produce sequential outputs. Next, by simultaneously calculating the final softmax scores and performing multiplication with the values in the \( V \) buffer.
%Since the final score computation also requires traversing all \( V \) blocks, we can also merge the remaining 1-pass softmax process into the final multiply module for parallel computation. This involves first computing the exponential function, multiplying it by the corresponding \( V_i \), and then dividing by the sum of all softmax scores.
\subsection{Reusable Linear Kernel}\label{linear}
It feels intuitive to implement parallel execution using multiple kernels for linear computations. However, as mentioned in Sec.~\ref{review}, the patch indices corresponding to the chosen expert are dynamic. In this case, using pre-configured allocation may result in some kernels being idle, leading to low utilization.

To address this challenge, we deploy \( N_{\text{L}} \) Compute Units (CUs), each dedicated solely to computation. A round-robin router efficiently manages data loading and storage between the CUs, ensuring that data is delivered in a balanced manner.

During execution, the router reads the first \( N_{\text{L}} \) unused patch indices, then cyclically loads the vectors in corresponding patches, distributing them in turn to different CUs. The weights are stored as vectors of size \( T_{\text{wt}} = T_{\text{in}} \times T_{\text{out}} \) and are broadcasted to each CU. Through this allocation method, each CU maintains the same computational workload during execution. 

Compared to directly using multiple linear kernels, in our design, only the router accesses activations. By simply changing the selection strategy, it can be employed for traditional dense linear computations. Also, due to weight sharing, our approach can reduce off-chip memory access pressure at runtime, making it more favorable for deploying larger-scale models.

%By using the indices with sequential order, this method can also be applied to FFN processes, providing reusability.

\section{Design Space Exploration}\label{DSE}

%In this section, our primary focus is on designing the resource-intensive attention modules. Due to space limitations, further details on modules such as linear and patch embedding will not be elaborated here.

\subsection{Accelerator Modeling}
\subsubsection{Resource Modeling}
We assume the original image is divided into \(\mathcal{N}\) patches with a feature dimension of \( \mathcal{F} \). The data bit-width is denoted as \( q \). Based on previous studies~\cite{chen2019cloud},~\cite{lou2023naf} and our design experience, we believe that excluding ultra-low bit-widths, DSP blocks, random-access memory (RAM), and off-chip bandwidth (BW) are typically the limiting factors in FPGA-based accelerator designs. Therefore, we conducted an in-depth analysis of DSP and BRAM utilization during the modeling process, while BW is dynamically allocated during the hardware generation process. Due to space limitations, we only present the modeling process of the attention kernel.

\textbf{DSP resources} are typically used for multiplication and accumulation. Hence, the DSP usage in the attention kernel depends on the bit-width of the input data, the degree of parallelism in the attention PEs, and the dimensions \(T_{\text{a}}\) after tiling. 
Overall, the total DSP utilization can be expressed as:
\begin{equation}
\mathcal{D}_{\text{attn}} = (2\Psi(q) \times T_{\text{a}}  + \mathcal{D}_{\text{exp}} \times h) \times N_{\text{a}}
\end{equation}
Specifically, $\Psi(q)$ represents the DSP resource consumption function for different bit-widths. In detail,  $\Psi(q) = 1$  when  8 \textless $q \leq 16$; $\Psi(q) = 0.5$  when  4 \textless $q \leq 8$, $\Psi(q) = 0$  when  $ q \leq 4$.

\textbf{BRAM utilization} can be divided into consumption within each PE or across multiple PEs. In our design, buffers inside PE are usually implemented by LUTs and Flip-Flops (FFs) due to the small block depth, aside from BRAMs used by the exponential computation. Therefore, the BRAM utilization is:
\begin{equation}
\mathcal{B}_{\text{attn}} =
%\sum \left\{
%\begin{array}{l}
2 \left\lceil{q}/{\text{bwidth}}\right\rceil \times \left\lceil   \mathcal{N}/{\text{bdepth}}  \right\rceil  +  
\mathcal{B}_{\exp}\times h \times N_{\text{a}}     
%\end{array}
%\right.
\end{equation}
\text{bwidth} and \text{bdepth} are the data width and depth provided by one BRAM, respectively.

\subsubsection{Performance Modeling}
By using stream processing, running cycles are determined by the slowest module. In our design, both attentio parts achieve the same latency, which is:
\begin{equation}
\mathcal{L}_{\text{attn}} = {N^2 \times \mathcal{F}}/({T_{\text{a}} \times N_{\text{a}}})
\end{equation}
%
%
%\subsection{Flexible Layer Running Modes}

%We categorize the use of Linear kernels in different directions: reusable and specialized modules. 
%, we can adjust the running modes to suit different computational processes. In the actual design, we can optimize the performance and resource consumption using the allocation strategy below:

%The design space for the linear kernel is quite extensive, as it is not only utilized in the MoE layer but also plays a crucial role in \( QKV \) generating and linear projection in the attention layer. 
%that each CU  performs an equal amount of computation when operating in parallel, which in turn means
%For specialized modules, implementing a full dataflow design by integrating the QKV and projection computations into the attention core significantly improves latency. However, it inevitably requires additional BRAM resources and latency due to the need to load all weight data onto the chip. The linear CU number \(N_{\text{L}} \) is also strictly constrained to be identical modules that are multiples of four, as different specifications of CUs can lead to imbalances in the data stream.
%For reusable modules, utilizing our custom round-robin router ensures that overall computation time and resource usage are approximately proportional. Because of this, the number of CUs inside the module is unrestrained, allowing for full utilization of on-chip resources. Meanwhile, the computation latency can cover the latency of loading activations and weights by double buffering and BW allocation, making this approach remain competitive.

\subsection{2-stage Heuristic Search}
\begin{algorithm}[t]
\caption{\textbf{2-stage Hardware Accelerator Search (HAS) Process}}
\label{HAS}
\footnotesize
$\digamma_c = [num,T_{a},N_{a},T_{in}, T_{out}, N_{L}]_c$\\
 Initialize the hardware constraint ($\mathcal{D}_{total}$, $\mathcal{B}_{total}$, $\mathcal{BW}_{total}$)\\
\leftline{\textcolor{blue}{// $\textbf{\emph{MoE stage part 1}}$}}
 Calculate the \textbf{best} latency \(\mathcal{L}_\text{MoE}\) of MoE block depending on the constraint ($\mathcal{D}_{total}$) \\
 \leftline{\textcolor{blue}{// $\textbf{\emph{MSA stage}}$}}
\For{$c$ \textbf{in} $num$}{
    Randomly initialize each individual \\
    Set Fit Scores to $ \mathcal{L}_\text{MoE}/\mathcal{L}_\text{MSA}$   \\
    \While{ $i$ \textless  Iteration }{
        Use traditional GA algorithm to calculate the best $\mathcal{L}_{\text{MSA}}$\\
        \If{ Fit Scores $\geq$ 1 }{
            Return latency ($\mathcal{L}_{\text{MoE}}$) and hardware parameters \\
        }    
    }    
}
\leftline{\textcolor{blue}{// $\textbf{\emph{MoE stage part 2}}$}}
Use binary search for the lowest resource usage on MoE depending on the upper bound latency $\mathcal{L}_{\text{MSA}}$ \\
        Return latency ($\mathcal{L}_{\text{MSA}}$) and hardware parameters c($\digamma^*$)\\
\end{algorithm}
As previously discussed, the total latency is determined by the execution time of the slowest block due to the double buffering mechanism. For example, integrating streaming linear computations can help reduce latency in the MSA block, but the overall latency may increase due to fewer computational resources being available. 

Based on the above scenario, we present a simple but efficient 2-stage Search Algorithm~\ref{HAS} that uses both Genetic Algorithm (GA)~\cite{9251929} and binary search to balance latency and resource usage. Since the computation pattern is static and unchanged, the linear operations within the MSA block can be deployed independently. As a result, we track the number of streaming modules, denoted as \(num\).

%Therefore, it is essential to configure the operating method for each operation separately to balance latency and resource usage effectively. 





% As mentioned earlier, due to the double buffering mechanism on the host side, the total latency depends on the execution time of the slowest block. Modules not part of the encoder are less critical  as they do not need to be executed multiple times. Therefore, we can dynamically adjust the target latency to lower bound $\mathcal{L}_{\text{MoE}}$ for the MSA block based on the MoE block, allowing us to stop computations early and avoid unnecessary processing.
% In cases where the MSA block remains the slowest after HAS, it's clear that the previously identified latency-optimized MoE module has become idle, leading to decreased utilization efficiency. Similarly, by using the dynamically adjusted latency upper bound $\mathcal{L}_{\text{Attn}}$ from the MSA block as a target, we can apply a simple binary search method to find acceptable parameters, then reduce the overall resource consumption of the framework.

%As previously discussed, the total latency is determined by the execution time of the slowest block due to the host-side double buffering mechanism.

In practice, modules outside the encoder are less critical since they are not repeatedly executed. Thus, the target latency for the MSA block can be dynamically adjusted to the lower bound, $\mathcal{L}_{\text{MoE}}$, set by the MoE block. This allows for early termination of computations, reducing unnecessary processing overhead.


While the MSA block remains the primary bottleneck after HAS, the previously optimized MoE module becomes idle, resulting in reduced utilization efficiency. To address this, we can set the dynamically adjusted latency upper bound, $\mathcal{L}_{\text{MSA}}$, of the MSA block as the target. By employing a binary search, we can identify suitable parameters, thereby minimizing overall resource consumption within the framework.


\section{Experiment}

\subsection{Experiment Setup}
To verify the effectiveness of \placeh, we deploy $\text{M}^3$ViT on Xilinx ZCU102 and Alveo U280 platforms, using Vitis HLS and Vivado (v2023.1). The resource consumption and layout routing results are shown in Table~\ref{Resource} and Fig.~\ref{fig:plate}, respectively. 
As a comparison, we implemented the corresponding GPU version using PyTorch (v2.0.1). Both batch size is set to 1. Besides, we also compared our optimization method with previous transformer accelerators to validate it.
\begin{figure}[htbp]
    \vspace{-1.8em}
    \centering
    \subfloat[ZCU102 result.]
    {
    \includegraphics[scale = 0.45]{route.pdf}
    }
    \subfloat[U280 result.]{
    \includegraphics[scale = 0.45]{route1.pdf} 
    }
    \caption{Implementation results of $\text{M}^3$ViT on both platforms.}
    \label{fig:plate}
    \vspace{-1.8em}
\end{figure}  
\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \caption{Resource Consumption of Deploying $\text{M}^3$ViT on ZCU102 and Alveo U280 FPGA Platforms} \vspace{-0.8em}
    \renewcommand{\arraystretch}{1.2}
    \resizebox{0.7\linewidth}{!}{
    \begin{threeparttable}{
    \begin{tabular}{c|cccc} \Xhline{3\arrayrulewidth}
         \textbf{Platform}  & \textbf{DSPs} & \textbf{BRAMs} & \textbf{LUTs} & \textbf{Flip-Flop (FFs)} \\ \hline \hline
         \textbf{ZCU102 (Edge)} & 1850 & 458 & 123.4K & 142.6K \\ \hline
            \textbf{Alveo U280 (Cloud)} & 3413 & 974 & 316.1K & 385.9K \\ \Xhline{3\arrayrulewidth}
    \end{tabular}}
    \end{threeparttable}}
    \vspace{-0.1em}
    \label{Resource}
\end{table}
\subsection{Compared with Related Work on \(\text{M}^3\)ViT}
We benchmarked \placeh~against leading works, as detailed in Table~\ref{M3vit}. To ensure a fair comparison of accelerator performance across different platforms, we use efficiency (GOPS/W) as the evaluation metric. On the ZCU102 platform, compared to GPU and Edge-MoE~\cite{fan2022m3vit}, we achieve 1.77×, 1.34× speedup, and 7.85×, 1.75× energy efficiency improvement. On the U280 platform, 
%the design performs better both in throughput and latency because of higher computational resources. 
due to the DSP consumption in the 32-bit multiplication process and the extra use of resources for data transfer between the host CPU and the platform, our proposed design offers a frequency of 200 MHz. Still, we achieve 7.451 GOPS/W, outperforming prior work (4.83 GOPS/W).

\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{1.pt}
\caption{Comparison with GPU and Edge-MoE on $\text{M}^3$ViT} \vspace{-1em}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
\Xhline{3\arrayrulewidth}
\textbf{Attribute} & \textbf{GPU} & \textbf{Edge-MoE \cite{fan2022m3vit} } & \multicolumn{2}{c}{\textbf{Ours} } \\ \hline \hline

\textbf{Platform} & Tesla V100S & ZCU102 & ZCU102 & U280 \\ \hline
\textbf{Bit-width} & FP32 & $W^{16}A^{32}$ & $W^{16}A^{32}$ & $W^{16}A^{32}$ \\ \hline
% \textbf{BRAMs} & - & 457 & 458 & 974 \\
% \hline
% \textbf{DSPs} & - & 1922 & 1850 & 3413\\
% \hline
% \textbf{LUTs} & - & 128.2k & 123.4k & 316.1k \\
% \hline
% \textbf{FFs} & - & 161.1k & 142.6k & 385.9k\\
% \hline 

\textbf{Frequency (Mhz)} & 1245 & 300 & 300 & 200 \\ \hline
\textbf{Power (W)} & 51 & 14.54 & 11.50 & 32.49 \\ \hline
\textbf{Latency (ms)} & 40.1 & 34.64 & 25.76 & 10.33 \\ \hline
\textbf{Throughput (GOPS)} & 54.86 & 72.15 & \textbf{97.04} & \textbf{242.01} \\ \hline
\textbf{Efficiency (GOPS/W)} & 1.075 & 4.83 & \textbf{8.438} & \textbf{7.451} \\ 
\Xhline{3\arrayrulewidth}
\end{tabular}} \label{M3vit} \vspace{-1.1em}
\end{table}

\begin{table}[htbp] 
\centering
\setlength{\tabcolsep}{1.pt}
 \caption{Comparison with Previous FPGA Implementations} \vspace{-1em}
 \renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c} 
\Xhline{3\arrayrulewidth}

\textbf{Attribute}& \textbf{HeatViT~\cite{dong2023heatvit}} & \textbf{\placeh-E} & \textbf{TECS'23 ~\cite{ye2023accelerating}} & \textbf{\placeh-C}\\
\hline \hline
\textbf{Model} & DeiT-S & ViT-T & BERT-B & ViT-S \\
\hline
\textbf{Platform} & ZCU102 & ZCU102 & U250 & U280 \\
\hline
\textbf{Bit-width} & INT8 & INT16 & INT8 & INT16 \\
\hline
% \textbf{Clock} & \multirow{2}{*}{1245}  & \multirow{2}{*}{300}  & \multirow{2}{*}{300} & \multirow{2}{*}{200}  \\
% \textbf{(Mhz)}& & & & \\ \hline
\textbf{Freq. (Mhz)} &300  & 300  & 300 & 250  \\ \hline
\textbf{Power (W)} & 10.697  & 9.94  & 77.168  & 31.36  \\
\hline
% \textbf{BRAMs} & 338.5 & 549 & 1781 & 1086 \\
% \hline
% \textbf{DSPs} & 1955 & 2076 & 4189 & 2043\\
% \hline
% \textbf{LUTs} & 145k & 158.0k & 736k & 239.2k \\
% \hline
% \textbf{FFs} & 100.4k & 199.8k & - & 361.4k\\
% \hline 

\textbf{Latency (ms)} & 9.15 & 8.20 & -  & 11.66 \\
\hline
\textbf{Throughput (GOPS)} & 220.6  & \textbf{304.84} & 1800  & \textbf{789.72}  \\ \hline
\textbf{Efficiency (GOPS/W)} & 20.62  & \textbf{30.66} & 23.32  & \textbf{25.16} \\
\Xhline{3\arrayrulewidth}
\end{tabular}} \label{vit compare} 
%\vspace{-1.7em}
\end{table}
\subsection{Comparison With Prior Transformer Accelerators}
Although our design is tailored for MoE-ViT rather than the standard ViT model, the lack of accelerator examples for MoE-ViT makes this comparison somewhat tenuous. 
Therefore, Table~\ref{vit compare} compares our work with related FPGA works on both edge (\placeh-E) and  cloud (\placeh-C) platforms. 

Using lower bit-width reduces the resource consumption of individual multiplication calculations, thereby providing a more extensive search space.
After balancing the on-chip resource usage and operating frequency, we achieved 304.84 GOPS on ZCU102 and 789.72 GOPS on U280, respectively. Although both the HeatViT\cite{dong2023heatvit} and TECS'23\cite{ye2023accelerating} use INT8 quantization and DSP packing optimization\cite{xilinx-conv}, even without these methods, our accelerator achieves higher efficiency on the corresponding platforms at 30.66 and 25.16 GOPS/W, validating the effectiveness of our design.

\section{Conclusion}
This paper presents \placeh, a ubiquitous Mixture-of-Experts Vision Transformer accelerator applicable to different scenarios. Specifically, We designed kernels with different computation patterns to achieve a trade-off between resources and latency. Furthermore, a two-stage algorithm was employed to compute the optimal solution under various deployment scenarios. Implemented results on U280 show that we can achieve up to 242.01 GOPS in throughput and 7.451 GOPS/W in energy efficiency, surpassing previous work.

\section{Acknowledgment}
This work was supported in part by the National Key R\&D Program of China under Grants 2022YFB4501600 and 2022YFB4501603, in part by the National Natural Science Foundation of China under Grants 62102383, 61976200, and 62172380, in part by Jiangsu Provincial Natural Science Foundation under Grant BK20241818, in part by Youth Innovation Promotion Association CAS under Grant Y2021121, in part by USTC Research Funds of the Double First-Class Initiative under Grant YD2150002011.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
