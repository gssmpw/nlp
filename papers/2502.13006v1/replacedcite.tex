\section{Related Work}
\label{sec:related-work}
In this section, we discuss several lines of research that are related to this work. 

\subsection{Offline Action Model Learning Algorithms}
\label{sec:related-offline-action-model-learning}


FAMA____ is an offline action model learning algorithm that can handle missing observations and outputs a classical planning domain model. 
It frames the task of learning an action model as a planning problem, ensuring that the returned action model is consistent with the provided observations.
NOLAM____ can learn action models even from noisy trajectories. 
LOCM____ learns an action model from observed sequences of actions and their signatures, without observing the states in the trajectory.

The Safe Action Model (SAM) learning algorithm____ differs from the above algorithm in that the action model it returns provides a form of ``safety'' guarantee: not only is it consistent with the given observations but it is also guaranteed that every plan created with the learned action model is \emph{sound} with respect to the real, unknown action model (Definition~\ref{def:safe}).
SAM has been extended to support lifted action model representation____, partial observability____, stochastic effects____, and conditional effects____. 
All SAM algorithms require observing all the actions in the given trajectories, and most
also require observing the states. 
The \nsam algorithm used in this work is a member of this family of algorithms. 

All the mentioned above algorithms assume the given observations are symbolic. 
Recent work explored how to learn action models from raw images. 
LatPlan____ learns propositional action models in the latent space using a variational autoencoder. 
They use the Gumbel-Softmax technique____ to convert the continuous output of an autoencoder into categorical variables. 
These categorical variables are used as propositional symbols in a symbolic reasoning system, which in LatPLan's case is a symbolic action model. 
ROSAME-I____, like LatPlan, learns action models from visual inputs. 
Unlike LatPlan, ROSAME-I requires knowing the set of possible propositions and action signatures as input.
ROSAME-I simultaneously learns classifiers for identifying propositions in a given image and infers a lifted, first-order action model defined over the given set of propositions and actions. 


% None of the algorithms described above can learn an action model for numeric planning. 
\nsam____ and 
PlanMiner____ are, to the best of our knowledge, the only algorithms capable of learning action models that include both discrete and continuous preconditions and effects.\footnote{Some other action model learning algorithms are capable of learning the numeric costs or rewards associated with actions____.} 
\nsam makes several simplifying assumptions that allow it to run in polynomial time.
Specifically, it assumes full, noise-free, observability, 
and that preconditions are conjunctions of linear inequalities while effects are conjunctions of linear equations involving numeric state variables. 
Under these assumptions, \nsam provides the same type of safety guarantee as other SAM learning algorithms. 
Unlike \nsam, PlanMiner can learn from noisy observations. 
However, it does not provide any guarantee on the learned action model, and it is unclear whether or not it supports numeric preconditions. 
In addition, it requires solving a symbolic regression problem, which, in general, is computationally intractable. 
For these reasons, we chose \nsam as our main model learning algorithm.


\subsection{Online Action Model Learning Algorithms}
\label{sec:related-online-action-model-learning}
Online action-model learning algorithms 
iteratively learn an incumbent action model and choose the next actions to perform in order to collect observations that enable further refinement of the incumbent action model. 
OLAM____ is an online action model learning algorithm that is designed for classical planning domains. It identifies in every iteration an action and a state where trying to execute that action is expected to refine the incumbent action model. 
Then, it uses a planner to find a plan to reach that state and attempts to execute the chosen action. 
GLIB____ follows a similar approach but is designed for stochastic environments, resulting in a Probabilistic PDDL (PPDDL) action model.
QACE____ is an action model learning algorithm that can also query a black-box expert. It outputs a PPDDL action model with the same capabilities as the black-box expert it trained from. 
% Query-based Autonomous Capability Estimation
Karia et al.____ extends QACE to address the non-stationarity of the environment, i.e., address cases where the environment dynamics change. QACE+ achieves this by interleaving planning and learning and focusing on learning only the models essential for the tasks at hand.
% ILM < GLIB < QACE
ILM____ employs an explore-exploit strategy: if it reaches a state from which the goal can be achieved, it exploits this state; otherwise, it explores through random walks.
% is also designed for non-stationarity environments, and results in a PPDDL action model.
Instead of focusing solely on reaching a specific goal, the agent can take a broader approach by exploring the environment and aiming for an interesting state in it.




Recent works explored integrating \rl and online learning of a symbolic action model____. 
The objective of these works is typically to maximize a cumulative reward metric, in contrast to action model learning algorithms like OLAM and ILM, whose objective is to learn a symbolic action model.
Sreedharan and Katz____ proposed such an algorithm, which we refer to as the SK algorithm. SK begins by initializing an optimistic symbolic model that assumes all actions are applicable in every state (i.e., no preconditions) and the effect of each action includes all grounded predicates. It then employs fast, diverse planners to generate potential paths toward the goal. While these paths are unlikely to be valid, they serve as exploration mechanisms to gather new information. 
Specifically, these plans are executed within the environment, with the outcomes used to train a reward-maximizing policy using Q-learning____.
This continuous process of exploration and symbolic model refinement is guaranteed to generate a goal-reaching policy. Our hybrid strategy for the online learning setting is somewhat similar to SK. However, SK is only designed for classical (non-numeric) planning, and its applicability to numeric planning remains uncertain. 
SORL____ is another online algorithm that integrates RL and learning symbolic action models to maximize the cumulative reward. 
It collects visual observations from the environment and assumes the existence of a mapping function from visual observations to symbolic states. 
SORL iteratively learns and creates 
a symbolic, higher-level action model, 
and a lower-level set of RL policies, referred to as \emph{symbolic options}. 
A planner uses the learned symbolic action model to create a high-level plan, and a meta-controller chooses or creates symbolic options to try to execute the high-level plan, exploring the environment as needed. 

While not explicitly specified, the action model learning algorithm SORL uses is not robust to missing or noisy observations. It assumes that an action's effects can be inferred by the difference between the states observed before and after applying that action and shows no support for numeric preconditions. Numeric effects are supported in a very limited way, only learning which actions increase the reward and by how much. Other numeric aspects, e.g., numeric state variables and preconditions, are not learned.


\subsection{Summary: Action Model Learning Algorithms}
\label{sec:summary-action-model-learning}

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{} & \textbf{Input} & \textbf{Numeric} & \textbf{Stochastic} & \textbf{NS} & \textbf{Noisy} & \textbf{Obs.} & \textbf{Online/Offline} \\
\hline
\textbf{FAMA____} & symbolic & No & No & No & No & Partial & Offline \\
\hline
\textbf{LOCM____} & symbolic & No & No & No & No & Only action & Offline \\
\hline
\textbf{OLAM____} & symbolic & No & No & No & No & Yes & Online \\
\hline
\textbf{NOLAM____} & symbolic & No & No & No & Yes & Yes & Offline \\
\hline
\textbf{ILM____} & symbolic & No & Yes & Yes & Yes & Yes & Online \\
\hline
\textbf{GLIB____} & symbolic & No & Yes & No & No & Yes & Online \\
\hline
\textbf{QACE____} & symbolic & No & Yes & No & No & Yes & Online \\
\hline
\textbf{QACE+____} & symbolic & No & Yes & Yes & No & Yes & Online \\
\hline
\textbf{SORL____} & visual\footnote{They assumed as input a perfect mapping from visual input to symbolic state.}  & Yes\footnote{The support for numeric planning is limited to only learning how much reward each action adds.} & No & No & - & Yes & Online \\
\hline
\textbf{SAM____} & symbolic & No & No & No & No & Yes & Offline \\
\hline
\textbf{NSAM____} & symbolic & Yes & No & No & No & Yes & Offline \\
\hline
\textbf{PlanMiner____} & symbolic & Yes & No & No & Yes & Partial & Offline \\
\hline
\textbf{SK____} & symbolic & No & No & No & No & Yes & Online \\
\hline
\textbf{JRK____} & visual & No & Yes\footnote{While they learn a PPDDL model, the experimental results all use a deterministic planner.} & No & - & Yes & Offline \\
\hline
\textbf{LATPLAN____} & visual & No & No & No & - & Yes & Offline \\
\hline
\textbf{ROSAME-I____} & visual & No & No & No & - & No\footnote{Note that the first and last states in every trajectory are assumed to be fully obseravable} & Offline \\
\hline
\rowcolor{yellow}
\textbf{\hybrid (our method)} & symbolic & Yes & No & No & No & Yes & Online  \\
\hline
\end{tabular}
}
\caption{Comparison of various action model learning algorithms, based on their support of given problem - numeric inputs, stochasticity, non-stationarity, noisy, observability, and online/offline learning capability.}
\label{tab:methods-comparison}
\end{table}

Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model. 





\subsection{AI Agents for Minecraft}
\label{sec:related-minecraft}
In our work, \mcraft is merely a benchmark for the evaluation of the proposed models and algorithms. 
However, many prior works have focused on the problem of solving various \mcraft tasks. 
Indeed, both Planning and \rl have been used to design AI agents for \mcraft. 
We provide a brief review of these efforts below. 



____ developed an AI \mcraft agent based on automated planning. They designed a custom PDDL+____ domain model for the \mcraft task. Their agent was able to achieve this task by using a PDDL+ planner and a goal reasoner. Adapting their PDDL+ domain to different tasks is not trivial. 
____ used PDDL modeling to solve complex construction tasks in \mcraft. 
They modeled house-construction tasks as classical and as Hierarchical Task Network (HTN)____ planning problems. 
They observed that even simple tasks pose challenges for classical planners as the size of the world increases. 
In contrast, the HTN planner scaled better but was too coupled with specific tasks.


____ proposed an algorithm for learning a PPDDL domain from visual input in \mcraft. We refer to their algorithm as the JRK algorithm. 
JRK processes the visual input to identify objects and learn a grounded PPDDL domain. 
Then, it learns a lifted PPDDL domain by grouping objects into types based on the similarity of their preconditions and effects. 
The domain they learned is purely propositional, while for the \mcraft tasks we require domain models that include both propositions and numeric state variables. 



% This part is about the core algorithm that solved the MineRL challenge
Hierarchical Deep Reinforcement Learning Network (H-DRLN)____  
is commonly used in successful MineRL agents. 
When using H-DRLN, the agent continuously learns multiple policies and adapts to new challenges within the game. 
The H-DRLN leverages a deep neural network to model the policy and value functions, resulting in high effectiveness across a variety of \mcraft tasks such as navigation, mining, and combat. Despite its success, this approach requires tens to hundreds of thousands of steps for single, non-hierarchical tasks, while more complex, multi-task hierarchical setups may demand up to several million steps to achieve success. 
Thus, it is less suitable for our context. 


% VOYGER
Recent work explored how to use Large Language Models (LLMs) to solve \mcraft tasks. 
The VOYAGER algorithm____ is an example of an LLM-based \mcraft agent. It comprises three key components: an automatic curriculum for exploration tasks, a skill library for complex behaviors, and an iterative prompting mechanism for generating executable code using JavaScript and the Mineflayer API____. It engages with GPT-4 via text prompts, refining generated code based on environmental feedback to successfully complete tasks. Skills accumulated in the library facilitate behavior reuse and unlock new capabilities. 
Using VOYGER has several disadvantages. 
First, it requires many calls to GPT-4, which may be limited and costly. 
Second, it may suffer from \emph{hallucinations}. 
In our case, this means it may ask the agent to perform non-existing actions. Moreover, it is not clear how well it will fare when crafting new recipes that it has not seen before, as needed in the tasks we consider. 

% VPT, MineClip, and STEVE-1
VPT____ is an AI \mcraft agent that uses a Minecraft video gameplay dataset to learn how to navigate and interact with the world. 
MineClip____ is another AI \mcraft agent that focuses on learning a reward function to guide the agent's actions toward specific objectives. 
STEVE-1____ is a more recent AI \mcraft agent that combines techniques from VPT and MineClip. 
Despite its advancements, STEVE-1 struggles with longer-horizon tasks like crafting and building without further training. It may also be unable to craft custom recipes to solve the \mcraft tasks we consider, such as the \pogotask in our case. 

DreamerV3____ is an advanced model-based reinforcement learning agent that efficiently learns long-horizon tasks in complex environments by leveraging latent world models. Instead of relying on extensive trial-and-error, the agent learned to predict future states, rewards, and environment dynamics based on past experiences, enabling it to ``imagine" different strategies in latent space. In the context of Minecraft, DreamerV3 was able to learn to mine diamonds (a very challenging task in Minecraft) using only 2.5 million frames, significantly fewer than traditional reinforcement learning methods.
In our case, where training lasts only a few thousand steps per task, even such an efficient algorithm would not be sufficient.