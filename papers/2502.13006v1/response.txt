\section{Related Work}
\label{sec:related-work}
In this section, we discuss several lines of research that are related to this work. 

\subsection{Offline Action Model Learning Algorithms}
\label{sec:related-offline-action-model-learning}


FAMA is an offline action model learning algorithm that can handle missing observations and outputs a classical planning domain model. 
It frames the task of learning an action model as a planning problem, ensuring that the returned action model is consistent with the provided observations.
NOLAM can learn action models even from noisy trajectories. 
LOCM learns an action model from observed sequences of actions and their signatures, without observing the states in the trajectory.

The Safe Action Model (SAM) learning algorithm differs from the above algorithm in that the action model it returns provides a form of ``safety'' guarantee: not only is it consistent with the given observations but it is also guaranteed that every plan created with the learned action model is \emph{sound} with respect to the real, unknown action model (Definition~\ref{def:safe}).
SAM has been extended to support lifted action model representation, partial observability, stochastic effects, and conditional effects. 
All SAM algorithms require observing all the actions in the given trajectories, and most
also require observing the states. 
The \nsam algorithm used in this work is a member of this family of algorithms. 

All the mentioned above algorithms assume the given observations are symbolic. 
Recent work explored how to learn action models from raw images. 
LatPlan developed an algorithm for learning a PPDDL domain model using visual input, where LATPLAN can handle noise due to its use of function approximation for image processing. 
They learned a lifted PPDDL domain by grouping objects into types based on the similarity of their preconditions and effects. 
The domain they learned is purely propositional, while for the \mcraft tasks we require domain models that include both propositions and numeric state variables. 



____ proposed an algorithm for learning a PPDDL domain from visual input in \mcraft. We refer to their algorithm as the JRK algorithm. 
JRK processes the visual input to identify objects and learn a grounded PPDDL domain. 
Then, it learns a lifted PPDDL domain by grouping objects into types based on the similarity of their preconditions and effects. 
The domain they learned is purely propositional, while for the \mcraft tasks we require domain models that include both propositions and numeric state variables. 



% This part is about the core algorithm that solved the MineRL challenge
Hierarchical Deep Reinforcement Learning Network (H-DRLN)  
is commonly used in successful MineRL agents. 
When using H-DRLN, the agent continuously learns multiple policies and adapts to new challenges within the game. 
The H-DRLN leverages a deep neural network to model the policy and value functions, resulting in high effectiveness across a variety of \mcraft tasks such as navigation, mining, and combat. Despite its success, this approach requires tens to hundreds of thousands of steps for single, non-hierarchical tasks, while more complex, multi-task hierarchical setups may demand up to several million steps to achieve success. 
Thus, it is less suitable for our context. 


% VOYGER
Recent work explored how to use Large Language Models (LLMs) to solve \mcraft tasks. 
The VOYAGER algorithm is an example of an LLM-based \mcraft agent. It comprises three key components: an automatic curriculum for exploration tasks, a skill library for complex behaviors, and an iterative prompting mechanism for generating executable code using JavaScript and the Mineflayer API. 
It engages with GPT-4 via text prompts, refining generated code based on environmental feedback to successfully complete tasks. Skills accumulated in the library facilitate behavior reuse and unlock new capabilities. 
Using VOYGER has several disadvantages. 
First, it requires many calls to GPT-4, which may be limited and costly. 
Second, it may suffer from \emph{hallucinations}. 
In our case, this means it may ask the agent to perform non-existing actions. Moreover, it is not clear how well it will fare when crafting new recipes that it has not seen before, as needed in the tasks we consider. 

% VPT, MineClip, and STEVE-1
VPT is an AI \mcraft agent that uses a Minecraft video gameplay dataset to learn how to navigate and interact with the world. 
MineClip is another AI \mcraft agent that focuses on learning a reward function to guide the agent's actions toward specific objectives. 
STEVE-1 is a more recent AI \mcraft agent that combines techniques from VPT and MineClip. 
Despite its advancements, STEVE-1 struggles with longer-horizon tasks like crafting and building without further training. It may also be unable to craft custom recipes to solve the \mcraft tasks we consider, such as the \pogotask in our case. 

DreamerV3 is an advanced model-based reinforcement learning agent that efficiently learns long-horizon tasks in complex environments by leveraging latent world models. Instead of relying on extensive trial-and-error, the agent learned to predict future states, rewards, and environment dynamics based on past experiences, enabling it to ``imagine" different strategies in latent space. In the context of Minecraft, DreamerV3 was able to learn to mine diamonds (a very challenging task in Minecraft) using only 2.5 million frames, significantly fewer than traditional reinforcement learning methods.
In our case, where training lasts only a few thousand steps per task, even such an efficient algorithm would not be sufficient.

Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model.  

Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model.  


Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model. 





\subsection{AI Agents for Minecraft}
\label{sec:related-minecraft}
In our work, \mcraft is merely a benchmark for the evaluation of the proposed models and algorithms. 
However, many prior works have focused on the problem of solving various \mcraft tasks. 
Indeed, both Planning and \rl have been used to design AI agents for \mcraft. 
We provide a brief review of these efforts below. 



____ developed an AI \mcraft agent based on automated planning. They designed a custom PDDL+  domain model for the \mcraft task. Their agent was able to achieve this task by using a PDDL+ planner and a goal reasoner. Adapting their PDDL+ domain to different tasks is not trivial. 
____ used PDDL modeling to solve complex construction tasks in \mcraft. 
They modeled house-construction tasks as classical and as Hierarchical Task Network (HTN)  planning problems. 
They observed that even simple tasks pose challenges for classical planners as the size of the world increases. 
In contrast, the HTN planner scaled better but was too coupled with specific tasks.


____ proposed an algorithm for learning a PPDDL domain from visual input in \mcraft. We refer to their algorithm as the JRK algorithm. 
JRK processes the visual input to identify objects and learn a grounded PPDDL domain. 
Then, it learns a lifted PPDDL domain by grouping objects into types based on the similarity of their preconditions and effects. 
The domain they learned is purely propositional, while for the \mcraft tasks we require domain models that include both propositions and numeric state variables. 



% This part is about the core algorithm that solved the MineRL challenge
Hierarchical Deep Reinforcement Learning Network (H-DRLN)  
is commonly used in successful MineRL agents. 
When using H-DRLN, the agent continuously learns multiple policies and adapts to new challenges within the game. 
The H-DRLN leverages a deep neural network to model the policy and value functions, resulting in high effectiveness across a variety of \mcraft tasks such as navigation, mining, and combat. Despite its success, this approach requires tens to hundreds of thousands of steps for single, non-hierarchical tasks, while more complex, multi-task hierarchical setups may demand up to several million steps to achieve success. 
Thus, it is less suitable for our context. 


% VOYGER
Recent work explored how to use Large Language Models (LLMs) to solve \mcraft tasks. 
The VOYAGER algorithm is an example of an LLM-based \mcraft agent. It comprises three key components: an automatic curriculum for exploration tasks, a skill library for complex behaviors, and an iterative prompting mechanism for generating executable code using JavaScript and the Mineflayer API. 
It engages with GPT-4 via text prompts, refining generated code based on environmental feedback to successfully complete tasks. Skills accumulated in the library facilitate behavior reuse and unlock new capabilities. 
Using VOYGER has several disadvantages. 
First, it requires many calls to GPT-4, which may be limited and costly. 
Second, it may suffer from \emph{hallucinations}. 
In our case, this means it may ask the agent to perform non-existing actions. Moreover, it is not clear how well it will fare when crafting new recipes that it has not seen before, as needed in the tasks we consider. 

% VPT, MineClip, and STEVE-1
VPT is an AI \mcraft agent that uses a Minecraft video gameplay dataset to learn how to navigate and interact with the world. 
MineClip is another AI \mcraft agent that focuses on learning a reward function to guide the agent's actions toward specific objectives. 
STEVE-1 is a more recent AI \mcraft agent that combines techniques from VPT and MineClip. 
Despite its advancements, STEVE-1 struggles with longer-horizon tasks like crafting and building without further training. It may also be unable to craft custom recipes to solve the \mcraft tasks we consider, such as the \pogotask in our case. 

DreamerV3 is an advanced model-based reinforcement learning agent that efficiently learns long-horizon tasks in complex environments by leveraging latent world models. Instead of relying on extensive trial-and-error, the agent learned to predict future states, rewards, and environment dynamics based on past experiences, enabling it to ``imagine" different strategies in latent space. In the context of Minecraft, DreamerV3 was able to learn to mine diamonds (a very challenging task in Minecraft) using only 2.5 million frames, significantly fewer than traditional reinforcement learning methods.
In our case, where training lasts only a few thousand steps per task, even such an efficient algorithm would not be sufficient.

Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model.  

Table~\ref{tab:methods-comparison} provides an overview of all action model learning algorithms described above. 
Every row represents a model-learning algorithm, and every column represents a property of action model-learning algorithms. 
Column ``Input'' refers to the type of input given to the learning algorithm, namely if it is symbolic or visual. 
Columns ``Numeric'' and ``Stochastic'' refer to whether the underlying environment includes numeric state variables and stochastic effects, respectively. 
Column ``NS'' (non-stationarity) 
refers to whether the dynamics of the underlying environment, i.e., the actions' preconditions and effects, may change during learning. 
Columns ``Noisy'' and ``Obs.'' refer to whether the states and actions in the given observations are noisy and fully observed, respectively. Note that if the ``Input'' is visual, the algorithm can handle noise due to its use of function approximation for image processing. This is indicated by ``-'' in Table~\ref{tab:methods-comparison}. 
The ``Online/Offline'' column refers to whether the learning algorithm is an online algorithm or an offline algorithm. 
As can be seen, the \hybrid hybrid strategy we propose in this work is the only online learning algorithm that supports learning a numeric action model.