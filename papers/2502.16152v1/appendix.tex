\appendix
\onecolumn 
\section{Background}
\subsection{Gaussian Process Regression}
\label{appendix:gp_background}
A Gaussian Process (GP)~\cite{williams_gaussian_1996} is a collection of random variables in which any finite subset follows a joint Gaussian distribution. Formally, a GP is specified by its mean function $m(x)$ and covariance (kernel) function $k(x, x')$. For any set of $p$ input points $x_1, \dots, x_p$, the joint distribution of the corresponding function values is
\begin{equation}
    f(x_1), \dots, f(x_p) 
    \;\sim\;
    \mathcal{N}(\mathbf{m}, \mathbf{K}),
\end{equation}
where $\mathbf{m}$ is the mean vector with entries $m(x_i)$, and $\mathbf{K}$ is the covariance matrix whose $(i,j)$-th entry is $k(x_i, x_j)$. 
Given training data $(X, y)$ and a new input $x_*$, the predictive distribution for $f(x_*)$ is:
\begin{equation}
    f(x_*) \;\bigm|\; X, y, x_* 
    \;\sim\; 
    \mathcal{N}(\mu_*, \sigma_*^2),
\end{equation}
with
\[
    \mu_* 
    = \mathbf{k}_*^T \bigl(\mathbf{K} + \sigma_n^2 \mathbf{I}\bigr)^{-1} y,
    \quad
    \sigma_*^2
    = k(x_*, x_*) 
    \;-\; \mathbf{k}_*^T\,\bigl(\mathbf{K} + \sigma_n^2 \mathbf{I}\bigr)^{-1}\,\mathbf{k}_*.
\]
Here, $\mathbf{k}_*$ is the vector of covariances between the training inputs $X$ and the new input $x_*$, $\sigma_n^2$ is the noise variance, and $\mathbf{I}$ is the identity matrix.
Key Features of Gaussian Processes:
\begin{itemize}
    \item \textbf{Flexibility:}
    GPs are non-parametric, allowing them to adapt to diverse datasets without being constrained by a fixed functional form.

    \item \textbf{Uncertainty Estimation:}
    GPs naturally quantify how certain (or uncertain) they are about each prediction, unlike many methods that provide only a point estimate.

    \item \textbf{Kernel Choice:}
    The kernel (covariance) function encodes assumptions about the underlying function. Popular options include the Radial Basis Function (RBF) and Mat\'ern kernels. Kernel parameters (e.g., length scale) are commonly learned by maximizing the log marginal likelihood.
\end{itemize}

\subsection{Optimal Transport Dataset Distance}
\label{appendix:otdd_background}
The concept of \textbf{Optimal Transport (OT)} originates from 18th-century France, where mathematician Gaspard Monge sought the most efficient method to transport soil. Consider a space $\mathcal{X}$ equipped with a probability measure $\mathcal{P}(\mathcal{X})$. For a joint measure $\pi \in \mathcal{P}(\mathcal{X} \times \mathcal{X})$, the marginals are denoted by $P_{1\#}\pi$ and $P_{2\#}\pi$, corresponding to the projection maps $P_1(x, x') = x$ and $P_2(x, x') = x'$, respectively.
Given two probability measures $\alpha, \beta \in \mathcal{P}(\mathcal{X})$, the Kantorovich formulation of the optimal transport problem is defined as:
\begin{equation}\label{eq:ot}
    \text{OT}(\alpha, \beta) \triangleq \min_{\pi \in \Pi(\alpha, \beta)} \int_{\mathcal{X} \times \mathcal{X}} c(x, x') \, d\pi(x, x'),
\end{equation}
where $\Pi(\alpha, \beta)$ denotes the set of all couplings between $\alpha$ and $\beta$:
\[
\Pi(\alpha, \beta) = \left\{ \pi \in \mathcal{P}(\mathcal{X} \times \mathcal{X}) \mid P_{1\#}\pi = \alpha, \, P_{2\#}\pi = \beta \right\},
\]
and $c: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^+$ is a cost function measuring the "transportation cost" between elements $x$ and $x'$.

\noindent A particularly important instance of optimal transport is the \textbf{squared Wasserstein distance}. When the cost function is chosen as the squared Euclidean distance $c(x, x') = \|x - x'\|^2$,
the Kantorovich formulation in Equation~\eqref{eq:ot} specializes to
\[
W_2^2(\alpha, \beta) \triangleq \min_{\pi \in \Pi(\alpha, \beta)} \int_{\mathcal{X} \times \mathcal{X}} \|x - x'\|^2 \, d\pi(x, x').
\]

\noindent Building upon OT, \citet{alvarez2020otdd} introduced the \emph{Optimal Transport Dataset Distance} (OTDD), which incorporates both features and labels of datasets. The OTDD between two datasets $D_A$ and $D_B$ is defined as:
\begin{align*}
\texttt{OTDD}(D_A, D_B) &= \min_{\pi \in \Pi(\mathbb{P}(A), \mathbb{P}(B))} \int d_Z\big( (x, y), (x', y') \big)^p \, d\pi\big( (x, y), (x', y') \big) \\
&= \min_{\pi \in \Pi(\mathbb{P}(A), \mathbb{P}(B))} \int \left[ d_{\mathcal{X}}(x, x')^p + d_{\mathcal{Y}}(y, y')^p \right] \, d\pi\big( (x, y), (x', y') \big),
\end{align*}
where $d_Z$ is a metric on the product space $Z = \mathcal{X} \times \mathcal{Y}$, $d_{\mathcal{X}}$ and $d_{\mathcal{Y}}$ are metrics on the feature space $\mathcal{X}$ and the label space $\mathcal{Y}$, respectively, and $p \geq 1$.

Let $D_A^c = \{ x \in D_A \mid y = c \}$ denote the subset of features in dataset $D_A$ associated with label $c$. Similarly, let $\mathbb{P}(D_A^y)$ represent the empirical distribution of features in $D_A$ conditioned on label $y$. With these definitions, the OTDD can be expressed as:
\[
\texttt{OTDD}(D_A, D_B) = \int \left( \| x - x' \|^p + W_p^p\big( \mathbb{P}(D_A^y), \mathbb{P}(D_B^{y'}) \big) \right)^{1/p} \, d\pi\big( (x, y), (x', y') \big),
\]
where $W_p^p\big( \mathbb{P}(D_A^y), \mathbb{P}(D_B^{y'}) \big)$ is the $p$-th power of the $p$-Wasserstein distance between the conditional feature distributions given labels $y$ and $y'$. For computational efficiency, we approximate the Wasserstein distance using the Sliced Wasserstein distance ($\SW_1$), thus setting:
\[
W_p^p\big( \mathbb{P}(D_A^y), \mathbb{P}(D_B^{y'}) \big) = \SW_1\big( \mathbb{P}(D_A^y), \mathbb{P}(D_B^{y'}) \big).
\]
This approximation enhances efficiency while maintaining a meaningful measure of the distance between datasets in terms of both their features and label distributions.

\section{Problem Formulation}\label{appendix:pos}
% \textcolor{red}{In the main paper, we have repeatedly mentioned valid kernel. So mentioning the valid word here is important. Positive semi definite too.}

\begin{definition}[Valid kernel]
A valid kernel is a function $k(\mathbf{x},\mathbf{x}')$ that corresponds to a scalar (inner) product in some (perhaps infinite dimensional) feature space.
\[
k(\mathbf{x}, \mathbf{x'}) = \Phi(\mathbf{x}){}^{\top}\Phi(\mathbf{x}).
\]
One consequence of this is that kernel functions must be symmetric, since $\Phi(\mathbf{x}){}^{\top}\Phi(\mathbf{y}) =  \Phi(\mathbf{y}){}^{\top}\Phi(\mathbf{x})$.
\end{definition}
\begin{definition}[Positive Semi-Definiteness]
A function $ k: X \times X \rightarrow \mathbb{R} $ is called a \emph{positive semi definite} if for any finite set $ \{ x_1, x_2, \dots, x_n \}  \subset X $ and any real numbers $ c_1, c_2, \dots, c_n $, it holds that:
  \[
  \sum_{i=1}^{n} \sum_{j=1}^{n} c_i c_j\, k(x_i, x_j) \geq 0.
  \]
\end{definition}

% \citet{Mercer1909} have a loose definite of the valid kernel based on positive semi-definite,
% \begin{theorem}[Mercer's Theorem \cite{Mercer1909}]\label{theo:mercer} $k(\mathbf{x}, \mathbf{x'})$ is valid if and only if for any finite set $\{x_1, ... x_n \}$, the Gram metrics of this is symmetric semi-positive definite
% \end{theorem}
Our positive semidefinite kernel definition is also referred to as positive definite kernel in \cite{Kanagawa2018GaussianPA}.

% \begin{definition}[Positive Definite Kernels]\cite{Kanagawa2018GaussianPA}\label{def:pk} Let $\mathcal{X}$ be be a nonempty set. A symmetric function $k: \mathcal{X}\times \mathcal{X} \rightarrow \mathbb{R}$ is called a positive definite kernel, if for any $n \in \mathbb{N}, (c_1, \hdots, c_n) \subset \mathbb{R}$ and $(x_1, \hdots x_n) \subset \mathcal{X},$
% \[
% \sum_{i=1}^n\sum_{j=1}^nc_ic_jk(x_i, x_j) \geq 0
% \]
% \end{definition}
% \textcolor{blue}{i feel like this definiton wrong, coz the condition is $\geq$, mean semi, not strictly positive definite itself. }

In Gaussian Process Regression, the choice of the kernel is important because it encodes our assumptions about the function we aim to learn. A positive definite kernel ensures that the covariance matrices constructed during GPR are nondegenerateâ€”that is, they have strictly positive eigenvalues and are invertible. This guarantees that the mathematical and computational procedures involved during training and prediction are well-defined. Without positive definiteness, the covariance matrix could be singular or ill-conditioned, leading to numerical instability and unreliable or undefined results.

% Note that from Remak 2.1 of \cite{Kanagawa2018GaussianPA}, the Def.~\ref{def:pk} is equivalently to Theo.~\eqref{theo:mercer}. To consistent with the proof below, we will use Def.~\ref{def:pk} as our valid kernel definition. 


% \textcolor{red}{follow \url{https://arxiv.org/pdf/1807.02582}. might not need to mention mercer's thoerem. }


% adapt and paraphrase from another paper: If the covariance function\kernel of the Gaussian process is strictly positive definite, it implies that the corresponding covariance matrix is nondegenerate and has a strictly positive least eigenvalue. This property is crucial in various classical statistical analyses, including inference, prediction, and parameter estimation. can see where positive definite is used in the gp kernel arxiv
\begin{proposition}(Proposition 14 in \cite{meunier2022slicedw} or Proposition 2.1 in \cite{Haasdonk})\label{pos:pd_kernel}.

Let $ M$ be a set and $d$ be a pseudo-distance on $M$. The following statements are equivalent:
\begin{itemize}
    \item $d$ is a Hilbertian pseudo-distance or $d$ is isometric to an $L^2-$norm.
    \item The function $ K(x, y) = e^{-\gamma d^{2\beta}(x, y)} $ is positive semidefinite for any $ \gamma \geq 0 ,  \beta \in [0, 1] $, and any $x$ and $y$ in $M$.
\end{itemize}
\end{proposition}

\subsection{Proof of Proposition~\ref{prop:sw}}
\begin{proof}
Our proof relies on the results from \cite{meunier2022slicedw}.
\begin{proposition}(Proposition 5 in \cite{meunier2022slicedw})
  The distance $\SW_2$ is Hilbertian.  
\end{proposition}
Then, applying Proposition~\ref{pos:pd_kernel}, we have  $\exp\left(-\gamma \cdot \SW_2^{2\rho}\left(\mathbb{P}(D_A), \mathbb{P}(D_B)\right)\right)$ is PSD kernel.
\begin{proposition}(Proposition~6 in \cite{meunier2022slicedw} )
  The distance $\sqrt{\SW_1}$ is a Hilbertian.  
\end{proposition}

Then, applying Proposition~\ref{pos:pd_kernel}, we have  $\exp\left(-\gamma \cdot \SW_1^{\rho}\left(\mathbb{P}(D_A), \mathbb{P}(D_B)\right)\right)$ is  a PSD kernel.

From (1), and (2) we have the proof of Proposition~\ref{prop:sw}.

% \textcolor{red}{At least reproduce the proposition here in our notation. Also, Proposition 7 may  not the correct one. Why do we care if the kernel is universal (definition before theorem 4)? It should be the hilbertian proposition (5,6) and proposition 13 in the appendix which cites another paper. so reproduce proposition 12 here, cite meunier and haasdonk. then reproduce 5 and 6 and cite meunier. }
% To begin with, 

\end{proof}
\subsection{Proof of Proposition~\ref{prop:ssw}}\label{app:proof_ssw}
\begin{proof}
\begin{align*}
\SSW_2^{2 \rho}(\mathbb{P}(D_A), \mathbb{P}(D_B); \mathcal{G}_{\eta}) &= {\SW}_2^{2\rho}(\mathbb{P}(\mathcal{G}_{\eta}(D_A)), \mathbb{P}(\mathcal{G}_{\eta}(D_B))) \\
&= {\SW}_2^{2\rho}(\mathbb{P}(Z_A), \mathbb{P}(Z_B)), 
\end{align*}    

where $Z_A = \mathcal{G}_{\eta}(A)$ and $Z_B = \mathcal{G}_{\eta}(B)$. Applying Proposition~\ref{prop:sw}, we have the result.

The proof for $\SW_1$ is the same.
\end{proof}

\subsection{Comparison of time complexity and properties of kernels}
\label{sec:A:compare}
% \textcolor{red}{What is $n$? In the paper, $n$ is the number of data owners. Here, I suspect it should be the number of data points which need a new notation.}

\begin{table}[htbp]
\centering
\caption{Time complexity to compute dataset distances and validity for various kernels. Let $k$ denote the size of the larger dataset.}
\label{tab:dis_compare}
\begin{tabular}{c|c|c}
\toprule
\textbf{Method} & \textbf{Time Complexity} & \textbf{Valid Kernel} \\ 
\midrule
\textbf{OTDD} & \( O(k^3 \log k) \) & \xmark  \\ 
\textbf{SW}   & \( O(k \log k) \)   & \cmark \\ 
% \textbf{MMD}  & \( O(n^2) \)        & \cmark \\ 
\textbf{SSW}  & \( O(k \log k) \)   & \cmark \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Semivalue Estimation}
\subsection{Proof of Proposition~\ref{pos:shapley-dist}}

\begin{proof}

\textbf{Shapley Value.}
From Equation~\eqref{eq:shapley}, we rewrite
\begin{align*}
 \phi_u(i) &\triangleq \sum_{C \subseteq N \setminus i} \frac{1}{n} {\binom{n-1}{|C|}}^{-1} \left[ u(C \cup \{i\}) - u(C) \right]\\
 &= \sum_{C \subseteq N \setminus \{i\}} \left( \frac{1}{n} \binom{n - 1}{|C|} \right)^{-1} u(C \cup \{i\}) + \sum_{C \subseteq N \setminus \{i\}} \left( -\frac{1}{n} \left( \binom{n - 1}{|C|} \right)^{-1} \right) u(C)\ .\\
\end{align*}

Let $\mathcal{P}(N)$ denote the power set of $N$. Let the vector $\mathbf{u}_{\mathcal{P}(N)}$ be the values $u(C)$ for each coalition $C \subseteq N$ and $\mathbf{w}_{\mathcal{P}(N)}^{i}$ be the vector of corresponding weights for player $i$. Then, the Shapley value of player $i$ can be written compactly as
$ \phi_u(i) = \mathbf{w}_{\mathcal{P}(N)}^{i\,\top} \mathbf{u}_{\mathcal{P}(N)} $.
For each coalition $C_j \subseteq N$, the $j$-th entry of $\mathbf{w}_{\mathcal{P}(N)}^{i}$ is defined as follows:
\[
\left( \mathbf{w}_{\mathcal{P}(N)}^{i} \right)_j =
\begin{cases}
\dfrac{1}{n \binom{n-1}{|C_j|-1}} & \text{if } i \in C_j, \\
-\dfrac{1}{n \binom{n-1}{|C_j|}} & \text{if } i \notin C_j.
\end{cases}
\]

\textbf{Semivalue.}
From Equation~\eqref{eq:semi}, we rewrite
\begin{align*}
\varphi_u(i) & \triangleq \sum_{C \subseteq N \setminus i}\omega_{|C|} \left[ u(C \cup \{i\}) - u(C) \right] \\
&= \sum_{C \subseteq N \setminus \{i\}} \omega_{|C|} u(C \cup \{i\}) + \sum_{C \subseteq N \setminus \{i\}} \left(-\omega_{|C|}\right) u(C).
\end{align*}
The semivalue can also be written compactly as $\varphi_u(i) = \mathbf{w}_{\mathcal{P}(N)}^{i\,\top} \mathbf{u}_{\mathcal{P}(N)} $
where the $j$-th entry of the weight vector $\mathbf{w}_{\mathcal{P}(N)}^{i}$ is now:
\[
\left( \mathbf{w}_{\mathcal{P}(N)}^{i} \right)_j =
\begin{cases}
\omega_{|C_j|-1} & \text{if } i \in C_j, \\
-\omega_{|C_j|} & \text{if } i \notin C_j.
\end{cases}
\]

\textbf{Both.}
We can partition $\mathcal{P}(N)$ into two sets of coalitions $\cA$ and $\cB$ such that $\hat{\phi}_i$ or $\hat{\varphi}_i$ is equals to 
\[
\mathbf{w}_\cA^{i\,\top} \mathbf{u}_\cA + \mathbf{w}_{\cB}^i{}^\top \hat{\mathbf{u}}_{\cB|\cA} .
\]
Based on the GP model, the predicted utilities $\hat{\mathbf{u}}_{\cB|\cA}$ are distributed according to a multivariate Gaussian distribution. Thus, $\hat{\phi}_i$ and $\hat{\varphi}_i$ are also Gaussians.
As we use the evaluated utilities $\mathbf{u}_\cA$ of coalitions in $\cA$ and only predict the utilities $\hat{\mathbf{u}}_{\cB|\cA}$ of coalitions in $\cB$, the variance in $\hat{\phi}_i$ is only from the latter.
$\hat{\phi}_i$ and $\hat{\varphi}_i$ are Gaussians with the following mean and variance
$\mathcal{N}( \mathbf{w}_{\cA}^i{}^\top \mathbf{u}_{\cA} + \mathbf{w}_{\cB}^i{}^\top \mathbb{E}[\hat{\mathbf{u}}_{\cB|\cA}], \mathbf{w}_{\cB}^i{}^\top \mathbb{V}[\hat{\mathbf{u}}_{\cB|\cA}] \mathbf{w}_{\cB}^i ) \ .$

\end{proof}

\subsection{Discussion about Remark~\ref{rem:estimate}}\label{app:uncer_dis}
 Consider the case where our method applies Monte Carlo estimates of Shapley value to limit the number of coalitions needed for evaluation. In this scenario, the uncertainty of the Shapley value or semivalue will be bounded as $\mathbb{V}[\hat{\phi}_i']  \leq \sigma_{GP}^2 + \sigma_{MC}^2 + 2 \sigma_{GP} \sigma_{MC}$.
\begin{align*}
    \mathbb{V}[\hat{\phi}_i'] &= \mathbb{E}[(\hat{\phi}_i' - \phi_i)^2] \\
    &= \mathbb{E}[((\hat{\phi}_i' - \phi'_i) + (\phi'_i - \phi_i))^2] \\
    &= \underbrace{\mathbb{E}[(\hat{\phi}_i' - \phi'_i)^2]}_{\sigma_{GP}^2} +   \underbrace{\mathbb{E}[(\phi'_i - \phi_i)^2]}_{\sigma_{MC}^2} \\
        &+ 2\mathbb{E}[(\hat{\phi}_i' - \phi'_i)(\phi'_i - \phi_i)] \\
    & \leq \sigma_{GP}^2 + \sigma_{MC}^2 + 2 \sigma_{GP} \sigma_{MC} .
\end{align*}


\subsection{Efficiently updating the Inverse in  Algorithm~\ref{alg:active}\label{appdix:algo2}}
According to \citet{bernstein2009matrix}, Proposition 3.9.7, we have:
\begin{equation}\label{eq:inverse}
 M^{-1} = 
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1} = \begin{bmatrix}
A^{-1} + A^{-1} B S^{-1} C A^{-1} & -A^{-1} B S^{-1} \\
- S^{-1} C A^{-1} & S^{-1}
\end{bmatrix} , 
\end{equation}
where $S = D - C A^{-1} B$ is the Schur complement of $A$ in $M$.\\
In our case, when adding a new coalition $C$ to set coalition $\mathcal{A}$. We have
\[
K_{{\mathcal{A} \oplus C, \mathcal{A} \oplus C}} = \begin{bmatrix}
K_{\mathcal{A}, \mathcal{A}} & k(\mathcal{A}, C) \\
k(C, \mathcal{A}) & k(C,C) + \sigma^2
\end{bmatrix}.
\]
Then, our inverse matrix will be calculated incrementally based on Equation~\eqref{eq:inverse}.

\subsection{Complementing existing CGT-based data valuation approximations}\label{appdix:samp_algo}
How can \texttt{DUPRE} complement existing CGT-based data valuation (e.g. Shapley) approximation methods?
These Monte Carlo approximations may require the evaluation of utilities of coalitions in $\mathcal{C}$ where $|\mathcal{C}| \ll 2^n$.

We can relate $\mathcal{C}$ to the actually evaluated $\cA$ and predicted $\cB$ in Figure~\ref{fig:framework} by considering two perspectives. 
One perspective is that we can set $\mathcal{A} = \mathcal{C}$. Thereafter, we can predict the utilities of more coalitions in $\mathcal{B}$ and use them to compute another Shapley valuation approximation with more sampled coalitions.
Another perspective is that $\mathcal{C}$ is partitioned into $\mathcal{A}$ and $\mathcal{B}$. We only evaluate a subset of the coalitions and predict the remaining coalitions.

The permutation algorithm is considered in Algorithm~\ref{algo:permu_samp} and Algorithm~\ref{algo:shapley_permu_samp}. 

\begin{algorithm}[ht]
\caption{Sampling based on Permutation Sampling Algorithm}\label{algo:permu_samp}
\begin{flushleft}
\textbf{Input}: The number of coalitions to sample $n_C$, number of data owners $n_d$\\
\textbf{Output}: Selected coalitions $\mathcal{C}$, and selected permutations $\mathcal{R}$
\end{flushleft}
\begin{algorithmic}[1]
    \STATE Initialize: $\mathcal{C} \gets \emptyset$
    \STATE Initialize: $\mathcal{R} \gets \emptyset$
    \WHILE{$|\mathcal{C}| < n_C$}
        \STATE Sample a permutation $\pi$ of data owners from the uniform distribution over all permutations of $1, \cdots, n_d$
        \STATE $\mathcal{R} \gets \mathcal{R} \cup \{\pi\}$
        \STATE $S \gets \emptyset$
        \FOR{data owner $i$ in $\pi$}
            \STATE $S \gets S \cup \{\text{i}\}$
            \STATE $\mathcal{C} \gets \mathcal{C} \cup \{S\}$  
        \ENDFOR
    \ENDWHILE

\STATE return $\mathcal{C}, \mathcal{R}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Shapley Estimate based on Permutation Sampling Algorithm}\label{algo:shapley_permu_samp}
\begin{flushleft}
\textbf{Input}: List of all permutations $\mathcal{R}$, list of actually evaluated coalitions $\cA$, list of predicted coalitions $\cB$, the utility function $u$\\
\textbf{Output}: List of Shapley values $(\phi_u(i))_{i \in N}$
\end{flushleft}
\begin{algorithmic}[1]
    \STATE Actually evaluate the utilities $\mathbf{u}_\cA \triangleq (u(A))_{A \in \cA}$ for each coalition $A$ in $\cA$
    \STATE Predict the utilities $ \mathbb{E}[\mathbf{\hat{u}}_{\cB|\cA}] =
  \mathbf{K}_{\cB,\cA} [\mathbf{K}_{\cA,\cA} + \sigma^2 \mathbf{I}]^{-1} \mathbf{u}_\cA $ for coalitions in $\cB$
    \FOR{$\pi$ in $\mathcal{R}$}
        \STATE $prev \gets 0$
        \STATE $S \gets \emptyset$ % Initialize the coalition set
        \FOR{$i$ in $\pi$}
            \STATE $S \gets S \cup \{i\}$ % Add player to coalition
            \IF{$S \in \cA$}
            \STATE $curr \gets u(S)$ saved in $\mathbf{u}_\cA$ 
            \ELSIF{$S \in \cB$}
            \STATE $curr \gets \hat{u}(S)$ saved in $ \mathbb{E}[\mathbf{\hat{u}}_{\cB|\cA}]$
            \ENDIF
            \STATE $\phi_{u}(i) \gets \phi_{u}(i) + (curr - prev) / |\mathcal{R}|$
            \STATE $prev \gets curr$
        \ENDFOR
    \ENDFOR
    % \STATE $\phi \gets \phi / |\mathcal{R}|$
\STATE return $(\phi_u(i))_{i \in N}$
\end{algorithmic}
\end{algorithm}

\clearpage


\section{Experiments}
\label{appendix:exp}

\subsection{Detailed Experiment Setup}

\textbf{Datasets and Models} Table~\ref{tab:datasize_per_owner} summarizes the details of the datasets, models, and hardware used in each experiment. Specifically, NN refers to a neural network architecture with three hidden layers, and MLP stands for a Multi-Layer Perceptron regressor with three hidden layers and two ReLU activation layers. For the MNIST custom data division, based on different numbers of data owners, the division will be \{0,1\}, \{2,3,4\}, \{3,4,5\}, \{6,7\}, \{8,9\} and $\{1, 4, 5, 7, 8\}, \{2\}, \{9\}, \{6\}, \{0\}$ 

\textbf{Software}: We use Python libraries, including, PyTorch and \texttt{pyDVL} \cite{TransferLab_team_pyDVL_2024}.

\textbf{Hardware}: We primarily run experiments on NVIDIA GeForce RTX 3080 (10GB) and NVIDIA L40 (40GB) GPUs.

\textbf{Training Procedure}: The ML model was trained for 100 epochs with a learning rate of 0.001. For CaliH and Moon Dataset, we use full-batch training. In contrast, for the CIFAR-10 and MNIST datasets, the batch sizes were set to 256 and 64, respectively. We run every experiment ten times with seed from 0 to 9.
\begin{table}[h]
\centering
\caption{Overview of Experiments and Datasets}
\label{tab:datasize_per_owner}
\begin{tabular}{|l|c|c|c|c|c|l|}
\hline
\textbf{Dataset} & \textbf{Num data-owners} & \textbf{Data Size per Owner}&  \textbf{Train/Valid} &  \textbf{Division} &  \textbf{ML model} & \textbf{Exp} \\ \hline
MNIST            & 10                             & 6,000                    & 60k/10k& per digit&  NN&   Section~\ref{sec:est_sampling_shapley} \\ \hline
CaliH            & 6                              & 2,700                    & 16k/4k  & random &  MLP& Section~\ref{sec:cup}\\ \hline
MNIST            & 6                             & 20,000 or 6,000            & 60k/10k&  custom &   NN&
Section~\ref{sec:cup}\ \\ \hline
MNIST            & 5                              & 12,000 (lowest) or 18,000    & 60k/10k  & custom &  NN& Section~\ref{sec:benfun} \& Section~\ref{subsec:ablation} \\ \hline
CIFAR            & 8                              & 5,000 or 10,000      & 50k/10k  & per label& Resnet-18 &Section~\ref{sec:exp_exact_shap}\\ \hline
IMDb            & 10                              & 2,500    & 25k/25k  & random& Resnet-18 &Section~\ref{subsec:nlp-datasets}\\ \hline
IMDb            & 20                              & 100 to 2000    & 25k/29k  & custom& Resnet-18 &Section~\ref{subsec:imdb-robustness}\\ \hline
\end{tabular}
\end{table}

\subsection{Experiments on an Additional NLP Dataset (SST-2)}
\label{appendix:SST-2}
In this section, we conducted additional experiments on the Stanford Sentiment Treebank (SST-2) involves classifying the sentiment of movie reviews as either positive or negative. We use a similar setting to Section~\ref{subsec:nlp-datasets} on 10 data owners. Following the OpenDataVal benchmark, we use DistilBERT embeddings for each review. We consider two evaluation setups:
\begin{itemize}
  \item \textbf{Setup~1 (Section \ref{sec:cup})} uses 256 evaluated coalitions and is used to study the agreement between the actual utility $u(C)$ and GP predicted utility $\hat{u}(C)$ of various coalitions $C$.
  \item \textbf{Setup~2 (Section \ref{sec:est_sampling_shapley})} uses 512 evaluated and 100 predicted coalitions and is used to study the agreement between the actual Shapley value $(\phi_i)_{i \in N}$ and the predicted Shapley value $(\hat{\phi}_i)_{i \in N}$. 
\end{itemize}

\begin{table*}[h!]
\centering
\caption{
\textbf{A comparison of the quality of utility predictions and Shapley value predictions for various methods on the SST-2 dataset with $10$ owners.}
\textbf{Setup~1 (similar to Section \ref{sec:cup})} compares the actual utility $u(C)$ with GP predicted utility $\hat{u}(C)$ of various coalition $C$.
\textbf{Setup~2 (similar to Section \ref{sec:est_sampling_shapley})} compares the actual Shapley value $\phi_i$ and the predicted Shapley value $(\hat{\phi}_i)$ of various owner $i$. 
The results are the mean $\pm$ std.\ over 5 runs.
Lower MSE and higher correlation are preferred.
}
\label{tab:sst2-results}
\begin{tabular}{l|ccc|cc}
\toprule
& \multicolumn{3}{c|}{\textbf{Setup 1 (Section \ref{sec:cup})}} 
& \multicolumn{2}{c}{\textbf{Setup 2 (Section \ref{sec:est_sampling_shapley}) }} \\
\textbf{Method} 
& $\text{MSE}(u(C), \hat{u}(C))$ 
& $\text{Pearson}\bigl(u(C), \hat{u}(C) \bigr)$
& $\text{Shapley Corr.}$
& $\text{MSE}((\phi_i)_{i \in N}, (\hat{\phi}_i)_{i \in N})$
& $\text{Pearson}((\phi_i)_{i \in N}, (\hat{\phi}_i)_{i \in N})$ \\
\midrule
\texttt{SSW (Ours)} 
& \(\mathbf{2.024\times10^{-5}\pm0.00012}\)
& \(\mathbf{0.93\pm0.04}\)
& \(\mathbf{0.91\pm0.06}\)
& \(\mathbf{0.0002\pm0.0038}\)
& \(\mathbf{0.50\pm0.234}\)\\

\texttt{GP-binary}  
& \(2.638\times10^{-5}\pm0.00018\)
& \(0.733\pm0.05\)
& \(0.84\pm0.06\)
& \(0.00033\pm0.0042\)
& \(0.35\pm0.26\) \\

\texttt{NN-binary}  
& \(0.005\pm0.007\) 
& \(0.422\pm0.19\)
& \(0.334\pm0.29\)
& \(0.00025\pm0.007\)
& \(0.37\pm0.186\) \\
\bottomrule
\end{tabular}
\end{table*}

In Table~\ref{tab:sst2-results}, \texttt{SSW} still leads to the lowest MSE and the highest correlation as compared to other kernels.

\subsection{Experiments on Another Semivalue: Banzhaf value}
\label{appendix:banzhaf}
We reuse the setup of Section~\ref{sec:exp_exact_shap} but consider computing the Banzhaf value instead of the Shapley value. In Table~\ref{tab:compare_corr_banzhaf}, we observe that the kernel based on $\texttt{SSW}$ leads to the highest correlation between the actual and predicted Banzhaf values.
\begin{table*}[ht]
\caption{A comparison of the quality of exact Banzhaf value approximation on the CIFAR-10 and CaliH datasets with $8$ data owners. A higher correlation is preferred.}
\centering
\label{tab:compare_corr_banzhaf}
\begin{tabular}{l|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} 
    & \multicolumn{2}{c}{\textbf{CIFAR-10}} 
    & \multicolumn{2}{c}{\textbf{CaliH}} \\ 
    & \textbf{Pearson} & \textbf{Kendall Tau} 
    & \textbf{Pearson} & \textbf{Kendall Tau}\\ 
    \midrule
    \texttt{SSW} &  $\mathbf{0.98 \pm 0.016} $    & $\mathbf{0.70 \pm 0.15}$ &     $\mathbf{0.97 \pm 0.02}$ & $\mathbf{0.9 \pm 0.069}$\\ 
    \texttt{OTDD} (invalid kernel baseline) &  $0.69 \pm 0.082$&  $0.52 \pm 0.14$ &   - &-    \\ 
    \texttt{GP-binary} (baseline 01 encoding) & $0.92 \pm 0.03$ &  $0.66 \pm 0.18$ & $0.95 \pm 0.06$ & $0.85 \pm 0.11$  \\ 
    \texttt{NN-binary} (baseline 01 encoding) &  $0.65 \pm 0.01$   & $0.57 \pm 0.14$  & $0.87 \pm 0.09$ & $0.72 \pm 0.062$ \\ 
    \texttt{LAVA} (baseline) &  $0.0034 \pm 0.045$& $-0.078 \pm 0.1$ & $0.135 \pm 0.147$ &  $0.028 \pm 0.15$   \\ 
    \bottomrule
\end{tabular}
\end{table*}