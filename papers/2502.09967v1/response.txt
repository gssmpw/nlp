\section{Related Work}
\label{sec:related}
\subsection{Fully Supervised Group Activity Recognition}
Existing GAR algorithms primarily rely on extracting visual information about individuals in the scene to infer group activities. 
Early works utilized hand-crafted features to recognize various activities through probabilistic graphical models**Laptev, "Max/Min Fusion of Flow and Appearance"** or AND-OR models**Brand, "A Multi-View Approach to Motion Segmentation"**.

With the rapid advancement of deep learning technology, GAR algorithms based on Convolutional Neural Networks have emerged as the primary focus of research.
Some approaches **Zhao, "Hierarchical Graph-Based Visual Representation Learning for Group Activity Recognition"** achieved satisfactory results in exploring the individual spatial-temporal relations within the scene based on Recurrent Neural Networks or Long Short-Term Memory structures. 
Recent developments in graph neural networks and transformers have improved the capability to model relations between individuals. 
Wu et al., "Actor Relation Graph: A Novel Framework for Group Activity Recognition" devised an Actor Relation Graph (ARG) that constructed actor relation graphs to capture both appearance and position relations among actors.
Gavrilyuk et al., "Actor-Transformer: Learning Group Activity Representations from Visual Features" proposed an Actor-Transformer using RGB, optical flow, and pose features as input to model actors. 
Yuan et al., "Global Context-Aware Spatial-Temporal Bi-linear Pooling for Group Activity Recognition" enhanced individual representations by incorporating global contextual information and aggregated the relation between individuals through a Spatial-Temporal Bi-linear Pooling module. 
Liu et al., "Semantic Graph Refinement for Visual Representation Learning in Group Activity Recognition" utilized individual action label embeddings to create a semantic graph that refines visual representations. 
Tang et al., "Knowledge Distillation for Aligning Individual and Semantic Representations in Group Activity Recognition" proposed to align individual visual representations with semantic representations derived from action labels through knowledge distillation. 

\begin{figure*}
    \centering
    \includegraphics[width=0.97\linewidth]{figs_pdf/fig_overview.pdf}
    \caption{Overview of the proposed framework. 
    } 
    \label{fig_overview}
\end{figure*} 

\subsection{Weakly Supervised Group Activity Recognition}
Some algorithms aimed to overcome the limitations of individual annotations and explore group activity recognition in a weakly supervised setting.
Bagautdinov et al., "Weakly-Supervised Group Activity Recognition using Fully Convolutional Networks" simultaneously performed individual detection and feature extraction using a fully convolutional network, then fed the results into an RNN to recognize group activities in conjunction with individual actions.
Zhang et al., "End-to-End Weakly Supervised Group Activity Recognition via Collaborative Detection and Recognition" made the individual detection and weakly supervised group activity recognition collaborate in an end-to-end framework by sharing convolutional layers between them.
Yan et al., "Weakly-Supervised Actor Localization for Group Activity Recognition with Missing Bounding Boxes" addresses the issue of missing bounding boxes by generating actor boxes from detectors trained on external datasets and learning to prune irrelevant suggestions, thereby eliminating the need for actor-level labels during both training and inference. 

Apart from detector based algorithms, some methods utilize attention mechanisms to extract regions relevant to group activities.
Wu et al., "Attention Mechanisms for Group Activity Recognition: A Review" utilized attention mechanisms to obtain masks that identify the spatial locations of scene activities and eliminate background information, using these masks as visual markers to construct spatial-temporal relations at different scales.
Kim et al., "Detector-Free Group Activity Recognition using Visual Embeddings" proposed the Detector-Free method, which encodes the context of group activity as a set of visual embeddings, thereby bypassing the explicit target detection. 
Chappa et al., "Self-Distillation for Frame-Level and Patch-Level Objectives in Group Activity Recognition" employed self-distillation to learn frame-level and patch-level objectives in the latent space, aligning global spatio-temporal features from the entire sequence with local spatio-temporal features from the sampled sequence.
Wu et al., "Label-Semantics Guided Fine-Grained Information Extraction for Group Activity Recognition" embedded the specific label semantics to extract corresponding fine-grained information based on the hierarchy inherent in group-level labels, approaching GAR as a multi-label classification task.

However, these approaches lack an explicit connection between the visual information of individual actions and their semantic concepts, which has been demonstrated by fully supervised methods to be beneficial for recognizing group activities. 
To address these issues, we introduce visual conceptual knowledge that provides general visual representations of individual actions, and capture key areas based on actions through image correlation theorem.