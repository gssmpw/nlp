%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


\input{math_commands}
% \usepackage{hyperref}
\usepackage{url}
% \usepackage{booktabs}
% \usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Scalable In-Context Learning on Tabular Data via Retrieval-Augmented Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{Scalable In-Context Learning on Tabular Data\\ via Retrieval-Augmented Large Language Models}
% \icmltitle{Scalable In-Context Learning on Tabular Data via Retrieval-Augmented LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
% \icmlsetsymbol{intern}{*}

\begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
% \icmlauthor{}{sch}
\icmlauthor{Xumeng Wen}{msra}
\icmlauthor{Shun Zheng}{msra}
\icmlauthor{Zhen Xu}{intern,uc}
\icmlauthor{Yiming Sun}{intern,up}
\icmlauthor{Jiang Bian}{msra}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlaffiliation{msra}{Microsoft Research Asia, Beijing, China}
\icmlaffiliation{uc}{The University of Chicago, Chicago, IL, USA}
\icmlaffiliation{up}{University of Pittsburgh, Pittsburgh, PA, USA}
\icmlaffiliation{intern}{Zhen and Yiming contributed to this study during their internship at Microsoft Research Asia.}

\icmlcorrespondingauthor{Shun Zheng}{shun.zheng@microsoft.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Tabular data, in-context learning, large language models}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains.
However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens.
To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data.
Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs.
This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior.
Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. 
These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning.
% \footnote{Code and model checkpoints will be publicly available.}
\end{abstract}

\input{sections/0_intro}
\input{sections/1_related_work}
\input{sections/2_method}
\input{sections/3_experiment}
\input{sections/4_conclusion}

\section*{Impact Statement}

This study has the potential to make significant contributions to multiple research communities and practical domains.

For the tabular learning community, this study introduces a novel paradigm for performing TabICL by leveraging text representations of tabular data. This paradigm demonstrates the capability to process tabular datasets across diverse domains and scales, from zero-shot and few-shot to any-shot scenarios. By moving beyond numeric-only representations, our approach opens new avenues for integrating tabular learning into broader, more flexible frameworks, making it possible to unify methodologies across heterogeneous datasets. Furthermore, our retrieval-augmented mechanism provides an adaptable framework for balancing performance and scalability, which may inspire future innovations in retrieval strategies and model architectures.

For the LLM community, this study demonstrates the potential of LLMs to move beyond traditional language-based applications to data understanding and learning tasks involving structured, numeric data. Our findings reveal that LLMs, when augmented with TabICL capabilities, can generalize effectively to tabular datasets and produce competitive results. This highlights the possibility of extending LLMs to domains where structured data plays a central role, such as finance, healthcare, and agriculture. By bridging the gap between text-based and numeric-based data representations, our approach paves the way for a new class of multimodal LLMs capable of unifying text and tabular data learning.

The integration of TabICL capabilities into LLMs may also have ethical and societal implications. On the positive side, these models can democratize access to advanced analytics, enabling users with minimal technical expertise to analyze and understand complex datasets through natural language queries. However, there are potential risks associated with misuse or unintended consequences, such as over-reliance on LLMs for critical decisions, inaccuracies in predictions due to biased training data, and challenges in ensuring transparency and accountability. Researchers and practitioners must prioritize the development of robust evaluation methods, ethical safeguards, and explainability mechanisms to mitigate these risks and ensure responsible deployment.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{sections/5_appendix}

\end{document}
