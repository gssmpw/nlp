\section{Formulations of TabICL}
\label{sec:formulation}

We begin by establishing formulations of the TabICL problem, with a focus on the differences in data representation between TabPFN- and LLM-based methodologies.

\paragraph{Notations}
Let $\mathcal{T}$ denote the family of all tabular learning tasks. A typical task $T \sim \mathcal{T}$ is defined as a mapping from an input variable $x^T \in \R^{h_x^T}$ to a label variable $y^T \in \R^{h_y^T}$, expressed as $T: x^T \mapsto y^T$, where $h_x^T$ and $h_y^T$ represent the dimensions of the feature and label spaces, respectively.
We denote the joint distribution of $x^T$ and $y^T$ by $D^T$, such that $(x^T, y^T) \sim D^T$. In practice, a training dataset is constructed by sampling $N$ observations from $D^T$, which we denote as $D^T_{\text{train}} = \{(x^T_i, y^T_i)\}_{i=1}^N$. The training set $D^T_{\text{train}}$ serves as an empirical approximation of the underlying distribution $D^T$.
The goal of traditional supervised learning is to derive an optimized model based on $D^T_{\text{train}}$ that accurately characterizes the conditional distribution $P(y^T_{\text{test}} | x^T_{\text{test}})$ for new samples $(x^T_{\text{test}}, y^T_{\text{test}}) \sim D^T$.

% We denote the family of all tabular learning tasks as $\mathcal{T}$.
% For a typical task $T \sim \mathcal{T}$, it defines a mapping from a input variable $x^T \in \R^{h_x^T}$ to a label variable $y^T \in \R^{h_y^T}$ as $T: x^T \rightarrow y^T$, where $h_x^T$ and $h_y^T$ denote the feature and label dimensions, respectively.
% We use $D^T$ to denote the joint distribution of $x^T$ and $y^T$, so we have $(x^T, y^T) \sim D^T$.
% In practice, we typically sample $N$ observations from $D^T$ to construct a training set, denoted as $D^T_{train} = \{ (x^T_i, y^T_i)\}_{i=1}^N$.
% The training set $D^T_{train}$ is regarded as an empirical distribution for $D^T$.
% Traditional supervised learning aims to obtain an optimized model based on $D^T_{train}$ that can characterize the underlying conditional distribution $P(y^T_{test} | x^T_{test})$, where $(x^T_{test}, y^T_{test}) \sim D^T$.

\paragraph{TabICL}
Similar to ICL in language tasks~\citep{Brown2020GPT-3}, TabICL regards the training set $D^T_{train}$ as a set of in-context instances.
Specifically, TabICL aims to characterize the new conditional distribution $P(y^T_{test} | x^T_{test}, D^T_{train})$, namely the label distribution of a testing example conditioned on its own features and other in-context instances, for any task $T$ and empirical observations sampled from $D^T$.
To this end, we need a parameterized model $Q_\theta$ to maximize the following log-likelihood:
\begin{align}
    \E_{ (x^T_{\cdot}, y^T_{\cdot}) \sim D^T, T \sim \mathcal{T} } \left[ \log Q_\theta \left( y^T_{test} | x^T_{test}, D^T_{train} \right) \right].
    \label{eq:TabICL}
\end{align}
In this way, we stimulate $Q_\theta$ to perform TabICL for various tasks.
During the inference stage, $Q_\theta(\cdot | x^{T'}_{test}, D^{T'}_{train})$ directly delivers the label distribution of $y^{T'}_{test}$ for any new task $T' \sim \mathcal{T}$ via a forward pass.

However, it is challenging to design $Q_\theta$ because of the dataset heterogeneity across different tabular learning tasks, namely the feature and label spaces ($\R^{h_x^T} \times \R^{h_y^T}$) differ significantly across different tasks.
To address this challenge, TabPFN models~\citep{hollmann2023TabPFN,hollmann2025TabPFNv2} impose maximal limits on feature and prediction dimensions, constructing separate models for classification and regression tasks.
These design choices ensure broad compatibility with most tabular datasets encountered in practice.

\paragraph{TabICL with LLMs}
In contrast to TabPFN models, which utilize numeric representations of tabular data, recent LLM-based TabICL approaches~\citep{wen2024GTL,gardner2024TabuLa} rely on a serialization function~\citep{dinh2022LIFT,hegselmann2023TabLLM} to convert tabular data into language-based instructions. Consequently, their training process can be framed as maximizing a modified log-likelihood function with an LLM model $Q^{LLM}_{\theta}$:
\begin{align}
    % \E_{T \sim \mathcal{T} \\ (x^T_{\cdot}, y^T_{\cdot}) \sim D^T}
    \E_{(x^T_{\cdot}, y^T_{\cdot}), T}
    \left[ \log Q^{LLM}_\theta \left( S_{y} \left( y^T_{test} \right) \middle\vert S_{x} \left( x^T_{test}, D^T_{train} \right) \right) \right],
    \label{eq:TabICL_LLM}
\end{align}
where $S_{y}(\cdot)$ and $S_{x}(\cdot, \cdot)$ are serialization functions that convert tabular data (including heterogeneous numerical and categorical features, as well as variable-sized in-context instances) into sequences of language tokens based on configurable templates. Depending on the chosen configuration, tabular data can be represented in language-like or Markdown-style formats, with the option to exclude $D^T_{\text{train}}$ for enabling zero-shot learning.

\paragraph{Challenges in TabICL with LLMs}
While ongoing efforts aim to scale the context lengths of LLMs~\citep{xiong2023effective,abdin2024phi3}, modern LLMs continue to face significant challenges, including substantial increases in computational overhead~\citep{liu2024RingAttn} and unexpected performance degradation in long-context scenarios~\citep{li2024LongICLBench}.
In the context of TabICL, the challenge of long contexts is particularly pronounced, as tabular instances typically comprise multiple numerical and categorical features, which consume a large number of tokens when represented in a language format. Consequently, the limited context length of $Q^{LLM}_\theta$ severely constrains the number of in-context instances that can be incorporated into $D^T_{\text{train}}$.

% $S_{y}(\cdot)$ and $S_{x}(\cdot, \cdot)$ are serialization functions to transform tabular data composed of heterogeneous numerical and categorical features into a sequence of language tokens, following some configurable templates. 
% Given different configurations, we can represent tabular data in language-alike or Markdown-syntax formats and exclude $D^T_{train}$ to enable zero-shot learning.
% Besides, we use $S_y^{-1}(\cdot)$ to denote a decoding function that transforms the generated tokens from $Q^{LLM}_\theta$ into desired outputs in numerical or categorical representations, so we have $y^T_{test} = S_y^{-1}(S_{y}(y^T_{test}))$.

% Although there are continuing efforts on scaling context lengths of LLMs~\citep{xiong2023effective,abdin2024phi3}, modern LLMs still suffer from significantly increased computational workloads~\citep{liu2024RingAttn} and unexpected performance degradation~\citep{li2024LongICLBench} in long-context scenarios.
% Regarding TabICL, the long-context challenge becomes more serious because a tabular instance usually involves multiple numerical and categorical features, which take up many tokens in a language format.
% Therefore, a limited context length of $Q^{LLM}_\theta$ greatly restricts the number of in-context examples that can be included in $D^T_{train}$.

\section{TabICL with Retrieval-Augmented LLMs}
\label{sec:method}

To enable scalable TabICL on any-shot settings, we develop a decoupled formulation for~\eqref{eq:TabICL_LLM} as
\begin{align}
\begin{split}
    % \E_{T \sim \mathcal{T}, (x^T_{\cdot}, y^T_{\cdot}) \sim D^T}
    \E_{(x^T_{\cdot}, y^T_{\cdot}), T}
    &\left[ \log Q^{LLM}_\theta \left( S_{y} \left( y^T_{test} \right) \middle\vert S_{x} \left( x^T_{test}, C_{x^T_{test}} \right) \right) \right], \\
    C_{x^T_{test}} &= \texttt{TabRAG} \left( x^T_{test}, D^T_{train}  \right),
    \label{eq:TabICL_LLM_RAG}
\end{split}
\end{align}
where $C_{x^T_{test}}$ denotes a set of in-context instances selected to support the prediction of $x^T_{test}$, with the size fixed as $N^C$, and we use \texttt{TabRAG} to denote the retrieval module for tabular data that accepts a query example, $x^T_{test}$, and a training set of any scale, $D^T_{train}$, as inputs.

Our formulation decouples essential capabilities for scalable TabICL into two distinct modules, addressing the constraint of limited context length.
The first module, $Q^{LLM}_\theta$, is responsible for predicting the desired outcomes for a query instance $x^T_{\text{test}}$ based on its highly relevant contexts $C_{x^T_{\text{test}}}$.
The second module, \texttt{TabRAG}, focuses on identifying the most relevant and supportive reference instances to facilitate predictions by $Q^{LLM}_\theta$.
To achieve competitive TabICL performance across any new task and any-shot settings, our approach offers opportunities for further improvement in three key areas: 1) the design of \texttt{TabRAG}, 2) TabICL of $Q^{LLM}_\theta$, and 3) the alignment between \texttt{TabRAG} and $Q^{LLM}_\theta$.

\paragraph{The Design of \texttt{TabRAG}}
The primary challenge in designing \texttt{TabRAG} lies in developing a universal retrieval protocol capable of addressing the inherent heterogeneity of tabular data. This includes accommodating variable-sized feature columns, diverse feature types, inconsistent feature importance, and the presence of noise or irrelevant features.
To address these challenges, we propose a non-parametric \texttt{TabRAG} module based on weighted feature-wise similarities as a default retrieval policy, which can be applied to any tabular dataset. Specifically, this requires proper normalization of each feature column and efficient quantification of feature importance, with implementation details provided in Appendix~\ref{app:rag_method}.
In the meanwhile, we observe that a non-parametric, universal retrieval policy may select sub-optimal in-context instances in certain cases, potentially deteriorating TabICL performance.
For instance, as highlighted in case studies of Appendix~\ref{app:case_study}, some slightly adjusted retrieval strategies can significantly boost performance in certain cases.
Therefore, we recommend integrating our basic \texttt{TabRAG} with domain knowledge and data insights in practical applications to further enhance its effectiveness.
We anticipate that ``retrieval engineering'' could emerge as a critical skill for optimizing LLM-based TabICL, akin to the importance of prompt engineering in transforming natural language processing~\citep{liu2023prompt_survey}.

\paragraph{TabICL with $Q^{LLM}_\theta$}
Enhancing the foundational TabICL capabilities remains a key objective in advancing tabular learning with LLMs.
In this work, we upgrade the base LLM used by~\citeauthor{wen2024GTL} from LLaMA-2~\citep{touvron2023llama2} to Phi-3~\citep{abdin2024phi3}, extending the maximum sequence length from 4K to 128K.
Our findings demonstrate the clear benefits of leveraging longer contexts, enabling more effective TabICL performance.
As this study primarily focuses on highlighting the importance and effectiveness of introducing a retrieval mechanism, we provide a few comparisons of different base LLMs in Appendix~\ref{app:exp_base_llm}.
Looking ahead, we anticipate that LLM-based TabICL will continue to benefit from advancements in the LLM field~\citep{liu2024deepseekv3}, further enhancing scalability and unlocking new opportunities for tabular data applications.


\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{main_figures/main_scaling_pool_and_context.pdf}}
\caption{
We investigate the effects of increasing the number of training instances ($|D_{\text{train}}^{T'}|$) and the number of in-context instances per test example ($N^C$) on the TabICL performance of Phi3-GTL models.
In each subplot, we compare the scaling effects of two Phi3-GTL models with different retrieval policies: one that randomly selects in-context instances, denoted as "Random," and the other employing our default \texttt{TabRAG} module, denoted as "RAG".
We use violin plots to visualize the performance distribution across multiple held-out datasets. Additionally, dashed lines are used to emphasize that the median prediction error of our approach follows a power-law relationship with the number of training instances.
}
\label{fig:scaling_pool_ctx}
\end{center}
\vskip -0.2in
\end{figure*}



\paragraph{The Alignment of \texttt{TabRAG} and $Q^{LLM}_\theta$}
To fully unleash the TabICL capabilities of LLMs, it is crucial to align them with the in-context selection patterns of a customized \texttt{TabRAG} module.
Rather than randomly selecting in-context instances, as in~\citep{wen2024GTL}, we post-train LLMs to adhere to the in-context distributions generated by our default \texttt{TabRAG} policy. This alignment results in improved generalization performance on held-out datasets.
These findings emphasize that future post-training of LLM-based TabICL could further benefit from integrating diverse retrieval policies, paving the way for flexible "retrieval engineering" tailored to downstream tabular prediction tasks.

% \paragraph{The Alignment of \texttt{TabRAG} and $Q^{LLM}_\theta$}
% When utilizing a customized \texttt{TabRAG} module, we find it essential to fully unleash the TabICL capabilities by aligning LLMs to follow the in-context selection patterns of \texttt{TabRAG}.
% Therefore, rather than randomly selecting in-context instances as did in~\citep{wen2024GTL}, we post-train LLMs to follow the in-context distributions selected by our customized \texttt{TabRAG} module and observe better generalization on held-out datasets.
% These findings highlight that the future post-training of LLM-based TabICL could be benefit from a diverse range of retrieval policies and allow flexible ``retrieval engineering'' for downstream tabular prediction tasks.


% \paragraph{Enhancing TabICL for $Q^{LLM}_\theta$}
% In the original evaluation of GTL-enhanced LLMs for TabICL~\citep{wen2024GTL}, the held-out evaluation datasets were relatively simple and may not fully capture the complexities encountered in real-world scenarios.
% To achieve scalable and robust TabICL in the wild, it is imperative to enhance the capabilities of $Q^{LLM}_\theta$.
% This involves not only improving the model's ability to generalize from limited context lengths but also ensuring its adaptability to diverse and complex datasets.
% In this work, we explore the use of synthetic data to augment the training process, allowing the model to experience a wider variety of data distributions and feature interactions.
% \todo{Update with specific implementations.}

% \paragraph{Jointly Optimizing \texttt{TabRAG} and $Q^{LLM}_\theta$}
% To fully realize the potential of scalable TabICL, it is necessary to jointly optimize \texttt{TabRAG} and $Q^{LLM}_\theta$.
% This joint optimization ensures that the retrieval module and the prediction model are aligned and work synergistically to maximize prediction performance.
% In practice, this involves an iterative training process where the retrieval mechanism is continuously refined based on the feedback from the prediction model, and vice versa.
% Techniques such as co-training and reinforcement learning can be employed to facilitate this iterative improvement.
% By optimizing both components together, we can ensure that the retrieved examples are not only relevant but also optimally tailored to enhance the performance of $Q^{LLM}_\theta$.
% This holistic approach allows us to leverage the strengths of both modules, resulting in a more cohesive and effective TabICL framework that can adapt to various data scales and complexities.
% \todo{Update with specific implementations.}

