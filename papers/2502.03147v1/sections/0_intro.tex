\section{Introduction}
\label{sec:intro}

Tabular data, a prevalent data modality comprising rows that represent distinct data instances and columns that define their features and labels, underpins critical machine learning applications across diverse domains, such as healthcare~\citep{johnson2016mimic}, commerce~\citep{bohanec2017PredSales}, and energy~\citep{miller2020EnergyPred}.

A longstanding challenge of learning on tabular data is addressing the heterogeneity in feature and label spaces. Specifically, it is crucial to account for variable-length feature columns and diverse feature types, such as numerical and categorical data with potentially unbounded values. Furthermore, identical feature values may carry entirely different meanings and significance across distinct feature columns and prediction tasks. Lastly, prediction tasks can vary on a case-by-case basis, encompassing both multi-class classification and regression labels.

This heterogeneity challenge has constrained most existing tabular learning approaches to case-by-case optimization or fine-tuning for each new dataset and task. These include early tree-based models~\citep{chen2016XGBoost,ke2017LightGBM,prokhorenkova2018catboost}, later developments in customized neural architectures~\citep{huang2020TabTransformer,arik2021TabNet, katzir2021Net-DNF,gorishniy2021revisit_tab_dnn,gorishniy2024TabR}, and recent explorations in self-supervised learning and pre-training~\citep{yoon2020VIME,somepalli2021SAINT,ucar2021SubTab,bahri2022SCARF,wang2022TransTab,levin2023transfer_tab_nn,zhu2023XTab,yang2023UniTabE,yan2024TP-BERTa,ye2024CM2,ye2024PTaRL}.

Motivated by the emergence of in-context learning (ICL) in large language models (LLMs)~\citep{Brown2020GPT-3}, several studies have begun exploring ICL for tabular data, which we refer to as TabICL in this paper for brevity. Research on TabICL has given rise to two independent threads.
The first pre-trains Transformer~\citep{vaswani2017attention} variants tailored to tabular data using a TabICL objective on synthetic datasets generated from structured causal networks~\citep{hollmann2023TabPFN,hollmann2025TabPFNv2}.
The second post-trains a base LLM using instructions derived from extensive, language-represented real-world tabular datasets, enabling both ICL and zero-shot generalization for new tasks~\citep{wen2024GTL,gardner2024TabuLa}.

These two paradigms each have unique advantages and limitations: the former using numeric representations is highly efficient but confined to the scope of TabICL, while the latter has the potential to seamlessly integrate TabICL with other LLM capabilities, such as conversational interactions, coding, and reasoning, thereby unlocking broader possibilities within a single model.
However, current LLM-based TabICL approaches are limited to few-shot scenarios due to the sequence length constraints of typical LLMs~\citep{wen2024GTL,gardner2024TabuLa}.
Representing a tabular instance in plain text often consumes a substantial number of tokens, significantly restricting their scalability for incorporating more in-context instances.

In this work, we address this limitation by developing a retrieval-augmented generation (RAG)~\citep{lewis2020RAG} approach tailored to tabular data, extending LLM-based TabICL from zero-shot and few-shot scenarios to any-shot settings.
Our approach is built on the assumption that \emph{for a specific test instance, a limited support set can suffice to govern an accurate prediction}, a concept inspired by the core idea of $k$-nearest neighbor algorithms~\citep{fix1951knn,cover1967knn_cls}.
This assumption shifts the optimization focus away from traditional approaches, which aim to compress all training instances into model parameters or hidden states.
Accordingly, we explore the design of a universal, non-parametric retrieval module capable of identifying the most relevant in-context instances across diverse datasets, while also revealing the potential for further enhancing TabICL performance through customized, dataset-specific retrieval policies.
Additionally, we find that aligning LLMs with specific retrieval patterns is crucial, leading to a retrieval-guided generative tabular learning~\citep{wen2024GTL} process for post-training LLMs.

We curate a comprehensive benchmark comprising 29 classification datasets and 40 regression datasets.
These datasets are sourced from different branches of studies~\citep{wen2024GTL,gorishniy2021revisit_tab_dnn,grinsztajn2022tree_gt_tab_nn}, ensuring diversity and fairness by encompassing a wide range of dataset characteristics that may favor different model types, including TabICL models~\citep{wen2024GTL,hollmann2023TabPFN,hollmann2025TabPFNv2}, neural tabular models~\citep{gorishniy2021revisit_tab_dnn,gorishniy2024TabR}, and tree-based models~\citep{chen2016XGBoost,ke2017LightGBM,prokhorenkova2018catboost}.
Our experimental findings reveal the following key takeaways.

\begin{itemize}
\item
% 1)
The retrieval mechanism significantly enhances LLM-based TabICL by leveraging the scaling of training data size ($D$). Our analysis shows that the median error on held-out datasets ($L$) follows a power-law relationship with $D$: $L(D) = (D_c / D)^{\alpha}$, where $\alpha \sim 0.102$ for classification and $\alpha \sim 0.053$ for regression.
\item
% 2)
Our approach exhibits remarkable properties, such as uncovering powerful TabICL algorithms comparable to classic models, generating distinct decision boundaries, enhancing ensemble diversity, and achieving superior performance on certain datasets.
\item
% 3) 
Overall, LLM-based TabICL with text-represented data still lags behind well-tuned numeric models. Additionally, no single tabular model consistently outperforms across all evaluation scenarios. While TabPFN, TabR, and LightGBM frequently rank among the top-performing models, they exhibit occasional performance degradations in certain cases, highlighting the importance of developing diverse models in practice.
\item
% 4)
Through per-dataset analyses and case studies, we find that LLM-based TabICL can be further enhanced by incorporating more effective retrieval policies (e.g., leveraging domain knowledge in practical applications) and by post-training on more diverse feature distributions, similar to the way TabPFN utilizes synthetic datasets. Therefore, we believe this approach, as a special TabICL paradigm, has significant potential to be unlocked.
\end{itemize}



% Dicompressing all training instances into model parameters or hidden states and then doing inference for each testing example,

% Tabular data learning has long been a critical research area in machine learning with widespread applications across various industrial sectors~\citep{borisov2022DNNTabSurvey}, including healthcare, commerce, energy, and finance~\citep{johnson2016mimic,bohanec2017PredSales,miller2020EnergyPred,ngai2011FraudDetec}.
% These applications underscore the importance of developing effective models for diverse datasets.

% A fundamental challenge in tabular data learning is the inherent heterogeneity of both data schemas and tasks. This heterogeneity is marked by substantial variations in numerical and categorical features, as well as differing prediction objectives across diverse domains.
% As a result, much of the research in this area has concentrated on training individual models for each dataset, emphasizing the development of novel learning algorithms~\citep{chen2016XGBoost, ke2017LightGBM,prokhorenkova2018catboost}, model architectures~\citep{huang2020TabTransformer,arik2021TabNet, katzir2021Net-DNF,gorishniy2021revisit_tab_dnn,gorishniy2024TabR}, and training regimens~\citep{yoon2020VIME, somepalli2021SAINT, ucar2021SubTab, bahri2022SCARF, ye2024PTaRL}.
% While some studies have begun to explore pre-training across different tabular datasets~\citep{wang2022TransTab,levin2023transfer_tab_nn,zhu2023XTab,yang2023UniTabE,yan2024TP-BERTa,ye2024CM2}, their methods still require non-trivial parameter tuning for new datasets due to the persistent challenge of dataset heterogeneity.

% More recently, a few studies~\citep{hollmann2023TabPFN,wen2024GTL} have tackled this challenge by introducing the in-context learning (ICL)~\citep{Brown2020GPT-3} from language modeling to tabular data learning.
% The obtained new paradigm, known as tabular in-context learning (TabICL), represents an vital advancement in tabular data learning as it significantly reduces the burden of model tuning in new applications, making advanced models more accessible to unskilled users.

% However, existing TabICL approaches face severe scalability challenges that limit their ability to handle more comprehensive tabular datasets.
% For instance, TabPFN~\citep{hollmann2023TabPFN} pre-trained a Transformer~\citep{vaswani2017attention} model designed for in-context classifications, but it is limited to specific datasets with up to $1,000$ training examples, $100$ numerical features, and $10$ classes.
% In another study,~\cite{wen2024GTL} developed a continual pre-training process called Generative Tabular Learning (GTL) to enable large language models (LLMs)~\citep{touvron2023llama2} to follow instructions on tabular data learning.
% While GTL-enhanced LLMs can handle TabICL for both classification and regression tasks, they still face scalability issues due to the limited context lengths of LLMs. Consequently, these models have only been evaluated in few-shot scenarios.

% In this work, we introduce a scalable TabICL framework by incorporating a tabular retrieval-augmented generation (TabRAG) mechanism into GTL-enhanced LLMs.
% Our primary motivation arises from the observation that, although the training dataset can be vast, the most relevant references for making predictions on a specific test sample usually involve only a small subset of the training data.
% To leverage this insight, we separate two fundamental capabilities: (1) performing universal TabICL for small-scale tabular learning problems, and (2) retrieving the most useful in-context samples from the entire training pool. These two functionalities are assigned to a GTL-enhanced LLM and a TabRAG module, respectively.

% We conduct extensive experiments across a wide range of datasets at different scales.
% Specifically, we not only include the original held-out datasets used in developing GTL for evaluation~\citep{wen2024GTL} but also incorporate other widely used datasets in tabular deep learning~\citep{gorishniy2021revisit_tab_dnn} and additional datasets considered more suitable for tree-based models~\citep{grinsztajn2022tree_gt_tab_nn}.
% In total, our evaluation spans several dimensions, including different objectives (\todo{23} classification and \todo{32} regression tasks), a range of data scales (with training sample sizes from \todo{1000} to \todo{100000}), and a variety of feature combinations (with the number of features varying from \todo{4} to \todo{138}).
% Moreover, we have compared with competitive tabular models, including classical tree-ensemble methods~\citep{chen2016XGBoost,prokhorenkova2018catboost} and cutting-edge neural models~\citep{hollmann2023TabPFN,gorishniy2024TabR}.
% Through these experiments and associated analyses, we evaluate the learning capabilities of our scalable TabICL approach, discuss its current limitations, and prospect its future potentials.

% We summarize our major contributions as follows.
% \begin{itemize}
%     \item To the best of our knowledge, this study is the first effort in exploring scalable TabICL without stringent restrictions on task types and data scales. This flexibility makes TabICL applicable to more tabular learning problems in practice, laying the foundation for the evolution of toolchains and user interfaces in tabular data science.
%     \item  We have developed a simple yet potent framework featuring a decoupled design. This includes a configurable TabRAG module for selecting the most informative in-context samples and an LLM that is optimized for TabICL with limited data samples.
%     \item Through extensive experiments across diverse datasets, our framework consistently demonstrates performance improvements as the training data scale increases. Additionally, it also showcases significant non-linear learning capabilities, such as notably outperforming basic linear models and nearest neighbor approaches, and even approaching the performance of heavily optimized tree-ensemble and neural models.
% \end{itemize}


