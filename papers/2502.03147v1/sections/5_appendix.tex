\section{Methodology Details}

\subsection{Our Default Retrieval Policy for Tabular Data}
\label{app:rag_method}
% @xumeng , just describe how you do in each step and cite specific references, I will re-organize this part.
In this section, we describe our default retrieval policy for selecting in-context instances for a query test case.
Our approach employs a non-parametric retrieval mechanism based on the idea of nearest neighbors.
The core idea is to identify the nearest context instances for a given test sample by computing distances between the test instance and the context sample pool (the training set).
We aggregate importance-weighted feature-wise similarity scores as a instance-level similarity score.
This process involves two key components: (1) calculating single-feature distances and (2) aggregating feature distances into instance distances. Below, we elaborate on each component in detail.

\textbf{Calculating Single Feature Distances}
Tabular data typically consists of two types of features: categorical and numerical. To compute distances between samples, we handle each feature type differently:
\begin{itemize}
    \item \textbf{Categorical Features}: For categorical features, the distance between two samples is defined as 1 if their feature values differ and 0 if they are the same.
    \item \textbf{Numerical Features}: For numerical features, we first apply quantile normalization using the statistics of the context pool. Next, we compute the absolute difference between the normalized feature values of the test sample and each context sample. To ensure consistency, we scale these distances to the range [0, 1] using min-max normalization across the context pool.
\end{itemize}
This approach ensures that the distance for each feature between the test sample and any context sample lies within the range [0, 1], providing a standardized measure of similarity.

\textbf{Aggregating Feature-level Distances into Instance-level Distances}
In tabular data, many features may be uninformative, and aggregating feature distances equally can result in selecting context samples that are not representative of important features. To address this, we introduce a parameter-free feature weighting mechanism.
\begin{itemize}
    \item \textbf{Feature Importance Scoring}: We use Pearson Correlation~\citep{cohen2009pearson} to measure the linear relationship between each feature and the target label in the context pool. To capture non-linear relationships, we fit decision trees to each feature and evaluate its contribution to target prediction in the context pool. This process does not require parameter optimization; instead, it iteratively identifies the best feature quantile based on prediction metrics. Since we fit a tree on each single feature, the computation is highly efficient. For implementation, we leverage the Python library PPS (Predictive Power Score)~\citep{florian_wetschoreck_2020_4091345}.
    \item \textbf{Distance Aggregation}: To aggregate feature distances into a sample distance, we employ a weighted L2-norm, defined as~\eqref{appeq:rag_dist}. 
    \begin{equation}
        D_{sample} = \sqrt{\Sigma_{i=1}^{n}(D_{i}^2 \cdot w_i)},
        \label{appeq:rag_dist}
    \end{equation}
    where \( D_i \) is the distance for the \( i \)-th feature, and \( w_i \) is the feature importance weight.
    We combine the two feature importance scores (Pearson Correlation and PPS) to aggregate single-feature distances into sample distances. For instance, if we have a quota of 128 context samples, we allocate half of the quota to the nearest samples based on Pearson Correlation and the other half to the nearest samples based on PPS.
\end{itemize}


\subsection{More Details on Aligning LLMs with Our Default Retrieval Policy}
\label{app:method_align_rag_llm}

% how we configure datasets, templates, contexts, shots, and training steps, batch size, learning rates, optimizer, ....
To enable generative tabular learning~\citep{wen2024GTL} on data with retrieved contexts, we curated over 300 public datasets from Kaggle, following the collection methodology outlined in their paper. To ensure the integrity of our evaluation, we carefully filtered these datasets to eliminate any potential data contamination between the training and held-out evaluation sets. After filtering, we retained 146 classification datasets and 173 regression datasets. To maximize the utility of these datasets, we expanded each dataset into up to four distinct tasks by designating different columns as task labels. We explored configurations with context sample sizes ranging from 4 to 128, all within a sequence length limit of 16,384 tokens. For each context setting, we randomly selected 16 target samples per task and formatted them using the anonymized template from~\citep{wen2024GTL}. This process yielded a total of over 100,000 data samples for generative tabular learning.

For training, we utilized 16 NVIDIA A100 GPUs. We employed a micro-batch size of 1, a learning rate of 1e-5, and the AdamW optimizer. No warmup or learning rate scheduler was used during training. The entire training process for the Phi-3 Medium base LLM on this dataset took approximately 10 hours.

\section{Dataset Construction}
\label{app:data_constr}

\subsection{Post-Training Datasets for LLMs}
\label{app:data_constr_train}

% collect from which sources
% If exactly identical to our kdd version, just point to that paper and mention which dataset tables
We initially collected all tabular datasets listed in~\citep{wen2024GTL}. To ensure the integrity of our evaluation, we manually filtered these datasets to remove any potential overlap with benchmark datasets used for evaluation. This careful filtering process resulted in a final collection of 146 classification datasets and 173 regression datasets, which were used exclusively for post-training LLMs.

\subsection{Benchmarking Datasets}
\label{app:data_constr_eval}

% collect from which sources
% how we filter and pre-process
% final dataset index and associated info (source, data stats)

To ensure a comprehensive evaluation of our model, we curated datasets from multiple studies focusing on different tabular learning paradigms: LLMs~\citep{wen2024GTL}, neural tabular models~\citep{gorishniy2021revisit_tab_dnn,gorishniy2024TabR}, and tree-based models~\citep{grinsztajn2022tree_gt_tab_nn}.
In total, we prepared 29 classification datasets and 40 regression datasets, enabling a thorough assessment of model performance across diverse tabular data tasks. The detailed dataset information and statics can be found in Table ~\ref{tab:heldout_data}

\textbf{Datasets from GTL~\citep{wen2024GTL}}. GTL is a research initiative focused on post-training LLMs for generalizable inference across diverse tabular tasks. A base LLM is continually trained on a comprehensive collection of 350 datasets and evaluated on an additional 50 datasets, encompassing both classification and regression tasks. This extensive pretraining enables GTL to generalize effectively across various tabular domains, delivering robust performance on a wide range of tasks without requiring task-specific fine-tuning. For our evaluation, we retained 16 test datasets after a strict filtering process based on feature quality and task validity.
We denote the source tag of these datasets as ``LLM''.

\textbf{Datasets from FTT~\citep{gorishniy2021revisit_tab_dnn} and TabR~\citep{gorishniy2024TabR}}. FTT is a widely recognized neural model for tabular learning. TabR represents a significant successor in retrieval-based tabular models. This research proposes an innovative approach that leverages embedding distance-based retrieval methods to enhance the accuracy and efficiency of tabular data processing. The model integrates deep learning architectures with carefully optimized hyperparameters and employs an ensemble strategy, further improving its generalization capabilities across diverse tasks. Through extensive experimentation, TabR has demonstrated superior performance across 11 datasets (those are the same as the ones used in FTT) spanning various scales and complexities, underscoring its adaptability to different tabular data characteristics. For our evaluation, we retained all 11 datasets as heldout datasets, including 5 classification and 6 regression datasets.
We denote the source tag of these datasets as ``Neural''.

\textbf{Datasets from~\citep{grinsztajn2022tree_gt_tab_nn}}, titled Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data? (referred to as \textbf{Tree}), is a comprehensive research effort that investigates the continued dominance of tree-based over deep learning approaches in tabular data tasks. The study highlights the inherent strengths of tree-based models, which outperform deep learning models across a wide range of tabular tasks, despite significant advancements in neural network architectures. To rigorously evaluate the performance of both model families, the authors curated a diverse collection of small-to-middle-scale datasets, with training samples <= 50,000, categorized into two series: the cat series (containing both numerical and categorical features) and the num series (consisting exclusively of numerical features). 
For our evaluation, we retained the same train-val-test splits as in the original benchmark and used only the first split for consistency. Following the preprocessing approach of TabR, we selected the original dataset when multiple versions of a dataset were available. Additionally, we removed the "Eye Movements" dataset due to potential label leakage concerns and excluded the "isolet" datasets because their large number of features (over 600) would significantly reduce the context quota available for LLMs. After these adjustments, we retained a total of 42 datasets, comprising 15 classification datasets and 27 regression datasets.
We denote the source tag of these datasets as ``Tree''.

\textbf{Post Preprocessing} For all benchmarking datasets, we imposed a restriction on the training size to a maximum of 100,000 samples. For the GTL benchmark datasets, we applied a random split ratio of 8:1:1 for training, validation, and test sets, respectively. For the TabR and Tree benchmarks, we retained the original dataset splits to ensure consistency with prior evaluations. Additionally, to accelerate the evaluation of LLM-based methods, we downsampled each test set randomly to a maximum of 512 samples.
We find this downsampling can ensure statistical significance while improving evaluation efficiency for comparing and investigating different variants of LLMs.
% This downsampling step maintains a balance between evaluation efficiency and the statistical reliability of the results.

\section{Details of Baselines}

% Detailed and Grouped Introduction of baselines and how you tuned their parameters (using which data partitions, how many runs, tune which parameters) 
% For R. + X, use default hyper-parameters?

% Tree-based Models
% Neural Models
% ICL Models
% R. + KNN
% R. + X


\textbf{Tree-based models} XGBoost, LightGBM, and CatBoost are three widely used Gradient Boosted Decision Tree (GBDT) frameworks, renowned for their ability to optimize speed and performance while handling large datasets, imbalanced data, and missing values. These frameworks are based on the principles of gradient boosting, which builds models sequentially by adding weak learners (decision trees) that correct the errors of prior models. For decades, tree-based models have been the dominant choice and state-of-the-art approach for tabular data tasks.

To ensure a fair comparison, we ran these GBDT models on our benchmarks using hyperparameters tuned for each specific configuration. Following the curated hyperparameter optimization space and search algorithm provided by TabR, we performed hyperparameter optimization using Optuna for 200 trials. This extensive tuning process allowed us to achieve nearly optimal performance for each tabular task. For evaluation, we used the best hyperparameters identified during tuning and ran experiments with 5 different random seeds, reporting the average score across these runs. The hyperparameter tuning space was kept identical to the configuration used in TabR~\citep{gorishniy2024TabR} to maintain consistency and comparability.

\textbf{Neural Baselines}
We evaluate three neural baselines to benchmark performance on tabular data: \textbf{MLP}, \textbf{FT-Transformer}~\citep{gorishniy2021revisit_tab_dnn}, and \textbf{TabR}~\citep{gorishniy2024TabR}. For our experiments, we use the implementations provided by~\citep{gorishniy2024TabR}. We adhere to their hyperparameter tuning process, employing Optuna to conduct 100 trials per dataset within the same tuning space as described in~\citep{gorishniy2024TabR}.

\begin{itemize}
    \item \textbf{MLP} is a simple multi-layer feedforward neural network. We include this baseline to establish a foundational understanding of how deep learning models perform, even at their most basic level.

    \item \textbf{FT-Transformer} (FTT) is a neural architecture specifically designed for tabular data, adapting the transformer framework to handle both categorical and numerical features. Its key innovation lies in its tokenization and processing of tabular data using transformer components traditionally employed in NLP. Specifically, FT-Transformer converts each feature (whether categorical or numerical) into a feature token, treating each feature in a row as an independent input token. This approach enables the model to effectively capture complex relationships within tabular data.

    \item \textbf{TabR} is a retrieval-based neural model for tabular data that integrates in-context retrieval. It encodes both the query dataset and stored datasets into a shared latent space using an embedding function, capturing the structure and relationships between features (both numerical and categorical). The encoded query is compared with pre-stored datasets to identify the most relevant matches in the latent space. By retrieving and leveraging relevant historical data, TabR can make informed predictions on new, unseen datasets without requiring extensive task-specific training. This approach allows TabR to generalize across diverse tabular tasks by utilizing the richness of previously encountered datasets.
\end{itemize}

\textbf{TabICL}
Tabular In-Context Learning (TabICL) methods enable models to adapt to new tabular datasets without explicit retraining by leveraging patterns learned from prior data. We explore two prominent TabICL approaches: \textbf{TabPFN} and \textbf{GTL}.

\begin{itemize}
    \item \textbf{TabPFN} is a prior-data fitted network designed for in-context learning on tabular data. Pretrained on a large corpus of synthetic data, TabPFN learns to recognize common tabular data patterns, such as feature interactions, correlations, and handling of missing values. When presented with a new dataset, TabPFN generalizes in-context without requiring retraining. The initial version, \textbf{TabPFN-v1}~\citep{hollmann2023TabPFN}, supports classification tasks but is limited to datasets with fewer than 1,000 context samples. The recently released \textbf{TabPFN-v2}~\citep{hollmann2025TabPFNv2} extends these capabilities, supporting both classification and regression tasks and handling up to 10,000 context samples through improved prior-data generation, model architecture, and code framework. In our experiments, we found that both TabPFN-v1 and TabPFN-v2 perform effectively with up to 10,000 context samples, with TabPFN-v1 showing improved performance at this scale compared to 1,000 samples. For datasets exceeding 10,000 samples, we randomly select 10,000 samples as the context for both versions.

    \item \textbf{GTL} (Generative Tabular Learning) is a post-training method designed to enhance the capabilities of large language models (LLMs) on tabular datasets. Unlike TabPFN, which relies on embedding representations to understand tabular data, GTL leverages text token representations, harnessing the power of LLMs to interpret and process tabular data. This approach significantly improves zero-shot and in-context learning performance for base LLMs. As demonstrated in~\citep{wen2024GTL}, GTL-enhanced LLMs achieve strong few-shot performance on both classification and regression tasks, though they face challenges due to sequence length limitations inherent to LLMs. In our experiments, we apply GTL to several LLMs using 319 public datasets, carefully filtered from~\citep{wen2024GTL} to ensure no overlap between the post-training datasets and our expanded held-out evaluation datasets. This ensures the integrity of our evaluation and prevents data contamination.
\end{itemize}

\textbf{RAG+KNN}
KNN (K-Nearest Neighbors) is a non-parametric, proof-of-concept model that makes inferences based solely on provided neighbors. In our experiments, we use the retrieved context samples as the neighbors for KNN. By combining KNN with our retrieval mechanism, we can analyze how LLMs learn and reason from context samples compared to simple average predictions.

\textbf{RAG+Other Models}
We evaluate several models enhanced with our retrieval context samples as baselines, including \textbf{RAG+LR} (Logistic Regression and Linear Regression), \textbf{RAG+XGBoost}, and \textbf{RAG+TabPFN}. For each test sample, we fit an individual baseline model using the same retrieval context samples provided to the LLMs. This setup allows us to compare how LLMs perform and make decisions relative to other models when given identical context samples. For \textbf{LR}, we normalize numerical features and labels and apply one-hot encoding to categorical features. For \textbf{XGBoost} and \textbf{TabPFN}, we fit the models directly on the original data. Notably, we use default configurations for LR and XGBoost without hyperparameter tuning to maintain simplicity and focus on the impact of retrieval-augmented context.
In few-shot regression settings, we observe that \textbf{TabPFN-v2} may encounter errors due to its internal normalization when all context labels or numerical feature values are identical. In such cases, we replace the sample prediction with the average of the context labels.

\section{Additional Experimental Results}
\label{app:exp}

\subsection{Retrieval Ablation Analysis: The Importance of Selecting Proper In-Context Instances}
\label{app:exp_rag_abla_test}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/rag_ablation.pdf}
    \caption{Ablation study of the retrieval mechanism across 29 classification and 40 regression datasets. Normalized metrics compare variants where individual components of the full method (Section~\ref{app:rag_method}) are removed.}
    \label{fig:rag_ablation}
\end{figure}

In this section, we conduct an ablation study of our retrieval mechanism, systematically removing components from the full method (Section~\ref{app:rag_method}) to assess their impact. We evaluate four variants:
% In this section, we conduct an ablation study of our retrieval mechanism (Figure~\ref{fig:rag_ablation =}), systematically removing components from the full method (Section~\ref{app:rag_method =}) to assess their impact. We evaluate four variants:
\begin{itemize}
\item \textbf{NoFeatImp}: Excludes feature importance scores, aggregating feature distances without feature weighting.
\item \textbf{NoNorm}: Disables numerical feature normalization during distance computation.
\item \textbf{NoCorr}: Uses only Predictive Power Score (PPS) by removing Pearson Correlation for feature importance.
\item \textbf{NoPPS}: Uses only Pearson Correlation by removing PPS for feature importance.
\end{itemize}

Three key findings emerge:
\textbf{Component Necessity}: Removing any component degrades performance, with \textbf{NoFeatImp} exhibiting the largest decline across tasks. This underscores the critical role of feature importance weighting in prioritizing informative features during context selection.

\textbf{Complementary Relationship Measures}: Both linear (Pearson) and non-linear (PPS) metrics contribute uniquely, as performance drops when either is removed (\textbf{NoCorr/NoPPS}).

\textbf{Normalization Effects}: While \textbf{NoNorm} degrades performance in classification tasks, it yields improvements in regression tasks—a counterintuitive divergence in normalization’s impact across tasks (analyzed in Section~\ref{app:case_study}). We hypothesize that normalization’s interaction with feature distributions explains this discrepancy: ill-suited normalization schemes can distort feature distance calculations, particularly in regression scenarios where raw numerical scales may inherently encode meaningful relationships. This observation underscores the necessity of dataset-specific retrieval strategies tailored to the statistical properties of the input data.

% \subsection{More Perspectives in Overall Performance Comparisons}
% Do not need this, we put all in the main paper

\subsection{More Results on Decision Boundary Analysis}
\label{app:exp_dec_bound}
We visualize decision boundaries across diverse data patterns, multiple noise levels, and varying training data sizes in Figure~\ref{fig:decision_cir}~\ref{fig:decision_moon}~\ref{fig:decision_line}. For TabPFN and RAG+Ph3-GTL, the training data serves as the context data samples. Notably, the LLM exhibits unique decision boundaries characterized by higher uncertainty, contrasting with the smoother boundaries of other models. This suggests potential opportunities for LLMs to excel in scenarios with especially limited context or higher noise levels. Interestingly, we also observe a case where TabPFN produces a near-random decision boundary on the circle dataset with noise=0.2 and a training size of 16 (See Figure~\ref{fig:decision_cir}). This indicates that, in specific few-shot scenarios, TabPFN may struggle to fit the data effectively and generalize to test data.

\subsection{More Results using Different Base LLMs}
\label{app:exp_base_llm}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/comp_base_llm.pdf}
    \caption{Comparison of base LLM performance with RAG and GTL approaches. Results are evaluated across 29 classification datasets and 40 regression datasets.}
    \label{fig:comp_base_llm}
\end{figure}

We evaluate the retrieval mechanism and GTL across diverse base LLMs, including Phi3-series models (Phi3-mini-128k, Phi3-medium-128k, Phi3.5-mini-128k) and LLaMA-series models (LLaMA2-7B, LLaMA2-13B, LLaMA3-8B). Our results confirm the generalizable effectiveness of these methods across architectures. Two critical findings emerge:

\textbf{Performance Gap in LLaMA3-8B} LLaMA3-8B exhibits a performance gap compared to other base models, producing significantly higher NMAE values in regression tasks (results in Figure~\ref{fig:comp_base_llm} are clipped at 1.0 for visualization). A plausible hypothesis for this discrepancy lies in LLaMA3’s multi-digit tokenization strategy: numerical values are split into three-digit segments (e.g., “123456” → “123” and “456”), whereas LLaMA2 and Phi3 models tokenize each digit individually. This tokenization approach increases the diversity of numerical tokens in the vocabulary, which—combined with standard pretraining protocols—may hinder the model’s ability to learn precise numerical relationships without additional training data. Further investigation is required to isolate the impact of tokenization from other architectural or training factors.

\textbf{Small Models Match Large Counterparts with Retrieval Context} Retrieval-augmented contexts enable smaller models to match the performance of larger ones while maintaining cost efficiency and deployment practicality. This suggests lightweight models, when combined with retrieval mechanisms, offer a viable path to resource-efficient AI deployment without sacrificing accuracy.

\subsection{Comparing with Tabula-8B}
\label{app:exp_comp_tabula}

\textbf{Comparison with Tabula-8B} We compare GTL-enhanced LLMs against Tabula-8B~\citep{gardner2024TabuLa}, a post-trained LLM for tabular data that similarly converts tabular data into language prompts. Tabula-8B builds on LLaMA3-8B but exhibits a significant performance gap relative to LLaMA3-8B-GTL and LLaMA2-13B-GTL in both classification and regression tasks (Figure~\ref{fig:comp_tabula_ori}).
So we mainly use GTL-enhanced LLMs in this work.
% , demonstrating the superior effectiveness of GTL for tabular learning.

\textbf{Evaluation on Regression-to-Classification tasks} Moreover, Tabula-8B cannot directly handle regression tasks, instead quantizing numerical labels into four bins (converting regression to classification). To ensure fair comparison, we replicate this methodology by transforming all 40 regression datasets in our benchmark into classification tasks. As shown in Figure~\ref{fig:comp_tabula_reg2cls}, GTL still substantially outperforms Tabula-8B in both absolute and normalized AUROC metrics.

\textbf{Limitations of Regression-to-Classification Conversion} We further investigate whether evaluating on quantized classification datasets reliably reflects performance on original regression tasks. Figure~\ref{fig:comp_reg2cls_per} compares the performance gap between Tabula-8B and LLaMA2-13B-GTL across regression datasets and their quantized classification counterparts. While AUROC gaps remain modest in specific datasets, normalized mean absolute error (NMAE) gaps are often substantial. This underscores the necessity of evaluating models on original regression tasks with true numerical targets—a critical requirement for advancing ensemble methods in tabular learning.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/comp_tabula_8b_ori_data.pdf}
    \caption{Comparison with Tabula-8B on 29 classification and 40 regression datasets.}
    \label{fig:comp_tabula_ori}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/comp_tabula_8b_reg2cls.pdf}
    \caption{Comparison with Tabula-8B on 40 regression-transformed classification datasets.}
    \label{fig:comp_tabula_reg2cls}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/show_reg2cls_bias.pdf}
    \caption{Compare performance gap between Tabula-8B and LLaMA-13B-GTL in regression datasets and the corresponding quantile-transformed classification datasets.}
    \label{fig:comp_reg2cls_per}
\end{figure}

\subsection{Detailed Results of All Models in Overall Comparisons}
\label{app:exp_detail_all_res}
% A large, multi-page table showing detailed results for all models
% mark the best model results with \textbf{} and the second best with \underline{}
% better to generate this latex table using a python script
We report detailed results in Table~\ref{tab:detail_res}, including all tuned baselines, TabPFN-v1 and TabPFN-v2, and retrieval-augmented approaches across all 69 benchmark datasets. For tuned baseline, we report the mean metrics over 5 random seeds evaluation after hyperparameters tuning with Optuna. For TabPFN, we conduct in-context learning with full training set (or up to 10,000 samples for larger datasets). For our retrieval methods, we provide as much context as possible (up to 128 context samples) within 16,384 token length limit.


\section{Additional Case Studies}
\label{app:case_study}
% put all case studies here first, and then we can select what to summarize and display in the main paper

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/california_case_study.pdf}
    \caption{Case Study on the California Dataset. The left figure displays the relationship between features and the label, with label values represented by point color. The middle figure shows feature importance scores computed using linear (Pearson Correlation) and non-linear (PPS) measurements. The right figure presents the distribution of average context distances across all test samples for both linear and non-linear measurements.}
    \label{fig:case_calif}
\end{figure}

\begin{table}
\centering
\caption{Normalized Mean Absolute Error (NMAE) scores for case study datasets. RAG-Tuned denotes results using dataset-specific retrieval methods (see Section~\ref{app:case_study}). The first group highlights datasets where default TabRAG underperformed; customized retrieval contexts significantly improved performance. The second group demonstrates scenarios where LLM in-context learning outperformed both TabPFN and tuned neural models, underscoring its unique strengths for specific data patterns.}
\label{tab:rag_tune}
\begin{tabular}{c|cc|ccc}
\hline
Dataset    & RAG-Tuned+Phi3-GTL & RAG+Phi3-GTL & TabPFN-v2          & CatBoost  & TabR          \\ \hline
R-27       & 0.0951             & 0.1113       & \textbf{0.0742}    & 0.0803    & 0.0775        \\
R-33       & 0.0819             & 0.1904       & \textbf{0.0342}    & 0.0591    & 0.0833        \\
R-25       & 0.1522             & 0.3754       & \textbf{0.1229}    & 0.1273    & 0.1334        \\ \hline
R-14       & -                  & \textbf{0.0045}       & 0.0081             & 0.0094    & 0.0122        \\
R-16       & -                  & \textbf{0.0634}       & 0.0840             & 0.0683    & 0.0729        \\ \hline
\end{tabular}
\end{table}

\textbf{[R-27]} We analyze how feature importance methods influence the selection of context samples on the \textit{regression-num-medium-0-california} dataset, which involves predicting California house prices. As shown in Figure~\ref{fig:case_calif}, the target label exhibits a strong relationship with longitude and latitude. This relationship is not easily captured by linear measurements like Pearson Correlation, whereas the non-linear Predictive Power Score (PPS) assigns high importance scores to both longitude and latitude. As a result, context samples selected using PPS exhibit closer distances to test samples, leading to improved performance. Specifically, compared to default RAG for Phi3-GTL, retrieving nearest context only using PPS can reduce NMAE score from 0.1113 to 0.0951, which is a \textbf{14.6\%} error reduction. This demonstrates that \textbf{both linear and non-linear feature relationships are essential} for identifying important features in tabular data.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/pol_case_study.pdf}
    \caption{Case Study on the Pol Dataset. The figure displays the distribution of an important feature under three transformations: (left) the original feature values, (middle) standard normalized values, and (right) quantile normalized values}
    \label{fig:case_pol}
\end{figure}

\textbf{[R-33]} The \textit{regression-num-medium-0-pol} dataset describes a telecommunication problem, and we identify one of the most important features using Pearson Correlation between features and the label. As shown in Figure~\ref{fig:case_pol}, the feature distribution varies significantly under different normalization methods. Specifically, the original feature does not follow a true continuous distribution, with over 50\% of its values concentrated at a single point. In this case, quantile normalization disperses this concentrated value and other feature values, which can lead to non-optimal context sample retrieval due to reduced robustness in feature distance calculation. By replacing quantile normalization with standard normalization, the NMAE score dramatically improves from 0.1904 to 0.0819, representing a \textbf{57.0\%} reduction in error. This highlights the significant impact of context quality on LLM performance. It also suggests that users can \textbf{customize retrieval strategies by selecting normalization methods based on feature distributions} and leveraging domain-specific knowledge for specific datasets.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/bike_case_study.pdf}
    \caption{The figure displays the relationship between features (e.g., Temperature and Hour) and the label. It also visualizes bike rental patterns across different years and working-day conditions.}
    \label{fig:case_bike}
\end{figure}


\textbf{[R-25]} We analyze a failure case on the \textit{regression-cat-medium-0-Bike\_Sharing\_Demand} dataset, where the task is to predict the total count of rental bikes. This target is highly correlated with environmental and seasonal factors, including features such as year, season, month, hour, working day, temperature, and humidity. The most important features are hour, temperature, working day, and year. As shown in Figure~\ref{fig:case_bike}, there are distinct patterns between working days and non-working days. On working days, bike rental peaks at 8 am and around 6 pm, while on non-working days, rentals are concentrated between 10 am and 6 pm. Additionally, differences exist between the years 2021 and 2022.

In this case, our retrieval mechanism fails to capture the importance of working day and year because both Pearson Correlation and PPS measure feature importance based on single-feature relationships with the label, rather than feature combinations. We find that restricting context samples to the same year and working-day condition, while retrieving the nearest samples based on other features, reduces the NMAE score from 0.3754 to 0.1522, representing a \textbf{59.5\%} error reduction. This highlights the importance of \textbf{considering feature combinations and leveraging domain knowledge to design customized retrieval strategies} for improved performance.

\begin{table}
\centering
\caption{AUROC scores of Phi3-Medium-GTL and baseline models in RAG scenarios and full data Scenarios on the \textit{classif-cat-medium-0-rl} dataset.}
\label{tab:rl_results}
\begin{tabular}{c|cccc}
\hline
          & Phi3-Medium-GTL & XGBoost & TabPFN-v1 & TabPFN-v2 \\ \hline
RAG       & 0.7811          & 0.7845  & 0.7430    & 0.8621    \\
Full Data & -               & 0.8700  & 0.7307    & 0.9212    \\ \hline
\end{tabular}
\end{table}

\textbf{[C-17]} The \textit{classif-cat-medium-0-rl} dataset is another failure case for our method. As the dataset is anonymized, we lack access to specific feature or task information. However, from the results comparison in Table~\ref{tab:rl_results}, we observe that with retrieved context, Phi-3-Medium-GTL achieves performance comparable to XGBoost and outperforms TabPFN-v1. Interestingly, TabPFN-v2 significantly improves performance on this dataset in both RAG and full data scenarios compared to TabPFN-v1, even surpassing fully tuned XGBoost by over 5\% in AUROC. This improvement may be attributed to the diverse patterns in TabPFN-v2's prior-data. These findings suggest that \textbf{generating synthetic data with diverse feature distributions and feature-label relationships could further enhance the capabilities of LLMs}.


% ICLR version by Zhen Xu

% \section{Benchmark Preparation}

% To ensure a comprehensive evaluation of our model, we curated datasets from three distinct research domains: GTL~\cite{wen2024GTL}, TabR~\cite{gorishniy2024TabR}, and Tree~\cite{grinsztajn2022tree_gt_tab_nn}. In total, we prepared 29 classification datasets and 40 regression datasets, enabling a thorough assessment of model performance across diverse tabular data tasks.

% The design of these benchmarks facilitates a robust and transparent comparison between tree-based models, deep learning approaches, and large language models (LLMs). This ensures that the results accurately reflect the generalization capabilities of each model class across a wide range of tabular data scenarios. The findings from this study provide valuable insights for real-world applications, particularly in domains where tabular data structures are prevalent.

% \subsection{GTL}
% GTL (Generative Tabular Learning) is a research initiative focused on pretraining foundation models for generalizable inference across diverse tabular tasks. The model is pretrained on a comprehensive collection of 350 datasets and evaluated on an additional 50 datasets, encompassing both classification and regression tasks. This extensive pretraining enables GTL to generalize effectively across various tabular domains, delivering robust performance on a wide range of tasks without requiring task-specific fine-tuning. In this work, we pretrained different large language models (LLMs) using GTL with carefully filtered data to ensure no overlap between the pretraining data and the heldout benchmark datasets.

% For the pretraining data preparation, we focused on filtering the data to to prevent data leakage with the heldout datasets across all three benchmarks. After this process, we obtained 146 classification datasets and 173 regression datasets for pretraining.

% For the heldout benchmark preparation, we refined the evaluation process by retaining only the 16 test datasets after a strict filtering process based on feature validation and dataset size, including 9 classification datasets and 7 regression datasets. During this process, we meticulously examined feature values to ensure data integrity and prevent issues such as data leakage. This careful curation ensures that the test datasets are representative, and the results are reliable across diverse real-world tabular tasks.

% \subsection{TabR}

% TabR represents a significant advancement in retrieval-based tabular models. This research introduces an innovative approach that leverages embedding distance-based retrieval methods to enhance the accuracy and efficiency of tabular data processing. The model also integrates deep learning architectures with carefully optimized hyperparameters and an ensemble strategy, further improving its generalization capabilities across various tasks. Through extensive experimentation, TabR has demonstrated superior performance across 11 datasets that vary in scale and complexity, highlighting its adaptability to different tabular data characteristics.

% In preparing the benchmark for evaluation, we focused on eliminating redundancy by de-duplicating datasets compared to the 350 datasets used for pretraining GTL. This step was essential as we reused the pretrained LLM checkpoints from GTL to build upon its learned representations. For particularly large datasets, we applied a downsampling strategy to reduce the dataset size to a manageable 100K samples. The details of this preprocessing, including dataset characteristics and sampling methods, are provided in Appendix~\ref{sec:datastats}, ensuring full transparency in our experimental setup. This preprocessing step helps to balance computational efficiency while preserving the datasets' representativeness, ensuring the robustness of TabR’s performance across different scales.

% \subsection{Tree}

% Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data (Tree) is a comprehensive research effort presented at NeurIPS 2022 that delves into the continued superiority of tree-based models, such as decision trees, random forests, and GBDT, over deep learning approaches in tasks involving tabular data. The study examines the inherent strengths of tree-based models, which have consistently outperformed deep learning models across a variety of tabular tasks, despite the rapid advancements in neural network architectures. The authors compiled a diverse set of datasets, categorized into cat series (containing both numerical and categorical features) and num series (consisting exclusively of numerical features), to rigorously evaluate the performance of both model families.

% In preparing the benchmark for this study, we followed a similar approach to the TabR benchmark. Specifically, we undertook a systematic de-duplication process to ensure that datasets used for training or evaluation were not overly similar to the 350 datasets employed in the pretraining phase of GTL or those included in the TabR benchmark. This step was crucial in maintaining the integrity of the evaluation by avoiding redundant or overly correlated datasets that could skew results and artificially inflate performance. Moreover, for datasets that contained multiple versions or splits, we retain only a single representative split, ensuring that the evaluation remained both fair and manageable while reducing potential biases introduced by repeated usage of similar datasets.


% \section{Baselines}

% Across our prepared benchmarks, we have compared our TabRAG method against a diverse set of tabular prediction methods, including previously dominant GBDT baselines,  deep learning baselines, and other efforts in in-context learning or retrieval-based methods like TabPFN and TabR.

% \textbf{LR} Linear regression or logistic regression depending on the tabular task (Regression or Classification). It is a proof-of-concept baseline to tabular model. The biggest limitation is its linear capacity as for tabular tasks, nonlinearity is often essential. 

% \textbf{KNN} K-Nearest-Neighbor is a proof-of-concept model that non-parametrically infers only by a provided neighbours. In our experiment, we use the retrieved context samples as the neighbours. By comparing KNN and TabRAG on the sample neighbours, we are able to conclude that how LLM helps in terms of semantic understanding of the task and features. 

% \textbf{GBDT models} XGBoost, LightGBM, and CatBoost are three widely used GBDT frameworks designed to optimize speed and performance while handling large datasets, imbalanced data, and missing values. They are based on the principles of gradient boosting, which builds a model sequentially by adding weak learners (decision trees) that correct the errors of prior models. For decades, GBDT models are the only choice and best models on tabular data tasks. The running of GBDT models on our benchmark are based on the hyperparameter tuned towards each configuration. For benchmarks of TabR and Tree (ID 17 - 55), the authors of TabR have provided a curated hyperparameter optimization space and search algorithm. Following this setting, we perform hyperparameter optimization 15 times and ensemble their performances to gain almost the best models possible on tabular tasks. The tuned hyperparameters include feature fraction from 0.5 to 1.0, lambda l2 from 0 to 10, learning rate from 1e-3 to 1.0, number of leaves from 4 to 768, and min\_sum\_hessian\_in\_leaf from 1e-4 to 100.


% \textbf{MLP} Basically a multi-layer feed forward neural network. We use this baseline as a basic indication of how deep learning models performance at least with this simplest model. As usual, a carefully chosen space for  hyperparameters is provided to search and ensemble, including number of layers from 1 to 6, hiddem dimension from 64 to 1024, dropour rate from 0 to 0.5, learning rate from 1e-5 to 1e-3 and weight decay from 0 to 1e-4.

% % \textbf{SAINT} introduces a novel neural network architecture tailored for tabular data, leveraging row-wise attention and contrastive pre-training to improve performance. SAINT employs self-attention on the feature dimension, allowing the model to focus on important features in the tabular data. This mechanism captures relationships between features within a row, similar to the way transformers capture token relationships in text.
% % Additionally, contrastive learning is used during pre-training. The curated hyperparameter space for SAINT includes hidden dimension from 2 to 32, and dimension head from 4 to 48, depth from 1 to 4, attention dropout from 0 to 0.5, learning rate from 1e-5 to 1e-3, and weight decay from 1 to 1e-3.

% \textbf{FT-Transformer} is a neural network architecture specifically designed for tabular data, which adapts the transformer architecture to handle both categorical and numerical features. The key innovation lies in how it tokenizes and processes tabular data using transformer components traditionally used in NLP. FT-Transformer converts each feature (both categorical and numerical) into a feature token. Each feature in a row is treated as an independent input token, similar to words in a sentence for NLP models. Categorical features are embedded using learned embeddings, while numerical features are embedded via learned embeddings or positional encodings after normalization. The curated hyperparameter space include dimension of token from 16 to 384, number of blocks from 1 to 4, attention dropout from 0 to 0.5, learning rate from 1e-5 to 1e-3, and weight decay from 0 to 1e-3.

% \textbf{TabPFN} is a probabilistic neural network designed for tabular data that performs in-context learning. Instead of retraining on each dataset, TabPFN is pretrained on a massive set of small tabular tasks, ranging from synthetic to real-world datasets. During this pretraining, the model learns how to recognize common feature interactions and relationships typical of tabular data (e.g., correlations between features, handling of missing values, categorical-numerical interactions). When TabPFN is presented with a new dataset, it doesn't need explicit retraining. Instead, it uses the relationships between features and targets within the dataset itself to generalize in-context. It learns to understand how the data points and features within the specific dataset are structured and applies this understanding to make predictions. TabPFN doesn't support regression tasks and for larger datasets, it has a higher risk of Out-of-Memory. When we run TabPFN, we provide its training data either with only the chosen context, or with the full training data as the other baselines.

% \textbf{TabR} is a retrieval-based foundation model for tabular data that integrates in-context retrieval. TabR encodes both the query dataset and the stored datasets in a latent space using an embedding function. This encoding captures the structure of the tabular data, including relationships between features (numerical and categorical). The encoded query is compared with the pre-stored datasets to identify those that are closest in the latent space, to assist with predictions on new, unseen datasets. By fetching relevant historical data, TabR can make informed predictions without the need for extensive task-specific training. It leverages the diversity and richness of previously seen datasets to generalize to new tabular tasks. The curated hyperparamter search space for TabR is hidden dimension from 96 to 384, context dropout from 0 to 0.6, dropout from 0 to 0.6, learning rate from 3e-5 to 1e-3 and weight decay from 0 to 1e-4.




% \section{Detailed results}

% % main table for kdd, default, tree

% % \input{tables/perf-overall-kdd}

% \section{Dataset statistics}
% \label{sec:datastats}

% % \input{tables/datasets-stats-all}
% \input{tables/heldout_datasets_info}

% \section{Data}
% \label{app:data_appen}
% % In this section, we introduce our data curation, data filtering and data preprocessing.

% \subsection{Data Curation}
% % To ensure a comprehensive evaluation of our model, we curated datasets from three distinct public benchmarks: GTL~\citep{wen2024GTL}, TabR~\citep{gorishniy2024TabR}, and Tree~\citep{grinsztajn2022tree_gt_tab_nn}. In total, we prepared 29 classification datasets and 40 regression datasets, enabling a thorough assessment of model performance across diverse tabular data tasks.

% % \textbf{The benchmark from ~\citep{wen2024GTL}}. GTL (Generative Tabular Learning) is a research initiative focused on pretraining foundation models for generalizable inference across diverse tabular tasks. The model is pretrained on a comprehensive collection of 350 datasets and evaluated on an additional 50 datasets, encompassing both classification and regression tasks. This extensive pretraining enables GTL to generalize effectively across various tabular domains, delivering robust performance on a wide range of tasks without requiring task-specific fine-tuning.

% % \textbf{The benchmark from ~\citep{gorishniy2024TabR}}. TabR represents a significant advancement in retrieval-based tabular models. This research introduces an innovative approach that leverages embedding distance-based retrieval methods to enhance the accuracy and efficiency of tabular data processing. The model also integrates deep learning architectures with carefully optimized hyperparameters and an ensemble strategy, further improving its generalization capabilities across various tasks. Through extensive experimentation, TabR has demonstrated superior performance across 11 datasets that vary in scale and complexity, highlighting its adaptability to different tabular data characteristics.

% % \textbf{The benchmark from ~\citep{grinsztajn2022tree_gt_tab_nn}}. Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data (Tree) is a comprehensive research effort presented at NeurIPS 2022 that delves into the continued superiority of tree-based models, such as decision trees, random forests, and GBDT, over deep learning approaches in tasks involving tabular data. The study examines the inherent strengths of tree-based models, which have consistently outperformed deep learning models across a variety of tabular tasks, despite the rapid advancements in neural network architectures. The authors compiled a diverse set of datasets, categorized into cat series (containing both numerical and categorical features) and num series (consisting exclusively of numerical features), to rigorously evaluate the performance of both model families.



% \subsection{Data Filtering}

% \subsection{Data Processing}


% \section{Baselines}
% Description of baseline models, their configurations, and implementation details.


% \section{Ablation Study of the RAG Module}
% Evaluation of the impact of different components or configurations in the Retrieval-Augmented Generation (RAG) method.

% \section{Case Studies}
% In-depth analysis of specific examples or scenarios to illustrate key findings.


% \section{Comparisons Across Different LLMs with GTL-R}
% Comparative analysis of performance across various large language models (LLMs).

% \section{Analysis on Regression-Transformed Classification Tasks}
% Discussion of performance variations and insights specific to regression versus corresponding regression-transformed classification tasks.


% \section{Detailed Results}
% Comprehensive presentation of experimental results.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/decision_boundary_appendix_circle.pdf}
    \caption{Decision boundaries for the toy dataset \textit{circle}, evaluated across noise levels from 0.1 to 0.3 and training data sizes from 16 to 128.}
    \label{fig:decision_cir}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/decision_boundary_appendix_moon.pdf}
    \caption{Decision boundaries for the toy dataset \textit{moon}, evaluated across noise levels from 0.1 to 0.3 and training data sizes from 16 to 128.}
    \label{fig:decision_moon}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/decision_boundary_appendix_linear.pdf}
    \caption{Decision boundaries for the toy dataset \textit{linear\_rotation}, evaluated across noise levels from 0.1 to 0.3 and training data sizes from 16 to 128.}
    \label{fig:decision_line}
\end{figure}

\input{tables/heldout_datasets_info}
\input{tables/overall_results}