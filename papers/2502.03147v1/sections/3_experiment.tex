\section{Experiments}
\label{sec:exp}

We conduct extensive experiments to address the following research questions:
1) How effectively can our retrieval mechanism enhance LLM-based TabICL in leveraging large-scale datasets?
2) How does LLM-based TabICL perform in comparison to numeric-based TabICL models and classic tabular models that are well-tuned on a case-by-case basis?
3) What are the unique strengths, current limitations, and potential future directions for LLM-based TabICL?


% \begin{figure*}[t]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\linewidth]{main_figures/main_scaling_pool_and_context.pdf}}
% \caption{
% We investigate the effects of increasing the number of training instances ($|D_{\text{train}}^{T'}|$) and the number of in-context instances per test example ($N^C$) on the TabICL performance of Phi3-GTL models.
% In each subplot, we compare the scaling effects of two Phi3-GTL models with different retrieval policies: one that randomly selects in-context instances, denoted as "Random," and the other employing our default \texttt{TabRAG} module, denoted as "RAG".
% We use violin plots to visualize the performance distribution across multiple held-out datasets. Additionally, dashed lines are used to emphasize that the median prediction error of our approach follows a power-law relationship with the number of training instances.
% }
% \label{fig:scaling_pool_ctx}
% \end{center}
% \vskip -0.2in
% \end{figure*}


\subsection{Experimental Setups}
\label{sec:exp_setup}

\paragraph{LLM Post-Training}
We use real-world tabular datasets to post-train a base LLM using generative tabular learning (GTL) objective as did in~\citep{wen2024GTL}.
However, unlike their approach, we adopt Phi-3~\citep{abdin2024phi3} as the base LLM, extending the effective context length from 4K to 128K and aligning it with our default retrieval policy.
Details of this post-training process are provided in Appendix~\ref{app:method_align_rag_llm}. For brevity and clearness, we denote our post-trained model as Phi3-GTL and refer to our approach as RAG+Phi3-GTL throughout the remainder of this paper.

\paragraph{Held-out Datasets}
We compile a comprehensive benchmark from the literature~\citep{gorishniy2021revisit_tab_dnn,grinsztajn2022tree_gt_tab_nn,gorishniy2024TabR,wen2024GTL}, ensuring diverse datasets that may favor different learning paradigms.
To avoid data leakage, we carefully examine and exclude any datasets used during the training of the Phi3-GTL model.
This process results in 29 classification datasets and 40 regression datasets for held-out evaluation, covering a wide range of domains, feature dimensions, types, and distributions.
Details of the data construction process are included in Appendix~\ref{app:data_constr}.

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.98\linewidth]{main_figures/main_overall_comp_raw_metric.pdf}}
\caption{
An overall performance comparison of all models. In the left subplot, we use violin plots to show the AUROC scores of different models across 29 classification tasks, while the right subplot displays the NMAE scores for 40 regression tasks. Models are sorted by their median metric score across the held-out datasets, with dashed lines indicating these median scores in each subplot. Our approach, RAG+Phi3-GTL, is prefixed with a marker (*), for quick identification.
}
\label{fig:overall_comp}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.90\linewidth]{main_figures/main_ensemble_norm_metric.pdf}}
\caption{
    Ensemble performance comparisons of RAG+Phi3-GTL, TabPFN-v2, LightGBM, and CatBoost are presented, where normalized AUROC or NMAE scores (min-max normalized across methods for each dataset) are plotted to highlight their relative strengths across multiple datasets, while omitting absolute metric differences.
}
\label{fig:ensemble_res}
\end{center}
\vskip -0.2in
\end{figure*}


\paragraph{Baselines}
We include Phi3-GTL and RAG+KNN as two ablated variants of RAG+Phi3-GTL: the former uses randomly selected in-context instances, while the latter employs the same default retrieval policy but relies on the K-Nearest Neighbors (KNN) algorithm~\citep{fix1951knn,cover1967knn_cls} for prediction.
We compare against TabPFN-v1~\citep{hollmann2023TabPFN}, which supports only classification tasks, and TabPFN-v2~\citep{hollmann2025TabPFNv2}, the state-of-the-art TabICL model utilizing numeric representations.
In addition, our baselines include other representative tabular models such as XGBoost~\citep{chen2016XGBoost}, LightGBM~\citep{ke2017LightGBM}, CatBoost~\citep{prokhorenkova2018catboost}, MLP, FTT~\citep{gorishniy2021revisit_tab_dnn}, and TabR~\citep{gorishniy2024TabR}, all of which are extensively tuned via hyperparameter search for each dataset.
We also include several ``RAG + X'' baselines, where "X" represents models trained and inferred on the selected in-context instances using the same retrieval policy as RAG+Phi3-GTL. These include Logistic Regression (LR), TabPFN-v1, TabPFN-v2, and XGBoost.
These baselines are designed to highlight the TabICL capability of LLMs given limited in-context instances.

\paragraph{Metrics}
For classification tasks, we use the Area Under the Receiver Operating Characteristic curve (AUROC) as the primary evaluation metric. For regression tasks, we employ the Mean Absolute Error normalized by the label mean (NMAE).
Additionally, to compare the relative performance across a group of methods, we utilize group-wise min-max normalized AUROC and NMAE metrics.


\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{main_figures/main_per_dataset_comp.pdf}}
\caption{
    Per-dataset performance comparisons between RAG+Phi3-GTL and the two most competitive baselines, TabPFN-v2 and CatBoost, are presented, with dataset IDs sorted by performance gaps. Dashed lines and annotations are used to indicate the proportion of datasets where RAG+Phi3-GTL outperforms these baselines and where it significantly lags behind.
}
\label{fig:per_dataset_comp}
\end{center}
\vskip -0.2in
\end{figure*}


\begin{figure*}[t]
\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=0.9\linewidth]{main_figures/main_decision_boundary.pdf}}
\centerline{\includegraphics[width=1.0\linewidth]{main_figures/main_decision_boundary.pdf}}
\caption{
Decision boundary comparisons of various models, where each row corresponds to a specific set of training instances generated from a given data distribution. The first column visualizes these training instances, while the subsequent columns illustrate the decision boundaries of different models. The top two rows represent the same data distribution but with varying numbers of training instances, whereas the bottom two rows depict a different data distribution.
}
\label{fig:decision_boundary}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Scaling with Available Training Instances}
\label{sec:exp_scaling_pool_ctx}

Figure~\ref{fig:scaling_pool_ctx} illustrates the performance variations as the size of the training data and the number of in-context instances increase for our approach under two retrieval policies: Random and RAG (our default retrieval policy).
It is evident that the RAG policy enables Phi3-GTL to effectively leverage larger training datasets, while the Random policy lacks this capability. Specifically, with the RAG policy, the median prediction error demonstrates a power-law relationship with the number of training instances, expressed as $L(D) = (D_c / D)^{\alpha}$. For classification tasks, $L=1-\text{AUROC}$, $D_c \sim 6.05e^{-5}$, and $\alpha \sim 0.102$, whereas for regression tasks, $L=\text{NMAE}$, $D_c \sim 8.05e^{-8}$, and $\alpha \sim 0.053$. This finding highlights a favorable statistical learning characteristic: given a distinguishable feature space and sufficient training instances, the expected prediction error approaches zero.

Moreover, the RAG policy reduces the number of in-context instances required for accurate predictions. As shown in the right two subplots of Figure~\ref{fig:scaling_pool_ctx}, the model using the Random policy benefits significantly from an increased number of in-context instances. In contrast, with the RAG policy, performance often saturates as the number of adaptive in-context instances increases. This indicates that, for most datasets, tens of training instances are sufficient to form a supportive context for inferring the label of a test instance.


\subsection{Overall Comparison}
\label{sec:exp_overall_comp}


Figure~\ref{fig:overall_comp} presents an overall comparison of all models by illustrating the error distributions across held-out datasets. TabPFN-v2 emerges as the most competitive baseline in terms of the median prediction error. However, its wider error bars indicate sub-optimal performance in certain cases. In contrast, well-tuned tree-based models such as LightGBM and CatBoost, as well as neural models like FTT and TabR, demonstrate more robust performance with narrower error distributions.

When comparing our approach, RAG+Phi3-GTL, with these baselines, we observe significant improvements over its ablated variants, Phi3-GTL and RAG+KNN, underscoring the importance of both retrieval and TabICL components. Furthermore, RAG+Phi3-GTL is among the top-performing of ``RAG + X'' baselines, even surpassing RAG+TabPFN-v2 in terms of median prediction performance across held-out datasets.
This highlights the potential of TabICL based on text representations, which can uncover novel and highly effective ICL algorithms by operating in a text space.
Besides, our approach achieves zero NMAE for a specific integer regression task without requiring explicit programming, whereas all numeric models, by default, produce float outputs.
Lastly, RAG+Phi3-GTL still lags behind well-tuned baseline models and TabPFN-v2 in overall performance.


\subsection{Ensemble Results}
\label{sec:exp_ensemble_res}

We further explore the potential of RAG+Phi3-GTL by investigating its contribution to ensemble diversity, as shown in Figure~\ref{fig:ensemble_res}.
When comparing RAG+Phi3-GTL with TabPFN-v2, LightGBM, and CatBoost, we observe that although RAG+Phi3-GTL underperforms the top-performing baselines overall, it exhibits unique strengths in certain scenarios.
Moreover, a comparison of Ensemble-All with TabPFN+LightGBM and TabPFN+CatBoost reveals that these unique strengths translate into ensemble diversity, enhancing the robustness of overall ensemble performance.
These findings highlight the potential of leveraging language as an alternative interface for tabular data learning, complementing existing tabular learning algorithms.

\subsection{Per-dataset Comparisons}
\label{sec:exp_per_ds_comp}

These findings further motivate us to conduct per-dataset comparisons to identify datasets where RAG+Phi3-GTL excels and those where it still underperforms.
Figure~\ref{fig:per_dataset_comp} summarizes these results.
We observe that on approximately 17\%-20\% of datasets, RAG+Phi3-GTL outperforms the state-of-the-art TabICL model, TabPFN-v2, as well as the classic tree-based model, CatBoost, which has been carefully tuned for each dataset.
Furthermore, on over 80\% of datasets, the performance of RAG+Phi3-GTL falls within a small gap of these two competitive baselines.
These results indicate that RAG+Phi3-GTL is already a strong prediction model for most tabular datasets, suggesting that we could not only engage with a well-prepared LLM conversationally but also leverage it to understand tabular data, provide accurate predictions, and offer potential explanations.


\paragraph{Analysis of Failure Cases}
Per-dataset comparison results also prompt an investigation into why the performance of RAG+Phi3-GTL falls short in certain cases. Case studies detailed in Appendix~\ref{app:case_study} provide insights into these failure scenarios.
In summary, most failure cases are attributed to the limitations of the default retrieval policy, which struggles to extract effective in-context instances due to specific data characteristics (e.g., datasets R-25, R-33, and R-27). In these cases, we find that slight adjustments to the retrieval strategy—such as applying alternative numerical normalization methods for feature similarity calculations or leveraging prior knowledge to define instance similarities—can lead to significant performance improvements for RAG+Phi3-GTL.
These findings suggest that a non-parametric, default retrieval mechanism may be insufficient in certain scenarios. Practitioners could potentially achieve better performance by employing "retrieval engineering" (as discussed in Section~\ref{sec:method}).
Additionally, some failure cases, such as dataset C-17, reveal limitations in Phi3-GTL’s ability to perform effective TabICL on specific data distributions, where TabPFN-v2 significantly outperforms. We hypothesize that this gap arises from the limited coverage of data patterns during Phi3-GTL’s post-training phase, which utilized approximately 300 real-world datasets from~\citeauthor{wen2024GTL}. In contrast, TabPFN-v2 likely benefits from pre-training on a much broader family of synthesized datasets.


\subsection{Decision Boundary Analysis}
\label{sec:exp_dec_bound}

Figure~\ref{fig:decision_boundary} compares the decision boundaries of various models across four groups of synthetic instances.
We observe that RAG+Phi3-GTL produces a distinctive, non-smooth decision boundary, which is entirely different from models relying on numeric representations. This boundary reflects case-by-case generalization from known training instances to unseen regions, leaving more uncertain areas when data samples are sparse.

In terms of shape, the decision boundary of RAG+Phi3-GTL bears some resemblance to that of Nearest Neighbors. However, LLM-based TabICL generalizes far beyond a simple rule-based average of neighboring training instances. We hypothesize that this unique behavior arises from the text-based representation of tabular data and the ICL capability of LLMs.
These findings also highlight opportunities for further improving LLM-based TabICL. Specifically, to encourage smoother decision boundaries when sufficient training data is available, one approach could involve generating large-scale synthetic data and fine-tuning LLMs to emulate such behaviors.



% \subsection{Case Analysis}
% \label{sec:exp_case_study}

% Winning Case analysis against TabPFN v2:
% regression-cat-medium-0-analcatdata_supreme

% Failure Case analysis:
% lack of learning certain feature distributions
% fail to select the best in-context examples
% \subsection{Experiment settints}

% \textbf{Data preparation.} For evaluating the performance of tree-based models versus large language models (LLMs) and deep learning approaches on tabular data tasks, we prepare datasets drawn from three research branches—GTL~\cite{wen2024GTL}, TabR~\cite{gorishniy2024TabR}, and Tree~\cite{grinsztajn2022tree_gt_tab_nn}, encompassing 32 regression and 23 classification tasks. This diverse dataset collection enables robust and transparent comparisons across model classes. For GTL, we filtered 50 test datasets from an initial pool of 350, focusing on feature validation to avoid data leakage and ensure accurate classification of features. Only datasets meeting size and integrity criteria were retained, ensuring robust evaluations across diverse tabular tasks. In TabR, we reused GTL’s pretrained checkpoints, eliminating redundancy by de-duplicating datasets from GTL’s collection. Large datasets were downsampled to 100K samples to ensure computational efficiency without sacrificing representativeness, as detailed in Appendix~\ref{}. For Tree, datasets with numerical and categorical features were curated with a focus on eliminating overlap with GTL and TabR datasets. Multiple versions or splits were consolidated to a single representative version, ensuring a fair and unbiased comparison across model classes, and preserving dataset diversity.

% \textbf{Baselines.}  We provide below a list of our used baselines. For most baselines, a hyperparameter search space is carefully prepared by the literature and we just follow this space setting and perform hyperparameter optimization on this then ensemble the results to boost the performance.

% \begin{itemize}
%     \item \textbf{LR} (Linear/Logistic Regression): A simple, linear baseline for tabular tasks that struggles with nonlinearity, limiting its performance on more complex datasets.
%     \item \textbf{KNN} (K-Nearest-Neighbor): A non-parametric model that makes predictions based on nearby samples. We use the same context samples as retrieved by TabRAG, providing a simple baseline for context ensemble
%     \item \textbf{GBDT} (XGBoost~\cite{chen2016XGBoost}, LightGBM~\cite{ke2017LightGBM}, CatBoost~\cite{prokhorenkova2018catboost}): Gradient boosting frameworks that build sequential decision tree models to optimize performance, particularly excelling in large, imbalanced, or missing data. Extensive hyperparameter tuning is applied for optimal performance.
%     \item \textbf{MLP} (Multi-layer Perceptron): A basic neural network used to evaluate the effectiveness of deep learning on tabular data. We applied a range of hyperparameters and ensemble strategies to maximize its performance.
%     \item \textbf{SAINT}~\cite{somepalli2021SAINT}: A novel architecture that uses self-attention and contrastive learning to capture feature relationships within rows of tabular data. It enhances performance by focusing on important features and pre-training on large tabular datasets.
%     \item \textbf{FT-Transformer}~\cite{gorishniy2021revisit_tab_dnn}: Adapts transformer architecture for tabular data by tokenizing both categorical and numerical features. It uses learned embeddings and positional encodings to process features, enabling the transformer to handle tabular structures effectively.
%     \item \textbf{TabPFN}~\cite{hollmann2022TabPFN}: A probabilistic model that uses in-context learning, pretrained on a wide variety of small tabular datasets to learn common feature interactions, allowing predictions without explicit retraining. It struggles with large datasets and regression tasks.
%     \item \textbf{TabR}~\cite{gorishniy2024TabR}: A retrieval-based foundation model that encodes query datasets and searches for similar datasets in latent space, enabling it to generalize to new tasks by leveraging historical data. Its predictions benefit from learned embeddings and in-context retrieval.
% \end{itemize}
