\section{Related Work}
\label{sec:rel_work}

\paragraph{An Overview of Tabular Data Learning}
Tabular data learning has a long-standing history in machine learning research. Prior to the advent of deep learning~\citep{lecun2015DL}, tree-ensemble models~\citep{chen2016XGBoost,ke2017LightGBM,prokhorenkova2018catboost} were developed and quickly became the preferred choice for various tabular data science competitions. Motivated by the success of automatic representation learning in deep neural networks, researchers began exploring effective neural architectures for tabular data~\citep{huang2020TabTransformer,arik2021TabNet,katzir2021Net-DNF,gorishniy2021revisit_tab_dnn}. To this day, some practitioners argue for the superiority of tree-based models, and the competition between tree-based and neural tabular models continues~\citep{shwartzziv2022tab_dl_is_not_all,gorishniy2024TabR}. However, a significant advantage of neural tabular models is their flexibility to integrate with other advanced learning mechanisms, such as self-supervised learning~\citep{yoon2020VIME,somepalli2021SAINT,ucar2021SubTab,bahri2022SCARF} and pre-training~\citep{wang2022TransTab,levin2023transfer_tab_nn,zhu2023XTab,yang2023UniTabE,yan2024TP-BERTa,ye2024CM2}. In recent years, breakthroughs in natural language processing~\citep{kenton2019BERT,Brown2020GPT-3} have led to a trend of combining language models with tabular data learning~\citep{dinh2022LIFT,hegselmann2023TabLLM,yan2024TP-BERTa,ye2024CM2,wen2024GTL,gardner2024TabuLa}.
In this work, we build upon a prior approach~\citep{wen2024GTL} that introduces generative tabular learning that post-trains LLMs to follow TabICL or zero-shot prediction instructions.
Our key contributions include defining a retrieval-augmented TabICL formulation, developing a universal non-parametric retrieval module, and introducing a retrieval-guided training process.


\paragraph{ICL}
ICL emerged from research on scaling language modeling~\citep{kaplan2020ScalingLaws,Brown2020GPT-3}, where it was discovered that LLMs could automatically mimic the behavior of a few examples provided in the context as demonstrations.
Here we summarize some representative follow-up studies that are closely related to this paper, such as selecting effective ICL examples for natural language tasks~\citep{liu2021GoodICLExa,xu2023KNNPrompt}, conducting ICL-style tuning to enhance ICL performance~\citep{wei2022FLAN,gu2023Pre-trainICL}, and understanding the ICL mechanism from the perspectives of Bayesian inference~\citep{muller2021TransDoBayInf,xie2022explainICL} and gradient descent~\citep{dai2023ICLasGD}.
For readers seeking a deeper understanding of ICL, we recommend the comprehensive survey by~\citeauthor{dong2022SurveyICL}.
However, despite the significant advancements of ICL in language modeling, TabICL has received relatively little attention within the tabular data learning community.

\paragraph{TabICL}
As discussed in the introduction, existing TabICL approaches can be broadly categorized into two groups: the TabPFN series~\citep{hollmann2023TabPFN,hollmann2025TabPFNv2} and LLM-based variants~\citep{wen2024GTL,gardner2024TabuLa}.
Both approaches utilize Transformer~\citep{vaswani2017attention} architectures as their backbones but differ in two major aspects.
First, the TabPFN series adopts numerical representations, whereas LLM-based approaches employ text representations. These representations offer distinct strengths and limitations: numerical representations are highly efficient but confined to specific tabular predictions, while text representations can accommodate fewer in-context instances yet integrate seamlessly with other LLM functionalities, infrastructures, and application interfaces.
Second, TabPFN models are trained on synthetic datasets generated from structured causal networks, whereas LLM-based approaches rely on post-training with real-world datasets. In principle, these two data recipes could be combined to achieve complementary benefits. However, to the best of our knowledge, the TabPFN series has not disclosed its synthesized pre-training datasets, particularly the critical data upgrade from TabPFN-v1~\citep{hollmann2023TabPFN} to TabPFN-v2~\citep{hollmann2025TabPFNv2}.
In contrast, LLM-based studies have released fully reproducible pipelines~\citep{wen2024GTL,gardner2024TabuLa}.
% In contrast, LLM-based studies have released fully reproducible pipelines, including generative tabular learning\footnote{\url{https://github.com/microsoft/Industrial-Foundation-Models}} by~\citeauthor{wen2024GTL} and tabular transfer learning\footnote{\url{https://github.com/mlfoundations/rtfm}} by~\citeauthor{gardner2024TabuLa}.
In this study, we focus on extending the applicability of LLM-based TabICL from zero-shot and few-shot to any-shot settings and explore the potential of leveraging language as a universal interface for data-driven learning.
% We primarily follow~\citeauthor{wen2024GTL}’s implementation in our experiments and include additional comparisons with~\citeauthor{gardner2024TabuLa}’s model in the appendix.


\paragraph{RAG}
RAG also emerged from language modeling research to alleviate the limitations of LLMs in handling knowledge-intensive language tasks~\citep{lewis2020RAG}.
This mechanism enables models to leverage external knowledge bases to supplement their representations, producing more accurate and informed responses.
\cite{gao2023SurveyRAG} conducted an extensive survey of numerous follow-up studies on RAG, categorizing them into three main areas: pre-training, fine-tuning, and inference.
In contrast, the application of RAG in tabular data learning remains relatively underexplored.
A recent success in this domain is TabR~\citep{gorishniy2024TabR}, which enhances representations for a neural tabular model by extracting nearest neighbors, thereby improving performance.
In this study, we demonstrate that RAG can enable LLM-based TabICL to effectively handle large-scale datasets.


% In this work, we aim to establish a systematic framework for integrating RAG with TabICL.
% Specifically, our goals are to understand its potential in substituting state-of-the-art optimization-based tabular models, identify current limitations of this new paradigm, and explore future opportunities.


