\section{Related Works}
\textbf{Circuit Based Interpretability in Language Models.} With the advent of language models, several recent works have focused on a mechanistic understanding of language models~\citep{meng2023locatingeditingfactualassociations, turner2024activationadditionsteeringlanguage, lieberum2023doescircuitanalysisinterpretability, mcdougall2023copysuppressioncomprehensivelyunderstanding, gould2023successorheadsrecurringinterpretable}. One of the primary benefit of transformer based language models is that the final logit representation can be decomposed as a sum of individual model components~\citep{elhage2021mathematical}. Based on this decomposition, one can extract task-specific causal sub-graphs (i.e., circuits) of internal model components in language models. Early works have extracted such circuits for indirect-object identification~\citep{wang2022interpretabilitywildcircuitindirect}, greater-than operation~\citep{hanna2023doesgpt2computegreaterthan} and more recently for entity-tracking~\citep{prakash2024finetuningenhancesexistingmechanisms}. %Recently, there has been an increasing focus on the practical aspects of mechanistic interpretability such as refusal mediation~\citep{arditi2024refusallanguagemodelsmediated, zheng2024promptdrivensafeguardinglargelanguage} or safety in general~\citep{zou2023representationengineeringtopdownapproach}. 
Circuits can also be constructed as sub-graphs of neurons in the language model, but it often comes with increased complexity of interpretation~\citep{elhage2022superposition}. Recently, there has been an increasing focus on the practical aspects of mechanistic interpretability such as refusal mediation~\citep{arditi2024refusallanguagemodelsmediated, zheng2024promptdrivensafeguardinglargelanguage} or safety in general~\citep{zou2023representationengineeringtopdownapproach}. In our paper, we focus on extracting circuits for a real-world task such as extractive QA with a particular emphasis on practical applications such as {\it attribution} and {\it steering}.

\textbf{Applications in Context-Augmented QA.} With the advent of retrieval-augmented generation~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, gao2024retrievalaugmentedgenerationlargelanguage} language models have been increasingly used for real-world Question-Answering (QA) tasks. One of the primary enhancement of context-augmented QA lies in the ability to provide reliable grounding (i.e., attribution) in the context for the generated answer~\citep{li2023surveylargelanguagemodels, khalifa2024sourceawaretrainingenablesknowledge, huang2024citationkeybuildingresponsible, ye2024effectivelargelanguagemodel}. In recent times, there have been a large set of works which improve LLM responses by reducing hallucinations and improving grounding in the input context~\citep{ye2024effectivelargelanguagemodel, asai2023selfraglearningretrievegenerate, xu2024searchinthechaininteractivelyenhancinglarge, zhang2024knowledgealignmentproblembridging}.
Beyond grounding, \citep{wu2024clashevalquantifyingtugofwarllms, xu2024knowledgeconflictsllmssurvey, mallen2023trustlanguagemodelsinvestigating, wang2023causalviewentitybias} investigate the interplay between model's use of parametric vs. context knowledge.
%We note that our paper tests the ability of the mechanistic insights from circuits towards performing these applications for extractive QA tasks. 
\vspace{-0.4cm}