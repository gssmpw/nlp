%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
%\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.

\usepackage{hyperref}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algpseudocode}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx} 
\usepackage{float}
\usepackage{wrapfig}
\usepackage{silence}
\WarningFilter{latex}{`h' float specifier changed to `ht'}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tabularx}


%\usepackage[noend]{algpseudocode}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\newcommand{\attnattrib}{\textsc{AttnAttrib}}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{lipsum}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On Mechanistic Circuits for Extractive Question-Answering}

\begin{document}

\twocolumn[
\icmltitle{On Mechanistic Circuits for Extractive Question-Answering}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Samyadeep Basu}{comp}
\icmlauthor{Vlad Morariu}{yyy}
\icmlauthor{Zichao Wang}{yyy}
\icmlauthor{Ryan Rossi}{yyy}
\icmlauthor{Cherry Zhao}{yyy}
\icmlauthor{Soheil Feizi}{comp}
\icmlauthor{Varun Manjunatha}{yyy}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Adobe Research}
\icmlaffiliation{comp}{University of Maryland, College Park}

\icmlcorrespondingauthor{Samyadeep Basu}{sbasu12@umd.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
 % otherwise use the standard text.

\begin{abstract}
Large language models are increasingly used to process documents and facilitate question-answering on them. In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information. We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable {\it data attribution by default}, thereby obtaining attribution for free in just the model's forward pass. Using this insight, we then introduce~\attnattrib{}, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution~\attnattrib{} as an additional signal during the forward pass. Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering. 

\end{abstract}

\section{Introduction}
\blfootnote{1: University of Maryland, College Park, 2: Adobe Research, Correspondence: sbasu12@umd.edu}In recent times, large language models have been used to process documents, webpages and transcripts as context and answer questions about them.  We refer to the task of answering a question by directly extracting words from the context/document as extractive Question-Answering (QA), in contrast to "abstractive QA" or "open-ended QA" where the words comprising the answer may not necessarily appear in the context. In the extractive QA case, a language model can either answer from the context, hallucinate entirely from its parametric memory or interpolate between the two. A mechanistic understanding of such a task with a \emph{circuit} (a sub-graph of the language model's computational graph) can not only provide insights on the inner workings of the model for this task, but can also enable downstream applications such as {\it data-attribution} (i.e., pointing to the source in the context which contributes to the answer) and {\it model steering} (i.e., enabling the model to answer from the context, rather than hallucinate from its parametric memory). Earlier works on mechanistic circuits~\citep{bereska2024mechanisticinterpretabilityaisafety, elhage2021mathematical} for large language models~\citep{touvron2023llama2openfoundation, jiang2023mistral7b, vicuna2023} have discovered circuits for language tasks such as entity tracking~\citep{prakash2024finetuningenhancesexistingmechanisms}, indirect object identification~\citep{wang2022interpretabilitywildcircuitindirect} or simple math operations such as ``greater than''~\citep{hanna2023doesgpt2computegreaterthan}. %Mechanistic circuits are inherently a directed acyclic sub-graph of the computational graph of the transformer-based language model due to the presence of residual connections~\citep{elhage2021mathematical}. 
%The sub-graph contains various nodes and edges, where the nodes are distinct components of the language models such as attention heads and MLP layers. 
While circuits are a principled way to mechanistically understand language models, we note certain limitations within existing works: 
(i) Tasks such as {\it entity tracking} or {\it indirect object identification} are inherently simple and may not capture the complexity of real-world applications for language models and (ii) It remains uncertain whether understanding language models through circuits will translate into practical applications.

\begin{figure*}
    \hskip 0.0cm
    \includegraphics[width=17.0cm, height=6.6cm]{./figs/teaser_2.pdf}
    \vspace{-0.3cm}
    \caption{\label{teaser} \textbf{Obtaining Circuits for Extractive QA in Language Models.} We use our probe dataset along with path patching to extract circuits corresponding to (i) {\it Context} and (ii) {\it Memory Faithfulness}. We find that a small set of attention heads from the circuit can be used towards performing data-attribution in one forward pass and also steering language models towards context faithfulness. In this figure, we provide one step of the patching operation and expand on it Sec.(\ref{patching_illustration}).}%
    \vspace{-0.6cm}
\end{figure*}

In our paper, we extract mechanistic circuits for a real-world extractive QA task and use insights from the mechanistic circuit to provide two downstream applications: (i) Data attribution to context and (ii) Steering the language model towards improved context faithfulness.  We focus on this task, due to the importance of retrieved-context augmented language models in recent times which unlocks various user-facing downstream applications~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, gao2024retrievalaugmentedgenerationlargelanguage, asai2023selfraglearningretrievegenerate}. We extract two kinds of circuits from language models: (i) {\it Context-Faithfulness Circuit: }A circuit used by the language model when it solely answers from the context and (ii) {\it Memory-Faitfulness Circuit: }A circuit used by the language model when it solely answers from its parametric memory. To extract these circuits,  we first design a probe dataset (with minimal assumptions about it's inherent structure such as fixed length) and use Causal Mediation Analysis (CMA)~\citep{wang2022interpretabilitywildcircuitindirect, pearl2013direct, zhang2024bestpracticesactivationpatching} to find the subset of nodes and edges in the computational graph of the language model which are causal to the model outputs. In particular, we observe that the circuits activated during the model's use of context differ significantly from those used for parametric memory. We validate different components of the circuit by various ablations and offer insightful mechanistic understandings.


With the extracted circuit components, we then investigate their roles for the task of extractive QA. We first find that a small set of attention heads in the circuit perform reliable {\it data attribution by default} (i.e., where the specific input data in the context used to produce an answer is identified), inherently obtaining data attribution in just one forward pass for each token generation. Leveraging this observation, we introduce~\attnattrib{}, which can reliably perform data attribution using \textbf{just one attention head} across various real-world QA benchmarks (e.g., HotPotQA, Natural-Questions, NQ-Swap) and white-box language models (Vicuna, Llama-3). In fact, through extensive empirical experiments, we show that~\attnattrib{} can obtain state-of-the-art data-attribution results when compared to other strong baselines for extractive QA tasks without any additional forward pass or auxiliary model, effectively obtaining \textbf{attribution for free}. 
%However, we also note certain limitations highlighting that the attribution is effective for QA tasks which does not involve any inherent reasoning \jw{i wonder if this statement is better deferred to the limitation section towards the end of the paper}.  
We also find that when the language model answers using the parametric memory circuit, the attribution heads still display a high attention to the answer tokens in the context. With this insight, we design a simple model steering method for improved context-faithfulness, by using the attributions from~\attnattrib{} as an additional source of information. Across various empirical experiments, we find that the addition of attribution during prompting leads to improvements upto 9$\%$ on popular extractive QA datasets.

 Overall, our paper extracts mechanistic circuits in language models for a real-world task of extractive QA. Beyond mechanistic interpretability of QA tasks, our paper highlights that certain components of the circuit can be useful for downstream applications such as {\it data-attribution} and also {\it steering language models} towards being more faithful to the context (thus improving generalization). In summary, our contributions are as follows:
\begin{itemize}
\item We extract mechanistic circuits (which provide a causal view) in language models for the real-world task of extractive QA for when the model answers from the context and from the parametric memory. 
\item We provide salient insights on the underlying mechanics of language models highlighting the interplay between parametric memory and context through the lens of extracted circuits.
% \item Through extensive experiments, we show that {\it only} one attention head from the circuit can be reliably used for data-attribution using~\attnattrib{} in extractive QA tasks, leading to strong results across various benchmarks. In addition, we show that the extracted attributions can be used in the prompt towards improved generalization. 
% \item Through extensive experiments, we demonstrate that {\it only one attention head} from the circuit can be reliably utilized for data attribution with \attnattrib{} in extractive QA tasks, yielding strong results across multiple benchmarks. Furthermore, we show that these extracted attributions can be incorporated into prompts to enhance generalization. 
\item  Using the insights from the circuit mechanism, we provide two practical applications: (i) Data-attribution to context with~\attnattrib{} and (ii) Model steering towards context-faithfulness using the attributions from~\attnattrib{} -- both reliable enhancements which can ensure that the model does not hallucinate.
%{\color{blue} Add steering / hallucination results}
\end{itemize}
\section{Related Works}
\textbf{Circuit Based Interpretability in Language Models.} With the advent of language models, several recent works have focused on a mechanistic understanding of language models~\citep{meng2023locatingeditingfactualassociations, turner2024activationadditionsteeringlanguage, lieberum2023doescircuitanalysisinterpretability, mcdougall2023copysuppressioncomprehensivelyunderstanding, gould2023successorheadsrecurringinterpretable}. One of the primary benefit of transformer based language models is that the final logit representation can be decomposed as a sum of individual model components~\citep{elhage2021mathematical}. Based on this decomposition, one can extract task-specific causal sub-graphs (i.e., circuits) of internal model components in language models. Early works have extracted such circuits for indirect-object identification~\citep{wang2022interpretabilitywildcircuitindirect}, greater-than operation~\citep{hanna2023doesgpt2computegreaterthan} and more recently for entity-tracking~\citep{prakash2024finetuningenhancesexistingmechanisms}. %Recently, there has been an increasing focus on the practical aspects of mechanistic interpretability such as refusal mediation~\citep{arditi2024refusallanguagemodelsmediated, zheng2024promptdrivensafeguardinglargelanguage} or safety in general~\citep{zou2023representationengineeringtopdownapproach}. 
Circuits can also be constructed as sub-graphs of neurons in the language model, but it often comes with increased complexity of interpretation~\citep{elhage2022superposition}. Recently, there has been an increasing focus on the practical aspects of mechanistic interpretability such as refusal mediation~\citep{arditi2024refusallanguagemodelsmediated, zheng2024promptdrivensafeguardinglargelanguage} or safety in general~\citep{zou2023representationengineeringtopdownapproach}. In our paper, we focus on extracting circuits for a real-world task such as extractive QA with a particular emphasis on practical applications such as {\it attribution} and {\it steering}.

\textbf{Applications in Context-Augmented QA.} With the advent of retrieval-augmented generation~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, gao2024retrievalaugmentedgenerationlargelanguage} language models have been increasingly used for real-world Question-Answering (QA) tasks. One of the primary enhancement of context-augmented QA lies in the ability to provide reliable grounding (i.e., attribution) in the context for the generated answer~\citep{li2023surveylargelanguagemodels, khalifa2024sourceawaretrainingenablesknowledge, huang2024citationkeybuildingresponsible, ye2024effectivelargelanguagemodel}. In recent times, there have been a large set of works which improve LLM responses by reducing hallucinations and improving grounding in the input context~\citep{ye2024effectivelargelanguagemodel, asai2023selfraglearningretrievegenerate, xu2024searchinthechaininteractivelyenhancinglarge, zhang2024knowledgealignmentproblembridging}.
Beyond grounding, \citep{wu2024clashevalquantifyingtugofwarllms, xu2024knowledgeconflictsllmssurvey, mallen2023trustlanguagemodelsinvestigating, wang2023causalviewentitybias} investigate the interplay between model's use of parametric vs. context knowledge.
%We note that our paper tests the ability of the mechanistic insights from circuits towards performing these applications for extractive QA tasks. 
\vspace{-0.4cm}
\section{Deciphering a Circuit for Extractive QA}
\label{hierarchy_0_circuit}
\textbf{Nodes and Edges in a Language Model Circuit.} Recent decoder-only large language models, denoted by $g_{\phi}$, such as Llama variants~\citep{touvron2023llama2openfoundation, dubey2024llama3herdmodels}, are built on the seminal transformer architecture~\citep{DBLP:journals/corr/VaswaniSPUJGKP17}. A notable characteristic of these architectures is that the token representation at any layer can be expressed as a function of internal model components, such as multi-layer perceptrons (MLPs) and attention heads, from earlier layers~\citep{elhage2021mathematical}. As a result, the computational graph underlying a language transformer is a directed acyclic graph, with nodes representing components like MLPs and attention heads (or layers), and edges representing connections formed by the residual stream. 

We are particularly interested in obtaining a sub-graph of the transformer's computational graph which is responsible towards context-augmented language modeling. In particular, we extract two circuits: (i) {\it Context-Faithfulness Circuit}, which is used when the underlying language model answers from the context, and (ii) {\it Memory-Faithfulness Circuit}, which is used when the language model solely answers from the parametric memory, ignoring the context. To extract the respective circuits, we first design a probe dataset mimicking both these conditions which we use with causal mediation analysis~\citep{wang2022interpretabilitywildcircuitindirect} and our interventional steps in Sec. (\ref{interventional_algo}). 
\begin{figure*}
    \hskip 0.6cm
    \includegraphics[width=15.5cm, height=9cm]{./figs/component_updated.pdf}
    \vspace{-0.3cm}
    \caption{\label{circuit_composite}(i) \textbf{Top Row (Context Circuit Components).} We find that a small set of attention layers and attention heads are sufficient towards a high \textbf{average metric score} across all the models. However we find that for Vicuna and Phi-3, patching MLPs do not lead to a high metric score. For Llama-3-8B, we find MLP-31 to have a high direct effect, which when greedily combined with other MLP layers obtain higher scores; (ii) \textbf{Bottom Row (Memory Circuit Components).} We find that a large number of attention heads and layers are required to obtain a high metric score. Unlike the context circuit, we find MLPs to be important for the memory circuit.}%
    \vspace{-0.6cm}
\end{figure*}
\vspace{-0.1cm}
\subsection{Designing the Probe Dataset}
\label{probe_dataset_design}
The design of a probe dataset is extremely crucial in extracting circuits for a language model task as shown in earlier works~\citep{wang2022interpretabilitywildcircuitindirect,hanna2023doesgpt2computegreaterthan}. We are interested in obtaining a circuit for context-faithfulness as well as one when the model answers from the parametric memory while ignoring the context. To this end, we design two probe datasets $\mathcal{D}_{copy}$ and $\mathcal{D}_{memory}$ respectively for them. 
Each example in $\mathcal{D}_{copy}$ and $\mathcal{D}_{memory}$ consists of factual questions sourced from the Known dataset~\citep{meng2023locatingeditingfactualassociations}. For each question $q_{i}$ in both datasets, we use Llama-3-70B-Instruct to generate a context $c_{i}$ related to the subject and answer for $q_{i}$. To guarantee that for each question in $\mathcal{D}_{copy}$, the language model \textbf{only} answers from the context (and not the memory), we replace the answer tokens in the context $c_{i}$ with a set of tokens which are semantically similar to the original answer(e.g., in Fig.(\ref{teaser}), we replace {\it Seattle} with {\it New York} in the original context {\it Space Needle is located in Seattle}, where the original answer was {\it Seattle}). 
In $\mathcal{D}_{memory}$, to force the model to answer from the parametric memory while ignoring the context, we replace the answer token with a token which is far away in semantic meaning from the original answer (e.g., replace {\it Seattle} with a punctuation of ``--''). In total, we curate 1000 questions (with their corresponding modified contexts) in $\mathcal{D}_{\text{copy}}$ and $\mathcal{D}_{\text{memory}}$.  We note that each entry $x_{i} \in \mathcal{D}_{\text{copy/memory}}$, contains a question $q_{i}$, a subject of the question $s_{i}$, ground-truth answer denoted by $a_{i}$, the modified context $c_{i}'$ and the original context $c_{i}$. Along with $c_{i}$ and $c_{i}'$, we add a corrupted context $c_{i, \text{corrupted}}$, where the subject and the answer token in the context is replaced by unrelated tokens and $q_{i, \text{corrupted}}$ where the subject in the question is replaced by a randomly sampled token. For e.g., as seen in Fig.(\ref{teaser}) the corrupted context ({\it Big Sur is in California}) is formed by replacing the subject and the answer tokens in the modified context. Full description of the dataset $\mathcal{D}$ can be accessed in Sec.(\ref{dataset_probe})

\textbf{Distinctions from Other Circuit Datasets.} Previous work on circuit extraction for entity tracking and indirect object identification relies on fixed templates for generating examples. However, for real-world tasks like extractive QA, probe datasets cannot use templates due to varying context lengths and unique information across examples.
\vspace{-0.3cm}
\subsection{Interventional Steps for Extracting Circuits}
\label{interventional_algo}
Our interventional method is developed on the foundational technique of causal mediation analysis~\citep{pearl2013direct}. The primary idea of causal mediation analysis is to find important paths in a causal graph, by performing an interventional operation on a small set of nodes and measuring the change in the final output. In our use-case, we adapt this method to find a sub-graph of internal model components such that ablating them leads to a decrease in ability of the model to perform QA (either through extraction from the context or using the parametric memory while ignoring the context). Below we provide the algorithmic description: 

\textbf{Algorithmic Description.} Given the language model $g_{\phi}$ and its associated computational graph $\mathcal{G}$, our objective is to extract a sub-graph (i.e., a circuit) $\mathcal{C} \in \mathcal{G}$ which is responsible towards the QA task. We obtain the nodes and edges of the circuit $\mathcal{C}$ in a hierarchical manner. First, we obtain a set of nodes and edges in hierarchy 0 denoted as $(\mathcal{N}_{0}, \mathcal{E}_{0})$ which have the highest direct effect to the final logit. In the next step for hierarchy 1, we obtain a set of nodes and edges $(\mathcal{N}_{1}, \mathcal{E}_{1})$, which have the highest direct effect on the nodes from hierarchy 0. For any hierarchy k, we obtain a set of nodes and edges $(\mathcal{N}_{k}, \mathcal{E}_{k})$ which have a high direct effect on the nodes $(\mathcal{N}_{k-1}, \mathcal{E}_{k-1})$ from the previous hierarchy. For obtaining the nodes at the $k^{th}$ hierarchy, we create two instantiations of the underlying language model $g_{\phi}$. The first instantiation is denoted as $g_{\phi, \text{clean}}$, with the original question $q_{i}$ and modified context $c_{i}'$ as the input. The second instantiation of the language model is $g_{\phi, \text{corrupted}}$, where the input context as well as the question is corrupted as $c_{i, \text{corrupted}}$ and $q_{i, \text{corrupted}}$ respectively. With this corrupted input, model $g_{\phi, \text{corrupted}}$ assigns a low probability to the generated answer tokens $a_{i}$ from $g_{\phi, \text{clean}}$. Using these two model instantiations, the goal of the patching operation is to copy the activations of a node $g_{j} \in \mathcal{G}$ from $g_{\phi, \text{corrupted}}$ to $g_{\phi, \text{clean}}$, while restoring the activations of all the other nodes in $g_{\phi, \text{clean}}$ to its original state. We denote the patched model as $g_{\phi, \text{patch}}$ and use $\text{score}(i, g_{j}) = 1 - \mathcal{P}_{g_{j}, \text{patch}}(a_{i})$ to measure the importance of the component $g_{j}$ for the $i^{th}$ example. For the component $g_{j}$, we then compute the \textbf{average metric score} as $\text{score}(g_{j}) = \sum_{i=1}^{|\mathcal{D}|} \text{score}(i,g_{j})/|\mathcal{D}|$. We then sort the scores of the various components in the computational graph as $\text{score}(g_{j}) \hspace{0.1cm} \forall j \in N$ in decreasing order as $\{ g_{j}\}_{j=1}^{N}$ and greedily select the minimum value of $k$, such that the \textbf{average metric score} of patching multiple components together: $\text{score}(\{g_{j}\}_{j=1}^{k}) \geq \delta$.  These selected components $\{g_{j}\}_{j=1}^{k}$ form the nodes in $\mathcal{N}_{k}$. In our experiments, we only use the MLPs, the attention heads and layers as the different model components which are patched. The final circuit $\mathcal{C}$ consists of the nodes $\{\mathcal{N}_{k}\}_{k=1}^{K}$ and their associated edges, where $K$ denotes the maximum hierarchy of the circuit. 
\begin{figure*}
    \hskip 0.35cm
    \includegraphics[width=16.0cm, height=4.4cm]{./figs/attention_select.pdf}
    \vspace{-0.4cm}
    \caption{\label{attention_select} 
    \textbf{We find that one attention head in the context faithfulness circuit obtains a low entropy value in the context window}. Qualitative results shows that this attention head for Vicuna leads to peaky attention values in the context span containing the answer, whereas other attention heads produce either diffused attentions or erroneous attentions. Further results on Llama-3 and Phi-3 in Appendix.}%
    \vspace{-0.2cm}
\end{figure*}

\textbf{Circuit for Context Faithfulness.} We extract the circuits using $\mathcal{D}_{copy}$ as the probe dataset for the patching operations. We perform the patching operation at the last residual stream position. We selected this position because the information in the last residual stream plays a crucial role in determining the probability distribution of the next generated token, which is also used in recent mechanistic interpretability works~\citep{arditi2024refusallanguagemodelsmediated, turner2024activationadditionsteeringlanguage}. 

\textbf{Circuit for Parametric-Memory Faithfulness.} In this case, we use $\mathcal{D}_{memory}$ as the probe dataset for the patching operation. We extract the circuit at the same token positions as the ones for context faithfulness. 
\begin{figure*}
    \hskip 1.0cm
    \includegraphics[width=14.5cm, height=6.3cm]{./figs/model_ablations_qa.pdf}
    \vspace{-0.5cm}
    \caption{\label{ablations_model} 
    \textbf{Ablating the extracted context-faithfulness circuit leads to a large drop in extractive QA accuracy for various datasets.} We ablate the edges from the extracted circuit and a random circuit in the language model and measure the extractive QA accuracy.} %
    \vspace{-0.2cm}
\end{figure*}

Empirically, we primarily extract our circuits for both context faithfulness and memory faithfulness corresponding to hierarchy-0 (which constitutes the first-order effects) and provide results for hierarchy-1 (which constitutes second-order effects) in Sec.(\ref{second_order_components}). We extract these circuits across Phi-3B, Vicuna-7B and Llama-3-8B. In Sec.(\ref{70b_circuit}), we provide further results on circuit components for Llama-3-70B. 

\textbf{Circuit Validation.} We validate the extracted circuit $\mathcal{C}$ by comparing to (i) Using a randomly extracted circuit $\mathcal{C}_{random}$ to measure the probability of the answer tokens; In this case the probability of the answer tokens will be low. 
%(ii) Within the clean instantiation of the model $g_{\phi, \text{clean}}$, we ablate the essential nodes of the circuit $\mathcal{C}$ and measure the probability of the answer tokens in $\mathcal{D}_{copy/memory}$. If the extracted circuit $C$ is correct, the \textbf{average metric score} after ablating $C$ will be high. 
(ii) We also ablate the context-faithfulness circuit (obtained using our probe dataset) across various extractive QA datasets commonly used in the community and measure the drop in the extractive QA accuracy. A large drop in accuracy signifies the validity of the extracted circuit.

In the next sections, we discuss the results corresponding to the mechanics of context-augmented language generation. 
\vspace{-0.2cm}
\subsection{Insights For Extractive QA through Circuits}
\label{circuit_insights}
In this section, we discuss the extracted circuit for both {\it context faithfulness} and {\it parametric memory faithfulness}. We first draw out their distinctions and validate the correctness of the circuit components. We then discuss the interpretable nature of a small set of attention heads in the circuit. 
\vspace{-0.0cm}
\subsubsection{Context Faithfulness Circuit Differs from Parametric Memory Circuit}
\textbf{Results for attention components.} We find the circuit components for {\it context faithfulness} and {\it memory faithfulness} to differ significantly. For context faithfulness, we find that patching a small group of 4-5 attention layers (or 10 attention heads) is sufficient to obtain a high average metric score of more than 0.95. However, for the memory faithfulness, we find that a significantly higher number of attention layers (e.g., $>$15) and attention heads (e.g., $>$30) are required to obtain a relatively high metric score. {\it This result shows that information from a small set of attention heads (or layers) primarily drive the circuit corresponding to context faithfulness than memory faithfulness.} In Sec.(\ref{circuit_components_total}), we also show that the top circuit components of attention layers (or heads) have a low overlap between the two circuits -- highlighting that the underlying language model elicits different circuits when answering from the context vs. parametric memory. 

\textbf{Results for MLP components.} We observe an intriguing pattern with MLPs in the extracted circuit. For context faithfulness, in Vicuna and Phi-3, MLPs appear to be less significant, as patching them results in a very low metric score. However, in Llama-3-8B, we identify one specific MLP (MLP-31) that individually achieves a high metric score of 0.9. This suggests that the type of pre-training might play a role in determining the relevant circuit components (with respect to MLPs) for context faithfulness.
For memory faithfulness, MLPs consistently obtain higher average metric scores across all three language models compared to the top MLPs in the context faithfulness circuit. This underscores the importance of MLPs when the language model retrieves information from parametric memory. Interestingly, we also find minimal overlap between the circuit components responsible for context faithfulness and those for memory faithfulness, even among MLPs. We provide the detailed list of all the circuit components for context faithfulness and memory faithfulness in Sec.(\ref{circuit_components_total}).
\subsubsection{Validation of the Extracted Circuit}
\textbf{Comparison with Random Circuit.} For all the language models, when using a randomly extracted circuit (for {\it context faithfulness}), the probability of the answers from the probe dataset $\mathcal{D}$ drops to 0.045 for Vicuna, 0.081 for Llama-3-8B and 0.07 for Phi-3, which shows the relevance of our extracted circuit. 

\textbf{Generalizability of the Circuit to Downstream Extractive QA Datasets.} To validate the circuits, in Fig.(\ref{ablations_model}), we ablate the context-faithfulness circuit components when answering questions from downstream datasets such as NQ-Swap, Natural-Questions and HotPotQA and measure the extractive QA accuracy. We compare with the extractive QA accuracy when a random circuit is ablated from the language model. Overall, we find that ablating the direct connections from the identified context-faithfulness circuit components, lead to the maximal drop in extractive QA accuracy. This result validates that the extracted context-faithfulness circuit generalizes to other commonly used extractive QA datasets.  We provide additional results on circuits across different knowledge partitions (e.g., country, language) in Fig.(\ref{circuit_across}) -- highlighting the similarity of context circuits amongst different knowledge partitions. 
%In particular, we find that different knowledge partitions within a extractive QA dataset follow similar circuits, potentially due to a small set of attention heads being the sole drivers of the context-faithfulness circuit. 

\vspace{-0.0cm}
\subsubsection{A Small Set of Attention Heads in the context circuit are interpretable}
\label{interpret_attn}
In Fig.(\ref{attention_select}), we observe that a small subset of attention heads in the extracted circuit for {\it context faithfulness} achieves a low entropy score with respect to the normalized attention values over the context. Upon further inspection, we find that these low-entropy attention heads predominantly focus on the answer token spans in the context. Conversely, some other attention heads in the circuit, while also highly attentive to the answer token spans, display more diffused attention patterns across other tokens. These findings are consistent across all three language models studied: Vicuna, Llama-3-8B, Phi-3 and Llama-3-70B (see Sec.(\ref{70b_circuit})). These results highlight the potential of a small set of attention heads from the circuit to be used for data attribution in language models (see more details in~Sec.(\ref{attribution})) for  extractive QA datasets.
\begin{figure*}
    \hskip 0.5cm
    \includegraphics[width=16cm, height=5.1cm]{./figs/attribution_.pdf}
    \vspace{-0.7cm}
    \caption{\label{attribution_composite} \textbf{Attribution through {\it one attention head} in our circuit via ~\attnattrib{} obtains strong attribution results.} Across various extractive QA benchmarks, we obtain improved performances over different attribution baselines. For HotPotQA, we measure the F1-score due to it being single-hop, whereas for other datasets, we measure the attribution accuracy. We present further results on long-form generations in Sec.(\ref{long_answer_generations}) and attribution results on other synthetic datasets in Sec.(\ref{full_results})}%
    \vspace{-0.2cm}
\end{figure*}
\vspace{-0.3cm}
% \subsection{Generalizability of the Circuit to Downstream Extractive QA Datasets}
% To validate the circuits, in Fig.(\ref{ablations_model}), we ablate the context-faithfulness circuit components when answering questions from downstream datasets such as NQ-Swap, Natural-Questions and HotPotQA and measure the extractive QA accuracy. We compare with the extractive QA accuracy when a random circuit is ablated from the language model. Overall, we find that ablating the direct connections from the identified context-faithfulness circuit components, lead to the maximal drop in extractive QA accuracy. This result validates that the extracted context-faithfulness circuit generalizes to other commonly used extractive QA datasets.  We also provide additional results on circuits across different question types in Fig.(\ref{circuit_across}).
% \begin{algorithm}[H]
% \caption{~\attnattrib{}: Data Attribution via {\it One Attention Head} \label{alg:cap}}
% \begin{algorithmic}
% \Require{$g_{\phi}(\text{Language model}),  q(\text{Question}),  C(\text{Context}),\newline k(\text{Number of Spans}), L(\text{Answer Length}), l(\text{Attn Layer}), \newline  h(\text{Attn Head}),  slength(\text{span-length})$}
% \Ensure{\text{Candidate attribution spans}}
% \State $S \gets \{\}$
% \State $A_{\text{total}} \gets \{\}$
% \For{$j\gets 1, \dots , L$}
%     %\State $C' \gets \{C_l\}_{l=j}^{j+m-1}$
%     \State $a_{j}, A_{j} = g_{\phi}(C, q)$ \Comment{$A_{j}$: Attention map over context, $a_{j}$: answer token}
%     \State $A_{\text{total}}.append(a_{j})$ \Comment{Add the answer token}
%     \State $A_{j, \text{relevant}} \gets A_{j}[l,h]$ \Comment{Extract the attention pattern for the given layer and head} 
%     \State $s_{j}, v_{j} = GetMaxSpan(A_{j, \text{relevant}}, C, slength)$ \Comment{Extract maximal attention span and value}
%     \State $S.append((s_{j},v_{j}))$ \Comment{Add the extracted span $s_{j}$ to the list along with its value $v_{j}$}
%     \EndFor

% \State \Return $\text{Sort}(S)[:k]$ \Comment{Sort extracted spans wrt attention value $v$ and use the top-k as attributions}
% \end{algorithmic}
% \end{algorithm}
\vspace{-0.0cm}
\subsubsection{One Can Switch Between Memory and Copy Faithfulness Circuits}
\label{switch}
To further validate the distinction between circuit components for {\it Context faithfulness} and {\it Memory faithfulness}, we conduct two ablation studies. Specifically, we use $\mathcal{D}_{memory}$, but force the language model to answer from the context, even when it originally retrieves answers from the parametric memory. We achieve this model forcing by: (i) upweighting the attention values at the answer token span in the context by a scaling factor $\beta$ in the top attention layers of the context faithfulness circuit, and (ii) mean-ablating the top MLPs from the memory faithfulness circuit.
\vspace{-0.4cm}
\begin{algorithm}[H]
\caption{~\attnattrib{}: Data Attribution via {\it One Attention Head} \label{alg:cap}}
\begin{algorithmic}
\Require{$g_{\phi}(\text{Language model}),  q(\text{Question}),  C(\text{Context}),\newline k(\text{Number of Spans}), L(\text{Answer Length}), l(\text{Attn Layer}), \newline  h(\text{Attn Head}),  slength(\text{span-length})$}
\Ensure{\text{Candidate attribution spans}}
\State $S \gets \{\}$
\State $A_{\text{total}} \gets \{\}$
\For{$j\gets 1, \dots , L$}
    %\State $C' \gets \{C_l\}_{l=j}^{j+m-1}$
    \State $a_{j}, A_{j} = g_{\phi}(C, q)$ \Comment{$A_{j}$: Attention map over context, $a_{j}$: answer token}
    \State $A_{\text{total}}.append(a_{j})$ \Comment{Add the answer token}
    \State $A_{j, \text{relevant}} \gets A_{j}[l,h]$ \Comment{Extract the attention pattern for the given layer and head} 
    \State $s_{j}, v_{j} = GetMaxSpan(A_{j, \text{relevant}}, C, slength)$ \Comment{Extract maximal attention span and value}
    \State $S.append((s_{j},v_{j}))$ \Comment{Add the extracted span $s_{j}$ to the list along with its value $v_{j}$}
    \EndFor

\State \Return $\text{Sort}(S)[:k]$ \Comment{Sort extracted spans wrt attention value $v$ and use the top-k as attributions}
\end{algorithmic}
\end{algorithm}
\vspace{-0.3cm}
Our findings show that with attention upweighting, 92$\%$ of the questions from $\mathcal{D}_{memory}$ are correctly answered using the answer tokens from the context instead of the parametric memory. Meanwhile, mean-ablating the MLPs results in 68$\%$ of the questions being answered with relevant answer tokens from the context. These results further validate the distinction in the circuit components for memory and context faithfulness and also shows that one can switch between the circuits by modifying a small set of components. We provide more details on this in Sec.(\ref{modification_component}).
%In the next section, we use our interpretability insights about attention heads from~Sec.(\ref{interpret_attn}) to design a fast and scalable data attribution algorithm for language models. 
\vspace{-0.4cm}
\section{Application 1: Attribution for Free Via One Attention Head}
\label{attribution}
Data attribution for extractive QA is crucial for language models processing external contexts, such as documents or personal files, not included in the pre-training corpora. For example, in a question like ``{\it What did Sarah Miller say during the all-hands meeting?}'', the correct answer comes from a specific section of the context (e.g., meeting transcript). Pointing to the source of the answer improves model reliability and helps users verify its correctness, especially since LLMs are prone to hallucinations~\citep{niu2024ragtruthhallucinationcorpusdeveloping}. In this section, we introduce \attnattrib{}, an efficient data attribution algorithm, leveraging insights from our mechanistic interpretations which outperforms existing QA baselines.
\vspace{-0.3cm}
\subsection{~\attnattrib{}: A Simple and Strong Data Attribution Method for Extractive QA}
\label{attn_head}
\vspace{-0.2cm}
In Sec.(\ref{circuit_insights}), we observe that a small set of attention heads from hierarchy 0 of the circuit attend to the answer token in the context. Thus, these attention heads from the extracted circuit for context faithfulness implicitly perform data attribution by default. However, real-world contexts can be noisy and contain multiple answer tokens, raising questions about the behavior of these attributable attention heads in practical settings.
In this section, we introduce~\attnattrib{}, which automatically generates attributions from the context during the forward pass by leveraging only one attention head from the context faithfulness circuit. Specifically,~\attnattrib{} uses the attention patterns from the relevant attention head to generate a span from the context for each generated answer token. These spans are ranked based on the maximum attention value within the span (a sentence from the context), and the top-k spans are selected for attribution. A detailed description of ~\attnattrib{} is provided in Algo. (\ref{alg:cap}).
Using ~\attnattrib{}, we explore the potential applications of mechanistic circuits for attribution in extractive QA. We note that we use \textbf{only one attention head} identified using our probe dataset, $\mathcal{D}_{copy}$, and test its effectiveness on different extractive QA benchmarks. 
\begin{figure*}
    \hskip 0.6cm
    \includegraphics[width=16cm, height=5.0cm]{./figs/extractive_acc.pdf}
    \vspace{-0.6cm}
    \caption{\label{extractive_qa} \textbf{ Augmenting the prompt with the attribution from~\attnattrib{} improves extractive QA accuracy.} (a) The attribution at the perturbed token in context through our extracted attention head, when the language model answers from the parametric memory ($\mathcal{D}_{\text{memory}}$) is high. (b) Vicuna-7B and (c) Llama-3-8B: Improvement in extractive QA accuracies for both Vicuna and Llama-3-8B when compared to baseline prompting and Context-aware Contrastive Decoding.}%
    \vspace{-0.1cm}
\end{figure*}
\vspace{-0.3cm}
\subsection{Evaluation on Extractive QA Benchmarks}
\textbf{Baselines.} We use the following baselines: (i) {\it Self-Attribution}: In this, we prompt the language model to generate an attribution from the context which is required to answer the question. This prompting technique is similar in principle to~\citep{gao2023enablinglargelanguagemodels} and ~\citep{buchmann2024attributeabstainlargelanguage}; (ii) {\it Iterative Prompting}: We first generate the answer from the language model, then perform another forward pass and prompt the language model to generate the attribution from the context for the generated answer. (iii) {\it Sentence Similarity.} We retrieve the most similar sentence from the context to the generated answer using an auxiliary language encoder (all-mpnet-base-v2). This choice is motivated by findings from~\citep{buchmann2024attributeabstainlargelanguage}, which identified this embedding model as one of the best-performing retrievers. (iv) {\it Gradient}: We find the gradient of the loss for a generated token with respect to the input context token embeddings~\citep{yin2022interpretinglanguagemodelscontrastive}. We then use this to select the span containing the token with the highest gradient value. 

\textbf{General Empirical Results.} We compute the exact match score with the ground-truth attributions across the synthetic dataset (used in our probing step), NQ-Swap~\citep{longpre2022entitybasedknowledgeconflictsquestion}, Natural-Questions~\citep{kwiatkowski-etal-2019-natural} and Single-Hop HotPotQA~\citep{DBLP:journals/corr/abs-1809-09600}.  A full evaluation dataset description is in Sec.(\ref{attrib_datasets}). Across all the datasets, we find that~\attnattrib{} leads to improved results over strong baselines. We note that the components (i.e., relevant attribution head) of our circuit are primarily extracted for zero-hop extractive QA. Inspite of this, we find that our method obtains better F1 scores ($\approx 20\%$ improvement) than the baselines for single-hop extractive QA. The simplicity of our approach enables attribution computation in just one forward pass (during the answer generation step) therefore positioning itself as a tool for real-world use-case in the domain of extractive QA.  In Fig.(\ref{fig:context_length}), we also find~\attnattrib{} to be robust towards larger context lengths for language models supporting long contexts (e.g., Llama-3-8B, Phi-3). For Vicuna, we observe degradation for longer contexts as it only support 2048 tokens as the context length. In Sec.(\ref{llama_70_context}), we show further results on Llama-3-70B showing the stability of~\attnattrib{} for longer contexts. 

% \textbf{Extending to Long Extractive Answer Generations.} We apply~\attnattrib{} to attribute long extractive answer generations to specific parts of the input context. For this purpose, we use CNN-Dailymail~\citep{DBLP:journals/corr/HermannKGEKSB15} and NQ-Long~\citep{kwiatkowski-etal-2019-natural}, with long-form extractive answers as ground-truth. Specifically, we select a subset of 1,000 examples from CNN-Dailymail and NQ-Long to prompt the language model to generate long extractive responses from the context. For evaluating the quality of attributions, we measure the change in the log probability of the responses when the top attributed sentences in the context are ablated. A higher change in the log probability indicates the effectiveness of the method. Overall in Sec.(\ref{long_answer_generations}), we show that~\attnattrib{} consistently obtains a high change in log probability score (when compared to other baselines) for both the datasets across both Llama-3-8B and Vicuna, indicating that our method can even be utilized for reliable attribution in long extractive answer generations settings. 
\textbf{Extending to Long Extractive Answer Generations.} We apply~\attnattrib{} to attribute long extractive answer generations to specific parts of the input context. For this purpose, we use 1000 examples each from the CNN-Dailymail~\citep{DBLP:journals/corr/HermannKGEKSB15} and NQ-Long~\citep{kwiatkowski-etal-2019-natural}. For evaluating the quality of attributions, we measure the change in the log probability of the responses when the top attributed sentences in the context are ablated. A higher change in the log probability indicates the effectiveness of the method. In Sec.(\ref{long_answer_generations}), we show that~\attnattrib{} consistently obtains a high change in log probability score (when compared to other baselines) for both the datasets, indicating that our method is scalable to long answer generations. 

\textbf{Scaling to Llama-3-70B}. We apply the circuit extraction steps from~Sec.(\ref{interventional_algo}) to identify the causal components that ensure context faithfulness in Llama-3-70B. Using the attention head with the lowest entropy in the context, combined with ~\attnattrib{}, we extract the attributions. As shown in Sec.(\ref{circuit_llama_70b}), our method yields reliable and robust attributions for larger language models such as Llama-3-70B which highlights the generalizability of our approach.

% \subsection{Limitations and Generalizability Beyond Single-Hop Extractive QA}
% In this paper, we extract mechanistic circuit components for extractive QA tasks using a probe dataset that is primarily 0-hop in nature. Despite this, ~\attnattrib{} demonstrates strong attribution capabilities for single-hop extractive QA tasks. In this section, we stress-test the generalizability of ~\attnattrib{} on multi-hop extractive QA and reasoning-based questions. Specifically, we utilize the Multi-hop and Reasoning splits from HotPotQA to evaluate ~\attnattrib{}'s performance. The results are provided below:

% \textbf{Multihop QA.} This form of QA requires some form of inherent reasoning towards accumulating different parts of the context towards the final answering. Overall we find that the average attribution F1-score for multi-hop questions are reasonable, but lower than single-hop ones using~\attnattrib{} (see Sec.(\ref{multihop_results})). We hypothesize that designing a probe dataset consisting of multi-hop questions and extracting circuits with it, will lead to improved results for attribution. 
% %we find that the precision and recall of the attributions with~\attnattrib{} degrade with an increase in the number of hops. We hypothesize that designing a probe dataset consisting of multi-hop questions and extracting circuits with it, will lead to improved results. 

% \textbf{Comparison-Based Reasoning Questions.} We evaluate~\attnattrib{} on comparison-based reasoning questions, where the ground-truth answer is binary (Yes/No). When the model is restricted to answering only "Yes" or "No," the attributions are imperfect, with an attribution F1 accuracy of 0.14. However, when the model is prompted to generate answers with supporting tokens from the context, the attribution F1 score improves to 0.48. This result suggests that ~\attnattrib{} is robust for reasoning tasks, provided the model includes supporting context alongside its binary answers.
\vspace{-0.4cm}
\section{Application 2: Towards Improved Context Faithfulness}
\vspace{-0.3cm}
In the experimental setup in Section~\ref{switch}, we observe that when the model answers from parametric memory, upweighting the attention at the answer tokens in the context can prompt the model to answer from the context instead. Further investigation reveals that even when the model retrieves answers from parametric memory, the attention maps from the attribution head used in Section~\ref{attn_head} still show a high focus on the perturbed answer tokens in the context (see Sec.(\ref{memory_pattern_sec}) for visualizations).
Fig.(~\ref{extractive_qa})-(a) illustrates the attribution accuracy concerning the perturbed context answer tokens when the language model answers from parametric memory. Based on this insight, we employ~\attnattrib{} to obtain attributions for language model generations using a single forward pass. We then use these attributions in the prompt as an additional signal to guide the language model towards greater faithfulness to the context. Below we provide the empirical results:

\textbf{Empirical Results.}  Across various extractive QA benchmarks including NQ-Swap, Natural-Questions and HotPotQA, we find that using the attributions extracted with~\attnattrib{} as an additional signal in the prompt improves the extractive QA performance by upto 9$\%$ (see Fig.(\ref{extractive_qa})-(b, c)). We observe consistent improvements across both the Vicuna and Llama-3-8B family of models when compared to baseline prompting and Context-aware decoding~\citep{shi2023trustingevidencehallucinatecontextaware}. This highlights the benefits of incorporating attributions from~\attnattrib{} in the prompt, for improved faithfulness to the context on real-world benchmarks. 
\vspace{-0.3cm}
\section{Conclusion}
\vspace{-0.1cm}
In this paper, we obtain mechanistic circuits for extractive QA, a popular real-world task. We identify key mechanistic differences when the model uses the {\it parametric memory} (ignoring the context) vs. when it uses the {\it context}. We then find that a small set of attention heads in the context circuit performs {\it data attribution by default}. Using this insight, we introduce~\attnattrib{}, an efficient data attribution algorithm which obtains strong results on  extractive QA benchmarks. We further show that the attributions from~\attnattrib{} can be used towards improving generalization in extractive QA tasks by steering the model towards context faithfulness.  Our paper shows that mechanistic insights can be strategically used for enhancing language models. 
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Qualitative Examples on Data Attribution}
\subsection{Vicuna}
\begin{figure}[H]
    \hskip -0.2cm
  \includegraphics[width=\columnwidth]{./figs/appendix_vicuna_attn_1.pdf}
  \vspace{-0.0cm}
    \caption{\label{vicuna_top}
\textbf{One of the attention heads ([18,30]) from the Vicuna circuit attends ``cleanly'' to the answer token span in the context.} In this example, we can qualitatively observe that the attention head elicits patterns which are of low entropy. We use this attention head in our data attribution algorithm~\attnattrib{}. 
    }
\end{figure}
\vspace{-1cm}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/appendix_vicuna_attn_2.pdf}
  \vspace{-0.0cm}
    \caption{\label{vicuna_top_2}
\textbf{A few other attention heads in the circuit attend to the answer token span, but do so less ``cleanly'' while attending to other tokens too.} (Top): This attention head attends to the last token itself; (Bottom): This attention head attends to the answer token, but also has attentions to other tokens in the context.}
\end{figure}
\subsection{Llama-3-8B}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/appendix_llama_attn_1.pdf}
  \vspace{-0.0cm}
    \caption{\label{llama_top_1}
\textbf{A small number of attention heads from the Llama-3 circuit attends to the answer tokens in context ``cleanly''}. (Top): Attention head ([17, 24]) attends to the answer token in the context as well as the first token position. However, the attention to the first token position is minimal. (Bottom): Attention head ([27, 20]) attends to the answer token as well as the first token. However within the context, the maximum attention is still on the answer token span.} 
\end{figure}

\subsection{Phi-3}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/appendix_phi_attn_1.pdf}
  \vspace{-0.0cm}
    \caption{\label{phi_top_1}
\textbf{The top attention head from the Phi-3 circuits attends ``cleanly'' to the answer token span in the context.} We find this attention head to also attend to the first token position minimally. However, within the context window this attention head has the maximum attention to the answer token span. } 
\end{figure}

\section{Note on Second-order Circuit Components}
\label{second_order_components}
In Sec.(\ref{hierarchy_0_circuit}), we  identify circuit components at hierarchy 0 that have the most significant direct impact on the final logit. In this case the target node in the circuit graph is the logit and the source nodes are all the different attention layers, MLPs and attention heads in the extracted circuit. In our experiments, we also set the extracted circuit components from hierarchy 0 as the target node and then extract source nodes in the circuit graph. We perform this operation at the last residual stream position. Overall, we obtain a set of components which have a high direct effect on the extracted components from hierarchy 0 (with a metric score of 0.71) for Llama-3-8B. However, on investigating the components further, we did not find any specific utility of {\it data attribution} or {\it model steering} using them. Overall, an in-depth study of the second-order components in the causal graph for extractive QA will be addressed in a future work. 

\section{On Modifying Circuit Components}
\label{modification_component}
In Sec.(\ref{switch}), we discuss the effect of scaling the attention heads from the context faithfulness circuit in the language model when it answers from the parametric memory. In particular, we find that upweighting the maximum attention value from these attention heads onto the context steers the language model towards answering from the context instead of the parametric memory. 

\textbf{Up-weighting the attention values.} We multiple a scalar value $\beta$ to the maximum attention value in the context before the softmax normalization operation. In our implementation, we perform this scaling operation across \textbf{all the attention heads in the top 3 attention layers} in the circuit. We set $\beta = 10$ in our experiments, for the best steering result. 

\textbf{Ablating the MLPs.} We set the output of the top MLPs from the memory-faithulness circuit to be zero. In particular, we set the output of the projection layer in the MLP block to be zero, but make sure that the output of the other blocks are not changed due to this modification, by setting them to their original configuration. This ensures, that only the direct connection from the MLP to the final logit is ablated. 

\section{Extracted Circuit Components Across Language Models}
\label{circuit_components_total}
\subsection{Vicuna}
\subsubsection{Context Faithfulness}
\textbf{Attention Layers.} [24, 20, 18, 28, 31, 22, 19, 29, 17]
%, 14, 16, 26, 25,  8,  6, 10,  1,  7, 0, 13,  2,  5,  4,  3, 11,  9, 12, 21, 15, 30, 27, 23]

\textbf{Attention Heads.} [[24, 8], [18, 30], [31, 24], [20, 1], [22, 30], [24, 15], [19, 4], [28, 7], [31, 27], [28, 14], [29, 10], [17, 11], [31, 16], [18, 10]]

\textbf{MLPs.} [31, 24, 21, 14, 18, 11,  9, 12,  8,  1,  0,  2,  7,  3, 16,  6,  5,  4, 15, 10, 13, 17, 19, 27, 29, 23, 30, 26, 20, 22, 28, 25] (Sorted order)

\subsubsection{Memory Faithfulness}
\textbf{Attention Layers.} [20, 24, 16, 31, 26, 28, 30, 29, 15, 22, 12, 13, 19]

\textbf{Attention Heads.} [[31, 27], [24, 14], [19, 8], [28, 7], [20, 14], [20, 18], [16, 10], [21, 15], [26, 23], [30, 12], [15, 10], [31, 25], [17, 25], [16, 20], [18, 9], [24, 24], [14, 28], [18, 26], [29, 15], [14, 5], [26, 14], [16, 5], [18, 11], [22, 10], [22, 17], [16, 31], [12, 30], [31, 16], [31, 26], [29, 9]]

\textbf{MLPs.} [22, 20, 23, 21, 31, 19, 30, 29, 14, 18]
\subsection{Llama-3-8B}
\subsubsection{Context Faithfulness}
\textbf{Attention Layers.} [27, 23, 31, 24, 25, 29, 21, 30]

\textbf{Attention Heads.} [[27, 20], [23, 27], [31, 7], [17, 24], [25, 12], [31, 20], [24, 27], [27, 6], [26, 13], [16, 1], [31, 6], [29, 31], [31, 3], [30, 12]]

\textbf{MLPs.}  [31, 28, 26, 25]
\subsubsection{Memory Faithfulness}
\textbf{Attention Layers.} [31, 24, 26,  9, 19, 17, 23,  8, 16, 28,  3,  1,  6,  5,  0,  4, 25,  2, 27, 21, 22,  7, 12, 20, 13, 30, 11, 18, 14, 29, 10, 15]

\textbf{Attention Heads.} [[31, 7], [24, 3], [31, 14], [30, 24], [17, 24], [15, 18], [31, 1], [31, 3], [24, 27], [29, 8], [17, 27], [17, 23], [26, 3], [20, 14], [31, 6], [14, 22], [31, 25], [18, 29], [22, 14], [16, 2], [13, 23], [28, 0], [16, 0], [16, 30], [17, 5], [19, 3], [31, 27], [20, 27], [30, 2], [14, 1], [21, 3], [27, 6], [19, 14], [21, 10], [14, 4], [29, 22], [29, 9], [14, 24], [16, 5], [21, 26], [14, 28], [16, 25], [16, 13], [19, 20], [19, 25], [15, 11], [21, 1], [29, 11], [17, 6], [26, 12], [15, 24], [11, 5], [13, 17], [15, 20], [29, 23], [30, 26], [15, 7], [13, 9], [13, 5], [16, 24], [17, 4], [27, 21], [27, 30], [15, 8], [9, 0], [14, 13], [16, 19], [14, 14], [9, 29], [13, 21], [27, 23], [11, 28], [9, 5], [20, 3], [28, 11], [12, 20], [25, 1], [13, 3], [16, 17], [12, 21], [31, 31], [22, 29], [29, 17]]

\textbf{MLPs.} [22, 21, 20, 23, 25, 24, 19,]
\subsection{Phi-3}
\subsubsection{Context Faithfulness}
\textbf{Attention Layers.} [29, 21, 31, 28, 25, 20, 23, 11]

\textbf{Attention Heads.} [[29, 31], [20, 1], [31, 4], [23, 7], [19, 14], [23, 23], [25, 6], [20, 21], [25, 18], [21, 21], [21, 16], [28, 28], [25, 9], [21, 22]]

\textbf{MLPs.} [31, 30, 27, 19, 14, 21, 15,  9,  6, 11,  7,  4,  3,  1,  5,  0,  8,  2, 10, 16, 13, 23, 12, 18, 17, 20, 28, 26, 22, 24, 25, 29]
\subsubsection{Memory Faithfulness}
\textbf{Attention Layers.} [23, 31, 20, 22, 19, 29, 21, 24, 18, 16, 25, 12]

\textbf{Attention Heads.} [[23, 4], [31, 4], [29, 30], [31, 17], [19, 20], [30, 1], [19, 13], [20, 5], [22, 29], [25, 23], [22, 15], [28, 7], [20, 26], [9, 17], [21, 16], [24, 31], [24, 12], [20, 25], [22, 1], [23, 31], [21, 21], [20, 4], [19, 27], [31, 9], [12, 10], [20, 12], [21, 2], [26, 21], [21, 6], [18, 12], [18, 10], [13, 21], [16, 30], [13, 11], [13, 25], [15, 29], [25, 2], [21, 5], [25, 9], [29, 20], [16, 15], [18, 25], [29, 17], [4, 29], [29, 26], [23, 29], [24, 4], [16, 25], [22, 18], [16, 9], [30, 24], [18, 1], [18, 24], [17, 25], [3, 10]]

\textbf{MLPs.} [23, 24, 22, 25, 21]
\subsection{Do we need a larger probe dataset?}
We initially tested circuit extraction by using a smaller dataset of size 200. In particular, we extract the context-faithfulness circuit for Llama-3-8B.  We find the following components: 

\textbf{Attention Layers.} [27, 23, 31, 24, 29, 25, 21, 30]

\textbf{Attention Heads.} [[27, 20], [23, 27], [31, 7], [17, 24], [31, 20], [25, 12], [24, 27], [27, 6], [26, 13], [16, 1], [29, 31], [31, 6], , [31, 3], [30, 12]]

\textbf{MLPs.}  [31, 28, 26, 25]

We find the sets of components in the circuit to be similar (except a couple of components get reordered) to the one extracted using 1000 examples.  This validates that a relatively smaller size of probe dataset can also be used towards finding a circuit for extractive QA. We also note that~\citep{prakash2024finetuningenhancesexistingmechanisms} use a similar smaller size probe dataset to find a circuit for entity tracking. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More Details on the Interventional Algorithm}
\label{patching_illustration}
\begin{figure}[H]
    \hskip -0.4cm
  \includegraphics[width=1.05\columnwidth]{./figs/patching_sequence.pdf}
  \vspace{-0.0cm}
    \caption{\label{patching}
\textbf{Different Steps of Patching with the Clean and Corrupted Model.} We provide the patching steps as follows: \textbf{Step 1}: Copy the activation of a node (e.g., {\it a12}) from the corrupted model to the clean model to create the patched model. \textbf{Step 2}: Patching a12 also affects {\it a13}, {\it m13} and {\it m12} as they are recomputed. \textbf{Step 3}: Restore back {\it a13}, {\it m13} and {\it m12} to its original configuration so that only the \textbf{direct edge path} effect from {\it a12} to the logit is measured. } 
\end{figure}
\section{Probe Dataset Details}
\label{dataset_probe}
As shown in~Sec.(\ref{probe_dataset_design}), the probe dataset consists of two partitions $\mathcal{D}_{\text{copy}}$ and $\mathcal{D}_{\text{memory}}$ which are used to elicit the context-faithfulness circuit and the memory-faithfulness circuit respectively. Below we provide a few qualitative examples.

\subsection{Example 1}
\textbf{Subject.} Vinson Massif

\textbf{Question.} Where is Vinson Massif located?

\textbf{Original Answer.} Antarctica 

\textbf{Context for Copy Faithfulness.} Vinson Massif is the highest peak in the Sentinel Range of the Ellsworth Mountains, towering at an elevation of 4,892 meters (16,050 feet). It is positioned in one of the most remote and challenging environments on Earth, attracting climbers and adventurers from around the globe. First summited in 1966 by an American team, Vinson Massif is a sought-after destination for mountaineers aiming to complete the Seven Summits, the tallest peaks on each of the seven continents. Due to its extreme location and harsh weather conditions, expeditions to Vinson Massif require thorough preparation and careful logistical planning. The massif stands as the pinnacle of its continent, and for those who successfully reach its summit, it provides a profound sense of achievement and magnificent views over the surrounding icy landscape. {\color{blue} Located in \textbf{Africa}, it is a testament to human endurance and the allure of pristine, untamed wilderness.}

\textbf{Context for Memory Faithfulness.} Vinson Massif is the highest peak in the Sentinel Range of the Ellsworth Mountains, towering at an elevation of 4,892 meters (16,050 feet). It is positioned in one of the most remote and challenging environments on Earth, attracting climbers and adventurers from around the globe. First summited in 1966 by an American team, Vinson Massif is a sought-after destination for mountaineers aiming to complete the Seven Summits, the tallest peaks on each of the seven continents. Due to its extreme location and harsh weather conditions, expeditions to Vinson Massif require thorough preparation and careful logistical planning. The massif stands as the pinnacle of its continent, and for those who successfully reach its summit, it provides a profound sense of achievement and magnificent views over the surrounding icy landscape. {\color{blue} Located in \textbf{---}, it is a testament to human endurance and the allure of pristine, untamed wilderness.}

\subsection{Example 2}
\textbf{Subject.} Beats Music

\textbf{Question.} Who owns Beats Music? 

\textbf{Original Answer.} Apple

\textbf{Context for Copy Faithfulness.} {\color{blue} Beats Music, a subscription-based online music streaming service, was acquired by \textbf{Netflix} in 2014 for $3$ billion.}

\textbf{Context for Memory Faithfulness.} {\color{blue} Beats Music, a subscription-based online music streaming service, was acquired by \textbf{---} in 2014 for $3$ billion.}

\section{Data Attribution Evaluation Dataset Descriptions}
\label{attrib_datasets}
\begin{itemize}
    \item {\it Synthetic 1}: Consists of the probe dataset $\mathcal{D}$ where the context is the one generated by Llama-3-70B.
    \item {\it Synthetic 2}: Consists of the probe dataset $\mathcal{D}$ where the context is perturbed such that the original answer token is replaced with a closely related answer token. 
    \item {\it NQ-Swap 1}: NQ-Swap dataset~\citep{longpre2022entitybasedknowledgeconflictsquestion} where the original context is used.
    \item {\it NQ-Swap 2}: NQ-Swap dataset~\citep{longpre2022entitybasedknowledgeconflictsquestion} where the original context is perturbed such that the original answer token is replaced with another token.
    \item {\it Natural-Questions}: A subset of Natural-Questions~\citep{kwiatkowski-etal-2019-natural} where the ground-truth answers are short. In total, there are 13.9k questions.
    \item {\it Single-Hop HotPotQA}: Consists of questions from HotPotQA~\citep{DBLP:journals/corr/abs-1809-09600} with zero-hop or single-hop extractive QA questions. 
\end{itemize}
\section{Qualitative Study of Attributions using AttnAttribute}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/qual_1.pdf}
  \vspace{-0.0cm}
    \caption{\label{qual_1} \textbf{\attnattrib{} can select the right attribution span containing the answer, even if the answer token is present at multiple locations.} In this example, Russia (which is the answer) is present at multiple places. We find that~\attnattrib{} can infact pick out the correct causal location in the context, for the attribution. } 
\end{figure}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/qual_2.pdf}
  \vspace{-0.0cm}
    \caption{\label{qual_2} \textbf{\attnattrib{} can select the right attribution span containing the answer, even if the answer token is present at multiple locations.} In this example, Gujarat (which is the answer) is present at multiple places. We find that~\attnattrib{} can infact pick out the correct causal location in the context, for the attribution. } 
\end{figure}

\section{Validating Long Extractive Answer Generations}
\label{long_answer_generations}
Extractive QA datasets such as {\it HotpotQA}, {\it NaturalQuestions} and {\it NQ-Swap} are particularly concerned with entities in the answer which consist of a few relevant tokens in length. Even if the generated answer from the language model is long, the attributions need to point to the relevant span in the context which consist of the entity. Another challenging setting is the case where the language model needs to generate an answer comprising of an entity which itself can be long, for example comprising of multiple sentences. We investigate two experimental settings in this respect for the following datasets: (i) {\it CNN-Dailymail}, where the language model is prompted to generate an extractive summary. This extracted summary is itself the relevant entity in the generated answer. (ii) {\it NQ-Long}, where the language model is prompted to generate an answer with exact sentences from the context (rather than only the entity). We note that in {\it NQ-Long}, the ground-truth answer consists of multiple sentences extracted from the context. 

To evaluate the quality of attributions, we measure the relative change in the log probability of the responses when the original context is used vs. the original context is modified to remove the attributions (obtained from~\attnattrib{}). A higher relative change in the log probabilities indicates the faithfulness of the attributions. In particular, given the language model $g_{\phi}$, the original context $C_{\text{orig}}$ and the ablated context where the attributed text has been removed as $C_{\text{ablated}}$, we define the relative change in log probability of a response $R$ as:
\begin{equation}
    \text{Rel-Score}(g_{\phi}, C_{\text{orig}}, C_{\text{ablated}}, R) = \left| \frac{\log(p_{g_{\phi}}(R)| C_{\text{orig}}) - \log(p_{g_{\phi}}(R)| C_{\text{ablated}})}{\log(p_{g_{\phi}}(R)| C_{\text{ablated}})} \right|
\end{equation}
\subsection{Results on CNN-Dailymail}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/long_cnn.pdf}
  \vspace{-0.0cm}
    \caption{\label{long_cnn}
\textbf{Removing the attributions obtained with~\attnattrib{} from the context leads to a large relative change in the log probability of the responses.} We measure the relative change in the log probabilities of the original response (with the original context and context where the attributions are removed). We use 1000 examples from the CNN-Dailymail dataset. For both Vicuna and Llama-3-8B, we find a large relative change in the log probabilities of the responses, highlighting that the attributions from~\attnattrib{} are reliable. } 
\end{figure}

\subsection{Results on NQ-Long}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/nq_long.pdf}
  \vspace{-0.0cm}
    \caption{\label{long_nq}
\textbf{Removing the attributions obtained with~\attnattrib{} from the context leads to a large relative change in the log probability of the responses.} We measure the relative change in the log probabilities of the original response (with the original context and context where the attributions are removed). We use 1000 examples from the NQ-Long dataset. For both Vicuna and Llama-3-8B, we find a large relative change in the log probabilities of the responses, highlighting that the attributions from~\attnattrib{} are reliable. } 
\end{figure}

\subsection{Qualitative Examples of Attribution}
\subsubsection{Example 1}
\textbf{Question.} what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya?

\textbf{Original Generation.} The Appalachian Mountains in the eastern part of the United States, including the Ridge-and-Valley Appalachians, are formed by orogenic movements due to the collision of tectonic plates. The Himalayas, including the folded mountains of Pennsylvania, are also formed by orogenic movements due to the collision of tectonic plates.

\textbf{Attribution Span.} The Ridge - and - Valley Appalachians in the eastern part of United States . The Ridge - and - Valley Appalachians in the eastern part of United States . $</$Li$>$ $</$Ul$>$', 'literature but has otherwise generally fallen out of use except as described below . The forces responsible for formation of fold mountains are called orogenic movements . The term orogenic has derived from a Greek..'

\textbf{Generation after attribution is removed.} Fold mountains form when two tectonic plates move towards each other at a convergent plate boundary. The Himalayas are a series of folded mountains that formed from sedimentary rocks that accumulated along the margins of continents. The Jura mountains are a series of sub-parallel mountainous ridges that formed by folding over a Triassic evaporite. {\color{red} Note: The new generation does not contain information about Appalachians and also the main entity of the answer which is about orogenic movements.}

\subsubsection{Example 2}
\textbf{Question.} where does saying bob's your uncle come from? 

\textbf{Original Generation.} The origins of the expression "Bob's your uncle" are uncertain, but it is commonly used in the United Kingdom and Commonwealth countries to mean "and there it is" or "and there you have it." It is believed to have arisen after Conservative Prime Minister Robert "Bob" Cecil appointed his nephew Arthur Balfour as Chief Secretary for Ireland in 1887, an act which was apparently both surprising and unpopular.

\textbf{Attribution Span.} ..his nephew Arthur Balfour as Chief Secretary for Ireland in 1887 , an act which was apparently both surprising and unpopular . Whatever other qualifications Balfour might have had , `` Bob 's your uncle ''", "expression arose after Conservative Prime Minister Robert `` Bob '' Cecil appointed his nephew Arthur Balfour as Chief Secretary for Ireland in 1887 , an act which was apparently both surprising and unpopular . Whatever other qualifications"

\textbf{Generation after attribution is removed.} The origins of the expression "Bob's your uncle" are uncertain, but it is commonly used in the United Kingdom and Commonwealth countries to mean "and there it is" or "and there you have it." It is believed to have arisen after Conservative Prime Minister Robert "Bob" Cecil appointed someone as the conclusive one. {\color{red} Note: The modified generation is missing the most important entity of the answer which is about appointing the nephew as the Chief Secretary.}

\subsubsection{Example 3}
\textbf{Question.} Which two regions grow most of China's rice?

\textbf{Original Generation.} Rice production in China - Wikipedia $<$H1$>$ Rice production in China $</$H1$>$ Jump to : navigation, search $<$Table$>$ $<$Tr$>$ $<$Td$>$ $</$Td$>$ $<$Td$>$ This article needs to be updated. Please update this article to reflect recent events or newly available information. ( November 2010 ) $</$Td$>$ $</$Tr$>$ $</$Table$>$ Rice fields in Tongshan County, Hubei $<$P$>$ Rice production in China is an important part of the national economy. $</$P$>$ $<$P$>$ China is the world's largest producer of rice, and the crop makes up a little less than half of the country's total grain output. China accounts for 30 $\%$ of all world rice production...

\textbf{Attribution Span.} $</$P$>$ $<$P$>$ China is the world's largest producer of rice, and the crop makes up a little less than half of the country's total grain output. China accounts for", "> Rice fields in Tongshan County, Hubei $<$P$>$ Rice production in China is an important part of the national economy. $</$P$>$ $<$P$>$ China is the world's largest ....

\textbf{Generation after attribution is removed.} 
The early rice crop grows primarily in provinces along the Yangtze River and in provinces in the south; it is planted in February to April and harvested in June and July and contributes about 34 percent to total rice output. Intermediate and single-crop late rice grows in the southwest and along the Yangtze; it is planted in March to June and harvested in October and November and also contributed about 34 percent to total rice output in the 1980s. Double-crop late rice, planted after the early crop is reaped, is harvested in October to November and adds about 25 percent to total rice production. {\color{red} Note: After removing the attribution, it is missing the main entity of Tongshan County which appears in the original generation.}

\section{Circuit Components and Data Attribution in Llama-3-70B}
\label{70b_circuit}
In this section, we use the circuit extraction algorithm to obtain the components for {\it context-faithfulness} in Llama-70B. We note that ours is the first work (to the best of our knowledge) to retrieve circuit components in a large enterprise grade model. 
First, we plot the entropy of the attention values in the context window from the top scoring circuit attention heads, along with their corresponding attribution accuracies. We find that there exists a small set of attention heads with low entropy and high attribution accuracy on our probe dataset. Below we provide the circuit components corresponding to {\it context-faithfulness}:

\textbf{Attention Layers.} [78, 54, 75, 77, 58, 52, 53, 35, 7,2]

\textbf{Attention Heads.} [[75, 27], [52, 19], [64, 26], [58, 4], [67, 60], [78, 26], [75, 30], [39, 40], [78, 25], [72, 39], [75, 26], [53, 1], [64, 27]]

Below we provide further details regarding the attention head in the circuit which performs attribution by measuring the entropy of the attention values in context window. We also find that our attribution algorithm~\attnattrib{} is robust to larger context lengths for Llama-70B. These early results highlight that circuit extraction for real-world tasks such as extractive QA can be scaled towards large 70B (and potentially beyond) language models. 

\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/llama_70_context.pdf}
  \vspace{-0.0cm}
    \caption{\label{llama_70_context}
\textbf{A small number of attention heads in the context faithfulness circuit from Llama-3-70B performs attribution.} (Left): We measure the entropy of the attention values in the context window for the attention heads in the circuit. {\color{brown} Brown} color marks the attribution accuracy on the probe dataset $\mathcal{D}$. (Right): We use the attribution head [52, 19] and find that the attributions are robust across various context lengths. 
}  
\end{figure}
\section{Attention Patterns in Context When the Language Model Answers from Memory}
\label{memory_pattern_sec}
\begin{figure}[H]
    \hskip -0.0cm
  \includegraphics[width=\columnwidth]{./figs/memory_attention.pdf}
  \vspace{-0.0cm}
    \caption{\label{memory_pattern}
\textbf{The attention head performing attribution in the Context-Faithulness Circuit still shows a higher attention on the perturbed answer token (e.g., punctuation token) in the context.} The above visualization results are for Llama-3-8B.
}  
\end{figure}
% \subsection{Results on Benchmarks}
% {\color{blue} Benchmark results on Synthetic, NQ-Short, HotpotQA, NQ-Swap}
\label{circuit_llama_70b}

\section{Multihop Results}
\label{multihop_results}
Following are the average F1-scores for the multi-hop development split from HotpotQA. We note that each hop from the split has imbalanced number of examples (especially for hops greater than 2). 

\textbf{Vicuna.} $\{$'hop-1': 0.57, 'hop-2': 0.47, 'hop-3': 0.50, 'hop-4': 0.49, 'hop-5': 0.36$\}$

\textbf{Llama-3-8B.} $\{$'hop-1': 0.59, 'hop-2': 0.51, 'hop-3': 0.53, 'hop-4': 0.50, 'hop-5': 0.43$\}$

Overall, our results indicate that although there is a moderate degradation in the attribution quality for multi-hop questions, the average F1-scores are still reasonable. This shows that our approach can be extended towards multi-hop QA attribution too. However, to obtain the best results, we suggest obtaining a circuit with a probe dataset consisting of multi-hop questions and then using the circuit components for data attribution. 
\section{Prompts Used in the Paper}
\subsection{Patching for Finding the Circuit Components}
\textbf{Prompt} = ``A chat between a human and an assistant for question-answering system. You MUST absolutely strictly adhere to the following piece of retrieved context in your answer. Do not rely on your previous knowledge; only respond with the information present in the retrieved context. Retrieved Context: {\it context}   Question:  {\it question}  . Answer ONLY in a few words without mentioning "  {\it subject}.  Answer:"

The field of {\it context, question, subject} are filled depending on the example. 
\subsection{Extractive QA Attribution}
\textbf{Prompt} = ``A chat between a human and an assistant for question-answering system. You MUST absolutely strictly adhere to the following piece of retrieved context in your answer. Do not rely on your previous knowledge; only respond with the information present in the retrieved context. Retrieved Context: {\it context}   Question:  {\it question}  . Answer ONLY in a few words.  Answer:"

The field of {\it context, question} are filled depending on the example. 
\subsection{CNN-Dailymail Summarization}
\textbf{Prompt} = A chat between a human and an assistant for an extractive summarization system. Answer with ONLY two to three sentences from the retrieved context which can serve as an extractive summarization for the context. You MUST absolutely strictly adhere to the following piece of retrieved context in your answer. Do not rely on your previous knowledge;  Retrieved Context:  {\it context}. Extractive Summary with only 2 to 3 sentences: 

The field of {\it context} is filled depending on the example. 
\subsection{Natural Questions - Long}
\textbf{Prompt} = A chat between a human and an assistant for question-answering system. Answer ONLY with exact sentences from the retrieved context. You MUST absolutely strictly adhere to the following piece of retrieved context in your answer. Do not rely on your previous knowledge; Retrieved Context: {\it context} .Question:   {\it question} "Answer in a few exact sentences from the retrieved context:

The field of {\it context, question} are filled depending on the example. 

\section{On Real-World Deployment of AttnAttribute as an Attribution Engine}
Our method, \attnattrib{} is suitable for attributing answers in Document-based QA or Web-search QA setting that uses LLMs. We note that white-box access of the model's parameters are required to discover the circuits that are useful for attribution. Thus, our method cannot directly be applied for Contextual QA applications where blackbox LLMs like Claude or ChatGPT are deployed. In the most basic form, \attnattrib{} provides attribution for every token generated in the answer.  Algorithm \ref{alg:cap} is a simple heuristic to aggregate these per-token attributions to provide an attribution for the entire answer-span. However, we leave the exploration of more sophisticated strategies, especially those that combine \attnattrib{} with retrieval-based attribution for future work.

\section{Full Data Attribution Results}
\label{full_results}
\begin{figure}[H]
    \hskip 1.7cm
    \includegraphics[width=13cm, height=8cm]{./figs/attribution.pdf}
    \vspace{-0.3cm}
    \caption{\label{attribution_composite_full} \textbf{Attribution through {\it one attention head} in our circuit via ~\attnattrib{} obtains strong attribution results.} Across various extractive QA benchmarks, we obtain improved performances over different attribution baselines. For HotPotQA, we measure the F1-score due to it being single-hop, whereas for other datasets, we measure the attribution accuracy. }%
    \vspace{-0.0cm}
\end{figure}

\section{Limitations and Generalizability Beyond Single-Hop Extractive QA}
In this paper, we extract mechanistic circuit components for extractive QA tasks using a probe dataset that is primarily 0-hop in nature. Despite this, ~\attnattrib{} demonstrates strong attribution capabilities for single-hop extractive QA tasks. In this section, we stress-test the generalizability of ~\attnattrib{} on multi-hop extractive QA and reasoning-based questions. Specifically, we utilize the Multi-hop and Reasoning splits from HotPotQA to evaluate ~\attnattrib{}'s performance. The results are provided below:

\textbf{Multihop QA.} This form of QA requires some form of inherent reasoning towards accumulating different parts of the context towards the final answering. Overall we find that the average attribution F1-score for multi-hop questions are reasonable, but lower than single-hop ones using~\attnattrib{} (see Sec.(\ref{multihop_results})). We hypothesize that designing a probe dataset consisting of multi-hop questions and extracting circuits with it, will lead to improved results for attribution. 
%we find that the precision and recall of the attributions with~\attnattrib{} degrade with an increase in the number of hops. We hypothesize that designing a probe dataset consisting of multi-hop questions and extracting circuits with it, will lead to improved results. 

\textbf{Comparison-Based Reasoning Questions.} We evaluate~\attnattrib{} on comparison-based reasoning questions, where the ground-truth answer is binary (Yes/No). When the model is restricted to answering only "Yes" or "No," the attributions are imperfect, with an attribution F1 accuracy of 0.14. However, when the model is prompted to generate answers with supporting tokens from the context, the attribution F1 score improves to 0.48. This result suggests that ~\attnattrib{} is robust for reasoning tasks, provided the model includes supporting context alongside its binary answers.

\section{Robustness to Context Lengths for Data Attribution}
\begin{figure}[H]
\hskip 5cm
\includegraphics[width=0.45\textwidth]{./figs/context_length_new.pdf} 
\caption{\textbf{~\attnattrib{} is robust to context lengths for language models supporting larger contexts.} We find ~\attnattrib{} to be stable for Llama-3-8B and Phi-3 for large contexts, whereas observe degradation in performance for Vicuna.}
\label{fig:context_length}
\vspace{-0.0cm}
\end{figure}

\section{Circuits Across Different Question Types in Extractive QA}
\label{circuit_across}
To validate if similar circuits are used across different knowledge types, we partition the probe dataset (of size 1000) into a set of 800 and 200. The set of 800 is used to find the parametric memory circuit and the set of 200 is where the extracted circuit is ablated. If the knowledge types across the 200 questions are indeed following very different circuits, then after ablating the circuit there should not be a large drop in extractive QA accuracy. We find that ablating the extracted circuit lead to a drop in accuracy of 0.72 to 0.21 (~70$\%$ drop in accuracy). This experiment is performed for Llama-3-8B.

In a similar vein, we note the experiment performed on the context-faithfulness circuit in Fig. (4), where the drop in extracted QA accuracy for NQ-Swap (containing different in-context knowledge type questions) when the circuit (computed from our probe dataset) is ablated is ~85$\%$ (drops from 0.84 to 0.12). This experiment is performed for Llama-3-8B.

This result shows that for the in-context cases, majority of the knowledge types share the same circuit as the drop is very large. For the parametric case, the drop is slightly lower, but still significant. Overall, the experimental conclusions are : (i) if the task is of extractive QA and the model is faithful to the context, then majority of the questions will follow similar circuits irrespective of the underlying knowledge type. This is potentially because a small set of attention heads are the primary driving components which write the answer from the context into the residual stream and ablating them leads to their absence from the stream, thus leading to the wrong answer. (ii) For parametric memory, there are more chances for different questions to follow different slightly different circuits.

% Given that a major focus of this paper is on the extractive QA circuit with also applications focusing on them, a more thorough investigation of parametric memory circuits [by constructing a well-annotated probe dataset with different knowledge types (e.g., celebrity knowledge vs. travel knowledge) with a mechanism to compare multiple individual circuits are scale] needs to be performed for the future work and we believe it can be a full paper in itself.

The major takeaway is:  {\it for in-context cases, majority of the questions will follow the same circuit as long as they belong to the family of extractive QA, but for parametric knowledge questions -- although a large number of questions will follow similar circuits, there can be slightly more cases of distinct circuits}.

We also performed a new experiment by averaging out the circuits across different knowledge partitions of the probe dataset. We have earlier saved the circuit component scores for each question in the probe dataset. For the context-faithfulness circuit, we partitioned the probe dataset into different knowledge types : (i) Country; (ii) Capital Cities; (iii) Language.

Following are the context-faithfulness circuit components for each category (Llama-3-8B) :

Knowledge about Country: Attention Layers : [27, 23, 31, 24, 25, 29, 30, 21]; Attention Heads: [[27, 20], [23, 27], [31, 7], [17, 24], [25, 12], [31, 20], [24, 27], [27, 6], [26, 13], [16, 1], [30, 12], [31, 6], [29, 31], [31, 3]]

Knowledge about Capital: Attention Layers : [27, 23, 31, 24, 25, 29, 21, 30]; Attention Heads: [[27, 20], [23, 27], [31, 7], [17, 24], [25, 12], [31, 20], [24, 27], [27, 6], [16, 1], [30, 12], [31, 6], [29, 31], [31, 3]]

Knowledge about Language: Attention Layers : [27, 23, 31, 25, 24, 29, 21, 30]; Attention Heads: [[27, 20], [23, 27], [31, 7], [25, 12], [31, 20], [17, 24], [24, 27], [27, 6], [30, 12], [31, 6], [29, 31], [31, 3]]

We observe that the circuit components are almost similar (with slight change in ordering only) across different categories (which we experimented with) for the extractive QA. This together with the generalization experiment in Fig. (4) in our paper — {\it highlights that as long as the task is of pure extractive QA, when the language model follows the context — the circuits are very similar, albeit with a slight change in ordering of the components}. 

%Although [2] extracts circuits for different knowledge types, in their paper — we did not find any significant information on the node overlap between knowledge questions (e.g., country vs. language) within a particular task (e.g., factual). However, we find it quite interesting that even for different tasks (e.g., commonsense, factual) — the circuit activation frequencies across layers show very similar trends (Fig. 2 in [2]).

\end{document}





% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
