
@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{bereska2024mechanisticinterpretabilityaisafety,
      title={Mechanistic Interpretability for AI Safety -- A Review}, 
      author={Leonard Bereska and Efstratios Gavves},
      year={2024},
      eprint={2404.14082},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.14082}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread}
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}


@misc{prakash2024finetuningenhancesexistingmechanisms,
      title={Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking}, 
      author={Nikhil Prakash and Tamar Rott Shaham and Tal Haklay and Yonatan Belinkov and David Bau},
      year={2024},
      eprint={2402.14811},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14811}, 
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@misc{hanna2023doesgpt2computegreaterthan,
      title={How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model}, 
      author={Michael Hanna and Ollie Liu and Alexandre Variengien},
      year={2023},
      eprint={2305.00586},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.00586}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{asai2023selfraglearningretrievegenerate,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11511}, 
}


@inproceedings{pearl2013direct, author = {Pearl, Judea}, title = {Direct and Indirect Effects}, year = {2001}, isbn = {1558608001}, publisher = {Morgan Kaufmann Publishers Inc.}, address = {San Francisco, CA, USA}, abstract = {The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models.}, booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence}, pages = {411–420}, numpages = {10}, location = {Seattle, Washington}, series = {UAI'01} }

@misc{zhang2024bestpracticesactivationpatching,
      title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods}, 
      author={Fred Zhang and Neel Nanda},
      year={2024},
      eprint={2309.16042},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.16042}, 
}

@misc{meng2023locatingeditingfactualassociations,
      title={Locating and Editing Factual Associations in GPT}, 
      author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
      year={2023},
      eprint={2202.05262},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.05262}, 
}

@misc{turner2024activationadditionsteeringlanguage,
      title={Activation Addition: Steering Language Models Without Optimization}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}

@misc{lieberum2023doescircuitanalysisinterpretability,
      title={Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla}, 
      author={Tom Lieberum and Matthew Rahtz and János Kramár and Neel Nanda and Geoffrey Irving and Rohin Shah and Vladimir Mikulik},
      year={2023},
      eprint={2307.09458},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.09458}, 
}

@misc{mcdougall2023copysuppressioncomprehensivelyunderstanding,
      title={Copy Suppression: Comprehensively Understanding an Attention Head}, 
      author={Callum McDougall and Arthur Conmy and Cody Rushing and Thomas McGrath and Neel Nanda},
      year={2023},
      eprint={2310.04625},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04625}, 
}

@misc{gould2023successorheadsrecurringinterpretable,
      title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild}, 
      author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
      year={2023},
      eprint={2312.09230},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.09230}, 
}

@misc{arditi2024refusallanguagemodelsmediated,
      title={Refusal in Language Models Is Mediated by a Single Direction}, 
      author={Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Panickssery and Wes Gurnee and Neel Nanda},
      year={2024},
      eprint={2406.11717},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11717}, 
}

@misc{zheng2024promptdrivensafeguardinglargelanguage,
      title={On Prompt-Driven Safeguarding for Large Language Models}, 
      author={Chujie Zheng and Fan Yin and Hao Zhou and Fandong Meng and Jie Zhou and Kai-Wei Chang and Minlie Huang and Nanyun Peng},
      year={2024},
      eprint={2401.18018},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.18018}, 
}

@misc{zou2023representationengineeringtopdownapproach,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01405}, 
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread}
}

@misc{li2023surveylargelanguagemodels,
      title={A Survey of Large Language Models Attribution}, 
      author={Dongfang Li and Zetian Sun and Xinshuo Hu and Zhenyu Liu and Ziyang Chen and Baotian Hu and Aiguo Wu and Min Zhang},
      year={2023},
      eprint={2311.03731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.03731}, 
}

@misc{khalifa2024sourceawaretrainingenablesknowledge,
      title={Source-Aware Training Enables Knowledge Attribution in Language Models}, 
      author={Muhammad Khalifa and David Wadden and Emma Strubell and Honglak Lee and Lu Wang and Iz Beltagy and Hao Peng},
      year={2024},
      eprint={2404.01019},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.01019}, 
}

@misc{huang2024citationkeybuildingresponsible,
      title={Citation: A Key to Building Responsible and Accountable Large Language Models}, 
      author={Jie Huang and Kevin Chen-Chuan Chang},
      year={2024},
      eprint={2307.02185},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.02185}, 
}

@misc{ye2024effectivelargelanguagemodel,
      title={Effective Large Language Model Adaptation for Improved Grounding and Citation Generation}, 
      author={Xi Ye and Ruoxi Sun and Sercan O. Arik and Tomas Pfister},
      year={2024},
      eprint={2311.09533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09533},
}


@misc{yin2022interpretinglanguagemodelscontrastive,
      title={Interpreting Language Models with Contrastive Explanations}, 
      author={Kayo Yin and Graham Neubig},
      year={2022},
      eprint={2202.10419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.10419}, 
}

@misc{gao2023enablinglargelanguagemodels,
      title={Enabling Large Language Models to Generate Text with Citations}, 
      author={Tianyu Gao and Howard Yen and Jiatong Yu and Danqi Chen},
      year={2023},
      eprint={2305.14627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14627}, 
}

@misc{longpre2022entitybasedknowledgeconflictsquestion,
      title={Entity-Based Knowledge Conflicts in Question Answering}, 
      author={Shayne Longpre and Kartik Perisetla and Anthony Chen and Nikhil Ramesh and Chris DuBois and Sameer Singh},
      year={2022},
      eprint={2109.05052},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.05052}, 
}

@article{DBLP:journals/corr/HermannKGEKSB15,
  author       = {Karl Moritz Hermann and
                  Tom{\'{a}}s Kocisk{\'{y}} and
                  Edward Grefenstette and
                  Lasse Espeholt and
                  Will Kay and
                  Mustafa Suleyman and
                  Phil Blunsom},
  title        = {Teaching Machines to Read and Comprehend},
  journal      = {CoRR},
  volume       = {abs/1506.03340},
  year         = {2015},
  url          = {http://arxiv.org/abs/1506.03340},
  eprinttype    = {arXiv},
  eprint       = {1506.03340},
  timestamp    = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HermannKGEKSB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}


@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}


@article{DBLP:journals/corr/abs-1809-09600,
  author       = {Zhilin Yang and
                  Peng Qi and
                  Saizheng Zhang and
                  Yoshua Bengio and
                  William W. Cohen and
                  Ruslan Salakhutdinov and
                  Christopher D. Manning},
  title        = {HotpotQA: {A} Dataset for Diverse, Explainable Multi-hop Question
                  Answering},
  journal      = {CoRR},
  volume       = {abs/1809.09600},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09600},
  eprinttype    = {arXiv},
  eprint       = {1809.09600},
  timestamp    = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09600.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{buchmann2024attributeabstainlargelanguage,
      title={Attribute or Abstain: Large Language Models as Long Document Assistants}, 
      author={Jan Buchmann and Xiao Liu and Iryna Gurevych},
      year={2024},
      eprint={2407.07799},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.07799}, 
}




@misc{xu2024knowledgeconflictsllmssurvey,
      title={Knowledge Conflicts for LLMs: A Survey}, 
      author={Rongwu Xu and Zehan Qi and Zhijiang Guo and Cunxiang Wang and Hongru Wang and Yue Zhang and Wei Xu},
      year={2024},
      eprint={2403.08319},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08319}, 
}


@misc{wang2023causalviewentitybias,
      title={A Causal View of Entity Bias in (Large) Language Models}, 
      author={Fei Wang and Wenjie Mo and Yiwei Wang and Wenxuan Zhou and Muhao Chen},
      year={2023},
      eprint={2305.14695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14695}, 
}


@misc{mallen2023trustlanguagemodelsinvestigating,
      title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories}, 
      author={Alex Mallen and Akari Asai and Victor Zhong and Rajarshi Das and Daniel Khashabi and Hannaneh Hajishirzi},
      year={2023},
      eprint={2212.10511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10511}, 
}

@misc{wu2024clashevalquantifyingtugofwarllms,
      title={ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence}, 
      author={Kevin Wu and Eric Wu and James Zou},
      year={2024},
      eprint={2404.10198},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10198}, 
}


@misc{xu2024searchinthechaininteractivelyenhancinglarge,
      title={Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks}, 
      author={Shicheng Xu and Liang Pang and Huawei Shen and Xueqi Cheng and Tat-Seng Chua},
      year={2024},
      eprint={2304.14732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.14732}, 
}

@misc{zhang2024knowledgealignmentproblembridging,
      title={The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models}, 
      author={Shuo Zhang and Liangming Pan and Junzhou Zhao and William Yang Wang},
      year={2024},
      eprint={2305.13669},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13669}, 
}

@misc{turner2024steeringlanguagemodelsactivation,
      title={Steering Language Models With Activation Engineering}, 
      author={Alexander Matt Turner and Lisa Thiergart and Gavin Leech and David Udell and Juan J. Vazquez and Ulisse Mini and Monte MacDiarmid},
      year={2024},
      eprint={2308.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10248}, 
}


@misc{shi2023trustingevidencehallucinatecontextaware,
      title={Trusting Your Evidence: Hallucinate Less with Context-aware Decoding}, 
      author={Weijia Shi and Xiaochuang Han and Mike Lewis and Yulia Tsvetkov and Luke Zettlemoyer and Scott Wen-tau Yih},
      year={2023},
      eprint={2305.14739},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14739}, 
}

@misc{niu2024ragtruthhallucinationcorpusdeveloping,
      title={RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models}, 
      author={Cheng Niu and Yuanhao Wu and Juno Zhu and Siliang Xu and Kashun Shum and Randy Zhong and Juntong Song and Tong Zhang},
      year={2024},
      eprint={2401.00396},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.00396}, 
}

