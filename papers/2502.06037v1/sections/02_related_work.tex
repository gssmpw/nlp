\paragraph{Reasoning in LLMs.} Reasoning is a topic of great interest in the study of LLMs where prior work has developed experiments to elucidate various forms of reasoning \citep{allenshu2023physics3.2, allenzhu2023physics3.1, wang2024grokked, soheeyang2024multihopreasoning, zhong2023mquake}. One of the most widely explored reasoning tasks is ``composition'', which has also been referred to as ``multi-hop'' or chain-of-thought'' reasoning. Compositional reasoning generally requires that a model combine multiple learned concepts observed during training to address a new question or prompt seen only during inference \citep{wang2024grokked, soheeyang2024multihopreasoning, zhong2023mquake}. For example, a composition task for an LLM might include training the model on specific sentences or facts represented as (subject, relation, object) triplets: ``John's friend is Jane" and ``Jane is from NYC." Given a new prompt at inference: ``Where is John's friend from?" the model is evaluated on whether it can generate the correct prediction of `NYC', which requires that the model combine concepts from two facts seen due training. Mathematical reasoning via `addition'-like prompts has also been studied and similarly requires that a model can compose concepts observed during training. For example, given a new prompt, ``Put together 5 and 40", prior work has shown that pre-trained LLMs implicitly compute addition with dimensions in the hidden state that represent numbers through sparse frequency-domain features \citep{zhou2024llmfourieraddition}. Through curated experiments, such as this, language model reasoning is evaluated based on prediction matches, which can only be achieved if the model takes advantage of a specific reasoning strategy being tested. Similarly, for time series reasoning studies, it is crucial to design tailored experiments that effectively uncover reasoning capabilities—similar to those established in prior language model studies—with clear definitions of concepts, compositions, and evaluation strategies.


\paragraph{Reasoning in Time Series Foundation Models.} 
Many foundation models have been proposed for time series forecasting \citep{wolff2024spade, ansari2024chronos, gao2024units, rasul2024lagllama, moirai2024, goswami2024moment, das2024TimesFM, garza2023timegpt1, liutimer, ekambaram2024ttms}. Recent work has primarily focused on extending the capabilities of these models using new architectures~\citep{moiraimoe, timemoe} or by accounting for long and multivariate context~\citep{infinichannelmixer, timerxl}. Fewer studies have focused on what these models are learning and what contributes to their success. These studies have largely explored the concepts that TSFMs learn \citep{wilinski2024exploring, goswami2024moment}, the impact of scaling the model~\citep{yao2024scaling, edwards2024scaling}, and their failure modes in controlled settings~\citep{aws2024chronos}. However, to the best of our knowledge, no studies have explored whether the success of TSFMs stems from memorizing training data or from inherent reasoning abilities. Our study examines whether models can implicitly reason through the addition of concepts to facilitate effective zero-shot generalization to complex OOD time series.

\begin{figure*}[ht]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/methods_actual.pdf}
        \caption{Traditional forecasting paradigm
        }
        \label{fig:methods_general}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/methods_bases.pdf}
        \caption{Compositional reasoning forecasting paradigm}
        \label{fig:comparison}
    \end{subfigure}
    \caption{\textbf{(a)} Traditional forecasting paradigm: models are trained directly on the training set of forecast target signals, with forecasts for the temporally subsequent test set generated using the preceding context window. \textbf{(b)} Compositional reasoning forecasting paradigm: models are trained on basis function series that compose the ground truth signal, with forecasts for the temporally subsequent test set generated using the preceding context window.}
    \label{fig:method_compositional}
\end{figure*}

