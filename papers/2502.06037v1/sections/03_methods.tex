
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth, trim=0 36 400 0, clip]{images/architecture_diagram.pdf}
    \caption{Transformer architecture based on the open-source T5 encoder backbone used in our controlled studies to evaluate various architecture components used across TSFMs. Architecture components for which no ablation studies are conducted are shown in gray.
    }
    \label{fig:t5_model_architecture}
\end{figure}

\subsection{Compositional Reasoning Forecasting Framework}
\label{section:compositional_reasoning}

In this section, we introduce our compositional reasoning forecasting framework and discuss its distinctions from the traditional forecasting paradigm.

\paragraph{Forecasting Task.} 
Let a time series be denoted by $\mathbf{y}(t)$, with time $t \in \{1,...,T+H\}$. We consider a special regression problem that aims to learn a function $f: \mathcal{X} \mapsto \mathcal{Y}$, where the domain $\mathcal{X}=\{\textbf{y}_{t-l:t}\}$ is composed of auto-regressive context of length $l$, and codomain is the $h$-step future of a series $\mathcal{Y}=\{\textbf{y}_{t+1:t+h} \}$. We denote the predictions:
\begin{align}
    \hat{\textbf{y}}_{t:t+h} = f(\textbf{y}_{t-l:t}).
\end{align}

\paragraph{Traditional Forecasting Paradigm.} 

In the traditional forecasting paradigm, depicted in Fig.~\ref{fig:methods_general}, the train and test sets are defined by the non-overlapping sections of a time series, before and after a time $T$.
\begin{align}
    \mathcal{D}^{(train)} &= \{(\textbf{y}_{t-l:t}, \textbf{y}_{t:t+h})\}
    \quad \mathrm{with} \quad t \leq T \\
    \mathcal{D}^{(test)} &= \{(\textbf{y}_{t-l:t}, \textbf{y}_{t:t+h})\}
    \quad \mathrm{with} \quad t > T    
\end{align}

Under reasonable stationarity assumptions the train and test datasets will have a stable distribution, also known as an in-distribution (ID) forecast scenario 
\begin{equation}
\mathbb{P}(\mathcal{D}^{(train)})= \mathbb{P}(\mathcal{D}^{(test)})
\end{equation}

The traditional forecasting paradigm optimizes a parametrized model $f_\theta$, which is used to generate forecasts for the test set. The model then is evaluated on the test set $\mathcal{D}^{(test)}$ using temporal cross-validation.

\paragraph{Compositional Reasoning Forecasting Paradigm.} 

To test the compositional reasoning capabilities of forecasting models, as illustrated in Fig.~\ref{fig:comparison}, we design a novel forecasting task where, instead of the original series $\mathbf{y}(t)$, the train dataset $\mathcal{D}^{(train)}$ consists of the top-$k$ largest spectral components of $\mathbf{y}(t)$, obtained using Discrete Fourier Transform: 
\begin{align}
    \text{If} \quad \mathbf{y}(t) &= \sum_{w=0}^T c_w e^{i \frac{2\pi w}{T} t}, \quad \text{then} \\
    \mathcal{D}^{(train)} &= 
    \left\{ 
    \left( e^{i \frac{2\pi w}{T} [t-l:t]},  e^{i \frac{2\pi w}{T} [t:t+h]} \right) 
    \right\} \;  \\
    &\text{with} \; t > T  \; \text{ and } |c_w| \in \text{top-}k \left( \{ |c_0|, |c_1|, \dots, |c_T| \} \right) \quad \notag \\
    \mathcal{D}^{(test)} &= \left\{ (\mathbf{y}_{t-l:t}, \mathbf{y}_{t:t+h}) \right\} \; \text{with} \; t > T,
\end{align}
where $c_w$ represents the Fourier coefficients associated with the frequency component $w$ in the Fourier series expansion of the time series $\textbf{y}(t)$. As discussed in Section~\ref{section:related_work}, compositional reasoning refers to a model's ability to synthesize well-defined concepts to make accurate predictions. In our proposed compositional reasoning paradigm, we train models on the spectral decomposition of the time series and evaluate them on the original series. This approach allows us to assess a model's ability to logically generalize to unseen periodic patterns through the synthesize of fundamental building blocksâ€”specifically, sine and cosine basis functions.


By adopting this framework, we move beyond traditional ID generalization and instead create carefully crafted OOD scenarios, where the model can still succeed through its reasoning capabilities.
\begin{equation}
\mathbb{P}(\mathcal{D}^{(train)}) \neq \mathbb{P}(\mathcal{D}^{(test)})
\end{equation}

As models are evaluated in a zero-shot forecasting setting, generalization can be achieved only through the addition of time series basis functions seen during training, or \textit{composition-type reasoning}. 


\subsection{Data}\label{section:methods_data}
The datasets used in this study span both synthetic and real-world time series. Together, these datasets offer a broad spectrum of temporal frequencies, patterns, and domains for evaluating model compositional reasoning capabilities.

\paragraph{Synthetic Dataset.} 
The synthetic dataset, characterized by controlled variations in frequency and amplitude, serves as a simplified benchmark for testing compositional capabilities of time series forecasting algorithms with ideal stationary and periodic series. We consider a set of sinusoidal functions, $\mathcal{F} = \{\mathcal{Y} : \mathcal{Y}(t) = a\sin(b 2\pi t), a\cos(b 2\pi t)\}$, where $a \in [1, 32]$ and $b \in [3, 32]$. For $N$ time series each with $m$ signal compositions, the functions $\{\sin, \cos\}$ and parameters $a$ and $b$ are randomly sampled $m \times N$ times without replacement to generate the time series for each of the $N$ compositional time series. The number of compositions is adjustable and serves as a parameter for experimentation. For our experiments, we generate $N=100$ time series each comprised of only $m=2$ signal compositions to first evaluate models on the simplest task possible with minimal compositions. More details about the dataset generation process are provided in Section \ref{apd:synthetic_data_parameters}. 

\paragraph{Real-World Datasets.} We use widely studied time series datasets available in open-source repositories, including GIFT-Eval. \citep{aksu2024giftevalbenchmark}. The \textbf{Electricity Transformer Temperature (ETTm2)} dataset records measurements from an electricity transformer in a region of a province in China. It includes data on oil temperature and various load types, such as high useful load and high useless load, collected at 15-minute intervals between July 2016 and July 2018. The \textbf{Electricity (ECL)} dataset contains the hourly electricity consumption of 321 customers from 2012 to 2014. The \textbf{Solar} dataset contains hourly data on solar power generation in the US in 2006 \citep{aksu2024giftevalbenchmark, aws2024chronos}. The \textbf{Subseasonal} dataset contains climate time series data at the daily level \citep{mouatadid2024subseasonalsubseasonaldataset, aksu2024giftevalbenchmark}. Modeling seasonal patterns at the daily resolution requires long context lengths, so we downsample to weekly frequency to capture seasonal information within a manageable context length. The \textbf{Loop Seattle} dataset contains spatio-temporal speed data collected every 5 minutes from inductive loop detectors deployed on Seattle freeways \citep{cui2018deep, cui2019traffic}. For our study, we use the downsampled 1-hour version of the dataset \citep{jiang2024libcityunifiedlibraryefficient, aksu2024giftevalbenchmark}. More information on the datasets can be found in section \ref{apd:realworld_data_parameters}.

\paragraph{Preprocessing.}
We preprocess the real-world datasets by segmenting them into patches of 1056 time steps with a stride of 528, ensuring a consistent training set size of 1008. The forecast horizon is 48, consistent with the short-term prediction benchmarks in GIFT-Eval, such as ETTm2, Solar, and Loop Seattle \citep{aksu2024giftevalbenchmark}. We focus on stationary periodic subseries aligned with the synthetic dataset, as this alignment enables controlled experiments while avoiding the additional complexities of nonstationarity, which we leave for future work. To evaluate the stationarity of each series segment, we apply the Augmented Dickey-Fuller test ($\alpha=0.001$) to identify and exclude series with extreme nonstationary characteristics from the final subset. From the refined set of stationary segments, we select the top 100 with the highest mean autocorrelation function (ACF) values for inclusion in the final dataset, ensuring consistency in the number of series across datasets.

\subsection{Models}

\paragraph{Baselines.} We utilized \ARIMA \citep{hyndman2008_arima,hyndman2024forecasting} and \ETS \citep{brown_1956_ets} as our statistical baseline models.

\paragraph{Deep Learning Models.} We trained models using 16 algorithms that cover a range of architectures and are widely used in practice. Trained models include: \textbf{Linear models}: \DLinear~\citep{zeng_2023_dlinear}, \NLinear~\citep{zeng_2023_dlinear}; \textbf{Multi-Layer Perceptron-based}: (\MLP)~\citep{rosenblatt1958_mlp}, \NHITS~\cite{challu_olivares2022_nhits}, 
\NBEATS~\cite{oreshkin2020nbeats, OlivaresChallu2022_nbeats}, and \TSMixer~\citep{chen2023tsmixer}; \textbf{Recurrent Neural Network-based}: Long-Short Term Memory (\LSTM)~\citep{sak2014_lstm}; \textbf{Convolutional Neural Network-Based}: Temporal Convolution Network (\TCN)~\citep{bai2018_tcn, oord2016_tcn} and \TimesNet~\citep{wu2023timesnettemporal2dvariationmodeling}; and \textbf{Transformer-Based}: \VanillaTransformer~\citep{vaswani_2021_attentionisallyouneed, zhou2021informerefficienttransformerlong}, inverted Transformer (\iTransformer)~\citep{liu2024itransformerinvertedtransformerseffective}, \Autoformer~\citep{wu_2021_autoformer}, \Informer~\citep{zhou2021informerefficienttransformerlong}, Temporal Fusion Transformer (\TFT)~\citep{lim2021_tft}, and patch time series Transformer (\PatchTST)~\citep{nie2023patchtst}. More information on these models, as well as model training and hyperparameters, is provided in Appendix~\ref{apd:first_models} and \ref{apd:first_hyperparameters}.


\paragraph{T5-based Time Series Foundation Models.} In addition to the aforementioned models, we aim to evaluate reasoning abilities of TSFMs. TSFMs are combinations of many architectural component design decisions, as summarized in Table~\ref{tab:tsfm_design_decisions_comparison}. Comparing the performance of open-source TSFM models on compositional reasoning tasks alone does not allow us to isolate performance improvements attributable to specific components. To address this, we implement a deep learning Transformer architecture using the open-source \Tfive backbone, designed with modular components to enable flexible experimentation with various design decisions commonly employed in TSFMs, allowing for controlled testing. We pick T5 as the Transformer backbone as these are an open source family of LLMs of varying sizes, with efficient implementations, both encoders and decoders, and used by existing TSFMs \citep{infinichannelmixer, goswami2024moment}. The basic Transformer architecture, illustrated in Fig.~\ref{fig:t5_model_architecture}, incorporates diverse design choices, including input series tokenization (e.g., patching, binning), model size (e.g., tiny, mini), projection layer types (e.g., linear layers, residual networks), scalers (e.g., standard, robust), loss functions (e.g., mean squared error, quantile loss), attention mechanisms (e.g., bidirectional, causal), and positional encodings (e.g., sincos, relative). Additionally, we conduct ablation studies on context length, token (patch) length, and input series decomposition, the latter being inspired by models such as \DLinear and \Autoformer. An outline of the various component types and parameters for each design decision ablation is included in Table~\ref{tab:tsfm_design_decisions_experiments}. This controlled setup is crucial for directly comparing architectures under identical conditions. By evaluating Transformer models with key components aligned with various TSFM design choices, our work provides valuable insights into which TSFM design decisions are better suited for reasoning in OOD scenarios. 

\subsection{Evaluation}\label{section:evaluation}
We evaluated model forecasts by computing the MAE across all dataset series, 
\begin{align}
    \mathrm{MAE}(\textbf{y}_{\tau}, \hat{\textbf{y}}_{\tau}) &= \frac{1}{H}\sum_{{\tau}=t+1}^{t+H}|\textbf{y}_{\tau} - \hat{\textbf{y}}_{\tau}|,
\end{align}
and report the average and standard deviation of the results for three random seeds. To assess compositional reasoning performance, we compute the error of the top $k=\{1, 2, \hdots, 100\}$ Fourier basis function compositions and calculate the average number of top $k$ compositions each model can outperform over the datasets. 

To compare different models, we use statistical tests used by prior work \citep{IsmailFawaz2018deep, goswami2024aqua}. For comparisons of three or more models, the Friedman test ($\alpha=0.2)$ is used to determine whether significant performance differences exist, testing the null hypothesis that all methods perform equally on average. If the null hypothesis is rejected, a post-hoc analysis is conducted using the Wilcoxon signed-rank test with Holm correction. The Wilcoxon test highlights specific pairs of models with statistically significant performance differences, while the Holm correction controls for Type I errors across multiple hypothesis tests. Critical difference (CD) diagrams proposed by \citet{demsar2006cddiagrams} were used visualize model average rank over the datasets based on forecasting error, or average MAE computed over three random seeds for each dataset. In addition to showing model rank, CD diagrams were used to highlight whether the performance of two models is significantly different based on the Wilcoxon signed-rank test with Holm correction. A thick horizontal line groups a set of models that are not significantly different.


\begin{figure*}[ht!]
    \centering
    \begin{minipage}{0.411\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0 0 310 0, clip]{images/subseasonal_id_forecast_example.pdf}
        \subcaption{Model forecasts for in-distribution series
        }
    \end{minipage}
    \hfill
    \begin{minipage}{0.584\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/subseasonal_ood_forecast_example.pdf}
        \subcaption{Model forecasts for out-of-distribution series}
    \end{minipage}
    \caption{\textbf{(a)} Forecasts for a ground truth series $\mathbf{y}(t)$ for the Subseasonal dataset for models trained using the traditional forecasting paradigm. \textbf{(b)} Forecasts for models trained using the compositional reasoning forecasting paradigm. Patch-based Transformer models and MLP-based models (top) demonstrate effective generalization to out-of-distribution time series, whereas other Transformer variants and linear models struggle to do so (bottom).}
    \label{fig:subseasonal_ood_forecast_example_main}
\end{figure*}

