Our experiments yield key insights into TSFM architecture design that enable both reasoning and generalization. We find that: (1) Transformer-based models adapted from LLMs, such as T5, rank among the best in generalization and compositional reasoning tasks; (2) input patching unlocks reasoning capabilities in Transformer-based TSFMs, whereas other input tokenization methods degrade T5 models performance; and (3) MLP-based architectures are a close second with an average MAE difference of 17\% across datasets. \NHITS is the second-best model, which is approximately 97\% more computationally efficient in terms of FLOPs and 86\% smaller in terms of the number of trainable parameters. For models trained on ID data, the \Tfive and MLP-based models, such as \NHITS, \NBEATS, and \MLP, rank among the best in generalization, followed by CNN-based models, \TimesNet and \TCN, respectively.

Our findings provide critical insights for the future development of scalable and reasoning-capable TSFMs. Based on our experiments, practitioners seeking to build more reasoning-capable TSFMs should explore input series tokenization strategies that preserve local information, such as `patching'. Hierarchical decomposition used by methods such as \NBEATS and \NHITS to simplify complex temporal patterns into structured concepts for additive modeling present a promising approach. However, extracting concepts like seasonality using moving average filters may be insufficient, as evidenced by the performance of \DLinear, \Autoformer, and the moving average decomposition ablation for \Tfive (Table~\ref{tab:composition_t5_results_table}). Instead, architectures with concept-specialized `stacked' processing, such as \NHITS and \NBEATS, show greater potential for enhancing model compositional reasoning performance. Additionally, while the \texttt{T5-efficient-base} model ranks the highest in performance for ID forecasting tasks, increased model size does not translate to improved compositional reasoning performance as shown by the statistically significant difference between \texttt{T5-efficient-mini} and \texttt{T5-efficient-base} in Fig.~\ref{fig:cd_size_ablation_component_main}. Therefore, future research on TSFMs should prioritize innovating new architectural components over simply scaling model size.

In our study, we identified models that do not show evidence of compositional reasoning using a simple yet effective metric: their ability to outperform the composition of the top $k=2$ basis functions, indicating minimal compositional capabilities. Our results show that only 7 of 16 models pass this test. This top $k$ metric provides a simple threshold for evaluating the reasoning capabilities of models, and can be scaled to higher and more challenging basis function composition thresholds. By ruling out compositional reasoning for some models, our findings suggest that those models may rely on alternative forms of reasoning or memorization to generalize in traditional forecasting paradigms. This work takes an initial step in exploring reasoning in TSFMs. By providing open-source code, we aim to facilitate future research, ultimately advancing our understanding of reasoning versus memorization in time series forecasting models.

\paragraph{Limitations.}\label{apd:limitations}
This study has two limitations in relation to data. First, we focus on stationary time series as an initial step in evaluating compositional reasoning capabilities. Future work should extend the provided compositional reasoning framework for assessments with non-stationary time series. For instance, evaluating model compositional reasoning capabilities for concepts such as trend in addition to the stationary basis functions used in this study. The second limitation is that, due to the computational cost and time required for training, we evaluated the compositional capabilities of the model using five open-source datasets. Although these data sets provide valuable information, incorporating more data sets may help to elucidate other significant performance differences between model architecture component methods. To address this limitation, we provide open source code for our compositional reasoning framework, allowing the community to incorporate additional datasets and conduct further experiments with minimal effort. 


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth, trim=43 10 0 0, clip]{images/flops_component_main.pdf}
    \caption{Comparison of average rank across datasets and random seeds versus model efficiency, measured in FLOPs. The size of each point represents the number of trainable parameters. Models in the lower left corner demonstrate better generalization in compositional reasoning tasks and lower computational complexity.}
    \label{fig:flops_model_comparison}
\end{figure}


\paragraph{Future Work.} We focus ablation experiments on the best-performing architecture, the \Tfive. Future work can expand ablations to other promising architectures, such as \NBEATS and \NHITS, and explore additional novel TSFM architecture components. Future work can also explore alternative hyperparameter configurations and convergence-based stopping criteria for model training, beyond early stopping with a fixed number of validation iterations. 

Reasoning in TSFMs, similar to LLMs, can take various forms, with this work focusing primarily on compositional reasoning. Other reasoning types, such as ``comparison" and ``inverse search," which have been explored in LLMs, may also play an important role in TSFM performance. Future research should investigate these alternative reasoning forms in time series models to better understand their impact on generalization across diverse datasets. The open-source code provided in this study can serve as a foundation for extending research on model reasoning and generalization by incorporating additional or novel TSFM architecture component methods.
