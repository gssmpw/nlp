\subsection{Extended Prior Work}\label{apd:extend_prior_work}

\paragraph{Reasoning in LLMs.} While models can recall individual facts well in single-hop Q\&A tasks, they generally struggle with multi-hop reasoning that requires `chain-of-thought' logic. \citet{soheeyang2024multihopreasoning} found evidence of latent multi-hop reasoning for specific fact compositions, but noted it is highly contextual. \citet{wang2024grokked} demonstrated that Transformers can learn implicit reasoning, but only after extended training beyond typical overfitting thresholds. Composition tasks have also been studied in machine translation, testing whether models can generalize learned command components to new conjunctions, such as repeating actions \citep{lake2018generalization}.

Other types of reasoning studied in LLMs include \textit{comparison} and \textit{inverse search} \citep{allenshu2023physics3.2, wang2024grokked}.  \textit{Comparison} reasoning involves models evaluating two or more entities to make judgments to answer prompts of whether the attribute value of one entity is greater or smaller than that of another \citep{allenshu2023physics3.2, wang2024grokked} or to complete numerical answers with appropriate `greater-than' logic \cite{hanna2023gptgreaterthan}. Inverse search tests a model's ability to generate predictions in the reverse order of the training task. For example, this could involve applying the model to identify an entity based on its attributes when it was originally trained to predict the attributes of entities. Allen-Zhu et al. found that generative models struggle with inverse search unless trained specifically for it \citep{allenshu2023physics3.2}.

\paragraph{Time Series Reasoning with LLMs.} Previous work has explored LLMs' ability to understand and interpret time series concepts, such as slope and frequency, and translate them into meaningful natural language features. \citep{cai2024timeseriesexam, chow2024llmstimeseriesreasoning}. This research differs from ours as it evaluates LLMs' reasoning about or with time series concepts. In contrast, our work does not study LLMs but instead examines whether time series forecasting models can reason with time series.


\paragraph{Frequency-Based Modeling.} While not focused on model reasoning, prior work in time series forecasting has used frequency-based features to improve performance. \NBEATS reconstructs and forecasts time series by predicting Fourier coefficients needed for reconstruction, while \BasisFormer similarly predicts these coefficients \citep{ni2024basisformer}. Yang et al. leverage the Fourier basis expansion to provide frequency features directly in the time domain \citep{yang2024rethinkingfourier}. \TimesNet utilizes the Fast Fourier Transform (FFT) to identify and extract significant frequency components indicative of periodic patterns \citep{wu2023timesnettemporal2dvariationmodeling}. Previous models aim to improve architectures by incorporating frequency-based features during training. In contrast, our study focuses on evaluating whether models can logically generalize to unseen periodic patterns through the synthesis of frequency-based featuresâ€”specifically, sine and cosine basis functions.

\subsection{Models}\label{apd:first_models}

\noindent\textbf{DLinear (\DLinear)} - Linear layers are employed to model the trend and seasonal components, with the decomposition achieved through a moving average filter to separate the time series into its trend and seasonal components \citep{zeng_2023_dlinear}. 

\noindent\textbf{NLinear (\NLinear)}  - A Linear layer is employed to model the series. The model first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making
the final prediction \citep{zeng_2023_dlinear}. 

\noindent\textbf{Multi Layer Perceptron (\MLP)} - A neural network architecture composed of stacked Fully Connected Neural Networks trained with backpropagation \citep{nair2010_mlp, fukushima1975_mlp, rosenblatt1958_mlp}. 

\noindent\textbf{Neural Hierarchical Interpolation for Time Series (\NHITS)} - A deep learning model that applies multi-rate input pooling, hierarchical interpolation, and backcast residual connections together to generate additive predictions with different signal bands \citep{challu_olivares2022_nhits}.

\noindent\textbf{Neural Basis Expansion Analysis for Time Series (\NBEATS)} - A deep learning, MLP-based model that leverages both backward and forward residual connections. The network can decomposes the input signal into trend and seasonality components using polynomial and harmonic basis projections or substitute the polynomial and harmonic basis for identity basis based on the specified configuration \citep{oreshkin2020nbeats}.

\noindent\textbf{Time-Series Mixer (\TSMixer)} - TSMixer is a deep learning, MLP-based model that uses stacked mixing layers to learn and combine temporal and cross-sectional representations through mixing operations along both time and feature dimensions \citep{chen2023tsmixer}.

\noindent\textbf{Long Short-Term Memory Recurrent Neural Network (\LSTM)} - A recurrent neural network (\RNN) architecture that transforms hidden states from a multi-layer \LSTM\ encoder into contexts which are used to generate forecasts using \MLP s  \citep{sak2014_lstm}.

\noindent\textbf{Temporal Convolution Network (\TCN)} - A 1D causal-convolutional network architecture that transforms hidden states into contexts which are used as inputs to \MLP\ decoders to generate forecasts. Causal convolutions are used to generate contexts by convolving the prediction at time $t$ only with elements from time $t$ and earlier \citep{bai2018_tcn, oord2016_tcn}.

\noindent\textbf{TimesNet (\TimesNet)} - A deep learning architecture that transforms the original 1D time series into a set of
2D tensors based on multiple periods to capture intra- and inter-period variations modeled by 2D kernels \citep{wu2023timesnettemporal2dvariationmodeling}.

\noindent\textbf{VanillaTransformer (\VanillaTransformer)} - An encoder-decoder architecture with a multi-head attention mechanism that uses autoregressive features from a convolution network, window-relative positional embeddings from harmonic functions, and absolute positional embeddings from calendar data. An MLP decoder outputs time series predictions in a single pass \citep{vaswani_2021_attentionisallyouneed, zhou2021informerefficienttransformerlong}.

\noindent\textbf{iTransformer (\iTransformer)} - An attention-based deep learning architecture that applies attention and feed-forward networks to inverted dimensions by embedding time points into variate tokens. The attention mechanism capture multivariate correlations while the feed-forward network learns nonlinear representations for each token \citep{liu2024itransformerinvertedtransformerseffective}.

\noindent\textbf{Autoformer (\Autoformer)} - An encoder-decoder architecture with a multi-head attention mechanism that uses autoregressive features from a convolution network and absolute positional embeddings from calendar data. Decomposed trend and seasonal components are obtained using a moving average filter and an Auto-Correlation mechanism is used to identify periodic dependencies and aggregate similar sub-series \citep{wu_2021_autoformer, vaswani_2021_attentionisallyouneed}.

\noindent\textbf{Informer (\Informer)} - An encoder-decoder architecture with a multi-head attention mechanism that has three key features: a ProbSparse self-attention mechanism with \(O(L \log L)\) complexity, a self-attention distilling process, and an MLP decoder that outputs time-series predictions in a single pass. It uses autoregressive features from a convolution network, window-relative positional embeddings from harmonic functions, and absolute positional embeddings from calendar data \citep{zhou2021informerefficienttransformerlong, 
vaswani_2021_attentionisallyouneed}.

\noindent\textbf{Temporal Fusion Transformer (\TFT)} - An attention-based deep learning architecture that learns temporal relationships at different scales using \LSTM s for local processing and self-attention layers to model long-term dependencies. It also leverages variable selection networks and a series of gating layers to suppress unnecessary processing in the architecture \citep{lim2021_tft}. 

\noindent\textbf{Patch Time Series Transformer (\PatchTST)} - An encoder-only, multi-head attention-based architecture that separates input time series into sub-series level patches as input tokens. Each channel is dedicated to a single univariate time series, and all channels use the same embedding and Transformer weights. \cite{nie2023patchtst}

\noindent\textbf{T5-Efficient} - A series of transformer-based architectures developed to enhance the efficiency of the original T5. Code and configuration files are open-source and obtained from the HuggingFace library \citep{wolf_2020_hgtransformers}.


\pagebreak
\subsection{TSFM Model Design Decision Comparison}\label{apd:TSFM_model_decision_decisions_comparison}
TSFMs involve combinations of various architectural component design decisions. We outline the design decisions of various TSFMs, including Chronos \citep{aws2024chronos, ansari2024chronos}, LagLlama \citep{rasul2024lagllama}, Moirai \citep{moirai2024}, MOMENT \citep{goswami2024moment}, Timer \citep{liutimer}, TimesFM \citep{das2024TimesFM}, and TinyTimeMixers \citep{ekambaram2024ttms} in Table~\ref{tab:tsfm_design_decisions_comparison}. These decisions encompass architectural choices, input series tokenization methods, projection layer types, and positional encodings, among others.
\input{tables/TSFM_decision_decisions_table}

\subsection{TSFM Design Decision Experiments}\label{apd:experiment_table}
Comparing the performance of open-source TSFMs on compositional reasoning tasks alone does not allow us to isolate performance improvements attributable to specific components. To address this, we implement a deep learning Transformer architecture with a modular design, built on the open-source T5-efficient backbone \citep{wolf_2020_hgtransformers} to enable controlled experimentation with common TSFM design choices. An outline of the various types of components and methods for each design decision ablation is shown in Table~\ref{tab:tsfm_design_decisions_experiments}.
\input{tables/experiment_table}

\subsection{Synthetic Data}\label{apd:synthetic_data_parameters}
We leverage open-source code from \MOMENT ~\citep{goswami2024moment} to generate synthetic sinusoidal time series with varying frequency $b$ and amplitude $a$: $\mathcal{F} = \{\mathcal{Y} : \mathcal{Y}(t) = a\sin(b 2\pi t), a\cos(b 2\pi t)\}$, where $a \in [1, 32]$ and $b \in [3, 32]$.

\subsection{Real-World Data}\label{apd:realworld_data_parameters}
Table \ref{tab:realworld_dataset_characteristics} summarizes the characteristics of the real-world datasets used in our experiments. The ``Pre" column denotes the dataset properties prior to preprocessing, while ``Post" reflects the properties after preprocessing. For the real-world datasets, we selected a horizon length of 48, following the GIFT-Eval benchmark \citep{aksu2024giftevalbenchmark}. Please see section \ref{section:methods_data} for more details regarding data preprocessing.


\begin{table}[ht]
\centering
\caption{Dataset characteristics including domain, frequency, number (\#) of series and number of forecast targets. The number of unique series in each dataset before (Pre) and after (Post) preprocessing are shown, as we selected 100 subseries to ensure consistent dataset sizes with an equal number of series and a training length of 1056 timesteps.}
\resizebox{1.0\textwidth}{!}{\begin{tabular}{r|ccccccccc}
\toprule
\textbf{Dataset} & \textbf{Domain} & \textbf{Frequency} & \textbf{\# Series (Pre)} & \textbf{\# Series (Post)} & \textbf{\# Targets} & \textbf{\# Subseries (Post)}  & \textbf{Series Length (Post)} & \textbf{Prediction Horizon} \\ 
\midrule
Synthetic Sinusoid & N/A & N/A & N/A & N/A & 1 & 100 & 1,200 & 192 \\ 
ECL & Energy & Hourly & 321 & 47 & 1 & 100 & 1056 & 48 \\ 
ETTm2 & Energy & 15 minute & 1 & 1 & 7 & 100 & 1056 & 48 \\ 
Solar & Energy & Hourly & 5166 & 99 & 1 & 100 & 1056 & 48 \\ 
Subseasonal & Climate & Weekly & 862 & 56 & 4 & 100 & 1056 & 48 \\ 
Loop Seattle & Transportation & Hourly & 323 & 36 & 1 & 100 & 1056 & 48 \\ 
\bottomrule
\end{tabular}}
\label{tab:realworld_dataset_characteristics}
\end{table}


\subsection{Model Training and Parameters}\label{apd:first_hyperparameters}
All models are open-source. Models trained on synthetic data can be found in the \texttt{Neuralforecast} library \citep{olivares2022library_neuralforecast}. 
All models were trained and evaluated on a computing cluster consisting of 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs each with 49 GiB RAM. The synthetic datasets used in our study will be released publicly with the full paper. The T5 backbone and configuration files are available in the Hugging Face library \citep{wolf_2020_hgtransformers}. 

We train the \Tfive with the following parameters outlined in Table~\ref{tab:hyperparameters} to ensure consistent evaluation across architectures. The training loss functions used in the `loss function' ablation experiment include Mean absolute Error (MAE), Mean Squared Error (MSE), Huber Loss, and Distribution Loss with the Student's t-distribution. $H$ refers to the forecast horizon, $t$ refers to the time point at which forecasts are generated, $\textbf{y}$ refers to the target signal values, and $\hat{\textbf{y}}$ refers to the model's predicted values:
\[
\begin{minipage}{0.35\textwidth}
\begin{align*}
    \mathrm{MAE}(\textbf{y}_i, \hat{\textbf{y}}_i) &= \frac{1}{H}\sum_{i=t+1}^{t+H}|\textbf{y}_i - \hat{\textbf{y}}_i|,\\
    \mathrm{MSE}(\textbf{y}_i, \hat{\textbf{y}}_i) &= \frac{1}{H}\sum_{i=t+1}^{t+H}(\textbf{y}_i - \hat{\textbf{y}}_i)^2,
\end{align*}
\end{minipage}%
\hfill
\begin{minipage}{0.64\textwidth}
\begin{align*}
    \mathrm{Huber}(\textbf{y}_i, \hat{\textbf{y}}_i, \delta) &= 
    \begin{cases}
    \frac{1}{2}(\textbf{y}_i - \hat{\textbf{y}}_i)^2, &\text{if } |\textbf{y}_i - \hat{\textbf{y}}_i| \leq \delta, \\
    \delta \cdot (|\textbf{y}_i - \hat{\textbf{y}}_i| - \frac{1}{2}\delta),  &\text{otherwise}.
    \end{cases}, \\
    \mathrm{DistributionLoss}(\theta) &= -\log(P(\textbf{y}_i | \theta)),
\end{align*}
\end{minipage}%
\hfill
\]
where the probability density function (PDF) of the Student's \emph{t}-distribution with location parameter \(\mu\), scale parameter \(\sigma\), and degrees of freedom \(\nu\) is given by:

\[
\mathbb{P}(\textbf{y}_\tau \mid \mu, \sigma, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \, \Gamma\left(\frac{\nu}{2}\right) \sigma} \left( 1 + \frac{1}{\nu} \left(\frac{\textbf{y}_\tau - \mu}{\sigma}\right)^2 \right)^{-\frac{\nu + 1}{2}},
\]


\begin{table}[ht!]
    \caption{Common hyperparameter search space. \textsuperscript{\textdagger} Based on the T5-efficient-tiny architecture}
    \centering
    \label{tab:hyperparameters}
    \begin{tabular}{rl} 
    \toprule
      \textbf{Hyperparameter} & \textbf{Considered Values}\\
      \hline
      Input size :& 256 (192 for tokenization experiments) \\
      Learning rate : & 1e-4 \\
      Batch size :& 4 \\
      Windows batch size :& 256 \\
      Dropout :& 0.0 \\
      Training steps :& 10000 \\
      Validation check steps :& 100 \\
      %Early stop patience steps & 5 \\
      Early stop patience steps :& 20 \\
      Random seed :& \{1, 5, 10\} \\
      Hidden size :& 256\textsuperscript{\textdagger} \\
      Linear hidden size :& 1024\textsuperscript{\textdagger} \\
      Model Encoder layers :& 4\textsuperscript{\textdagger} \\
      Model Decoder layers :& 0 \\
      Number of attention heads :& 4\textsuperscript{\textdagger} \\
      Patch length :& 96 for fixed-length patches (1 for other tokenization methods) \\
      Stride :& 8 for fixed-length patches (1 for other tokenization methods) \\
      \bottomrule
    \end{tabular}
  \centering
\end{table}

\newpage
\subsection{Evaluation Metrics}\label{apd:eval_metrics}
We use \textbf{Mean Absolute Error (MAE)} to evaluate model performance. Here, $H$ refers to the forecast horizon, $t$ refers to the time point at which forecasts are generated, $\textbf{y}$ refers to the target signal values, and $\hat{\textbf{y}}$ refers to the model's predicted values:\\
\begin{align}
    \mathrm{MAE}(\textbf{y}_{\tau}, \hat{\textbf{y}}_{\tau}) &= \frac{1}{H}\sum_{{\tau}=t+1}^{t+H}|\textbf{y}_{\tau} - \hat{\textbf{y}}_{\tau}|.
\end{align}
