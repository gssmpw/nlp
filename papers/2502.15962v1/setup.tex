\section{Preliminaries}\label{sec:setup}

The PAC learning framework was proposed by \citet{valiant1984theory}. Let $\calU$ and $\calB$ be the instance and label space, respectively. It is assumed that there is an underlying distribution $D$ on $\calU \times \calB$ such that all samples are drawn from $D$. Let $\calH$ be a hypothesis class that maps $\calU$ to $\calB$. The error rate of $h \in \calH$ is defined as $\err_D(h) := \Pr_{(u, b) \sim D}( h(u) \neq b)$. Under the realizable setting, there exists a target hypothesis $h^* \in \calH$, such that with probability $1$, $b = h^*(u)$ for $(u, b) \sim D$.

In contrastive learning, an instance $u \in \calU$ is often a tuple of the form $u = (x, y, z)$, where $x, y, z$ are from $\calX \subset \R^{d}$. For example, $\calX$ can be the space of image patches with size $d$, and $u$ consists of three image patches. More generally, $u$ may contain a number of patches $x, y, z_1, \dots, z_k$, where the $z_i$'s are often referred to as negative examples in the literature and $y$ is referred to as positive example. In our main results, we do not pursue such generalization to keep our algorithm and theory concise.

With $\calU = \calX^3$ and $\calB = \{-1, 1\}$ in mind, a sample $(x, y, z, b)$ of contrastive learning should be interpreted as follows: if $b = 1$, it indicates that $y$ is closer to $x$ than $z$ is to; otherwise, $z$ is closer to $x$. More formally, there exists a distance function $\rho^*: \calX \times \calX \rightarrow \R_{\geq 0}$, such that $h^*(x, y, z) = \sign( \rho^*(x, z) - \rho^*(x, y))$. We note that \citet{alon2024contrastive} aimed to learn such unparameterized distance functions over a finite domain, while most prior works assumed certain parameterized form such as $\rho^*(x, y) = \twonorm{W x - W y}$, as in this work. Once we confine ourselves to the specific distance function, we can think of the mapping $Wx$ as a new representation of $x$. Thus, sometimes the problem of contrastive learning is also regarded as representation learning. Denote
\begin{equation}
g_W(x, y, z) = \twonorm{Wx - Wz}^2 - \twonorm{Wx - Wy}^2.
\end{equation}
Observe that $h^*(x, y, z) = \sign( g_{W^*}(x, y, z))$.

As typical in machine learning, one may want to impose certain constraint on $W$ in order to prevent overfitting. Of particular interest would be the Frobenius ball $\calW_F = \{W \in \R^{d' \times d}: \fronorm{W} \leq r_F \}$, the $\ell_1$-ball $\calW_1 = \{W \in \R^{d' \times d}: \onenorm{W} \leq r_1 \}$ for sparsity, or the nuclear ball $\calW_{*} = \{W \in \R^{d' \times d}: \nuclearnorm{W} \leq r_*\}$ for low-rankness. Different constraints will lead to different generalization bounds, which will be shown in a later section.

For a square matrix $M$, we write $\tr(M)$ for its trace. The inner product of two matrices $A$ and $B$ with same size is defined as $\inner{A}{B} := \tr(A\trans B)$, where sometimes we simply write $A \cdot B$. In addition to the matrix norms that are just mentioned, we may also use the spectral norm; it is denoted by $\spenorm{M}$.

We will mainly be interested in the hinge loss. Denote
\begin{align}
L(W; U) &= \max\{0, 1 - W\trans W \cdot U\}\\
\tilde{L}(G; U) &=\max\{0, 1 - G \cdot U\}.
\end{align}
The $W$, $G$, and $U$ will be matrices in this paper.

Let $\calF$ be a class of real-valued functions on $\calU \times \calB$ and $S = \{s_i\}_{i=1}^n$ be a sample set of $\calU \times \calB$. The empirical Rademacher complexity of $S$ under $\calF$ is defined as $\calR(\calF \circ S) = \frac{1}{n} \EXP_{\sigma} \sup_{f \in \calF} \sigma_i \cdot f(s_i)$, where $\sigma = (\sigma_1, \dots, \sigma_n)$ is the Rademacher random variable.
