\section{Algorithms and Performance Guarantees}\label{sec:alg}


Let $S = \{ (x_i, y_i, z_i, b_i) \}_{i=1}^n$ be a set of samples independently drawn from $D$, where the tuple $(x_i, y_i, z_i) \in \calU$ and $b_i \in \calB$. Recall that we study the realizable PAC learning. Thus there exists an unknown $W^* \in \calH$ such that for all $(x, y, z, b) \sim D$, $b = g_{W^*}(x, y, z)$.

At first glance, one may seek a hypothesis $W \in \calH$ that minimizes the empirical risk. That is,
\begin{equation}\label{eq:erm-org}
\min_{W \in \calW}  \frac{1}{n} \sum_{i=1}^{n} \ind{b_i \cdot g_W(x_i, y_i, z_i) < 0},
\end{equation}
where $\ind{E}$ is the indicator function which outputs $1$ if the event $E$ occurs and $0$ otherwise.

Since $g_W(\cdot)$ is quadratic in $W$, it is easy to show by algebraic calculation that:
\begin{lemma}\label{lem:Wx-Wy}
$\twonorm{Wx - Wy}^2 = \inner{W\trans W}{(x-y)(x-y)\trans}$.
\end{lemma}

Therefore, let 
\begin{equation}\label{eq:U_i}
U_i = b_i \cdot ( (x_i-z_i)(x_i-z_i)\trans - (x_i-y_i)(x_i-y_i)\trans ).
\end{equation}
We can write
\begin{equation*}
b_i \cdot g_W(x_i, y_i, z_i) = \inner{W\trans W}{U_i}.
\end{equation*}
Plugging the above into \eqref{eq:erm-org} gives
\begin{equation}
\min_{W \in \calW} \frac{1}{n} \sum_{i=1}^n \ind{ \inner{W\trans W}{U_i} < 0 }.
\end{equation}
Solving the above program is intractable, due to the 1) non-convexity of the $0/1$-loss, and 2) the quadratic formula with respect to $W$. In the following, we propose approaches based on semi-definite programming, that is solvable in polynomial time.

First, by standard technique, we could alternatively minimize the hinge loss:
\begin{equation}\label{eq:ERM}
\min_{W \in \calW} \frac{1}{n} \sum_{i=1}^n L(W; U_i).
\end{equation}
Observe that the problem may still be non-convex, since $U_i$ may have negative eigenvalues. Thus, we consider replacing the variable $W\trans W$ with a new variable $G$, so that $\inner{G}{U_i}$ is a linear function with respect to $G$, and this turns the objective function into convex. As far as we have convex constraint on $G$, the overall program becomes convex. Suppose that based on the fact $W \in \calW$, we obtain $G \in \calG$. Then the empirical risk minimization program that we are going to analyze is given as follows:
\begin{equation}\label{eq:ERM-convex}
\min_{G \in \calG} \frac{1}{n} \sum_{i=1}^n \tilde{L}(G; U_i).
\end{equation}

\subsection{Rademacher complexity}\label{subsec:Rad}

We provide bounds on the Rademacher complexity of \eqref{eq:ERM-convex} for two popular choices of $\calW$ (and thus $\calG$).

\subsubsection{Frobenius ball}

We first consider the Frobenius ball, one of the most widely used constraints in machine learning. That is, $\calW = \calW_F := \{W \in \R^{d' \times d}: \fronorm{W} \leq r_F\}$ for some parameter $r_F > 0$. Since $G = W\trans W$, by singular value decomposition, it is not hard to show that $\nuclearnorm{G} \leq r_F^2$ where $\nuclearnorm{\cdot}$ denotes the nuclear norm (also known as the trace norm). Therefore, we can choose 
\begin{equation}
\calG = \calG_* := \{G \in \R^{d \times d}: G \succeq 0, \nuclearnorm{G} \leq r_F^2\}.
\end{equation}


\begin{lemma}\label{lem:Rad-nuclear}
Consider the function class $\Theta_* := \{ \theta: U \mapsto \tilde{L}(G; U), G \in \calG_* \}$. Let $S = \{U_i\}_{i=1}^n$ and assume $\max_{U_i \in S} \spenorm{U_i} \leq \alpha$. Then the empirical Rademacher complexity 
\begin{equation*}
\calR(\Theta_* \circ S) \leq r^2_F \cdot \alpha \cdot \sqrt{\frac{\log d}{n}}.
\end{equation*}
\end{lemma}
\begin{proof}
Let $\sigma = (\sigma_1, \dots, \sigma_n)$ be the Rademacher random variable. By the contraction property of Rademacher complexity, we have
\begin{align*}
n \cdot  \calR(\Theta_* \circ S) &= \EXP_{\sigma} \sup_{G \in \calG_*} \sum_{i=1}^{n} \sigma_i \max\{0, 1 - G \cdot U_i\}\\
&\leq \EXP_{\sigma} \sup_{G \in \calG_*} \sum_{i=1}^{n} \sigma_i  G \cdot U_i\\
&\leq r^2_F \cdot \max_{U_i \in S} \spenorm{U_i} \cdot \sqrt{ n \log d},
\end{align*}
where the last inequality follows from \citet{KST12matrix} (see Table~1 therein). The result follows by noting that the spectral norm of $U_i$ is upper bounded by $\alpha$.
\end{proof}

Recall that $U_i$ was defined in \eqref{eq:U_i}. Suppose that the example space $\calX$ is a subset of a bounded $\ell_2$-norm ball, say $\calX \subset \{x: \twonorm{x} \leq \kappa\}$. Then we can show that
\begin{equation*}
\spenorm{U_i} \leq \twonorm{x_i - y_i}^2 + \twonorm{x_i - z_i}^2 \leq 2 \kappa^2.
\end{equation*}
Plugging back to Lemma~\ref{lem:Rad-nuclear} gives the following:
\begin{corollary}\label{cor:nuclear}
Consider the function class $\Theta_* := \{ \theta: U \mapsto \tilde{L}(G; U), G \in \calG_* \}$. Suppose $\calX \subset \{x: \twonorm{x} \leq \kappa\}$ and let $S = \{U_i\}_{i=1}^n$ be a draw of sample set from $\calX^3$. Then
\begin{equation*}
\calR(\Theta_* \circ S) \leq 2 r^2_F \cdot \kappa^2 \cdot \sqrt{\frac{\log d}{n}}.
\end{equation*}
\end{corollary}


\subsubsection{$\ell_1$ ball (sparsity)}

Now we consider that the linear representation matrix $W$ is constrained by an $\ell_1$-norm, which typically promotes sparsity patterns \citep{tibshirani1996regression,chen1998atomic,candes2005decoding}. That is, $\calW = \calW_F := \{W \in \R^{d' \times d}: \onenorm{W} \leq r_1\}$ for some parameter $r_1 > 0$. Now we derive the $\ell_1$-norm for $W\trans W$. To do so, let us write $W$ in a column form: $W = (w_1, \dots, w_{d})$ where $w_i$ denotes the $i$-th column of $W$. It follows that
\begin{align*}
\onenorm{W\trans W} &= \sum_{1 \leq i, j \leq d} \abs{w_i \cdot w_j} \\
&\leq \sum_{1 \leq i, j \leq d} \onenorm{w_i} \cdot \infnorm{w_j}\\
&= \sum_{1 \leq j \leq d} \infnorm{w_j} \sum_{1 \leq i \leq d} \onenorm{w_i}\\
&\leq \sum_{1 \leq j \leq d} \onenorm{w_j} \cdot r_1 \leq r_1^2.
\end{align*}
This suggests that we could choose 
\begin{equation}
\calG = \calG_1 := \{G \in \R^{d \times d}: G \succeq 0, \onenorm{G} \leq r_1^2\}.
\end{equation}

\begin{lemma}\label{lem:Rad-l1}
Consider the function class $\Theta_1 := \{ \theta: U \mapsto \tilde{L}(G; U), G \in \calG_1 \}$. Let $S = \{U_i\}_{i=1}^n$ and assume $\max_{U_i \in S} \infnorm{U_i} \leq \alpha$. Then the empirical Rademacher complexity 
\begin{equation*}
\calR(\Theta_1 \circ S) \leq r^2_1 \cdot \alpha \cdot \sqrt{\frac{4\log (2d)}{n}}.
\end{equation*}
\end{lemma}
\begin{proof}
For a matrix $M$, let $\vec{M}$ be the vector obtained by concatenating all columns of $M$.

Let $\sigma = (\sigma_1, \dots, \sigma_n)$ be the Rademacher random variable. By the contraction property of Rademacher complexity, we have
\begin{align*}
n \cdot  \calR(\Theta_1 \circ S) &= \EXP_{\sigma} \sup_{G \in \calG_1} \sum_{i=1}^{n} \sigma_i \max\{0, 1 - G \cdot U_i\}\\
&\leq \EXP_{\sigma} \sup_{G \in \calG_1} \sum_{i=1}^{n} \sigma_i  G \cdot U_i\\
&= \EXP_{\sigma} \sup_{G \in \calG_1} \sum_{i=1}^{n} \sigma_i  \vec{G} \cdot \vec{U_i}\\
&\leq r_1^2 \cdot \alpha \cdot \sqrt{2n \log(2d^2)},
\end{align*}
where the last inequality follows from Lemma~26.11 of \citet{shalev2014understanding}. Dividing both sides by $n$ completes the proof.
\end{proof}

Suppose that $\calX \subset \{x: \infnorm{x} \leq \kappa\}$. Then we can show that
\begin{equation*}
\infnorm{U_i} \leq \infnorm{x_i - y_i}^2 + \infnorm{x_i - z_i}^2 \leq 2 \kappa^2.
\end{equation*}
This implies the following corollary.
\begin{corollary}\label{cor:l1}
Consider the function class $\Theta_1 := \{ \theta: U \mapsto \tilde{L}(G; U), G \in \calG_1 \}$. Suppose $\calX \subset \{x: \infnorm{x} \leq \kappa\}$ and let $S = \{U_i\}_{i=1}^n$ be a draw of sample set from $\calX^3$. Then
\begin{equation*}
\calR(\Theta_1 \circ S) \leq 2 r^2_1 \cdot \kappa^2 \cdot \sqrt{\frac{4\log (2d)}{n}}.
\end{equation*}
\end{corollary}

\subsection{PAC guarantees}

We analyze the PAC guarantees under a new type of margin condition, which we call the contrastive large-margin condition.

\begin{definition}[Contrastive large-margin condition]
We say that the data distribution $D$ satisfies the contrastive large-margin condition if there exists $W^* \in \calW$, such that for all $(x, y, z, b) \sim D$, the following holds with probability $1$: $b( \twonorm{W^*x - W^*z}^2 - \twonorm{W^*x - W^*y}^2) \geq 1$.
\end{definition}

Geometrically, this condition ensures that there is a non-trivial separation between positive examples and negative examples for any given anchor $x$. It follows that when the condition is satisfied, \eqref{eq:ERM} attains an optimal objective value of $0$. Since the feasible set of convex program of \eqref{eq:ERM-convex} contains that of \eqref{eq:ERM}, it is easy to get the following.

\begin{lemma}
 Then there exists $\hat{G} \in \calG$, such that the objective value of \eqref{eq:ERM-convex} at $\hat{G}$ equals $0$.
\end{lemma}

Now we can prove the main result of this section, the PAC guarantees. 

\begin{theorem}\label{thm:main}
Assume that the contrastive large-margin condition is satisfied for some $W^* \in \calW$, and $\calG$ is such that $\calG \supset \{ W\trans W: W \in \calW\}$. Let $\hat{G} \in \calG$ be an optimal solution to \eqref{eq:ERM-convex} and let $\hat{G} = V \Sigma V\trans$ be its eigenvalue decomposition. Let $\hat{W} := \Sigma^{1/2} V\trans$. Then by drawing contrastive sample set $S = \{(x_i, y_i, z_i, b_i)\}_{i=1}^n$, with probability at least $1-\delta$, it holds that 
\begin{equation*}
\err_D(\hat{W}) \leq 2 \calR(\Theta \circ S) + 5c \sqrt{\frac{2 \log(8/\delta)}{n}}.
\end{equation*}
\end{theorem}
\begin{proof}
Let $\Theta := \{\theta: U \mapsto \tilde{L}(G; U), G \in \calG\}$. Let $c := \sup_{G \in \calG, U \in \calU \times \calB} \abs{ \tilde{L}(G; U)}$.

We apply standard uniform concentration via Rademacher complexity \citep{bartlett2002rademacher} to obtain that with probability $1-\delta$,
\begin{align*}
&\EXP_{U \sim D} \tilde{L}(\hat{G}; U) \\
&\leq \EXP_{U \sim D} L(W^*; U) + 2 \calR(\Theta \circ S) + 5c \sqrt{\frac{2 \log(8/\delta)}{n}}.
\end{align*}
In view of the contrastive large-margin condition, we have $L(W^*; U) = 0$. On the other hand, we always have
\begin{equation*}
\tilde{L}(G; U) \geq \ind{\hat{G} \cdot U < 0}.
\end{equation*}
This implies
\begin{equation}\label{eq:tmp-1}
\EXP_{U \sim D} \ind{\hat{G} \cdot U < 0} \leq 2 \calR(\Theta \circ S) + 5c \sqrt{\frac{2 \log(8/\delta)}{n}}.
\end{equation}
Now recall that $U = b \big( (x-z)(x-z)\trans - (x-y)(x-y)\trans \big)$ as in \eqref{eq:U_i}, and $\hat{G} = \hat{W}\trans \hat{W}$ by the eigenvalue decomposition. Therefore,
\begin{equation*}
\hat{G} \cdot U = b \cdot \bigg( \twonorm{\hat{W}x - \hat{W}z}^2 - \twonorm{\hat{W}x - \hat{W}y}^2 \bigg).
\end{equation*}
Thus, \eqref{eq:tmp-1} is equivalent to
\begin{equation}
\err_D(\hat{W}) \leq 2 \calR(\Theta \circ S) + 5c \sqrt{\frac{2 \log(8/\delta)}{n}}.
\end{equation}
The proof is complete.
\end{proof}

Theorem~\ref{thm:main}, in allusion to the Rademacher complexity bounds in Section~\ref{subsec:Rad}, lead to the sample complexity bounds for contrastive PAC learning. 

\begin{corollary}
Assume same conditions as in Theorem~\ref{thm:main}. Consider $\Theta = \Theta_*$ as in Corollary~\ref{cor:nuclear}. Suppose $\calX \subset \{x: \twonorm{x} \leq \kappa \}$. Then by drawing $n = \big( \frac{5 + 5 r_F^2 \kappa^2}{\epsilon} \big)^2 \log\frac{8d}{\delta} $ contrastive samples from $D$, we have $\err_D(\hat{W}) \leq \epsilon$ with probability $1-\delta$.
\end{corollary}
\begin{proof}
We just need to compute the supremum of $\abs{ \tilde{L}(G; U)}$. It turns out that $\abs{ \tilde{L}(G; U)} \leq 1 + \abs{G \cdot U} \leq 1 + \nuclearnorm{G} \cdot \spenorm{U} \leq 1 + r_F^2 \kappa^2$. The result follows by plugging this upper bound and the Rademacher complexity in Corollary~\ref{cor:nuclear} into the main theorem.
\end{proof}


\begin{corollary}
Assume same conditions as in Theorem~\ref{thm:main}. Consider $\Theta = \Theta_1$ as in Corollary~\ref{cor:l1}. Suppose $\calX \subset \{x: \infnorm{x} \leq \kappa \}$. Then by drawing $n = \big( \frac{5 + 5 r_1^2 \kappa^2}{\epsilon} \big)^2 \log\frac{8d}{\delta} $ contrastive samples from $D$, we have $\err_D(\hat{W}) \leq \epsilon$ with probability $1-\delta$.
\end{corollary}

\subsection{Extension to multiple negative examples}

One important extension of our contrastive PAC learning framework is to consider multiple negative samples, which are commonly used in practice. That is, suppose the label $b=1$, in addition to the anchor example $x$ and a positive example $y$, a learner collects $k$ negative examples $z_1, \dots, z_k$. Together, these serve as a sample $(x, y, z_1, \dots, z_k, 1)$. Therefore, the instance space $U = \calX^{k+2}$ while the label space remains same as before. The learning paradigm still follows from Definition~\ref{def:CPAC}. More generally, one can think of an instance as $(x, u_1, \dots, u_{k+1})$ and a label $b \in \{1, \dots, k+1\}$ that specifies the index among all $u_i$'s that is closest to $x$. Since we can always reorder the examples $u_1, \dots, u_{k+1}$ such that the closest example is arranged at the first place, without loss of generality, we will always assume $b = 1$ and the example following $x$ is closest, which we denote as $y$, and the remaining examples are denoted by $z_1, \dots, z_k$. This is also a notation typically seen in the literature.

Now given a contrastive sample set $S = \{ (x_i, y_i, z_{i1}, \dots, z_{ik})\}_{i=1}^n$ where the samples are independently drawn from $D$, we aim to establish PAC guarantees as the case $k=1$. For any $i$, we know by the realizability assumption that $\twonorm{W^* x_i - W^* y_i} \leq \twonorm{W^* x_i - W^* z_{ij}}$ for all $1 \leq j \leq k$. Define
\begin{equation}
U_{ij} = (x_i - z_{ij})(x_i - z_{ij})\trans - (x_i - y_i)(x_i - y_i)\trans.
\end{equation}
By Lemma~\ref{lem:Wx-Wy}, we have $\inner{(W^*)\trans W^*}{U_{ij}} \geq 0$ for all $1 \leq j \leq k$. This is equivalent to $\min_{1 \leq j \leq k} \inner{(W^*)\trans W^*}{U_{ij}} \geq 0$.
Thus, a natural empirical risk, based on hinge loss, is as follows:
\begin{equation}
\min_{W \in \calW} \frac{1}{n}\sum_{i=1}^{n} \max\{0, 1 - \min_{1 \leq j \leq k} \inner{W\trans W}{U_{ij}} \}.
\end{equation}

As discussed in the preceding subsection, the above program is non-convex, and we will consider SDP as convex relaxation. This gives the following program:
\begin{equation}\label{eq:erm-k-neg}
\min_{G \in \calG} \frac{1}{n}\sum_{i=1}^{n} \max\{0, 1 - \min_{1 \leq j \leq k} \inner{G}{U_{ij}} \}.
\end{equation}

Consider the function class $\calQ = \{q_G: (x, y, z_1, \dots, z_k) \mapsto \max\{0, 1 - \min_{1 \leq j \leq k} G \cdot U_{\cdot j} \}, G \in \calG \}$, where $U_{\cdot j} =(x - z_j)(x - z_j)\trans - (x - y)(x-y)\trans$. Let $c := \sup_{G \in \calG, (x, y, z_1, \dots, z_k) \in \calX^{k+2}} \abs{ q_G(x, y, z_1, \dots, z_k) }$ and denote $\hat{G}$ a global optimum of \eqref{eq:erm-k-neg}. Write $u = (x, y, z_1, \dots, z_k)$. Then standard concentration results tell that 
\begin{align*}
&\EXP_{u \sim D} q_{\hat{G}}(u) \\
&\leq \EXP_{u \sim D} q_{G^*}(u) + 2 \calR( \calQ \circ S) + 5c \sqrt{\frac{2 \log(8/\delta)}{n}},
\end{align*}
where $G^* = (W^*)\trans W^*$. Under the contrastive large-margin condition, we have $q_{G^*}(u) = 0$. Thus, it remains to bound the empirical Rademacher complexity $\calR( \calQ \circ S)$. To this end, we think of the function $q_G \in \calQ$ as a composition of two functions: $q_G = \tilde{q} \circ \bar{q}_G$, where $\bar{q}_G(u) = (G \cdot U_{\cdot 1}, \dots, G \cdot U_{\cdot k}) \in \R^k$ and $\tilde{q}(v_1, \dots, v_k) = \max\{0, 1 - \min_{1 \leq j \leq k} v_j\}$. By Corollary~4 of \citet{maurer16}, we have
\begin{equation}
n \cdot \calR(\calQ \circ S) \leq \sqrt{2} L \EXP_{\sigma} \sup_{G \in \calG} \sum_{i=1}^{n} \sum_{j=1}^{k} \sigma_{ij} G \cdot U_{ij},
\end{equation}
where $L$ denotes the Lipschitz constant of $\tilde{q}$.

When $\calG = \calG_*$ and $\calX \subset \{x: \twonorm{x} \leq  \kappa\}$, we have shown that the expectation on right-hand side is less than $\sqrt{nk \log d} \cdot r_F^2 \kappa^2$. Therefore, it remains to estimate $L$. Observe that $\tilde{q}$ can further be thought of as $\tilde{q}(t) = \max\{0, 1 - t\}$ and $t = \min_{1 \leq j \leq k} v_j$. The Lipschitz constant of $t$ with respect to $(v_1, \dots, v_k)$ is upper bounded by $1$. Thus, $L=1$.

Putting together gives
\begin{equation}
\EXP_{u \sim D} q_{\hat{G}}(u) \leq 2\sqrt{2} r_F^2 \kappa^2 \sqrt{\frac{k \log d}{n}} + 5c \sqrt{\frac{2 \log(8/\delta)}{n}}
\end{equation}
when $\calG = \calG_*$. We note that $c = 1 + r_F^2 \kappa^2$ by algebraic calculation.

Lastly, similar to the proof of Theorem~\ref{thm:main}, the above implies PAC guarantee of $\hat{W}$ with $\hat{G} = \hat{W}\trans \hat{W}$.

