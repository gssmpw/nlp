\begin{abstract}
We study contrastive learning under the PAC learning framework. While a series of recent works have shown statistical results for learning under contrastive loss, based either on the VC-dimension or Rademacher complexity, their algorithms are inherently inefficient or not implying PAC guarantees. In this paper, we consider contrastive learning of the fundamental concept of linear representations. Surprisingly, even under such basic setting, the existence of efficient PAC learners is largely open. We first show that the problem of contrastive PAC learning of linear representations is intractable to solve in general. We then show that it can be relaxed to a semi-definite program when the distance between contrastive samples is measured by the $\ell_2$-norm. We then establish generalization guarantees based on Rademacher complexity, and connect it to PAC guarantees under certain contrastive large-margin conditions. To the best of our knowledge, this is the first efficient PAC learning algorithm for contrastive learning.
\end{abstract}


\section{Introduction}

Contrastive learning has been a successful learning paradigm in modern machine learning \cite{GH10contrastive,LL18}. In general, it is assumed that a learner has access to an anchor example $x$, a positive example $y$, and a number of negative examples $\{z_1, \dots, z_k\}$, and the goal of contrastive learning is to learn a representation function $f$ on the examples such that $y$ is closer to $x$ than all $z_i$'s under $f$.

Motivated by the empirical success of contrastive learning, there have been a surge of recent interests that attempt to understand it from a theoretical perspective, primarily through the lens of Rademacher complexity or that of VC-theory.
For example, \citet{arora2019contrastive} initiated the study of generalization ability of contrastive learning by analyzing the Rademacher complexity of a commonly used contrastive loss, and showed that under certain structural assumptions on the data, minimizing an unsupervised contrastive loss leads to small classification error. There were a few follow-up works in this line.

Orthogonal to the Rademacher-based theory, a very recent work of \citet{alon2024contrastive} proposed to study this problem under the classical probably approximately correct (PAC) learning framework \citep{valiant1984theory}. Unlike prior works that assumed a rich structure for the data distribution in order to estimate the classification error from contrastive loss, \citet{alon2024contrastive} considered that there is an unknown distribution on the instances and labels, where labels are produced by an unknown distance function. Tight bounds on sample complexity were established for arbitrary distance functions, $\ell_p$-distances, and tree metrics.

In this work, we follow the contrastive PAC learning framework of \citet{alon2024contrastive}. Let $\calX \subset \R^{d}$ be the space of examples (i.e. image patches). An instance $u$ is a tuple $(x, y, z) \in \calX^3$; thus we denote by $\calU := \calX^3$. The label, $b$, of a tuple $(x, y, z)$ is either $-1$ or $1$; here, we write $\calB := \{-1, 1\}$ as the label space. Let $\calH := \{ h: \calU \rightarrow \calB \}$ be a hypothesis class. Suppose that there is an unknown distribution $D$ on $\calU \times \calB$. We are mainly interested in the realizable setting in this paper, namely, there exists an $h^* \in \calH$, such that for all $(u, b) \sim D$, it holds almost surely that $b = h^*(u)$. Now for any hypothesis $h \in \calH$, we can define its error rate as follows: $\err_D(h) := \Pr_{(u, b) \sim D}( h(u) \neq b) = \Pr_{u \sim D_U}( h(u) \neq h^*(u))$, where $D_U$ denotes the marginal distribution of $D$ on $\calU$. We are now in the position to define the contrastive PAC learning problem.


\begin{definition}[Contrastive PAC learning]\label{def:CPAC}
Let $\epsilon, \delta \in (0, 1)$ be a target error rate and failure probability, respectively. An adversary $\oracle$ chooses a distribution $D_U$ on $\calU$ and $h^* \in \calH$ and fixes them throughout the learning process. Each time the learner requests a sample from the adversary, the adversary draws a sample $u$ from $D_U$, labels it by $b = h^*(u)$ and returns $(u, b)$ to the learner. The goal of the learner is to find a $\hat{h} \in \calH$, such that with probability at least $1-\delta$ (over the random draws of samples and all internal randomness of the learning algorithm), it holds that $\err_D(\hat{h}) \leq \epsilon$ for any $D$, $h^*$.
\end{definition}

One example of the hypothesis class is $\calH = \{h: (x, y, z) \mapsto \sign\big( \norm{ f(x) -  f(z)}_p - \norm{ f(x) - f(y)}_p \big) \}$, where both $f(\cdot)$ and $p$ are to be learned from samples. This is a contrastive PAC learning problem considered in \cite{alon2024contrastive}. We note that since learning distance functions is inherently challenging, PAC guarantees of \cite{alon2024contrastive} were established only for finite domain, i.e. $\abs{\calX}$ is finite, and the learning algorithm is inherently inefficient. On the other side, \cite{arora2019contrastive} and many of its follow-up works considered a fixed and known distance function, e.g. $p = 2$, and aimed to learn the representation function $f(\cdot)$ among a certain family. This makes the problem more tractable, though in general, it is still inefficient due to the non-convexity of the contrastive loss. In addition, the approach due to \cite{arora2019contrastive} was not immediately implying PAC guarantees.

In this paper, we investigate the contrastive PAC learning problem for fixed $p = 2$ and we aim to develop efficient algorithms with PAC guarantees. Our setup is thus interpolating \citet{arora2019contrastive} and \citet{alon2024contrastive}. Despite the relatively new setup, it is surprising that even efficient contrastive PAC learning for linear representation functions on $\Rd$ is largely open. Indeed, as to be shown later, this is already a non-trivial problem from the computational perspective.

From now on, we will focus on the very fundamental class of linear representation functions:
\begin{equation}\label{eq:F}
\calF = \{f_W: x \mapsto W x, W \in \calW\}.
\end{equation}
In the above, $\calW$ can be certain constraint set such as the Frobenius ball. We will discuss in more detail the choice of $\calW$ and related results later. Denote
\begin{equation}\label{eq:g}
g_W(x, y, z) := \twonorm{Wx - Wz}^2 - \twonorm{Wx - Wy}^2.
\end{equation}
 Now we can spell out the hypothesis class to be learned:
\begin{equation}\label{eq:H}
\calH = \{ h_W: (x, y, z) \mapsto \sign\big( g_W(x, y, z) \big).
\end{equation}

\subsection{Main results}

Our main results for contrastive PAC learning of \eqref{eq:H} is as follows.

\begin{theorem}[Theorem~\ref{thm:main}, informal]\label{thm:main-informal}
Suppose that $b \cdot g_{W^*}(x, y, z) \geq 1$ for all $(x, y, z, b) \sim D$. There exists an algorithm $\calA$ satisfying the following. By drawing $\mathrm{poly}(1/\epsilon, \log1/\delta)$ samples from $D$, with probability $1-\delta$, $\calA$ outputs a hypothesis $\hat{W}$ such that $\err_D(\hat{W}) \leq \epsilon$. In addition, $\calA$ runs in $\mathrm{poly}(1/\epsilon, \log1/\delta)$ time.
\end{theorem}

We remark that the condition $b \cdot g_{W^*}(x, y, z) \geq 1$ is similar to the large-margin condition for learning halfspaces. Such large-margin condition was broadly assumed to analyze performance of learning algorithms such as Perceptron \citep{rosenblatt1958perceptron} and boosting \citep{schapire2012boosting}. Our condition is adapted to the contrastive samples, and we will call it contrastive large-margin condition. The constant $1$ therein can be readily replaced by a parameter $\gamma$, which will then lead to a sample complexity proportional to $1/\gamma^2$ by our analysis. However, to keep out results concise, we did not pursue it.

Our sample complexity in Theorem~\ref{thm:main-informal} omits dependence on other quantities such as the magnitude of samples and the constraint set $\calW$. A complete description can be found in Theorem~\ref{thm:main}.

What we really hope to highlight in the informal version is that we developed a polynomial-time algorithm that PAC learns a fundamental concept class from contrastive samples, and this is the first efficient PAC learner in the literature.

\subsection{Overview of our techniques}

We first view the contrastive PAC learning problem as binary classification, as suggested in \eqref{eq:H}. We then apply standard learning principles such as empirical risk minimization with a suitable loss function. It turns out, however, that the quadratic form of $g_W$ makes the problem inherently intractable even under a hinge loss. We thus make use of the property that quadratic functions can be linearized by introducing a new matrix variable, which turns the problem as a semi-definite program (SDP) that can be solved in polynomial time. In order to analyze the error rate, we establish generalization bounds via Rademacher complexity on the SDP. We then show that with the contrastive large-margin condition, the empirical risk goes to zero on the target concept $W^*$. This implies that the error rate of a solution to the SDP can be as small as $\epsilon$. Lastly, we apply eigenvalue decomposition on the SDP solution to obtain a linear representation, which completes the proof.

\subsection{Roadmap}

A concrete problem setup as well as a collection of useful notations are presented in Section~\ref{sec:setup}. In Section~\ref{sec:alg}, we elaborate on our algorithm and the theoretical guarantees. Section~\ref{sec:con} concludes this paper and proposes a few open questions.
