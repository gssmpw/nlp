


\section{QuOTE}

\begin{algorithm}[ht]
\caption{Building the QuOTE Index (Pseudocode)}
\label{algo:build-quote-index}
\begin{algorithmic}[1]
\Require Corpus C, LLM, VectorDB, NumQuestions
\Function{BuildQuoteIndex}{C, LLM, VectorDB, NumQuestions}
    \State $\mathcal{P} \gets \text{SplitCorpus}(C)$ \Comment{Split into chunks/passages}
    \ForAll{$p \in \mathcal{P}$}
        \State $Q \gets \text{LLMGenerateQuestions}(p,\text{NumQuestions})$
        \ForAll{$q \in Q$}
            \State $doc \gets q \| p$ \Comment{concatenate or store separately}
            \State \text{VectorDB.Add}(doc, \text{metadata}=\{\text{originalChunk}: p\})
        \EndFor
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Querying the QuOTE Index (Pseudocode)}
\label{algo:quote-query}
\begin{algorithmic}[1]
\Require UserQuery $u$, VectorDB, $k$, $M$
\Statex
\Function{QuoteQuery}{$u$, VectorDB, $k$, $M$}
    \State $u_{emb} \gets \Call{Embed}{u}$
    \State $\mathcal{R} \gets \Call{VectorDB.Query}{u_{emb}, top=k \times M}$
    \State uniqueResults $\gets \Call{Deduplicate}{\mathcal{R}}$
    \State finalContexts $\gets \Call{TopK}{uniqueResults, k}$
    \State answer $\gets \Call{LLM}{UserQuery \parallel finalContexts}$
    \State \Return answer
\EndFunction
\end{algorithmic}
\end{algorithm}




QuOTE can be viewed
in the lineage of
query reformulation~\cite{query-formulation-6}
and multi-hop reasoning~\cite{multi-hop-7}, but takes a unique perspective by focusing on question generation as a fundamental indexing strategy. 
As discussed earlier, the naive RAG approach can
fail to capture the \emph{intent} behind user queries, especially when queries are succinct 
(e.g., entity lookups) or require extracting specific details from a chunk. 
In QuOTE we transform
each chunk of text into \emph{multiple}
(question + chunk) representations
capturing a range of opportunities for retrieval.
Note that each generated question (plus chunk) is then stored as a separate ``document'' or embedding in the vector database. 

See Algorithm~\ref{algo:build-quote-index} for pseudocode to illustrate how QuOTE builds an index, 
and 
Algorithm~\ref{algo:quote-query}
for how it is queried.
We next describe key stages of the pipeline (see Fig. \ref{fig:quote_overview}):

\subsection{Question Generation at Pre-Query Time}
We split the corpus into smaller passages (or chunks). For each
chunk, we prompt an LLM to generate a set of questions that the chunk can answer. While question generation is a well studied topic in NLP~\cite{heilman_smith_2009, qgen2, heilman_smith_2010}. 
The quality and diversity of generated questions play a significant role in QuOTE’s effectiveness. We use an LLM with
prompt engineering (see Section~\ref{subsec:prompt-effects}) to create a representative set of questions with specificity and coverage. By creating multiple question-based embeddings for each chunk, QuOTE better captures diverse user queries that reference the same text in different ways. 
If a user’s query is similar (semantically) to one of the chunk-generated questions, that chunk becomes more likely to rank highly, 
leading to more accurate retrieval.


\subsection{Embedding}
 Instead of storing just the original chunk embedding, \emph{we store each generated question} (along with the original chunk) in the vector database. 
 In Section~\ref{sec:quote-orthogonality} we demonstrate that the performance
 of QuOTE is agnostic to the
 choice of embedding model.

 \subsection{Retrieval and Deduplication at Query Time}
 During query time, multiple retrieved
``documents'' often reference the same underlying chunk. Hence,
a deduplication step is necessary to ensure we select the top-k distinct chunks, avoiding wasted slots.
To this purpose we `over-retrieve'
top-$k \times M$ results (for some value of $M$) from the question-based embeddings. 
(Note that this de-duplication step is unique to the QuOTE pipeline and is not a feature in classical RAG pipelines.)



\iffalse
\subsection{Implementation Details}
\narenc{Fold 3.2 and 3.4 together, in 3.2. Then in the text, place references to specific lines from Algorithms 1 and 2.}
\paragraph{Question Generation.} 
Question generation is a well studied topic in NLP~\cite{qgen1, qgen2, qgen3}. 
The quality and diversity of generated questions play a significant role in QuOTE’s effectiveness. We use an LLM with
prompt engineering to create a representative set of questions with specificity and coverage.

\paragraph{Embedding Model.}
QuOTE is orthogonal to the choice of embedding model. 
It can be used with dense or sparse retrievers, or an ensemble of both. 
\paragraph{Deduplication.} 
During query time, multiple retrieved documents often reference the same underlying chunk. 
Hence, deduplication ensures that we select the top-$k$ \emph{distinct} chunks, avoiding wasted slots.
\fi


%%########################################################
\section{Datasets and Metrics}


\begin{table*}[t]
\centering
\caption{Overview of candidate datasets for RAG evaluation. \textbf{SQuAD}, \textbf{MultiHop-RAG}, and \textbf{Natural Questions} are included to help evaluate retrieval performance.
Other datasets (\textbf{ELI5}, \textbf{HotpotQA}, \textbf{Frames}, \textbf{TriviaQA}) were considered but are excluded either because they are geared toward assessing generation quality or for other reasons described.}
\label{tab:dataset-comparison}
\renewcommand{\arraystretch}{1.15}
\small
\begin{tabular}{p{2.2cm} p{7.0cm} p{1.8cm} p{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Description / Focus} & 
\textbf{Included or Excluded?} &
\textbf{Rationale} \\
\midrule

\textbf{SQuAD}~\cite{squad1} 
& 
Over 100k crowd-sourced QA pairs from Wikipedia articles (single-hop). 
Requires precise extractive answers (spans).
& 
\textcolor{green}{\Huge \checkmark}
& Classic reading comprehension benchmark with moderate passage lengths. Ideal for testing single-hop retrieval. \\

\textbf{MultiHop-RAG}~\cite{multihoprag-paper}
& 
2{,}555 queries requiring retrieval from multiple documents. 
Each question links to a list of fact sentences across different sources, ensuring genuine multi-hop reasoning.
& 
\textcolor{green}{\Huge \checkmark}
&
Explicitly tests multi-document retrieval and cross-sentence reasoning. 
Straightforward corpus chunking (4-sentence blocks) with traceable ground truth. \\

\textbf{Natural Questions (NQ)}~\cite{nq}
& 
Real Google Search queries mapped to Wikipedia, featuring both short and long answers. 
Demands chunking of lengthy documents.
& 
\textcolor{green}{\Huge \checkmark}
&
Reflects real user queries. Large scale and diverse. 
Useful for evaluating open-domain QA in a practical setting. \\

\textbf{ELI5}~\cite{eli5}
& 
A long-form QA dataset (272k questions from the “Explain Like I’m Five” subreddit). 
Open-ended questions requiring paragraph-length answers.
& 
\textcolor{red}{\Huge \ding{55}}
&
Official data no longer accessible due to Reddit API changes. 
No well-defined short-answer ground truth chunks; original passages are difficult to re-obtain. \\

\textbf{HotpotQA}~\cite{hotpotqa}
& 
113k Wikipedia-based QA pairs designed for multi-hop reasoning. 
Includes supporting sentence-level “facts.”
& 
\textcolor{red}{\Huge \ding{55}}
&
Context is provided in disjointed sentence lists, complicating chunk-based retrieval. 
MultiHop-RAG’s more document-centric structure provides a natural benchmark. \\

\textbf{Frames}~\cite{frame} 
& 
824 multi-hop questions requiring 2–15 Wikipedia articles. 
Complex domain with no provided corpus (web scraping needed).
& 
\textcolor{red}{\Huge \ding{55}}
&
Highly specialized queries with small question count (824), 
and no pre-fetched Wikipedia corpus. Overly complex to integrate. \\

\textbf{TriviaQA}~\cite{triviaqa}
& 
650k question-answer-evidence triples from various sources, often requiring multi-document reasoning. 
Focuses on “trivia-style” queries.
& 
\textcolor{red}{\Huge \ding{55}}
&
Not all questions have explicit matching documents. 
Already have coverage of multi-hop via MultiHop-RAG. 
Would require extensive chunking / ground-truth alignment. \\

\bottomrule
\end{tabular}
\label{dataset-table}
\end{table*}


While there exist a variety of datasets for RAG evaluation (see Table~\ref{dataset-table}) not all are geared toward evaluating retrieval performance as distinct from generation, which is our focus here. For instance, QA datasets where we are evaluated against the quality of the generated answer, or where the original ground truth chunks are not available, do not support assessing the performance of QuOTE in helping improve retrieval of relevant chunks. Accordingly, we focus on 
three benchmark datasets commonly used for question answering: 
\emph{Natural Questions} (NQ)~\cite{nq}, \emph{SQuAD}~\cite{squad1,squad2}, and \emph{MultiHop-RAG}~\cite{multihoprag-paper}. 
These datasets vary in complexity, domain coverage, and the style of questions, providing a broad platform to test the retrieval capabilities of our approach.

\subsection{Natural Questions (NQ)}
\label{subsec:nq}
The Natural Questions (NQ) dataset~\cite{nq} is a large-scale benchmark,
with questions directly sourced
from real user queries and answers keyed to Wikipedia articles. The dataset is split into 
\iffalse
primarily aimed at advancing research in question answering (QA) systems. 
In contrast to synthetic or semi-synthetic QA datasets, NQ \textbf{uses real user queries} from the Google search engine, 
coupled with \textbf{Wikipedia articles} that contain correct answers. This realistic data collection process exposes models 
to the \emph{diversity and complexity of genuine user queries}.

\paragraph{Key Features.}
\begin{itemize}
    \item \textbf{Authentic Queries.} Questions are directly sourced from real user queries, 
          ensuring that linguistic ambiguities and colloquialisms are well-represented.
    \item \textbf{Wikipedia-Based Answers.} Each query is aligned to one Wikipedia article. 
          Annotators identify both \emph{long} and \emph{short} answers in the text, covering various granularity levels.
    \item \textbf{Rich Variety of Topics.} The dataset includes straightforward fact-based questions 
          (e.g., “What is the capital of France?”) and more involved ones (e.g., “How does photosynthesis work?”).
\end{itemize}

\paragraph{Dataset Structure.}
\fi
approximately 307k training examples and roughly 7.8k each in the development and test sets. 
For each query, the dataset provides the relevant passages (long answer) and the precise phrases or entities (short answer) 
where the answer resides.

One non-trivial issue pertains to {\it multiple, highly similar passages 
in the same article}.
For example, consider passages about the song ``'Heroes''' by David Bowie from the Wikipedia article titled ``Heroes (David Bowie song)''. This article has two passages that
are nearly identical, differing only in minor wording (e.g., “in the UK” vs. “in the United Kingdom”). These slight variations do not change the factual content but result in multiple, nearly duplicate contexts.

Such minor differences unnecessarily fragment the dataset into multiple contexts, each labeled as distinct. This discrepancy complicates retrieval-based evaluations because systems are penalized if they return an almost-correct chunk that differs by only a few words from the one labeled as ground-truth.

To address this issue, we merge highly similar chunks based on a text-similarity threshold, combining their respective questions into a single context group. This merging strategy reduces noise and ensures that semantically equivalent passages (or chunks) are treated as one, allowing retrieval mechanisms to focus on true distinctions in content rather than trivial rephrasings.

\subsection{SQuAD} The Stanford Question Answering Dataset (SQuAD)~\cite{squad1} is widely recognized as a benchmark for reading comprehension 
and extractive QA. 
%Released in 2016, SQuAD 1.1 includes over 100,000 human-generated questions derived from more than 
%500 Wikipedia articles. 
Each question is associated with an exact answer span in the corresponding article, ideal for our extractive evaluation purposes.

\iffalse
\paragraph{Dataset Structure.}
In SQuAD 1.1, \emph{one Wikipedia article can be split into multiple paragraphs (contexts) under a single title}. 
Thus, for each title, there are often several distinct passages, each with its own set of QA pairs. 
In practice, these paragraphs are treated as individual \emph{contexts} from which an answer can be extracted. 
For instance, if an article titled \textit{``Barack Obama''} contains eight paragraphs, each one may feature 
a unique set of questions, effectively yielding eight candidate contexts under the same title.


\paragraph{Challenges.}
While SQuAD’s question-answer pairs are more \emph{controlled} than real user queries, 
the dataset still poses significant challenges:
\begin{itemize}
    \item \textbf{Multiple Contexts Per Title.} Since a single Wikipedia article is often split into 
    several distinct paragraphs, it is not enough for the system to simply retrieve the right \emph{title}. 
    The model must also identify the \emph{correct context (paragraph)} under that title. An error in selecting 
    the appropriate paragraph can lead to an irrelevant snippet—thus failing to answer the question accurately.
    \item \textbf{Varied Question Complexity.} SQuAD questions can range from simple fact lookups 
    (e.g., \emph{``What year was X established?''}) to more nuanced queries that involve limited 
    reasoning or reference resolution.
\end{itemize}

In summary, the presence of multiple paragraphs per title, coupled with strict extractive requirements, 
makes \emph{SQuAD 1.1} a strong benchmark for evaluating whether a retrieval-augmented model can 
(1) find the relevant segment among many possible paragraphs, and 
(2) extract precisely the correct answer to the query.
\fi

\subsection{MultiHop-RAG} \emph{MultiHop-RAG}~\cite{multihoprag-paper} is specifically designed to test multi-hop question answering. 
%In contrast to single-hop QA, multi-hop queries require retrieving and reasoning over multiple pieces of evidence 
%from separate documents before arriving at a final answer. These cross-document dependencies pose a unique challenge 
%that goes beyond merely extracting an answer span from a single context.
%\paragraph{Dataset Composition.}
Unlike SQuAD and NQ, which pair each question with a single relevant paragraph or article, 
MultiHop-RAG associates multiple ground-truth documents with each query. 
For instance, a query such as:
\emph{``Which company is being scrutinized by multiple news outlets for anticompetitive practices 
and is also suspected of foul play by individuals in other reports?''}
will require cross-referencing two or more articles to gather the necessary evidence. 

\iffalse
Hence, the dataset includes:
\begin{itemize}
    \item \textbf{Corpus Documents.} Each document is typically drawn from news articles and may be chunked into smaller passages.
    \item \textbf{Queries.} Each query is tied to an \emph{evidence list}: multiple facts (or references to particular documents) needed to form a complete answer.
\end{itemize}

By correlating each query with the specific documents it requires, we preserve the multi-hop nature of the task.

\paragraph{Key Challenges.}
\begin{itemize}
    \item \textbf{Cross-Document Reasoning.} Many queries demand assembling facts from multiple sources; 
    a retrieval system must \emph{collect} the right set of documents rather than stopping at the first plausible snippet.
    \item \textbf{Partial Evidence Matches.} Even if a system retrieves \emph{some} relevant documents, 
    missing just one critical piece of evidence can lead to an incorrect final answer. 
    Evaluations often measure both \emph{full} and \emph{partial} retrieval success.
\end{itemize}

\paragraph{Comparison with NQ and SQuAD.}
Unlike \emph{SQuAD 1.1}  and \emph{Natural Questions}, \emph{MultiHop-RAG} \textbf{explicitly} mandates multi-document retrieval. 
A single ``context'' is often insufficient; the system \emph{must} locate \emph{all} relevant pieces of text and combine them. 
This added complexity makes MultiHop-RAG an ideal benchmark for analyzing \emph{how well} retrieval-augmented models handle queries 
that cross document boundaries and require \emph{holistic} evidence gathering.
\fi








\subsection{Evaluation Metrics}
\label{subsec:nq-metrics}
For \emph{Natural Questions (NQ)}
and \emph{SQuAD},
each query typically has a \emph{single} correct Wikipedia article and a specific paragraph in that article as ground truth. 
We use the following metrics aimed at capturing whether QuOTE can precisely isolate the article along with the correct answer span.

\begin{itemize}
    \item \textbf{Context Accuracy (C@k):} The fraction of queries for which the correct \emph{paragraph-level} context 
    is retrieved within the top-$k$ results. If a system retrieves the exact paragraph containing the short
    answer at any rank $\le k$, we consider it a successful retrieval.

    \item \textbf{Title Accuracy (T@k):} The fraction of queries for which the correct \emph{article-level} title 
    is found among the top-$k$ results. This is a coarser (i.e., easier) measure compared to paragraph-level context accuracy 
    but still offers insight into whether the system can identify the right document (for instance, the correct Wikipedia page).
\end{itemize}

\iffalse
\vspace{1em}




\subsubsection{Evaluation Metrics}
\label{subsec:multihop-metrics}
\fi
\noindent
\emph{MultiHop-RAG} queries can reference \emph{multiple} relevant documents. Consequently, we employ:

\begin{itemize}
    \item \textbf{Full Match Accuracy (Full@k):} All evidence pieces required by the query must be found 
    within the top-$k$ retrieved results. If even one piece of critical evidence is missing, 
    the query is marked as a failure under this measure.

    \item \textbf{Partial Match Accuracy (Part@k):} Because missing one or more documents can still lead to a partially correct answer, 
    we measure the \emph{percentage of required evidence} found in the top-$k$ results. This measure 
    highlights how retrieval errors degrade performance. For instance, a system might retrieve 2 of the 3 
    needed documents (66.7\% partial match), which can be useful for partial downstream reasoning but might 
    not yield the fully correct answer.
\end{itemize}

This two-level evaluation (full vs.\ partial) captures the difficulty of multi-hop retrieval 
where multiple documents must be combined to arrive at a final answer.