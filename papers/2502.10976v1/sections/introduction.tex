\section{Introduction}

Retrieval-augmented generation (RAG~\cite{rag1,rag2,rag3})
serves as a significant contribution to the deployment and acceptance of LLMs in practice.
Given a user's prompt, RAG retrieves relevant information from a document collection, augments (prefixes) it to the user's prompt, thus helping ensure that any generated content can be accurate, pertinent, and grounded in up-to-date information.
In a typical RAG implementation, at pre-query time, the corpus is broken down into chunks, which are stored as vector embeddings. At query time,  these chunks are searched and used to augment the user's prompt.
Several variants of RAG have been proposed over the years~\cite{rag-variant1, rag-variant2, rag-variant3} to address specific use cases and challenges. 

RAG has helped reinforce the criticality of information retrieval (IR) as a vital component of modern NLP and AI pipelines. Despite this resurgence, much of the focus has been on enhancing the G (generation) component, often leaving advancements in the R (retrieval) aspect comparatively underexplored. Recently, some notable efforts have emerged to address this imbalance.

For example, Anthropic introduced contextual retrieval~\cite{anthropic_contextual_retrieval}
where each
chunk is augmented with additional context before embedding; this approach is claimed to reduce incorrect chunk retrieval rates by up to 67\%.
%\narenc{not sure if prompt caching is the right example here.}
Similarly, recent works have explored prompt caching~\cite{prompt-caching-RAG}, a strategy to reuse previously retrieved or generated results to optimize latency and computation costs in iterative or repetitive query scenarios.

Our work aligns with this vein of `advancing R for G', particularly focusing on improving the modeling of document chunks as they are embedded. One of our key insights is that documents can often be more effectively represented by the questions they can answer, rather than solely by their direct content. To this end, for each chunk, we propose generating a set of questions that the chunk is likely to answer, embedding these alongside the original content. We refer to such embeddings as Question-Oriented  Text Embeddings (QuOTE). See Fig.~\ref{fig:quote_overview} for how QuOTE works.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\columnwidth]{figures/quote-mode.png}
    \caption{Overview of QuOTE. Documents are split into chunks and processed by a question generator (LLM) to create relevant questions. Chunks along with the questions they purport to answer are embedded in a vector database. At query time, a retriever and deduplicator processes user queries to generate final responses.}
    \label{fig:quote_overview}
\end{figure}


This paper makes the following contributions.
\begin{enumerate}
\item We demonstrate that 
the idea of embedding (hypothetical) questions along with text chunks significantly enhances retrieval performance, particularly in scenarios where nuanced understanding of the content is required. This idea holds promise beyond RAG by opening up the possibility of question generation as a fundamental indexing strategy.
\item 
We prioritize retrieval performance 
rather than
generation quality in our evaluation, and
conduct an exhaustive empirical analysis of QuOTE with multiple language models, several key datasets, a range of query workloads, and compare it versus other RAG benchmarks. This gives insight into specific regions of the configuration space where QuOTE performs best and future directions of research.
\item Beyond empirical results, we characterize the features of RAG settings 
where (and why) QuOTE works, and how we can anticipate performance improvements prior to embarking on QuOTE-style indexing for given corpora. 
\end{enumerate}
