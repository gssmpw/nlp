\section{Related Work}
Many studies have
highlighted the impact of key design choices for the success of a RAG implementation~\cite{ragimp1,ragimp2, ragimp3}. 

\subsection{Dense vs Sparse Retrievers}
The debate between dense and sparse retrievers continues into
RAG
research~\cite{sparse-vs-dense1,sparse-vs-dense2}. Dense retrievers, such as those based on vector embeddings, excel at capturing semantic similarity, making them particularly effective for nuanced queries. However, sparse retrievers like BM25 and TF-IDF continue to dominate in scenarios where explicit token matches, such as named entities, acronyms, or abbreviations, are critical to relevance. This distinction has led to hybrid approaches in many RAG systems, which combine dense and sparse retrievers. For example, a typical implementation involves first running a keyword-based sparse retrieval to gather an initial pool of relevant chunks, followed by a dense retrieval to refine the results. 

\subsection{Retrievers vs Rerankers}
Many RAG systems employ a two-step pipeline: a fast retriever selects the top-k candidate chunks, and a reranker, typically a computationally intensive cross-encoder, reorders these candidates for final use. While rerankers generally improve the quality of retrieved results, recent research~\cite{Jacob-Drozdov-drowning-in-documents} cautions against extending reranking to larger candidate sets. Beyond a certain threshold, performance tends to plateau and may even degrade, likely due to noise introduced in larger retrieval pools. These findings underscore the importance of balancing efficiency and effectiveness in the retrieval-reranking pipeline.

\subsection{Exact search vs Approximate Nearest Neighbors (ANN)}
Approximate nearest neighbor (ANN) techniques ~\cite{ann-ir-paper} have become the de facto standard for scalable dense retrieval due to their ability to handle large corpora efficiently. However, exact search methods, while computationally more demanding, offer greater precision in certain use cases, such as high-stakes QA tasks. Several studies~\cite{anncomp1, anncomp2} compare these approaches, highlighting trade-offs in latency, accuracy, and robustness to query variations. For instance, ANN methods may struggle with long-tail queries or datasets containing subtle semantic distinctions.

\subsection{Distractions vs Noise in RAG}
Cuconasu et al.~\cite{cuconasu} study the performance of RAG for QA tasks in the presence of so-called {\it distracting} and {\it noise} documents. Distracting documents are those with high retrieval scores, but that do not contain the answer; noise documents are picked at random from the corpus. The interesting finding from this study was that while distracting documents lead to performance deterioration as expected, noise documents lead to improved performance, presumably due to better reliance on pretrained reasoning. 
However, these findings are somewhat questioned by recent work~\cite{leto2024toward}, which suggests that noise documents can degrade system reliability in certain settings, calling for further investigation.

%\narenc{Talk about why this is so.}
%\narenc{Although this paper \url{https://arxiv.org/pdf/2411.07396} \cite{leto2024toward}contradicts this.}

\subsection{Real vs Hypothetical Embeddings}
Contextual retrieval techniques, such as Anthropic's approach to augmenting chunks with additional information before embedding, have emerged as promising ways to reduce retrieval errors. Similarly, Hypothetical Document Embeddings (HyDE)~\cite{hyde2022} involve generating synthetic text based on the query and embedding it alongside real documents. These methods aim to capture query-specific nuances, resulting in more robust retrieval in open-domain and QA contexts. Our work builds on these approaches by leveraging question-based chunk representations for improved relevance.

%\narenc{Talk about contextual retrieval of Anthropic, HyDE}
%\url{https://arxiv.org/pdf/2408.14906} \cite{russak2024writing}

\subsection{Supporting Asymmetric QA Tasks}
In many QA scenarios, particularly in customer support and enterprise search, there exists a fundamental asymmetry: user queries are often brief, while answers require detailed, structured information. RAG systems addressing this imbalance have incorporated techniques such as hierarchical retrieval~\cite{raga}, multi-hop reasoning~\cite{mavi2022multihop}, and weighted retrieval pipelines~\cite{ragc} to bridge this gap. Recent efforts in this domain include query-expansion strategies~\cite{ragf} and retrieval conditioning~\cite{ragg} to better align user intent with document granularity.

\subsection{Neural Information Retrieval}
Neural information retrieval methods aim to model complex semantic relationships and contextual relevance more effectively than traditional approaches.
While approaches like ColBERT~\cite{colbert-3} and DPR~\cite{dpr-4} have made significant strides in dense retrieval, they continue to struggle with nuanced information seeking behaviors, involving hierarchical relationships, managing distributed information across multiple documents, and dealing with context-dependent relevance ranking.

\subsection{End-to-End RAG Systems}
Fully integrated, end-to-end RAG systems (e.g., from companies like Vectorize.io) are becoming increasingly popular for tasks requiring seamless interaction between retrieval and generation. Recent work~\cite{10.1145/3626772.3657957} has focused on optimizing these systems for efficiency, scalability, and robustness. End-to-end designs often integrate prompt caching, hybrid retrieval, and adaptive reranking to achieve state-of-the-art performance across diverse NLP tasks. 


%\url{https://dl.acm.org/doi/abs/10.1145/3626772.3657957} \cite{10.1145/3626772.3657957}