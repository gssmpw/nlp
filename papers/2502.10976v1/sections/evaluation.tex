\section{Evaluation}


We conduct a comprehensive evaluation
to answer the below questions:
\begin{enumerate}
\item (Section~\ref{subsec:prompt-effects}) Is QuOTE able to automatically generate questions that improve the performance of retrieval-augmented generation?
\item 
(Section~\ref{sec:quote-orthogonality})
How sensitive is QuOTE performance to the choice of embedding model?
\item (Section~\ref{subsec:num-questions})
How many questions must be generated for QuOTE to be effective?
\item (Section~\ref{subsec:compare-hyde}) How does QuOTE compare to HyDE, the state-of-the-art approach to query enrichment? 
\item (Section~\ref{subsec:dedup-effect}) How negligible or significant is QuOTE's deduplication overhead?
\item (Section~\ref{subsec:cheaper-models}) Because QuoTE uses an LLM for question generation as well as for answer generation, can we employ a cheaper model for question generation and does this significantly affect performance?
\item (Section~\ref{subsec:context-vs-accuracy}) Can we characterize the properties of contexts for which QuOTE has selective superiority?
\end{enumerate}


\subsection{Effect of Different Prompts to Generate Questions}
\label{subsec:prompt-effects}



\begin{table*}[t]
\centering
\caption{Prompt templates for 
Natural Questions (NQ), SQuAD, and
MultiHop-RAG.}
%\narenc{this can be fixed - no need to do this.} \narenc{Flip the order - you presented MultiHOP last earlier, so it should be the last two rows.}
\label{tab:all-prompts}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{p{3.2cm} p{13.0cm}}
\toprule
\textbf{Dataset + Prompt} & \textbf{Prompt Text (Single String)} \\
\midrule

\textbf{NQ and SQuAD (Basic)}
&
\footnotesize
\texttt{"Generate numerous questions to properly capture all the important parts of the text. Separate each question-answer pair by a new line only; do not use bullets. Format each question-answer pair on a single line as 'Question? Answer' without any additional separators or spaces around the question mark. Text:\textbackslash\{chunk\_text\textbackslash\}"} \\[1em]

\textbf{NQ and SQuAD (Complex)}
&
\footnotesize
\texttt{"Read the following text and generate numerous factual question-answer pairs designed to resemble authentic user search queries and natural language variations. Each question should accurately and semantically capture important aspects of the text, with varying lengths and complexities that mirror real-world search patterns. Include both shorter, keyword-focused questions such as 'who founded Tesla Motors' and longer, natural style questions like 'when did Elon Musk first start Tesla company'. Incorporate 'how' and 'why' questions to reflect genuine user curiosity. Avoid using phrases like 'according to the text' and abstain from pronouns by specifying names or entities. Ensure questions are not overly formal or artificial, maintaining a natural query style. Immediately follow each question with its precise answer on the same line, formatted as 'Question? Answer', without any additional formatting or commentary. Each pair should be on its own line. Text:\textbackslash\{chunk\_text\textbackslash\}"} \\[1em]
\textbf{MultiHop-RAG (Basic)} 
& 
\footnotesize
\texttt{"Generate enough multi-hop questions along with their answers to properly capture all the important parts of the text. These questions should require integrating multiple pieces of information to answer. Separate each question-answer pair by a new line only; do not use bullets. Format each question-answer pair on a single line as 'Question? Answer' without any additional separators or spaces around the question mark. Text:\textbackslash\{chunk\_text\textbackslash\}"} \\[1em]
\textbf{MultiHop-RAG (Complex)} 
& 
\footnotesize
\texttt{"Read the following text and generate complex, multi-hop questions that require integrating multiple pieces of information from the text to answer. The questions should involve reasoning and synthesis, referring to different parts or aspects of the text. Do not use phrases like 'according to the text', 'mentioned in the text', or 'in the text'. All questions should be one sentence long. Never use pronouns in questions; instead, use the actual names or entities. Format each question on a single new line as Question? Answer without any additional separators or spaces around the question mark. Text:\textbackslash\{chunk\_text\textbackslash\}"} \\[1em]
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\small
\caption{Key retrieval results across Natural Questions (NQ), SQuAD, and MultiHop-RAG for 
\textbf{Naive}, \textbf{Basic}, and \textbf{Complex} prompting strategies. 
For NQ and SQuAD, we report \textbf{Top-1} (\(C@1\)) and \textbf{Top-5} (\(C@5\)) Context Accuracy, 
along with \textbf{Top-1} (\(T@1\)) and \textbf{Top-5} (\(T@5\)) Title Accuracy. 
For MultiHop-RAG, we present \textbf{Full Match} at \(k=5\) and \(k=20\) (\(\text{Full@}5\), \(\text{Full@}20\)), 
and \textbf{Partial Match} (\(\text{Part@}5\), \(\text{Part@}20\)). 
Bolded entries denote the best performance for each metric.}
\label{tab:combined-results}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Method}} 
& \multicolumn{4}{c|}{\textbf{Natural Questions (NQ)}} 
& \multicolumn{4}{c|}{\textbf{SQuAD}} 
& \multicolumn{4}{c}{\textbf{MultiHop-RAG}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& \textbf{C@1} & \textbf{C@5} & \textbf{T@1} & \textbf{T@5} 
& \textbf{C@1} & \textbf{C@5} & \textbf{T@1} & \textbf{T@5}
& \textbf{Full@5} & \textbf{Full@20} & \textbf{Part@5} & \textbf{Part@20} \\
\midrule
\textbf{Naive} 
& 32.92\% & 89.23\% & \textbf{99.85\%} & \textbf{100.00\%}
& 82.58\% & 96.00\% & 98.70\% & 99.13\%
& 8.00\%  & 21.50\% & 29.6\%  & 50.0\%  \\
\textbf{Basic} 
& 34.77\% & \textbf{92.46\%} & \textbf{99.85\%} & \textbf{100.00\%}
& 85.82\% & 98.16\% & 99.46\% & \textbf{99.89\%}
& 3.00\%  & 16.50\% & 27.7\%  & 47.5\%  \\
\textbf{Complex} 
& \textbf{38.00\%} & 92.15\% & 99.69\% & \textbf{100.00\%}
& \textbf{90.26\%} & \textbf{98.81\%} & \textbf{99.68\%} & 99.78\%
& \textbf{8.50\%} & \textbf{26.50\%} & \textbf{31.7\%} & \textbf{54.9\%} \\
\bottomrule
\end{tabular}
\end{table*}




A central consideration for QuOTE-style indexing is how the prompt itself influences the \emph{quality} of generated questions. We compare two main prompt templates:
\begin{itemize}
    \item \textbf{Basic Prompt:} Instructs the model to “Generate enough questions to properly capture all the important parts of the text”. The questions are short, direct, and do not include advanced reasoning cues.
    \item \textbf{Complex Prompt:} Adds instructions for more detailed or multi-hop reasoning. In MultiHop-RAG, for example, the complex prompt explicitly requests multi-hop questions referencing multiple pieces of information. In SQuAD or NQ, it encourages short factual queries without referencing the text directly, thereby aiming for more robust coverage of the chunk’s content.
\end{itemize}
\noindent
We compare the performance of both these prompts with a naive RAG implementation.
As
Table~\ref{tab:combined-results}
shows,
the Complex Prompt achieves the highest
Top-1 Context Accuracy overall.
Title Accuracy metrics remain near-perfect across all methods beyond Top-1, indicating that differences among prompts are most pronounced at the paragraph selection level. These observations suggest that more advanced prompting yields modest but meaningful improvements in precisely identifying relevant
questions for specific
passages.

\iffalse
\vspace{1em}
\subsubsection{Results on Natural Questions (NQ)}
\label{subsubsec:prompt-nq}
For \emph{Natural Questions}, Table~\ref{tab:combined-results} summarizes the Top-1 Context Accuracy for Naive, Basic, and Complex prompting strategies. As shown in the table, the Complex prompt achieves the highest Top-1 Context Accuracy, followed by Basic and then Naive. Title Accuracy metrics remain near-perfect across all methods beyond Top-1, indicating that differences among prompts are most pronounced at the paragraph selection level. These observations suggest that more advanced prompting yields modest but meaningful improvements in precisely identifying the correct passage.

\paragraph{Discussion (NQ).}
The table demonstrates that both Basic and Complex prompts outperform the Naive approach on NQ, with Complex prompts providing the highest improvement in Top-1 Context Accuracy. Although Title Accuracy remains uniformly high, the enhanced context retrieval by Complex prompts indicates more effective question generation, despite only moderate overall gains.

\vspace{1em}
\subsubsection{Results on SQuAD}
\label{subsubsec:prompt-squad}
Table~\ref{tab:combined-results} presents the retrieval performance for SQuAD under Naive, Basic, and Complex prompting strategies. Notably, the Complex prompt considerably increases Top-1 Context Accuracy compared to Naive and Basic, while Title Accuracy remains consistently high across methods. This suggests that the Complex prompt is particularly effective in single-hop scenarios like SQuAD, improving the system’s precision in selecting the correct paragraph.

\paragraph{Discussion (SQuAD).}
Referencing the table, the progression from Naive to Basic to Complex prompts in SQuAD shows a clear improvement in Top-1 Context Accuracy. The Complex prompt's superior performance underscores the benefits of more sophisticated question generation in environments with focused, well-structured paragraphs.

\vspace{1em}
\subsubsection{Results on MultiHop-RAG}
\label{subsubsec:prompt-multihop-updated}
For \emph{MultiHop-RAG}, Table~\ref{tab:combined-results} summarizes key metrics including Full Match Accuracy at \(k=5\) and \(k=20\) as well as Partial Match statistics at those points for each prompting strategy. The Complex prompt shows the highest Full Match and Partial Match values among the three approaches, particularly at higher \(k\) levels, although the overall percentages remain relatively low. This indicates that while more advanced prompts improve multi-hop retrieval, significant challenges persist.

\paragraph{Discussion (MultiHop-RAG).}
The data in the table reveal that the Complex prompt outperforms both Naive and Basic strategies in terms of Full and Partial Match metrics. Despite the modest absolute gains, these improvements suggest that complex instructions help in retrieving a greater portion of the required evidence for multi-hop queries, albeit with increased computational costs.


\vspace{1em}
\paragraph{Overall Observations.}
\begin{itemize}
    \item \textbf{SQuAD Gains with Complex Prompt:} As evidenced in Table~\ref{tab:combined-results}, SQuAD experiences the largest improvement in Top-1 Context Accuracy when moving from Naive to Basic and then to Complex prompting. The Complex prompt, in particular, significantly enhances precision in selecting the correct paragraph, likely because SQuAD’s smaller, more focused paragraphs are well-suited to the detailed instructions provided.
    
    \item \textbf{MultiHop-RAG Improvements:} The table shows that while Basic and Complex prompts yield similar partial-match rates in MultiHop-RAG, the Complex prompt consistently offers slightly better full-match and partial-match performance across various \(k\) values compared to both Naive and Basic methods. However, the differences remain relatively modest, suggesting that multi-hop retrieval may require more specialized techniques or in-context examples to achieve substantial gains.
    
    \item \textbf{Moderate Gains on NQ:} For Natural Questions, both Basic and Complex prompts improve Top-1 Context Accuracy over Naive retrieval. Although the improvements are moderate, the Complex prompt demonstrates a clear edge over the Basic approach, indicating that advanced prompting provides additional benefits even in more variable, real-user query settings.
\end{itemize}

Thus, the \emph{Complex} prompt style can yield notable improvements—especially in single-hop scenarios like SQuAD—while offering moderate enhancements for multi-hop or more open-ended datasets like NQ. Future directions might include integrating in-context exemplars or designing specialized prompts tailored to the unique challenges of multi-hop reasoning for tasks akin to \emph{MultiHop-RAG}.
\fi


\subsection{QuOTE Performance vis-a-vis Embedding Model}
\label{sec:quote-orthogonality}

Table~\ref{tab:embeddings-comparison} compares \textbf{Naive} vs.\ \textbf{QuOTE} 
 modes on three datasets---\textbf{SQuAD} (single-hop), \textbf{NQ} (single-hop), 
and \textbf{MultiHop-RAG} (multi-hop) across a range of datasets. We report Top-$k$ context/title accuracy for SQuAD and NQ, 
and full/partial match for MultiHop. Despite large differences in baseline quality
(e.g., \texttt{jinaai} vs.\ \texttt{WhereIsAI} vs.\ \texttt{Alibaba}),
note that QuOTE generally improves 
retrieval metrics (especially Top-1 Context Accuracy or Full@20) \emph{regardless of the underlying embedding model}.
QuOTE often raises Top-1 Context Accuracy by 5--17 points on SQuAD and 1--3 points on NQ, and can improve Full@20 by up to several points in MultiHop-RAG.
MultiHop-RAG remains challenging, as even large gains may yield relatively modest absolute numbers (e.g., 9\% or 10\% full match at \(k=5\)). However, QuOTE still outperforms or closely matches a naive approach across all embedding models.

%In this section, we demonstrate that \emph{QuOTE} yields consistent improvements across a variety of embedding models. 

\begin{table*}[t]
\centering
\small
\caption{Performance of \textbf{Naive} vs.\ \textbf{QuOTE} modes on SQuAD, NQ, and MultiHop-RAG, 
across five embedding models. Per-row bolded entries denote the better value for that metric. %\narenc{C@1 , Full@5 etc should be defined in the evaluation section.} \narenc{What do you mean by Standard - do you mean Naive RAG? be consistent. Use the same name everywhere.}
}
\label{tab:embeddings-comparison}
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{ll|cc|cc|cccc}
\toprule
\multirow{2}{*}{\textbf{Embedding Model}} 
& \multirow{2}{*}{\textbf{Approach}} 
& \multicolumn{2}{c|}{\textbf{SQuAD}} 
& \multicolumn{2}{c|}{\textbf{NQ}} 
& \multicolumn{4}{c}{\textbf{MultiHop-RAG}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-10}
& 
& \textbf{C@1} & \textbf{C@20}
& \textbf{C@1} & \textbf{C@20}
& \textbf{Full@5} & \textbf{Full@20} & \textbf{Part@5} & \textbf{Part@20} \\
\midrule
\multirow{2}{*}{\texttt{Alibaba-NLP/gte-base-en-v1.5}} 
& Naive 
  & 59.94 & 96.35
  & 54.25 & 91.59
  & 6.50 & 25.50 & 29.2 & 55.7 \\
& QuOTE 
  & \textbf{77.16} & \textbf{97.68}
  & \textbf{57.18} & \textbf{92.72}
  & \textbf{9.00} & \textbf{27.50} & \textbf{32.0} & \textbf{56.9} \\
\midrule
\multirow{2}{*}{\texttt{text-embedding-3-small}} 
& Naive 
  & 68.59 & \textbf{97.58}
  & 51.15 & \textbf{92.59}
  & 7.00 & \textbf{32.50} & 33.6 & \textbf{62.0} \\
& QuOTE   
  & \textbf{79.07} & 96.69
  & \textbf{51.57} & 90.16
  & 7.00 & 27.50 & \textbf{33.7} & 56.0 \\
\midrule
\multirow{2}{*}{\texttt{WhereIsAI/UAE-Large-V1}}
& Naive
  & 69.49 & 96.98
  & 56.84 & 94.14
  & 4.00 & 23.00 & \textbf{31.8} & \textbf{57.9} \\
& QuOTE
  & \textbf{80.64} & \textbf{97.51}
  & \textbf{58.35} & \textbf{94.31}
  & \textbf{9.00} & \textbf{27.00} & 31.4 & 54.9 \\
\midrule
\multirow{2}{*}{\texttt{jinaai/jina-embeddings-v3}}
& Naive
  & 65.45 & 94.45
  & 53.87 & \textbf{93.76}
  & 4.50 & \textbf{25.00} & 26.0 & \textbf{53.0} \\
& QuOTE
  & \textbf{76.97} & \textbf{96.94}
  & \textbf{55.17} & 93.72
  & \textbf{6.50} & 24.00 & \textbf{29.9} & 51.6 \\
\midrule
\multirow{2}{*}{\texttt{sentence-transformers/all-MiniLM-L6-v2}}
& Naive
  & 65.53 & 95.03
  & 50.77 & 87.23
  & 5.00 & \textbf{20.50} & 26.8 & \textbf{51.2} \\
& QuOTE
  & \textbf{72.35} & \textbf{97.75}
  & \textbf{51.19} & \textbf{88.57}
  & \textbf{5.50} & 20.00 & \textbf{26.9} & 47.0 \\
\bottomrule
\end{tabular}
\end{table*}



%In sum, these results confirm that \textbf{QuOTE} is \emph{orthogonal} to the specific embedding model choice; 
%its core advantage arises from representing each chunk through multiple question embeddings, 
%thereby capturing user query variety more effectively.


\iffalse
\subsection{Effect of kNN}

How many chunks should be retrieved and is perf improvement consistent over all values of k?

Use Naive RAG as baseline and show \% improvement?
To squeeze all datasets into one plot
\fi



\subsection{Effect of Number of Questions}
\label{subsec:num-questions}

One key factor in \emph{question-oriented} retrieval is deciding how many questions an LLM should generate for each chunk of text. 
Generating too few may overlook critical details, while generating too many can introduce redundancy or noise. 
We therefore tested multiple settings across our three datasets (\emph{Natural Questions}, \emph{SQuAD}, and \emph{MultiHop-RAG}), 
varying the number of questions (1, 5, 10, 15, 20, 30) and also including an `LLM decides' setting.
In each case, we measure how \textbf{Context Accuracy} and \textbf{Title Accuracy} changes, or in the case of MultiHop-RAG, 
how \textbf{Full Match} and \textbf{Partial Match} scores are affected.

To systematically investigate the effect of varying the number of generated questions, we parameterize our LLM prompt to either generate:
\begin{itemize}
    \item \textbf{Fixed \# Questions:} If a desired quantity \textit{num\_questions} is provided, the prompt includes a directive such as:

\begin{verbatim}
"Generate exactly {num_questions} questions 
to properly capture all the important parts
of the text."
\end{verbatim}

    \item An \textbf{LLM Decides \# Questions}: Here, the LLM is simply instructed to:

\begin{verbatim}
"Generate enough questions to properly capture
all the important parts of the text."
\end{verbatim}
\end{itemize}

%Under the hood, our pipeline embeds the resulting questions for each chunk and stores these question embeddings in the vector database. At query time, if an LLM has generated more questions, that chunk will be represented by a richer set of question embeddings, potentially increasing the likelihood of matching more user queries accurately. However, too many questions may introduce overlap or noise, prompting our investigation into the \emph{optimal} number of questions for each dataset.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/nq_num_questions.png} 
%     \caption{Comparison of Top-1 Context and Title for NQ across various number of questions.}
%     \label{fig:cheaper-models}
% \end{figure}


\paragraph{SQuAD}
Table~\ref{tab:questions-comparison} shows that as the number of generated questions per chunk increases from 5 
to around 10 or 20, Top-1 Context Accuracy rises from about 73\% to as high as 76\%, and \emph{Top-5} surpasses 97\% in most settings. 
\emph{Title Accuracy} also remains consistently high, crossing 99\% even at Top-1 for 10+ questions. 
Interestingly, letting the LLM decide how many questions to generate (``LLM Decides'') yields a strong Top-1 Context Accuracy of 76.17\% 
and Top-1 Title Accuracy of 99.30\%.

\begin{itemize}
    \item \textbf{Naive vs. 10 questions.} A naive approach (66.60\% Top-1 Context) significantly lags behind 
    generating 10 questions (74.91\% Top-1), showing that question augmentation dramatically helps correct chunk retrieval.
    \item \textbf{Diminishing returns.} Beyond 10–15 questions, the gains in Top-1 Context Accuracy plateau 
    around 74–76\%. For instance, 20 questions achieve 76.66\%, comparable to 10 questions at 74.91\%.
\end{itemize}



\paragraph{Natural Questions (NQ)}

\begin{table*}[t]
\centering
\caption{Performance comparison across different numbers of generated questions on SQuAD, NQ, and MultiHop-RAG datasets. Results show Context and Title Accuracy at different k values for SQuAD and NQ, and Full/Partial Match for MultiHop-RAG. Bolded entries denote the best performance per metric.}
\label{tab:questions-comparison}
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Questions}} 
& \multicolumn{4}{c|}{\textbf{SQuAD}} 
& \multicolumn{4}{c|}{\textbf{NQ}} 
& \multicolumn{4}{c}{\textbf{MultiHop-RAG}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& \textbf{C@1} & \textbf{C@5} & \textbf{T@1} & \textbf{T@5}
& \textbf{C@1} & \textbf{C@5} & \textbf{T@1} & \textbf{T@5}
& \textbf{Full@5} & \textbf{Full@20} & \textbf{Part@5} & \textbf{Part@20} \\
\midrule
\texttt{Naive}
& 67.11 & 90.06 & 96.45 & 98.34
& 32.92 & 89.23 & 99.85 & 100.00
& 8.00 & 22.50 & 29.8 & 50.7 \\
\midrule
\texttt{1 Question}
& 66.02 & 90.12 & 96.07 & 98.58
& 32.46 & 91.85 & 100.00 & 100.00
& 15.00 & 30.00 & 37.0 & 61.2 \\
\texttt{5 Questions}
& 77.05 & 94.90 & 97.81 & \textbf{99.24}
& 35.85 & 92.46 & 99.69 & 100.00
& 12.00 & 33.00 & 37.9 & 61.6 \\
\texttt{10 Questions}
& 77.58 & \textbf{95.03} & 97.39 & 98.41
& 35.85 & \textbf{92.46} & 99.54 & 100.00
& 16.00 & 35.00 & 38.2 & 62.2 \\
\texttt{15 Questions}
& 77.22 & 94.05 & 96.49 & 97.47
& \textbf{38.31} & 90.92 & 99.69 & 100.00
& 14.00 & 34.00 & 32.1 & 60.3 \\
\texttt{20 Questions}
& 76.24 & 93.07 & 95.73 & 96.45
& 35.85 & 92.31 & 99.69 & 100.00
& 14.00 & \textbf{35.00} & 34.9 & \textbf{63.8} \\
\texttt{30 Questions}
& 76.05 & 92.18 & 95.05 & 95.73
& 34.00 & 90.15 & 99.54 & 99.69
& 11.00 & 30.00 & 31.5 & 58.5 \\
\midrule
\texttt{LLM Decides}
& \textbf{77.65} & 93.97 & \textbf{96.30} & 97.13
& 38.00 & 92.15 & 99.69 & 100.00
& \textbf{18.00} & \textbf{35.00} & \textbf{41.9} & 62.7 \\
\bottomrule
\end{tabular}
\end{table*}



We observe a similar pattern in \emph{Natural Questions} (Table~\ref{tab:questions-comparison}). The naive approach 
(and letting the LLM decide automatically) both hover around 61–64\% Top-1 Context Accuracy. 
Generating 15 or 20 questions per chunk can push Top-1 Context Accuracy slightly higher, surpassing 64\%. 
In general, \emph{Title Accuracy} improves more noticeably, approaching or exceeding 79\% at Top-1 when 15+ questions are used.

\begin{itemize}
    \item \textbf{Moderate Gains.} Unlike SQuAD, the gains from adding more questions in NQ are more modest 
    (e.g., from 61\% to 65\% in Top-1 Context Accuracy).
    \item \textbf{Title Accuracy.} By contrast, Title Accuracy climbs above 79\% at Top-1 (with 15–20 questions), 
    indicating that question generation consistently helps the system find the right \emph{article}, even if 
    the precise paragraph-level retrieval remains challenging.
\end{itemize}





\paragraph{MultiHop-RAG}
Because MultiHop-RAG tasks require retrieving \emph{all relevant documents}, we track both \emph{Full Match Accuracy} 
and \emph{Partial Match Statistics}. As seen in Table~\ref{tab:questions-comparison}:

\begin{itemize}
    \item \textbf{Full Match Accuracy.} Baseline naive retrieval (LLM Naive) achieves only about 10\% at $k=5$ 
    and 37\% at $k=20$. Using question generation with 5 or 10 questions can improve the $k=5$ Full Match 
    from 10–12\% to 15–16\%, and from 19\% to 24–25\% at $k=10$. Even at higher $k$ values (15 or 20), 
    best-case Full Match remains in the 30–35\% range, underscoring the challenge of truly multi-hop retrieval. 
    Interestingly, letting the LLM decide (without specifying a question count) yields 18\% at $k=5$ and 35\% at $k=20$.
    \item \textbf{Partial Match.} We also evaluate the \emph{average percentage of required evidence} retrieved. 
    Generating around 5–20 questions consistently pushes partial match rates above 50–60\% at $k=15$ or $k=20$, 
    compared to 35–45\% with naive retrieval. This indicates that even when the system does not achieve a complete 
    full match, it still locates \emph{some} of the essential documents for partial reasoning.
\end{itemize}

These findings confirm that, while \emph{multi-hop} queries remain significantly harder, carefully chosen question sets 
(i.e., 10–20 questions) yield noticeable improvements over a naive approach.

%\subsection*{Summary of Findings.}
\begin{itemize}
    \item Generating more questions typically improves retrieval performance, but returns diminish beyond 
    about \emph{10–15 questions per chunk}.
    \item Even a moderate number of questions (5–10) can outperform naive retrieval by a wide margin in both 
    single-hop (NQ, SQuAD) and multi-hop (RAG) settings.
    \item In \emph{multi-hop} scenarios, \emph{partial match} is also improved by question generation, 
    indicating that the system at least retrieves some relevant documents more reliably.
\end{itemize}

Overall, most datasets show a \emph{sweet spot} around 10–15 questions, balancing coverage with potential redundancy. 
Although letting the LLM fully decide the number of questions can yield strong results in certain cases (e.g., SQuAD), 
the performance varies by dataset. Consequently, the optimal question count appears to depend on domain complexity 
and the specifics of the QA task.

\subsection{Comparison with HyDE}
\label{subsec:compare-hyde}



A popular technique for query enrichment is \textbf{HyDE}, which generates a hypothetical document at \emph{query time} before embedding it and retrieving relevant chunks. Although HyDE can improve coverage, it requires an LLM call for each incoming query, introducing significant latency. In contrast, \textbf{QuOTE} moves question generation to \emph{index time}, incurring a one-time cost but speeding up the overall querying process. We compare \textbf{Naive RAG} (no query transformations), \textbf{HyDE}, and \textbf{QuOTE} on all three benchmarks. For \textbf{SQuAD} and \textbf{NQ} we showcase
\emph{Top-1 Context Accuracy}
and for \textbf{MultiHop-RAG (multi-hop)}, because queries need multiple pieces of evidence, we focus on \emph{Full Match} at \(k=20\). 
%\narenc{is k the right variable? earlier we used k for something else? be consistent.}

Table~\ref{tab:hyde-comparison} shows that while HyDE sometimes boosts accuracy compared to a Naive approach, its {\bf per-query LLM calls lead to a drastic rise in average retrieval time} (\(\mathbf{+1}\text{--}2 \text{ seconds}\) per query). By contrast, QuOTE often equals or surpasses Naive’s retrieval accuracy with only a modest query-time overhead, as most of its work is done in the indexing phase.

\iffalse
\paragraph{Observations.}
\begin{itemize}
    \item \textbf{Speed vs.\ Accuracy:} HyDE can outperform Naive on multi-hop retrieval (23.50\% vs.\ 23.00\% full-match at \(k=20\)) but imposes a $\sim$5--6\,second query overhead (see MultiHop). By contrast, QuOTE is faster at query time and, for SQuAD, yields a large accuracy boost (90.03\% vs.\ 79.31\% Naive, 76.60\% HyDE).
    \item \textbf{Index-Time vs.\ Query-Time Generation:} QuOTE \emph{frontloads} question generation at index time—leading to a longer addition phase—but saves time during repeated queries, whereas HyDE regenerates text for every query.
    \item \textbf{Natural Questions Nuance:} In NQ, HyDE slightly surpasses Naive (75.24\% vs.\ 73.54\%), but QuOTE ends up near 74.20\%. Both HyDE and QuOTE remain beneficial for retrieving more relevant contexts than Naive alone.
\end{itemize}
\fi

Overall, these findings illustrate that while \textbf{HyDE} can be valuable in certain multi-hop or complex queries, it incurs a substantial latency cost. 
In fact, \textbf{HyDE} can be viewed as primarily a {\bf test-time compute} innovation.
\textbf{QuOTE} offers significant advantages: higher retrieval accuracy than Naive in most cases, a one-time, amortized cost for question generation, and dramatic query latency improvements over HyDE.


\begin{table*}[t]
\centering
\caption{Comparison of \textbf{Naive}, \textbf{HyDE}, and \textbf{QuOTE} across three QA tasks. 
The \textbf{fastest} (lowest time) and \textbf{most accurate} (highest accuracy) entries in each column are \textbf{bolded}. 
In \textbf{SQuAD}, \texttt{Naive} is fastest while \texttt{QuOTE} achieves the highest accuracy; 
for \textbf{NQ}, \texttt{Naive} runs fastest while \texttt{HyDE} slightly outperforms the others in accuracy; 
and in \textbf{MultiHop-RAG}, \texttt{Naive} remains fastest, whereas \texttt{QuOTE} attains the highest full-match rates.}
\label{tab:hyde-comparison}
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|cccc|cccc|cccc}
\toprule
& \multicolumn{4}{c|}{\textbf{SQuAD}} 
& \multicolumn{4}{c|}{\textbf{NQ}} 
& \multicolumn{4}{c}{\textbf{MultiHop-RAG}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
\textbf{Approach} 
& \textbf{Time(s)} & \textbf{ms/q} & \textbf{C@1} & \textbf{C@5}
& \textbf{Time(s)} & \textbf{ms/q} & \textbf{C@1} & \textbf{C@5}
& \textbf{Time(s)} & \textbf{ms/q} & \textbf{Full@5} & \textbf{Full@20} \\
\midrule
\textbf{Naive} & 
\textbf{99.97} & \textbf{108.31} & 79.31\% & 95.88\% & 
\textbf{198.96} & \textbf{187.34} & 32.92\% & 89.23\% & 
\textbf{3.10} & \textbf{15.14} & 7.00\% & 23.00\% \\
\textbf{HyDE} &
1176.20 & 1274.32 & 76.60\% & 92.63\% & 
2707.56 & 2549.49 & 33.23\% & 90.46\% &
1155.80 & 4157.83 & 6.00\% & 23.50\% \\
\textbf{QuOTE} &
130.56 & 141.46 & \textbf{90.03}\% & \textbf{98.48}\% & 
388.38 & 365.71 & \textbf{38.00}\% & \textbf{92.15}\% &
926.41 & 100.75 & \textbf{8.00}\% & \textbf{29.00}\% \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Effect of Deduplication}
\label{subsec:dedup-effect}

Deduplication is essential in \textbf{QuOTE} because each chunk can be indexed multiple times---once per generated question---leading to redundant matches at query time. Again, We compare \textbf{Naive RAG},
\textbf{HyDE} and
\textbf{QuOTE}.
When $k=1$, deduplication is unnecessary, as only one chunk is retrieved. However, when $k \in \{5,10,20\}$, \texttt{QuOTE} systems 
fetch more than $k$ results from the vector index (e.g., $k \times 5$) 
%\narenc{again different use of k} 
and then deduplicate by original chunk text. This extra step 
introduces a small overhead, but we find that QuOTE remains much faster than HyDE (which invokes an LLM at \emph{each} query) 
and substantially outperforms Naive in Top-1 Context Accuracy.

Table~\ref{tab:dedup-quote} shows a head-to-head comparison of the three approaches on a SQuAD subset. 
Each approach processes 923 queries %\narenc{where did 923 come from? sounds like a magic number.}
for all benchmarks, we see that
\textbf{Naive} is the fastest and \textbf{QuOTE} the most accurate.
The added overhead incurred by \textbf{QuOTE} over \textbf{Naive}
small relative to the cost of \emph{per-query} generation in HyDE. 
Hence, \textbf{QuOTE} obtains both \emph{superior accuracy} and \emph{faster query times} than HyDE, 
while incurring a one-time cost for indexing. In settings where repeated queries are common, 
paying a higher index-time cost can significantly improve responsiveness and end-user experience.

\iffalse
For the SQuAD benchmark, observe that
\textbf{Naive} is moderately fast at query time ($\sim$100~s for the entire benchmark) 
    but caps out at only 79.31\% Top-1 Context Accuracy.
    \item \textbf{HyDE} actually \emph{reduces} accuracy to 76.60\%, while incurring \emph{significantly higher} query-time overhead (1,176~s). 
    It transforms each query into a “Wikipedia-style” paragraph, which is time-intensive.
    \item \textbf{QuOTE} delivers the best Top-1 Context Accuracy (90.03\%) while completing the benchmark in 130.56~s---far less than HyDE’s 1,176~s. 
    Index-time question generation does extend the addition phase to 170~s, but \emph{query-time} remains comparatively swift, 
    despite deduplication.

\noindent \textbf{Deduplication Overhead.} 
For $k=5$ or $k=10$, QuOTE retrieves $k\times M$ (e.g., $5\times 5=25$) and prunes duplicates back down to $k$. 
We find this overhead is a fraction of the total query time, so the net throughput still exceeds HyDE’s. In short:
\begin{itemize}
    \item \textbf{Quality vs.\ Latency.} QuOTE obtains higher accuracy than Naive (by $+10.7\%$) and still runs $\sim$9$\times$ faster than HyDE at query time.
    \item \textbf{Index-Time Trade-Off.} QuOTE shifts cost to indexing (170~s vs.\ 10~s), but this is a one-time penalty. 
    For repeated queries, the reduced per-query latency pays off quickly.
\end{itemize}
\fi

\begin{table}[t]
\centering
\caption{Comparison of retrieval approaches. Index=time to build database (seconds), Query=time to process all queries (seconds), ms/q=milliseconds per query, C@1=Context accuracy, T@1=Title accuracy. Bold indicates best per column.}
\label{tab:dedup-quote}
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{lccccc}
\toprule
\textbf{App} & \textbf{Index} & \textbf{Query} & \textbf{ms/q} & \textbf{C@1} & \textbf{T@1} \\
\midrule
Naive & 10.36 & \textbf{99.97} & \textbf{108.31} & 79.31\% & 96.64\% \\
HyDE & \textbf{10.14} & 1176.20 & 1274.32 & 76.60\% & 94.58\% \\
QuOTE & 170.32 & 130.56 & 141.46 & \textbf{90.03\%} & \textbf{98.37\%} \\
\bottomrule
\end{tabular}
\end{table}




%\subsection{Something about information coverage of questions??}


\subsection{Can we use a Cheaper LLM for Question Generation?}
\label{subsec:cheaper-models}
An important practical consideration in RAG-based pipelines is whether \emph{cheaper, smaller models} can generate 
effective questions for indexing, or if premium, large-scale LLMs (e.g., GPT-4) are necessary. To investigate, 
we experimented with a variety of local language models (e.g., \texttt{gemma2-9b}, \texttt{llama3-8b}, and \texttt{qwen2.5-7b}), 
as well as \texttt{gpt-4o-mini}, \texttt{gpt-4o}, and a baseline \texttt{Naive} approach that relies solely on 
the chunk text without question generation. All runs were conducted on a \emph{SQuAD-based subset}. Table~\ref{tab:cheaper-models} 
summarizes the results in terms of 
\textbf{Top-$k$ Context Accuracy} and \textbf{Top-$k$ Title Accuracy}.


\begin{table}[t]
\centering
\caption{Comparison of different models on a SQuAD subset. We report Context Accuracy (C@k) and Title Accuracy (T@k) at k=1 and k=5. Best value(s) in each column are \textbf{bolded}.}
\label{tab:cheaper-models}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Model} & \textbf{C@1} & \textbf{C@5} & \textbf{T@1} & \textbf{T@5} \\
\midrule
\texttt{Naive}       
& 66.60 & 92.80 & 98.32 & 99.44 \\
\midrule
\texttt{QuOTE gemma2-9b}   
& 74.49 & 96.93 & \textbf{99.09} & 99.72 \\
\texttt{QuOTE gpt-4o-mini} 
& \textbf{76.73} & \textbf{97.20} & 98.81 & 99.30 \\
\texttt{QuOTE gpt-4o}      
& 76.38 & 96.72 & \textbf{99.09} & 99.58 \\
\texttt{QuOTE llama3-8b}   
& 73.58 & 95.95 & \textbf{99.09} & \textbf{99.79} \\
\texttt{QuOTE llama3.1-8b} 
& 71.42 & 96.09 & \textbf{99.09} & 99.72 \\
\texttt{QuOTE llama3.2-3b} 
& 70.86 & 94.90 & 98.81 & 99.65 \\
\texttt{QuOTE phi4} 
& 74.07 & 95.95 & 98.81 & 99.30 \\
\texttt{QuOTE qwen2.5-7b}  
& 72.05 & 96.23 & 99.02 & 99.72 \\
\bottomrule
\end{tabular}
\end{table}

We observe that even
smaller models such as 
    \texttt{llama3.2-3b} achieve over 70\% Top-1 Context Accuracy---only a few percentage points behind the more capable 
    \texttt{gpt-4o-mini} or \texttt{gpt-4o} models.
For nearly all models, \emph{Top-1 Title Accuracy} remains around 
    or above 98\%, indicating that the question generation step—regardless of the model size—helps QuOTE 
hone in on the correct article.
Once we allow for more retrieved chunks 
    (Top-10 or Top-20), nearly all approaches exceed 98--99\% Context Accuracy. This suggests that 
    question augmentation significantly reduces misalignment with relevant passages.
    The main trade-off is that \texttt{gpt-4o} and \texttt{gpt-4o-mini} 
    exhibit slightly higher Top-1 Context Accuracy (up to $\sim76\%$) compared to cheaper models 
    (70--74\%); however, local LLMs still offer near-parity in mid- to high-$k$ retrieval settings 
    with notably lower inference cost.


\iffalse
\noindent\textbf{Conclusion.} 
Although large-scale models like GPT-4 consistently achieve slightly higher Top-1 Context Accuracy, 
smaller models (\emph{e.g.}, LLaMA variants around 3B--9B parameters) remain competitive, especially 
once $k$ surpasses 5. These results suggest that \emph{cheaper} LLMs can feasibly serve as a 
question-generation engine without drastically sacrificing accuracy in RAG workflows.
\fi



\subsection{Effect of the Number of Contexts on Retrieval Accuracy}
\label{subsec:context-vs-accuracy}

Analysis of the impact of the number of contexts on retrieval performance reveals distinct patterns between SQuAD and NQ, reflecting their fundamentally different dataset characteristics.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth]{figures/squad_contexts_histogram.png}
\caption{Distribution of contexts per title in SQuAD (N=442 titles). The mean of 42.74 contexts per title and maximum of 149 contexts demonstrate the dataset's high context density.}
\label{fig:squad_distribution}
\end{figure}

SQuAD exhibits a rich context structure, with 442 titles having a mean of 42.74 contexts per title (median=36). This substantial density, ranging from 5 to 149 contexts per title, creates significant potential for confusion with naive retrieval approaches, particularly when similar passages exist within the same document.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth]{figures/nq_contexts_histogram.png}
\caption{Distribution of contexts per title in Natural Questions (N=48,525 titles). The highly concentrated distribution around a median of 1 context per title indicates predominantly singular contexts.}
\label{fig:nq_distribution}
\end{figure}

In stark contrast, NQ presents a much sparser context landscape. Across its 48,525 titles, NQ maintains a mean of just 1.52 contexts per title, with a median of 1, indicating that most titles have unique contexts.

\begin{figure}[ht]
\centering
\includegraphics[width=1\columnwidth]{figures/context_vs_accuracy.png}
\caption{Percentage increase in Top-1 retrieval accuracy with QuOTE compared to naive retrieval across the number of contexts. SQuAD shows steady improvement that grows with the number of contexts, reaching 20.7\% improvement at size 100, while NQ shows consistent but variable gains up to 18.3\%.}
\label{fig:improvement}
\end{figure}

These structural differences manifest clearly in QuOTE's relative performance gains (Figure \ref{fig:improvement}). For SQuAD, we observe a steady increase in QuOTE's advantage as the number of contexts grows. Starting from around 8\% improvement in a small number of contexts, it increases consistently to about 16\% at moderate number of contexts, and continues to improve to exceed the improvement 20\% for a larger number of contexts. This steady improvement trend aligns with the increasing challenge of disambiguating similar contexts in longer documents.

NQ shows a more constrained but generally positive pattern, reflecting its simpler context structure. The improvements start at about 6\% for a small number of contexts and reach peaks of approximately 18\% for a moderate number of contexts. Although the magnitude of the improvements varies, QuOTE consistently enhances the retrieval accuracy in most contexts, although its impact is more variable than in SQuAD.

These insights demonstrate how dataset characteristics fundamentally influence QuOTE's effectiveness. For collections with many contexts per document like SQuAD, QuOTE provides increasingly valuable disambiguation as document length grows. For collections like NQ where most documents contain just a single relevant chunk, QuOTE still provides consistent benefits, though the magnitude varies with the number of contexts.