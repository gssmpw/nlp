%%%%%%%%%%%
% CONTEXT %
%%%%%%%%%%%

\section{Background}

%%%%%%%%%%%%%%%%%%%%
% SOFTMAX DISTANCE %
%%%%%%%%%%%%%%%%%%%%

% SOFTMAX DISTANCE USED AS ANOMALY DETECTION

% The concept of using the entire set of softmax prediction probabilities, rather than solely relying on the maximum output, has been extensively studied in the context of enhancing the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions provided by the softmax output, more reliable and informative prediction pipelines may be developed, that go beyond point estimates. This approach enables the exploration of uncertainty quantification, anomaly detection, and other techniques that contribute to building safer, more robust, and trustworthy autonomous systems.
% \citet{hendrycks2018baseline} proposed a softmax prediction probability baseline for error and out-of-distribution detection across various architectures and datasets. \citet{klaus2022anomaly} uses the mean parameters of softmax function prior distribution as the cluster centroids, and unlike our approach do not use k-means to generate centroids and cluster assignments.

\textbf{Softmax prediction probabilities}: The concept of using the entire set of softmax prediction probabilities, rather than solely relying on the maximum output, has been extensively studied in the context of enhancing the safety, robustness, and trustworthiness of machine learning models. By considering the complete distribution of class predictions provided by the softmax output, more reliable and informative prediction pipelines may be developed, that go beyond point estimates \cite{gal2016dropout}. This approach enables the exploration of uncertainty quantification, anomaly detection, and other techniques that contribute to building safer, more robust, and trustworthy autonomous systems.

Uncertainty quantification is a crucial aspect of reliable machine learning systems, as it allows for the estimation of confidence in the model's predictions \cite{kendall2017uncertainties}. 
%By leveraging the softmax probabilities, techniques such as Monte Carlo dropout \cite{gal2016dropout} and ensembling \cite{lakshminarayanan2017simple} can be employed to estimate the model's uncertainty. 
These methods help identify instances where the model is less confident, enabling the system to defer to human judgment or take a more conservative action in safety-critical scenarios \cite{michelmore2018evaluating}.

Moreover, the softmax probabilities can be utilized for anomaly detection, which is essential for identifying out-of-distribution (OOD) samples or novel classes that the model has not encountered during training \cite{hendrycks17baseline}. By monitoring the softmax probabilities, thresholding techniques can be applied to detect anomalies based on the distribution of the predictions \cite{liang2018enhancing}. This enables the system to flag potentially problematic inputs and take appropriate actions, such as requesting human intervention or triggering fallback mechanisms.

The use of softmax probabilities also facilitates the development of more robust models that can handle adversarial examples and other types of input perturbations \cite{goodfellow2014explaining}. Adversarial attacks aim to fool the model by crafting input samples that lead to incorrect predictions with high confidence \cite{szegedy2013intriguing}. By considering the entire softmax distribution, defensive techniques such as adversarial training \cite{madry2017towards} and input transformations \cite{guo2018countering} can be applied to improve the model's robustness against these attacks.

Furthermore, the softmax probabilities provide valuable information for interpretability and explanability of the model's decisions \cite{ribeiro2016should}. By analyzing the distribution of the predictions, insights can be gained into the model's reasoning process and the factors that contribute to its outputs. This transparency is crucial for building trust in the system and facilitating human-machine collaboration \cite{doshi2017towards}.

The importance of leveraging the entire softmax distribution extends to various domains, including autonomous vehicles \cite{michelmore2018evaluating}, medical diagnosis \cite{leibig2017leveraging}, and financial risk assessment \cite{feng2018deep}. In these safety-critical applications, the consequences of incorrect predictions can be severe, and relying solely on the maximum softmax output may not provide sufficient safeguards. By considering the full distribution of predictions, more informed and reliable decisions can be made, reducing the risk of catastrophic failures.

However, the use of softmax probabilities is not without challenges. The calibration of the model's predictions is an important consideration, as poorly calibrated models may lead to overconfident or underconfident estimates \cite{guo2017calibration}. Techniques such as temperature scaling \cite{guo2017calibration} and isotonic regression \cite{zadrozny2002transforming} can be applied to improve the calibration of the softmax probabilities, ensuring that they accurately reflect the model's uncertainty.

By considering the complete distribution of class predictions, techniques can be employed to develop more reliable and informative prediction pipelines. 

%%%%%%%%%%%%%%
% CLUSTERING %
%%%%%%%%%%%%%%

\textbf{Clustering}: Clustering algorithms are essential for discovering structures and patterns in data across various domains \cite{jain2010data, xu2015comprehensive}. K-means, a widely used algorithm, efficiently assigns data points to the nearest centroid and updates centroids iteratively \cite{lloyd1982least}. However, it requires specifying the number of clusters and is sensitive to initial centroid placement \cite{arthur2007k}. Hierarchical clustering creates a tree-like structure by merging or dividing clusters \cite{johnson1967hierarchical} but may not scale well to large datasets \cite{mullner2011modern}. Density-based algorithms, like DBSCAN, identify clusters as dense regions separated by lower density areas \cite{ester1996density, schubert2017dbscan}.

Other applications include image segmentation for object detection \cite{shi2000normalized}, anomaly detection for fraud and intrusion detection \cite{chandola2009anomaly}, customer segmentation for targeted marketing \cite{ngai2009application}, and bioinformatics for gene expression analysis and disease subtype identification \cite{eisen1998cluster, jiang2004cluster}. The choice of algorithm depends on data characteristics, desired cluster properties, and computational resources \cite{rodriguez2019clustering}.

Given the context and to the best of our knowledge, no prior work exists in using softmax distance to class centroid to put a threshold on the trust in the model's predictive accuracy in classification tasks.


