\section*{[ECAI-2024] Notification for submission \#1654}

Dear Daniel,

We regret to inform you that your submission entitled ``When to Trust Automated Predictions and When to Defer to Human Judgment?'' (\#1654) has not been accepted for publication at the 27th European Conference on Artificial Intelligence (ECAI-2024).

As for every edition of the conference, the reviewing process was thorough and highly selective. We received a total of 2,344 submissions and accepted 547 of them, corresponding to an acceptance rate of 23\%. The list of accepted papers will become available at the conference website next week.

Every submission was reviewed by at least three experts in the field. The initial reviews and your rebuttal (if you chose to submit one) were then discussed by those reviewers and a senior member of the programme committee, who summarised the outcome of the discussion in a brief metareview. In a small number of cases, additional experts were consulted during this discussion phase. Every discussion was monitored and every metareview approved by an area chair. You will find your final reviews both at the end of this message and on EasyChair. We hope that you will find the feedback received helpful.

You may want to consider submitting a revised version of your paper to one of the many exciting workshops associated with ECAI-2024. While some deadlines have passed already, several workshops have a fast-track option for papers rejected from the main conference. We will send you a separate message about this later today.

Either way, we would be delighted to see you in Santiago de Compostela this October, and for you to be part of the 50th anniversary of ECAI.

All the best,

Ulle Endriss and Francisco Melo\\
ECAI-2024 PC Chairs

Conference website: \url{https://www.ecai2024.eu/}\\
EasyChair: \url{https://easychair.org/conferences/?conf=ecai24}

Note that a copy of the review form used is available from the FAQ site.

\subsection*{SUBMISSION: 1654}
\textbf{TITLE: When to Trust Automated Predictions and When to Defer to Human Judgment?}

\subsection*{METAREVIEW}
The paper deals with the problem of estimating the confidence of a prediction in order to defer judgment to human experts, in particular in the case of distribution shifts.

While the problem is interesting, the reviewers raised a number of issues, notably the fact that the paper was not always very clear (they provided numerous examples to support this statement), and also the fact that it was unclear to which extent the experiments were specifically addressing the problem of distribution shift.

I would also add that since the classifiers is concerned with an estimation of the confidence of prediction, the paper should also be positioned with respect to the large uncertainty quantification literature and calibration methods existing around, that also try to achieve this purpose.

\subsection*{REVIEW 1}
\textbf{SUBMISSION:} 1654\\
\textbf{TITLE:} When to Trust Automated Predictions and When to Defer to Human Judgment?\\
\textbf{AUTHORS:} Daniel Sikar, Artur Garcez, Tillman Weyde, Robin Bloomfield and Kaleem Peeroo

\subsubsection*{Summary}
The paper presents a new method for assessing the reliability of neural network predictions under distribution shifts. The approach involves: clustering outputs, confidence metrics and definition of a safety threshold for predictions. The method is tested on the MNIST and CIFAR-10 datasets using different neural network models (Convolutional Neural Network and Vision Transformer). The results indicate that this metric is consistent across datasets and models, suggesting it can efficiently determine when automated decisions are reliable and when human intervention is necessary. The approach is also being evaluated in the CARLA simulator for self-driving scenarios under noisy conditions.

\subsubsection*{Strengths}
\begin{itemize}
    \item The paper is well structured and clear
    \item The assessment method proposed is presented in details, the algorithmic representations are also provided
    \item The approach is tested by using validated state of the art datasets
\end{itemize}

\subsubsection*{Weaknesses}
\begin{itemize}
    \item Figures are too small, by zooming in the digital version the details are clearly displayed, but not visible at all in the printed version. It would be useful providing readers with additional materials in which the figures are at the proper size.
\end{itemize}

\subsubsection*{Detailed comments}
The paper introduces a straightforward method to improve accuracy thresholding in image classification. The authors trained two different neural network models on two well-known datasets, demonstrating consistent results. They proposed a threshold to determine when predictions can be accepted. They also highlighted the need for human judgment since the threshold values are domain-specific and need experts intervention to balance automated decision.

The method is applied to self-driving car safety using the CARLA simulator, enhancing system safety by flagging scenarios where human judgment is needed, especially in out-of-distribution (OOD) situations. Additionally, the study explores using the softmax output as a training dataset for a multi-layer perceptron binary classifier, comparing this with their softmax distance approach and a regressor network to study optimal threshold values. The paper is well organized and in line with the conference topics.

\subsubsection*{Score}
SCORE: 4 (poor (fair attempt but too many concerns, so probably should be rejected))

\subsection*{REVIEW 2}
\textbf{SUBMISSION:} 1654\\
\textbf{TITLE:} When to Trust Automated Predictions and When to Defer to Human Judgment?\\
\textbf{AUTHORS:} Daniel Sikar, Artur Garcez, Tillman Weyde, Robin Bloomfield and Kaleem Peeroo

\subsubsection*{Summary}
The method proposed provides a way to estimate the confidence a model has in its classification results, in the shape of a softmax distance from a pre-computed cluster centroid. If the distance is above a threshold, the task is asked to a human to classify himself the sample. MNIST and CIFAR-10 have been used for this purpose, respectively with a simple MLP and a pre-trained ViT. The authors claim to measure the distribution shifts in the data, a problem with large impacts in the real-world settings (like under noisy weather conditions for self-driving cars).

\subsection*{Strengths}
\begin{itemize}
    \item The problem to address is clear, and is indeed related to real-world business concerns. It deserves to be treated as it is proposed here.
    \item Clustering classifier outputs and measure the distance between the inferred output and the cluster centroid is a good idea.
    \item Numerous evidences are provided, from the confusion matrices to the detailed distances distribution for all the classes.
\end{itemize}

\subsubsection*{Weaknesses}
\begin{itemize}
    \item It is not obvious to claim that the train and test sets of the mnist or cifar-10 come with distribution shifts. Both sets come from the same overall distribution, so the argument that the approach covers the distribution shift does not hold.
    \item Also, there is no comparison with competitor methods (as KL or MMD introduced in the 1st section), thus no way to understand the added value of the method.
    \item Datasets and models are not really representative of real-world settings, thus it is difficult to ensure that an autonomous safety system, like self-driving decision-making, can raise good results with this approach.
    \item It would be fine to see the clustering plots, in order to have a visual of the centroid positions, the spread in the clusters, the overlaps etc ...
    \item Sometimes difficult to follow (long sentences, repetitions, confusion, ...).
\end{itemize}

\subsubsection*{Detailed comments}
[Detailed comments omitted for brevity]

\subsubsection*{Questions for rebuttal}
\begin{enumerate}
    \item Why do you consider distribution shifts in the train and test set of MNIST or CIFAR-10?
    \item How does your method perform against other competitor ones?
    \item Why did you choose a CNN and a pre-trained ViT as classifiers (is there an explanation)?
\end{enumerate}

\subsubsection*{Score}
SCORE: 4 (poor (fair attempt but too many concerns, so probably should be rejected))

\subsubsection*{Comments added after rebuttal}
I'm not convinced by the author answers, who don't really explain neither the distribution shifts in the datasets, nor the comparison with the competitors. Therefore, no score change from my side.

\section*{REVIEW 3}
\textbf{SUBMISSION:} 1654\\
\textbf{TITLE:} When to Trust Automated Predictions and When to Defer to Human Judgment?\\
\textbf{AUTHORS:} Daniel Sikar, Artur Garcez, Tillman Weyde, Robin Bloomfield and Kaleem Peeroo

\subsubsection*{Summary}
The paper proposes a softmax method to quantify the reliability of CNN predictions.

\subsubsection*{Strengths}
\begin{itemize}
    \item Overall interesting topic
    \item 2 datasets
\end{itemize}

\subsubsection*{Weaknesses}
\begin{itemize}
    \item Lots of unclarity throughout the manuscript
    \item Figures (with important content) are not readable
    \item The introduction and conclusion do not well connect with the main part of the paper (non-stringent story)
\end{itemize}

\subsubsection*{Detailed comments}
[Detailed comments omitted for brevity]

\subsubsection*{Score}
SCORE: 2 (insufficient (arguably was not yet ready for submission))

\section{[ECAI-2024] Review Transfer Option for ECAI Satellite Workshops (deadline:  11 July 2024)}

Dear Daniel,

You are receiving this message as an author of a submission to ECAI-2024 that did not make it into the programme of the main conference.

ECAI-2024 will have a rich programme of satellite workshops and several of these workshops are offering a fast-track option for papers rejected from the main track. We can transfer your reviews to the organisers of one of the participating workshops, which will allow them to make a very fast decision.

If you are interested, please visit this page for full information:

https://bit.ly/ecai24transfer

The deadline for requesting the transfer of your reviews is next Thursday, 11 July 2024.

Make sure you also check the website of the workshop of your choice to find out how and by when to submit your paper. Please direct any questions to the organisers of your chosen workshop.

All the best,

Ulle Endriss and Francisco Melo
ECAI-2024 PC Chairs

\subsection{Workshop themes}

\begin{itemize}
\item AAAPEI: Adjustable Autonomy and and Physical Embodied Intelligence
\item ActSynt: Highlights of Reasoning about Actions, Planning and Reactive Synthesis
\item AIEB: Implementing AI Ethics through a Behavioural Lens
\item AIEER: International Workshop on AI in Education and Educational Research
\item AISUCMA: Workshop on Artificial Intelligence in Supply Chain Management
\item AREA: 4th Workshop on Agents and Robots for reliable Engineered Autonomy
\item ATT: 13th International Workshop on Agents in Traffic and Transportation
\item CLD2: Workshop on Classifier Learning From Difficult Data
\item CODAI: COuntering Disinformation with AI
\item CompAI: Workshop on Composite AI
\item CREAI: Workshop on Artificial Intelligence and Creativity
\item DAFUSAI: 2nd Workshop on Data Fusion for Artificial Intelligence
\item DAO-XAI: Data Meets Applied Ontologies in Explainable AI
\item EXPLIMED: Explainable Artificial Intelligence for the Medical Domain
\item FCA4AI: 12th Workshop on "What can FCA do for Artificial Intelligence?"
\item HAII5: Embracing Human-Aware AI in Industry 5.0
\item HYDRA: 3rd International Workshop on HYbrid Models for Coupling Deductive and Inductive ReAsoning
\item IMIS: Workshop on Intelligent Management Information Systems
\item LUHME: Language Understanding in the Human Machine Era
\item M-PREF: 15th Multidisciplinary Workshop on Advances in Preference Handling
\item MAI-XAI: Multimodal, Affective and Interactive eXplainable AI
\item ML-DE: Machine Learning Meets Differential Equations
\item PMAI: 3rd International Workshop on Process Management in the AI era
\item SC4AI: Social Choice for AI Ethics and Safety
\item VECOMP: International Workshop on AI Value Engineering and AI Compliance Mechanisms
\item WSCL: Workshop on Weakly Supervised and Cautious Learning: Bridging Machine Learning and Uncertainty Management
\item XRIA: First Workshop on "eXtended Reality \& Intelligent Agents"
\end{itemize}
