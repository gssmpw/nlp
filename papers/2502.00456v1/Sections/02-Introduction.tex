%%%%%%%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%%%%%%%

\section{Introduction}

Ensuring the reliability and safety of automated decision-making systems is important, particularly in high-impact areas where there exists potential for significant harm from errors \cite{amodei2016concrete}. 
Machine learning (ML) models, while powerful, are susceptible to making erroneous predictions when faced with data that differs from the distribution that they were trained on \cite{hendrycks2021many}. 
This phenomenon, known as distribution shift, poses a significant challenge in deploying ML in real-world scenarios \cite{quinonero2009dataset}.

Distribution shift is a pervasive issue in ML, occurring when the distribution of the data used to train a model differs from the distribution of the data that the model encounters during deployment \cite{quinonero2009dataset}. This discrepancy can lead to a significant degradation in model performance, as it may struggle to generalize to the new, unseen data \cite{hendrycks2019benchmarking}. 

Distribution shifts can manifest itself in various forms, such as covariate shift, concept drift, and domain shift \cite{moreno2012unifying}. Covariate shift arises when the input data distribution changes while the conditional distribution of the output given the input remains the same \cite{shimodaira2000improving}. Concept drift occurs when the relationship between the input and output variables changes over time \cite{gama2014survey}. Domain shift refers to the situation where the model is trained on data from one domain but applied to data from a different domain \cite{patel2015visual}.

To quantify and address distribution shift, researchers have developed various metrics and techniques. One common approach is to use statistical divergence measures, such as Kullback-Leibler (KL) divergence \cite{kullback1951information} or Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel}, to assess differences in training and test data distributions. These metrics provide a quantitative understanding of the extent of the distribution shift.

Another approach is to employ domain adaptation techniques, which aim to align the feature distributions of the source and target domains \cite{wang2018deep}. This can be achieved through methods such as importance weighting \cite{sugiyama2007covariate}, feature transformation \cite{pan2009survey}, or adversarial learning \cite{ganin2016domain}. These techniques seek to mitigate the impact of distribution shift by making the model more robust to changes in the data distribution.

Recent work has also focused on developing algorithms that can detect and adapt to distribution shift in real-time \cite{lu2018learning}. These methods often rely on monitoring the model's performance on a stream of data and adjusting the model's parameters or architecture when a significant drop in performance is detected \cite{baena2006early}. Such adaptive approaches are particularly relevant in dynamic environments where the data distribution is likely to change over time.

Therefore, distribution shift is a significant challenge  that can lead to poor model performance if not addressed properly. %Researchers have proposed various metrics and techniques to quantify and mitigate the impact of distribution shift, ranging from statistical divergence measures to domain adaptation and adaptive algorithms. 
As ML models are increasingly deployed in real-world applications, developing robust methods to handle distribution shift remains an important area of research.

%Leaving aside domain adaptation, we can see that there are approaches based on measuring the shift and approaches based on measuring the drop in accuracy. In this paper we seek to relate/combine the two? Use both to find a threshold... we need to say what the benefit is here.

%Maybe we can say this: neural network predictions, different from other (Bayesian) ML approaches do not offer a measure of confidence in the predictions. Confidence in the prediction is key to deal with the problem of distribution shift. It has been argued recently that \textit{knowing when the system does not know} should be a main metric for ML \cite{DBLP:conf/uai/VentolaB0MK23}. The relationships established in this paper between metric-based and model performance approaches to  distribution shift seek to offer such a connection between accuracy and confidence values. A trained network that has minimized entropy in a softmax layer offers a window into the link between accuracy and confidence values in that a lower entropy should indicate a higher class discrimination (a \textit{pointy} softmax distribution) and a higher entropy should correspond to a softmax distribution that is closer to the uniform distribution. By investigating how the softmax distribution changes with different forms of distribution shift (different types of noise systematically added to the data) we are able to identify trends in the drop of accuracy over time in comparison with the usual distribution shift metrics such as KL divergence. As a result of the above analysis of accuracy and confidence, we are able to define a confidence threshold as the distance from class centroids (explained next) to the first false positive example in the test set. Beyond this threshold, predictions should be deferred to human judgement. In practice, the threshold may vary from an application to another depending on the criticality of the task and the risk of making a wrong decision. 

In this paper, we propose a novel approach for quantifying the reliability of predictions made by neural networks under distribution shift. Our method leverages clustering techniques to measure the distances between the outputs of a trained neural network and class centroids in the softmax distance space. By analyzing these distances, we develop a metric that provides insight into the confidence that one may attribute to the model's predictions. We adopt the most conservative threshold value at which model predictions are expected to be 100\% accurate. The proposed metric offers a computationally efficient, practical way to determine when to accept an automated prediction and when human intervention may be necessary.
%, though in some time-critical scenarios, asking a human for information may not be an option \cite{amodei2016concrete}.

Furthermore, we explore the relationship between the distance to class centroids and the model's predictive accuracy. Our findings confirm as expected that classes predicted more accurately by the model tend to have lower softmax distances to their respective centroids. This observation highlights the potential for using the changes observed in the softmax distribution as a proxy for our confidence in the model's predictions. The objective is to establish a closer link than thus far identified by the literature between the distance-based metrics proposed for distribution shift and the approaches that measure the drop in model accuracy. It is expected to offer a better understanding of the connection between model performance and reliability, that is, accuracy and how confident we are in the predictions of neural networks. 

%NEED TO SUMMARISE THE RESULTS HERE: SAY THAT CNN AND VIT WERE USED, THE APPROACH IS EFFICIENT / LIGHTWEIGHT / SCALES WELL AND THE RESULTS ARE CONSISTENT FOR CNN AND VIT... 

%We can't claim this yet:
%The implications of our work extend beyond the realm of image classification. We are currently applying these concepts to enhance the safety of autonomous systems, specifically in the context of self-driving cars using the CARLA simulator. By identifying scenarios where human judgment is preferable to the autonomous system's assessment, our methodology aims to improve the overall safety and reliability of decision-making in complex, real-world environments.

The main contributions of this paper are:
\begin{itemize}
\item A new lightweight approach for quantifying the reliability of neural networks using distance metrics and clustering to take full advantage of the learned softmax distributions. The method requires minimal computational overhead, using only the existing softmax outputs and a set of centroids and thresholds, without additional network modifications or training.
\item A combined analysis of the metric-based and accuracy-based methods to tackle distribution shift, with experimental results showing consistency of the proposed approach across CNN and ViT architectures.
\end{itemize}

% You can make this disclaimer shorter maybe as a footnote, because the paper is more about the accuracy versus confidence argument than real systems. 

% \subsection{Disclaimer}

% While identifying and mitigating safety risks within the classification model itself is crucial, it does not guarantee the overall safety, robustness, reliability, trustworthiness, and confident performance of the entire system in which the model is deployed.

% In critical applications, the classification model is often just one component of a larger, complex system. The safe and reliable operation of such a system depends on the proper functioning and interaction of all its components, including data acquisition, preprocessing, decision-making, and actuation. Even if the classification model is designed to mitigate safety risks, failures or vulnerabilities in other components can still compromise the overall safety and reliability of the system.

% Furthermore, the deployment environment and context in which the classification system operates can introduce additional challenges and risks that may not be fully captured or addressed during the model development phase. Factors such as data distribution shifts, adversarial attacks, or unexpected edge cases can impact the model's performance and lead to potential safety issues.

% Therefore, it is important to recognize that developing a safe and reliable classification model is necessary but not sufficient for ensuring the overall safety and reliability of the deployed system. A holistic approach that considers the entire system architecture, interfaces, and deployment context is essential. This includes rigorous testing, validation, and monitoring of the entire system, as well as establishing robust failsafe mechanisms and human oversight to handle unexpected situations and maintain safe operation.

% In summary, while identifying and mitigating safety risks within the classification model is important, it is crucial to acknowledge that it does not guarantee the safe and reliable performance of the entire deployed system. A comprehensive approach considering all components, their interactions, and the deployment context is necessary to ensure the safe, robust, reliable, trustworthy, and confident operation of critical applications.


