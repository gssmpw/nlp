\clearpage
\section{Artur's feedback}

Hi Daniel,

I've read the paper now and I wanted to give you some feedback because it is important to continue to develop this paper before starting on a new one.

Ideally, we'd have a meeting but the main point I wanted to make is that the relationship between the clustering and the softmax is rather dry without a connection with the earlier distance functions and systematic addition of noise.

In my mind, \textbf{the main idea is that given a trained network and some data, adding noise to the data and checking the output will produce changes in the softmax that we're trying to measure}. Clustering can show us when such changes start to overlap with the noise added to a different class. These distances in the clustering can be visualized but also measured in different ways in a high-dimensional space. We need to find a connection between this and the KL divergence and other metrics from your transfer report.   

We can claim that we've seen the same pattern in the MNIST and CIFAR cases but I have a feeling that the reviewers will find this insufficient. However, rather than more experiments (the reviewers may find any number of experiments insufficient) if we can analyse results w.r.t. KL and other metrics then we should be ok.  

As discussed, a CARLA example will add a lot of value: Increasing "rain" noise will produce a similar softmax profile. Increasing "glare" or some other noise would offer a visualization of the clusters. But only the above distance metrics (KL, etc) will offer results without information loss from reducing dimensionality.

The good thing about having a simulator is that you can now test the noisy cases for "crash" or "no-crash" in addition to obtaining the softmax from the network. So with the simulator you now have the softmax results but also something we can call a "ground-truth proxy". With this proxy we should be able to analyse the value of adopting each of the above distances. Any choice of threshold can be compared now with the crash/no-crash results.       

I'm not sure if ECAI will have a rebuttal period. If it does then anything you do between now and the rebuttal period will be useful to help increase chances of getting accepted. In any case, I think the above gives a roadmap for your PhD thesis. Let me know what you think.

Best,
Artur

\section{Adding noise to the data and checking the output will produce changes in the softmax that we're trying to measure}
We demonstrate how adding noise to images degrades accuracy, and how accuracy degradation, like the general case for incorrectly classfied images, is negatively correlated to increase is the softmax distance of the class prediction to the class centroid.
We create the PerMNIST (perturbed MNIST) dataset, and the NCIFAR-10 (the noisy CIFAR-10 dataset).

\subsection{The PerMNIST dataset}
We apply 12 different types of perturbations (Brightness, Contrast, Defocus Blur, Fog,
Frost, Gaussian Noise, Impulse Noise, Motion Blur, Pixelation, Shot Noise, Snow and Zoom Blur) at 10 intensity levels (1 to 10), create 120 additional images for every image in the original training and testing datasets, resulting in 7,260,000 and 1,260,000 images respectively. We keep the original images in the perturbed image datasets, plus add a file with the perturbation type and intensity level.



\lipsum[2]

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Perturbations_digit_5_10x12.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 where columns are from left to right perturbation intensity levels 1 to 10 and rows are perturbation types Brightness, Contrast, Defocus Blur, Fog, Frost, Gaussian Noise, Impulse Noise, Motion Blur, Pixelation, Shot Noise, Snow and Zoom Blur.}
    \label{fig:Perturbations_digit_5_10x12}
\end{figure*}

% image generated with function call display_image_with_histogram(image_array)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/Digit_5_zoom_blur_level_1.png}
    \caption{Digit 5 Zoom Blur perturbation at intensity 1 - bottom left in Figure \ref{fig:Perturbations_digit_5_10x12}}
    \label{fig:Digit_5_zoom_blur_level_1.png}
\end{figure}

% image generated with function call display_image_with_histogram(image_array)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/Digit_5_zoom_blur_level_10.png}
    \caption{Digit 5 Zoom Blur perturbation at intensity 1 - bottom right in Figure \ref{fig:Perturbations_digit_5_10x12}}
    \label{fig:Digit_5_zoom_blur_level_10.png}
\end{figure}

% generated with function call plot_images_and_histograms(perturbations[0:10])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Brightness_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Brightness applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Brightness_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[10:20])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Contrast_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Contrast applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Contrast_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[20:30])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/defocus_blur_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Defocus Blur applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:defocus_blur_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[30:40])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/fog_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Fog applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:fog_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[40:50])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/frost_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Frost applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:frost_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[50:60])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Gaussian_noise_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Gaussian Noise applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Gaussian_noise_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[60:70])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/impulse_noise_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Impulse Noise applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:impulse_noise_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[70:80])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Motion_blur_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Motion Blur applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Motion_blur_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[80:90])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Pixelation_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Pixelation applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Pixelation_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[90:100])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Shot_noise_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Shot Noise applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Shot_noise_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[100:110])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Snow_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Snow applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Snow_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call plot_images_and_histograms(perturbations[110:120])
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Zoom_blur_Digit_5_images_histogramsx10.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 5 with 10 levels of Zoom Blur applied. The second row shows image the corresponding histograms for the images above.}
    \label{fig:Zoom_blur_Digit_5_images_histogramsx10}
\end{figure*}

% generated with function call visualize_accuracy_heatmap(accuracy_matrix, perturbation_types, intensities)
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/PerturbedMNISTAccuracyHeatmap.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Perturbed MNIST Accuracy Heatmap, where perturbation intensity level increases left to right.}
    \label{fig:PerturbedMNISTAccuracyHeatmap}
\end{figure*}

% generated with function call 
% plot_perturbations(perturbed_imgs_list)
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/plot_perturbations_7.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST digit 7 where columns are from left to right perturbation intensity levels 1 to 10 and rows are perturbation types Brightness, Contrast, Defocus Blur, Fog, Frost, Gaussian Noise, Impulse Noise, Motion Blur, Pixelation, Shot Noise, Snow and Zoom Blur.}
    \label{fig:plot_perturbations_7}
\end{figure*}

% generated with function calls
% intensities = np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
%euclidean_matrix, bhattacharya_matrix, histogram_intersection_matrix, kl_divergence_matrix = average_distances_by_perturbation(test_np_with_distances)
%visualize_average_distances(euclidean_matrix, bhattacharya_matrix, histogram_intersection_matrix, kl_divergence_matrix, intensities)
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/visualize_average_distances_mnist_perturbed_testing_dataset.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Distances}
    \label{fig:visualize_average_distances_mnist_perturbed_testing_dataset}
\end{figure*}

% Question
% After clustering, how many of the correctly classified classes have been assigned to incorrect clusters ?

% Answer 2

\subsection{Images assigned to incorrect centroids}

Using the trained CNN model, and the 60k MNIST training dataset, we take the 59,074 correct predictions, compute the average softmax grouped by digit class, then run the K-Means algorithm to determine the 10-dimensional class centroids. Two images are assigned to the incorrect clusters, there are the image index 8688 - this is a number 6, correctly classified as number six then  assigned to cluster 5, and image index 22561 - a number 6, correctly classified then assigned to cluster 8.

% plot_image(img, f'Image id {img_id} predicted {img_prediction} assigned to cluster {cluster_label}')
% 'Image id 8688 predicted 6 assigned to cluster 5'
% 'Image id 22561 predicted 6 assigned to cluster 8'
% Then joined images manually

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{Figures/PerMNIST_MisClustered_images_8688_22561.png}
    \caption{The two "misclustered images in the correctly classified MNIST training dataset. Both are labelled as digit six. The example on the left is assigned to cluster 5 while the example on the right is assigned to cluster 8}
    \label{fig:PerMNIST_MisClustered_images_8688_22561}
\end{figure}

We are interested in studying what was the prediction i.e., the softmax output for both images in Figure \ref{fig:PerMNIST_MisClustered_images_8688_22561}, as well as the distance to every cluster centroid. Figure \ref{fig:8688_22561_Softmax_Output} shows that image index 8688 has a Softmax output with high probability scores for both images 5 and 6, which is marginally higher.

% Tables generated with function call:
% arr = train_np[8688 ][0:10] 
% title = "Delta from Maximum Prediction Value Image Index 8688"
% caption = "Table showing the distances of each digit from the maximum value."
% latex_output = format_array_latex_table(arr, title, caption)
% print(latex_output) 
\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Delta for Image Index 8688}} \\
    \hline
    Class & Delta \\
    \hline
    0 & $-4.84 \times 10^{-1}$ \\
    1 & $-4.95 \times 10^{-1}$ \\
    2 & $-4.95 \times 10^{-1}$ \\
    3 & $-4.95 \times 10^{-1}$ \\
    4 & $-4.95 \times 10^{-1}$ \\
    5 & $-4.09 \times 10^{-3}$ \\
    6 & $0$ \\
    7 & $-4.95 \times 10^{-1}$ \\
    8 & $-4.94 \times 10^{-1}$ \\
    9 & $-4.93 \times 10^{-1}$ \\
    \hline
  \end{tabular}
  \caption{Table showing the distances of each digit from the maximum value (digit 6) The table shows that digit 5 is the smallest delta, that is the model has some the degree of confidence that the prediction may be 5.}
  \label{tab:deltas_8688}
\end{table}

% arr = train_np[22561][0:10]
% title = "Delta from Maximum Prediction Value Image Index 22561"
% caption = "Table showing the distances of each digit from the maximum value."
% latex_output = format_array_latex_table(arr, title, caption)
% print(latex_output) 
\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Deltas for Image Index 22561}} \\
    \hline
    Class & Delta \\
    \hline
    0 & $-7.99 \times 10^{-2}$ \\
    1 & $-1.35 \times 10^{-1}$ \\
    2 & $-2.97 \times 10^{-1}$ \\
    3 & $-2.91 \times 10^{-1}$ \\
    4 & $-2.96 \times 10^{-1}$ \\
    5 & $-2.63 \times 10^{-1}$ \\
    6 & $0$ \\
    7 & $-2.96 \times 10^{-1}$ \\
    8 & $-1.39 \times 10^{-2}$ \\
    9 & $-2.97 \times 10^{-1}$ \\
    \hline
  \end{tabular}
  \caption{Table showing the distances of each digit from the maximum value (digit 6) The table shows that digit 8 is the smallest delta, while deltas for digits 0 and 1 are also small, that is the model has some the degree of confidence that the prediction may be 0, 1 or 8, in addition to 6.}
  \label{tab:distances}
\end{table}

% Generated with function calls:
% softmax_output = train_np[8688][0:10]
% title = 'Softmax output for image index 8688'
% plot_softmax_output(softmax_output, title)
% softmax_output = train_np[22561][0:10]
% title = 'Softmax output for image index 8688'
% plot_softmax_output(softmax_output, title)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{Figures/8688_22561_Softmax_Output.png}
    \caption{The Softmax output for the two "misclustered images in the correctly classified MNIST training dataset. Both are labelled as digit six. The example on the left is assigned to cluster 5 while the example on the right is assigned to cluster 8}
    \label{fig:8688_22561_Softmax_Output}
\end{figure}

% Clustering
% Repo: https://github.com/dsikar/work-in-progress.git
% retrieve data
% train_np = np.load('train_np.npy')
% train_np_with_indices = add_row_index(train_np)
% correct_preds = train_np_with_indices[train_np[:, 10] == train_np_with_indices[:, 11]]


% clustering_mismatches = find_clustering_mismatches(cluster_labels, correct_preds)

% Code to retrieve image
% Repo: https://github.com/dsikar/pmnist.git
% filename = '/home/daniel/git/work-in-progress/scripts/data/MNIST/raw/train-images-idx3-ubyte'
% index = 8688
% display_mnist_img(filename, index, verbose = False) 
% index = 22561
% display_mnist_img(filename, index, verbose = False)


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\columnwidth]{Figures/distances_to_centroids_8688_22561.png}
    \caption{The Softmax prediction distance to centroids for MNIST training images indexes 8688 (left) and 22561 (right), where both images are digit class 6, and the left image is misclassified as digit class 5, while the right image is misclassified as digit class 8.}
    \label{fig:distances_to_centroids_8688_22561} 
\end{figure}

Figure \ref{fig:distances_to_centroids_8688_22561} shows Softmax distances to digit class centroids, the deltas are shown in tables \ref{tab:Softmax_distances_to_centroids_8688}

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Centroid Deltas 8688}} \\
    \hline
    Class & Delta \\
    \hline
    0 & 0.5050 \\
    1 & 0.5151 \\
    2 & 0.5064 \\
    3 & 0.5058 \\
    4 & 0.5096 \\
    5 & \textbf{0.0000} \\
    6 & \textbf{0.0010} \\
    7 & 0.5095 \\
    8 & 0.4897 \\
    9 & 0.5028 \\
    \hline
  \end{tabular}
  \caption{Predicted class is 6, nearest class centroid is 5}
  \label{tab:distances_from_min}
\end{table}

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Centroid Deltas 22561}} \\
    \hline
    Class & Delta \\
    \hline
    0 & 0.1019 \\
    1 & 0.1612 \\
    2 & 0.3063 \\
    3 & 0.3040 \\
    4 & 0.3108 \\
    5 & 0.2795 \\
    6 & \textbf{0.0090} \\
    7 & 0.3107 \\
    8 & \textbf{0.0000} \\
    9 & 0.3056 \\
    \hline
  \end{tabular}
  \caption{Predicted class is 6, nearest class centroid is 8}
  \label{tab:distances_from_min}
\end{table}

% STOPPED HERE, NEXT
% 1. SHOW a case where softmax is close to centroid
% 1.5 Write function to find closest softmax to class centroid for correctly predicted class, maybe also for closest softmax to centroid for incorrectly predicted class.
% 2. Plot  bar charts for testing dataset, y axis has dgits, x axis has noise levels, cells are average distance to softmax for all classifications.
% 3. Same as 2. But maybe just for correct and incorrect

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/digit_6_mnist_train_35537_image_softmax_centroids.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{MNIST training dataset image index 35537 (6) is plotted on the left. In the middle is the softmax output from the classifier for the same image, where digit 6 has the highest score, and on the right are the distances to all class centroids for the softmax output of the classifier, where 6 is the nearest to centroid.}
    \label{fig:digit_6_mnist_train_35537_image_softmax_centroids}
\end{figure*}

Figure \ref{fig:digit_6_mnist_train_35537_image_softmax_centroids} shows the image nearest to its class centroid. The image is labelled as digit 6. The plots show a marked difference from the misclustered images represented in Figures \ref{fig:distances_to_centroids_8688_22561} and \label{fig:8688_22561_Softmax_Output}, where the prediction is distinct outstanding


Figure \ref{fig:distances_to_centroids_8688_22561}  showcases two examples (image indexes 8688 and 22561) where the classifier correctly predicts the digit class as 6, yet the softmax outputs reveal notable probabilities assigned to other classes (5 and 8, respectively). These instances highlight the potential for underlying uncertainty or confusion with visually similar classes, even when the classifier's ultimate prediction is accurate. Such cases underscore the necessity of comprehending the classifier's confidence beyond mere prediction correctness.
Figure \ref{fig:distances_to_centroids_8688_22561}  reinforces this observation by presenting the distances between the softmax outputs and class centroids. For image indexes 8688 and 22561, the softmax outputs are closer to the centroids of classes 5 and 8, respectively, rather than the true class 6. These edge cases illustrate the potential misalignment between the classifier's output and the true class, raising concerns about the reliability and interpretability of the classifier's decisions. This is particularly crucial in safety-critical applications where trust is of utmost importance.
In contrast, Figure \ref{fig:digit_6_mnist_train_35537_image_softmax_centroids} presents an example (image index 35537) where the softmax output and distance to centroids strongly align with the true class 6. This case demonstrates the classifier's ability to make confident and reliable predictions when the input closely matches the learned patterns of the correct class.

\textbf{Trust}:
The presented results highlight the importance of examining edge cases to establish trust in a classifier's decisions. While the classifier correctly predicts the digit class in the given examples, the softmax outputs reveal potential uncertainty or confusion with visually similar classes. This raises concerns about the trustworthiness of the classifier's predictions, particularly in critical applications. To foster trust, it is crucial to develop robust evaluation and interpretation techniques that go beyond mere accuracy and consider the nuances of the classifier's decision-making process. By addressing these challenges, we can build classification systems that inspire trust and confidence in their decisions.

\textbf{Reliability}:
The analysis of the softmax outputs and distances to class centroids reveals potential issues with the reliability of the classifier's decisions. In edge cases, such as image indexes 8688 and 22561, the classifier's output aligns more closely with incorrect classes, despite making the correct prediction. This misalignment raises concerns about the reliability of the classifier's decisions, especially in situations where consistent and dependable performance is required. Addressing these reliability challenges is crucial for developing classification systems that can be trusted to perform accurately and consistently across a wide range of inputs and scenarios.

\textbf{Confidence}:
The presented results demonstrate the importance of assessing the confidence of a classifier's decisions. While the classifier correctly predicts the digit class in the given examples, the softmax outputs reveal varying levels of confidence. In edge cases, the classifier assigns significant probabilities to incorrect classes, indicating underlying uncertainty. On the other hand, example 35537 shows a case where the classifier's confidence aligns strongly with the correct class. Evaluating the confidence of a classifier's decisions is essential for understanding its limitations and determining when to trust its predictions. By developing techniques to measure and interpret confidence, we can build classification systems that provide reliable and informative outputs.

\textbf{Safety}:
The analysis highlights the importance of considering safety when deploying classifiers in critical applications. The presence of edge cases, where the classifier's output aligns more closely with incorrect classes, raises concerns about the potential for misclassifications that could lead to unsafe outcomes. In safety-critical domains, such as healthcare or autonomous vehicles, the consequences of misclassifications can be severe. To ensure safety, it is essential to thoroughly evaluate classifiers, paying special attention to edge cases and potential sources of uncertainty. By identifying and mitigating safety risks, we can develop classification systems that can be reliably deployed in critical applications without compromising safety.


The background colour picking equation:

Color picking equation:
\begin{equation}
\vec{C}(a) = 
\begin{cases}
(255, 200, 200) & \text{if } a < \xi \\
(\beta, \gamma, \delta) & \text{if } \xi \leq a < \zeta \\
(200, 255, 255) & \text{if } a \geq \zeta
\end{cases}
\end{equation}

where:
- $\vec{C}(a)$ represents the color vector for a given accuracy value $a$.
- $\xi$ is the lower accuracy threshold.
- $\zeta$ is the upper accuracy threshold.
- $(\beta, \gamma, \delta)$ is the intermediate color calculated based on the accuracy value $a$, interpolated between the lower and upper color thresholds.

Colour picking algorithm:

\begin{algorithm}
\caption{Color Picking Algorithm}
\begin{algorithmic}[1]
\Procedure{PickColor}{$a$, $\xi$, $\zeta$}
    \If{$a < \xi$}
        \State \textbf{return} (255, 200, 200)
    \ElsIf{$a \geq \zeta$}
        \State \textbf{return} (200, 255, 255)
    \Else
        \State $t \gets \frac{a - \xi}{\zeta - \xi}$
        \State $\beta \gets \lfloor (1 - t) \cdot 255 + t \cdot 200 \rfloor$
        \State $\gamma \gets \lfloor (1 - t) \cdot 200 + t \cdot 255 \rfloor$
        \State $\delta \gets \lfloor (1 - t) \cdot 200 + t \cdot 255 \rfloor$
        \State \textbf{return} $(\beta, \gamma, \delta)$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Notes:

1. If the accuracy value $a$ is less than the lower threshold $\xi$, return the colour (255, 200, 200), representing low accuracy.
2. If the accuracy value $a$ is greater than or equal to the upper threshold $\zeta$, return the colour (200, 255, 255), representing high accuracy.
3. If the accuracy value $a$ falls between the lower threshold $\xi$ and the upper threshold $\zeta$:
   - Calculate the interpolation factor $t$ based on the position of $a$ between $\xi$ and $\zeta$.
   - Calculate the intermediate colour $(\beta, \gamma, \delta)$ by interpolating between the lower colour (255, 200, 200) and the upper colour (200, 255, 255) using the interpolation factor $t$.
   - Return the intermediate color $(\beta, \gamma, \delta)$.

The colour picking algorithm takes the accuracy value $a$, the lower threshold $\xi$, and the upper threshold $\zeta$ as inputs and returns the corresponding color based on the accuracy value.

Note: The $\lfloor \cdot \rfloor$ notation represents the floor function, which rounds down the result to the nearest integer.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/softmax_mosaic.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Distance to centroid bar chart matrix}
    \label{fig:softmax_mosaic}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/softmax_mosaic_barchart_heatmap.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Distance to centroid bar chart matrix with heatmap}
    \label{fig:softmax_mosaic_barchart_heatmap}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/softmax_mosaic_class_y_level_x.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{Distance to centroid bar chart matrix with heatmap, where digit is on y axis and level is on x axis.}
    \label{fig:softmax_mosaic_class_y_level_x}
\end{figure*}

\subsection{Perturbed data and distances to centroids}
Given a cluster with ten class centroids, obtained from the average softmax output, from all correctly classified images in the MNIST training dataset, grouped by digit class, we are interested in using the distance to class centroid to quantify trust, reliability and safety. We examined the scenario where, having obtained the greatest distance to centroid of all correctly classified MNIST training images which we will call $t_c$ for the threshold of correctly classified images, grouped by digit class, we looked at distance to class centroid from all misclassified images in the testing dataset, which we will call $t_i$ for threshold of incorrectly classified images, to see how many correctly classified image have distance to centroid greater than $t_i$. We found that in some cases the were none, and some digits such as 8, that is oftern misclassifed for 3, 5 and 6, had a considerable number of correctly classified images where the distance to class centroid was greater than $t_i$. For the sake of trusting our model, we decided to not trust the prediction in such cases, leaving the decision to a human.
We now add noise to the data, and the expectation is that, as the confidence of the model decreases, the distance to the centroid will increase and incrementally a greater number of previously correct predictions, will move further and further from the class centroid, until 1. the prediction will no longer be trusted or 2. the prediction will change, and another class rather than determined in the original prediction, will be predicted. This creates an issue in that given enough noise, the softmax output may become close enough to a different cluster, such that the incorrect prediction, being below a $t_i$ for a different class, will be deemed trustworthy.
We will examine such cases.
First, we determine the threshold for the original MNIST training dataset. We examine the set of all incorrectly predicted classes and find the nearest distance to the predicted class centroid, i.e., digit 4 was incorrectly classified as digit 5, we are make a note of such a distance all all alike. We group by incorrectly predicted class and get the nearest distance to centroid of the incorrectly predicted class. This value is $t_i$.
We are also interested in how far the correct predictions are moving away from the centroids 


\subsection{Analyzing Misclassification Likelihoods in Digit Recognition using Centroid Distances
}

Title: 
Abstract:
In this study, we present a method for analyzing the likelihood of misclassification between digit classes in a digit recognition task. By calculating the distances between class centroids and the nearest correctly classified examples, we construct a matrix of misclassification likelihoods. This approach provides insights into the similarity between digit classes and helps identify pairs of digits that are more prone to misclassification.

1. Introduction:
Digit recognition is a fundamental problem in computer vision and machine learning. While modern deep learning models have achieved high accuracy in this task, understanding the potential for misclassification between different digit classes remains important. In this study, we propose a method to analyze the likelihood of misclassification between digit classes based on the distances between class centroids and the nearest correctly classified examples.

2. Data and Methods:

a. Obtaining the Centroids:
Let $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$ be the training dataset, where $x_i \in \mathbb{R}^d$ represents the input features and $y_i \in \{0, 1, ..., 9\}$ represents the corresponding digit class label. We train a neural network classifier $f_\theta(x)$ on this dataset, where $\theta$ represents the learned parameters of the model.

To obtain the class centroids, we first collect the softmax outputs of the trained classifier for each correctly classified example in the training set. Let $S_c = \{s_1, s_2, ..., s_m\}$ be the set of softmax outputs for examples correctly classified as class $c$, where $s_i \in \mathbb{R}^{10}$. We calculate the centroid $\mu_c$ for class $c$ as the mean of the softmax outputs:

$\mu_c = \frac{1}{m} \sum_{i=1}^m s_i$

We repeat this process for each digit class $c \in \{0, 1, ..., 9\}$ to obtain the centroids $\mu_0, \mu_1, ..., \mu_9$.

b. Obtaining the Distance to a Neighboring Centroid:
Given a test example $x$ with true class label $y$, we obtain the softmax output $s = f_\theta(x)$. To calculate the distance between $s$ and a neighboring class centroid $\mu_c$, where $c \neq y$, we use the Euclidean distance:

$d(s, \mu_c) = \sqrt{\sum_{i=1}^{10} (s_i - \mu_{c,i})^2}$

This distance measures how close the softmax output of the test example is to the centroid of a different class.

c. Obtaining the Nearest Distances Between Digit Classes Matrix:
To construct the matrix of nearest distances between digit classes, we iterate over all test examples and calculate the nearest distance to each neighboring class centroid. Let $X_y$ be the set of test examples with true class label $y$. For each test example $x \in X_y$, we calculate the distance $d(f_\theta(x), \mu_c)$ to each neighboring class centroid $\mu_c$, where $c \neq y$.

We then find the minimum distance among all test examples in $X_y$ to each neighboring class centroid:

$D_{y,c} = \min_{x \in X_y} d(f_\theta(x), \mu_c)$

The resulting matrix $D \in \mathbb{R}^{10 \times 10}$ contains the nearest distances between each pair of digit classes, where $D_{y,c}$ represents the nearest distance from class $y$ to class $c$.

d. Obtaining the Misclassification Likelihood Matrix:
To obtain the misclassification likelihood matrix, we transform the nearest distance matrix $D$ by taking the reciprocal of each element and normalizing the rows:

$L_{y,c} = \frac{\frac{1}{D_{y,c}}}{\sum_{c=0}^9 \frac{1}{D_{y,c}}}$

The resulting matrix $L \in \mathbb{R}^{10 \times 10}$ contains the misclassification likelihoods between each pair of digit classes, where $L_{y,c}$ represents the likelihood of an example from class $y$ being misclassified as class $c$. Higher values indicate a higher likelihood of misclassification, while lower values indicate a lower likelihood.

3. Results and Discussion:
By visualizing the misclassification likelihood matrix $L$ as a heatmap, we can gain insights into the similarity between digit classes and identify pairs of digits that are more prone to misclassification. This information can be valuable for understanding the limitations of the classifier and guiding efforts to improve its performance.

For example, if the misclassification likelihood between digits 3 and 5 is high, it suggests that the classifier may struggle to distinguish between these two classes. This could prompt further investigation into the features that differentiate these digits and potential strategies to enhance the classifier's ability to discriminate between them.

Furthermore, the misclassification likelihood matrix can be used to identify digit classes that are more easily distinguishable from others. If a particular digit class has low misclassification likelihoods across all other classes, it indicates that the classifier can reliably distinguish this class from the rest.

4. Conclusion:
In this study, we presented a method for analyzing the likelihood of misclassification between digit classes based on the distances between class centroids and the nearest correctly classified examples. By constructing a misclassification likelihood matrix, we can gain insights into the similarity between digit classes and identify pairs of digits that are more prone to misclassification.

This approach provides a valuable tool for understanding the limitations and strengths of digit recognition classifiers. It can guide efforts to improve classifier performance by focusing on the most challenging digit pairs and informing the development of targeted strategies to enhance discrimination between similar classes.

Future work could explore the use of alternative distance metrics, such as cosine similarity or Mahalanobis distance, to capture different aspects of the relationship between softmax outputs and class centroids. Additionally, this method could be extended to other classification tasks beyond digit recognition to analyze misclassification likelihoods in various domains.


\begin{table*}[h]
\centering
\caption{Misclassification Likelihoods between Digit Classes}
\label{tab:misclassification}
\begin{tabular}{c|cccccccccc}
\toprule
 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
\midrule
0 & 0.0000 & 0.0669 & 0.0858 & 0.0608 & 0.0632 & 0.0821 & 0.1724 & 0.3313 & 0.0763 & 0.0612 \\
1 & 0.0552 & 0.0000 & 0.2634 & 0.0514 & 0.0466 & 0.0606 & 0.3193 & 0.1047 & 0.0480 & 0.0509 \\
2 & 0.3658 & 0.0183 & 0.0000 & 0.0095 & 0.5320 & 0.0058 & 0.0057 & 0.0476 & 0.0073 & 0.0080 \\
3 & 0.2395 & 0.0304 & 0.0381 & 0.0000 & 0.0306 & 0.1510 & 0.0303 & 0.1269 & 0.3066 & 0.0467 \\
4 & 0.0352 & 0.0371 & 0.0400 & 0.0293 & 0.0000 & 0.0292 & 0.0742 & 0.1252 & 0.1390 & 0.4908 \\
5 & 0.3731 & 0.0011 & 0.0012 & 0.0148 & 0.0011 & 0.0000 & 0.5207 & 0.0818 & 0.0015 & 0.0046 \\
6 & 0.0443 & 0.6143 & 0.0032 & 0.0063 & 0.0876 & 0.2328 & 0.0000 & 0.0029 & 0.0056 & 0.0029 \\
7 & 0.0251 & 0.0672 & 0.6781 & 0.0596 & 0.0157 & 0.0141 & 0.0139 & 0.0000 & 0.0230 & 0.1034 \\
8 & 0.3883 & 0.0026 & 0.2044 & 0.1612 & 0.0760 & 0.0076 & 0.0070 & 0.1279 & 0.0000 & 0.0250 \\
9 & 0.1041 & 0.0521 & 0.0048 & 0.0437 & 0.7096 & 0.0171 & 0.0039 & 0.0567 & 0.0080 & 0.0000 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[h!]
\centering
\caption{Misclassification Likelihoods between Digit Classes}
\begin{tabular}{c|cccccccccc}
\hline
\textbf{True \textbackslash Classified} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ \hline
\textbf{0} & 0.000000 & 0.066872 & 0.085777 & 0.060845 & 0.063174 & 0.082124 & 0.172389 & 0.331250 & 0.076341 & 0.061228 \\
\textbf{1} & 0.055158 & 0.000000 & 0.263446 & 0.051377 & 0.046612 & 0.060578 & 0.319334 & 0.104682 & 0.047951 & 0.050861 \\
\textbf{2} & 0.365840 & 0.018278 & 0.000000 & 0.009488 & 0.531984 & 0.005790 & 0.005742 & 0.047575 & 0.007281 & 0.008021 \\
\textbf{3} & 0.239528 & 0.030363 & 0.038092 & 0.000000 & 0.030569 & 0.150983 & 0.030309 & 0.126876 & 0.306621 & 0.046658 \\
\textbf{4} & 0.035248 & 0.037103 & 0.039998 & 0.029344 & 0.000000 & 0.029223 & 0.074155 & 0.125190 & 0.138960 & 0.490779 \\
\textbf{5} & 0.373089 & 0.001134 & 0.001158 & 0.014837 & 0.001146 & 0.000000 & 0.520747 & 0.081836 & 0.001453 & 0.004600 \\
\textbf{6} & 0.044329 & 0.614260 & 0.003244 & 0.006332 & 0.087622 & 0.232773 & 0.000000 & 0.002916 & 0.005595 & 0.002929 \\
\textbf{7} & 0.025119 & 0.067182 & 0.678087 & 0.059575 & 0.015651 & 0.014111 & 0.013868 & 0.000000 & 0.023020 & 0.103385 \\
\textbf{8} & 0.388266 & 0.002603 & 0.204442 & 0.161151 & 0.076001 & 0.007598 & 0.007027 & 0.127918 & 0.000000 & 0.024995 \\
\textbf{9} & 0.104093 & 0.052118 & 0.004759 & 0.043716 & 0.709551 & 0.017140 & 0.003859 & 0.056725 & 0.008039 & 0.000000 \\
\hline
\end{tabular}
\end{table*}


5. Testing the Hypothesis:
To test the hypothesis that some misclassifications are more likely to occur than others, we can analyze how the misclassification likelihoods change as we introduce perturbations to the input images. By comparing the misclassification likelihoods at different perturbation levels, we can determine if certain digit pairs are consistently more prone to misclassification.

Let $L^{(p)} \in \mathbb{R}^{10 \times 10}$ be the misclassification likelihood matrix at perturbation level $p$, where $p \in \{0, 1, ..., 9\}$. Each element $L^{(p)}_{y,c}$ represents the likelihood of an example from class $y$ being misclassified as class $c$ at perturbation level $p$.

To obtain $L^{(p)}$, we follow a similar process as described in Section 2.d, but we use the distances calculated from the perturbed examples at level $p$. Let $X^{(p)}_y$ be the set of test examples with true class label $y$ at perturbation level $p$. For each test example $x \in X^{(p)}_y$, we calculate the distance $d(f_\theta(x), \mu_c)$ to each neighboring class centroid $\mu_c$, where $c \neq y$.

We then find the minimum distance among all test examples in $X^{(p)}_y$ to each neighboring class centroid:

$$D^{(p)}_{y,c} = \min_{x \in X^{(p)}_y} d(f_\theta(x), \mu_c)$$

The resulting matrix $D^{(p)} \in \mathbb{R}^{10 \times 10}$ contains the nearest distances between each pair of digit classes at perturbation level $p$.

To obtain the misclassification likelihood matrix $L^{(p)}$, we apply the same transformation as in Section 2.d:

$$L^{(p)}_{y,c} = \frac{\frac{1}{D^{(p)}_{y,c}}}{\sum_{c=0}^9 \frac{1}{D^{(p)}_{y,c}}}$$

We repeat this process for each perturbation level $p$ to obtain a set of misclassification likelihood matrices $\{L^{(0)}, L^{(1)}, ..., L^{(9)}\}$.

To test the hypothesis, we can analyze the changes in misclassification likelihoods across different perturbation levels. We can calculate the average misclassification likelihood for each digit pair $(y, c)$ across all perturbation levels:

$$\bar{L}_{y,c} = \frac{1}{10} \sum_{p=0}^9 L^{(p)}_{y,c}$$

We can then identify the digit pairs with the highest average misclassification likelihoods. If these pairs consistently have high misclassification likelihoods across different perturbation levels, it suggests that they are more prone to misclassification compared to other digit pairs.

To quantify the consistency of misclassification likelihoods across perturbation levels, we can calculate the standard deviation of the misclassification likelihoods for each digit pair:

$$\sigma_{y,c} = \sqrt{\frac{1}{10} \sum_{p=0}^9 (L^{(p)}_{y,c} - \bar{L}_{y,c})^2}$$

A low standard deviation indicates that the misclassification likelihood for a digit pair remains relatively stable across perturbation levels, supporting the hypothesis that certain misclassifications are more likely to occur consistently.

6. Conclusion:
By analyzing the misclassification likelihood matrices at different perturbation levels, we can test the hypothesis that some misclassifications are more likely to occur than others. By calculating the average misclassification likelihoods and their standard deviations across perturbation levels, we can identify digit pairs that are consistently more prone to misclassification.

This analysis provides valuable insights into the robustness of the digit recognition classifier and highlights specific digit pairs that may require further attention to improve the classifier's performance. By focusing on the most problematic digit pairs and developing targeted strategies to enhance their discrimination, we can potentially reduce the overall misclassification rate and improve the classifier's reliability.

Future work could involve exploring more advanced techniques for analyzing the misclassification likelihoods, such as statistical tests to determine the significance of differences between digit pairs or clustering algorithms to group digits based on their misclassification patterns. Additionally, this methodology could be extended to other classification tasks and domains to investigate the consistency of misclassifications and identify areas for improvement.

% RESULTS

% Average Misclassification Likelihoods:
% [[0.         0.04589003 0.08733481 0.01600669 0.05028394 0.02450315
%   0.12941659 0.26547361 0.35286885 0.02822233]
%  [0.00696264 0.         0.16790996 0.10119414 0.04086901 0.01294107
%   0.11787246 0.12420778 0.38726756 0.04077538]
%  [0.24275133 0.27622163 0.         0.03378986 0.1324924  0.00566534
%   0.06866688 0.12849584 0.10579089 0.00612583]
%  [0.0514494  0.17997211 0.07648062 0.         0.01197889 0.13512509
%   0.00493103 0.28396858 0.21889841 0.03719586]
%  [0.12695529 0.10994202 0.07295048 0.0179491  0.         0.01051492
%   0.12419738 0.08067458 0.35986874 0.09694749]
%  [0.20455725 0.2019983  0.00308344 0.10658431 0.02498632 0.
%   0.1760474  0.12764603 0.11532218 0.03977478]
%  [0.16211025 0.38645202 0.03439961 0.04252382 0.12063421 0.11722695
%   0.         0.00286896 0.12962576 0.00415843]
%  [0.01815132 0.17441301 0.28553001 0.10584565 0.08511987 0.04185258
%   0.00639359 0.         0.20699499 0.07569898]
%  [0.19415324 0.25586704 0.04580845 0.06488091 0.10839543 0.06966731
%   0.08900392 0.11749699 0.         0.0547267 ]
%  [0.24362303 0.09976879 0.00913563 0.03235761 0.25753808 0.06354832
%   0.00688642 0.1705483  0.11659381 0.        ]]

% Standard Deviations of Misclassification Likelihoods:
% [[0.         0.03516574 0.07882539 0.00681692 0.03340428 0.01292942
%   0.13279241 0.13942703 0.09993014 0.0121811 ]
%  [0.0019806  0.         0.11039954 0.09824653 0.02881888 0.00551863
%   0.17296048 0.08690995 0.07953435 0.10466228]
%  [0.14786125 0.20949832 0.         0.03502986 0.0980855  0.0030173
%   0.10019107 0.06273818 0.04431707 0.00958921]
%  [0.03068918 0.19619773 0.04703588 0.         0.0113189  0.0787779
%   0.00226695 0.12003086 0.10887724 0.04340405]
%  [0.15792303 0.07128898 0.05094901 0.01612023 0.         0.00402161
%   0.12421511 0.06898397 0.12116148 0.05998781]
%  [0.11361326 0.15956683 0.0016181  0.05377323 0.0542732  0.
%   0.1676987  0.04825489 0.0354435  0.07479883]
%  [0.13114704 0.10652613 0.04337531 0.10127714 0.05561    0.09820077
%   0.         0.00202455 0.03766545 0.00482569]
%  [0.01125168 0.13543068 0.12005961 0.06489769 0.05554484 0.06502782
%   0.00490285 0.         0.07697289 0.06777867]
%  [0.14605839 0.20730602 0.04929954 0.04353724 0.05497643 0.07809966
%   0.12618888 0.04909242 0.         0.05127549]
%  [0.12955606 0.06220377 0.00993894 0.02246296 0.06695745 0.05434996
%   0.01010882 0.07712524 0.04559795 0.        ]]

% Top 5 Digit Pairs with High Misclassification Likelihood:
% True Class: 1, Misclassified Class: 8, Likelihood: 0.3873
% True Class: 6, Misclassified Class: 1, Likelihood: 0.3865
% True Class: 4, Misclassified Class: 8, Likelihood: 0.3599
% True Class: 0, Misclassified Class: 8, Likelihood: 0.3529
% True Class: 7, Misclassified Class: 2, Likelihood: 0.2855

% Digit Pairs with Consistent Misclassification Likelihood (Std Dev < 0.1):
% True Class: 0, Misclassified Class: 0, Std Dev: 0.0000
% True Class: 0, Misclassified Class: 1, Std Dev: 0.0352
% True Class: 0, Misclassified Class: 2, Std Dev: 0.0788
% True Class: 0, Misclassified Class: 3, Std Dev: 0.0068
% True Class: 0, Misclassified Class: 4, Std Dev: 0.0334
% True Class: 0, Misclassified Class: 5, Std Dev: 0.0129
% True Class: 0, Misclassified Class: 8, Std Dev: 0.0999
% True Class: 0, Misclassified Class: 9, Std Dev: 0.0122
% True Class: 1, Misclassified Class: 0, Std Dev: 0.0020
% True Class: 1, Misclassified Class: 1, Std Dev: 0.0000
% True Class: 1, Misclassified Class: 3, Std Dev: 0.0982
% True Class: 1, Misclassified Class: 4, Std Dev: 0.0288
% True Class: 1, Misclassified Class: 5, Std Dev: 0.0055
% True Class: 1, Misclassified Class: 7, Std Dev: 0.0869
% True Class: 1, Misclassified Class: 8, Std Dev: 0.0795
% True Class: 2, Misclassified Class: 2, Std Dev: 0.0000
% True Class: 2, Misclassified Class: 3, Std Dev: 0.0350
% True Class: 2, Misclassified Class: 4, Std Dev: 0.0981
% True Class: 2, Misclassified Class: 5, Std Dev: 0.0030
% True Class: 2, Misclassified Class: 7, Std Dev: 0.0627
% True Class: 2, Misclassified Class: 8, Std Dev: 0.0443
% True Class: 2, Misclassified Class: 9, Std Dev: 0.0096
% True Class: 3, Misclassified Class: 0, Std Dev: 0.0307
% True Class: 3, Misclassified Class: 2, Std Dev: 0.0470
% True Class: 3, Misclassified Class: 3, Std Dev: 0.0000
% True Class: 3, Misclassified Class: 4, Std Dev: 0.0113
% True Class: 3, Misclassified Class: 5, Std Dev: 0.0788
% True Class: 3, Misclassified Class: 6, Std Dev: 0.0023
% True Class: 3, Misclassified Class: 9, Std Dev: 0.0434
% True Class: 4, Misclassified Class: 1, Std Dev: 0.0713
% True Class: 4, Misclassified Class: 2, Std Dev: 0.0509
% True Class: 4, Misclassified Class: 3, Std Dev: 0.0161
% True Class: 4, Misclassified Class: 4, Std Dev: 0.0000
% True Class: 4, Misclassified Class: 5, Std Dev: 0.0040
% True Class: 4, Misclassified Class: 7, Std Dev: 0.0690
% True Class: 4, Misclassified Class: 9, Std Dev: 0.0600
% True Class: 5, Misclassified Class: 2, Std Dev: 0.0016
% True Class: 5, Misclassified Class: 3, Std Dev: 0.0538
% True Class: 5, Misclassified Class: 4, Std Dev: 0.0543
% True Class: 5, Misclassified Class: 5, Std Dev: 0.0000
% True Class: 5, Misclassified Class: 7, Std Dev: 0.0483
% True Class: 5, Misclassified Class: 8, Std Dev: 0.0354
% True Class: 5, Misclassified Class: 9, Std Dev: 0.0748
% True Class: 6, Misclassified Class: 2, Std Dev: 0.0434
% True Class: 6, Misclassified Class: 4, Std Dev: 0.0556
% True Class: 6, Misclassified Class: 5, Std Dev: 0.0982
% True Class: 6, Misclassified Class: 6, Std Dev: 0.0000
% True Class: 6, Misclassified Class: 7, Std Dev: 0.0020
% True Class: 6, Misclassified Class: 8, Std Dev: 0.0377
% True Class: 6, Misclassified Class: 9, Std Dev: 0.0048
% True Class: 7, Misclassified Class: 0, Std Dev: 0.0113
% True Class: 7, Misclassified Class: 3, Std Dev: 0.0649
% True Class: 7, Misclassified Class: 4, Std Dev: 0.0555
% True Class: 7, Misclassified Class: 5, Std Dev: 0.0650
% True Class: 7, Misclassified Class: 6, Std Dev: 0.0049
% True Class: 7, Misclassified Class: 7, Std Dev: 0.0000
% True Class: 7, Misclassified Class: 8, Std Dev: 0.0770
% True Class: 7, Misclassified Class: 9, Std Dev: 0.0678
% True Class: 8, Misclassified Class: 2, Std Dev: 0.0493
% True Class: 8, Misclassified Class: 3, Std Dev: 0.0435
% True Class: 8, Misclassified Class: 4, Std Dev: 0.0550
% True Class: 8, Misclassified Class: 5, Std Dev: 0.0781
% True Class: 8, Misclassified Class: 7, Std Dev: 0.0491
% True Class: 8, Misclassified Class: 8, Std Dev: 0.0000
% True Class: 8, Misclassified Class: 9, Std Dev: 0.0513
% True Class: 9, Misclassified Class: 1, Std Dev: 0.0622
% True Class: 9, Misclassified Class: 2, Std Dev: 0.0099
% True Class: 9, Misclassified Class: 3, Std Dev: 0.0225
% True Class: 9, Misclassified Class: 4, Std Dev: 0.0670
% True Class: 9, Misclassified Class: 5, Std Dev: 0.0543
% True Class: 9, Misclassified Class: 6, Std Dev: 0.0101
% True Class: 9, Misclassified Class: 7, Std Dev: 0.0771
% True Class: 9, Misclassified Class: 8, Std Dev: 0.0456
% True Class: 9, Misclassified Class: 9, Std Dev: 0.0000

% DISCUSSION OF RESULTS

The output provides valuable insights into the misclassification likelihoods between digit classes and the consistency of these likelihoods across different perturbation levels. Let's discuss the results and assess whether they support the hypothesis that some misclassifications are more likely to occur than others.

1. Average Misclassification Likelihoods:
   The average misclassification likelihoods matrix shows the likelihood of a digit being misclassified as another digit, averaged across all perturbation levels. Higher values indicate a higher likelihood of misclassification. Some notable observations:
   - Digit 1 is most likely to be misclassified as 8 (0.3873)
   - Digit 6 is most likely to be misclassified as 1 (0.3865)
   - Digit 4 is most likely to be misclassified as 8 (0.3599)
   - Digit 0 is most likely to be misclassified as 8 (0.3529)
   - Digit 7 is most likely to be misclassified as 2 (0.2855)

   These high misclassification likelihoods suggest that certain digit pairs are more prone to confusion than others.

2. Standard Deviations of Misclassification Likelihoods:
   The standard deviations matrix shows the variability of misclassification likelihoods across different perturbation levels. Lower values indicate more consistent misclassification likelihoods. The output lists digit pairs with standard deviations below a threshold of 0.1, suggesting that these pairs have consistent misclassification patterns across perturbation levels.

3. Top 5 Digit Pairs with High Misclassification Likelihood:
   The output identifies the top 5 digit pairs with the highest average misclassification likelihoods. These pairs are:
   - True Class: 1, Misclassified Class: 8 (Likelihood: 0.3873)
   - True Class: 6, Misclassified Class: 1 (Likelihood: 0.3865)
   - True Class: 4, Misclassified Class: 8 (Likelihood: 0.3599)
   - True Class: 0, Misclassified Class: 8 (Likelihood: 0.3529)
   - True Class: 7, Misclassified Class: 2 (Likelihood: 0.2855)

   These pairs represent the most common confusions between digit classes.

4. Digit Pairs with Consistent Misclassification Likelihood:
   The output lists digit pairs with standard deviations below 0.1, indicating consistent misclassification patterns across perturbation levels. Many digit pairs have consistent misclassification likelihoods, suggesting that the confusions between these pairs are stable and persistent.

Based on these results, the hypothesis that some misclassifications are more likely to occur than others is strongly supported. The average misclassification likelihoods matrix and the top 5 digit pairs with high misclassification likelihood clearly demonstrate that certain digit pairs are more prone to confusion. Moreover, the consistency of misclassification likelihoods across perturbation levels, as evidenced by the low standard deviations for many digit pairs, further reinforces the idea that these confusions are persistent and not merely random occurrences.

In conclusion, the results confirm the hypothesis, showing that certain digit pairs have higher misclassification likelihoods and that these misclassifications are consistent across different perturbation levels. This information can be valuable for identifying the limitations of the digit recognition model and guiding efforts to improve its performance by focusing on the most challenging digit pairs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MOVING AWAY FROM CENTROID HYPOTHESIS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hypothesis 2: Softmax Output Deviation with Added Noise}
Hypothesis 2: Softmax Output Deviation with Added Noise

In addition to the hypothesis that some misclassifications are more likely to occur than others, we propose another hypothesis regarding the behavior of the softmax output as noise is added to the input images. We hypothesize that as the level of perturbation increases, the softmax output of the classifier starts deviating from the true class centroid and moves closer to the centroids of potential misclassification classes.

Mathematically, let $s_i^{(p)}$ denote the softmax output of the classifier for the $i$-th test example at perturbation level $p$. The true class of the $i$-th example is denoted by $y_i$, and the centroid of class $c$ is represented by $\mu_c$.

We hypothesize that as the perturbation level $p$ increases, the distance between the softmax output $s_i^{(p)}$ and the true class centroid $\mu_{y_i}$ increases, while the distance between $s_i^{(p)}$ and the centroids of potential misclassification classes decreases.

To formalize this hypothesis, we define the distance between the softmax output and a class centroid as:

$d(s_i^{(p)}, \mu_c) = \sqrt{\sum_{j=1}^{10} (s_{ij}^{(p)} - \mu_{cj})^2}$

where $s_{ij}^{(p)}$ represents the $j$-th element of the softmax output $s_i^{(p)}$, and $\mu_{cj}$ represents the $j$-th element of the centroid $\mu_c$.

Our hypothesis can be expressed as:

$\frac{\partial d(s_i^{(p)}, \mu_{y_i})}{\partial p} > 0 \quad \text{and} \quad \frac{\partial d(s_i^{(p)}, \mu_c)}{\partial p} < 0 \quad \text{for} \quad c \neq y_i$

In other words, we expect the distance between the softmax output and the true class centroid to increase with increasing perturbation levels, while the distance between the softmax output and the centroids of potential misclassification classes decreases.

To test this hypothesis, we can analyze the behavior of the softmax output distances across different perturbation levels. We can calculate the average distance between the softmax output and the true class centroid for each perturbation level:

$\bar{d}_{y_i}^{(p)} = \frac{1}{|X_{y_i}^{(p)}|} \sum_{i \in X_{y_i}^{(p)}} d(s_i^{(p)}, \mu_{y_i})$

where $X_{y_i}^{(p)}$ represents the set of test examples with true class $y_i$ at perturbation level $p$.

Similarly, we can calculate the average distance between the softmax output and the centroids of potential misclassification classes:

$\bar{d}_c^{(p)} = \frac{1}{|X_c^{(p)}|} \sum_{i \in X_c^{(p)}} d(s_i^{(p)}, \mu_c) \quad \text{for} \quad c \neq y_i$

where $X_c^{(p)}$ represents the set of test examples misclassified as class $c$ at perturbation level $p$.

By analyzing the trends of $\bar{d}_{y_i}^{(p)}$ and $\bar{d}_c^{(p)}$ across different perturbation levels, we can assess whether the softmax output indeed deviates from the true class centroid and moves closer to the centroids of potential misclassification classes as noise is added.

If the hypothesis is supported, we expect to observe an increasing trend in $\bar{d}_{y_i}^{(p)}$ and a decreasing trend in $\bar{d}_c^{(p)}$ as the perturbation level $p$ increases. This would provide evidence that the addition of noise causes the softmax output to deviate from the true class centroid and move towards the centroids of potential misclassification classes, leading to increased misclassification likelihood.

Testing this hypothesis can provide valuable insights into the behavior of the classifier under noisy conditions and help understand the mechanisms underlying misclassifications. It can also guide the development of strategies to improve the robustness of the classifier against noise and enhance its ability to maintain accurate predictions even in the presence of perturbations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUBSECTION CIFAR-10 PERTURBED IMAGES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\subsection{CIFAR-10 Perturbed Images}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/CIFAR-10-Images-20imgs-2rows-10cols.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{First 20 images in the CIFAR-10 dataset}
    \label{fig:CIFAR-10-Images-20imgs-2rows-10cols}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/CIFAR-10-Pixelated-Dog-Image-16.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{CIFAR-10 10 Levels of Pixelation, Image Index 16}
    \label{fig:CIFAR-10-Pixelated-Dog-Image-16}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/CIFAR-10-Pixelated-Horse-Image-17.png}   \captionsetup{justification=raggedright,singlelinecheck=false}
    \caption{CIFAR-10 10 Levels of Pixelation, Image Index 11}
    \label{fig:CIFAR-10-Pixelated-Horse-Image-17.png}
\end{figure*}