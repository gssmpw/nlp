\section{Conclusion}
\label{sec:conclusion}
\anshuman{Can consider combining this and discussion section into one; would avoid some repetition}

In this work, we demonstrated that both the learning configuration of the FL system and the attacker’s strategy play a critical role in the success of targeted backdoor attacks. Existing defenses often operate under narrow and fixed assumptions, focusing on specific configurations and attack scenarios, which limits their applicability in real-world FL deployments. To bridge this gap, we proposed \textit{DROP}, a novel defense mechanism that is agnostic to both attacker strategies and learning configurations. By integrating clustering, activity tracking, and knowledge distillation, DROP offers a comprehensive solution for mitigating targeted backdoor attacks, ensuring reliable protection across diverse FL setups. Our extensive evaluation across multiple datasets, varying data distributions, and a wide range of attacker configurations highlights DROP’s robustness. Unlike existing baselines, which often fail under certain learning configurations or in the presence of stealthy attacks, DROP consistently achieves near-zero attack success rate despite a noticeable drop in main task accuracy. Nevertheless, our results also reveal that defending against targeted backdoor attacks in more complex learning tasks, such as CIFAR-10, remains an open challenge, particularly in highly non-IID settings. This underscores the need for further research into adaptive, configuration-independent defenses capable of handling evolving adversarial strategies. Looking forward, an important avenue for future work is the development of a systematic study of threat models and evaluation protocols to establish standardized benchmarks for FL defense research. Such an effort would help identify areas for improvement, enhance reproducibility, enable fairer comparisons across proposed defenses, and encourage the design of more practical, real-world solutions. By adopting an exhaustive, breadth-first evaluation methodology and releasing a comprehensive codebase\footnote{\textit{The codebase will be available upon acceptance.}\anshuman{I still feel we should provide an anonymized repo link.}}, we hope to provide a testbed for future researchers to rigorously assess new defenses and foster advancements in the field. Our work lays the groundwork for designing robust, universal defense mechanisms that can generalize across diverse FL environments, contributing to the long-term security and reliability of federated learning systems.
