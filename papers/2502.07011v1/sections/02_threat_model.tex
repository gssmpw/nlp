% \section{Setup}
% \label{sec:setup}

% Here we describe the basic setup for federated-learning, along with the threat model we consider in this work (\Cref{sec:threat_model}).

% \todo{Describe slightly-more formally (or maybe not? we probably don't need to describe a lot)}

% \subsection{Threat Model}
% \label{sec:threat_model}

% In this work, we consider a robust and comprehensive adversarial threat model that reflects realistic attack scenarios in FL systems. 

% \shortsection{Adversary's Goals} The adversary's primary objective is to embed a backdoor into the global model \( \mathbf{w}_T \) over the course of \( T \) training rounds. A backdoor attack is considered \emph{successful} if it meets two key criteria:  
% (1) it is \emph{stealthy}, meaning it evades detection by deployed defensive mechanisms with high confidence, and  
% (2) it is \emph{consistent}, ensuring that during inference, the global model \( \mathbf{w}_T \) reliably misclassifies inputs containing the backdoor trigger into the \emph{target class} chosen by the adversary.

% \shortsection{Adversary's Capabilities} The adversary is assumed to have the capability to compromise a certain fraction of clients in the system, while the central server remains trusted and secure. Once a client is compromised, the adversary gains full control over that client’s local resources, including its training data and the ability to dictate its attack strategy. This allows the adversary to manipulate training data (\ie by injecting poisoned examples), alter the training process (\eg training set vs batch-level poisoning), or directly modify the model updates sent to the server. These capabilities enable the adversary to execute sophisticated attacks aimed at undermining the global model’s integrity, including both targeted and untargeted poisoning strategies.

% % \shortsection{Adversary's Objective} As summarized by \citet{shejwalkar2022back}, the adversary's objectives for FL poisoning can be broadly categorized into the following:
% % \begin{enumerate}
% %     \item \textbf{Targeted Attacks}: Aim to misclassify only a specific set/classes of inputs.
% %     \item \textbf{Untargeted Attacks}: Aim to reduce model accuracy on arbitrary inputs.
% %     \item \textbf{Backdoor Attacks}: Aim to trigger misclassification for only certain inputs, \eg input with specific properties (semantic backdoors), inputs with specific trigger patterns (artificial backdoors).
% % \end{enumerate}
% % \todo{Describe our realistic threat model and why it is accurate. From how the clients are selected to the data conditions that are followed.}

% % \shortsection{Adversary's Capabilities}

% The client-sampling strategy in the FL system is assumed to be random, without any mechanisms to explicitly include or exclude specific clients. This randomness introduces significant variability in the number of adversary-controlled clients selected in each round. Unlike prior works that assume a fixed number of malicious clients in every round \citep{blanchard2017machine}, our threat model considers the possibility of variation in adversarial influence across rounds. In some rounds, the number of malicious clients may exceed the number of benign clients in the selected pool, enabling the adversary to exert substantial control over the aggregated update. Conversely, there may also be rounds where no adversarial clients are selected, reflecting the dynamic and unpredictable nature of real-world client participation. \anshuman{I think we should list both and include results for both, but default to no-ability-to-select as the default (can say ``we also explore strategies that allow forcing a certain number of adv clients per round in Appendix ..."}

% %The adversary’s objectives can be broadly categorized into two types: untargeted poisoning and targeted poisoning. In untargeted poisoning attacks, the adversary’s goal is to degrade the overall performance of the global model, reducing its accuracy across tasks and data distributions. On the other hand, targeted poisoning attacks aim to introduce specific biases into the model, such as causing misclassifications for certain classes, inputs, or tasks, while leaving the model’s overall performance largely unaffected. The adversary’s ability to control both the data and the update process on compromised clients provides flexibility to pursue these objectives under various scenarios.

% \shortsection{Data Distribution}
% Our threat model considers both independent and identically distributed (IID) and non-IID data conditions among the clients, ensuring a comprehensive evaluation of adversarial scenarios.
% % While we include IID distributions for completeness and to provide a baseline for comparison, the primary focus of this work is on non-IID settings.
% Unlike prior works that often rely on artificially constructed or overly simplified non-IID setups, our framework strives to replicate the complexity and variability of real-world data distributions. In our model, the data assigned to each client follows no predefined distribution, resulting in heterogeneous and highly imbalanced datasets. Each client can possess a completely random number of training examples per class, with certain classes potentially being overrepresented or absent altogether. This setup mirrors the chaotic and unpredictable data characteristics encountered in practical deployments, making it a more realistic and rigorous testing ground for adversarial defenses.
% \anshuman{Dirichlet for non-iid seems pretty prevelant. We should mention that we evaluate across various "levels" of non-iid, and also comment on how the non-iid nature of data impacts targeted backdoors attacks even more}

% This dynamic threat model captures the inherent unpredictability of FL systems, where the influence of adversarial clients may fluctuate over time. It highlights the significant challenges in designing robust defenses, particularly when malicious clients can dominate certain rounds or evade detection altogether in others. By considering such a stochastic and realistic adversarial setup, this work aims to address the complexities of defending FL systems against highly adaptive and resourceful adversaries.

% Here we describe the basic setup for federated learning, along with the threat model considered in this work (\Cref{sec:threat_model}).

% ========================================

\subsection{Threat Model}
\label{sec:threat_model}

\shortsection{Adversary's Goals} 
The adversary's primary objective is to embed a backdoor into the global model in a way that satisfies two key properties:
\newline
\newline
\underline{\textbf{Property 1}}: \textit{Stealthiness}. The backdoor should avoid arousing suspicion by not degrading performance on data without any triggers, whilst circumventing any defense measures the server may deploy.
\newline
\newline
% \underline{\textbf{Property 2}}: \textit{Consistency}. The backdoor must consistently misclassify inputs with the adversary's trigger \(\mathbf{t}\) into a specific \textit{target class} \(y_{\text{target}}\). Let \(\text{ASR}_t\) denote the attack success rate at round \(t\), defined as the fraction of inputs \(\mathbf{x} \in \mathcal{D}_{\text{victim}}\) misclassified as \(y_{\text{target}}\) by the global model \(\mathbf{w}_t\).
% %
% \anshuman{How about we do something like this:}
% \begin{align}
%     K = \operatorname*{arg\,top-}k_{t} MTA_t \\
%     \arg\max_{k\in K} ASR_k \leq \tau
% \end{align}
% \anshuman{Basically 'ASR for any round where the MTA is decent should be low' so that irrespective of which one the rounds is picked (maybe the MTA was 89\% v/s 89.1\%), the attack should not get through. Could instead look at models with MTA above some threshold $\lambda$ but the gist of it remains the same}
% To ensure a loose upward trend, the attack success rate should not show a persistent decline. This is formalized by ensuring the cumulative sum of increases dominates decreases over a window \(k\):
% \begin{equation}
%     \frac{1}{k} \sum_{i=t-k+1}^{t} \text{ASR}_i \leq \text{ASR}_{t+1}, \quad \forall t \in \{k, \dots, T-1\},
% \end{equation}
% where \(k\) controls trend smoothness. Additionally, the attack must achieve a minimum success rate \(\tau\) by the end of training:
% \begin{equation}
%     \lim_{t \to T} \text{ASR}_t \geq \tau,
% \end{equation}
% where \(\tau\) is a predefined threshold (e.g., \(\tau = 0.8\)). This ensures the backdoor remains effective despite fluctuations.
\underline{\textbf{Property 2}}: \textit{Consistency}. The backdoor attack must maintain a consistently high \textit{Attack Success Rate} (ASR) when the model achieves acceptable MTA, ensuring the adversary's success is not reliant on opportunistic fluctuations in ASR across rounds. Specifically, let \(\text{MTA}_t\) and \(\text{ASR}_t\) denote the main task accuracy and attack success rate at round \(t\), respectively. We define the set of rounds \(K\) where the MTA is above some given threshold \(\lambda\) as:  
\begin{equation}
    K = \{t \mid \text{MTA}_t \geq \lambda\},
\end{equation}  
and require that the ASR across these rounds remains above a specified threshold \(\tau\):  
\begin{equation}
    \min_{t \in K} \text{ASR}_t \geq \tau.
\end{equation}  
This formulation ensures that the backdoor remains effective throughout training, preventing its success from being undermined by transient fluctuations in ASR.
% \anshuman{This section is about the adversary's goals, but the second property is written from the defender's perspective. Should make sure both properties are from the same perspective.} \georgios{Understood what you mean and refined text to use the attacker perspective for property 2.}

\shortsection{Adversary's Capabilities} 
The adversary is successful in compromising and gaining control of a fraction $\rho$ out of all clients \(\mathcal{N}\) in the system, which is referred to as the \textit{Malicious Client Ratio} (MCR). This set of compromised clients $\mathcal{N}_\text{adv}$ is fixed throughout the federation.
% Once a client \(c \in \mathcal{N}_\text{adv}\) is compromised, the adversary gains full control over the client's local training process. This control extends to the client's training data \(\mathcal{D}_c\), local training procedure, and the updates \(\mathbf{w}_c^t\) sent back to the server at each round \(t\). The adversary can modify the local dataset \(\mathcal{D}_c\) by injecting \textit{poisoned} samples of the form \((\mathbf{x}_i + \mathbf{t}, y_{\text{target}})\), where \(\mathbf{t}\) is the adversarial trigger. 
Once a client \(c \in \mathcal{N}_\text{adv}\) is compromised, the adversary gains full control over the client’s local training process, including its dataset \(\mathcal{D}_c\), training procedure, and model updates \(\mathbf{w}_c^t\) submitted to the server at each round \(t\). 

In a targeted backdoor attack, the adversary focuses on a specific class, known as the \textit{victim} class, and poisons only samples from this class. Specifically, the adversary injects \textit{poisoned} samples into the client’s local dataset by applying a trigger \(\mathbf{t}\) to inputs belonging to the victim class and flipping their labels to a designated \textit{target} class \(y_{\text{target}}\). Formally, for a subset \(\mathcal{D}_{\text{poisoned}} \subset \mathcal{D}_{\text{victim}}\), each input is modified as:
\begin{equation}
    \mathcal{D}_{\text{poisoned}} = \{(\mathbf{x}_i + \mathbf{t}, y_{\text{target}}) \mid (\mathbf{x}_i, y_i) \in \mathcal{D}_{\text{victim}}\},
\end{equation}
where \(\mathcal{D}_{\text{victim}}\) denotes the set of samples from the victim class.
%
The proportion of poisoned samples relative to the total number of clean samples is referred to as the \textit{Data Poisoning Rate} (DPR). Additionally, the adversary may alter the client's update \(\mathbf{w}_c^t\) before submission to the server, embedding adversarial changes that facilitate the backdoor's propagation to the global model. For convenience, definitions of all acronyms used throughout the paper can be found in \Cref{tab:glossary}.

% Beyond data manipulation, the adversary has complete control over the local training process, allowing modifications to batch composition, epoch count, and the loss function. This level of control enables sophisticated attack strategies that can be adjusted adaptively over the course of training. 

% We formalize the adversary’s capabilities to control local data, manipulate updates, and adjust training strategies, thereby capturing a broad spectrum of potential attack strategies.

\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{ll|ll}
    \toprule
    \textbf{Acronym} & \textbf{Definition} & \textbf{Acronym} & \textbf{Definition} \\
    \midrule
    DPR & Data Poisoning Rate & MTA & Main Task Accuracy \\
    MCR & Malicious Client Ratio & ASR & Attack Success Rate \\
    \bottomrule
    \end{tabular}
    \caption{Glossary of acronyms used throughout the paper.}
    \label{tab:glossary}
\end{table}
