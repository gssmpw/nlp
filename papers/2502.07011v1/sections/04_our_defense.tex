\section{Our Defense: DROP}
\label{sec:proposed_defense}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{assets/system_final.png}
    \caption{Overview of the proposed DROP defense. Each round \( t \) begins with the server broadcasting the global model to all clients and selecting a subset for local training, which may include both benign (green) and malicious (red) clients. After updates are submitted, DROP employs: (1) \textbf{Agglomerative Clustering} to detect anomalous updates, (2) \textbf{Activity Monitoring \& Penalization} to track and penalize suspicious clients, and (3) \textbf{Knowledge Distillation}, where a GAN-generated synthetic dataset and client logits guide the distillation of the global model. The final model \( \mathbf{w}_{t+1} \) serves as the global model for round \( t+1 \).}
    \label{fig:drops} 
\end{figure*}

From our analysis, it is clear that the learning configuration of a FL system plays a pivotal role in the success of targeted backdoor attacks. Certain configurations exhibit inherent vulnerabilities, allowing adversarial updates to bypass detection and achieve high ASR. Moreover, the performance of existing defenses shows significant variability across different configurations, revealing their lack of robustness across learning setups. These findings emphasize the urgent need for a \textit{universal} defense mechanism that is resilient against a wide array of attack strategies and remains agnostic to the underlying learning configuration. 

To address this challenge, we introduce \textbf{DROP} (\textit{\textbf{D}istillation-based \textbf{R}eduction \textbf{O}f \textbf{P}oisoning}), a federated framework designed to counter three critical adversarial scenarios:
\begin{itemize}
    \item aggressive adversaries that may resort to high-DPR attacks,
    \item diverse MCRs  within the federation, and
    \item stealthy, low-DPR attacks that exploit specific learning configurations.
\end{itemize}
These scenarios each present unique challenges that require a cascade of countermeasures, as summarized in the following sections. An overview of our approach is given in \Cref{fig:drops}.
%
% DROP operates within the FedAvg \citep{mcmahan2017communication} paradigm (described in \Cref{sec:background}). \anshuman{FedAvg is presumably the default choice for FL; do we need to state it here explicitly while describing DROP?}
% At each training round $t$, the local updates from participating clients $\mathcal{C}_t$ undergo a series of three countermeasures to ensure robustness against targeted backdoor attacks. 

The first countermeasure, \textit{Agglomerative Clustering} (\Cref{subsec:agglo_clustering}), identifies and isolates malicious updates by clustering submitted models based on their pairwise Euclidean distances.
% The updates are divided into two clusters: a benign cluster \(C_b\) and a suspicious cluster \(C_s\). The smaller of the two clusters, \(C_s\), is pruned, and the updates within it are deemed \textit{suspicious}.
This process is particularly effective against high-DPR attacks, where malicious updates deviate significantly from benign updates.  
%
The second countermeasure, \textit{Activity Monitoring} (\Cref{subsec:activity_monitoring}), tracks client behavior across training rounds. By maintaining a reputation score based on prior clustering results, this mechanism penalizes clients frequently flagged as suspicious and reduces their influence in future aggregations. This reputation-based approach ensures robustness against per round benign-to-malicious client ratio fluctuations, where the proportion of malicious clients can vary widely across rounds due to random client selection.  

The third and final countermeasure, \textit{Knowledge Distillation} (\Cref{subsec:kd}), addresses the most challenging class of attacks: stealthy, low-DPR strategies. These attacks encode adversarial objectives subtly into model updates, making them indistinguishable from benign updates and enabling them to bypass clustering schemes. To neutralize these residual adversarial signals, we use synthetic data generated by a GAN trained via logit-driven distillation to produce a \textit{cleansed} version of the global model. This process ensures robustness against low-DPR attacks and renders the defense agnostic to learning configurations.

Through this cascade of countermeasures, DROP achieves the following three goals:  
\begin{enumerate}
    \item Resilience against aggressive, high data poisoning rates.
    \item Adaptability to diverse malicious client ratios.
    \item Robustness against stealthy, low data poisoning rates.  
\end{enumerate}
% \anshuman{Just mentioned these 3 at the start of the section- maybe refer to those instead of repeating here?}
Next, we provide a detailed explanation of the design and motivation behind each component. The entire DROP algorithm is presented in Algorithm \ref{alg:drop}. 

% DROP ALGORITHM
\input{sections/drops_algo}

% \subsection{Agglomerative Clustering}
% \label{subsec:agglo_clustering}

% The first line of defense in our system is \textit{Agglomerative Clustering} (AC) \citep{müllner2011modernhierarchicalagglomerativeclustering, auror}, chosen for its suitability in detecting subtle deviations caused by targeted backdoor attacks. Clustering is a widely adopted countermeasure in filtering-based FL frameworks \citep{nguyen2022flame, fung2018mitigating, baybfed, mesas} due to its effectiveness in identifying dissimilarities within a population of model updates. These dissimilarities are often the result of adversarial strategies employing a high DPR that aggressively embed backdoors into updates. We choose AC for clustering due to its flexibility and hierarchical structure; unlike K-means, which is based on ranking distances that are too small to convey useful information \anshuman{What does that mean?}\alina{k-means has a number of limitations: sensitivity to initial centroid selection, susceptibility to outliers, etc.}, or HDBSCAN \citep{campello2013density}, which relies on density-based assumptions \anshuman{Why would that not work in our scenario?}\alina{That is not the issue, but need to set some hyper-params}, AC dynamically merges clusters until a stopping criterion is met. \alina{The use of AC needs better motivation, e.g., less hyper-parameters to tune}

% The use of \textit{Ward linkage} \citep{Ward01031963} is they key to AC's effectiveness \anshuman{Is ward-linkage a feature of AC, or one of several ways to perform part of the clustering? If former, can shorten this part a bit} \georgios{Yes it is exclusive to agglomerative clustering as far as I know. I have not seen it being used in other clustering methods.}. \alina{AC can be done with multiple linkage methods, and Ward is one of them, you can motivate why it's the right choice, as it min the variance of clusters.} By minimizing intra-cluster variance during the merging process, Ward linkage is particularly adept at detecting fine-grained and stealthy backdoors that can be embedded within benign-looking updates. This property ensures that subtle differences in high-dimensional weight spaces—indicative of adversarially perturbed updates—are captured and separated.

\subsection{Agglomerative Clustering}  
\label{subsec:agglo_clustering}  

The first line of defense in our system is \textit{Agglomerative Clustering} (AC) \citep{müllner2011modernhierarchicalagglomerativeclustering}, selected for its flexibility and hierarchical structure, which make it well-suited for detecting subtle deviations caused by targeted backdoor attacks. Clustering is a widely adopted countermeasure in filtering-based FL frameworks \citep{nguyen2022flame, fung2018mitigating, baybfed, mesas, auror} due to its ability to identify dissimilarities within a population of model updates, often resulting from adversarial strategies that embed backdoors aggressively with a high DPR. We chose AC over methods like K-means and HDBSCAN due to its lower sensitivity to hyper-parameter selection and its dynamic merging process, which eliminates the need to pre-specify the number of clusters. Unlike K-means, which can be sensitive to initial centroid selection and outliers, AC provides more consistent results by iteratively merging clusters based on similarity. Similarly, while HDBSCAN \citep{campello2013density} relies on density-based assumptions that require careful tuning of hyper-parameters, AC operates without such assumptions, making it more robust in diverse FL scenarios.

The use of \textit{Ward linkage} \citep{Ward01031963} further enhances AC's effectiveness by ensuring that intra-cluster variance is minimized during the merging process. Among the various linkage methods available for AC, Ward linkage is particularly effective for FL tasks, as it prioritizes clusters with low internal variance, which is crucial for distinguishing fine-grained and stealthy backdoors embedded within benign-looking updates. By capturing and separating subtle differences in high-dimensional weight spaces, Ward linkage ensures that adversarial updates are reliably isolated from legitimate ones.  

Let \(\mathcal{W} = \{\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_n\}\) denote the set of model updates from \(n\) participating clients in a given training round \(t\). Let $d(\mathbf{w}_i, \mathbf{w}_j) = \| \mathbf{w}_i - \mathbf{w}_j \|_2$ be the Euclidean distance between two model updates \(\mathbf{w}_i\) and \(\mathbf{w}_j\).
%
The clustering process starts with each client update \(\mathbf{w}_i\) being treated as a singleton cluster. At each step, the two clusters \(\mathcal{A}\) and \(\mathcal{B}\) with the smallest inter-cluster distance are merged. Using \textit{Ward linkage}, the distance between two clusters \(\mathcal{A}\) and \(\mathcal{B}\) is defined as the increase in total intra-cluster variance caused by merging the two clusters:
\begin{equation}
    d_{\text{Ward}}(\mathcal{A}, \mathcal{B}) = \frac{|\mathcal{A}| \, |\mathcal{B}|}{|\mathcal{A}| + |\mathcal{B}|} \, \| \boldsymbol{\mu}_{\mathcal{A}} - \boldsymbol{\mu}_{\mathcal{B}} \|_2^2,
\end{equation}
where \(|\mathcal{A}|\) and \(|\mathcal{B}|\) are the sizes (number of points) of clusters \(\mathcal{A}\) and \(\mathcal{B}\), and \(\boldsymbol{\mu}_{\mathcal{A}}\) and \(\boldsymbol{\mu}_{\mathcal{B}}\) are their respective centroids.
% \begin{equation}
%     \boldsymbol{\mu}_{\mathcal{A}} = \frac{1}{|\mathcal{A}|} \sum_{\mathbf{w} \in \mathcal{A}} \mathbf{w}, \quad 
%     \boldsymbol{\mu}_{\mathcal{B}} = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{w} \in \mathcal{B}} \mathbf{w}.
% \end{equation}

The hierarchical clustering process proceeds iteratively until a stopping criterion is met. In our case, the process halts upon forming exactly two predefined clusters, \(\{\mathcal{C}_b, \mathcal{C}_s\}\), which effectively capture natural groupings of benign and malicious model updates.
In the context of FL, it is assumed that benign client updates will cluster together due to the shared training objective, while adversarial updates will form separate, smaller clusters due to their deviation from normal update patterns.

Minimizing intra-cluster variance ensures that the clustering process captures subtle distinctions in high-dimensional weight spaces, where even minor variations can signal meaningful differences, such as those between adversarially perturbed weights and legitimate ones. The hierarchical structure produced by Ward linkage allows flexibility in examining update patterns at various levels of similarity, enabling dynamic control over how clusters are merged. This approach is particularly valuable in FL, where client updates are naturally noisy due to heterogeneous data distributions but can also be adversarially manipulated. By clustering updates, this method enables the system to separate and potentially eliminate outliers or manipulated model updates.
% \newline
% \framedtext{\underline{\textbf{Goal 1}}: The clustering component aims to eliminate malicious updates that deviate significantly from their benign counterparts in each training round, offering resilience against high data poisoning rate attacks.}\label{goal1}
\goalbox{\underline{\textbf{Goal 1}}: The clustering component aims to eliminate malicious updates that deviate significantly from their benign counterparts in each training round, offering resilience against high data poisoning rate attacks.}\label{goal1}

\subsection{Activity Monitoring}
\label{subsec:activity_monitoring}

The second line of defense, complementing the clustering component, is an \textit{activity monitoring mechanism}, which tracks the behavior of participating clients over the course of training. 

This mechanism addresses a limitation of clustering-based defenses in federated learning, which assume that the number of malicious updates in a round is smaller than the number of benign ones. In real-world scenarios, however, client selection is random, and the defender has no prior knowledge of the number of malicious clients in any given round. This randomness can lead to rounds where adversarial clients outnumber benign ones, causing misidentification of the smaller cluster.
%To mitigate this, prior works commonly assume an upper bound on the proportion of malicious clients, often constraining the number of malicious participants in each round to less than 50\% of the selected clients. While this assumption allows clustering-based defenses to reliably identify and prune smaller clusters, enforcing it typically requires operating under low MCRs or artificially controlling client sampling to maintain a fixed benign-to-malicious client ratio across rounds \citep{zhang2023flip}, both of which limit the practical applicability of such methods.
Prior works often assume an upper bound on the proportion of malicious clients (typically less than 50\%), but enforcing this requires low MCRs or artificially controlled client sampling  to maintain a fixed benign-to-malicious client ratio across rounds \citep{zhang2023flip}, limiting practical applicability.
It is important to note that although the total proportion of adversarial clients in the federation (MCR) remains fixed, the actual ratio of malicious clients in any given round can fluctuate due to the random selection of clients.

The maximum tolerable MCR of a defense—the fraction of clients in the federation that can be adversarial while still allowing the defense to mitigate an attack—depends on the total number of clients in the federation, \(N\), and the number of clients sampled in each round, \(C\). 
%\anshuman{What do we mean by 'supported' threshold?} \georgios{It is the fraction of clients being malicious in the federation that a defense can tolerate to mitigate an attack. I changed the text a bit to make it more clear.} 
Under a random sampling strategy, \(C\) clients are drawn uniformly from the total \(N\) clients. Let $\rho$ be the MCR \ie proportion of malicious clients in the entire federation, so the expected number of malicious clients ($\mathcal{M})$ in a round is \(\mathbb{E}[\mathcal{M}] = \rho \cdot C\). By modeling the selection of clients as a Binomial distribution, we can compute a lower bound on the probability of malicious clients outnumbering benign ones in any given round as:
\begin{align}
    P\left(\mathcal{M} \geq \frac{C}{2}\right) \geq  1 - \left(4\rho(1-\rho)\right)^{\frac{C}{2}}.
\end{align}
For a derivation of the above, please see \Cref{app:bound_analysis}.
% By applying the Chernoff bound \citep{chernoff}, we can bound the probability that the number of malicious clients \(M\) significantly deviates from its expectation. Specifically, for any \(\delta > 0\):
% \begin{equation}
    % \mathbb{P}\left[\mathcal{M} \geq (1 + \delta) \cdot \rho \cdot C\right] \leq \exp\left(-\frac{\delta^2 \cdot \rho \cdot C}{2 + \delta}\right).
% \end{equation}
This bound implies that as \(C\) increases, the likelihood of selecting a disproportionately large number of malicious clients diminishes exponentially. Conversely, when \(C\) is small,
% the variability in the number of malicious clients per round increases, raising
the probability that malicious clients outnumber benign clients within the selected subset increases. 
%
For example, consider an FL system with \(N = 100\) total clients and a MCR of 40\% (\(\rho = 0.4\)). If \(C = 20\) clients are randomly selected in a given round, the expected number of malicious clients in the subset is \(\mathbb{E}[\mathcal{M}] = \rho \cdot C = 0.4 \cdot 20 = 8\).
However, with the inequality above we can see the probability of selecting more than 10 malicious clients can be \underline{non-trivial} ($\approx 0.34$), potentially resulting in a subset where malicious clients constitute more than 50\% of the selected participants (\(M > C/2\)). In fact, the bound suggests that in about a third of the FL training rounds, the malicious clients will be the majority. 
% However, due to the randomness of client selection, the actual number of malicious clients can deviate significantly from this expectation.
% Using the Chernoff bound and setting \( (1 + \delta) \cdot \rho \cdot C = C/2\), the probability of selecting more than 10 malicious clients\footnote{The detailed steps for the calculation of the bound can be found in \cref{sec:chernoff_appendix}.} (\(M > 10\)) can be \underline{non-trivial} (\(\mathbb{P}\left[\mathcal{M} \geq 10\right] \lesssim 0.8\)), potentially resulting in a subset where malicious clients constitute more than 50\% of the selected participants (\(M > C/2\)).
% This scenario undermines clustering-based defenses, as the smaller cluster would incorrectly represent benign clients.

To address scenarios where the number of malicious clients exceeds the benign ones in certain rounds, we propose a reputation-based mechanism. This approach tracks client behavior across rounds, penalizing suspicious clients identified by the clustering component (\Cref{subsec:agglo_clustering}) and gradually reducing their influence over the global model. By doing so, our method dynamically adjusts to varying levels of malicious participation without relying on rigid assumptions about MCR or artificially controlling client sampling. For instance, FLIP~\cite{zhang2023flip} enforces a fixed per-round MCR by '\textit{randomly}' selecting 10 clients per round, ensuring exactly 4 adversaries and 6 benign clients, thus artificially tampering with the randomness of client selection.
% \anshuman{Clarify that reputation helps account for misclassifications in rounds where adversary outnumbers benign clients; maybe can use the bound to show that on average as long as MCR is less than 50\%, in the long run there will be more benign clients identified successfully than incorrectly classified as adversaries} \georgios{Would the following paragraph that I have added work?}
By incorporating a reputation system, our defense mitigates errors caused by transient imbalances where adversarial clients may temporarily outnumber benign ones. Over multiple rounds, as long as the overall MCR remains below 50\%, the mechanism ensures that benign clients are correctly identified and retained more frequently than adversarial clients. This design helps prevent long-term accumulation of adversarial influence, even in rounds where clustering misclassifications may occur.

Our reputation-based mechanism employs a penalty and reward system that essentially tracks the trustworthiness of each client, enabling the server to mitigate persistent malicious activity while accounting for potential false positives. Specifically, by maintaining a cumulative penalty score \(\pi(c)\) for each client \(c\), the server can distinguish between clients with occasional false-positive detections and those consistently submitting suspicious updates. This ensures that benign clients incorrectly flagged as suspicious in a few rounds do not face permanent exclusion from the system.

\shortsection{Calculating Penalty Scores}
We begin by initializing the penalty score \(\pi(c) = 0\) for every client. As training proceeds, this score is dynamically updated based on the clustering results from each round.
% \newline
% \newline
% \underline{\textbf{Rule 1}}:
During each training round \( t \), the clustering component (\Cref{subsec:agglo_clustering}) determines whether a client's update is benign or suspicious by assigning the client with a penalty score:
\begin{align}
    \pi(c) =
    \begin{cases}
    \pi(c) + p, & \text{if } c \in \mathcal{C}_s\\
    \max(0, \pi(c) - r),              & \text{otherwise}
    \end{cases}
\end{align}
\newline
Essentially, if a client's update is flagged as \textit{suspicious} (\( c \in \mathcal{C}_s \)), its penalty score is increased by a constant penalty value \( p \). Otherwise, it is reduced by a constant reward value \( r \), ensuring the score remains non-negative. Capping the penalty score at zero prevents potentially malicious clients from gaining undue advantage in cases where they are repeatedly flagged as benign due to false positives. $p$ and $r$ are hyper-parameters determined by the server.

The server uses these scores to regulate client participation. Clients with a clean history \ie consistently flagged as benign (\(\pi(c) = 0\)) are deemed trustworthy and allowed to contribute to the global model, whereas clients with a record of suspicious update submissions (\(\pi(c) > 0\)) are restricted from participating in the aggregation process. However, depending on the server administration policy, the penalty system could be even stricter. Clients that accumulate excessive penalty points due to repeated suspicious updates are permanently \textit{blacklisted} from contributing in subsequent rounds:
\begin{comment}
\newline
\newline
\underline{\textbf{Rule 2}}: \textbf{\textit{(Optional)}} Specifically, the server permanently bans any client whose penalty score exceeds a predefined threshold \( \tau \):
\begin{equation}
    \pi(c) \geq \tau \implies \text{ban}(c).
\end{equation}
\georgios{Another option would be to rename Rule 2 to shortsection “Calculating Penalty/Trust Scores” and omit the rule formulation but still keep the text around it}
\end{comment}
Optionally, the server could permanently ban any client whose penalty score exceeds a predefined threshold.
% \newline
% \framedtext{\underline{\textbf{Goal 2}}: The activity monitoring component helps prevent malicious actors (based on their accumulated penalty points) from poisoning the global model during rounds in which the malicious outnumber the benign participants.}
\goalbox{\underline{\textbf{Goal 2}}: The activity monitoring component helps prevent malicious actors (based on their accumulated penalty points) from poisoning the global model during rounds in which the malicious outnumber the benign participants.}\label{goal2}



\subsection{Knowledge Distillation}
\label{subsec:kd}

Having tackled high-DPR and dynamic-MCR attacks, the most challenging category to defend against is low-DPR attacks.
These attacks are difficult to detect due to their stealthy nature, where adversarial objectives are subtly embedded into model weights, making the updates nearly indistinguishable from benign ones. This smooth encoding allows them to bypass traditional clustering-based defenses that rely on statistical or geometric properties of the updates. To counter this, DROP introduces a knowledge distillation-based cleansing mechanism that neutralizes residual adversarial signals in the global model using clean, synthetic data, effectively mitigating the impact of stealthy backdoor updates while preserving the model’s utility for legitimate tasks.

% Traditional clustering-based defenses rely on identifying significant deviations in update patterns to separate malicious updates from benign ones. However, low-DPR attacks are specifically designed to minimize these deviations, allowing adversarial updates to bypass clustering filters entirely.  To address this challenge, DROP introduces a knowledge distillation-based cleansing component that removes the residual adversarial signals embedded in the global model. By distilling the knowledge from the global model onto a surrogate model using clean, synthetic data, this component isolates and neutralizes the effects of stealthy backdoor updates while preserving the utility of the global model for legitimate tasks.
% \anshuman{Above two paragraphs can be shortened by a lot}

This approach leverages model stealing attacks, such as the \textit{MAZE} model stealing framework \citep{kariyappa2021maze}, which achieves state-of-the-art performance in extracting machine learning models under black-box conditions. The goal here is not to engage in model stealing as traditionally intended, but to clone the model without introducing any malicious behavior from compromised clients. \textit{MAZE} combines two key components: query synthesis and knowledge distillation. Knowledge Distillation \cite{hinton2015distillingknowledgeneuralnetwork} is a technique where a "student" model learns to approximate the output logits of a larger "teacher" model, enabling the transfer of knowledge while preserving essential decision boundaries. A generative adversarial network (GAN) \cite{radford2016unsupervisedrepresentationlearningdeep} synthesizes inputs to probe a target model's decision boundaries, and knowledge distillation transfers the target model's behavior to a surrogate model trained on these synthetic queries. By minimizing reliance on labeled data, \textit{MAZE} efficiently approximates the target model's functionality.

In our approach, we adapt and extend the \textit{MAZE} framework to take advantage of the white-box access available to the server in FL, as the server owns the global model. This adjustment eliminates the need for zeroth-order gradient estimation used in black-box settings and allows for direct backpropagation, improving both efficiency and accuracy. To ensure high-quality synthetic data, we initialize the GAN with a set of \( n \) clean samples, denoted as \( \mathcal{D}_\text{clean} = \{\mathbf{x}_i \}_{i=1}^n \) in step 4 of Algorithm \ref{alg:drop}, which helps align the generated queries with the original data distribution. Our approach requires only a small set of clean samples—typically less or equal than the size of a single client's dataset—making it practical and feasible for real-world FL scenarios.

In a training round $t$, instead of relying solely on the global model’s logits, we aggregate logits from client updates that pass the filtering process (\ie $\mathcal{C}_b$). Let the logit output from the global model be \( \mathbf{z}_t = f(\mathbf{x}; \mathbf{w}_t) \), where \( \mathbf{w}_t \) represents the global model parameters at round \( t \). Similarly, let the logit outputs from benign client models \( \mathcal{C}_b \subseteq \mathcal{C}_t \) be:
\begin{equation}
    \mathbf{z}_{t}^c = f(\mathbf{x}; \mathbf{w}_t^c), \quad \forall c \in \mathcal{C}_b.
\end{equation}
We compute the ensemble logits \( \mathbf{\bar{z}}_t \) as the average of the logits from benign clients:
\begin{equation}
    \mathbf{\bar{z}}_t = \frac{1}{|\mathcal{C}_b|} \sum_{c \in \mathcal{C}_b} \mathbf{z}_{t}^c.
\end{equation}

The generator \( G \) uses the ensemble logits \( \mathbf{\bar{z}}_t \) as feedback to synthesize new queries \( \mathcal{D}_{\text{synthetic}} \). These synthetic queries, denoted as \( \mathbf{x}_{\text{gen}} \), are used to guide a \textit{clone network}, which serves as a cleansed version of the global model. The clone network, parameterized by \( \mathbf{w}^{\text{clone}}_t \), is trained to align its predictions with the ensemble logits. Instead of the KL divergence used in the original \textit{MAZE} framework, we employ the \(\ell_1\)-loss, which provides more stable performance \citep{truong2021data}. The training objective for the clone network is defined as:
\begin{equation}
    \mathcal{L}_{\text{distill}} = \mathbb{E}_{\mathbf{x} \sim \mathcal{D}_{\text{synthetic}}} \left[ \| \mathbf{\bar{z}}_t - \mathbf{z}_t^{\text{clone}} \|_1 \right],
\end{equation}
where \( \mathbf{z}_t^{\text{clone}} = f(\mathbf{x}; \mathbf{w}^{\text{clone}}_t) \) represents the logit predictions of the clone network on synthetic data \( \mathbf{x} \).

The key difference between this approach and the original MAZE framework is that, instead of distilling knowledge from a single victim model, we distill knowledge from the aggregated logits of benign clients. The intuition behind this approach is that the ensemble logits \( \mathbf{\bar{z}}_t \), derived from benign client models, act as a \textit{consensus signal} to overwrite any adversarial influence introduced by poisoned updates. By aligning the clone network’s behavior with the aggregated benign logits, the distillation process effectively cleanses the model of any stealthy backdoor or poisoned behavior that might have evaded detection in previous defense layers. 
% \newline
% \framedtext{\underline{\textbf{Goal 3}}: The collective logit-driven knowledge distillation framework ensures that any subtle adversarial updates which have made their way to the global model are neutralized, restoring the global model’s robustness and reliability.}
\goalbox{\underline{\textbf{Goal 3}}: The collective logit-driven knowledge distillation framework ensures that any subtle adversarial updates which have made their way to the global model are neutralized, restoring the global model’s robustness and reliability.}\label{goal3}

% \subsection{DROPlet: a lightweight version of DROP}
% \label{subsec:droplet}

% While DROP provides comprehensive protection against targeted backdoor attacks through its cascading components, the knowledge distillation component introduces a non-trivial computational overheads. To offer a faster and more streamlined alternative \anshuman{are we saying that DROP is not streamlined? Focus more on lightweight part and less computationally heavy}, we propose \textit{DROPlet}, a lightweight, server-side plugin designed for ease of deployment and compatibility with various types of data. Unlike DROP, DROPlet omits the knowledge distillation component, focusing solely on agglomerative clustering and activity monitoring to detect and mitigate adversarial influences.

% The primary motivation behind DROPlet is to provide an efficient and domain-agnostic defense mechanism that can be easily integrated into existing FL systems without requiring significant computational resources. Since DROPlet does not depend on task-specific features or data modality assumptions, it is applicable across a wide range of FL tasks, including image, text, and tabular data. This versatility ensures that DROPlet can serve as a general-purpose defense framework in diverse real-world FL applications.

% DROPlet also excels in terms of speed and scalability. By excluding the knowledge distillation step, it achieves faster round completion times and reduces the server-side processing load, making it particularly suitable for large-scale FL deployments where computational efficiency is a critical concern. Despite its lightweight nature, DROPlet maintains strong defenses by relying on robust clustering to detect anomalies and activity monitoring to penalize persistently malicious clients. \anshuman{Forward-reference that it does not work in all settings. In fact, this could even be a new "baseline", replacing the likes of Multi-KRUM and Median} This combination ensures reliable performance against a wide range of adversarial strategies, making DROPlet another practical choice for defending FL systems in environments with limited computational resources.

\subsection{DROPlet: a Lightweight, Scalable Defense Mechanism}  
\label{subsec:droplet}  

To provide a faster, more resource-efficient alternative to DROP, we introduce \textit{DROPlet}, a lightweight, server-side plugin designed for easy deployment and high scalability.
Unlike DROP, DROPlet omits the knowledge distillation component and focuses solely on agglomerative clustering and activity monitoring to mitigate adversarial influences with minimal overhead.  

DROPlet is task-agnostic, with no assumptions on the underlying data modality, making it applicable to various FL tasks. This versatility allows DROPlet to function as a general-purpose defense mechanism in diverse FL deployments. By eliminating the computationally intensive knowledge distillation step, it achieves faster round completion times, making it particularly suitable for large-scale FL systems where computational efficiency is crucial.  

Despite its lightweight design, DROPlet offers robust protection by leveraging clustering to detect anomalies and activity monitoring to penalize malicious clients. However, it may not be effective against stealthy, low-DPR attacks (\cref{shortsec:droplet_results}).
% \anshuman{Forward-reference that it does not work in all settings. In fact, this could even be a new "baseline", replacing the likes of Multi-KRUM and Median} \georgios{Good idea, refined the text.}
Nevertheless, DROPlet remains a competitive baseline for FL defense evaluations, offering significant protection while being faster and easier to deploy compared to classical methods like Multi-KRUM and Median. 
