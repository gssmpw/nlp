\section{Background and Threat Model}
\label{sec:background}

This section introduces the fundamentals of federated learning, describes the threat of poisoning attacks with a focus on backdoor attacks, and defines the threat model considered in this work.

\subsection{Background}

\subsubsection{Federated Learning}

Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a global model while preserving the privacy of their local data, addressing regulatory and privacy concerns \citep{mcmahan2017communication}. The most widely used FL algorithm is Federated Averaging (\textbf{FedAvg}) \citep{mcmahan2017communication}, which operates iteratively through a series of communication rounds.  

At each round \(t\), the central server shares the global model parameters \(\mathbf{w}_t\) with a randomly selected subset of clients \(\mathcal{C}_t\). Each client \(c \in \mathcal{C}_t\) trains the model locally on its dataset \(\mathcal{D}_c\) and returns the updated parameters \(\mathbf{w}_c^t\). The server aggregates the updates to form the updated global model \(\mathbf{w}_{t+1}\):
\begin{equation}
    \mathbf{w}_{t+1} = \frac{1}{|\mathcal{C}_t|} \sum_{\substack{c \in \mathcal{C}_t}} \mathbf{w}_c^t.
\end{equation}
This process continues until the global model converges or a predefined number of rounds is completed.  

A typical FL system is parameterized by two categories of parameters: \textit{communication} parameters (\eg the number of clients \(C_t\) participating in each round \(t\)) and \textit{learning} parameters. Learning parameters, collectively referred to as the \textit{learning configuration}, define the local training process at each client and include the local \textit{learning rate}, the number of local training \textit{epochs}, and the \textit{batch size}. These parameters have a significant impact on the performance of the global model, which is typically measured using the \textit{Main Task Accuracy} (MTA)—the accuracy achieved by the global model on its intended learning task.  

%In this work, we focus on the interplay between these learning configurations and the robustness of the global model, particularly in the presence of adversarial clients. By understanding the role of learning configurations, we aim to evaluate and enhance the resilience of FL systems to adversarial manipulation.


\subsubsection{Poisoning Attacks}

%Poisoning attacks are a class of adversarial strategies that aim to compromise the integrity of a machine learning model by introducing malicious modifications during the training process. These attacks exploit the model's reliance on training data and updates by manipulating them to degrade model performance or embed hidden behaviors. In the context of FL, poisoning attacks are particularly dangerous due to the decentralized nature of training, where the server has no direct access to the raw data used by clients. This opacity provides adversaries with opportunities to inject malicious updates without immediate detection.

%A common form of poisoning attack is \textbf{data poisoning}, where the adversary modifies or labels their local training data to influence the model. For example, by injecting mislabeled or out-of-distribution examples into the training set, the adversary can degrade the global model's performance or steer its predictions toward specific, incorrect outputs. These attacks can range from untargeted strategies, which degrade the model's performance on all tasks, to targeted strategies, which focus on subverting the model for specific inputs or tasks while preserving overall performance.


%In FL, a broader category of attacks, referred to as \textbf{Byzantine attacks}, encompasses any behavior by clients that deviates from honest participation, whether intentional or due to faulty systems. Byzantine attacks can include data poisoning, malicious manipulation of gradients, or even complete misrepresentation of updates sent to the server. These attacks often aim to disrupt the global training process by exploiting the federated system's reliance on client-provided updates.

% Within the scope of poisoning attacks, a particularly insidious subcategory is the \textbf{backdoor attack}. In a backdoor attack, an adversary seeks to implant a hidden behavior or trigger into the global model. This trigger, often a specific pattern in the input, activates malicious behavior in the model (e.g., misclassification of the input to a target label) while maintaining high accuracy on legitimate tasks. In FL, this can be achieved by crafting poisoned local updates that subtly embed the backdoor during training rounds. Due to the decentralized nature of FL and the variability of client updates, backdoor attacks are challenging to detect, making them a severe threat to system integrity.

%This paper focuses on a specialized form of backdoor attack known as the \textbf{targeted backdoor attack}. Unlike traditional backdoor attacks that affect a wide range of inputs containing a trigger, targeted backdoor attacks aim to compromise the model’s behavior on a specific subpopulation of data, referred to as the \textit{victim class}. The adversary selectively poisons training data belonging to this victim class, ensuring that the backdoor is activated only for inputs that both belong to this subpopulation and contain the trigger. This nuanced attack design allows the adversary to embed a highly covert backdoor, which maintains overall model utility while surreptitiously targeting the chosen victim class. 

%By focusing on this specific type of targeted backdoor attack, this paper highlights the limitations of existing defenses and proposes a robust mechanism to mitigate such sophisticated adversarial strategies.

% FL offers a promising framework for collaborative learning, but its deployment in real-world systems presents several challenges. Among these is the threat of malicious clients, who can subtly manipulate their local updates to introduce hidden behaviors (e.g., backdoors) into the global model while preserving high accuracy on legitimate tasks. Defending against such adversaries is particularly challenging, as their updates often appear statistically similar to those of benign clients.  

Poisoning attacks involve either degrading overall performance or introducing additional training objectives to  embed selective malicious behavior. In the context of FL, the lack of direct access to clients' raw data makes it harder to defend against such poisoning attacks. This opacity enables adversaries to inject malicious updates while remaining undetected.

Data poisoning, a common attack type, involves adversaries manipulating local training data to influence the global model. This includes injecting mislabeled or out-of-distribution examples to degrade performance (\textbf{untargeted attacks} \citep{biggio2013poisoningattackssupportvector, jagielski_manipulating, Mei_Zhu_2015, xiao_feature_selection, shejwalkar2021manipulating, fangbyzantinerobust}) or subverting predictions for specific tasks (\textbf{targeted attacks} \citep{koh_black_box, shafahi_poison_frogs, suciu_fail}). A more covert form, the \textbf{backdoor attack} \citep{gu_badnets, bagdasaryan_howto, attacktails, sun2019reallybackdoorfederatedlearning}, implants hidden behaviors, causing the model to misclassify inputs with a predefined trigger while maintaining accuracy on legitimate data.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{assets/section3/backdoor_sample.pdf}
    \caption{Examples of CIFAR-10 images from the \textit{plane} class with an added backdoor trigger (top-left corner). The presence of the trigger causes the model to misclassify these inputs as the \textit{horse} class, illustrating the effect of a targeted backdoor attack.}
    \label{fig:backdoor_samples}
\end{figure}

In this work, we focus on \textbf{targeted backdoor attacks} \citep{targeted_backdoors}, where adversaries poison a specific subpopulation (victim class) with a hidden \emph{trigger}. The backdoor activates only for victim class inputs containing the trigger, remaining covert while exploiting client update variability and the decentralized nature of FL systems, making detection challenging. An example is illustrated in \Cref{fig:backdoor_samples}.