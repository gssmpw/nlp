\section{Conclusion}
\label{sec:conclusion}

Our study highlights the critical role that learning configurations and attacker strategies play in the success of targeted backdoor attacks in FL. Existing defenses often focus on narrow and fixed configurations, making it difficult to assess their robustness under realistic and diverse setups while the performance across different learning configurations (\ie local learning rate, batch size, training epochs) varies dramatically. We believe future evaluations should consider a broader range of configurations to better capture real-world deployment scenarios. Our results provide a starting point by identifying "danger zones"—configurations that enable stealthy attacks to succeed—and we encourage researchers to test their defenses across these configurations to better understand where defenses are effective and where they fail. This approach ensures that robustness claims are not limited to specific configurations but hold across diverse settings. We recommend future evaluations to include, among other configurations, \textbf{low learning rates and small batch sizes, as these have been shown to increase the stealthiness of backdoor attacks}.

To address this gap, we propose \textit{DROP}, a novel defense mechanism that is agnostic to learning configurations and the adversary's stealthiness. By integrating clustering, activity tracking, and knowledge distillation, DROP offers a comprehensive solution for mitigating targeted backdoor attacks, ensuring reliable protection across diverse FL setups. Our extensive evaluation across multiple datasets, varying data distributions, and a wide range of attacker configurations highlights DROP's robustness. Unlike existing baselines, which often fail under certain learning configurations or in the presence of stealthy attacks, DROP consistently achieves near-zero attack success rate despite a drop in main task accuracy.

% Anshuman: Commenting out for now based on Cristina's suggestion. I kinda agree here that we're not adding much with this text
\begin{comment}
\shortsection{Future Directions}
A key direction for future research is evaluating defenses under adaptive adversaries capable of evolving their attack strategies over time. Such adversaries could dynamically adjust their behavior based on observed defense mechanisms or leverage additional clean data for post-attack fine-tuning. \anshuman{Emphasize that adaptive adversaries are a broader idea for FL and we don't just mean adaptive attacks against our defense, but adaptive attacks against defenses in general in the context of FL (are there any such attacks?)} \georgios{Yes they do exist and some FL papers mention them and test against them. One very simple example is to have another Loss term which all it does is ensure that the euclidean distance between a benign and a malicious update is as low as possible. Of course, this would have taken running all these experiments that we have 2x. Maybe we can do it for the rebuttal?}\anshuman{I think we should plan some anticipatory adaptive attack experiments, and also think of ways we could improve non-iid (need not be 0 ASR, but at least better than others)}
To address the inconsistent assumptions across FL defense works—such as client sampling, static vs. adaptive adversaries, and non-IID data splits—that hinder fair comparisons and reproducibility, we advocate for a systematic study of threat models and evaluation protocols. A "Systematization of Knowledge" (SoK) could establish standardized benchmarks, fostering more consistent evaluations and enabling meaningful progress in the field. Additionally, future work could focus on mitigating the MTA loss through improved model-distillation techniques or more effective aggregation mechanisms.
\end{comment}

\shortsection{Limitations}
% While DROP demonstrates robust performance against targeted backdoor attacks, it has certain limitations. 
%First, DROP is evaluated only on image-based classification tasks, which constrains its applicability to other data modalities such as text or tabular data commonly used in real-world FL scenarios. However, it is worth noting that DROPlet, due to its domain-agnostic design, remains applicable across these modalities, making it a more versatile option for broader FL deployments.
% \anshuman{Mention that DROPlet could work there still} \georgios{Done.}.
Although DROP's additional compute overhead from the knowledge distillation step is manageable, it is still significant compared to lightweight baselines. However, this overhead is notably lower than client-side approaches like FLIP, which involve adversarial training. Second, despite the near-total mitigation of backdoor attacks, the reduction in MTA may not be acceptable in certain high-utility applications. However, it is worth noting that DROPlet, our more lightweight variant, offers a balance between robustness and accuracy, while being more computationally efficient than DROP.
Our results also reveal that defending against targeted backdoor attacks in complex learning tasks like CIFAR-10, particularly in highly non-IID settings, remains an open challenge. This highlights the need for further research into adaptive, configuration-independent defenses capable of handling evolving adversarial strategies.

By adopting an exhaustive evaluation methodology and releasing a comprehensive codebase%\footnote{\textit{The codebase has been attached to the submission.}}
, we hope to provide a testbed for future researchers to rigorously assess new defenses and foster advancements in the field. Our work lays the groundwork for designing robust, universal defense mechanisms that can generalize across diverse FL environments, contributing to the long-term security and reliability of federated learning systems.