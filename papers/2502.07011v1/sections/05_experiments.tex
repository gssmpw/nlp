\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments under a wide range of adversarial conditions, data distributions, and model architectures. Our evaluations focus on various learning configurations of interest, as discussed in  \cref{sec:defenses_fail_across_configs}. We also vary DPRs, MCRs, and use both independent and identically distributed (IID) and non-IID data distributions, ensuring a comprehensive assessment of DROP in realistic FL settings.

\subsection{Setup}

\shortsection{Datasets and Model Architecture} 
We evaluated our defense using three standard FL datasets: CIFAR-10, CINIC-10, and EMNIST.
CIFAR-10 \citep{krizhevsky2009learning} contains 60,000 32x32 color images across 10 diverse classes.
CINIC-10 \citep{darlow2018cinic} is an extension of CIFAR-10, incorporating additional images from ImageNet \citep{imagenet} to increase dataset size and variability. It consists of 270,000 images distributed across the same 10 classes as CIFAR-10, providing a more challenging classification task with greater intra-class diversity.  
EMNIST \citep{cohen2017emnistextensionmnisthandwritten} extends MNIST \citep{mnist} with 62 handwritten character and digit classes, making it suitable for FL classification tasks. We use a ResNet-18 architecture \citep{he2015deepresiduallearningimage} for all experiments.

\shortsection{Data Distribution} 
We evaluate our defense on IID and across various levels of non-IID data to reflect the heterogeneity typically observed in real-world FL deployments. To control the degree of non-IID-ness, we partition the global dataset using a Dirichlet distribution $Dir(\alpha)$ \citep{li2021federatedlearningnoniiddata} (see \Cref{sec:results}) and use the distributional parameter $\alpha$ to vary the degree of non-IID-ness.

\shortsection{Federation Settings} The federation consists of 50, 100, and 150 clients for CIFAR-10, EMNIST, and CINIC-10 respectively. 50\% clients are randomly selected to participate each training round for CIFAR-10 and EMNIST, while for CINIC-10 20\% clients are randomly selected per training round.
% At each round \(t\), a subset of $m$ clients \(\mathcal{C}_t \subseteq \mathcal{N}\) is randomly selected to participate in local training.
Unlike prior works \citep{blanchard2017machine,zhang2023flip} that assume a fixed number of adversarial clients in every round $t$, our model allows for the number of adversarial clients in \(\mathcal{C}_t\) to vary.

\begin{table*}[h]
    \centering
    \begin{tabular}{l|ccc|ccc|cc|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{6}{c}{CIFAR-10} & \multicolumn{4}{c}{CINIC-10} & \multicolumn{4}{c}{EMNIST} \\
     & \multicolumn{3}{c}{MTA (\%)} & \multicolumn{3}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} \\
     \cline{2-15}
     \multicolumn{1}{r|}{DPR (\%)} & 1.25 & 2.5 & 5 & 1.25 & 2.5 & 5 & 1.25 & 2.5 & 1.25 & 2.5 & 1.25 & 2.5 & 1.25 & 2.5 \\
    \midrule
    Undefended & 87.55 & 87.32 & 87.40 & 85.5 & 96.6 & 98.4 & 74.61 & 74.49 & 94.6 & 98.65 & 89.20 & 89.11 & 98.50 & 97.75 \\
    \hline
    FLAME & 84.86 & 85.00 & 85.03 & 88.7 & 96.8 & 99.4 & 73.16 & 72.77 & 96.63 & 98.6 & 87.83 & 88.0 & 5.0 & 12.75 \\
    FLARE & 85.06 & 81.08 & 84.41 & 92.5 & 75.8 & 3.5 & 72.18 & 71.09 &  96.71 & 93.02 & 89.07 & 88.92 & 96.00 & 84.50 \\
    FLIP & 83.09 & 81.53 & 82.88 & 53.4 & 73.4 & 48.80 & 70.08 & 69.60 & 80.98 & 38.82 & 88.53 & 88.32 & 46.50 & 48.75 \\
    FLTrust & 87.93 & 87.52 & 87.62 & 90.4 & 96.7 & 98.1 & 74.74 & 74.54 & 90.83 & 98.72 & 89.19 & 89.18 & 97.0 & 90.0 \\
    Fool's Gold & 83.85 & 83.76 & 83.92 & 83.0 & 92.6 & 94.3 & 70.05 & 69.96 & 89.33 & 95.29 & 87.85 & 87.80 & 77.0 & 46.50 \\
    Median & 87.48 & 87.33 & 87.21 & 95.3 & 96.3 & 98.8 & 73.49 & 73.38 & 97.6 & 98.87 & 89.20 & 89.20 & 96.75 & 96.25 \\
    Multi-KRUM & 85.96 & 86.00 & 85.87 & 86.0 & 97.5 & 98.6 & 73.57 & 73.58 & 76.35 & 99.74 & 88.70 & 88.58 & 78.75 & 5.0 \\
    \hline
    DROPlet (ours) & 87.49 & 87.05 & 87.20 & 91.3 & \textbf{0.1} & 0.9 & 74.05 & 74.27 & 15.96 & \textbf{0.48} & 88.98 & 88.96 & 1.50 & 1.00 \\
    DROP (ours) & 76.05 & 76.53 & 75.18 & \textbf{1.3} & 1.1 & \textbf{0.5} & 65.65 & 65.72 & \textbf{1.22} & 1.44 & 88.62 & 89.13 & \textbf{0.75} & \textbf{1.00} \\
    \bottomrule
    \end{tabular}
    % \caption{MTA (\%) and ASR (\%) metrics at different Data Poisoning Rates (DPR) for configuration C4 for CIFAR-10 and C5 for EMNIST. The MCR is fixed at 20\%. DROP achieves low ASR for all DPR, while existing defenses fail to mitigate the attack. The lightweight variant DROPlet achieves higher MTA than DROP and is robust in all cases, except for extremely stealthy attacks (1.25\% DPR).
    % % Results for FLIP are shown only for 1.25\% and 2.5\% DPR due to its extreme computational overhead, as the method requires adversarial training for each client at every round.
    % }
    \caption{MTA (\%) and ASR (\%) metrics at different Data Poisoning Rates (DPR) for the CIFAR-10, CINC-10 and EMNIST datasets. The MCR is fixed at 20\%. DROP achieves low ASR for all DPR, while existing defenses fail to mitigate the attack. The lightweight variant DROPlet achieves higher MTA than DROP and is robust in all cases, except for some extremely stealthy attacks (1.25\% DPR, CIFAR-10). The learning configurations are C4 for CIFAR-10 (same for CINIC-10) and C5 for EMNIST. Refer to \cref{tab:fl_setup_exps} and \cref{tab:fl_setup_exps_emnist} for learning configuration details.   
    % Results for FLIP are shown only for 1.25\% and 2.5\% DPR due to its extreme computational overhead, as the method requires adversarial training for each client at every round.
    }
    \label{tab:fl_vary_dpr}
\end{table*}

\shortsection{Attacks} We evaluate our defense against a targeted backdoor attack (described in \Cref{sec:threat_model}), where adversarial clients poison inputs from a specific victim class \(y_{\text{victim}}\) to embed malicious behavior in the global model. The adversary applies a trigger \(\mathbf{t}\) to a fraction of victim-class inputs and flips their labels to a target class \(y_{\text{target}}\). Attack success is measured by the \textit{Attack Success Rate (ASR)}, while maintaining high \textit{Main Task Accuracy (MTA)} ensures stealthiness. We evaluate robustness by varying the \textbf{DPR} at 1.25\%, 2.5\%, and 5\%, and the \textbf{MCR} at 20\% and 40\%, to simulate a range of  adversarial compromise.

\shortsection{Baselines} To benchmark the effectiveness of DROP, we compare its performance against representative FL poisoning defense mechanisms from the literature. Median Aggregation \citep{yin2018byzantine} is a classical robust aggregation technique that uses the coordinate-wise median of client updates to neutralize the effect of outliers. Multi-Krum \citep{blanchard2017machine} selects and aggregates the most "trustworthy" client updates by iteratively excluding updates that are furthest from the majority, making it resilient to Byzantine failures. FLTrust \citep{cao2021fltrust} introduces a server-side reference model to measure the "trustworthiness" of client updates, ensuring that only updates aligning with the reference model are included in the aggregation. FoolsGold \citep{fung2018mitigating} employs similarity-based clustering to mitigate the impact of poisoned updates by reducing their contribution in the aggregation process. FLAME \citep{nguyen2022flame} is a robust aggregation method designed to address both Byzantine and poisoning attacks in FL by selectively penalizing anomalous updates. FLIP \citep{zhang2023flip} estimates client-side trigger reconstruction using adversarial training combined with low-confidence refusals. Finally, FLARE \citep{wang2022flare} leverages penultimate layer representations (PLR) to evaluate the trustworthiness of model updates, assigning trust scores based on PLR discrepancies and aggregating updates weighted by these scores to defend against model poisoning attacks.
%
For further implementation details on the baselines, please see \Cref{app:baseline_details}.

\subsection{Results}
\label{sec:results}
Our evaluation reveals the strengths and weaknesses of existing defenses and demonstrates the superior robustness of our proposed defense, DROP, across various attack scenarios.

\subsubsection{Robustness Across FL Learning Configurations}

A key strength of DROP is its robustness across diverse FL learning configurations, ensuring reliable protection against targeted backdoor attacks independent of the specific learning parameter settings. While other defenses exhibit significant performance variability depending on the enforced learning parameters (\Cref{fig:baselines_heatmap}), DROP consistently maintains near-zero ASR across all configurations of interest, as shown in \Cref{tab:drop_across_configs}. This consistency demonstrates DROP’s adaptability and makes it a learning configuration agnostic defense, capable of handling varying client learning rates, batch sizes, and local training epochs without compromising on security. This robustness is particularly important for real-world FL deployments, where learning configurations can vary dynamically across tasks. By ensuring reliable performance across a wide range of setups, DROP offers practical, consistent protection against adversarial threats, making it well-suited for heterogeneous FL environments.

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{lcc|lcc}
    \toprule
    \textbf{Config} & \textbf{MTA (\%)} & \textbf{ASR (\%)} & \textbf{Config} & \textbf{MTA (\%)} & \textbf{ASR (\%)} \\
    \midrule
    C1 & 69.47 & 1.8 & C6 & 62.61 & 1.6\\
    C2 & 71.77 & 0.7 & C7 & 66.17 & 1.7\\
    C3 & 80.20 & 2.8 & C8 & 64.71 & 1.6\\ 
    C4 & 76.05 & 1.3 & C9 & 80.44 & 0.8\\
    C5 & 75.24 & 1.5 & C10 & 80.50 & 5.5\\
    \bottomrule
    \end{tabular}
    \caption{MTA (\%) and ASR (\%) for DROP on CIFAR-10 for 1.25\% DPR and 20\% MCR, across all 10 configurations identified in \Cref{sec:defenses_fail_across_configs}. DROP achieves low ASR {\bf across all configurations}.}
    \label{tab:drop_across_configs}
\end{table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.97\linewidth]{assets/section5/across_rounds.pdf}
    \caption{MTA (a) and ASR (b) across rounds for various defenses, for CIFAR-10 with 1.25\% DPR and 20\% MCR for configuration C4. Certain defenses like FLIP and FLAME have wildly fluctuating ASR across rounds, making them unreliable. DROP instead achieves consistently low ASR in all rounds.}
    \label{fig:multiple_round_progression}
\end{figure*}

\shortsection{Consistency Across Rounds} A robust defense must ensure low ASR during rounds where the MTA remains acceptable, preventing randomness in ASR fluctuations from determining the outcome of FL training. While some baselines (\eg FLIP, FLAME) initially achieve low ASR, their performance deteriorates over subsequent rounds, resulting in inconsistent suppression of the attack. This variability is problematic because in a real-world setting, FL training might terminate at any round, and reliance on transient phases of low ASR does not guarantee reliable protection. In \Cref{fig:multiple_round_progression}, we compare the performance of various defensive baselines with DROP over 200 rounds. Although some baselines degrade more quickly (\eg Median, FLTrust), others (\eg FLIP, FLAME) exhibit slower degradation, yet suffer from significant ASR fluctuations across rounds. This inconsistency makes it difficult to guarantee robust defense throughout the training process. In contrast, DROP consistently maintains near-zero ASR across all rounds, offering reliable protection regardless of when training might terminate. As described in the Property 2 (\Cref{sec:threat_model}), \textbf{\textit{this stability ensures that practitioners do not have to depend on chance for effective defense, making DROP a practical and dependable solution for real-world FL systems.}}

\subsubsection{Adaptability to Varying Attack Stealthiness}
Varying DPR simulates different levels of attack stealthiness, while varying MCR represents scenarios where the fraction of malicious clients in the federation changes.


\shortsection{Varying DPR}  
By varying the attacker's DPR, we simulate increasingly stealthy targeted backdoor attacks, with DPR as low as 1.25\%. We observe that all baseline defenses fail to defend against targeted backdoors across CIFAR-10, CINIC-10, and EMNIST, regardless of attack stealthiness (\Cref{tab:fl_vary_dpr}).  
Some defenses perform adequately only when the attack is overt. For instance, for CIFAR-10, FLARE mitigates the attack effectively at 5\% DPR, achieving 3.5\% ASR, but fails entirely when the DPR is reduced (\eg 2.5\%). Similarly, for CINIC-10, FLIP demonstrates moderate robustness at 2.5\% DPR, achieving 38\% ASR, but succumbs to the attack at 1.25\% DPR. In contrast, DROP consistently defends against targeted backdoor attacks across all levels of DPR, maintaining near-zero ASR (1.5\%, 1.2\%, and 0.5\%) in all configurations.

\begin{table*}[h]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{4}{c}{CIFAR-10} & \multicolumn{4}{c}{CINIC-10} & \multicolumn{4}{c}{EMNIST} \\
     & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} \\
     \cline{2-13}
     \multicolumn{1}{r|}{MCR (\%)} & 20 & 40 & 20 & 40 & 20 & 40 & 20 & 40 & 20 & 40 & 20 & 40 \\
    \midrule
    Undefended & 86.56 & 86.05 & 92.1 & 99.6 & 74.49 & 74.47 & 98.65 & 99.6 & 87.78 & 87.65 & 94.75 & 100.0 \\
    \hline
    FLAME & 80.78 & 81.11 & 91.3 & 99.8 & 72.77 & 72.62 & 98.6 & 99.32 & 86.43 & 86.71 & 2.50 & 89.50 \\
    FLARE & 83.28 & 83.74 & 58.3 & 98.9 & 71.09 & 71.65 & 93.02 & 97.27 & 86.72 & 86.52 & 27.50 & 96.50 \\
    FLIP & 82.25 & 83.43 & 11.2 & 72.3 & 69.6 & 71.65 & 38.82 & 92.94 & 87.48 & 87.54 & 12.25 & 84.75 \\
    FLTrust & 87.38 & 87.24 & 99.5 & 99.4 & 74.54 & 74.65 & 98.72 & 99.73 & 88.47 & 88.68 & 78.75 & 99.50 \\
    Fool's Gold & 83.80 & 83.73 & 86.2 & 99.7 & 69.96 & 69.93 & 95.29 & 98.77 & 85.91 & 85.82 & 25.50 & 96.75 \\
    Median & 86.10 & 85.54 & 98.9 & 99.1 & 73.38 & 73.26 & 98.87 & 99.69 & 87.66 & 87.71 & 94.25 & 99.75 \\
    Multi-KRUM & 84.41 & 84.04 & 99.2 & 99.8 & 73.58 & 73.58 & 99.74 & 99.55 & 87.13 & 87.20 & 1.00 & 99.25 \\
    \hline
    DROPlet (ours) & 85.49 & 84.87 & \textbf{0.9} & \textbf{0.8} & 74.27 & 74.65 & \textbf{0.48} & 99.73 & 87.66 & 87.37 & 1.50 & 0.75 \\
    DROP (ours) & 69.11 & 70.71 & 1.3 & 1.4 & 65.72 & 64.97 & 1.22 & \textbf{2.36} & 88.67 & 88.43 & \textbf{0.75} & \textbf{0.75} \\
    \bottomrule
    \end{tabular}
     \caption{MTA (\%) and ASR (\%) metrics at different Malicious Client Ratios (MCR) for CIFAR-10, CINIC-10 and EMNIST datasets. The DPR is fixed at 2.5\%. Existing defenses fail to prevent the attack, particularly at 40\% MCR. DROP is resilient in all cases, despite some reduction in MTA. DROPlet performs adequately in most cases independently of the MCR. The learning configurations are C1 for CIFAR-10 (C4 for CINIC-10) and C9 for EMNIST. Refer to \cref{tab:fl_setup_exps} and \cref{tab:fl_setup_exps_emnist} for learning configuration details.}
    \label{tab:fl_vary_mcr}
\end{table*}


\shortsection{Varying MCR}  
By varying the MCR, we simulate scenarios where the attacker controls a smaller (20\%) or larger (40\%) fraction of clients in the federation. A high MCR, such as 40\%, significantly increases the likelihood that malicious clients will outnumber benign ones in certain FL rounds due to the random client selection process. This imbalance enables adversarial updates to dominate the aggregation process, amplifying the attack's effectiveness and exposing vulnerabilities in clustering-based defenses. Once again, we observe that most defensive baselines fail to provide adequate protection against the attack under such conditions (\Cref{tab:fl_vary_mcr}). For instance, FLIP reduces the ASR to 11.2\% and 38.82\% for 20\% MCR on CIFAR-10 and CINIC-10, but fails to mitigate the attack at 40\% MCR. FLAME, due to its overly aggressive adaptive clipping scheme, struggles to achieve convergence. Both FLAME and Multi-Krum achieve resilience for 20\% MCR on EMNIST, but do not thwart the attack at 40\% MCR. In contrast, DROP consistently demonstrates resilience across both 20\% and 40\% MCR setups, achieving near-zero ASR while, highlighting its robustness even in highly adversarial settings.

\subsubsection{Robustness - Accuracy Tradeoff} While DROP demonstrates strong effectiveness against backdoor attacks, it comes with a notable trade-off in MTA compared to some baselines. For example, in \Cref{tab:fl_vary_dpr} with a DPR of 1.25\%, DROP achieves an MTA of 74.74\%, which is lower than FLTrust (87.93\%) and Median (87.14\%). Similarly, in \Cref{tab:fl_vary_mcr} with an MCR of 20\%, DROP records an MTA of 68.74\%, compared to 86.88\% for FLTrust. This reduction in accuracy primarily stems from the strict distillation-based cleansing mechanism, which aims to ensure near-total suppression of adversarial influences. Notably, unlike clustering or aggregation-only defenses, DROP applies knowledge distillation by using a model stealing technique (i.e., MAZE), which contributes significantly to the observed MTA drop while exhibiting strong robustness. The trade-off between robustness and accuracy is a well-documented challenge in adversarial machine learning \citep{tsipras2019robustnessoddsaccuracy}.

It is worth noting that one could focus on configurations such as C3, C9, or C10, where we achieve $\sim 80\%$ MTA and claim a smaller drop in MTA. While it would indeed be preferable for a practical FL system to utilize such learning configurations, the ability to select configurations without sacrificing protection is a unique strength of our defense.  By prioritizing security, DROP is a more reliable choice for high-stakes applications where even a minor vulnerability could lead to significant consequences.

It is also worth highlighting that advancements in knowledge distillation techniques and model-stealing attacks could directly enhance DROP. Improved versions of these methods can serve as drop-in replacements for the current MAZE component, potentially reducing the accuracy trade-off while preserving DROP’s robust defensive capabilities.

\shortsection{DROPlet - Lightweight Variant of DROP}\label{shortsec:droplet_results}
DROPlet (\Cref{subsec:droplet}) is designed for environments where computational efficiency is a priority. Unlike DROP, DROPlet omits the knowledge distillation component, thereby eliminating the associated computational overhead. This makes it particularly suitable for FL deployments with limited resources or scenarios where minimizing server-side processing time is critical. Notably, execution time for a single non-IID run of DROPlet is comparable to other baselines, completing in approximately 30 minutes, whereas DROP takes 2 hours, and FLIP requires an extensive 57 hours for the same setup. 

While DROPlet may not perform as well as DROP against \textit{certain} highly stealthy, low-DPR attacks (e.g., CIFAR-10, 1.25\% DPR in \Cref{tab:fl_vary_dpr}) or high-MCR attacks (e.g., CINIC-10, 40\% MCR in \cref{tab:fl_vary_mcr}), it still provides strong protection in many practical configurations, achieving near-zero ASR in less covert attack scenarios and maintaining robustness across a broad range of learning setups (\cref{tab:fl_vary_dpr} and \cref{tab:fl_vary_mcr}). For simpler or moderately stealthy attack scenarios with substantial adversarial presence within the federation, DROPlet performs comparably to DROP without the additional computational cost incurred by the knowledge distillation step.

However, it is important to note that an adaptive adversary aware of this defense could lower its DPR to evade detection, exploiting the absence of the distillation component to bypass DROPlet’s defenses. This limitation makes DROPlet less suitable for environments where adversaries are likely to adopt highly stealthy strategies. Nevertheless, its lightweight design offers significant practical value as a plugin to existing FL defenses, requiring minimal code modifications while enhancing robustness without incurring substantial overhead. By striking a balance between computational efficiency and robustness, DROPlet provides a viable defense solution for resource-constrained FL settings.

\begin{table*}[th]
    \centering
    \begin{tabular}{l|cccc|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{8}{c}{CIFAR-10} & \multicolumn{8}{c}{EMNIST} \\
     & \multicolumn{4}{c}{MTA (\%)} & \multicolumn{4}{c}{ASR (\%)} & \multicolumn{4}{c}{MTA (\%)} & \multicolumn{4}{c}{ASR (\%)} \\
     \cline{2-17}
     \multicolumn{1}{r|}{$Dir(\alpha)$} & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ \\
    \midrule
    Undefended & 86.30 & 87.32 & 87.07 & 87.55 & 94.7 & 86.3 & 95.3 & 85.5 & 87.29 & 87.72 & 87.53 & 87.78 & 93.00 & 94.50 & 93.50 & 94.75  \\
    \hline
    FLAME & 59.43 & \textcolor{red}{10.00} & 82.34 & 84.86 & \textbf{0.4} & \textcolor{red}{0.0} & 31.5 & 88.7 & 85.91 & 86.48 & 85.54 & 85.86 & \textbf{13.5} & 56.75 & 3.0 & 3.25  \\
    FLARE & 83.81 & 81.37 & 82.55 & 85.06 & 48.3 & 59.9 & \textbf{15.4} & 94.0 & 86.48 & 86.24 & 86.57 & 86.86 & 84.0 & 28.25 & 3.75 & 63.5 \\
    FLIP & 78.50 & 81.90 & 83.1 & 83.09 & 54.0 & \textbf{25.4} & 47.8 & 53.4 & 87.36 & N/A & N/A & 88.52 & 14.75 & N/A & N/A & 46.50 \\
    FLTrust & 86.40 & 86.84 & 85.34 & 87.93 & 99.2 & 97.4 & 76.6 & 90.4 & 88.21 & 88.43 & 88.36 & 89.34 & 92.5 & 94.5 & 95.75 & 97.75 \\
    Fool's Gold & 80.00 & 82.93 & 83.95 & 83.85 & 79.7 & 73.1 & 87.0 & 83.0 & 85.51 & 85.76 & 85.70 & 86.71 & 28.25 & 20.75 & 17.50 & 41.75 \\
    Median & 86.32 & 86.31 & 87.10 & 87.48 & 97.3 & 93.5 & 98.3 & 95.3 & 87.24 & 87.61 & 87.51 & 87.72 & 92.0 & 91.5 & 91.5 & 97.0 \\
    Multi-KRUM & 82.74 & 84.56 & 85.15 & 85.96 & 97.8 & 98.2 & 73.9 & 86.0 & 86.49 & 86.75 & 86.84 & 87.11 & 80.75 & 16.75 & 2.5 & 1.0 \\
    \hline
    DROPlet (ours) & 86.62 & 86.08 & 86.99 & 87.49 & 94.5 & 94.4 & 96.1 & 91.3 & 87.14 & 87.51 & 87.74 & 87.66 & 93.0 & 91.75 & 86.0 & 1.5 \\
    DROP (ours) & 69.28 & 73.05 & 72.42 & 76.05 & 88.7 & 93.0 & 80.6 & \textbf{1.3} & 87.66 & 84.48 & 84.16 & 88.57 & 16.50 & \textbf{4.75} & \textbf{1.75} & \textbf{0.75} \\
    \bottomrule
    \end{tabular}
    \caption{MTA and ASR across rounds for various defenses for non-IID data with varying $Dir(\alpha)$, for 1.25\% DPR and 20\% MCR, using configuration C4 for CIFAR-10  and C9 for EMNIST. FLAME fluctuates wildly and often ends up with models that have not learned anything effectively, leading to random-guess MTA (\textcolor{red}{red}) and consequently, zero ASR. Results for FLIP are shown only for $Dir(\alpha) = 1$ due to its extreme computational overhead, as the method requires adversarial training for each client at every round. As observed, DROP is the most resilient defense on EMNIST and all  defenses  struggle  on the more complex CIFAR-10 learning task.
    %On our equipment (NVIDIA GeForce RTX 4090), the run took 57 hours(!)
    }
    \label{tab:noniid}
\end{table*}


\subsubsection{Hardness Under Non-IID Settings}
The non-IID nature of client datasets introduces significant challenges for defending against targeted backdoor attacks, as data distributions among clients can be highly imbalanced. In such settings, the imbalance and diversity of client data make it difficult to distinguish whether differences in model updates arise from the non-IID nature of the data or from benign versus malicious clients. This makes it difficult for clustering-based defenses to differentiate between malicious and benign updates, thereby increasing the stealthiness of the attack. Consequently, defenses that perform well under IID conditions often exhibit reduced effectiveness when faced with non-IID data, underscoring the need for adaptive approaches capable of handling diverse and heterogeneous client data.

\Cref{tab:noniid} presents the results of evaluating various defenses under increasingly non-IID conditions for CIFAR-10 and EMNIST. While both datasets are commonly used in FL research, CIFAR-10 poses a greater challenge due to its higher intra-class diversity, which allows malicious updates to blend more seamlessly with legitimate ones, making it harder to detect and defend against backdoor attacks. This complexity is reflected in the relatively poor performance of most defenses on CIFAR-10 under non-IID conditions. FLIP for instance, while reducing ASR compared to most other defenses, still suffers from significant fluctuations in ASR, sometimes exceeding 60\% under non-IID conditions. This variability highlights the challenge of achieving reliable defense in high-stakes applications where consistency is crucial. 

The results for DROP, however, reveal a nuanced performance. For EMNIST, DROP maintains near-zero ASR across a range of $\alpha$ values, with a slight increase to 16.5\% at the most skewed setting ($\alpha=1$). This demonstrates DROP’s strong resilience in handling less complex, smaller-scale datasets. On the other hand, for CIFAR-10, DROP struggles, with ASR fluctuations persisting even under relatively moderate non-IID conditions. We believe this performance gap may stem from the inherent difficulty of the CIFAR-10 task, which, due to its high class diversity and the complex nature of the data, challenges any defense’s ability to effectively distinguish between benign and adversarial updates.

While our defense performs well in many non-IID scenarios, the fluctuating ASR observed in more complex tasks like CIFAR-10 underscores the difficulty of defending against backdoor attacks under such conditions. These results suggest that \textbf{more research is needed to develop defenses capable of better identifying malicious updates in the presence of significant data heterogeneity}.
