\section{Defenses Falter Against Targeted Backdoors}
\label{sec:existing_finicky}

Several works have proposed defenses against poisoning attacks in FL \citep{zhang2023flip, nguyen2022flame, blanchard2017machine, yin2018byzantine,fung2018mitigating,cao2021fltrust,wang2022flare,pillutla2022robust,shejwalkar2021manipulating}. It is standard for such works to focus on a specific learning configuration, for instance setting a specific learning rate and batch size for clients. While demonstrating superior performance in a particular setup is useful, it provides no guarantees about how well the defense would work in other valid learning configurations. We begin with an exploration of some of these learning configurations (\Cref{sec:fl_setup_matters}) and observe that there exist several equally-valid learning configurations where the model achieves acceptable MTA and the adversary's objective is preserved, thus making them all equally valid FL configurations. However, we find that all existing defenses we evaluated are effective only for a subset of these valid configurations and no defense is resilient across all configurations (\Cref{sec:defenses_fail_across_configs}).

\subsection{Learning Configuration Matters for Attack Success}
\label{sec:fl_setup_matters}

To understand the impact of the learning configuration on model robustness and adversarial susceptibility, we conduct a grid-search analysis over key hyperparameters, varying the client's learning rate, batch size, and number of epochs on the CIFAR-10 dataset.
Our findings in \Cref{fig:fl_setup_impact} reveal that small changes in the learning configuration can significantly alter the model’s vulnerability to backdoor attacks.
\begin{figure}[h!]
    \includegraphics[width=.98\linewidth]{assets/section4/undefended_asr.pdf}
    \caption{Visualizing the impact of the learning configuration (learning rate, batch size, and number of local epochs) on ASR, for 1.25\% DPR and 20\% MCR, for CIFAR-10 with IID data. We only visualize configurations with MTA $\geq80\%$. The attack is successful on multiple configurations (in yellow).}
    \label{fig:fl_setup_impact}
\end{figure}
% Certain configurations produce extreme outcomes, such as low MTA or low ASR, but more interestingly, many configurations exhibit simultaneously high MTA and high ASR. This combination indicates a "\textbf{danger zone}" of configurations that yield high model utility while remaining highly vulnerable to adversarial manipulation.
% \anshuman{Could compress below further to be 3-4 lines each at most.} \georgios{Just did. Check it out.}

%One particularly concerning observation is that low learning rates (e.g., 0.01) and small batch sizes (e.g. 64) create conditions where targeted backdoor attacks become extremely stealthy. In this regime, adversarial updates remain close to benign updates, making it difficult for anomaly-based defenses to detect them. This also allows the global model to maintain high MTA while the attack achieves a high ASR.Importantly, this phenomenon arises even in the absence of adversarial manipulation, as natural variability in client updates creates overlapping patterns between benign and adversarial updates. Given the natural variability, it is difficult to distinguish benign updates from adversarial updates, making the FL setup itself a critical factor in system vulnerability.

\subsubsection{Impact of learning rate}

%In general, learning configurations with a lower learning rate (e.g. 0.01) generally yield local updates that are smoother with respect to the client's local data and also consistently yield results with high MTA and ASR. On the other hand, higher learning rates (e.g. 0.1) lead to local updates that when aggregated cause performance degradation both on the MTA and the ASR. \georgios{A counter for 0.1 lr instability would be adopting a moving average aggregation for server aggregation.}

%Lower learning rates (\eg 0.01) produce local updates that are smoother and more possibly aligned with the client's local data distribution. This characteristic not only enhances the MTA but also enables the adversary to inject backdoors with minimal deviation from benign updates, resulting in consistently high ASR. By contrast, higher learning rates (\eg 0.1) lead to noisier local updates, which, when aggregated at the server, can cause performance degradation on both MTA and ASR. This instability at higher learning rates poses challenges for backdoor attacks but also negatively impacts the utility of the global model.

Lower learning rates (\eg 0.01, 0.025) allow for smaller, more gradual updates to the model parameters, making it easier for adversarial objectives to be embedded into the global model with minimal deviation from benign updates, resulting in a high ASR. In contrast, higher learning rates (\eg 0.1) cause larger, less stable updates that disrupt the optimization process, leading to degraded performance, with large drops in MTA and ASR alike.

\subsubsection{Impact of batch size}

%A larger batch size enables setups with different learning rates to achieve high MTA and ASR. On the other hand, a smaller batch size is making it more difficult to inject a targeted backdoor and only very low learning rates (e.g. 0.01) can successfully inject the backdoor.

% Batch size has a significant impact on both MTA and ASR.

% Larger batch sizes (\eg 256) reduce the variability in local updates, creating conditions where high MTA and ASR are achievable across a range of learning rates. This makes large batch sizes particularly susceptible to backdoor attacks, as adversarial updates blend seamlessly with benign updates. Conversely, smaller batch sizes (\eg 64) increase the variability in client updates, making it more difficult for the adversary to inject a targeted backdoor. In such setups, only very low learning rates (\eg 0.01) succeed in achieving a high ASR, as they produce smoother adversarial updates that can evade detection despite the higher noise introduced by small batch sizes.

Larger batch sizes (\eg 256) reduce variability in local updates, enabling high MTA and ASR across learning rates, making them more vulnerable to backdoor attacks. Smaller batch sizes (\eg 64) increase update variability, requiring very low learning rates (\eg 0.01) for effective backdoor injection.


\subsubsection{Impact of training epochs}

%Configurations with a small number of local training epochs (e.g. 2, 5) seem to be the ones that are more susceptible to backdoor poisoning. In general, the larger the number of local training epochs the clients use on locally training their models, the harder it is for the adversary to inject a targeted backdoor. 

% The number of local training epochs is another critical factor influencing the success of targeted backdoor attacks. Fewer local epochs (\eg 2 or 5) create conditions where clients submit updates that reflect only shallow optimization on their local data. This reduces the distinction between benign and adversarial updates, making the system more susceptible to backdoor poisoning. In contrast, larger numbers of local epochs (\eg 10 or 20) allow clients to perform more thorough local optimization, amplifying the differences between benign and adversarial updates. This makes it harder for adversarial updates to successfully inject backdoors while maintaining high MTA.

Fewer local epochs (\eg 2 or 5) lead to shallow optimization, reducing distinctions between benign and adversarial updates, making backdoor attacks more effective. Training with more local epochs (\eg 10 or 20) induces a \textit{polarization} effect, where clients' local models become more tightly aligned with their respective datasets. This stronger alignment reduces the influence of malicious updates during aggregation, as the backdoor signal becomes diluted and less effective at propagating into the global model. This polarization is even stronger when high LRs are used in conjunction with more training epochs, as model's MTA also suffers.

\Cref{tab:fl_setup_exps} highlights a range of configurations that we term the "\textbf{danger zones}"—settings where the FL system achieves both high MTA (above 80\%) and adequately high ASR (above 85\%). These configurations are of great interest because they strike a balance between utility and vulnerability; accurate for legitimate tasks while remaining highly susceptible to targeted backdoor attacks. This highlights the critical need for a defense to be resilient across multiple learning configurations, ensuring robustness regardless of the setup and achieving true \textit{learning configuration independence}. This is particularly desirable because relying on a single configuration, even if effective, is not always feasible in practice. Various constraints, such as the batch size supported by a specific device, the number of epochs a client is willing to commit to, or other resource limitations, can dictate configurations in practice. Therefore, a defense mechanism that can adapt to different setups without compromising its efficacy is essential for practical and widespread deployment.

%To demonstrate the impact of FL setup, we conducted a series of grid-search experiments using the CIFAR-10 dataset in an iid client data setting, varying the learning rate, batch size, and other hyperparameters. In the absence of any defense, we found that certain configurations led to significantly higher ASR and lower MTA, while others resulted in low ASR but at the cost of degraded MTA. Crucially, some configurations produced both high ASR and high MTA, highlighting the existence of "danger zones" where targeted backdoor attacks are especially effective. A key finding is that low learning rates (e.g., 0.01) and small batch sizes create conditions where the attack becomes exceptionally stealthy. In this regime, the attack remains covert, preserving high MTA while maintaining a high ASR, which poses a significant challenge for defense strategies. This observation reveals that even without explicit adversarial intervention, some FL configurations are inherently vulnerable. \georgios{"inherently" vulnerable is a bold statement, maybe we should opt for a different term?}

% \todo{Start with a grid-search style result for experiments without any defense to show how some configurations lead to very bad MTA and/or ASR but more importantly, there are several successful candidates. Give general rule-of-thumb for configs to use in evaluations}

\subsection{Limitations of Existing Defenses}
\label{sec:defenses_fail_across_configs}

Based on our analysis, we identify 10 learning configurations where the attack is stealthy (minimal impact on MTA) and highly successful (has ASR higher than 85\%), as given in \Cref{tab:fl_setup_exps}. For our evaluation we consider key, prominent defense methods which offer diverse strategies to combat backdoor attacks in FL. These defense strategies include coordinate-based approaches like Median \citep{yin2018byzantine} and Multi-Krum \citep{blanchard2017machine}, trust-based methods such as FLTrust \citep{cao2021fltrust}, reputation-based schemes like FoolsGold \citep{fung2018mitigating} and FLARE \citep{wang2022flare}, anomaly detection frameworks like FLAME \citep{nguyen2022flame} and local adversarial training methods like FLIP \citep{zhang2023flip}. For more details on these defenses and other related works, see \Cref{sec:experiments} and \Cref{sec:related_work}.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{llcc|cc}
    \toprule
    \textbf{Config} & \textbf{LR} & \textbf{BS} & \textbf{Epochs} & \textbf{MTA (\%)} & \textbf{ASR (\%)} \\
    \midrule
    C1 & 0.05 & 128 & 2 & 85.08 & 96.5 \\
    C2 & 0.05 & 256 & 2 & 86.29 & 95.8 \\
    C3 & 0.025 & 256 & 5 & 88.33 & 94.8 \\
    C4 & 0.01 & 64 & 2 & 87.03 & 93.9  \\
    C5 & 0.025 & 128 & 2 & 86.65 & 92.9 \\
    C6 & 0.025 & 256 & 2 & 82.97 & 91.3 \\
    C7 & 0.1 & 256 & 2 & 85.55 & 91.1 \\
    C8 & 0.01 & 128 & 2 & 84.47 & 89.7 \\ 
    C9 & 0.01 & 128 & 5 & 89.04 & 86.6\\
    C10 & 0.01 & 256 & 10 & 87.97 & 85.6 \\
    \bottomrule
    \end{tabular}
    \caption{Client FL configurations for successful stealthy attacks on CIFAR-10 \ie cases with MTA $\geq 80\%$ and ASR $\geq 85\%$. }
    \label{tab:fl_setup_exps}
\end{table}
%Each of these methods aims to limit the influence of malicious updates during aggregation. For example, Median evaluate their defense on a single, simpler learning task --MNIST \citep{} and only mention the total number of clients that participate in the federation. The authors do not mention anything related to the client learning setup (lr, batch size or number of local training epochs). Multi-Krum attempt to reduce the impact of outliers by using robust statistics. In their paper the authors again only mention how the data is partitioned amongst clients without any significant mention of the client local learning setup. They do however evaluate their method on various different batch sizes. FLTrust relies on a trusted server-side reference model to filter out anomalous updates. The authors report the use of a 'combined' (i.e. the product of the server and local lr) learning rate of 0.002, a batch size of 64 and 1 as the total number of local training epochs. FoolsGold tracks client updates to identify and penalize suspiciously similar contributions. They do not explicitly report a learning rate and the number of local training epochs for their evaluation setup. They report a batch size of 10 and 50, depending on the dataset. FLAME leverages anomaly detection to flag potentially malicious gradients. They do not mention anything about the local learning setup apart from how the federation is structured. 
% \anshuman{Too long; it's nice to have these details mentioned for sure, but should not overlead reader before we get to our main results. Can give a 1-2 sentence summary here} \georgios{What do you mean by too long?? Are you talking about the following section in which I describe the differences per setup?}\anshuman{I mean the defense-wise discussion that follows this comment, about exactly which settings previous defenses considered. We can give an example or two and give a blanket statement about a lack of consistency in evaluation setup (and explain them in detail somewhere in the appendix)} \georgios{Understood.}
% MOVE TO APPENDIX.
%Each of these methods aims to limit the influence of malicious updates during aggregation. However, most works provide limited or inconsistent details about their evaluation setups, particularly concerning client learning configurations such as learning rate, batch size, and the number of local training epochs. For instance, Median is evaluated on a simpler learning task (MNIST) and specifies only the total number of participating clients. The authors provide no details about the client learning setup, including learning rate, batch size, or number of local training epochs. Similarly, Multi-Krum reduces the impact of outliers using robust statistics but focuses primarily on how the data is partitioned among clients. While it does evaluate the method across different batch sizes, it lacks significant discussion of the broader local training setup. FLTrust adopts a trusted server-side reference model to filter anomalous updates. The authors report using a "combined" learning rate of 0.002, a batch size of 64, and a single local training epoch. In contrast, FoolsGold, which identifies and penalizes suspiciously similar client contributions, does not explicitly report the learning rate or number of local training epochs in its evaluation. Instead, it mentions using batch sizes of 10 or 50 depending on the dataset. FLAME, which leverages anomaly detection to flag potentially malicious gradients, describes the structure of the federation but provides no information about the local learning setup, such as learning rate, batch size, or training epochs.
% Each of these methods aims to limit the influence of malicious updates during aggregation. However, %
Evaluation setups for these defenses are often inconsistent, with significant variation in client learning configurations such as learning rate, batch size, and the number of local training epochs. For instance, some works (e.g., FLTrust) specify fixed learning rates and batch sizes, while others (e.g., Multi-Krum, FLAME) provide little to no details about the client training setup, focusing instead on aggregation logic. This lack of standardization in evaluation protocols raises concerns about the reproducibility and generalizability of reported results. A detailed comparison of these evaluation setups is provided in \Cref{app:baseline_details}.

% Shifted to discussion section
% This lack of clarity and standardization in reporting client learning configurations limits the reproducibility of these methods and raises concerns about their robustness across diverse FL setups.

% Under certain learning configurations, these defenses fail to detect or mitigate backdoor attacks, allowing adversaries to achieve high ASR.

\begin{figure}[h]
    \includegraphics[width=.9\linewidth]{assets/section3/defenses_heatmap.pdf}
    \caption{ASR (\%) for our defense (DROP) and various existing defenses for 10 FL configurations (1.25\% DPR, 20\% MCR) where stealthy attacks are possible. No existing defense provides consistent protection across all configurations.}
    \label{fig:baselines_heatmap}
\end{figure}

% When applying existing defenses under these same conditions, we observe that none of them consistently prevent the backdoor attack
We find that these approaches often struggle against targeted backdoor attacks due to the attack's stealthy nature (\Cref{fig:baselines_heatmap}). Median and Multi-Krum fail to exclude the adversary’s updates since the poisoned gradients remain close to the statistical norm, while FLTrust struggles to identify the malicious contributions due to the minimal deviation from expected update patterns. Similarly, FoolsGold proves largely ineffective because the adversary’s contributions exhibit insufficient variability across rounds, making them difficult to detect. FLAME performs reasonably well in certain configurations but fails against attacks that exploit low learning rates. Among the defensive baselines, FLIP is the only one that slightly reduces the ASR, though its mitigation is insufficient to provide robust protection.
Existing defenses thus have a severe limitation when it comes to stealthy backdoors: \textbf{their resilience is sensitive to the specific FL learning configuration.}
