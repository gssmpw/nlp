\section{Introduction}
\label{sec:introduction}

Machine learning (ML) systems greatly benefit from scaling the amount of data used for training ML models \citep{Covert2024ScalingLF, Parthasarathi2019RealizingPS, fl_benefits, Wu2021SustainableAE}. While centralized training is ideal, data contributors often hesitate to share their raw data with a centralized party. Federated learning (FL) \citep{kairouz2021advances} provides a middle-ground to help alleviate such privacy concerns, while potentially offloading compute onto clients. This design supposedly improves privacy, but it comes at the cost of less control over data and gradient ingestion, allowing malicious clients to introduce malicious goals into the global model through data poisoning \citep{gu_badnets, Wang2024LinkageOS} or gradient manipulation \citep{bagdasaryan_howto, Goldblum2020DatasetSF}.

This lack of control over client contributions enables a particularly insidious form of adversarial behavior known as \textit{backdoor poisoning attacks} \citep{gu_badnets, liu2018trojaning, Alam2023GetRO, Zhang2024ConcealingBM, Chai2023ASF}. Unlike traditional data poisoning, which aims to degrade overall model performance and mount an availability attack \citep{rosenfeld2020certified, biggio2013poisoningattackssupportvector, fangbyzantinerobust, Tolpegin2020DataPA}, backdoor attacks embed hidden triggers in the model that lead to selective misclassifications for certain inputs. This allows the global model to perform well on clean data while remaining vulnerable to targeted exploitation. The decentralized nature of FL amplifies this threat, as the server has limited visibility into local training data and model updates, making it difficult to distinguish between honest contributions and adversarial modifications. As a result, backdoor poisoning has emerged as one of the most concerning attack vectors in federated learning.

Although numerous defenses have been proposed to counter poisoning attempts in FL \citep{yin2018byzantine, blanchard2017machine, cao2021fltrust, fung2018mitigating, auror, wang2022flare, nguyen2022flame, zhang2023flip, mesas}, their evaluation frequently lacks consistency across different federation configurations and adversarial strategies. Many existing works focus on narrow or suboptimal configurations and attack scenarios, limiting their practical applicability \citep{khan2023pitfalls}. We identify multiple critical learning configurations---comprising variations in the local number of training epochs, learning rate, and batch size---where existing FL defenses fail to consistently mitigate targeted backdoor attacks. Small changes to these configurations can significantly influence the success of both the attack and the defense, underscoring the need for more robust and generalizable defensive solutions against backdoor poisoning attacks.

In this paper, we address the challenge of defending against \textit{targeted backdoor poisoning attacks}, a more sophisticated variant of traditional backdoor attacks where the adversary targets a specific ``victim'' class within the data distribution. This focused strategy makes the attack more stealthy and difficult to detect, as the backdoor trigger affects only a subset of inputs rather than the entire input space. To counter this threat, we propose \textbf{DROP}$^1$\footnote{$^1$ Code available at \url{https://github.com/gsiros/drop}} (\textit{Distillation-based Reduction of Poisoning Signals}), a comprehensive, multi-layered defense framework. DROP integrates three key components: (1) \textbf{Agglomerative Clustering} to identify and separate anomalous updates, (2) \textbf{Activity Monitoring} to track and penalize suspicious clients over multiple rounds, and (3) \textbf{Knowledge Distillation} to cleanse the global model using a synthetic dataset guided by the consensus logits from benign client updates. Moreover, we propose DROPlet, a lightweight variant of DROP designed for seamless integration into existing FL defense frameworks.  We evaluate DROP against a range of existing defenses across diverse FL configurations and attack scenarios, demonstrating its robustness and adaptability. Unlike existing defenses, which often rely on fixed client-selection strategies or specific attack assumptions, DROP generalizes to dynamic, real-world FL environments where adversarial strategies and learning configurations may vary significantly depending on the learning task. Our results show that DROP effectively mitigates stealthy targeted backdoor attacks, reducing both attack success rates and model degradation across a broad spectrum of configurations.

\shortsection{Contributions} 
Our contributions are summarized as follows:  

\begin{itemize}  
    \item We evaluate seven existing defenses against targeted backdoor attacks and find that most defenses fail to provide adequate protection across a wide range of different learning configurations (learning rate, batch size, number of epochs), exhibiting high sensitivity to changes in the overall FL configuration (\Cref{sec:existing_finicky}).  
    
    \item Motivated by the lack of robust defenses against stealthy targeted backdoors, we propose DROP (\textit{Poison Dilution via Knowledge Distillation for Federated Learning}), a novel defense mechanism that leverages clustering and activity monitoring techniques with  knowledge distillation to tackle adversaries trying to exploit vulnerable learning configurations and different attack strategies (\Cref{sec:proposed_defense}).  
    \item Through extensive experimental evaluation across multiple datasets and FL setups, we demonstrate that DROP offers enhanced protection against backdoor attacks from adversaries under varying degrees of stealthiness (\Cref{sec:experiments}). DROP shows resilience across a wide range of learning configurations and provides consistent attack mitigation throughout training rounds, outperforming existing defenses. For example, we observe an average attack success rate of just $1.93\%$ across 10 diverse FL configurations. However, defending against stealthy attacks under non-IID data distributions remains challenging, with most defenses struggling to offer complete protection in complex dataset scenarios.
\end{itemize}
Finally, we emphasize the need for researchers to assess robustness across diverse FL configurations (\Cref{sec:conclusion}).
We also release our source code as a comprehensive framework that includes implementations of existing defenses, enabling unified comparisons of attack and defense efficacy and facilitating the integration of new attacks and defenses.