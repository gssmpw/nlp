\section{Related Work}
\label{sec:related_work}

% \todo{Introduce related works and highlight gaps- no good defense that works across multiple DPR assumptions, iid and non-iid, etc. also highlight requirement of clean-data which defenses use, which is even more so unlikely to be present for FL. if we want, could even showcase a simple ``finetune at end with clean data" baseline and demonstrate that it beats (I'd think it would?) most existing ``defenses"}

% \anshuman{Mention how FLIP slightly changes the defender's capabilities by allowing sample rejection, but as we find, this also ends up rejecting a good percentage (I believe it was 10 or so but can check later) of clean samples that do not have any triggers (also need to calculate what percentage of these were classified correctly) Metrics under this can be slightly deceptive, as our defense with the same rejection capacity can claim MTA of 82\% but that ignores benign data that is 'rejected'. In fact iirc (we can add concrete numbers later) our conditional clean accuracy is not too far from theirs (not for Table 2 at least)}  

The increasing adoption of FL has spurred extensive research on defending against poisoning attacks, particularly backdoor attacks, which pose a serious threat due to their ability to embed hidden malicious behaviors without degrading the model’s overall performance. Several defense mechanisms have been proposed to mitigate these threats, including robust aggregation methods, trust-based filtering, and anomaly detection techniques. Despite these efforts, existing defenses often exhibit limitations when evaluated across diverse attack configurations, especially under varying data poisoning rates, malicious client ratios within the federation and non-IID client data distributions.

\textit{Robust aggregation-based defenses.} Coordinate-wise median aggregation \citep{yin2018byzantine} and Multi-Krum \citep{blanchard2017machine} are two classical approaches designed to mitigate Byzantine failures by robustly aggregating model updates. The coordinate-wise median computes the median value for each model parameter across client updates, neutralizing the impact of outliers. Multi-Krum, on the other hand, iteratively selects and aggregates updates that are closest to the majority based on pairwise distances, making it resilient to adversarial updates. While these methods are effective against simple poisoning attacks, they struggle to defend against stealthy backdoor attacks, particularly when the data poisoning rate is low or the client data distribution is highly skewed. Additionally, both methods assume IID client data, limiting their robustness in realistic non-IID FL settings.

\textit{Trust-based defenses.} FLTrust \citep{cao2021fltrust} introduces a server-side reference model trained on a small trusted dataset to measure the trustworthiness of client updates. Only updates that align closely with the reference model are aggregated, providing a strong baseline against various adversarial strategies. However, FLTrust’s performance can degrade when the trusted dataset is not fully representative of the overall data distribution and if the attack is very stealthy.  

\textit{Similarity and anomaly detection-based defenses.} Fool’s Gold \citep{fung2018mitigating} uses similarity-based clustering to detect and penalize clients that contribute updates with similar gradients across multiple rounds, under the assumption that adversaries tend to behave similarly. While this approach reduces the contribution of malicious clients, its reliance on similarity metrics makes it susceptible to adaptive adversaries that can evade detection by introducing slight randomness in their updates. FLAME \citep{nguyen2022flame}, which employs adaptive clipping to detect and filter anomalous gradients, also shows promise in defending against targeted attacks. However, as we observe in our experiments, FLAME’s adaptive clipping can become overly aggressive, preventing convergence in some scenarios.  

\textit{Penultimate layer representation-based defenses.} FLARE \citep{wang2022flare} leverages discrepancies in penultimate layer representations (PLR) of model updates to assign trust scores and filter out potentially malicious updates. This approach works well for overt poisoning attempts but, as we demonstrate, struggles to mitigate stealthy attacks with low DPR, where poisoned updates closely resemble benign ones.  

\textit{Adversarial training-based and sample rejection defenses.} FLIP \citep{zhang2023flip} represents a different class of defenses by introducing the ability to reject individual samples during aggregation. By leveraging adversarial training and low-confidence refusals, FLIP aims to reconstruct client-side triggers and reject poisoned updates. While effective to some extent, FLIP’s rejection mechanism often discards a non-negligible fraction of clean samples, which can negatively impact the main task accuracy (MTA). As observed in our experiments, this trade-off in rejection-based defenses can lead to deceptively high MTA metrics, as benign samples that are incorrectly rejected are excluded from evaluation. In contrast, our proposed defense does not require such sample-level rejection, thereby preserving the overall integrity of the clean data and achieving a better balance between MTA and ASR across varying configurations. A crucial consideration when designing FL frameworks is that participating nodes often have limited computational resources, which can constrain the feasibility of complex operations. \eg FLIP relies on client-side adversarial training—a computationally expensive process. While this approach is viable in their setup, where only 10 clients are sampled per round, increasing the number of selected clients significantly extends the duration of each round, making it impractical for large-scale deployments.

Despite the progress made by these defenses, a significant gap remains: no existing method consistently performs well across different DPR levels, IID and non-IID data distributions, and varying MCR. Our proposed defense, DROP, addresses these issues by providing a robust, configuration-agnostic solution that is effective across a broad range of attack and learning configurations. 

% By releasing our codebase, we aim to provide a consistent testbed that future researchers can use to evaluate attacks and defenses under diverse scenarios, fostering more reproducible and standardized research in FL security.
