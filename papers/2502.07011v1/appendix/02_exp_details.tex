\section{Experimental Details}
\label{app:exp_details}

\subsection{Baselines}
\label{app:baseline_details}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l|cccccc}
%     \toprule
%     Method & Data Distribution & Poisoning Start & Attack-Agnostic & Client Selection & Malicious Clients & DPR\\
%     \midrule
%          FLIP \citep{zhang2023flip} & iid, d(0.5) & After Convergence & No & Enforced & 40\% & ...\\
%          FL-Trust \citep{cao2021fltrust} &  iid, weird &  & & & 20\%\\
%          FoolsGold \citep{fung2018mitigating} & \\
%          MultiKrum \citep{blanchard2017machine} & \\
%          FLAME \citep{nguyen2022flame} & \\
%          DROP (Ours) & iid, d(1.0) & Beginning & Yes & Random & 20\% & ...\\
%     \bottomrule
%     \end{tabular}
%     \caption{\anshuman{In hindsight, this table can go to the Appendix.}}
%     \label{tab:my_label}
% \end{table*}

% Each of these methods aims to limit the influence of malicious updates during aggregation. However, most works provide limited or inconsistent details about their evaluation setups, particularly concerning client learning configurations such as learning rate, batch size, and the number of local training epochs. For instance, Median is evaluated on a simpler learning task (MNIST) and specifies only the total number of participating clients. The authors provide no details about the client learning setup, including learning rate, batch size, or number of local training epochs. Similarly, Multi-Krum reduces the impact of outliers using robust statistics but focuses primarily on how the data is partitioned among clients. While it does evaluate the method across different batch sizes, it lacks significant discussion of the broader local training setup. FLTrust adopts a trusted server-side reference model to filter anomalous updates. The authors report using a "combined" learning rate of 0.002, a batch size of 64, and a single local training epoch. In contrast, FoolsGold, which identifies and penalizes suspiciously similar client contributions, does not explicitly report the learning rate or number of local training epochs in its evaluation. Instead, it mentions using batch sizes of 10 or 50 depending on the dataset. FLAME, which leverages anomaly detection to flag potentially malicious gradients, describes the structure of the federation but provides no information about the local learning setup, such as learning rate, batch size, or training epochs.

% \begin{itemize}
%     \item \textbf{FLIP \citep{zhang2023flip}}: We adapt the original implementation and hyper-parameters. The original defense assumes that both the defense and malicious client poisoning are triggered after model convergence, citing interference in convergence if the malicious activity begins earlier. However, our observations show that targeted backdoors do not disrupt model convergence even if initiated at the beginning of FL training. Starting the defense at convergence is thus ineffective as the poisoning has already occurred. On the other hand, starting too early results in suboptimal performance. Therefore, we activate the defense after the first 10/3 rounds for CIFAR-10/EMNIST respectively.
% \end{itemize}

Each of the defense methods which were presented aims to limit the influence of malicious updates during aggregation. However, most works provide limited or inconsistent details about their evaluation setups, particularly concerning client learning configurations such as learning rate, batch size, and the number of local training epochs. For instance, Median \citep{yin2018byzantine} is evaluated on a simpler learning task (MNIST \cite{mnist}) and specifies only the total number of participating clients, without providing key details about the client learning setup, such as the learning rate, batch size, or the number of local epochs. Similarly, Multi-Krum \citep{blanchard2017machine} reduces the impact of outliers using robust statistics but primarily focuses on how the data is partitioned among clients. While it does evaluate the method across different batch sizes, it lacks a detailed discussion of the broader local training setup. FLTrust \citep{cao2021fltrust} adopts a trusted server-side reference model to filter anomalous updates. The authors report using a "combined" learning rate of 0.002, a batch size of 64, and a single local training epoch. In contrast, FoolsGold \citep{fung2018mitigating}, which identifies and penalizes suspiciously similar client contributions, does not explicitly report the learning rate or the number of local epochs in its evaluation, only mentioning batch sizes of 10 or 50 depending on the dataset. FLAME \citep{nguyen2022flame}, which employs anomaly detection to flag potentially malicious gradients, describes the structure of the federation but omits critical information about the local learning setup, such as learning rate, batch size, or the number of epochs. FLIP \citep{zhang2023flip}, on the other hand, presents a more detailed setup. We adapted the original implementation and hyperparameters for our evaluation. The original defense assumes that both the defense and malicious client poisoning are triggered after model convergence, citing interference in convergence if malicious activity begins earlier. However, our observations show that targeted backdoors do not disrupt model convergence even when initiated at the beginning of FL training. Thus, starting the defense only after convergence is ineffective since poisoning has already occurred by that point. On the other hand, initiating the defense too early leads to suboptimal performance. Therefore, we activate the defense after the first 10 rounds for CIFAR-10 and after the first 3 rounds for EMNIST to balance effectiveness and performance.


\subsection{DROP Parameters}
\label{sec:drop_params}

% Knowledge distillation, particularly in the context of model stealing attacks, is inherently imperfect and cannot replicate the target model exactly. As a result, a minor decrease in MTA is expected. To mitigate this and ensure convergence in the FL setting, the knowledge distillation component of DROP is applied every \(K\) rounds instead of every round, allowing the system to recover any lost MTA in intermediate rounds. For CIFAR-10, \(K = 5\) with a MAZE query budget of 5M per round. For EMNIST, \(K = 40\) with a MAZE query budget of 4M per round.

Knowledge distillation, particularly in the context of model stealing attacks, is inherently imperfect and cannot replicate the target model exactly, resulting in a minor decrease in MTA. To address this and ensure convergence in the FL setting, the knowledge distillation component is applied every \(K\) rounds instead of every round, allowing the system to recover lost MTA during intermediate rounds. The budget parameter in model stealing attacks and knowledge distillation determines the number of queries used to generate synthetic samples, which are then employed to guide the distillation process. A sufficient query budget ensures the generation of high-quality synthetic data that aligns closely with the target model’s decision boundaries, thereby enhancing the effectiveness of knowledge distillation. For CIFAR-10, we set \(K = 5\) with a query budget of 5M queries. For EMNIST, \(K = 40\) with a query budget of 4M queries. These values strike a balance between computational efficiency and the quality of the distilled global model.


\subsection{EMNIST Grid-Search}

\begin{figure}[ht]
    \includegraphics[width=.98\linewidth]{assets/emnist_undefended_asr.png}
    \caption{Visualizing the impact of the FL setup (particularly the learning-rate, batch-size, and number of epochs used by clients) on main-task accuracy attack success rate when poisoned clients aim to inject a targeted backdoor for the EMNIST dataset.}
    \label{fig:fl_setup_impact_emnist}
\end{figure}


In the same fashion as \cref{sec:fl_setup_matters} for CIFAR-10, we conduct a grid-search analysis over key hyperparameters, varying the client’s learning rate, batch size, and number of epochs on the EMNIST \citep{cohen2017emnistextensionmnisthandwritten} dataset.
Our findings in \cref{fig:fl_setup_impact_emnist} indicate that targeted backdoor attacks are more likely to succeed across a wider range of learning parameter combinations, with numerous setups yielding an ASR greater than 80\%. In \cref{tab:fl_setup_exps_emnist}, we highlight ten specific learning configurations where the attack achieves high ASR while maintaining a high MTA, underscoring the vulnerability of these setups to adversarial manipulation.

\begin{table}[ht]
    \centering
    \begin{tabular}{llcc|cc}
    \toprule
    \textbf{Config} & \textbf{LR} & \textbf{BS} & \textbf{Epochs} & \textbf{MTA (\%)} & \textbf{ASR (\%)} \\
    \midrule
    C1 & 0.1 & 32 & 2 & 89.23 & 99.00 \\
    C2 & 0.1 & 64 & 5 & 88.22 & 99.00 \\
    C3 & 0.05 & 32 & 5 & 88.48 & 98.75 \\
    C4 & 0.1 & 32 & 1 & 89.59 & 98.75 \\
    C5 & 0.1 & 64 & 1 & 89.20 & 98.50 \\
    C6 & 0.1 & 32 & 5 & 88.73 & 98.50 \\
    C7 & 0.1 & 64 & 2 & 88.87 & 98.25 \\
    C8 & 0.05 & 32 & 1 & 89.21 & 98.25 \\ 
    C9 & 0.01 & 32 & 2 & 87.73 & 97.25 \\
    C10 & 0.025 & 128 & 5 & 86.61 & 96.00 \\
    \bottomrule
    \end{tabular}
    \caption{Client FL configurations for successful stealthy attacks on EMNIST \ie cases where the MTA $\geq 80\%$ and ASR $\geq 95\%$.}
    \label{tab:fl_setup_exps_emnist}
\end{table}
\clearpage
