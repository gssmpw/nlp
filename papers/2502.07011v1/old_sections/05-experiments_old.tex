\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments under a wide range of adversarial conditions, data distributions, and model architectures. Our evaluations focus on various learning configurations of interest, as discussed in  \cref{sec:defenses_fail_across_configs}. We also vary DPRs, MCRs, and use both independent and identically distributed (IID) and non-IID data distributions, ensuring a comprehensive assessment of DROP in realistic FL settings.

\subsection{Setup}

%\shortsection{Datasets and Model Architecture} To assess the generalizability of our defense, we used two standard datasets for FL, EMNIST and CIFAR-10, which contain diverse image classes suitable for evaluating the impact of both targeted and untargeted attacks. EMNIST \citep{cohen2017emnistextensionmnisthandwritten} is an extension of the MNIST \citep{mnist} dataset, consisting of handwritten character and digit images. It includes a larger set of 62 classes (both uppercase and lowercase letters, along with digits) and is widely used for evaluating classification tasks in FL due to its simple, lightweight structure and relevance to real-world handwriting recognition scenarios. CIFAR-10 \cite{krizhevsky2009learning} is a widely-used image classification dataset containing 60,000 32x32 color images across 10 classes, including objects like airplanes, cars, and animals. Its diversity and complexity make it a standard benchmark for evaluating the robustness and generalization of models in computer vision and FL contexts. For the model architecture, we tested our defense on the state-of-the-art Residual Network architecture \citep{he2015deepresiduallearningimage} and the ResNet-18 model.

\shortsection{Datasets and Model Architecture} 
We evaluated our defense using two standard FL datasets, EMNIST and CIFAR-10. EMNIST \citep{cohen2017emnistextensionmnisthandwritten} extends MNIST \citep{mnist} with 62 handwritten character and digit classes, making it suitable for FL classification tasks. CIFAR-10 \citep{krizhevsky2009learning} contains 60,000 32x32 color images across 10 diverse classes. 

\shortsection{Data Distribution} 
We evaluate our defense on IID and across various levels of non-IID data to reflect the heterogeneity typically observed in real-world FL deployments. To control the degree of non-IID-ness, we partition the global dataset using a Dirichlet distribution $Dir(\alpha)$ \citep{li2021federatedlearningnoniiddata} (see \Cref{sec:results}) and use the distributional parameter $\alpha$ to vary the degree of non-IID-nes.

\shortsection{Federation Settings} The federation consists of a total of 50/100 clients for CIFAR-10/EMNIST, with 50\% clients randomly selected to participate in each training round. At each round \(t\), a subset of $m$ clients \(\mathcal{C}_t \subseteq \mathcal{N}\) is randomly selected to participate in local training. Unlike prior works \citep{blanchard2017machine,zhang2023flip} that assume a fixed number of adversarial clients in every round, our model allows for the number of adversarial clients in \(\mathcal{C}_t\) to vary. We use a ResNet-18 architecture \citep{he2015deepresiduallearningimage} for all experiments.

% We evaluate our defense using a targeted backdoor attack \citep{} where adversarial clients aim to embed malicious behavior into the global model by poisoning inputs from a specific \texttt{victim} class. The attack operates as follows: an adversary selects a \texttt{victim} class \(y_{\text{victim}}\) and applies a predefined trigger pattern \(\mathbf{t}\) to a fraction of the inputs belonging to this class. The labels of these triggered inputs are then flipped to a specified \texttt{target} class \(y_{\text{target}}\). Formally, let \(\mathcal{D}_{\text{victim}} = \{(\mathbf{x}_i, y_i) \mid y_i = y_{\text{victim}}\}\) denote the set of inputs in the victim class. The adversary modifies a subset \(\mathcal{D}_{\text{poisoned}} \subset \mathcal{D}_{\text{victim}}\), such that for each \((\mathbf{x}_i, y_i) \in \mathcal{D}_{\text{poisoned}}\), the poisoned dataset becomes:
% \begin{equation}
%     \mathcal{D}_{\text{poisoned}} = \{(\mathbf{x}_i + \mathbf{t}, y_{\text{target}}) \mid (\mathbf{x}_i, y_i) \in \mathcal{D}_{\text{victim}}\},
% \end{equation}
% where \(\mathbf{t}\) represents the backdoor trigger, such as a small patch or pixel pattern added to the input.  

% The poisoned dataset \(\mathcal{D}_{\text{poisoned}}\) is used by malicious clients during local training to generate updates that embed the backdoor into the global model. The effectiveness of the attack is measured by the \textit{Attack Success Rate (ASR)}, which quantifies the proportion of inputs containing the trigger that are misclassified as \(y_{\text{target}}\). At the same time, the adversary attempts to maintain a high \textit{Main Task Accuracy (MTA)} on clean inputs to ensure stealthiness and evade detection.  

% We vary the \textbf{DPR}—the proportion of poisoned inputs in the adversarial client dataset—across three levels: 1.25\%, 2.5\%, and 5\%. These rates control the fraction of poisoned samples in \(\mathcal{D}_{\text{poisoned}}\), allowing us to evaluate the robustness of our defense against both low and high DPR scenarios. Additionally, we simulate federations with different \textbf{MCRs} by varying the proportion of adversarial clients in the federation to 20\% and 40\%. These configurations enable us to assess the defense under dynamic adversarial influence, where the number of malicious clients may significantly impact the attack's effectiveness and the defense's performance. 

\begin{table*}[th]
    \centering
    \begin{tabular}{l|ccc|ccc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{6}{c}{CIFAR-10} & \multicolumn{4}{c}{EMNIST} \\
     & \multicolumn{3}{c}{MTA (\%)} & \multicolumn{3}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} \\
     \cline{2-11}
     \multicolumn{1}{r|}{DPR (\%)} & 1.25 & 2.5 & 5 & 1.25 & 2.5 & 5 & 1.25 & 2.5 & 1.25 & 2.5 \\
    \midrule
    Undefended & 87.55 & 87.32 & 87.40 & 85.5 & 96.6 & 98.4 & 89.20 & 89.11 & 98.50 & 97.75 \\
    \hline
    FLAME & 84.86 & 85.00 & 85.03 & 88.7 & 96.8 & 99.4 & 87.81 & 87.74 & 15.50 & 17.25 \\
    FLARE & 85.06 & 81.08 & 84.41 & 92.5 & 75.8 & 3.5 & 89.07 & 89.00 & 96.00 & 73.00 \\
    FLIP & 83.09 & 81.53 & - & 53.4 & 73.4 & -  & (86.79) & 88.32 & (96.25) & 48.75 \\
    FLTrust & 87.93 & 87.52 & 87.62 & 90.4 & 96.7 & 98.1 & 89.36 & 89.41 & 98.50 & 98.50 \\
    Fool's Gold & 83.85 & 83.76 & 83.92 & 83.0 & 92.6 & 94.3 & 88.28 & 88.31 & 91.75 & 81.50 \\
    Median & 87.48 & 87.33 & 87.21 & 95.3 & 96.3 & 98.8 & 89.22 & 89.16 & 98.25 & 98.50 \\
    Multi-KRUM & 85.96 & 86.00 & 85.87 & 86.0 & 97.5 & 98.6 & 88.79 & 88.64 & 92.25 & 10.75 \\
    \hline
    DROPlet (ours) & 87.49 & 87.05 & 87.20 & 91.3 & \textbf{0.1} & 0.9 & 88.98 & 88.96 & 1.50 & 1.00 \\
    DROP (ours) & 76.05 & 76.53 & 75.18 & \textbf{1.3} & 1.1 & \textbf{0.5} & 88.36 & 89.43 & \textbf{1.25} & \textbf{1.00} \\
    \bottomrule
    \end{tabular}
    \caption{MTA (\%) and ASR (\%) metrics at different Data Poisoning Rates (DPR) for configuration C4 for CIFAR-10 and C5 for EMNIST. The MCR is fixed at 20\%. DROP achieves low ASR for all DPR, while existing defenses fail to mitigate the attack. The lightweight variant DROPlet achieves higher MTA than DROP and is robust in all cases, except for extremely stealthy attacks (1.25\% DPR) on CIFAR-10. \georgios{Change to 80 rounds for emnist.}\anshuman{Change to reporting 'best' MTA and corresponding ASR}}
    \label{tab:fl_vary_dpr}
\end{table*}

\shortsection{Attacks} We evaluate our defense against a targeted backdoor attack (described in \Cref{sec:threat_model}), where adversarial clients poison inputs from a specific victim class \(y_{\text{victim}}\) to embed malicious behavior in the global model. The adversary applies a trigger \(\mathbf{t}\) to a fraction of victim-class inputs and flips their labels to a target class \(y_{\text{target}}\). Attack success is measured by the \textit{Attack Success Rate (ASR)}, while maintaining high \textit{Main Task Accuracy (MTA)} ensures stealthiness. We evaluate robustness by varying the \textbf{DPR} at 1.25\%, 2.5\%, and 5\%, and the \textbf{MCR} at 20\% and 40\%, to simulate a range of  adversarial compromise.

% To benchmark the effectiveness of DROP, we compared its performance against several state-of-the-art defense mechanisms from the literature:
% \begin{itemize}
%     \item \textbf{Median Aggregation \citep{yin2018byzantine}}: a classical robust aggregation technique that uses the coordinate-wise median of client updates to neutralize the effect of outliers.
%     \item \textbf{Multi-Krum \citep{blanchard2017machine}}: selects and aggregates the most "trustworthy" client updates by iteratively excluding updates that are furthest from the majority, making it resilient to Byzantine failures.
%     \item \textbf{FLTrust \citep{cao2021fltrust}}: introduces a server-side reference model to measure the "trustworthiness" of client updates, ensuring that only updates that align with the reference model are included in the aggregation.
%     \item \textbf{FoolsGold \citep{fung2018mitigating}}: employs similarity-based clustering to mitigate the impact of poisoned updates by reducing their contribution in the aggregation process.
%     \item \textbf{FLAME \citep{nguyen2022flame}}: a robust aggregation method designed to address both Byzantine attacks and poisoning attacks in FL by selectively penalizing anomalous updates.
%     \item \textbf{FLIP \citep{zhang2023flip}}: estimates client-side trigger reconstruction with adversarial training in conjunction with low-confidence refusals.
%     \item \textbf{FLARE \citep{wang2022flare}}:  leverages penultimate layer representations (PLR) to evaluate the trustworthiness of model updates, assigning trust scores based on PLR discrepancies and aggregating updates weighted by these scores to defend against model poisoning attacks.
    

% \end{itemize}

\shortsection{Baselines} To benchmark the effectiveness of DROP, we compare its performance against seven state-of-the-art \anshuman{Not all of them are SOTA, certainly not Median/Multi-KRUM} FL poisoning defense mechanisms from the literature. Median Aggregation \citep{yin2018byzantine} is a classical robust aggregation technique that uses the coordinate-wise median of client updates to neutralize the effect of outliers. Multi-Krum \citep{blanchard2017machine} selects and aggregates the most "trustworthy" client updates by iteratively excluding updates that are furthest from the majority, making it resilient to Byzantine failures. FLTrust \citep{cao2021fltrust} introduces a server-side reference model to measure the "trustworthiness" of client updates, ensuring that only updates aligning with the reference model are included in the aggregation. FoolsGold \citep{fung2018mitigating} employs similarity-based clustering to mitigate the impact of poisoned updates by reducing their contribution in the aggregation process. FLAME \citep{nguyen2022flame} is a robust aggregation method designed to address both Byzantine and poisoning attacks in FL by selectively penalizing anomalous updates. FLIP \citep{zhang2023flip} estimates client-side trigger reconstruction using adversarial training combined with low-confidence refusals. Finally, FLARE \citep{wang2022flare} leverages penultimate layer representations (PLR) to evaluate the trustworthiness of model updates, assigning trust scores based on PLR discrepancies and aggregating updates weighted by these scores to defend against model poisoning attacks.
%
For further implementation details on the baselines, please see \Cref{app:baseline_details}.

\subsection{Results}
\label{sec:results}
\anshuman{Categorize shortsections into smaller subsubsections- too many right now. I'd added a comment about it but can't find it here}
Our evaluation reveals the strengths and weaknesses of existing defenses and demonstrates the superior robustness of our proposed defense, DROP, across various attack scenarios.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{assets/section5/across_rounds.pdf}
    \caption{MTA (a) and ASR (b) across rounds for various defenses, for CIFAR-10 with 1.25\% DPR and 20\% MCR for configuration C4. Certain defenses like FLIP and FLAME have wildly fluctuating ASR across rounds, making them unreliable. DROP instead achieves consistently low ASR in all rounds.}
    \label{fig:multiple_round_progression}
\end{figure*}

\shortsection{Robustness across FL learning configurations} One of the key strengths of DROP is its ability to provide reliable protection against targeted backdoor attacks across a wide range of FL learning configurations. While prominent baselines exhibit significant variability in performance depending on the enforced learning parameters, DROP consistently ensures near-zero ASR across all configurations of interest, as shown in \Cref{fig:baselines_heatmap}. Specifically, Multi-KRUM, Median, Fool’s Gold, and FLTrust fail to mitigate the attack across all evaluated configurations, with consistently high ASR values. FLAME and FLARE offer partial protection, but their effectiveness is limited to a small subset of configurations, indicating a lack of generalization. While FLIP appears to be the most reliable among the baselines, it still struggles with high ASR values exceeding 50\% across all configurations \anshuman{It does achieve acceptablle ASR in some configurations; update text based on figure}, demonstrating its inability to fully suppress stealthy attacks. \anshuman{We had this defense-wise discussion in Section 3.2 already, need not repeat here with this depth}In contrast, DROP achieves superior performance by maintaining near-zero ASR across all tested configurations, making it a learning configuration agnostic defense. This robustness highlights DROP’s practicality for real-world FL deployments, where learning configurations may vary dynamically, ensuring reliable protection against adversarial threats regardless of parameter settings.

\shortsection{Consistency Across Rounds} A robust defense must ensure low ASR during rounds where the MTA remains acceptable, preventing randomness in ASR fluctuations from determining the outcome of FL training. While some baselines (\eg FLIP, FLAME) initially achieve low ASR, their performance deteriorates over subsequent rounds, resulting in inconsistent suppression of the attack. This variability is problematic because in a real-world setting, FL training might terminate at any round, and reliance on transient phases of low ASR does not guarantee reliable protection. In \Cref{fig:multiple_round_progression}, we compare the performance of various defensive baselines with DROP over 200 rounds. Although some baselines degrade more quickly (\eg Median, FLTrust), others (\eg FLIP, FLAME) exhibit slower degradation, yet suffer from significant ASR fluctuations across rounds. This inconsistency makes it difficult to guarantee robust defense throughout the training process. In contrast, DROP consistently maintains near-zero ASR across all rounds, offering reliable protection regardless of when training might terminate. \textbf{\textit{This stability ensures that practitioners do not have to depend on chance for effective defense, making DROP a more practical and dependable solution for real-world FL systems.}} \anshuman{Connect to Property 2 that we described in Section 2.2}

\shortsection{Robustness for Stealthy Attacks - Varying DPR} 
By varying the attacker's DPR, we simulate  increasingly stealthy targeted backdoor attacks, with DPR as low as 1.25\%. We observe that all baseline defenses fail to defend against targeted backdoors across both CIFAR-10 and EMNIST, regardless of the stealthiness of the attack induced via DPR (\Cref{tab:fl_vary_dpr}).
Some defenses perform adequately only when the attack is overt. For instance, for CIFAR-10 FLARE mitigates the attack effectively with at 5\% DPR, achieving 3.5\% ASR, but fails entirely when the DPR is reduced (\eg 2.5\%). In contrast, DROP consistently defends against targeted backdoor attack across all levels of DPR. It achieves near-zero ASR (1.5\%, 1.2\%, and 0.5\% respectively) across all configurations.

% \shortsection{Attacker presence in the Federation - Varying MCR} By varying the MCR, we simulate conditions where the attacker controls a smaller (20\%) and larger (40\%) portion of clients within the federation. By controlling multiple clients, the attacker is able to outnumber the benign clients in FL rounds due to the random client selection that takes place. From Table~\cref{tab:fl_vary_mcr}, we can see that, again, most defensive baselines fail to protect the FL system from the attack. In the current learning configuration that the attacker is exploiting, we can see that FLIP is able to mitigate the attack down to ~20\% ASR, while FLAME, due to its adaptive clipping scheme, is clipping the submitted model updates too aggressively and thus is not being able to achieve convergence. On the contrary DROP is again able to provide resilience against the attack in both 20\% and 40\% MCR setups by sacrificing a small amount of MTA. 

\begin{table*}[h]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{4}{c}{CIFAR-10} & \multicolumn{4}{c}{EMNIST} \\
     & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} & \multicolumn{2}{c}{MTA (\%)} & \multicolumn{2}{c}{ASR (\%)} \\
     \cline{2-9}
     \multicolumn{1}{r|}{MCR (\%)} & 20 & 40 & 20 & 40 & 20 & 40 & 20 & 40 \\
    \midrule
    Undefended & 86.56 & 86.05 & 92.1 & 99.6 & 87.78 & 87.65 & 94.75 & 100.0 \\
    \hline
    FLAME & 80.78 & 81.11 & 91.3 & 99.8 & 85.86 & 86.74 & 3.25 & 98.25 \\
    FLARE & 83.28 & 83.74 & 58.3 & 98.9 & 86.86 & 86.69 & 63.50 & 98.00 \\
    FLIP & 82.25 & 83.43 & 11.2 & 72.3 & - & - & - & - \\
    FLTrust & 87.38 & 87.24 & 99.5 & 99.4 & 89.34 & 89.26 & 97.75 & 99.75 \\
    Fool's Gold & 83.80 & 83.73 & 86.2 & 99.7 & 86.71 & 86.55 & 41.75 & 99.50 \\
    Median & 86.10 & 85.54 & 98.9 & 99.1 & 87.72 & 87.84 & 97.0 & 99.75 \\
    Multi-KRUM & 84.41 & 84.04 & 99.2 & 99.8 & 87.11 & 87.11 & 1.00 & 99.00 \\
    \hline
    DROPlet (ours) & 85.49 & 84.87 & 0.9 & 0.8 & 87.66 & 87.37 & 1.50 & 0.75 \\
    DROP (ours) & 69.11 & 70.71 & \textbf{1.3} & \textbf{1.4} & 88.57 & 88.43 & \textbf{0.75} & \textbf{0.75} \\
    \bottomrule
    \end{tabular}
    \caption{MTA and ASR at different MCR for configuration C1 for CIFAR-10 and C9 for EMNIST. The DPR is fixed at 2.5\%. Existing defenses fail to prevent the attack, particularly at 40\% MCR. Both DROP and DROPlet are resilient in all cases, but DROP induces some reduction in MTA.  \georgios{Change to 80 rounds for emnist.}\anshuman{Change to reporting 'best' MTA and corresponding ASR}}
    \label{tab:fl_vary_mcr}
\end{table*}

\shortsection{Robustness for Different Levels of Attacker Compromise in the Federation - Varying MCR}  
By varying the MCR, we simulate scenarios where the attacker controls a smaller (20\%) or larger (40\%) fraction of clients in the federation. A high MCR, such as 40\%, significantly increases the likelihood that malicious clients will outnumber benign ones in certain FL rounds due to the random client selection process. This imbalance enables adversarial updates to dominate the aggregation process, amplifying the attack's effectiveness and exposing vulnerabilities in clustering-based defenses. Once again, we observe that that most defensive baselines fail to provide adequate protection against the attack under such conditions (\Cref{tab:fl_vary_mcr}). For instance, FLIP reduces the ASR to 11.2\% for 20\% MCR on CIFAR-10, but fails to mitigation the attack at 40\% MCR.  FLAME, due to its overly aggressive adaptive clipping scheme, struggles to achieve convergence. Both FLAME and Multi-Krum achieve resilience for 20\% MCR on EMNIST, but do not thwart the attack at 40\% MCR. In contrast, DROP consistently demonstrates resilience across both 20\% and 40\% MCR setups, achieving near-zero ASR while, highlighting its robustness even in highly adversarial settings.

\shortsection{DROPlet - Lightweight variant of DROP} DROPlet (introduced in ~\cref{sec:proposed_defense}) is designed for scenarios where computational efficiency is a priority. Unlike DROP, DROPlet omits the knowledge distillation component, thereby avoiding the cumulative MTA degradation observed over the course of training. This design choice makes DROPlet well-suited for environments with limited computational resources or where minimizing overhead is crucial. While DROPlet may not be as effective against stealthy, low-DPR attacks (\cref{tab:fl_vary_dpr}, CIFAR-10, 1.25\% DPR), it still provides substantial protection in many configurations, ensuring near-zero ASR in a broad range of settings. As shown in our evaluations (\cref{tab:fl_vary_dpr} and \cref{tab:fl_vary_mcr}), DROPlet performs comparably to DROP for both classification tasks and less stealthy attack scenarios, demonstrating strong robustness without incurring the additional computational cost of knowledge distillation. However, for more complex tasks like CIFAR-10, where the higher information content (RGB images) provides attackers with a larger attack surface to conceal backdoors, DROPlet’s omission of the distillation component leads to a decrease in its effectiveness against highly stealthy attackers. Despite this limitation, DROPlet remains a domain-agnostic solution, alike DROP. By offering a trade-off between computational efficiency and robustness, DROPlet provides a practical alternative for FL deployments where computational resources are constrained, while still ensuring reliable protection in a variety of learning configurations.
% \shortsection{Robustness - Accuracy Tradeoff} DROP's effectiveness against backdoor attacks comes with a slight \anshuman{88 to 76 isn't slight; we should be frank about the cases where it drops, and emphasize on the protection we get} trade-off in MTA compared to some baselines. For instance, in \Cref{tab:fl_vary_dpr} with DPR = 1.25\%, DROP achieves an MTA of 74.74\%, slightly lower than FLTrust (87.93\%) and Median (87.14\%). Similarly, in \Cref{tab:fl_vary_mcr} with MCR = 20\%, DROP records an MTA of 68.74\%, while FLTrust achieves 86.88\%. This trade-off reflects the strict filtering and penalization mechanisms employed by DROP to ensure security. \anshuman{Is it because of those? I thought it was mostly MAZE, since droplet's MTA is just fine} However, it is important to note that the reduction in MTA ensures \textbf{\textit{near-total mitigation of backdoor attacks, making DROP a more reliable choice for high-stakes applications where security is paramount}}. \alina{Mention that the tradeoff between robustness and accuracy is well known in adv ML literature, cite: tsipras2019robustnessoddsaccuracy. Mention that we also have DROPlet, which achieves higher MTA, but might struggle with extremely stealthy attacks.} \anshuman{Top it off by mentioning how advancements in distillation and model-stealing attacks would also directly benefit us, since they would be a drop-in replacement for the MAZE part.}

\shortsection{Robustness - Accuracy Tradeoff} While DROP demonstrates strong effectiveness against backdoor attacks, it comes with a notable trade-off in MTA compared to some baselines. For example, in \Cref{tab:fl_vary_dpr} with a DPR of 1.25\%, DROP achieves an MTA of 74.74\%, which is lower than FLTrust (87.93\%) and Median (87.14\%). Similarly, in \Cref{tab:fl_vary_mcr} with an MCR of 20\%, DROP records an MTA of 68.74\%, compared to 86.88\% for FLTrust. This reduction in accuracy primarily stems from the strict distillation-based cleansing mechanism, which aims to ensure near-total suppression of adversarial influences. Notably, unlike clustering or aggregation-only defenses, DROP applies knowledge distillation via MAZE, which contributes significantly to the observed MTA drop while guaranteeing strong robustness. The trade-off between robustness and accuracy is a well-documented challenge in adversarial machine learning \citep{tsipras2019robustnessoddsaccuracy}. By prioritizing security, DROP becomes a more reliable choice for high-stakes applications where even a minor vulnerability could lead to significant consequences. Furthermore, we introduce DROPlet as a lighter variant of DROP, which achieves higher MTA while being suitable for scenarios where extremely stealthy attacks are less likely. It is also worth emphasizing that advancements in knowledge distillation techniques and model-stealing attacks would directly benefit DROP, as they could serve as improved replacements for the current MAZE component, potentially reducing the accuracy trade-off while maintaining robust defensive capabilities.
\anshuman{We should also call out existing defense works for not trying to achieve good MTA for their undefended FL setups, which may look drops in MTA for their defense look minimal. Most papers I have seen on FL defenses report 70-75\% MTA for their undefended setups whereas we easily achieve close to 90\%. This of course will make it seems like our defense takes a very large hit}


% \shortsection{Consistency across rounds} It is necessary for a defensive framework to guarantee consistency and reliability when it comes to successfully defending from a targeted backdoor attack across rounds. In Figure~\ref{fig:multiple_round_progression},  we can see the performance of the defensive baselines and how DROP compares against them over 100 rounds. It is clear that the performance of some baselines starts to degrade faster (\eg Median, FL-Trust) than others (\eg FLIP and FLAME), however the round-to-round ASR varies a lot which means that no defensive baseline can guarantee consistent suppression of the attack. On the contrary, DROP consistently defends the system from the targeted backdoor attack and maintains a near zero ASR in all 100 rounds of evaluation.

% \shortsection{Consistency Across Rounds} A robust defensive framework must ensure consistent and reliable protection against targeted backdoor attacks over multiple training rounds \anshuman{Less about being consistent across multiple trainign rounds (some defenses like FLIP spike a bit in the first 15-20 rounds but then converge to low ASRs), but having low ASR in a region of the loss curve where MTA is decent and thus, in a counterfactual world, the FL could have terminated at some earlier point. We don't want the FL trainer to win or lose because of randomness arising from ASR fluctuations (and I think this also ties back to the way we have framed Property 2 right now in Section 2.2)} \georgios{I understand where you're coming from but I am afraid to talk about plausible termination of the FL training. A reviewer might say oh so why don't you stop earlier and avoid all of this?}\anshuman{In that case, why care what the ASR trends look like? A defense could have 100\% ASR for 99 rounds but 0\% ASR in round 100- this is not ideal and Section 2.2 tries to capture that but we do need some tangible justification as to why this is desirable, if we strictly stop at the end (we want to emphasize that 100 or 200 rounds is not some magical number and we ideally want the attack to not pass through if we had stopped a few rounds earlier/later either)}. In Figure~\ref{fig:multiple_round_progression}, we observe the performance of various defensive baselines compared to DROP over 100 rounds. While some baselines degrade more quickly (\eg Median, FLTrust) and others (\eg FLIP, FLAME) show relatively slower degradation, the ASR fluctuates significantly across rounds. This variability highlights the inability of existing defenses to provide consistent suppression of the attack. In contrast, DROP consistently defends the system, maintaining near-zero ASR throughout all 100 rounds of evaluation.

% \shortsection{Consistency Across Rounds}  
% A robust defensive framework must maintain low ASR while preserving high MTA in regions of the training process where the FL system could plausibly terminate. In Figure~\ref{fig:multiple_round_progression}, we observe the performance of various defensive baselines compared to DROP over 100 rounds. While some baselines (\eg FLIP, FLAME) achieve low ASR after significant training progress, their early-round ASR spikes introduce variability that could lead to suboptimal model termination in a counterfactual scenario. Other defenses, such as Median and FLTrust, degrade more quickly, further exposing their inability to reliably suppress the attack. In contrast, DROP maintains near-zero ASR throughout all rounds, ensuring stable performance and minimizing the randomness associated with ASR fluctuations. This stability aligns closely with Property 2 outlined in Section~\ref{sec:property2}, emphasizing DROP's ability to consistently defend the system across a wide range of training configurations.



\begin{table*}[th]
    \centering
    \begin{tabular}{l|cccc|cccc|cccc|cccc}
    \toprule
    \multirow{2}{*}{\textbf{Defense}} & \multicolumn{8}{c}{CIFAR-10} & \multicolumn{8}{c}{EMNIST} \\
     & \multicolumn{4}{c}{MTA (\%)} & \multicolumn{4}{c}{ASR (\%)} & \multicolumn{4}{c}{MTA (\%)} & \multicolumn{4}{c}{ASR (\%)} \\
     \cline{2-17}
     \multicolumn{1}{r|}{$Dir(\alpha)$} & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ & 1 & 10 & $10^2$ & $\infty$ \\
    \midrule
    Undefended & 86.45 & 87.36 & 87.07 & 87.55 & 92.8 & 90.2 & 95.3 & 85.5 & 87.29 & 87.72 & 87.53 & 87.78 & 93.00 & 94.50 & 93.50 & 94.75  \\
    \hline
    FLAME & 61.92 & 80.65 & 83.79 & 84.86 & \textbf{5.6} & 83.9 & 30.3 & 88.7 & 85.91 & 86.48 & 85.54 & 85.86 & \textbf{13.5} & 56.75 & 3.0 & 3.25  \\
    FLARE & 84.40 & 85.19 & 84.49 & 85.06 & 48.3 & \textbf{14.4} & 89.6 & 92.5 & 86.48 & 86.24 & 86.57 & 86.86 & 84.0 & 28.25 & 3.75 & 63.5 \\
    FLIP & 79.38 & 82.99 & 83.10 & 83.09 & 40.9 & 20.5 & \textbf{47.8} & 53.4 & - & - & - & - & - & - & - & - \\
    FLTrust & 86.69 & 86.92 & 86.24 & 87.93 & 98.3 & 93.6 & 96.6 & 90.4 & 88.21 & 88.43 & 88.36 & 89.34 & 92.5 & 94.5 & 95.75 & 97.75 \\
    Fool's Gold & 80.00 & 83.38 & 83.95 & 83.85 & 79.7 & 76.5 & 87.0 & 83.0 & 85.51 & 85.76 & 85.70 & 86.71 & 28.25 & 20.75 & 17.50 & 41.75 \\
    Median & 86.32 & 86.84 & 87.10 & 87.48 & 97.3 & 95.3 & 98.3 & 95.3 & 87.24 & 87.61 & 87.51 & 87.72 & 92.0 & 91.5 & 91.5 & 97.0 \\
    Multi-KRUM & 83.45 & 85.44 & 85.80 & 85.96 & 91.3 & 95.2 & 77.5 & 86.0 & 86.49 & 86.75 & 86.84 & 87.11 & 80.75 & 16.75 & 2.5 & 1.0 \\
    \hline
    DROPlet (ours) & 86.62 & 87.03 & 86.99 & 87.49 & 94.5 & 96.2 & 96.1 & 91.3 & 87.14 & 87.51 & 87.74 & 87.66 & 93.0 & 91.75 & 86.0 & 1.5 \\
    DROP (ours) & 72.83 & 75.61 & 75.61 & 76.05 & 92.9 & 96.6 & 83.3 & \textbf{1.3} & 87.66 & 84.48 & 84.16 & 88.57 & 16.50 & \textbf{4.75} & \textbf{1.75} & \textbf{0.75} \\
    \bottomrule
    \end{tabular}
    \caption{MTA and ASR across rounds for various defenses, for 1.25\% DPR and 20\% MCR, using configuration C4 for CIFAR-10 with  and C9 for EMNIST, with varying $Dir(\alpha)$.} 
    %$\alpha=\infty$ corresponds to IID data-splits.
    %(lower values of $\alpha$ indicate more skewed data distributions across clients, making the setting increasingly challenging to defend against the attack.)}
    \label{tab:noniid}
\end{table*}


\shortsection{Hardness Under Non-IID Settings}\label{sec:non_iid} The non-IID nature of client datasets poses significant challenges for defending against targeted backdoor attacks. When clients possess data from only a small subset of classes, adversaries can exploit this skewed distribution by selectively poisoning a specific victim class. In such cases, adversarial updates closely resemble benign updates, making it harder for clustering-based defenses to distinguish between the two. This increases the stealthiness of the attack, particularly under non-IID settings, where client data distributions are naturally imbalanced. Consequently, defenses that perform well under IID conditions often exhibit reduced effectiveness when faced with non-IID data, underscoring the need for adaptive approaches capable of handling diverse and heterogeneous client data.

\Cref{tab:noniid} presents the results of evaluating various defenses under increasingly non-IID conditions for CIFAR-10 and EMNIST. While both datasets are commonly used in FL research, CIFAR-10 represents a more complex task due to its greater class diversity, higher information content (RGB images), and the increased potential for adversarial triggers to be hidden within training samples. This inherent complexity makes defending against backdoor attacks in CIFAR-10 more challenging, as evidenced by the relatively poor performance of most defenses across different levels of data heterogeneity. Under strongly non-IID conditions (\(\alpha = 1\)), many baselines, including FLTrust, Median, and Multi-KRUM, suffer from high ASR despite retaining decent MTA. While FLIP holds up relatively well in some cases, particularly for CIFAR-10, its performance fluctuates, with ASR sometimes exceeding 30\% under non-IID conditions. This inconsistency limits its reliability, especially in high-stakes applications where consistent suppression of backdoor attacks is crucial. These observations highlight the difficulty of applying existing defenses to more complex FL tasks, especially under highly skewed data distributions. \anshuman{A reviewer may say "but FLIP seems to work decently well compared to ours, so it's not like it isn't doable. Maybe there is fluctuation across rounds? Or something that we can use to justify our performance.} \georgios{Can you check the output logs for FLIP for ASR fluctuations?}

In contrast, DROP demonstrates improved robustness under non-IID settings for EMNIST while it struggles for CIFAR-10. For EMNIST, DROP achieves near-zero ASR across all \(\alpha\) values, except under the most skewed setting (\(\alpha = 1\)) for which it scores ASR as low as 16.5\%. While CIFAR-10 remains a harder task, DROP \georgios{I do not know how to put it that we are doing very bad for CIFAR-10. Any ideas?}


\shortsection{Summary} Overall, DROP demonstrates unparalleled resilience against targeted backdoor attacks across all evaluated configurations. While there is a considerable trade-off in MTA, this sacrifice is justified by the near-complete suppression of adversarial influence. 
To address scenarios where minimizing MTA loss is crucial, we introduce DROPlet—a lightweight alternative inspired by DROP. Unlike DROP, DROPlet omits the knowledge distillation component, thereby avoiding cumulative MTA degradation over the course of training. Although DROPlet may not be as effective against stealthy, low-DPR attacks, it still provides substantial protection in many configurations, ensuring near-zero ASR in a broad range of settings. This positions DROP as a robust and reliable defense mechanism for federated learning systems, particularly in adversarial environments where maintaining high ASR stability and low backdoor susceptibility are critical.


