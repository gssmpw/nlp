%
\section{A semi-parametric additive errors model}
\label{sec:additive}
In the additive errors (AE) regime, we model $y$ as a deterministic function plus mean zero i.i.d. noise,
\begin{equation}
\label{eqn:additive_errors}
\begin{aligned}
(y \mid x, h=0) &=& \mu_0(x) + \epsilon  & &\\
(y \mid x, h=1) &=& \mu_1(x) + \epsilon &=& \mu_0(x) + \tau(x) + \epsilon\\
\epsilon &\sim& g(\epsilon)\,, && \mathbb{E}[\epsilon] = 0  \, , 
\end{aligned}
\end{equation}
where $\tau(x) = \mu_1(x) - \mu_0(x)$ is the expected difference in outcome between the responder and non-responder models. \cref{eqn:additive_errors} is common in many causal inference setups, particularly when estimating the conditional average treatment effect \citep[cf.][]{hahn:etal:2017:bcf,wager:athey:2018:causal-forests}. This setting can be thought of as a conditionally semi-parametric model. Conditioned on covariates $x$, the model consists of the parameters $\mu_0(x), \mu_1(x)$ and the infinite-dimensional model for the noise $\epsilon$. The AE model is strictly more flexible than a parametric model with a location parameter. Despite the added flexibility, we can show that this model is identifiable, meaning we can directly estimate the underlying parameters.

\begin{theorem}
\label{thm:additive-identify}
The model in \cref{eqn:additive_errors} is identifiable at every $x$.
\end{theorem}

The upshot to \cref{thm:additive-identify} is that we can fit a semi-parametric model to \cref{eqn:additive_errors} and use the resulting model to test the hypothesis $H_0: \mathbb{E}[y - \mu_0(x)] = 0$. 
We can also directly interpret $\tau(x)$ as the CARE for a sample with covariates $x$. 
Additionally, after fitting a semi-parametric model to \cref{eqn:additive_errors}, we can also recover the ERPF from our estimate of $\pi$.


To fit a model to \cref{eqn:additive_errors}, we follow a stagewise procedure. First, we fit a nonparametric regression function $\hat{\mu}_0(x)$ to the expected outcomes for the untreated population. Next, we use a nonparametric density estimator to marginally model the residual error distribution $\hat{g}$ of $y - \hat{\mu}_0(x)$ on the untreated population. Finally, we use an EM algorithm to fit nonparametric regression functions $\hat{\pi}$ and $\hat{\mu}_1$ for the prior and responder model, respectively, on the treated population, utilizing $\hat{g}$ and $\hat{\mu}_0$. The remainder of this section details the above approach.


\subsection{Estimating the non-response distribution}
\label{subsec:additive:null}
We model $\hat{\mu}_0$ using kernel ridge regression with a radial basis function kernel, and we fit it on the untreated population, using generalized cross-validation to select the bandwidth and regularization parameters~\citep{golub:etal:1979:gen-cv}. After fitting $\hat{\mu}_0$, we compute the leave-one-out predictions for each point in the untreated population, which can be calculated efficiently in closed form. We use these conditional expectations to estimate the residual noise distribution $g$. Cross-validation generally produces a biased-upward estimate of the true error distribution \citep{efron:gong:1983:leisurely-cv}. By overestimating the tails of the residual distribution, we conservatively bias the posterior probability of a response downward and the local FDR estimates upward.

We estimate the distribution of residuals $\hat{g}$ nonparametrically. We use predictive recursion \citep{martin:tokdar:2009:predictive-recursion} for its good rates of convergence and strong performance in other two-groups models \citep{scott:etal:2014:fdr-regression,tansey:etal:2018:fdr-smoothing}.
We choose the bandwidth by maximizing the predictive recursion marginal likelihood (PRML) \citep{martin:tokdar:2011:pr-marginal-likelihood}. This yields a tighter fit to the data and makes the density estimation routine fully auto-tuned and data-adaptive.

\subsection{Estimating the prior and response distributions}
\label{subsec:additive:alternative}
With $\hat{g}$ and $\hat{\mu}_0$ in hand, we form an estimate of the alternative distribution for the treated population. Specifically, we model each member of the treatment population as arising from the mixture model
\begin{equation}
\label{eqn:residual_mixture}
(y \mid x, t=1) \sim (1-\pi(x)) \hat{g}(y - \hat{\mu}_0(x)) + \pi(x) \hat{g}(y - \mu_1(x)) \, .
\end{equation}
We fit \cref{eqn:residual_mixture} via expectation-maximization with the following steps.

\paragraph{E-step.} Fix the estimates $\hat{\pi}$ and $\hat{\mu}_1$. Calculate the expected posterior probability of each data point coming from $\hat{g}(y - \mu_0(x))$,
\begin{equation*}
\label{eqn:additive_e_step}
\hat{w}_i = \frac{(1-\hat{\pi}(x_i)) \hat{g}(y_i - \hat{\mu}_0(x))}{(1-\hat{\pi}(x_i)) \hat{g}(y_i - \hat{\mu}_0(x)) + \hat{\pi}(x_i) \hat{g}(y_i - \hat{\mu}_1(x))} \, .
\end{equation*}
The posterior expectations $\hat{w}$ then serve as weights in the M-step.

\paragraph{M-step.} Fixing the weights $\hat{w}$, the optimization problem is separable in $\mu_1$ and $\pi$,
\begin{equation*}
\label{eqn:additive_m_step}
\begin{aligned}
\hat{\pi} &= \underset{\pi}{\text{argmax}} \sum_{i=1}^n \left[ \hat{w}_i \log(1-\pi(x_i)) + (1-\hat{w}_i) \log(\pi(x_i)) \right] \\
\hat{\mu}_1 &= \underset{\mu_1}{\text{argmax}} \sum_{i=1}^n (1-\hat{w}_i) \log(\hat{g}(y_i - \mu_1(x_i))) \, .
\end{aligned}
\end{equation*}
Both $\hat{\mu}_1$ and $\hat{\pi}$ (in logit-space) are encoded as linear models on top of random Fourier features, which approximate the full kernelized models and can be fit using gradient-based methods~\citep{rahimi:recht:2007:random-fourier-features}. The bandwidth parameters are selected using $k$-fold cross validation, and final predictions for the treated population are made on each held-out fold.

\subsection{Selecting responders}
\label{subsec:method:selection}
The final E-step estimate $\hat{w}$ has the appeal of both frequentist and Bayesian interpretations. Frequentists can interpret $\hat{w}\times 100$ as a local false discovery rate. Bayesians can interpret $1-\hat{w}$ as a posterior probability of treatment response. To select responders, we order the $\hat{w}$ values in ascending order and select the largest subset $\hat{S} \subset [n]$ such that $\frac{1}{|\hat{S}|} \sum_{i \in \hat{S}} \hat{w}_i \leq \alpha$, for a target $\alpha$-level FDR. While FDR control is guaranteed if we have the true posteriors~\citep{efron2012}, in practice with finite samples we will have some estimation error. Under some regularity assumptions, we can show that that excess FDR can be bounded above as a function of the estimation error.

\begin{assumption}
\label{assump:add-assumptions}
Let $\epsilon, \lambda, L, U > 0$ be given. Suppose the following holds for all $i=1,\ldots, n$:
\begin{itemize}
    \item[(i)] $g$ is $\lambda$-Lipschitz and bounded above by $U$,
    \item[(ii)] $(1-\pi(x_i))g(y_i - \mu_0(x_i)) + \pi(x_i) g(y_i - \mu_1(x_i)) \geq L$,
    \item [(iii)] $|\pi(x_i) - \hat{\pi}(x_i)|, |\mu_0(x_i) - \hat{\mu}_0(x_i)|, |\mu_1(x_i) - \hat{\mu}_1(x_i)| \leq \epsilon$, and
    \item[(iv)] $|g(y) - \hat{g}(y)| \leq \epsilon$ for all $y \in \R$.
\end{itemize}
\end{assumption}
Assumptions (i) and (ii) require the residual distribution and the treatment distributions to be well-behaved and are similar to standard assumptions made in the density estimation literature. Assumptions (iii) and (iv) reflect the quality of the estimators. This approximation error linearly translates to FDR.
\begin{theorem}
\label{thm:additive-fdr}
Suppose \cref{assump:add-assumptions} holds. If $\epsilon \leq \min \{1, L/4(U+2(\lambda + 1)) \}$, then the procedure outlined above results in FDR bounded by $\alpha + \delta$, where $\delta = \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon$.
\end{theorem}
Thus, so long as we are reasonably accurate in our estimates, the realized FDR will be close to the target FDR.
Further, the power of the procedure can also be bounded from below as a function of the error, where the power of a procedure that selects a subset $V \subset \{ i \in [n]: t_i=1 \}$ is the expected fraction of individuals with $h_i = 1$, i.e. $\E\left[ \frac{1}{n} \sum_{i \in V} \ind[h_i = 1] \right]$. As in the FDR bound, approximation error also translates linearly to power.
\begin{theorem}
\label{thm:additive-power}
Suppose \cref{assump:add-assumptions} holds. The power of the procedure is bounded below by 
\[ \frac{1-\alpha - \delta}{1-\alpha + \delta + 1/(n_{\opt}(\alpha - \delta) +1)} \opt(\alpha - \delta), \]
where $\opt(\beta)$ is the power of the Bayes' optimal procedure that achieves FDR bounded above by $\beta$, $n_{\opt}(\beta)$ is the corresponding number of selections, and $\delta = \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon$.
\end{theorem}
The number of selections plays a role in the quality of the power approximation in \cref{thm:additive-power}. This factor arises due to the discrete nature of the selection problem and the fact that it may not be possible to choose a subset with predicted FDR exactly equal to some quantity. However, as the number of optimal selections increases, the approximation ratio tends to $(1-\alpha-\delta)/(1-\alpha + \delta)$. Thus, with small model error, the selection procedure tends toward optimal power.
