\subsection{Proof of \Cref{thm:general-fdr}}

To prove \Cref{thm:general-fdr} we first need to prove some lemmas. The first lemma shows that under the conditions of \Cref{thm:general-fdr},  we can estimate $\pi^\star$ accurately.

\begin{lemma}
\label{lem:w_star-estimate}
Let $\epsilon, L, U > 0$ be given. Suppose the following holds for all $i=1, \ldots n$ and $y \in \Ycal$:
\begin{itemize}
    \item[(i)] $L \leq f_t(y \mid x_i), f_0(y \mid x_i) \leq U$,
    \item [(ii)] $|f_t(y \mid x_i) - \hat{f}_t(y \mid x_i)| \leq \epsilon$, and
    \item[(iii)] $|f_0(y \mid x_i) - \hat{f}_0(y \mid x_i)| \leq \epsilon$.
\end{itemize}
If $\epsilon \leq L/2$, then $|\hat{\pi}^\star(x_i) - \pi^\star(x_i)| \leq \frac{8 U \epsilon}{L^2}$ for all $i=1,\ldots, n$, where
$ \hat{\pi}^\star(x) = 1 - \min_y \frac{\hat{f}_t(y \mid x)}{\hat{f}_0(y \mid x)} .$
\end{lemma}
\begin{proof}
Pick some index $i$, and let $x = x_i$. Let $y = \argmin_y \frac{f_t(y \mid x)}{f_0(y \mid x)}$ and $\hat{y} = \argmin_y \frac{\hat{f}_t(y \mid x)}{\hat{f}_0(y \mid x)}$. Then we can write
\begin{align*}
\hat{\pi}^\star(x) - \pi^\star(x)
&=  \frac{f_t(y \mid x)}{f_0(y \mid x)} - \frac{\hat{f}_t(\hat{y} \mid x)}{\hat{f}_0(\hat{y} \mid x)}  \\
&= \frac{f_t(y \mid x)}{f_0(y \mid x)} - \frac{f_t(\hat{y} \mid x)}{f_0(\hat{y} \mid x)} + \frac{f_t(\hat{y} \mid x)}{f_0(\hat{y} \mid x)} - \frac{\hat{f}_t(\hat{y} \mid x)}{\hat{f}_0(\hat{y} \mid x)} \\
&\leq \frac{f_t(\hat{y} \mid x)}{f_0(\hat{y} \mid x)} - \frac{\hat{f}_t(\hat{y} \mid x)}{\hat{f}_0(\hat{y} \mid x)} \\
&= \frac{f_t(\hat{y} \mid x) \hat{f}_0(\hat{y} \mid x) - f_0(\hat{y} \mid x)\hat{f}_t(\hat{y} \mid x)}{f_0(\hat{y} \mid x) \hat{f}_0(\hat{y} \mid x)} \\
&\leq \frac{(\hat{f}_t(\hat{y} \mid x) + \epsilon )(f_0(\hat{y} \mid x) + \epsilon) - f_0(\hat{y} \mid x)\hat{f}_t(\hat{y} \mid x)}{f_0(\hat{y} \mid x) \hat{f}_0(\hat{y} \mid x)} \\
&\leq \frac{2\epsilon(U + \epsilon)}{L^2/2} \leq \frac{8U\epsilon}{L^2}.
\end{align*}
The first inequality comes from the definition of $y$, the second inequality comes from Assumptions (ii) and (iii), and the last two inequalities follow from Assumption (i). A symmetric line of reasoning that demonstrates $\pi^\star(x) - \hat{\pi}^\star(x) \leq \frac{8U\epsilon}{L^2}$ completes the proof.
\end{proof}

Our second lemma shows that the posterior estimate in \cref{eqn:generalized_w_estimate} is close to the ground truth value.

\begin{lemma}
\label{lem:posterior-approximation}
Let $\epsilon, L, U > 0$ be given. Suppose the following holds for all $i=1, \ldots n$ and $y \in \Ycal$:
\begin{itemize}
    \item[(i)] $L \leq f_t(y \mid x_i), f_0(y \mid x_i) \leq U$,
    \item [(ii)] $|f_t(y \mid x_i) - \hat{f}_t(y \mid x_i)| \leq \epsilon$, and
    \item[(iii)] $|f_0(y \mid x_i) - \hat{f}_0(y \mid x_i)| \leq \epsilon$.
\end{itemize}
If $\epsilon \leq \min (L/2, 1)$, then for all $i=1,\ldots, n$,
\[ \left|\frac{(1 - \pi^\star(x_i)) f_0(y_i \mid x_i)}{f_t(y_i \mid x_i)} - \frac{(1 - \hat{\pi}^\star(x_i)) \hat{f}_0(y_i \mid x_i)}{\hat{f}_t(y_i \mid x_i)} \right| \leq \frac{8U}{L^2} \left( 1 + \frac{12U}{L^2}\right) \epsilon.  \]
\end{lemma}
\begin{proof}
Let $x=x_i$, $y = y_i$. Then we have
\begin{align*}
&\frac{(1 - \pi^\star(x)) f_0(y \mid x)}{f_t(y \mid x)} - \frac{(1 - \hat{\pi}^\star(x)) \hat{f}_0(y \mid x)}{\hat{f}_t(y \mid x)}\\
&\hspace{3em}= \frac{(1 - \pi^\star(x)) f_0(y \mid x) \hat{f}_t(y \mid x) - (1 - \hat{\pi}^\star(x)) \hat{f}_0(y \mid x)f_t(y \mid x)}{f_t(y \mid x)\hat{f}_t(y \mid x)} \\
&\hspace{3em}\leq \frac{ ((1 - \hat{\pi}^\star(x)) + \frac{8U\epsilon}{L^2}) (\hat{f}_0(y \mid x) + \epsilon)(f_t(y \mid x) + \epsilon)- (1 - \hat{\pi}^\star(x)) \hat{f}_0(y \mid x)f_t(y \mid x)}{f_t(y \mid x)\hat{f}_t(y \mid x)} \\
&\hspace{3em}\leq \frac{(U + \epsilon)\epsilon + U\epsilon + \epsilon^2 + \frac{8U^2 \epsilon}{L^2}(U+\epsilon) + \frac{8U \epsilon}{L^2}\epsilon^2 (U + \epsilon) + \frac{8U^2 \epsilon^2}{L^2} + \frac{8U \epsilon^3}{L^2} }{L^2/2} \\
&\hspace{3em}\leq \frac{8U}{L^2} \left( 1 + \frac{12U}{L^2}\right) \epsilon. 
\end{align*}
Here, we have used \cref{lem:w_star-estimate} in the first inequality.
\end{proof}

We now turn to the proof of \cref{thm:general-fdr}, which is nearly identical to the proof of \cref{thm:additive-fdr}.

\begin{proof}[Proof of \cref{thm:general-fdr}]
Let $(x_1, y_1, t_1, h_1), \ldots, (x_n, y_n, t_n, h_n)$ denote draws from \cref{eqn:two_groups} (where $h_1,\ldots, h_n$ are unobserved). From the structure of \cref{eqn:two_groups}, we have that, conditioned on $(x_1, y_1, t_1), \ldots, (x_n, y_n, t_n)$, the random variables $\ind[h_i = 0]$ are Bernoulli random variables with biases $p_i = \pr(h_i = 0 \mid x_i, y_i, t_i)$.

From \Cref{lem:posterior-approximation}, we have that
\[\hat{w}_i \leq \pr(H=0 \mid x_i, y_i, t_i=1) + \frac{8U}{L^2} \left( 1 + \frac{12U}{L^2}\right) \epsilon \]
for all $i=1,\ldots, n$. If $S$ is a set of indices satisfying $t_i=1$ for all $i \in S$ and $\frac{1}{|S|} \sum_{i\in S} \hat{w}_i \leq \alpha$, then
\begin{align*}
\E \left[ \frac{1}{|S|} \sum_{i \in S} \ind[h_i = 0] \mid (x_1, y_1, t_1), \ldots, (x_n y_n, t_n) \right] 
&= \frac{1}{|S|} \sum_{i \in S} \E\left[ \ind[h_i = 0] \mid (x_1, y_1, t_1), \ldots, (x_n y_n, t_n) \right] \\
&=  \frac{1}{|S|} \sum_{i \in 
S} \hat{w}_i + \frac{8U}{L^2} \left( 1 + \frac{12U}{L^2}\right) \epsilon \\
&\leq \alpha + \frac{8U}{L^2} \left( 1 + \frac{12U}{L^2}\right)\epsilon.
\end{align*}
To prove the power statement, we first observe that the Bayes' optimal procedure is to select the largest set such that $\frac{1}{|V|} \sum_{i\in V} p_i \leq \alpha$, where $p_i = \pr(h_i = 0 \mid x_i, y_i, t_i)$ is formed using the prior $\pi^\star$ from \cref{lem:conservative-pi}. Then \cref{lem:prob-subset-sum} finishes the argument.
\end{proof}