\subsection{Proof of \Cref{thm:additive-fdr}}

We require the following result showing that we our posterior estimates are accurate.

\begin{lemma}
\label{lem:additive-posterior-approximation}
Suppose that the conditions of \Cref{thm:additive-fdr} hold. Then for all $i=1,\ldots, n$, the absolute difference
\[ \left|\frac{(1-{\pi}(x_i)) {g}(y_i - \mu_0(x_i))}{(1-{\pi}(x_i)) {g}(y_i - \mu_0(x_i)) + {\pi}(x_i) {g}(y_i - \mu_1(x_i))} - \frac{(1-\hat{\pi}(x_i)) \hat{g}(y_i - \hat{\mu_0}(x_i))}{(1-\hat{\pi}(x_i)) \hat{g}(y_i - \hat{\mu_0}(x_i)) + \hat{\pi}(x_i) \hat{g}(y_i - \hat{\mu_1}(x_i))} \right| \]
is bounded above by $\frac{2U(U+2(\lambda + 1))}{L^2} \epsilon. $
\end{lemma}
\begin{proof}
Pick an index $i=1, \ldots, n$. We use the following notation
\begin{align*}
g_0 = {g}(y_i - \mu_0(x_i)), \, 
g_1 = {g}(y_i - \mu_1(x_i)), \,
\hat{g_0} = \hat{g}(y_i - \hat{\mu_0}(x_i)), \,
\hat{g_1} = \hat{g}(y_i - \hat{\mu_1}(x_i)).
\end{align*}
The smoothness of $g$ combined with the approximation factor of $\hat{\mu_0}$ implies
\[ |g_0 - \hat{g_0}| \leq |{g}(y_i - \mu_0(x_i)) - {g}(y_i - \hat{\mu_0}(x_i))| + |{g}(y_i - \hat{\mu_0}(x_i)) - \hat{g}(y_i - \hat{\mu_0}(x_i))| \leq \epsilon(\lambda + 1).\]
A symmetrical argument shows the same holds for $|g_1 - \hat{g_1}|$. Thus, we can conclude the following
\begin{align*}
|\pi(x_i) g_1 - \hat{\pi}(x_i)\hat{g}_1| &\leq \epsilon(U + 2(\lambda + 1) ) \\
|(1-\pi(x_i)) g_0 - (1-\hat{\pi}(x_i))\hat{g}_0 | &\leq \epsilon(U + 2(\lambda + 1) ) \\
(1-\hat{\pi}(x_i))\hat{g}_0  + \hat{\pi}(x_i)\hat{g}_1 &\geq L/2 \\ 
(1-{\pi}(x_i)){g}_0  + {\pi}(x_i){g}_1 &\leq U.
\end{align*}
Using the shorthand $a = (1-\pi(x_i)) g_0$, $\hat{a} = (1-\hat{\pi}(x_i))\hat{g}_0$, $b = \pi(x_i) g_1$, and $\hat{b} = \hat{\pi}(x_i)\hat{g}_1$, we have
\begin{align*}
\left| \frac{a}{a+b} - \frac{\hat{a}}{\hat{a} + \hat{b}} \right| 
&= \left|\frac{\hat{a}b - \hat{b}a}{(a+b)(\hat{a} + \hat{b})} \right| \\
&\leq \frac{2}{L^2}\left|\hat{a}b - \hat{b}a\right| \\
&\leq \frac{2}{L^2} (a+b) \epsilon (U + 2) = \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon. \qedhere
\end{align*}

\end{proof}


Our next lemma is an elementary result concerning sums of probabilities.
\begin{lemma}
\label{lem:prob-subset-sum}
Let $p_1, \hat{p}_1, p_2, \hat{p_2}, \ldots, p_n, \hat{p}_n, \alpha, \delta \in [0,1]$ such that $\left|p_i - \hat{p_i}\right| \leq \delta$ for all $i=1,\ldots, n$. Let $V, \hat{V} \subset \{1,\ldots, n\}$ be the largest subsets such that 
\[ \frac{1}{|V|} \sum_{i\in V} p_i \leq \alpha - \delta \text{ and }  \frac{1}{|\hat{V}|} \sum_{i\in \hat{V}} \hat{p}_i \leq \alpha .  \]
Then 
\[\sum_{i\in \hat{V}} 1 - {p}_i \geq \frac{1 - \alpha - \delta}{1 - \alpha + \delta + 1/(|V|+1)}\sum_{i\in {V}} 1 - {p}_i.\]
\end{lemma}
\begin{proof}
Observe first that $|\hat{V}| \geq |V|$, since 
\[ \frac{1}{|{V}|} \sum_{i\in {V}} \hat{p}_i \leq \frac{1}{|{V}|} \sum_{i\in {V}} ({p}_i + \delta) \leq \alpha. \]
Now observe that
\[ \sum_{i\in \hat{V}} 1 - {p}_i \geq \sum_{i\in \hat{V}} 1 - \hat{p}_i - \delta = |\hat{V}|(1 - \alpha - \delta) \geq |{V}|(1 - \alpha - \delta). \]
On the other hand, by considering adding a single element to $V$, we have
\[  \frac{1}{|V|} \sum_{i \in V} p_i \geq \alpha - \delta - \frac{1}{|V|+1}.  \]
Thus,
\[ \sum_{i\in {V}} 1 - {p}_i \leq |V|\left( 1 - \alpha + \delta + \frac{1}{|V|+1} \right). \]
Taking ratios completes the proof.
\end{proof}


We now turn to the proof of \cref{thm:additive-fdr}.

\begin{proof}[Proof of \cref{thm:additive-fdr}]
Let $(x_1, y_1, t_1, h_1), \ldots, (x_n, y_n, t_n, h_n)$ denote the samples from \cref{eqn:additive_errors} (where $h_1,\ldots, h_n$ are unobserved). From the structure of \cref{eqn:additive_errors}, we have that, conditioned on $(x_1, y_1, t_1), \ldots, (x_n, y_n, t_n)$, the random variables $\ind[h_i = 0]$ are Bernoulli random variables with biases $p_i = \pr(h_i = 0 \mid x_i, y_i, t_i)$.

From \Cref{lem:additive-posterior-approximation}, we have that
\[ \left| \hat{w}_i - \pr(H=0 \mid x_i, y_i, t_i=1)\right| \leq \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon \]
for all $i=1,\ldots, n$. If $S$ is a set of indices satisfying $t_i=1$ for all $i \in S$ and $\frac{1}{|S|} \sum_{i\in S} \hat{w}_i \leq \alpha$, then
\begin{align*}
\E \left[ \frac{1}{|S|} \sum_{i \in S} \ind[h_i = 0] \mid (x_1, y_1, t_1), \ldots, (x_n y_n, t_n) \right] 
&= \frac{1}{|S|} \sum_{i \in S} \E\left[ \ind[h_i = 0] \mid (x_1, y_1, t_1), \ldots, (x_n y_n, t_n) \right] \\
&=  \frac{1}{|S|} \sum_{i \in 
S} \hat{w}_i +\frac{2U(U+2(\lambda + 1))}{L^2} \epsilon \\
&\leq \alpha + \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon. 
\end{align*}

To prove the power statement, observe that the Bayes' optimal procedure is to choose the largest set $V$ such that $ \frac{1}{|V|}\sum_{i \in V} p_i \leq \alpha$. Then the power conclusion follows directly from \cref{lem:prob-subset-sum} with $\delta = \frac{2U(U+2(\lambda + 1))}{L^2} \epsilon$.
\end{proof}