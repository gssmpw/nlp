\subsection{Proof of \cref{prop:normal-identifiability}}

Fix $x$. In \Cref{eqn:two_groups}, we observe distributions $f_0(y \mid x)$ and $f_t(y \mid x)$. Suppose we are given $\mu_0 = \mu_0(x)$, $\mu_1 = \mu_1(x)$, $\mu_2 = \mu_2(x)$, $\sigma_0^2 = \sigma_0^2(x)$, $\sigma_1^2 = \sigma_1^2(x)$, $\sigma_2^2 = \sigma_2^2(x)$,  $\pi_1 = \pi_1(x)$, and $\pi_2 = \pi_2(x)$, where $\pi_1, \pi_2 \in (0,1)$. By translating and scaling the space, we can assume without loss of generality that $\mu_0 = 0$ and $\sigma_0^2 = 1$. Our goal is to show that if
\begin{align}
\label{eqn:normal-identifiability}
(1-\pi_1)\Ncal(y \mid 0, 1) + \pi_1 \Ncal(y \mid \mu_1, \sigma_1^2) = (1-\pi_2)\Ncal(y \mid 0, 1) + \pi_2 \Ncal(y \mid \mu_2, \sigma_2^2) 
\end{align}
for all $y \in \R$, then we must have $\pi_1=\pi_2$, $\mu_1 = \mu_2$, and $\sigma_1^2 = \sigma_2^2$. There are two cases to consider here.

\paragraph{Case 1: $\pi_1 = \pi_2$.} In this case, we immediately have
\[ \Ncal(y \mid \mu_1, \sigma_1^2) = \Ncal(y \mid \mu_2, \sigma_2^2) \]
for all $y \in \R$. Taking the first moment of both sides, we have $\mu_1 = \mu_2$. Conditioned on this fact, we can calculate the variance of both sides to conclude $\sigma_1^2 = \sigma_2^2$. 

\paragraph{Case 2: $\pi_1 \neq \pi_2$.} In this case, we will come to a contradiction. Assume without loss of generality that $\pi_1 > \pi_2$. Letting $\beta = \frac{\pi_2}{\pi_1 - \pi_2} > 0$, we can rearrange \cref{eqn:normal-identifiability} to get
\[ \Ncal(y \mid 0, 1) = (1+\beta) \Ncal(y \mid \mu_1, \sigma_1^2) - \beta \Ncal(y \mid \mu_2, \sigma_2^2).  \]
Multiplying both sides by $e^{ty}$ and integrating $y \in \R$, the moment generating function of the normal distribution gives us
\begin{align}
\label{eqn:normal-identifiability-mgf}
\exp\left( t^2/2 \right)
= (1+\beta)\exp\left( \mu_1 t + \sigma_1^2 t^2/2 \right) - \beta \exp\left( \mu_2 t + \sigma_2^2 t^2/2 \right)   
\end{align}
for all $t \in \R$. We will analyze \cref{eqn:normal-identifiability-mgf} in the limit as $t\rightarrow \infty$.

Observe first that we must have $\sigma_2^2 \leq \sigma_1^2$, since otherwise as $t\rightarrow \infty$, the RHS of \cref{eqn:normal-identifiability-mgf} tends to $-\infty$ while the LHS is positive. 

Now consider the case where $\sigma_2^2 < \sigma_1^2$. Multiplying \cref{eqn:normal-identifiability-mgf} through by $e^{-\mu_1 t - \sigma_1^2 t^2/2}$, we have
\[ \exp\left( -\mu_1 t - (\sigma_1^2 - 1) t^2/2 \right) = 1+\beta - \beta \exp\left( (\mu_2 - \mu_1) t - (\sigma_1^2 -\sigma_2^2) t^2/2 \right).  \]
As $t\rightarrow \infty$, the RHS tends to $1 + \beta$. On the other hand, the LHS must tend to either 0 or $+\infty$. Thus we must have $\sigma_1^2 = \sigma_2^2$. In which case, we can rearrange \cref{eqn:normal-identifiability-mgf} to obtain
\[ 1 = \left[(1+\beta)\exp(\mu_1 t) - \beta \exp(\mu_2 t) \right] \exp\left( (\sigma_1^2 - 1)t^2/2 \right). \]
If $\sigma_1^2 < 1$, then the RHS must tend to 0. If $\sigma_1^2 > 1$, then the RHS must tend to $\pm \infty$. Thus, we can conclude $\sigma_1^2 = 1$, and the above equation becomes
\[ 1 = \left[(1+\beta)\exp(\mu_1 t) - \beta \exp(\mu_2 t) \right]. \]
The only way for this to hold for all $t$ is if $\mu_1 = \mu_2=0$.