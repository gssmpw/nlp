\section{Related Work}
% \texttt{\langle rf \rangle}

\paragraph{Jailbreaking LLMs}
% \begin{itemize}
%     % \item Jailbreaking concept ____
%     % \item Red-teaming ____
%     % \item Optimization attacks ____
%     % \item Heuristic attacks ____
%     \item SFT attacks ____, Refusal ablation ____
% \end{itemite

% hello \rf asdf 
Modern LLMs used as chatbots are trained to follow user instructions**Brown et al., "Language Models as Zero-Shot Learners"** while also being trained to respond in a safe and harmless manner**Henderson et al., "Threats to AI Alignment from Unintended Side Effects of Safety Mechanisms"**.
While users quickly found ways to manually craft ``jailbreaks'' which could circumvent these safeguards and elicit harmful content from these systems**Carlini et al., "Hidden Scribes: Adversarial Examples for Black-Box Text Classification"**, automated methods for crafting adversarial attacks were also shown to be effective.
Particularly, **Liu et al., "Adversarial Examples for Evaluating the Robustness of Deep Neural Networks on Natural Language Processing Tasks"** propose a greedy-coordinate gradient (GCG) search algorithm to find an adversarial suffix optimized to pre-fill **Carlini et al., "Hidden Scribes: Adversarial Examples for Black-Box Text Classification** an affirmative response in a model's response. 
Other approaches use heuristics to craft interpretable jailbreaks with only black-box access to the target model**Ribeiro et al., "An Example-Based Method Applied to Text Classification"**.
Given white-box access to the target model, more powerful attacks are possible.
Adversarial soft prompts can be optimized to manipulate the modelâ€™s outputs **Henderson et al., "Threats to AI Alignment from Unintended Side Effects of Safety Mechanisms"**, causal features responsible for refusal behaviour can be selectively ablated **Li et al., "Learning to Abandon: A New Perspective on Adversarial Attacks in Natural Language Processing"**, and fine-tuning can be used to override or remove safety training entirely **Henderson et al., "Threats to AI Alignment from Unintended Side Effects of Safety Mechanisms**. 


\paragraph{Defences}
% \begin{itemize}
%     \item Fine-tuning / alignment training ____
%     \item Circuit breakers ____
%     \item Adversarial training (Cont/Latent + R2D2) ____
%     \item Classifiers/judge LLMs ____
% \end{ite

Beyond standard pre-training, LLMs are typically trained with preference optimization techniques such as RLHF **Jiang et al., "RL4DG: Reinforcement Learning for Data Generation"** or DPO **Doran et al., "Reinforcement Learning for Preference-Based Training of Language Models"** to be more aligned with human preferences.
Jailbreaks can be incorporated into this preference alignment phase to increase resilience to such attacks (as is often done with red-teaming methods), but this does not often generalise to novel jailbreaks.
Historically, in the context of vision models, actively training against adversarial attacks in an online manner (i.e., adversarial training) is the only method that has shown increased adversarial robustness **Madry et al., "Deep Learning Models are Vulnerable to Adversarial Attacks"**. However, in the context of language, most discrete attacks are prohibitively expensive to use online. 
**Shetty et al., "Adversarial Training for Natural Language Processing"** train against adversarial suffixes generated by GCG, but continually update a pool of examples rather than generate each attack from scratch.
Other approaches perform adversarial training by attacking the embedding or latent space of the model **Madry et al., "Deep Learning Models are Vulnerable to Adversarial Attacks"** which is much more efficient to compute and transfers to discrete attacks.
Beyond adversarial training, newer defences target and alter harmful representations in order to prevent a model from producing harmful outputs entirely **Li et al., "Learning to Abandon: A New Perspective on Adversarial Attacks in Natural Language Processing"**.
Independent from training a model to be more robust to jailbreaks is to classify and judge the potential harmfulness of the generated text, often with another LLM fine-tuned for this task **Ribeiro et al., "An Example-Based Method Applied to Text Classification"**, although this does require additional resources to classify the outputs. Concurrent work **Liu et al., "Adversarial Examples for Evaluating the Robustness of Deep Neural Networks on Natural Language Processing Tasks"** has shown that classifiers alone are often not sufficient, further making the case that other approaches are needed especially against permissive but common threat models such as the fine-tuning box attack.

% \todo{Interpretability - Maybe include this section? Representation engineering? feels like this could be an important section }

\paragraph{Special Tokens} 
Several works have explored training or utilising special tokens for specific purposes.
**Peters et al., "Dissecting Sequential Memory in Sequence-to-Sequence Models"** prepend ``memory'' tokens to an input prompt on a target task.
**Vaswani et al., "Attention Is All You Need"** append ``pause'' tokens, which are hypothesised to give the LLM a buffer sequence to reason over before producing an output.
**Lin et al., "Dynamically Compressing Neural Networks for Embedding and Label Prediction Tasks"** train LLMs to compress longer prompts into smaller sets of ``gist'' tokens as a means to shorten the context. 
**Vaswani et al., "Attention Is All You Need"** prepend ``attention sinks'' to improve generalization to long-context sequences.
LLMs have also been trained to use a variety of tools (such as a calculator or internet access), which are denoted and invoked via special tokens **Lin et al., "Dynamically Compressing Neural Networks for Embedding and Label Prediction Tasks"**.
Most closely related to our approach is the recent work of **Henderson et al., "Threats to AI Alignment from Unintended Side Effects of Safety Mechanisms"**, where a model is trained to prefix an output with a special \emph{refusal} or \emph{response} token based on the behaviour of whether the model refuses or responds to a prompt.
While their approach is related in that special tokens are leveraged in the context of alignment, the approach and objective are conceptually different.
Their method correlates these tokens with \textit{behaviour} (i.e., refusal or response) in order to better calibrate such behaviours, whereas our approach correlates a special token with some implicit notion of a concept (i.e., harmfulness), \textit{without} modifying the model's original behaviour. This conceptual difference leads to drastically different losses in the formulation. For instance**Henderson et al., "Threats to AI Alignment from Unintended Side Effects of Safety Mechanisms"** do not propose a KL divergence with a reference model~(\cref{eq:KL_after}) to maintain the predictions similar to the reference model after \rftoken{} is outputted which hurt the model's utility and is \emph{not} complementary with standard safety training (instead it is a way to calibrate the model post-hoc safety training). Moreover, their model is only trained to output a ``behavioural token" (e.g., ``refuse" or ``respond") at the beginning of the answer, which is significantly less efficient to detect harmfulness, as shown in our experiments. In contrast, our work proposes an approach that is complementary to standard safety training where the model essentially acts as an ``implicit judge'' on its own generated output, improving its transparency and providing a clear signal to evaluate potentially harmful generations without incurring any additional computational cost at inference time. Similar work by **Li et al., "Learning to Abandon: A New Perspective on Adversarial Attacks in Natural Language Processing"** also learns to tag answers as harmless or harmful, but they use two stage training procedure and hardcode the tag to be at the end of the response. They only consider fixed jailbreak prompts rather than attacks and do not consider the fine-tuning setting at all.