\section{Related Work}
% \texttt{\langle rf \rangle}

\paragraph{Jailbreaking LLMs}
% \begin{itemize}
%     % \item Jailbreaking concept \cite{wei_jailbroken_2023}
%     % \item Red-teaming \cite{perez_red_2022}
%     % \item Optimization attacks \citep{zou_universal_2023, schwinn_soft_2024}
%     % \item Heuristic attacks \citep{chao_jailbreaking_2023,vega_bypassing_2023, liu_autodan_2023, zeng_johnny_2024}
%     \item SFT attacks \citep{qi_fine-tuning_2023}, Refusal ablation \citep{arditi_refusal_2024}
% \end{itemize}

% hello \rf asdf 
Modern LLMs used as chatbots are trained to follow user instructions~\citep{ouyang_training_2022} while also being trained to respond in a safe and harmless manner~\citep{perez_red_2022}.
While users quickly found ways to manually craft ``jailbreaks'' which could circumvent these safeguards and elicit harmful content from these systems~\citep{wei_jailbroken_2023}, automated methods for crafting adversarial attacks were also shown to be effective.
Particularly,~\citet{zou_universal_2023} propose a greedy-coordinate gradient (GCG) search algorithm to find an adversarial suffix optimized to pre-fill \citep{vega_bypassing_2023} an affirmative response in a model's response. 
Other approaches use heuristics to craft interpretable jailbreaks with only black-box access to the target model~\citep{chao_jailbreaking_2023, liu_autodan_2023, zeng_johnny_2024}.
Given white-box access to the target model, more powerful attacks are possible.
Adversarial soft prompts can be optimized to manipulate the modelâ€™s outputs \citep{schwinn_soft_2024}, causal features responsible for refusal behaviour can be selectively ablated \citep{arditi_refusal_2024}, and fine-tuning can be used to override or remove safety training entirely \citep{qi_fine-tuning_2023}. 


\paragraph{Defences}
% \begin{itemize}
%     \item Fine-tuning / alignment training \citep{ouyang_training_2022, rafailov_direct_2023}
%     \item Circuit breakers \citep{zou_improving_2024}
%     \item Adversarial training (Cont/Latent + R2D2) \citep{xhonneux_efficient_2024, sheshadri_targeted_2024, mazeika_harmbench_2024}
%     \item Classifiers/judge LLMs \citep{inan_llama_2023, feuer_style_2024}
% \end{itemize}

Beyond standard pre-training, LLMs are typically trained with preference optimization techniques such as RLHF \citep{ouyang_training_2022} or DPO \citep{rafailov_direct_2023} to be more aligned with human preferences.
Jailbreaks can be incorporated into this preference alignment phase to increase resilience to such attacks (as is often done with red-teaming methods), but this does not often generalise to novel jailbreaks.
Historically, in the context of vision models, actively training against adversarial attacks in an online manner (i.e., adversarial training) is the only method that has shown increased adversarial robustness \citep{madry_towards_2017}. However, in the context of language, most discrete attacks are prohibitively expensive to use online. 
\citet{mazeika_harmbench_2024} train against adversarial suffixes generated by GCG, but continually update a pool of examples rather than generate each attack from scratch.
Other approaches perform adversarial training by attacking the embedding or latent space of the model~\citep{xhonneux_efficient_2024, sheshadri_latent_2024} which is much more efficient to compute and transfers to discrete attacks.
Beyond adversarial training, newer defences target and alter harmful representations in order to prevent a model from producing harmful outputs entirely \citep{zou_improving_2024}.
Independent from training a model to be more robust to jailbreaks is to classify and judge the potential harmfulness of the generated text, often with another LLM fine-tuned for this task \citep{inan_llama_2023, feuer_style_2024}, although this does require additional resources to classify the outputs. Concurrent work \citet{huang_virus_2025} has shown that classifiers alone are often not sufficient, further making the case that other approaches are needed especially against permissive but common threat models such as the fine-tuning box attack.

% \todo{Interpretability - Maybe include this section? Representation engineering? feels like this could be an important section }

\paragraph{Special Tokens} 
Several works have explored training or utilising special tokens for specific purposes.
\citet{burtsev_memory_2020} prepend ``memory'' tokens to an input prompt on a target task.
\citet{goyal_think_2023} append ``pause'' tokens, which are hypothesised to give the LLM a buffer sequence to reason over before producing an output.
\citet{mu_learning_2023} train LLMs to compress longer prompts into smaller sets of ``gist'' tokens as a means to shorten the context. 
\citet{xiao_efficient_2023} prepend ``attention sinks'' to improve generalization to long-context sequences.
LLMs have also been trained to use a variety of tools (such as a calculator or internet access), which are denoted and invoked via special tokens \citep{schick_toolformer_2023}.
Most closely related to our approach is the recent work of \citet{jain_refusal_2024}, where a model is trained to prefix an output with a special \emph{refusal} or \emph{response} token based on the behaviour of whether the model refuses or responds to a prompt.
While their approach is related in that special tokens are leveraged in the context of alignment, the approach and objective are conceptually different.
Their method correlates these tokens with \textit{behaviour} (i.e., refusal or response) in order to better calibrate such behaviours, whereas our approach correlates a special token with some implicit notion of a concept (i.e., harmfulness), \textit{without} modifying the model's original behaviour. This conceptual difference leads to drastically different losses in the formulation. For instance~\citet{jain_refusal_2024} do not propose a KL divergence with a reference model~(\cref{eq:KL_after}) to maintain the predictions similar to the reference model after \rftoken{} is outputted which hurt the model's utility and is \emph{not} complementary with standard safety training (instead it is a way to calibrate the model post-hoc safety training). Moreover, their model is only trained to output a ``behavioural token" (e.g., ``refuse" or ``respond") at the beginning of the answer, which is significantly less efficient to detect harmfulness, as shown in our experiments. In contrast, our work proposes an approach that is complementary to standard safety training where the model essentially acts as an ``implicit judge'' on its own generated output, improving its transparency and providing a clear signal to evaluate potentially harmful generations without incurring any additional computational cost at inference time. Similar work by \citet{wang_self-guard_2024} also learns to tag answers as harmless or harmful, but they use two stage training procedure and hardcode the tag to be at the end of the response. They only consider fixed jailbreak prompts rather than attacks and do not consider the fine-tuning setting at all.