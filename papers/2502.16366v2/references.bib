
@misc{mehrotra_tree_2023,
	title = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
	shorttitle = {Tree of {Attacks}},
	url = {http://arxiv.org/abs/2312.02119},
	abstract = {While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of humandesigned jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80\% of the prompts using only a small number of queries. This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks.},
	language = {en},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02119 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Comment: An implementation of the presented method is available at https://github.com/RICommunity/TAP},
	file = {Mehrotra et al. - 2023 - Tree of Attacks Jailbreaking Black-Box LLMs Autom.pdf:/Users/sophiex/Zotero/storage/R9BQTZQ2/Mehrotra et al. - 2023 - Tree of Attacks Jailbreaking Black-Box LLMs Autom.pdf:application/pdf},
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={ICLR},
  year={2021}
}

@misc{mazeika_harmbench_2024,
	title = {{HarmBench}: {A} {Standardized} {Evaluation} {Framework} for {Automated} {Red} {Teaming} and {Robust} {Refusal}},
	shorttitle = {{HarmBench}},
	url = {http://arxiv.org/abs/2402.04249},
	abstract = {Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a largescale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github. com/centerforaisafety/HarmBench.},
	language = {en},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04249 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://www.harmbench.org},
	file = {Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for.pdf:/Users/sophiex/Zotero/storage/HF9X9GB6/Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for.pdf:application/pdf},
}

@misc{he_data_2024,
	title = {Data {Poisoning} for {In}-context {Learning}},
	url = {http://arxiv.org/abs/2402.02160},
	abstract = {In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL’s susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT4 model, demonstrate that ICL’s performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.},
	language = {en},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {He, Pengfei and Xu, Han and Xing, Yue and Liu, Hui and Yamada, Makoto and Tang, Jiliang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02160 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {He et al. - 2024 - Data Poisoning for In-context Learning.pdf:/Users/sophiex/Zotero/storage/XDRSLBCW/He et al. - 2024 - Data Poisoning for In-context Learning.pdf:application/pdf},
}

@misc{wichers_gradient-based_2024,
	title = {Gradient-{Based} {Language} {Model} {Red} {Teaming}},
	url = {http://arxiv.org/abs/2401.16656},
	abstract = {Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses than a strong reinforcement learning-based red teaming approach, and succeeds even when the LM has been fine-tuned to produce safer outputs.},
	language = {en},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wichers, Nevan and Denison, Carson and Beirami, Ahmad},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16656 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EACL 2024 main conference},
	file = {Wichers et al. - 2024 - Gradient-Based Language Model Red Teaming.pdf:/Users/sophiex/Zotero/storage/3S6A8592/Wichers et al. - 2024 - Gradient-Based Language Model Red Teaming.pdf:application/pdf},
}

@misc{sitawarin_pal_2024,
	title = {{PAL}: {Proxy}-{Guided} {Black}-{Box} {Attack} on {Large} {Language} {Models}},
	shorttitle = {{PAL}},
	url = {http://arxiv.org/abs/2402.09674},
	abstract = {Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84\% attack success rate (ASR) on GPT3.5-Turbo and 48\% on Llama-2-7B, compared to 4\% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94\% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found at https://github.com/chawins/pal.},
	language = {en},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Sitawarin, Chawin and Mu, Norman and Wagner, David and Araujo, Alexandre},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {2402.09674.pdf:/Users/sophiex/Zotero/storage/UGBNDXWG/2402.09674.pdf:application/pdf},
}

@misc{casper_defending_2024,
	title = {Defending {Against} {Unforeseen} {Failure} {Modes} with {Latent} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2403.05030},
	abstract = {AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
	language = {en},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf:/Users/sophiex/Zotero/storage/JMG2RYF7/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf:application/pdf},
}

@article{huang_catastrophic_2024,
	title = {{CATASTROPHIC} {JAILBREAK} {OF} {OPEN}-{SOURCE} {LLMS} {VIA} {EXPLOITING} {GENERATION}},
	abstract = {Content warning: This paper contains examples of harmful language. The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as “jailbreaks”. These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from 0\% to more than 95\% across 11 language models including LLAMA2, VICUNA, FALCON, and MPT families, outperforming state-of-the-art attacks with 30× lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models1.},
	language = {en},
	author = {Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
	year = {2024},
	file = {Huang et al. - 2024 - CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXP.pdf:/Users/sophiex/Zotero/storage/9SN9G825/Huang et al. - 2024 - CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXP.pdf:application/pdf},
}

@misc{casper_defending_2024-1,
	title = {Defending {Against} {Unforeseen} {Failure} {Modes} with {Latent} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2403.05030},
	abstract = {AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
	language = {en},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf:/Users/sophiex/Zotero/storage/7RQMMVYR/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf:application/pdf},
}

@misc{chacko_adversarial_2024,
	title = {Adversarial {Attacks} on {Large} {Language} {Models} {Using} {Regularized} {Relaxation}},
	url = {http://arxiv.org/abs/2410.19160},
	abstract = {As powerful Large Language Models (LLMs) are now widely used for numerous practical applications, their safety is of critical importance. While alignment techniques have significantly improved overall safety, LLMs remain vulnerable to carefully crafted adversarial inputs. Consequently, adversarial attack methods are extensively used to study and understand these vulnerabilities. However, current attack methods face significant limitations. Those relying on optimizing discrete tokens suffer from limited efficiency, while continuous optimization techniques fail to generate valid tokens from the model's vocabulary, rendering them impractical for real-world applications. In this paper, we propose a novel technique for adversarial attacks that overcomes these limitations by leveraging regularized gradients with continuous optimization methods. Our approach is two orders of magnitude faster than the state-of-the-art greedy coordinate gradient-based method, significantly improving the attack success rate on aligned language models. Moreover, it generates valid tokens, addressing a fundamental limitation of existing continuous optimization methods. We demonstrate the effectiveness of our attack on five state-of-the-art LLMs using four datasets.},
	language = {en},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Chacko, Samuel Jacob and Biswas, Sajib and Islam, Chashi Mahiul and Liza, Fatema Tabassum and Liu, Xiuwen},
	month = oct,
	year = {2024},
	note = {arXiv:2410.19160 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 6 figures},
	file = {Chacko et al. - 2024 - Adversarial Attacks on Large Language Models Using.pdf:/Users/sophiex/Zotero/storage/IJDA6CJM/Chacko et al. - 2024 - Adversarial Attacks on Large Language Models Using.pdf:application/pdf},
}

@misc{boreiko_realistic_2024,
	title = {A {Realistic} {Threat} {Model} for {Large} {Language} {Model} {Jailbreaks}},
	url = {https://arxiv.org/abs/2410.16222v1},
	abstract = {A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. In their original settings, these methods all largely succeed in coercing the target output, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model combines constraints in perplexity, measuring how far a jailbreak deviates from natural text, and computational budget, in total FLOPs. For the former, we build an N-gram model on 1T tokens, which, in contrast to model-based perplexity, allows for an LLM-agnostic and inherently interpretable evaluation. We adapt popular attacks to this new, realistic threat model, with which we, for the first time, benchmark these attacks on equal footing. After a rigorous comparison, we not only find attack success rates against safety-tuned modern models to be lower than previously presented but also find that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent N-grams, either selecting N-grams absent from real-world text or rare ones, e.g. specific to code datasets.},
	language = {en},
	urldate = {2024-11-29},
	journal = {arXiv.org},
	author = {Boreiko, Valentyn and Panfilov, Alexander and Voracek, Vaclav and Hein, Matthias and Geiping, Jonas},
	month = oct,
	year = {2024},
	file = {Full Text PDF:/Users/sophiex/Zotero/storage/YCZUZD3P/Boreiko et al. - 2024 - A Realistic Threat Model for Large Language Model Jailbreaks.pdf:application/pdf},
}

@misc{huang_virus_2025,
	title = {Virus: {Harmful} {Fine}-tuning {Attack} for {Large} {Language} {Models} {Bypassing} {Guardrail} {Moderation}},
	shorttitle = {Virus},
	url = {http://arxiv.org/abs/2501.17433},
	doi = {10.48550/arXiv.2501.17433},
	abstract = {Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100{\textbackslash}\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: {\textbackslash}textbf\{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack\}, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
	month = jan,
	year = {2025},
	note = {arXiv:2501.17433 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/3V7M9KW5/Huang et al. - 2025 - Virus Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/WC8JXRAU/2501.html:text/html},
}


@misc{qi_safety_2024,
	title = {Safety {Alignment} {Should} {Be} {Made} {More} {Than} {Just} a {Few} {Tokens} {Deep}},
	url = {http://arxiv.org/abs/2406.05946},
	doi = {10.48550/arXiv.2406.05946},
	abstract = {The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05946},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/LYRI8UMN/Qi et al. - 2024 - Safety Alignment Should Be Made More Than Just a Few Tokens Deep.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/RPZLFK35/2406.html:text/html},
}

@misc{qi_fine-tuning_2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {http://arxiv.org/abs/2310.03693},
	doi = {10.48550/arXiv.2310.03693},
	abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03693 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/LA4YDTCV/Qi et al. - 2023 - Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/TCPYEVXT/2310.html:text/html},
}

@misc{jain_refusal_2024,
	title = {Refusal {Tokens}: {A} {Simple} {Way} to {Calibrate} {Refusals} in {Large} {Language} {Models}},
	shorttitle = {Refusal {Tokens}},
	url = {http://arxiv.org/abs/2412.06748},
	doi = {10.48550/arXiv.2412.06748},
	abstract = {A key component of building safe and reliable language models is enabling the models to appropriately refuse to follow certain instructions or answer certain questions. We may want models to output refusal messages for various categories of user queries, for example, ill-posed questions, instructions for committing illegal acts, or queries which require information past the model's knowledge horizon. Engineering models that refuse to answer such questions is complicated by the fact that an individual may want their model to exhibit varying levels of sensitivity for refusing queries of various categories, and different users may want different refusal rates. The current default approach involves training multiple models with varying proportions of refusal messages from each category to achieve the desired refusal rates, which is computationally expensive and may require training a new model to accommodate each user's desired preference over refusal rates. To address these challenges, we propose refusal tokens, one such token for each refusal category or a single refusal token, which are prepended to the model's responses during training. We then show how to increase or decrease the probability of generating the refusal token for each category during inference to steer the model's refusal behavior. Refusal tokens enable controlling a single model's refusal rates without the need of any further fine-tuning, but only by selectively intervening during generation.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Jain, Neel and Shrivastava, Aditya and Zhu, Chenyang and Liu, Daben and Samuel, Alfy and Panda, Ashwinee and Kumar, Anoop and Goldblum, Micah and Goldstein, Tom},
	month = dec,
	year = {2024},
	note = {arXiv:2412.06748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 19 pages},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/9QYEC4RX/Jain et al. - 2024 - Refusal Tokens A Simple Way to Calibrate Refusals in Large Language Models.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/J8NFSLKW/2412.html:text/html},
}

@misc{fort_scaling_2023,
	title = {Scaling {Laws} for {Adversarial} {Attacks} on {Language} {Model} {Activations}},
	url = {http://arxiv.org/abs/2312.02780},
	doi = {10.48550/arXiv.2312.02780},
	abstract = {We explore a class of adversarial attacks targeting the activations of language models. By manipulating a relatively small subset of model activations, \$a\$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens \$t\$. We empirically verify a scaling law where the maximum number of target tokens \$t\_{\textbackslash}mathrm\{max\}\$ predicted depends linearly on the number of tokens \$a\$ whose activations the attacker controls as \$t\_{\textbackslash}mathrm\{max\} = {\textbackslash}kappa a\$. We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance \${\textbackslash}chi\$) is remarkably constant between \${\textbackslash}approx 16\$ and \${\textbackslash}approx 25\$ over 2 orders of magnitude of model sizes for different language models. Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits. This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces. A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input. This opens up a new, broad attack surface. By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Fort, Stanislav},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02780 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 9 figures},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/DQAACLT3/Fort - 2023 - Scaling Laws for Adversarial Attacks on Language Model Activations.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/6Y9L52WQ/2312.html:text/html},
}


@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/QUC2UZ7J/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/WDGR6CFF/1711.html:text/html},
}


@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{ilharco_editing_2023,
	title = {Editing {Models} with {Task} {Arithmetic}},
	url = {http://arxiv.org/abs/2212.04089},
	doi = {10.48550/arXiv.2212.04089},
	abstract = {Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around {\textbackslash}textit\{task vectors\}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
	month = mar,
	year = {2023},
	note = {arXiv:2212.04089 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: In Proceedings of the 11th International Conference on Learning Representations (ICLR 2023)},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/H2HFA7GG/Ilharco et al. - 2023 - Editing Models with Task Arithmetic.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/X2K38FBV/2212.html:text/html},
}


@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/XZADERKG/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}


@misc{haider_phi-3_2024,
	title = {Phi-3 {Safety} {Post}-{Training}: {Aligning} {Language} {Models} with a "{Break}-{Fix}" {Cycle}},
	shorttitle = {Phi-3 {Safety} {Post}-{Training}},
	url = {http://arxiv.org/abs/2407.13833},
	doi = {10.48550/arXiv.2407.13833},
	abstract = {Recent innovations in language model training have demonstrated that it is possible to create highly performant models that are small enough to run on a smartphone. As these models are deployed in an increasing number of domains, it is critical to ensure that they are aligned with human preferences and safety considerations. In this report, we present our methodology for safety aligning the Phi-3 series of language models. We utilized a "break-fix" cycle, performing multiple rounds of dataset curation, safety post-training, benchmarking, red teaming, and vulnerability identification to cover a variety of harm areas in both single and multi-turn scenarios. Our results indicate that this approach iteratively improved the performance of the Phi-3 models across a wide range of responsible AI benchmarks. Finally, we include additional red teaming strategies and evaluations that were used to test the safety behavior of Phi-3.5-mini and Phi-3.5-MoE, which were optimized for multilingual capabilities.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Haider, Emman and Perez-Becker, Daniel and Portet, Thomas and Madan, Piyush and Garg, Amit and Ashfaq, Atabak and Majercak, David and Wen, Wen and Kim, Dongwoo and Yang, Ziyi and Zhang, Jianwen and Sharma, Hiteshi and Bullwinkel, Blake and Pouliot, Martin and Minnich, Amanda and Chawla, Shiven and Herrera, Solianna and Warreth, Shahed and Engler, Maggie and Lopez, Gary and Chikanov, Nina and Dheekonda, Raja Sekhar Rao and Jagdagdorj, Bolor-Erdene and Lutz, Roman and Lundeen, Richard and Westerhoff, Tori and Bryan, Pete and Seifert, Christian and Kumar, Ram Shankar Siva and Berkley, Andrew and Kessler, Alex},
	month = aug,
	year = {2024},
	note = {arXiv:2407.13833 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/W45PWWUZ/Haider et al. - 2024 - Phi-3 Safety Post-Training Aligning Language Models with a Break-Fix Cycle.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/4N3756TC/2407.html:text/html},
}


@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Grattafiori, Aaron et al.},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/N6LI2L38/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/NQBVGNIA/2407.html:text/html},
}



@misc{xhonneux_efficient_2024,
	title = {Efficient {Adversarial} {Training} in {LLMs} with {Continuous} {Attacks}},
	url = {http://arxiv.org/abs/2405.15589},
	doi = {10.48550/arXiv.2405.15589},
	abstract = {Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Xhonneux, Sophie and Sordoni, Alessandro and Günnemann, Stephan and Gidel, Gauthier and Schwinn, Leo},
	month = nov,
	year = {2024},
	note = {arXiv:2405.15589 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 4 figures},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/LBFWQCE9/Xhonneux et al. - 2024 - Efficient Adversarial Training in LLMs with Continuous Attacks.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/LTZHY9LT/2405.html:text/html},
}

@misc{alon_detecting_2023,
	title = {Detecting {Language} {Model} {Attacks} with {Perplexity}},
	url = {http://arxiv.org/abs/2308.14132},
	doi = {10.48550/arXiv.2308.14132},
	abstract = {A novel hack involving Large Language Models (LLMs) has emerged, exploiting adversarial suffixes to deceive models into generating perilous responses. Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content. By evaluating the perplexity of queries with adversarial suffixes using an open-source LLM (GPT-2), we found that they have exceedingly high perplexity values. As we explored a broad range of regular (non-adversarial) prompt varieties, we concluded that false positives are a significant challenge for plain perplexity filtering. A Light-GBM trained on perplexity and token length resolved the false positives and correctly detected most adversarial attacks in the test set.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Alon, Gabriel and Kamfonas, Michael},
	month = nov,
	year = {2023},
	note = {arXiv:2308.14132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/B64FKBBM/Alon and Kamfonas - 2023 - Detecting Language Model Attacks with Perplexity.pdf:application/pdf},
}


@misc{arditi_refusal_2024,
	title = {Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}},
	url = {http://arxiv.org/abs/2406.11717},
	doi = {10.48550/arXiv.2406.11717},
	abstract = {Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
	month = oct,
	year = {2024},
	note = {arXiv:2406.11717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/TA9QQRB6/Arditi et al. - 2024 - Refusal in Language Models Is Mediated by a Single Direction.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/5NYKQ59E/2406.html:text/html},
}


@misc{inan_llama_2023,
	title = {Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}},
	shorttitle = {Llama {Guard}},
	url = {http://arxiv.org/abs/2312.06674},
	doi = {10.48550/arXiv.2312.06674},
	abstract = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/2X3UJJHV/Inan et al. - 2023 - Llama Guard LLM-based Input-Output Safeguard for Human-AI Conversations.pdf:application/pdf},
}



@misc{huang_catastrophic_2023,
	title = {Catastrophic {Jailbreak} of {Open}-source {LLMs} via {Exploiting} {Generation}},
	url = {http://arxiv.org/abs/2310.06987},
	doi = {10.48550/arXiv.2310.06987},
	abstract = {The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0\% to more than 95\% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with \$30{\textbackslash}times\$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak\_LLM.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06987 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/PPV38L55/Huang et al. - 2023 - Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/2XD82MXX/2310.html:text/html},
}



@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o {System} {Card}},
	url = {http://arxiv.org/abs/2410.21276},
	doi = {10.48550/arXiv.2410.21276},
	abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {OpenAI and Hurst,  Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam et al.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/TMNSYX5H/OpenAI et al. - 2024 - GPT-4o System Card.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/43SZGEFF/2410.html:text/html},
}


@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Models and code are available at https://mistral.ai/news/announcing-mistral-7b/},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/4KVW346E/Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/DV4U9A2I/2310.html:text/html},
}


@misc{andriushchenko_jailbreaking_2024,
	title = {Jailbreaking {Leading} {Safety}-{Aligned} {LLMs} with {Simple} {Adaptive} {Attacks}},
	url = {http://arxiv.org/abs/2404.02151},
	doi = {10.48550/arXiv.2404.02151},
	abstract = {We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve 100\% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at https://github.com/tml-epfl/llm-adaptive-attacks.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
	month = oct,
	year = {2024},
	note = {arXiv:2404.02151 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B), jailbreak artifacts for all attacks are available, evaluation with different judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots over iterations, ablation on the suffix length for random search), examples of jailbroken generation},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/B625KQFL/Andriushchenko et al. - 2024 - Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.pdf:application/pdf},
}



@misc{wei_jailbroken_2023,
	title = {Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?},
	shorttitle = {Jailbroken},
	url = {http://arxiv.org/abs/2307.02483},
	doi = {10.48550/arXiv.2307.02483},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02483 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/UXNKCRS2/Wei et al. - 2023 - Jailbroken How Does LLM Safety Training Fail.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/ULTQB988/2307.html:text/html},
}



@misc{sheshadri_latent_2024,
	title = {Latent {Adversarial} {Training} {Improves} {Robustness} to {Persistent} {Harmful} {Behaviors} in {LLMs}},
	url = {http://arxiv.org/abs/2407.15549},
	doi = {10.48550/arXiv.2407.15549},
	abstract = {Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and Casper, Stephen},
	month = aug,
	year = {2024},
	note = {arXiv:2407.15549 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/B3T8925B/Sheshadri et al. - 2024 - Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/NU55YL3M/2407.html:text/html},
}


@misc{huang_endless_2024,
	title = {Endless {Jailbreaks} with {Bijection} {Learning}},
	url = {http://arxiv.org/abs/2410.01294},
	doi = {10.48550/arXiv.2410.01294},
	abstract = {Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings whose complexity can be tightly controlled. We leverage in-context learning to teach models bijective encodings, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English. Our attack is extremely effective on a wide range of frontier language models. Moreover, by controlling complexity parameters such as number of key-value mappings in the encodings, we find a close relationship between the capability level of the attacked LLM and the average complexity of the most effective bijection attacks. Our work highlights that new vulnerabilities in frontier models can emerge with scale: more capable models are more severely jailbroken by bijection attacks.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Huang, Brian R. Y. and Li, Maximilian and Tang, Leonard},
	month = dec,
	year = {2024},
	note = {arXiv:2410.01294 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/YGCYUU8G/Huang et al. - 2024 - Endless Jailbreaks with Bijection Learning.pdf:application/pdf},
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% David's references
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@ARTICLE{vega_bypassing_2023,
  title         = "Bypassing the safety training of open-source {LLMs} with
                   priming attacks",
  author        = "Vega, Jason and Chaudhary, Isha and Xu, Changming and Singh,
                   Gagandeep",
  journal       = "arXiv [cs.CR]",
  abstract      = "With the recent surge in popularity of LLMs has come an
                   ever-increasing need for LLM safety training. In this paper,
                   we investigate the fragility of SOTA open-source LLMs under
                   simple, optimization-free attacks we refer to as
                   $\textit{priming attacks}$, which are easy to execute and
                   effectively bypass alignment from safety training. Our
                   proposed attack improves the Attack Success Rate on Harmful
                   Behaviors, as measured by Llama Guard, by up to $3.3\times$
                   compared to baselines. Source code and data are available at
                   https://github.com/uiuc-focal-lab/llm-priming-attacks.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR"
}

@ARTICLE{feuer_style_2024,
  title         = "Style outweighs substance: Failure modes of {LLM} judges in
                   alignment benchmarking",
  author        = "Feuer, Benjamin and Goldblum, Micah and Datta, Teresa and
                   Nambiar, Sanjana and Besaleli, Raz and Dooley, Samuel and
                   Cembalest, Max and Dickerson, John P",
  journal       = "arXiv [cs.LG]",
  abstract      = "The release of ChatGPT in November 2022 sparked an explosion
                   of interest in post-training and an avalanche of new
                   preference optimization (PO) methods. These methods claim
                   superior alignment by virtue of better correspondence with
                   human pairwise preferences, often measured by LLM-judges. In
                   this work, we attempt to answer the following question -- do
                   LLM-judge preferences translate to progress on other, more
                   concrete metrics for alignment, and if not, why not? We
                   define a concrete metric for alignment, and introduce
                   SOS-Bench (Substance Outweighs Style Benchmark), which is to
                   the best of our knowledge the largest standardized,
                   reproducible LLM meta-benchmark to date. We find that (1)
                   LLM-judge preferences do not correlate with concrete measures
                   of safety, world knowledge, and instruction following; (2)
                   LLM-judges have powerful implicit biases, prioritizing style
                   over factuality and safety; and (3) the supervised
                   fine-tuning (SFT) stage of post-training, and not the PO
                   stage, has the greatest impact on alignment, with data
                   scaling and prompt diversity as the driving factors. Our
                   codebase and complete results can be found at
                   https://github.com/penfever/sos-bench.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{perez_red_2022,
  title         = "Red teaming language Models with language Models",
  author        = "Perez, Ethan and Huang, Saffron and Song, Francis and Cai,
                   Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia
                   and McAleese, Nat and Irving, Geoffrey",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language Models (LMs) often cannot be deployed because of
                   their potential to harm users in hard-to-predict ways. Prior
                   work identifies harmful behaviors before deployment by using
                   human annotators to hand-write test cases. However, human
                   annotation is expensive, limiting the number and diversity of
                   test cases. In this work, we automatically find cases where a
                   target LM behaves in a harmful way, by generating test cases
                   (``red teaming'') using another LM. We evaluate the target
                   LM's replies to generated test questions using a classifier
                   trained to detect offensive content, uncovering tens of
                   thousands of offensive replies in a 280B parameter LM
                   chatbot. We explore several methods, from zero-shot
                   generation to reinforcement learning, for generating test
                   cases with varying levels of diversity and difficulty.
                   Furthermore, we use prompt engineering to control
                   LM-generated test cases to uncover a variety of other harms,
                   automatically finding groups of people that the chatbot
                   discusses in offensive ways, personal and hospital phone
                   numbers generated as the chatbot's own contact info, leakage
                   of private training data in generated text, and harms that
                   occur over the course of a conversation. Overall, LM-based
                   red teaming is one promising tool (among many needed) for
                   finding and fixing diverse, undesirable LM behaviors before
                   impacting users.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{zou_improving_2024,
  title         = "Improving alignment and robustness with short circuiting",
  author        = "Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek
                   and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan
                   and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan",
  journal       = "arXiv [cs.LG]",
  abstract      = "AI systems can take harmful actions and are highly vulnerable
                   to adversarial attacks. We present an approach, inspired by
                   recent advances in representation engineering, that
                   ``short-circuits'' models as they respond with harmful
                   outputs. Existing techniques aimed at improving alignment,
                   such as refusal training, are often bypassed. Techniques such
                   as adversarial training try to plug these holes by countering
                   specific attacks. As an alternative to refusal training and
                   adversarial training, short-circuiting directly controls the
                   representations that are responsible for harmful outputs in
                   the first place. Our technique can be applied to both
                   text-only and multimodal language models to prevent the
                   generation of harmful outputs without sacrificing utility --
                   even in the presence of powerful unseen attacks. Notably,
                   while adversarial robustness in standalone image recognition
                   remains an open challenge, short-circuiting allows the larger
                   multimodal system to reliably withstand image ``hijacks''
                   that aim to produce harmful content. Finally, we extend our
                   approach to AI agents, demonstrating considerable reductions
                   in the rate of harmful actions when they are under attack.
                   Our approach represents a significant step forward in the
                   development of reliable safeguards to harmful behavior and
                   adversarial attacks.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{zeng_johnny_2024,
  title         = "How Johnny can persuade {LLMs} to jailbreak them: Rethinking
                   persuasion to challenge {AI} safety by humanizing {LLMs}",
  author        = "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi
                   and Jia, Ruoxi and Shi, Weiyan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Most traditional AI safety research has approached AI models
                   as machines and centered on algorithm-focused attacks
                   developed by security experts. As large language models
                   (LLMs) become increasingly common and competent, non-expert
                   users can also impose risks during daily interactions. This
                   paper introduces a new perspective to jailbreak LLMs as
                   human-like communicators, to explore this overlooked
                   intersection between everyday language interaction and AI
                   safety. Specifically, we study how to persuade LLMs to
                   jailbreak them. First, we propose a persuasion taxonomy
                   derived from decades of social science research. Then, we
                   apply the taxonomy to automatically generate interpretable
                   persuasive adversarial prompts (PAP) to jailbreak LLMs.
                   Results show that persuasion significantly increases the
                   jailbreak performance across all risk categories: PAP
                   consistently achieves an attack success rate of over $92\%$
                   on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials,
                   surpassing recent algorithm-focused attacks. On the defense
                   side, we explore various mechanisms against PAP and, found a
                   significant gap in existing defenses, and advocate for more
                   fundamental mitigation for highly interactive LLMs",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{chao_jailbreaking_2023,
  title         = "Jailbreaking black box large language models in twenty
                   queries",
  author        = "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and
                   Hassani, Hamed and Pappas, George J and Wong, Eric",
  journal       = "arXiv [cs.LG]",
  abstract      = "There is growing interest in ensuring that large language
                   models (LLMs) align with human values. However, the alignment
                   of such models is vulnerable to adversarial jailbreaks, which
                   coax LLMs into overriding their safety guardrails. The
                   identification of these vulnerabilities is therefore
                   instrumental in understanding inherent weaknesses and
                   preventing future misuse. To this end, we propose Prompt
                   Automatic Iterative Refinement (PAIR), an algorithm that
                   generates semantic jailbreaks with only black-box access to
                   an LLM. PAIR -- which is inspired by social engineering
                   attacks -- uses an attacker LLM to automatically generate
                   jailbreaks for a separate targeted LLM without human
                   intervention. In this way, the attacker LLM iteratively
                   queries the target LLM to update and refine a candidate
                   jailbreak. Empirically, PAIR often requires fewer than twenty
                   queries to produce a jailbreak, which is orders of magnitude
                   more efficient than existing algorithms. PAIR also achieves
                   competitive jailbreaking success rates and transferability on
                   open and closed-source LLMs, including GPT-3.5/4, Vicuna, and
                   PaLM-2.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{zou_universal_2023,
  title         = "Universal and transferable adversarial attacks on aligned
                   language models",
  author        = "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson,
                   Matt",
  journal       = "arXiv [cs.CL]",
  abstract      = "Because ``out-of-the-box'' large language models are capable
                   of generating a great deal of objectionable content, recent
                   work has focused on aligning these models in an attempt to
                   prevent undesirable generation. While there has been some
                   success at circumventing these measures -- so-called
                   ``jailbreaks'' against LLMs -- these attacks have required
                   significant human ingenuity and are brittle in practice. In
                   this paper, we propose a simple and effective attack method
                   that causes aligned language models to generate objectionable
                   behaviors. Specifically, our approach finds a suffix that,
                   when attached to a wide range of queries for an LLM to
                   produce objectionable content, aims to maximize the
                   probability that the model produces an affirmative response
                   (rather than refusing to answer). However, instead of relying
                   on manual engineering, our approach automatically produces
                   these adversarial suffixes by a combination of greedy and
                   gradient-based search techniques, and also improves over past
                   automatic prompt generation methods. Surprisingly, we find
                   that the adversarial prompts generated by our approach are
                   quite transferable, including to black-box, publicly released
                   LLMs. Specifically, we train an adversarial attack suffix on
                   multiple prompts (i.e., queries asking for many different
                   types of objectionable content), as well as multiple models
                   (in our case, Vicuna-7B and 13B). When doing so, the
                   resulting attack suffix is able to induce objectionable
                   content in the public interfaces to ChatGPT, Bard, and
                   Claude, as well as open source LLMs such as LLaMA-2-Chat,
                   Pythia, Falcon, and others. In total, this work significantly
                   advances the state-of-the-art in adversarial attacks against
                   aligned language models, raising important questions about
                   how such systems can be prevented from producing
                   objectionable information. Code is available at
                   github.com/llm-attacks/llm-attacks.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{liao_amplegcg_2024,
  title         = "{AmpleGCG}: Learning a universal and transferable generative
                   model of adversarial suffixes for jailbreaking both open and
                   closed {LLMs}",
  author        = "Liao, Zeyi and Sun, Huan",
  journal       = "arXiv [cs.CL]",
  abstract      = "As large language models (LLMs) become increasingly prevalent
                   and integrated into autonomous systems, ensuring their safety
                   is imperative. Despite significant strides toward safety
                   alignment, recent work GCG~\citep{zou2023universal} proposes
                   a discrete token optimization algorithm and selects the
                   single suffix with the lowest loss to successfully jailbreak
                   aligned LLMs. In this work, we first discuss the drawbacks of
                   solely picking the suffix with the lowest loss during GCG
                   optimization for jailbreaking and uncover the missed
                   successful suffixes during the intermediate steps. Moreover,
                   we utilize those successful suffixes as training data to
                   learn a generative model, named AmpleGCG, which captures the
                   distribution of adversarial suffixes given a harmful query
                   and enables the rapid generation of hundreds of suffixes for
                   any harmful queries in seconds. AmpleGCG achieves near 100\%
                   attack success rate (ASR) on two aligned LLMs
                   (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest
                   attack baselines. More interestingly, AmpleGCG also transfers
                   seamlessly to attack different models, including
                   closed-source LLMs, achieving a 99\% ASR on the latest
                   GPT-3.5. To summarize, our work amplifies the impact of GCG
                   by training a generative model of adversarial suffixes that
                   is universal to any harmful queries and transferable from
                   attacking open-source LLMs to closed-source LLMs. In
                   addition, it can generate 200 adversarial suffixes for one
                   harmful query in only 4 seconds, rendering it more
                   challenging to defend.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{sheshadri_targeted_2024,
  title         = "Targeted latent adversarial training improves robustness to
                   persistent harmful behaviors in {LLMs}",
  author        = "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch,
                   Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and
                   Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell,
                   Dylan and Casper, Stephen",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models (LLMs) can often be made to behave in
                   undesirable ways that they are explicitly fine-tuned not to.
                   For example, the LLM red-teaming literature has produced a
                   wide variety of `jailbreaking' techniques to elicit harmful
                   text from models that were fine-tuned to be harmless. Recent
                   work on red-teaming, model editing, and interpretability
                   suggests that this challenge stems from how (adversarial)
                   fine-tuning largely serves to suppress rather than remove
                   undesirable capabilities from LLMs. Prior work has introduced
                   latent adversarial training (LAT) as a way to improve
                   robustness to broad classes of failures. These prior works
                   have considered untargeted latent space attacks where the
                   adversary perturbs latent activations to maximize loss on
                   examples of desirable behavior. Untargeted LAT can provide a
                   generic type of robustness but does not leverage information
                   about specific failure modes. Here, we experiment with
                   targeted LAT where the adversary seeks to minimize loss on a
                   specific competing task. We find that it can augment a wide
                   variety of state-of-the-art methods. First, we use targeted
                   LAT to improve robustness to jailbreaks, outperforming a
                   strong R2D2 baseline with orders of magnitude less compute.
                   Second, we use it to more effectively remove backdoors with
                   no knowledge of the trigger. Finally, we use it to more
                   effectively unlearn knowledge for specific undesirable tasks
                   in a way that is also more robust to re-learning. Overall,
                   our results suggest that targeted LAT can be an effective
                   tool for defending against harmful behaviors from LLMs.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{schwinn_soft_2024,
  title         = "Soft prompt threats: Attacking safety alignment and
                   unlearning in open-source {LLMs} through the embedding space",
  author        = "Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel,
                   Gauthier and Gunnemann, Stephan",
  journal       = "arXiv [cs.LG]",
  abstract      = "Current research in adversarial robustness of LLMs focuses on
                   discrete input manipulations in the natural language space,
                   which can be directly transferred to closed-source models.
                   However, this approach neglects the steady progression of
                   open-source models. As open-source models advance in
                   capability, ensuring their safety also becomes increasingly
                   imperative. Yet, attacks tailored to open-source LLMs that
                   exploit full model access remain largely unexplored. We
                   address this research gap and propose the embedding space
                   attack, which directly attacks the continuous embedding
                   representation of input tokens. We find that embedding space
                   attacks circumvent model alignments and trigger harmful
                   behaviors more efficiently than discrete attacks or model
                   fine-tuning. Furthermore, we present a novel threat model in
                   the context of unlearning and show that embedding space
                   attacks can extract supposedly deleted information from
                   unlearned LLMs across multiple datasets and models. Our
                   findings highlight embedding space attacks as an important
                   threat model in open-source LLMs. Trigger Warning: the
                   appendix contains LLM-generated text with violence and
                   harassment.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{liu_autodan_2023,
  title         = "{AutoDAN}: Generating stealthy jailbreak prompts on aligned
                   Large Language Models",
  author        = "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
  journal       = "arXiv [cs.CL]",
  abstract      = "The aligned Large Language Models (LLMs) are powerful
                   language understanding and decision-making tools that are
                   created through extensive alignment with human feedback.
                   However, these large models remain susceptible to jailbreak
                   attacks, where adversaries manipulate prompts to elicit
                   malicious outputs that should not be given by aligned LLMs.
                   Investigating jailbreak prompts can lead us to delve into the
                   limitations of LLMs and further guide us to secure them.
                   Unfortunately, existing jailbreak techniques suffer from
                   either (1) scalability issues, where attacks heavily rely on
                   manual crafting of prompts, or (2) stealthiness problems, as
                   attacks depend on token-based algorithms to generate prompts
                   that are often semantically meaningless, making them
                   susceptible to detection through basic perplexity testing. In
                   light of these challenges, we intend to answer this question:
                   Can we develop an approach that can automatically generate
                   stealthy jailbreak prompts? In this paper, we introduce
                   AutoDAN, a novel jailbreak attack against aligned LLMs.
                   AutoDAN can automatically generate stealthy jailbreak prompts
                   by the carefully designed hierarchical genetic algorithm.
                   Extensive evaluations demonstrate that AutoDAN not only
                   automates the process while preserving semantic
                   meaningfulness, but also demonstrates superior attack
                   strength in cross-model transferability, and cross-sample
                   universality compared with the baseline. Moreover, we also
                   compare AutoDAN with perplexity-based defense methods and
                   show that AutoDAN can bypass them effectively.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{ouyang_training_2022,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{rafailov_direct_2023,
  title         = "Direct Preference Optimization: Your language model is
                   secretly a reward model",
  author        = "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and
                   Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
  journal       = "arXiv [cs.LG]",
  abstract      = "While large-scale unsupervised language models (LMs) learn
                   broad world knowledge and some reasoning skills, achieving
                   precise control of their behavior is difficult due to the
                   completely unsupervised nature of their training. Existing
                   methods for gaining such steerability collect human labels of
                   the relative quality of model generations and fine-tune the
                   unsupervised LM to align with these preferences, often with
                   reinforcement learning from human feedback (RLHF). However,
                   RLHF is a complex and often unstable procedure, first fitting
                   a reward model that reflects the human preferences, and then
                   fine-tuning the large unsupervised LM using reinforcement
                   learning to maximize this estimated reward without drifting
                   too far from the original model. In this paper, we leverage a
                   mapping between reward functions and optimal policies to show
                   that this constrained reward maximization problem can be
                   optimized exactly with a single stage of policy training,
                   essentially solving a classification problem on the human
                   preference data. The resulting algorithm, which we call
                   Direct Preference Optimization (DPO), is stable, performant
                   and computationally lightweight, eliminating the need for
                   fitting a reward model, sampling from the LM during
                   fine-tuning, or performing significant hyperparameter tuning.
                   Our experiments show that DPO can fine-tune LMs to align with
                   human preferences as well as or better than existing methods.
                   Notably, fine-tuning with DPO exceeds RLHF's ability to
                   control sentiment of generations and improves response
                   quality in summarization and single-turn dialogue while being
                   substantially simpler to implement and train.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{goyal_think_2023,
  title         = "Think before you speak: Training Language Models With Pause
                   Tokens",
  author        = "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon,
                   Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models generate responses by producing a series of
                   tokens in immediate succession: the $(K+1)^{th}$ token is an
                   outcome of manipulating $K$ hidden vectors per layer, one
                   vector per preceding token. What if instead we were to let
                   the model manipulate say, $K+10$ hidden vectors, before it
                   outputs the $(K+1)^{th}$ token? We operationalize this idea
                   by performing training and inference on language models with
                   a (learnable) $\textit{pause}$ token, a sequence of which is
                   appended to the input prefix. We then delay extracting the
                   model's outputs until the last pause token is seen, thereby
                   allowing the model to process extra computation before
                   committing to an answer. We empirically evaluate
                   $\textit{pause-training}$ on decoder-only models of 1B and
                   130M parameters with causal pretraining on C4, and on
                   downstream tasks covering reasoning, question-answering,
                   general understanding and fact recall. Our main finding is
                   that inference-time delays show gains when the model is both
                   pre-trained and finetuned with delays. For the 1B model, we
                   witness gains on 8 of 9 tasks, most prominently, a gain of
                   $18\%$ EM score on the QA task of SQuAD, $8\%$ on
                   CommonSenseQA and $1\%$ accuracy on the reasoning task of
                   GSM8k. Our work raises a range of conceptual and practical
                   future research questions on making delayed next-token
                   prediction a widely applicable new paradigm.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{schick_toolformer_2023,
  title         = "Toolformer: Language models can teach themselves to use tools",
  author        = "Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and
                   Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and
                   Cancedda, Nicola and Scialom, Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) exhibit remarkable abilities to solve
                   new tasks from just a few examples or textual instructions,
                   especially at scale. They also, paradoxically, struggle with
                   basic functionality, such as arithmetic or factual lookup,
                   where much simpler and smaller models excel. In this paper,
                   we show that LMs can teach themselves to use external tools
                   via simple APIs and achieve the best of both worlds. We
                   introduce Toolformer, a model trained to decide which APIs to
                   call, when to call them, what arguments to pass, and how to
                   best incorporate the results into future token prediction.
                   This is done in a self-supervised way, requiring nothing more
                   than a handful of demonstrations for each API. We incorporate
                   a range of tools, including a calculator, a Q\&A system, two
                   different search engines, a translation system, and a
                   calendar. Toolformer achieves substantially improved
                   zero-shot performance across a variety of downstream tasks,
                   often competitive with much larger models, without
                   sacrificing its core language modeling abilities.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}
@ARTICLE{sukhbaatar_augmenting_2019,
  title         = "Augmenting self-attention with persistent memory",
  author        = "Sukhbaatar, Sainbayar and Grave, Edouard and Lample,
                   Guillaume and Jegou, Herve and Joulin, Armand",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transformer networks have lead to important progress in
                   language modeling and machine translation. These models
                   include two consecutive modules, a feed-forward layer and a
                   self-attention layer. The latter allows the network to
                   capture long term dependencies and are often regarded as the
                   key ingredient in the success of Transformers. Building upon
                   this intuition, we propose a new model that solely consists
                   of attention layers. More precisely, we augment the
                   self-attention layers with persistent memory vectors that
                   play a similar role as the feed-forward layer. Thanks to
                   these vectors, we can remove the feed-forward layer without
                   degrading the performance of a transformer. Our evaluation
                   shows the benefits brought by our model on standard character
                   and word level language modeling benchmarks.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{mu_learning_2023,
  title         = "Learning to compress prompts with gist tokens",
  author        = "Mu, Jesse and Li, Xiang Lisa and Goodman, Noah",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prompting is the primary way to utilize the multitask
                   capabilities of language models (LMs), but prompts occupy
                   valuable space in the input context window, and repeatedly
                   encoding the same prompt is computationally inefficient.
                   Finetuning and distillation methods allow for specialization
                   of LMs without prompting, but require retraining the model
                   for each task. To avoid this trade-off entirely, we present
                   gisting, which trains an LM to compress prompts into smaller
                   sets of ``gist'' tokens which can be cached and reused for
                   compute efficiency. Gist models can be trained with no
                   additional cost over standard instruction finetuning by
                   simply modifying Transformer attention masks to encourage
                   prompt compression. On decoder (LLaMA-7B) and encoder-decoder
                   (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of
                   prompts, resulting in up to 40\% FLOPs reductions, 4.2\% wall
                   time speedups, and storage savings, all with minimal loss in
                   output quality.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{xiao_efficient_2023,
  title         = "Efficient streaming language models with attention sinks",
  author        = "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han,
                   Song and Lewis, Mike",
  journal       = "arXiv [cs.CL]",
  abstract      = "Deploying Large Language Models (LLMs) in streaming
                   applications such as multi-round dialogue, where long
                   interactions are expected, is urgently needed but poses two
                   major challenges. Firstly, during the decoding stage, caching
                   previous tokens' Key and Value states (KV) consumes extensive
                   memory. Secondly, popular LLMs cannot generalize to longer
                   texts than the training sequence length. Window attention,
                   where only the most recent KVs are cached, is a natural
                   approach -- but we show that it fails when the text length
                   surpasses the cache size. We observe an interesting
                   phenomenon, namely attention sink, that keeping the KV of
                   initial tokens will largely recover the performance of window
                   attention. In this paper, we first demonstrate that the
                   emergence of attention sink is due to the strong attention
                   scores towards initial tokens as a ``sink'' even if they are
                   not semantically important. Based on the above analysis, we
                   introduce StreamingLLM, an efficient framework that enables
                   LLMs trained with a finite length attention window to
                   generalize to infinite sequence lengths without any
                   fine-tuning. We show that StreamingLLM can enable Llama-2,
                   MPT, Falcon, and Pythia to perform stable and efficient
                   language modeling with up to 4 million tokens and more. In
                   addition, we discover that adding a placeholder token as a
                   dedicated attention sink during pre-training can further
                   improve streaming deployment. In streaming settings,
                   StreamingLLM outperforms the sliding window recomputation
                   baseline by up to 22.2x speedup. Code and datasets are
                   provided at https://github.com/mit-han-lab/streaming-llm.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{burtsev_memory_2020,
  title         = "Memory Transformer",
  author        = "Burtsev, Mikhail S and Kuratov, Yuri and Peganov, Anton and
                   Sapunov, Grigory V",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based models have achieved state-of-the-art
                   results in many natural language processing tasks. The
                   self-attention architecture allows transformer to combine
                   information from all elements of a sequence into
                   context-aware representations. However, information about the
                   context is stored mostly in the same element-wise
                   representations. This might limit the processing of
                   properties related to the sequence as a whole more difficult.
                   Adding trainable memory to selectively store local as well as
                   global representations of a sequence is a promising direction
                   to improve the Transformer model. Memory-augmented neural
                   networks (MANNs) extend traditional neural architectures with
                   general-purpose memory for representations. MANNs have
                   demonstrated the capability to learn simple algorithms like
                   Copy or Reverse and can be successfully trained via
                   backpropagation on diverse tasks from question answering to
                   language modeling outperforming RNNs and LSTMs of comparable
                   complexity. In this work, we propose and study few extensions
                   of the Transformer baseline (1) by adding memory tokens to
                   store non-local representations, (2) creating memory
                   bottleneck for the global information, (3) controlling memory
                   update with dedicated layer. We evaluate these memory
                   augmented Transformers and demonstrate that presence of
                   memory positively correlates with the model performance for
                   machine translation and language modelling tasks.
                   Augmentation of pre-trained masked language model with memory
                   tokens shows mixed results for tasks from GLUE benchmark.
                   Visualization of attention patterns over the memory suggest
                   that it improves the model's ability to process a global
                   context.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@misc{wang_self-guard_2024,
	title = {Self-{Guard}: {Empower} the {LLM} to {Safeguard} {Itself}},
	shorttitle = {Self-{Guard}},
	url = {http://arxiv.org/abs/2310.15851},
	doi = {10.48550/arXiv.2310.15851},
	abstract = {The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experiment has demonstrated that Self-Guard is robust against jailbreak attacks. In the bad case analysis, we find that LLM occasionally provides harmless responses to harmful queries. Additionally, we evaluated the general capabilities of the LLM before and after safety training, providing evidence that Self-Guard does not result in the LLM's performance degradation. In sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM but also can even mitigate this issue.},
	urldate = {2025-02-21},
	publisher = {arXiv},
	author = {Wang, Zezhong and Yang, Fangkai and Wang, Lu and Zhao, Pu and Wang, Hongru and Chen, Liang and Lin, Qingwei and Wong, Kam-Fai},
	month = mar,
	year = {2024},
	note = {arXiv:2310.15851 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/BXABW4YT/Wang et al. - 2024 - Self-Guard Empower the LLM to Safeguard Itself.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/7IG4R848/2310.html:text/html},
}

@ARTICLE{madry_towards_2017,
  title         = "Towards Deep Learning Models Resistant to Adversarial Attacks",
  author        = "Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig
                   and Tsipras, Dimitris and Vladu, Adrian",
  journal       = "arXiv [stat.ML]",
  abstract      = "Recent work has demonstrated that deep neural networks are
                   vulnerable to adversarial examples---inputs that are almost
                   indistinguishable from natural data and yet classified
                   incorrectly by the network. In fact, some of the latest
                   findings suggest that the existence of adversarial attacks
                   may be an inherent weakness of deep learning models. To
                   address this problem, we study the adversarial robustness of
                   neural networks through the lens of robust optimization. This
                   approach provides us with a broad and unifying view on much
                   of the prior work on this topic. Its principled nature also
                   enables us to identify methods for both training and
                   attacking neural networks that are reliable and, in a certain
                   sense, universal. In particular, they specify a concrete
                   security guarantee that would protect against any adversary.
                   These methods let us train networks with significantly
                   improved resistance to a wide range of adversarial attacks.
                   They also suggest the notion of security against a
                   first-order adversary as a natural and broad security
                   guarantee. We believe that robustness against such
                   well-defined classes of adversaries is an important stepping
                   stone towards fully resistant deep learning models. Code and
                   pre-trained models are available at
                   https://github.com/MadryLab/mnist\_challenge and
                   https://github.com/MadryLab/cifar10\_challenge.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}

@article{loshchilov_decoupled_2017,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@ARTICLE{hughes_best-of-n_2024,
  title         = "Best-of-{N} Jailbreaking",
  author        = "Hughes, John and Price, Sara and Lynch, Aengus and Schaeffer,
                   Rylan and Barez, Fazl and Koyejo, Sanmi and Sleight, Henry
                   and Jones, Erik and Perez, Ethan and Sharma, Mrinank",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce Best-of-N (BoN) Jailbreaking, a simple black-box
                   algorithm that jailbreaks frontier AI systems across
                   modalities. BoN Jailbreaking works by repeatedly sampling
                   variations of a prompt with a combination of augmentations -
                   such as random shuffling or capitalization for textual
                   prompts - until a harmful response is elicited. We find that
                   BoN Jailbreaking achieves high attack success rates (ASRs) on
                   closed-source language models, such as 89\% on GPT-4o and
                   78\% on Claude 3.5 Sonnet when sampling 10,000 augmented
                   prompts. Further, it is similarly effective at circumventing
                   state-of-the-art open-source defenses like circuit breakers.
                   BoN also seamlessly extends to other modalities: it
                   jailbreaks vision language models (VLMs) such as GPT-4o and
                   audio language models (ALMs) like Gemini 1.5 Pro, using
                   modality-specific augmentations. BoN reliably improves when
                   we sample more augmented prompts. Across all modalities, ASR,
                   as a function of the number of samples (N), empirically
                   follows power-law-like behavior for many orders of magnitude.
                   BoN Jailbreaking can also be composed with other black-box
                   algorithms for even more effective attacks - combining BoN
                   with an optimized prefix attack achieves up to a 35\%
                   increase in ASR. Overall, our work indicates that, despite
                   their capability, language models are sensitive to seemingly
                   innocuous changes to inputs, which attackers can exploit
                   across modalities.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}


@ARTICLE{zou_representation_2023,
  title         = "Representation Engineering: A Top-Down Approach to {AI}
                   Transparency",
  author        = "Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James
                   and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin,
                   Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and
                   Goel, Shashwat and Li, Nathaniel and Byun, Michael J and
                   Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo,
                   Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J Zico
                   and Hendrycks, Dan",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this paper, we identify and characterize the emerging area
                   of representation engineering (RepE), an approach to
                   enhancing the transparency of AI systems that draws on
                   insights from cognitive neuroscience. RepE places
                   population-level representations, rather than neurons or
                   circuits, at the center of analysis, equipping us with novel
                   methods for monitoring and manipulating high-level cognitive
                   phenomena in deep neural networks (DNNs). We provide
                   baselines and an initial analysis of RepE techniques, showing
                   that they offer simple yet effective solutions for improving
                   our understanding and control of large language models. We
                   showcase how these methods can provide traction on a wide
                   range of safety-relevant problems, including honesty,
                   harmlessness, power-seeking, and more, demonstrating the
                   promise of top-down transparency research. We hope that this
                   work catalyzes further exploration of RepE and fosters
                   advancements in the transparency and safety of AI systems.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{tigges_linear_2023,
  title         = "Linear representations of sentiment in Large Language Models",
  author        = "Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus
                   and Nanda, Neel",
  journal       = "arXiv [cs.LG]",
  abstract      = "Sentiment is a pervasive feature in natural language text,
                   yet it is an open question how sentiment is represented
                   within Large Language Models (LLMs). In this study, we reveal
                   that across a range of models, sentiment is represented
                   linearly: a single direction in activation space mostly
                   captures the feature across a range of tasks with one extreme
                   for positive and the other for negative. Through causal
                   interventions, we isolate this direction and show it is
                   causally relevant in both toy tasks and real world datasets
                   such as Stanford Sentiment Treebank. Through this case study
                   we model a thorough investigation of what a single direction
                   means on a broad data distribution. We further uncover the
                   mechanisms that involve this direction, highlighting the
                   roles of a small subset of attention heads and neurons.
                   Finally, we discover a phenomenon which we term the
                   summarization motif: sentiment is not solely represented on
                   emotionally charged words, but is additionally summarized at
                   intermediate positions without inherent sentiment, such as
                   punctuation and names. We show that in Stanford Sentiment
                   Treebank zero-shot classification, 76\% of above-chance
                   classification accuracy is lost when ablating the sentiment
                   direction, nearly half of which (36\%) is due to ablating the
                   summarized sentiment direction exclusively at comma
                   positions.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{marks_geometry_2023,
  title         = "The geometry of truth: Emergent linear structure in large
                   language model representations of true/false datasets",
  author        = "Marks, Samuel and Tegmark, Max",
  journal       = "arXiv [cs.AI]",
  abstract      = "Large Language Models (LLMs) have impressive capabilities,
                   but are also prone to outputting falsehoods. Recent work has
                   developed techniques for inferring whether a LLM is telling
                   the truth by training probes on the LLM's internal
                   activations. However, this line of work is controversial,
                   with some authors pointing out failures of these probes to
                   generalize in basic ways, among other conceptual issues. In
                   this work, we curate high-quality datasets of true/false
                   statements and use them to study in detail the structure of
                   LLM representations of truth, drawing on three lines of
                   evidence: 1. Visualizations of LLM true/false statement
                   representations, which reveal clear linear structure. 2.
                   Transfer experiments in which probes trained on one dataset
                   generalize to different datasets. 3. Causal evidence obtained
                   by surgically intervening in a LLM's forward pass, causing it
                   to treat false statements as true and vice versa. Overall, we
                   present evidence that language models linearly represent the
                   truth or falsehood of factual statements. We also introduce a
                   novel technique, mass-mean probing, which generalizes better
                   and is more causally implicated in model outputs than other
                   probing techniques.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}


@misc{tamirisa_tamper-resistant_2025,
	title = {Tamper-{Resistant} {Safeguards} for {Open}-{Weight} {LLMs}},
	url = {http://arxiv.org/abs/2408.00761},
	doi = {10.48550/arXiv.2408.00761},
	abstract = {Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after hundreds of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that progress on tamper-resistance is possible, opening up a promising new avenue to improve the safety and security of open-weight LLMs.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {Tamirisa, Rishub and Bharathi, Bhrugu and Phan, Long and Zhou, Andy and Gatti, Alice and Suresh, Tarun and Lin, Maxwell and Wang, Justin and Wang, Rowan and Arel, Ron and Zou, Andy and Song, Dawn and Li, Bo and Hendrycks, Dan and Mazeika, Mantas},
	month = feb,
	year = {2025},
	note = {arXiv:2408.00761 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Website: https://www.tamper-resistant-safeguards.com},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/PHGXAIYY/Tamirisa et al. - 2025 - Tamper-Resistant Safeguards for Open-Weight LLMs.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/KVK4FZ2W/2408.html:text/html},
}


@ARTICLE{turner_activation_2023,
  title         = "Activation Addition: Steering language models without
                   optimization",
  author        = "Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin
                   and Udell, David and Vazquez, Juan J and Mini, Ulisse and
                   MacDiarmid, Monte",
  journal       = "arXiv [cs.CL]",
  abstract      = "Reliably controlling the behavior of large language models is
                   a pressing open problem. Existing methods include supervised
                   finetuning, reinforcement learning from human feedback,
                   prompt engineering and guided decoding. We instead
                   investigate activation engineering: modifying activations at
                   inference-time to predictably alter model behavior. We bias
                   the forward pass with a 'steering vector' implicitly
                   specified through natural language. Past work learned these
                   steering vectors; our Activation Addition (ActAdd) method
                   instead computes them by taking activation differences
                   resulting from pairs of prompts. We demonstrate ActAdd on a
                   range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining
                   SOTA on detoxification and negative-to-positive sentiment
                   control. Our approach yields inference-time control over
                   high-level properties of output like topic and sentiment
                   while preserving performance on off-target tasks. ActAdd
                   takes far less compute and implementation effort than
                   finetuning or RLHF, allows users control through natural
                   language, and its computational overhead (as a fraction of
                   inference time) appears stable or improving over increasing
                   model size.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}
