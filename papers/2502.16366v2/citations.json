[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei_jailbroken_2023",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "perez_red_2022",
        "author": "Perez, Ethan and Huang, Saffron and Song, Francis and Cai,\nTrevor and Ring, Roman and Aslanides, John and Glaese, Amelia\nand McAleese, Nat and Irving, Geoffrey",
        "title": "Red teaming language Models with language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zou_universal_2023",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson,\nMatt",
        "title": "Universal and transferable adversarial attacks on aligned\nlanguage models"
      },
      {
        "key": "schwinn_soft_2024",
        "author": "Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel,\nGauthier and Gunnemann, Stephan",
        "title": "Soft prompt threats: Attacking safety alignment and\nunlearning in open-source {LLMs} through the embedding space"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chao_jailbreaking_2023",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and\nHassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty\nqueries"
      },
      {
        "key": "vega_bypassing_2023",
        "author": "Vega, Jason and Chaudhary, Isha and Xu, Changming and Singh,\nGagandeep",
        "title": "Bypassing the safety training of open-source {LLMs} with\npriming attacks"
      },
      {
        "key": "liu_autodan_2023",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "{AutoDAN}: Generating stealthy jailbreak prompts on aligned\nLarge Language Models"
      },
      {
        "key": "zeng_johnny_2024",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi\nand Jia, Ruoxi and Shi, Weiyan",
        "title": "How Johnny can persuade {LLMs} to jailbreak them: Rethinking\npersuasion to challenge {AI} safety by humanizing {LLMs}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "qi_fine-tuning_2023",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "arditi_refusal_2024",
        "author": "Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel",
        "title": "Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ouyang_training_2022",
        "author": "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo\nand Wainwright, Carroll L and Mishkin, Pamela and Zhang,\nChong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex\nand Schulman, John and Hilton, Jacob and Kelton, Fraser and\nMiller, Luke and Simens, Maddie and Askell, Amanda and\nWelinder, Peter and Christiano, Paul and Leike, Jan and Lowe,\nRyan",
        "title": "Training language models to follow instructions with human\nfeedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "perez_red_2022",
        "author": "Perez, Ethan and Huang, Saffron and Song, Francis and Cai,\nTrevor and Ring, Roman and Aslanides, John and Glaese, Amelia\nand McAleese, Nat and Irving, Geoffrey",
        "title": "Red teaming language Models with language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wei_jailbroken_2023",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zou_universal_2023",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson,\nMatt",
        "title": "Universal and transferable adversarial attacks on aligned\nlanguage models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "vega_bypassing_2023",
        "author": "Vega, Jason and Chaudhary, Isha and Xu, Changming and Singh,\nGagandeep",
        "title": "Bypassing the safety training of open-source {LLMs} with\npriming attacks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chao_jailbreaking_2023",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and\nHassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty\nqueries"
      },
      {
        "key": "liu_autodan_2023",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "{AutoDAN}: Generating stealthy jailbreak prompts on aligned\nLarge Language Models"
      },
      {
        "key": "zeng_johnny_2024",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi\nand Jia, Ruoxi and Shi, Weiyan",
        "title": "How Johnny can persuade {LLMs} to jailbreak them: Rethinking\npersuasion to challenge {AI} safety by humanizing {LLMs}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "schwinn_soft_2024",
        "author": "Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel,\nGauthier and Gunnemann, Stephan",
        "title": "Soft prompt threats: Attacking safety alignment and\nunlearning in open-source {LLMs} through the embedding space"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "arditi_refusal_2024",
        "author": "Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel",
        "title": "Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "qi_fine-tuning_2023",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ouyang_training_2022",
        "author": "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo\nand Wainwright, Carroll L and Mishkin, Pamela and Zhang,\nChong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex\nand Schulman, John and Hilton, Jacob and Kelton, Fraser and\nMiller, Luke and Simens, Maddie and Askell, Amanda and\nWelinder, Peter and Christiano, Paul and Leike, Jan and Lowe,\nRyan",
        "title": "Training language models to follow instructions with human\nfeedback"
      },
      {
        "key": "rafailov_direct_2023",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and\nErmon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct Preference Optimization: Your language model is\nsecretly a reward model"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zou_improving_2024",
        "author": "Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek\nand Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan\nand Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan",
        "title": "Improving alignment and robustness with short circuiting"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "xhonneux_efficient_2024",
        "author": "Xhonneux, Sophie and Sordoni, Alessandro and G\u00fcnnemann, Stephan and Gidel, Gauthier and Schwinn, Leo",
        "title": "Efficient {Adversarial} {Training} in {LLMs} with {Continuous} {Attacks}"
      },
      {
        "key": "sheshadri_targeted_2024",
        "author": "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch,\nAengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and\nStickland, Asa Cooper and Perez, Ethan and Hadfield-Menell,\nDylan and Casper, Stephen",
        "title": "Targeted latent adversarial training improves robustness to\npersistent harmful behaviors in {LLMs}"
      },
      {
        "key": "mazeika_harmbench_2024",
        "author": "Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan",
        "title": "{HarmBench}: {A} {Standardized} {Evaluation} {Framework} for {Automated} {Red} {Teaming} and {Robust} {Refusal}"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "inan_llama_2023",
        "author": "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian",
        "title": "Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}"
      },
      {
        "key": "feuer_style_2024",
        "author": "Feuer, Benjamin and Goldblum, Micah and Datta, Teresa and\nNambiar, Sanjana and Besaleli, Raz and Dooley, Samuel and\nCembalest, Max and Dickerson, John P",
        "title": "Style outweighs substance: Failure modes of {LLM} judges in\nalignment benchmarking"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ouyang_training_2022",
        "author": "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo\nand Wainwright, Carroll L and Mishkin, Pamela and Zhang,\nChong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex\nand Schulman, John and Hilton, Jacob and Kelton, Fraser and\nMiller, Luke and Simens, Maddie and Askell, Amanda and\nWelinder, Peter and Christiano, Paul and Leike, Jan and Lowe,\nRyan",
        "title": "Training language models to follow instructions with human\nfeedback"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "rafailov_direct_2023",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and\nErmon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct Preference Optimization: Your language model is\nsecretly a reward model"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "madry_towards_2017",
        "author": "Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig\nand Tsipras, Dimitris and Vladu, Adrian",
        "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "mazeika_harmbench_2024",
        "author": "Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan",
        "title": "{HarmBench}: {A} {Standardized} {Evaluation} {Framework} for {Automated} {Red} {Teaming} and {Robust} {Refusal}"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "xhonneux_efficient_2024",
        "author": "Xhonneux, Sophie and Sordoni, Alessandro and G\u00fcnnemann, Stephan and Gidel, Gauthier and Schwinn, Leo",
        "title": "Efficient {Adversarial} {Training} in {LLMs} with {Continuous} {Attacks}"
      },
      {
        "key": "sheshadri_latent_2024",
        "author": "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and Casper, Stephen",
        "title": "Latent {Adversarial} {Training} {Improves} {Robustness} to {Persistent} {Harmful} {Behaviors} in {LLMs}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "zou_improving_2024",
        "author": "Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek\nand Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan\nand Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan",
        "title": "Improving alignment and robustness with short circuiting"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "inan_llama_2023",
        "author": "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian",
        "title": "Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}"
      },
      {
        "key": "feuer_style_2024",
        "author": "Feuer, Benjamin and Goldblum, Micah and Datta, Teresa and\nNambiar, Sanjana and Besaleli, Raz and Dooley, Samuel and\nCembalest, Max and Dickerson, John P",
        "title": "Style outweighs substance: Failure modes of {LLM} judges in\nalignment benchmarking"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "huang_virus_2025",
        "author": "Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling",
        "title": "Virus: {Harmful} {Fine}-tuning {Attack} for {Large} {Language} {Models} {Bypassing} {Guardrail} {Moderation}"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "burtsev_memory_2020",
        "author": "Burtsev, Mikhail S and Kuratov, Yuri and Peganov, Anton and\nSapunov, Grigory V",
        "title": "Memory Transformer"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "goyal_think_2023",
        "author": "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon,\nAditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
        "title": "Think before you speak: Training Language Models With Pause\nTokens"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "mu_learning_2023",
        "author": "Mu, Jesse and Li, Xiang Lisa and Goodman, Noah",
        "title": "Learning to compress prompts with gist tokens"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "xiao_efficient_2023",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han,\nSong and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "schick_toolformer_2023",
        "author": "Schick, Timo and Dwivedi-Yu, Jane and Dess\u00ec, Roberto and\nRaileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and\nCancedda, Nicola and Scialom, Thomas",
        "title": "Toolformer: Language models can teach themselves to use tools"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "jain_refusal_2024",
        "author": "Jain, Neel and Shrivastava, Aditya and Zhu, Chenyang and Liu, Daben and Samuel, Alfy and Panda, Ashwinee and Kumar, Anoop and Goldblum, Micah and Goldstein, Tom",
        "title": "Refusal {Tokens}: {A} {Simple} {Way} to {Calibrate} {Refusals} in {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "jain_refusal_2024",
        "author": "Jain, Neel and Shrivastava, Aditya and Zhu, Chenyang and Liu, Daben and Samuel, Alfy and Panda, Ashwinee and Kumar, Anoop and Goldblum, Micah and Goldstein, Tom",
        "title": "Refusal {Tokens}: {A} {Simple} {Way} to {Calibrate} {Refusals} in {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "wang_self-guard_2024",
        "author": "Wang, Zezhong and Yang, Fangkai and Wang, Lu and Zhao, Pu and Wang, Hongru and Chen, Liang and Lin, Qingwei and Wong, Kam-Fai",
        "title": "Self-{Guard}: {Empower} the {LLM} to {Safeguard} {Itself}"
      }
    ]
  }
]