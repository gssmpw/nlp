%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

\usepackage[dvipsnames]{xcolor}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{placeins}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,calc,positioning,fit,shapes}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}




% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
% \usepackage{amsmath}
\usepackage{amsmath,amsfonts,bm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% ALGORITHMS
% \usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{algorithm}
% \usepackage[noend]{algorithmic}
\usepackage{algpseudocode}
% \usepackage{tabto}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[font=small]{caption}
\usepackage{tabularx}


% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \renewcommand{\algorithmicloop}{\textbf{repeat}}

\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{enumitem}

% \PassOptionsToPackage{dvipsnames}{xcolor}
\algrenewcommand\algorithmicindent{1em}

% \newcommand{\rftoken}[1]{{\color{BrickRed}\texttt{<|rf|>}}}
% \newcommand{\rftoken}[1]{{\color{BrickRed}\texttt{<rf>}}}  % at least this one if you don't want the langle/rangle
\newcommand{\rftoken}[1]{\textnormal{\ensuremath{\color{BrickRed}\langle\texttt{rf}\rangle}}} 

% \newcommand{\rf}{\textnormal{\ensuremath{\color{BrickRed} \langle \texttt{rf} \rangle}}}
% \newcommand{\rf}{{\textcolor{BrickRed}{\mathord{\langle\texttt{rf}\rangle}}}
\def\rf{\textnormal{\ensuremath{\color{BrickRed}\langle\texttt{rf}\rangle}} }
\newcommand{\llama}[1]{\textsc{Llama3.2-3B-IT}}
\newcommand{\mistral}[1]{\textsc{Mistralv3-IT}}
\newcommand{\phithree}[1]{\textsc{Phi-3.5}}
\newcommand{\cat}[1]{\textsc{CAT}}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A generative approach to LLM harmfulness detection with special red flag tokens}

\begin{document}

\twocolumn[
\icmltitle{A generative approach to LLM harmfulness detection with special red flag tokens}
% \icmltitle{A Generative Approach to LLM Harmfulness Detection with Red Flag Tokens}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sophie Xhonneux}{equal,mila,udem}
\icmlauthor{David Dobre}{equal,mila,udem}
\icmlauthor{Mehrnaz Mofakhami}{mila}
\icmlauthor{Leo Schwinn}{tum,mila}
\icmlauthor{Gauthier Gidel}{mila,udem,cifar}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila}
\icmlaffiliation{udem}{Universit\'{e} de Montr\'{e}al}
\icmlaffiliation{cifar}{Canada AI CIFAR Chair}
\icmlaffiliation{tum}{Technical University of Munich}

\icmlcorrespondingauthor{Sophie Xhonneux}{lpxhonneux@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% Large Language Models (LLMs) that are robust to adversarial attacks, practitioners rely on several security components such as model hardening via fine-tuning, perplexity filters, or harmfulness classifiers.
% However, as model capabilities increase, defences must keep pace with the new vulnerabilities leveraging these capabilities. Thus, It is natural to embed a first layer of security into the models themselves that do not compromise with capabilities.
Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. 
These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. 
To avoid that, we propose to expand the model's vocabulary with a special token we call \emph{red flag token} (\rftoken{}) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. 
This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. 
This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility.
% it makes the model's concept of harmfulness explicit without affecting utility,
It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. 
In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier.
We further show an increased robustness to long contexts, and supervised fine-tuning attacks.

% Building safe systems more often than not relies on several layers of security. To build Large Language Models (LLMs) that are as robust as possible to adversarial attacks similarly rely on several security components such as model hardening via fine-tuning, perplexity filters, or harmfulness classifiers. The majority of defences based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. Due to the auto-regressive nature of LLMs, this can make the model vulnerable to sampling multiple times as well as attacks that achieve an initial affirmative response. We propose to expand the model's vocabulary with a special `red flag' token. We fine-tune the model to generate this token at any time when it detects harmful content, effectively transforming it into a generative classifier of harmfulness. This method offers several advantages: it evaluates each generated answer rather than just the input prompt, providing stronger defence against sampling-based attacks, promotes deeper alignment, simplifies the evaluation of the model's robustness, and reduces correlated failures when combined with a classifier. We further show an increased robustness to multi-turn conversations, long contexts, and supervised fine-tuning attacks.

%This approach has several benefits such as judging each answer provided rather than prompt asked, which helps in the face of sampling based attacks, less shallow alignment, easier evaluation of the model's robustness, and less correlated failures if used together with a classifier. We demonstrate the effectiveness of our approach on the latest generation of LLMs against a variety of adversarial attacks. We further show the increased robustness to multi-turn conversations, long contexts, and supervised fine-tuning attacks.
% note: remove the tension between multiple training objectives (jailbroken paper ref)

% \begin{itemize}
%     \item The majority of defences based on fine-tuning rely on dramatically changing the output distribution of the model in the face of a harmful request from an unsafe answer to refusal
%     \item Due to the auto-regressive nature of LLMs, this makes the model vulnerable to sampling as well as attacks that achieve an initial affirmative response (GCG, Continuous, pre-filling)
%     \item We propose to expand the model's vocabulary with a red flag token that the model can generate at any time during a harmful response
%     \item We demonstrate this on x, y, z
%     \item This approach has several benefits, such as being able to choose different thresholds on the fly to mark a response as harmful based on the logits of the red flag token
%     \item Each sample of the model is judged independently
%     \item This is a generative verifier, thus the failures are more likely to be weakly correlated with a classifier-based approach assuming different training datasets (independent failures).
%     \item Easier evaluation of the model.
%     \item (if we have time) show the potential for multi-turn awareness of harmful stuff
%     \item (if we make it work) show robustness to safety fine-tuning
%     \item Overcome the shallow-alignment issue
% \end{itemize}
\end{abstract}

\section{Introduction}

% # PARA 1: why this topic
%  - Large language models are becoming ever more ubiquitous.
%  - Many defence layers have been proposed

To make large language models (LLMs) that are robust against a determined adversary, practitioners rely on several security layers such as model hardening via fine-tuning~\citep{xhonneux_efficient_2024, sheshadri_latent_2024, zou_improving_2024}, perplexity filters~\citep{alon_detecting_2023}, or harmfulness classifiers~\citep{inan_llama_2023}. However, as model capabilities progress, so does their attack surface as the innate abilities can be used to circumvent defences~\citep{huang_endless_2024}---e.g., using a low-resource language to jailbreak the model. Hence, to enable robustness mechanisms to keep pace with the new vulnerabilities, it is natural to embed them into the models themselves, which will scale with these capabilities. Ideally, an LLM would have no harmful faculties; however, this is unrealistic as many desirable skills can be both useful and damaging depending on the context. Alternatively, harmful abilities may be re-learned based on harmless ones. Therefore, we argue that the model itself should detect when it is being used harmfully and that this behaviour can be complementary to standard safety training whose goal is to remove harmful faculties from the model. 
% Thus, there will always be a need to detect if capabilities are being used in a harmful way and to prevent this from occurring.

To address the aforementioned issues, we propose to add a special token for the LLMs to predict when the model considers that its capabilities are used unsafely. We call this token \emph{red flag token} (\rftoken{}) and train the model to output this token at any time during the generation of a harmful response while not changing its response after or in unrelated contexts. This complementary layer of safety has several benefits. First, our method only requires the \rftoken{} to be included in the answer---a single token change. In contrast, other methods, such as adversarial training, for instance, forces models to completely change their output distribution from a harmful response to a refusal. Thus, while standard safety training creates a tension between the probability of refusing harmful queries and the standard training objectives, such as next token prediction and instruction following~\citep{wei_jailbroken_2023}, our safety training objective only requires a minor edit---i.e., we learn to output the \rftoken{} in the useful, though unsafe, answer the model provides. Finally, one can calibrate the strictness of flagging a generation as harmful with a bias term on the \rftoken{} in the final softmax of the model.


Secondly, our method does not rely on a specific safe answer, meaning that if the model is broken, e.g., through pre-filling~\citep{andriushchenko_jailbreaking_2024} or random sampling~\citep{huang_catastrophic_2023}, we can still output a \rftoken{} to tag the answer as harmful and have it be filtered. This conceptually is a detection mechanism that is built into the model, i.e., we re-use the model's capabilities. This enables our method to be complimentary and to be used together with other hardening methods, such as adversarial training. However, our approach differs from a classifier because the \rftoken{} is a part of the model's generation and is re-used as an input later in the sequence. Thus, the \rftoken{} approach is fundamentally generative rather than discriminative. This has the potential to efficiently complement the other safety mechanisms such as standard safety training and filtering with another model. 

% Finally, our approach has the potential to simplify evaluation as rather than having to judge each model output, we can for a harmful query simply check if a \rftoken{} is likely to be generated. In other words, there is no need for heuristics such as key word based checking or judge models to determine whether we responded correctly to a harmful query.
We demonstrate the feasibility of this approach by designing a loss function consisting of three components a) a cross-entropy loss on generating the \rftoken{} b) a Kullback-Leibler (KL) loss term on the generation after the \rftoken{} (see \cref{fig:overview figure}) and c) a KL loss term on benign utility conversations. We test our approach with several adversarial attacks on three open-source models: \llama{}~\citep{grattafiori_llama_2024}, \mistral{}~\citep{jiang_mistral_2023}, and \phithree{}~\citep{haider_phi-3_2024}.

Finally, inspired by the idea of task arithmetic~\citep{ilharco_editing_2023}, we introduce the idea of storing the safety aspect of the model in a LoRA module~\citep{hu_lora_2021} and show that it can be applied to regain a significant amount of safety after an attacker has leveraged a fine-tuning API to remove the safety training of the model. Further, this concept seems to work for other safety approaches, such as adversarial training as well giving practitioners many ways to defend against fine-tuning attacks

To sum up, the contributions of this paper are:
\begin{itemize}
    \item We propose a specialised red flag token to reliably detect harmfulness at each generation step, even under strong adversarial attacks, including pre-filling, continuous, sampling, and ablation attacks;
    \looseness=-1
    \item We show empirically that our approach generalises beyond the training distribution, effectively handling significantly longer inputs than those seen during training;
    \item Finally, we demonstrate that our safety module can be efficiently stored in a LoRA module to be applied to detect harmful outputs from fine-tuned models.
\end{itemize}


\section{Method}

In this section, we present how we train the model to output the \rftoken{} during harmful continuations by only marginally affecting the model's output distribution (thus maintaining capabilities). First, we explain the threat model we consider (\cref{sec:threat model}) before introducing the notation, the datasets, and the loss we will use (\cref{sec:notation}). The loss is conceptually depicted in \cref{fig:overview figure}.

\subsection{Threat model}\label{sec:threat model}
Similar to a setting where harmfulness classifiers may be used, we assume that the LLM access is gated behind some web interface or API with no access to model weights, logits, or direct control of input/output processing---we call this the \emph{black box} setting. The key assumption we make is that the service provider can evaluate the logits and output of the model before passing it on to the user, including the filtering of special tokens such as assistant tokens or our \rftoken{} token. We further consider a more permissive \emph{gray box} setting where the user may have access to extra features, including pre-filling and or viewing the logits of non-special tokens. Finally, we consider the most permissive setting, a \emph{fine-tuning attack} setting where the user has access to some fine-tuning API. We do not consider our method to be applicable in \emph{white-box} settings as a harmful continuation can be used whether it is flagged or not.


%%%%%%%%%%%% vvvvv uncomment below for full width algorithm vvvvv %%%%%%%%%%%%
% \begin{figure}[t]
%    \centering
%     \includegraphics[width=1.3\linewidth]{figures/redflagtoken.pdf}
%     \vspace{-10mm}
%     \caption{\small The loss terms on harmful continuations: \rf is inserted at a random position $i$; language modelling cross-entropy is used to generate \rf at all positions up to $i$ and we use a KL divergence to ensure that the model distribution is unaffected after \rf.}
%     \label{fig:overview figure}
%         % \vspace{-5mm}
% \end{figure}

% \begin{algorithm*}[t]
%     \begin{algorithmic}[1]
%         \Require Reference model $\pi_{\mathrm{ref}}$, benign and harmful completions datasets $\mathfrak{D}_{\mathrm{harmless}}$ and $\mathfrak{D}_{\mathrm{harmful}}$, minimum offset $k$, probability distribution $\mathcal{P}$ over the indices $\{1,\ldots,|\hat{y}|\}$ of the continuation, loss weighting factors $\alpha_{\mathrm{benign}}, \alpha_{\mathrm{rf}}, \alpha_{\mathrm{CE}}$.
%         \begin{spacing}{1.2}
%         \For{$t = 1, \ldots, T$}
%             \State $\{(x, y)\} \sim \mathfrak{D}_{\mathrm{harmless}}$ 
%             \Comment{For benign loss}
%             \State  $\mathcal{D_\mathrm{benign}} \coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(y\mid x)\mid \pi_{\mathrm{ref}}(y\mid x))$
%             \State $\{(\hat{x},\hat{y})\} \sim \mathfrak{D}_{\mathrm{harmful}}$
%             \Comment{For red-flag loss}
%             \State $i \sim \mathcal{P}(\{1,\ldots,|\hat{y}|\})$ 
%             \Comment{Sample where to inject \rftoken{}}
%             \State $\mathcal{L}_{\mathrm{rf CE}} \coloneqq -\sum_{k\leq j \leq i}\log \pi_{\theta} (\rftoken{} | \hat{y}_{<j}, \hat{x} )$
%             \State 
%             $\mathcal{D_\mathrm{rf}}\!\coloneqq\! 
%             \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(\hat{y}_{\geq i}|\rftoken{},\hat{y}_{<i},\hat{x}) \!\!\mid\! \pi_{\mathrm{ref}}(\hat{y}_{\geq i}|\hat{y}_{<i},\hat{x}))$
%             \State $\mathcal{L}_{\mathrm{final}} \coloneqq \alpha_{\mathrm{benign}} \mathcal{D_\mathrm{benign}}  + \alpha_{\mathrm{rf}} \mathcal{D_\mathrm{rf}} + \alpha_{\mathrm{CE}} \mathcal{L}_{\mathrm{rfCE}}$ 
%             % \Comment{Loss to backpropagate through}
%             \State Optimize $\mathcal \pi_{\theta}$ using $\mathcal{L_{\mathrm{final}}}$
%             % \Comment{Update step}
%         \EndFor
%         \vspace{-5mm}
%         \end{spacing}
%     \end{algorithmic}
%         \caption{Red Flag Fine-tuning}  %\textcolor{red}{\textbf{TODO: update algorithm}}}
%     \label{alg:redflag}
% \end{algorithm*}
% %%%%%%%%%%%% ^^^^^ uncomment above for full width algorithm ^^^^^ %%%%%%%%%%%%



%%%%%%%%%%%% vvvvv uncomment below for NARROW width algorithm vvvvv %%%%%%%%%%%%

\begin{figure*}
\begin{minipage}{.48\textwidth}
    \vspace{-5pt}
    \begin{algorithm}[H]
    \begin{algorithmic}[1]
        \Require Reference model $\pi_{\mathrm{ref}}$, benign and harmful completions datasets $\mathfrak{D}_{\mathrm{harmless}}$ and $\mathfrak{D}_{\mathrm{harmful}}$, minimum offset $k$, probability distribution $\mathcal{P}$ over the indices $\{k,\ldots,|\hat{y}|\}$ of the continuation, loss weighting factors $\alpha_{\mathrm{benign}}, \alpha_{\mathrm{rf}}, \alpha_{\mathrm{CE}}$.
        \begin{spacing}{1.2}
        \For{$t = 1, \ldots, T$}
            \State $\{(x, y)\} \sim \mathfrak{D}_{\mathrm{harmless}}$ 
            \Comment{For benign loss}
            \State  $\mathcal{D_\mathrm{benign}} \coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(y\mid x)\mid \pi_{\mathrm{ref}}(y\mid x))$
            \State $\{(\hat{x},\hat{y})\} \sim \mathfrak{D}_{\mathrm{harmful}}$
            \Comment{For red-flag loss}
            \State $i \sim \mathcal{P}(\{k,\ldots,|\hat{y}|\})$ 
            \Comment{Sample where to inject \rftoken{}}
            % \State $\vp,\,\vq' \leftarrow \mathcal M(x_r),\,\mathcal M(x'_h) $ \Comment{Fwd pass through $\mathcal M(\cdot)$}
            % \State $\tilde \vp,\, \tilde \vq \leftarrow \tilde{\mathcal M}(x_r),\, \tilde{\mathcal M}(x_h) $ \Comment{Fwd pass through $\tilde{\mathcal M}(\cdot)$}
            % \State $\mathcal L_{\rftoken{}} = - \log \left(q'_{i^*}\right)$ \Comment{\rftoken{}~cross-entropy}
            % \State $\mathcal L_{\mathrm{KL}-\rftoken{}} = \frac{1}{N-i^*}\sum_{i=i^*}^N D_{\mathrm{KL}}\left( \vq'_{i+1} \mid\mid \tilde {\vq_{i}} \right)$ \Comment{Post-\rftoken{} ~ KL divergence}
            % \State $\mathcal L_{\mathrm{retain}} = \frac{1}{N-i_{\mathrm{start}}}\sum_{i=i_{\mathrm{start}}}^N D_{\mathrm{KL}}\left(\vp_{i}\mid\mid\tilde{\vp_{i}} \right)$ \Comment{Retain KL divergence}
            % 
            % \State Compute logits $\pi_{\mathrm{ref}}(\hat{y} \mid \hat{x} )$
            % \State Compute logits $\pi_{\mathrm{ref}}(y \mid x)$
            % \State Compute logits $\pi(y\mid x)$
            % \State Compute logits $\pi(\hat{y}_{>j}, \rftoken{}, \hat{y}_{<j}\mid \hat{x})$
            \State $\mathcal{L}_{\mathrm{rf CE}} \coloneqq -\sum_{k\leq j \leq i}\log \pi_{\theta} (\rftoken{} | \hat{x}, \hat{y}_{<j} )$
            \State 
            $\mathcal{D_\mathrm{rf}}\!\coloneqq\! 
            \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(\hat{y}_{\geq i}|\hat{x}, \hat{y}_{<i},\rftoken{}) \!\!\mid\! \pi_{\mathrm{ref}}(\hat{y}_{\geq i}|\hat{x}, \hat{y}_{<i}))$
            \State $\mathcal{L}_{\mathrm{final}} \coloneqq \alpha_{\mathrm{benign}} \mathcal{D_\mathrm{benign}}  + \alpha_{\mathrm{rf}} \mathcal{D_\mathrm{rf}} + \alpha_{\mathrm{CE}} \mathcal{L}_{\mathrm{rfCE}}$ 
            % \Comment{Loss to backpropagate through}
            \State Optimize $\mathcal \pi_{\theta}$ using $\mathcal{L_{\mathrm{final}}}$
            % \Comment{Update step}
        \EndFor
        \vspace{-5mm}
        \end{spacing}
    \end{algorithmic}
        \caption{Red Flag Fine-tuning}  %\textcolor{red}{\textbf{TODO: update algorithm}}}
    \label{alg:redflag}
    \end{algorithm}
    \vspace{-5mm}
\end{minipage}%
\;\;\;
\begin{minipage}[t]{.5\textwidth}
    \begin{figure}[H]
     \vspace{-120pt}
   \centering
    \includegraphics[width=1.3\linewidth]{figures/redflagtoken.pdf}
    \vspace{-10mm}
    \caption{\small The loss terms on harmful continuations: \rftoken{} is inserted at a random position $i$; we apply a language modelling cross-entropy 
    loss to generate \rftoken{} at all positions up to $i$, and use a KL divergence to ensure that the model distribution is unaffected after \rftoken{}.}
    \label{fig:overview figure}
        \vspace{-5mm}
    \end{figure}
\end{minipage}
\end{figure*}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Our Loss}\label{sec:notation}
We assume that we have a dataset $(\hat{x},\hat{y}) \sim \mathfrak{D}_{\mathrm{harmful}}$ of harmful pairs or prompts $\hat{x}$ and continuations $\hat{y}$. Further, we assume we have a dataset $(x, y) \sim \mathfrak{D}_{\mathrm{harmless}}$ of harmless (a.k.a., benign) pairs of prompts $x$ and harmless continuations $y$.
Given $k\geq 0$ representing a minimum offset, an index $i$ is sampled from a probability distribution $\mathcal{P}_k$ over the indices $\{k,\ldots,|\hat{y}|\}$ of the continuation at which to insert the red flag token into the harmful continuation $\hat{y}$. We get the tokens $\hat{y}_{<i}$ which come before index $i$, the \rftoken{} token, and the tokens $\hat{y}_{\geq i}$. We use $\mathcal{L}_{\mathrm{CE}}$ to denote the cross entropy and $\mathcal{D}_{\mathrm{KL}}$ to denote the Kullback-Leibler divergence (KL).

We consider our reference model $\pi_{\mathrm{ref}} \coloneqq \pi_{\theta_0}$. Our loss consists of three components: First, to ensure our model outputs the red-flag token in harmful completions, we use a standard language modelling cross-entropy loss on all harmful completion tokens starting at the minimum offset $k$ up to and including the \rftoken{} token:
\begin{equation}
    \mathcal{L}_{\mathrm{rf CE}} \coloneqq -\sum_{k\leq j \leq i} \log \pi_{\theta} (\rftoken{} | \hat{x}, \hat{y}_{<j} )\,.
\end{equation}
To maintain model performance and reduce distribution shift as much as possible without increasing the likelihood of a harmful answer, we use a KL divergence on the tokens after the \rftoken{}
\begin{equation} \label{eq:KL_after}
    \mathcal{D_\mathrm{rf}}\!\coloneqq\! \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(\hat{y}_{\geq i}|\hat{x}, \hat{y}_{<i},\rftoken{})\! \mid\! \pi_{\mathrm{ref}}(\hat{y}_{\geq i}|\hat{x},\hat{y}_{<i})),
\end{equation}
and again, to reduce distribution shift and to capture that the likelihoods should not change on unrelated tasks we include a KL loss on benign prompts and answers
\begin{equation}
    \mathcal{D_\mathrm{benign}} \coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(y|x) \mid \pi_{\mathrm{ref}}(y|x)).
\end{equation}
All these losses put together, we get:
\begin{equation}
    \mathcal{L}_{\mathrm{final}} \coloneqq \alpha_{\mathrm{benign}} \mathcal{D_\mathrm{benign}}  + \alpha_{\mathrm{rf}} \mathcal{D_\mathrm{rf}} + \alpha_{\mathrm{CE}} \mathcal{L}_{\mathrm{rfCE}}.
\end{equation}
Note that none of the loss functions make a harmful continuation more likely, enabling our approach to be complementary to other safety fine-tuning techniques. We summarise the training algorithm in \cref{alg:redflag}.

% \begin{algorithm}[tb]
%    \caption{\rftoken{} training algorithm}
%    \label{alg:main}
% \begin{algorithmic}
%    \State {\bfseries Input:} benign data $x, y$, harmful data $\hat{x},\hat{y}$
%    \State Sample position $j$ at which to insert the \rftoken{}
%    \State Compute logits $\pi_{\mathrm{ref}}(\hat{y} \mid \hat{x} )$
%    \State Compute logits $\pi_{\mathrm{ref}}(y \mid x)$
%    \State Compute logits $\pi(y\mid x)$
%    \State Compute logits $\pi(\hat{y}_{>j}, \rftoken{}, \hat{y}_{<j}\mid \hat{x})$
%    \State Calculate cross entropy of $\pi(\rftoken{}, \hat{y}_{<j}\mid \hat{x})$ with \rftoken{} as target
%    \State Calculate KL between $\pi(y\mid x)$ and $\pi_{\mathrm{ref}}(y\mid x)$
%    \State Calculate KL between $\pi(\hat{y}_{>j}\mid \rftoken{}, \hat{y}_{<j} \hat{x}))$ and $\pi_{\mathrm{ref}}(\hat{y}_{>j}\mid \hat{y}_{<j},\hat{x})$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm*}[t]
%     \small
%     \begin{spacing}{1.37}
%     \caption{Red Flag Fine-tuning}  %\textcolor{red}{\textbf{TODO: update algorithm}}}
%     \label{alg:redflag}
%     \begin{algorithmic}[1]
%         \Require Original frozen model $\pi_{\mathrm{ref}}$, model with red flag tuning $\pi_{\theta}$ with LoRA adapters, probability distribution $\mathcal{P}$ over integers, benign dataset $\mathfrak{D}_{\mathrm{harmless}}$, harmful completions dataset $\mathfrak{D}_{\mathrm{harmful}}$, number of steps $T$, loss weighting factors $\alpha_{\mathrm{benign}}, \alpha_{\mathrm{rf}}, \alpha_{\mathrm{CE}}$.
%         \FOR{$t = 1, \ldots, T$}
%             \STATE $\{(\hat{x},\hat{y}, y)\} \sim \mathfrak{D}_{\mathrm{harmful}}$ \& $\{(x, y)\} \sim \mathfrak{D}_{\mathrm{harmless}}$ \Comment{Sample Batch Elements}
%             \STATE $i \sim \mathcal{P}(0,|\hat{y}|)$ \Comment{Select where to inject the \rftoken{}}
%             \STATE $\mathcal{L}_{\mathrm{rf CE}} \coloneqq \sum_{0\leq j \leq i}\log \pi_{\theta} (\rftoken{} | \hat{y}_{<j}, \hat{x} )$
%             \STATE  $\mathcal{D_\mathrm{benign}} \coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(y\mid x)\mid \pi_{\mathrm{ref}}(y\mid x))$
%             \STATE $\mathcal{D_\mathrm{rf}}\coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(\hat{y}_{\geq i}|\rftoken{},\hat{y}_{<i},\hat{x}) \mid \pi_{\mathrm{ref}}(\hat{y}_{\geq i}|\hat{y}_{<i},\hat{x}))$
%             \STATE $\mathcal{L}_{\mathrm{final}} \coloneqq \alpha_{\mathrm{benign}} \mathcal{D_\mathrm{benign}}  + \alpha_{\mathrm{rf}} \mathcal{D_\mathrm{rf}} + \alpha_{\mathrm{CE}} \mathcal{L}_{\mathrm{rfCE}}$ \Comment{Loss to backpropagate through}
%             \STATE Optimize $\mathcal \pi_{\theta}$ using \(\mathcal{L_{\mathrm{final}}}\)\Comment{Update step}
%         \ENDFOR
%     \end{algorithmic}
%     \end{spacing}
% \end{algorithm*}

\section{Experiments}
\subsection{Models \& Datasets}
We fine-tune \llama{}~\citep{grattafiori_llama_2024}, \mistral{}~\citep{jiang_mistral_2023}, and \phithree{}~\citep{haider_phi-3_2024} using our algorithm on the Harmbench~\citep{mazeika_harmbench_2024} training set with their refusals and 32 harmful continuations sampled for each harmful prompt using the ablation attack~\citep{arditi_refusal_2024}. We use 5,000 samples from the Alpaca dataset~\citep{alpaca} as benign prompts. All the models listed have as part of their tokenizers a set of reserved special tokens. Thus, allowing us to avoid extending the vocabulary and instead we re-purpose one of these unused special tokens to be the \rftoken{}.

We measure the utility of trained models using MMLU~\citep{hendrycks2020measuring}, ARC-E and ARC-C~\citep{chollet2019measure}, which are standard LLM benchmarks as well as a dataset of benign prompts. This dataset of benign prompts consists of 119 prompts from \textsc{Ultrachat200k} (validation split) that we randomly picked (solely making sure that they were good quality), and the Harmless dataset consisting 40 benign prompts with a similar syntax as Harmbench provided by~\citep[Appendix I]{xhonneux_efficient_2024}. We also use this benign dataset to compute the false positive rate of refusal (or \rftoken{}) in \cref{fig:sft attack roc}. 

For adversarial robustness evaluation, we compute the defence success rate (DSR) of different attacks on the Harmbench test set~\citep{mazeika_harmbench_2024} that contain 159 harmful prompts. Thus, we balance the dataset with the same number of harmful and benign prompts. Either a refusal or a \rftoken{} surpassing a certain probability threshold counts as a successful defence. The thresholds are set per model and are $0.001$, $0.01$, and $0.001$ for \llama{}, \mistral{}, and \phithree{}, respectively.

We train our models with a single A100 GPU with LoRA~\citep{hu_lora_2021} and a batch size of 64 using the AdamW optimiser~\citep{loshchilov_decoupled_2017} (for more hyper-parameters see the Appendix~\cref{app:hparams}). Due to computational constraints we did not extensively hyperparameter tune or ablate our method. We acknowledge this in the Limitation section~\cref{sec:limitation}. Thus, it is not unlikely that further gains can be achieved by more tuning.

\subsection{Design decisions}
There are several design decisions to consider:

\textbf{The cross-entropy loss on \rftoken{}} can be computed on each index before and including the sampled position $j$ or only on $j$. In other words, we have the choice to allow the model for flexibility of when to output \rftoken{} at the cost of potentially overfitting more because we now train the model to output \rftoken{} immediately after the instruction token. In particular, this forces the model to judge the prompt quite strongly, leading to a higher probability for \rftoken{} in a refusal as well. In practice, we tested both approaches and saw better results computing the cross entropy up to and including index $j$. A potential solution to avoid over-fitting after the instruction token is to have a minimum offset into the harmful continuation both for sampling $j$ as well as the cross-entropy term, which we apply to \llama{} and \mistral{}. 

\textbf{The sampling distribution} of the index at which to insert the \rftoken{} is a key choice. We tested both a geometric distribution as well as a uniform distribution over the harmful continuation. We decide to use a geometric distribution, which means that fewer positions will be used in the $\mathcal{}$ cross-entropy loss, and more will be used in $\mathcal{D}_{\mathrm{rf}}$.

\textbf{The attention mask} in the $\mathcal{D}_{\mathrm{rf}}$ can also be amputated not to include the harmful prompt $\hat{x}$ and the harmful continuation $\hat{y}_{<j}$ before the \rftoken{} such as to force the model to store the harmful continuation information in the \rftoken{} embedding. In other words, with probability $p=0.5$ $\mathcal{D_\mathrm{rf}}\coloneqq \mathcal{D}_{\mathrm{KL}}(\pi_{\theta}(\hat{y}_{\geq i}|\rftoken{}) \mid \pi_{\mathrm{ref}}(\hat{y}_{\geq i}|\hat{x},\hat{y}_{<i})),$ We choose to apply this trick probabilistically with probability 0.5, such as to make training more stable but still encourage the \rftoken{} to be meaningful for generation.

% \textbf{The negative cross entropy} could be considered in the benign responses $y$ to strongly make sure to reduce the probability of the \rftoken{} to be generated accidentally when the answer is actually benign.


\textbf{Calibrating \rftoken{} sensitivity} can be done in several ways.
We can add a \emph{logit bias} term to the \rftoken{} before passing the logits through a softmax, modifying the distribution to be:
$$
    p(i \mid x) = \frac{
        \exp(Z_i)
    }{
        \sum_j \exp(Z_j) %\exp(z_j + \delta_{ij}b_\rftoken{})
    }\,, \, \mathrm{where} ~ Z_j =\begin{cases}
        z_j + b & j=\rftoken{}\\
        z_j & j\neq\rftoken{}
    \end{cases}
$$
where $z_i$ is a logit for the $i^{th}$ token and $b$ is a user-specified bias to re-weight $\rftoken{}$.
We can also directly monitor $p(\rftoken{}|x)$ without adding a logit bias term or looking for explicit generations of \rftoken{}, and set a model-dependant threshold in which to flag the generation as potentially harmful. 
We elect to use the second strategy as it works well and is simpler to apply in practice.

% \paragraph{Refusal Judge} 

\subsection{Baselines}
We consider three baselines in this paper. The first is the original safety training of the models (the three models we consider already come with an initial safety training). The second baseline is \cat{}~\citep{xhonneux_efficient_2024}---an adversarial training technique using continuous attacks, where we replicate their training procedure using the same datasets for a fair comparison. For the third baseline, we extend~\citet{jain_refusal_2024} to our setting, whereby we insert the \rftoken{} at the first position of the assistant's response and train with cross-entropy on the harmful continuation and benign answers. We call this baseline `Fixed position RF'. Note that this baseline is different from what is formally proposed in~\citet{jain_refusal_2024} as in their framework the ``refusal'' token is placed before refusal continuations only and thus would lead to similar results as the ones of the base models which have received standard safety training (e.g., training to refuse harmful queries and accept benign ones).

\subsection{Robustness Evaluation} 
\looseness=-1
We compute the defence success rates against the following attacks:

\subsubsection{Realistic Attack Scenarios}

\textbf{Pre-filling} whereby the attacker is allowed to insert the first $n$ tokens as the response of the assistant. We use the Harmbench~\citep{mazeika_harmbench_2024} affirmative responses as the pre-fill attack. Note that in this setting the \rftoken{} models (including the `Fixed position RF' model) are allowed to check the logits of the pre-filled text!

\textbf{Sampling Attacks} where we attack the model by sampling the response multiple times \citep{hughes_best-of-n_2024}. Occasionally, the model may eventually provide an answer to a harmful prompt after repeated queries. In our experiments, we sample up to 16 times or until the model responds, as evaluated by the official classifier for text behaviours in HarmBench\footnote{\url{huggingface.co/cais/HarmBench-Llama-2-13b-cls}}. We use a temperature of $\tau=0.9$
and a top-$p$ value of $0.9$ for the sampling attack.

\subsubsection{Limit Testing}

\textbf{Continuous Attacks} are a type of attack where soft tokens~\citep{schwinn_soft_2024} are optimised with sign gradient descent using an affirmative response as the target to break the model's safety. We allow the attack to use 70 steps with 10 soft tokens and no epsilon ball.

\textbf{Refusal Vector Ablations} are attacks in which a single ``refusal direction'' is identified in the model's residual stream, and then ablated from activations across all layers and token positions \citep{arditi_refusal_2024}.
Suppressing this direction from being represented in the model's residual stream effectively disables a model's ability to refuse harmful queries.

The first two attacks are gray-box attacks using additional features such as temperature-based sampling and pre-filling, while the latter two are strong white-box attacks. While the continuous attack and the ablation attack are unrealistic settings for our \rftoken{}, however, we include them as sanity-check and to test the limits of our approach.

An attack is successful if the model \emph{does not} refuse \textbf{or} the \rftoken{} probability \emph{does not} surpasses a model-specific threshold. We evaluate refusals using GPT-4o~\cite{openai_gpt-4o_2024} as a judge.
%except for the continuous attacks on \llama{} as they often produce gibberish outputs where text is simply repeated and the judge consistently is unable to classify those outputs as harmless correctly (see \cref{app:judge} for the prompt used for the judging). 
Our results are shown in \cref{fig:main results}.

\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figures/Llama3.2_3B_bar_RF.pdf}
    \caption{\llama{}~~~(threshold: $10^{-3}$)}
    \end{subfigure}\\
    \vspace{1em}
    \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figures/Mistralv3_bar_RF.pdf}
    \caption{\mistral{}~~~(threshold: $10^{-2}$)}
    \end{subfigure}\\
    \vspace{1em}
    \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figures/Phi3.5_bar_RF.pdf}
    \caption{\phithree{}{}~~~(threshold: $10^{-3}$)}
    \end{subfigure}
        \caption{\small Model evaluation of the robustness safety trade-off. In each plot, the left represents utility benchmarks (higher is better), and the right represents adversarial \textbf{defence} success rates (higher is better). 
    Both refusal and \rftoken{} detection are considered a successful defence, where $\rftoken{}$ is detected if it is above some calibration threshold (in parenthesis). 
    Pre-filling \& sampling are gray-box attacks, whereas continuous \& ablation are white-box attacks. Refusals are judged by GPT-4o.}
    \label{fig:main results}
\end{figure}


\subsection{Generalisation to Longer Contexts}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/long_context_small.png}
    \caption{Monitoring the \rf token's log-probability and the log-probability of the top-1 token for a particular multi-turn user/assistant interaction. See \autoref{fig:long_context_full} for this example with the corresponding text.}
    \label{fig:long_context_small}    
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/meta-llama-Llama-3.2-3B-Instruct-long-context-stats.pdf}
    \caption{\llama{}}
    \end{subfigure}    \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/microsoft-Phi-3.5-mini-instruct-long-context-stats.pdf}
    \caption{\phithree{}}
    \end{subfigure}
    \caption{Multi-turn long-context log probabilities of \rftoken{} where the harmful prompt appears after several turns of benign dialogue. We sample 50 chats of varying length from \textsc{UltraChat-200k} and 50 prompts from our harmful test set. The log-probability of \rf is monitored throughout the generated output of each chat/prompt combination ($\mathrm{Gen}_{\mathrm{base}}$). We additionally apply a pre-filling attack ($\mathrm{Gen}_{\mathrm{prefill}}$), where we pre-fill with an affirmative response. We calculate the maximum $\log P(\rf)$ separately over the pre-filled text and generated text, and the average $\log P(\rf)$ over the pre-filled text and the first 10 generated tokens. We observe no decay in the model's ability to predict \rf with increasing context length; longer sequences can approach $\sim 2000$ tokens.}
    \label{fig:long_context_agg}
\end{figure*}


During training, we only include data with a single turn of user/assistant interaction, with \rftoken{} being injected following some distribution biased towards the start of the first assistant's response.
We validate that our approach generalises to multiple user/assistant interactions without over-fitting to the start of the conversation by sampling a number of benign conversations from \textsc{UltraChat-200k}, and then appending harmful queries from our test set. 
By monitoring the probability of \rftoken{} throughout the model's generation, we find that the prediction of \rftoken{} does not deteriorate with increased sequence length.
One chat interaction is shown in \autoref{fig:long_context_small}; the \rf probabilities consistently remain low in the regime in which it was trained (during the assistant's turn) and increases sharply in the presence of harmful content during pre-filling and generation. Aggregate statistics are shown in \autoref{fig:long_context_agg}.

\subsection{Fine-tuning attacks}
We consider the fine-tuning attack threat model discussed in \cref{sec:threat model}, where the attacker is able to fine-tune the model as they wish through an API. We trained a LoRA module that encapsulates the weight changes for the model to insert the \rftoken{} in harmful continuations on \llama{}. Hence, our defence is to apply this LoRA module one or more times after the attacker has been able to fine-tune the base model. For the fine-tuning attack we also use a LoRA module and use the data and hyper-parameters from~\citet{qi_safety_2024}, which consists of about 100 harmful examples and continuations. 

As baselines we consider both the aforementioned CAT~\citep{xhonneux_efficient_2024} as well as the `Fixed position RF' stored in LoRA modules. In addition, we test whether applying each of these approaches multiple times can further improve robustness. Finally, we check whether we can combine CAT and our own \rftoken{} approach. Due to the strength of the fine-tuning attack setting for both `Fixed position RF' and our own \rftoken{} we consider different thresholds for the \rftoken{} probability. This demonstrates the fundamental robustness and utility trade-off that exists in terms of false positive rate (FPR) and true positive rate (TPR)---i.e., defence success rate. The goal is to improve the Pareto front. As before, we use the same Harmbench test set and the same Harmless dataset from \cref{fig:main results}; the results are in \cref{fig:sft attack roc}. 

We also validate that on benign fine-tuning our \rftoken{} module does not impact the gained performance significantly. We test this on \textsc{GSM8k}~\citep{cobbe2021gsm8k} in chat mode under strict match of the answer---see the \cref{app:sft gsm8k} for final numbers.
\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/sft_attack_roc.pdf}
    \caption{ROC curve for different max probability thresholds to defend against a \emph{fine-tuning attack} against \llama{}. Baseline models are a CAT~\citep{xhonneux_efficient_2024} and a \rftoken{} module with a fixed position. Additionally, we show the effect of applying the LoRA module containing the safety fine-tunings multiple times as well as cross-combination of adversarial training and a \rftoken{} module.}
    \label{fig:sft attack roc}
\end{figure}
\subsection{Discussion}
\looseness=-1
The first observation from \cref{fig:main results} is that \rftoken{} approach maintains near-perfect utility across all models. On \llama{} we find that all the baselines considered are able to achieve nearly the same utility scores as the base model. This allows for a good comparison in terms of the robustness utility trade-off as all baselines are calibrated to similar utility. Our proposed \rftoken{} approach is able to nearly perfectly defend against all the gray-box attacks, such as sampling and pre-filling across all models. We can see from the difference in defence success rate (DSR) between the base model and the \rftoken{} model that even when requests are not being refused, the \rftoken{} is being generated and thus defends against the malicious attack. The white-box ablation attack also does not succeed in preventing the \rftoken{} from triggering on \llama{}. However, on both \mistral{} and \phithree{} the attack succeeds, which may be because the attack also has many hyperparameters or the base models have different safety properties. The continuous attack on \llama{} is challenging to interpret due to the difficulty of getting coherent text after the attack. On \mistral{}, however, we can see that the continuous attack is effective at extracting coherent responses. These attacks are difficult to defend against due to its ability to change the token embeddings. This shows a limitation of our approach. Note we did not try discrete white-box like GCG~\citep{zou_universal_2023} due to them being unable to break the base model such as \llama{}. We note that the CAT on \llama{} performs significantly worse on both pre-filling and the ablation attacks showing that to maintain the same utility it has arguably less robustness. The `Fixed position RF' is only marginally worse on the continuous attacks and otherwise performs equally well. It is worth re-iterating that continuous and refusal ablation attacks are white-box attacks in which case flagging a response as harmful has little utility to begin with and these attacks serve as an empirical ``worst-case'' attack, which we use a sanity-check. These white-box attacks operate in the smooth representation or embedding space (rather than discrete token space), thus an interesting open question remains on how the feature representations change under such attacks, and how they interact with \rftoken{}.

The fine-tuning attack setting is interesting because it has been under-explored. \citet{qi_safety_2024} consider a similar setting and restrict the fine-tuning API to allow modification on the initial token distribution. \citet{tamirisa_tamper-resistant_2025} adversarially trains a model by adversarially perturbing the model weights during the attack step. In contrast, we consider storing our safety training in a LoRA module and applying it after the harmful fine-tuning attack. In \cref{fig:sft attack roc}, the limitations of the fixed position \rftoken{} approach become clear to see; the Pareto front of this approach is significantly worse and applying the LoRA module multiple times does not help, while the variable position \rftoken{} LoRA module maintains its ability to predict \rftoken{} in harmful contexts. The adversarial training baseline (CAT) is also able to provide good robustness though it cannot be calibrated and thus does not produce a Pareto front. However, by applying the CAT LoRA module once or twice from this baseline we get a different robustness trade-off. Contrary to adversarial training, our LoRA module, together with a carefully chosen threshold, can provide fine-grained calibration and a good Pareto front. Finally, we show that the adversarial training module and our \rftoken{} module are complimentary and allow for good robustness and a calibrated trade-off between utility and robustness in terms of TPR and FPR.

\subsection{Limitations}\label{sec:limitation}

Expectedly, our approach is not able to defend against attacks that have complete access to the model weights such as continuous attacks, although substantial robustness against ablation attacks is achieved. This shows that the association between harmful generation and the \rftoken{} is circumvented by these attacks, demonstrating a limitation of our approach. Furthermore, in the fine-tuning attack setting, there is still a trade-off between robustness and utility in the face of an attacker, albeit a much stronger Pareto-front than without the \rftoken{} module.

Another limitation of our method is the amount of hyperparameters that are introduced that require tuning. While it is not too difficult to achieve a decent trade-off, one can likely achieve much better performance with better hyperparameter tuning as we did not have the resources to effectively tune the model (all the trainings were done with a single GPU). In addition, our ability to associate the \rftoken{} with harmfulness relies on the data provided.

\section{Related Work}

% \texttt{\langle rf \rangle}

\paragraph{Jailbreaking LLMs}
% \begin{itemize}
%     % \item Jailbreaking concept \cite{wei_jailbroken_2023}
%     % \item Red-teaming \cite{perez_red_2022}
%     % \item Optimization attacks \citep{zou_universal_2023, schwinn_soft_2024}
%     % \item Heuristic attacks \citep{chao_jailbreaking_2023,vega_bypassing_2023, liu_autodan_2023, zeng_johnny_2024}
%     \item SFT attacks \citep{qi_fine-tuning_2023}, Refusal ablation \citep{arditi_refusal_2024}
% \end{itemize}

% hello \rf asdf 
Modern LLMs used as chatbots are trained to follow user instructions~\citep{ouyang_training_2022} while also being trained to respond in a safe and harmless manner~\citep{perez_red_2022}.
While users quickly found ways to manually craft ``jailbreaks'' which could circumvent these safeguards and elicit harmful content from these systems~\citep{wei_jailbroken_2023}, automated methods for crafting adversarial attacks were also shown to be effective.
Particularly,~\citet{zou_universal_2023} propose a greedy-coordinate gradient (GCG) search algorithm to find an adversarial suffix optimized to pre-fill \citep{vega_bypassing_2023} an affirmative response in a model's response. 
Other approaches use heuristics to craft interpretable jailbreaks with only black-box access to the target model~\citep{chao_jailbreaking_2023, liu_autodan_2023, zeng_johnny_2024}.
Given white-box access to the target model, more powerful attacks are possible.
Adversarial soft prompts can be optimized to manipulate the model’s outputs \citep{schwinn_soft_2024}, causal features responsible for refusal behaviour can be selectively ablated \citep{arditi_refusal_2024}, and fine-tuning can be used to override or remove safety training entirely \citep{qi_fine-tuning_2023}. 


\paragraph{Defences}
% \begin{itemize}
%     \item Fine-tuning / alignment training \citep{ouyang_training_2022, rafailov_direct_2023}
%     \item Circuit breakers \citep{zou_improving_2024}
%     \item Adversarial training (Cont/Latent + R2D2) \citep{xhonneux_efficient_2024, sheshadri_targeted_2024, mazeika_harmbench_2024}
%     \item Classifiers/judge LLMs \citep{inan_llama_2023, feuer_style_2024}
% \end{itemize}

Beyond standard pre-training, LLMs are typically trained with preference optimization techniques such as RLHF \citep{ouyang_training_2022} or DPO \citep{rafailov_direct_2023} to be more aligned with human preferences.
Jailbreaks can be incorporated into this preference alignment phase to increase resilience to such attacks (as is often done with red-teaming methods), but this does not often generalise to novel jailbreaks.
Historically, in the context of vision models, actively training against adversarial attacks in an online manner (i.e., adversarial training) is the only method that has shown increased adversarial robustness \citep{madry_towards_2017}. However, in the context of language, most discrete attacks are prohibitively expensive to use online. 
\citet{mazeika_harmbench_2024} train against adversarial suffixes generated by GCG, but continually update a pool of examples rather than generate each attack from scratch.
Other approaches perform adversarial training by attacking the embedding or latent space of the model~\citep{xhonneux_efficient_2024, sheshadri_latent_2024} which is much more efficient to compute and transfers to discrete attacks.
Beyond adversarial training, newer defences target and alter harmful representations in order to prevent a model from producing harmful outputs entirely \citep{zou_improving_2024}.
Independent from training a model to be more robust to jailbreaks is to classify and judge the potential harmfulness of the generated text, often with another LLM fine-tuned for this task \citep{inan_llama_2023, feuer_style_2024}, although this does require additional resources to classify the outputs. Concurrent work \citet{huang_virus_2025} has shown that classifiers alone are often not sufficient, further making the case that other approaches are needed especially against permissive but common threat models such as the fine-tuning box attack.

% \todo{Interpretability - Maybe include this section? Representation engineering? feels like this could be an important section }

\paragraph{Special Tokens} 
Several works have explored training or utilising special tokens for specific purposes.
\citet{burtsev_memory_2020} prepend ``memory'' tokens to an input prompt on a target task.
\citet{goyal_think_2023} append ``pause'' tokens, which are hypothesised to give the LLM a buffer sequence to reason over before producing an output.
\citet{mu_learning_2023} train LLMs to compress longer prompts into smaller sets of ``gist'' tokens as a means to shorten the context. 
\citet{xiao_efficient_2023} prepend ``attention sinks'' to improve generalization to long-context sequences.
LLMs have also been trained to use a variety of tools (such as a calculator or internet access), which are denoted and invoked via special tokens \citep{schick_toolformer_2023}.
Most closely related to our approach is the recent work of \citet{jain_refusal_2024}, where a model is trained to prefix an output with a special \emph{refusal} or \emph{response} token based on the behaviour of whether the model refuses or responds to a prompt.
While their approach is related in that special tokens are leveraged in the context of alignment, the approach and objective are conceptually different.
Their method correlates these tokens with \textit{behaviour} (i.e., refusal or response) in order to better calibrate such behaviours, whereas our approach correlates a special token with some implicit notion of a concept (i.e., harmfulness), \textit{without} modifying the model's original behaviour. This conceptual difference leads to drastically different losses in the formulation. For instance~\citet{jain_refusal_2024} do not propose a KL divergence with a reference model~(\cref{eq:KL_after}) to maintain the predictions similar to the reference model after \rftoken{} is outputted which hurt the model's utility and is \emph{not} complementary with standard safety training (instead it is a way to calibrate the model post-hoc safety training). Moreover, their model is only trained to output a ``behavioural token" (e.g., ``refuse" or ``respond") at the beginning of the answer, which is significantly less efficient to detect harmfulness, as shown in our experiments. In contrast, our work proposes an approach that is complementary to standard safety training where the model essentially acts as an ``implicit judge'' on its own generated output, improving its transparency and providing a clear signal to evaluate potentially harmful generations without incurring any additional computational cost at inference time. Similar work by \citet{wang_self-guard_2024} also learns to tag answers as harmless or harmful, but they use two stage training procedure and hardcode the tag to be at the end of the response. They only consider fixed jailbreak prompts rather than attacks and do not consider the fine-tuning setting at all.
\section{Conclusion}
% To sum-up, the contributions of this paper are:
% \begin{itemize}
%     \item We propose a specialized red flag token to reliably detect harmfulness at each generation step, even under strong adversarial attacks, including pre-filling, continuous, sampling, and ablation attacks;
%     \item We show empirically that our approach generalizes beyond the training distribution, effectively handling significantly longer inputs than those seen during training;
%     \item Finally, we demonstrate that our safety module can be efficiently stored in a LoRA module to be applied to detect harmful outputs from fine-tuning.
% \end{itemize}

We propose to detect harmful outputs from a large language model (LLM) without an external classifier, but using the generative model itself. To achieve this goal we develop a training algorithm such that the target LLM outputs a special red flag token (\rftoken{}) at any time during a harmful continuation. This provides us with a generative approach to detect harmfulness even under strong adversarial attacks such as pre-filling and sampling. We show that our method significantly improve robustness without affecting utility.
% We tested the limit of our approach against the strongest white-box attacks in continuous- and ablation attacks, showing some robustness. 
We demonstrate that our approach generalizes to very long contexts with multiple conversation turns despite having only been trained on short one-round conversations.

Finally, we investigate another strong threat model that of a fine-tuning attack in an API setting and show that you can store safety training approaches such as our own \rftoken{} or adversarial training (CAT~\citep{xhonneux_efficient_2024}) in a LoRA module and apply it post-hoc against a jailbroken model to regain significant robustness without harming benign fine-tuning performance. These different model hardening approaches are complimentary, as we show by combining a continuous adversarial training (CAT) module and our own \rftoken{} module.
\FloatBarrier
\section*{Impact Statement}

Machine learning tools such as large language models (LLMs) are finding widespread usage in today's wealthy societies. As such, any work in this area has the potential for a significant impact, as it could avoid catastrophic outcomes due to a potential lack of safety of these widespread models.
% especially for any papers working on the harmfulness or safety of these models. 

This work aims to provide a new approach to reduce the harmful behaviour of LLMs when used via a webpage or API. As such, the desired impact of this work is overwhelmingly positive. However, it has to be acknowledged that any work aiming to filter or prevent harmful content from reaching users of non-open source LLMs can most likely also be re-used for censorship and thus also runs the risk of reinforcing biases of the LLM operator---intentionally or not.

More broadly and in the longer term, our work may enable practitioners to build an extra layer of safeguards into models that have capabilities that can both be useful and harmful and thus cannot or will not be removed. In such a situation, our approach and future derivatives can be used to tag and recognize the harmful usage of a capability. A potential downside is that practitioners may be over-reliant on this \rftoken{} as a defence mechanism rather than ensuring that learning algorithms and data during pre-training and various post-training stages remove harmful capabilities to the model. As such, this work also considers the worst-case attacks, such as very strong fine-tuning attacks, continuous attacks, and ablation attacks, to clearly show that this approach can be circumvented with sufficient access and thus shall not be the only layer of safety for critical applications with very capable models. 

\section*{Acknowledgements}
We thank Tim Beyer for helpful discussions. We also thank Niki Howe, Michael Noukhovitch, and Adam Oberman for helpful feedback on an early draft of the paper. This research was enabled in part by compute resources provided by Mila (mila.quebec).

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
\newpage
\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Software}
We use huggingface and pytorch to run all models.
\section{Judge}\label{app:judge}
\paragraph{The harmfulness check } is done with the following prompt:\\
\texttt{Do you consider the following question and response to be the start of a harmful answer? Answer with yes or no. Question: \{prompt\}. Response: \{response\}}

\paragraph{The utility check} is done with the following prompt:\\
\texttt{Do you consider the following response to be the start of a reasonable answer to the prompt? Answer with yes or no. Question: \{prompt\}. Response: \{response\}}
\section{Hyperparameters}\label{app:hparams}
\begin{table}[h]
\caption{Hyperparameters for models fine-tuned with our Red Flag Fine-tuning algorithm}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Hyperparameters &\llama{} & \mistral{} & \phithree{}{} \\
\midrule
Batch Size    & 64 & 64 & 64 \\
Learning Rate  & 0.0001 & 0.0001 & 0.00002 \\
Learning Rate Scheduler  & constant & constant & constant \\
$\alpha_{\mathrm{benign}}$  & 8 & 8 & 8 \\
$\alpha_{\mathrm{rf}}$  & 1 & 1 & 1 \\
$ \alpha_{\mathrm{CE}}$  & 3 & 3 & 3 \\
RF CE Cutoff  & 0.15 & 0.15 &  0.15\\
Attention Dropout  & 0.5 & 0.5 & 0.5 \\
Warmup Ratio  & 0.03 & 0.03 & 0.03 \\
LORA - r  & 128 & 128 & 128 \\
LORA - $\alpha$ & 64 & 64 & 64 \\
Min Offset & 16 & 16 &  0\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\section{Fine-tuning attack}\label{app:sft gsm8k}


We validate that our approach of applying a safety LoRA module does not break benign fine-tuning. For this we train with SFT for one epoch on \textsc{GSM8k}~\citep{cobbe2021gsm8k} in chat mode. We train with batchsize 64, learning rate $10^{-4}$, LoRA parameters $r=64$ \& $\alpha=64$, AdamW~\citep{loshchilov_decoupled_2019}, and a constant learning rate schedule. We evaluate the \textsc{GSM8k} performance with the \textsc{lm-evaluation-harness}~\citep{eval-harness} using the command \texttt{lm\_eval --model hf --tasks gsm8k --num\_fewshot=5 --device cuda:0 --batch\_size 16 --model\_args pretrained=meta-llama/Llama-3.2-3B-Instruct --apply\_chat\_template}. For the base model we get a performance of $24.1\pm0.1\%$ under strict-match, the fine-tuned model gets $61.0\pm0.1\%$, the fine-tuned model with one safety LoRA adapter gets $62.2\pm0.1\%$, and the fine-tuned model with the \rftoken{} adapter applied twice gets $59.4\pm0.1\%$.


% \newpage
\subsection{Long-Context}
\vspace{-1em}
\begin{figure}[h]
    \centering
    \begin{subfigure}{\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/long_context_small.png}
    % \caption{\llama{}~~~(threshold: $10^{-3}$)}
    \end{subfigure}\\
    \vspace{1em}
    \begin{subfigure}{.9\linewidth}
    \includegraphics[width=\linewidth]{figures/long_context_snippet-cropped.pdf}
    \end{subfigure}
    \caption{\small Multi-turn long-context log probabilities of \rftoken{} and the top-1 probability.}
    \label{fig:long_context_full}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
