\section{Related Work}
% \texttt{\langle rf \rangle}

\paragraph{Jailbreaking LLMs}
% \begin{itemize}
%     % \item Jailbreaking concept ____
%     % \item Red-teaming ____
%     % \item Optimization attacks ____
%     % \item Heuristic attacks ____
%     \item SFT attacks ____, Refusal ablation ____
% \end{itemize}

% hello \rf asdf 
Modern LLMs used as chatbots are trained to follow user instructions____ while also being trained to respond in a safe and harmless manner____.
While users quickly found ways to manually craft ``jailbreaks'' which could circumvent these safeguards and elicit harmful content from these systems____, automated methods for crafting adversarial attacks were also shown to be effective.
Particularly,____ propose a greedy-coordinate gradient (GCG) search algorithm to find an adversarial suffix optimized to pre-fill ____ an affirmative response in a model's response. 
Other approaches use heuristics to craft interpretable jailbreaks with only black-box access to the target model____.
Given white-box access to the target model, more powerful attacks are possible.
Adversarial soft prompts can be optimized to manipulate the modelâ€™s outputs ____, causal features responsible for refusal behaviour can be selectively ablated ____, and fine-tuning can be used to override or remove safety training entirely ____. 


\paragraph{Defences}
% \begin{itemize}
%     \item Fine-tuning / alignment training ____
%     \item Circuit breakers ____
%     \item Adversarial training (Cont/Latent + R2D2) ____
%     \item Classifiers/judge LLMs ____
% \end{itemize}

Beyond standard pre-training, LLMs are typically trained with preference optimization techniques such as RLHF ____ or DPO ____ to be more aligned with human preferences.
Jailbreaks can be incorporated into this preference alignment phase to increase resilience to such attacks (as is often done with red-teaming methods), but this does not often generalise to novel jailbreaks.
Historically, in the context of vision models, actively training against adversarial attacks in an online manner (i.e., adversarial training) is the only method that has shown increased adversarial robustness ____. However, in the context of language, most discrete attacks are prohibitively expensive to use online. 
____ train against adversarial suffixes generated by GCG, but continually update a pool of examples rather than generate each attack from scratch.
Other approaches perform adversarial training by attacking the embedding or latent space of the model____ which is much more efficient to compute and transfers to discrete attacks.
Beyond adversarial training, newer defences target and alter harmful representations in order to prevent a model from producing harmful outputs entirely ____.
Independent from training a model to be more robust to jailbreaks is to classify and judge the potential harmfulness of the generated text, often with another LLM fine-tuned for this task ____, although this does require additional resources to classify the outputs. Concurrent work ____ has shown that classifiers alone are often not sufficient, further making the case that other approaches are needed especially against permissive but common threat models such as the fine-tuning box attack.

% \todo{Interpretability - Maybe include this section? Representation engineering? feels like this could be an important section }

\paragraph{Special Tokens} 
Several works have explored training or utilising special tokens for specific purposes.
____ prepend ``memory'' tokens to an input prompt on a target task.
____ append ``pause'' tokens, which are hypothesised to give the LLM a buffer sequence to reason over before producing an output.
____ train LLMs to compress longer prompts into smaller sets of ``gist'' tokens as a means to shorten the context. 
____ prepend ``attention sinks'' to improve generalization to long-context sequences.
LLMs have also been trained to use a variety of tools (such as a calculator or internet access), which are denoted and invoked via special tokens ____.
Most closely related to our approach is the recent work of ____, where a model is trained to prefix an output with a special \emph{refusal} or \emph{response} token based on the behaviour of whether the model refuses or responds to a prompt.
While their approach is related in that special tokens are leveraged in the context of alignment, the approach and objective are conceptually different.
Their method correlates these tokens with \textit{behaviour} (i.e., refusal or response) in order to better calibrate such behaviours, whereas our approach correlates a special token with some implicit notion of a concept (i.e., harmfulness), \textit{without} modifying the model's original behaviour. This conceptual difference leads to drastically different losses in the formulation. For instance____ do not propose a KL divergence with a reference model~(\cref{eq:KL_after}) to maintain the predictions similar to the reference model after \rftoken{} is outputted which hurt the model's utility and is \emph{not} complementary with standard safety training (instead it is a way to calibrate the model post-hoc safety training). Moreover, their model is only trained to output a ``behavioural token" (e.g., ``refuse" or ``respond") at the beginning of the answer, which is significantly less efficient to detect harmfulness, as shown in our experiments. In contrast, our work proposes an approach that is complementary to standard safety training where the model essentially acts as an ``implicit judge'' on its own generated output, improving its transparency and providing a clear signal to evaluate potentially harmful generations without incurring any additional computational cost at inference time. Similar work by ____ also learns to tag answers as harmless or harmful, but they use two stage training procedure and hardcode the tag to be at the end of the response. They only consider fixed jailbreak prompts rather than attacks and do not consider the fine-tuning setting at all.