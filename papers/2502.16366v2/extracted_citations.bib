@misc{arditi_refusal_2024,
	title = {Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}},
	url = {http://arxiv.org/abs/2406.11717},
	doi = {10.48550/arXiv.2406.11717},
	abstract = {Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
	month = oct,
	year = {2024},
	note = {arXiv:2406.11717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/TA9QQRB6/Arditi et al. - 2024 - Refusal in Language Models Is Mediated by a Single Direction.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/5NYKQ59E/2406.html:text/html},
}

@ARTICLE{burtsev_memory_2020,
  title         = "Memory Transformer",
  author        = "Burtsev, Mikhail S and Kuratov, Yuri and Peganov, Anton and
                   Sapunov, Grigory V",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based models have achieved state-of-the-art
                   results in many natural language processing tasks. The
                   self-attention architecture allows transformer to combine
                   information from all elements of a sequence into
                   context-aware representations. However, information about the
                   context is stored mostly in the same element-wise
                   representations. This might limit the processing of
                   properties related to the sequence as a whole more difficult.
                   Adding trainable memory to selectively store local as well as
                   global representations of a sequence is a promising direction
                   to improve the Transformer model. Memory-augmented neural
                   networks (MANNs) extend traditional neural architectures with
                   general-purpose memory for representations. MANNs have
                   demonstrated the capability to learn simple algorithms like
                   Copy or Reverse and can be successfully trained via
                   backpropagation on diverse tasks from question answering to
                   language modeling outperforming RNNs and LSTMs of comparable
                   complexity. In this work, we propose and study few extensions
                   of the Transformer baseline (1) by adding memory tokens to
                   store non-local representations, (2) creating memory
                   bottleneck for the global information, (3) controlling memory
                   update with dedicated layer. We evaluate these memory
                   augmented Transformers and demonstrate that presence of
                   memory positively correlates with the model performance for
                   machine translation and language modelling tasks.
                   Augmentation of pre-trained masked language model with memory
                   tokens shows mixed results for tasks from GLUE benchmark.
                   Visualization of attention patterns over the memory suggest
                   that it improves the model's ability to process a global
                   context.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{chao_jailbreaking_2023,
  title         = "Jailbreaking black box large language models in twenty
                   queries",
  author        = "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and
                   Hassani, Hamed and Pappas, George J and Wong, Eric",
  journal       = "arXiv [cs.LG]",
  abstract      = "There is growing interest in ensuring that large language
                   models (LLMs) align with human values. However, the alignment
                   of such models is vulnerable to adversarial jailbreaks, which
                   coax LLMs into overriding their safety guardrails. The
                   identification of these vulnerabilities is therefore
                   instrumental in understanding inherent weaknesses and
                   preventing future misuse. To this end, we propose Prompt
                   Automatic Iterative Refinement (PAIR), an algorithm that
                   generates semantic jailbreaks with only black-box access to
                   an LLM. PAIR -- which is inspired by social engineering
                   attacks -- uses an attacker LLM to automatically generate
                   jailbreaks for a separate targeted LLM without human
                   intervention. In this way, the attacker LLM iteratively
                   queries the target LLM to update and refine a candidate
                   jailbreak. Empirically, PAIR often requires fewer than twenty
                   queries to produce a jailbreak, which is orders of magnitude
                   more efficient than existing algorithms. PAIR also achieves
                   competitive jailbreaking success rates and transferability on
                   open and closed-source LLMs, including GPT-3.5/4, Vicuna, and
                   PaLM-2.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{feuer_style_2024,
  title         = "Style outweighs substance: Failure modes of {LLM} judges in
                   alignment benchmarking",
  author        = "Feuer, Benjamin and Goldblum, Micah and Datta, Teresa and
                   Nambiar, Sanjana and Besaleli, Raz and Dooley, Samuel and
                   Cembalest, Max and Dickerson, John P",
  journal       = "arXiv [cs.LG]",
  abstract      = "The release of ChatGPT in November 2022 sparked an explosion
                   of interest in post-training and an avalanche of new
                   preference optimization (PO) methods. These methods claim
                   superior alignment by virtue of better correspondence with
                   human pairwise preferences, often measured by LLM-judges. In
                   this work, we attempt to answer the following question -- do
                   LLM-judge preferences translate to progress on other, more
                   concrete metrics for alignment, and if not, why not? We
                   define a concrete metric for alignment, and introduce
                   SOS-Bench (Substance Outweighs Style Benchmark), which is to
                   the best of our knowledge the largest standardized,
                   reproducible LLM meta-benchmark to date. We find that (1)
                   LLM-judge preferences do not correlate with concrete measures
                   of safety, world knowledge, and instruction following; (2)
                   LLM-judges have powerful implicit biases, prioritizing style
                   over factuality and safety; and (3) the supervised
                   fine-tuning (SFT) stage of post-training, and not the PO
                   stage, has the greatest impact on alignment, with data
                   scaling and prompt diversity as the driving factors. Our
                   codebase and complete results can be found at
                   https://github.com/penfever/sos-bench.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{goyal_think_2023,
  title         = "Think before you speak: Training Language Models With Pause
                   Tokens",
  author        = "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon,
                   Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models generate responses by producing a series of
                   tokens in immediate succession: the $(K+1)^{th}$ token is an
                   outcome of manipulating $K$ hidden vectors per layer, one
                   vector per preceding token. What if instead we were to let
                   the model manipulate say, $K+10$ hidden vectors, before it
                   outputs the $(K+1)^{th}$ token? We operationalize this idea
                   by performing training and inference on language models with
                   a (learnable) $\textit{pause}$ token, a sequence of which is
                   appended to the input prefix. We then delay extracting the
                   model's outputs until the last pause token is seen, thereby
                   allowing the model to process extra computation before
                   committing to an answer. We empirically evaluate
                   $\textit{pause-training}$ on decoder-only models of 1B and
                   130M parameters with causal pretraining on C4, and on
                   downstream tasks covering reasoning, question-answering,
                   general understanding and fact recall. Our main finding is
                   that inference-time delays show gains when the model is both
                   pre-trained and finetuned with delays. For the 1B model, we
                   witness gains on 8 of 9 tasks, most prominently, a gain of
                   $18\%$ EM score on the QA task of SQuAD, $8\%$ on
                   CommonSenseQA and $1\%$ accuracy on the reasoning task of
                   GSM8k. Our work raises a range of conceptual and practical
                   future research questions on making delayed next-token
                   prediction a widely applicable new paradigm.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@misc{huang_virus_2025,
	title = {Virus: {Harmful} {Fine}-tuning {Attack} for {Large} {Language} {Models} {Bypassing} {Guardrail} {Moderation}},
	shorttitle = {Virus},
	url = {http://arxiv.org/abs/2501.17433},
	doi = {10.48550/arXiv.2501.17433},
	abstract = {Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100{\textbackslash}\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: {\textbackslash}textbf\{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack\}, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
	month = jan,
	year = {2025},
	note = {arXiv:2501.17433 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/3V7M9KW5/Huang et al. - 2025 - Virus Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/WC8JXRAU/2501.html:text/html},
}

@misc{inan_llama_2023,
	title = {Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}},
	shorttitle = {Llama {Guard}},
	url = {http://arxiv.org/abs/2312.06674},
	doi = {10.48550/arXiv.2312.06674},
	abstract = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06674 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/2X3UJJHV/Inan et al. - 2023 - Llama Guard LLM-based Input-Output Safeguard for Human-AI Conversations.pdf:application/pdf},
}

@misc{jain_refusal_2024,
	title = {Refusal {Tokens}: {A} {Simple} {Way} to {Calibrate} {Refusals} in {Large} {Language} {Models}},
	shorttitle = {Refusal {Tokens}},
	url = {http://arxiv.org/abs/2412.06748},
	doi = {10.48550/arXiv.2412.06748},
	abstract = {A key component of building safe and reliable language models is enabling the models to appropriately refuse to follow certain instructions or answer certain questions. We may want models to output refusal messages for various categories of user queries, for example, ill-posed questions, instructions for committing illegal acts, or queries which require information past the model's knowledge horizon. Engineering models that refuse to answer such questions is complicated by the fact that an individual may want their model to exhibit varying levels of sensitivity for refusing queries of various categories, and different users may want different refusal rates. The current default approach involves training multiple models with varying proportions of refusal messages from each category to achieve the desired refusal rates, which is computationally expensive and may require training a new model to accommodate each user's desired preference over refusal rates. To address these challenges, we propose refusal tokens, one such token for each refusal category or a single refusal token, which are prepended to the model's responses during training. We then show how to increase or decrease the probability of generating the refusal token for each category during inference to steer the model's refusal behavior. Refusal tokens enable controlling a single model's refusal rates without the need of any further fine-tuning, but only by selectively intervening during generation.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Jain, Neel and Shrivastava, Aditya and Zhu, Chenyang and Liu, Daben and Samuel, Alfy and Panda, Ashwinee and Kumar, Anoop and Goldblum, Micah and Goldstein, Tom},
	month = dec,
	year = {2024},
	note = {arXiv:2412.06748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 19 pages},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/9QYEC4RX/Jain et al. - 2024 - Refusal Tokens A Simple Way to Calibrate Refusals in Large Language Models.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/J8NFSLKW/2412.html:text/html},
}

@ARTICLE{liu_autodan_2023,
  title         = "{AutoDAN}: Generating stealthy jailbreak prompts on aligned
                   Large Language Models",
  author        = "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
  journal       = "arXiv [cs.CL]",
  abstract      = "The aligned Large Language Models (LLMs) are powerful
                   language understanding and decision-making tools that are
                   created through extensive alignment with human feedback.
                   However, these large models remain susceptible to jailbreak
                   attacks, where adversaries manipulate prompts to elicit
                   malicious outputs that should not be given by aligned LLMs.
                   Investigating jailbreak prompts can lead us to delve into the
                   limitations of LLMs and further guide us to secure them.
                   Unfortunately, existing jailbreak techniques suffer from
                   either (1) scalability issues, where attacks heavily rely on
                   manual crafting of prompts, or (2) stealthiness problems, as
                   attacks depend on token-based algorithms to generate prompts
                   that are often semantically meaningless, making them
                   susceptible to detection through basic perplexity testing. In
                   light of these challenges, we intend to answer this question:
                   Can we develop an approach that can automatically generate
                   stealthy jailbreak prompts? In this paper, we introduce
                   AutoDAN, a novel jailbreak attack against aligned LLMs.
                   AutoDAN can automatically generate stealthy jailbreak prompts
                   by the carefully designed hierarchical genetic algorithm.
                   Extensive evaluations demonstrate that AutoDAN not only
                   automates the process while preserving semantic
                   meaningfulness, but also demonstrates superior attack
                   strength in cross-model transferability, and cross-sample
                   universality compared with the baseline. Moreover, we also
                   compare AutoDAN with perplexity-based defense methods and
                   show that AutoDAN can bypass them effectively.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{madry_towards_2017,
  title         = "Towards Deep Learning Models Resistant to Adversarial Attacks",
  author        = "Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig
                   and Tsipras, Dimitris and Vladu, Adrian",
  journal       = "arXiv [stat.ML]",
  abstract      = "Recent work has demonstrated that deep neural networks are
                   vulnerable to adversarial examples---inputs that are almost
                   indistinguishable from natural data and yet classified
                   incorrectly by the network. In fact, some of the latest
                   findings suggest that the existence of adversarial attacks
                   may be an inherent weakness of deep learning models. To
                   address this problem, we study the adversarial robustness of
                   neural networks through the lens of robust optimization. This
                   approach provides us with a broad and unifying view on much
                   of the prior work on this topic. Its principled nature also
                   enables us to identify methods for both training and
                   attacking neural networks that are reliable and, in a certain
                   sense, universal. In particular, they specify a concrete
                   security guarantee that would protect against any adversary.
                   These methods let us train networks with significantly
                   improved resistance to a wide range of adversarial attacks.
                   They also suggest the notion of security against a
                   first-order adversary as a natural and broad security
                   guarantee. We believe that robustness against such
                   well-defined classes of adversaries is an important stepping
                   stone towards fully resistant deep learning models. Code and
                   pre-trained models are available at
                   https://github.com/MadryLab/mnist\_challenge and
                   https://github.com/MadryLab/cifar10\_challenge.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}

@misc{mazeika_harmbench_2024,
	title = {{HarmBench}: {A} {Standardized} {Evaluation} {Framework} for {Automated} {Red} {Teaming} and {Robust} {Refusal}},
	shorttitle = {{HarmBench}},
	url = {http://arxiv.org/abs/2402.04249},
	abstract = {Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a largescale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github. com/centerforaisafety/HarmBench.},
	language = {en},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and Forsyth, David and Hendrycks, Dan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04249 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://www.harmbench.org},
	file = {Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for.pdf:/Users/sophiex/Zotero/storage/HF9X9GB6/Mazeika et al. - 2024 - HarmBench A Standardized Evaluation Framework for.pdf:application/pdf},
}

@ARTICLE{mu_learning_2023,
  title         = "Learning to compress prompts with gist tokens",
  author        = "Mu, Jesse and Li, Xiang Lisa and Goodman, Noah",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prompting is the primary way to utilize the multitask
                   capabilities of language models (LMs), but prompts occupy
                   valuable space in the input context window, and repeatedly
                   encoding the same prompt is computationally inefficient.
                   Finetuning and distillation methods allow for specialization
                   of LMs without prompting, but require retraining the model
                   for each task. To avoid this trade-off entirely, we present
                   gisting, which trains an LM to compress prompts into smaller
                   sets of ``gist'' tokens which can be cached and reused for
                   compute efficiency. Gist models can be trained with no
                   additional cost over standard instruction finetuning by
                   simply modifying Transformer attention masks to encourage
                   prompt compression. On decoder (LLaMA-7B) and encoder-decoder
                   (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of
                   prompts, resulting in up to 40\% FLOPs reductions, 4.2\% wall
                   time speedups, and storage savings, all with minimal loss in
                   output quality.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{ouyang_training_2022,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{perez_red_2022,
  title         = "Red teaming language Models with language Models",
  author        = "Perez, Ethan and Huang, Saffron and Song, Francis and Cai,
                   Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia
                   and McAleese, Nat and Irving, Geoffrey",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language Models (LMs) often cannot be deployed because of
                   their potential to harm users in hard-to-predict ways. Prior
                   work identifies harmful behaviors before deployment by using
                   human annotators to hand-write test cases. However, human
                   annotation is expensive, limiting the number and diversity of
                   test cases. In this work, we automatically find cases where a
                   target LM behaves in a harmful way, by generating test cases
                   (``red teaming'') using another LM. We evaluate the target
                   LM's replies to generated test questions using a classifier
                   trained to detect offensive content, uncovering tens of
                   thousands of offensive replies in a 280B parameter LM
                   chatbot. We explore several methods, from zero-shot
                   generation to reinforcement learning, for generating test
                   cases with varying levels of diversity and difficulty.
                   Furthermore, we use prompt engineering to control
                   LM-generated test cases to uncover a variety of other harms,
                   automatically finding groups of people that the chatbot
                   discusses in offensive ways, personal and hospital phone
                   numbers generated as the chatbot's own contact info, leakage
                   of private training data in generated text, and harms that
                   occur over the course of a conversation. Overall, LM-based
                   red teaming is one promising tool (among many needed) for
                   finding and fixing diverse, undesirable LM behaviors before
                   impacting users.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@misc{qi_fine-tuning_2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {http://arxiv.org/abs/2310.03693},
	doi = {10.48550/arXiv.2310.03693},
	abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03693 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/LA4YDTCV/Qi et al. - 2023 - Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/TCPYEVXT/2310.html:text/html},
}

@ARTICLE{rafailov_direct_2023,
  title         = "Direct Preference Optimization: Your language model is
                   secretly a reward model",
  author        = "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and
                   Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
  journal       = "arXiv [cs.LG]",
  abstract      = "While large-scale unsupervised language models (LMs) learn
                   broad world knowledge and some reasoning skills, achieving
                   precise control of their behavior is difficult due to the
                   completely unsupervised nature of their training. Existing
                   methods for gaining such steerability collect human labels of
                   the relative quality of model generations and fine-tune the
                   unsupervised LM to align with these preferences, often with
                   reinforcement learning from human feedback (RLHF). However,
                   RLHF is a complex and often unstable procedure, first fitting
                   a reward model that reflects the human preferences, and then
                   fine-tuning the large unsupervised LM using reinforcement
                   learning to maximize this estimated reward without drifting
                   too far from the original model. In this paper, we leverage a
                   mapping between reward functions and optimal policies to show
                   that this constrained reward maximization problem can be
                   optimized exactly with a single stage of policy training,
                   essentially solving a classification problem on the human
                   preference data. The resulting algorithm, which we call
                   Direct Preference Optimization (DPO), is stable, performant
                   and computationally lightweight, eliminating the need for
                   fitting a reward model, sampling from the LM during
                   fine-tuning, or performing significant hyperparameter tuning.
                   Our experiments show that DPO can fine-tune LMs to align with
                   human preferences as well as or better than existing methods.
                   Notably, fine-tuning with DPO exceeds RLHF's ability to
                   control sentiment of generations and improves response
                   quality in summarization and single-turn dialogue while being
                   substantially simpler to implement and train.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{schick_toolformer_2023,
  title         = "Toolformer: Language models can teach themselves to use tools",
  author        = "Schick, Timo and Dwivedi-Yu, Jane and Dess√¨, Roberto and
                   Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and
                   Cancedda, Nicola and Scialom, Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) exhibit remarkable abilities to solve
                   new tasks from just a few examples or textual instructions,
                   especially at scale. They also, paradoxically, struggle with
                   basic functionality, such as arithmetic or factual lookup,
                   where much simpler and smaller models excel. In this paper,
                   we show that LMs can teach themselves to use external tools
                   via simple APIs and achieve the best of both worlds. We
                   introduce Toolformer, a model trained to decide which APIs to
                   call, when to call them, what arguments to pass, and how to
                   best incorporate the results into future token prediction.
                   This is done in a self-supervised way, requiring nothing more
                   than a handful of demonstrations for each API. We incorporate
                   a range of tools, including a calculator, a Q\&A system, two
                   different search engines, a translation system, and a
                   calendar. Toolformer achieves substantially improved
                   zero-shot performance across a variety of downstream tasks,
                   often competitive with much larger models, without
                   sacrificing its core language modeling abilities.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{schwinn_soft_2024,
  title         = "Soft prompt threats: Attacking safety alignment and
                   unlearning in open-source {LLMs} through the embedding space",
  author        = "Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel,
                   Gauthier and Gunnemann, Stephan",
  journal       = "arXiv [cs.LG]",
  abstract      = "Current research in adversarial robustness of LLMs focuses on
                   discrete input manipulations in the natural language space,
                   which can be directly transferred to closed-source models.
                   However, this approach neglects the steady progression of
                   open-source models. As open-source models advance in
                   capability, ensuring their safety also becomes increasingly
                   imperative. Yet, attacks tailored to open-source LLMs that
                   exploit full model access remain largely unexplored. We
                   address this research gap and propose the embedding space
                   attack, which directly attacks the continuous embedding
                   representation of input tokens. We find that embedding space
                   attacks circumvent model alignments and trigger harmful
                   behaviors more efficiently than discrete attacks or model
                   fine-tuning. Furthermore, we present a novel threat model in
                   the context of unlearning and show that embedding space
                   attacks can extract supposedly deleted information from
                   unlearned LLMs across multiple datasets and models. Our
                   findings highlight embedding space attacks as an important
                   threat model in open-source LLMs. Trigger Warning: the
                   appendix contains LLM-generated text with violence and
                   harassment.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@misc{sheshadri_latent_2024,
	title = {Latent {Adversarial} {Training} {Improves} {Robustness} to {Persistent} {Harmful} {Behaviors} in {LLMs}},
	url = {http://arxiv.org/abs/2407.15549},
	doi = {10.48550/arXiv.2407.15549},
	abstract = {Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and Casper, Stephen},
	month = aug,
	year = {2024},
	note = {arXiv:2407.15549 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/B3T8925B/Sheshadri et al. - 2024 - Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/NU55YL3M/2407.html:text/html},
}

@ARTICLE{sheshadri_targeted_2024,
  title         = "Targeted latent adversarial training improves robustness to
                   persistent harmful behaviors in {LLMs}",
  author        = "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch,
                   Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and
                   Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell,
                   Dylan and Casper, Stephen",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models (LLMs) can often be made to behave in
                   undesirable ways that they are explicitly fine-tuned not to.
                   For example, the LLM red-teaming literature has produced a
                   wide variety of `jailbreaking' techniques to elicit harmful
                   text from models that were fine-tuned to be harmless. Recent
                   work on red-teaming, model editing, and interpretability
                   suggests that this challenge stems from how (adversarial)
                   fine-tuning largely serves to suppress rather than remove
                   undesirable capabilities from LLMs. Prior work has introduced
                   latent adversarial training (LAT) as a way to improve
                   robustness to broad classes of failures. These prior works
                   have considered untargeted latent space attacks where the
                   adversary perturbs latent activations to maximize loss on
                   examples of desirable behavior. Untargeted LAT can provide a
                   generic type of robustness but does not leverage information
                   about specific failure modes. Here, we experiment with
                   targeted LAT where the adversary seeks to minimize loss on a
                   specific competing task. We find that it can augment a wide
                   variety of state-of-the-art methods. First, we use targeted
                   LAT to improve robustness to jailbreaks, outperforming a
                   strong R2D2 baseline with orders of magnitude less compute.
                   Second, we use it to more effectively remove backdoors with
                   no knowledge of the trigger. Finally, we use it to more
                   effectively unlearn knowledge for specific undesirable tasks
                   in a way that is also more robust to re-learning. Overall,
                   our results suggest that targeted LAT can be an effective
                   tool for defending against harmful behaviors from LLMs.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{vega_bypassing_2023,
  title         = "Bypassing the safety training of open-source {LLMs} with
                   priming attacks",
  author        = "Vega, Jason and Chaudhary, Isha and Xu, Changming and Singh,
                   Gagandeep",
  journal       = "arXiv [cs.CR]",
  abstract      = "With the recent surge in popularity of LLMs has come an
                   ever-increasing need for LLM safety training. In this paper,
                   we investigate the fragility of SOTA open-source LLMs under
                   simple, optimization-free attacks we refer to as
                   $\textit{priming attacks}$, which are easy to execute and
                   effectively bypass alignment from safety training. Our
                   proposed attack improves the Attack Success Rate on Harmful
                   Behaviors, as measured by Llama Guard, by up to $3.3\times$
                   compared to baselines. Source code and data are available at
                   https://github.com/uiuc-focal-lab/llm-priming-attacks.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR"
}

@misc{wang_self-guard_2024,
	title = {Self-{Guard}: {Empower} the {LLM} to {Safeguard} {Itself}},
	shorttitle = {Self-{Guard}},
	url = {http://arxiv.org/abs/2310.15851},
	doi = {10.48550/arXiv.2310.15851},
	abstract = {The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experiment has demonstrated that Self-Guard is robust against jailbreak attacks. In the bad case analysis, we find that LLM occasionally provides harmless responses to harmful queries. Additionally, we evaluated the general capabilities of the LLM before and after safety training, providing evidence that Self-Guard does not result in the LLM's performance degradation. In sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM but also can even mitigate this issue.},
	urldate = {2025-02-21},
	publisher = {arXiv},
	author = {Wang, Zezhong and Yang, Fangkai and Wang, Lu and Zhao, Pu and Wang, Hongru and Chen, Liang and Lin, Qingwei and Wong, Kam-Fai},
	month = mar,
	year = {2024},
	note = {arXiv:2310.15851 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/BXABW4YT/Wang et al. - 2024 - Self-Guard Empower the LLM to Safeguard Itself.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/7IG4R848/2310.html:text/html},
}

@misc{wei_jailbroken_2023,
	title = {Jailbroken: {How} {Does} {LLM} {Safety} {Training} {Fail}?},
	shorttitle = {Jailbroken},
	url = {http://arxiv.org/abs/2307.02483},
	doi = {10.48550/arXiv.2307.02483},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02483 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/UXNKCRS2/Wei et al. - 2023 - Jailbroken How Does LLM Safety Training Fail.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/ULTQB988/2307.html:text/html},
}

@misc{xhonneux_efficient_2024,
	title = {Efficient {Adversarial} {Training} in {LLMs} with {Continuous} {Attacks}},
	url = {http://arxiv.org/abs/2405.15589},
	doi = {10.48550/arXiv.2405.15589},
	abstract = {Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Xhonneux, Sophie and Sordoni, Alessandro and G√ºnnemann, Stephan and Gidel, Gauthier and Schwinn, Leo},
	month = nov,
	year = {2024},
	note = {arXiv:2405.15589 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 4 figures},
	file = {Preprint PDF:/Users/sophiex/Zotero/storage/LBFWQCE9/Xhonneux et al. - 2024 - Efficient Adversarial Training in LLMs with Continuous Attacks.pdf:application/pdf;Snapshot:/Users/sophiex/Zotero/storage/LTZHY9LT/2405.html:text/html},
}

@ARTICLE{xiao_efficient_2023,
  title         = "Efficient streaming language models with attention sinks",
  author        = "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han,
                   Song and Lewis, Mike",
  journal       = "arXiv [cs.CL]",
  abstract      = "Deploying Large Language Models (LLMs) in streaming
                   applications such as multi-round dialogue, where long
                   interactions are expected, is urgently needed but poses two
                   major challenges. Firstly, during the decoding stage, caching
                   previous tokens' Key and Value states (KV) consumes extensive
                   memory. Secondly, popular LLMs cannot generalize to longer
                   texts than the training sequence length. Window attention,
                   where only the most recent KVs are cached, is a natural
                   approach -- but we show that it fails when the text length
                   surpasses the cache size. We observe an interesting
                   phenomenon, namely attention sink, that keeping the KV of
                   initial tokens will largely recover the performance of window
                   attention. In this paper, we first demonstrate that the
                   emergence of attention sink is due to the strong attention
                   scores towards initial tokens as a ``sink'' even if they are
                   not semantically important. Based on the above analysis, we
                   introduce StreamingLLM, an efficient framework that enables
                   LLMs trained with a finite length attention window to
                   generalize to infinite sequence lengths without any
                   fine-tuning. We show that StreamingLLM can enable Llama-2,
                   MPT, Falcon, and Pythia to perform stable and efficient
                   language modeling with up to 4 million tokens and more. In
                   addition, we discover that adding a placeholder token as a
                   dedicated attention sink during pre-training can further
                   improve streaming deployment. In streaming settings,
                   StreamingLLM outperforms the sliding window recomputation
                   baseline by up to 22.2x speedup. Code and datasets are
                   provided at https://github.com/mit-han-lab/streaming-llm.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{zeng_johnny_2024,
  title         = "How Johnny can persuade {LLMs} to jailbreak them: Rethinking
                   persuasion to challenge {AI} safety by humanizing {LLMs}",
  author        = "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi
                   and Jia, Ruoxi and Shi, Weiyan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Most traditional AI safety research has approached AI models
                   as machines and centered on algorithm-focused attacks
                   developed by security experts. As large language models
                   (LLMs) become increasingly common and competent, non-expert
                   users can also impose risks during daily interactions. This
                   paper introduces a new perspective to jailbreak LLMs as
                   human-like communicators, to explore this overlooked
                   intersection between everyday language interaction and AI
                   safety. Specifically, we study how to persuade LLMs to
                   jailbreak them. First, we propose a persuasion taxonomy
                   derived from decades of social science research. Then, we
                   apply the taxonomy to automatically generate interpretable
                   persuasive adversarial prompts (PAP) to jailbreak LLMs.
                   Results show that persuasion significantly increases the
                   jailbreak performance across all risk categories: PAP
                   consistently achieves an attack success rate of over $92\%$
                   on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials,
                   surpassing recent algorithm-focused attacks. On the defense
                   side, we explore various mechanisms against PAP and, found a
                   significant gap in existing defenses, and advocate for more
                   fundamental mitigation for highly interactive LLMs",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{zou_improving_2024,
  title         = "Improving alignment and robustness with short circuiting",
  author        = "Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek
                   and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan
                   and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan",
  journal       = "arXiv [cs.LG]",
  abstract      = "AI systems can take harmful actions and are highly vulnerable
                   to adversarial attacks. We present an approach, inspired by
                   recent advances in representation engineering, that
                   ``short-circuits'' models as they respond with harmful
                   outputs. Existing techniques aimed at improving alignment,
                   such as refusal training, are often bypassed. Techniques such
                   as adversarial training try to plug these holes by countering
                   specific attacks. As an alternative to refusal training and
                   adversarial training, short-circuiting directly controls the
                   representations that are responsible for harmful outputs in
                   the first place. Our technique can be applied to both
                   text-only and multimodal language models to prevent the
                   generation of harmful outputs without sacrificing utility --
                   even in the presence of powerful unseen attacks. Notably,
                   while adversarial robustness in standalone image recognition
                   remains an open challenge, short-circuiting allows the larger
                   multimodal system to reliably withstand image ``hijacks''
                   that aim to produce harmful content. Finally, we extend our
                   approach to AI agents, demonstrating considerable reductions
                   in the rate of harmful actions when they are under attack.
                   Our approach represents a significant step forward in the
                   development of reliable safeguards to harmful behavior and
                   adversarial attacks.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{zou_universal_2023,
  title         = "Universal and transferable adversarial attacks on aligned
                   language models",
  author        = "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson,
                   Matt",
  journal       = "arXiv [cs.CL]",
  abstract      = "Because ``out-of-the-box'' large language models are capable
                   of generating a great deal of objectionable content, recent
                   work has focused on aligning these models in an attempt to
                   prevent undesirable generation. While there has been some
                   success at circumventing these measures -- so-called
                   ``jailbreaks'' against LLMs -- these attacks have required
                   significant human ingenuity and are brittle in practice. In
                   this paper, we propose a simple and effective attack method
                   that causes aligned language models to generate objectionable
                   behaviors. Specifically, our approach finds a suffix that,
                   when attached to a wide range of queries for an LLM to
                   produce objectionable content, aims to maximize the
                   probability that the model produces an affirmative response
                   (rather than refusing to answer). However, instead of relying
                   on manual engineering, our approach automatically produces
                   these adversarial suffixes by a combination of greedy and
                   gradient-based search techniques, and also improves over past
                   automatic prompt generation methods. Surprisingly, we find
                   that the adversarial prompts generated by our approach are
                   quite transferable, including to black-box, publicly released
                   LLMs. Specifically, we train an adversarial attack suffix on
                   multiple prompts (i.e., queries asking for many different
                   types of objectionable content), as well as multiple models
                   (in our case, Vicuna-7B and 13B). When doing so, the
                   resulting attack suffix is able to induce objectionable
                   content in the public interfaces to ChatGPT, Bard, and
                   Claude, as well as open source LLMs such as LLaMA-2-Chat,
                   Pythia, Falcon, and others. In total, this work significantly
                   advances the state-of-the-art in adversarial attacks against
                   aligned language models, raising important questions about
                   how such systems can be prevented from producing
                   objectionable information. Code is available at
                   github.com/llm-attacks/llm-attacks.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

