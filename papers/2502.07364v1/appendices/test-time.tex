% \subsection{Variants}
% \label{sec:variants}

% We will quickly extend the theoretical results for \inline{DropEdge} to DropNode, DropAgg and DropGNN. Specifically, we will derive their expected propagation matrices, and study how its entries are affected. This will allow us to predict their effect on over-squashing, like we did in \autoref{sec:theory}.

% \textbf{DropNode} \citep{feng2020dropnode} independently samples nodes and sets their features to $0$, followed by rescaling to make the feature matrix unbiased. This is equivalent to setting the corresponding columns of the propagation matrix to $0$. Therefore, the expectation of the propagation matrix is simply 
% $$
%     \expectation_{\mathsf{DN}}\sb{\frac{1}{1-q}\propagation} = \frac{1}{1-q} \times \rb{1-q} \propagation = \propagation
% $$

% That is, the expected propagation matrix in a \inline{DropNode} model is the same as in a \inline{NoDrop} model, suggesting that it should not aggravate over-squashing levels (or at least not as much as \inline{DropEdge} does).

% \textbf{DropAgg} \citep{jiang2023dropagg} samples nodes that don't aggregate messages from their neighbors. This is equivalent to dropping the corresponding rows of the adjacency matrix. When $\propagation = \propagation^\asym$, the expected propagation matrix in a \inline{DropAgg} model is given by
% \begin{align*}
%     \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}} &= q + \frac{1-q}{d_i+1} = \frac{1+d_iq}{d_i+1} > \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ii}} \\
%     \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ij}} &= \frac{1}{d_i}\rb{1-\expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}}} < \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ij}}
% \end{align*}

% This suggests that \inline{DropAgg} not only aggravates the over-squashing problem, but does it to a greater extent than \inline{DropEdge}.

% \textbf{DropGNN} \citep{papp2021dropgnn} samples nodes which neither propagate nor aggregate messages in a given message-passing step. Here, again, we assume $\propagation = \propagation^\asym$. From any node's perspective, if it is not sampled, then its aggregation weights are computed as in the case of \inline{DropEdge}:
% \begin{align*}
%     \expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ii}} &= q + \rb{1-q} \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ii}} = q + \frac{1-q^{d_i+1}}{d_i+1} > \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}} \\
%     \expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ij}} &= \frac{1}{d_i}\rb{1-\expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ii}}} < \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ij}}
% \end{align*}

% Therefore, we may expect \inline{DropGNN} to be even more harmful than \inline{DropAgg} when it comes to capturing LRIs.

\section{Test-time Monte-Carlo Averaging}

In \autoref{sec:theory}, we focused on the expected sensitivity of the stochastic representations in models using \inline{DropEdge}-like regularization strategies. This corresponds to their training-time behavior, wherein the activations are random. At test-time, the standard practice is to turn these methods off by setting $q=0$. However, this raises the over-smoothing levels back up \cite{xuanyuan2023shedding}. Another way of making predictions is to perform multiple stochastic forward passes, as during training, and then averaging the model outputs. This is similar to Monte-Carlo Dropout, which is an efficient way of ensemble averaging in MLPs \cite{gal2016mcd}, CNNs \cite{gal2016bayesianconvolutionalneuralnetworks} and RNNs \cite{gal2016rnn}. In addition to alleviating over-smoothing, this approach also outperforms the standard implementation \cite{xuanyuan2023shedding}. We can study the effect of random edge-dropping in this setting by examining the sensitivity of the \textit{expected representations}:
$$
    % \norm{\partial \expectation_{\mathsf{DE}} \sb{\representation\layer{L}_i} / \partial \feature_j}_1
    \norm{\frac{\partial}{\partial \feature_j} \expectation \sb{\representation\layer{L}_i}}_1
$$

% This corresponds to test-time Monte-Carlo averaging over the model ensemble \citep{gal2016mcd,xuanyuan2023shedding}.
In linear models, the order of the two operations -- expectation and sensitivity computation -- is irrelevant:
\begin{align}
    \expectation \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1} 
    % = \expectation_{\mathsf{DE}} \sb{\propagation_{ij}} \norm{\weights}_1
    = \norm{\expectation \sb{\propagation_{ij}}\weights}_1
    = \norm{\expectation \sb{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}}_1
    = \norm{\frac{\partial}{\partial \feature_j} \expectation \sb{\representation\layer{L}_i}}_1
\end{align}

In general, the two quantities can be related using the convexity of norms and Jensen's inequality:
\begin{align}
    \norm{\frac{\partial}{\partial \feature_j} \expectation \sb{\representation\layer{L}_i}}_1
    \leq
    \expectation \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1}
\end{align}

Therefore, the discussion in the previous subsections extends to the MC-averaged representations as well. Although tighter bounds may be derived for this setting, we will leave that for future works.

% Therefore, the expected propagation matrix is given by
% \begin{align*}
%     \expectation\sb{\hat{\adjacency}_{ii}} &= \rb{1-q}\rb{\frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}}} + q\rb{\frac{1+qd_i}{d_i+1}} \\
%     \expectation\sb{\hat{\adjacency}_{ij}} &= \frac{1-q}{d_i+1} < \dot{\transition}_{ij}
% \end{align*}