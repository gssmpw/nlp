\section{Proofs}
\label{sec:proofs}

\newtheorem*{lemma*}{Lemma}
\newtheorem*{theorem*}{Theorem}

% In this section, we prove \autoref{thm:sensitivity-1-layer} and \autoref{thm:sensitivity-l-layer-dec}.

\subsection{Expected Propagation Matrix under DropEdge}
\label{sec:proofs-lemma}

\begin{lemma*}
    When using \inline{DropEdge}, the expected propagation matrix is given as:
    \begin{align*}
        \expectation_{\mathsf{DE}} \sb{\propagation\layer{1}_{ii}} &= \frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}} \\
        \expectation_{\mathsf{DE}} \sb{\propagation\layer{1}_{ij}} &= \frac{1}{d_i}\rb{1 - \frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}}}
    \end{align*}
    where $\rb{j\to i} \in \edges$; $\dot{\transition}_{ij} = 0$ otherwise.
\end{lemma*}

\begin{proof}
    Recall that under DropEdge, a self-loop is added to the graph \textit{after} the edges are dropped, and then the normalization is performed. In other words, the self-loop is never dropped. 
    Therefore, given the \iid masks, $m_1, \ldots, m_{d_i} \sim \Bernoulli\rb{1-q}$, on incoming edges to node $i$, the total number of messages is given by 
    $$
        1+\sum_{k=1}^{d_i} m_{k} = 1+M_i
    $$
    where $M_i \sim \Binomial\rb{d_i, 1-q}$. Under asymmetric normalization (see \autoref{sec:gnns}), the expected weight of the message along the self-loop is computed as follows:
    \begin{align}
        \expectation_{\mathsf{DE}} \sb{\propagation\layer{1}_{ii}} 
        &= \expectation_{m_1,\ldots,m_{d_i}} \sb{\frac{1}{1+\sum_{k=1}^{d_i} m_{k}}} \\
        &= \expectation_{M_i}\sb{\frac{1}{1+M_i}} \\
        &= \sum_{k=0}^{d_i} \binom{d_i}{k} \rb{1-q}^k \rb{q}^{d_i-k} \rb{\frac{1}{1+k}} \\
        &= \frac{1}{\rb{1-q}\rb{d_i+1}} \sum_{k=0}^{d_i} \binom{d_i+1}{k+1} \rb{1-q}^{k+1} \rb{q}^{d_i-k} \\
        &= \frac{1}{\rb{1-q}\rb{d_i+1}} \sum_{k=1}^{d_i+1} \binom{d_i+1}{k} \rb{1-q}^{k} \rb{q}^{d_i+1-k} \\
        &= \frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}}
    \end{align}
    
    Similarly, if the Bernoulli mask corresponding to $j\to i$ is $1$, then the total number of incoming messages to node $i$ is given by 
    $$
        2+\sum_{k=1}^{d_i-1} m_k
    $$
    including one self-loop, which is never dropped, as noted earlier. On the other hand, the weight of the edge is simply $0$ if the corresponding Bernoulli mask is $0$. Using the Law of Total Expectation, the expected weight of the edge $j\to i$ can be computed as follows:
    \begin{align}
        \expectation_{\mathsf{DE}} \sb{\propagation\layer{1}_{ij}} 
        &= q\cdot 0 + \rb{1-q} \expectation_{m_1,\ldots,m_{d_i-1}} \sb{\frac{1}{2+\sum_{k=1}^{d_i-1} m_{k}}} \\
        &= \rb{1-q} \sum_{k=0}^{d_i-1} \binom{d_i-1}{k} \rb{1-q}^k \rb{q}^{d_i-1-k} \rb{\frac{1}{2+k}} \\
        &= \sum_{k=0}^{d_i-1} \frac{\factorial{d_i-1}}{\factorial{k+2}\factorial{d_i-1-k}} \rb{1-q}^{k+1} \rb{q}^{d_i-1-k} \rb{k+1} \\
        &= \sum_{k=2}^{d_i+1} \frac{\factorial{d_i-1}}{\factorial{k}\factorial{d_i+1-k}} \rb{1-q}^{k-1} \rb{q}^{d_i+1-k} \rb{k-1} \\
        &= \frac{1}{d_i\rb{d_i+1}\rb{1-q}} \sum_{k=2}^{d_i+1} \binom{d_i+1}{k} \rb{1-q}^{k} \rb{q}^{d_i+1-k} \rb{k-1} \\
        &= \frac{1}{d_i\rb{d_i+1}\rb{1-q}} \sb{\rb{d_i+1}\rb{1-q} - 1 + q^{d_i+1}} \\
        &= \frac{1}{d_i} \rb{1-\expectation_{\mathsf{DE}} \sb{\propagation\layer{1}_{ii}}} 
    \end{align}
\end{proof}

\subsection{Sensitivity in L-layer Linear GCNs}
\label{sec:proofs-thm}

\begin{theorem*}
    In an L-layer linear \inline{GCN} with $\propagation = \propagation^\asym$, using \inline{DropEdge}, DropAgg or DropGNN decreases the sensitivity of a node $i\in\vertices$ to another node $j\in \hopnbr^{\rb{L}}\rb{i}$, thereby reducing its effective receptive field. 
    \begin{align}
        \expectation_{\ldots} \sb{\rb{\propagation^L}_{ij}} = \sum_{\rb{u_0,\ldots,u_L}\in\paths\rb{j\to i}} \prod_{\ell=1}^L \expectation_{\ldots} \sb{\propagation_{u_{\ell}u_{\ell-1}}} < \expectation_{\mathsf{ND}} \sb{\rb{\propagation^L}_{ij}}
    \end{align}
    where $\mathsf{ND}$ refers to a NoDrop model ($q=0$), the placeholder $\cdots$ can be replaced with one of the edge-dropping methods $\mathsf{DE}$, $\mathsf{DA}$ or $\mathsf{DG}$, and the corresponding entries of $\expectation_{\ldots} [\propagation]$ can be plugged in from \autoref{eqn:dropedge}, \autoref{eqn:dropagg} and \autoref{eqn:dropgnn}, respectively. Moreover, the sensitivity monotonically decreases as the dropping probability is increased.
\end{theorem*}

\begin{proof}
    Recall that $\dot{\transition}$ can be viewed as the transition matrix of a non-uniform random walk, such that $\dot{\transition}_{uv} = \mathbb{P}\rb{u\to v}$. Intuitively, since there is no self-loop on any given L-length path connecting nodes $i$ and $j$ (which are assumed to be L-hops away), the probability of each transition on any path connecting these nodes is reduced. Therefore, so is the total probability of transitioning from $i$ to $j$ in exactly L hops.
    
    More formally, denote the set of paths connecting the two nodes by
    $$
        \paths\rb{j\to i} = \cb{\rb{u_0,\ldots,u_L}: u_0 = j; u_L = i; \rb{u_{\ell-1} \to u_{\ell}}\in\edges, \forall \ell\in\sb{L}}
    $$
    
    The $\rb{i,j}$-entry in the propagation matrix is given by
    \begin{align}
        \rb{\dot{\transition}^L}_{ij} &= \sum_{\rb{u_0,\ldots,u_L}\in\paths\rb{j\to i}} \prod_{\ell=1}^L \dot{\transition}_{u_{\ell}u_{\ell-1}}
    \label{eqn:total-transition-prob}
    \end{align}
    
    Since there is no self-loop on any of these paths,
    \begin{align}
        \rb{\dot{\transition}^L}_{ij} &= \sum_{\rb{u_0,\ldots,u_L}\in\paths\rb{j\to i}} \prod_{\ell=1}^L \frac{1}{d_{u_{\ell}}}\rb{1-\frac{1-q^{d_{u_{\ell}}+1}}{\rb{1-q}\rb{d_{u_{\ell}}+1}}} \\
        &< \sum_{\rb{u_0,\ldots,u_L}\in\paths\rb{j\to i}} \prod_{\ell=1}^L \rb{\frac{1}{d_{u_{\ell}}+1}} \label{eqn:temp}
    \end{align}
    
    The right hand side of the inequality is the $\rb{i,j}$-entry in the L\textsuperscript{th} power of the propagation matrix of a \inline{NoDrop} model. From \autoref{eqn:dropagg} and \autoref{eqn:dropgnn}, we know that \autoref{eqn:temp} is true for DropAgg and DropGNN as well. We conclude the first part of the proof using \autoref{thm:sensitivity-l-layer} -- the sensitivity of node $i$ to node $j$ is proportional to $(\dot{\transition}^L)_{ij}$. 
    
    Next, we recall the geometric series for any $q$:
    \begin{align}
        1+q+\ldots+q^d = \frac{1-q^{d+1}}{1-q}
    \end{align}
    Each of the terms on the right are increasing in $q$, hence, all the $\dot{\transition}_{u_{\ell}u_{\ell-1}}$ factors are decreasing in $q$. Using this result with \autoref{eqn:total-transition-prob}, we conclude the second part of the theorem.
    
\end{proof}