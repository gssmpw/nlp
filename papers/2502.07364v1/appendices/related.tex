\section{Related Works}
\label{sec:related}

\subsection{Methods for Alleviating Over-smoothing}
% \textbf{Graph-rewiring Methods}.
% \textbf{Other Methods}.

A popular choice for reducing over-smoothing in GNNs is to regularize the model. Recall that DropEdge \citep{rong2020dropedge} implicitly regularizes the model by adding noise to the learning trajectory (\autoref{sec:drop-edge}). Similarly, Dropout \cite{srivastava2014dropout} drops entries from the input features to each layer, and DropMessage \citep{Fang2022DropMessageUR} drops entries from the message matrix (before the aggregation step). Graph Drop Connect (GDC) \citep{hasanzadeh2020bayesian} combines DropEdge and DropMessage together, resulting in a layer-wise sampling scheme that uses a different subgraph for message-aggregation over each feature dimension. These methods successfully addressed the over-smoothing problem, enabling the training of deep GNNs, and performed competitively on several benchmarking datasets.

Another powerful form of implicit regularization is feature normalization, which has proven crucial in enhancing the performance and stability of several types of neural networks \citep{huang2020normalizationtechniquestrainingdnns}. Exploiting the inductive bias in graph-structured data, normalization techniques like PairNorm \citep{Zhao2020PairNorm}, Differentiable Group Normalization (DGN) \citep{zhou2020towards} and NodeNorm \citep{zhou2021understanding} have been proposed to reduce over-smoothing in GNNs. On the other hand, Energetic Graph Neural Networks (EGNNs) \cite{zhou2021dirichlet} explicitly regularize the optimization by constraining the layer-wise Dirichlet energy to a predefined range.

In a different vein, motivated by the success of residual networks (ResNets) \citep{he_2016_residual} in computer vision, \cite{li2019point-clouds} proposed the use of residual connections to prevent the smoothing of representations. Residual connections successfully improved the performance of GCN on a range of graph-learning tasks. \cite{chen20vsimpledeep} introduced GCNII, which uses skip connections from the input to all hidden layers. This layer wise propagation rule has allowed for training of ultra-deep networks -- up to 64 layers. Some other architectures, like the Jumping Knowledge Network (JKNet) \cite{xu2018jknet} and the Deep Adaptive GNN (DAGNN) \cite{liu2020towards}, aggregate the representations from \textit{all} layers, $\{\representation\layer{\ell}_i\}_{\ell=1}^L$, before processing them through a readout layer.

\subsection{Methods for Alleviating Over-squashing}

In this section, we will review some of the graph rewiring methods proposed to address the problem of over-squashing. Particularly, we wish to emphasize a commonality among these methods -- edge addition is necessary. %, sometimes along with edge removal to preserve certain statistical properties of the graphs. 
As a reminder, \textit{graph rewiring} refers to modifying the edge set of a graph by adding and/or removing edges in a systematic manner. In a special case, which includes many of the rewiring techniques we will discuss, the original topology is completely discarded, and only the rewired graph is used for message-passing.

\textit{Spatial} rewiring methods use the topological relationships between the nodes in order to come up with a rewiring strategy. That is the graph rewiring is guided by the objective of optimizing some chosen topological properties. For instance, \citet{alon2021on} introduced a fully-adjacent (FA) layer, wherein messages are passed between all nodes. GNNs using a FA layer in the final message-passing step were shown to outperform the baselines on a variety of long-range tasks, revealing the importance of information exchange between far-off nodes which standard message-passing cannot facilitate. \citet{topping2022understanding} proposed a curvature-based rewiring strategy, called the Stochastic Discrete Ricci Flow (SDRF), which aims to reduce the ``bottleneckedness'' of a graph by adding suitable edges, while simultaneously removing edges in an effort to preserve the statistical properties of the original topology. \citet{black2023resistance} proposed the Greedy Total Resistance (GTR) technique, which optimizes the graph's total resistance by greedily adding edges to achieve the greatest improvement. One concern with graph rewiring methods is that unmoderated densification of the graph, \eg using a fully connected graph for propagating messages, can result in a loss of the inductive bias the topology provides, potentially leading to over-fitting. Accordingly, \citet{gutteridge2023drew} propose a Dynamically Rewired (DRew) message-passing framework that gradually \textit{densifies} the graph. Specifically, in a given layer $\ell$, node $i$ aggregates messages from its entire $\ell$-hop receptive field instead of just the immediate neighbors. This results in an improved communication over long distances while also retaining the inductive bias of the shortest distance between nodes.

\textit{Spectral} methods, on the other hand, use the spectral properties of the matrices encoding the graph topology, \eg the adjacency or the Laplacian matrix, to design rewiring algorithms. For example, \citet{rodriguez2022diffwire} proposed a differentiable graph rewiring layer based on the Lov\a'asz bound \cite[Corollary 3.3]{lovasz1993random}. Similarly, \citet{banerjee2022expansion} introduced the Random Local Edge Flip (RLEF) algorithm, which draws inspiration from the ``Flip Markov Chain'' \cite{feder2006markovflip,mahlmann2005ksplitter} -- a sequence of such steps can convert a connected graph into an \textit{expander graph} -- a sparse graph with good connectivity (in terms of Cheeger's constant) --  with high probability \cite{allen2016expanders,giakkoupis2022expanders,cooper2019flipmarkov,feder2006markovflip,mahlmann2005ksplitter}, thereby enabling effective information propagation across the graph.

Some other rewiring techniques don't exactly classify as spatial or spectral methods. For instance, Probabilistically Rewired MPNN (PR-MPNN) \cite{qian2024prmpnn} learns to probabilistically rewire a graph, effectively mitigating under-reaching as well as over-squashing. Finally, \cite{br√ºelgabrielsson2023positional} proposed connecting all nodes at most $r$-hops away, for some $r\in\natural$, and introducing positional embeddings to allow for distance-aware aggregation of messages.

\subsection{Towards a Unified Treatment}

Several studies have shown that an inevitable trade-off exists between the problems of over-smoothing and over-squashing, meaning that optimizing for one will compromise the other. For instance, \citet{topping2022understanding,nguyen2023revisiting} showed that negatively curved edges create bottlenecks in the graph resulting in over-squashing of information. On the other hand, \citet[Proposition 4.3]{nguyen2023revisiting} showed that positively curved edges in a graph contribute towards the over-smoothing problem. To address this trade-off, they proposed Batch Ollivier-Ricci Flow (BORF), which adds new edges adjacent to the negatively curved ones, and simultaneously removes positively curved ones.
In a similar vein, \citet{giraldo2023trading} demonstrated that the minimum number of message-passing steps required to reach a given level of over-smoothing is inversely related to the Cheeger's constant, $h_{\graph}$. This again implies an inverse relationship between over-smoothing and over-squashing. 
To effectively alleviate the two issues together, they proposed the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which adds edges that result in high improvement in the curvature of existing edges, while simultaneously removing those that have low curvature.

Despite the well-established trade-off between over-smoothing and over-squashing, some works have successfully tackled them together despite \textit{only} adding or removing edges. One such work is \citet{karhadkar2023fosr}, which proposed a rewiring algorithm that adds edges to the graph but does not remove any. The First-order Spectral Rewiring (FoSR) algorithm computes, as the name suggests, a first order approximation to the spectral gap of $\laplacian^{\sym}$, and adds edges with the aim of maximizing it. Since the spectral gap directly relates to Cheeger's constant through Cheeger's inequality \citep{alon85,alon86,sinclair89}, this directly decreases the over-squashing levels. Moreover, \citet[Figure 5]{karhadkar2023fosr} empirically demonstrated that addition of (up to a small number of) edges selected by FoSR can lower the Dirichlet energy of the representations. Taking a completely opposite approach, CurvDrop \citep{liu2023curvdrop} adapted DropEdge to remove negatively curved edges sampled from a distribution proportional to their curvatures. CurvDrop directly reduces over-squashing and, as a side benefit of operating on a sparser subgraph, also mitigates over-smoothing.