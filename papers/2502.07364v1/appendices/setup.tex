\section{Experiment Details}
\label{sec:exp-setup}

In this section, we expand on the details of the experiment in \autoref{sec:exp}.

\textbf{Descriptions of the Datasets}. Cora \cite{mccallum2000cora} and CiteSeer \cite{giles1998citeseer} are citation networks -- their nodes represent scientific publications and an edge between two nodes indicates that one of them has cited the other. The features of each publication are represented by a binary vector, where each index indicates whether a specific word from a dictionary is present or absent. Several studies have showed that these datasets have high homophily in node labels \cite{lim2021new,zhu2020heterophily} and that they are modelled much better by shallower networks than by deeper ones \cite{zhou2020towards}. Chameleon and Squirrel \cite{musae} are networks of English Wikipedia web pages on the respective topics, and the edges between web pages indicate links between them. The task is to predict the average-monthly traffic on each of the web pages. Finally, TwitchDE \cite{musae} is a network of Twitch users in Germany, with the edges between them representing their mutual follower relationships. The node features are embeddings of the games played by the users. The task is to predict whether the users use explicit language.

\textbf{Training Hyperparameters}. We standardise most of the hyperparameters across all experiments in order to isolate the effect of dropping probability. Specifically, we fix the size of the hidden representations in each layer at 64, and a linear readout layer is used to compute the node-level logits. The models are trained using the Adam optimizer \cite{KingBa15}, with a learning rate of $3 \times 10^{-3}$ and a weight decay of $5 \times 10^{-4}$, for a total of 300 epochs. For GCN, we use symmetric normalization of the adjacency matrix to compute the edge weights \cite{kipf2017gcn}.

\textbf{GAT Runs}. It is quite problematic to train deep \inline{GAT} models due to vanishing gradients \citep{dasoulas20121grad}. Accordingly, we discard the runs where the model fails to learn, and performs just as well as a random classifier. Specifically, we compute the class distribution in each of the networks, and discard the runs where the test performance does not exceed the maximum proportion. This comes out to be $0.3021$, $0.2107$, $0.2288$, $0.2003$ and $0.6045$ for Cora, CiteSeer, Chameleon, Squirrel and TwitchDE, respectively. 