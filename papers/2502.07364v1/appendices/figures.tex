\section{Additional Figures}

In this section, we present some additional figures that demonstrate the negative effects of random edge-dropping, particularly focusing on providing empirical evidence for scenarios not covered by the theory in \autoref{sec:theory}. Additionally, we provide visual evidence of the negative effects of DropNode, despite the fact that it preserves sensitivity between nodes (in expectation).

\subsection{Symmetrically Normalized Propagation Matrix}
\label{sec:fig-sym-norm}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/linear-gcn_symmetric_Proteins.png}
    % \includegraphics[width=0.48\linewidth]{assets/linear-gcn_symmetric_MUTAG.png}
    \caption{Entries of $\ddot{\transition}^6$, averaged after binning node-pairs by their shortest distance.}
    \label{fig:linear-gcn_symmetric}
\end{figure}

The results in \autoref{sec:linear} correspond to the use of $\hat{\adjacency} = \propagation^\asym$ for aggregating messages -- in each message passing step, only the in-degree of node $i$ is used to compute the aggregation weights of the incoming messages. In practice, however, it is more common to use the symmetrically normalized propagation matrix, $\propagation = \propagation^\sym$, which ensures that nodes with high degree do not dominate the information flow in the graph \cite{kipf2017gcn}. As in \autoref{thm:sensitivity-l-layer}, we are looking for 
$$
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\prod_{l=1}^L \propagation\layer{\ell}} = \ddot{\transition}^L
$$
where $\ddot{\transition} \coloneqq \expectation_{\mathsf{DE}} [\propagation^{\sym}]$. While $\ddot{\transition}$ is analytically intractable, we can approximate it using Monte-Carlo sampling. Accordingly, we use 20 samples of $\mask$ to compute an approximation of $\ddot{\transition}$, and plot out the entries of $\ddot{\transition}^L$, as we did for $\dot{\transition}^L$ in \autoref{fig:linear-gcn_asymmetric}. The results are presented in \autoref{fig:linear-gcn_symmetric}, which shows that while the sensitivity between nearby nodes is affected to a lesser extent compared to those observed in \autoref{fig:linear-gcn_asymmetric}, that between far-off nodes is significantly reduced, same as earlier.

\subsection{Upper Bound on Expected Sensitivity}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/linear-gcn_black-extension_Proteins.png}
    % \includegraphics[width=0.48\linewidth]{assets/linear-gcn_black-extension_MUTAG.png}
    \caption{Entries of $\sum_{l=0}^6 \dot{\transition}^l$, averaged after binning node-pairs by their shortest distance.}
    \label{fig:black_extension}
\end{figure}

\citet{black2023resistance} showed that the sensitivity between any two nodes in a graph can be bounded using the sum of the powers of the propagation matrix. In \autoref{sec:nonlinear}, we extended this bound to random edge-dropping methods with independent edge masks smapled in each layer:

\begin{align*}
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1}
    =
    \zeta_3^{\rb{L}} \rb{\sum_{\ell=0}^L \expectation \sb{\propagation}^{\ell}}_{ij}
\end{align*}

Although this bound does not have a closed form, we can use real-world graphs to study its entries. We randomly sample 100 molecular graphs from the Proteins dataset \cite{dobson2003proteins} and plot the entries of $\sum_{l=0}^6 \dot{\transition}^\ell$ (corresponding to \inline{DropEdge}) against the shortest distance between node-pairs. The results are presented in \autoref{fig:black_extension}. We observe a polynomial decline in sensitivity as the \inline{DropEdge} probability increases, suggesting that it is unsuitable for capturing LRIs.

\subsection{Test Accuracy versus DropNode Probability}

\begin{figure}[t]
    \centering
    \subcaptionbox{Homophilic datasets}{\includegraphics[width=0.7\linewidth]{assets/DropNode_homophilic.png}} \\
    \vspace{2mm}
    \subcaptionbox{Heterophilic datasets}{\includegraphics[width=\linewidth]{assets/DropNode_heterophilic.png}}
    \caption{Dropping probability versus test accuracy of DropNode-GCN.}
    \label{fig:dropnode}
\end{figure}

In \autoref{eqn:dropnode}, we noted that the expectation of sensitivity remains unchanged when using DropNode. However, these results were only in expectation. In practice, a high DropNode probability will result in poor communication between distant nodes, preventing the model from learning to effectively model LRIs. This is supported by the results in \autoref{tab:correlation}, where we observed a negative correlation between the test accuracy and DropNode probability. Moreover, DropNode was the only algorithm which recorded negative correlations on homophilic datasets. In \autoref{fig:dropnode}, we visualize these relationships, noting the stark contrast with \autoref{fig:acc-trends}, particularly in the trends with homophilic datasets.