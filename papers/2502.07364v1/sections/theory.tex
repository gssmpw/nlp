\section{Sensitivity Analysis}
\label{sec:theory}

In this section, we perform a theoretical analysis of the expectation -- \wrt random edge masks -- of sensitivity of node representations. This will allow us to predict how \inline{DropEdge}-variants affect communication between nodes at various distances, which is relevant for predicting their suitability towards learning LRIs.

\subsection{Linear GCNs}
\label{sec:linear}

We start our analysis with linear \inline{GCNs}, and treat more general \inline{MPNN} architectures in the following subsection. % The layer-wise propagation rule of this model is given as:
% \begin{align}
% \begin{split}
%     \mask\layer{\ell} 
%     &\simtext{iid} \cb{\text{Bern}\rb{1-q}}^{N\times N} \\
%     \widetilde{\adjacency}\layer{\ell} 
%     &= \mask\layer{\ell}\circ \adjacency + \identity_N \\
%     \widetilde{\degree}\layer{\ell} 
%     &= \tdiag{\widetilde{\adjacency}\layer{\ell}\ones_{N}} \\
%     \propagation\layer{\ell} 
%     &= \rb{\widetilde{\degree}\layer{\ell}}\inv \widetilde{\adjacency}\layer{\ell} \\
%     \representations\layer{\ell} 
%     &= \propagation\layer{\ell} \representations\layer{\ell-1} \weights\layer{\ell}
% \end{split} \label{eqn:asym-gcn-l-layers}
% \end{align}
% Accordingly, 
In this model, the final node representations can be summarised as
\begin{align}
    \representations\layer{L} = \rb{\prod_{\ell=1}^L \propagation\layer{\ell}} \features \weights \in \real^{N\times \hidden\layer{L}}
\end{align}

where % $\propagation \coloneqq \prod_{\ell=1}^L \propagation\layer{\ell} \in \real^{N\times N}$ and 
$\weights \coloneqq \prod_{\ell=1}^L \weights\layer{\ell} \in \real^{\hidden\layer{0} \times \hidden\layer{L}}$. Using the \iid assumption on the distribution of edge masks in each layer, the expected sensitivity of node $i$ to node $j$ can be shown to be
\begin{align}
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1} = \rb{\expectation\sb{\propagation}^L}_{ij} \norm{\weights}_1 
\label{thm:sensitivity-l-layer}
\end{align}

% -- it can be thought of as the transition matrix of a \inline{DropEdge} walk, \ie a random walk wherein edges are randomly dropped at each step. 
To keep things simple, we will ignore the effect of DropEdge-variants on the optimization trajectory. Accordingly, it is sufficient to study $\expectation[\propagation]$ in order to predict their effect on over-squashing. To maintain analytical tractability, we assume the use of an asymmetrically normalized adjacency matrix for message-passing, $\propagation = \propagation^{\asym}$. \\

\begin{lemma}
\label{thm:exp-prop-matrix}
    The expected propagation matrix under \inline{DropEdge} is given as:
    \begin{align}
    \begin{split}
        \dot{\transition}_{ii} &\coloneqq \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ii}} = \frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}} \\
        \dot{\transition}_{ij} &\coloneqq \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ij}} = \frac{1}{d_i}\rb{1 - \frac{1-q^{d_i+1}}{\rb{1-q}\rb{d_i+1}}}
    \end{split} \label{eqn:dropedge}
    \end{align}
    where $q\in[0, 1)$ is the dropping probability. % and $\dot{\transition} \coloneqq \expectation_{\mathsf{DE}} [\propagation^\asym]$ is the expected propagation matrix in a DropEdge model.
    % where $\rb{j\to i} \in \edges$; $\dot{\transition}_{ij} = 0$ otherwise.
    % where $m_k \simtext{iid} \Bernoulli\rb{1-q}$.
\end{lemma}

% Fix the fraction, $c\in\rb{0,1}$, of information lost over any edge. Then the dropping probability for the edge $j\to i$ can be given by:
% \begin{flalign*}
%     \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ij}} = c\cdot\expectation_{\mathsf{ND}}\sb{\hat{\adjacency}_{ij}} 
%     % \Longrightarrow\ \frac{1}{d_i}\rb{1 - \frac{1-q_i^{d_i+1}}{\rb{1-q_i}\rb{d_i+1}}} = c\cdot\frac{1}{d_i+1} 
%     \Longrightarrow\ d_i\rb{1-c}\rb{1-q_i} - q_i + q_i^{d_i+1} = 0
% \end{flalign*}

See \autoref{sec:proofs-lemma} for a proof. 
% \subsection{Variants}
% \label{sec:variants}

\textbf{Other Variants}. We will similarly derive the expected propagation matrix for other random edge-dropping algorithms. First off, DropNode \citep{feng2020dropnode} samples nodes and drops corresponding columns from the aggregation matrix directly, followed by rescaling of its entries: 
\begin{align}
\begin{split}
    \expectation_{\mathsf{DN}}\sb{\frac{1}{1-q}\propagation} = \frac{1}{1-q} \times \rb{1-q} \propagation = \propagation
\end{split} \label{eqn:dropnode}
\end{align}

That is, the expected propagation matrix is the same as in a \inline{NoDrop} model ($q=0$). 

Nodes sampled by DropAgg \cite{jiang2023dropagg} don't aggregate messages. Therefore, if $\propagation = \propagation^\asym$, then the expected propagation matrix is given by
\begin{align}
\begin{split}
    \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}} &= q + \frac{1-q}{d_i+1} = \frac{1+d_iq}{d_i+1} > \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ii}} \\
    \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ij}} &= \frac{1}{d_i}\rb{1-\expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}}} < \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ij}}
\end{split}
\label{eqn:dropagg}
\end{align}

Finally, DropGNN \citep{papp2021dropgnn} samples nodes which neither propagate nor aggregate messages. From any node's perspective, if it is not sampled, then its aggregation weights are computed as for \inline{DropEdge}:
\begin{align}
\begin{split}
    \expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ii}} &= q + \rb{1-q} \expectation_{\mathsf{DE}}\sb{\hat{\adjacency}_{ii}} = q + \frac{1-q^{d_i+1}}{d_i+1} > \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ii}} \\
    \expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ij}} &= \frac{1}{d_i}\rb{1-\expectation_{\mathsf{DG}}\sb{\hat{\adjacency}_{ii}}} < \expectation_{\mathsf{DA}}\sb{\hat{\adjacency}_{ij}}
\end{split}
\label{eqn:dropgnn}
\end{align}

% As a sanity check, consider the limit $q \to 1$, in which every \inline{DropEdge}-variant \emph{almost surely} disconnects the entire graph. In this case $\dot{\transition} \to \identity_N$, which implies the following: 1. $(\dot{\transition}^L)_{ij} \to 0 \Longrightarrow \|\partial \representation\layer{L}_i / \partial \feature_j\|_1 \to 0$ for $i\neq j$, \ie the sensitivity of a node's representation to all other nodes' input features vanishes, and 2. $(\dot{\transition}^L)_{ii} \to 1 \neq 
% 0$, \ie the representation is sensitive only to the node's features.

\textbf{1-Layer Linear GCNs}. $\forall q\in\rb{0,1}$ we have
\begin{alignat}{3}
\begin{split}
    \dot{\transition}_{ii} &= \frac{1}{d_i+1} \sum_{k=0}^{d_i} q^k > \frac{1}{d_i+1} \\
    \dot{\transition}_{ij} &= \frac{1}{d_i}\rb{1-\dot{\transition}_{ii}} < \frac{1}{d_i+1}
\end{split} \label{eqn:over-squashing}
\end{alignat}

where the right-hand sides of the two inequalities are the corresponding entries in the propagation matrix of a \inline{NoDrop} model. % Given the relationships between the expected propagation matrices, as derived in \autoref{eqn:dropagg} and \autoref{eqn:dropgnn}, the same relationship hold between DropAgg and DropGNN and a \inline{NoDrop} model.
\autoref{eqn:dropedge} to \autoref{eqn:over-squashing} together imply the following result:

\begin{lemma} % [1-Layer Linear \inline{GCN}]
\label{thm:sensitivity-1-layer}
    In a 1-layer linear \inline{GCN} with $\propagation = \propagation^{\asym}$, using \inline{DropEdge}, DropAgg or DropGNN
    \begin{enumerate}
        \item increases the sensitivity of a node's representations to its own input features, and
        \item decreases the sensitivity to its neighbors' features.
    \end{enumerate}
    % Moreover, the magnitudes of the changes monotonically increase with the \inline{DropEdge} probability.
\end{lemma}

In other words, \inline{DropEdge}-variants prevent a 1-layer \inline{GCN} from fully utilizing neighborhood information when learning node representations. Ignoring the graph topology this way makes the model resemble an \inline{MLP}, limiting its expressiveness and hindering its ability to model graph-data.

\begin{figure}
    \centering
    \includegraphics[width=0.66\linewidth]{assets/linear-gcn_asymmetric_Proteins.png}
    % \includegraphics[width=0.48\linewidth]{assets/linear-gcn_asymmetric_MUTAG.png}
    \includegraphics[width=0.33\linewidth]{assets/linear-gcn_compare-drop_Proteins.png}
    \caption{Entries of $\dot{\transition}^6$, averaged over molecular graphs sampled from the Proteins dataset. \textit{Left}: Sensitivity between nodes decays at a polynomial rate \wrt their distance. \textit{Middle}: Similarly, it decays at a polynomial rate \wrt the DropEdge probability. \textit{Right}: It decays more quickly with DropAgg than with DropEdge, and even more rapidly with DropGNN.}
    \label{fig:linear-gcn_asymmetric}
\end{figure}

\textbf{L-layer Linear GCNs}. Unfortunately, we cannot draw similar conclusions in L-layer networks, for nodes at arbitrary distances. To see this, view $\dot{\transition}$ as the transition matrix of a non-uniform random walk. % \footnote{This is a random walk wherein edges are independently dropped at each step of the walk, and then self-loops are added; each adjacent edge is then traversed uniformly.}.
This walk has higher self-transition ($i = j$) probabilities than in a uniform augmented random walk ($\transition = \propagation^{\asym}$, $q=0$), but lower inter-node ($i \neq j$) transition probabilities. Note that $\dot{\transition}^L$ and $\transition^L$ store the L-step transition probabilities in the corresponding walks. Then, since the paths connecting the nodes $i\in\vertices$ and $j\in\recfld\layer{L-1}\rb{i}$ may involve self-loops, $(\dot{\transition}^L)_{ij}$ may be lower or higher than $(\transition^L)_{ij}$. Therefore, we cannot conclude how sensitivity between nodes separated by at most $L-1$ hops changes. For nodes L-hops away, however, we can show that \inline{DropEdge} always decreases the corresponding entry in $\dot{\transition}^L$, reducing the effective reachability of \inline{GCNs}. Using \autoref{eqn:dropagg} and \autoref{eqn:dropgnn}, we can show the same for DropAgg and DropGNN, respectively.

\begin{theorem}
\label{thm:sensitivity-l-layer-dec}
    In an L-layer linear \inline{GCN} with $\propagation = \propagation^\asym$, using \inline{DropEdge}, DropAgg or DropGNN decreases the sensitivity of a node $i\in\vertices$ to another node $j\in \hopnbr^{\rb{L}}\rb{i}$, thereby reducing its effective receptive field. Moreover, the sensitivity monotonically decreases as the dropping probability is increased.
\end{theorem}

See \autoref{sec:proofs-thm} for a precise quantitative statement and the proof.

\textbf{Nodes at Arbitrary Distances}. Although no general statement could be made about the change in sensitivity between nodes up to $L-1$ hops away, we can analyze such pairs empirically. We sampled 100 molecular graphs from the Proteins dataset \citep{dobson2003proteins}, % and MUTAG \citep{kazius2005mutag}
binned the node-pairs in each graph by the shortest distance between them, and then plotted the average of the corresponding entries in $\dot{\transition}^L$, $L=6$. 

The results are shown in \autoref{fig:linear-gcn_asymmetric}. In the left subfigure, we observe that the sensitivity between two nodes decays at a \textit{polynomial} rate with increasing distance between them. Moreover, \inline{DropEdge} increases the expected sensitivity between nodes close to each other (0-hop and 1-hop neighbors) in the original topology, but reduces it between nodes farther off. In the middle subfigure, we show how the average sensitivity at different distances changes with the \inline{DropEdge} probability. Specifically, we observe that the decay in sensitivity to nodes at large distances is polynomial in the \inline{DropEdge} probability, which suggests that the algorithm would not be suitable for capturing LRIs. Similar conclusions can be made with the symmetrically normalized propagation matrix (see \autoref{sec:fig-sym-norm}). Finally, in the right subfigure, we compare the DropEdge-variants by plotting the sensitivity at $d_{\graph}\rb{i,j} = L = 6$. Although the analytical form of all the expected propagation matrices are available to us, we approximate them using Monte-Carlo sampling. As expected from \autoref{eqn:dropagg}, we observe that \inline{DropAgg} not only decreases the sensitivity of node representations to distant nodes, but does it to a greater extent than \inline{DropEdge}. Similarly, \autoref{eqn:dropgnn} suggested that \inline{DropGNN} could be even more harmful than \inline{DropAgg}, and this is validated by the empirical results. In fact, for $q > 0.7$, nodes 6-hops away are mostly insensitive to each other.

\subsection{Nonlinear MPNNs}
\label{sec:nonlinear}

While linear networks are useful in simplifying the theoretical analysis, they are often not practical. In this subsection, we will consider the upper bounds on sensitivity established in previous works, and extend them to the \inline{DropEdge} setting.

\textbf{ReLU GCNs}. \citet{xu2018jknet} considered the case of \inline{ReLU} nonlinearity, so that the update rule is $\representations\layer{\ell} = \relu(\propagation \representations\layer{\ell-1} \weights\layer{\ell})$. Additionally, it makes the simplifying assumption that each path in the computational graph is \textit{active} with a fixed probability, $\rho$ \cite[Assumption A1p-m]{kawaguchi2016minima}. Accordingly, the sensitivity (in expectation) between any two nodes is given as
\begin{align}
    \norm{\expectation_{\relu}\sb{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}}_1 = \sb{\rho \norm{\prod_{\ell=1}^L \weights\layer{\ell}}_1} \rb{\propagation^L}_{ij} 
    = \zeta_1\layer{L} \rb{\propagation^L}_{ij}
\end{align}
where $\zeta_1\layer{L}$ is independent of the choice of nodes $i,j\in\vertices$. Taking an expectation \wrt the random edge masks, we get
\begin{align}
% \begin{split}
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\norm{\expectation_{\relu} \sb{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}}_1}
    &=
    \zeta_1^{\rb{L}} \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\rb{\prod_{\ell=1}^L \propagation\layer{\ell}}_{ij}} \\
    &=
    \zeta_1\layer{L} \rb{\expectation\sb{\propagation}^{L}}_{ij}
    \label{eqn:xu}
% \end{split}
\end{align}

Using \autoref{thm:sensitivity-l-layer-dec}, we conclude that in a ReLU-GCN, DropEdge, DropAgg and DropGNN will reduce the expected sensitivity between nodes L-hops away. Empirical observations in \autoref{fig:linear-gcn_asymmetric} and \autoref{fig:linear-gcn_symmetric} suggest that we may expect an increase in sensitivity to neighboring nodes, but a significant decrease in sensitivity to those farther away.

\textbf{Source-only Message Functions}. \citet[Lemma 3.2]{black2023resistance} considers MPNNs with aggregation functions of the form
\begin{align}
    \aggregate\layer{\ell} \rb{\representation_i^{\rb{\ell-1}}, \cb{\representation_j^{\rb{\ell-1}}: j\in\hopnbr^{\rb{1}}\rb{i}}} =
    \sum_{j\in\recfld^{\rb{1}}\rb{i}} \propagation_{ij} \message\layer{\ell} \rb{\representation_j^{\rb{\ell-1}}} 
\end{align}
and $\update$ and $\message$ functions with bounded gradients. In this case, the sensitivity between two nodes $i,j\in\vertices$ %, and not just those separated by L-hops:
can be bounded as
\begin{align}
    \norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1 \leq \zeta_2^{\rb{L}} \rb{\sum_{\ell=0}^L \propagation^{\ell}}_{ij}
\end{align}

As before, we can use the independence of edge masks to get an upper bound on the expected sensitivity:
\begin{align}
% \begin{split}
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1}
    &\leq
    \zeta_2^{\rb{L}} \rb{\expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{I_N + \sum_{\ell=1}^L \prod_{k=1}^{\ell} \propagation\layer{k}}}_{ij} \\
    &=
    \zeta_2^{\rb{L}} \rb{\sum_{\ell=0}^L \expectation \sb{\propagation}^{\ell}}_{ij}
% \end{split}
\end{align}

\autoref{fig:black_extension} shows the plot of the entries of $\sum_{\ell=0}^6 \dot{\transition}^{\ell}$ (\ie for DropEdge), as in the upper bound above, with $\propagation = \propagation^\asym$. %, against the shortest distance between corresponding nodes. 
We observe that the sensitivity between nearby nodes marginally increases, while that between distant nodes notably decreases (similar to \autoref{fig:linear-gcn_asymmetric}), suggesting significant over-squashing. Similar observations can be made with $\propagation = \propagation^\sym$, and for other DropEdge-variants.

\textbf{Source-and-Target Message Functions}. \citet[Lemma 1]{topping2022understanding} showed that % for a certain class of (non-linear) \inline{MPNNs} -- which includes \inline{GCNs} -- 
if the aggregation function is instead given by 
\begin{align}
    \aggregate\layer{\ell} \rb{\representation_i^{\rb{\ell-1}}, \cb{\representation_j^{\rb{\ell-1}}: j\in\hopnbr^{\rb{1}}\rb{i}}} =
    \sum_{j\in\recfld^{\rb{1}}\rb{i}} \propagation_{ij} \message\layer{\ell} \rb{\representation_i^{\rb{\ell-1}}, \representation_j^{\rb{\ell-1}}}    
\end{align}
then the sensitivity between nodes $i\in\vertices$ and $j\in \hopnbr^{\rb{L}}\rb{i}$ can be bounded as
\begin{align}
    \norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1 \leq \zeta_3^{\rb{L}} \rb{\propagation^{L}}_{ij}
\end{align}

With random edge-dropping, this bound can be adapted as follows: % Following the steps in \autoref{eqn:drop-edge-sens-xu},
\begin{align}
    \expectation_{\mask\layer{1}, \ldots, \mask\layer{L}} \sb{\norm{\frac{\partial \representation\layer{L}_i}{\partial \feature_j}}_1}
    \leq
    \zeta_3^{\rb{L}} \rb{\expectation\sb{\propagation}^{L}}_{ij}
% \label{eqn:drop-edge-sens-topping}
\end{align}

which is similar to \autoref{eqn:xu}, only with a different proportionality constant, that is anyway independent of the choice of nodes. Here, again, we invoke \autoref{thm:sensitivity-l-layer-dec} to conclude that $(\expectation[\propagation]^L)_{ij}$ decreases monotonically with increasing \inline{DropEdge} probability $q$. This implies that, in a non-linear \inline{MPNN} with $\propagation = \propagation^\asym$, \inline{DropEdge} lowers the sensitivity bound given above. Empirical results in \autoref{fig:linear-gcn_symmetric} support the same conclusion for $\propagation = \propagation^\sym$.

\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
     \textbf{Message of the Section}: Studying the expected propagation matrix allows us to predict the effect of random edge-dropping methods on information propagation in MPNNs. Specifically, DropEdge, DropAgg and DropGNN increase the sensitivity of nodes to their neighbors, but decrease it to nodes farther off. This suggests that these methods would be unsuitable for long-range tasks, where it is imperative to facilitate communication between distant nodes.
\end{tcolorbox}