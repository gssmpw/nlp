\section{Background}

Consider a directed graph $\unweightedgraph$, with $\vertices = \sb{N} \coloneqq \cb{1,\ldots,N}$ denoting the node set and $\edges\subset\vertices\times\vertices$ the edge set; $\rb{j \to i} \in \edges$ if there's an edge from node $j$ to node $i$. Let $\adjacency \in \cb{0,1}^{N\times N}$ denote its adjacency matrix, such that $\adjacency_{ij} = 1$ if and only if $\rb{j \to i} \in \edges$, and let $\degree \coloneqq \tdiag{\adjacency\ones_N}$ denote the in-degree matrix. The geodesic distance, $d_{\graph}\rb{j, i}$, from node $j$ to node $i$ is the length of the shortest path starting at node $j$ and ending at node $i$. Accordingly, the $\ell$-hop neighborhood of a node $i$ can be defined as the set of nodes that can reach it in exactly $\ell\in\natural_0$ steps, $\hopnbr^{\rb{\ell}}\rb{i} = \cb{j\in\vertices: d_{\graph}\rb{j,i} = \ell}$. % The input to the \inline{GNN} is the tuple $\rb{\graph, \features}$, where $\features \in \real^{N\times \hidden\layer{0}}$ collects the node features.

\subsection{Graph Neural Networks}
\label{sec:gnns}

Graph neural networks (\inline{GNNs}) operate on inputs of the form $\rb{\graph,\features}$, where $\graph$ encodes the graph topology and $\features \in \real^{N\times \hidden\layer{0}}$ collects the node features\footnote{To keep things simple, we will ignore edge features.}. Message-passing neural networks (\inline{MPNNs}) \cite{gilmer2017mpnn} are a special class of \inline{GNNs} which recursively aggregate information from the 1-hop neighborhood of each node using \textit{message-passing layers}. An L-layer \inline{MPNN} is given as
\begin{align}
\begin{split}
    &\representation_i\layer{\ell} 
    =
    \update\layer{\ell} \rb{\representation_i\layer{\ell-1}, \aggregate\layer{\ell} \rb{\representation_i\layer{\ell-1}, \cb{\representation_j\layer{\ell-1}: j\in\hopnbr^{\rb{1}}\rb{i}}}}, \quad \forall \ell \in \sb{L} \\
    &\mpnn_{\theta}\rb{\graph,\features} = \cb{\readout\rb{\representation_i\layer{L}}: i\in\vertices}
\end{split}
\end{align}
where $\representations\layer{0} = \features$, $\aggregate\layer{\ell}$ denotes the \textit{aggregation functions}, $\update\layer{\ell}$ the \textit{update functions}, and $\readout$ the \textit{readout function}. Since the final representation of node $i$ is a function of the input features of nodes at most L-hops away from it, its \textit{receptive field} is given by $\recfld^{\rb{L}}\rb{i} \coloneqq \cb{j\in\vertices: d_{\graph}\rb{j,i} \leq L}$.

For example, a \inline{GCN} \cite{kipf2017gcn} updates node representations as the weighted sum of its neighbors' representations: 
\begin{align}
    \representations\layer{\ell} = \sigma\rb{\propagation \representations\layer{\ell-1}\weights\layer{\ell}}
\end{align}
where $\sigma$ is a point-wise nonlinearity, \eg \inline{ReLU}, the propagation matrix, $\propagation$, is a \textit{graph shift operator}, \ie $\propagation_{ij} \neq 0$ if and only if $\rb{j \to i} \in \edges$ or $i=j$, and $\weights\layer{\ell} \in \real^{\hidden\layer{\ell-1}\times\hidden\layer{\ell}}$ is a weight matrix. The original choice for $\propagation$ was the symmetrically normalized adjacency matrix $\propagation^\sym \coloneqq \widetilde{\degree}^{-1/2} \widetilde{\adjacency} \widetilde{\degree}^{-1/2}$ \cite{kipf2017gcn}, where $\widetilde{\adjacency} = \adjacency + \identity_N$ and $\widetilde{\degree} = \text{diag}(\widetilde{\adjacency}\ones_N)$. However, several influential works have also used the asymmetrically normalized adjacency, $\propagation^\asym \coloneqq \widetilde{\degree}^{-1} \widetilde{\adjacency}$ \cite{hamilton2017ppi,schlichtkrull2017modelingrelationaldatagraph,Li_Han_Wu_2018}.

\subsection{DropEdge}
\label{sec:drop-edge}

\inline{DropEdge} is a random data augmentation technique that works by sampling a subgraph of the original input graph in each layer, and uses that for message passing \cite{rong2020dropedge}:
\begin{align}
\begin{split}
    \mask\layer{\ell} &\sim \cb{\text{Bern}\rb{1-q}}^{N\times N} \\
    \widetilde{\adjacency}\layer{\ell} &= \mask\layer{\ell} \circ \adjacency + \identity_N
\end{split}
\end{align}

Several variants of \inline{DropEdge} have also been proposed, forming a family of random edge-dropping algorithms for tackling the over-smoothing problem. For example, DropNode \citep{feng2020dropnode} independently samples nodes and sets their representations to $0$, followed by rescaling to make the feature matrix unbiased. This is equivalent to setting the corresponding columns of the propagation matrix to $0$.
% \begin{align*}
%     \bm\layer{l} &\sim \cb{\text{Bern}\rb{1-q}}^{N\times 1} \\
%     \propagation\layer{l} &\leftarrow \frac{1}{1-q} \rb{\ones_N \rb{\bm\layer{l}}\transpose} \circ \propagation
% \end{align*}
In a similar vein, DropAgg \citep{jiang2023dropagg} samples nodes that don't aggregate messages from their neighbors. This is equivalent to dropping the corresponding rows of the adjacency matrix.
% \begin{align*}
%     % \bm\layer{l} &\sim \cb{\text{Bern}\rb{1-q}}^{N\times 1} \\
%     \widetilde{\adjacency}\layer{l} &= \rb{\bm\layer{l} \ones_N\transpose} \circ \adjacency + \identity_N
% \end{align*}
Combining these two approaches, DropGNN \citep{papp2021dropgnn} samples nodes which neither propagate nor aggregate messages in a given message-passing step.
% \begin{align*}
%     % \bm\layer{l} &\sim \cb{\text{Bern}\rb{1-q}}^{N\times 1} \\
%     \widetilde{\adjacency}\layer{l} &= \rb{\bm\layer{l} \rb{\bm\layer{l}}\transpose} \circ \adjacency  + \identity_N
% \end{align*}
These algorithms alleviate over-smoothing by reducing the number of messages being propagated in the graph, thereby slowing down the convergence of node representations.

\subsection{Over-squashing}

Over-squashing refers to the problem of information from exponentially growing neighborhoods \cite{chen2018stochastic} being squashed into finite-sized node representations \cite{alon2021on}. This results in a loss of information as it is propagated over long distances, disallowing \inline{MPNNs} from capturing long-range interactions (LRIs) and limiting their applications to short-range tasks. \citet{topping2022understanding} formally characterized over-squashing in terms of the Jacobian of the node-level representations \wrt the input features: $\|\partial \representation\layer{L}_i / \partial \feature_j\|_1$. Accordingly, over-squashing can be understood as low sensitivity between distant nodes, \ie small perturbations in a node's features don't effect other distant nodes' representations. 

Several works have linked over-squashing in an \inline{MPNN} with topological properties like Cheeger's constant \cite{giraldo2023trading,karhadkar2023fosr}, curvature of edges \cite{topping2022understanding,nguyen2023revisiting,liu2023curvdrop}, effective resistance between nodes \cite{black2023resistance,rodriguez2022diffwire} and the expected commute time between them \cite{di2023over,giovanni2024how}. These results have inspired the design of several graph rewiring techniques that strategically add edges to improve the connectivity in the graph, thereby alleviating over-squashing. % \cite{akansha2024oversquashinggraphneuralnetworks}.