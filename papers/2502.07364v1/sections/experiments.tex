\section{Experiments}
\label{sec:exp}

Our theoretical analysis indicates that \inline{DropEdge} may degrade the performance of \inline{GNNs} on tasks that depend on capturing LRIs. In this section, we test this hypothesis by evaluating \inline{DropEdge} models on both short-range and long-range tasks.

\subsection{Setup}

\textbf{Datasets}. Although identifying whether a task requires modeling LRIs can be challenging, understanding the structure of the datasets can provide some insight. For example, homophilic datasets have local consistency in node labels, \ie nodes closely connected to each other have similar labels. On the other hand, in heterophilic datasets, nearby nodes often have dissimilar labels. Since DropEdge-variants increase the sensitivity of a node's representations to its immediate neighbors, and reduce its sensitivity to distant nodes, we expect it to improve performance on homophilic datasets but harm performance on heterophilic ones (see \citet{topping2022understanding}). In this work, we use Cora \citep{mccallum2000cora} and CiteSeer \citep{giles1998citeseer} as representatives of homophilic datasets \citep{zhu2020heterophily}, and Squirrel, Chameleon and TwitchDE \citep{musae} to represent heterophilic datasets \citep{lim2021new}. The networks' statistics are presented in \autoref{tab:datasets}, where we can note the significantly lower homophily measures of heterophilic datasets.

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        % \rowcolor[gray]{0.9}
        \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges} & \textbf{Features} & \textbf{Classes} & \textbf{Homophily} \\ \hline
        Cora & 2,708 & 10,556 & 1,433 & 7 & 0.766 \\ \hline
        CiteSeer & 3,327 & 9,104 & 3,703 & 6 & 0.627 \\ \hline
        Chameleon & 2,277 & 36,051 & 2,325 & 5 & 0.062 \\ \hline
        Squirrel & 5,201 & 216,933 & 2,089 & 5 & 0.025 \\ \hline
        TwitchDE & 9,498 & 306,276 & 128 & 2 & 0.142 \\ \hline
    \end{tabular}
    \caption{Dataset statistics. Number of edges excludes self-loops. Homophily measures from \cite{lim2021new}.}
    \label{tab:datasets}
\end{table}

\textbf{Models}. We use two \inline{MPNN} architectures, \inline{GCN} and \inline{GAT}, to demonstrate the effect of \inline{DropEdge}. \inline{GCN} satisfies the model assumptions made in all the theoretical results presented in \autoref{sec:theory}, while \inline{GAT} does not fit into any of them. Therefore, \inline{GCN} and \inline{GAT} together provide a broad representation of different \inline{MPNN} architectures. For \inline{GAT}, we use 2 attentions heads in order to keep the computational load manageable, while at the same time harnessing the expressiveness of the multi-headed self-attention mechanism. As for the model depth, we vary it as $L = 2, 4, 6, 8$.

\textbf{Dropout Methods}. In addition to the DropEdge and its variants -- DropNode, DropAgg and DropGNN -- discussed in \autoref{sec:theory}, we evaluate related stochastic regularization methods like regular dropout \citep{srivastava2014dropout} (referred as Dropout), DropMessage \citep{Fang2022DropMessageUR} and SkipNode \cite{lu2024skipnode}. This allows us to study the pervasiveness of the problem of over-squashing over a large set of methods designed for alleviating over-smoothing.

\textbf{Dropping Probability}. For all the methods, the dropping probabilities are varied as $q=0.0, 0.1, \ldots, 0.9$, so as to reliably capture the trends in model accuracy. We adopt the common practice of turning the methods off at test-time ($q=0$), isolating their effect on optimization and generalization, which our theory does not address. 

Other experimental details are reported in \autoref{sec:exp-setup}. We conduct 5 independent runs for each dataset$-$model$-$dropout$-$probability configuration and report the average test accuracy (with early-stopping using the validation set). % The readout module is a simple linear transformation which outputs the predicted class-logits for each node.

\subsection{Results}

\newcommand{\positive}{ForestGreen}
\newcommand{\negative}{Bittersweet} % BrickRed

\begin{table}[t]

    \centering
    \renewcommand{\arraystretch}{1.3}
    \resizebox{0.85\textwidth}{!}{%
    \begin{tabular}{|c|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.6cm}|}
    \hline
    \multirow{2}{*}{\textbf{Dropout}} & \multirow{2}{*}{\textbf{GNN}} & \multicolumn{2}{|c|}{\textbf{Homophilic}} & \multicolumn{3}{|c|}{\textbf{Heterophilic}} \\
        \cline{3-7}
        % \rowcolor[gray]{0.9}
        & & \textbf{Cora} & \textbf{CiteSeer} & \textbf{Chameleon} & \textbf{Squirrel} & \textbf{TwitchDE} \\ \hline
        \multirow{2}{*}{DropEdge} & GCN & \cellcolor{\positive!39.627} $+0.396$ & \cellcolor{\positive!44.826} $+0.448$ & \cellcolor{\negative!72.293} $-0.723$ & \cellcolor{\negative!61.424} $-0.614$ & \cellcolor{\negative!50.857} $-0.509$ \\ \cline{2-7}
         & GAT & \cellcolor{\positive!54.705} $+0.547$ & \cellcolor{\positive!54.808} $+0.548$ & \cellcolor{\negative!23.593} $-0.236$ & \cellcolor{\negative!64.090} $-0.641$ & \cellcolor{\negative!39.009} $-0.390$ \\ \hline
        \multirow{2}{*}{DropNode} & GCN & \cellcolor{\negative!32.007} $-0.320$ & \cellcolor{\negative!66.202} $-0.662$ & \cellcolor{\negative!86.346} $-0.863$ & \cellcolor{\negative!79.000} $-0.790$ & \cellcolor{\negative!46.453} $-0.465$ \\ \cline{2-7}
         & GAT & \cellcolor{\negative!70.631} $-0.706$ & \cellcolor{\negative!73.463} $-0.735$ & \cellcolor{\negative!85.535} $-0.855$ & \cellcolor{\negative!28.590} $-0.286$ & \cellcolor{\negative!51.238} $-0.512$ \\ \hline
        \multirow{2}{*}{DropAgg} & GCN & \cellcolor{\negative!1.484} $-0.015$ & \cellcolor{\positive!29.433} $+0.294$ & \cellcolor{\negative!33.366} $-0.334$ & \cellcolor{\negative!31.702} $-0.317$ & \cellcolor{\negative!56.554} $-0.566$ \\ \cline{2-7}
         & GAT & \cellcolor{\negative!1.949} $-0.019$ & \cellcolor{\positive!39.179} $+0.392$ & \cellcolor{\negative!35.288} $-0.353$ & \cellcolor{\negative!39.758} $-0.398$ & \cellcolor{\negative!2.163} $-0.022$ \\ \hline
        \multirow{2}{*}{DropGNN} & GCN & \cellcolor{\positive!35.971} $+0.360$ & \cellcolor{\positive!46.840} $+0.468$ & \cellcolor{\negative!74.597} $-0.746$ & \cellcolor{\negative!59.965} $-0.600$ & \cellcolor{\negative!67.104} $-0.671$ \\ \cline{2-7}
         & GAT & \cellcolor{\positive!23.594} $+0.236$ & \cellcolor{\positive!52.512} $+0.525$ & \cellcolor{\negative!50.664} $-0.507$ & \cellcolor{\negative!30.402} $-0.304$ & \cellcolor{\negative!13.919} $-0.139$ \\ \hline
        \multirow{2}{*}{Dropout} & GCN & \cellcolor{\positive!3.359} $+0.034$ & \cellcolor{\negative!30.893} $-0.309$ & \cellcolor{\negative!60.978} $-0.610$ & \cellcolor{\negative!46.478} $-0.465$ & \cellcolor{\negative!29.989} $-0.300$ \\ \cline{2-7}
         & GAT & \cellcolor{\positive!11.978} $+0.120$ & \cellcolor{\negative!10.967} $-0.110$ & \cellcolor{\negative!83.644} $-0.836$ & \cellcolor{\negative!54.462} $-0.545$ & \cellcolor{\negative!39.715} $-0.397$ \\ \hline
        \multirow{2}{*}{DropMessage} & GCN & \cellcolor{\negative!22.040} $-0.220$ & \cellcolor{\negative!60.676} $-0.607$ & \cellcolor{\negative!40.399} $-0.404$ & \cellcolor{\negative!36.406} $-0.364$ & \cellcolor{\negative!19.887} $-0.199$ \\ \cline{2-7}
         & GAT & \cellcolor{\positive!2.844} $+0.028$ & \cellcolor{\negative!36.797} $-0.368$ & \cellcolor{\negative!52.980} $-0.530$ & \cellcolor{\negative!69.747} $-0.697$ & \cellcolor{\negative!14.844} $-0.148$ \\ \hline
        \multirow{2}{*}{SkipNode} & GCN & \cellcolor{\positive!16.080} $+0.161$ & \cellcolor{\positive!1.737} $+0.017$ & \cellcolor{\negative!52.787} $-0.528$ & \cellcolor{\negative!28.505} $-0.285$ & \cellcolor{\negative!24.390} $-0.244$ \\ \cline{2-7}
         & GAT & \cellcolor{\positive!40.347} $+0.403$ & \cellcolor{\positive!46.590} $+0.466$ & \cellcolor{\negative!43.627} $-0.436$ & \cellcolor{\positive!24.821} $+0.248$ & \cellcolor{\positive!27.763} $+0.278$ \\ \hline
    \end{tabular}}
    \caption{Spearman correlation between dropping probability and test accuracy. Note the positive correlations for homophilic datasets and negative correlations for heterophilic datasets. Results suggest that random edge-dropping, although effective at improving generalization in short-range tasks, is unsuitable for long-range ones.}
    \label{tab:correlation}
\end{table}

In \autoref{tab:correlation}, we report the rank correlation between the dropping probability and test accuracy of different dataset$-$model$-$dropout combinations. The statistics are computed over $10\ (q=0.0,0.1,\ldots,0.9) \times 5\ (\text{samples}) = 50$ runs for each $L = 2, 4, 6, 8$, and then averaged. It is clear to see that in most combinations with the homophilic datasets Cora and CiteSeer, the correlation is positive, indicating that DropEdge and its variants improve test-time performance at short-range tasks. On the other hand, with the heterophilic datasets Chameleon, Squirrel and TwitchDE, the correlation values are negative. This suggests that the edge-dropping methods harm generalization in long-range tasks by forcing the model to overfit to short-range signals. % Such a phenomenon was also hypothesized in \citet{alon2021on}, albeit for general \inline{MPNN} learning on long-range tasks.

\begin{figure}[t]
    \centering
    \subcaptionbox{Homophilic datasets \label{fig:homgenous-datasets}}{\includegraphics[width=0.7\linewidth]{assets/DropEdge_homophilic.png}} \\
    \vspace{2mm}
    \subcaptionbox{Heterophilic datasets \label{fig:heterophilic-datasets}}{\includegraphics[width=\linewidth]{assets/DropEdge_heterophilic.png}}
    \caption{Dropping probability versus test accuracy of DropEdge-GCN. The theory the explains the contrasting trends as follows: random edge-dropping pushes models to fit to local information during training, which is suitable for short-range tasks, but harms test-time performance in long-range ones.}
    \label{fig:acc-trends}
\end{figure}

\autoref{fig:acc-trends} allows us to visualize these trends for the homophilic and heterophilic datasets. For conciseness, we only display the results for \inline{DropEdge}-\inline{GCN}s. Clearly, on Cora and CiteSeer, the model's test-time performance improves with increasing dropping probability, as has been reported earlier. However, on Chameleon, Squirrel and TwitchDE, the performance degrades with increasing dropping probability, as was suggested by our theoretical results. This highlights an important gap in our understanding of dropout methods -- while their positive effects on model performance have been well-studied, making them popular choices for training deep \inline{GNNs}, their evaluation has been limited to short-range tasks, leaving their negative impact on capturing LRIs overlooked.

\textbf{Remark on DropNode}. In \autoref{eqn:dropnode}, we noted that \inline{DropNode} does not suffer from loss in sensitivity. However, note that those results were in expectation. Moreover, our analysis did not account for the effects on the learning trajectory. In practice, a high DropNode probability would make it hard for information in the node features to reach distant nodes. This would prevent the model from learning to effectively combine information from large neighborhoods, harming generalization. In fact, in \autoref{tab:correlation}, we can see that it is the only dropping method with a negative correlation between dropping probability and test accuracy for each dataset$-$model combination, including homophilic ones. See \autoref{fig:dropnode} for a visualization of its negative effects on test accuracy.

% Our study demonstrates the detrimental effect of random edge-dropping on model performance in long-range tasks. Similarly, other methods designed for training deep \inline{GNNs} may also suffer from similar limitations when modelling LRIs. Indiscriminate use of such techniques may result in degrading model performance.

\subsection{Over-squashing or Under-fitting?}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/ablation.png}
    \caption{\inline{DropEdge} probability versus training accuracy of GCNs. The training performance improves with $q$, suggesting that the models are not underfitting. Instead, the reason for poor test-time performance (\autoref{fig:homgenous-datasets}) is that models are over-fitting to short-range signals during training, resulting in poor generalization.}
    \label{fig:ablation}
\end{figure}

The results in the previous subsection suggest that using random edge-dropping to regularize model training leads to poor test-time performance. We hypothesize that this occurs because the models struggle to propagate information over long distances, causing overfitting of node representations to local neighborhoods. However, a confounding factor is at play: DropEdge variants reduce the generalization gap by preventing overfitting to the training set, which manifests as reduced training performance. If this regularization is too strong, it could lead to underfitting, which could also explain the poor test-time performance on heterophilic datasets. This concern is particularly relevant because the heterophilic networks are much larger than homophilic ones (\autoref{tab:datasets}), making them more prone to underfitting. To investigate this, we plot the training accuracies of deep DropEdge-GCNs on the heterophilic datasets; \autoref{fig:ablation} shows the results. It is clear that the models do not underfit as the dropping probability increases. In fact, somewhat unexpectedly, the training metrics improve. Together with the results in \autoref{fig:acc-trends}, we conclude that DropEdge-like methods are detrimental in long-range tasks, as they cause overfitting to short-range artifacts in the training data, resulting in poor generalization at test-time.

\begin{tcolorbox}[boxsep=0mm,left=2.5mm,right=2.5mm]
    \textbf{Message of the Section}: Random edge-dropping algorithms reduce the receptive field of MPNNs, forcing them to fit to short-range signals in the training set. While this may make deep GNNs suitable for homophilic datasets, it results in overfitting on heterophilic datasets, which leads to poor test-time performance. Therefore, these methods should be used only after carefully understanding the task at hand.
\end{tcolorbox}