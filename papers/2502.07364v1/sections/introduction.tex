\section{Introduction}

Graph-structured data is ubiquitous -- it is found in social media platforms, online retail platforms, molecular structures, transportation networks, and even computer systems. % Understanding and modeling these graphs has become increasingly important.
\textit{Graph neural networks} (\inline{GNNs}) \citep{scarselli_2009_gnns,li_2016_graph-seq-nns} are powerful neural models developed for modelling graph-structured data, and have found applications in several real-world scenarios \citep{gao2018large,you_2020_l2-gcn,you_2020_contrastive,you_2022_byov,pmlr-v119-you20a,ying_2018_gcnn-rcmd,zheng2022cold,NIPS2017_2eace51d,marinka_2017_multicellular,wale2006chemical}. % Owing to their impressive performance, \inline{GNNs} have recently become a popular choice for graph analysis.
A popular class of \inline{GNNs}, called \textit{message-passing neural networks} (\inline{MPNNs}) \citep{gilmer2017mpnn}, recursively process % node-level and edge-level features 
neighborhood information using message-passing layers. These layers are stacked to allow each node to aggregate information from increasingly larger neighborhoods, akin to how \textit{convolutional neural networks} (\inline{CNNs}) learn hierarchical features for images \citep{cnns_1989_lecun}. However, unlike in image-based deep learning, where \textit{ultra-deep} \inline{CNN} architectures have led to performance breakthroughs \citep{szegedy_2015_deeper,he_2016_residual}, shallow \inline{GNNs} often outperform deeper models on many graph learning tasks \citep{zhou2021understanding}. This is because deep \inline{GNNs} suffer from unique issues like \textit{over-smoothing} \citep{oono2020graph} and \textit{over-squashing} \citep{alon2021on}, which makes training them notoriously difficult.

Over-smoothing refers to the problem of node representations becoming \textit{too similar} as they are recursively processed. This is undesirable since it limits the \inline{GNN} from effectively utilizing the information in the input features. The problem has garnered significant attention from the research community, resulting in a suite of algorithms designed to address it \cite{rusch2023surveyoversmoothinggraphneural}. Amongst these methods are a collection of random edge-dropping algorithms, including \inline{DropEdge} \citep{rong2020dropedge} and its variants -- DropNode \citep{feng2020dropnode}, DropAgg \cite{jiang2023dropagg} and DropGNN \citep{papp2021dropgnn} -- %These algorithms have been shown to reduce over-smoothing by 
which act as \textit{message-passing reducers}. % Despite that, however, the performance metrics on most real-world datasets progressively worsen as more layers are stacked \citep{zhou2021understanding}. This implies that there continue to be issues that \inline{DropEdge}-variants fail to resolve. Or, perhaps, \emph{they introduce new problems which have remained unnoticed}.

The other issue specific to GNNs is over-squashing. In certain graph structures, neighborhood size grows exponentially with distance from the source \citep{chen2018fastgcn}, causing information to be lost as it passes through graph bottlenecks \citep{alon2021on}. This limits MPNNs' ability to enable communication between distant nodes, which is crucial for good performance on long-range tasks.
% The other issue specific to \inline{GNNs} is over-squashing. For certain graph structures, the neighborhood size grows exponentially with distance from the source \citep{chen2018stochastic}. In such cases, information may be lost as it is \textit{squashed} through graph \textit{bottlenecks} \citep{alon2021on}.This makes it hard for \inline{MPNNs} to facilitate communication between nodes at long distances, which is imperative for good performance on long-range tasks.
To alleviate over-squashing, several graph-rewiring techniques have been proposed, which aim to improve graph connectivity by adding edges in a strategic manner\footnote{Sometimes, along with removal of some edges to preserve statistical properties of the original topology.} \citep{karhadkar2023fosr,deac2022expander,black2023resistance,nguyen2023revisiting,alon2021on}. In contrast, the DropEdge-variants only remove edges, which should, in principle, amplify over-squashing levels.

The empirical evidence in support of methods designed for training deep \inline{GNNs} has been majorly collected on short-range tasks. That is, it simply suggests that \emph{these methods prevent loss of local information, but it remains inconclusive if they facilitate capturing long-range interactions (LRIs)}. Of course, on long-range tasks, deeper \inline{GNNs} are useless if they cannot capture LRIs. This is especially a concern for \inline{DropEdge}-variants since evidence suggests that alleviating over-smoothing with graph rewiring could exacerbate over-squashing \citep{nguyen2023revisiting,giraldo2023trading}. % Unfortunately, possibly because over-squashing is a more recently discovered phenomenon, there has been little research into how these methods affect it. However, 
% With our growing understanding of over-squashing and its negative impacts, it is essential to revisit these algorithms to determine their applicability to long-range tasks.

\textbf{Theoretical Contributions}. In this work, we uncover the effects of random edge-dropping algorithms on over-squashing in \inline{MPNNs}. By explicitly computing the expected \textit{sensitivity} of the node representations to the node features \citep{topping2022understanding} (inversely related to over-squashing) in a linear \inline{GCN} \cite{kipf2017gcn}, we show that these methods provably reduce the \textit{effective receptive field} of the model. Precisely speaking, the rate at which sensitivity between distant nodes decays is \emph{polynomial} \wrt the dropping probability. Finally, we extend the existing theoretical results on sensitivity in nonlinear \inline{MPNNs} \cite{black2023resistance,di2023over,xu2018jknet} to the random edge-dropping setting, again showing that these algorithms exacerbate the over-squashing problem. % Finally, we show that our analysis of \inline{DropEdge} can be easily generalized to DropNode, DropAgg and DropGNN, which also limit the model's capacity to capture LRIs.

\textbf{Experimental Results}. We evaluate the \inline{DropEdge}-variants on node classification tasks using GCN and GAT \cite{veličković2018gat} architectures. Specifically, we assess their performance on homophilic datasets -- Cora \cite{mccallum2000cora} and CiteSeer \cite{giles1998citeseer} -- which represent short-range tasks, and heterophilic datasets -- Chameleon, Squirrel, TwitchDE \cite{musae} -- which correspond to long-range tasks. Our results indicate an increasing trend in test accuracy for homophilic datasets and a declining trend for heterophilic datasets as the dropping probability increases. Accordingly, we hypothesize that edge-dropping algorithms improve performance on short-range tasks, as has been reported earlier, by reducing the receptive field of the \inline{GNN} and increasing model-dataset alignment, However, for long-range tasks, they decrease the model-dataset alignment, resulting in poor generalization.