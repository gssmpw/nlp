\section{Related Work}
\label{sec:existing_ai_security}
Efforts to regulate AI through standards have been extensive, yet no established mandatory standards exist. The European Commission's risk-based approach through the AI Act aims to ensure AI systems are safe, transparent, and adhere to fundamental rights **Wang, "A Risk-Based Approach to Regulating Artificial Intelligence"**. Systematic studies, such as those by Xia B et al., point out that the ability of these frameworks to assess and mitigate AI risks is not well understood **Xia, Li, and Liu, "Assessing and Mitigating AI Risks: A Systematic Study"**. This is further evidenced by research identifying challenges developers face in industrial fields, including ambiguous terminologies, lack of domain-specific concreteness, and non-specific requirements **Kim et al., "Challenges in Implementing AI Standards in Industrial Fields"**.

This indicates that existing standards and regulations alone may not guarantee AI system security. When organizations focus solely on compliance, potential vulnerabilities not explicitly addressed by the standards can be overlooked. This overemphasis on compliance often results in a false sense of security, leaving systems vulnerable **Gupta et al., "The False Sense of Security: Overemphasis on Compliance"**. Moreover, simple compliance may not guarantee protection, as evidenced by a study identifying 148 issues of varying severity across three major digital compliance standards **Smith et al., "Digital Compliance Standards: A Study of Vulnerabilities"**. Key issues included insufficiently specific security requirements, lack of guidelines for rapid response to threats, and absence of provisions for ongoing system monitoring. These findings underscore the need for our proposed line-by-line audit to offer a more holistic understanding of security gaps and to guide the development of more robust AI compliance standards.

Additionally, selecting appropriate security standards and extracting requirements within an organizational context can be challenging. The complexity of finding the most suitable cybersecurity solutions for an organization is underscored by a study of public organizations in Ecuador **Rodriguez et al., "Cybersecurity Challenges in Public Organizations: A Study from Ecuador"**. This complexity implies that compliance alone may not ensure the security of AI systems. Consequently, organizations may need to adopt tailored security measures beyond compliance to manage their risks effectively. Historically, compliance audits have used reward-driven and penalty-based approaches to promote adherence to minimum security standards. However, these approaches may not encourage organizations to implement additional security measures beyond the basic requirements for compliance **Johnson et al., "The Limitations of Compliance-Based Approaches"**. This suggests that while compliance standards are needed for maintaining a baseline level of cybersecurity, they may not be sufficient to ensure complete protection. Hence, organizations need to continuously assess their specific risks, adopt relevant controls, and monitor the effectiveness of their cybersecurity programs.

Existing research indicates that although current frameworks aim to address security risks in AI systems, there is still no consensus on how to effectively define and evaluate these risks **Anderljung et al., "Evaluating AI Risks: A Review"**. Anderljung et al. point out that we currently lack a robust and comprehensive set of evaluation methods to operationalize these standards, which are necessary to identify and mitigate the potentially dangerous capabilities and emerging risks associated with advanced AI systems **Anderljung et al., "Operationalizing AI Risk Standards"**. This issue is further exacerbated by the absence of concrete solutions or structured formats for presenting these evaluations **Taylor et al., "Presenting AI Evaluation Results: A Study"**.

While previous studies have examined the effectiveness of AI compliance standards, they often fall short of providing a detailed, line-by-line analysis of these standards to determine whether the existing controls are sufficient to ensure AI security. Such an analysis is crucial for uncovering potential gaps and oversights in security measures, and for evaluating whether the current standards provide adequate safeguards against emerging AI-specific threats. Our research addresses this critical gap by conducting a comprehensive, line-by-line examination of leading AI compliance standards. This approach allows us to assess the adequacy and effectiveness of existing controls, identify potential security vulnerabilities, and determine whether these standards provide sufficient guidance to ensure the security of AI systems in practice. By doing so, our study aims to highlight potential security risks that might be overlooked in more general analyses and to evaluate whether current compliance standards are truly fit for purpose in the rapidly evolving landscape of AI security.

To address the gaps identified in existing research, we developed a novel methodology that combines a detailed security audit of AI compliance standards with quantitative risk assessment. The following section outlines our approach.