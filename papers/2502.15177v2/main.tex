\documentclass[sigconf]{acmart}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{none}
\usepackage{amsmath}
\usepackage[color=yellow]{todonotes}
\setuptodonotes{inline}
\newcommand{\notsure}[1]{{\color{blue}#1}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}


\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title[Optimizing Product Provenance Verification using Data Valuation Methods]{Optimizing Product Provenance Verification\\ using Data Valuation Methods}

\author{Raquib Bin Yousuf}
\affiliation{%
  \institution{Virginia Tech}
  \city{Alexandria}
  \state{VA}
  \country{USA}
}
\email{raquib@vt.edu}

\author{Hoang Anh Just}
\affiliation{%
  \institution{Virginia Tech}
  \city{Blacksburg}
  \state{VA}
  \country{USA}
}
\email{just@vt.edu}

\author{Shengzhe Xu}
\affiliation{%
  \institution{Virginia Tech}
  \city{Alexandria}
  \state{VA}
  \country{USA}
}
\email{shengzx@vt.edu}

\author{Brian Mayer}
\affiliation{%
  \institution{Virginia Tech}
  \city{Alexandria}
  \state{VA}
  \country{USA}
}
\email{bmayer@cs.vt.edu}

\author{Victor Deklerck}
\affiliation{%
  \institution{Meise Botanic Garden}
  \city{Meise}
  \country{Belgium}
}
\email{victor.deklerck@worldforestid.org}

\author{Jakub Truszkowski}
\affiliation{%
  \institution{Word Forest ID}
  \city{Washington, DC}
  \country{USA}
}
\email{jakub.truszkowski@worldforestid.org}

\author{John C. Simeone}
\affiliation{%
 \institution{Simeone Consulting, LLC}
 \city{Littleton}
 \state{New Hampshire}
 \country{USA}}
\email{simeoneconsulting@gmail.com}

\author{Jade Saunders}
\affiliation{%
  \institution{Word Forest ID}
  \city{Washington, DC}
  \country{USA}
}
\email{jade.saunders@worldforestid.org}

\author{Chang-Tien Lu}
\affiliation{%
  \institution{Virginia Tech}
  \city{Alexandria}
  \state{VA}
  \country{USA}
}
\email{clu@vt.edu}

\author{Ruoxi Jia}
\affiliation{%
  \institution{Virginia Tech}
  \city{Blacksburg}
  \state{VA}
  \country{USA}
}
\email{ruoxijia@vt.edu}

\author{Naren Ramakrishnan}
\affiliation{%
  \institution{Virginia Tech}
  \city{Alexandria}
  \state{VA}
  \country{USA}
}
\email{naren@cs.vt.edu}


\renewcommand{\shortauthors}{Yousuf et al.}

\begin{abstract}
  Determining and verifying product provenance remains a critical challenge in global supply chains, particularly as geopolitical conflicts and shifting borders create new incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or agriculture grown on illegally cleared land. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes, has emerged as a powerful tool for geographic origin verification. However, the effectiveness of these models is often constrained by data scarcity and suboptimal dataset selection. In this work, we introduce a novel data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied in SIRA. By prioritizing high-informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. We validate our methodology with extensive experiments, demonstrating its potential to significantly enhance provenance verification, mitigate fraudulent trade practices, and strengthen regulatory enforcement of global supply chains.
\end{abstract}

\keywords{Data valuation, Data Shapley, Stable Isotope Ratio Analysis (SIRA), Gaussian processes}

\maketitle
\pagestyle{plain}

\section{Introduction}
Global natural resource supply chains are opaque,  especially since natural resources are often transformed from raw materials into finished consumer-facing products. These complex supply chains often involve multiple countries, with intermediate outputs being traded internationally and being used as inputs into further manufacturing processes. In addition to business-commerce decisions driving supply chain sourcing, the economics of natural resource trade are often closely linked with geopolitics. Determining and verifying product provenance is a challenge in global supply chains, as geopolitics and the lure of ``don't ask, don't tell'' with respect to the ecological and social cost creates incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or agriculture grown on illegally cleared land.


Product identification and provenance verification of traded natural resources have emerged as promising research areas, with various combinations of methods used based on the specific natural resource sector and the level of granularity of species identification and origin-provenance determination. For example, for wood and forest products, determining species identification and geographic harvest provenance requires utilizing multiple testing methods and tools \cite{wood_forensic, schmitz:hal-02936035, dormontt2015forensic}. Specific to geographic origin verification, Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes has emerged as a powerful tool \cite{truszkowski2025, mortier2024framework}. 

SIRA can be useful for understanding the origin of organic products. Stable isotopes are chemical variants of elements that do not undergo radioactive decay. The ratio of stable isotopes denotes the relative enrichment of different elemental stable isotopes in a sample which is typically measured by mass spectrometry \cite{barrie1996automated} and allows us to understand the enrichment of these isotopes in that sample. The natural variation observed for this ratio is determined by underlying mechanisms that are affected by a range of different factors including but not limited to environmental, atmospheric, soil, metabolic fraction, or other characteristics specific to a species \cite{siegwolf_stable_2022, wang2021possible, vystavna2021temperature}. Previous work has shown how it can be used to trace the origin of items such as timber, seafood, agricultural products, and cotton \cite{truszkowski2025, mortier2024framework, watkinson2022case, cusa_future_2022, wang2020tracing, meier-augenstein_discrimination_2014}.

However, the effectiveness of these models is often constrained by data scarcity and suboptimal dataset selection. The use of SIRA to identify origin is limited by several factors, though chief among them is the limited number of reference samples used as input data for modeling isoscapes. Reference sampling campaigns are expensive, and often sampling locations are chosen based on ease of sampling rather than areas that yield a gain in model prediction accuracy \cite{gasson2021worldforestid}. There is a need to optimize reference sample collection efforts, in such a way that would enable the ability to ``value'' reference samples thus enabling more judicious sample collection campaigns. 

Here, we introduce a novel data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied to SIRA. 
Our key contributions are:
\begin{enumerate}
    \item We bring the growing literature on data
valuation in machine learning to bear upon the
pressing problem of optimizing product provenance
verification.
By prioritizing highly informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. 
\item 
We have \textbf{deployed} our approach of optimizing the selection of training samples which improves our model accuracy. These new models are being used by European enforcement agencies to stop the trade in sanctioned Russian timber by demonstrating that a claimed harvest location other than Russia is not viable. See coverage of our work from the \textit{New York Times} ~\cite{nytimes_nazaryan_2024}.
Due to confidentiality reasons, we use a global dataset of Oak ({\it Quercus} spp.) reference samples to illustrate our methodology.
\item  We validate our methodology with extensive experiments, demonstrating its potential to significantly enhance valuation of data, improve
the configuration of models, and in this manner strengthen regulatory enforcement of global supply chains.
\end{enumerate}


\section{Related Work}
SIRA has been widely
employed to discriminate geographic origin of various
plant and animal-based products in global supply chains, such as garlic \cite{pianezze_geographical_2019}, Chinese tea \cite{liu_c_2020}, olive oil \cite{bontempo_characterisation_2019}, cheese \cite{camin_application_2004}, and timber \cite{mortier2024framework, truszkowski2025}. 
By bringing data valuation methods to bear upon
SIRA pipelines we aim to improve
the verification of product provenance.

Prior work in data valuation is typically
seen in the context of explainable machine learning and enhancing model performance~\cite{wu2024data, covert2024stochastic}. Existing methods primarily rely on leave-one-out retraining and influence functions \cite{koh2017understanding}, Shapley values \cite{jia2019towards, ghorbani2019data, shapley_jia_wang2023note}, Least Cores \cite{yan2021if}, the Banzhaf value \cite{banzhaf_wang2022data}, Beta Shapley \cite{kwon2021beta}, and reinforcement learning \cite{yoon2020data}. Furthermore, data valuation has been applied across various domains to enhance model development and interpretability, including health data \cite{health_pandl2021trustworthy}, medical imaging \cite{medical_image_tang2021data}, and the Internet of Things \cite{iot_shi2024data}. This paper
is the first to formally apply data
valuation techniques to SIRA.

\section{Methods}
\label{sec:methods}

\subsection{Notation}
Let $\mathcal{X}$ be a set of locations where
data is collected, where $x\in \mathcal{X}$ typically is specified by a longitude and latitude. Let $\mathcal{Y}$ be the set of measurements made
over $\mathcal{X}$, here 
denoting stable isotope ratio values (e.g. $\delta^{13}C$, $\delta^2H$, $\delta^{15} N$, $\delta^{18}O$, $\delta^{34}S$), or trace element values (e.g. Si, Cu, S, Ba, Rb).
We denote $f$ and $g$ as functions of interest (defined below). For evaluation purposes, we split our data into training and test datasets, where $D=\{z_i\}_{i=1}^N$ represents the training dataset with $N$ data points and $z_i = (x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$. Similarly, $T=\{z_i\}_{i=1}^M$ denotes the test dataset with $M$ data points. We let $v(h, A, B)$ denote the performance of the model $h$ trained on a dataset $A$ and evaluated on the dataset $B$, where $v$ would return a numerical value, $v(h, A, B) \in \mathbb{R}$. In cases when the function and the test dataset are known, we drop the dependence in the notation to simply say $v(A)$.

\subsection{Forward and Backward Models}

\paragraph{\textbf{Forward Models.}} For the forward model, the task is to predict the stable isotope values for a given location, i.e., $f : \mathcal{X} \rightarrow \mathcal{Y}$. The motivation is to verify whether the characteristics of the location (denoting specified harvest origin) would align with general isotopic values associated with the specified location. Such models can be decision trees, random forests, or XGBoost to fit such functions. Recent works have proposed using Gaussian process regression models with high performance~\cite{truszkowski2025, mortier2024framework}. 

\paragraph{\textbf{Backward Models.}} In the backward case, one wants to identify the location given
measured stable isotope values. We model this relation as $g : \mathcal{Y} \rightarrow \mathcal{X}$. The fitted model $g$ would help identify whether the declared harvest location of a species sample aligns with the predicted location from $g$. Similarly, here, one can use a range of machine learning models to fit this function. Moreover, \citet{mortier2024framework} reversed the fitted Gaussian process regression models using the Bayes' rule to predict locations from measured isotope ratios.

\paragraph{\textbf{Atmospheric Variables.}} In addition, to support either forward or backward models, we often have available a range of atmospheric variables associated with locations. Such variables can be used as either additional inputs to a forward model or auxiliary information in a backward model.

\paragraph{\textbf{Random Forest Models.}} Random forests (RF) offer a robust and versatile approach for both forward and backward modeling in this context. As an ensemble learning method, it leverages the power of multiple decision trees to improve prediction accuracy and reduce overfitting.  For the forward model $f: \mathcal{X} \rightarrow \mathcal{Y}$, an RF constructs $B$ decision trees $\{f_t\}_{t=1}^B$. Each tree $f_t$ is trained on a bootstrap sample $D_t$ of the training dataset $D$, where $D_t$ is obtained by randomly sampling $N$ data points from $D$ with replacement.  Furthermore, during the construction of each tree $f_t$, specifically at each node split, a random subset of features $F \subseteq \mathcal{X}$ is considered from the full feature space $\mathcal{X}$. Let $M$ be the number of features randomly sampled at each split ($M < |\mathcal{X}|$). For a given location $x \in \mathcal{X}$, each tree $f_t$ provides a prediction $f_t(x)$. The final RF prediction $\hat{f}(x)$ is then obtained by aggregating these individual predictions, typically through averaging for regression tasks:
$$ \hat{f}(x) = \frac{1}{B} \sum_{t=1}^{B} f_t(x; D_t, F), $$
where $f_t(x; D_t, F)$ denotes the prediction of the $t$-th tree trained on bootstrap dataset $D_t$ and using random feature subsets $F$ at each split. Similarly, for the backward model $g: \mathcal{Y} \rightarrow \mathcal{X}$, an RF comprised of $B$ trees $\{g_t\}_{t=1}^B$ can be trained. Each tree $g_t$ is trained on a bootstrap sample $D_t$ of $D$ to predict the location $x \in \mathcal{X}$ based on features $y \in \mathcal{Y}$, again using random feature subsets from $\mathcal{Y}$ at each split. The ensemble prediction for the backward model $\hat{g}(y)$ given a feature vector $y \in \mathcal{Y}$ is calculated as:
$$ \hat{g}(y) = \frac{1}{B} \sum_{t=1}^{B} g_t(y; D_t, F') $$
Here, $g_t(y; D_t, F')$ represents the prediction of the $t$-th tree for the backward model, trained on bootstrap sample $D_t$ and using random feature subsets $F' \subseteq \mathcal{Y}$ during tree construction. The performance $v(\hat{f}, D, T)$ and $v(\hat{g}, D, T)$ can then be evaluated for the forward and backward random forest models, respectively, using the provided metric $v$.

\paragraph{\textbf{Gaussian Process Regression Models.}} Gaussian process regression (GPR) models, as explored by \cite{truszkowski2025, mortier2024framework}, offer a powerful approach for both forward and backward modeling settings.
For the forward model $f: \mathcal{X} \rightarrow \mathcal{Y}$, GPR can be used to predict isotope ratios $y \in \mathcal{Y}$ at a given location $x \in \mathcal{X}$. We can construct ``isoscapes'' by fitting independent GP regression models to each feature in $\mathcal{Y}$.  Considering our training dataset $D=\{z_i\}_{i=1}^N = \{(x_i, y_i)\}_{i=1}^N$, for each feature $y_j \in \mathcal{Y}$, a GP model is trained to predict isotope values at a new location $x^* \in \mathcal{X}$. The predicted distribution for the feature $y_j$ at location $x^*$ is Gaussian, with a mean and variance given by:
$$E[y_j|x^*, \mathbf{X}] = \mu_j + \mathbf{k}^{(j)T}(\mathbf{K}^{(j)} + \sigma_j^2\mathbf{I})^{-1}(\mathbf{y}_j - \mu_j),$$
$$ V(y_j|x^*, \mathbf{X}) = k^{(j)}(\mathbf{x^*}, \mathbf{x^*}) + \sigma_j^2 - \mathbf{k}^{(j)T}(\mathbf{K}^{(j)} + \sigma_j^2\mathbf{I})^{-1}\mathbf{k}^{(j)T}.$$
Here, $\mathbf{X} = \{x_1, \dots, x_N\}$ represents the training locations and $\mathbf{y}_j = [y_{1j}, \dots, y_{Nj}]^T$ are the values of the $j$-th feature in the training set. $\mu_j$ is the baseline mean for feature $y_j$, $\mathbf{K}^{(j)}$ is the covariance matrix evaluated at all pairs of training locations, $\mathbf{k}^{(j)}$ is the covariance vector between the test location $x^*$ and the training locations, $k^{(j)}(x^*, x^*)$ is the covariance of $x^*$ with itself, and $\sigma_j^2$ is the noise variance for feature $y_j$.  For the backward model $g: \mathcal{Y} \rightarrow \mathcal{X}$, we leverage Bayesian inference to reverse the prediction. Given a set of features $y^* \in \mathcal{Y}$ from a location of unknown origin, the posterior probability of its origin being location $x^* \in \mathcal{X}$ is calculated using Bayes' theorem:
$$ p(x^*|y^*, D) = \frac{p(y^*|x^*, D)p(x^*)}{\int_{x \in \mathcal{X}} p(y^*|x, D)p(x) dx}.$$
The likelihood $p(y^*|x^*, D)$ is derived from the forward GP model, assuming independence of features and using the predicted Gaussian distributions:
$$ p(y^*|x^*, D) = \prod_{j \in \mathcal{Y}} \frac{1}{\sqrt{2\pi V(y_j|x^*, D)}} \exp\left(-\frac{(y_j^* - E[y_j|x^*, D])^2}{2V(y_j|x^*, D)}\right).$$
The prior $p(x^*)$ can incorporate prior knowledge about the distribution of tree harvest locations. This Bayesian approach provides a posterior probability map over $\mathcal{X}$, indicating the most likely origin locations for an observation with features $y^*$.  The performance of both forward and backward GPR models can be assessed using the metric $v(h, D, T)$, where $h$ is the GPR model ($f$ or $g$).


\paragraph{\textbf{Performance Metrics.}} 
The primary metric we will employ is:

\[
\text{RMSE} = \sqrt{\int_{\mathbf{x} \in A} \big( d(\mathbf{x}_t, \mathbf{x}) \big)^2 \, p(\mathbf{x}|\mathbf{y}^*, D) \, d\mathbf{x}},
\]
\noindent
where \( d(\mathbf{x}_t, \mathbf{x}) \) is the great circle distance. 
Comparing RMSE across different GP models helps identify which model minimizes large prediction errors and provides overall reliable estimates.

\subsection{Data Valuation}
Data valuation is crucial for understanding the contribution of individual training samples to the performance of trained models for timber provenance. The  Shapley value, introduced by \cite{shapley1953value}, offers a principled approach to quantify this contribution, identifying both highly informative and potentially detrimental data points. The Shapley value, $\phi_i$, for a data point $i$ is computed as the weighted average of its marginal contribution to model performance across all possible subsets of the training data: 
$$\phi_i = \sum_{S \subseteq D \setminus \{z_i\}} \frac{|S|!(|D| - |S| - 1)!}{|D|!} [v(S \cup \{z_i\}) - v(S)].$$
Here, $D$ is the full training set, $S$ is a subset excluding $i$, and $v(S)$ is the model performance (e.g., negative mean absolute error) when trained on subset $S$. High positive Shapley values indicate highly valuable data points that significantly improve performance, while low or negative values suggest redundancy or detrimental effects, possibly due to outliers, measurement errors, or model misspecification. The Shapley value is not an arbitrary metric; it is uniquely characterized by satisfying a set of desirable axioms, ensuring fairness and consistency in data valuation:

\begin{enumerate}
\item   \textbf{Efficiency:} The sum of the Shapley values for all data points equals the difference in performance between the model trained on the full dataset and the model trained on an empty dataset: $\sum_{i \in D} \phi_i = v(D) - v(\emptyset)$. This means the total value is fully distributed among the data points.
\item   \textbf{Symmetry (or Null Player):} If a data point $i$ has zero marginal contribution to every possible subset (i.e., $v(S \cup \{i\}) = v(S)$ for all $S$), then its Shapley value is zero: $\phi_i = 0$. Useless data points receive zero value.
\item   \textbf{Linearity:} If the performance metric $v$ is a linear combination of two other performance metrics, $v = a \cdot v_1 + b \cdot v_2$, then the Shapley values for $v$ are the same linear combination of the Shapley values for $v_1$ and $v_2$. This ensures consistency across different performance measures.
\item   \textbf{Dummy:} if two data points $i$ and $j$ always have the same marginal contribution to every subset of $D$ then their Shapley value must be equal. $\phi_i = \phi_j$.

\end{enumerate}
These axioms provide a strong theoretical justification for using the Shapley value. Furthermore, the Shapley value  can be equivalently expressed as a sum over permutations of the dataset \citep{shapley1953value}:
$$
\phi_i = \sum_{\pi \sim \Pi(D)} \left[ v(P^{\pi}_i \cup z_i) - v(P^{\pi}_i ) \right],
$$
where $\Pi(D)$ is the set of all permutations of data points in $D$, $\pi$ is a permutation sampled uniformly at random from $\Pi(D)$, and $P^{\pi}_i$ is the set of data points preceding instance $z_i$ in permutation $\pi$.

\textbf{Truncated Monte Carlo Shapley Value.} The exact computation of Shapley values is computationally prohibitive due to the factorial term, requiring evaluation of $2^{|S|}$ subsets. We address this shortcoming by employing the Truncated Monte Carlo Shapley estimation. This method, proposed in \cite{ghorbani2019data}, approximates the Shapley values by randomly sampling a limited number of subsets instead of exhaustively considering all possibilities. The key idea is that each random permutation provides an unbiased estimate of the marginal contribution of each data point. By averaging these estimates over multiple permutations, the approximation converges to the true Shapley values. The number of iterations controls the trade-off between computational cost and accuracy. Algorithm~\ref{alg:tmc shapley} shows each step in detail.

By employing the data Shapley value, particularly with the Truncated Monte Carlo approximation, we can gain valuable insights into the training data, identify influential samples, diagnose potential issues, and ultimately improve the accuracy and robustness of their GP-based timber provenance models. This data-centric approach, grounded in a strong axiomatic foundation, complements model-centric evaluations and facilitates a more efficient use of resources for data collection and model refinement.

\begin{algorithm}
        \caption{Truncated Monte Carlo Shapley \cite{ghorbani2019data}.}
        \label{alg:tmc shapley}
        \begin{algorithmic}
        \Require datasets $D$ and $T$, function $h$, performance metrics $v$
        \Ensure Shapley values $\phi_i, \forall i$ 
        
           \noindent $\text {Initialize } \phi_i=0, \forall i \text { and } t=0$
           \While{ not converged  }\\
                $t \leftarrow t+1$\\
                $\pi^t \leftarrow \text { Random permutation of train data points }$\\
                $v_0^t \leftarrow v(h, \emptyset, T)$
                \ForAll  {$j=1$ to $N$}
                    \If {$\text{ if }\left|v(D)-v_{j-1}^t\right|<\text { Performance Tolerance}$}
                    \State  $v_j^t=v_{j-1}^t$ 
                    \Comment{Reuse Previous Subset Performance}
                    \Else
                    \State  $v_j^t= v\left(h, P^{\pi^t}_{j} \cup \pi^t[j], T\right)$
                    \Comment{Subset Performance}
                    \EndIf
                    \State $\phi_{\pi^t[j]} \leftarrow \frac{t-1}{t} \phi_{\pi^{t}[j]}+\frac{1}{t}\left(v_j^t-v_{j-1}^t\right)$ \Comment{Update Value}
                \EndFor
           \EndWhile\\
           \Return{$\phi_i, \forall i$}
        \end{algorithmic}
    \end{algorithm}

\subsection{Data Selection with Shapley Values}
Acquiring high-quality isotope values for model training is a significant expense. Furthermore, standard data valuation techniques reveal a critical issue: not all data points contribute equally to predictive performance. In fact, some points may even be detrimental. Therefore, our objective is to strategically select a subset $D' \subseteq D$ from the original training data $D$ that maximizes model accuracy for both forward and backward prediction tasks. We propose to leverage Shapley values computed once on the full dataset to identify data point importance. We hypothesize that by using these initial global valuations, we can efficiently identify and sequentially remove less valuable samples. Our data selection methodology consists of three distinct steps. Initially, we compute data values for all points in $D$ using the entire dataset. Subsequently, we sort the data points and remove the least valuable one according to these pre-calculated values. Finally, we evaluate the model's performance using the test dataset. This sequential removal process continues as long as model performance improves, relying on the initial single valuation. Please refer to Algorithm~\ref{alg:data select} for a detailed algorithmic description. This simplified data selection method offers a computationally efficient approach to valuation, aiming to maximize model performance while minimizing the need for costly and potentially redundant data acquisition through a single valuation and sequential removal strategy.

\begin{algorithm}
        \caption{Data Selection with Iterative Shapley.}
        \label{alg:data select}
        \begin{algorithmic}
        \Require full datasets $D$ and $T$, function $h$, performance metrics $v$
        \Ensure selected training dataset $D'$
        
           \noindent $\text {Initialize } D' \leftarrow D$
           \State $\phi_i \leftarrow \text{Compute Data Values for }D'$ \Comment{Algorithm~\ref{alg:tmc shapley}}
           \While{ $v(h, D', T)$ not converged  } 
                \State $\text{min\_i} \leftarrow \arg \min_i \phi_i$ \Comment{Find $z_i$ with Lowest Value}
                \State $D' \leftarrow D' \setminus \{z_{\text{min\_i}}\}$ \Comment{Remove $z_i$ with Lowest Value}
           \EndWhile\\
           \Return{$D'$}
        \end{algorithmic}
    \end{algorithm}

\vspace{-1\baselineskip}
\section{Experiments}
Our experiments are aimed at answering the below questions:

\begin{enumerate}
    \item RQ1: What is the role of data Shapley values in the domain of SIRA? (Section \ref{ssec:0_dist})
    \item RQ2: How does model architecture influence data Shapley values and the performance of the proposed data valuation framework? (Section \ref{ssec:1_gp_rf})
    \item RQ3: Can the proposed data valuation framework enhance outcomes in cases involving data imputation for missing or noisy data? (Section \ref{ssec:2_imputation})
    \item RQ4: Can data selection methods based on data valuation improve the performance of both directions in SIRA? (Section \ref{ssec:3_direction})
    \item RQ5: Given a specific model and data valuation framework, what level of granularity is optimal for effective data selection? (Section \ref{ssec:4_clustering})
    \item RQ6: Do different genera and species within the dataset exhibit varying data Shapley values? Can we identify the most and least important genera or species within the dataset? (Section \ref{ssec:5_species})
\end{enumerate}

\subsection{Datasets}
We utilized data from two datasets of the genus {\it Quercus}, collected from various regions worldwide. Two broad datasets were combined in this study
with the first dataset comprising
tree samples distributed globally, while the second dataset 
was
focuses specifically on European countries. 
Stable isotope ratio measurements were performed following the protocols outlined in \citep{watkinson2020development, boner2007stable}. 
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\linewidth]{figures/usa/dist_combined_rev_fwd.png}
    \vspace{-1\baselineskip}
    \caption{Distribution of data Shapley values (top: backward model, bottom: forward model) (Section \ref{ssec:0_dist})}
    \label{fig:dist}
\end{figure}
\begin{figure*}[t]
\centering

% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/gp_error.png}
        \caption{Error plot for Gaussian process regression}
        \label{fig:gp_error}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/rf_error.png}
        \caption{Error plot for random forest}
        \label{fig:rf_error_gp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \raisebox{0.20\height}{\includegraphics[width=\linewidth]{figures/usa_gp/gp_map_white.png}}
        \caption{Data selection on a map with Gaussian process regression (perturbed for privacy)}
        \label{fig:map_gp}
    \end{subfigure}
    % }
    
% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/overlap.png}
        \caption{Data Shapley rank overlap with Gaussian process regression and random forest}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/scatter.png}
        \caption{Data Shapley rank scatter-scatter plot with Gaussian process regression and random forest}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/jaccard.png}
        \caption{Data Shapley rank Jaccard similarity with Gaussian process regression and random forest}
    \end{subfigure}
    % }
    \vspace{-1\baselineskip}
    \caption{Effect of model architecture on data valuation framework (Section \ref{ssec:1_gp_rf})}
    \label{fig:gp_rf}
\end{figure*}

\begin{figure*}[htbp]
\centering

% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/491/error_rev_flat.png}
        \caption{Error plot for median-based imputation}
        \label{fig:error_median}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/162/error_rev_flat.png}
        \caption{Error plot for listwise deletion}
        \label{fig:error_listwise}
    \end{subfigure}
    % }
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/491/dist_rev.png}
        \caption{Data Shapley value distribution for median-based imputation}
        \label{fig:median_dist}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/162/dist_rev.png}
        \caption{Data Shapley value distribution for listwise deletion}
        \label{fig:listwise_dist}
    \end{subfigure}
    
    \vspace{-1\baselineskip}    
    \caption{Enhancing missing data strategies through data valuation (Section \ref{ssec:2_imputation})}
    \label{fig:imputation}
\end{figure*}

\begin{figure*}[t]
\centering

% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/491/unimportant.png}
        \caption{Error plot for removing low-value data points}
        \label{fig:location_unimp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/491/important.png}
        \caption{Error plot for removing high-value data points}
        \label{fig:location_imp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
            \includegraphics[width=\linewidth]{figures/491/species_dist.png}
          \caption{Data Shapley value distribution per species (Section \ref{ssec:5_species})}
          \label{fig:species_dist}
    \end{subfigure}
    % }
    \vspace{-1\baselineskip}
    \caption{Optimal Granularity in data selection (Section \ref{ssec:4_clustering}) and Data Shapley value distribution per species (Section \ref{ssec:5_species})}
    \label{fig:clustering}
\end{figure*}

\begin{figure*}[t]
\centering

% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa/error_rf_reverse.png}
        \caption{Error plot for backward direction}
        \label{fig:usa_rf_reverse}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa_gp/rf_error.png}
        \caption{Error plot for random forest on forward direction}
        \label{fig:usa_rf_fwd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \raisebox{0.20\height}{\includegraphics[width=\linewidth]{figures/usa/map_rf_reverse_white.png}}
        \caption{Data selection on USA map with backward direction (perturbed for privacy)}
        \label{fig:usa_map_rf}
    \end{subfigure}
    % }
    
% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa/overlap.png}
        \caption{Data Shapley rank overlap for forward vs backward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa/scatter.png}
        \caption{Data Shapley rank scatter-scatter plot for forward vs backward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/usa/jaccard.png}
        \caption{Data Shapley rank Jaccard similarity for forward vs backward direction}
    \end{subfigure}
    % }
    \vspace{-1\baselineskip}
    \caption{Random Forest-based data valuation on USA only data (Section \ref{ssec:3_direction})}
    \label{fig:direction_usa}
\end{figure*}

\begin{figure*}[t]
\centering

% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/europe/error_reverse.png}
        \caption{Error plot for backward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/europe/error_fwd.png}
        \caption{Error plot for random forest on forward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.25\linewidth}
        \centering
        \includegraphics[trim={2cm 2cm .5cm 4cm},clip, width=\linewidth]{figures/europe/map_reverse.png}
        \caption{Data selection on the map of Europe with backward direction (perturbed for privacy)}
    \end{subfigure}
    % }

    
% \resizebox{0.9\linewidth}{!}{
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/europe/overlap.png}
        \caption{Data Shapley rank overlap for forward vs backward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/europe/scatter.png}
        \caption{Data Shapley rank scatter-scatter plot for forward vs backward direction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/europe/jaccard.png}
        \caption{Data Shapley rank Jaccard similarity for forward vs backward direction}
    \end{subfigure}
    % }
    \vspace{-1\baselineskip}
    \caption{Random Forest-based data valuation on Europe only data (Section \ref{ssec:3_direction})}
    \label{fig:direction_eu}
\end{figure*}

\subsection{Data Valuation and Selection}
\label{ssec:implementation}
As described in Section \ref{sec:methods}, the objective of data valuation is to strategically select subsets of the dataset that maximize model performance. Instead of solely selecting the most valuable data points, we implement an iterative approach. Given a dataset $D$ consisting of data points $(z_1, z_2, \dots, z_i)$ and their corresponding data Shapley values $(\phi_1, \phi_2, \dots, \phi_i)$, we conduct experiments by removing the low value data points $z_k$, where the data Shapley value $\phi_k$ satisfies $\phi_k < \phi_j \quad \forall j \neq k$, or equivalently, $\phi_k = \min_{j} \phi_j$. Furthermore, we extend our experiments by investigating the opposite case: removing the high-value data points $z_m$, where the Shapley value $\phi_m$ satisfies $\phi_m > \phi_j \quad \forall j \neq m$, or equivalently, $\phi_m = \max_{j} \phi_j$. We then evaluate model performance and analyze how the data valuation framework influences the overall model behavior. We report the root mean square error (RMSE) for all performance plots, defined as $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$, where $y_i$ and $\hat{y}_i$ represent the actual and predicted multivariate outputs involving variables $\mathcal{X}$ or $\mathcal{Y}$, on both forward and backward directions.

\subsection{RQ1: Role of Data Shapley Values in SIRA}
\label{ssec:0_dist}
For this experiment, we explore the relevance and application of data Shapley values in the context of SIRA. 
Fig.~\ref{fig:dist} 
presents distribution plots for one subset of the dataset, evaluated in both directions of SIRA analysis, i.e., forward and backward, as described in Section \ref{sec:methods}. This analysis highlights a significant variation in data Shapley values depending on the dataset and the machine learning model utilized. These results indicate that subsets with Shapley values exhibiting greater extremities will correspond to larger performance gains following the data selection process. 

\subsection{RQ2: Influence of Model Architecture}
\label{ssec:1_gp_rf}
We conducted two types of experiments, removing both high value and low value data points, as described in Section \ref{ssec:implementation}. We present the error plots showing the effect of removing these two types of data points for Gaussian process regression and random forest models. In both models, the RMSE increases (indicating performance degradation) when data points with high Shapley values are removed. Similarly, the RMSE decreases (indicating performance improvement) or remains stable when data points with low Shapley values are removed (Figure \ref{fig:gp_error} and Figure \ref{fig:rf_error_gp}). Figure \ref{fig:map_gp} illustrates one such case, where performance improves by removing specific data points (marked as red x on the map).

We further investigate whether the Shapley values from two different models agree with each other. We present three types of rank comparison plots in Figure \ref{fig:gp_rf}(d-f). All three representations demonstrate a high degree of agreement between the two models in this experiment, indicating that data value ranks can remain consistent across different model architectures.

\subsection{RQ3: Data Valuation to support Missing Data Imputation}
\label{ssec:2_imputation}
We conduct experiments with two types of approaches for handling missing data: \textit{(i)} median imputation, where missing values are replaced with the median of the dataset, and \textit{(ii)} listwise deletion, where data points containing missing values are excluded entirely from the training process. Additionally, we perform data selection experiments by removing both high-value and low-value data points and comparing the corresponding performance rankings.

We conducted experiments involving the removal of both high-value and low-value data points, as described in Section \ref{ssec:implementation}. The error plots (Figures \ref{fig:error_median} and \ref{fig:error_listwise}) illustrate the effects of removing these data points under both missing data handling strategies. For both strategies, the RMSE increases (indicating performance degradation) when data points with high Shapley values are removed. Conversely, the RMSE decreases (indicating performance improvement) or remains stable when data points with low Shapley values are removed.

We observe significant performance improvements when low-value data points are removed from both strategies, with the improvement being more pronounced for median imputation (26.66\%) compared to listwise deletion (5.88\%). This result demonstrates the effectiveness of data valuation as a strategy for improving missing data handling. Moreover, the Shapley values for the median imputation strategy exhibit higher magnitudes compared to listwise deletion (Figures \ref{fig:median_dist} and \ref{fig:listwise_dist}), which aligns with the greater performance improvements observed for median imputation.

\subsection{RQ4: Data Selection Methods for Forward and Backward directions in SIRA}
\label{ssec:3_direction}
In this experiment, we investigate whether selecting data based on valuation metrics can improve performance in both directions of SIRA. For this analysis, we present the results of applying the random forest model in both forward and backward directions using two distinct datasets: USA-only data and Europe-only data.

\textbf{USA-only data:} We present the error plots showing the effect of removing high-value versus low-value data points on the performance of the random forest model. In both directions, the RMSE increases (indicating performance degradation) when data points with high Shapley values, are removed. Similarly, the RMSE either decreases (indicating performance improvement) or remains stable when data points with low Shapley values, are removed (Figures \ref{fig:usa_rf_reverse} and \ref{fig:usa_rf_fwd}). Figure \ref{fig:usa_map_rf} illustrates a specific case where performance improves following the removal of certain data points, marked as red x on the map.

We observe performance improvements when low-value data points are removed from the dataset, with the improvement being more pronounced in the backward direction (27.13\%) compared to the forward direction (0.14\%). This finding demonstrates that performance improvement can indeed vary depending on the direction of SIRA. Moreover, consistent with the observations made in Section \ref{ssec:2_imputation}, the Shapley values associated with the backward direction exhibit higher magnitudes compared to the forward direction (Figure \ref{fig:dist}), which aligns with the greater performance improvements observed for the backward direction.

We further investigate whether the Shapley values from both directions show agreement. To this end, we present three types of rank comparison plots in Figure \ref{fig:direction_usa}(d-f). All three representations indicate a high degree of agreement between the two directions, suggesting that data value ranks remain relatively consistent across different directions of SIRA.

\textbf{Europe-only data:} We conduct the same set of experiments and analyses on the Europe-only dataset. Similar patterns of performance improvement and deterioration are observed when low-value and high-value data points are removed, respectively (Figure \ref{fig:direction_eu}). In the backward direction, performance improved by 5.08\%, while in the forward direction, it improved by 7.81\%. 

We also compared the Shapley value ranks of the data points. For the Europe-only dataset, the similarities were less ideal than other cases, indicating that the ranking of data importance can differ across directions in different datasets. Figure \ref{fig:direction_eu}(d-f) illustrates the corresponding plots for this case.

\subsection{RQ5: Optimal Granularity in Data Selection}
\label{ssec:4_clustering}
Here, we analyze the appropriate level of granularity for data selection to maximize performance when using a specific model and a data valuation framework. Instead of removing one data point at a time, we consider clusters of fixed distances (in kilometers) and remove all data points within that distance if a data point is selected for removal during the data selection process. We performed this location-based data selection for both high value (Figure \ref{fig:location_imp}) and low value (Figure \ref{fig:location_unimp}) data points to understand the impact on model performance.

Consistent with the results of previous experiments, we observe performance improvements when low-value data points are removed. Furthermore, the cluster-based removal approach enhances the model's performance more effectively than the one-by-one removal approach. This finding suggests that certain regions within the dataset are not essential for the best-performing model, or the improvements may be attributed to errors in the data extraction pipelines affecting specific regions.

\subsection{RQ6: Species-Specific Data Shapley Values and Their Implications}
\label{ssec:5_species}
For this experiment, we explore the variations in data Shapley values across different genera and species, aiming to identify the most and least significant contributors within the dataset. We present a sample distribution plot of data Shapley values for individual species within one dataset (Figure \ref{fig:species_dist}). The plot demonstrates that certain species exhibit significantly higher data Shapley values compared to others, indicating their greater contribution to the model's performance.

\section{Description of Deployment}
As described earlier, our approach is part of
a deployed ML pipeline that supports provenance identification for timber products. 
Our framework is in use by European enforcement agencies to assist claims verification of wood suspected of being in violation of sanctions against Russia by demonstrating that a claimed harvest location other than Russia is not viable. See coverage of our work from the \textit{New York Times} \cite{nytimes_nazaryan_2024}. 
Due to confidentiality reasons,
we have presented results on a {\it Quercus} dataset.

\section{Conclusion and Future Work}
Data valuation based on Shapley values demonstrates
promising results in SIRA analytics.
Additionally, region-based data selection has been shown to further enhance model performance by effectively removing low-value data clusters. Our analysis highlights that the greatest performance improvements are observed when low-value data points with higher absolute Shapley values are removed, and similarly, larger performance drops occur when high-value data points with higher absolute Shapley values are excluded. This demonstrates that data values inferred by our approach are meaningful and can be contextualized for use in product provenance verification. Future work is aimed at generalizing data valuation methodologies to span more parts of natural resource supply chains.


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
