\section{Literature Review}
\label{sec:kg-survey}
In this section, we provide a review on knowledge grounding, contributions of LLMs in finance, and popular financial datasets for financial text mining. We also talk about the limitations of some of the popular knowledge grounding approaches when applied to the area of finance.

\begin{figure}[t]
  \centering %{.46\textwidth}
  \includegraphics[width=.9\textwidth]{Financial_Agent_Example.pdf}
  \caption{Working of the proposed architecture: An example}
  \label{fig:Financial Agent-exmp}
\end{figure}

\subsection{Knowledge Grounding of LLMs}
The usual strategy for broadening the applicability of an LLM to a particular downstream task has conventionally entailed refining the model's parameters. This involves the process of training certain or all layers of the LLM on a bespoke dataset tailored to the specific requirements of the given downstream task \citep{radford2019language}. However, these strategies exhibit a considerable computational cost, especially if the data is dynamic and the model has to be updated frequently. %Moreover, they lead to the proliferation of modified model copies, each dedicated to a distinct downstream task, resulting in substantial storage demands. Given the large dimensions of contemporary LLMs, this approach becomes markedly impractical from both computational and resource allocation perspectives.
The fundamental rationale for knowledge grounding in LLMs resides in the motivation to equip these models with the faculties of reasoning and contextual comprehension based on relevant external data, as opposed to them being regarded as repositories of static knowledge.

In recent years, a plethora of viable alternatives have emerged, offering potential enhancements to the conventional finetuning methodology.
\cite{wei2022chain} introduced ``chain-of-thought'' (CoT), a few-shot prompting technique for LLMs. CoT employs sequential examples within a prompt, comprising task inputs and intermediate steps, to drive contextual understanding and reasoning in large models. \cite{brown2020language} demonstrated GPT-3's ability to learn complex tasks in a few-shot setting.
But few-shot prompting requires manual effort to design the optimal prompt for any required downstream task. \cite{reynolds2021prompt} argued that few-shot learning isn't a method of task learning but rather a method of task location in the existing space of the model's learned tasks. They introduced the concept of metaprompt programming, through which the job of writing task-specific prompts can be assigned to the LLM itself. \cite{gao2020making} proposed an improvement in the few-shot strategy where rather than relying on the arbitrary selection of random examples and their inclusion in the query, which lacks a guarantee of emphasizing the most informative demonstrations, their approach involved sequential random sampling of a single example from each class for every input resulting in the creation of multiple concise demonstration sets. \cite{shin2020autoprompt} introduced AUTOPROMPT, an approach that automatically constructs prompts by combining primary task inputs with an array of trigger tokens, adhering to a predetermined template. The set of trigger tokens used is same for all the inputs and is learned through a specialized adaptation of the gradient-based search strategy. The composite prompt is then supplied as input to a Mask Language Model. It was observed that for Natural Language Inference (NLI) tasks, AUTOPROMPT was comparable to a supervised finetuned BERT model.

Another limitation inherent in the few-shot prompting methodology stems from the constrained token capacity that the LLMs can intake. This might have thousands of examples that we need the LLM to learn from. A number of parameter efficient fine-tuning methodologies have been proposed to tackle this issue. 
\cite{liu2021gpt} proposed P-Tuning, a technique where the prompt tokens in the input embedding (containing context tokens and prompt tokens) are treated as pseudo tokens and mapped as trainable embedding tensors. This continuous prompt is modelled using a prompt encoder consisting of a bidirectional LSTM and is then optimized using the downstream loss function. Another unique approach is Prefix Tuning \citep{li2021prefix}, where a sequence of continuous task-specific vectors are prepended to the input of an autoregressive LM, or to both the encoder and decoder layers of a Encoder-Decoder Model. The prefix consists of trainable parameters which do not correspond to real tokens in a model's embedding. Instead of training the model's parameters on the loss function, only the parameters in the prefix are optimized. A similar methodology was used in Prompt Tuning \citep{lester2021power}, but without any intermediate-layer prefixes. \cite{dettmers2024qlora} proposed QLoRA, which introduces a memory-efficient finetuning method for LLMs by quantizing the pre-trained model to 4-bits and using Low Rank Adapters (LoRA), achieving comparable performance to 16-bit fine-tuning with reduced memory

An alternate approach for the knowledge grounding of LLMs involves equipping them with a retriever module that can access information relevant to the user's query from a database. \cite{lewis2020retrieval} introduced a retrieval-augmented generation (RAG) model, where they integrated a pre-trained retriever module with a pre-trained seq2seq model. The retriever provides latent documents conditioned on the input, and the {\it seq2seq} model then conditions on these latent documents together with the input to generate the output. \cite{dinan2018wizard} designed a dialogue model, where within the conversational framework, the chatbot is equipped with access to a curated collection of passages that maintain relevance to the ongoing discourse. At each turn, using a standard information retrieval system, the chatbot retrieves the top 7 articles for the last two turns of conversation. An attention mechanism is used to perform refined selection of specific sentences that will be used to create the next response. 
\cite{izacard2022few} created ATLAS, a RAG Model which is capable of few-shot learning.
\cite{peng2023check} presented LLM-Augmenter, a module which, in addition to knowledge retrieval and prompt generation, checks the response generated by a fixed LLM for hallucinations. If the response is not correct, it generates a feedback message which is used to improve the prompt. This cycle continues until the response by the LLM is verified. \cite{li2021knowledge} used a three step data cleaning procedure to supply an LLM with relevant context: retrieving associated triples from a knowledge graph, finding related triples by computing cosine similarity between the triples and the query, further refining the choices by estimating the semantic similarity score. 

Despite significant advancements in knowledge grounding of LLMs, efficient deployment of them for real-time processing of enormous data flowing at high frequency, such as financial data, remains a challenge. 
%It is obvious that most of the studies discussed above will not be able to handle the dynamic nature of financial dataset.

\subsection{Limitations of Traditional RAG Models in Retrieving Financial Data}
Recent years have witnessed a surge in the development and application of Retrieval-Augmented Generation (RAG) systems. By combining the power of large language models (LLMs) with information retrieval techniques, RAG systems have the potential to revolutionize various industries. However, despite their promise, these systems still face several limitations that hinder their widespread adoption, particularly in complex and regulated domains. 

One of the core challenges in RAG systems lies in the effectiveness of the retrieval mechanism. As highlighted by \cite{gupta2024comprehensive}, while powerful, the retrieval process can struggle with ambiguous queries and niche knowledge domains. The reliance on dense vector representations can sometimes lead to the retrieval of irrelevant documents, which can negatively impact the quality of the generated response. \cite{cuconasu2024power} further emphasized this point, demonstrating that the highest-scoring retrieved documents which are not directly relevant to the query, lead to a decline in LLM effectiveness. Moreover, the quality of generated output can be influenced by the number of retrieved passages, as outlined in \cite{jin2024long}. While increasing the number of passages can initially improve the quality, it can eventually lead to a decline in performance.

When integrating knowledge graphs (KGs) into RAG systems, challenges arise in effectively utilizing the structured information. 
\cite{agrawal2024mindful} identified several critical failure points in existing KG-based RAG methods, including insufficient focus on question intent and inadequate context gathering from KG facts. The real-world applications of RAG systems, especially in expert domains, are often characterized by complex and nuanced requirements. As noted by \cite{zhao2024retrieval}, the one-size-fits-all approach to data augmentation may not be suitable for all scenarios. In compliance-regulated sectors, RAG systems must adhere to stringent data privacy, security, and governance requirements. 
\cite{bruckhaus2024rag} highlighted the importance of ensuring that sensitive data is not exposed or misused during the retrieval and generation process. This is crucial when working with enterprises in compliance-regulated sectors. Dealing with financial data also presents certain challenges to traditional RAG based systems, as discussed below:
\begin{enumerate}
    \item Difficulties in Representing Financial Data: Financial data is predominantly stored in structured, tabular formats, such as spreadsheets and databases. These formats prioritize organization, analysis, and manipulation of data points. While advantageous for financial tasks, this structured format presents difficulties for RAG models. Unlike textual data, tables lack inherent relationships and reasoning patterns that RAG models can readily exploit through their natural language processing capabilities. Converting vast financial datasets into textual representations for RAG models would be impractical and inefficient. Textual storage introduces significant redundancy and increases the risk of errors or inconsistencies during data manipulation. Moreover, the structured format of financial data tables ensures data integrity and facilitates efficient retrieval of specific data points.
    \item Challenges in Retriever Accuracy: A core component of RAG models, the retriever, is tasked with fetching relevant information from the knowledge base to answer a user query. When dealing with financial data, the retriever faces a unique challenge. Financial queries often involve specific financial metrics, time-frames, and financial companies. The retriever must be highly selective in its retrieval process, ensuring it gathers only the precise data points required to answer the query accurately. Incomplete data retrieval can lead to inaccurate or misleading answers, while retrieving excessive data burdens the reasoning component with unnecessary information processing.
    \item Inability to handle high velocity, high volume data: RAG models typically rely on static databases and pre-trained retrieval mechanisms, which struggle to keep pace with the rapid influx and dynamic nature of financial data. Additionally, the need for frequent updates and high-latency responses in financial contexts can overwhelm traditional RAG architectures, leading to delayed and potentially outdated outputs.
\end{enumerate}

\begin{table}[!h]
\footnotesize
    \centering
    \caption{Summary of Financial Datasets and Resources}
    \begin{tabular}{|p{0.15\textwidth}|p{0.6\textwidth}|p{0.15\textwidth}|}
        \hline
        \textbf{Resource} & \textbf{Description} & \textbf{Reference} \\ \hline
        SentiWordNet & Establishes a lexical resource based on WordNet, assigning sentiment scores (objective, positive, negative) to individual word senses (synsets). & \cite{baccianella2010sentiwordnet} \\ \hline
        Financial Phrase Bank & Contains almost 5,000 snippets of text, gathered from financial news and press releases about Finnish companies traded on the NASDAQ OMX Nordic Exchange. Each snippet is labeled as positive, negative, or neutral. & \cite{malo2014good} \\ \hline
        SenticNet & A dataset for understanding sentiment in financial texts, combining artificial intelligence and Semantic Web technologies to better recognize, interpret, and process opinions within this context. & \cite{cambria2012senticnet} \\ \hline
        SEntFiN 1.0 & Incorporates entity-sentiment annotation to address the challenge of headlines containing multiple entities with potentially conflicting sentiments. This human-annotated dataset of over 10,700 news headlines provides insights into sentiment variations depending on specific entities in financial news. & \cite{sinha2022sentfin} \\ \hline
        Trillion Dollar Words & A dataset of tokenized and annotated Federal Open Market Committee (FOMC) speeches, meeting minutes, and press conference transcripts. This resource aims to understand the influence of monetary policy on financial markets by analyzing the language used by central bank. & \cite{shah2023trillion} \\ \hline
        REFinD & An annotated dataset of relations within financial documents with approximately 29,000 instances and 22 relation types across eight entity pairs (e.g., person-title, org-money), extracted from SEC 10-X filings. Useful for identifying relationships between entities in financial reports. & \cite{kaur2023refind} \\ \hline
        Gold Commodity Dataset & Compiled from diverse news sources and evaluated by human experts, allowing researchers to explore how news headlines influence price movements, asset comparisons, and other financial events. & \cite{sinha2021impact} \\ \hline
        FiNER & A dataset designed for Named Entity Recognition (NER) within the financial landscape, aiding in identifying and classifying financial companies within text data. & \cite{shah2023finer} \\ \hline
        MULTIFIN & Provides a multilingual financial NLP dataset with real-world financial article headlines in 15 languages. Annotated with high-level and low-level topics, facilitating multi-class and multi-label classification tasks. & \cite{jorgensen2023multifin} \\ \hline
        FINQA & A collection of question-answer pairs with in-depth analysis of financial reports. Written by financial experts, it focuses on deep reasoning over financial data, providing training data for answering complex financial queries. & \cite{chen2021finqa} \\ \hline
    \end{tabular}
    \vspace{-5mm}
    \label{tab:financial-datasets}
\end{table}

\subsection{Financial Datasets Review}   
The domain of financial language processing has witnessed a significant rise in the development of annotated datasets, each serving distinct purposes within the field. Through Table~\ref{tab:financial-datasets}, we have highlighted some prominent examples that have been widely used in the area of finance for training models. In the next sections, we discuss the core contributions of the paper in detail. The key contribution is the creation of a Financial Agent that is capable of understanding the context needed for a user query, extracting the relevant data from the databases, and supporting an LLM to handle the contextualized query effectively.

\begin{comment}
\begin{enumerate}
    \item SentiWordNet (\cite{baccianella2010sentiwordnet}) establishes a lexical resource based on WordNet, assigning sentiment scores (objective, positive, negative) to individual word senses (synsets).
    \item The Financial Phrase Bank (\cite{malo2014good}) contains almost 5,000 snippets of text, gathered from financial news and press releases about Finnish companies traded on the NASDAQ OMX Nordic Exchange. Each snippet is labeled as positive, negative, or neutral.
    \item SenticNet (\cite{cambria2012senticnet}) is another dataset for understanding sentiment in financial texts. It combines artificial intelligence and Semantic Web technologies to better recognize, interpret, and process people's opinions within this specific context.
    \item SEntFiN 1.0 (\cite{sinha2022sentfin})  incorporates entity-sentiment annotation, addressing the challenge of headlines containing multiple entities with potentially conflicting sentiments. This human-annotated dataset of over 10,700 news headlines provides valuable insights into how sentiment can vary depending on the specific entities mentioned in financial news.
    \item Trillion Dollar Words (\cite{shah2023trillion}) provides a massive dataset of tokenized and annotated Federal Open Market Committee (FOMC) speeches, meeting minutes, and press conference transcripts. This resource aims to understand the influence of monetary policy on financial markets by analyzing the language used by central bank officials.
    \item REFinD (\cite{kaur2023refind}) is a large-scale annotated dataset of relations within financial documents. It encompasses approximately 29,000 instances and 22 relation types across eight entity pairs (e.g., person-title, org-money), extracted from SEC 10-X filings. This dataset proves invaluable for tasks like identifying relationships between entities within financial reports.
    \item The Gold Commodity Dataset (\cite{sinha2021impact}) is compiled from diverse news sources and evaluated by human experts, and allows researchers to explore how news headlines influence price movements (direction, future vs. past), asset comparisons, and other financial events.
    \item FiNER (\cite{shah2023finer}) is a dataset especially designed for Named Entity Recognition (NER) within the financial landscape, helping researchers and developers to effectively identify and classify financial companies within text data.
    \item MULTIFIN (\cite{jorgensen2023multifin}) addresses the need for multilingual financial NLP models by providing a publicly available dataset of real-world financial article headlines in 15 languages. This dataset is annotated with both high-level and low-level topics, facilitating multi-class and multi-label classification tasks. 
    \item FINQA (\cite{chen2021finqa} )presents a large-scale collection of question-answer pairs requiring in-depth analysis of financial reports, written by financial experts. FINQA focuses on deep reasoning over financial data through complex  question-answer pairs, hence providing valuable training data for developing models capable of answering complex financial queries.
\end{enumerate}
\end{comment}