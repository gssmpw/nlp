\documentclass[twoside,11pt]{article}
%\documentclass[final,3p,times]{elsarticle}
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{lscape}
\usepackage{caption}
\usepackage{array}
\usepackage{lmodern}
\usepackage{multirow}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{url}

\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
% \usepackage{titlesec}

% \setcounter{secnumdepth}{4}

% \titleformat{\paragraph}
% {\normalfont\normalsize}{\theparagraph}{1em}{}
% \titlespacing*{\paragraph}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

%\ShortHeadings{Knowledge Grounding Large Language Model}{}
%\firstpageno{1}

\begin{document}

%\begin{frontmatter}

\title{FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data}

\author{\name Ankur Sinha \email asinha@iima.ac.in \\
       \addr Department of Operations and Decision Sciences\\
       Indian Institute of Management Ahmedabad\\
       Gujarat, India 380015
       \AND
       \name Chaitanya Agarwal \email chaitanyaa@iima.ac.in \\
       \addr Brij Disa Centre for Data Science and AI\\
       Indian Institute of Management Ahmedabad\\
       Gujarat, India 380015
       \AND
       \name Pekka Malo \email pekka.malo@aalto.fi \\
       \addr Department of Information and Service Economy\\
       Aalto University School of Business\\
       Helsinki, Finland}

%\title{FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data}

%\author[iima]{Ankur Sinha\corref{cor1}}
%\ead{asinha@iima.ac.in}

%\author[iima]{Chaitanya Agarwal}
%\ead{chaitanyaa@iima.ac.in}

%\author[aalto]{Pekka Malo}
%\ead{pekka.malo@aalto.fi}

%\cortext[cor1]{Corresponding author}

%\address[iima]{Department of Operations and Decision Sciences, Indian Institute of Management Ahmedabad, Gujarat, India 380015}

%\address[aalto]{Department of Information and Service Economy, Aalto University School of Business, Helsinki, Finland}


%\editor{Editor: None}
\editor{~}

\maketitle

\begin{abstract}
Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.
%Large language models (LLMs) are trained on massive text datasets to generate human-like responses for a number of tasks. Although these models have impressive capabilities for generating content that is convincing, they often tend to suffer while dealing with interactive tasks that require access to real-time information. For instance, in the context of finance, one would rarely like to use an LLM that is unaware of the most recent news items and movements in prices. While such models may be useful for educational purposes, one may not like to interact with the model to make real-time decisions if the model does not have access to the most recent data. In this paper, we introduce Financial Agent, where we suggest an approach for knowledge grounding of LLMs so that it can handle financial queries while accounting for the continuously flowing text and tabular data. Our approach is based on using an agent which is a custom LLM with 7 billion parameters that accesses real-time data from an external data source and builds the context needed for the larger LLM such that user queries can be handled effectively. Our contribution in this paper is multi-fold: Firstly, we create a custom Financial Context Dataset consisting of over 50,000 financial queries and the corresponding context needed to answer each query. Secondly, we train a 7 billion parameter LLM, referred to as FinBloom 7B, for performing versatile financial tasks by training on over 14 million financial news articles procured from Reuters and Deutsche Presse-Agentur (DPA). Additionally, in the training phase we also include publicly available Securities and Exchange Commission (SEC) filings data consisting of over 12 million documents. Finally, we finetune FinBloom 7B using the Financial Context Dataset, to serve as a Financial Agent, for the purpose of financial context generation for any given user query. The context supports extraction of the relevant real-time financial data which often flows at high velocity to effectively answer the user query with additional support from a larger LLM. Our approach is beneficial in reducing the latency that would occur in interacting with LLMs regarding financial queries as in such situation users often themselves search and provide accurate financial data to LLMs to answer their queries.
\end{abstract}

%The 7 then subsequently finetuned this LLM on our custom dataset allowing us to obtain the context required and subsequently extract the tabular as well as news data to answer the query. 
%We perform a user study to show how investors and analysts perceive the output of the proposed knowledge grounded approach. 
%Our proposed approach would make real-time financial decisions, algorithmic trading and other related tasks much more streamlined for the users.

%\begin{keyword}
%Financial Large Language Model \sep Generative Pre-Trained Transformer \sep Knowledge Grounding \sep Natural Language Processing (NLP)
%\end{keyword}

%\end{frontmatter}

%\maketitle

\section{Introduction}
Large Language Models (LLMs) use deep learning techniques to learn from massive text datasets to solve a variety of tasks using natural language \citep{devlin2018bert,brown2020language,scao2022bloom,zhang2022opt}. The most common tasks where these models have shown its proficiency, include, natural language generation, question-answering, reasoning, translation, summarization, etc. As the general-purpose models grow in size and learn from larger data corpus, we observe that they are get more versatile, and develop the ability to handle multiple tasks that earlier required separate specialized models. 
%However, we are still away from one-size-fits-all language models that can cater to the needs of all the areas. 
However, areas like finance, healthcare, science, law, among others, often have their own domain-specific vocabulary that necessitates finetuning of general-purpose models to carry out domain-specific tasks \citep{wu2023bloomberggpt,luo2022biogpt,taylor2022galactica}. As the LLMs get even larger, it will be interesting to see whether general-purpose LLMs can work equally well as domain-specific LLMs simply with appropriate prompts. A rather more urgent problem that needs attention is that many of the domains require access to real-time data arising from several sources to handle various tasks with high accuracy and efficiency, which is currently beyond the capability of frozen LLMs. For instance, in the context of finance, a user is likely to prefer using an LLM that is knowledge-grounded with real-time news and asset prices as they would want to make decisions based on accurate financial news and tabular data related to their query . Unless one uses a financial LLM for educational purposes, it will be of limited use as most of its response will be based on stale data that is of little value to investors or analysts. However, there are innumerable challenges when it comes to training or finetuning an LLM on real-time data, that we list below:
\begin{enumerate}
\item Training or finetuning an LLM is very costly: Financial data such as asset prices change with a high frequency and similarly news data with an ability to impact asset prices is generated continuously in various parts of the world. Training or finetuning LLMs require an update in the model parameters that is quite time consuming. Even if one attempts a small duration of finetuning based on batches of most recent numeric and text data, the LLM will still remain outdated as financial data flows at high velocity. Unless LLMs have the capability of quick online learning, knowledge grounding based on frequent training or finetuning will not entirely help.
\item Lack of knowledge on what the LLM has learned: Most of the LLMs learn from massive data corpus, but as users of the system, we do not know what the model has learned and where it may commit errors. Only an intensive testing of the models often reveal its weakness. Therefore, without a complete knowledge that LLM has learned from the new data appropriately, one may not put it to use in financial contexts where decisions have to be made based on new data.
\end{enumerate}
Handling the above challenges would be useful for any general-purpose or domain-specific LLM; therefore, there is a large body of recent research on knowledge grounding of LLMs \citep{li2021knowledge,peng2023check,carta2023grounding}. An alternative approach to solve the above problem is to keep that LLM frozen, that is, do not change or finetune the weights of the LLM, rather add modules to the LLM for knowledge grounding. The first module is the data module that provides access to real-time information that is flowing, and the second module consists of a Financial Agent that interprets the user query and extracts relevant data from the data module, thereby constructing the financial context required to answer the query. More modules or agents for video interpretation module, financial news analysis and event analysis can also be added to the framework. 

In this paper, we present a Financial Agent for grounding large language models in financial knowledge, enabling them to address financial queries effectively while managing dynamic text and tabular data streams. Our methodology leverages an external agent comprising a 7 billion parameter LLM integrated with real-time data sources to construct the contextual information required by a larger LLM (GPT 3.5) for accurate query resolution. Our contributions in the paper are as follows:
\begin{enumerate}
\item We develop a custom Financial Context Dataset\footnote{Dataset: \url{https://huggingface.co/datasets/Chaitanya14/Financial_Context_Dataset}}, containing over 50,000 financial queries paired with the corresponding contextual information needed for the resolution of the queries.
\item We train a 7 billion parameter LLM, referred to as FinBloom 7B\footnote{FinBloom 7B LLM: \url{https://huggingface.co/Chaitanya14/FinBloom_7B}}, on over 14 million financial news articles sourced from Reuters and Deutsche Presse-Agentur (DPA), equipping it with versatile capabilities for financial tasks. We also use a random sample of 25\% from 12 million documents from Securities and Exchange (SEC) filings along with news data.
\item We finetune FinBloom 7B on the Financial Context Dataset to create a Financial Agent\footnote{FinBloom 7B Agent: \url{https://huggingface.co/Chaitanya14/Financial_Agent}} that generates the context required to answer a user query and extracts data from a data module to effectively handle the financial query.
%\item The dataset and the models are being released publicly and can be accessed through the following links:
%\begin{enumerate}[leftmargin=0pt]
%\url{https://datasetAndModel.com}
%\end{enumerate}
\end{enumerate}
The proposed approach facilitates the extraction of high-velocity real-time financial data, enabling the effective resolution of user queries with support from the larger LLM (say GPT 3.5 or GPT 4). As a part of this study, we release the 50,000 Financial Context Dataset, 7 billion parameter domain-specific LLM and finetuned Financial Agent for context generation. We are unable to release the raw data, i.e. 14 million financial news articles from Reuters and Deutsche Presse-Agentur (DPA), because of contractual reasons. 
%We also release a package which integrates our contributions in this paper with other modules making the deployment of state-of-the-art artificial intelligence approaches simple for finance.

%To enable the Financial Agent to interpret the user's query and get relevant context to answer it, we built a custom dataset of over 50000 user queries and their corresponding contexts required to answer them, and then trained an LLM on this dataset which would take user's queries as inputs and then output the relevant context required to answer the query. Using the context provide by the LLM, relevant data can be extracted from the data module. The extracted data is then combined with the user query in the form of conversation history and then passed to the LLM. This allows the LLM to incorporate recent information while answering user queries.

\begin{figure}[t]
  \centering %{.46\textwidth}
  \includegraphics[width=.9\textwidth]{Financial_Agent_Architecture.pdf}
  \caption{Left hand side shows an approach without knowledge grounding and the right hand side shows that the context (relevant real-time data) is provided for knowledge grounding along with the query. The context is generated using the Financial Agent, which is the main contribution of the paper highlighted within the rectangle with dashed lines.}
  \label{fig:Financial Agent-arch}
\end{figure}

Existing approaches to knowledge grounding in LLMs follow different methodologies, which have a number of similarities, but are known in the literature with different terminologies. We address these approaches in the later part of the paper. We adopt an agent-based procedure to perform knowledge grounding keeping in mind that financial data is high volume and high velocity data for which high latency approaches may not be suitable.
In the financial domain, real-time data is frequently presented in tabular or textual formats. To accommodate this, we maintain two repositories with streaming data: a tabular database containing up-to-date prices, financial metrics, financial statements, and a text database storing the latest news. These repositories collectively form the data module, which is accessible to the Financial Agent.
When a user query ($x$) is received, the Financial Agent analyzes the query and retrieves relevant news and tabular data ($d$) from the data module. The retrieved data is then converted into a text format ($c(d)$) and appended to the user query ($x$). This combined input ($c(d), x$) is passed to the LLM as part of a conversation, enabling it to generate an informed response that incorporates the contextualized real-time data.
The system architecture is illustrated in Figure~\ref{fig:Financial Agent-arch}.

An example of the operation of the proposed architecture is provided in Figure~\ref{fig:Financial Agent-exmp}. The example in the figure demonstrates how the Financial Agent leverages knowledge grounding to process and respond to a financial query. When the user inputs the query ``Explain P/E ratio taking an example of Google,'' the Financial Agent performs contextualization by accessing the data module, which contains real-time information from tabular and textual repositories. Relevant data, such as Googleâ€™s share price (\$108.90) and earnings per share (EPS) (\$4.20), is extracted from the data module. This information is converted into a textual representation to create a context ($c(d)$) that complements the original query. The query enrichment process then appends this context to the user query, forming a consolidated input ($c(d), x$) for the LLM. The enriched input enables the LLM to generate a detailed response, such as an explanation of the P/E ratio, its significance, and a calculation example using the provided data. By integrating real-time data extraction, contextualization, and query enrichment, the Financial Agent ensures precise and contextually relevant answers, demonstrating its capability for effective knowledge grounding in dynamic financial scenarios.

%The existing approaches for knowledge grounding of LLMs follow a similar procedure; however, researchers choose to refer to these modules with different names. In this paper, we perform knowledge grounding using the stated procedure for our LLM. Note that in the context of finance, the real-time data is often in a tabular or text form. We maintain a repository of recent prices and financials in a tabular database, and a repository of recent news in a text database. This module, referred to as the data module, is accessible to the Financial Agent. Whenever, a user query ($x$) arrives, the Agent analyses the query and extracts news and tabular data ($d$) from the data module that is relevant to provide an appropriate answer to the user query. The extracted output is then converted into the form of text ($c (d)$), and is then appended to the user query ($x$). The appended tuple ($c (d),x$) is then passed to the LLM in the form of a conversation, which then answers the query by taking the relevant data into account. The architecture is explained through Figure~\ref{fig:Financial Agent-arch} and an example is provided through Figure~\ref{fig:Financial Agent-exmp}.

The structure of this paper is as follows. Section~\ref{sec:kg-survey} presents a comprehensive survey of recent advancements in knowledge grounding of LLMs and financial datasets. Section~\ref{sec:fincontextdataset} provides details on the Financial Context Dataset consisting of 50,000 queries followed by Section~\ref{sec:finbloom} where we introduce a domain-specific LLM, referred to as FinBloom 7B. An extensive comparison of FinBloom 7B is performed against other popular models. In Section~\ref{sec:FinancialAgent}, we finetune FinBloom 7B on the Financial Context Dataset and create a Financial Agent capable of context generation and data retrieval. We also discuss its working in our proposed knowledge grounding framework. Finally, conclusions and future directions for research are discussed in Section~\ref{sec:conclusions}.

%and Financial Agent, a domain-specific LLM for finance designed to leverage real-time data for effective knowledge grounding. Finetuning and experimental results are presented in Section~\ref{sec:results}, showcasing the performance and capabilities of the proposed system. 

%The paper is structured as follows. To begin with, we survey the recent work on knowledge grounding of LLMs and financial datasets in Section~\ref{sec:kg-survey}. Thereafter, in Section~\ref{sec:Financial Agent} we discuss the architecture of Financial Agent in detail, which is a domain-specific LLM for finance that is knowledge grounded with real-time data. We then provide the results from our experiments in Section~\ref{sec:results}. Finally, we conclude in Section~\ref{sec:conclusions}.

\section{Literature Review}\label{sec:kg-survey}
In this section, we provide a review on knowledge grounding, contributions of LLMs in finance, and popular financial datasets for financial text mining. We also talk about the limitations of some of the popular knowledge grounding approaches when applied to the area of finance.

\begin{figure}[t]
  \centering %{.46\textwidth}
  \includegraphics[width=.9\textwidth]{Financial_Agent_Example.pdf}
  \caption{Working of the proposed architecture: An example}
  \label{fig:Financial Agent-exmp}
\end{figure}

\subsection{Knowledge Grounding of LLMs}
The usual strategy for broadening the applicability of an LLM to a particular downstream task has conventionally entailed refining the model's parameters. This involves the process of training certain or all layers of the LLM on a bespoke dataset tailored to the specific requirements of the given downstream task \citep{radford2019language}. However, these strategies exhibit a considerable computational cost, especially if the data is dynamic and the model has to be updated frequently. %Moreover, they lead to the proliferation of modified model copies, each dedicated to a distinct downstream task, resulting in substantial storage demands. Given the large dimensions of contemporary LLMs, this approach becomes markedly impractical from both computational and resource allocation perspectives.
The fundamental rationale for knowledge grounding in LLMs resides in the motivation to equip these models with the faculties of reasoning and contextual comprehension based on relevant external data, as opposed to them being regarded as repositories of static knowledge.

In recent years, a plethora of viable alternatives have emerged, offering potential enhancements to the conventional finetuning methodology.
\cite{wei2022chain} introduced ``chain-of-thought'' (CoT), a few-shot prompting technique for LLMs. CoT employs sequential examples within a prompt, comprising task inputs and intermediate steps, to drive contextual understanding and reasoning in large models. \cite{brown2020language} demonstrated GPT-3's ability to learn complex tasks in a few-shot setting.
But few-shot prompting requires manual effort to design the optimal prompt for any required downstream task. \cite{reynolds2021prompt} argued that few-shot learning isn't a method of task learning but rather a method of task location in the existing space of the model's learned tasks. They introduced the concept of metaprompt programming, through which the job of writing task-specific prompts can be assigned to the LLM itself. \cite{gao2020making} proposed an improvement in the few-shot strategy where rather than relying on the arbitrary selection of random examples and their inclusion in the query, which lacks a guarantee of emphasizing the most informative demonstrations, their approach involved sequential random sampling of a single example from each class for every input resulting in the creation of multiple concise demonstration sets. \cite{shin2020autoprompt} introduced AUTOPROMPT, an approach that automatically constructs prompts by combining primary task inputs with an array of trigger tokens, adhering to a predetermined template. The set of trigger tokens used is same for all the inputs and is learned through a specialized adaptation of the gradient-based search strategy. The composite prompt is then supplied as input to a Mask Language Model. It was observed that for Natural Language Inference (NLI) tasks, AUTOPROMPT was comparable to a supervised finetuned BERT model.

Another limitation inherent in the few-shot prompting methodology stems from the constrained token capacity that the LLMs can intake. This might have thousands of examples that we need the LLM to learn from. A number of parameter efficient fine-tuning methodologies have been proposed to tackle this issue. 
\cite{liu2021gpt} proposed P-Tuning, a technique where the prompt tokens in the input embedding (containing context tokens and prompt tokens) are treated as pseudo tokens and mapped as trainable embedding tensors. This continuous prompt is modelled using a prompt encoder consisting of a bidirectional LSTM and is then optimized using the downstream loss function. Another unique approach is Prefix Tuning \citep{li2021prefix}, where a sequence of continuous task-specific vectors are prepended to the input of an autoregressive LM, or to both the encoder and decoder layers of a Encoder-Decoder Model. The prefix consists of trainable parameters which do not correspond to real tokens in a model's embedding. Instead of training the model's parameters on the loss function, only the parameters in the prefix are optimized. A similar methodology was used in Prompt Tuning \citep{lester2021power}, but without any intermediate-layer prefixes. \cite{dettmers2024qlora} proposed QLoRA, which introduces a memory-efficient finetuning method for LLMs by quantizing the pre-trained model to 4-bits and using Low Rank Adapters (LoRA), achieving comparable performance to 16-bit fine-tuning with reduced memory

An alternate approach for the knowledge grounding of LLMs involves equipping them with a retriever module that can access information relevant to the user's query from a database. \cite{lewis2020retrieval} introduced a retrieval-augmented generation (RAG) model, where they integrated a pre-trained retriever module with a pre-trained seq2seq model. The retriever provides latent documents conditioned on the input, and the {\it seq2seq} model then conditions on these latent documents together with the input to generate the output. \cite{dinan2018wizard} designed a dialogue model, where within the conversational framework, the chatbot is equipped with access to a curated collection of passages that maintain relevance to the ongoing discourse. At each turn, using a standard information retrieval system, the chatbot retrieves the top 7 articles for the last two turns of conversation. An attention mechanism is used to perform refined selection of specific sentences that will be used to create the next response. 
\cite{izacard2022few} created ATLAS, a RAG Model which is capable of few-shot learning.
\cite{peng2023check} presented LLM-Augmenter, a module which, in addition to knowledge retrieval and prompt generation, checks the response generated by a fixed LLM for hallucinations. If the response is not correct, it generates a feedback message which is used to improve the prompt. This cycle continues until the response by the LLM is verified. \cite{li2021knowledge} used a three step data cleaning procedure to supply an LLM with relevant context: retrieving associated triples from a knowledge graph, finding related triples by computing cosine similarity between the triples and the query, further refining the choices by estimating the semantic similarity score. 

Despite significant advancements in knowledge grounding of LLMs, efficient deployment of them for real-time processing of enormous data flowing at high frequency, such as financial data, remains a challenge. 
%It is obvious that most of the studies discussed above will not be able to handle the dynamic nature of financial dataset.

\subsection{Limitations of Traditional RAG Models in Retrieving Financial Data}
Recent years have witnessed a surge in the development and application of Retrieval-Augmented Generation (RAG) systems. By combining the power of large language models (LLMs) with information retrieval techniques, RAG systems have the potential to revolutionize various industries. However, despite their promise, these systems still face several limitations that hinder their widespread adoption, particularly in complex and regulated domains. 

One of the core challenges in RAG systems lies in the effectiveness of the retrieval mechanism. As highlighted by \cite{gupta2024comprehensive}, while powerful, the retrieval process can struggle with ambiguous queries and niche knowledge domains. The reliance on dense vector representations can sometimes lead to the retrieval of irrelevant documents, which can negatively impact the quality of the generated response. \cite{cuconasu2024power} further emphasized this point, demonstrating that the highest-scoring retrieved documents which are not directly relevant to the query, lead to a decline in LLM effectiveness. Moreover, the quality of generated output can be influenced by the number of retrieved passages, as outlined in \cite{jin2024long}. While increasing the number of passages can initially improve the quality, it can eventually lead to a decline in performance.

When integrating knowledge graphs (KGs) into RAG systems, challenges arise in effectively utilizing the structured information. 
\cite{agrawal2024mindful} identified several critical failure points in existing KG-based RAG methods, including insufficient focus on question intent and inadequate context gathering from KG facts. The real-world applications of RAG systems, especially in expert domains, are often characterized by complex and nuanced requirements. As noted by \cite{zhao2024retrieval}, the one-size-fits-all approach to data augmentation may not be suitable for all scenarios. In compliance-regulated sectors, RAG systems must adhere to stringent data privacy, security, and governance requirements. 
\cite{bruckhaus2024rag} highlighted the importance of ensuring that sensitive data is not exposed or misused during the retrieval and generation process. This is crucial when working with enterprises in compliance-regulated sectors. Dealing with financial data also presents certain challenges to traditional RAG based systems, as discussed below:
\begin{enumerate}
    \item Difficulties in Representing Financial Data: Financial data is predominantly stored in structured, tabular formats, such as spreadsheets and databases. These formats prioritize organization, analysis, and manipulation of data points. While advantageous for financial tasks, this structured format presents difficulties for RAG models. Unlike textual data, tables lack inherent relationships and reasoning patterns that RAG models can readily exploit through their natural language processing capabilities. Converting vast financial datasets into textual representations for RAG models would be impractical and inefficient. Textual storage introduces significant redundancy and increases the risk of errors or inconsistencies during data manipulation. Moreover, the structured format of financial data tables ensures data integrity and facilitates efficient retrieval of specific data points.
    \item Challenges in Retriever Accuracy: A core component of RAG models, the retriever, is tasked with fetching relevant information from the knowledge base to answer a user query. When dealing with financial data, the retriever faces a unique challenge. Financial queries often involve specific financial metrics, time-frames, and financial companies. The retriever must be highly selective in its retrieval process, ensuring it gathers only the precise data points required to answer the query accurately. Incomplete data retrieval can lead to inaccurate or misleading answers, while retrieving excessive data burdens the reasoning component with unnecessary information processing.
    \item Inability to handle high velocity, high volume data: RAG models typically rely on static databases and pre-trained retrieval mechanisms, which struggle to keep pace with the rapid influx and dynamic nature of financial data. Additionally, the need for frequent updates and high-latency responses in financial contexts can overwhelm traditional RAG architectures, leading to delayed and potentially outdated outputs.
\end{enumerate}

\begin{table}[!h]
\footnotesize
    \centering
    \caption{Summary of Financial Datasets and Resources}
    \begin{tabular}{|p{0.15\textwidth}|p{0.6\textwidth}|p{0.15\textwidth}|}
        \hline
        \textbf{Resource} & \textbf{Description} & \textbf{Reference} \\ \hline
        SentiWordNet & Establishes a lexical resource based on WordNet, assigning sentiment scores (objective, positive, negative) to individual word senses (synsets). & \cite{baccianella2010sentiwordnet} \\ \hline
        Financial Phrase Bank & Contains almost 5,000 snippets of text, gathered from financial news and press releases about Finnish companies traded on the NASDAQ OMX Nordic Exchange. Each snippet is labeled as positive, negative, or neutral. & \cite{malo2014good} \\ \hline
        SenticNet & A dataset for understanding sentiment in financial texts, combining artificial intelligence and Semantic Web technologies to better recognize, interpret, and process opinions within this context. & \cite{cambria2012senticnet} \\ \hline
        SEntFiN 1.0 & Incorporates entity-sentiment annotation to address the challenge of headlines containing multiple entities with potentially conflicting sentiments. This human-annotated dataset of over 10,700 news headlines provides insights into sentiment variations depending on specific entities in financial news. & \cite{sinha2022sentfin} \\ \hline
        Trillion Dollar Words & A dataset of tokenized and annotated Federal Open Market Committee (FOMC) speeches, meeting minutes, and press conference transcripts. This resource aims to understand the influence of monetary policy on financial markets by analyzing the language used by central bank. & \cite{shah2023trillion} \\ \hline
        REFinD & An annotated dataset of relations within financial documents with approximately 29,000 instances and 22 relation types across eight entity pairs (e.g., person-title, org-money), extracted from SEC 10-X filings. Useful for identifying relationships between entities in financial reports. & \cite{kaur2023refind} \\ \hline
        Gold Commodity Dataset & Compiled from diverse news sources and evaluated by human experts, allowing researchers to explore how news headlines influence price movements, asset comparisons, and other financial events. & \cite{sinha2021impact} \\ \hline
        FiNER & A dataset designed for Named Entity Recognition (NER) within the financial landscape, aiding in identifying and classifying financial companies within text data. & \cite{shah2023finer} \\ \hline
        MULTIFIN & Provides a multilingual financial NLP dataset with real-world financial article headlines in 15 languages. Annotated with high-level and low-level topics, facilitating multi-class and multi-label classification tasks. & \cite{jorgensen2023multifin} \\ \hline
        FINQA & A collection of question-answer pairs with in-depth analysis of financial reports. Written by financial experts, it focuses on deep reasoning over financial data, providing training data for answering complex financial queries. & \cite{chen2021finqa} \\ \hline
    \end{tabular}
    \vspace{-5mm}
    \label{tab:financial-datasets}
\end{table}

\subsection{Financial Datasets Review}   
The domain of financial language processing has witnessed a significant rise in the development of annotated datasets, each serving distinct purposes within the field. Through Table~\ref{tab:financial-datasets}, we have highlighted some prominent examples that have been widely used in the area of finance for training models. In the next sections, we discuss the core contributions of the paper in detail. The key contribution is the creation of a Financial Agent that is capable of understanding the context needed for a user query, extracting the relevant data from the databases, and supporting an LLM to handle the contextualized query effectively.

\begin{comment}
\begin{enumerate}
    \item SentiWordNet (\cite{baccianella2010sentiwordnet}) establishes a lexical resource based on WordNet, assigning sentiment scores (objective, positive, negative) to individual word senses (synsets).
    \item The Financial Phrase Bank (\cite{malo2014good}) contains almost 5,000 snippets of text, gathered from financial news and press releases about Finnish companies traded on the NASDAQ OMX Nordic Exchange. Each snippet is labeled as positive, negative, or neutral.
    \item SenticNet (\cite{cambria2012senticnet}) is another dataset for understanding sentiment in financial texts. It combines artificial intelligence and Semantic Web technologies to better recognize, interpret, and process people's opinions within this specific context.
    \item SEntFiN 1.0 (\cite{sinha2022sentfin})  incorporates entity-sentiment annotation, addressing the challenge of headlines containing multiple entities with potentially conflicting sentiments. This human-annotated dataset of over 10,700 news headlines provides valuable insights into how sentiment can vary depending on the specific entities mentioned in financial news.
    \item Trillion Dollar Words (\cite{shah2023trillion}) provides a massive dataset of tokenized and annotated Federal Open Market Committee (FOMC) speeches, meeting minutes, and press conference transcripts. This resource aims to understand the influence of monetary policy on financial markets by analyzing the language used by central bank officials.
    \item REFinD (\cite{kaur2023refind}) is a large-scale annotated dataset of relations within financial documents. It encompasses approximately 29,000 instances and 22 relation types across eight entity pairs (e.g., person-title, org-money), extracted from SEC 10-X filings. This dataset proves invaluable for tasks like identifying relationships between entities within financial reports.
    \item The Gold Commodity Dataset (\cite{sinha2021impact}) is compiled from diverse news sources and evaluated by human experts, and allows researchers to explore how news headlines influence price movements (direction, future vs. past), asset comparisons, and other financial events.
    \item FiNER (\cite{shah2023finer}) is a dataset especially designed for Named Entity Recognition (NER) within the financial landscape, helping researchers and developers to effectively identify and classify financial companies within text data.
    \item MULTIFIN (\cite{jorgensen2023multifin}) addresses the need for multilingual financial NLP models by providing a publicly available dataset of real-world financial article headlines in 15 languages. This dataset is annotated with both high-level and low-level topics, facilitating multi-class and multi-label classification tasks. 
    \item FINQA (\cite{chen2021finqa} )presents a large-scale collection of question-answer pairs requiring in-depth analysis of financial reports, written by financial experts. FINQA focuses on deep reasoning over financial data through complex  question-answer pairs, hence providing valuable training data for developing models capable of answering complex financial queries.
\end{enumerate}
\end{comment}

\section{Financial Context Dataset}\label{sec:fincontextdataset}
For any given user query, a Financial Agent is expected to provide the relevant context, which includes numeric and text data, which can be appended to the user query for further processing. In this section, we discuss the first core contribution of the paper, which is the Financial Context Dataset, consisting of 50,000 samples of user query and its corresponding context. The proposed dataset can be used to train a Financial Agent specifically for context identification and data extraction. In this section, we present a detailed account of the dataset creation and its description.

To train our model effectively, the dataset should be diverse in terms of sentence structure and comprehensive in terms of financial metrics and companies. Specifically, the dataset should include queries in a variety of forms, such as simple yes/no questions, comparative questions, and open-ended questions. This will help the model to learn to understand the different ways that users can present their queries. Additionally, the dataset should include a large number of financial metrics and companies, in the form of their many possible names and other ways they are referred to generally. This diversity in the dataset will enable our model to generalize well to a wide range of user queries, ensuring that it can accurately identify the required information. To construct this dataset, which consists of natural language user queries and their corresponding structured data requests, we followed a multi-method query sampling approach. The first method involved collecting queries sent by retail customers over technology channels to a brokerage firm, like Email or Web-based forms, etc. The second method involved interaction and consultation with 10 financial advisors on the kind of queries retail investors commonly have. The third method involved interviews with 20 retail investors about common questions that come to their mind before investing in a particular stock or forming a portfolio. Based on three methods of data collection, we created 5000 query templates, in which 4000 templates were extracted from technology channels, 500 were created with financial advisors, and another 500 were created with retail investors. This approach ensured the incorporation of a broad scope of financial queries, resulting in a diverse dataset. In order to extend the dataset to 50000 queries, the 5000 templates were scaled-up by randomly varying various aspects within each template. Table~\ref{tab:template} shows 4 queries and the corresponding template that we created. Once the template is created, it is possible to map various kinds of queries to each template, for instance, Table~\ref{tab:template2} in Appendix~\ref{sec:templateApp} provides randomization of various aspects in the template leading to a larger set of queries in our dataset.

\begin{table}[t]
\centering
\caption{Example queries and their corresponding templates.}
\begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
\hline
\textbf{Query} & \textbf{Template} \\ \hline
Judging by Apple's revenue and profit margins in Q4 2024, is it a good investment opportunity? & \textit{Judging by [company]'s [metrics][date], is it a good investment opportunity?} \\ \hline
Evaluate the return-on-equity ratio associated with Infosys in 2022. & \textit{Evaluate the [metrics] associated with [company][date].} \\ \hline
List the top 5 companies in Q3 2023 in terms of stock price growth. & \textit{List the top 5 companies [date] in terms of [metrics].} \\ \hline
Were there any improvements seen in the market share of Samsung in 2023 as compared to Apple? & \textit{Were there any improvements seen in the [metrics] of [company1][date] as compared to [company2]?} \\ \hline
\end{tabular}
\label{tab:template}
\end{table}

It is obvious that not all financial metrics or companies can be mapped to each of the queries because of the differences in how the queries are structured. For example, the metrics `revenue breakdown', `market segmentation', or `customer demographics' can be mapped to the template, \textit{Provide details about the [metrics] of companies [date].}, but not to the template, \textit{Did the [metrics] of [companies] increase [date]?} because they may not inherently be quantitative or trackable over time. 
%Descriptive metrics, such as revenue breakdown, customer demographics, and market segmentation, are well-suited to templates like \textit{Provide details about the [metrics] of companies [date]}, as they allow for detailed elaboration. Conversely, metrics that exhibit clear numerical trends over time, such as profit margins, sales growth, or stock prices, are better suited to templates like \textit{Did the [metrics] of [companies] increase [date]?}, which require measurable changes or directional insights. This classification ensures that each metric is mapped appropriately, preventing mismatches and enhancing the relevance of the generated queries.
Metrics that exhibit clear numerical trends over time, such as profit margins, sales growth, or stock prices, are better suited to templates like \textit{Did the [metrics] of [companies] increase [date]?}, which require measurable changes or directional insights. Therefore, the metric set maintained by us is carefully curated to align with the specific structure and intent of each template. 
%Our approach ensures that each metric is mapped appropriately, preventing mismatches and enhancing the relevance of the generated queries.

We maintain a comprehensive repository of financial metrics \texttt{[metrics]}, company names \texttt{[company]}, industries \texttt{[industry]}, and augment it with time range variations \texttt{[date]} and numeric variations \texttt{[number]} to systematically generate a diverse set of natural language queries. This template-driven approach to query generation ensures that the resulting dataset is both highly diverse and semantically accurate. By leveraging predefined templates, we produce a dataset that encompasses a broad spectrum of financial metrics and company-specific contexts. %significantly surpassing the diversity and informativeness of datasets created through purely randomized query generation. 
Furthermore, this approach eliminates the need for manual extraction and labeling of financial information, thereby enhancing both efficiency and scalability in dataset creation.

%\begin PLACEHOLDER for column 2 and 3 description
After creating 50000 queries using a template-based approach, we create two additional columns in the dataset, namely, ``Required Data'' and ``Structured Data Request''. The ``Required Data'' column specifies the necessary information required for answering the query, comprising company names, their specific metrics required for analysis, and the corresponding date ranges for data retrieval. Each metric is associated with a list of ``Related Metrics'', which are also to be retrieved from the data module. A snippet of our dataset, 3 out of 50000 rows, with all three columns is shown in Table~\ref{tab:snippet}. The inclusion of related metrics serves four primary purposes: (1) It provides additional contextual information for an efficient processing of the query by an LLM. (2) It enhances the robustness of the analysis by mitigating potential ambiguities or inaccuracies in the data. Financial metrics often have complex interdependencies, and providing related metrics ensures that even if the primary metric is misunderstood, incomplete, or unavailable, the LLM can derive or approximate insights using the supplementary data. This redundancy in data retrieval acts as a safeguard against potential errors, thereby improving the reliability of the query responses. (3) It enables the generation of enriched and multidimensional insights that go beyond the scope of the original query. By retrieving additional metrics linked to the primary ones, the system can identify trends, correlations, or anomalies that might not be apparent when considering the primary metric alone. This capability allows the LLM to provide more comprehensive, informed, and actionable responses, thereby increasing the value of the analysis for end-users. (4) By leveraging ``Related Metrics'', the LLM can perform multi-metric calculations (e.g., ratios, averages, or differences), which can be useful for educational queries or may also lead to insights for further interactions. For instance, see Figure~\ref{fig:Financial Agent-exmp}, where the query is about the P / E ratio of a company, but it can be effectively tackled only when the data on the ``share price'' and the ``earnings per share'' are known.

\begin{landscape}
\begin{table}[h!]
\begin{center}
 \begin{tabular}{|p{7cm}|p{7cm}|p{7cm}|}
 \hline
 \textbf{Query} & \textbf{Required Data} & \textbf{Structured Data Request}\\
 \hline
 Based on Amcor's Acid Test Ratio, Bid Size and Cash Conversion Efficiency Ratio for the previous 6 months, should I invest in it? & Companies: Amcor plc \newline Metrics: Quick Ratio (Related Metrics: Cash, Cash Equivalents, Marketable Securities, Accounts Receivable, Current Liabilities); Bid Size (Related Metrics: Quantity of shares, Multiple bid prices, Depth of the market, Order book); Cash Conversion Efficiency Ratio (Related Metrics: Cash flow from operations, Net income)\newline Dates: for the previous 6 months & (Amcor plc) \newline (Quick Ratio; Cash; Cash Equivalents; Marketable Securities; Accounts Receivable; Current Liabilities; Bid Size; Quantity of shares; Multiple bid prices; Depth of the market; Order book; Cash Conversion Efficiency Ratio; Cash flow from operations; Net income)\newline (7/1/2024 - 7/7/2024)\\
 \hline
 Give an overview of adobe and its competitor's Sep 2018 sales revenue, EVA. & Companies: Adobe Inc.; Adobe Inc. Peers\newline
Metrics: Sales Revenue (Related Metrics: Total Revenue); Economic Value Added (Related Metrics: Net operating profit after tax (NOPAT), Cost of capital)\newline
Dates: Sep 2018 &  (Adobe Inc.; Adobe Inc. Peers)\newline
 (Sales Revenue; Total Revenue; Economic Value Added; Net operating profit after tax (NOPAT); Cost of capital)\newline
 (1/9/2018 - 30/9/2018)\\
\hline
What were the return on average assets and ROWC of Halliburton co. and other companies in the energy sector from Apr 2016 to Jul 2017 compared to their Gross Profit Margin and CROAFA? & Companies: Halliburton Co.; energy Companies\newline
Metrics: Return on Average Assets (Related Metrics: Net income, Average total assets); Return on Working Capital (Related Metrics: Net income, Working capital); Gross Profit Margin (Related Metrics: Revenue, Cost of Goods Sold (COGS)); Cash Return on Average Fixed Assets (Related Metrics: Operating cash flow, Average fixed assets)\newline
Dates: from Apr 2016 to Jul 2017 & (Halliburton Co.; Energy Companies)\newline (Return on Average Assets; Net income; Average total assets; Return on Working Capital; Net income; Working capital; Gross Profit Margin; Revenue; Cost of Goods Sold (COGS); Cash Return on Average Fixed Assets; Operating cash flow; Average fixed assets)\newline
 (1/4/2016 - 1/7/2017)\\
\hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Table showing snippet of the proposed dataset containing financial queries and their corresponding data request columns. A set of 3 out of 50000 rows are shown.}
\label{tab:snippet}
\end{center}
\end{table}
\end{landscape}

We have closely studied all the financial metrics that occur in financial text and generated a predefined mapping that links each financial metric to a set of related metrics. Identification and selection of these related metrics were carried out through a meticulous human-driven process to ensure accuracy and domain relevance. While modern techniques, such as computing cosine similarity using transformer-based models, could have been employed to identify related metrics automatically, we prioritized a human-based approach to maintain correctness and contextual appropriateness. This effort ensures a robust and reliable dataset, ensuring that the relationships between metrics are meaningful and aligned with practical financial analysis.
Finally, the ``Structured Data Request'' field is prepared that represents a formalized, machine-readable version of the ``Required Data'' field, designed for direct input into the data module for efficient data retrieval. 

In a later section, we will utilize the prepared dataset for finetuning an LLM-based Financial Agent for extracting relevant data for answering the query. Before the finetuning exercise, in the next section we first create a financial LLM that can be finetuned for any downstream task.

%\end PLACEHOLDER for column 2 and 3 description


%first created 3 distinct supporting datasets and then leveraged a combination of them to arrive at the final dataset. The first two datasets are a comprehensive list of various financial metrics and companies, respectively. Herein, we provide a description of the third dataset which is the Financial Query Templates Dataset:\\

 %\\\textbf{Financial Query Templates:} This dataset enables the creation of natural language queries in a manner that investors may pose them. Additionally, using templates to form queries is motivated by the fact that we have all the information about the metrics, companies, and other details that will be used to form the queries through these templates. Therefore, the labels for these queries, which are structured format data requests containing the information required to solve the user's query, can be constructed with 100\% accuracy using the available information. This has a significant advantage over choosing a dataset of solely randomized financial queries, which would not be as exhaustive in the information they would encompass in terms of financial metrics and companies, and would also necessitate manual extraction of this information and subsequent labeling of each query.
 %The dataset contains over 5000 query templates designed to capture the diverse sentence structures in which investors may frame their queries. The templates include a wide range of variations, such as queries that compare two or more companies based on certain metrics, queries that request a list of best-performing or worst-performing companies based on a set of metrics, queries that inquire about investment decisions based on certain metrics, etc.
 

%    \\Therefore, the queries are assigned to two sets: a metric set for the types of metrics that can be mapped to the query, and an entity set for the types of companies that can be mapped to the query. Then, based on the queries they can be mapped to, each financial metric and entity is also assigned to a corresponding set type. Elements can be mapped to more than one set. The labels, which contain structured information required to answer the query are simply in the form of three lists containing the financial companies, the metrics and the date/date ranges respectively. The companies and metrics in the label are in the form in which the data module recognizes. The date/date ranges in the label however, are in the same form as they are present in the natural language queries. They shall be changed into the appropriate date format for the data module when the final data requests are constructed in section ~\ref{sec:training}. To fully capture the diversity of financial companies, metrics, dates, their other names and references, and to ensure that the model became familiar with these diverse query types properly, we generated 10 random natural language queries and their corresponding labels for each query template, which gives us more than 50000 examples for model training.


%\end{subsubsection}

\section{FinBloom 7B: A Large Language Model for Finance}\label{sec:finbloom}
Our approach for addressing user queries requires a Financial Agent for forming a structured request of the financial data that the Data Module can understand and respond to. The user's query is typically in natural language, which means that it may not explicitly contain all of the necessary information, like recent numbers and news, that is needed to answer it. %Additionally, the data that is needed may not be present in the query in a format that the Data Module can interpret. 
Hence, the Agent needs to analyze the user's query and intelligently identify the essential financial metrics, as well as any related metrics and news, that are needed to answer the query. It also needs to identify the companies for which the data is required, as well as the date ranges for which the data is needed. It finally needs to ensure that these requirements are presented in the form of a data request function that the data module can interpret. Manually specifying a set of hard rules to handle all possible user queries and perform these tasks would be very tedious and complicated. Therefore, a better approach is to train a text-to-text generation model on a large dataset of natural language user queries for generating the corresponding structured query formats which contain all the information required to answer the user query. There are quite a few financial LLMs such as FiMA (\cite{xie2023pixiu}), FinGPT (\cite{yang2023fingpt}), CFGPT (\cite{li2023cfgpt}), InvestLM (\cite{yang2023investlmlargelanguagemodel}) available that can be finetuned to serve as an agent for the task of creating structured data request. However, most of these LLMs exhibit limitations that may hinder their effectiveness in real-world applications, especially for the purpose of identifying financial context for a given query.

Many domain-specific models are typically developed by starting with a general-purpose base model, which is subsequently finetuned using human-curated financial datasets (for example, FinMA (\cite{xie2023pixiu}), InvestLM (\cite{yang2023investlmlargelanguagemodel})) such as instruction datasets, question-answer datasets, structured datasets, or synthetic datasets. To the best of our knowledge, most existing financial LLMs are not trained on high-quality, large-scale domain-specific text corpora. This limitation primarily arises from the scarcity of extensive and ethically\footnote{Ethical use in the context of financial LLM training involves ensuring that the text corpora used for model development are obtained with proper permissions and comply with copyright, privacy, and data ownership regulations. This includes avoiding the use of proprietary data without authorization and adhering to fair-use policies when sourcing publicly available content.} usable financial text corpora, as such resources are often difficult to access or compile from online channels. Many human-curated financial datasets used for training finance-specific LLMs are of limited size and highly structured; therefore, it may fail to convey domain-specific nuances and may not capture the complex inter-dependencies inherent in financial metrics and text. Human-curated datasets are also often inherent with human biases, leading to LLMs with shallow understanding of financial text. Using financial text corpora, such as large volumes of written financial text (books, news, filings, etc.), enriches the modelâ€™s knowledge of domain-relevant terminology, concepts, and context. Even in cases where one desires to train domain-specific instruction models, it is advisable to take a hybrid approach involving training on text-corpora followed by training on instruction dataset. The existing domain-specific LLMs lack this phase of training on financial text corpora. Some financial LLMs such as FinGPT (\cite{yang2023fingpt}), CFGPT (\cite{li2023cfgpt}) are trained on publicly available financial text corpora, but are not competitive when evaluated on financial benchmarks in the later part of this section.

To create the financial LLM, we choose a foundational model with robust general language understanding and the potential for domain-specific specialization in finance. We began by selecting the Bloom 7B parameter model as the base, leveraging its strong linguistic capabilities as a foundation for finetuning. To equip the model with financial expertise, we used a large corpus of financial news articles procured from Reuters and Deutsche Presse-Agentur (DPA). This dataset encompasses a wide array of financial topics, including market trends, economic indicators, corporate developments, and regulatory changes. The Reuters dataset spans the period from January 1, 2003, to December 31, 2012, while the DPA dataset covers the timeframe from June 1, 2001, to May 31, 2011. The combined dataset consists of over 14 million articles.
The high-quality financial data used for training is expected to lead to an LLM that understands interactions between various financial metrics and concepts well. We also utilized the publicly available SEC filings data from the period 31st Mar 2009 to 31st Oct 2023. The SEC filings consisted of overall 12.23 million documents but we utilized a random sample of 25\% for the purpose of fine-tuning to avoid biasing the model with a large volume of ``regulatory reporting data''. The details of the three datasets are provided in Table~\ref{tab:dataset-details}.
\begin{table}[h!]
\small
\centering
\begin{tabular}{|l|r|r|r|l|}
\hline
\textbf{Dataset} & \textbf{Documents} & \textbf{Mean Words} & \textbf{Mean Tokens} & \textbf{Time Period} \\ \hline
Reuters & 14,574,641   & 369.23        & 459.09             & 1st Jan 2003-31st Dec 2012                     \\ \hline
DPA     & 387,187   & 286.20        & 390.37             & 1st Jun 2001-31st May 2011                     \\ \hline
SEC     & 12,238,570 & 379.96       & 536.56             & 31st Mar 2009-31st Oct 2023                    \\ \hline
\end{tabular}
\caption{Dataset statistics with with mean words and mean tokens per document.}
\label{tab:dataset-details}
\end{table}
Finetuning was performed using the efficient QLoRA method (\cite{dettmers2024qlora}), which facilitated significant performance improvements with minimal computational resources. The training process spanned four epochs on a Tesla T4 GPU, requiring over 2000 hours. The model derived from this training will be referred to as ``FinBloom 7B'' in the later parts of the paper.

To evaluate the capabilities of FinBloom 7B across a diverse range of financial tasks, we adopted the FinBen benchmark (\cite{xie2024finben}). This comprehensive benchmark consists of multiple datasets, each assessing specific competencies such as information extraction, textual analysis, question answering, text generation, forecasting, risk management, and decision-making. Details of the FinBen benchmark are provided in Appendix~\ref{sec:finben}. By subjecting our model to this rigorous evaluation, we aimed to measure its effectiveness in handling various real-world financial applications. Table~\ref{table:comparison1} presents the performance comparison of FinBloom 7B against other widely used LLMs (FinMA-7B ( \cite{xie2023pixiu}), FinGPT 7b-lora (\cite{yang2023fingpt}), CFGPT sft-7B-Full (\cite{li2023cfgpt}) across the FinBen benchmark, providing insights into its relative strengths and specialization within the financial domain. There are 35 datasets in the FinBen benchmark out of which we first consider 25 datasets for which one can compute the F1 score or its variation. On these 25 datasets, FinBloom 7B shows the best average performance. It is interesting to note that FinBloom 7B has not seen any of the data in Table~\ref{table:comparison1} during the training phase. FinMA 7B that is a close competitor to FinBloom 7B has seen significant amount of data (4 out of 25 datasets) during the training phase.

We have provided a comparison of all these models against popular LLMs like ChatGPT, GPT 4, Gemini and LLaMA on FinBen benchmark in Table~\ref{tab:allComparison} in Appendix~\ref{sec:finben}. Clearly GPT-4 is the best performer among all LLMs. Though the number of model parameters in GPT-4 is officially unknown, it is estimated to have more than a trillion model parameters.

\begin{table}[h!]
\centering
\setlength{\arrayrulewidth}{0.1mm}
\setlength{\tabcolsep}{4.1pt}
\renewcommand{\arraystretch}{1.7}
\fontsize{8pt}{7pt}\selectfont
\begin{tabular} 
{ m{2.1cm} m{2.0cm} m{2.3cm} m{2.5cm} m{2.0cm} m{2.1cm} }
\hline
\textbf{Dataset} & \textbf{Metrics} & \textbf{FinMA 7B} & \textbf{FinGPT 7B-lora} & \textbf{CFGPT \newline sft-7B-Full} & \textbf{FinBloom 7B} \\
\hline
NER & EntityF1 & \textbf{0.69} & 0.00 & 0.00 & 0.00 \\
FINER-ORD & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
FinRED & F1 & 0.00 & 0.00 & 0.00 & 0.00 \\
SC & F1 & 0.19 & 0.00 & 0.15 & \textbf{0.84} \\
CD & F1 & 0.00 & 0.00 & 0.00 & 0.00 \\
FNXL & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
FSRL & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
\hline
FPB  & F1 & \textbf{0.88} & 0.00 & 0.35 & 0.32 \\
                      % & Acc & \textbf{0.88} & 0.00 & 0.26 & 0.31 \\
FiQA-SA & F1 & \textbf{0.79} & 0.00 &0.42 & 0.47 \\

Headlines & AvgF1 & \textbf{0.97} & 0.60 & 0.61 & 0.45 \\
FOMC  & F1 & \textbf{0.49} & 0.00 & 0.16 & 0.20\\
                      % & Acc & 0.46 & 0.00 & 0.21 & 0.25 \\
FinArg-AUC & MicroF1 & 0.27 & 0.00 & 0.05 & \textbf{0.81} \\
FinArg-ARC & MicroF1 & 0.08 & 0.00 & 0.05 & \textbf{0.20} \\
MultiFin & MicroF1 & 0.14 & 0.00 & 0.05 & \textbf{0.25} \\
MA & MicroF1 & 0.45 & 0.00 & 0.25 & \textbf{0.48} \\
MLESG & MicroF1 & 0.00 & 0.00 & 0.01 & \textbf{0.07} \\
\hline
{German}  & F1 & 0.17 & 0.52 & 0.53 & \textbf{0.53} \\
                      % & MCC & 0.00 & 0.00 & 0.00 & -0.04 \\
{Australian}  & F1 & 0.41 & 0.38 & 0.29 & \textbf{0.51} \\
                      % & MCC & 0.00 & 0.11 & -0.10 & 0.08 \\
{LendingClub}  & F1 & \textbf{0.61} & 0.00 & 0.05 & 0.53 \\
                      % & MCC & 0.00 & 0.00 & 0.01 & -0.02 \\
{ccf}  & F1 & 0.00 & \textbf{1.00} & 0.00 & 0.70 \\
                      % & MCC & 0.00 & 0.00 & 0.00 & -0.01 \\
{ccfraud}  & F1 & 0.01 & 0.00 & 0.03 & \textbf{0.77} \\
                      % & MCC & -0.06 & 0.00 & 0.01 & -0.01 \\
{polish}  & F1 & \textbf{0.92} & 0.30 & 0.40 & 0.30 \\
                      % & MCC & -0.01 & 0.00 & -0.02 & 0.00 \\
{taiwan}  & F1 & \textbf{0.95} & 0.60 & 0.70 & 0.32 \\
                      % & MCC & 0.00 & -0.02 & 0.00 & 0.02 \\
{portoseguro}  & F1 & 0.04 & \textbf{0.96} & 0.00 & 0.41 \\
                      % & MCC & \textbf{0.01} & 0.00 & 0.00 & 0.00 \\
{travelinsurance}  & F1 & 0.00 & \textbf{0.98} & 0.03 & 0.50 \\
                      % & MCC & 0.00 & 0.00 & 0.01 & 0.01 \\
\hline
\textbf{Average F1~score} & & 0.3244 & 0.2136 & 0.1652 & \textbf{0.3464} \\
\hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Table showing performance of different Financial LLMs on datasets from the FinBen benchmark. The results are the average of three runs.}
\label{table:comparison1}
\end{table}

\begin{comment}
\begin{table}[h!]
\centering
\setlength{\arrayrulewidth}{0.1mm}
\setlength{\tabcolsep}{4.1pt}
\renewcommand{\arraystretch}{1.7}
\fontsize{8pt}{7pt}\selectfont
\begin{tabular} 
{ m{2.5cm} m{2.5cm} m{2.5cm} m{2.5cm} m{2.2cm} m{2.1cm} }
\hline
\textbf{Dataset} & \textbf{Metrics} & \textbf{FinMA 7B} & \textbf{FinGPT 7B-lora} & \textbf{CFGPT \newline sft-7B-Full} & \textbf{FinBloom 7B} \\
\hline
TSA & RMSE & 0.80 & - & 1.05 & 0.77 \\
\hline
FinQA & EmAcc & \textbf{0.04} & 0.00 & 0.00 & 0.00 \\
TATQA & EmAcc & 0.00 & 0.00 & 0.00 & 0.00 \\
\multirow{2}{*}{Regulations}  & Rouge-1 & 0.12 & 0.01 & 0.14 & \textbf{0.21} \\
                     & BertScore & 0.59 & 0.40 & 0.57 & \textbf{0.80} \\
ConvFinQA & EmAcc & \textbf{0.20} & 0.00 & 0.01 & 0.00 \\
\hline
\multirow{2}{*}{EDTSUM}  & Rouge-1 & \textbf{0.13} & 0.00 & 0.01 & 0.08 \\
                      & BertScore & 0.38 & 0.52 & 0.51 & \textbf{0.79} \\
\multirow{2}{*}{ECTSUM}  & Rouge-1 & 0.00 & 0.00 & 0.00 & 0.00 \\
                      & BertScore & 0.00 & 0.00 & 0.00 & 0.00 \\
\hline
\multirow{2}{*}{BigData22}  & Acc & \textbf{0.51} & 0.45 & 0.45 & 0.37 \\
                      & MCC & 0.02 & 0.00 & \textbf{0.03} & -0.02 \\
\multirow{2}{*}{ACL18}  & Acc & \textbf{0.51} & 0.49 & 0.48 & 0.29 \\
                      & MCC & \textbf{0.03} & 0.00 & -0.03 & 0.01 \\
\multirow{2}{*}{CIKM18}  & Acc & \textbf{0.50} & 0.42 & 0.41 & 0.44 \\
                      & MCC & \textbf{0.08} & 0.00 & -0.07 & -0.01 \\
\hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Table showing performance of different Financial LLMs on datasets from the FinBen benchmark. The results are the average of three runs.}
\label{table:comparison2}
\end{table}
\end{comment}

%\begin PLACEHOLDER Discuss table with financial LLMs

%\end PLACEHOLDER Discuss table with financial LLMs

%and can be easily formatted into data request(s) that the data module can interpret.

\section{Financial Agent for Context Generation and Data Retrieval}\label{sec:FinancialAgent}
This section describes the Financial Agent's structure and function, explaining how it works with the Data Module to provide relevant context for an LLM to answer user queries. The Financial Agent was developed by fine-tuning the FinBloom 7B model on the the custom dataset detailed in Section \ref{sec:fincontextdataset}. This fine-tuning process employed the parameter-efficient Prompt Tuning methodology as proposed by \citep{lester2021power}. We split our dataset in a 4 to 1 ratio for training and evaluation sets. We trained the model for 5 epochs. The scores achieved for the training and evaluation sets are reported in Table~\ref{tab:lossPerplexity}
\begin{table}[h!]
    \centering
    \newcommand{\myline}{\cline{2-3}}
    \newcommand{\noline}[1]{\multicolumn{1}{c}{#1}}
    \begin{tabular}{c|c|c|}
        \noline{} & \noline{Loss} & \noline{Perplexity} \\
        \myline
        Training Set & 0.0375 & 1.0382 \\
        \myline
        Evaluation Set & 0.0076 & 1.0076 \\
        \myline
    \end{tabular}
    \caption{Loss and perplexity scores for training and evaluation data.}
    \label{tab:lossPerplexity}
\end{table}

To further assess the accuracy of the finetuned FinBloom 7B LLM on our custom dataset, we generated an additional 10,000 natural queries and their corresponding labels to create a test set. We then passed the same queries to the model as inputs and recorded its outputs. Considering these recorded outputs from the model as the generated text and the original labels in the test set as the reference text, we calculated the similarity between the two sets using the BLEU (Bigram Language Evaluation Understudy) (\cite{papineni2002bleu}) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (\cite{lin2004rouge}) metrics. These metrics provide a numerical score reflecting the degree of overlap between the model-generated text and the expected response, which are reported in Table~\ref{tab:bleuRouge}. As we can see from these scores, the model is excellent in identifying the required data to answer the queries. 
\begin{table}[h!]
\begin{center}
 \begin{tabular}{ |c|c| } 
 \hline
 \textbf{Metric} & \textbf{Score}\\
 \hline
 BLEU & 0.9614 \\ 
 \hline
 ROUGE-1 (F1) & 0.9774 \\ 
 \hline
 ROUGE-2 (F1) & 0.9693 \\ 
 \hline
 ROUGE-L (F1) & 0.9771 \\ 
 \hline
\end{tabular}   
\caption{BLUE and ROUGE scores for 10,000 test samples.}
\label{tab:bleuRouge}
\end{center}
\end{table}

%Following this, the model outputs (see Column 2 in Table \ref{tab:snippet}) require some reformatting to conform to the data module's required format (see Column 3 in Table \ref{tab:snippet}). The date ranges in the model outputs will remain in the same format as it is in the corresponding query. This was done to allow the model to easily extract the dates/date ranges from the user's query.
%The extracted dates/date ranges were converted to the appropriate format for the structured data requests using a proprietary date parser. The structured data requests can then be passed onto the Data Module. 

The Data Module serves as the repository for financial data, encompassing tabular financial information as well as real-time news data pertaining to diverse companies, sectors, and industries. It operates in a dynamic fashion, regularly ingesting and incorporating fresh data to ensure that the information it provides remains current and up-to-date. The module is designed to process financial data requests. Upon receiving a data request from the Financial Agent, the data module constructs a dataframe containing pertinent financial data, conforming to the specifications of the data request. Concurrently, the module extracts the relevant news data, considered beneficial for complete query resolution, based on semantic matching of news items with the user query. We use RoBERTa-based \citep{liu2019roberta} semantic search to detect similarity between the requested query and the news headlines.
%The data module contains the financial metrics, based on their reporting frequency, and leverages this knowledge to effectively present data. 
When a data request containing company/industry name(s), financial metric(s) and specific date range(s) is received, the module identifies the closest date range(s) within which the financial data is available. For example, net income is reported quarterly, so the module presents the quarterly net incomes for the closest available date range to the one in the request. Share price, on the other hand, is reported at a higher frequency, so the module presents the share price data for the exact date range as requested in the query. If there are no dates/date ranges present in the request, then the latest date range for which the data is available is chosen. The module then retrieves the relevant metric data for that date range and in addition, communicates the corresponding date range and frequency for which this metric's data is provided. Consequently, when this data is integrated with the initial user input and supplied to the LLM, the LLM receives accurate information regarding the metric and its associated date range. 
%As mentioned earlier, for retrieval of the news data, we use RoBERTa-based semantic search. 
%An illustrative instance of a data request alongside the corresponding response generated by the module can be seen in the example presented in Appendix \ref{appendix:example}. 
In our proposed framework, the various modules interoperate as follows:
\begin{enumerate}
    \item The user's initial query, $x$, is received by the Financial Agent.
    \item The Agent identifies the required data to answer this query and converts it into a structured data request to be passed onto the Data Module.
    \item The Data Module receives this data request and returns the relevant financial and news data.
    \item This data is converted into a string form \textit{c} and appended to the initial query \textit{x} along with instructions for the LLM to answer the given financial query using the data provided.
    \item The LLM receives the contextualized and enriched query, $(c,x)$, to generate the output.
\end{enumerate}
An illustrative example to demonstrate how the Financial Agent architecture works in knowledge grounding of LLMs and helps to answer financial queries comprehensively has been provided in the Appendix \ref{appendix:example}.

\section{Conclusions}\label{sec:conclusions}
%This study highlights the importance of knowledge grounding in LLMs, particularly for interactive tasks that demand access to real-time information. The paper makes multiple contributions to efficiently achieve knowledge grounding in finance. We developed a Financial Context Dataset comprising over 50,000 financial queries paired with the contextual information necessary for their resolution. We then trained FinBloom 7B, a 7 billion-parameter language model, on 14 million financial news articles sourced from Reuters and Deutsche Presse-Agentur (DPA) and 12 million documents sourced from SEC filings. Finally, we finetuned FinBloom 7B on the Financial Context Dataset, creating a Financial Agent capable of generating the required context for user queries and retrieving it from a data module to handle financial queries efficiently. Multiple interactions with large-scale LLMs (for example, GPT 3.5 or GPT 4) may lead to high latency for financial decision making. To avoid this, a Financial Agent has been trained that can be paired with any large-scale LLM. The user provides the query, the Financial Agent understands the context and retrieves data, and finally the large-scale LLM processes the contextual information and query. The framework has low latency and therefore better capability in handling high velocity data. 

This study emphasizes the role of knowledge grounding in LLMs, particularly for interactive tasks that require real-time information access. 
%It presents multiple contributions toward efficient knowledge grounding in the domain of finance.
First, we developed the Financial Context Dataset, which comprises over 50,000 financial queries paired with the contextual information necessary for their resolution. Building on this foundation, we trained FinBloom 7B, a 7 billion parameter language model, using a corpus of over 14 million financial news articles sourced from Reuters and Deutsche Presse-Agentur (DPA) and 12 million documents from SEC filings. Subsequently, we fine-tuned FinBloom 7B on the Financial Context Dataset to create a Financial Agent. This agent is capable of generating the required context for user queries and efficiently retrieving it from a structured data module to address financial queries.
A key challenge in financial decision-making lies in mitigating the high latency associated with multiple interactions with large-scale LLMs (e.g., GPT-3.5 or GPT-4). To address this, the trained Financial Agent seamlessly integrates with any large-scale LLM. In our proposed framework, the user submits a query, the Financial Agent processes and retrieves the necessary context, and the LLM subsequently analyzes the query along with the contextual information. This design ensures low latency, enabling effective handling of high-velocity financial data effectively.

Future research could expand this framework into a multi-agent system, integrating additional agents such as financial video analysis tools to further enhance its capabilities. With these advancements, our work holds the potential to transform the delivery of financial services, making them more efficient, personalized, and accessible to a broader audience.


%We propose a domain-specific approach through FinBloom 7B, a custom-trained LLM designed to handle versatile financial queries more effectively than LLMs of similar size. By leveraging a Financial Agent framework that combines real-time data integration with a robust Financial Context Dataset, FinBloom 7B demonstrates superior performance in generating accurate, context-rich responses.

%Our findings underscore the potential of domain-specific LLMs, like FinBloom 7B, to reduce latency and enhance decision-making processes, addressing key limitations of general-purpose LLMs in high-stakes domains such as finance. The proposed approach not only streamlines real-time decision-making and algorithmic trading but also lays the groundwork for creating accessible and personalized financial advisory systems.



%Knowledge grounding of LLMs with up-to-date information is essential in improving their utility, especially in the context of finance. In this paper, we proposed a system that answers any financial query by retrieving and analyzing relevant data required to answer it. We created an agent-based framework where an agent extracts the relevant context
%One agent would act as the extractor and get the relevant context required to answer any financial query, and another agent would provide the required tabular and news data using that context. 
%In future work, other agents such as a financial news video analysis agent can be added to this framework. We have demonstrated that given a financial query, we can accurately identify the context required to solve the query, retrieve the relevant tabular and news data, and supply it along with the original query to an LLM. We built a custom dataset of over 50000 user queries and their corresponding contexts. We also developed an Open Source 7 Billion parameter LLM for diverse financial applications and subsequently finetuned it on our custom dataset for the task of creating the required context for any user query. Our proposed methodology demonstrates that LLMs do not need to be continuously trained on new financial data, as today's LLMs are able to perform really well with the information provided to them in a prompt. This tool can be immensely helpful for financial investors and other people in general who are looking for investment opportunities, want to conduct various kinds of financial analysis, or even just broaden their knowledge about the financial markets. It will help save time and effort in sourcing the data they need, and also provide them with insights based on the data.

%Our research has the potential to significantly transform the way financial services are delivered. Our tool could be used to create personalized financial advice chatbots that can provide tailored guidance to users based on their individual needs and circumstances. This could make financial advice more accessible and affordable for everyone, regardless of their income or wealth. Our aim is to make finance and financial markets a more inclusive space. By providing people with access to the information and tools they need to make informed financial decisions, we can help to level the playing field and create a more equitable financial system.\\
% Acknowledgements should go at the end, before appendices and references

%\acks{The authors would like to acknowledge support for this project from ABC.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\vskip 0.2in
%\bibliographystyle{elsarticle-num}
%\bibliography{references}
\bibliographystyle{plain}
\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2024)Agrawal, Kumarage, Alghamdi, and Liu]{agrawal2024mindful}
Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, and Huan Liu.
\newblock {Mindful-RAG}: {A} study of points of failure in retrieval augmented generation.
\newblock \emph{arXiv preprint arXiv:2407.12216}, 2024.

\bibitem[Alvarado et~al.(2015)Alvarado, Verspoor, and Baldwin]{alvarado2015domain}
Julio Cesar~Salinas Alvarado, Karin Verspoor, and Timothy Baldwin.
\newblock Domain adaption of named entity recognition to support credit risk assessment.
\newblock In \emph{Proceedings of the Australasian Language Technology Association Workshop 2015}, pages 84--90, 2015.

\bibitem[Baccianella et~al.(2010)Baccianella, Esuli, Sebastiani, et~al.]{baccianella2010sentiwordnet}
Stefano Baccianella, Andrea Esuli, Fabrizio Sebastiani, et~al.
\newblock Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.
\newblock In \emph{Lrec}, volume~10, pages 2200--2204, 2010.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Bruckhaus(2024)]{bruckhaus2024rag}
Tilmann Bruckhaus.
\newblock {RAG} does not work for enterprises.
\newblock \emph{arXiv preprint arXiv:2406.04369}, 2024.

\bibitem[Cambria et~al.(2012)Cambria, Havasi, and Hussain]{cambria2012senticnet}
Erik Cambria, Catherine Havasi, and Amir Hussain.
\newblock Senticnet 2: A semantic and affective resource for opinion mining and sentiment analysis.
\newblock In \emph{Twenty-Fifth international FLAIRS conference}, 2012.

\bibitem[Carta et~al.(2023)Carta, Romac, Wolf, Lamprier, Sigaud, and Oudeyer]{carta2023grounding}
Thomas Carta, Cl{\'e}ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.
\newblock Grounding large language models in interactive environments with online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2302.02662}, 2023.

\bibitem[Chen et~al.(2023)Chen, Tseng, Kang, Lhuissier, Day, Tu, and Chen]{chen2023multi}
Chung-Chi Chen, Yu-Min Tseng, Juyeon Kang, Ana{\"\i}s Lhuissier, Min-Yuh Day, Teng-Tsai Tu, and Hsin-Hsi Chen.
\newblock Multi-lingual {ESG} issue identification.
\newblock In \emph{Proceedings of the Fifth Workshop on Financial Technology and Natural Language Processing and the Second Multimodal AI For Financial Forecasting}, pages 111--115, 2023.

\bibitem[Chen et~al.(2021)Chen, Chen, Smiley, Shah, Borova, Langdon, Moussa, Beane, Huang, Routledge, et~al.]{chen2021finqa}
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et~al.
\newblock Finqa: A dataset of numerical reasoning over financial data.
\newblock \emph{arXiv preprint arXiv:2109.00122}, 2021.

\bibitem[Chen et~al.(2022)Chen, Li, Smiley, Ma, Shah, and Wang]{chen2022convfinqa}
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William~Yang Wang.
\newblock Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.
\newblock \emph{arXiv preprint arXiv:2210.03849}, 2022.

\bibitem[Cortis et~al.(2017)Cortis, Freitas, Daudert, Huerlimann, Zarrouk, Handschuh, and Davis]{cortis-etal-2017-semeval}
Keith Cortis, Andr{\'e} Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Handschuh, and Brian Davis.
\newblock {S}em{E}val-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news.
\newblock In \emph{Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)}, pages 519--535, August 2017.
\newblock \doi{10.18653/v1/S17-2089}.

\bibitem[Cuconasu et~al.(2024)Cuconasu, Trappolini, Siciliano, Filice, Campagnano, Maarek, Tonellotto, and Silvestri]{cuconasu2024power}
Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri.
\newblock The power of noise: {R}edefining retrieval for {RAG} systems.
\newblock In \emph{Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 719--729, 2024.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dinan et~al.(2018)Dinan, Roller, Shuster, Fan, Auli, and Weston]{dinan2018wizard}
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston.
\newblock Wizard of {W}ikipedia: {K}nowledge-powered conversational agents.
\newblock \emph{arXiv preprint arXiv:1811.01241}, 2018.

\bibitem[Feng et~al.(2023)Feng, Dai, Huang, Zhang, Xie, Han, Chen, Lopez-Lira, and Wang]{feng2023empowering}
Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu Chen, Alejandro Lopez-Lira, and Hao Wang.
\newblock Empowering many, biasing a few: Generalist credit scoring through large language models.
\newblock \emph{arXiv preprint arXiv:2310.00566}, 2023.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Gupta et~al.(2024)Gupta, Ranjan, and Singh]{gupta2024comprehensive}
Shailja Gupta, Rajesh Ranjan, and Surya~Narayan Singh.
\newblock A comprehensive survey of retrieval-augmented generation ({RAG}): {E}volution, current landscape and future directions.
\newblock \emph{arXiv preprint arXiv:2410.12837}, 2024.

\bibitem[Hofmann(1994)]{statlog_(german_credit_data)_144}
Hans Hofmann.
\newblock {Statlog (German Credit Data)}.
\newblock UCI Machine Learning Repository, 1994.
\newblock {DOI}: https://doi.org/10.24432/C5NC77.

\bibitem[Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2022few}
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.
\newblock Few-shot learning with retrieval augmented language models.
\newblock \emph{arXiv preprint arXiv:2208.03299}, 2022.

\bibitem[Jin et~al.(2024)Jin, Yoon, Han, and Arik]{jin2024long}
Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan~O Arik.
\newblock Long-context {LLM}s meet {RAG}: {O}vercoming challenges for long inputs in {RAG}.
\newblock \emph{arXiv preprint arXiv:2410.05983}, 2024.

\bibitem[J{\o}rgensen et~al.(2023)J{\o}rgensen, Brandt, Hartmann, Dai, Igel, and Elliott]{jorgensen2023multifin}
Rasmus J{\o}rgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott.
\newblock {MultiFin}: A dataset for multilingual financial nlp.
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2023}, pages 894--909, 2023.

\bibitem[Kaur et~al.(2023)Kaur, Smiley, Gupta, Sain, Wang, Siddagangappa, Aguda, and Shah]{kaur2023refind}
Simerjot Kaur, Charese Smiley, Akshat Gupta, Joy Sain, Dongsheng Wang, Suchetha Siddagangappa, Toyin Aguda, and Sameena Shah.
\newblock {REFinD}: Relation extraction financial dataset.
\newblock In \emph{Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 3054--3063, 2023.

\bibitem[Lamm et~al.(2018)Lamm, Chaganty, Manning, Jurafsky, and Liang]{lamm2018textual}
Matthew Lamm, Arun~Tejasvi Chaganty, Christopher~D Manning, Dan Jurafsky, and Percy Liang.
\newblock Textual analogy parsing: What's shared and what's compared among analogous facts.
\newblock \emph{arXiv preprint arXiv:1809.02700}, 2018.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Li et~al.(2023)Li, Bian, Wang, Lei, Cheng, Ding, and Jiang]{li2023cfgpt}
Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and Changjun Jiang.
\newblock {CFGPT}: Chinese financial assistant with large language model.
\newblock \emph{arXiv preprint arXiv:2309.10654}, 2023.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Li et~al.(2021)Li, Peng, Shen, Mao, Liden, Yu, and Gao]{li2021knowledge}
Yu~Li, Baolin Peng, Yelong Shen, Yi~Mao, Lars Liden, Zhou Yu, and Jianfeng Gao.
\newblock Knowledge-grounded dialogue generation with a unified knowledge representation.
\newblock \emph{arXiv preprint arXiv:2112.07924}, 2021.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Liu et~al.(2021)Liu, Zheng, Du, Ding, Qian, Yang, and Tang]{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.
\newblock {GPT} understands, too.
\newblock \emph{arXiv preprint arXiv:2103.10385}, 2021.

\bibitem[Liu(2019)]{liu2019roberta}
Yinhan Liu.
\newblock {RoBERTa}: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 364, 2019.

\bibitem[Luo et~al.(2022)Luo, Sun, Xia, Qin, Zhang, Poon, and Liu]{luo2022biogpt}
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.
\newblock {BioGPT}: {G}enerative pre-trained transformer for biomedical text generation and mining.
\newblock \emph{Briefings in Bioinformatics}, 23\penalty0 (6), 2022.

\bibitem[Maia et~al.(2018)Maia, Handschuh, Freitas, Davis, McDermott, and Zarrouk]{maia2018financial}
M~Maia, S~Handschuh, A~Freitas, B~Davis, R~McDermott, and M~Zarrouk.
\newblock Financial opinion mining and question answering.
\newblock \emph{Von https://sites. google. com/view/fiqa}, 2018.

\bibitem[Malo et~al.(2014)Malo, Sinha, Korhonen, Wallenius, and Takala]{malo2014good}
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala.
\newblock Good debt or bad debt: Detecting semantic orientations in economic texts.
\newblock \emph{Journal of the Association for Information Science and Technology}, 65\penalty0 (4):\penalty0 782--796, 2014.

\bibitem[Mariko et~al.(2020)Mariko, Akl, Labidurie, Durfort, De~Mazancourt, and El-Haj]{mariko2020financial}
Dominique Mariko, Hanna~Abi Akl, Estelle Labidurie, Stephane Durfort, Hugues De~Mazancourt, and Mahmoud El-Haj.
\newblock Financial document causality detection shared task (fincausal 2020).
\newblock \emph{arXiv preprint arXiv:2012.02505}, 2020.

\bibitem[Mukherjee et~al.(2022)Mukherjee, Bohra, Banerjee, Sharma, Hegde, Shaikh, Shrivastava, Dasgupta, Ganguly, Ghosh, et~al.]{mukherjee2022ectsum}
Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et~al.
\newblock Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts.
\newblock \emph{arXiv preprint arXiv:2210.12467}, 2022.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: {A} method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.]{peng2023check}
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu~Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et~al.
\newblock Check your facts and try again: {I}mproving large language models with external knowledge and automated feedback.
\newblock \emph{arXiv preprint arXiv:2302.12813}, 2023.

\bibitem[Quinlan(1987)]{statlog_(australian_credit_approval)_143}
Ross Quinlan.
\newblock {Statlog (Australian Credit Approval)}.
\newblock UCI Machine Learning Repository, 1987.
\newblock {DOI}: https://doi.org/10.24432/C59012.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Reynolds and McDonell(2021)]{reynolds2021prompt}
Laria Reynolds and Kyle McDonell.
\newblock Prompt programming for large language models: {B}eyond the few-shot paradigm.
\newblock In \emph{Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}, pages 1--7, 2021.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Shah et~al.(2023{\natexlab{a}})Shah, Paturi, and Chava]{shah2023trillion}
Agam Shah, Suvan Paturi, and Sudheer Chava.
\newblock Trillion dollar words: A new financial dataset, task \& market analysis.
\newblock \emph{arXiv preprint arXiv:2305.07972}, 2023{\natexlab{a}}.

\bibitem[Shah et~al.(2023{\natexlab{b}})Shah, Vithani, Gullapalli, and Chava]{shah2023finer}
Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava.
\newblock Finer: Financial named entity recognition dataset and weak-supervision model.
\newblock \emph{arXiv preprint arXiv:2302.11157}, 2023{\natexlab{b}}.

\bibitem[Sharma et~al.(2022)Sharma, Nayak, Bose, Meena, Dasgupta, Ganguly, and Goyal]{sharma2022finred}
Soumya Sharma, Tapas Nayak, Arusarka Bose, Ajay~Kumar Meena, Koustuv Dasgupta, Niloy Ganguly, and Pawan Goyal.
\newblock {FinRED}: A dataset for relation extraction in financial domain.
\newblock In \emph{Companion Proceedings of the Web Conference 2022}, pages 595--597, 2022.

\bibitem[Sharma et~al.(2023)Sharma, Khatuya, Hegde, Shaikh, Dasgupta, Goyal, and Ganguly]{sharma2023financial}
Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan Goyal, and Niloy Ganguly.
\newblock Financial numeric extreme labelling: A dataset and benchmarking.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 3550--3561, 2023.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh.
\newblock Autoprompt: {E}liciting knowledge from language models with automatically generated prompts.
\newblock \emph{arXiv preprint arXiv:2010.15980}, 2020.

\bibitem[Sinha and Khandait(2021)]{sinha2021impact}
Ankur Sinha and Tanmay Khandait.
\newblock Impact of news on the commodity market: Dataset and results.
\newblock In \emph{Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2}, pages 589--601. Springer, 2021.

\bibitem[Sinha et~al.(2022)Sinha, Kedas, Kumar, and Malo]{sinha2022sentfin}
Ankur Sinha, Satishwar Kedas, Rishu Kumar, and Pekka Malo.
\newblock {SEntFiN 1.0}: Entity-aware sentiment analysis for financial news.
\newblock \emph{Journal of the Association for Information Science and Technology}, 73\penalty0 (9):\penalty0 1314--1335, 2022.

\bibitem[Soun et~al.(2022)Soun, Yoo, Cho, Jeon, and Kang]{soun2022accurate}
Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U~Kang.
\newblock Accurate stock movement prediction with self-supervised learning from sparse noisy tweets.
\newblock In \emph{2022 IEEE International Conference on Big Data (Big Data)}, pages 1691--1700. IEEE, 2022.

\bibitem[Sy et~al.(2023)Sy, Peng, Huang, Lin, and Chang]{sy2023fine}
Eugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, Heng-Yu Lin, and Yung-Chun Chang.
\newblock Fine-grained argument understanding with {BERT} ensemble techniques: A deep dive into financial sentiment analysis.
\newblock In \emph{Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)}, pages 242--249, 2023.

\bibitem[Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic]{taylor2022galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: {A} large language model for science.
\newblock \emph{arXiv preprint arXiv:2211.09085}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2018)Wu, Zhang, Shen, and Wang]{wu2018hybrid}
Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang.
\newblock Hybrid deep sequential modeling for social text-driven stock prediction.
\newblock In \emph{Proceedings of the 27th ACM international conference on information and knowledge management}, pages 1627--1630, 2018.

\bibitem[Wu et~al.(2023)Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann]{wu2023bloomberggpt}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
\newblock {BloombergGPT}: {A} large language model for finance.
\newblock \emph{arXiv preprint arXiv:2303.17564}, 2023.

\bibitem[Xie et~al.(2023)Xie, Han, Zhang, Lai, Peng, Lopez-Lira, and Huang]{xie2023pixiu}
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang.
\newblock {PIXIU}: A large language model, instruction data and evaluation benchmark for finance.
\newblock \emph{arXiv preprint arXiv:2306.05443}, 2023.

\bibitem[Xie et~al.(2024)Xie, Han, Chen, Xiang, Zhang, He, Xiao, Li, Dai, Feng, et~al.]{xie2024finben}
Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et~al.
\newblock The {FinBen}: An holistic financial benchmark for large language models.
\newblock \emph{arXiv e-prints}, pages arXiv--2402, 2024.

\bibitem[Xu and Cohen(2018)]{xu2018stock}
Yumo Xu and Shay~B Cohen.
\newblock Stock movement prediction from tweets and historical prices.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1970--1979, 2018.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Liu, and Wang]{yang2023fingpt}
Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang.
\newblock {FinGPT}: Open-source financial large language models.
\newblock \emph{arXiv preprint arXiv:2306.06031}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2020)Yang, Kenny, Ng, Yang, Smyth, and Dong]{yang2020generating}
Linyi Yang, Eoin~M Kenny, Tin Lok~James Ng, Yi~Yang, Barry Smyth, and Ruihai Dong.
\newblock Generating plausible counterfactual explanations for deep transformers in financial text classification.
\newblock \emph{arXiv preprint arXiv:2010.12512}, 2020.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Tang, and Tam]{yang2023investlmlargelanguagemodel}
Yi~Yang, Yixuan Tang, and Kar~Yan Tam.
\newblock {InvestLM}: A large language model for investment using financial domain instruction tuning, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2309.13064}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: {O}pen pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2024)Zhao, Yang, Wang, He, Qiu, and Qiu]{zhao2024retrieval}
Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna~K Qiu, and Lili Qiu.
\newblock Retrieval augmented generation ({RAG}) and beyond: {A} comprehensive survey on how to make your llms use external data more wisely.
\newblock \emph{arXiv preprint arXiv:2409.14924}, 2024.

\bibitem[Zhu et~al.(2021)Zhu, Lei, Huang, Wang, Zhang, Lv, Feng, and Chua]{zhu2021tat}
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua.
\newblock {TAT-QA}: A question answering benchmark on a hybrid of tabular and textual content in finance.
\newblock \emph{arXiv preprint arXiv:2105.07624}, 2021.

\end{thebibliography}

\appendix

\section{Template and Generate Queries}\label{sec:templateApp}
Table~\ref{tab:template2} provides templates and the corresponding queries generated from it. The table shows how 5000 templates have been used to synthesize a dataset with 50000 queries.
\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{|p{0.25\textwidth}|p{0.65\textwidth}|}
\hline
\textbf{Template} & \textbf{Generated Queries} \\ \hline
\textit{Judging by [company]'s [metrics][date], is it a good investment opportunity?} & 
\begin{itemize}
    \item Judging by Tesla's market capitalization growth in 2023, is it a good investment opportunity?
    \item Judging by Google's revenue in Q1 2024, is it a good investment opportunity?
\end{itemize} \\ \hline
\textit{Evaluate the [metrics] associated with [company][date].} & 
\begin{itemize}
    \item Evaluate the customer retention rates associated with Netflix in 2023.
    \item Evaluate the price-to-earnings ratio associated with TCS in 2022.
\end{itemize} \\ \hline
\textit{List the top 5 companies [date] in terms of [metrics].} & 
\begin{itemize}
    \item List the top 5 companies in 2024 in terms of annual revenue.
    \item List the top 5 companies in Q3 2023 in terms of stock price growth.
\end{itemize} \\ \hline
\textit{Were there any improvements seen in the [metrics] of [company1][date] as compared to [company2]?} & 
\begin{itemize}
    \item Were there any improvements seen in the renewable energy adoption metrics of Shell in 2023 as compared to ExxonMobil?
    \item Were there any improvements seen in the employee satisfaction scores of Microsoft in 2023 as compared to Google?
\end{itemize} \\ \hline
\end{tabular}
\caption{Templates and Realistic Queries}
\label{tab:template2}
\end{table}

\section{FinBen Benchmark and Comparison Results}\label{sec:finben}
To rigorously evaluate our model's performance across a wide spectrum of financial tasks, we employed the comprehensive FinBen (\cite{xie2024finben}) benchmark. This benchmark comprises a collection of datasets categorized by the specific large language model (LLM) abilities they are designed to evaluate. These categories are as follows:
\begin{enumerate}
\item Information Extraction: This category includes the NER (\cite{alvarado2015domain}), FINER-ORD (\cite{shah2023finer}), FinRED (\cite{sharma2022finred}), SC(\cite{mariko2020financial}), CD (\cite{mariko2020financial}), FNXL (\cite{sharma2023financial}), and FSRL (\cite{lamm2018textual}) datasets, designed to assess the LLM's capacity for extracting structured information from unstructured text.
\item Textual Analysis: This section focuses on evaluating textual understanding and analytical capabilities, utilizing the Financial Phrase Bank (FPB) (\cite{malo2014good}), FiQA-SA (\cite{maia2018financial}), TSA (\cite{cortis-etal-2017-semeval}), Headlines (\cite{sinha2021impact}), FOMC (\cite{shah2023trillion}), FinArg-AUC (\cite{sy2023fine}), FinArg-ARC (\cite{sy2023fine}), Multifin (\cite{jorgensen2023multifin}), MA (\cite{yang2020generating}), and MLESG (\cite{chen2023multi}) datasets.
\item Question Answering: This category assesses the LLM's ability to comprehend and answer questions based on given text, employing the FinQA (\cite{chen2021finqa}), TATQA (\cite{zhu2021tat}), Regulations (\cite{xie2024finben}), and ConvFinQA (\cite{chen2022convfinqa}) datasets.
\item Text Generation: This section evaluates the LLM's capacity to generate coherent and contextually relevant text, utilizing the EDTSUM (\cite{xie2024finben}) and ECTSUM (\cite{mukherjee2022ectsum}) datasets.
\item Forecasting: This category tests the LLM's ability to predict future trends or outcomes, using the BigData22 (\cite{soun2022accurate}), ACL18 (\cite{xu2018stock}), and CIKM18 (\cite{wu2018hybrid}) datasets.
\item Risk Management: This section evaluates the LLM's ability to assess and manage risk, leveraging datasets such as German (\cite{statlog_(german_credit_data)_144}), Australian (\cite{statlog_(australian_credit_approval)_143}), LendingClub(\cite{feng2023empowering}), ccf (\cite{feng2023empowering}), ccfraud (\cite{feng2023empowering}), Polish (\cite{feng2023empowering}), Taiwan (\cite{feng2023empowering}), portoseguro (\cite{feng2023empowering}), and travelinsurance (\cite{feng2023empowering}).
\end{enumerate}

Table~\ref{tab:allComparison} provides an extensive comparison of multiple LLMs on FinBen benchmark that consists of 35 datasets.

\begin{table}[h]
\centering
\setlength{\arrayrulewidth}{0.1mm}
\setlength{\tabcolsep}{4.1pt}
\renewcommand{\arraystretch}{1.7}
\fontsize{4pt}{5pt}\selectfont
\begin{tabular} 
{ m{1.2cm} m{0.7cm} m{0.6cm} m{0.6cm} m{0.6cm} m{0.8cm} m{0.7cm} m{0.7cm} m{0.6cm} m{0.8cm} m{0.7cm} m{0.6cm} m{0.6cm} m{1cm} m{0.8cm} }
\hline
\textbf{Dataset} & \textbf{Metrics} & \textbf{Chat GPT} & \textbf{GPT 4} & \textbf{Gemini} & \textbf{LLaMA2 7B-Chat} & \textbf{LLaMA2 70B} & \textbf{LLaMA3 8B} & \textbf{FinMA 7B} & \textbf{FinGPT 7B-lora} & \textbf{InternLM 7B} & \textbf{Falcon 7B} & \textbf{Mixtral 7B} & \textbf{CFGPT sft-7B-Full} & \textbf{FinBloom 7B} \\
\hline
NER & EntityF1 & 0.77 & \textbf{0.83} & 0.61 & 0.18 & 0.04 & 0.08 & 0.69 & 0.00 & 0.00 & 0.00 & 0.24 & 0.00 & 0.00 \\
FINER-ORD & EntityF1 & 0.28 & \textbf{0.77} & 0.14 & 0.02 & 0.07 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.05 & 0.00 & 0.00 \\
FinRED & F1 & 0.00 & \textbf{0.02} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
SC & F1 & 0.80 & 0.81 & 0.74 & 0.85 & 0.61 & 0.69 & 0.19 & 0.00 & \textbf{0.88} & 0.67 & 0.83 & 0.15 & 0.84 \\
CD & F1 & 0.00 & 0.01 & \textbf{0.03} & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
FNXL & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
FSRL & EntityF1 & 0.00 & 0.01 & \textbf{0.03} & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\hline
\multirow{2}{*}{FPB}  & F1 & 0.78 & 0.78 & 0.77 & 0.39 & 0.73 & 0.52 & \textbf{0.88} & 0.00 & 0.69 & 0.07 & 0.29 & 0.35 & 0.32 \\
                      & Acc & 0.78 & 0.76 & 0.77 & 0.41 & 0.72 & 0.52 & \textbf{0.88} & 0.00 & 0.69 & 0.05 & 0.37 & 0.26 & 0.31 \\
FiQA-SA & F1 & 0.60 & 0.80 & 0.81 & 0.76 & \textbf{0.83} & 0.70 & 0.79 & 0.00 & 0.81 & 0.77 & 0.16 & 0.42 & 0.47 \\
TSA & RMSE & 0.53 & 0.50 & 0.37 & 0.71 & 0.57 & 0.25 & 0.80 & 0.00 & \textbf{0.29} & 0.50 & 0.50 & 1.05 & 0.77 \\
Headlines & AvgF1 & 0.77 & 086 & 0.78 & 0.72 & 0.63 & 0.60 & \textbf{0.97} & 0.60 & 0.60 & 0.45 & 0.60 & 0.61 & 0.45 \\
\multirow{2}{*}{FOMC}  & F1 & 0.64 & \textbf{0.71} & 0.40 & 0.35 & 0.49 & 0.40 & 0.49 & 0.00 & 0.36 & 0.30 & 0.37 & 0.16 & 0.20\\
                      & Acc & 0.60 & \textbf{0.69} & 0.60 & 0.49 & 0.47 & 0.41 & 0.46 & 0.00 & 0.35 & 0.30 & 0.35 & 0.21 & 0.25 \\

FinArg-ACC & MicroF1 & 0.50 & 0.60 & 0.31 & 0.46 & 0.58 & 0.51 & 0.27 & 0.00 & 0.39 & 0.23 & 0.39 & 0.05 & \textbf{0.81} \\
FinArg-ARC & MicroF1 & 0.39 & 0.40 & \textbf{0.60} & 0.27 & 0.36 & 0.28 & 0.08 & 0.00 & 0.33 & 0.32 & 0.57 & 0.05 & 0.20 \\
MultiFin & MicroF1 & 0.59 & \textbf{0.65} & 0.62 & 0.20 & 0.63 & 0.39 & 0.14 & 0.00 & 0.34 & 0.09 & 0.37 & 0.05 & 0.25 \\
MA & MicroF1 & 0.85 & 0.79 & 0.84 & 0.70 & \textbf{0.86} & 0.34 & 0.45 & 0.00 & 0.78 & 0.39 & 0.34 & 0.25 & 0.48 \\
MLESG & MicroF1 & 0.25 & \textbf{0.35} & 0.34 & 0.03 & 0.31 & 0.12 & 0.00 & 0.00 & 0.14 & 0.06 & 0.17 & 0.01 & 0.07 \\
\hline
FinQA & EmAcc & 0.58 & \textbf{0.63} & 0.00 & 0.00 & 0.06 & 0.00 & 0.04 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
TATQA & EmAcc & 0.00 & 0.13 & \textbf{0.18} & 0.03 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 \\
\multirow{2}{*}{Regulations}  & Rouge-1 & 0.12 & 0.11 & - & 0.24 & - & 0.10 & 0.12 & 0.01 & 0.04 & 0.03 & - & 0.14 & 0.21 \\
                      & BertScore & 0.64 & 0.62 & - & 0.65 & - & 0.60 & 0.59 & 0.40 & 0.57 & 0.14 & - & 0.57 & 0.80 \\
ConvFinQA & EmAcc & 0.60 & \textbf{0.76} & 0.43 & 0.00 & 0.25 & 0.00 & 0.20 & 0.00 & 0.00 & 0.00 & 0.31 & 0.01 & 0.00 \\
\hline
\multirow{2}{*}{EDTSUM}  & Rouge-1 & 0.17 & 0.20 & \textbf{0.39} & 0.17 & 0.25 & 0.14 & 0.13 & 0.00 & 0.13 & 0.15 & 0.12 & 0.01 & 0.08 \\
                      & BertScore & 0.66 & 0.67 & 0.72 & 0.62 & 0.68 & 0.60 & 0.38 & 0.52 & 0.48 & 0.57 & 0.61 & 0.51 & \textbf{0.79} \\
\multirow{2}{*}{ECTSUM}  & Rouge-1 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
                      & BertScore & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\hline
\multirow{2}{*}{BigData22}  & Acc & 0.53 & 0.54 & 0.55 & 0.54 & 0.47 & 0.55 & 0.51 & 0.45 & \textbf{0.56} & 0.55 & 0.46 & 0.45 & 0.37 \\
                      & MCC & -0.025 & 0.03 & 0.04 & 0.05 & 0.00 & 0.02 & 0.02 & 0.00 & \textbf{0.08} & 0.00 & 0.02 & 0.03 & -0.02 \\
\multirow{2}{*}{ACL18}  & Acc & 0.50 & \textbf{0.52} & 0.52 & 0.51 & 0.51 & 0.52 & 0.51 & 0.49 & 0.51 & 0.51 & 0.49 & 0.48 & 0.29 \\
                      & MCC & 0.005 & 0.02 & \textbf{0.04} & 0.01 & 0.01 & 0.02 & 0.03 & 0.00 & 0.02 & 0.00 & 0.00 & -0.03 & 0.01 \\
\multirow{2}{*}{CIKM18}  & Acc & 0.55 & \textbf{0.57} & 0.54 & 0.55 & 0.49 & 0.57 & 0.50 & 0.42 & 0.57 & 0.47 & 0.42 & 0.41 & 0.44 \\
                      & MCC & 0.01 & 0.02 & 0.02 & -0.03 & -0.07 & 0.03 & \textbf{0.08} & 0.00 & -0.03 & -0.06 & -0.05 & -0.07 & -0.01 \\
\hline
\multirow{2}{*}{German}  & F1 & 0.20 & 0.55 & 0.52 & \textbf{0.57} & 0.17 & 0.56 & 0.17 & 0.52 & 0.41 & 0.23 & 0.53 & 0.53 & 0.53 \\
                      & MCC & -0.10 & -0.02 & 0.00 & \textbf{0.03} & 0.00 & 0.05 & 0.00 & 0.00 & -0.30 & -0.07 & 0.00 & 0.00 & -0.04 \\
\multirow{2}{*}{Australian}  & F1 & 0.41 & \textbf{0.74} & 0.26 & 0.26 & 0.41 & 0.26 & 0.41 & 0.38 & 0.34 & 0.26 & 0.26 & 0.29 & 0.51 \\
                      & MCC & 0.00 & \textbf{0.47} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.11 & 0.13 & 0.00 & 0.00 & -0.10 & 0.08 \\
\multirow{2}{*}{LendingClub}  & F1 & 0.20 & 0.55 & 0.65 & \textbf{0.72} & 0.17 & 0.10 & 0.61 & 0.00 & 0.59 & 0.02 & 0.61 & 0.05 & 0.53 \\
                      & MCC & -0.10 & -0.02 & \textbf{0.19} & 0.00 & 0.00 & -0.15 & 0.00 & 0.00 & 0.15 & -0.01 & 0.08 & 0.01 & -0.02 \\
\multirow{2}{*}{ccf}  & F1 & 0.20 & 0.55 & 0.96 & 0.00 & 0.17 & 0.01 & 0.00 & 1.00 & \textbf{1.00} & 0.10 & 0.00 & 0.00 & 0.70 \\
                      & MCC & -0.10 & -0.02 & -0.01 & \textbf{0.00} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 \\
\multirow{2}{*}{ccfraud}  & F1 & 0.20 & 0.55 & \textbf{0.90} & 0.25 & 0.17 & 0.36 & 0.01 & 0.00 & 0.57 & 0.62 & 0.48 & 0.03 & 0.77 \\
                      & MCC & -0.10 & -0.02 & 0.00 & -0.16 & 0.00 & -0.03 & -0.06 & 0.00 & -0.13 & -0.02 & \textbf{0.16} & 0.01 & -0.01 \\
\multirow{2}{*}{polish}  & F1 & 0.20 & 0.55 & 0.86 & 0.92 & 0.17 & 0.83 & 0.92 & 0.30 & 0.92 & 0.76 & \textbf{0.92} & 0.40 & 0.30 \\
                      & MCC & -0.10 & -0.02 & \textbf{0.14} & 0.00 & 0.00 & -0.06 & -0.01 & 0.00 & 0.07 & 0.05 & 0.00 & -0.02 & 0.00 \\
\multirow{2}{*}{taiwan}  & F1 & 0.20 & 0.55 & \textbf{0.95} & 0.95 & 0.17 & 0.26 & 0.95 & 0.60 & 0.95 & 0.00 & 0.95 & 0.70 & 0.32 \\
                      & MCC & -0.10 & -0.02 & \textbf{0.00} & -0.01 & 0.00 & -0.07 & 0.00 & -0.02 & -0.01 & 0.00 & 0.00 & 0.00 & 0.02 \\
\multirow{2}{*}{portoseguro}  & F1 & 0.20 & 0.55 & 0.95 & 0.01 & 0.17 & 0.94 & 0.04 & \textbf{0.96} & 0.96 & 0.95 & 0.72 & 0.00 & 0.41 \\
                      & MCC & -0.10 & -0.02 & 0.00 & -0.05 & 0.00 & -0.01 & \textbf{0.01} & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 \\
\multirow{2}{*}{travelinsurance}  & F1 & 0.20 & 0.55 & 0.00 & 0.00 & 0.17 & 0.00 & 0.00 & \textbf{0.98} & 0.89 & 0.77 & 0.00 & 0.03 & 0.50 \\
                      & MCC & -0.10 & -0.02 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & \textbf{0.12} & -0.03 & 0.00 & 0.01 & 0.01 \\
\hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Table showing performance of different LLMs on the FinBen benchmark. The results are the average of three runs.}
\label{tab:allComparison}
\end{table}

\newpage
\section{Financial Agent knowledge grounding an LLM}\label{appendix:example}
% the \\ insures the section title is centered below the phrase: AppendixA
We  present an illustrative example showcasing the Financial Agent Framework processing a user's query, building the relevant context, constructing the modified query and then interacting with an LLM (GPT-3.5). 
%GPT-3.5 has been used for producing the final  generated by GPT-3.5 to the modified query:\\\\
\\ \\
\noindent\textbf{Query:} Based on their net income in the last quarter, should I invest in Pepsi or Coca Cola?\\
\newline \textbf{Financial Agent Formatted Data Request:}
%\newline Companies: PepsiCo, Inc.; Coca-Cola Co\\ Metrics: Net Income; Total Revenue; Cost of Revenue; Operating Expense; Depreciation and Amortization; Interest Expense\\Dates: last quarter\\
%\newline \textbf{Formatted Query as a data request for Data Module:} 
{\fontfamily{qcr}\selectfont
 (PepsiCo, Inc.; Coca-Cola Co) (Net Income; Total Revenue; Cost of Revenue; Operating Expense; Depreciation and Amortization; Interest Expense) \\(31/3/2023 - 30/6/2023)\\
}\\
\textbf{Data Module Response:} Retrieved tabular and news data 
\begin{center}
 \begin{tabular}{|p{1.3cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.9cm}|p{1.8cm}|}
 \hline
 \multicolumn{7}{|c|}{Table: 31/3/2023 - 30/6/2023}\\
 \hline
 \textbf{\scriptsize Entity}&
 \textbf{\scriptsize Net Income\newline (in \newline thousands) \newline (30/3/2023-29/6/2023) \newline (Quarterly)} &
 \textbf{\scriptsize Total Revenue \newline (in \newline thousands) \newline (30/3/2023-29/6/2023) \newline (Quarterly)} &
 \textbf{\scriptsize Cost of Revenue \newline (in \newline thousands)\newline (30/3/2023-29/6/2023) \newline (Quarterly)} &
 \textbf{\scriptsize Operating Expense\newline (in \newline thousands) \newline (30/3/2023-29/6/2023) \newline (Quarterly)} &
 \textbf{\scriptsize Depreciation and Amortization \newline (in \newline thousands)\newline (30/3/2023-29/6/2023) \newline (Quarterly)} &
 \textbf{\scriptsize Interest Expense \newline (in \newline thousands)\newline (30/3/2023-29/6/2023) \newline (Quarterly)}\\
 \hline
 \small PepsiCo, Inc. &\small [1932000, 2748000] &\small [17846000, 22322000]&\small [7988000, 10121000]&\small [7229000, 8542000]	&\small [706000, 810000]&\small [200000, 201000]\\ 
 \hline
 \small Coca-Cola Co &\small [3107000, 2547000]&\small [10980000, 11972000] &\small [4317000, 4912000]&\small [3296000, 4659000]&\small [286000, 281000] &\small [372000, 374000]\\ 
 \hline
 % \small Coca-Cola Co & \small [9771000,9542000]\\
 % \hline
\end{tabular}

\end{center}
\textit{[News: While PepsiCo's current performance is strong,
 some analysts expressed concern about the potential impact of inflation on consumer spending habits in the coming $\ldots$]}\\
\textbf{\\Enriched query provided to GPT 3.5:}\\
\textit{
\\You are an expert financial advisor. You will be provided with financial data and a financial query, and you have to answer the query based on the analysis of the data.\\
\\Financial Data:\\
\\Net Income (in thousands) (30/3/2023 - 29/6/2023) (Quarterly):\\
PepsiCo, Inc. : 1932000, 2748000\\
Coca-Cola Co : 3107000, 2547000\\
$\ldots$\\
$\ldots$\\
Interest Expense (in thousands) (30/3/2023 - 29/6/2023) (Quarterly):\\
\\PepsiCo, Inc. : 200000, 201000
\\Coca-Cola Co : 372000, 374000\\
\\News:  While PepsiCo's current performance is strong,
 some analysts expressed concern about the potential impact of inflation on consumer spending habits in the coming $\ldots$\\
\\Financial Query:
Based on their net income in the last quarter, should I invest in Pepsi or Coca Cola?
}\\
\\\textbf{GPT-3.5 response:}\\
\textit{
\\To determine whether you should invest in PepsiCo or Coca-Cola based on their net income in the last quarter, let's analyze the financial data:\\
\\Net Income (in thousands) for the last quarter (30/3/2023 - 29/6/2023):
\\- PepsiCo, Inc.: 2,748,000
\\- Coca-Cola Co: 2,547,000\\
\\Now, let's calculate some key financial ratios and provide additional insights:\\
\\1. **Profit Margin**: This ratio measures the percentage of revenue that turns into profit.
   \\- PepsiCo, Inc. Profit Margin = (Net Income / Total Revenue) * 100
   \\- PepsiCo, Inc. Profit Margin = (2,748,000 / 22,322,000) * 100 = 12.31\%
   \\- Coca-Cola Co Profit Margin = (Net Income / Total Revenue) * 100
   \\- Coca-Cola Co Profit Margin = (2,547,000 / 11,972,000) * 100 = 21.28\%\\
   \\2. **Comparison**: Coca-Cola Co has a higher profit margin (21.28\%) compared to PepsiCo, Inc (12.31\%) for the last quarter. This indicates that Coca-Cola is more efficient at converting its revenue into profit during this period.\\
   \\3. **Considerations**: While profit margin is an important metric, it's essential to consider other factors like the company's growth prospects, competitive positioning, dividend yield, and long-term financial stability when making investment decisions. You should also consider your investment goals, risk tolerance, and portfolio diversification strategy.\\
\\4. **Conclusion**: Based solely on the net income in the last quarter, Coca-Cola Co appears to have a stronger performance in terms of profit margin. However, this is just one aspect of the investment decision. It's recommended to conduct a more comprehensive analysis and consider the broader financial health and strategic outlook of both companies before making an investment decision. Remember that investing involves risks, and it's advisable to consult with a financial advisor and conduct thorough research before making any investment decisions. Additionally, past performance may not guarantee future results, so it's essential to consider a company's overall financial health and long-term prospects.
}

\end{document}



% \begin{table}[h!]
% \centering
% \setlength{\arrayrulewidth}{0.1mm}
% \setlength{\tabcolsep}{4.1pt}
% \renewcommand{\arraystretch}{1.7}
% \fontsize{7pt}{6pt}\selectfont
% \begin{tabular} 
% { m{2.5cm} m{2.5cm} m{2.5cm} m{2.5cm} m{2.2cm} m{2.1cm} }
% \hline
% \textbf{Dataset} & \textbf{Metrics} & \textbf{FinMA 7B} & \textbf{FinGPT 7B-lora} & \textbf{CFGPT \newline sft-7B-Full} & \textbf{FinBloom 7B LLM} \\
% \hline
% NER & EntityF1 & 0.69 & 0.00 & 0.00 & 0.00 \\
% FINER-ORD & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
% FinRED & F1 & 0.00 & 0.00 & 0.00 & 0.00 \\
% SC & F1 & 0.19 & 0.00 & 0.15 & 0.84 \\
% CD & F1 & 0.00 & 0.00 & 0.00 & 0.00 \\
% FNXL & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
% FSRL & EntityF1 & 0.00 & 0.00 & 0.00 & 0.00 \\
% \hline
% FPB  & F1 & \textbf{0.88} & 0.00 & 0.35 & 0.32 \\
%                       % & Acc & \textbf{0.88} & 0.00 & 0.26 & 0.31 \\
% FiQA-SA & F1 & 0.79 & 0.00 &0.42 & 0.47 \\
% % TSA & RMSE & 0.80 & 0.00 & 1.05 & 0.77 \\
% Headlines & AvgF1 & \textbf{0.97} & 0.60 & 0.61 & 0.45 \\
% FOMC  & F1 & 0.49 & 0.00 & 0.16 & 0.20\\
%                       % & Acc & 0.46 & 0.00 & 0.21 & 0.25 \\
% FinArg-AUC & MicroF1 & 0.27 & 0.00 & 0.05 & \textbf{0.81} \\
% FinArg-ARC & MicroF1 & 0.08 & 0.00 & 0.05 & 0.20 \\
% MultiFin & MicroF1 & 0.14 & 0.00 & 0.05 & 0.25 \\
% MA & MicroF1 & 0.45 & 0.00 & 0.25 & 0.48 \\
% MLESG & MicroF1 & 0.00 & 0.00 & 0.01 & 0.07 \\
% \hline
% FinQA & EmAcc & 0.04 & 0.00 & 0.00 & 0.00 \\
% TATQA & EmAcc & 0.00 & 0.00 & 0.00 & 0.00 \\
% \multirow{2}{*}{Regulations}  & Rouge-1 & 0.12 & 0.01 & 0.14 & 0.21 \\
%                       & BertScore & 0.59 & 0.40 & 0.57 & 0.80 \\
% ConvFinQA & EmAcc & 0.20 & 0.00 & 0.01 & 0.00 \\
% \hline
% \multirow{2}{*}{EDTSUM}  & Rouge-1 & 0.13 & 0.00 & 0.01 & 0.08 \\
%                       & BertScore & 0.38 & 0.52 & 0.51 & \textbf{0.79} \\
% \multirow{2}{*}{ECTSUM}  & Rouge-1 & 0.00 & 0.00 & 0.00 & 0.00 \\
%                       & BertScore & 0.00 & 0.00 & 0.00 & 0.00 \\
% \hline
% \multirow{2}{*}{BigData22}  & Acc & 0.51 & 0.45 & 0.45 & 0.37 \\
%                       & MCC & 0.02 & 0.00 & 0.03 & -0.02 \\
% \multirow{2}{*}{ACL18}  & Acc & 0.51 & 0.49 & 0.48 & 0.29 \\
%                       & MCC & 0.03 & 0.00 & -0.03 & 0.01 \\
% \multirow{2}{*}{CIKM18}  & Acc & 0.50 & 0.42 & 0.41 & 0.44 \\
%                       & MCC & \textbf{0.08} & 0.00 & -0.07 & -0.01 \\
% \hline
% \multirow{2}{*}{German}  & F1 & 0.17 & 0.52 & 0.53 & 0.53 \\
%                       & MCC & 0.00 & 0.00 & 0.00 & -0.04 \\
% \multirow{2}{*}{Australian}  & F1 & 0.41 & 0.38 & 0.29 & 0.51 \\
%                       & MCC & 0.00 & 0.11 & -0.10 & 0.08 \\
% \multirow{2}{*}{LendingClub}  & F1 & 0.61 & 0.00 & 0.05 & 0.53 \\
%                       & MCC & 0.00 & 0.00 & 0.01 & -0.02 \\
% \multirow{2}{*}{ccf}  & F1 & 0.00 & 1.00 & 0.00 & 0.70 \\
%                       & MCC & 0.00 & 0.00 & 0.00 & -0.01 \\
% \multirow{2}{*}{ccfraud}  & F1 & 0.01 & 0.00 & 0.03 & 0.77 \\
%                       & MCC & -0.06 & 0.00 & 0.01 & -0.01 \\
% \multirow{2}{*}{polish}  & F1 & 0.92 & 0.30 & 0.40 & 0.30 \\
%                       & MCC & -0.01 & 0.00 & -0.02 & 0.00 \\
% \multirow{2}{*}{taiwan}  & F1 & 0.95 & 0.60 & 0.70 & 0.32 \\
%                       & MCC & 0.00 & -0.02 & 0.00 & 0.02 \\
% \multirow{2}{*}{portoseguro}  & F1 & 0.04 & \textbf{0.96} & 0.00 & 0.41 \\
%                       & MCC & \textbf{0.01} & 0.00 & 0.00 & 0.00 \\
% \multirow{2}{*}{travelinsurance}  & F1 & 0.00 & \textbf{0.98} & 0.03 & 0.50 \\
%                       & MCC & 0.00 & 0.00 & 0.01 & 0.01 \\
% \hline
% \textbf{Average} & & 0.2267 & 0.1576 & 0.1337 & \textbf{0.2378} \\
% \hline
% \end{tabular}
% \label{table:2}
% \captionsetup{justification=centering}
% \caption{Table showing performance of different Financial LLMs on the FinBen benchmark. The results are the average of three runs.}
% \end{table}\\