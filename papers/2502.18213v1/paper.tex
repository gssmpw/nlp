\documentclass[11pt]{article}
\usepackage[a4paper, margin = 1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb, amsthm, epsfig}
%\usepackage{euscript}
\usepackage{mathrsfs}
%\usepackage{bbold}

\usepackage{graphicx}
\usepackage{verbatim}
%\usepackage{color}
%\usepackage{epstopdf}
\usepackage{microtype}
\usepackage{mathtools}
%\usepackage{float}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}

\usepackage{xspace}
\usepackage{xcolor}
\usepackage{varwidth}
%\usepackage{wrapfig}
\usepackage{bbm}
\usepackage[colorlinks]{hyperref}
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={cyan!50!black},
	urlcolor={blue!80!black}
}
%\usepackage{multirow}
\usepackage{cleveref}
\usepackage{natbib} 











\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bA}{\bm{A}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bQ}{\bm{Q}}
\newcommand{\bR}{\bm{R}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bX}{\bm{X}}
\newcommand{\be}{\bm{e}}
\newcommand{\bq}{\bm{q}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\fixed}{\mathsf{fixed}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}

\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Lip}{\mathrm{Lip}}
\newcommand{\opt}{\mathsf{OPT}}
\newcommand{\OPT}{\mathsf{OPT}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\Roi}{\mathcal{R}}
\newcommand{\whatfixed}{\hat{\bw}_\fixed}
% \newcommand{\qed}{\tag*{$\square$}}
% \newcommand{\fxw}[2]{f(\langle \bx_{#1}, #2 \rangle)}
\newcommand{\Net}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\reg}{\mathsf{reg}}
\newcommand{\Err}{\mathsf{Err}}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Diam}{Diam}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\colspace}{colspace}
















\newcommand{\s}[1]{{\small \textsf{#1}}}
%\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\newcommand{\Eu}[1]{\ensuremath{\EuScript{#1}}}
% \newcommand{\eps}{\varepsilon}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}
% \newcommand{\R}{\ensuremath{\mathbb{R}}}

\newcommand{\dir}{\ensuremath{\mathsf{d}}}

\newcommand{\etal}{et al.\xspace}

% \newcommand{\disc}{\textsf{disc}}
%\newcommand{\area}{\textsf{area}}
%\newcommand{\vol}{\textsf{vol}}
%\newcommand{\rad}{\textsf{rad}}
%\newcommand{\width}{\textsf{wid}}
% \DeclareMathOperator{\kde}{\textsc{kde}}
\DeclareMathOperator{\poly}{\mathsf{poly}}
% \newcommand{\herdisc}{\textsf{herdisc}}
\newcommand{\Var}{\ensuremath{\textsf{Var}}}
%\newcommand{\prob}[1]{\ensuremath{\textbf{{\sffamily Pr}}\hspace{-.8mm}\left[#1\right]}}

%\newcommand{\km}{\ensuremath{\hat{\mu}}}
%\renewcommand{\H}{\ensuremath{{\Eu{H}_K}}}

\DeclarePairedDelimiterX\setdef[2]{\{}{\}}{\,#1 \;\delimsize\vert\; #2\,}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\inner}[2]{\langle}{\rangle}{#1 , #2}
\DeclarePairedDelimiterXPP\normp[1]{}{\lVert}{\rVert}{_p}{#1}
\DeclarePairedDelimiterXPP\normtwo[1]{}{\lVert}{\rVert}{_2}{#1}
\DeclarePairedDelimiterXPP\norminf[1]{}{\lVert}{\rVert}{_\infty}{#1}
\DeclarePairedDelimiterXPP\normpI[1]{}{\lVert}{\rVert}{_{I,p}}{#1}
\DeclarePairedDelimiterXPP\norminfI[1]{}{\lVert}{\rVert}{_{I,\infty}}{#1}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\Unif}{\ensuremath{\mathsf{Unif}}}

%\newcommand{\DK}{\ensuremath{\texttt{D}^2_K}}
%\newcommand{\DKo}{\ensuremath{\texttt{D}_K}}

% \newcommand{\E}{\ensuremath{\textsf{E}}}
%\newcommand{\G}{\mathcal{G}}
%\newcommand{\Gr}{\textsf{Grid}}
%\newcommand{\GPi}{\mathcal{G}^\Pi}
%\newcommand{\Gb}{\overline{\mathcal{G}}}
%\newcommand{\ilog}{\textup{ilog}}
%\newcommand{\Tr}{\ensuremath{\textsf{Tr}}}

\newcommand{\waiming}[1]{{\color{red}$\langle$#1$\rangle$}}
\newcommand{\todo}[1]{{\color{blue}$\langle$TODO: #1$\rangle$}}
%\newcommand{\E}{\ensuremath{\textsf{E}}}


\newcommand{\given}{\,|\,}  % verbose alternative to \|
\newcommand{\conv}{*} 
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\diag}{diag}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{problem}[theorem]{Problem}


\usepackage[inline]{enumitem}







% Convenient macro for editing, internal notes, etc
\newcommand{\edit}[1]{\textcolor{red}{\textbf{[#1]}}}
\newcommand{\hilite}[1]{\textcolor{red}{#1}}
\newcommand{\extra}[1]{\textcolor{black}{#1}}
\newcommand{\note}[1]{\textcolor{blue}{\textbf{\texttt{[#1]}}}}
\newcommand{\ans}[1]{\textcolor{magenta}{\textbf{\texttt{[#1]}}}}

\newcommand{\bZ}{\mathbf{Z}}
% \newcommand{\opt}{\mathsf{OPT}}
\newcommand{\wh}[1]{\widehat{#1}}
% \newcommand{\poly}{\mathsf{poly}}
\newcommand{\lip}{\mathsf{Lip}}
% \newcommand{\wf}{\widehat{f}}
% \newcommand{\wb}{\widehat{b}}
% \newcommand{\wx}{\widehat{x}}


\title{Near-optimal Active Regression of Single-Index Models}
\author{
    Yi Li\thanks{Supported in part by Singapore Ministry of Education AcRF Tier 2 grant MOE-T2EP20122-0001 and Tier 1 grant RG75/21.} \\
    School of Physical and Mathematical Sciences \\ and College of Computing and Data Science \\
    Nanyang Technological University \\
    \texttt{yili@ntu.edu.sg}
    \and
    Wai Ming Tai\thanks{Supported by Singapore Ministry of Education AcRF Tier 2 grant MOE-T2EP20122-0001 when the author was affiliated with Nanyang Technological University, where  most of this work was done.} \\ 
    Independent Researcher \\ 
    \texttt{taiwaiming2003@gmail.com}
}

\begin{document}
\maketitle




\begin{abstract}
The active regression problem of the single-index model is to solve $\min_x \lVert f(Ax)-b\rVert_p$, where $A$ is fully accessible and $b$ can only be accessed via entry queries, with the goal of minimizing the number of queries to the entries of $b$.
When $f$ is Lipschitz, previous results only obtain constant-factor approximations. This work presents the first algorithm that provides a $(1+\varepsilon)$-approximation solution by querying $\tilde{O}(d^{\frac{p}{2}\vee 1}/\varepsilon^{p\vee 2})$ entries of $b$. This query complexity is also shown to be optimal up to logarithmic factors for $p\in [1,2]$ and the $\varepsilon$-dependence of $1/\varepsilon^p$ is shown to be optimal for $p>2$.
\end{abstract}

\section{Introduction}
Active regression, an extension of the classical regression model, has gained increasing attention in recent years. In its most basic form, active regression aims to solve $\min_{x\in\R^d} \norm*{Ax-b}_p$ ($p\geq 1$), where the matrix $A\in \R^{n\times d}$ represents $n$ data points in $\R^d$ and the vector $b\in \R^n$ represents the corresponding labels. However, since label access can be expensive, the challenge is to minimize the number of entries of $b$ that are accessed while still solving the regression problem approximately. 
A typical guarantee of the approximate solution is to find $\hat x\in\R^d$ such that
\begin{equation}\label{eqn:classical_active_regression}
	\norm*{A\hat x-b}_p \leq (1+\eps)\norm*{A x^\ast - b}_p,
\end{equation}
where $x^\ast$ is the optimal solution to the regression problem, i.e., $x^* = \argmin_{x\in\mathbb{R}^d} \norm*{Ax-b}_p$. 

Research on this problem often focuses on randomized algorithms with constant failure probability, i.e., the entries of $b$ are sampled randomly (but typically not uniformly) and the output $\hat x$ satisfies the error guarantee above with probability at least a large constant. When $p=2$, \citet{CP19} showed sampling $O(d/\eps)$ entries of $b$ suffices, and when $p=1$, \citet{PPP21} showed an optimal query complexity of $\Theta(d/\eps^2)$. The case of general $p$ was later settled by \citet{musco2022active}, who showed a query complexity of $\tilde{O}(d/\eps)$ for $p\in (1,2]$ and $\tilde{O}(d^{p/2}/\eps^{p-1})$ for $p > 2$. Their proof was later refined by \citet{taisuke:thesis}.

A more general form of the regression problem is the single-index model, which, in our context, asks to solve
\[
\min_{x\in \R^d} \norm*{f(Ax) - b}_p,
\]
where $f:\R \to \R$ is a nonlinear function and, for $u\in \R^n$, we abuse the notation slightly to denote $f(u) = (f(u_1),\dots,f(u_n))$, i.e., applying $f$ entrywise. This formulation arises naturally in neural networks and has received significant recent attention (see, e.g., \citep{diakonikolas20,gajjar2023active,ICLR24} and the references therein). A neural network can be viewed as the composition of a network backbone and a linear prediction layer $x\in\R^d$ with an activation function $f$, where a typical choice of $f$ is the ReLU function. The network's prediction is given by $f(Ax)$, where the matrix $A$ is the feature matrix generated by the network backbone from the dataset. The goal is to learn the linear prediction layer $x$, which corresponds to solving the regression problem.

In this paper, we consider $f$ to be a general $L$-Lipschitz function. 
It is tempting to expect an analogous guarantee as \Cref{eqn:classical_active_regression} for the canonical active regression problem, i.e. to find $\hat{x}\in \mathbb{R}^d$ such that
\begin{align*}
	\normp{f(A\hat{x}) - b} \leq (1+\eps) \normp{f(Ax^*) - b}
\end{align*}
where $x^* = \argmin_{x\in \mathbb{R}^d} \normp{f(Ax) - b}$.
However, \citet{gajjar2023active} showed that achieving this guarantee with a $\poly(d)$ query complexity is impossible even when $f$ is a ReLU function and $\eps$ is a constant.
Hence, we can only expect a weaker guarantee.
The single-index regression problem was studied for $p=2$ in the same paper~\citep{gajjar2023active}, which showed that sampling $O(d^2/\eps^4)$ entries suffices to find an $\hat x\in\R^d$ such that
\begin{equation}\label{eqn:constant_approximation_p=2}
	\normtwo{f(A\hat{x}) - b}^2 \leq C(\normtwo{f(Ax^\ast) - b}^2 + \eps L^2 \normtwo{Ax^*}^2),
\end{equation}
where $x^\ast = \argmin_{x\in\mathbb{R}^d} \norm*{f(Ax)- b}_2$ is the minimizer and $C$ is an absolute constant.
For general $p$, \citet{ICLR24} obtained
\begin{equation}\label{eqn:constant_approximation_p}
	\normp{f(A\hat{x}) - b}^p \leq C(p)(\normp{f(Ax^\ast) - b}^p + \eps L^p \normp{Ax^*}^p)
\end{equation}
for some constant $C(p)$ depending only on $p$, using $O(d/\eps^4)$ samples when $1\leq p\leq 2$ and $O(d^{p/2}/\eps^4)$ samples when $p>2$. For $p=2$, \citet{gajjar2023improved} also independently obtained $\tilde{O}(d/\eps^4)$ samples  and, with additional co-authors, further improved the query complexity to $\tilde{O}(d/\eps^2)$ in \citep{COLT24}. 

The main drawback of the existing results for the single-index model, compared to the basic form of active regression, is that \Cref{eqn:constant_approximation_p=2} and \Cref{eqn:constant_approximation_p} only achieve constant-factor approximations,  whereas \Cref{eqn:classical_active_regression} achieves a guarantee of $(1+\eps)$-approximation. The goal of this paper is to obtain a $(1+\eps)$-approximation for the single-index model.

Note that all existing results assume access to an oracle solver for the regression problem of the form $\argmin_{x\in \mathbb{R}^d} \normp{f(A'x)-b'}$ or its regularized variant, which may not be a convex programme and where the objective function may be non-differentiable due to $f$. 
In this work, we retain the assumption of having such an oracle solver.

\subsection{Problem Definition}

Now we define our problem formally. For $L\geq 0$, let $\lip_L$ denote the set of $L$-Lipschitz functions $f$ such that $f(0) = 0$, i.e. 
\[
\lip_L := \setdef[\big]{f:\mathbb{R}\to \mathbb{R}}{f(0)=0\text{ and } \abs{f(x)-f(y)}\leq L\cdot\abs{x-y}\text{ for all $x,y\in \mathbb{R}$}}.
\]
Suppose we are given a function $f\in\lip_L$, 
% an error parameter $\eps > 0$, 
a matrix $A\in\mathbb{R}^{n\times d}$ and a query access to the entries of an unknown $n$-dimensional vector $b\in \mathbb{R}^n$.
Define
\begin{align*}
	\opt
	:=
	\min_{x\in\mathbb{R}^d}\normp{f(Ax) - b}^p 
	\quad\text{and}\quad
	x^*
	:=
	\argmin_{x\in \mathbb{R}^d: \normp{f(Ax)-b}^p=\opt} \normp{Ax}^p.
\end{align*}
That is, if there are multiple minimizers $x^*$, we choose an arbitrary one that minimizes $\norm*{Ax^\ast}_p$.
As noted in~\citep{COLT24}, there is no loss of generality in assuming $f(0) = 0$ because one can shift both $f(x)$ and $b$ by $f(0)$.
For an error parameter $\eps > 0$, our goal is to find an $\hat{x}\in\R^d$ such that 
\[
\normp{f(A\hat{x}) - b}^p
\leq (1+\eps)\opt + C\eps \normp{Ax^*}^p,
\]
where $C$ is some constant that depends only on $L$ and $p$ while the number of queries to the entries of $b$ is minimized.
Therefore, we would like to ask:
\begin{quote}
	\emph{What is the minimum number (in terms of $d$ and $\eps$) of queries to the entries of $b$ needed in order to achieve this goal?}
\end{quote}
We will solve this problem in this paper.

\subsection{Our Results}

We first present the main result of this paper that querying $O(d/\eps^2 \poly\log n)$ entries of $b$ is sufficient for a $(1+\eps)$-approximation when $1\leq p\leq 2$ and $O(d^{p/2}/\eps^p \poly\log n)$ entries when $p > 2$. Our result achieves the same query complexity (up to logarithmic factors) for the constant-factor approximation algorithm in~\citep{COLT24} when $p=2$.

\begin{theorem}\label{thm:main_upper}
	There is a randomized algorithm, when given $A\in \R^{n\times d}$, $b\in \R^n$, $f\in \lip_L$ and an arbitrary sufficient small $\eps > 0$,
	with probability at least $0.9$, makes $O\big(d^{1\vee\frac{p}{2}}/\eps^{2\vee p} \cdot \poly\log(d/\eps)\big)$ queries to the entries of $b$ and returns an $\hat{x}\in \R^d$ such that
	\begin{equation}\label{eq:general_error_guarantee}
		\normp{f(A\hat{x}) - b}^p
		\leq
		\opt + \eps(\opt + L^p\normp{Ax^*}^p).
	\end{equation} 
	The hidden constant in the bound on number of queries depends on $p$ only.
\end{theorem}

Recall that in the canonical active regression problem, i.e.\ $f(x) = x$, the query complexity is $\tilde{\Theta}(d/\eps)$ when $1 < p\leq 2$ and $\tilde{\Theta}(d^{p/2}/\eps^{p-1})$ when $p>2$~\citep{musco2022active}. We can show that accommodating a general $f$ pushes up the $\eps$-dependence to $1/\eps^2$ and $1/\eps^p$, respectively. In particular, for $1\leq p\leq 2$, we show a lower bound of $\Omega(d/\eps^2)$ queries, which suggests that our upper bound is tight up to logarithmic factors. The following are formal statements on our lower bound.

\begin{theorem}\label{thm:main_lower}
	Suppose that $p\geq 1$, $\eps > 0$ is sufficiently small and $n\gtrsim (d\log d)/\eps^2$. Any randomized algorithm that, given $A\in \R^{n\times d}$, a query access to the entries of an unknown $b\in \R^n$ and $f\in \lip_1$, outputs a  $d$-dimensional vector $\hat{x}$ such that \Cref{eq:general_error_guarantee} holds with probability at least $4/5$ must make $\Omega(d/\eps^2)$ queries to the entries of $b$.
\end{theorem}

\begin{theorem}\label{thm:main_lower_p>2}
	Suppose that $p > 2$, $\eps > 0$ is sufficiently small and $n\gtrsim d/\eps^p$. Any randomized algorithm that, given $A\in \R^{n\times d}$, a query access to the entries of an unknown $b\in \R^n$ and $f\in \lip_1$, outputs a  $d$-dimensional vector $\hat{x}$ such that \Cref{eq:general_error_guarantee} holds with probability at least $4/5$ must make $\Omega(d/\eps^p)$ queries to the entries of $b$.
\end{theorem}


\section{Proof Overview}

In this section, we will provide a proof overview of our theorems. 

\subsection{Upper Bound} \label{sec:upper_overview}

\paragraph{General Observations}
We follow the usual ``sample-and-solve'' paradigm as in many previous active regression algorithms~\citep{CP19,musco2022active,gajjar2023active,CLS2022,ICLR24}. 
Querying entries of $b$ can be viewed as multiplying $b$ with a sampling matrix $S$, which is a diagonal matrix with sparse diagonal entries; the nonzero entries in $Sb$ correspond to the queried entries of $b$. 
Hence, we would like to minimize the number of nonzero diagonal entries in $S$.
The same sampling matrix $S$ is also applied to $f(Ax)$, leading to a natural attempt at solving the optimization problem $\min_{x\in \R^d} \normp{S(f(Ax) - b)}^p$. 
However, we preview that this optimization problem is not the one we seek and we will provide more explanation on how to modify it.

The natural question is how to design the sampling matrix $S$. In all previous work, $S$ is a row-sampling matrix with respect to Lewis weights; see the statement of Lemma~\ref{lem:SE}. 
In this paper, we adopt the same approach.
This means that (i) (unbiased estimator) $\E \normp{Sv}^p = \normp{v}^p$ for each $v\in\R^n$ and (ii) (subspace embedding) when $S$ has sufficiently many nonzero rows, $\normp{SAx} \approx \normp{Ax}$ for all $x\in\R^d$. Note that the query complexity for active regression is usually higher than necessary for subspace embedding alone.

% We will present the full algorithm in \Cref{sec:full_alg}.


\paragraph{Formulating the Concentration Bounds}
Let $\hat{x} = \argmin_{x\in \mathbb{R}^d} \normp{S(f(Ax) - b)}^p$. 
Although $\E\normp{S(f(Ax) - b)}^p = \normp{f(Ax) - b}^p$ for \emph{each}  $x\in \mathbb{R}^d$, it is unclear what $\E\normp{S(f(A\hat{x}) - b)}^p$ is 
% whether $\E\normp{S(f(A\hat{x}) - b)}^p = \normp{f(A\hat{x}) - b}^p$ 
since $\hat{x}$ depends on $S$. 
To address this, we instead argue that the sampling error $\abs*{\normp{S(f(Ax) - b)}^p - \normp{f(Ax) - b}^p}$ is small for \emph{all} $x\in T$, where $T$ is a ``small'' bounded domain containing $\hat{x}$. 
Here, the small size of $T$ is critical for a small error bound since it controls the number of points in $T$ at which the error needs to be  small  when applying Dudley's integral, a classical extension of the net argument.
% However,  this sampling error is unlikely to be small with high probability for each $x$ and it is only more unlikely that the sampling error remains small over the entire domain $T$. 
% To address this, 
To further reduce the variance,
we choose $\normp{S(f(Ax^\ast) - b)}^p - \normp{f(Ax^\ast) - b}^p$ as a reference point and argue that the difference
\begin{align*}
	\Err(x) = \abs{(\normp{S(f(Ax) - b)}^p - \normp{f(Ax) - b}^p ) - (\normp{S(f(Ax^*) - b)}^p - \normp{f(Ax^*) - b}^p)}.\numberthis\label{eq:concen_intro}
\end{align*}
is uniformly small over $T$. 
Note that the reference point can be $\normp{S(f(A\bar{x}) - b)}^p - \normp{f(A\bar{x}) - b}^p$ for any $\bar{x}\in \R^d$.
This approach has been employed by \citet{taisuke:thesis} for 
the canonical active regression (i.e.\xspace $f(x) = x$). 
However, for general Lipschitz functions $f$, 
a key challenge is to identify an appropriate domain $T$, which we shall discuss below.

\paragraph{Regularized Regression}
% If we simply take $\hat{x} = \argmin_{x\in \mathbb{R}^d} \normp{S(f(Ax) - b)}^p$, it turns out
As previously noted, the optimization problem $\min_{x\in \R^d} \normp{S(f(Ax) - b)}^p$ is not the one we are seeking.
It turns out that there is a fundamental challenge to argue that $\hat{x} = \argmin_{x\in \mathbb{R}^d} \normp{S(f(Ax) - b)}^p$ satisfies the desired guarantee~\Cref{eq:general_error_guarantee}.
In the canonical active regression, 
one can show that
\begin{equation}\label{eq:standard_reg_radius}
	\normp{A(\hat{x} - x^*)} \lesssim \normp{Ax^* - b}.
\end{equation}
This suggests that $\hat{x}\in T$ for $T = \setdef{x\in\mathbb{R}^d}{\normp{A(x-x^*)} \lesssim \normp{Ax^* - b}}$, which is a ``\emph{small}'' bounded region.
% Here, the small size of $T$ is critical for a small error bound. 
% \note{if we have space we can put the explanation of having finite points in $T$ being helpful when we apply dudley integral. otherwise, it seems there is a gap between small $T$ and small error.}
Unfortunately, for a Lipschitz function $f$, it is not clear how to obtain an analogy to \Cref{eq:standard_reg_radius} and thus a bounded region $T$ containing $\hat{x}$. 
Hence, we still seek a bound on the norm $\normp{A\hat{x}}^p$ to keep $T$ bounded and ideally small.
For constant-factor approximations, previous work \citep{gajjar2023active,ICLR24} restrict the domain in the regression problem and solve $\min_{x\in T} \normp{S(f(Ax) - b)}^p$ for some ``small" $T$, but this leads to a poor $\eps$-dependence of $1/\eps^4$ in query complexity. 

To improve the $\eps$-dependence, one can consider a regularized regression problem
\[
\min_{x\in \mathbb{R}^d}\ \normp{S(f(Ax) - b)}^p + \tau \cdot \normp{Ax}^p,
\]
where $\tau > 0$ is a regularization parameter. This approach, as demonstrated by \citet{COLT24}, improves the $\eps$-dependence for constant-factor approximations when $p=2$, and will therefore be adopted in this paper. 
To ease the notation, assume that the Lipschitz constant $L=1$ from now on.

An important question is how to choose the regularization parameter $\tau$. 
Recall that we want the sampling error $\Err(x)$ (defined in \Cref{eq:concen_intro}) to be small, ideally close to $0$, when $x=\hat{x}$. Then
\begin{align*}
	\normp{f(A\hat{x}) -b}^p - \opt
	& =
	\normp{f(A\hat{x}) -b}^p - \normp{f(Ax^*) - b}^p \\
	& \leq 
	\normp{S(f(A\hat{x}) -b)}^p - \normp{S(f(Ax^*) - b)}^p + \text{error terms}  \\
	& \leq
	\tau \cdot  \normp{Ax^*}^p + \text{error terms},
\end{align*}
where the last inequality follows from the optimality of $\hat{x}$.
The desired guarantee \Cref{eq:general_error_guarantee} has an additive error $\eps\normp{Ax^*}^p$, indicating that $\tau$ should be set smaller than $\eps$.
On the other hand, the main purpose of regularization is to bound $\normp{A\hat{x}}^p$.
By the optimality of $\hat{x}$ and rearranging the terms, we have
\begin{align*}
	\normp{A\hat{x}}^p
	& \leq
	\frac{1}{\tau}\cdot \bigg(\normp{S(f(Ax^*) - b)}^p - \normp{S(f(A\hat{x}) - b)}^p\bigg) + \normp{Ax^*}^p.
\end{align*}
This suggests that $\tau$ should be set as large as possible for a better bound on $\normp{A\hat{x}}^p$.
Therefore, we set $\tau = \eps$ and the optimization problem becomes
\begin{align*}
	\min_{x\in \mathbb{R}^d} \normp{S(f(Ax) - b)}^p + \eps \cdot \normp{Ax}^p.\numberthis\label{eq:overview_reg_prob}
\end{align*}

\paragraph{Bounding the Error}

To avoid overloading our notation, we focus on $1\leq p\leq 2$. A similar argument works for $p\geq 2$. Recall that our intention is to bound
% As noted in \Cref{eq:concen_intro}, 
% let
% \begin{align*}
	%     \Err(x):= \abs*{(\normp{S(f(Ax) - b)}^p - \normp{S(f(Ax^*) - b)}^p) - (\normp{f(Ax) - b}^p - \normp{f(Ax^*) - b}^p) }
	% \end{align*}
% and we aim to find an upper bound of the following sampling error:
\begin{align*}
	\sup_{x\in T} \Err(x),
\end{align*}
where $\Err(x)$ is the sampling error defined in \Cref{eq:concen_intro}.
The first issue we need to resolve is defining the domain $T$.
By the optimality of $\hat{x}$ in \Cref{eq:overview_reg_prob} and rearranging the terms, we have
\begin{align*}
	\normp{A\hat{x}}^p
	& \leq
	\frac{1}{\eps}\bigg(\normp{S(f(Ax^*) - b)}^p - \normp{S(f(A\hat{x}) - b)}^p\bigg) + \normp{Ax^*}^p \numberthis\label{eq:radius_recur}\\
	& \leq
	\frac{1}{\eps}\normp{S(f(Ax^*) - b))}^p + \normp{Ax^*}^p \\
	& \lesssim
	\frac{1}{\eps}\opt + \normp{Ax^*}^p \quad \text{by Markov inequality.} \numberthis\label{eq:first_attemp_rad_cal}
\end{align*}
Hence, we can set
\begin{align*}
	R
	& =
	\frac{1}{\eps}\opt + \normp{Ax^*}^p
	\quad \text{and}\quad
	T
	=
	\setdef{x\in\R^d}{\normp{Ax}^p \lesssim R}.
\end{align*}
Then $\hat{x}\in T$.
Now, while we omit the details, we obtain the following concentration bound in \Cref{eq:main_concen_first_attempt} by following the standard technique of upper bounding the supremum of a stochastic process using Dudley's integral, which has been the central tool in the work on subspace embeddings~\citep{BLM89,LT91,CP15} and previous work on active regression~\citep{musco2022active,CLS2022,ICLR24}.
In short, when the number of queries is $m$, we can obtain that with probability at least $1-\delta$,
\begin{align*}
	\sup_{x\in T}\Err(x) \lesssim
	\sqrt{\frac{d\poly(\log n, \log \delta^{-1})}{m}}\cdot R. \numberthis\label{eq:main_concen_first_attempt}
\end{align*}
We preview here that this concentration bound will yield a weaker result, but it serves to illustrate the key idea and will guide us in refining the analysis later. 

Suppose that $m\sim \frac{d}{\eps^4}\poly\log n$.
By plugging $m$ and $R$ into \Cref{eq:main_concen_first_attempt}, we have with constant probability, 
\begin{align*}
	\sup_{x\in T}\Err(x) \lesssim
	\eps(\opt + \eps \normp{Ax^*}^p).
\end{align*}
Since $\hat{x}\in T$, we have
\begin{align*}
	\normp{f(A\hat{x}) - b}^p - \opt
	& \lesssim
	\normp{S(f(A\hat{x}) - b)}^p - \normp{S(f(Ax^*) - b)}^p + \eps (\opt + \eps\normp{Ax^*}^p) \\
	& \leq
	\eps \normp{Ax^*}^p + \eps (\opt + \eps\normp{Ax^*}^p) \quad\text{by the optimality of $\hat{x}$ in \Cref{eq:overview_reg_prob}} \\
	& \lesssim 
	\eps(\opt + \normp{Ax^*}^p), \numberthis\label{eq:first_attemp_err_cal}
\end{align*}
which achieves the desired guarantee \Cref{eq:general_error_guarantee} by a rescaling of $\eps$.





\paragraph{Attempt to Improve}
% \note{point out the difference between our recursive steps and the recursive steps in the focs paper}
The reason we previously set $m\sim \frac{d}{\eps^4}\poly\log n$ is because $R = \frac{1}{\eps}\opt + \normp{Ax^*}^p$ and we need the square root term in \Cref{eq:main_concen_first_attempt} to be $\eps^2$ so that the overall error is at most $\eps(\opt + \normp{Ax^*}^p)$.
Indeed, when we compare to the canonical case of $f(x)=x$ and bound the radius of the domain in \Cref{eq:standard_reg_radius}, this large $R$ is the main reason why extra factors of $\frac{1}{\eps}$ are needed.

Notice that the term $\normp{S(f(Ax) - b)}^p - \normp{S(f(Ax^*) - b)}^p$ in $\Err(x)$ also appears in \Cref{eq:radius_recur}. This means that we can plug the bound on $\Err(x)$ into \Cref{eq:radius_recur} and improve the radius $R$.

For example, let $m\sim \frac{d}{\eps^3}\poly\log n$. Here, the exponent $3$ can be replaced by any value \emph{strictly} larger than $2$ and we simply choose this number for demonstration purposes.
By plugging $m$ and $R$ into \Cref{eq:main_concen_first_attempt}, we have with constant probability
\begin{align*}
	\sup_{x\in T}\Err(x) 
	&=  
	\sup_{x\in T}\abs{(\normp{S(f(Ax) - b)}^p - \normp{S(f(Ax^*) - b)}^p) - (\normp{f(Ax) - b}^p - \normp{f(Ax^*) - b}^p) } \\
	& \lesssim
	\eps^{\frac{1}{2}}\opt + \eps^{\frac{3}{2}} \normp{Ax^*}^p.\numberthis\label{eq:main_concen_first_improve}
\end{align*}
Since $\hat{x}\in T$, by plugging \Cref{eq:main_concen_first_improve} into \Cref{eq:radius_recur} and a similar calculation to that in \Cref{eq:first_attemp_rad_cal}, we have
\begin{align*}
	\normp{A\hat{x}}^p
	& \lesssim
	\frac{1}{\eps^{\frac{1}{2}}}\opt + \normp{Ax^*}^p
	\quad \text{which is smaller than $R$.}
\end{align*}
If we set 
\begin{align*}
	R'
	& =
	\frac{1}{\eps^{\frac{1}{2}}}\opt + \normp{Ax^*}^p
	\quad\text{and}\quad
	T'
	=
	\setdef{x\in\R^d}{\normp{Ax}^p \lesssim R'},
\end{align*}
then $\hat{x}\in T'$.
By plugging $m$, $R'$ and $T'$ into \Cref{eq:main_concen_first_attempt}, we have with constant probability,
\begin{align*}
	\sup_{x\in T'}\Err(x)  \lesssim
	\eps\opt + \eps^{\frac{3}{2}} \normp{Ax^*}^p.
\end{align*}
This is an improved error bound compared to \Cref{eq:main_concen_first_improve}. It follows from $\hat{x}\in T'$ and a similar calculation to that in \Cref{eq:first_attemp_err_cal} that
\begin{align*}
	\normp{f(A\hat{x}) - b}^p - \opt
	& \lesssim 
	\eps(\opt + \normp{Ax^*}^p),
\end{align*}
which achieves the guarantee \Cref{eq:general_error_guarantee}.
Therefore, we have successfully improved the query complexity from $\frac{d}{\eps^4}\poly\log n$ to $\frac{d}{\eps^3}\poly\log n$.
Although this bootstrapping idea of reusing the error guarantee to shrink $T$ has appeared in previous work~\citep{musco2022active,taisuke:thesis}, we emphasize that there is a fundamental difference in the detailed analysis for general Lipschitz functions $f$, due to the lack of convexity of $\normp{f(A(\cdot)) - b}^p$. 

\paragraph{Further Improvement}

Recall that we are targeting a query complexity of $\frac{d}{\eps^2}\poly\log n$.
One may immediately check that setting $m\sim \frac{d}{\eps^2}\poly\log n$ in the above argument is not helpful.
To address this issue, we refine the analysis of \Cref{eq:main_concen_first_attempt} and improve the bound as follows.
Recall that we set $R=\frac{1}{\eps}\opt + \normp{Ax^*}^p$ such that $\normp{A\hat{x}}^p\lesssim R$.
If we further restrict the domain $T$ and set it to be 
\begin{align*}
	T
	=
	\setdef*{x\in \R^d}{\normp{Ax}^p\lesssim R \quad \text{and}\quad \normp{f(Ax)-f(Ax^*)}^p \lesssim F}\quad \text{for some $F\geq \opt$}
\end{align*}
then \Cref{eq:main_concen_first_attempt} can be improved to 
\begin{align*}
	\sup_{x\in T}\Err(x)
	& \lesssim 
	\sqrt{\frac{d\poly(\log n, \log \delta^{-1})}{m}}\cdot \sqrt{FR}. \numberthis\label{eq:main_concen_second_attempt}
\end{align*}
Note that, by the Lipschitz condition and the fact that $\normp{Ax^*}^p\lesssim R$, we have
\begin{align*}
	\normp{f(Ax) - f(Ax^*)}^p
	& \leq
	\normp{Ax-Ax^*}^p
	\lesssim 
	R
\end{align*}
and hence one can set $F=R$.
That means \Cref{eq:main_concen_second_attempt} is always no worse than \Cref{eq:main_concen_first_attempt}.
To apply \Cref{eq:main_concen_second_attempt}, we need to show that $\hat{x}\in T$, i.e. find a suitable $F$ such that $\normp{f(A\hat{x}) - f(Ax^*)}^p\leq F$.

In the proof of a constant-factor approximation by \citet{COLT24}, a key step is 
\begin{align*}
	\normp{f(A\hat{x}) - f(Ax^*)}^p 
	& \lesssim
	\opt+\eps\normp{Ax^*}^p
\end{align*}
when $p=2$.
A straightforward modification extends it to general $p$, which means that we can set $F = \opt + \eps \normp{Ax^*}^p$.

Now, we pick $m\sim \frac{d}{\eps^2}\poly\log n$.
By plugging $m$, $R$ and $F$ into \Cref{eq:main_concen_second_attempt}, we have
\begin{align*}
	\sup_{x\in T}\Err(x)
	& \lesssim
	\eps \cdot \sqrt{(\opt + \eps \normp{Ax^*}^p)\cdot (\frac{1}{\eps}\opt + \normp{Ax^*}^p)}
	\lesssim
	\eps^{\frac{1}{2}}\opt + \eps^{\frac{3}{2}} \normp{Ax^*}^p.\numberthis\label{eq:eps2_first_step}
\end{align*}
Following the same argument before and plugging it into \Cref{eq:radius_recur}, we have
\begin{align*}
	\normp{A\hat{x}}^p
	& \lesssim
	\frac{1}{\eps^{\frac{1}{2}}}\opt + \normp{Ax^*}^p
\end{align*}
which allows us to refine further the radius $R$ and thus the domain $T$, leading to a better bound on $\normp{A\hat{x}}^p$. Iterate this process 
% Then, if we iteratively use the refined $R$ and $T$ and 
and apply \Cref{eq:main_concen_second_attempt} $\log\log\frac{1}{\eps}$ times, we shall arrive at  the bound
\begin{align*}
	\Err(\hat{x})
	& =
	\abs{(\normp{S(f(A\hat{x}) - b)}^p - \normp{S(f(Ax^*) - b)}^p) - (\normp{f(A\hat{x}) - b}^p - \normp{f(Ax^*) - b}^p) }\\
	& \lesssim
	\eps \cdot (\opt + \normp{Ax^*}^p).
\end{align*}
Finally, we follow a calculation similar to that in \Cref{eq:first_attemp_err_cal} to achieve the desired guarantee~\Cref{eq:general_error_guarantee}.
The caveat here is that repeatedly applying the concentration bound \Cref{eq:main_concen_second_attempt} in the iterative process causes the failure probability to accumulate.
We resolve this by setting $\delta = 1/\log\log(1/\eps)$ in \Cref{eq:main_concen_second_attempt}, keeping $\log(1/\delta)$ at most $\log n$.
Hence, the query complexity remains $(d/\eps^2)\poly\log n$.



\paragraph{Dependence on $n$}

Although we have achieved the query complexity of $\frac{d}{\eps^2}\poly\log n$, it may not be desirable when $n$ is large and we seek to further remove the dependence on $n$.
The $\poly\log n$ factor arises from estimating a covering number when bounding Dudley's integral. 
Indeed, by using a simple net argument with a sampling matrix of $\poly(d/\eps)$ non-zero rows, the solution guarantee can still be achieved. While the dependence on $d$ and $\eps$ are both worse, the query complexity is independent of $n$.
To take the advantage of this trade-off, a standard approach involves using two query matrices $S^\circ$ and $S$, where $S^\circ$ has an suboptimal number of nonzero rows, and then solving the following new regularized problem
\begin{align*}
    \hat{x}
    =
    \argmin_{x\in \R^d}\normp{SS^\circ(f(Ax) - b))}^p + \eps \normp{S^\circ A x}^p.
\end{align*}
We need to pay close attention to the fact that we are not simply using $S^\circ A$ as the input matrix $A$ in the original statement because of the function $f$.


To bound the error, a natural attempt is to use the concentration bounds and show that
\begin{align*}
    \MoveEqLeft \normp{f(A\hat{x}) - b}^p - \normp{f(Ax^*) - b}^p \\
    & \leq 
    \normp{S^\circ (f(A\hat{x}) - b)}^p - \normp{S^\circ (f(Ax^*) - b)}^p  + \text{error terms} \numberthis\label{eq:no_n_first_concen} \\
    & \leq
    \normp{SS^\circ (f(A\hat{x}) - b)}^p - \normp{SS^\circ (f(Ax^*) - b)}^p  + \text{error terms}\numberthis\label{eq:no_n_second_concen} 
\end{align*}
and then we use the optimality of $\hat{x}$ to complete the proof.
However, we remind the readers that, to apply the concentration bounds, it is important to check that the relevant points $x^*,\hat{x}$ are in the domain of interest for the corresponding bounds.
It turns out that we can obtain \Cref{eq:no_n_first_concen} but arguing \Cref{eq:no_n_second_concen} is the main obstacle because of that.
While we will omit the detail, we note that we need a proxy point $x^\circ$ to circumvent this obstacle when using the concentration bound for $S$.
The proxy point $x^\circ$ is defined as
\begin{align*}
    x^\circ
    =
    \argmin_{x\in \R^d} \normp{S^\circ(f(Ax) - b)}^p + \eps^2\normp{Ax}^p
\end{align*}
and this proxy point allows us to show that $\hat{x}$ lies within the domain of interest. 
We can then modify the above argument by continuing from \Cref{eq:no_n_first_concen}
\begin{align*}
    \MoveEqLeft \normp{S^\circ (f(A\hat{x}) - b)}^p - \normp{S^\circ (f(Ax^*) - b)}^p \\
    & \leq
    \normp{S^\circ (f(A\hat{x}) - b)}^p - \normp{S^\circ (f(Ax^\circ) - b)}^p + \eps^2\normp{Ax^*}^p \\
    & \leq
    \normp{SS^\circ (f(A\hat{x}) - b)}^p - \normp{SS^\circ (f(Ax^\circ) - b)}^p + \text{error terms}
\end{align*}
and use the optimality of $\hat{x}$ to finish the proof.









\subsection{Lower Bound}

\paragraph{General Observations}
As in the previous results~\citep{musco2022active,taisuke:thesis}, via Yao's minimax theorem, the proof of the lower bounds is reduced to distinguishing between two distributions (which are called hard instance). Specifically,
we construct two ``hard-to-distinguish'' distributions on the vector $b$, and it requires a certain number of queries to the entries of $b$ to distinguish between these distributions with constant probability. The reduction is using an approximation solution $\hat{x}$ to determine from which distribution $b$ is drawn.
We construct our hard instances for $1\leq p\leq 2$ and $p\geq 2$ separately. These instances are inspired by those  in~\citep{musco2022active,taisuke:thesis} but are more complex, as we are showing a higher lower bound. 
For the purpose of exposition, we assume $d=1$, in which case the matrix $A$ degenerates to a vector $a\in\R^n$.
We shall then extend the result to the general $d$.


\paragraph{Hard Instance for $1\leq p\leq 2$}
We pair up the entries (say $2i-1$ and $2i$).
Let 
\begin{align*}
	u
	& =
	\begin{bmatrix}
		3\\
		2
	\end{bmatrix}
	\quad \text{and} \quad 
	v
	=\begin{bmatrix}
		2\\
		3
	\end{bmatrix}.
\end{align*}
Let $D_0$ (resp. $D_1$) be the distribution on $b\in \R^{2n}$ that, for all $i=1,\dots,n$, each pair
\begin{align*}
	\begin{bmatrix}
		b_{2i-1} \\
		b_{2i}
	\end{bmatrix}
	& =
	\begin{cases}
		u & \text{with probability $\frac{1}{2}+\eps$ (resp. $\frac{1}{2}-\eps$)} \\
		v & \text{with probability $\frac{1}{2}-\eps$ (resp. $\frac{1}{2} + \eps$).}
	\end{cases}
\end{align*}
By the standard information-theoretic lower bounds, one needs to query $\Omega(\frac{1}{\eps^2})$ entries of $b$ to distinguish $D_0$ and $D_1$.

To reduce this problem to our problem, we set 
\begin{align*}
	f(x)
	& =
	\begin{cases}
		2 & \text{if $x\leq -6$} \\
		-x-4 & \text{if $-6 \leq x \leq -4 $} \\
		0 & \text{if $-4 \leq x \leq 0$} \\
		\frac{1}{2}x & \text{if $0\leq x$} \\
	\end{cases}
	\quad \text{and}\quad 
	\begin{bmatrix}
		a_{2i-1} \\
		a_{2i}
	\end{bmatrix}
	=
	\begin{bmatrix}
		1 \\
		-1
	\end{bmatrix} \quad \text{for $i=1,\dots,n$.}
\end{align*}
Let $k$ be the number of $u$'s in $b$.
The objective function becomes
\begin{align*}
	\normp{f(a\cdot x) - b}^p
	& =
	k\cdot \normp{f(\begin{bsmallmatrix}
			x\\
			-x
		\end{bsmallmatrix}) - u}^p + (n-k)\cdot \normp{f(\begin{bsmallmatrix}
			x \\ 
			-x
		\end{bsmallmatrix}) - v}^p.
\end{align*}
The takeaway of this construction is one can view $f(\begin{bsmallmatrix}
	x\\
	-x
\end{bsmallmatrix})$ as a locus in $\R^2$ as $x$ varies, illustrated in \Cref{fig:locus_overview}.
Suppose $b$ is drawn from $D_0$.
It implies that $k\approx \frac{n}{2} + \eps n$ and hence $n-k<k$.
One can view each component as the $\ell_p$ distance between the locus and $u$ or $v$.
As seen in \Cref{fig:locus_overview}, the locus passes through $u$ and $v$.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.3\textwidth]{p_12_minimizer_overview_locus.pdf}
	\includegraphics[width=0.3\textwidth]{p_12_minimizer_overview_opt.pdf}
 \includegraphics[width=0.3\textwidth]{p_12_minimizer_overview_wrongside.pdf}
    \vspace{-17pt}
	\caption{(left) Plot of the locus $f(\begin{bsmallmatrix}
	    x\\
     -x
	\end{bsmallmatrix})$, where the red (resp. blue) part corresponds to $x\leq 0$ (resp. $x\geq 0$);
 (middle) $f(\begin{bsmallmatrix}
     -6\\
     6
 \end{bsmallmatrix})$ is the point on the red part that is closest to $u$ and $v$ in the $\ell_p$-distance;
 (right) $f(\begin{bsmallmatrix}
     6\\
     -6
 \end{bsmallmatrix})$ is the point on the blue part that is closest to $u$ and $v$ in the $\ell_p$-distance}
	\label{fig:locus_overview}
\end{figure}
When $x = -6$, we have 
$f(\begin{bsmallmatrix}
	x\\
	-x
\end{bsmallmatrix}) = u$ and so
\begin{align*}
	\opt 
	& \leq
	k\cdot 0 + (n-k)\cdot \normp{u-v}^p
	=
	2^p(n-k)
	\approx 
	2^{p-1}n(1-2\eps).
\end{align*}
On the other hand, \Cref{fig:locus_overview} also suggests that, when $x > 0$, we have
\begin{align*}
	\normp{f(a\cdot x) - b}^p
	& \geq 
	k\cdot \normp{u-v}^p + (n-k)\cdot 0
	\geq 
	2^pk
	\approx
	2^{p-1}n(1+2\eps).
\end{align*}
Suppose we have a solution $\hat{x}$ such that
\[
    \normp{f(a\cdot \hat{x}) - b}^p
    \leq 
	(1+c \cdot \eps) \opt + c\cdot \eps \cdot\normp{a\cdot x^*}^p.
\]
It then follows that 
\begin{align*}
	\normp{f(a\cdot \hat{x}) - b}^p
	& \leq 
	(1+c \cdot \eps) 2^p (n-k) + c\cdot \eps \cdot\normp{a\cdot x^*}^p \\
	&\approx 
	(1+c\cdot \eps)\cdot 2^{p-1}n(1-2\eps) + c\cdot \eps \cdot 6^p\cdot 2n \\
	& =
	2^{p-1}n(1-\Omega(\eps)) \\
	& <
	2^{p-1}n(1+2\eps),
\end{align*}
which implies that $\hat{x}<0$.
Similarly, suppose $b$ is drawn from $D_1$, one can show the symmetric result.
We can declare $b$ is drawn from $D_0$ if $\hat{x}<0$ and $D_1$ otherwise. This concludes our reduction.

\paragraph{Hard Instance for $p\geq 2$}
We start with the all-one vector $b\in \R^{2n}$.
Then, we pick a random index $i^*$ from $\{1,\dots,2n\}$ uniformly and update $b_{i^*} \gets b_{i^*} + 1/\eps$. 
Our question is to determine whether $i^*\leq n$ or $i^\ast > n$, and it follows from a straightforward probability calculation that $\Omega(n)$ queries to the entries of $b$ are required.
Recall that we are targeting a query complexity of $\Omega(1/\eps^p)$ and hence we set $n=1/\eps^p$.

To reduce this problem to our problem, we set 
\begin{align*}
    f(x)
    & =
    \begin{cases}
        0 & \text{if $x\leq 0$} \\
        x & \text{if $0\leq  x\leq 1$} \\
        1 & \text{if $1\leq x$.}
    \end{cases}
    \quad \text{and}\quad 
    a_i
    =
    \begin{cases}
        1 & \text{if $i=1,\dots,n$} \\
        -1 & \text{if $i=n+1,\dots,2n$.}
    \end{cases}
\end{align*}
Suppose that $i^*\leq n$.
If $x=1$, we have the following.
For $i=1,\dots, n$ and $i\neq i^*$, we have $f(a\cdot x)_i = f(1) = 1 = b_i$.
Recall that $b_{i^*} = 1+1/\eps$.
Hence, we have $\sum_{i=1}^n \abs{f(a\cdot x)_i - b_i}^p = 1/\eps^p = n$.
For $i=n+1,\dots,2n$, we have $f(a\cdot x)_i = f(-1) = 0$ and therefore we have $\sum_{i=n+1}^{2n} \abs{f(a\cdot x)_i - b_i}^p = \sum_{i=n+1}^{2n} 1^p = n$.
Namely, we have
\begin{align*}
    \opt
    & \leq
    \normp{f(a\cdot x) - b}^p
    =
    2n.
\end{align*}
On the other hand, it is easy to check that, when $x<0$, we have
\begin{align*}
    \normp{f(a\cdot x) - b}^p
    & \geq 
    \sum_{i=1}^n \abs{f(a\cdot x)_i - b_i}^p
    \geq 
    n-1 + (\frac{1}{\eps} + 1)^p
    \geq 
    2n(1+\eps)
\end{align*}
Suppose we have a solution $\hat{x}$ such that
\begin{align*}
    \normp{f(a\cdot \hat{x}) - b}^p
    & \leq
    (1+c\cdot \eps)\opt + c\cdot\eps\normp{a\cdot x^*}^p \\
    & \leq
    (1+c\cdot \eps)\cdot 2n + c\cdot\eps\cdot 1^p\cdot n\\
    & <
    2n(1+\eps) \quad\text{for a sufficiently small $c>0$}
\end{align*}
which implies $\hat{x}>0$.
Similarly, suppose that $i^*\geq n+1$, one can show the symmetric result.
We can declare $i^*\leq n$ if $\hat{x}>0$ and $i^\ast > n$ otherwise. This concludes our reduction.




\paragraph{Extension to $d > 1$}

We consider the problem of solving multiple independent copies of hard instances of $d=1$ and reduce this new problem to the regression.
The formal construction is as follows.
Let $m = \Theta(1/\eps^{p\vee 2})$.
We have a $dm$-dimensional vector $b$, which can be partitioned into $d$ blocks of $m$-dimensional vectors, with each block drawn from either $D_0$ or $D_1$ (the hard instances introduced earlier depending on $p$).
By a straightforward probability calculation, it can be shown that $\Omega(dm)$ queries to the entries of $b$ are needed to correctly answer, with constant probability, which distribution each block of $m$-dimensional vector is drawn from, for at least $2d/3$ blocks.

To reduce it to our problem, let $A$ be a $dm$-by-$d$ block-diagonal matrix, partitioned into $d^2$ blocks of $m$-dimensional vectors. 
Each diagonal block is the vector $a$ which we constructed earlier.
The function $f$ remains the same as before.
Suppose we have a solution $\hat{x}$ satisfying \Cref{eq:general_error_guarantee}.
By the independence between blocks in $b$ and the block-diagonal structure of $A$, we can argue that \Cref{eq:general_error_guarantee} can be decomposed into the sum of the objective functions for each independent block and declare that each block is drawn from $D_0$ or $D_1$ based on the same criteria as in the case of $d=1$.
By the standard counting techniques, at least $2d/3$ of the $d$ answers are correct and this completes the reduction.
Hence, we achieve the query complexity of $d/\eps^{p\vee 2}$.

We point out that for the canonical case of $f(x)=x$ and $p\geq 2$, the previous result of \citet{taisuke:thesis} gives a stronger lower bound, in terms of $d$, of $\Omega(d^{p/2}/\eps^{p-1})$.
Unfortunately, it is still not clear how to apply the techniques in our setting.

\section{Preliminaries}\label{sec:prelim}

\paragraph{Notation} 
For a distribution $\mathcal{D}$, we write $X\sim \mathcal{D}$ to denote a random variable $X$ drawn from $\mathcal{D}$ and $\beta\cdot\mathcal{D}$ to denote the distribution of the scaled random variable $\beta X$, where $X\sim \mathcal{D}$.
For any $0\leq p\leq 1$ and positive integer $n$, we use $\Ber(p)$ to denote the Bernoulli distribution with expected value $p$ and $\Bin(n,p)$ to denote the Binomial distribution with $n$ trials and success probability $p$ for each trial.
That is, if $X\sim \Ber(p)$ then
\begin{align*}
	X
	=
	\begin{cases}
		1 & \text{with probability $p$}\\
		0 & \text{with probability $1-p$}
	\end{cases}
\end{align*}
and if $X\sim \Bin(n,p)$ then $X$ can be expressed as $\sum_{i=1}^n X_i$ where $X_1,\dots, X_n$ are i.i.d.\ $\Ber(p)$ variables.


For a matrix $A$, we use $A_{i,\cdot}$ to denote its $i$-th row and $A_{\cdot,i}$ its $i$-th column. For $\lambda_1,\dots,\lambda_n\in\R$, we use $\diag\{\lambda_1,\dots,\lambda_n\}$ to denote a diagonal matrix whose diagonal entries are $\lambda_1,\dots,\lambda_n$.

In a normed space $(X,\|\cdot\|)$, the unit ball $B(X)$ is defined as $B(X) = \setdef{x\in X}{\norm*{x}\leq 1}$. When $X$ is clear from the context, we may omit the space and write only $B$ for the unit ball. When $X$ is the column space of a matrix $A$, we also write the unit ball as $B(A)$. If the norm has a subscript $\norm*{\cdot}_{\scriptscriptstyle\square}$, we shall include the subscript of the norm and denote the associated unit ball by $B_{\scriptscriptstyle\square}$ (or $B_{\scriptscriptstyle\square}(A)$ if $X$ is the column space of $A$). In $\R^n$, the standard $\ell_p$-norm and the weighted $\ell_p$-norm, denoted by $\norm*{\cdot}_p$ and $\norm*{\cdot}_{w,p}$, are defined as $\norm*{x}_p = (\sum_{i=1}^n |x_i|^p)^{1/p}$ and $\norm*{x}_{w,p} = (\sum_{i=1}^n w_i |x_i|^p)^{1/p}$, respectively, where $w\in \mathbb{R}^n$ and $w_i>0$ for $i\in [n]$.

We shall use $C$, $C_1$, $C_2$, ..., $c$, $c_1$, $c_2$, ... to denote absolute constants. 
We also write $\max\{a,b\}$ and $\min\{a,b\}$ as $a\vee b$ and $a\wedge b$, respectively.
We use $O,\Omega, \Theta$ and $\lesssim, \gtrsim, \sim$ interchangeably.


\paragraph{Lewis Weights} We now define an important concept regarding matrices, which have played critical roles in the construction of space-efficient subspace embeddings. 

\begin{definition}[$\ell_p$-Lewis weights]
	Let $A\in \R^{n\times d}$ and $p\geq 1$. For each $i\in [n]$, the $\ell_p$-Lewis weight of $A$ for the $i$-th row is defined to be $w_i$ that satisfies
	\begin{align*}
		w_i(A)
		& =
		(a_i^\top (A^\top W^{1-\frac{2}{p}} A)^{\dagger} a_i)^{\frac{p}{2}}
	\end{align*}
	where $a_i$ is the $i$-th row of $A$ (as a column vector), $W = \diag\{w_1,\dots,w_n\}$ and $\dagger$ denotes the pseudoinverse.
\end{definition}
When the matrix $A$ is clear in context, we will simply write $w_i(A)$ as $w_i$. Adopting that $0\cdot\infty = 0$, we have $w_i(A) = 0$ if $a_i = 0$. The following are a few important properties of Lewis weights; see, e.g., \citep{W91book} for a proof.
\begin{lemma}[Properties of Lewis weights]\label{lem:lewis_weight_properties}
	Suppose that $A\in \R^{n\times d}$ has full column rank and Lewis weights $w_1,\dots,w_n$. Let $W = \diag\{w_1,\dots,w_n\}$. The following properties hold.
	\begin{enumerate}[label={(\alph*)},nosep]
		\item $\sum_i w_i = d$;
		\item There exists a matrix $U\in \R^{n\times d}$ such that 
		\begin{enumerate}[label={(\roman*)},nosep]
			\item the column space of $U$ is the same as that of $A$;
			\item $w_i = \norm*{U_{i,\cdot}}_2^p$;
			\item $W^{\frac{1}{2}-\frac{1}{p}}U$ has orthonormal columns;
		\end{enumerate}
		\item It holds for all vectors $u$ in the column space of $A$ that $\norm{W^{\frac{1}{2}-\frac{1}{p}}u}_2\leq d^{\frac{1}{2}-\frac{1}{2\vee p}}\norm{u}_p$.
		\item It holds for all vectors $u$ in the column space of $A$ that $|u_i| \leq d^{\frac{1}{2}-\frac{1}{2\vee p}}w_i^{\frac{1}{p}}\norm{u}_p$.
	\end{enumerate}
\end{lemma}

% \note{colt result and subspace embedding}

\paragraph{Subspace Embeddings} Suppose that $A\in\R^{n\times d}$ and $\eps \in (0,1)$. We say a matrix $S\in\R^{m\times n}$ is an $\ell_p$-subspace embedding matrix for $A$ with distortion $1+\eps$ if $(1+\eps)^{-1}\normp{Ax} \leq \normp{SAx}\leq (1+\eps)\normp{Ax}$. The prevailing method to construct $\ell_p$-subspace embedding matrices is to sample the rows of $A$ according to its Lewis weights. 
\begin{lemma} \label{lem:SE}
	Suppose that $A\in\R^{n\times d}$ has Lewis weights $w_1,\dots,w_n$. Let $p_i\in [0,1]$ satisfy that $p_i\geq (\beta w_i)\wedge 1$ and $S\in \R^{n\times n}$ be a diagonal matrix with independent diagonal entries $S_{ii} \sim p_i^{-1/p}\Ber(p_i)$. Then with probability at least $0.99$, $S$ is an $\ell_p$-subspace embedding matrix for $A$ with distortion $1+\eps$ if 
	\[\beta \gtrsim_p
	\begin{cases}
		\frac{1}{\eps^2}\log\frac{d}{\eps}(\log\log\frac{d}{\eps})^2, & 1 <p<2 \\
		\frac{1}{\eps^2}\log \frac{d}{\eps}, & p=1,2 \\
		\frac{d^{\frac{p}{2}-1}}{\eps^2}(\log d)^2\log\frac{d}{\eps} & p>2.
	\end{cases}
	\]
\end{lemma}
The results for $p\in [1,2]$ are due to \citet{CP15}, based on earlier work of \citet{talagrand:p1,talagrand:1<p<2}. The result for $p > 2$ can be found in  \citep{taisuke:thesis,ICLR24}, which improves upon the previous bound $\beta \gtrsim (d^{p/2-1}/\eps^5) (\log d)\log(1/\eps)$ in \citep{BLM89,CP15}. 

\paragraph{Covering Numbers and Dudley's Integral} Suppose that $T$ is a pseudometric space endowed with a pseudometric $d$. The diameter of $T$, denoted by $\Diam(T,d)$, is defined as $ \Diam(T,d) := \sup_{t,s\in T} \rho(t,s)$.

Given an $r > 0$, an $r$-covering of $(T,d)$ is a subset $X\subseteq T$ such that for every $t\in T$, there exists $x\in X$ such that $d(t,x) \leq r$. The covering number $\mathcal{N}(T,d,r)$ is the minimum number $K$ such that there exists an $r$-covering of cardinality $K$. 

The covering numbers are intrinsically related to a subgaussian process on the space $T$ that conforms to the pseudometric $d$. This relationship is captured by the well-known Dudley's integral.

\begin{lemma}[Dudley's integral {\citet{Vers18}}]\label{lem:dudley}
	Let $X_t$ be a zero-mean stochastic process that is subgaussian w.r.t.\ a pseudo-metric $d$ on the indexing set $T$. Then it holds that
	\[
	\Pr\left\{ \sup_{t,s\in T} \abs{X_t - X_s} > C\left(\int_0^\infty \sqrt{\log N(T,d,\eps)}d\eps + u\cdot \Diam(T)\right) \right\} \leq 2\exp(-u^2),
	\]
	where $C$ is an absolute constant.
	As a consequence,
	\[
	\E \left(\sup_{t,s\in T} \abs{X_t - X_s}\right)^\ell \leq C'\cdot C^\ell\left[\left(\int_0^\infty \sqrt{\log N(T,d,\eps)}d\eps\right)^\ell + (\sqrt{\ell}\Diam(T))^\ell  \right],
	\]
	where $C$ and $C'$ are absolute constants.
\end{lemma}
Note that when $r > \Diam(T,d)$, the covering number $\mathcal{N}(T,d,r) = 1$ and thus the integrand becomes $0$. Hence, Dudley's integral is in fact taken over the finite interval $[0,\Diam(T,d)]$.

The following covering numbers related to $\norm{\cdot}_{w,p}$ will be useful our analysis.  These are not novel results though we include a proof in \Cref{sec:entropy_estimates} for completeness.
\begin{lemma}\label{lem:entropy_estimates_consolidated}
	Suppose that $A\in \R^{n\times d}$ has full column rank and $W$ is a diagonal matrix whose diagonal entries are the Lewis weights of $A$. It holds that
	\[
	\log \mathcal{N}(B_{w,p}(W^{-1/p} A), \norm*{\cdot}_{w,q}, t)
	\lesssim 
	\begin{cases}
		d \log\frac{1}{t} & q = p\geq 1 \\
		t^{-p} q \sqrt{\log d} & 1\leq p\leq 2 \text{ and } q>2 \\
		t^{-2} q d^{1-\frac{2}{p}+\frac{2}{q}}  & p,q\geq 2.
	\end{cases}
	\]
\end{lemma}

\paragraph{Lower Bound} 
The following two lemmata, \Cref{lem:lb_ber} and \Cref{lem:lb_idx_loc}, are needed in the proof of our lower bounds for $p\leq 2$ and $p\geq 2$, respectively. \Cref{lem:lb_ber} is a classical result, whose proof can be found, for example, in~\citet[p711]{kiltz2022theory}. 
The proof of \Cref{lem:lb_idx_loc} is postponed to \Cref{sec:prelim_proofs}.
\begin{lemma}\label{lem:lb_ber}
	Let $m$ be a positive integer.
	Suppose we have an $m$-dimensional vector whose entries are i.i.d. samples all drawn from either $\Ber(\frac{1}{2}+\frac{1}{\sqrt{m}})$ or  $\Ber(\frac{1}{2}-\frac{1}{\sqrt{m}})$.
	It requires $\Omega(m)$ queries to distinguish $\Ber(\frac{1}{2}+\frac{1}{\sqrt{m}})$ and $\Ber(\frac{1}{2}-\frac{1}{\sqrt{m}})$ with probability at least $3/5$.
\end{lemma}

\begin{lemma}\label{lem:lb_idx_loc}
	Let $m$ be a positive integer.
	Suppose that $x\in \R^{2m}$ is a random vector in which all but one of the entries are the same and the distinct entry $x_{i^\ast}$ is located at a uniformly random position $i^*\in [2m]$.
	Any deterministic algorithm that determines with probability at least $3/5$ whether $i^\ast$ lies within $\{1,\dots,m\}$ or $\{m+1,\dots,2m\}$ must read $\Omega(m)$ entries of $x$.
\end{lemma}


The next lemma extends the previous two lemmata to multiple instances of the problem considered therein. 
The proof is postponed to \Cref{sec:prelim_proofs}.

\begin{lemma} \label{lem:distinguishing_dist}
	Let $d$ and $m$ be positive integers.
	Suppose that $D_0$ and $D_1$ are two distributions in $\mathbb{R}^m$ and distinguishing whether a vector is drawn from $D_0$ or $D_1$ with probability at least $3/5$ requires querying $\beta m$ entries of the vector for some constant $\beta>0$.
	Consider a $dm$-dimensional random vector consisting of $d$ blocks, each of which is an $m$-dimensional vector drawn from either $D_0$ or $D_1$.
	Every deterministic algorithm that correctly distinguishes, with probability at least $2/3$, the distributions in $2d/3$ instances requires $\Omega(dm)$ entry queries to this $dm$-dimensional random vector.
\end{lemma}




\section{Upper Bound}

\begin{algorithm}[bt]
	\caption{Generating a Sampling Matrix $\texttt{GSM}(k_1,\dots,k_n,\alpha)$}\label{alg:sampling}
	\begin{algorithmic}[1]
		\REQUIRE{
			%            \begin{varwidth}[t]{\linewidth}
				$n$ integers $k_1,\dots,k_n\geq 0$; a sampling rate $\alpha<1$ \strut
				%            \end{varwidth}
		}
		\STATE $S \gets$ an $n\times n$ diagonal matrix, initialized to a zero matrix
		\FOR{$i=1,\dots,n$}
		\IF{$k_i > 0$}
		\STATE Generate a binomial random variable $N_i\sim \Bin(k_i,\alpha)$
		\STATE $S_{ii} \gets (\frac{N_i}{\alpha k_i})^{\frac{1}{p}}$ \label{step:row_split}
		\ENDIF
		\ENDFOR
		\STATE Return $S$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[bt]
	\caption{Algorithm for Active Learning without Dependence on $n$}\label{alg:main_general_no_n}
	\begin{algorithmic}[1]
		\REQUIRE{
			\begin{varwidth}[t]{\linewidth}
				a matrix $A \in\mathbb{R}^{n \times d}$ \\
				a query access to the entries of the vector $b\in\mathbb{R}^n$ \\
				a function $f \in \lip_L$ \\
				an error parameter $\eps$ \\
				two sampling rates $\alpha <\alpha^\circ< 1$ \strut
		\end{varwidth}}
		\STATE Compute the Lewis weights of $A$, denoted by $w_1(A),\dots,w_n(A)$\label{step:dimreduce_start}
		\FOR{$i=1,\dots,n$}
		\STATE $k_i^\circ \gets \lceil \frac{n \cdot w_i(A)}{d} \rceil$ 
		\ENDFOR
		\STATE $S^\circ \gets \texttt{GSM}(k_1^\circ,\dots,k_n^\circ,\alpha^\circ)$ from \Cref{alg:sampling} \label{step:dimreduce_end}
		\STATE $m\gets $ number of nonzero rows in $S^\circ$
		\STATE Compute the Lewis weights of $S^\circ A$, denoted by $w_1(S^\circ A),\dots, w_n(S^\circ A)$ \label{step:main_start}
		\FOR{$i=1,\dots,n$}
		\STATE $k_i \gets \lceil \frac{m \cdot w_i(S^\circ A)}{d} \rceil$ 
		\ENDFOR
		\STATE $S \gets \texttt{GSM}(k_1,\dots,k_n,\alpha)$ from \Cref{alg:sampling} \label{step:main_end}
		\STATE Solve the minimization problem $\hat{x} := \arg\min_{x\in \mathbb{R}^d} \normp{SS^\circ f(Ax) - SS^\circ b}^p + \eps\normp{S^\circ Ax}^p$
		\STATE Return the vector $\hat{x} \in\mathbb{R}^d$
	\end{algorithmic}
\end{algorithm}

\subsection{Algorithm}\label{sec:full_alg}

To complement the proof overview in \Cref{sec:upper_overview}, we present our full algorithm in \Cref{alg:main_general_no_n} and explain the explicit implementation. 

It first constructs a sampling matrix $S^\circ$ (line \ref{step:dimreduce_start} to line \ref{step:dimreduce_end} of \Cref{alg:main_general_no_n}) and applies it to $A$, $f(A(\cdot))$ and $b$.
This sampling matrix $S^\circ$ is generated using \Cref{alg:sampling}.
When applying $S^\circ$ to $A$, in step \ref{step:row_split} of \Cref{alg:sampling}, it is equivalent to splitting the rows of $A$ such that all rows have uniformly bounded Lewis weights of $O(d/n)$.
To achieve this, it needs the Lewis weights of $A$ and they can be computed as in~\citep{CP15} for $p<4$ and as in~\citep{FLPS22} for $p\geq 4$.
Afterwards, we sample each row with the same probability $\alpha^\circ$.
This row-splitting approach has been used in the proofs of \citet{CP15,taisuke:thesis} and in the algorithms in \citep{gajjar2023improved,COLT24}.
Details of this row-splitting technique can be found in \Cref{sec:generalized}. 

We set the sampling rate 
$\alpha^\circ = \poly(d/\eps)/n$. 
This effectively reduces the dimension from $n$, the number of rows of $A$, to $m \sim \alpha^\circ n = \poly(d/\eps)$, the number of non-zero rows of $S^\circ A$. 
Therefore, it removes the dependence on $n$ in our bound.

It then constructs the main sampling matrix $S$ (line \ref{step:main_start} to line \ref{step:main_end} of \Cref{alg:main_general_no_n}) with the sampling rate $\alpha = d^{\frac{p}{2}\vee 1}/(\eps^{p\vee 2}m) \poly\log(m)$, whereby avoiding dependence on $n$ as previously discussed, and applies to $S^\circ A$, $S^\circ f(A(\cdot))$ and $S^\circ b$.
That means that the number of non-zero entries of $SS^\circ b$ is, with high probability, at most $2\alpha m \sim d^{\frac{p}{2}\vee 1}/(\eps^{p\vee 2}) \poly\log(d/\eps)$, which is the query complexity we are looking for.
Note that $S$ is also generated using \Cref{alg:sampling} and hence satisfies the property of uniformly bounded Lewis weights through the previously mentioned row-splitting techniques.
Finally, the algorithm outputs the optimal solution $\hat{x}$ of the regularized problem 
\begin{align*}
    \min_{x\in \mathbb{R}^d} \normp{SS^\circ f(Ax) - SS^\circ b}^p + \eps\normp{S^\circ Ax}^p
\end{align*}
and that completes our full algorithm.


\begin{algorithm}[t]
	\caption{Algorithm for Active Learning}\label{alg:main_general}
	\begin{algorithmic}[1]
		\REQUIRE{
			\begin{varwidth}[t]{\linewidth}
				a matrix $A \in\mathbb{R}^{n \times d}$ \\
				a query access to the entries of the vector $b\in\mathbb{R}^n$ \\
				a function $f \in \lip_L$ \\
				an error parameter $\eps$ \\
				a sampling rate $\alpha < 1$ \strut
		\end{varwidth}}
		\STATE Compute the Lewis weights $w_1,\dots,w_n$ of $A$
		\FOR{$i=1,\dots,n$}
		\STATE $k_i \gets \lceil \frac{n \cdot w_i}{d} \rceil$
		\ENDFOR
		\STATE $S \gets \texttt{GSM}(k_1,\dots,k_n,\alpha)$ from \Cref{alg:sampling}
		\STATE Solve the minimization problem $\hat{x} := \arg\min_{x\in \mathbb{R}^d} \normp{Sf(Ax) - Sb}^p + \eps\normp{Ax}^p$
		\STATE Return the vector $\hat{x} \in\mathbb{R}^d$
	\end{algorithmic}
\end{algorithm}

\subsection{Equivalent Statement}\label{sec:generalized}


We shall first reduce the problem to the case where $A$ has uniformly bounded Lewis weights, before proving in the next section that the output of Algorithm \ref{alg:main_general} with a suitable $\alpha$ satisfies \Cref{eq:main_obj} with probability $0.99$.

We start with the following observation.
Let $k_i$ be $\lceil \frac{n \cdot w_i}{d} \rceil$ for $i=1,\dots,n$ which is the same term also defined in Algorithm \ref{alg:main_general}.
Hence, we rewrite
\begin{align*}
	\normp{f(Ax) - b}^p
	& =
	\sum_{i=1}^n \abs{(f(Ax) -b)_i}^p
	=
	\sum_{i=1}^n k_i \cdot \frac{1}{k_i}\abs{(f(Ax) -b)_i}^p
	=
	\sum_{i=1}^n \sum_{j=1}^{k_i}  \frac{1}{k_i}\abs{(f(Ax) -b)_i}^p.
\end{align*}
Now, suppose that we duplicate the $i$-th term, $\abs{(f(Ax) - b)_i}^p$, $k_i$ times and assign a weight of $1/k_i^{\frac{1}{p}}$ to each duplicate term.
Formally, let 
\begin{itemize}[noitemsep]
	\item $n' = \sum_{i=1}^n k_i$,
	\item $A'$ be an $n'$-by-$d$ matrix in which $A'_{j,\cdot} = A_{i,\cdot}$ if $\sum_{a=1}^{i-1}k_a  < j \leq \sum_{a=1}^{i}k_a$,
	\item $b'$ be an $n'$-dimensional vector in which $b'_j = b_i$ if $\sum_{a=1}^{i-1}k_a  < j \leq \sum_{a=1}^{i}k_a$,
	\item $\Lambda$ be an $n'$-by-$n'$ diagonal matrix in which $\Lambda_{jj} = {k_i}^{-\frac{1}{p}}$ if $\sum_{a=1}^{i-1}k_a  < j \leq \sum_{a=1}^{i}k_a$,
\end{itemize}
In other words, we have
\begin{align*}
	\normp{f(Ax) - b}^p
	& =
	\sum_{j=1}^{n'} \Lambda_{jj}^p\abs{(f(A'x) - b')_j}^p
	=
	\normp{\Lambda f(A'x) - \Lambda b'}^p
\end{align*}
Note that we still have
\begin{equation}\label{eqn:x star and opt}
	\opt 
	=
	\min_{x\in \mathbb{R}^d}\normp{\Lambda f(A'x) - \Lambda b'}^p
	\quad \text{and} \quad x^*
	=
	\argmin_{\substack{x\in \mathbb{R}^d\\ \normp{\Lambda f(A'x)-\Lambda b'}^p=\opt}}\normp{\Lambda A'x}^p.
\end{equation}

On the other hand, in Algorithm \ref{alg:main_general}, recall that $N_i\sim \Bin(k_i,\alpha)$ for $i=1,\dots,n$, it can be rewritten as 
\begin{align*}
	N_i
	& =
	\sum_{j=1}^{k_i} N_{i,j},
\end{align*}
where $N_{i,1},\dots,N_{i,k_i}$ are i.i.d.\xspace $\Ber(\alpha)$ variables. 
In other words, we have
\begin{align*}
	\normp{Sf(Ax) - Sb}^p
	& =
	\sum_{i=1}^n S_{ii}^p\abs{(f(Ax) - b)_i}^p
	=
	\sum_{i=1}^n \sum_{j=1}^{k_i} \frac{N_{i,j}}{\alpha} \frac{1}{k_i}\abs{(f(Ax) - b)_i}^p.
\end{align*}
Let $S'$ be an $n'$-by-$n'$ diagonal matrix in which $S'_{jj} = \big(\frac{N_{i,j'}}{\alpha}\big)^{\frac{1}{p}}$ if $j=\sum_{a=1}^{i-1}k_a + j'$ for $j'=1,\dots,k_i$.
Then 
\begin{align*}
	\normp{Sf(Ax) - Sb}^p
	& =
	\sum_{j=1}^{n'} {S_{jj}'}^p\Lambda_{jj}^p\abs{(f(A'x) - b')_j}^p
	=
	\normp{S'\Lambda f(A'x) - S' \Lambda b'}^p.
\end{align*}
Also, it is easy to check that
\begin{align*}
	\normp{Ax}^p
	& =
	\normp{\Lambda A'x}^p.
\end{align*}
We still have 
\begin{equation}\label{eqn:x hat}
	\hat{x}
	=
	\argmin_{x\in\mathbb{R}^d} \normp{S'\Lambda f(A'x) - S' \Lambda b'}^p + \eps \normp{\Lambda A'x}^p.
\end{equation}

The advantage of introducing the diagonal matrix $\Lambda$ is to bound the Lewis weights.
Formally, we have the following observation.
By the definition of Lewis weights, the $j$-th Lewis weight of $\Lambda A'$ is $\frac{w_i}{k_i}$ if $j=\sum_{a=1}^{i-1}k_a + 1,\dots,\sum_{a=1}^{i}k_a$ for $j=1,\dots,n'$.
Recall that $k_i = \lceil \frac{n \cdot w_i}{d} \rceil$ and we have
\begin{align*}
	\frac{w_i}{k_i} 
	= 
	\frac{w_i}{\lceil \frac{n \cdot w_i}{d} \rceil}
	\leq
	\frac{d}{n}
	\quad\text{and}\quad
	n'
	=
	\sum_{i=1}^n k_i
	\leq
	\sum_{i=1}^n (\frac{n \cdot w_i}{d}+1)
	=
	2n.
\end{align*}
Therefore, we generalize our statement to be the following.
Let $A'$ be an $n'$-by-$d$ matrix, $f\in \lip_L$, $b'$ be an $n'$-dimensional vector, $\Lambda$ be an arbitrary $n'$-by-$n'$ positive diagonal matrix such that the Lewis weights of $\Lambda A'$ is at most $\frac{2d}{n'}$.
%Note that $\Lambda$ can be any positive diagonal matrix as long as it satisfies the Lewis weights condition. 
Define $\opt$ and $x^\ast$ as in~\Cref{eqn:x star and opt}.
Furthermore, let $S'$ be an $n'$-by-$n'$ diagonal random matrix in which the diagonal entries are i.i.d.\ $\alpha^{-\frac{1}{p}}\cdot\Ber(\alpha)$ variables, i.e.
\begin{align*}
	{S'_{ii}} = \begin{cases}
		{\alpha}^{-\frac{1}{p}} & \text{with probability $\alpha$}\\
		0 & \text{with probability $1-\alpha$}
	\end{cases}
\end{align*}
and define $\hat{x}$ as in~\Cref{eqn:x hat}.
Our goal is to show, for a suitable $\alpha$, we have in correspondence to \Cref{eq:general_error_guarantee}
\[
\normp{\Lambda f(A' \hat{x}) - \Lambda b'}^p 
\leq 
(1+\eps)\opt + L^p \eps \normp{\Lambda A' x^*}^p.
\]





\subsection{Correctness}


We would like to prove that the output of Algorithm \ref{alg:main_general} satisfies \Cref{eq:general_error_guarantee} with probability $0.99$. 
In view of \Cref{sec:generalized}, we can replace $A$ with $\Lambda A$, where the Lewis weights of $\Lambda A $ are uniformly bounded by $2d/n$. The desired error guarantee is therefore
\begin{equation} \label{eqn:general_error_guarantee2}
	\normp{\Lambda f(A \hat{x}) - \Lambda b}^p 
	\leq 
	(1+\eps)\opt + L^p \eps \normp{\Lambda A x^*}^p.	
\end{equation}
By replacing $f(x)$ with $f(x)/L$ and $b$ with $b/L$, we can henceforce assume that $L = 1$ and the error guarantee \Cref{eq:general_error_guarantee} 
becomes
\begin{equation}\label{eq:main_obj}
	\normp{\Lambda f(A\hat{x}) - \Lambda b}^p
	\leq
	(1+\eps)\opt + \eps\normp{\Lambda Ax^*}^p.
\end{equation}

We shall first prove a weaker version of \Cref{thm:main_upper} with query complexity containing $\log n$ factors and then show how to remove the $\log n$ factors in \Cref{sec:log n factors}. 
The weaker version of \Cref{thm:main_upper} is stated formally below.
\begin{theorem}\label{thm:main_upper_weaker}
%	\note{change it to $O(d/n)$ for all lewis weight bounds}
%	\note{mention $\bar{x}$. may need to satisfy some conditions.}
    Let $A\in \R^{n\times d}$, $\bar{x}\in \mathbb{R}^d$, $b\in \R^n$, $f\in\lip_1$, $\eps \in (0,1)$ be sufficiently small
	and $\Lambda$ be an $n\times n$ diagonal matrix satisfying that $\Lambda_{ii} > 0$ and $w_i(\Lambda A) \lesssim d/n$ for all $i$.
	There is a randomized algorithm which, with probability at least $0.9$, makes $O\big(d^{1\vee\frac{p}{2}}/\eps^{2\vee p} \cdot \poly\log n\big)$ queries to the entries of $b$ and returns an $\hat{x}\in\R^d$ satisfying 
	\begin{align*}
		\normp{\Lambda (f(A\hat{x}) - b)}^p
		\leq
		(1+\eps)\normp{\Lambda (f(A\bar{x} - b) }^p + \eps\normp{\Lambda A\bar{x}}^p.
	\end{align*}
	The hidden constant in the bound on number of queries depends on $p$ only.
\end{theorem}
Note that we introduce a vector $\bar{x}\in \mathbb{R}^d$.
If we take $\bar{x} = x^*$, \Cref{thm:main_upper_weaker} becomes \Cref{thm:main_upper} except that the query complexity contains $\log n$ factors.
The reason we introduce $\bar{x}$ is because when we remove the $\log n$ factors in \Cref{sec:log n factors} we can reuse the theorem using a different $\bar{x}$.
Now, to prove \Cref{thm:main_upper_weaker}, we first provide a concentration bound in \Cref{lem:main_concen_final}.


\begin{lemma}\label{lem:main_concen_final}
	Let $A\in \R^{n\times d}$, $f\in\lip_1$, $\eps \in (0,1)$ be sufficiently small and $\Lambda$ be an $n\times n$ diagonal matrix satisfying that $\Lambda_{ii} > 0$ and $w_i(\Lambda A) \lesssim d/n$ for all $i\in [n]$.
	Also, let $S$ be an $n$-by-$n$ random diagonal matrix in which the diagonal entries are i.i.d.\ $\alpha^{-\frac{1}{p}}\cdot \Ber(\alpha)$ variables where $\alpha \gtrsim \frac{d^{\frac{p}{2}\vee 1}}{n \eps^{p\vee 2}}\cdot \poly\log n$.
	If $\hat{x},\bar{x}\in \mathbb{R}^d$ satisfy
	\begin{gather*}
		\hat{x}
		=
		\argmin_{x\in \mathbb{R}^d} \normp{S\Lambda(f(Ax) - b)}^p + \eps \normp{\Lambda A x}^p\\
		\intertext{and} 
		\normp{\Lambda(f(A \bar{x}) - b)}^p - \normp{\Lambda(f(A \hat{x}) - b)}^p 
		\lesssim 
		\eps(\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps \norm{\Lambda A\bar{x}}^p)
	\end{gather*}
	then, with probability at least $0.9$, 
	\begin{align*}
		\MoveEqLeft \abs{(\normp{S \Lambda(f(A\hat{x}) - b)}^p - \normp{S \Lambda(f(A\bar{x}) - b)}^p) - (\normp{\Lambda(f(A\hat{x}) - b)}^p - \normp{\Lambda(f(A\bar{x}) - b)}^p)} \\
		& \leq
		\eps \cdot (\normp{\Lambda (f(A\bar{x}) - b)}^p + \normp{\Lambda A \bar{x}}^p).
	\end{align*}
	
\end{lemma}


We now show how \Cref{lem:main_concen_final} can be used to prove \Cref{thm:main_upper_weaker}.
The proof of \Cref{lem:main_concen_final} will be presented in \Cref{sec:main_concen}.

\begin{proof}[Proof of \Cref{thm:main_upper_weaker}]
	We shall apply \Cref{lem:main_concen_final} with $\bar{x} = x^*$ to prove \Cref{thm:main_upper_weaker}. First, we verify the conditions in \Cref{lem:main_concen_final}.
	Clearly, the output $\hat{x}$ of \Cref{alg:main_general} satisfies 
	\begin{align*}
		\hat{x}
		=
		\argmin_{x\in \mathbb{R}^d} \normp{S\Lambda(f(Ax) - b)}^p + \eps \normp{\Lambda A x}^p
	\end{align*}
	and, by the optimality of $x^*$, we also have
	\begin{align*}
		\normp{\Lambda(f(A x^*) - b)}^p - \normp{\Lambda(f(A \hat{x}) - b)}^p 
		& \leq
		0.
	\end{align*}
	Recall that $\normp{\Lambda(f(Ax^*) - b)}^p = \opt$.
	By \Cref{lem:main_concen_final}, with probability at least $0.9$, we have
	\begin{align*}
		\MoveEqLeft \abs{(\normp{S \Lambda(f(A\hat{x}) - b)}^p - \normp{S \Lambda(f(Ax^*) - b)}^p) - (\normp{\Lambda(f(A\hat{x}) - b)}^p - \opt)} \\
		& \leq
		\eps \cdot (\opt + \normp{\Lambda A x^*}^p),
	\end{align*}
	which implies that
	\begin{align*}
		\MoveEqLeft \normp{\Lambda (f(A\hat{x})-b)}^p - \opt \\
		& \leq
		\normp{S \Lambda(f(A\hat{x}) - b)}^p - \normp{S \Lambda(f(Ax^*) - b)}^p + \eps \cdot (\opt + \normp{\Lambda A x^*}^p) \\
		& \leq
		\eps \cdot \normp{\Lambda Ax^*}^p + \eps \cdot (\opt + \normp{\Lambda A x^*}^p) \quad \text{by the optimality of $\hat{x}$} \\
		& \lesssim
		\eps \cdot (\opt + \normp{\Lambda A x^*}^p).
	\end{align*}
	This completes the proof of \Cref{thm:main_upper_weaker}.
\end{proof}




\subsection{Concentration Bounds}\label{sec:main_concen}


\subsubsection{Proof of \Cref{lem:main_concen_final}}


To prove \Cref{lem:main_concen_final}, we rely on the following concentration bound provided in \Cref{lem:main_concen} and provide the proof in \Cref{sec:main_concen_proof}.



\begin{lemma}\label{lem:main_concen}
	% \note{also need to change $x^*$ to $\bar{x}$ in the proof.}
	% \note{need to revisit the statement}
	% \note{probably absorb the first two terms here.}
	Let $A\in \R^{n\times d}$, $f\in\lip_1$, $\eps \in (0,1)$ be sufficiently small and $\Lambda$ be an $n\times n$ diagonal matrix satisfying that $\Lambda_{ii} > 0$ and $w_i(\Lambda A) \lesssim d/n$ for all $i\in [n]$.
	Additionally, suppose that $\bar{x}\in \mathbb{R}^d$ and $v\in \mathbb{R}^n$ are fixed vectors, $0\leq \alpha \leq 1$, $R$ is any value that $R\geq \normp{\Lambda A\bar{x}}^p$, $F$ is any value that $F\geq V := \normp{\Lambda(f(A\bar{x}) - v)}^p$ and $T$ is any subset of $\R^d$ that $\{\bar{x}\}\subseteq T \subseteq \setdef*{x\in\mathbb{R}^d}{\normp{\Lambda Ax}^p\leq R}$.
	Let $S$ be an $n$-by-$n$ random diagonal matrix in which the diagonal entries are i.i.d.\ $\alpha^{-\frac{1}{p}}\cdot \Ber(\alpha)$ variables.
	When conditioned on the event that
	\begin{align*}
		\normp{S\Lambda(f(A\bar{x}) - v)}^p 
		\lesssim
		V
		\quad\text{and}\quad       
		\sup_{x\in T}\,\normp{S\Lambda (f(Ax) - f(A\bar{x}))}^p 
		\lesssim 
		F, 
		% \numberthis\label{eq:main_concen_condition}
	\end{align*}
	it holds with probability at least $1-\delta$ that
	\begin{align*}
		\MoveEqLeft \sup_{x\in T}\abs*{(\normp{S\Lambda (f(Ax)-v)}^p - \normp{S\Lambda (f(A\bar{x})-v)}^p) - (\normp{\Lambda (f(Ax)-v)}^p - \normp{\Lambda (f(A\bar{x})-v)}^p)} \\
		& \leq  
		C\cdot \bigg(\eps V + \frac{d^{1\vee\frac{p}{2}}}{\alpha n}R + \Gamma \cdot \left(\log^{\frac{5}{4}} d\sqrt{\log\frac{n}{\eps d}} + \sqrt{\log\frac{1}{\delta}}\right)\bigg),
	\end{align*}
	where $C$ is an absolute constant and
	\begin{equation}\label{eq:Gamma}
		\Gamma = 
		\begin{cases}
			(d / (\alpha n))^{\frac{1}{2}} F^{\frac{1}{2}}R^{\frac{1}{2}}   & \text{when $1 \leq p \leq 2$}\\
			(d^{\frac{p}{2}} /(\alpha n))^{\frac{1}{p}} F^{1-\frac{1}{p}} R^{\frac{1}{p}} & \text{when $p > 2$.}
		\end{cases}
	\end{equation}
\end{lemma}

With \Cref{lem:main_concen}, we immediately have the following two corollaries.


\begin{corollary}\label{cor:colt_concen}
	
	Let $A\in \R^{n\times d}$, $f\in\lip_1$, $\Lambda \in \R^{n\times n}$, $\bar{x}\in \mathbb{R}^d$, $\alpha\in (0,1)$, $R\in \R^d$, $T\subseteq\R^d$ and $S\in \R^{n\times n}$ be as defined in Lemma~\ref{lem:main_concen} and satisfy the same constraints.
	Additionally, suppose that $\alpha\gtrsim d^{\frac{p}{2}\vee 1}/n$.
	When conditioned on the event that $\normp{S \Lambda Ax}^p \lesssim \normp{\Lambda A x}^p$ for all $x\in \mathbb{R}^d$, 
	it holds with probability at least $1-\delta$ that
	\begin{align*}
		\sup_{x\in T}\abs*{\normp{S\Lambda (f(Ax)-f(A\bar{x}))}^p  - \normp{\Lambda (f(Ax)-f(A\bar{x}))}^p}  \\
		\leq  
		C\cdot \frac{d^{\frac{1}{2}}}{(\alpha n)^{\frac{1}{2\vee p}}} R\cdot \left(\log^{\frac{5}{4}} d\sqrt{\log\frac{n}{d}} + \sqrt{\log\frac{1}{\delta}}\right),
	\end{align*}
	where $C$ is an absolute constant.
\end{corollary}

\begin{proof}
	In \Cref{lem:main_concen}, we take $v = f(A\bar{x})$ and $\eps$ to be a constant.
	Note that $V = \normp{S\Lambda (f(A\bar{x}) - v)}^p = 0$.
	For any $x\in T$, if we take $F=2^pR$ then 
	\begin{align*}
		\normp{S\Lambda (f(Ax) - f(A\bar{x})))}^p
		& \leq
		\normp{S\Lambda (Ax - A\bar{x}))}^p & \text{by the Lipschitz condition} \\
		& \lesssim
		\normp{\Lambda (Ax-A\bar{x})}^p &\text{by the assumption of $\normp{S \Lambda Ax}^p \lesssim \normp{\Lambda A x}^p$}\\
		& \leq
		2^{p}R & \text{by $x,\bar{x} \in T$}
	\end{align*}
	and hence the result follows by \Cref{lem:main_concen}.
\end{proof}


\begin{corollary}\label{cor:main_concen}
	
	Let $A\in \R^{n\times d}$, $f\in\lip_1$, $\eps \in (0,1)$, $\Lambda \in \R^{n\times n}$, $\bar{x}\in \mathbb{R}^d$, $\alpha\in (0,1)$, $R\in \R$, $F\in \R$, $T \subseteq\R^d$ and $S\in \R^{n\times n}$ be as defined in Lemma~\ref{lem:main_concen} and satisfy the same constraints.
	Additionally, suppose that $\alpha  \gtrsim \frac{d^{\frac{p}{2}\vee 1}}{n\eps}$ and $F\gtrsim \eps R$.
	When conditioned on the event that
	\begin{align*}
		\normp{S\Lambda(f(A\bar{x}) - b)}^p 
		\lesssim
		\normp{\Lambda(f(A\bar{x}) - b)}^p
		\quad\text{and}\quad       
		\sup_{x\in T}\,\normp{S\Lambda (f(Ax) - f(A\bar{x}))}^p 
		\lesssim 
		F, 
		% \numberthis\label{eq:main_concen_condition}
	\end{align*}
	it holds with probability at least $1-\delta$ that
	\begin{align*}
		\MoveEqLeft \sup_{x\in T}\abs*{(\normp{S\Lambda (f(Ax)-b)}^p - \normp{S\Lambda (f(A\bar{x})-b)}^p) - (\normp{\Lambda (f(Ax)-b)}^p - \normp{\Lambda (f(A\bar{x})-b)}^p)} \\
		& \leq  
		C\, \Gamma\cdot \left(\log^{\frac{5}{4}} d\sqrt{\log\frac{n}{\eps d}} + \sqrt{\log\frac{1}{\delta}}\right),
	\end{align*}
	where $C$ is an absolute constant and $\Gamma$ is as defined in \Cref{eq:Gamma}.
\end{corollary}

\begin{proof}
	In \Cref{lem:main_concen}, we take $v=b$.
	Note that we have $ V=\normp{\Lambda(f(A\bar{x}) - b)}^p$ and hence the result follows, noticing that the last term in the error bound of \Cref{lem:main_concen} is the dominating term.    
\end{proof}





Now, we are ready to complete the proof of \Cref{lem:main_concen_final}.

\begin{proof}[Proof of \Cref{lem:main_concen_final}]
	
	Without loss of generality, we can assume that $n\gtrsim d^{\frac{p}{2}\vee 1}/\eps^{p\vee 2}$. 
	We rely on \Cref{cor:main_concen} in this proof. 
	To apply the corollary, we need to pick a suitable subset $T$ so that the output $\hat{x}\in T$.
	The set $T$ will be defined through suitable bounds for $R$ and $F$ and 
	the main part of the proof will focus on obtaining these bounds. 
	
	Before doing so, we present some useful inequalities.
	First, by Markov inequality, with probability at least $0.99$, we have
	\begin{align*}
		\normp{S\Lambda(f(A\bar{x}) - b)}^p
		& \leq
		100\normp{\Lambda(f(A\bar{x}) - b)}^p. \numberthis\label{eq:markov_opt}
	\end{align*}
	We condition on this event in the remainder of the proof. By the optimality of $\hat{x}$, we have
	\begin{align*}
		\normp{S\Lambda(f(A\hat{x}) - b)}^p + \eps \normp{\Lambda A\hat{x}}^p
		& \leq
		\normp{S\Lambda(f(A \bar{x}) - b)}^p + \eps \normp{\Lambda A \bar{x}}^p.\numberthis\label{eq:x_hat_optimality}
	\end{align*}
	It implies that, by \Cref{eq:markov_opt} and  \Cref{eq:x_hat_optimality},
	\begin{align*}
		\normp{\Lambda A\hat{x}}^p
		& \leq
		\frac{1}{\eps}\normp{S\Lambda(f(A \bar{x}) - b)}^p +  \normp{\Lambda A \bar{x}}^p
		\leq
		\underbrace{\frac{100}{\eps}\normp{\Lambda(f(A\bar{x}) - b)}^p +  \normp{\Lambda A \bar{x}}^p}_{:=R_0} \numberthis\label{eq:r_0_bound}
	\end{align*}
	Throughout the remainder of the proof, we assume that 
	\[
	\alpha \gtrsim \frac{d^{1\vee \frac{p}{2}}}{n\eps^{p\vee 2}}\poly\log n \quad\text{ and }\quad \delta \sim \frac{1}{\log\log(1/\eps)}
	\]
	so that the error term in \Cref{cor:main_concen} can be upper bounded as
	\[
	\Gamma\cdot (\poly\log n + \sqrt{\log(1/\delta)}) \lesssim \eps F^{\theta}R^{\beta},
	\]
	where $\beta = \frac{1}{2}\wedge \frac{1}{p}$ and $\theta = (1-\frac{1}{p})\vee \frac{1}{2}$. Note that $\beta + \theta = 1$.
	
	\paragraph{Bounding $F$ in \Cref{cor:main_concen}}
	We would like to first use \Cref{cor:colt_concen} and let 
	\begin{align*}
		T_{-1} = \setdef{x\in\mathbb{R}^d}{\normp{\Lambda Ax}^p \leq R_0 }.
	\end{align*}
	Now, we check the conditions.
	Our choice of $\alpha$ satisfies that $\alpha \gtrsim \frac{d^{1\vee \frac{p}{2}}}{n}\poly\log n$, thus, by \Cref{lem:SE}, $S$ is a constant-distortion subspace embedding for $\Lambda A$ with probability at least $0.99$, i.e. $\normp{S\Lambda  Ax} \leq 2\normp{\Lambda Ax}$ for all $x\in \R^d$.
	Recall that
	\begin{align*}
		R_0 
		& =
		\frac{100}{\eps}\normp{\Lambda(f(A\bar{x}) - b)}^p +  \normp{\Lambda A \bar{x}}^p.
	\end{align*}
	Hence, by \Cref{cor:colt_concen} with our choice of $\alpha$ and $R=R_0$, it holds with probability $0.99$ that
	\begin{align*}
		\sup_{x\in T_{-1}}\abs*{\normp{S\Lambda (f(Ax) - f(A\bar{x}))}^p - \normp{\Lambda (f(Ax) - f(A\bar{x}))}^p}
		& \leq
		C_1 \eps R_0, \numberthis\label{eq:colt_concen_1}
	\end{align*}
	where $C_1$ is a constant that depends only on $p$. Below we shall use $C_2, C_3,\dots$ to denote constants that depend only on $p$.
	Conditioning on the event in \Cref{eq:colt_concen_1}, it follows that
	\begin{align*}
		&\quad\ \normp{\Lambda(f(A\hat{x}) - f(A\bar{x}))}^p\\
		& \leq
		\normp{S\Lambda (f(A\hat{x}) - f(A\bar{x}))}^p + C_1 \eps R_0 \\
		& \leq
		2^p \left( \normp{S\Lambda (f(A\hat{x}) - b)}^p + \normp{S\Lambda (f(A\bar{x}) - b)}^p \right) + C_1\eps R_0 \\
		& \stackrel{(A)}{\leq}
		2^p \left( \normp{S\Lambda (f(A\bar{x}) - b)}^p + \eps \normp{\Lambda A \bar{x}}^p + \normp{S\Lambda (f(A\bar{x}) - b)}^p \right) + C_1\eps R_0 \\
		& =
		2^p \left( 2\normp{S\Lambda (f(A\bar{x}) - b)}^p + \eps \normp{\Lambda A \bar{x}}^p \right) + C_1\eps R_0\\
		& \stackrel{(B)}{\leq}
		2^p \left( 2\cdot 100\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps \normp{\Lambda A \bar{x}}^p \right) + C_1 \eps (\frac{100}{\eps}\normp{\Lambda(f(A\bar{x}) - b)}^p + \normp{\Lambda A\bar{x}}^p) \\
		&\leq 
		C_2(\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps\normp{\Lambda A \bar{x}}^p) \quad \text{for some large constant $C_2$}, \numberthis\label{eq:x_hat_in_T}
	\end{align*}
	where (A) is due to \Cref{eq:x_hat_optimality}, the optimality of $\hat{x}$, and (B) to \Cref{eq:markov_opt}, the Markov inequality for $\normp{\Lambda (f(A\bar{x}) - b)}^p$, and the definition of $R_0$.
	
	Define $F_0$ to be the RHS of \Cref{eq:x_hat_in_T}, i.e.
	\begin{align*}
		F_0
		& :=
		C_2(\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps\normp{\Lambda A \bar{x}}^p).
	\end{align*}
	We preview that the set $T$ we use in \Cref{cor:main_concen} contains the element $x$ satisfying the inequality
	\begin{align*}
		\normp{\Lambda(f(Ax) - f(A\bar{x}))}^p
		& \leq 
		F_0
	\end{align*}
	Hence, \Cref{eq:x_hat_in_T} suggests that $\hat{x}$ is in the domain of interest and hence $F_0$ is the bound we will use in \Cref{cor:main_concen}.
	
	
	
	\paragraph{Bounding $R$ in \Cref{cor:main_concen}}
	
	Now, we would like to use \Cref{cor:main_concen}.
	Recall that
	\begin{align*}
		R_0
		& = 
		\frac{100}{\eps}\normp{\Lambda(f(A\bar{x}) - b)}^p +  \normp{\Lambda A \bar{x}}^p.
	\end{align*}
	We can apply \Cref{cor:main_concen} with $R_0$ but it will give a weaker result.
	However, we shall still use this weaker result and improve the bounds iteratively.
	Specifically, we shall define $R_i$ based on $R_{i-1}$, ensuring that $R_i\leq R_0$ and that each $R_i$ has the form of $X_i\normp{\Lambda(f(A\bar{x}) - b)}^p + Y_i\normp{\Lambda A \bar{x}}^p$ for some $X_i, Y_i\geq 1$ (for example, $X_0 = \frac{100}{\eps}$ and $Y_0 = 1$). Furthermore, let 
	\begin{align*}
		T_i = \setdef{x\in\R^d}{\normp{\Lambda Ax}^p\leq  R_i \text{ and } \normp{\Lambda(f(Ax) - f(A\bar{x}))}^p \leq F_0 },
	\end{align*}
	so that $T_i\subseteq T_0$. 
	More specifically, we shall use $T_i$ to estimate an upper bound of $\normp{\Lambda A\hat{x}}^p$ and define $R_{i+1}$ based on the upper bound, ensuring that $\bar{x}\in T_i$. This guarantees that $T_i$ satisfies the subset condition in \Cref{cor:main_concen}. We shall also verify other conditions of \Cref{cor:main_concen}.
	
	It is clear that $R_i\leq R_0 \lesssim \frac{1}{\eps} F_0$.
	By \Cref{eq:markov_opt}, we have $\normp{S \Lambda (f(A\bar{x}) - b)}^p \lesssim \normp{ \Lambda (f(A\bar{x}) - b)}^p$ and, by \Cref{eq:colt_concen_1} and the fact that $T_i\subseteq T_{-1}$, we have
	\begin{align*}
		\sup_{x\in T_i}\normp{S\Lambda(f(Ax) - f(A\bar{x}))}^p
		& \leq
		\sup_{x\in T_i}\normp{\Lambda(f(Ax) - f(A\bar{x}))}^p + C_1\eps R_0
		\lesssim 
		F_0. 
	\end{align*}
	We invoke \Cref{cor:main_concen} with our choice of $\alpha$, $R=R_i$ and $F=F_0$.
	Hence, with probability $1-\delta$,
	\begin{align*}
		&\quad\ \sup_{x\in T_i}\abs*{(\normp{S\Lambda (f(Ax)-b)}^p - \normp{S\Lambda (f(A\bar{x})-b)}^p) - (\normp{\Lambda (f(Ax)-b)}^p - \normp{\Lambda (f(A\bar{x})-b)}^p)} \\
		& \leq
		C_3 \cdot \eps R_i^{\beta} F_0^{\theta} \quad \text{for some constant $C_3$.} \numberthis\label{eq:T_i_main_concen}
	\end{align*}
	To use \Cref{eq:T_i_main_concen}, we would like to argue that the solution $\hat{x} \in T_i$.
	For $T_0$, we have
	\begin{align*}
		\normp{\Lambda A \hat{x}}^p \leq R_0\quad \text{by \Cref{eq:r_0_bound} and} \quad \normp{\Lambda(f(A\hat{x}) - f(A\bar{x}))}^p \quad \text{by \Cref{eq:x_hat_in_T}}
	\end{align*}
	and hence $\hat{x}\in T_0$.
	From now on, suppose that $\hat{x}\in T_i$ and we will argue that $\hat{x}\in T_{i+1}$.
	
	
	
	We continue to bound \Cref{eq:T_i_main_concen}.
	Assume that $K Y_i/X_i \geq \eps$ for some $K\geq 1$, then we can upper bound $R_i^{\beta} F_0^{\theta}$ as follows.
	\begin{align*}
		R_i^{\beta} F_0^{\theta} 
		&=  
		\left(X_i \normp{\Lambda(f(A\bar{x}) - b)}^p + Y_i\normp{\Lambda A \bar{x}}^p\right)^\beta \cdot C_2^\theta \left(\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps\normp{\Lambda A \bar{x}}^p\right)^\theta \\
		&\leq  
		C_2^\theta \left(X_i \normp{\Lambda(f(A\bar{x}) - b)}^p + Y_i\normp{\Lambda A \bar{x}}^p\right)^\beta \left(\normp{\Lambda(f(A\bar{x}) - b)}^p + \frac{K Y_i}{X_i}\normp{\Lambda A \bar{x}}^p\right)^\theta \\
		&\leq 
		\left(\frac{C_2}{X_i}\right)^\theta (X_i \normp{\Lambda(f(A\bar{x}) - b)}^p + K Y_i\normp{\Lambda A \bar{x}}^p)\quad \text{note that $\beta+\theta=1$}. \numberthis\label{eq:bound_for_rf}
	\end{align*}
	Thus,
	\begin{align*}
		\normp{\Lambda A\hat{x}}^p
		& \leq
		\frac{1}{\eps}\left(\normp{S\Lambda(f(A \bar{x}) - b)}^p - \normp{S\Lambda(f(A \hat{x}) - b)}^p\right) +  \normp{\Lambda A \bar{x}}^p \\
		&\leq 
		\frac{1}{\eps}\cdot (\normp{\Lambda(f(A \bar{x}) - b)}^p - \normp{\Lambda(f(A \hat{x}) - b)}^p + C_3 \cdot \eps R_i^{\beta}F_0^\theta) + \normp{\Lambda A\bar{x}}^p \quad \text{by \Cref{eq:T_i_main_concen}} 
	\end{align*}
	From our assumption, we have
	\begin{align*}
		\MoveEqLeft \normp{\Lambda(f(A \bar{x}) - b)}^p - \normp{\Lambda(f(A \hat{x}) - b)}^p \\
		& \lesssim
		\eps(\normp{\Lambda(f(A\bar{x}) - b)}^p + \eps \norm{\Lambda A\bar{x}}^p) \\
		& \lesssim
		\eps R_i^{\beta}F_0^\theta \quad \text{by the definition of $F_0$ and $F_0\lesssim R_i$.}
	\end{align*}
	In other words, we have
	\begin{align*}
		\normp{\Lambda A\hat{x}}^p
		& \leq
		C_4\cdot (R_i^{\beta}F_0^\theta) + \normp{\Lambda A\bar{x}}^p \quad \text{for some constant $C_4$}\\
		&\leq
		C_4\left(\frac{C_2}{X_i}\right)^\theta (X_i \normp{\Lambda(f(A\bar{x}) - b)}^p + K Y_i\normp{\Lambda A \bar{x}}^p)  + \normp{\Lambda A \bar{x}}^p \\
		&\leq
		X_{i+1}\normp{\Lambda(f(A\bar{x}) - b)}^p + Y_{i+1}\normp{\Lambda A \bar{x}}^p, \numberthis\label{eq:r_i+1_rhs}
	\end{align*}
	where
	\begin{align*}
		X_{i+1} = C_4 C_2^\theta X_i^{1-\theta}
		\quad\text{and}\quad 
		Y_{i+1} = 1 + \frac{C_4 C_2^\theta K Y_i}{X_i^\theta}.
	\end{align*}
	Define $R_{i+1}$ to be the minimum of $R_0$ and the expression in \Cref{eq:r_i+1_rhs}, i.e.
	\begin{align*}
		R_{i+1}
		:=
		R_0\wedge (X_{i+1}\normp{\Lambda(f(A\bar{x}) - b)}^p + Y_{i+1}\normp{\Lambda A \bar{x}}^p).
	\end{align*}
	We immediately have $\hat{x}\in T_{i+1}$, which is needed to iterate the argument.
	
	Let $X_0 = 100/\eps$ and $Y_0 = 1$. By induction, one can show that 
	\[
	X_{i} = C_5^{\frac{1-(1-\theta)^i}{\theta}}\left(\frac{100}{\eps}\right)^{(1-\theta)^{i}}
	\]
	for $C_5 = C_4 C_2^\theta$. Then $C_6 \leq X_i\leq 100 C_5^{1/\theta}/\eps$ for some constant $C_6$ for all $i\leq r$, thus $Y_{i+1} \leq 1 + C_7 Y_i\leq C_8 Y_i$ for some constants $C_7$ and $C_8$. 
	When $r \sim_p \log\log(1/\eps)$, we have \begin{align*}
		X_r \leq C_9
		\quad \text{and}\quad 
		Y_r \leq C_4(C_8)^{r-1} = \poly\log\frac{1}{\eps}.
	\end{align*}
	We shall also verify that $KY_i/X_i\geq \eps$ for some $\eps$. 
	Indeed, $Y_i/X_i\geq 1 /(100 C_5^{1/\theta}/\eps) \geq \eps/K$ for $K = 100C_5^{1/\theta}$. 
	
	Iterating the argument above $r$ times. The total failure probability is at most $\delta r + 0.03 = 0.1$ since $\delta\sim 1/r$. It then follows from \Cref{eq:T_i_main_concen} with $i = r-1$ that
	\begin{align*}
		\MoveEqLeft\normp{ \Lambda(f(A\hat{x})- b) }^p - \normp{ \Lambda(f(A\bar{x})- b) }^p \\
		&\leq 
		\normp{S\Lambda (f(A\hat{x})-b)}^p - \normp{S\Lambda (f(A\bar{x})-b)}^p  + C_3 \cdot \eps R_{r-1}^{\beta }F_0^\gamma \\
		&\leq
		\eps \normp{\Lambda A\bar{x}}^p + C_3 \cdot \eps R_{r-1}^{\beta}F_0^\gamma \quad \text{by the optimality of $\hat{x}$}\\
		& \lesssim
		\eps \normp{\Lambda A\bar{x}}^p + \eps(X_r \normp{\Lambda(f(A\bar{x}) - b)}^p + Y_r \normp{\Lambda A\bar{x}}^p) \quad \text{by \Cref{eq:bound_for_rf}}\\
		&\lesssim 
		\eps\normp{\Lambda(f(A\bar{x}) - b)}^p  +  \left(\eps\poly\log\frac{1}{\eps}\right) \normp{\Lambda A\bar{x}}^p.
	\end{align*}
	Rescaling $\eps\poly\log\frac{1}{\eps}$ to $\eps$ proves the claimed result of the theorem.
\end{proof}


\subsubsection{Proof of \Cref{lem:main_concen}}\label{sec:main_concen_proof}

In this section, we will prove \Cref{lem:main_concen}.
Recall that $\bar{x}$ is an arbitrary fixed vector in $\mathbb{R}^d$,  $v$ is an arbitrary fixed vector in $\mathbb{R}^n$, $V=\normp{\Lambda(f(A\bar{x}) - v)}^p$, $R\geq \normp{\Lambda A \bar{x}}^p$, $F\geq V$ and $\{\bar{x}\} \subseteq T \subseteq \setdef{x\in \mathbb{R}^d}{\normp{\Lambda A x}^p\leq R}$.
Also, we would like to bound the following expression
\begin{align*}
	\sup_{x\in T}\abs*{(\normp{S\Lambda(f(Ax)-v)}^p - \normp{S\Lambda(f(A\bar{x})-v)}^p) - (\normp{\Lambda(f(Ax)-v)}^p - \normp{\Lambda(f(A\bar{x})-v)}^p)},
\end{align*}
which can be written as
\begin{equation}\label{eq:main_concen_expr}
	\sup_{x\in T}\abs*{ \sum_{i=1}^n (S_{ii}^p - 1)\Lambda_{ii}^p(\abs{(f(Ax) - v)_i}^p - \abs{(f(A\bar{x}) - v)_i}^p)}.
\end{equation}
We shall bound \Cref{eq:main_concen_expr} from above by, up to a constant factor, 
\begin{align*}
	\eps V + \frac{d^{1\vee\frac{p}{2}}}{\alpha n}R + \Gamma \cdot \left(\poly\log n+\sqrt{\log\frac{1}{\delta}}\right) \numberthis\label{eq:main_concen_bound}
\end{align*}
with probability $1-\delta$.
Recall that, as defined in \Cref{eq:Gamma}, 
\begin{align*}
	\Gamma = 
	\begin{cases}
		(d / (\alpha n))^{\frac{1}{2}} F^{\frac{1}{2}}R^{\frac{1}{2}}   & \text{when $1 \leq p \leq 2$}\\
		(d^{\frac{p}{2}} /(\alpha n))^{\frac{1}{p}} F^{1-\frac{1}{p}} R^{\frac{1}{p}} & \text{when $p > 2$.}
	\end{cases}
\end{align*}


We preview that the first term $\eps V$ comes from \Cref{lem:bad_terms}, the second term $\frac{d^{1\vee\frac{p}{2}}\eps^p}{\alpha n}R$ from \Cref{lem:low_lewis_terms} and the third term $\Gamma\cdot (\poly\log n+\sqrt{\log\frac{1}{\delta}})$ from Dudley's integral (\Cref{lem:dudley}).
We first present a useful lemma.


\begin{lemma}\label{lem:term_bound}
	
	For any $R\geq \normp{\Lambda A\bar{x}}^p$, let $T$ be a set that $\{\bar{x}\} \subseteq T \subseteq \setdef{x\in \mathbb{R}^d}{\normp{\Lambda A x}^p\leq R}$.
	Also, let $w_1,\dots,w_n$ be the Lewis weights of $\Lambda A$.
	It holds for all $x\in T$ and $i\in [n]$ that
	\begin{align*}
		\abs{\Lambda_{ii}(f(Ax) - f(A\bar{x}))_i}
		& \leq
		2d^{\frac{1}{2}-\frac{1}{2\vee p}}w_i^{\frac{1}{p}}R^{\frac{1}{p}}.
	\end{align*}
\end{lemma}

\begin{proof}
	
	
	Note that
	\begin{align*}
		\abs{\Lambda_{ii} (f(Ax) - f(A\bar{x}))_i}  
		& \leq
		\abs{\Lambda_{ii}(Ax - A\bar{x})_i} & \text{by the Lipschitz condition} \\
		& \leq
		d^{\frac{1}{2}-\frac{1}{2\vee p}}w_i^{\frac{1}{p}}\normp{\Lambda Ax- \Lambda A\bar{x}} & \text{by \Cref{lem:lewis_weight_properties}(d)}
	\end{align*}
	Since $x,\bar{x}$ are both in $T$, we have
	\begin{align*}
		\normp{\Lambda Ax- \Lambda A\bar{x}}
		& \leq
		\normp{\Lambda Ax} + \normp{\Lambda A\bar{x}}
		\leq
		2R^{\frac{1}{p}}.
	\end{align*}
	The desired result follows.  
\end{proof}

Define the set $G$ of ``good'' indices to be
\begin{align*}
	G
	& :=
	\setdef*{i\in [n]}{\abs{\Lambda_{ii}(f(A\bar{x})-v)_i} \leq \frac{d^{\frac{1}{2}-\frac{1}{2\vee p}}w_i^{\frac{1}{p}}R^{\frac{1}{p}}}{\eps}}.\numberthis\label{eq:gd_idx_def}
\end{align*}
We shall first take care of the terms with ``bad'' indices in \Cref{eq:main_concen_expr}, i.e. the indices \emph{not} in $G$, and hence obtain the first term $\eps V$ in \Cref{eq:main_concen_bound}.
We highlight that only the property of the nonnegativity of the diagonal entries of $S$ is used in \Cref{lem:bad_terms}.

\begin{lemma}\label{lem:bad_terms}
	
	For any $R\geq \normp{\Lambda A\bar{x}}^p$ and $\eps>0$, let $T$ be a set that $\{\bar{x}\} \subseteq T \subseteq \setdef{x\in \mathbb{R}^d}{\normp{\Lambda A x}^p\leq R}$ and $G$ be the set defined in \Cref{eq:gd_idx_def}.
	Suppose that $S$ is an $n$-by-$n$ diagonal matrix with nonnegative diagonal entries and
	\[
	\normp{S\Lambda (f(A\bar{x}) - v)}^p
	\lesssim
	V, 
	\]
	where $V=\normp{\Lambda(f(A\bar{x})-v)}^p$. Then, we have
	\begin{align*}
		\sup_{x\in T}\abs*{ \sum_{i\notin G} (S_{ii}^p-1) \Lambda_{ii}^p\big( \abs{(f(Ax) - v)_i}^p - \abs{(f(A\bar{x}) - v)_i}^p \big) } 
		\lesssim
		\eps V.
	\end{align*}
	
\end{lemma}



\begin{proof}
	
	To ease the notations, let 
	\begin{align*}
		u_x
		& :=
		f(Ax) \quad \text{for all $x\in \mathbb{R}^d$}
		\quad \text{and} \quad 
		\lambda_i
		:=
		\Lambda_{ii} \quad \text{for $i\in [n]$.}
	\end{align*}
	Note that by the triangle inequality,
	\[
	\abs*{ \sum_{i\notin G} (S_{ii}^p-1)\lambda_i^p \big( \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p \big) } 
	\leq 
	\sum_{i\notin G} (S_{ii}^p + 1)\lambda_i^p \abs[\big]{ \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p }
	\]
	Furthermore, by the inequality $\abs[\big]{ \abs{a}^p - \abs{b}^p } \leq p\abs{a-b}\, \abs*{ \abs{a}^{p-1} + \abs{b}^{p-1} }$, we have
	\[
	\lambda_i^p \abs[\Big]{ \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p } \\
	\leq
	p\abs{\lambda_i(u_x - u_{\bar{x}})_i} \cdot \abs*{\abs{\lambda_i(u_{x} - v)_i}^{p-1} + \abs{\lambda_i(u_{\bar{x}} - v)_i}^{p-1}}.
	\]
	For any $i\notin G$ and $x\in T$, by Lemma \ref{lem:term_bound} and the definition of $G$, we have
	\begin{gather*}
		\abs{\lambda_i(u_x - u_{\bar{x}})_i}
		\leq
		2\eps\abs{\lambda_i(u_{\bar{x}} - v)_i}, \\
		\abs{\lambda_i(u_x - v)_i}
		\leq
		\abs{\lambda_i(u_x - u_{\bar{x}})_i}+\abs{\lambda_i(u_{\bar{x}} - v)_i}
		\leq
		(1+2\eps) \abs{\lambda_i(u_{\bar{x}} - v)_i} .
	\end{gather*}
	It follows that
	\begin{align*}
		\lambda_i^p \abs[\Big]{ \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p }
		\lesssim
		\eps \abs{\lambda_i(u_{\bar{x}}  -v)_i}^p.
	\end{align*}
	which implies that
	\begin{align*}
		\MoveEqLeft \sup_{x\in T}\abs*{ \sum_{i\notin G} (S_{ii}^p-1) \lambda_i^p \big( \abs{(u_x-v)_i}^p - \abs{(u_{\bar{x}}-v)_i}^p \big) }  \\
		& \lesssim
		\eps \sum_{i\notin G} (S_{ii}^p+1) \abs{\lambda_i(u_{\bar{x}} - v)_i}^p
		\leq 
		\eps \sum_{i=1}^n (S_{ii}^p+1) \abs{\lambda_i(u_{\bar{x}}  -v)_i}^p
		\lesssim 
		\eps \cdot V,
	\end{align*}
	where we used the assumption of the lemma in the last step.
	
\end{proof}

Now, we also define the set of indices whose term has a high Lewis weight within $G$. Let
\begin{align*}
	J
	& :=
	\setdef*{i\in G}{w_i > \frac{\eps^{p} d}{n^2}}. \numberthis\label{eq:low_lw_idx_def}
\end{align*}
We now take care of the terms with low Lewis weights in \Cref{eq:main_concen_expr}, i.e. the indices $i\not\in J$, and hence obtain the second term $\frac{d^{1\vee\frac{p}{2}}\eps^p}{\alpha n}R$ in \Cref{eq:main_concen_bound}.
We highlight that only the property of the diagonal entries of $S$ being in $[0,\frac{1}{\alpha}]$ is used in \Cref{lem:low_lewis_terms}.

\begin{lemma}\label{lem:low_lewis_terms}
	
	For any $R \geq \normp{\Lambda A\bar{x}}^p$ and $\eps>0$, let $T$ be a set that $\{\bar{x}\} \subseteq T \subseteq \setdef{x\in \mathbb{R}^d}{\normp{\Lambda A x}^p\leq R}$ and $J$ be the set defined in \Cref{eq:low_lw_idx_def}.
	Suppose $S$ is an $n$-by-$n$ diagonal matrix whose entries satisfy $0\leq S_{ii}^p \leq \frac{1}{\alpha}$ for any $\alpha>0$.
	Then, we have
	\begin{align*}
		\sup_{x\in T} \abs*{ \sum_{i\in G\backslash J} (S_{ii}^p-1)\Lambda_{ii}^p\big( \abs{(f(Ax) - v)_i}^p - \abs{(f(A\bar{x}) - v)_i}^p \big) } 
		& \lesssim 
		\frac{d^{1\vee\frac{p}{2}}}{\alpha n}R. 
	\end{align*}
	
\end{lemma}


\begin{proof}
	
	To ease the notations, let 
	\begin{align*}
		u_x
		& :=
		f(Ax) \quad \text{for all $x\in \mathbb{R}^d$}
		\quad \text{and} \quad 
		\lambda_i
		:=
		\Lambda_{ii} \quad \text{for $i\in [n]$.}
	\end{align*}
	Note that by the triangle inequality,
	\[
	\abs*{ \sum_{i\in G\backslash J} (S_{ii}^p-1)\lambda_i^p \big( \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p \big) } 
	\leq 
	\sum_{i\in G\backslash J} (S_{ii}^p + 1)\lambda_i^p \abs[\big]{ \abs{(u_{x} - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p }.
	\]
	Since $i\in G$, by Lemma \ref{lem:term_bound}, we have
	\begin{gather*}
		\abs{\lambda_i(u_x-v)_i}
		\leq 
		\abs{\lambda_i(u_x - u_{\bar{x}})_i} + \abs{\lambda_i(u_{\bar{x}} - v)_i}
		\leq
		(2+\frac{1}{\eps}) d^{\frac{1}{2}-\frac{1}{2\vee p}}w_i^{\frac{1}{p}}R^{\frac{1}{p}}, \\
		\intertext{which, together with $i\not\in J$, implies that}
		\abs{\lambda_i(u_x-v)_i}^p
		\lesssim 
		\frac{d^{(\frac{p}{2}\vee 1)-1}}{\eps^p}w_i R
		\lesssim
		\frac{d^{1\vee\frac{p}{2}}}{ n^2}R.
	\end{gather*}
	Recall that we assume $S_{ii}^p \leq \frac{1}{\alpha}$.
	Therefore, 
	\[
	\sup_{x\in T} \abs*{ \sum_{i\in G\backslash J} (S_{ii}^p-1) \lambda_i^p \big( \abs{(u_x-v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p \big) } 
	\lesssim
	\sum_{i\in G\backslash J} (\frac{1}{\alpha} + 1) \cdot \frac{d^{1\vee\frac{p}{2}}}{ n^2}R 
	\lesssim 
	\frac{d^{1\vee\frac{p}{2}}}{\alpha n}R. \qedhere
	\]
	
\end{proof}


With \Cref{lem:bad_terms} and \Cref{lem:low_lewis_terms}, we only need to take care of the indices in $J$.
Namely the set of indices $i$ such that
\begin{align*}
	\abs{(f(A\bar{x}) - v)_i} \leq \frac{d^{\frac{1}{2} - \frac{1}{2\vee p}} w_i^{\frac{1}{p}} R^{\frac{1}{p}}}{\eps}
	\quad \text{and} \quad
	w_i > \frac{\eps^{p} d}{n^2}.
\end{align*}
Now, we would like to bound the following expression
\begin{align*}
	\sup_{x\in T} \abs*{ \sum_{i\in J} (S_{ii}^p-1) \Lambda_{ii}^p\big( \abs{(f(Ax)-v)_i}^p - \abs{(f(A\bar{x})- v)_i}^p \big) }.
\end{align*}
We consider bounding its $\ell$-th moment and then apply Markov's inequality for some $\ell$ to be determined later. To that end, consider
\begin{align*}
	\Theta_{S}
	& :=
	\sup_{x\in T} \abs*{ \sum_{i\in J} (S_{ii}^p-1) \Lambda_{ii}^p\big( \abs{(f(Ax)-v)_i}^p - \abs{(f(A\bar{x})- v)_i}^p \big) }
\end{align*}
By the standard symmerization trick, we have
\begin{equation}\label{eq:main_obj_1}
	\E_S \Theta_{S}^\ell
	\leq
	2^\ell \E_{\xi, S} \left(\sup_{x\in T} \abs*{\sum_{i\in J} \xi_i \cdot S_{ii}^p \Lambda_{ii}^p\big( \abs{(f(Ax)-v)_i}^p - \abs{(f(A\bar{x})- v)_i}^p \big) }\right)^\ell,
\end{equation}
where $\xi$ is a $\abs{J}$-dimensional vector whose entries are independent Rademacher random variable, i.e. each $\xi_i$ is uniform on $\{-1,1\}$.

Next, we condition on $S$.
Recall that $S_{ii}^p$ is either $\frac{1}{\alpha}$ or $0$, let $I\subseteq J$ be the set of indices $i$ such that $S_{ii}^p = \frac{1}{\alpha}$.
For any $x\in \mathbb{R}^d$, we define $z_x$ to be the $n$-dimensional vector whose $i$-th entry is 
\begin{align*}
	(z_x)_i
	& :=
	\abs{\Lambda_{ii}(f(Ax) - v)_i}^p - \abs{\Lambda_{ii}(f(A\bar{x}) - v)_i}^p \numberthis\label{eq:z_def}
\end{align*}
Also, we define a pseudometric $\rho$ to be
\begin{align*}
	\rho(x,x')
	& :=
	\norm{(z_x)_I - (z_{x'})_I}_2 \quad \text{for any $x,x'\in \mathbb{R}^d$} \\
	& =
	\bigg( \sum_{i\in I} \big(\abs{\Lambda_{ii}(f(Ax) - v)_i}^p - \abs{\Lambda_{ii}(f(Ax') - v)_i}^p\big)^2 \bigg)^{1/2}
\end{align*}
Recall that $(\cdot)_I$ means we shrink the vector by only retaining the entries whose index is in $I$.
Now, in order to upper bound the right-hand side of \Cref{eq:main_obj_1}, we seek to upper bound
\[
\E_{\xi} \left(\sup_{x\in T} \abs*{\inner{\xi_I}{(z_x)_I}}\right)^\ell .
\]
Since $\bar{x}\in T$, the $\ell$-moment of the supremum can be upper bounded using Dudley's integral (\Cref{lem:dudley}) as 
\begin{equation}\label{eq:main_obj_2}
	\E_{\xi} \left(\sup_{x\in T} \abs*{\inner{\xi_I}{(z_x)_I}}\right)^\ell
	\lesssim 
	C^\ell \left[\left(\int_{0}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T, \rho, r)} dr\right)^\ell + (\sqrt{\ell}\Diam(T,\rho))^\ell\right].
\end{equation}
Recall that $\mathcal{N}(T,\rho, r)$ is the covering number of $T$ w.r.t.\ $\rho$ and $r$.
We will prove in \Cref{sec:dudley_bound} that
\begin{align*}
	\int_{0}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr
	& \lesssim
	\alpha \cdot \Gamma \log^{\frac{5}{4}} d\sqrt{\log\frac{n}{\eps d}}\quad\text{and}\quad \Diam(T,\rho) \lesssim \alpha\cdot \Gamma
	\numberthis\label{eq:dudley_bound}
\end{align*}
where
\[
\Gamma = \left(\frac{d^{\frac{p}{2}\vee 1} R F^{(p-1)\vee 1}}{\alpha n }\right)^{\frac{1}{p}\wedge \frac{1}{2}}.	
\]
Taking expectation over $S$, it follows from \Cref{eq:main_obj_1}, \Cref{eq:main_obj_2} and \Cref{eq:dudley_bound} that
\begin{align*}
	\E_S \Theta_S^\ell \lesssim (C'\Gamma)^\ell\left(\left(\log^{\frac{5}{4}} d\sqrt{\log\frac{n}{\eps d}}\right)^\ell + \sqrt{\ell}^\ell\right).
\end{align*}
Take $\ell = \log(1/\delta)$. By Markov inequality, it holds with probability $1-\delta$ that
\[
\Theta_S = \sup_{x\in T}\abs*{ \sum_{i\in J} \xi_i\cdot S_{ii}^p \Lambda_{ii}^p \big( \abs{(u_x - v)_i}^p - \abs{(u_{\bar{x}} - v)_i}^p \big) } 
\lesssim
\Gamma\cdot \left(\log^{\frac{5}{4}} d\sqrt{\log\frac{n}{\eps d}} + \sqrt{\log\frac{1}{\delta}}\right).
\]
Note that it is the third term in \Cref{eq:main_concen_bound}.
Combining \Cref{lem:bad_terms,lem:low_lewis_terms} proves \Cref{lem:main_concen}.



\subsubsection{Diameter Estimates}\label{sec:diameter}
In order to bound Dudley's integral in \Cref{eq:dudley_bound}, we need to bound the covering number $\mathcal{N}(T,\rho,r)$. To this end, we shall bound the metric, $\rho$, and the diameter $\Diam(T,\rho)$. The proof imitates the proofs in earlier works, e.g., \citet{LT91,ICLR24,COLT24}, on subspace embeddings and active regression problems.


\begin{lemma}\label{lem:metric_transform_combined}
	% \note{need to rewrite}
	Let $A\in \R^{n\times d}$, $\bar{x}\in \mathbb{R}^d$, $v\in\R^n$, $f\in\lip_1$, $\Lambda \in \R^{n\times n}$, $\alpha\in [0,1]$ and $S\in \R^{n\times n}$ be as defined in Lemma~\ref{lem:main_concen} and satisfy the same constraints.
	Suppose that $0\leq \alpha \leq 1$, $R\geq \normp{\Lambda A\bar{x}}^p$ and $F\geq \normp{\Lambda(f(A\bar{x}) - v)}^p$. 
	Let $T$ be a set that $\{\bar{x}\} \subseteq T \subseteq \setdef{x\in \mathbb{R}^d}{\normp{\Lambda A x}^p\leq R}$.
	If $I$ is a subset of $J$ such that
	\begin{align*}
		\normp{(\Lambda(f(A\bar{x}) - v))_I}^p 
		& \lesssim
		\alpha \cdot \normp{\Lambda(f(A\bar{x}) - v)}^p
		\quad \text{and} \quad
		\sup_{x\in T}\normp{(\Lambda(f(Ax) - f(A\bar{x})))_I}^p
		\leq 
		\alpha \cdot F
	\end{align*}
	then, for any $x,x'\in T$ and $q=\log(\frac{n}{\eps d})$, we have
	\begin{gather*}
		\rho(x,x')
		\lesssim
		K\cdot \left(\norm*{W^{-\frac{1}{p}}\Lambda A(x-x')}_{w,q}^{\frac{p}{2}\wedge 1} \wedge d^{\frac{1}{2}-\frac{1}{2\vee p}} \norm*{W^{-\frac{1}{p}}\Lambda A(x-x')}_{w,p}^{\frac{p}{2}\wedge 1} \right) \\
		\intertext{and}
		\Diam(T,\rho) 
		\lesssim d^{\frac{1}{2}-\frac{1}{2\vee p}} K R^{\frac{1}{2}\wedge \frac{1}{p}},
	\end{gather*}
	where $W = \diag\{w_1,\dots,w_n\}$ and 
	\[
	K = \begin{cases}
		\sqrt{\alpha dF/n} & \text{for $1\leq p\leq 2$}; \\
		(\alpha^{p-1} d F^{p-1} / n)^{1/p} & \text{for $p > 2$}.
	\end{cases}
	\]
\end{lemma}
\begin{proof}
	As in the proof of \Cref{lem:bad_terms}, we let $u_x = f(Ax)$ and $\lambda_i = \Lambda_{ii}$ to simplify the notation. We further define
	semi-norms $\norminfI{u} := \max_{i\in I} |u_i|$ and $\normpI{u} = (\sum_{i\in I} |u_i|^p)^{1/p}$.
	
	For $i\in I$ and $x,x'\in T$, we have
	\begin{align*}
		\abs{(z_x)_i - (z_{x'})_i}
		& \leq
		\abs*{ \abs{\lambda_i(u_x-v)_i}^p - \abs{\lambda_i(u_{x'}-v)_i}^p }\\
		& \leq
		p\abs{\lambda_i(u_x-u_{x'})_i}\cdot \left(\abs{\lambda_i (u_x-v)_i}^{p-1} + \abs{\lambda_i (u_{x'}-v)_i}^{p-1}\right) 
	\end{align*}
	where the first inequality is due to the definition in \Cref{eq:z_def} and the second to the fact that $\abs{\abs{a}^p - \abs{b}^p} \leq p\abs{a-b}\cdot \abs{\abs{a}^{p-1} + \abs{b}^{p-1}}$.
	It follows that
	\begin{align*}
		\rho(x,x')^2 &\leq \sum_{i\in I} p^2\abs{\lambda_i(u_{x} - u_{x'})_i}^2\left(\abs{\lambda_i(u_x-v)_i}^{p-1} + \abs{\lambda_i(u_{x'}-v)_i}^{p-1}\right)^2 \\
		&\lesssim \sum_{i\in I} \abs{\lambda_i(u_{x} - u_{x'})_i}^2 \left(\abs{\lambda_i(u_x-v)_i}^{2p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{2p-2}\right). \numberthis\label{eq:rho_first_bound}
	\end{align*}
	
	When $1\leq p\leq 2$, we further bound \Cref{eq:rho_first_bound} by 
	\begin{align*}
		& \sum_{i\in I} \abs{\lambda_i(u_{x} - u_{x'})_i}^2 \left(\abs{\lambda_i(u_x-v)_i}^{2p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{2p-2}\right) \\
		\leq\, & 
		\max_{i\in I}\left\{\abs{\lambda_i(u_{x} - u_{x'})_i}^p\right\} \cdot \sum_{i\in I} \abs{\lambda_i(u_{x} - u_{x'})_i}^{2-p} \left(\abs{\lambda_i(u_x-v)_i}^{2p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{2p-2}\right).
	\end{align*}
	We can then proceed as
	\begin{align*}
		& \sum_{i\in I} \abs{\lambda_i(u_{x} - u_{x'})_i}^{2-p} \left(\abs{\lambda_i(u_x-v)_i}^{2p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{2p-2}\right) \\
		\lesssim\,& 
		\sum_{i\in I} \lambda_i^p \abs{(u_{x} - u_{x'})_i}^{2-p}  \max\{\abs{(u_x-v)_i}^{2p-2}, \abs{(u_{x'}-v)_i}^{2p-2}\}\\
		\lesssim\,& 
		\left(\sum_{i\in I} \lambda_i^p \abs{(u_{x} - u_{x'})_i}^{p} \right)^{\frac{2-p}{p}} \left( \sum_{i\in I} \lambda_i^p \max\{\abs{(u_x-v)_i}^{p}, \abs{(u_{x'}-v)_i}^{p}\} \right)^{\frac{2p-2}{p}} \\
		\leq\,& 
		\normpI{\Lambda(u_x-u_{x'})}^{2-p} \left(\normpI{\Lambda(u_x-v)}^p + \normpI{\Lambda( u_{x'}-v)}^p\right)^{\frac{2p-2}{p}}. 
	\end{align*}
	For the $\ell_p$-norms in the preceding line, we remind the readers that they have been restricted to the indices in $I$ and do not refer to the $\ell_p$-norm of the entire vector.
	
	Since $u_x-v = (u_{\bar{x}} - v) + (u_x - u_{\bar{x}})$, by our assumptions,
	\begin{align*}
		\normpI{\Lambda(u_x-v)}^p &\leq 2^{p-1}(\normpI{\Lambda(u_{\bar{x}}-v)}^p + \normpI{\Lambda(u_x - u_{\bar{x}})}^p ) \\
		&\lesssim \alpha\cdot \normp{\Lambda(u_{\bar{x}}-v)}^p + \alpha \cdot F \\
		&\lesssim \alpha\cdot F. \numberthis\label{eq:normi_bound}
	\end{align*}
	Similarly, 
	\[
	\normpI{\Lambda(u_x-u_{x'})} \leq \normpI{\Lambda(u_x-v)} + \normpI{\Lambda(u_{x'}-v)} \lesssim (\alpha F)^{\frac{1}{p}}.
	\]
	It follows that
	\begin{equation} \label{eqn:p<=2_intermediate_rho}
		\rho(x, x')^2 \lesssim \norminfI{\Lambda(u_x-u_{x'})}^p (\alpha F)^{\frac{2-p}{p}}  (\alpha F)^{\frac{2p-2}{p}} \leq \alpha F\cdot \norminfI{\Lambda(u_x-u_{x'})}^p.
	\end{equation}
	
	When $p>2$, we use the fact that $\abs{z_i}^{2p-2} \leq \abs{z_i}^{p} \norm{z}_\infty^{p-2}$ for a vector $z$ and so we can proceed from \Cref{eq:rho_first_bound} as
	\begin{align*}
		& \sum_{i\in I} \abs{\lambda_i(u_{x} - u_{x'})_i}^{2} \left(\abs{\lambda_i(u_x-v)_i}^{2p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{2p-2}\right) \\
		\leq\,& 
		\sum_{i\in I} \norminfI{\Lambda(u_x-u_{x'})}^2  (\abs{\lambda_i (u_x-v)_i}^{p}\norminfI{\Lambda(u_x-v)}^{p-2} + \abs{\lambda_i(u_{x'}-v)_i}^{p}\norminfI{\Lambda (u_{x'}-v)}^{p-2})\\
		=\,& 		
		\norm{\Lambda(u_x-u_{x'})}_{I,\infty}^2 \left(\normpI{\Lambda(u_x-v)}^p\norminfI{\Lambda(u_x-v)}^{p-2} + \normpI{\Lambda(u_{x'}-v)}^p\norminfI{\Lambda(u_{x'}-v)}^{p-2}\right) \\
		\lesssim\, & 
		\alpha F \cdot \norm{\Lambda(u_x-u_{x'})}_{I,\infty}^2 \left(\norminfI{\Lambda(u_x-v)}^{p-2} + \norminfI{\Lambda(u_{x'}-v)}^{p-2}\right)  \quad \text{by \Cref{eq:normi_bound}}\\
		\leq\, & 
		\alpha F \cdot \norm{\Lambda(u_x-u_{x'})}_{I,\infty}^2 \left(\normpI{\Lambda(u_x-v)}^{p-2} + \normpI{\Lambda(u_{x'}-v)}^{p-2}\right).
	\end{align*}
	Since
	\begin{align*}
		\normpI{\Lambda(u_x - v)}
		\leq \normpI{\Lambda(u_x - u_{\bar{x}})} + \normpI{\Lambda(u_{\bar{x}} - v)} \lesssim
		(\alpha F)^{\frac{1}{p}}
	\end{align*}
	we have
	\begin{equation}\label{eqn:p>2_intermediate_rho}
		\rho(x,x')^2 \lesssim (\alpha F)^{2-\frac{2}{p}} \cdot \norminfI{\Lambda(u_x-u_{x'})}^2.
	\end{equation}
	
	Combining \Cref{eqn:p<=2_intermediate_rho} and \Cref{eqn:p>2_intermediate_rho} yields
	\begin{equation}\label{eqn:intermediate_rho}
		\rho(x,x') \lesssim \left(\alpha F\right)^{(1-\frac{1}{p})\vee \frac{1}{2}}\norminfI{\Lambda(u_x-u_{x'})}^{\frac{p}{2}\wedge 1}
	\end{equation}
	and our next task to upper bound $\norminfI{\Lambda(u_x-u_{x'})}$.
	
	Recall that $w_i\leq 2d/n$, we have by the Lipschitz condition and \Cref{lem:lewis_weight_properties}(d),
	\begin{equation}\label{eqn:coordinate_bound}
		\begin{aligned}
			\abs{\lambda_i(u_x-u_{x'})_i} \leq \abs{(\Lambda(Ax - Ax'))_i} &\leq w_i^{\frac{1}{p}} d^{\frac{1}{2}-\frac{1}{2\vee p}} \norm{\Lambda A(x-x')}_p \\
			&\lesssim \left(\frac{d}{n}\right)^{\frac{1}{p}} d^{\frac{1}{2}-\frac{1}{2\vee p}}\norm{\Lambda A(x-x')}_p.
		\end{aligned}
	\end{equation}
	Plugging this result into \Cref{eqn:intermediate_rho} immediately leads to
	\begin{equation}\label{eq:rho_bound_1}
		\rho(x,x') \lesssim K\cdot d^{\frac{1}{2}-\frac{1}{2\vee p}} \norm*{\Lambda A(x-x')}_{p}^{\frac{p}{2}\wedge 1}.
	\end{equation}
	Alternatively,
	\begin{align*}
		\abs{\lambda_i(u_x-u_{x'})_i} 
		&\lesssim \left(\frac{d}{n}\right)^{\frac{1}{p}} \cdot \frac{\abs{\Lambda_{ii}(Ax - Ax')_i}}{w_i^{\frac{1}{p}}} \\
		&\lesssim \left(\frac{d}{n}\right)^{\frac{1}{p}} \left(w_i\big(\frac{\abs{\Lambda_{ii}(Ax - Ax')_i}}{w_i^{\frac{1}{p}}}\big)^q\right)^{\frac{1}{q}} \quad \text{since $w_i>\frac{ \eps^p d}{n^2}$} \\
		&= \left(\frac{d}{n}\right)^{\frac{1}{p}} \norm*{W^{-\frac{1}{p}}\Lambda A(x-x')}_{w,q} \quad \text{recall that $\norm*{x}_{w,p} = (\sum_{i=1}^n w_i |x_i|^q)^{1/q}$}
	\end{align*}
	and thus
	\begin{equation}\label{eq:rho_bound_2}
		\rho(x,x') \lesssim K\cdot \norm*{W^{-\frac{1}{p}}\Lambda A(x-x')}_{w,q}^{\frac{p}{2}\wedge 1}.
	\end{equation}
	as claimed. 
	
	Finally, we bound $\Diam(T,\rho)$. By the definition of $T$ and \Cref{eq:rho_bound_1}, we have that $\normp{\Lambda A(x-x')}^p \lesssim R$.	
	The claimed upper bound on $\Diam(T,\rho)$ follows immediately.
\end{proof}

\subsubsection{Bounding Dudley's Integral}\label{sec:dudley_bound}

In this section, we will prove \Cref{eq:dudley_bound}, i.e.
\begin{align*}
	\int_{0}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr
	& \lesssim
	\alpha \cdot \Gamma \cdot \poly\log n,
\end{align*}
where $\Gamma$ is as defined in \Cref{eq:Gamma}. To further simplify the notation, let 
\[
\phi = \frac{p}{2}\wedge 1, \quad \beta = \frac{1}{p}\wedge \frac{1}{2}, \quad \gamma = \frac{1}{2} - \frac{1}{p\vee 2},\quad  \theta = (1-\frac{1}{p}) \vee \frac{1}{2}.
\]
Note that 
\begin{align*}
	\log \mathcal{N}(T,\rho,r)
	& \leq
	\log \mathcal{N}(R^{\frac{1}{p}}\cdot B_p(\Lambda A), Kd^\gamma\norm{W^{-\frac{1}{p}}\Lambda A(\cdot)}_{w,p}^{\phi}, r) & \text{by \Cref{lem:metric_transform_combined}} \\
	& =
	\log \mathcal{N}(B_p(\Lambda A), \norm{W^{-\frac{1}{p}}\Lambda A(\cdot)}_{w,p}, \frac{1}{R^{\frac{1}{p}}}(\frac{r}{K d^\gamma})^{\frac{1}{\phi}})  \\
	& =
	\log \mathcal{N}(B_{w,p}(E), \norm{\cdot}_{w,p}, \frac{1}{R^{\frac{1}{p}} d^\gamma}(\frac{r}{K})^{\frac{1}{\phi}}) & \text{since $\gamma/\phi = \gamma$}
\end{align*}
where $E=\colspace(W^{-\frac{1}{p}}\Lambda A)$ and is endowed with norms $\norm{\cdot}_{w,p}$ for $p\geq 1$.

Now, we shall split the integral domain into two parts $[0,\lambda]$ and $[\lambda, \Diam(T,\rho)]$ for some $\lambda\leq \frac{1}{2}KR^{\beta} d^\gamma$ to be determined later (note that $\phi = p\beta$). Note that when $r\leq \lambda$, we have $(r/K)^{1/\phi}/(R^{\frac{1}{p}}d^\gamma) \leq (\frac{1}{2})^\phi < 1$.

By \Cref{lem:entropy_estimates_consolidated} Case 1, we have (letting $\lambda' = \lambda/(KR^{\beta} d^\gamma)$)
\begin{align*}
	\int_{0}^\lambda \sqrt{\log \mathcal{N}(T,\rho,r)} dr 
	& \lesssim
	\int_{0}^{\lambda} \sqrt{\log \mathcal{N}(B_{w,p}(E), \norm*{\cdot}_{w,p},\frac{1}{R^{\frac{1}{p}}d^\gamma}(\frac{r}{K})^{\frac{1}{\phi}})} dr \\
	& =
	\int_{0}^{\lambda'} \sqrt{\log \mathcal{N}(B_{w,p}(E), \norm*{\cdot}_{w,p}, s^{\frac{1}{\phi}})} K d^\gamma R^{\beta} ds \\
	& \lesssim
	\int_{0}^{\lambda'} \sqrt{d\log\frac{1}{s}} K d^\gamma R^{\beta} ds \\
	& \lesssim
	\sqrt{d} \cdot K d^\gamma R^{\beta} \int_0^{\lambda'} \log\frac{1}{s} ds \\
	& \lesssim
	\sqrt{d} \cdot K d^\gamma R^{\beta} \cdot \lambda' \log\frac{1}{\lambda'} \\
	& \lesssim 
	\lambda \sqrt{d} \log \frac{d^\gamma R^{\beta} K}{\lambda}. \numberthis\label{eq:dudley_small_r}
\end{align*}
To handle the integral over $[\lambda, \Diam(T,\rho)]$, we bound 
\begin{align*}
	\log \mathcal{N}(T,\rho,r)
	& \leq
	\log \mathcal{N}(R^{\frac{1}{p}}\cdot B_p(\Lambda A), K\norm{W^{-\frac{1}{p}}\Lambda A(\cdot)}_{w,q}^{\phi}, r) & \text{by \Cref{lem:metric_transform_combined}} \\
	& =
	\log \mathcal{N}(B_p(\Lambda A), \norm{W^{-\frac{1}{p}}\Lambda A(\cdot)}_{w,q}, \frac{1}{R^{\frac{1}{p}}}(\frac{r}{K })^{\frac{1}{\phi}})  \\
	& =
	\log \mathcal{N}(B_{w,p}(E), \norm{\cdot}_{w,q}, \frac{1}{R^{\frac{1}{p}}}(\frac{r}{K})^{\frac{1}{\phi}})
\end{align*}
where $E=\colspace(W^{-\frac{1}{p}}\Lambda A)$ and is endowed with norms $\norm{\cdot}_{w,q}$ for $q\geq 1$.
We further divide the estimates into two cases. 
For $p\in [1,2]$, we invoke \Cref{lem:entropy_estimates_consolidated} Case 2 and obtain that
\begin{align*}
	\int_{\lambda}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr 
	& \lesssim
	\int_{\lambda}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(B_{w,p}(E), \norm*{\cdot}_{w,q},\frac{1}{R^{\frac{1}{p}}}(\frac{r}{K})^{\frac{2}{p}})} dr \\
	& \lesssim
	R^{\frac{1}{2}}K\sqrt{\log(\frac{n}{\eps d})\sqrt{\log d}}
	\int_{\lambda}^{\Diam(T,\rho)} \frac{1}{r} dr \\
	& \lesssim
	R^{\frac{1}{2}}K\sqrt{\log \frac{n}{\eps d} \sqrt{\log d}}\log  \frac{\Diam(T,\rho)}{\lambda} . \numberthis\label{eq:dudley_large_r_small_p}
\end{align*}
For $p > 2$, we invoke \Cref{lem:entropy_estimates_consolidated} Case 3 and obtain that
\begin{align*}
	\int_{\lambda}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr
	\lesssim\; &
	\int_{\lambda}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(B_{w,p}(E), \norm*{\cdot}_{w,q},\frac{r}{R^{\frac{1}{p}}\cdot K})} dr \\
	\lesssim\; &
	d^{\frac{1}{2}-\frac{1}{p}}\sqrt{\log \frac{n}{\eps d} }R^{\frac{1}{p}}K\int_{\lambda}^{\Diam(T,\rho)} \frac{1}{r} dr \\
	\lesssim\; &
	d^{\frac{1}{2}-\frac{1}{p}}\sqrt{\log \frac{n}{\eps d} }R^{\frac{1}{p}}K \log \frac{\Diam(T,\rho)}{\lambda}. \numberthis\label{eq:dudley_large_r_large_p}
\end{align*}
Combining \Cref{eq:dudley_large_r_small_p} and \Cref{eq:dudley_large_r_large_p} yields
\begin{equation}\label{eq:dudley_large_r}    
	\int_{\lambda}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr 
	\lesssim d^{\gamma} R^{\beta} K \sqrt{\log \frac{n}{\eps d} \sqrt{\log d}}\log  \frac{\Diam(T,\rho)}{\lambda}.
\end{equation}
Recall that by Lemma \ref{lem:metric_transform_combined},
\begin{align*}
	\Diam(T,\rho)
	\lesssim
	d^{\gamma}KR^{\beta}, \quad \text{where}\quad  K = (\alpha F)^{\theta}\left(\frac{d}{n}\right)^{\beta}.
\end{align*}
Combining \Cref{eq:dudley_small_r} and \Cref{eq:dudley_large_r} and taking $\lambda = (d^{\gamma-\frac{1}{2}}R^{\beta} K) \wedge (\frac{1}{2}d^\gamma R^{\beta} K)$, we have
\begin{align*}
	\int_{0}^{\Diam(T,\rho)} \sqrt{\log \mathcal{N}(T,\rho,r)} dr
	&\lesssim \lambda\sqrt{d} \log\frac{d^\gamma R^{\beta} K}{\lambda} + d^\gamma R^{\beta} K\sqrt{\log\frac{n}{\eps d}}\log\frac{d^\gamma KR^{\beta}}{\lambda} \\
	&\lesssim d^{\gamma} R^{\beta} K \log d\sqrt{\log\frac{n}{\eps d}\sqrt{\log d}} \\
	&= \alpha \cdot \Gamma \cdot \log^{\frac54} d\sqrt{\log\frac{n}{\eps d}}.
\end{align*}


\subsection{Removing the Dependence on $n$}\label{sec:log n factors}

In this section, we reduce the $\log n$ factors in \Cref{thm:main_upper_weaker} to $\log(d/\eps)$ factors, thereby proving \Cref{thm:main_upper}. This is achieved by first applying a sampling matrix $S^\circ$ to reduce the dimension of the regression problem from $n$ to $\poly(d/\eps)$ before invoking \Cref{alg:main_general}; see \Cref{alg:main_general_no_n} for the full algorithm. The sampling matrix $S^\circ$ uses a larger sampling rate $\alpha^\circ$, which allows for controlling the error in Lemma~\ref{lem:main_concen} via Bernstein's inequality with a simple net argument instead of the chaining argument or Dudley's integral and thus avoiding the $\log n$ factor from entropy estimates.







% \note{more explanation}
% \note{explain that we have $\Lambda$.}
Recall that, in \Cref{sec:generalized}, we introduce the matrix $\Lambda$ to ensure that the Lewis weights are bounded uniformly.
We will include the matrix $\Lambda$ in our proof and abuse the notations by dropping the prime mark as indicated in \Cref{sec:generalized}.


The following is a weaker version of \Cref{lem:main_concen} for reducing $n$ to $\poly(d/\eps)$.
\begin{lemma}\label{lem:main_concen_weak}
Let $A\in \R^{n\times d}$, $v\in\R^n$, $f\in\lip_1$, $\eps > 0$, $\Lambda \in \R^{n\times n}$, $\alpha\in [0,1]$, $S\in \R^{n\times n}$ and $R\in\R$ be as defined in Lemma~\ref{lem:main_concen} and satisfy the same constraints.
Suppose that $0\leq \alpha \leq 1$ such that $\alpha n \gtrsim (d^{(\frac{p}{2}\vee 1)+1}/\eps^{p+2})\log(1/\eps)$ and $R\geq \normp{\Lambda Ax^*}^p \vee \normp{\Lambda(f(Ax^\ast)-v)}^p$. 
Let $T = \setdef*{x\in\mathbb{R}^d}{\normp{\Lambda Ax}^p\leq R}$.
When conditioned on
\begin{align*}
    \normp{S\Lambda(f(Ax^*) - v)}^p 
    \lesssim
    R,
\end{align*}
it holds with probability at least $0.99$ that
\begin{align*}
    \MoveEqLeft \sup_{x\in T}\abs*{(\normp{S\Lambda (f(Ax)-v)}^p - \normp{S\Lambda (f(Ax^*)-v)}^p) - (\normp{\Lambda (f(Ax)-v)}^p - \normp{\Lambda (f(Ax^*)-v)}^p)} \\
    & \leq  
    C \eps R, \numberthis\label{eq:weak_concen}
\end{align*}
where $C$ is an absolute constant.
\end{lemma}
\begin{proof}
	Recall that the error bound in \Cref{lem:main_concen} consists of three terms. By the same proofs as in \Cref{lem:bad_terms,lem:low_lewis_terms}, the first two terms remain the same, both of which are now bounded by $C\eps R^p$ under our assumptions. The rest of the proof is devoted to deriving a similar bound for the third term. Recall that we need to upper bound 
	\[
	\sup_{x\in T} \abs*{\sum_{i\in J} S_{ii}^p \xi_i (z_x)_i},
	\]
	where $z_x$ is as defined in \Cref{eq:z_def} and $\{\xi_i\}$ are independent Rademacher variables. We shall use a net argument here. 
	
	Fix an $x\in T$. Let $W_i = S_{ii}^p \xi_i (z_x)_i$, then $\E W_i = 0$ and
	\begin{align*}
		\abs{W_i} &\leq \frac{1}{\alpha}\abs{(z_x)_i}  \\
		&= \frac{1}{\alpha} \abs{(\Lambda(f(Ax) - v)_i^p - (\Lambda(f(Ax^*) - v)_i^p} \\
		&\leq \frac{1}{\alpha} \cdot 2^{p-1}(\abs{\Lambda_{ii}(f(Ax) - f(Ax^\ast))_i}^p + \abs{\Lambda_{ii}(f(Ax^\ast) - v)_i}^p ) + \abs{\Lambda_{ii}(f(Ax^*) - v)_i}^p\\
		&\lesssim \frac{1}{\alpha} \left( \abs{\Lambda_{ii}(Ax - Ax^\ast)_i}^p + \abs{\Lambda_{ii}(f(Ax^*) - v)_i}^p\right).
	\end{align*}
	Now we use \Cref{eqn:coordinate_bound} for the first term in the brackets and the definition of $G$ in \Cref{eq:gd_idx_def} for the second term. We proceed as
	\begin{align*}
		|W_i| 
		&\lesssim 
		\frac{1}{\alpha}\left( \frac{d}{n}\cdot d^{(\frac{p}{2}-1)\vee 0} \normp*{\Lambda A(x-x^\ast)}^p
		+ \frac{1}{\eps^p} d^{(\frac{p}{2}-1)\vee 0}\cdot \frac{d}{n}\cdot R \right)
		\lesssim 
		\underbrace{\frac{1}{\alpha n \eps^p} \cdot d^{\frac{p}{2}\vee 1} R}_{:=\Delta}.
	\end{align*}
	Next we bound $\E (\sum_{i\in J} W_i^2)$.
	\[
	\E \sum_{i\in J} W_i^2 = \sum_{i\in J} \E S_{ii}^{2p} (z_x)_i^2 \\
	\leq \sum_i \frac{1}{\alpha} \cdot \max_i \abs{(z_x)_i}\cdot \abs{(z_x)_i} \lesssim \Delta \cdot \sum_{i\in J} \abs{(z_x)_i}.
	\]
	Note that 
	\begin{align*}
		\sum_{i\in J} \abs{(z_x)_i} 
		&\leq \normp{\Lambda(f(Ax) - v)}^p + \normp{\Lambda(f(Ax^\ast) - v)}^p \\
		&\leq 2^{p-1}( \normp{\Lambda(f(Ax) - f(Ax^\ast))}^p + \normp{\Lambda(f(Ax^\ast) - v)}^p ) +  \normp{\Lambda(f(Ax^\ast) - v)}^p\\
		&\leq 2^{p-1} \normp{\Lambda A(x - x^\ast)}^p + (2^{p-1} + 1)\normp{\Lambda(f(Ax^\ast) - v)}^p \\
		&\lesssim \normp{\Lambda Ax}^p + \normp{\Lambda Ax^\ast}^p + \normp{\Lambda(f(Ax^\ast) - v)}^p \\
		&\lesssim R.
	\end{align*}
	Hence
	\[
	\E \sum_{i\in J} W_i^2 \lesssim \Delta\cdot R.
	\]
	It follows from Bernstein's inequality that
	\begin{align*}
		\Pr\left\{ \abs*{\sum_{i\in J} W_i} \geq \eta R \right\} &\leq 2\exp\left(-c\frac{\eta^2 R^{2}}{\Delta R^p + \Delta \cdot \eta R}\right)  \\
		&\leq 2\exp\left(-c'\frac{\eta^2 R}{\Delta}\right) \\
		&\leq \exp\left(-Cd\log\frac{1}{\eta}\right),
	\end{align*}
	provided that
	\[
	\eta^2 R \gtrsim d\cdot \Delta = 
	\begin{cases}
		d^{\frac{p}{2}+1}\log\frac{1}{\eta}\cdot R/(\alpha n \eps^p) & p > 2\\
		d^{2}R\log\frac{1}{\eta}/(\alpha n \eps^p)	& 1\leq p < 2		
	\end{cases}
	\]
	or
	\begin{equation}\label{eq:alpha_condition_weak}
		\alpha n\gtrsim \frac{d^{1+(\frac{p}{2}\vee 1)}}{\eta^2 \eps^p}\log\frac{1}{\eta}.
	\end{equation}
	To summarize, we have shown that when \Cref{eq:alpha_condition_weak}, for each fixed $x\in T$  that with probability at least $1-\delta$ (where $\delta = \exp(-Cd\log(1/\eta))$), it holds
	\[
	\abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x)_i} \leq \eta R.
	\]
	To obtain an upper bound for the supremum over $x\in T$, we employ a standard net argument. Let $N\subseteq T$ be an $(\eta R^{\frac{1}{p}})$-net of $T$ such that $\abs{N}\leq (3/\eta)^d$. By a union bound, we have with probability at least $1-\abs{N}\delta\geq 0.99$ that
	\[
	\sup_{x\in N} \abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x)_i} \leq \eta R.
	\]
	For an $\Lambda A x\in T$, there exists $\Lambda A y\in N$ such that $\norm{\Lambda A(x-y)}_p \leq \eta R^{\frac{1}{p}}$. Thus
	\begin{align*}
		\sup_{x\in T}\abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x)_i} 
		&\leq 
		\sup_{y\in N}\abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_y)_i} +  \sup_{x\in T} \abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x - z_y)_i} \\
		&\leq 
		\eta R +  \sup_{x\in T}\abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x - z_y)_i}.
	\end{align*}
	We bound the error term by H\"older's inequality as 
	\begin{align*}
		&\quad\ \abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x - z_y)_i}  \\
		&=
		\sum_{i\in J}  S_{ii}^p \abs*{(z_x - z_y)_i} \\
		&= \sum_{i\in J} S_{ii}^p \abs*{\abs{\Lambda_{ii}(u_x-v)_i}^p - \abs{\Lambda_{ii}(u_y-v)_i}^p}\\
		&\lesssim 
		\sum_{i\in J} S_{ii}^p \abs{\Lambda_{ii}(u_x - u_y)_i} \left(\abs{\Lambda_{ii}(u_x-v)_i}^{p-1} + \abs{\Lambda_{ii}(u_y-v)_i}^{p-1}\right)\\
		&\lesssim \left(\sum_{i\in J}  S_{ii}^p \abs{\Lambda_{ii}(Ax -  Ay)_i}^p\right)^{\frac{1}{p}} \left(\left(\sum_{i\in J} S_{ii}^p\abs{\Lambda_{ii}(u_x-v)_i}^{p}\right)^{1-\frac{1}{p}} + \left(\sum_{i\in J} S_{ii}^p\abs{\Lambda_{ii}(u_y-v)_i}^{p}\right)^{1-\frac{1}{p}}\right) \\
		&\leq \norm{S\Lambda A(x-y)}_p (\normp{S\Lambda(f(Ax)-v)}^{p-1} + \normp{S\Lambda(f(Ay)-v)}^{p-1}) 
	\end{align*}
	Using the the subspace embedding property of $S$,
	\[
	\norm{S\Lambda A(x-y)}_p \leq 2\norm{\Lambda A(x-y)}_p \leq 2\eta R^{\frac{1}{p}}
	\]
	and
	\begin{align*}
		\normp{S\Lambda(f(Ax)-v)}^{p-1} &\lesssim \normp{S\Lambda(f(Ax)-f(Ax^\ast))}^{p-1} + \normp{S\Lambda(f(Ax^\ast)-v)}^{p-1} \\
		&\leq \normp{S\Lambda A(x - x^\ast)}^{p-1} + \normp{S\Lambda(f(Ax^\ast)-v)}^{p-1} \\
		&\lesssim \normp{\Lambda A(x - x^\ast)}^{p-1} + \normp{S\Lambda(f(Ax^\ast)-v)}^{p-1} \\
		&\lesssim R^{1-\frac{1}{p}}.
	\end{align*}
	Hence, 
	\begin{align*}
		\sup_{x\in T} \abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x - z_y)_i} 
		\lesssim 
		\eta R.
	\end{align*}
	Therefore, 
	\[
	\abs*{\sum_{i\in J} S_{ii}^p \xi_i(z_x)_i} \lesssim \eta R
	\]
	and the claimed result follows from setting $\eta \sim \eps$.
\end{proof}




To prove that the output $\hat{x}$ of \Cref{alg:main_general_no_n} satisfies \Cref{eq:main_obj}, let $x^\circ \in \mathbb{R}^d$ be 
\begin{align*}
	x^\circ
	& :=
	\argmin_{x\in \mathbb{R}^d} \normp{S^\circ \Lambda (f(Ax) -b)}^p + \eps^2 \normp{\Lambda Ax}^p
\end{align*}
where $\Lambda$ is the matrix ensuring the Lewis weights of $\Lambda A$ are uniformly bounded.
Note that we set the regularized parameter to be $\eps^2$ instead of $\eps$.
We highlight that this $x^\circ$ is for the purpose of analysis and we do not actually compute it in the algorithm.
From now on, we set
\begin{align*}
	\alpha \sim \frac{d^{\frac{p}{2}\vee 1}}{m \eps^{p\vee 2}} \cdot \poly \log m
	\quad \text{and}\quad 
	\alpha^\circ \sim \frac{d^{(\frac{p}{2}\vee 1)+1}}{n(\eps^4)^{p+2}}\log(1/\eps)
\end{align*}
where $m$ is the number of nonzero rows in $S^\circ A$ which is the same as the $m$ defined in \Cref{alg:main_general_no_n}.
Note that our choice of $\alpha^\circ$ implies that $m\sim \frac{d^{(\frac{p}{2}\vee 1)+1}}{\eps^{4p+8}}\log(1/\eps) = \poly(d,\frac{1}{\eps})$.
We preview that when we use \Cref{lem:main_concen_weak} we aim for the error of $C\eps^2 R$ in \Cref{eq:weak_concen}.
Combining with the regularized parameter $\eps^2$, 
we set $\eps^4$ in $\alpha^\circ$.
Recall that our goal is to simply reduce $n$ to $\poly(\frac{d}{\eps})$ and thus these choices of the exponents of $\eps$ may not be optimized.
Nonetheless, they are sufficient to achieve our objective.



% We first condition on $S^\circ$.
We begin with using \Cref{lem:main_concen_final} with $\bar{x} = x^\circ$ and $S^\circ \Lambda$ as the matrix ensuring the Lewis weights of $S^\circ \Lambda A$ are uniformly bounded.
We now verify the conditions in \Cref{lem:main_concen_final}.
By \Cref{sec:appendix_lewis}, the Lewis weights of $S^\circ \Lambda A$ are uniformly bounded by $O(d/m)$ with probability at least $0.99$.
Clearly, the output of \Cref{alg:main_general_no_n} satisfies
\begin{align*}
	\hat{x}
	=
	\argmin_{x\in \mathbb{R}^d} \normp{S S^\circ \Lambda (f(Ax) - b)}^p + \eps \normp{S^\circ \Lambda A x}^p
\end{align*}
and, by the optimality of $x^\circ$, we have 
\begin{align*}
	\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p - \normp{S^\circ \Lambda (f(A\hat{x}) - b)}^p
	& \leq
	\eps^2 \normp{\Lambda A \hat{x}}^p \numberthis\label{eq:final_concen_condi}
\end{align*}
We need to upper bound $\normp{\Lambda A \hat{x}}^p$.
By the optimality of $\hat{x}$, we have
\begin{align*}
	\normp{S S^\circ \Lambda (f(A\hat{x}) - b)}^p + \eps \normp{S^\circ \Lambda A \hat{x}}^p
	& \leq
	\normp{S S^\circ \Lambda (f(Ax^\circ) - b)}^p + \eps \normp{S^\circ \Lambda A x^\circ}^p
\end{align*}
which implies
\begin{align*}
	\normp{S^\circ\Lambda A \hat{x}}^p
	& \leq
	\frac{1}{\eps} \normp{SS^\circ \Lambda (f(Ax^\circ) - b)}^p + \normp{S^\circ \Lambda A x^\circ}^p. \numberthis\label{eq:circ_x_hat_1}
\end{align*}
Since $S^\circ$ is an $\ell_p$-subspace embedding matrix for $\Lambda A$ with constant distortion with probability $0.99$ because of our choice of $\alpha^\circ$ and we condition on it from now on, we have
\begin{align*}
	\frac{1}{2}\normp{\Lambda A \hat{x}}^p
	& \leq
	\normp{S^\circ\Lambda A \hat{x}}^p. \numberthis\label{eq:circ_sub_embed}
\end{align*}
Also, by Markov inequality, with probability at least $0.99$, we have
\begin{align*}
	\normp{SS^\circ \Lambda (f(Ax^\circ) - b)}^p
	& \leq
	100\normp{ S^\circ \Lambda (f(Ax^\circ) - b)}^p. \numberthis\label{eq:ss_circ_markov}
\end{align*}
Plugging \Cref{eq:circ_sub_embed} and \Cref{eq:ss_circ_markov} into \Cref{eq:circ_x_hat_1}, we have
\begin{align*}
	\normp{\Lambda A\hat{x}}^p
	& \lesssim
	\frac{1}{\eps}\normp{ S^\circ \Lambda (f(Ax^\circ) - b)}^p + \normp{S^\circ\Lambda A x^\circ}^p.
\end{align*}
and when we further plug this into \Cref{eq:final_concen_condi} we have
\begin{align*}
	\MoveEqLeft \normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p - \normp{S^\circ \Lambda (f(A\hat{x}) - b)}^p \\
	& \lesssim
	\eps^2 \cdot  (\frac{1}{\eps}\normp{ S^\circ \Lambda (f(Ax^\circ) - b)}^p + \normp{S^\circ\Lambda A x^\circ}^p) \\
	& =
	\eps \cdot (\normp{ S^\circ \Lambda (f(Ax^\circ) - b)}^p + \eps \normp{S^\circ\Lambda A x^\circ}^p)
\end{align*}
which completes the condition verification for \Cref{lem:main_concen_final}.

By \Cref{lem:main_concen_final}, with probability $0.99$, we have
\begin{align*}
	\MoveEqLeft \abs{(\normp{SS^\circ \Lambda(f(A\hat{x}) - b)}^p - \normp{SS^\circ \Lambda(f(Ax^\circ) - b)}^p) - (\normp{S^\circ \Lambda(f(A\hat{x}) - b)}^p - \normp{S^\circ \Lambda(f(Ax^\circ) - b)}^p)} \\
	& \leq
	\eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p).
\end{align*}
By rearranging the terms, we have
\begin{align*}
	\MoveEqLeft \normp{S^\circ \Lambda(f(A\hat{x}) - b)}^p - \normp{S^\circ \Lambda(f(Ax^\circ) - b)}^p \\
	& \leq
	\normp{SS^\circ \Lambda(f(A\hat{x}) - b)}^p - \normp{SS^\circ \Lambda(f(Ax^\circ) - b)}^p + \eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p) \\
	& \leq
	\eps\cdot \normp{S^\circ \Lambda Ax^\circ}^p + \eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p) \quad \text{by the optimality of $\hat{x}$}\\
	& \lesssim
	\eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p) \numberthis\label{eq:circ_concen_1}
\end{align*}

Now, we would like to use \Cref{lem:main_concen_weak} with $R \sim \frac{1}{\eps}\opt + \normp{\Lambda A x^*}^p$ for $x=\hat{x}$ and hence we need to verify $\hat{x}\in T$.
By the optimality of $\hat{x}$, we have
\begin{align*}
	\normp{S S^\circ \Lambda (f(A\hat{x}) - b)}^p + \eps \normp{S^\circ \Lambda A \hat{x}}^p
	& \leq
	\normp{S S^\circ \Lambda (f(Ax^*) - b)}^p + \eps \normp{S^\circ \Lambda A x^*}^p
\end{align*}
which implies
\begin{align*}
	\normp{S^\circ\Lambda A \hat{x}}^p
	& \leq
	\frac{1}{\eps} \normp{SS^\circ \Lambda (f(Ax^*) - b)}^p + \normp{S^\circ \Lambda A x^*}^p. \numberthis\label{eq:circ_x_hat_ast}
\end{align*}
Recall that we condition that $S^\circ$ is an $\ell_p$-subspace embedding matrix for $\Lambda A$ with constant distortion.
We have
\begin{align*}
	\frac{1}{2}\normp{\Lambda A \hat{x}}^p
	& \leq
	\normp{S^\circ\Lambda A \hat{x}}^p
	\quad \text{and} \quad
	\normp{S^\circ\Lambda A x^*}^p
	\leq
	2\normp{\Lambda A x^*}^p.\numberthis\label{eq:circ_sub_embed_ast}
\end{align*}
Also, by Markov inequality, with probability $0.99$, we have
\begin{align*}
	\normp{SS^\circ \Lambda (f(Ax^*) - b)}^p
	\leq
	100\normp{\Lambda (f(Ax^*) - b)}^p
	=
	100\opt. \numberthis\label{eq:ss_circ_markov_ast}
\end{align*}
Plugging \Cref{eq:circ_sub_embed_ast} and \Cref{eq:ss_circ_markov_ast} into \Cref{eq:circ_sub_embed_ast}, we have
\begin{align*}
	\normp{\Lambda A \hat{x}}^p
	& \lesssim
	\frac{1}{\eps} \opt + \normp{\Lambda A x^*}^p
	\quad \text{which implies $\hat{x}\in T$.}
\end{align*}
By \Cref{lem:main_concen_weak} with our choice of $\alpha^\circ$, with probability $0.99$, we have
\begin{align*}
	\MoveEqLeft \abs*{(\normp{S^\circ\Lambda (f(A\hat{x})-b)}^p - \normp{S^\circ\Lambda (f(Ax^*)-b)}^p) - (\normp{\Lambda (f(A\hat{x})-b)}^p - \normp{\Lambda (f(Ax^*)-b)}^p)} \\
	& \lesssim 
	\eps^4 \cdot (\frac{1}{\eps} \opt + \normp{\Lambda A x^*}^p) \\
	& \lesssim
	\eps \cdot  (\opt + \normp{\Lambda A x^*}^p). \numberthis\label{eq:s_circ_concen}
\end{align*}
By the optimality of $x^\circ$, we have
\begin{align*}
    -\normp{S^\circ\Lambda(f(Ax^*) - b)}^p
    & \leq
    -\normp{S^\circ\Lambda(f(Ax^\circ) - b)}^p + \eps^2\normp{\Lambda Ax^*}^p
\end{align*}
which implies
\begin{align*}
    \MoveEqLeft \normp{S^\circ \Lambda(f(A\hat{x}) - b)}^p - \normp{S^\circ \Lambda(f(Ax^*) - b)}^p \\
    & \leq
    \normp{S^\circ \Lambda(f(A\hat{x}) - b)}^p -\normp{S^\circ\Lambda(f(Ax^\circ) - b)}^p + \eps^2\normp{\Lambda Ax^*}^p \\
    & \lesssim
    \eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p) + \eps^2\normp{\Lambda Ax^*}^p\quad  \text{by \Cref{eq:circ_concen_1}.}
\end{align*}


By rearranging the terms in \Cref{eq:s_circ_concen}, we have
\begin{align*}
    \MoveEqLeft\normp{\Lambda(f(A\hat{x}) - b)}^p - \normp{ \Lambda(f(Ax^*) - b)}^p \\
    & \lesssim
    \eps \cdot (\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p + \norm{S^\circ \Lambda A x^\circ}^p + \opt + \norm{\Lambda A x^*}^p) \numberthis\label{eq:circ_conen_final}
\end{align*}
It means that we need to upper bound the terms $\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p$ and $\norm{S^\circ \Lambda A x^\circ}^p$.
For $\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p$, we have
\begin{align*}
	\normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p
	& \leq
	\normp{S^\circ \Lambda (f(Ax^*) - b)}^p + \eps^2\normp{\Lambda A x^*}^p \quad \text{by the optimality of $x^\circ$} \\
	& \leq
	100\opt + \eps^2 \normp{\Lambda A x^*}^p \numberthis\label{eq:final_concen_check_1},
\end{align*}
where the last inequality holds with probability $0.99$ by Markov inequality.
For $\norm{S^\circ \Lambda A x^\circ}^p$, we have
\begin{align*}
	\norm{S^\circ \Lambda A x^\circ}^p
	& \lesssim
	\norm{\Lambda A x^\circ}^p \quad \text{recall that $S^\circ$ is an $\ell_p$ subspace embedding.}
\end{align*}
To further bound the term $\norm{\Lambda A x^\circ}^p$, we have
\begin{align*}
	\normp{\Lambda Ax^\circ}^p 
	& \leq
	\frac{1}{\eps^2}\normp{S^\circ \Lambda (f(Ax^*) - b)}^p  + \normp{\Lambda Ax^*}^p  \quad \text{by the optimality of $x^\circ$} \\
	& \leq
	\frac{100}{\eps^2}\opt + \normp{\Lambda Ax^*}^p \quad \text{by Markov inequality with probability $0.99$.}
\end{align*}
Note that this bound is not enough to finish our proof.
However, it implies that $x^\circ\in T$ where $T$ is the set defined in \Cref{lem:main_concen_weak} with $R = \frac{100}{\eps^2}\opt + \normp{\Lambda Ax^*}^p$.
By \Cref{lem:main_concen_weak} with our choice of $\alpha^\circ$, with probability $0.99$, we have
\begin{align*}
	\MoveEqLeft \abs*{(\normp{S^\circ\Lambda (f(Ax^\circ)-v)}^p - \normp{S^\circ\Lambda (f(Ax^*)-v)}^p) - (\normp{\Lambda (f(Ax^\circ)-v)}^p - \normp{\Lambda (f(Ax^*)-v)}^p)} \\
	& \lesssim
	\eps^4 \cdot (\frac{1000}{\eps^2}\opt + \normp{\Lambda A x^*}^p) \\
	& \lesssim
	\eps^2 \cdot (\opt + \normp{\Lambda A x^*}^p).
\end{align*}
Then, we have
\begin{align*}
	\normp{\Lambda Ax^\circ}^p 
	& \leq
	\frac{1}{\eps^2}\bigg(\normp{S^\circ \Lambda (f(Ax^*) - b)}^p - \normp{S^\circ \Lambda (f(Ax^\circ) - b)}^p \bigg) + \normp{\Lambda Ax^*}^p  \quad \text{by the optimality of $x^\circ$} \\
	& \lesssim
	\frac{1}{\eps^2}\bigg(\normp{\Lambda (f(Ax^\circ)-v)}^p - \normp{\Lambda (f(Ax^*)-v)}^p + \eps^2 \cdot (\opt + \normp{\Lambda A x^*}^p) \bigg) + \normp{\Lambda Ax^*}^p  \\
	& \lesssim
	\opt+\normp{\Lambda A x^*}^p \quad \text{by the optimality of $x^*$.} \numberthis\label{eq:final_concen_check_2}
\end{align*}
Plugging \Cref{eq:final_concen_check_1} and \Cref{eq:final_concen_check_2} into \Cref{eq:circ_conen_final}, we have
\begin{align*}
	\normp{\Lambda(f(A\hat{x}) - b)}^p - \opt
	& \lesssim
	\eps \cdot (\opt + \norm{\Lambda A x^*}^p).
\end{align*}
This completes the proof for the query complexity without dependence on $n$. 
The overall failure probability is at most $0.1$ in the above argument.






\section{Lower Bound}\label{sec:lower}


\subsection{Case of $p\in [1,2]$}
By Yao's minimax theorem, it suffices to show the following theorem. 
\begin{theorem}\label{thm:p<=2_deterministic}
	Suppose that $p\geq 1$, $\eps > 0$ is sufficiently small and $n\gtrsim_p (d\log d)/\eps^2$. 
	There exist a deterministic function  $f\in \lip_1$, a deterministic matrix $A\in\R^{n\times d}$ and a distribution over $b\in\R^n$ such that the following holds: every deterministic algorithm that outputs $\hat{x}\in\R^d$ which with probability at least $4/5$ over the randomness of $b$ satisfies \Cref{eq:main_obj} must make $\Omega\big(d/\eps^2\big)$ queries to the entries of $b$.
\end{theorem}
We remark that the lower bound holds for all $p\geq 1$, and is tight up to logarithmic factors for $p\in [1,2]$.
To prove \Cref{thm:p<=2_deterministic}, we reduce \Cref{prob:lb_p<=2} below to our problem.





\begin{problem}\label{prob:lb_p<=2}
	Suppose that $0<\eps<1$, $d$ is a positive integer, $m \sim (\log d)/\eps^2$ and $n=2md$.
	Let 
	\begin{align*}
		u
		& =
		\begin{bmatrix}
			3\\
			2
		\end{bmatrix}
		\quad \text{and} \quad 
		v
		=\begin{bmatrix}
			2\\
			3
		\end{bmatrix}.
	\end{align*}
	Let $D_0$ be the distribution on the $(2m)$-dimensional vector $b^\prime$ such that, for $i=1,\dots,m$,
	\begin{align*}
		\begin{bmatrix}
			b^\prime_{2i-1}\\
			b^\prime_{2i}
		\end{bmatrix}
		& =
		\begin{cases}
			u & \text{with probability $\frac{1}{2}+\eps$} \\
			v & \text{with probability $\frac{1}{2}-\eps$}
		\end{cases}
	\end{align*}
	and $D_1$ be the distribution on the $(2m)$-dimensional vector $b^\prime$ such that, for $i=1,\dots,m$,
	\begin{align*}
		\begin{bmatrix}
			b^\prime_{2i-1}\\
			b^\prime_{2i}
		\end{bmatrix}
		& =
		\begin{cases}
			u & \text{with probability $\frac{1}{2}-\eps$} \\
			v & \text{with probability $\frac{1}{2}+\eps$.}
		\end{cases}
	\end{align*}
	Let $b$ be the $n$-dimensional random vector formed by  concatenating $d$ i.i.d.\ random vectors $b^{(1)},\dots,b^{(d)}$, where each $b^{(i)}$ is drawn from $D_0$ with probability $1/2$ and from $D_1$ with probability $1/2$.
	
	Given a query access to the entries of $b$, we would like to, with probability at least $2/3$, correctly identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$.
\end{problem}

By \Cref{lem:lb_ber} and \Cref{lem:distinguishing_dist}, any deterministic algorithm that solves this problem requires $\Omega\big(d/\eps^2\big)$ queries to the entries of $b$.

Now, we construct the reduction.
Let $f$ be the function
\begin{align*}
	f(x)
	& =
	\begin{cases}
		2 & \text{if $x\leq -6$} \\
		-x-4 & \text{if $-6 \leq x \leq -4 $} \\
		0 & \text{if $-4 \leq x \leq 0$} \\
		\frac{1}{2}x & \text{if $0\leq x$} \\
	\end{cases}
\end{align*}
Let $a$ be the $(2m)$-dimensional vector such that
\begin{align*}
	\begin{bmatrix}
		a_{2i-1}\\
		a_{2i}
	\end{bmatrix}
	& =
	\begin{bmatrix}
		1 \\
		-1
	\end{bmatrix}
	\quad \text{for $i=1,\dots,m$}
\end{align*}
and $A$ be the $n\times d$ block-diagonal matrix whose diagonal blocks are the same $a\in \R^{2m\times 1}$. 

Given a deterministic algorithm $\mathcal{A}$ that takes $f,A,\eps$ and a query access to the entries of $b$ as inputs and returns $\hat{x}\in \mathbb{R}^d$ satisfying \Cref{eq:main_obj}, we claim that $\hat{x}$ can be used to solve \Cref{prob:lb_p<=2}.
This claim is proved in \Cref{lem:p<=2_main_claim}.

\begin{lemma}\label{lem:p<=2_main_claim}
	Let $f$, $A$, $b$ be as specified above. There exists $K = K(p)$, a constant only depending on $p$ such that given an $\hat x\in \R^d$ satisfying
	\[
	\normp{f(A\hat{x}) - b}^p
	\leq
	(1+\frac{\eps}{K})\opt + \frac{\eps}{K}\normp{Ax^*}^p,
	\]
	we can, with probability at least $99/100$ (over the randomness of $b$), identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$.
\end{lemma} 
We need the next lemma, whose proof is postponed to \Cref{sec:LB_aux}, to prove \Cref{lem:p<=2_main_claim}.
\begin{lemma}\label{lem:p<=2_LB_aux}
	Let $b'$ be a $(2m)$-dimensional vector in which 
	\[
	\begin{bmatrix}
		b^\prime_{2i-1}\\
		b^\prime_{2i}
	\end{bmatrix}
	=
	u
	\text{ or } 
	v,
	\quad i = 1,\dots, m.
	\]
	Then 
	\begin{enumerate}[label=(\alph*)]
		\item it holds for $x \leq 0$ that $\norm*{f(a x)-b'}_p^p\geq \norm*{f(-6a)-b'}_p^p$, and
		\item it holds for $x \geq 0$ that $\norm*{f(a x)-b'}_p^p\geq \norm*{f(6a)-b'}_p^p$.
	\end{enumerate}
\end{lemma}


Now we are ready to prove \Cref{lem:p<=2_main_claim}. 
\begin{proof}[Proof of \Cref{lem:p<=2_main_claim}]
	
	To prove the statement, we first give a bound for $\opt$.
	We have
	\begin{align*}
		\opt
		& =
		\min_{x\in \mathbb{R}^d} \normp{f(Ax) - b}^p
		=
		\sum_{i=1}^d \min_{x_i\in \mathbb{R}} \normp{f(ax_i) - b^{(i)}}^p \quad \text{by the structure of $A$}\numberthis\label{eq:lower_p<=2_opt_1}
	\end{align*}
	and hence we can look at each term $\min_{x_i\in \mathbb{R}}\normp{f(ax_i) - b}^p$ individually.
	By \Cref{lem:p<=2_LB_aux}, we have 
	\begin{align*}
		\min_{x_i\in \mathbb{R}} \normp{f(ax_i) - b^{(i)}}^p
		& =
		\min\{\norm{f(-6a)-b^{(i)}}_p^p,\norm{f(6a)-b^{(i)}}_p^p\}. \numberthis\label{eq:lower_p<=2_opt_2}
	\end{align*}
	
	For $i=1,\dots,d$, let $k_i$ be the number of occurrences of $u = \begin{bsmallmatrix} 3\\ 2\end{bsmallmatrix}$ in $b^{(i)}$. 
	Recall that $m \sim (\log d)/\eps^2$. By choosing the hidden constant to be large enough, we have by a Chernoff bound that for every $i=1,\dots,d$, with probability at least $1-\frac{1}{100d}$,
	\begin{align*}
		k_i
		& \in 
		\begin{cases}
			[\frac{m}{2}+(1-\beta)\eps m,\frac{m}{2}+(1+\beta)\eps m] & \text{if $b^{(i)}$ is drawn from $D_0$} \\
			[\frac{m}{2}-(1+\beta)\eps m,\frac{m}{2}-(1-\beta)\eps m] & \text{if $b^{(i)}$ is drawn from $D_1$}
		\end{cases}, \numberthis\label{eq:lower_p<=2_k}
	\end{align*} 
	where $\beta > 0$ is a constant to be determined. 
	Taking a union bound, with probability at least $99/100$, every $k_i$ satisfies this condition. We condition on this event below.
	
	Note that
	\begin{align*}
		\normp{f(6a)-b^{(i)}}^p
		& =
		2(m-k_i) 
		\quad \text{and} \quad 
		\normp{f(-6a) - b^{(i)}}^p
		=
		2k_i
	\end{align*}
	By \Cref{eq:lower_p<=2_k}, if $b^{(i)}$ is drawn from $D_0$, we have
	\begin{align*}
		m - 2(1+\beta)\eps m \leq \normp{f(6a)-b^{(i)}}^p
		& \leq
		m - 2(1-\beta)\eps m \\
		%	\quad \text{and}\quad 
		\normp{f(-6a)-b^{(i)}}^p
		& \geq 
		m + 2(1-\beta)\eps m.
	\end{align*}
	Similarly, if $b^{(i)}$ is drawn from $D_1$,  we have
	\begin{align*}
		\normp{f(6a)-b^{(i)}}^p
		&\geq
		m + 2(1-\beta)\eps m \\
		%	\quad \text{and}\quad 
		m - 2(1+\beta)\eps m \leq \normp{f(-6a)-b^{(i)}}^p
		&\leq 
		m - 2(1-\beta)\eps m.
	\end{align*}
	By plugging them into \Cref{eq:lower_p<=2_opt_2} and \Cref{eq:lower_p<=2_opt_1}, we have
	\begin{align*}
		\opt \leq d \left(m - 2(1-\beta)\eps m\right).
	\end{align*}
	
	Now, suppose that a solution $\hat{x}$ satisfies 
	\begin{align*}
		\normp{f(A\hat{x}) - b}^p
		& \leq
		(1+\frac{\eps}{K})\opt + \frac{\eps}{K}\normp*{Ax^*}^p \\
		& \leq
		\left(1+\frac{\eps}{K}\right)d \left(m - 2(1-\beta)\eps m\right) + \frac{\eps}{K}\cdot 2md\cdot 6^p \\
		& \leq
		md - \left(2(1 - \beta) - \frac{C_p}{K}\right)\eps md \\
		& \leq
		md - (2 - 3\beta)\eps md,
	\end{align*}
	provided that $K \geq C_p/\beta$. Here $C_p$ is a constant that depends only on $p$.
	
	We declare $b^{(i)}$ is drawn from $D_0$ if $\hat x_i > 0$ and from $D_1$ otherwise. Suppose that our declaration is wrong on $\ell$ indices, then by Lemma~\ref{lem:p<=2_LB_aux}, 
	\begin{align*}
		\normp{f(A\hat{x}) - b}^p
		&\geq
		\ell\cdot (m + 2(1-\beta)\eps m) + (d-\ell) \cdot (m - 2(1+\beta)\eps m) \\
		&=
		md - 2(1+\beta)\eps md + 4\eps \ell m.
	\end{align*}
	Therefore,
	\[
	md - 2(1+\beta)\eps md + 4\eps \ell m \leq md - (2 - 3\beta)\eps md
	\]
	which implies that
	\[
	\ell \leq \frac{5}{4}\beta d.
	\]
	We can conclude that we have used an approximate solution $\hat{x}$ to deduce the distribution of $b^{(i)}$ for at least $(1-5\beta/4)d$ indices of $i=1,\dots,d$. Choosing $\beta=4/15$ and $K=4C_p$ completes the proof of \Cref{lem:p<=2_main_claim}.
\end{proof}

To finish the proof of \Cref{thm:p<=2_deterministic}, by \Cref{lem:p<=2_main_claim}, with probability $1-1/100-1/5 > 2/3$, we can correctly identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$, i.e. we solve \Cref{prob:lb_p<=2}.
Hence, we conclude that $\mathcal{A}$ must make $\Omega\big(d/\eps^2\big)$ queries to the entries of $b$.






\subsection{Case of $p\geq 2$}

By Yao's minimax theorem, it suffices to show the following theorem. 
\begin{theorem}\label{thm:p>2_deterministic}
	Suppose that $p\geq 1$, $\eps > 0$ is sufficiently small and $n\gtrsim_p d/\eps^p$. 
	There exist a deterministic function $f\in \lip_1$, a deterministic matrix $A\in\R^{n\times d}$ and a distribution over $b\in\R^n$ such that the following holds: every deterministic algorithm that outputs $\hat{x}\in\R^d$ which with probability at least $2/3$ over the randomness of $b$ satisfies \Cref{eq:main_obj} must make $\Omega\big(d/\eps^p\big)$ queries to the entries of $b$.
\end{theorem}


We remark that the lower bound holds for all $p\geq 1$.
% , and it is tight up to logarithmic factors  for $p\in [1,2]$.
To prove \Cref{thm:p>2_deterministic}, we reduce \Cref{prob:lb_p>2} below to our problem.

\begin{problem}\label{prob:lb_p>2}
	Suppose that $0<\eps<1$, $d$ is a positive integer, $m= 1/\eps^p$ and $n=2md$. 
	Let $v$ be the $2m$-dimensional vector whose entries are all $1$, i.e. $v = [1,\dots,1]^\top$, $D_0$ be the uniform distribution on $\{v + (1/\eps)\cdot e_1, \dots, v+(1/\eps)\cdot e_{m}\}$ and $D_1$ be the uniform distribution on $\{v + (1/\eps)\cdot e_{m+1}, \dots, v+(1/\eps)\cdot e_{2m}\}$ where $e_1,\dots,e_{2m}$ are the canonical basis vectors in $\R^{2m}$. %, i.e. $e_i = [0,\dots,0,1,0,\dots,0]^\top$ where the $1$ is at the $i$-th coordinate.
	Let $b$ be the $n$-dimensional random vector formed by concatenating $d$ i.i.d. random vectors $b^{(1)},\dots,b^{(d)}$, where each $b^{(i)}$ is drawn from $D_0$ with probability $1/2$ and from $D_1$ with probability $1/2$, i.e.\ each $b^{(i)}$ is an all one vector with a value $1/\eps$ planted at a uniformly random entry.
	
	Given a query access to the entries of $b$, we would like to, with probability at least $2/3$, correctly identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$.
\end{problem}

By \Cref{lem:lb_idx_loc} and \Cref{lem:distinguishing_dist}, any deterministic algorithm that solves this problem requires $\Omega(d/\eps^p)$ queries to the entries of $b$.

Now, we construct the reduction.
Let $f$ be the function
\begin{align*}
	f(x)
	& =
	\begin{cases}
		0 & \text{if $x\leq 0$} \\
		x & \text{if $0\leq  x\leq 1$} \\
		1 & \text{if $1\leq x$.}
	\end{cases}
\end{align*}
Let $a$ be the $2m$-dimensional vector such that 
\begin{align*}
	a_i
	& =
	\begin{cases}
		1 & \text{for $i=1,\dots,m$} \\
		-1 & \text{for $i=m+1,\dots,2m$}
	\end{cases}
\end{align*}
and $A$ be the $n\times d$ block-diagonal matrix whose diagonal blocks are the same $a\in \R^{2m\times 1}$. 

Given a deterministic algorithm $\mathcal{A}$ that takes $f,A,\eps$ and a query access to the entries of $b$ as inputs and returns $\hat{x}\in \mathbb{R}^d$ satisfying \Cref{eq:main_obj}, we claim that $\hat{x}$ can be used to solve \Cref{prob:lb_p>2}.
This claim is proved in \Cref{lem:p>2_main_claim}.
\begin{lemma}\label{lem:p>2_main_claim}
	
	Let $f$, $A$, $b$ be as specified above. There exists $K$, a constant, such that given an $\hat x\in \R^d$ satisfying
	\[
	\normp{f(A\hat{x}) - b}^p
	\leq
	(1+\frac{\eps}{K})\opt + \frac{\eps}{K}\normp{Ax^*}^p,
	\]
	we can identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$.
\end{lemma}



% \Cref{thm:p>2_deterministic} follows immediately from \Cref{lem:p>2_main_claim}, \Cref{lem:lb_idx_loc} and \Cref{lem:distinguishing_dist} with a rescaling of $\eps$ (which will require $\eps$ to be sufficiently small), and so our goal is now to show \Cref{lem:p>2_main_claim}. 
We need the next lemma, whose proof is postponed to \Cref{sec:LB_aux}.

\begin{lemma}\label{lem:p>2_LB_aux}
	
	Let $b'$ be a $2m$-dimensional vector in which all entries are $1$ except one of them is $1+\frac{1}{\eps}$.
	Then 
	\begin{enumerate}[label=(\alph*)]
		\item it holds for $x \leq 0$ that $\norm*{f(a x)-b'}_p^p\geq \norm*{f(-a)-b'}_p^p $, and
		\item it holds for $x \geq 0$ that $\norm*{f(a x)-b'}_p^p\geq \norm*{f(a)-b'}_p^p$.
	\end{enumerate}
	
\end{lemma}



Now we are ready to prove \Cref{lem:p>2_main_claim}.



\begin{proof}[Proof of \Cref{lem:p>2_main_claim}]
	
	To prove the statement, we first give a bound for $\opt$.
	We have
	\begin{align*}
		\opt
		& =
		\min_{x\in \mathbb{R}^d} \normp{f(Ax) - b}^p
		=
		\sum_{i=1}^d \min_{x_i\in \mathbb{R}} \normp{f(ax_i) - b^{(i)}}^p \numberthis\label{eq:lower_p>2_opt_1}
	\end{align*}
	and hence we can look at each term $\min_{x_i\in \mathbb{R}}\normp{f(ax_i) - b}^p$ individually.
	By \Cref{lem:p>2_LB_aux}, we have 
	\begin{align*}
		\min_{x_i\in \mathbb{R}} \normp{f(ax_i) - b^{(i)}}^p
		& =
		\min\{\norm{f(-a)-b^{(i)}}_p^p,\norm{f(a)-b^{(i)}}_p^p\}. \numberthis\label{eq:lower_p>2_opt_2}
	\end{align*}
	%
	Recall that $m=\frac{1}{\eps^p}$.
	For $i=1,\dots,d$, if $b^{(i)}$ is drawn from $D_0$, we have
	\begin{align*}
		\normp{f(a) - b^{(i)}}^p = m+\frac{1}{\eps^p} = 2m
		\quad \text{and} \quad 
		\normp{f(-a)-b^{(i)}}^p = (1+\frac{1}{\eps})^p + m-1 \geq (2+\eps)m
	\end{align*}
	and, if $b^{(i)}$ is drawn from $D_1$, we have
	\begin{align*}
		\normp{f(-a) - b^{(i)}}^p = m+\frac{1}{\eps^p} = 2m
		\quad \text{and} \quad 
		\normp{f(a)-b^{(i)}}^p = (1+\frac{1}{\eps})^p + m-1 \geq (2+\eps)m.
	\end{align*}
	By plugging them into \Cref{eq:lower_p>2_opt_2} and \Cref{eq:lower_p>2_opt_1}, we have
	\begin{align*}
		\opt \leq 2dm.
	\end{align*}
	
	Now, suppose that a solution $\hat{x}$ satisfies 
	\begin{align*}
		\normp{f(A\hat{x}) - b}^p
		& \leq
		(1+\frac{\eps}{K})\opt + \frac{\eps}{K} \normp{Ax^*}^p \\
		& \leq
		(1+\frac{\eps}{K})\cdot (2dm) + \frac{\eps}{K} \cdot 2dm \cdot 1^p \\
		& \leq
		2dm + \frac{4\eps dm}{K}.
	\end{align*}
	
	
	We declare that $b^{(i)}$ is drawn from $D_0$ if $\hat{x}_i>0$ and from $D_1$ otherwise.
	Suppose that our declaration is wrong on $\ell$ indices, then by \Cref{lem:p>2_main_claim},
	\begin{align*}
		\normp{f(A\hat{x}) - b}^p
		& \leq 
		\ell (2+\eps)m  + (d-\ell) (2m)
		=
		2dm + \eps \ell m .
	\end{align*}
	Therefore, 
	\[
	2dm + \eps \ell m \leq 2dm + \frac{4\eps dm}{K} ,
	\]
	which implies, when $K=12$, that
	\[
	\ell \leq \frac{d}{3},
	\]
	completing the proof of \Cref{lem:p>2_main_claim}.   
\end{proof}

To finish the proof of \Cref{thm:p>2_deterministic}, by \Cref{lem:p>2_main_claim}, with probability $2/3$, we can correctly identify whether $b^{(i)}$ is drawn from $D_0$ or $D_1$ for at least $2d/3$ indices $i$, i.e. we solve \Cref{prob:lb_p>2}.
Hence, we conclude that $\mathcal{A}$ must make $\Omega\big(d/\eps^p\big)$ queries to the entries of $b$.



\section{Conclusion}

In this paper, we consider the active regression problem of the single-index model, which asks to solve $\min_x \normp{f(Ax)-b}$, with $f$ being a Lipschitz function, $A$ fully accessible and $b$ only accessible via entry queries. 
The goal is to minimize the number of queries to the entries of $b$ while achieving an accurate solution to the regression problem.
Previous work on single-index model has only achieved constant-factor approximations~\citep{gajjar2023active,gajjar2023improved,ICLR24,COLT24}.
In this paper, we achieve a $(1+\eps)$-approximation with $\tilde{O}(d^{\frac{p}{2}\vee 1}/\eps^{p\vee 2})$ queries and we show that this query complexity is tight for $1\leq p\leq 2$ up to logarithmic factors. Furthermore, we prove that the $1/\eps^p$ dependence is tight for $p > 2$ and we leave the full tightness of $d^{p/2}/\eps^p$ as an open problem for future work.

\bibliography{ref}
\bibliographystyle{plainnat}

\appendix

\section{Omitted Proofs in \Cref{sec:prelim}}\label{sec:prelim_proofs}
\begin{proof}[Proof of \Cref{lem:lb_idx_loc}]
	Let $Q$ be the set of indices the algorithm reads and $\mathcal{A}(Q,i^*)$ be the output of the algorithm.
	Note that, if $i^*\notin Q$, then $\mathcal{A}(Q,i^*)$ does not depend on $i^*$ and we write it $\mathcal{A}(Q)$.
	
	Now, let $\mathcal{E}$ be the event of $\mathcal{A}(Q,i^*)$ being the correct set and $\mathcal{I}$ be the event of $i^*$ being chosen among these $q$ queries.
	Then, we have
	\begin{align*}
		\Pr( \mathcal{E})
		& =
		\Pr(  \mathcal{E} \mid \mathcal{I}) \Pr(\mathcal{I})+\Pr(  \mathcal{E} \mid \overline{\mathcal{I}}) \Pr(\overline{\mathcal{I}}).
	\end{align*}
	Note that 
	\begin{align*}
		\Pr(  \mathcal{E} \mid \mathcal{I}) \leq 1, \quad 
		\Pr(\mathcal{I}) = \frac{q}{2m}
		\quad \text{and}\quad 
		\Pr(\overline{\mathcal{I}}) = 1-\frac{q}{2m}.
	\end{align*}
	where $q=\abs{Q}$.
	
	Now, we evaluate $\Pr(\mathcal{E}\mid \overline{\mathcal{I}})$.
	Let $q_1$ (resp.\xspace $q_2$) be the size of the set $Q\cap \{1,\dots,m\}$ (resp.\xspace $Q\cap \{m+1,\dots,2m\}$), so $q_1+q_2=q$.
	Given the event $\overline{\mathcal{I}}$, recall that we have $\mathcal{A}(Q,i^*) = \mathcal{A}(Q)$.
	If $\mathcal{A}(Q)=\{1,\dots,m\}$, the event of $\mathcal{E}\mid \overline{\mathcal{I}}$ is equivalent to the event that $i^*$ belongs to $\{1,\dots,m\}$ but is not queried, thus we have
	\begin{align*}
		\Pr(\mathcal{E}\mid \overline{\mathcal{I}})
		& =
		1-\frac{m - q_1}{2m-q}
		=
		\frac{m - q_2}{2m-q}.
	\end{align*}
	Similarly, if $\mathcal{A}(Q)=\{m+1,\dots,2m\}$, we have
	\begin{align*}
		\Pr(\mathcal{E}\mid \overline{\mathcal{I}})
		& =
		1-\frac{m - q_2}{2m-q}
		=
		\frac{m - q_1}{2m-q}.
	\end{align*}
	Hence, we have
	\begin{align*}
		\Pr(\mathcal{E}\mid \overline{\mathcal{I}})
		& \leq
		\max\{ \frac{m - q_2}{2m-q}, \frac{m - q_1}{2m-q}\}.
	\end{align*}
	
	Namely, we have
	\begin{align*}
		\Pr( \mathcal{E})
		& \leq
		\frac{q}{2m}+\max\{ \frac{m - q_2}{2m-q}, \frac{m - q_1}{2m-q}\} (1-\frac{q}{2m}) \\
		& =
		\frac{q}{2m}+\max\{ \frac{1}{2} - \frac{q_1}{2m}, \frac{1}{2} - \frac{q_2}{2m}\} \\
		& =
		\frac{1}{2} + \max \{\frac{q_1}{2m}, \frac{q_2}{2m}\}.
	\end{align*}
	If $q\leq \frac{m}{5}$, it implies $\max\{q_1,q_2\} \leq \frac{m}{5}$ and hence $\Pr(\mathcal{E}) \leq \frac{3}{5}$.
\end{proof}

\begin{proof}[Proof of \Cref{lem:distinguishing_dist}]
	Suppose that an algorithm makes fewer than $\frac{\beta}{10}dm$ queries in total.
	Then there exist $\frac{9}{10}d$ blocks, each of which makes fewer than $\beta m$ queries. 
	Therefore, each of these blocks will make an error in distinguishing the distributions with probability at least $2/5$. 
	By a Chernoff bound, with probability at least $1/3$, at least $d'=\frac{2}{5}\cdot\frac{9}{10}\cdot d - \Theta(\sqrt{d})$ instances make an error. 
	It means that $d' > d/3$ and we arrive at a contradiction against the assumption on the correctness of the algorithm.
\end{proof}


\section{Omitted Proofs in \Cref{sec:lower}}
\label{sec:LB_aux}

\begin{proof}[Proof of \Cref{lem:p<=2_LB_aux}]
	Suppose that $b'$ contains $k$ occurrences of $u = \begin{bsmallmatrix} 3 \\ 2 \end{bsmallmatrix}$ and $\theta = k/m$. Then
	\[
	\frac{1}{m}\norm*{f(ax)-b'}_p^p 
	=  \theta (g(x))^p + (1-\theta) (h(x))^p,
	\]
	where
	\[
	g(x) = \norm*{\begin{bmatrix} f(x) \\ f(-x) \end{bmatrix} - \begin{bmatrix} 3\\ 2\end{bmatrix}}_p \quad\text{and}\quad
	h(x) = \norm*{\begin{bmatrix} f(x) \\ f(-x) \end{bmatrix} - \begin{bmatrix} 2\\ 3\end{bmatrix}}_p. 
	\]
	It suffices to show that both $h(x)$ and $g(x)$ attain a local minimum at $x=-6$ when $x\leq 0$ and at $x=6$ when $x\geq 0$.
	
	Now, we view the $2$-dimensional vectors as points in $\mathbb{R}^2$.
	For any $x\in \mathbb{R}$, let $\zeta(x)$ be the point $(f(x), f(-x))$.
	Also, let $\gamma\subset \R^2$ be the locus of $\zeta(x) = (f(x),f(-x))$, i.e. $\gamma := \setdef{\zeta(x)}{x\in \mathbb{R}}$. 
	It has a positive branch  $\gamma^+ := \setdef{\zeta(x)}{x \geq 0}$ and a negative branch $\gamma^- := \setdef{\zeta(x)}{x \leq 0}$. 
	
	We first consider $x\leq 0$.
	Note that $\zeta(-6)=(2,3)$ is on $\gamma^-$ and $x = -6$ is the only value such that $\zeta(x) = (2,3)$.
	Hence, we immediately have that $h(x)=0$ attains a local minimum at $x=-6$.
	For $g(x)$, consider the smallest $\ell_p$-ball centered at $(3,2)$ that touches $(2,3)$ on its boundary.
	It is not difficult to verify that this $\ell_p$-ball does not intersect $\gamma^-$ at any other point. (\Cref{fig:locus} provides a geometric intuition.)
	
    \begin{figure}[tb]
		\centering
		\includegraphics[width=0.3\textwidth]{p_12_minimizer_locus.pdf}
		\includegraphics[width=0.3\textwidth]{p_12_minimizer_neg.pdf}
		\includegraphics[width=0.3\textwidth]{p_12_minimizer_pos.pdf}
		\caption{Illustration of the locus $\gamma$ (left), the minimizers when $x\leq 0$ (middle) and the minimizer when $x\geq 0$ (right)}
		\label{fig:locus}
	\end{figure}
 
	Since $\gamma^+$ and $\gamma^-$ are symmetric about $y=x$, we can show the symmetric result for $x\geq 0$.
\end{proof}


\begin{proof}[Proof of \Cref{lem:p>2_LB_aux}]
	
	We would like to show that the function $\normp{f(ax) - b'}^p$ attains a local minimum at $x=-1$ when $x\leq 0$ and at $x=1$ when $x\geq 0$.
	Note that, by the construction of $f$, $f(x) = f(1)=1$ for all $x\geq 1$ and $f(x) = f(-1) = 0$ for all $x\leq -1$.
	Therefore, we now restrict our domain to be $x\in [-1,1]$.
	
	Suppose that the index of the entry whose value is $1+\frac{1}{\eps}$ is in $\{1,\dots, m\}$.
	Then
	\begin{align*}
		\normp{f(ax) - b'}^p
		& =
		(1+\frac{1}{\eps} - f(x))^p + (m-1)(1-f(x))^p + m(1-f(-x))^p.
	\end{align*}
	Note that we drop the absolute value sign because $f(x)\leq 1$ on $\R$.
	When $-1\leq x\leq 0$, we have
	\begin{align*}
		\normp{f(ax) - b'}^p
		& =
		(1+\frac{1}{\eps})^p + (m-1)+ m(1+x)^p \\
		& \geq 
		(1+\frac{1}{\eps})^p + (m-1).
	\end{align*}
	where the equality holds if $x=-1$.
	When $0\leq x\leq 1$, we have
	\begin{align*}
		\normp{f(ax) - b'}^p
		& =
		(1+\frac{1}{\eps} - x)^p + (m-1)(1-x)^p + m \\
		& \geq
		\frac{1}{\eps^p} + m 
	\end{align*}
	where the equality holds if $x=1$.
	
	Similarly, we can prove the same result when the index of the entry whose value is $1+\frac{1}{\eps}$ is in $\{m+1,\dots, 2m\}$.
\end{proof}


\section{Lewis Weights of Row-Sampled Matrix}\label{sec:appendix_lewis}
In this section, we shall show the following theorem. 
\begin{theorem}
	Suppose that $A\in\R^{n\times d}$ has uniformly bounded Lewis weights $w_i(A) \lesssim d/n$. Let $S$ be an $n\times n$ diagonal matrix in which the diagonal elements are i.i.d.\ $\alpha^{-1/p}\Ber(\alpha)$ for some sampling rate $\alpha\in (0,1)$. When $\alpha n\gtrsim_p d\log d$ when $p < 4$ or $\alpha n\gtrsim_p d^2\log d$ when $p \geq 4$, it holds with probability at least $0.99$ that $w_i(SA)\lesssim d/m$, where $m$ is the number of nonzero rows of $S$.
\end{theorem}
This theorem is similar to~\citet[Lemma A.3]{CLS2022}, where the sampling rates are proportional to Lewis weights and no assumptions on the bounds of $w_i(A)$ were made. Our proof is also similar.
\begin{proof}
	Let $w_1,\dots,w_n$ denote the Lewis weights of $A$ and suppose that $S = \diag\{\sigma_1,\dots,\sigma_n\}$. We first show that with probability at least $0.995$, 
	\[
	\frac{1}{2}\sum_i w_i^{1-\frac{2}{p}} a_i a_i^\top \preceq \sum_i (\frac{w_i}{\alpha})^{1-\frac{2}{p}} (\sigma_i a_i) (\sigma_i a_i)^\top \preceq \frac{3}{2}\sum_i w_i^{1-\frac{2}{p}} a_i a_i^\top.
	\]
 Here, the $\preceq$ sign denotes semi-positive definiteness.
	We prove this claim by the matrix Bernstein inequality. Notice that 
	\[
	\sum_i (\frac{w_i}{\alpha})^{1-\frac{2}{p}} (\sigma_i a_i) (\sigma_i a_i)^\top = \sum_i \frac{\xi_i}{\alpha} w_i^{1-\frac{2}{p}} a_i a_i^\top,
	\]
	where $\xi_i$ are i.i.d.\ $\Ber(\alpha)$ variables.	
	Let $M = \sum_i w_i^{1-\frac{2}{p}}a_i a_i^\top$, $a_i' = M^{-\frac{1}{2}} a_i$, and $X_i  = \frac{\xi_i}{\alpha} w_i^{1-\frac{2}{p}} a_i' (a_i')^\top -  w_i^{1-\frac{2}{p}} a_i' (a_i')^\top $, then $\E X_i = 0$ and, by the definition of Lewis weights, $\norm{a_i'}^2 = w_i^{2/p}$. We bound
	\[
	\normtwo{X_i} \leq \frac{1}{\alpha} w_i^{1-\frac{2}{p}} \normtwo{a_i'}^2 = \frac{1}{\alpha} w_i \lesssim \frac{d}{\alpha n}.
	\]
	Also,
	\begin{align*}
		\normtwo*{\sum_i \E X_i X_i^\top} \lesssim \frac{1}{\alpha}\normtwo*{\sum_i w_i^{2(1-\frac{2}{p})} \normtwo{a_i}^2 a_i' (a_i')^\top} &= \frac{1}{\alpha}\normtwo*{\sum_i w_i \cdot w_i^{1-\frac{2}{p}} a_i' (a_i')^\top}\\
		&\lesssim \frac{d}{\alpha n}\normtwo*{\sum_i w_i^{1-\frac{2}{p}} a_i' (a_i')^\top} \\
		&= \frac{d}{\alpha n} \normtwo{I} \\
		&= \frac{d}{\alpha n}.
	\end{align*}
	It follows from matrix Bernstein inequality that
	\[
	\Pr\left\{ \normtwo*{\sum_i X_i}\geq \eta \right\} \leq 2d\exp\left( -c\frac{\alpha n \eta^2}{d} \right) \leq 0.005,
	\]
	provided that $\alpha n\gtrsim \eta^{-2}d\log d$. This shows that 
	\[
	(1-\eta) I \preceq \sum_i \frac{\xi_i}{\alpha}  w_i^{1-\frac{2}{p}} a_i' (a_i')^\top \preceq (1+\eta) I,
	\]
	which is equivalent to our claim. 
	
	When $\sigma_i > 0$,
	\begin{align*}
		&\quad\ (\sigma_i a_i)^\top \left(\sum_j \left(\frac{w_j}{\alpha}\right)^{1-\frac{2}{p}} (\sigma_j a_j) (\sigma_j a_j)^\top\right)^{-1} (\sigma_i a_i) \\
		&\leq \frac{1}{1-\eta} \sigma_i^2 a_i^\top \left(\sum_j w_j^{1-\frac{2}{p}}a_j a_j^\top\right)^{-1} a_i \\
		&= \frac{1}{1-\eta}\sigma_i^2 w_i^{\frac{2}{p}} \\
		&= \frac{1}{1-\eta}\left(\frac{w_i}{\alpha}\right)^{\frac{2}{p}}.
	\end{align*}
	and, similarly,
	\[
	(\sigma_i a_i)^\top \left(\sum_j (\frac{w_j}{\alpha})^{1-\frac{2}{p}} (\sigma_j a_j) (\sigma_j a_j)^\top\right)^{-1} (\sigma_i a_i)\geq \frac{1}{1+\eta} \left(\frac{w_i}{\alpha}\right)^{\frac{2}{p}}.
	\]
	We take $\eta$ to be a constant depending on $p$ for $p < 4$ and $\eta = 1/(C_p \sqrt{d})$ for $p \geq 4$. 
	It then follows from \citet[Lemmas 5.3 and 5.4]{CP15} that $w_{i}(SA) \lesssim w_{i'}(A)/\alpha \lesssim d/(\alpha n)$, where $i'$ is the index of the corresponding row in $A$. 
	
	By a Chernoff bound, with probability at least $0.995$, $m \lesssim \alpha n$. The result then follows.
\end{proof}



\section{Entropy Estimates} \label{sec:entropy_estimates}
In this section we provide a proof of \Cref{lem:entropy_estimates_consolidated} for completeness. The proof is decomposed into the following three lemmata, \Cref{lem:covering_small}, \Cref{lem:covering_large_1} and \Cref{lem:covering_large_2}.

\begin{lemma}\label{lem:covering_small}
	Suppose that $A\in \R^{n\times d}$ has full column rank and $W$ is a diagonal matrix whose diagonal entries are the Lewis weights of $A$.  It holds for $p\geq 1$ that
	\begin{align*}
		\log \mathcal{N}(B_{w,p}(W^{-1/p} A), \norm*{\cdot}_{w,p}, t)
		& \lesssim 
		d \log\frac{1}{t}.
	\end{align*}
\end{lemma}
\begin{proof}

This is a standard result following from a standard volume argument, which we reproduce below for completeness. 
Suppose that $E$ is the column space of $W^{-1/p} A$ and is endowed with norms $\norm*{\cdot}_{w,p}$. 
Using the notation simplification defined in \Cref{sec:prelim}, we denote by $B_{w,p}$ the unit ball of $E$ w.r.t.~$\norm*{\cdot}_{w,q}$, i.e. $B_{w,p} = B_{w,p}(W^{-1/p}A)$.
Consider a maximal $t$-separation set $N\subseteq B_{w,p}$, then the balls $x + \frac{t}{2}B_{w,p}$ ($x\in N)$ are contained in $(1+\frac{t}{2})B_{w,p}$ and are nearly disjoint (intersection has zero volume). 
Hence $\sum_{x\in N} \operatorname{vol}(x + \frac{t}{2}B_{w,p}) \leq \operatorname{vol}((1+\frac{t}{2})B_{w,p})$, that is, $|N| \cdot \operatorname{vol}(\frac{t}{2}B_{w,p}) \leq \operatorname{vol}((1+\frac{t}{2} )B_{w,p})$, leading to $|N| \leq (1+2/t)^d$.
It is easy to check that $N$ is a $t$-covering of $(B_{w,p}, \norm*{\cdot}_{w,p})$ and it implies $\mathcal{N}(B_{w,p}(W^{-1/p} A), \norm*{\cdot}_{w,p}, t) \leq |N| \leq  (1+2/t)^d$.


\end{proof}

\begin{lemma}\label{lem:covering_large_1}
	Suppose that $A\in \R^{n\times d}$ has full column rank and $W$ is a diagonal matrix whose diagonal entries are the Lewis weights of $A$. When $1 \leq p \leq 2$ and $q > 2$, it holds that
	\begin{align*}
		\log \mathcal{N}(B_{w,p}(W^{-1/p} A), \norm*{\cdot}_{w,q}, t)
		& \lesssim
		\frac{q\sqrt{\log d}}{t^p}
	\end{align*}
\end{lemma}
\begin{proof}

Suppose that $E$ is the column space of $W^{-1/p} A$ and is endowed with norms $\norm*{\cdot}_{w,p}$ and an inner product $\langle\cdot,\cdot\rangle_w$.
We first have
\[
    \log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,q}, t) \leq \log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,2}, \lambda) + \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,q}, \frac{t}{\lambda}\right).
\]
Recall that $B_{w,p} = B_{w,p}(W^{-1/p}A)$ and $B_{w,2} = B_{w,2}(W^{-1/p}A)$.
For the second term, we can apply \Cref{lem:covering_large_2} directly and obtain that
\[
    \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,q}, \frac{t}{\lambda}\right) \lesssim \frac{\lambda^2}{t^2} d^{2/q} q.
\]	
Next we deal with the first term. 

We first consider the case $1<p\leq 2$. Let $p'$ be the conjugate index of $p$ and $r\geq p'$ to be determined. Define $\theta\in [0,1]$ by
\[
    \frac{1}{p'} = \frac{1-\theta}{2} + \frac{\theta}{r}.
\]
For $x,y\in B_{w,2}$, by H\"older's inequality,
\[
    \norm*{x-y}_{w,p'} \leq \norm*{x-y}_{w,2}^{1-\theta} \norm*{x-y}_{w,r}^\theta \leq 2^{1-\theta}\norm*{x-y}_{w,r}^\theta. 
\]
This implies that 
\[
    \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,p'}, \lambda\right) 
\leq 
\log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,r}, 2\left(\frac{\lambda}{2}\right)^{1/\theta}\right) \\
\lesssim \left(\frac{2}{\lambda}\right)^{2/\theta} r d^{2/r},
\]
where the last inequality follows from \Cref{lem:covering_large_2}. Since
\[
    \frac{1}{\theta} = \frac{p'}{p'-2} \left(1 - \frac{2}{r}\right) = \frac{p}{2 - p} \left(1 - \frac{2}{r}\right),
\]
it follows that
\[
    \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,p'}, \lambda\right) \lesssim \left(\frac{2}{\lambda}\right)^{\frac{2p}{2-p}} r d^{2/r}.
\]
Choose $r = p'\vee(\log d)$, 
\[
    \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,p'}, \lambda\right) \lesssim \left(\frac{2}{\lambda}\right)^{\frac{2p}{2-p}} r.
\]
By duality~\citep{AMS04},
\[
    \log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,2}, \lambda) \lesssim \left(\frac{2}{\lambda}\right)^{\frac{2p}{2-p}} r.
\]

Therefore,
\[
    \log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,q}, t) \lesssim \left(\frac{2}{\lambda}\right)^{\frac{2p}{2-p}} r + \frac{\lambda^2}{t^2} d^{2/q} q.
\]
Optimizing $\lambda$ yields that
\[
    \log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,q}, t) \lesssim \frac{q^{p/2}r^{1-p/2}}{t^p} \lesssim \left(\frac{1}{\sqrt{p-1}} + \sqrt{\log d}\right)\cdot \frac{q}{t^p}.
\]
This completes the proof for $1<p\leq 2$.

When $p=1$, Maurey's empirical method gives that (using the fact that, by \Cref{lem:lewis_weight_properties}(c), $\norm*{\cdot}_{w,2} \leq \norm*{\cdot}_{w,1}$ in $E$)
\[
    \log \mathcal{N}(B_{w,1}, \norm*{\cdot}_{w,2}, \lambda) \lesssim \frac{\log d}{\lambda^2}
\]
and thus
\[
    \log \mathcal{N}(B_{w,1}, \norm*{\cdot}_{w,q}, t) \lesssim \frac{\log d}{\lambda^2} + \frac{\lambda^2}{t^2} d^{2/q} q.
\]
Optimizing $\lambda$ yields that
\[
    \log \mathcal{N}(B_{w,1}, \norm*{\cdot}_{w,q}, t) \lesssim \frac{\sqrt{q\log d} \cdot d^{1/q}}{t}.
\]

\end{proof}

\begin{lemma}\label{lem:covering_large_2}
	Suppose that $A\in \R^{n\times d}$ has full column rank and $W$ is a diagonal matrix whose diagonal entries are the Lewis weights of $A$. 
	When $p, q \geq 2$, it holds that
	\begin{align*}
		\log \mathcal{N}(B_{w,p}(W^{-1/p}A), \norm*{\cdot}_{w,q}, t)
		& \lesssim
		\frac{d^{1-\frac{2}{p} + \frac{2}{q}}q}{t^2}
	\end{align*}
\end{lemma}
\begin{proof}

Suppose that $E$ is the column space of $W^{-1/p}A$ and is endowed with norms $\norm*{\cdot}_{w,p}$ and an inner product $\langle\cdot,\cdot\rangle_w$. By Lemma~\ref{lem:lewis_weight_properties}(b), there exist $u_1,\dots,u_d\in \R^n$ such that
\begin{align*}
    \inner{u_i}{u_{i'}}_w = 0, \quad
    \norm{u_i}_{w,p}=1
    \quad\text{and}\quad
    \sum_{j=1}^d u_{ij}^2=1
    \quad\text{for any $i,i'=1,\dots,n$.}
\end{align*}

Recall that $B_{w,p} = B_{w,p}(W^{-1/p}A)$ and $B_{w,2} = B_{w,2}(W^{-1/p}A)$.
First, observe, by Lemma~\ref{lem:lewis_weight_properties}(c), that $B_{w,p} \subseteq d^{1/2-1/p}\cdot B_{w,2}$, thus 
\[
\log \mathcal{N}(B_{w,p}, \norm*{\cdot}_{w,q}, t) \leq 
\log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,q}, \frac{t}{d^{1/2-1/p}}\right)
\]
and it suffices to show that
\[
\log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,q}, t\right) \lesssim \frac{qd^{2/q}}{t^2}.
\]
Let $q'$ be the conjugate index of $q$, i.e. $\frac{1}{q} + \frac{1}{q'}=1$. 
By dual Sudakov minorization,
\[
    \log \mathcal{N}\left(B_{w,2}, \norm*{\cdot}_{w,q}, t\right) \lesssim \frac{1}{t^2} \left(\E_{g\sim N(0,I_d)} \sup_{x\in B_{w,q'}}\abs*{\inner*{\sum_{i=1}^d g_i u_i}{x}_w} \right)^2,
\]
By duality again,
\[
    \E_{g\sim N(0,I_d)} \sup_{x\in B_{w,q'}}\abs*{\inner*{\sum_{i=1}^d g_i u_i}{x}_w} = \E_{g\sim N(0,I_d)} \norm*{\sum_{i=1}^d g_i u_i}_{w,q}.
\]
Then,
\begin{align*}
    \E_{g\sim N(0,I_d)} \norm*{\sum_{i=1}^d g_i u_i}_{w,q} 
    & =
    \E_{g\sim N(0,I_d)} \left(\sum_{j=1}^n w_j \abs*{\sum_{i=1}^d g_i u_{ij}}^q\right)^{1/q} \\
    &\leq 
    \left(\E_{g\sim N(0,I_d)} \sum_{j=1}^n w_j \abs*{\sum_{i=1}^d g_i u_{ij}}^q\right)^{1/q} \\
    &\leq 
    \left(\bigg(\sum_{j=1}^n w_j \abs*{\bigg(\sum_{i=1}^d u_{ij}^2\bigg)^{\frac{1}{2}}}^q\bigg)\cdot \E_{g\sim N(0,1)} |g|^q\right)^{1/q} \\
    &\leq 
    \left(\E_{g\sim N(0,1)} |g|^q\right)^{1/q} \left(\sum_{j=1}^n w_j \right)^{1/q}  \qquad \text{since $\sum_{i=1}^d u_{ij}^2=1$}\\
    &\lesssim 
    \sqrt{q} d^{1/q}. \qedhere
\end{align*}


\end{proof}







\end{document}

















