@misc{lubin2021imlcamachinelearningpowerediterative,
      title={iMLCA: Machine Learning-powered Iterative Combinatorial Auctions with Interval Bidding}, 
      author={Benjamin Lubin and Sven Seuken and Manuel Beyeler and Gianluca Brero},
      year={2021},
      eprint={2009.13605},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2009.13605}, 
}

@article{dutting2019,
author={D{\"u}tting, Paul and Feng, Zhe and Narasimhan, Harikrishna and Parkes, David and Ravindranath, Sai Srivatsa},
title={Optimal Auctions through Deep Learning: Advances in Differentiable Economics},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
journal = {J. ACM},
note = {An earlier version appeared in {\em Proceedings of the 36th International Conference on Machine Learning}. ICML 2019}
}

@inproceedings{sandholm2000improved,
  title={Improved algorithms for optimal winner determination in combinatorial auctions and generalizations},
  author={Sandholm, Tuomas and Suri, Subhash},
  booktitle={AAAI/IAAI},
  pages={90--97},
  year={2000}
}
@inproceedings{nisan2000bidding,
  title={Bidding and allocation in combinatorial auctions},
  author={Nisan, Noam},
  booktitle={Proceedings of the 2nd ACM Conference on Electronic Commerce},
  pages={1--12},
  year={2000}
}
@inproceedings{fujishima1999taming,
  title={Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches},
  author={Fujishima, Yuzo and Leyton-Brown, Kevin and Shoham, Yoav},
  booktitle={IJCAI},
  volume={99},
  pages={548--553},
  year={1999}
}
@inproceedings{feng2018deep,
  title={Deep learning for revenue-optimal auctions with budgets},
  author={Feng, Zhe and Narasimhan, Harikrishna and Parkes, David C},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems},
  pages={354--362},
  year={2018}
}

@article{tacchetti2019neural,
  title={A neural architecture for designing truthful and efficient auctions},
  author={Tacchetti, Andrea and Strouse, DJ and Garnelo, Marta and Graepel, Thore and Bachrach, Yoram},
  journal={arXiv preprint arXiv:1907.05181},
  year={2019}
}



@article{kuo2020proportionnet,
  title={Proportionnet: Balancing fairness and revenue for auction design with deep learning},
  author={Kuo, Kevin and Ostuni, Anthony and Horishny, Elizabeth and Curry, Michael J and Dooley, Samuel and Chiang, Ping-yeh and Goldstein, Tom and Dickerson, John P},
  journal={arXiv preprint arXiv:2010.06398},
  year={2020}
}
@misc{Coursea,
  title = {Course {{Match}} ({{Registration}}) {\textbar} {{Students}}},
  urldate = {2025-01-30},
author = {{Columbia Business School}},
  howpublished = {https://students.business.columbia.edu/records-registration/course-match-registration}
}
@misc{Course,
  title = {Course {{Match}}},
    author = {{University of Pennsylvania}},
  journal = {MBA Inside},
  urldate = {2025-01-30}
}

@misc{Huang25Accelerated,
  title = {Accelerated {{Preference Elicitation}} with {{LLM-Based Proxies}}},
  author = {Huang, David and {Marmolejo-Coss{\'i}o}, Francisco and Lock, Edwin and Parkes, David},
  year = {2025},
  month = jan,
  number = {arXiv:2501.14625},
  eprint = {2501.14625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14625},
  urldate = {2025-01-30},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning}
}

@article{ravindranath2021deep,
  title={Deep learning for two-sided matching},
  author={Ravindranath, Sai Srivatsa and Feng, Zhe and Li, Shira and Ma, Jonathan and Kominers, Scott D and Parkes, David C},
  journal={arXiv preprint arXiv:2107.03427},
  year={2021}
}

@article{daskalakis2024charting,
  title={Charting the Shapes of Stories with Game Theory},
  author={Daskalakis, Constantinos and Gemp, Ian and Jiang, Yanchen and Leme, Renato Paes and Papadimitriou, Christos and Piliouras, Georgios},
  journal={arXiv preprint arXiv:2412.05747},
  year={2024}
}

@article{mensfelt2024autoformalization,
  title={Autoformalization of Game Descriptions using Large Language Models},
  author={Mensfelt, Agnieszka and Stathis, Kostas and Trencsenyi, Vince},
  journal={arXiv preprint arXiv:2409.12300},
  year={2024}
}

@article{mensfelt2024autoformalizing,
  title={Autoformalizing and Simulating Game-Theoretic Scenarios using LLM-augmented Agents},
  author={Mensfelt, Agnieszka and Stathis, Kostas and Trencsenyi, Vince},
  journal={arXiv preprint arXiv:2412.08805},
  year={2024}
}

@article{deng2025natural,
  title={From Natural Language to Extensive-Form Game Representations},
  author={Deng, Shilong and Wang, Yongzhao and Savani, Rahul},
  journal={arXiv preprint arXiv:2501.17282},
  year={2025}
}

@inproceedings{ravindranathdata,
 author = {Ravindranath, Sai Srivatsa and Jiang, Yanchen and Parkes, David C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {6662--6689},
 publisher = {Curran Associates, Inc.},
 title = {Data Market Design through Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1577ea3eaf8dacb99f64e4496c3ecddf-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{wang2024gemnet,
author = {Wang, Tonghan and Jiang, Yanchen and Parkes, David C.},
title = {GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep Learning},
year = {2024},
isbn = {9798400707049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670865.3673454},
doi = {10.1145/3670865.3673454},
abstract = {Differentiable economics uses deep learning for automated mechanism design. Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\"{u}tting et al., 2024] to the multi-bidder setting. The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility). GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space. This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder. Mixed-integer linear programs are used for menu transforms and through a number of optimizations enabled by deep learning, including adaptive grids and methods to skip menu elements, we scale to large auction design problems. GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability. The full version of this paper is available at https://arxiv.org/abs/2406.07428.},
booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation},
pages = {1100},
numpages = {1},
keywords = {mechanism design, machine learning, auction design, strategy-proof microeconomics},
location = {New Haven, CT, USA},
series = {EC '24}
}

@inproceedings{Jawahar2019WhatDB,
  title={What Does BERT Learn about the Structure of Language?},
  author={Ganesh Jawahar and Beno{\^i}t Sagot and Djam{\'e} Seddah},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:195477534}
}

@article{zhong2024evaluation,
  title={Evaluation of openai o1: Opportunities and challenges of agi},
  author={Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others},
  journal={arXiv preprint arXiv:2409.18486},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}


@techreport{horton2023large,
  title={Large language models as simulated economic agents: What can we learn from homo silicus?},
  author={Horton, John J},
  year={2023},
  institution={National Bureau of Economic Research}
}

@article{park2024generative,
  title={Generative agent simulations of 1,000 people},
  author={Park, Joon Sung and Zou, Carolyn Q and Shaw, Aaron and Hill, Benjamin Mako and Cai, Carrie and Morris, Meredith Ringel and Willer, Robb and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2411.10109},
  year={2024}
}

@article{brand2023using,
  title={Using GPT for market research},
  author={Brand, James and Israeli, Ayelet and Ngwe, Donald},
  journal={Harvard Business School Marketing Unit Working Paper},
  number={23-062},
  year={2023}
}

@techreport{manning2024automated,
  title={Automated social science: Language models as scientist and subjects},
  author={Manning, Benjamin S and Zhu, Kehang and Horton, John J},
  year={2024},
  institution={National Bureau of Economic Research}
}

@article{fish2024algorithmic,
  title={Algorithmic Collusion by Large Language Models},
  author={Fish, Sara and Gonczarowski, Yannai A and Shorrer, Ran I},
  journal={arXiv preprint arXiv:2404.00806},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@misc{zhuevidence,
  title={Evidence from the Synthetic Laboratory: Language Models as Auction Participants},
  author={Zhu, Kehang and Horton, John Joseph and Jiang, Yanchen and Parkes, David C. and Shah, Anand V.},
  note={Presented at the NeurIPS 2024 Workshop on Behavioral Machine Learning},
  year={2024}
}

@phdthesis{weissteiner2023integrating,
  title={Integrating advanced machine learning methods into market mechanisms},
  author={Weissteiner, Jakob},
  year={2023},
  school={University of Zurich}
}
@inproceedings{conitzer2007eliciting,
  title={Eliciting single-peaked preferences using comparison queries},
  author={Conitzer, Vincent},
  booktitle={Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems},
  pages={1--8},
  year={2007}
}
@inproceedings{balcan2012learning,
  title={Learning valuation functions},
  author={Balcan, Maria Florina and Constantin, Florin and Iwata, Satoru and Wang, Lei},
  booktitle={Conference on Learning Theory},
  pages={4--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}
@inproceedings{zhang2020learning,
  title={Learning the Valuations of a $ k $-demand Agent},
  author={Zhang, Hanrui and Conitzer, Vincent},
  booktitle={International Conference on Machine Learning},
  pages={11066--11075},
  year={2020},
  organization={PMLR}
}
@article{lock2022learning,
  title={Learning strong substitutes demand via queries},
  author={Lock, Edwin and Goldberg, Paul W and Marmolejo-Coss{\'\i}o, Francisco},
  journal={ACM Transactions on Economics and Computation},
  volume={10},
  number={2},
  pages={1--22},
  year={2022},
  publisher={ACM New York, NY}
}
@misc{maruo2024efficientpreferenceelicitationiterative,
      title={Efficient Preference Elicitation in Iterative Combinatorial Auctions with Many Participants}, 
      author={Ryota Maruo and Hisashi Kashima},
      year={2024},
      eprint={2403.19075},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2403.19075}, 
}

@inproceedings{estermann2023deep,
  title={Deep Learning-Powered Iterative Combinatorial Auctions with Active Learning},
  author={Estermann, Benjamin and Kramer, Stefan and Wattenhofer, Roger and Wang, Ye},
  booktitle={Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages={2919--2921},
  year={2023}
}


@article{Soumalias2024MLCCA, title={Machine Learning-Powered Combinatorial Clock Auction}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28850}, DOI={10.1609/aaai.v38i9.28850}, abstractNote={We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders’ preferences via value queries (i.e., “What is your value for the bundle {A, B}?’’). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., “At prices p, what is your most preferred bundle of items?’’). We make two key technical contributions: First, we present a novel method for training an ML model on demand queries. Second, based on those trained ML models, we introduce an efficient method for determining the demand query with the highest clearing potential, for which we also provide a theoretical foundation. We experimentally evaluate our ML-based demand query mechanism in several spectrum auction domains and compare it against the most established real-world ICA: the combinatorial clock auction (CCA). Our mechanism significantly outperforms the CCA in terms of efficiency in all domains, it achieves higher efficiency in a significantly reduced number of rounds, and, using linear prices, it exhibits vastly higher clearing potential. Thus, with this paper we bridge the gap between research and practice and propose the first practical ML-powered ICA.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Soumalias, Ermis Nikiforos and Weissteiner, Jakob and Heiss, Jakob and Seuken, Sven}, year={2024}, month={Mar.}, pages={9891-9900} }

@article{ongie2019function,
  title={A function space view of bounded norm infinite width relu nets: The multivariate case},
  author={Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1910.01635},
  year={2019},
  URL={https://arxiv.org/pdf/1910.01635.pdf}
}

@PHDTHESIS{HeissInductiveBias2024,
	copyright = {In Copyright - Non-Commercial Use Permitted},
	year = {2024},
	type = {Doctoral Thesis},
	author = {Heiss, Jakob},
	size = {228 p.},
	abstract = {This thesis is concerned with the inductive bias of (deep) neural networks (NNs) NN_θ ∶ X → Y. For commonly used loss functionals L, a large set of functions f ∶ X → Y minimizes L equally well. Therefore, it is up to the learning method (e.g., a NN architecture, with certain hyper-parameters including a certain training algorithm) to choose one function among all functions with sufficiently small loss. The preference underlying this choice is an inductive bias. Our main theorem derives a regularization-functional P on function space that exactly mimics the preferences of ℓ2-regularized deep ReLU-NNs with sufficiently many neurons per layer. Interestingly, we can prove that this preference structure over functions cannot be mimicked by any shallow Gaussian process (GP). This result contrasts prominent results stating that other large-width limits of NNs are equivalent to GPs [Jacot et al., 2018, Neal, 1996].We analyze the differences between “deep inductive biases” such as those we have characterized for deep ℓ2-regularized ReLU NNs (with various hyperparameters) and “shallow inductive biases” such as those of GPs (including certain large width limits of deep NNs). Both in the context of multi-task learning and in the context of uncertainty quantification, we can pinpoint these differences very precisely.From our main theory, we derived a lossless compression algorithm that can provably reduce the number of neurons of an ℓ2-regularized ReLU-NN without changing the function represented by the NN. In our numerical experiments, we can reduce the size of trained NNs by a factor of up to100 almost without changing their predictions.We also extend the theory of a NN-based learning method (PD-NJ-ODE), which can forecast irregularly, incompletely, and noisily observed time series, and discuss its inductive bias.Further, we developed multiple NN-based improvements for combinatorial auctions. To this end, we introduced a NN architecture with an inductive bias tailored to monotonic value functions. Additionally, we introduced an uncertainty quantification method for NNs and used it for exploration in the spirit of Bayesian optimization. Moreover, we developed a new training algorithm that can deal with loss functionals, which enforce global linear inequalities.},
	keywords = {Deep Learning; inductive bias; Multi-task learning; Machine Learning; Machine learning (artificial intelligence); Machine Learning (stat.ML); neural networks; Deep ReLU neural networks; deep neural networks; Bayesian neural networks; artificial intelligence; regularization; generalization; Overparametrized Neural Networks; Large width limit; Infinite width limit; Uncertainty Quantification; uncertainty; Epistemic uncertainty; Model uncertainity; Size Reduction of Neural Networks; Market Design; auction; Auctions and market-based systems; optimization; Bayesian optimization; Black box optimisation; monotone regression; TIME SERIES ANALYSIS (MATHEMATICAL STATISTICS); time series; Neural ODE; stochastic filtering; forecasting and prediction; implicit regularization; gradient descent; spline; Deep learning architectures and techniques},
	language = {en},
	address = {Zurich},
	publisher = {ETH Zurich},
	DOI = {10.3929/ethz-b-000699241},
	title = {Inductive Bias of Neural Networks and Selected Applications},
	school = {ETH Zurich},
	URL = {https://www.research-collection.ethz.ch/handle/20.500.11850/699241}
}

@inproceedings{williams2019gradient,
  title={Gradient dynamics of shallow univariate relu networks},
  author={Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8378--8387},
  year={2019},
  URL={http://papers.nips.cc/paper/9046-gradient-dynamics-of-shallow-univariate-relu-networks.pdf}
}

@article{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1902.05040},
  year={2019},
  URL={https://arxiv.org/abs/1902.05040}
}

@article{parhi2022kinds,
  title={What kinds of functions do deep neural networks learn? Insights from variational spline theory},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={464--489},
  year={2022},
  publisher={SIAM}
}

@book{Rockafellar1970,
url = {https://doi.org/10.1515/9781400873173},
title = {Convex Analysis},
author = {Ralph Tyrell Rockafellar},
publisher = {Princeton University Press},
address = {Princeton},
doi = {doi:10.1515/9781400873173},
isbn = {9781400873173},
year = {1970},
lastchecked = {2023-06-15}
}

@phdthesis{Bertsekas1971ControlOU,
  title={Control of uncertain systems with a set-membership description of the uncertainty},
  author={Bertsekas, Dimitri P.},
  year={1971},
  school={Massachusetts Institute of Technology}
}

@book{bertsekas1999nonlinear,
  title={Nonlinear Programming},
  author={Bertsekas, Dimitri P.},
  isbn={9781886529007},
  lccn={99732088},
  series={Athena scientific optimization and computation series},
  url={https://books.google.ch/books?id=TgMpAQAAMAAJ},
  year={1999},
  publisher={Athena Scientific}
}

@COMMENT { BibTex package created from National Library of Australia Catalogue https://catalogue.nla.gov.au  }
@Book{Danskin1967nla.cat-vn2047560,
author = { Danskin, John M. },
title = { The theory of Max-Min and its application to weapons allocation problems [by] John M. Danskin },
publisher = { Springer-Verlag Berlin, New York },
pages = { viii, 126 p. },
year = { 1967 },
type = { Book },
language = { English },
subjects = { Games of strategy (Mathematics); Maxima and minima. },
life-dates = { 1967 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn2047560 },
}


@misc{EfficientAndScalableBNNsWithRank1Factors,
    doi = {10.48550/ARXIV.2005.07186},
    url = {https://arxiv.org/abs/2005.07186},
    author = {Dusenberry, Michael W. and Jerfel, Ghassen and Wen, Yeming and Ma, Yi-An and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{sandholm2006preference,
  title={Preference elicitation in combinatorial auctions},
  author={Sandholm, Tuomas and Boutilier, Craig},
  journal={Combinatorial auctions},
  volume={10},
  year={2006}
}

@article{de2021greed,
  title={Greed is good: Exploration and exploitation trade-offs in Bayesian optimisation},
  author={De Ath, George and Everson, Richard M and Rahat, Alma AM and Fieldsend, Jonathan E},
  journal={ACM Transactions on Evolutionary Learning and Optimization},
  volume={1},
  number={1},
  pages={1--22},
  year={2021},
  publisher={ACM New York, NY}
}

@article{guo2010gaussian,
  title={Gaussian process preference elicitation},
  author={Bonilla, Edwin V and Guo, Shengbo and Sanner, Scott},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@article{liew2016bounded,
  title={Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems},
  author={Liew, Shan Sung and Khalil-Hani, Mohamed and Bakhteri, Rabia},
  journal={Neurocomputing},
  volume={216},
  pages={718--734},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{weissteiner2022monotone,
			  title     = {Monotone-Value Neural Networks: Exploiting Preference Monotonicity in Combinatorial Assignment},
			  author    = {Weissteiner, Jakob and Heiss, Jakob and Siems, Julien and Seuken, Sven},
			  booktitle = {Proceedings of the Thirty-First International Joint Conference on
			               Artificial Intelligence, {IJCAI-22}},
			  publisher = {International Joint Conferences on Artificial Intelligence Organization},
			  pages     = {541--548},
			  year      = {2022},
			  month     = {7},
			  note      = {Main Track},
			  doi       = {10.24963/ijcai.2022/77},
			  url       = {https://doi.org/10.24963/ijcai.2022/77},
}


@article{you2017deep,
  title={Deep lattice networks and partial monotonic functions},
  author={You, Seungil and Ding, David and Canini, Kevin and Pfeifer, Jan and Gupta, Maya},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{osband2021epistemic,
  title={Epistemic Neural Networks},
  author={Osband, Ian and Wen, Zheng and Asghari, Mohammad and Ibrahimi, Morteza and Lu, Xiyuan and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2107.08924},
  year={2021}
}


@article{liu2020certified,
  title={Certified monotonic neural networks},
  author={Liu, Xingchao and Han, Xing and Zhang, Na and Liu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15427--15438},
  year={2020}
}


@article{sill1998monotonic,
  title={Monotonic networks},
  author={Sill, Joseph},
  year={1998},
  journal={Advances in Neural Information Processing Systems},
  publisher={MIT Press},
  volume={10},
  pages={661-667}
}


@article{wehenkel2019unconstrained,
  title={Unconstrained monotonic neural networks},
  author={Wehenkel, Antoine and Louppe, Gilles},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1545--1555},
  year={2019}
}


@article{weissteiner2020deep,
		 title={Deep Learning—Powered Iterative Combinatorial Auctions},
		 volume={34},
		 url={https://ojs.aaai.org/index.php/AAAI/article/view/5606},
	     DOI={10.1609/aaai.v34i02.5606},
	     abstractNote={&lt;p&gt;In this paper, we study the design of deep learning-powered iterative combinatorial auctions (ICAs). We build on prior work where preference elicitation was done via kernelized support vector regressions (SVRs). However, the SVR-based approach has limitations because it requires solving a machine learning (ML)-based winner determination problem (WDP). With expressive kernels (like gaussians), the ML-based WDP cannot be solved for large domains. While linear or quadratic kernels have better computational scalability, these kernels have limited expressiveness. In this work, we address these shortcomings by using deep neural networks (DNNs) instead of SVRs. We first show how the DNN-based WDP can be reformulated into a mixed integer program (MIP). Second, we experimentally compare the prediction performance of DNNs against SVRs. Third, we present experimental evaluations in two medium-sized domains which show that even ICAs based on relatively small-sized DNNs lead to higher economic efficiency than ICAs based on kernelized SVRs. Finally, we show that our DNN-powered ICA also scales well to very large CA domains.&lt;/p&gt;},
	     number={02},
	     journal={Proceedings of the AAAI Conference on Artificial Intelligence},
	     author={Weissteiner, Jakob and Seuken, Sven},
	     year={2020},
	     month={Apr.},
	     pages={2284-2293}
      }

@article{weissteiner2023bayesian,
		 title={Bayesian Optimization-based Combinatorial Assignment},
		 volume={37},
		 abstractNote={We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian Optimization-based Combinatorial Assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism to properly explore (and not just exploit) the bundle space during its preference elicitation phase. We run computational experiments in several spectrum auction domains to evaluate BOCA's performance. Our results show that BOCA achieves higher allocative efficiency than state-of-the-art approaches.},
		 journal={Proceedings of the AAAI Conference on Artificial Intelligence},
		 author={Weissteiner, Jakob and Heiss, Jakob and Siems, Julien and Seuken, Sven},
		 year={2023},
  }

@inproceedings{masterov2015canary,
  title={Canary in the e-commerce coal mine: Detecting and predicting poor experiences using buyer-to-seller messages},
  author={Masterov, Dimitriy V and Mayer, Uwe F and Tadelis, Steven},
  booktitle={Proceedings of the Sixteenth ACM Conference on Economics and Computation},
  pages={81--93},
  year={2015}
}
@article{bansak2018improving,
  title={Improving refugee integration through data-driven algorithmic assignment},
  author={Bansak, Kirk and Ferwerda, Jeremy and Hainmueller, Jens and Dillon, Andrea and Hangartner, Dominik and Lawrence, Duncan and Weinstein, Jeremy},
  journal={Science},
  volume={359},
  number={6373},
  pages={325--329},
  year={2018},
  publisher={American Association for the Advancement of Science}
}
@book{milgrom201923,
  title={23. How Artificial Intelligence and Machine Learning Can Impact Market Design},
  author={Milgrom, Paul R and Tadelis, Steven},
  year={2019},
  publisher={University of Chicago Press}
}

@Article{ausubel2017practical,
  author    = {Ausubel, Lawrence M and Baranov, Oleg},
  title     = {A practical guide to the combinatorial clock auction},
  journal   = {Economic Journal},
  year      = {2017},
  volume    = {127(605)},
  pages     = {F334-F350},
  publisher = {Oxford University Press Oxford, UK},
}

@article{Srinivas_2012,
   title={Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting},
   volume={58},
   ISSN={1557-9654},
   url={http://dx.doi.org/10.1109/TIT.2011.2182033},
   DOI={10.1109/tit.2011.2182033},
   number={5},
   journal={IEEE Transactions on Information Theory},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias W.},
   year={2012},
   month={May},
   pages={3250–3265}
}

@article{gomez2018automatic,
  title={Automatic chemical design using a data-driven continuous representation of molecules},
  author={G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n},
  journal={ACS central science},
  volume={4},
  number={2},
  pages={268--276},
  year={2018},
  publisher={ACS Publications}
}

@article{martinez2009bayesian,
  title={A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot},
  author={Martinez-Cantin, Ruben and De Freitas, Nando and Brochu, Eric and Castellanos, Jos{\'e} and Doucet, Arnaud},
  journal={Autonomous Robots},
  volume={27},
  number={2},
  pages={93--103},
  year={2009},
  publisher={Springer}
}

@article{ghahramani2015probabilistic,
  title={Probabilistic machine learning and artificial intelligence},
  author={Ghahramani, Zoubin},
  journal={Nature},
  volume={521},
  number={7553},
  pages={452--459},
  year={2015},
  publisher={Nature Publishing Group}
}
@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{DBLP:journals/corr/AmodeiOSCSM16,
  author    = {Dario Amodei and
               Chris Olah and
               Jacob Steinhardt and
               Paul F. Christiano and
               John Schulman and
               Dan Man{\'{e}}},
  title     = {Concrete Problems in {AI} Safety},
  journal   = {CoRR},
  volume    = {abs/1606.06565},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.06565},
  archivePrefix = {arXiv},
  eprint    = {1606.06565},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AmodeiOSCSM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{LeCun2015,
	risfield_0_da = {2015/05/01},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	doi = {10.1038/nature14539},
	issn = {1476-4687},
	journal = {Nature},
	number = {7553},
	pages = {436–444},
	title = {Deep learning},
	volume = {521},
	year = {2015},
	url={https://www.nature.com/articles/nature14539}
}

@article{rassenti1982combinatorial,
	title={A combinatorial auction mechanism for airport time slot allocation},
	author={Rassenti, Stephen J and Smith, Vernon L and Bulfin, Robert L},
	journal={The Bell Journal of Economics},
	pages={402--417},
	year={1982},
	publisher={JSTOR}
}
@article{cramton2013spectrumauctions,
	title={Spectrum auction design.},
	author={Cramton, Peter},
	journal = {Review of Industrial Organization},
	volume = {42},
	number = {2},
	pages = {161-190},
	year={2013}
}
@InBook{bichler2006procurement,
  chapter   = {Industrial procurement auctions},
  pages     = {593-612},
  title     = {Combinatorial Auctions},
  publisher = {MIT Press},
  year      = {2006},
  author    = {Bichler, M and Davenport, A and Hohner, G and Kalagnanam, Jayant},
  editor    = {Peter Cramton, Yoav Shoham, and Richard Steinberg},
  month     = {01},
  journal   = {Combinatorial Auctions},
}

@article{nisan2006communication,
	title={The communication requirements of efficient allocations and supporting prices},
	author={Nisan, Noam and Segal, Ilya},
	journal={Journal of Economic Theory},
	volume={129},
	number={1},
	pages={192--224},
	year={2006},
	publisher={Elsevier}
}
@article{brero2021workingpaper,
  author = {Brero, Gianluca and Lubin, Benjamin and Seuken, Sven},
  title  = {Machine Learning-powered Iterative Combinatorial Auctions},
  journal = {arXiv preprint arXiv:1911.08042},
  year   = {2021},
  month   = {Jan},
}
@InProceedings{brero2018combinatorial,
  author    = {Brero, Gianluca and Lubin, Benjamin and Seuken, Sven},
  title     = {Combinatorial Auctions via Machine Learning-based Preference Elicitation.},
  booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year      = {2018},
}
@InProceedings{dutting2019optimal,
  author    = {D{\"u}tting, Paul and Feng, Zhe and Narasimhan, Harikrishna and Parkes, David C and Ravindranath, Sai Srivatsa},
  title     = {Optimal auctions through deep learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
}

@InProceedings{rahme2020permutation,
    title={A Permutation-Equivariant Neural Network Architecture For Auction Design},
    booktitle={Proceedings of the 35th AAAI Conference on Artificial Intelligence},
    author={Rahme, Jad and Jelassi, Samy and Bruno, Joan and Weinberg, S. Matthew},
    year={2021},
}


@InProceedings{brero2019fast,
  author    = {Brero, Gianluca and Lahaie, S{\'e}bastien and Seuken, Sven},
  title     = {Fast Iterative Combinatorial Auctions via Bayesian Learning},
  booktitle = {Proceedings of the 33rd AAAI Conference of Artificial Intelligence},
  year      = {2019},
}
@InProceedings{brero2018bayesian,
  author    = {Brero, Gianluca and Lahaie, S{\'e}bastien},
  title     = {A Bayesian clearing mechanism for combinatorial auctions},
  booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence},
  year      = {2018},
}

@InProceedings{heiss2022nomu,
			  title = 	 {{NOMU}: Neural Optimization-based Model Uncertainty},
			  author =       {Heiss, Jakob M and Weissteiner, Jakob and Wutte, Hanna S and Seuken, Sven and Teichmann, Josef},
			  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
			  pages = 	 {8708--8758},
			  year = 	 {2022},
			  volume = 	 {162},
			  series = 	 {Proceedings of Machine Learning Research},
			  month = 	 {17--23 Jul},
			  publisher =    {PMLR},
			  pdf = 	 {https://proceedings.mlr.press/v162/heiss22a/heiss22a.pdf},
			  url = 	 {https://proceedings.mlr.press/v162/heiss22a.html},
			  abstract = 	 {We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access to its training data. We evaluate NOMU in various regressions tasks and noiseless Bayesian optimization (BO) with costly evaluations. In regression, NOMU performs at least as well as state-of-the-art methods. In BO, NOMU even outperforms all considered benchmarks.}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}


@article{budish2011combinatorial,
  title={The combinatorial assignment problem: Approximate competitive equilibrium from equal incomes},
  author={Budish, Eric},
  journal={Journal of Political Economy},
  volume={119},
  number={6},
  pages={1061--1103},
  year={2011},
  publisher={University of Chicago Press Chicago, IL}
}
@article{budish2012multi,
  title={The multi-unit assignment problem: Theory and evidence from course allocation at Harvard},
  author={Budish, Eric and Cantillon, Estelle},
  journal={American Economic Review},
  volume={102},
  number={5},
  pages={2237--71},
  year={2012}
}
@article{budish2017course,
  title={Course match: A large-scale implementation of approximate competitive equilibrium from equal incomes for combinatorial allocation},
  author={Budish, Eric and Cachon, G{\'e}rard P and Kessler, Judd B and Othman, Abraham},
  journal={Operations Research},
  volume={65},
  number={2},
  pages={314--336},
  year={2017},
  publisher={INFORMS}
}
@article{budish2021can,
  title={Can Market Participants Report Their Preferences Accurately (Enough)?},
  author={Budish, Eric and Kessler, Judd B},
  journal={Management Science},
  year={2021},
  publisher={INFORMS}
}


@article{bronstein2017geometric,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}

@article{brochu2010tutorial,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}


@article{cayton2005algorithms,
  title={Algorithms for manifold learning},
  author={Cayton, Lawrence},
  journal={Univ. of California at San Diego Tech. Rep},
  volume={12},
  number={1-17},
  pages={1},
  year={2005}
}

@article{devore2017data,
  title={Data assimilation and sampling in Banach spaces},
  author={DeVore, Ronald and Petrova, Guergana and Wojtaszczyk, Przemyslaw},
  journal={Calcolo},
  volume={54},
  number={3},
  pages={963--1007},
  year={2017},
  publisher={Springer}
}

@article{brahma2015deep,
  title={Why deep learning works: A manifold disentanglement perspective},
  author={Brahma, Pratik Prabhanjan and Wu, Dapeng and She, Yiyuan},
  journal={IEEE transactions on neural networks and learning systems},
  volume={27},
  number={10},
  pages={1997--2008},
  year={2015},
  publisher={IEEE}
}

@article{fefferman2016testing,
  title={Testing the manifold hypothesis},
  author={Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  journal={Journal of the American Mathematical Society},
  volume={29},
  number={4},
  pages={983--1049},
  year={2016}
}


@inproceedings{goodfellow2014gen,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {2672--2680},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}


@article{wenzel2020good,
  title={How good is the bayes posterior in deep neural networks really?},
  author={Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S and {\'S}wiatkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  journal={arXiv preprint arXiv:2002.02405},
  year={2020}
}

@article{wahba1978improper,
  title={Improper priors, spline smoothing and the problem of guarding against model errors in regression},
  author={Wahba, Grace},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={40},
  number={3},
  pages={364--372},
  year={1978},
  publisher={Wiley Online Library},
  url={https://doi.org/10.1111/j.2517-6161.1978.tb01050.x}
}


@inproceedings{10.1145/168304.168306,
author = {Hinton, Geoffrey E. and van Camp, Drew},
title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
year = {1993},
isbn = {0897916115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168304.168306},
doi = {10.1145/168304.168306},
booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory},
pages = {5–13},
numpages = {9},
location = {Santa Cruz, California, USA},
series = {COLT '93}
}


@inproceedings{nix1994estimating,
  title={Estimating the mean and variance of the target probability distribution},
  author={Nix, David A and Weigend, Andreas S},
  booktitle={Proceedings of 1994 ieee international Conference on neural networks (ICNN'94)},
  volume={1},
  pages={55--60},
  year={1994},
  organization={IEEE}
}
@book{efron1994introduction,
  title={An introduction to the bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J},
  year={1994},
  publisher={CRC press}
}



@article{chryssolouris1996confidence,
	title={Confidence Interval Prediction for Neural Network Models},
	author={Chryssolouris, George and Lee, Moshin and Ramsey, Alvin},
	journal={IEEE Transactions on Neural Networks},
	volume={7},
	number={1},
	pages={229--232},
	year={1996},
	publisher={IEEE}
}



@inproceedings{quinonero2005evaluating,
  title={Evaluating predictive uncertainty challenge},
  author={Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  booktitle={Machine Learning Challenges Workshop},
  pages={1--27},
  year={2005},
  organization={Springer}
}



@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  year={2006},
  publisher={MIT press Cambridge, MA}
}



@article{doi:10.1029/2009GL040142,
author = {Steinhilber, F. and Beer, J. and Fröhlich, C.},
title = {Total solar irradiance during the Holocene},
journal = {Geophysical Research Letters},
volume = {36},
number = {19},
pages = {},
keywords = {rec-TSI},
doi = {10.1029/2009GL040142},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2009GL040142},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2009GL040142},
year = {2009}
}


@article{khosravi2010lower,
	title={Lower Upper Bound Estimation Method for Construction of Neural Network-Based Prediction Intervals},
	author={Khosravi, Abbas and Nahavandi, Saeid and Creighton, Doug and Atiya, Amir F},
	journal={IEEE transactions on neural networks},
	volume={22},
	number={3},
	pages={337--346},
	year={2010},
	publisher={IEEE}
}



@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in neural information processing systems},
  pages={2348--2356},
  year={2011},
  url={http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf}
}



@inproceedings{hernandez2015probabilistic,
  title={Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={1861--1869},
  year={2015},
  url={http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf}
}
@InProceedings{blundell2015weight,
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  title={Weight uncertainty in neural networks},
  booktitle={32nd International Conference on Machine Learning (ICML)},
  year={2015}
}



@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems},
  pages={2378--2386},
  year={2016}
}
@InProceedings{gal2016dropout,
	title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	author={Gal, Yarin and Ghahramani, Zoubin},
	booktitle={33rd International Conference on Machine Learning (ICML)},
	pages={1050--1059},
	year={2016}
}



@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}
@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017},
  url={http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf}
}
@InBook{cheng2017maximum,
  chapter      = {Maximum resilience of artificial neural networks},
  title        = {Automated Technology for Verification and Analysis. Lecture Notes in Computer Science},
  publisher    = {Springer, Cham},
  year         = {2017},
  author       = {Cheng, Chih-Hong and N{\"u}hrenberg, Georg and Ruess, Harald},
  editor       = {D'Souza D., Narayan Kumar K.},
  volume       = {10482},
  booktitle    = {International Symposium on Automated Technology for Verification and Analysis},
  organization = {Springer},
}
@article{serra2017bounding,
	title={Bounding and counting linear regions of deep neural networks},
	author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
	journal={arXiv preprint arXiv:1711.02114},
	year={2017}
}
@InProceedings{pearce2018high,
	title={High-quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach},
	author={Pearce, Tim and Zaki, Mohamed and Brintrup, Alexandra and Neely, Andy},
	booktitle={35th International Conference on Machine Learning (ICML)},
	pages={4075-4084},
	year={2018}
}
@inproceedings{baptista2018bayesian,
  title={Bayesian optimization of combinatorial structures},
  author={Baptista, Ricardo and Poloczek, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={462--471},
  year={2018},
  organization={PMLR}
}

@Article{Fischetti2018,
	author= {Matteo Fischetti and Jason Jo},
	title= {Deep neural networks and mixed integer linear optimization},
	journal= {Constraints},
	year={2018},
	month={Jul},
	day={01},
	volume={23},
	number={3},
	pages={296--309},
	issn={1572-9354},
	doi={10.1007/s10601-018-9285-6},
	url={https://doi.org/10.1007/s10601-018-9285-6}
}

@inproceedings{singh2018fast,
	title={Fast and effective robustness certification},
	author={Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
	booktitle={Proceedings of the 32nd Conference on Neural Information Processing Systems},
	year={2018},
}



@article{heiss2019implicit1,
  title={How implicit regularization of Neural Networks affects the learned function--Part I},
  author={Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  journal={arXiv preprint arXiv:1911.02903},
  year={2019}
}
@article{wang2019function,
  title={Function space particle optimization for Bayesian neural networks},
  author={Wang, Ziyu and Ren, Tongzheng and Zhu, Jun and Zhang, Bo},
  journal={arXiv preprint arXiv:1902.09754},
  year={2019},
}
@article{emtiyaz2019approximate,
	title={Approximate Inference Turns Deep Networks into Gaussian Processes},
	author={Emtiyaz Khan, Mohammad and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
	journal={arXiv preprint arXiv:1906.01930},
	year={2019}
}
@InProceedings{pmlr-v80-kuleshov18a,
  title = 	 {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
  author =       {Kuleshov, V. and Fenner, N. and Ermon, S.},
  pages = 	 {2796--2804},
  year = 	 {2018},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/kuleshov18a.html},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  abstract = 	 {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate {—} for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.}
}

@article{zhao2020active,
      title={Active Learning for Sound Event Detection}, 
      author={Shuyang Zhao and Toni Heittola and Tuomas Virtanen},
      year={2020},
      journal={arXiv preprint arXiv:2002.05033},
}
@article{heiss2020implicit2,
  title={How implicit regularization of Neural Networks affects the learned function--Part II},
  author={Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  journal={arXiv preprint TODO XXXXXXXXXXXXXXXXXXX},
  year={2019}
}
@article{heiss2020implicit3,
  title={How implicit regularization of Neural Networks affects the learned function--Part III},
  author={Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  journal={arXiv preprint TODO XXXXXXXXXXXXXXXXXXX},
  year={2020}
}

@article{HeissPart3MultiTask,
  doi = {10.3929/ETHZ-B-000550890},
  hideurl = {https://arxiv.org/abs/2112.15577},
  author = {Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  keywords = {Multi-task learning, Machine Learning, Machine learning (artificial intelligence), Machine Learning (stat.ML), Neural Networks, Deep Learning, Deep ReLU neural networks, Deep neural networks, Bayesian Neural Networks, Bayesian neural network, artificial intelligence, machine learning, Artificial neural networks, Regularization, Tikhonov regularization, Generalization, Overparametrized Neural Networks, Large width limit, Infinite width limit, info:eu-repo/classification/ddc/004, info:eu-repo/classification/ddc/510, Data processing, computer science, Mathematics, FOS: Mathematics},
  language = {en},
  title = {How Infinitely Wide Neural Networks Can Benefit from Multi-task Learning - an Exact Macroscopic Characterization},
  publisher = {ETH Zurich},
  journal={arXiv preprint arXiv:2112.15577},
  year = {2022}
}

@article{HeissPart3Arxiv,
  author    = {Jakob Heiss and
               Josef Teichmann and
               Hanna Wutte},
  title     = {How Infinitely Wide Neural Networks Can Benefit from Multi-task Learning - an Exact Macroscopic Characterization},
  Commentjournal   = {CoRR},
journal={arXiv preprint arXiv:2112.15577},
  Commentvolume    = {abs/2112.15577},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.15577},
  eprinttype = {arXiv},
  eprint    = {2112.15577},
  timestamp = {Sat, 09 Apr 2022 12:27:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-15577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
doi = {10.3929/ETHZ-B-000550890},
      archivePrefix={arXiv},
oldtitle={Infinite wide (finite depth) Neural Networks benefit from multi-task
               learning unlike shallow Gaussian Processes - an exact quantitative
               macroscopic characterization}
}

@misc{implReg2,
      title={How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer}, 
      author={Jakob Heiss and Josef Teichmann and Hanna Wutte},
      year={2023},
      eprint={2303.11454},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.11454}
}

@misc{implReg1,
    title="{How implicit regularization of Neural Networks affects the learned function {--} Part I}",
    author={Jakob Heiss and Josef Teichmann and Hanna Wutte},
    year={2019},
    month=Nov,
    eprint={1911.02903},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/1911.02903}
} 

@InProceedings{YangFeatureLearningInfiniteWidth,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  hidepdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  hideurl = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@misc{malinin2020regression,
      title={Regression Prior Networks}, 
      author={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},
      year={2020},
      eprint={2006.11590},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/pdf/2006.11590.pdf}
}

@inproceedings{malinin2018predictive,
  title={Predictive uncertainty estimation via prior networks},
  author={Malinin, Andrey and Gales, Mark},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7047--7058},
  year={2018},
  url={https://papers.nips.cc/paper/2018/file/3ea2db50e62ceefceaf70a9d9a56a6f4-Paper.pdf}
}
@misc{wen2020batchensemble,
      title={BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning}, 
      author={Yeming Wen and Dustin Tran and Jimmy Ba},
      year={2020},
      eprint={2002.06715},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/pdf/2002.06715.pdf}
}

@article{malinin2019ensemble,
  title={Ensemble distribution distillation},
  author={Malinin, Andrey and Mlodozeniec, Bruno and Gales, Mark},
  journal={arXiv preprint arXiv:1905.00076},
  year={2019}
}

@article{wenzel2020hyperparameter,
  title={Hyperparameter ensembles for robustness and uncertainty quantification},
  author={Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
  journal={arXiv preprint arXiv:2006.13570},
  year={2020}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}

@inproceedings{caruana2004ensemble,
  title={Ensemble selection from libraries of models},
  author={Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
  booktitle={Proceedings of the twenty-first international Conference on Machine learning},
  pages={18},
  year={2004}
}


@article{ovadia2019can,
  title={Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={arXiv preprint arXiv:1906.02530},
  year={2019}
}
@inproceedings{gustafsson2020evaluating,
  title={Evaluating scalable bayesian deep learning methods for robust computer vision},
  author={Gustafsson, Fredrik K and Danelljan, Martin and Schon, Thomas B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={318--319},
  year={2020}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}
@article{ashukha2020pitfalls,
  title={Pitfalls of in-domain uncertainty estimation and ensembling in deep learning},
  author={Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2002.06470},
  year={2020}
}
@inproceedings{
havasi2021training,
title={Training independent subnetworks for robust prediction},
author={Marton Havasi and Rodolphe Jenatton and Stanislav Fort and Jeremiah Zhe Liu and Jasper Snoek and Balaji Lakshminarayanan and Andrew Mingbo Dai and Dustin Tran},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=OGg9XnKxFAH}
}
@article{ober2019benchmarking,
  title={Benchmarking the neural linear model for regression},
  author={Ober, Sebastian W and Rasmussen, Carl Edward},
  journal={arXiv preprint arXiv:1912.08416},
  year={2019}
}

@article{wendler2020learning,
  title={Learning Set Functions that are Sparse in Non-Orthogonal Fourier Bases},
  author={Wendler, C. and Amrollahi, A. and Seifert, B. and Krause, A. and P{\"u}schel, M.},
  journal={arXiv preprint arXiv:2010.00439},
  year={2020}
}


@inproceedings{hassanieh2012nearly,
  title={Nearly optimal sparse fourier transform},
  author={Hassanieh, Haitham and Indyk, Piotr and Katabi, Dina and Price, Eric},
  booktitle={Proceedings of the forty-fourth annual ACM symposium on Theory of computing},
  pages={563--578},
  year={2012}
}

@inproceedings{abraham2012combinatorial,
  title={Combinatorial auctions with restricted complements},
  author={Abraham, Ittai and Babaioff, Moshe and Dughmi, Shaddin and Roughgarden, Tim},
  booktitle={Proceedings of the 13th ACM Conference on Electronic Commerce},
  pages={3--16},
  year={2012}
}


@inproceedings{djolonga2016variational,
  title={Variational inference in mixed probabilistic submodular models},
  author={Djolonga, Josip and Tschiatschek, Sebastian and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1759--1767},
  year={2016}
}


@article{bernasconi1996fourier,
  title={On the Fourier analysis of Boolean functions},
  author={Bernasconi, A and Codenotti, B and Simon, J},
  journal={preprint},
  pages={1--24},
  year={1996}
}

@book{o2014analysis,
  title={Analysis of boolean functions},
  author={O'Donnell, Ryan},
  year={2014},
  publisher={Cambridge University Press}
}

@article{scheibler2015fast,
  title={A fast Hadamard transform for signals with sublinear sparsity in the transform domain},
  author={Scheibler, Robin and Haghighatshoar, Saeid and Vetterli, Martin},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={4},
  pages={2115--2132},
  year={2015},
  publisher={IEEE}
}

@inproceedings{chakrabarty2012testing,
  title={Testing coverage functions},
  author={Chakrabarty, Deeparnab and Huang, Zhiyi},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={170--181},
  year={2012},
  organization={Springer}
}


@inproceedings{puschel2018discrete,
  title={A discrete signal processing framework for set functions},
  author={P{\"u}schel, Markus},
  booktitle={Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4359--4363},
  year={2018},
  organization={IEEE}
}


@article{puschel2020discrete,
  title={Discrete Signal Processing with Set Functions},
  author={P{\"u}schel, Markus and Wendler, Chris},
  journal={arXiv preprint arXiv:2001.10290},
  year={2020}
}


@inproceedings{stobbe2012learning,
  title={Learning {F}ourier sparse set functions},
  author={Stobbe, Peter and Krause, Andreas},
  booktitle={Artificial Intelligence and Statistics},
  pages={1125--1133},
  year={2012}
}

@inproceedings{amrollahi2019efficiently,
  title={Efficiently learning {F}ourier sparse set functions},
  author={Amrollahi, Andisheh and Zandieh, Amir and Kapralov, Michael and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15120--15129},
  year={2019}
}


@InCollection{ausubel2006clock,
	title={The clock-proxy auction: A practical combinatorial auction design},
	pages={115-138},
	booktitle     = {Combinatorial Auctions},
	publisher = {MIT Press},
	year      = {2006},
	author    = {Ausubel, Lawrence M and Cramton, Peter and Milgrom, Paul},
	editor    = {Peter Cramton and Yoav Shoham and Richard Steinberg},
}


@article{ausubel2011auction,
  author  = {Lawrence Ausubel and Peter Cramton},
  title   = {Auction design for wind rights},
  journal = {Report to Bureau of Ocean Energy Management, Regulation and Enforcement},
  year    = {2011},
}
@article{goeree2010hierarchical,
	title={Hierarchical package bidding: A paper \& pencil combinatorial auction},
	author={Goeree, Jacob K and Holt, Charles A},
	journal={Games and Economic Behavior},
	volume={70},
	number={1},
	pages={146--169},
	year={2010},
	publisher={Elsevier}
}
@article{smola2004tutorial,
	title={A tutorial on support vector regression},
	author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
	journal={Statistics and computing},
	volume={14},
	number={3},
	pages={199--222},
	year={2004},
	publisher={Springer}
}
@InCollections{parkes2006iterative,
  title   = {Iterative Combinatorial Auctions},
  pages     = {41-78},
  booktitle = {Combinatorial Auctions},
  publisher = {MIT Press},
  year      = {2006},
  author    = {David C. Parkes},
  editor    = {Peter Cramton and Yoav Shoham and Richard Steinberg},
}

@article{scheffel2012impact,
	title={On the impact of package selection in combinatorial auctions: an experimental study in the context of spectrum auction design},
	author={Scheffel, Tobias and Ziegler, Georg and Bichler, Martin},
	journal={Experimental Economics},
	volume={15},
	number={4},
	pages={667--692},
	year={2012},
	publisher={Springer}
}


@InProceedings{kingma2014adam,
  author    = {Kingma, Diederik and Ba, Jimmy},
  title     = {Adam: A Method for Stochastic Optimization},
  booktitle = {Proceedings of the 3rd International Conference on Learning Representations},
  year      = {2015},
}


@InProceedings{brero2017probably,
	title={Probably approximately efficient combinatorial auctions via machine learning},
	author={Brero, Gianluca and Lubin, Benjamin and Seuken, Sven},
	booktitle={Proceedings of the 31st AAAI Conference on Artificial Intelligence},
	year={2017}
}


@InProceedings{golowich2018deep,
  author    = {Golowich, Noah and Narasimhan, Harikrishna and Parkes, David C},
  title     = {Deep Learning for Multi-Facility Location Mechanism Design.},
  booktitle = {Proceedings of the Twenty-seventh International Joint Conference on Artificial Intelligence and the Twenty-third European Conference on Artificial Intelligence},
  year      = {2018},
  pages     = {261--267},
}
@InProceedings{narasimhan2016automated,
	title={Automated mechanism design without money via machine learning},
	author={Narasimhan, Harikrishna and Agarwal, Shivani Brinda and Parkes, David C},
	booktitle={Proceedings of the 25th International Joint Conference on Artificial Intelligence},
	year={2016}
}


@Article{dutting2015payment,
  author    = {D{\"u}tting, Paul and Fischer, Felix and Jirapinyo, Pichayut and Lai, John K and Lubin, Benjamin and Parkes, David C},
  title     = {Payment rules through discriminant-based classifiers},
  journal   = {ACM Transactions on Economics and Computation},
  year      = {2015},
  volume    = {3},
  number    = {1},
  pages     = {5},
  publisher = {ACM},
}

@InProceedings{lahaie2004applying,
  author    = {Lahaie, Sebastien M and Parkes, David C},
  title     = {Applying learning algorithms to preference elicitation},
  booktitle = {Proceedings of the 5th ACM Conference on Electronic Commerce},
  year      = {2004},
}
@article{blum2004preference,
	title={Preference elicitation and query learning},
	author={Blum, Avrim and Jackson, Jeffrey and Sandholm, Tuomas and Zinkevich, Martin},
	journal={Journal of Machine Learning Research},
	volume={5},
	pages={649--667},
	year={2004}
}


@Article{Fischetti2018,
	author= {Matteo Fischetti and Jason Jo},
	title= {Deep neural networks and mixed integer linear optimization},
	journal= {Constraints},
	year={2018},
	month={Jul},
	day={01},
	volume={23},
	number={3},
	pages={296--309},
	issn={1572-9354},
	doi={10.1007/s10601-018-9285-6},
	url={https://doi.org/10.1007/s10601-018-9285-6}
}

@InBook{cheng2017maximum,
  chapter      = {Maximum resilience of artificial neural networks},
  title        = {Automated Technology for Verification and Analysis. ATVA 2017. Lecture Notes in Computer Science},
  publisher    = {Springer, Cham},
  year         = {2017},
  author       = {Cheng, Chih-Hong and N{\"u}hrenberg, Georg and Ruess, Harald},
  editor       = {D'Souza D., Narayan Kumar K.},
  volume       = {10482},
  booktitle    = {International Symposium on Automated Technology for Verification and Analysis},
  organization = {Springer},
}
@article{serra2017bounding,
	title={Bounding and counting linear regions of deep neural networks},
	author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
	journal={arXiv preprint arXiv:1711.02114},
	year={2018}
}
@inproceedings{tjeng2018evaluating,
	title={Evaluating Robustness of Neural Networks with Mixed Integer Programming},
	author={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},
	booktitle={International Conference on Learning Representations},
	year={2019}
}
@article{camm1990cutting,
	title={Cutting big M down to size},
	author={Camm, Jeffrey D and Raturi, Amitabh S and Tsubakitani, Shigeru},
	journal={Interfaces},
	volume={20},
	number={5},
	pages={61--66},
	year={1990},
	publisher={INFORMS}
}
@article{vielma2015mixed,
	title={Mixed integer linear programming formulation techniques},
	author={Vielma, Juan Pablo},
	journal={Siam Review},
	volume={57},
	number={1},
	pages={3--57},
	year={2015},
	publisher={SIAM}
}
@inproceedings{singh2018fast,
	title={Fast and effective robustness certification},
	author={Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={10802--10813},
	year={2018}
}


@inproceedings{dobzinski2007limitations,
title = {Limitations of VCG-Based Mechanisms},
author = {Dobzinski, Shahar and Nisan, Noam},
booktitle = {Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing},
pages = {338–344},
year = {2007},
}

@inproceedings{othman2010envy,
  title={Envy Quotes and the Iterated Core-Selecting Combinatorial Auction.},
  author={Othman, Abraham and Sandholm, Tuomas},
  booktitle={AAAI},
  pages={829-835},
  year={2010}
}
@inproceedings{Zhang18Generalized,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zhilu and Sabuncu, Mert},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-19}
}
@misc{Bai22Training,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.05862},
  urldate = {2023-03-13},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}


@misc{Dwaracherla24Efficient,
  title = {Efficient Exploration for LLMs},
  author = {Dwaracherla, Vikranth and Asghari, Seyed Mohammad and Hao, Botao and Roy, Benjamin Van},
  year = {2024},
  month = jun,
  number = {arXiv:2402.00396},
  eprint = {2402.00396},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.00396},
  urldate = {2025-01-19},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@article{bradleyterry1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@inproceedings{parkes2000iterative,
  title={Iterative combinatorial auctions: Theory and practice},
  author={Parkes, David C and Ungar, Lyle H},
  booktitle={Proceedings of the seventeenth national Conference on artificial intelligence},
  year={2000},
  pages={74–81},
}

@article{levin2016properties,
  title={Properties of the combinatorial clock auction},
  author={Levin, Jonathan and Skrzypacz, Andrzej},
  journal={American Economic Review},
  volume={106},
  number={9},
  pages={2528--51},
  year={2016}
}

@article{efron2004least,
  title={Least angle regression},
  author={Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert and others},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@article{rothunver,
  title={Kidney exchange},
  author={Roth, Alvin E and S{\"o}nmez, T},
  journal={The Quarterly Journal of Economics},
  year = {2004}
}

@article{roth2005pairwise,
  title={Pairwise kidney exchange},
  author={Roth, Alvin E and S{\"o}nmez, Tayfun and {\"U}nver, M Utku},
  journal={Journal of Economic theory},
  volume={125},
  number={2},
  pages={151--188},
  year={2005},
  publisher={Elsevier}
}

@article{roth1999redesign,
  title={The redesign of the matching market for American physicians: Some engineering aspects of economic design},
  author={Roth, Alvin E and Peranson, Elliott},
  journal={American economic review},
  volume={89},
  number={4},
  pages={748--780},
  year={1999}
}
@article{roth2002economist,
  title={The economist as engineer: Game theory, experimentation, and computation as tools for design economics},
  author={Roth, Alvin E},
  journal={Econometrica},
  volume={70},
  number={4},
  pages={1341--1378},
  year={2002},
  publisher={Wiley Online Library}
}
@article{brams1979prisoners,
  title={Prisoners' dilemma and professional sports drafts},
  author={Brams, Steven J and Straffin Jr, Philip D},
  journal={The American Mathematical Monthly},
  volume={86},
  number={2},
  pages={80--88},
  year={1979},
  publisher={Taylor \& Francis}
}
@article{sonmez2010course,
  title={Course bidding at business schools},
  author={S{\"o}nmez, Tayfun and {\"U}nver, M Utku},
  journal={International Economic Review},
  volume={51},
  number={1},
  pages={99--123},
  year={2010},
  publisher={Wiley Online Library}
}
@article{abdulkadirouglu2003school,
  title={School choice: A mechanism design approach},
  author={Abdulkadiro{\u{g}}lu, Atila and S{\"o}nmez, Tayfun},
  journal={American economic review},
  volume={93},
  number={3},
  pages={729--747},
  year={2003}
}

@article{klaus2002strategy,
  title={Strategy-proofness, solidarity, and consistency for multiple assignment problems},
  author={Klaus, Bettina and Miyagawa, Eiichi},
  journal={International Journal of Game Theory},
  volume={30},
  number={3},
  pages={421--435},
  year={2002},
  publisher={Springer}
}
@article{papai2001strategyproof,
  title={Strategyproof and nonbossy multiple assignments},
  author={P{\'a}pai, Szilvia},
  journal={Journal of Public Economic Theory},
  volume={3},
  number={3},
  pages={257--271},
  year={2001},
  publisher={Wiley Online Library}
}


@inproceedings{weissteiner2022fourier,
			  title     = {Fourier Analysis-based Iterative Combinatorial Auctions},
			  author    = {Weissteiner, Jakob and Wendler, Chris and Seuken, Sven and Lubin, Ben and Püschel, Markus},
			  booktitle = {Proceedings of the Thirty-First International Joint Conference on
		               Artificial Intelligence, {IJCAI-22}},
			  publisher = {International Joint Conferences on Artificial Intelligence Organization},
			  pages     = {549--556},
			  year      = {2022},
			  month     = {7},
			  note      = {Main Track},
			  doi       = {10.24963/ijcai.2022/78},
			  url       = {https://doi.org/10.24963/ijcai.2022/78},
}

@incollection{Pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{weiss2017sats,
  title={Sats: A universal spectrum auction test suite},
  author={Weiss, Michael and Lubin, Benjamin and Seuken, Sven},
  booktitle={Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  pages={51--59},
  year={2017}
}

@inproceedings{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={International Conference on learning and intelligent optimization},
  pages={507--523},
  year={2011},
  organization={Springer}
}
@article{bichler2019designing,
  title={Designing combinatorial exchanges for the reallocation of resource rights},
  author={Bichler, Martin and Fux, Vladimir and Goeree, Jacob K},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={3},
  pages={786--791},
  year={2019},
  publisher={National Acad Sciences}
}
@inproceedings{
Loshchilov2017SGDRSG,
 title={SGDR: Stochastic Gradient Descent with Warm Restarts},
  author={I. Loshchilov and F. Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
}


@InProceedings{pmlr-v162-papalexopoulos22a,
  title = 	 {Constrained Discrete Black-Box Optimization using Mixed-Integer Programming},
  author =       {Papalexopoulos, P and Tjandraatmadja, C. and Anderson, R. and Vielma, J. P. and Belanger, D.},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {17295--17322},
  year = 	 {2022},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/papalexopoulos22a/papalexopoulos22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/papalexopoulos22a.html},
  abstract = 	 {Discrete black-box optimization problems are challenging for model-based optimization (MBO) algorithms, such as Bayesian optimization, due to the size of the search space and the need to satisfy combinatorial constraints. In particular, these methods require repeatedly solving a complex discrete global optimization problem in the inner loop, where popular heuristic inner-loop solvers introduce approximations and are difficult to adapt to combinatorial constraints. In response, we propose NN+MILP, a general discrete MBO framework using piecewise-linear neural networks as surrogate models and mixed-integer linear programming (MILP) to optimize the acquisition function. MILP provides optimality guarantees and a versatile declarative language for domain-specific constraints. We test our approach on a range of unconstrained and constrained problems, including DNA binding, constrained binary quadratic problems from the MINLPLib benchmark, and the NAS-Bench-101 neural architecture search benchmark. NN+MILP surpasses or matches the performance of black-box algorithms tailored to the constraints at hand, with global optimization of the acquisition problem running in a few minutes using only standard software packages and hardware.}
}

@inproceedings{kandasamy2017multi,
  title={Multi-fidelity bayesian optimisation with continuous approximations},
  author={Kandasamy, Kirthevasan and Dasarathy, Gautam and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  booktitle={International Conference on Machine Learning},
  pages={1799--1808},
  year={2017},
  organization={PMLR}
}

@inproceedings{Chapelle2011AnEE,
  title={An Empirical Evaluation of Thompson Sampling},
  author={Olivier Chapelle and Lihong Li},
  booktitle={NIPS},
  year={2011}
}



@inproceedings{soumalias2024machine,
author = {Soumalias, Ermis and Zamanlooy, Behnoosh and Weissteiner, Jakob and Seuken, Sven},
title = {Machine Learning-Powered Course Allocation},
year = {2024},
isbn = {9798400707049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670865.3673573},
doi = {10.1145/3670865.3673573},
abstract = {We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7\%--11\% and 17\%--29\%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM while making the upgrade process simple for universities and seamless for their students.},
booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation},
pages = {1099},
numpages = {1},
keywords = {course allocation, preference elicitation, combinatorial assignment},
location = {New Haven, CT, USA},
series = {EC '24}
}



@article{tvadauctions,
author = {Goetzendorff, Andor and Bichler, Martin and Shabalin, Pasha and Day, Robert W.},
title = {Compact Bid Languages and Core Pricing in Large Multi-item Auctions},
journal = {Management Science},
volume = {61},
number = {7},
pages = {1684-1703},
year = {2015},
doi = {10.1287/mnsc.2014.2076},
URL = { 
        https://doi.org/10.1287/mnsc.2014.2076
},
eprint = { 
        https://doi.org/10.1287/mnsc.2014.2076
}
,
    abstract = { We introduce an auction design framework for large markets with hundreds of items and complex bidder preferences. Such markets typically lead to computationally hard allocation problems. Our new framework consists of compact bid languages for sealed-bid auctions and methods to compute second-price rules such as the Vickrey–Clarke–Groves or bidder-optimal, core-selecting payment rules when the optimality of the allocation problem cannot be guaranteed. To demonstrate the efficacy of the approach for a specific, complex market, we introduce a compact bidding language for TV advertising markets and investigate the resulting winner-determination problem and the computation of core payments. For realistic instances of the respective winner-determination problems, very good solutions with a small integrality gap can be found quickly, although closing the integrality gap to find marginally better solutions or prove optimality can take a prohibitively large amount of time. Our subsequent adaptation of a constraint-generation technique for the computation of bidder-optimal core payments to this environment is a practically viable paradigm by which core-selecting auction designs can be applied to large markets with potentially hundreds of items. Such auction designs allow bidders to express their preferences with a low number of parameters, while at the same time providing incentives for truthful bidding. We complement our computational experiments in the context of TV advertising markets with additional results for volume discount auctions in procurement to illustrate the applicability of the approach in different types of large markets. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2014.2076. This paper was accepted by Lorin Hitt, information systems. }
}

@article{bikhchandani2002package,
  title={The package assignment model},
  author={Bikhchandani, Sushil and Ostroy, Joseph M},
  journal={Journal of Economic theory},
  volume={107},
  number={2},
  pages={377--406},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{poganvcic2020differentiation,
  title={Differentiation of blackbox combinatorial solvers},
  author={Pogan{\v{c}}i{\'c}, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inbook{bichler2017coalition,
title = "Coalition-based pricing in ascending combinatorial auctions",
author = "Martin Bichler and Zhen Hao and Gediminas Adomavicius",
year = "2017",
month = oct,
day = "26",
doi = "10.1017/9781316471609.025",
language = "English (US)",
isbn = "9781107135345",
pages = "493--528",
booktitle = "Handbook of Spectrum Auction Design",
publisher = "Cambridge University Press",
}

@article{kwasnica2005iterative,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/20110340},
 abstract = {In this paper we present a new improved design for multiobject auctions and report on the results of experimental tests of that design. We merge the better features of two extant but very different auction processes, the Simultaneous Multiple Round (SMR) design used by the FCC to auction the electromagnetic spectrum and the Adaptive User Selection Mechanism (AUSM) of Banks et al. (1989, "Allocating uncertain and unresponsive resources: An experimental approach," "RAND Journal of Economics," Vol. 20, No. 1, pp. 1-25). Then, by adding one crucial new feature, we are able to create a new design, the Resource Allocation Design (RAD) auction process, which performs better than both. Our experiments demonstrate that the RAD auction achieves higher efficiencies, lower bidder losses, higher net revenues, and faster times to completion without increasing the complexity of a bidder's problem.},
 author = {Anthony M. Kwasnica and John O. Ledyard and Dave Porter and Christine DeMartini},
 journal = {Management Science},
 number = {3},
 pages = {419--434},
 publisher = {INFORMS},
 title = {A New and Improved Design for Multiobject Iterative Auctions},
 urldate = {2023-06-12},
 volume = {51},
 year = {2005}
}

@article{milgrom2017designing,
  title={Designing the US incentive auction},
  author={Milgrom, Paul and Segal, Ilya},
  journal={Handbook of spectrum auction design},
  pages={803--812},
  year={2017},
  publisher={Cambridge University Press}
}


@misc{balcan2023generalization,
      title={Generalization Guarantees for Multi-item Profit Maximization: Pricing, Auctions, and Randomized Mechanisms}, 
      author={Maria-Florina Balcan and Tuomas Sandholm and Ellen Vitercik},
      year={2023},
      eprint={1705.00243},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cole_rougharden2014samplecomplexity,
author = {Cole, Richard and Roughgarden, Tim},
title = {The Sample Complexity of Revenue Maximization},
year = {2014},
isbn = {9781450327107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591796.2591867},
doi = {10.1145/2591796.2591867},
abstract = {In the design and analysis of revenue-maximizing auctions, auction performance is typically measured with respect to a prior distribution over inputs. The most obvious source for such a distribution is past data. The goal of this paper is to understand how much data is necessary and sufficient to guarantee near-optimal expected revenue.Our basic model is a single-item auction in which bidders' valuations are drawn independently from unknown and nonidentical distributions. The seller is given m samples from each of these distributions "for free" and chooses an auction to run on a fresh sample. How large does m need to be, as a function of the number k of bidders and ε 0, so that a (1 -- ε)-approximation of the optimal revenue is achievable?We prove that, under standard tail conditions on the underlying distributions, m = poly(k, 1/ε) samples are necessary and sufficient. Our lower bound stands in contrast to many recent results on simple and prior-independent auctions and fundamentally involves the interplay between bidder competition, non-identical distributions, and a very close (but still constant) approximation of the optimal revenue. It effectively shows that the only way to achieve a sufficiently good constant approximation of the optimal revenue is through a detailed understanding of bidders' valuation distributions. Our upper bound is constructive and applies in particular to a variant of the empirical Myerson auction, the natural auction that runs the revenue-maximizing auction with respect to the empirical distributions of the samples. To capture how our sample complexity upper bound depends on the set of allowable distributions, we introduce α-strongly regular distributions, which interpolate between the well-studied classes of regular (α = 0) and MHR (α = 1) distributions. We give evidence that this definition is of independent interest.},
booktitle = {Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {243–252},
numpages = {10},
keywords = {Myerson's auction, sample complexity},
location = {New York, New York},
series = {STOC '14}
}


@inproceedings{morgensternRoughgarden2015pseudodimension,
author = {Morgenstern, Jamie and Roughgarden, Tim},
title = {The Pseudo-Dimension of near-Optimal Auctions},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper develops a general approach, rooted in statistical learning theory, to learning an approximately revenue-maximizing auction from data. We introduce t-level auctions to interpolate between simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby balancing the competing demands of expressivity and simplicity. We prove that such auctions have small representation error, in the sense that for every product distribution F over bidders' valuations, there exists a t-level auction with small t and expected revenue close to optimal. We show that the set of t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning error. One consequence of our results is that, in arbitrary single-parameter settings, one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {136–144},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@article{morgensternRoughgarden2016learningSimpleAuctions,
author = {Morgenstern, Jamie and Roughgarden, Tim},
year = {2016},
month = {04},
pages = {},
title = {Learning Simple Auctions}
}

@inproceedings{lahaielubin2019adaptive,
author = {Lahaie, S\'{e}bastien and Lubin, Benjamin},
title = {Adaptive-Price Combinatorial Auctions},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329615},
doi = {10.1145/3328526.3329615},
abstract = {This work introduces a novel iterative combinatorial auction that aims to achieve both high efficiency and fast convergence across a wide range of valuation domains. We design the first fully adaptive-price combinatorial auction that gradually extends price expressivity as the rounds progress. We implement our auction design using polynomial prices and show how to detect when the current price structure is insufficient to clear the market, and how to expand the polynomial structure to guarantee progress. An experimental evaluation confirms that our auction is competitive with bundle-price auctions in regimes where these excel, namely multi-minded valuations, but also performs well in regimes favorable to linear prices, such as valuations with pairwise synergy.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {749–750},
numpages = {2},
keywords = {market clearing, nonlinear pricing, combinatorial auction},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@article{hatfield2009strategy,
  title={Strategy-proof, efficient, and nonbossy quota allocations},
  author={Hatfield, John William},
  journal={Social Choice and Welfare},
  volume={33},
  number={3},
  pages={505--515},
  year={2009},
  publisher={Springer}
}

@misc{canadianCCA,
author = {Industry\;Canada},
year = {2013},
month = {},
pages = {},
title = {"Responses to Clarification Questions on the Licensing Framework for Mobile Broadband Services (MBS) — 700 MHz Band"}
}

@article{ausubel2014,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/42920978},
 author = {Lawrence M. Ausubel and Oleg V. Baranov},
 journal = {The American Economic Review},
 number = {5},
 pages = {446--451},
 publisher = {American Economic Association},
 title = {Market Design and the Evolution of the Combinatorial Clock Auction},
 urldate = {2023-08-17},
 volume = {104},
 year = {2014}
}

@article{ausubel2020,
author = {Ausubel, Lawrence M. and Baranov, Oleg},
title = {Revealed Preference and Activity Rules in Dynamic Auctions},
journal = {International Economic Review},
volume = {61},
number = {2},
pages = {471-502},
doi = {https://doi.org/10.1111/iere.12431},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/iere.12431},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/iere.12431},
abstract = {Abstract Activity rules—constraints that limit bidding in future rounds based on past bids—are intended to limit strategic bidding delays in high-stakes auctions. This article provides a general treatment of activity rules. Traditional point-based rules are effective for homogeneous goods and reasonably suited for substitute goods. However, they are simultaneously too strong and too weak for general environments; they allow parking, while sometimes preventing straightforward bidding. We prove that the activity rule operationalizing the generalized axiom of revealed preference (GARP) is essentially the unique rule that enforces the Law of Demand while enabling straightforward bidding and never producing “dead ends.”},
year = {2020}
}

@misc{ausubel2019iterative,
  title={Iterative Vickrey Pricing in Dynamic Auctions},
  author={Ausubel, Lawrence M and Baranov, Oleg},
year = {2019}
}

@misc{canada_3800mhz_auction_2023,
  title        = {3800 MHz Auction - Provisional Results},
  author       = {{Innovation, Science and Economic Development Canada}},
  year         = {2023},
  url          = {https://ised-isde.canada.ca/site/spectrum-management-telecommunications/en/spectrum-allocation/3800-mhz-auction-provisional-results#t1},
  note         = {Accessed: 2024-10-08},
}


@misc{soumalias2024pricesbidsvalueseverything,
      title={Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All}, 
      author={Ermis Soumalias and Jakob Heiss and Jakob Weissteiner and Sven Seuken},
      year={2025},
      eprint={2411.09355},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2411.09355}, 
}

@article{ghosh2015,
author = {Ghosh, Aritra and Manwani, Naresh and Sastry, P.S.},
title = {Making risk minimization tolerant to label noise},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {160},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.09.081},
doi = {10.1016/j.neucom.2014.09.081},
abstract = {In many applications, the training data, from which one needs to learn a classifier, is corrupted with label noise. Many standard algorithms such as SVM perform poorly in the presence of label noise. In this paper we investigate the robustness of risk minimization to label noise. We prove a sufficient condition on a loss function for the risk minimization under that loss to be tolerant to uniform label noise. We show that the 0-1 loss, sigmoid loss, ramp loss and probit loss satisfy this condition though none of the standard convex loss functions satisfy it. We also prove that, by choosing a sufficiently large value of a parameter in the loss function, the sigmoid loss, ramp loss and probit loss can be made tolerant to non-uniform label noise also if we can assume the classes to be separable under noise-free data distribution. Through extensive empirical studies, we show that risk minimization under the 0-1 loss, the sigmoid loss and the ramp loss has much better robustness to label noise when compared to the SVM algorithm.},
journal = {Neurocomput.},
month = jul,
pages = {93–107},
numpages = {15},
keywords = {Classification, Label noise, Loss function, Noise tolerance, Risk minimization}
} 


@inproceedings{duetting2024mdforllms,
author = {D\"{u}tting, Paul and Mirrokni, Vahab and Paes Leme, Renato and Xu, Haifeng and Zuo, Song},
title = {Mechanism Design for Large Language Models},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645511},
doi = {10.1145/3589334.3645511},
abstract = {We investigate auction mechanisms to support the emerging format of AI-generated content. We in particular study how to aggregate several LLMs in an incentive compatible manner. In this problem, the preferences of each agent over stochastically generated contents are described/encoded as an LLM. A key motivation is to design an auction format for AI-generated ad creatives to combine inputs from different advertisers. We argue that this problem, while generally falling under the umbrella of mechanism design, has several unique features. We propose a general formalism---the token auction model---for studying this problem. A key feature of this model is that it acts on a token-by-token basis and lets LLM agents influence generated contents through single dimensional bids.We first explore a robust auction design approach, in which all we assume is that agent preferences entail partial orders over outcome distributions. We formulate two natural incentive properties, and show that these are equivalent to a monotonicity condition on distribution aggregation. We also show that for such aggregation functions, it is possible to design a second-price auction, despite the absence of bidder valuation functions. We then move to designing concrete aggregation functions by focusing on specific valuation forms based on KL-divergence, a commonly used loss function in LLM. The welfare-maximizing aggregation rules turn out to be the weighted (log-space) convex combination of the target distributions from all participants. We conclude with experimental results in support of the token auction formulation.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {144–155},
numpages = {12},
keywords = {large language models, mechanism design, online advertising},
location = {Singapore, Singapore},
series = {WWW '24}
}


@misc{soumalias2024truthful,
      title={Truthful Aggregation of LLMs with an Application to Online Advertising}, 
      author={Ermis Soumalias and Michael J. Curry and Sven Seuken},
      year={2024},
      eprint={2405.05905},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2405.05905}, 
}

@misc{mohammad2024rag,
      title={Ad Auctions for LLMs via Retrieval Augmented Generation}, 
      author={MohammadTaghi Hajiaghayi and Sébastien Lahaie and Keivan Rezaei and Suho Shin},
      year={2024},
      eprint={2406.09459},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2406.09459}, 
}

@misc{bergemann2024datadrivenmd,
      title={Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information}, 
      author={Dirk Bergemann and Marek Bojko and Paul Dütting and Renato Paes Leme and Haifeng Xu and Song Zuo},
      year={2024},
      eprint={2412.16132},
      archivePrefix={arXiv},
      primaryClass={econ.TH},
      url={https://arxiv.org/abs/2412.16132}, 
}