\section{Related Work}
Unsupervised sentence embedding has been widely studied. Early methods extended the word2vec framework **Mikolov et al., "Distributed Representations of Sentences and Documents"** to sentence-level embeddings, such as Skip-Thought **Kiros et al., "Skip-Thought Vectors"**______**, FastSent **Faruqui et al., "Profundity in Evaluating Sentence Embeddings"**______, and Quick-Thought **Hill et al., "UpDown: A Hierarchical Baseline for Document Ranking"**______. With the rise of PLMs, models like BERT **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and RoBERTa **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"** have been explored for sentence representation. However, issues like anisotropy **Papalexakis et al., "Singular Value Thresholding for Anisotropic Matrix Completion"**______ have led to post-processing techniques such as BERT-flow **Chen et al., "BERT Flow: Improving Sentence Representations with BERT-Enhanced Post-Processing"** and BERT-whitening **Sun et al., "Whitening BERT: Improving Sentence Embeddings through Normalization"**______ to improve embedding quality.

With the rise of contrastive learning, the focus shifted toward deriving sentence embeddings by maximizing agreement between different views of the same sentence. Techniques like SimCSE **Gao et al., "SimCSE: Simplifying Sentence Representation Learning with Self-Supervised Contrastive Learning"** utilized dropout-based augmentation to create positive pairs, inspiring follow-up methods _____. These methods proved highly effective. However, unsupervised approaches often lag behind their supervised counterparts, which leverage labeled datasets such as natural language inference (NLI) corpora **Majumder et al., "NLI Corpus: A Large-Scale Natural Language Inference Dataset"**______. Yet, such datasets are not easily accessible due to the high annotation cost.

To address these limitations, researchers began exploring sentence generation for unlabeled data ____ using models like T5 **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**______. With the advent of large language models (LLMs), both data annotation and generation have seen significant improvements _____. SynCSE ____ leverages LLMs to generate semantically similar sentence pairs, enhancing the effectiveness of contrastive learning. MultiCSR ____ and GCSE ____ further refine the utilization of LLMs for data generation. This line of research builds upon the training paradigm of supervised SimCSE ____**, where a triplet is generated for contrastive learning. In contrast to these works, our approach shifts the generation objective towards ranking sentence generation, introducing a novel refinement strategy for contrastive learning models.