\section{Related Work}
Unsupervised sentence embedding has been widely studied. Early methods extended the word2vec framework \cite{mikolov2013distributed} to sentence-level embeddings, such as Skip-Thought \cite{kiros2015skip}, FastSent \cite{hill2016learning}, and Quick-Thought \cite{logeswaran2018efficient}. With the rise of PLMs, models like BERT \cite{kenton2019bert} and RoBERTa \cite{liu2019roberta} have been explored for sentence representation. However, issues like anisotropy \cite{ethayarajh2019contextual} have led to post-processing techniques such as BERT-flow \cite{li2020sentence} and BERT-whitening \cite{su2021whitening} to improve embedding quality.

With the rise of contrastive learning, the focus shifted toward deriving sentence embeddings by maximizing agreement between different views of the same sentence. Techniques like SimCSE \cite{gao2021simcse} utilized dropout-based augmentation to create positive pairs, inspiring follow-up methods \cite{wang2022improving,chuang2022diffcse,liu2023rankcse,jiang2022promptbert,wu2022pcl,miao2023debcse}. These methods proved highly effective. However, unsupervised approaches often lag behind their supervised counterparts, which leverage labeled datasets such as natural language inference (NLI) corpora \cite{bowman2015large,williams2018broad}. Yet, such datasets are not easily accessible due to the high annotation cost.

To address these limitations, researchers began exploring sentence generation for unlabeled data \cite{chen2022generate,ye2022progen} using models like T5 \cite{chung2024scaling}. With the advent of large language models (LLMs), both data annotation and generation have seen significant improvements \cite{gilardi2023chatgpt,alizadeh2023open,alizadeh2025open}. SynCSE \cite{zhang2023contrastive} leverages LLMs to generate semantically similar sentence pairs, enhancing the effectiveness of contrastive learning. MultiCSR \cite{wang2024large} and GCSE \cite{lai2024enhancing} further refine the utilization of LLMs for data generation. This line of research builds upon the training paradigm of supervised SimCSE \cite{gao2021simcse}, where a triplet is generated for contrastive learning. In contrast to these works, our approach shifts the generation objective towards ranking sentence generation, introducing a novel refinement strategy for contrastive learning models.