\section{Related Work}
Unsupervised sentence embedding has been widely studied. Early methods extended the word2vec framework ____ to sentence-level embeddings, such as Skip-Thought ____, FastSent ____, and Quick-Thought ____. With the rise of PLMs, models like BERT ____ and RoBERTa ____ have been explored for sentence representation. However, issues like anisotropy ____ have led to post-processing techniques such as BERT-flow ____ and BERT-whitening ____ to improve embedding quality.

With the rise of contrastive learning, the focus shifted toward deriving sentence embeddings by maximizing agreement between different views of the same sentence. Techniques like SimCSE ____ utilized dropout-based augmentation to create positive pairs, inspiring follow-up methods ____. These methods proved highly effective. However, unsupervised approaches often lag behind their supervised counterparts, which leverage labeled datasets such as natural language inference (NLI) corpora ____. Yet, such datasets are not easily accessible due to the high annotation cost.

To address these limitations, researchers began exploring sentence generation for unlabeled data ____ using models like T5 ____. With the advent of large language models (LLMs), both data annotation and generation have seen significant improvements ____. SynCSE ____ leverages LLMs to generate semantically similar sentence pairs, enhancing the effectiveness of contrastive learning. MultiCSR ____ and GCSE ____ further refine the utilization of LLMs for data generation. This line of research builds upon the training paradigm of supervised SimCSE ____, where a triplet is generated for contrastive learning. In contrast to these works, our approach shifts the generation objective towards ranking sentence generation, introducing a novel refinement strategy for contrastive learning models.