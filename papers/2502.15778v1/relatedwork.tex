\section{Related Work}
Traditional MCDM methods have been widely applied in areas such as supplier evaluation and environmental impact assessment. These methods typically rely on the judgments of human experts involving the weighing and comparison of multiple criteria. Although effective in practical applications, traditional MCDM methods can be time-consuming and resource-intensive, particularly when dealing with large datasets and complex scenarios, due to their heavy reliance on manual input. In recent years, LLMs have emerged as a promising solution to address these challenges. Several studies have explored the use of LLMs as virtual judges to automate the assessment and decision-making process~\cite{30-li2024generation,33-pasch2025llm,34-bennie2025panda}. For example, Wang and Wu proposed the application of ChatGPT for supplier evaluation, demonstrating that its performance was highly consistent with the results of manual expert evaluations~\cite{14-wang2024can}. Similarly, Svoboda et al. introduced a multi-criteria decision analysis framework for cybersecurity that combines AHP with GPT-4~\cite{31-svoboda2024enhancing}. This framework automates decision making while improving both the efficiency and reliability of decisions by using GPT-4 as a virtual expert. Furthermore, Dong et al.~\cite{32-dong2024can} introduced linguistic uncertainty estimation to improve evaluation consistency for high-confidence samples. Their findings showed that in certain tasks, the performance of LLM-based evaluations matched or even exceeded that of human experts. These studies highlight a more reliable and scalable approach to leveraging LLMs for personalized evaluations, offering innovative opportunities to automate and enhance decision-making processes. However, a single general MCDM framework should be explored to serve as a general expert in solving different MCDM tasks to reduce the dependence on domain knowledge.