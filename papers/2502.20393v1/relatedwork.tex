\section{Related Work}
\label{sec_related_work}
\textbf{Interpretability of Deep Neural Network Models}: 
Interpretability methods in DNN models can be broadly classified into post-hoc and ante-hoc methods. Post-hoc methods aim to interpret model predictions after training \citep{8237336,chen2020adapting,8354201,sattarzadeh2021integrated,yvinec2022singe,benitez2023ante,sundararajan2020many,wang2021shapley,jethani2021fastshap,wang2021shapley,NIPS2017_0060ef47,fong2017interpretable,petsiuk2018rise,montavon2019explainable}. Recent efforts have highlighted the issues with post-hoc methods and their reliability in reflecting a model's reasoning \citep{rudin2019stop,vilone2021notions,nauta2023anecdotal}. On the other hand, ante-hoc methods that jointly learn to explain and predict provide models that are inherently interpretable \citep{sokol2021explainability,benitez2023ante}. Ante-hoc methods have also been found to provide interpretatations that help make the model more robust and reliable \citep{alvarezmelis2018robust, 9964439}. We focus on this genre of methods in this work. \cite{cbms} proposed Concept Bottleneck Models (CBMs), a method that uses interpretable, human-defined concepts, combining them linearly to perform classification. CBMs also allow human interventions on concept activations \citep{shin2023closer,steinmann2023learning} to steer the final prediction of a model. \citep{kim2023concept,collins2023human,LearningConcise} obtained the intermediate semantic concepts by replacing domain experts with Large Language Models (LLMs). This allows for ease and flexibility in obtaining the concept set.
Using LLMs to obtain concepts also allow grounding of neurons in a bottleneck layer to a human-understandable concept, an issue with CBMs that was highlighted in \citep{margeloiu2021concept}. Other concept-based methods \citep{alvarezmelis2018robust,Chen_2020,kazhdan2020cme,rigotti2021attention,benitez2023ante} use a different notion of concepts based on prototype representations (see appendix \S A1).

\noindent \textbf{Concept-Based Incremental Learning}:
While Incremental Learning in standard supervised settings has been widely explored \citep{wang2023comprehensive}, Concept-Based Incremental Learning has remained largely unstudied. We identify \citep{marconato2022catastrophic} as an early effort in this direction; however, this work trains CBMs in a continual setting under an assumption that all concepts, including those required for unseen classes, are accessible from the first experience itself, which does not emulate a real-world setting. More recently, \cite{rymarczyk2023icicle} proposed an interpretable CL method that uses part-based prototypes as concepts. As mentioned earlier, our notion of concepts allows us to go beyond parts of an object category, as in CBM-based models.