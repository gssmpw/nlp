% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% add
% \usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
% \usepackage{colortbl} % for coloring table cells
% \usepackage{xcolor} % for color definitions
% \usepackage{multirow} % for multi-row cells


\title{\textsc{Primus}: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  \normalsize\textbf{Yao-Ching Yu}\thanks{\scriptsize Primary Contributor.}, 
  \textbf{Tsun-Han Chiang}\footnotemark[1]\thanks{\scriptsize Equal Contribution.}, 
  \textbf{Cheng-Wei Tsai}\footnotemark[1]\footnotemark[2], 
  \textbf{Chien-Ming Huang}\footnotemark[1]\footnotemark[2], 
  \textbf{Wen-Kwang Tsao}
 \\
 \normalsize AI Lab, TrendMicro
 \\
 \normalsize{\texttt{\{yaoching\_yu,james\_chiang,dennis\_tsai,liam\_huang,spark\_tsao\}@trendmicro.com}}
  % First Author \\
  % Affiliation / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  % \texttt{email@domain} \\\And
  % Second Author \\
  % Affiliation / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  % \texttt{email@domain} \\
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a \emph{\textbf{15.88\%}} improvement  in the aggregate score, while reasoning distillation leads to a \emph{\textbf{10\%}} gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community.\footnote{\scriptsize \textbf{For access to all datasets and model weights, please refer to this \href{https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243}{link}.}}
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have significantly advanced artificial intelligence by leveraging massive data and sophisticated neural architectures, such as \emph{ChatGPT} \cite{ouyang2022training}, \emph{Llama} \cite{dubey2024llama} and \emph{DeepSeek} \cite{guo2025deepseek}. These models excel at understanding and generating human language \cite{wei2022emergent,minaee2024large} and adapt well when collaborating with domain experts \cite{ge2023openagi}, enabling tailored applications in fields like medicine, law, and education \cite{lai2024large,zhou2023survey,yan2024practical}. Meanwhile, in cybersecurity, as cyber threats continue to evolve \cite{li2021comprehensive,ghelani2022cyber}, traditional methods such as signature- and rule-based systems are struggling to keep up. Advances in AI, particularly through LLMs, therefore offer promising new avenues for enhancing cybersecurity \cite{ferrag2024generative}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{overview.pdf}
  \caption{Overview of our training pipeline. \textsc{Primus-Pretraining}, \textsc{Primus-Instruct}, and \textsc{Primus-Reasoning} are the datasets of different training stages.}
  \label{fig:overview}
\end{figure}

Common training methods for LLMs include pre-training (PT) \cite{radford2018improving}, supervised fine-tuning (SFT) \cite{zhang2023instruction}, and reinforcement learning (RL) \cite{wang2024reinforcement}. Recent studies suggest LLMs acquire knowledge primarily during PT, and continual pre-training (CPT) \cite{wu2024continual}, which further trains pre-trained models on large amounts of unlabeled domain-specific text, can enhance their grasp of domain knowledge. In contrast, SFT may introduce hallucinations as new knowledge is learned \cite{gekhman-etal-2024-fine}. More recently, collecting reflection data from reasoning models for distillation has also become a trend \cite{huang2024o1}. Typically, obtaining a domain-specific LLM may require applying multiple training methods, as in our pipeline (Fig.\ref{fig:overview}).

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{stats_cyber_lm.pdf}
  \caption{Motivation behind \textsc{Primus}. Statistics of existing cybersecurity language models, where \emph{reasoning} means training models to reason via distillation or RL.}
  % where \emph{reasoning} refers to distilling models on reasoning data.
  \label{fig:stats_cyber_lm}
\end{figure}

The cybersecurity field has yet to fully benefit from this transformative technology, which requires domain expertise due to its broad and complex nature. Our statistics on cybersecurity LLM survey papers \cite{zhang2024llms,xu2024large} indicate that most existing research focuses on SFT to align model outputs, while PT or CPT is largely performed on non-natural language data such as assembly code \cite{jiang2023nova,wang2024clap,sun2023dexbert}, as shown in Fig.\ref{fig:stats_cyber_lm}. Clearly, these approaches have limited effectiveness in improving the general cybersecurity knowledge of LLMs. On the other hand, models pre-trained on cybersecurity knowledge \cite{park2023pretrained,ranade2021cybert,secbbert_github,aghaei2022securebert} are limited to small ones like BERT \cite{kenton2019bert}, and none of them have released datasets. To the best of our knowledge, LLMs pre-trained on cybersecurity knowledge or distilled on reasoning data from cybersecurity tasks remain \emph{unexplored}.

To address this gap, we extend prior work on domain-specific LLMs like medicine \cite{labrak-etal-2024-biomistral} and law \cite{colombo2024saullm} to cybersecurity. Our contributions are as follows:


\vspace{0.2\baselineskip}
\noindent\textbullet~\textbf{\emph{A Collection of Cybersecurity Datasets.}} We create a series of carefully curated datasets covering multiple stages of LLM training, including pre-training (\textsc{Primus-Pretraining}), instruction fine-tuning (\textsc{Primus-Instruct}), and reasoning fine-tuning (\textsc{Primus-Reasoning}), as shown in Fig.\ref{fig:overview}. Extensive ablation studies and evaluations on cybersecurity benchmarks show that these datasets can effectively improve cybersecurity capabilities. All datasets will be released under a ODC-BY license to encourage further research in the community.

\vspace{0.2\baselineskip}
\noindent\textbullet~\textbf{\emph{A Family of Cybersecurity LLMs.}} We present a family of cybersecurity LLMs designed to tackle domain-specific challenges, including \emph{Llama-Primus-Base}, a model continually pre-trained with cybersecurity knowledge text based on \emph{Llama-3.1-8B-Instruct}, achieving a \textbf{\emph{15.88\%}} improvement on aggregated cybersecurity benchmarks; \emph{Llama-Primus-Merged}, an instruction-tuned variant merged with \emph{Llama-3.1-8B-Instruct}, which retains instruction-following capability while significantly improving cybersecurity performance; and \emph{Llama-Primus-Reasoning}, which is distilled from reasoning steps with reflection generated by a larger reasoning LLM on cybersecurity tasks, providing it long-thought capabilities and yielding a \textbf{\emph{10\%}} gain on security certification. Likewise, all models will be released under an MIT license.



\section{Training Datasets}
\subsection{Overview}
We build our dataset in multiple stages. First, we collect high-quality cybersecurity texts from reputable sources to form \textsc{Primus-Seed} (Sec.\ref{sec:primus-seed}), which is valuable but covers only a small fraction of cybersecurity content on the web. To extend it, we train a cybersecurity text classifier using \textsc{Primus-Seed} as positive samples and sampled data from FineWeb~\cite{penedo2024the}, a refined version of Common Crawl~\cite{commoncrawl}, as negative samples. This classifier filters cybersecurity-related content from FineWeb, producing \textsc{Primus-FineWeb} (Sec.\ref{sec:primus-fineweb}). By combining both datasets, we derive \textsc{Primus-Pretraining}. Next, we introduce \textsc{Primus-Instruct} (Sec.\ref{sec:primus-instruct}), which contains about 1k carefully curated cybersecurity tasks and general dialogues for instruction fine-tuning (IFT). Finally, \textsc{Primus-Reasoning} (Sec.\ref{sec:primus-reasoning}) provides reasoning steps generated by a stronger reasoning LLM on cybersecurity tasks for distillation.


\subsection{\textsc{Primus-Seed}}
\label{sec:primus-seed}
\subsubsection{Composition}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{>{\raggedright\arraybackslash}p{3.4cm}rrr} % 调整列宽
    \toprule
    \textbf{Category} & \textbf{Samples} & \textbf{Tokens} & \textbf{\textit{Avg.}} \\
    \midrule
    \multicolumn{4}{c}{\emph{Web Crawl / Official Dump}} \\
    \midrule
    Cybersecurity Blogs/News & 2,946 & 9,751,002 & 3,309.9 \\
    Cybersecurity Books & 6,499 & 2,910,464 & 447.8 \\
    Cybersecurity Companies Websites & 76,919 & 65,798,561 & 855.4 \\
    Cybersecurity Wikipedia & 6,636 & 9,567,196 & 1,441.7 \\
    MITRE & 3,432 & 2,435,118 & 709.5 \\
    \midrule
    \multicolumn{4}{c}{\emph{Expert Curation}} \\
    \midrule
    Campaigns & 136 & 37,106 & 272.8 \\
    Intrusion Sets & 343 & 60,524 & 176.5 \\
    Malware & 7,301 & 1,362,681 & 186.6 \\
    Reports & 11,317 & 934,954 & 82.6 \\
    Threat Actors & 27 & 2,264 & 83.9 \\
    Tools & 238 & 19,926 & 83.7 \\
    Vulnerabilities & 559,054 & 98,006,720 & 175.3 \\
    \midrule
    \textbf{Total} & 674,848 & 190,886,516 & 282.9 \\
    \bottomrule
\end{tabular}
\caption{Token statistics of different sources in the \textsc{Primus-Seed} dataset.}
\label{table:primus-seed}
\end{table}

We collect cybersecurity text through two main approaches. First, we gather data from reputable sources via official dumps or web crawling, converting raw HTML to readable Markdown using \texttt{dom-to-semantic-markdown}\footnote{\scriptsize \href{https://github.com/romansky/dom-to-semantic-markdown}{https://github.com/romansky/dom-to-semantic-markdown}}. Second, we incorporate curated cyber threat intelligence (CTI) manually collected by threat experts. The statistics of \textsc{Primus-Seed} are summarized in Tab.\ref{table:primus-seed}.


\paragraph{Official Dump and Web Crawl.}
We specifically collect cybersecurity-related text from diverse sources, including Blogs, News, Books, Websites, Wikipedia, and MITRE, guided by prior pretraining work~\cite{aghaei2022securebert}. For \textbf{Blogs} and \textbf{News}, we select content from government agencies, standards bodies, cybersecurity companies, media, and forums. Meanwhile, \textbf{Books} cover a wide range of cybersecurity topics, and we exclude covers, tables of contents, and appendices while treating each extracted page as a separate sample. We also collect \textbf{Webpages} from well-known cybersecurity companies, which may include product descriptions, company profiles, FAQs, and API documentation. In addition, \textbf{Wikipedia} does not provide a predefined cybersecurity subset, so we perform a custom filtering process. Each Wikipedia article is associated with one or more category tags, which can be further expanded into subcategory tags. Starting from the root category "\emph{Computer Security}", we recursively traverse its subcategories, using GPT-4o to determine whether a category is cybersecurity-related\footnote{\scriptsize The prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-wiki-category-classifier})}. This process yields 375 relevant categories, from which we extract corresponding Wikipedia articles. For \textbf{MITRE}, we leverage obsidian-mitre-attack\footnote{\scriptsize \href{https://github.com/vincenzocaputo/obsidian-mitre-attack}{https://github.com/vincenzocaputo/obsidian-mitre-attack}}, which converts STIX data from the official repository into readable Markdown.

\paragraph{Expert Curation.}
Another part of the data consists of CTI manually collected by our threat experts, categorized into Campaigns, Intrusion Sets, Malware, Threat Actors, Tools, Vulnerabilities, and Reports. Experts curate intelligence from open-source intelligence (OSINT), underground forums, and honeypots. OSINT includes public cybersecurity knowledge bases (e.g., MITRE ATT\&CK, CAPEC, CVE, CWE), government advisories (e.g., CISA, Europol), and threat intelligence sharing platforms that provide structured insight into attack patterns, vulnerabilities, and emerging threats. In addition, experts monitor underground forums for discussions of cybercriminal activity, while honeypots capture real-world attack data to enhance intelligence gathering.

\subsubsection{Preprocessing Pipeline}
% CCNet FineWeb, C4
Considering the varying quality of texts from different sources, we adopt a preprocessing pipeline inspired by previous dataset works~\cite{wenzek-etal-2020-ccnet,penedo2024the,raffel2019exploringc4}. Each source undergoes a dynamic combination of the following preprocessing steps.

\paragraph{LM Filtering.} 
% KenLM
We use perplexity from a language model trained on English Wikipedia as a quality score. Specifically, we use a 5-gram KenLM language model~\cite{heafield-2011-kenlm} due to its efficiency in processing large amounts of data. With this setup, we manually inspect and determine an appropriate perplexity threshold for each source, and remove texts whose perplexity exceeds the threshold.

\paragraph{Deduplication.} 
Deduplication has been correlated with improvements in model performance~\cite{lee-etal-2022-deduplicating}. We adopt FineWeb's deduplication strategy, using a fuzzy hash-based approach with MinHash, which scales efficiently across many CPU nodes and allows tuning of similarity thresholds by adjusting the number of hashes per bucket. Specifically, we extract the 5 grams of each document and compute MinHashes using 112 hash functions, divided into 14 buckets of 8 hashes each to target documents that are at least 75\% similar. Documents that share the same 8 MinHashes in any bucket are considered duplicates.
% and duplicate clusters satisfy transitivity (i.e., if A and C, as well as B and C, are duplicates, then A, B, and C are grouped together even if A and B do not match in any bucket). One randomly selected document is retained from each duplicate cluster while the remaining duplicates are removed.

\paragraph{C4 Filtering.} We also apply the quality filters from the C4 dataset \cite{raffel2019exploringc4}. Although being smaller than FineWeb, C4 performs well on certain benchmarks and remains a common component in the pretraining mix of recent models such as LLaMA1 \cite{touvron2023llama}. Its filtering rules include dropping lines without a terminal punctuation mark, mentioning javascript, or containing "\emph{terms-of-use}"/"\emph{cookie policy}" statements, and dropping documents that are too short or contain "\emph{lorem ipsum}" or a curly bracket (\texttt{\{}). We apply all of these filters except for the terminal punctuation and curly bracket filters.

\paragraph{Heuristic Filtering.}  
In addition to the above filters, we manually inspect each source and develop heuristic rules to further remove low-quality documents and outliers. For example, text containing phrases such as "\emph{Your download will begin in a few seconds}" will be dropped.

\subsubsection{Augmentation}  

We find that some web-scraped data contains valuable information but suffers from poor readability due to irregular formatting, such as inconsistent line breaks. To address this, we adopt a rewriting approach inspired by Cosmopedia\footnote{\scriptsize \href{https://github.com/huggingface/cosmopedia}{https://github.com/huggingface/cosmopedia}}, a reproduction of the high-quality synthetic dataset used in phi-1.5~\cite{li2023textbooks}. Specifically, we prompt an LLM to rewrite the given text into a specific style, including blog posts, textbooks, and Q\&A formats\footnote{\scriptsize The prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-pt-augmentation})}. To increase diversity, the rewriting LLM is randomly selected from GPT-4o, Llama-3.1-405B-Instruct, DBRX \cite{Mosaic2024DBRX}, and Claude 3.5 Sonnet \cite{anthropic2024claude35sonnet}.




\subsection{\textsc{Primus-FineWeb}}
\label{sec:primus-fineweb}

\subsubsection{Cybersecurity Classifier}  
Despite our efforts to collect as much cybersecurity text as possible in \textsc{Primus-Seed}, it likely covers only a small fraction of the cybersecurity-related content on the internet. To further expand our dataset, we train a binary classifier based on TinyBERT~\cite{jiao-etal-2020-tinybert} to distinguish cybersecurity-related text from non-cybersecurity text and apply it to FineWeb, a cleaned dataset derived from Common Crawl. Specifically, we use \textsc{Primus-Seed} as positive samples. Since cybersecurity text is only a small fraction of the web, we randomly take ten times as many samples from FineWeb and use them as negative samples to balance the dataset.


\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fineweb-threshold-tokens.pdf}
  \caption{Cumulative token count in \textsc{FineWeb} for texts with a cybersecurity score exceeding various thresholds.}
  \label{fig:fineweb-threshold-tokens}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fineweb-cyber-ratio.pdf}
  \caption{Ratio of cybersecurity-related text across different score bins in \textsc{FineWeb}.}
  \label{fig:fineweb-cyber-ratio}
\end{figure}

We then use the classifier to score all FineWeb texts on a scale from 0 to 1, where higher scores indicate greater cybersecurity relevance. The distribution in Fig.\ref{fig:fineweb-threshold-tokens} shows that lower scores correspond to a significant increase in text volume. To determine an appropriate threshold for filtering, we first verify that \textbf{\emph{whether texts with higher scores are truly cybersecurity-related}}. To do this, we leverage GPT-4o for accurate evaluation by dividing the scores into multiple bins, with dynamically adjusted bin sizes---smaller bins for lower scores---to account for the increased volume of data in lower score ranges. We randomly sample 50 texts from each bin and prompt GPT-4o\footnote{\scriptsize The prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-fineweb-cybersecurity-classifier})} for classification. As shown in Fig.\ref{fig:fineweb-cyber-ratio}, relevant text proportions remain above 60\% at higher scores, but drop below 50\% when scores fall below 0.003. Although incorporating some general text can help mitigate catastrophic forgetting~\cite{Sun2019LAMOLLM}, we prioritize maintaining a majority of cybersecurity content. Therefore, we set the final threshold at 0.003, which corresponds to 15.3B of FineWeb data.

\subsubsection{Deduplication Analysis}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{ccrrr} % 调整列宽
    \toprule
    \textbf{Threshold} & \textbf{\textit{Dedup.}} & \textbf{Samples} & \textbf{Tokens}& \textbf{\textit{Avg.}} \\
    \midrule
    0.003 & \emph{False} & 20,345,616 & 15.30B & 751.88 \\
    0.003 & \emph{True}  & 3,386,733  & 2.57B  & 759.11 \\
    0.9   & \emph{False} & 2,017,959  & 1.21B  & 600.37 \\
    0.9   & \emph{True}  & 393,154    & 0.23B    & 584.75 \\
    \bottomrule
\end{tabular}
\caption{Statistics of token counts before and after deduplication at different thresholds in the FineWeb.}
\label{table:primus-fineweb}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fineweb-dedup-ablation.pdf}
  \caption{Comparison of deduplication on FineWeb cybersecurity data filtered at a classifier threshold 0.9.}
  \label{fig:fineweb-dedup-ablation}
\end{figure}

Upon inspecting the 15.3B dataset, we observed a significant amount of duplicate content. This occurs because FineWeb's ablation study found that deduplicating each Common Crawl snapshot separately yields better results than global deduplication, so FineWeb does not apply global deduplication. However, since our filtered dataset is much smaller, we conducted our own ablation study. Specifically, we extracted and deduplicated 1.21B tokens with a score above 0.9, reducing the number to 0.23B (pre- and post-deduplication token counts are listed in Tab.\ref{table:primus-fineweb}), and we also sampled 0.23B tokens directly from the 1.21B set as an undeduplicated control group. We pre-trained Llama-3.1-8B-Instruct for two epochs on both datasets and found that the deduplicated dataset significantly outperformed the undeduplicated one on our aggregate of multiple-choice question (MCQ) cybersecurity tasks (to be introduced in Sec.\ref{sec:benchmarks}), as shown in Fig.\ref{fig:fineweb-dedup-ablation}. Based on this observation, we finalized \textsc{Primus-FineWeb} with 2.57B deduplicated tokens filtered at a threshold of 0.003.





\subsection{\textsc{Primus-Instruct}}
\label{sec:primus-instruct}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{>{\raggedright\arraybackslash}p{5.5cm}r} % 调整列宽
    \toprule
    \textbf{Task} & \textbf{Samples} \\
    \midrule
    \multicolumn{2}{c}{\emph{Cybersecurity-related Tasks}} \\
    \midrule
    Alert Explanation & 100 \\
    Retrieved Security Doc QA & 100 \\
    Suspicious Command Analysis & 100 \\
    Security Event Query Generation & 100 \\
    Terraform Security Misconfiguration Fix & 96 \\
    \midrule
    \multicolumn{2}{c}{\emph{General (Multi-turn)}} \\
    \midrule
    General Instruction Following & 339 \\
    \bottomrule
\end{tabular}
\caption{Task distribution and corresponding sample counts in the \textsc{Primus-Instruct} dataset.}
% , categorized into cybersecurity-related tasks and multi-turn general instruction following.
\label{table:primus-instruct}
\end{table}

After pre-training, we use \textsc{Primus-Instruct} for instruction fine-tuning to restore the instruction-following capability of the model. To achieve this, we design several hundred cybersecurity tasks covering common business scenarios, including explaining detected alerts, answering questions about retrieved security documents, analyzing executed suspicious commands, generating query languages for retrieving security events, and providing security recommendations and risk assessments for Terraform configurations. Each example is answered by GPT-4o, and we further use Claude 3.5 Sonnet as a judge\footnote{\scriptsize The judge prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-primus-instruct-judge})} to discard samples with insufficiently helpful answers. In addition, we include several hundred multi-turn conversations on general topics generated by GPT-4o. As a result, these form \textsc{Primus-Instruct}, with statistics in Tab.\ref{table:primus-instruct}.



\subsection{\textsc{Primus-Reasoning}}
\label{sec:primus-reasoning}

% o1 journey part2
With the release of OpenAI's reasoning model o1, an increasing number of studies have attempted to replicate its reasoning capabilities. One widely recognized approach is distillation, where reasoning samples with \emph{self-reflection} from existing reasoning models are used to guide models in acquiring long-thought capabilities~\cite{huang2024o1,liu2024deepseek}. To this end, we select the following cybersecurity reasoning tasks from CTI-Bench~\cite{Alam2024CTIBench} and prompt o1-preview one to two times per question to generate solutions with reasoning steps and reflection\footnote{\scriptsize The prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-o1-reasoning})}, applying rejection sampling to retain only the correctly answered samples. The dataset statistics are shown in Tab.\ref{table:primus-reasoning}.    

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{7pt}
\begin{tabular}{>{\raggedright\arraybackslash}p{2.2cm}rrr} % 调整列宽
    \toprule
    \textbf{Dataset} & \textbf{Total} & \textbf{Accepted} & \textbf{\textit{Avg. Tokens}} \\
    \midrule
    CTI-MCQ & 1000 & 806 & 691.67 \\
    CTI-RCM & 1000 & 728 & 761.10 \\
    CTI-RCM-2021 & 1000 & 635 & 766.47 \\
    CTI-VSP & 1000 & 231 & 1155.83 \\
    CTI-ATE & 60 & 2 & 1313.50 \\
    \bottomrule
\end{tabular}
\caption{Statistics of the \textsc{Primus-Reasoning} dataset, distilled from o1-preview using CTI-Bench, with only accepted correct samples.}
\label{table:primus-reasoning}
\end{table}

\paragraph{CTI-RCM (Root Cause Mapping).} This task maps Common Vulnerabilities and Exposures (CVE) descriptions to Common Weakness Enumeration (CWE) categories, essentially classifying vulnerabilities. CWE consists of over 900 categories, often with subtle differences that make misclassification highly likely. The model must reason about the true root cause of the vulnerability and \textbf{\emph{infer}} the most appropriate weakness type rather than relying on textual matches.

\paragraph{CTI-VSP (Vulnerability Severity Prediction).} Given a vulnerability description, the task is to calculate its CVSS (Common Vulnerability Scoring System) score, which assesses severity. CVSS scoring dimensions include attack vectors (AV), required privileges, impact scope, and more. However, CVE descriptions often do not explicitly provide this information. The model must understand the vulnerability mechanism, \textbf{\emph{infer}} possible exploitation methods and impact scope, and map them to CVSS metrics.

\paragraph{CTI-ATE (Attack Technique Extraction).} This task extracts MITRE ATT\&CK technique IDs from a given threat behavior description. Threat descriptions are often non-standardized and context-dependent, using different terminology or embedding multiple attack techniques. The model must \textbf{\emph{reason}} about the attack process, synthesizing scattered information to identify possible tactics, techniques, and procedures (TTPs) and map them to the correct MITRE ATT\&CK technique IDs.

\paragraph{CTI-MCQ.} This task consists of multiple-choice questions based on authoritative sources and standards such as NIST, MITRE, and GDPR, and covers key CTI concepts such as threat identification, detection strategies, mitigation techniques, and best practices. While some questions focus on factual recall, our review found many require cross-concept \textbf{\emph{reasoning}}, such as inferring applicable scenarios for different attack techniques, evaluating the effectiveness of security strategies, or understanding the potential impact of certain vulnerabilities.

\section{Evaluation Protocol}  
In this section, we first introduce the cybersecurity benchmarks used to evaluate training performance (Sec.\ref{sec:benchmarks}), followed by the specific evaluation settings (Sec.\ref{sec:eval_settings}).  

\subsection{Benchmarks}  
\label{sec:benchmarks}
To assess the performance and training effectiveness of \textsc{Primus} models, we evaluate them against seven cybersecurity benchmarks to measure their robustness and comprehensive understanding of security concepts, which we describe below.

\paragraph{CISSP.} The Certified Information Systems Security Professional (CISSP) is a widely recognized certification in the field of cybersecurity. It assesses both technical expertise and managerial competence in designing, building, and managing an organization's security posture. We construct an evaluation set based on multiple-choice questions taken from the assessment tests within the CISSP learning materials.

\paragraph{CTI-Bench.} As introduced in Sec.\ref{sec:primus-reasoning}, CTI-Bench is a benchmark for evaluating the reasoning and knowledge capabilities of LLMs in CTI. It consists of several subtasks, including CTI-RCM, CTI-VSP, CTI-ATE, and CTI-MCQ, which assess a model's ability to analyze vulnerabilities, infer security risks, extract attack techniques, and understand cybersecurity concepts.

\paragraph{CyberMetric.} CyberMetric~\cite{cybermetric} is a widely recognized benchmark designed to assess LLMs' cybersecurity knowledge across multiple domains. It includes high-quality, human-verified multiple-choice questions covering cryptography, network security, penetration testing, and compliance. We select the 500-question subset for evaluation as it provides a balanced and representative assessment of cybersecurity knowledge.

\paragraph{SecEval.} SecEval~\cite{li2023seceval} is a benchmark specifically designed to assess cybersecurity knowledge. It consists of over 2,000 multiple-choice questions across nine domains, including software security, cryptography, and network security. The dataset was generated by prompting GPT-4 with authoritative sources such as open-licensed textbooks, official documentation, and industry standards. Given its broad coverage and rigorous quality control, SecEval serves as a reliable benchmark for assessing the cybersecurity proficiency of LLMs.


\subsection{Evaluation Settings} 
\label{sec:eval_settings}

We integrate the above benchmarks into the \texttt{lm-evaluation-harness}~\cite{eval-harness} to ensure a standardized evaluation process. All evaluations are performed in the same environment to ensure fairness. We adopt the following two evaluation settings to evaluate models at different stages.

\paragraph{\emph{5-shot, w/o Chain-of-Thought (CoT).}} We prepend the first five questions from the benchmark along with their answers as context before the current question, guiding the model to output the correct answer directly instead of generating free-form responses. This setting is used to evaluate models after pretraining, when output formatting is more difficult to control.

\paragraph{\emph{0-shot, w/ CoT}.} We follow the evaluation setup used in the OpenAI technical report benchmarks with \texttt{simple-eval}\footnote{\scriptsize \href{https://github.com/openai/simple-evals}{https://github.com/openai/simple-evals}}, using a standardized prompt\footnote{\scriptsize The prompt is provided in the Appx.\ref{sec:appendix-prompts} (Fig.\ref{fig:prompt-simple-evals})} that allows the model to articulate its reasoning before producing the final answer. Due to the formatting variability of CoT responses, we use GPT-4o-mini to extract the final answers before scoring.

\section{Training and Results}
\subsection{Overview}  
\label{sec:train-overview}

In this section, we present the entire training pipeline, which consists of four key stages. First, we expand the model's cybersecurity expertise and understanding through continual pre-training (Sec.\ref{sec:train-pretraining}), which reinforces key cybersecurity concepts and enables the model to provide accurate information on security threats and mitigation strategies. Next, we restore its instruction-following capability through instruction fine-tuning (Sec.\ref{sec:train-instruct-merge}), and further refine it through model merging to balance instruction-following and cybersecurity expertise. Finally, we train the model to develop reasoning capabilities on cybersecurity tasks (Sec.\ref{sec:train-reason-ft})\footnote{\scriptsize The training hyperparameters for each stage are provided in the Appx.\ref{sec:appendix-hyperparameters}}.


\subsection{Pre-Training}
\label{sec:train-pretraining}

\begin{table*}[ht]
\footnotesize
\centering
\setlength{\tabcolsep}{1.7pt} % 调整列间距
\begin{tabular}{>{\raggedright\arraybackslash}p{3.9cm} cccccccc} % 控制第一列宽度
    \toprule
    \textbf{Model} & \textbf{CISSP} & \textbf{CTI-MCQ} & \textbf{CTI-RCM} & \textbf{CTI-VSP} & \textbf{CTI-ATE} & \textbf{CyberMetric} & \textbf{SecEval} & \textbf{\textit{Agg.}} \\
    \midrule
    Llama-3.1-8B-Instruct & 0.7073 & 0.6420 & 0.5910 & 1.2712 & 0.2721 & 0.8560 & 0.4966 & 2.29\\
    \hspace{0.5mm} \textsc{+ Primus-Seed} & 0.7132 & 0.6608 & 0.6100 & 1.2848 & 0.2829 & 0.8600 & 0.4998 & 2.34$\uparrow$2.10\% \\
    \hspace{0.5mm} \textsc{+ Primus-FineWeb} & 0.7191 & 0.6600 & 0.6680 & 1.1499 & 0.3006 & 0.8620 & 0.4984 & 2.56$\uparrow$11.53\% \\
    \hspace{0.5mm} \textsc{+ Primus-Seed+FineWeb} & \textbf{0.7230} & \textbf{0.6676} & \textbf{0.6780} & \textbf{1.0912} & \textbf{0.3140} & \textbf{0.8660} & \textbf{0.5007} & \textbf{2.66$\uparrow$15.88\%} \\
    \bottomrule
\end{tabular}

\caption{Performance of continual pretraining on Llama across cybersecurity benchmarks. The last three rows indicate pretraining with \textsc{Primus-Seed}, \textsc{Primus-FineWeb}, and their combination. CTI-VSP is scored using Mean Absolute Deviation \textbf{\emph{(lower is better)}}, CTI-ATE uses F1 score, and the others use accuracy. The aggregate score \emph{(Agg.)} is the sum of all benchmarks, with CTI-VSP negated. The best results are highlighted in \textbf{bold}.}

\label{table:primus-pretraining-performance}
\end{table*}
% \footnote{\scriptsize  CTI-VSP uses Mean Absolute Error; \emph{lower is better.}}


We use Llama-3.1-8B-Instruct as our base model due to its wide community adoption and strong performance at the same parameter scale. We perform continual pre-training on two cybersecurity datasets: \textsc{Primus-Seed} (Sec.\ref{sec:primus-seed}), which consists of curated cybersecurity text, and \textsc{Primus-FineWeb} (Sec.\ref{sec:primus-fineweb}), a filtered subset of cybersecurity content from FineWeb, to expand the model's cybersecurity expertise and understanding. To assess performance improvements, we evaluate the model against the seven cybersecurity benchmarks described in Sec.\ref{sec:benchmarks} (5-shot, w/o CoT).

We train the model using the \texttt{NeMo} \cite{NVIDIA_NeMo} on four 8$\times$H200 nodes, with training hyperparameters and details provided in Appx.\ref{sec:appendix-hyperparameters}. To analyze the impact of different datasets, we conduct an ablation study by pre-training the model separately on each dataset and jointly on both for two epochs. The results in Tab.\ref{table:primus-pretraining-performance} show that pre-training on either dataset improves the cybersecurity performance in the aggregate evaluation score. However, the largest improvement, \textbf{\emph{15.88\%}}, is observed when pre-training on the combined dataset, so we adopt this model as the Llama-Primus-Base for subsequent training stages.


\subsection{Instruction Fine-Tuning and Merge}
\label{sec:train-instruct-merge}

\begin{table*}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{1.2pt} % 调整列间距
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm} ccccccccc} % 控制第一列宽度
    \toprule
    \textbf{Model} & \textbf{CISSP} & \textbf{CTI-MCQ} & \textbf{CTI-RCM} & \textbf{CTI-VSP} & \textbf{CTI-ATE} & \textbf{CyberMetric} & \textbf{SecEval} & \textbf{MT-Bench} & \textbf{\textit{Agg.}} \\
    \midrule
    Llama-3.1-8B-Instruct & 0.7073 & 0.6420 & 0.5910 & 1.2712 & 0.2721 & 0.8560 & 0.4966 & \textbf{8.3491} & 4.11 \\
    % Llama-Primus-Base & \textbf{0.7230} & \textbf{0.6676} & \textbf{0.6780} & \textbf{1.0912} & 0.3140 & \textbf{0.8660} & 0.5007 & - & - \\
    Llama-Primus-Instruct & 0.7132 & \textbf{0.6660} & \textbf{0.6660} & \textbf{1.1161} & 0.3348 & 0.8640 & 0.4943 & 7.9063 & 4.21$\uparrow$2.4\% \\
    Llama-Primus-Merged & \textbf{0.7191} & 0.6656 & 0.6620 & 1.1233 & \textbf{0.3387} & \textbf{0.8660} & \textbf{0.5062} & 8.2938 & \textbf{4.33$\uparrow$5.4\%} \\
    \bottomrule
\end{tabular}
\caption{Performance comparison of Llama, the instruction-tuned Primus model, and their merge on cybersecurity and general benchmarks. The aggregated score \emph{(Agg.)} is computed as $0.3 \times$ MT-Bench + $0.7 \times$ aggregated cybersecurity score (sum of all benchmarks except MT-Bench, with CTI-VSP negated due to the use of Mean Absolute Deviation, where lower is better). The best results are highlighted in \textbf{bold}.}
\label{table:primus-instruction-performance}
\end{table*}

While Llama-Primus-Base gains enhanced cybersecurity knowledge and understanding from pre-training, it tends to perform text completion rather than follow instructions. To address this, we further fine-tune it using the \texttt{LLaMA-Factory} \cite{zheng2024llamafactory} on 4$\times$A100 GPUs for two epochs with \textsc{Primus-Instruct} (Sec.\ref{sec:primus-instruct}), a carefully curated mixed dataset of cybersecurity tasks and general conversations, resulting in Llama-Primus-Instruct. In addition to the cybersecurity benchmarks, we also introduce MT-Bench~\cite{zheng2023judging}, a multi-turn instruction-following evaluation benchmark spanning multiple domains using GPT-4 as a judge, which scores helpfulness on a scale of 1 to 10, allowing us to evaluate the overall instruction-following performance of the model. The results are shown in Tab.\ref{table:primus-instruction-performance}, where the MT-Bench score and the aggregated cybersecurity benchmark score are further aggregated with a weight of 30/70 in the rightmost column.

Llama-Primus-Instruct maintains its advantage in cybersecurity while achieving an MT-Bench score of 7.91. However, this remains lower than the 8.35 of Llama, resulting in a limited improvement in the aggregated score (2.4\%). To mitigate this, we apply DARE-TIES~\cite{yu2024languageDARES, yadav2023tiesmerging}, a model merging technique that balances diverse capabilities---specifically, instruction-following and cybersecurity expertise in our case. We conduct a grid search over the merging ratio, setting Llama-Primus-Instruct:Llama-3.1-8B-Instruct to $(0.5+w)$:$(0.5-w)$ and varying $w$ from 0 to 0.5 in steps of 0.05. The optimal ratio that maximizes the aggregated score is found to be 0.75:0.25, with the merged model chosen as Llama-Primus-Merged. Notably, this configuration retains cybersecurity performance comparable to Llama-Primus-Instruct while restoring the MT-Bench to 8.29, almost equal to Llama, resulting in a \emph{\textbf{5.4\%}} improvement in the aggregated score.


\subsection{Reasoning Fine-Tuning}
\label{sec:train-reason-ft}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{0.5pt} % 调整列间距
\begin{tabular}{>{\raggedright\arraybackslash}p{4.1cm} cc} % 控制第一列宽度
    \toprule
    \textbf{Model} & \textbf{CISSP} & \textbf{\textit{Avg. Tokens}} \\
    \midrule
    \multicolumn{3}{c}{\emph{w/o CoT, 5-shot}} \\
    \midrule
    Llama-3.1-8B-Instruct & 0.7073 & 1 \\
    Llama-Primus-Merged & 0.7191 $\uparrow$1.67\% & 1 \\
    \midrule
    \multicolumn{3}{c}{\emph{w/ CoT, 0-shot}} \\
    \midrule
    Llama-3.1-8B-Instruct & 0.7288$\uparrow$3.03\% & 279.69 \\
    DeepSeek-R1-Distill-Llama-8B & 0.7399$\uparrow$4.61\% & 1542.10 \\
    Llama-Primus-Merged & 0.7603$\uparrow$7.49\% & 241.92 \\
    \midrule
    \multicolumn{3}{c}{\emph{Finetuned on} \textsc{Primus-Reasoning}} \\
    \midrule
    Llama-3.1-8B-Reasoning & 0.7583$\uparrow$7.21\% & 646.94 \\
    Llama-Primus-Reasoning & 0.7780$\uparrow$\textbf{10.0\%} & 726.96 \\
    \midrule
    o1-preview & 0.8035 & 1054.91 \\
    \bottomrule
\end{tabular}
\caption{Effect of \textsc{Primus-Reasoning} fine-tuning, evaluated on CISSP. $\uparrow$ indicates the percentage improvement over Llama without CoT and in the 5-shot setting. The best improvement is highlighted in \textbf{bold}.}
\label{table:cissp-token-comparison}
\end{table}

We further distill Llama-Primus-Merged using \textsc{Primus-Reasoning} (Sec.\ref{sec:primus-reasoning}), a high-quality dataset of cybersecurity task reasoning steps obtained from o1-preview, to equip it with reasoning and self-reflection capabilities. This approach has been successfully demonstrated in previous work such as S1~\cite{muennighoff2025s1} and Sky-T1~\cite{sky_t1_2025}. Since \textsc{Primus-Reasoning} is constructed from CTI-Bench tasks, we exclude them from the evaluation and choose CISSP as a representative metric, as it also emphasizes reasoning rather than just factual recall. The results are presented in Tab.\ref{table:cissp-token-comparison}.

As shown in the table, both Llama-3.1-8B-Instruct and Llama-Primus-Merged improve with CoT over direct answer generation. Notably, Llama-Primus-Merged achieves the largest gain, even outperforming DeepSeek-R1-Distill-Llama-8B\footnote{\scriptsize \href{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B}{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B}} with the fewest tokens used, suggesting that stronger cybersecurity knowledge benefits reasoning. After fine-tuning on \textsc{Primus-Reasoning} (lower part of the table), we observe that reasoning tokens usage triples while accuracy improves further, with Llama-Primus-Reasoning achieving the largest improvement \textbf{\emph{(10\%)}}. Interestingly, comparing Llama-3.1-8B-Reasoning and DeepSeek-R1-Distill-Llama-8B may suggest that domain-specific reasoning distillation yields better in-domain performance than general-domain distillation.

\section{Conclusion}
In this work, we explore the adaptation of other successful domain-specific LLM approaches to cybersecurity and contribute a series of datasets covering different stages of LLM training, including pre-training, instruction fine-tuning, and reasoning distillation, each of which has been validated to improve cybersecurity performance. To our knowledge, this is the first study to systematically strengthen the cybersecurity skills of an LLM across multiple stages of training, and we will release all datasets and models to encourage further community research.


\section*{Limitations}
Although this work covers the various stages of LLM training, it has the following limitations:

\vspace{0.2\baselineskip}
\noindent\textbullet~Due to limited computational resources, our experiments are limited to 8B scale models, leaving the effectiveness of scaling to larger models (e.g., 70B or 405B) unknown.

\vspace{0.2\baselineskip}
\noindent\textbullet~Our exploration of RL remains limited. Recent work by DeepSeek-R1 has demonstrated that GRPO \cite{zhang2024deepseekmath} combined with only rule-based rewards (e.g., correctness and format compliance) can achieve performance comparable to o1. We believe this is also a promising direction for cybersecurity applications and leave it as future work.

\section*{Ethics Statement}
We used Garak \cite{garak}, a toolkit that probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other vulnerabilities, to evaluate Llama-Primus-Merged. The results showed no significant differences compared to Llama (Appx.\ref{sec:appendix-safety}). However, we still emphasize that the user is are solely responsible for the content generated with the Primus model, as it lacks mechanisms to handle the disclosure of harmful, biased, or toxic content. Therefore, we strongly recommend that Primus be used for research purposes only. If used in production for natural language generation, users should independently assess the risks and implement appropriate safeguards.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}


\appendix
\newpage
\section{Prompts}
\label{sec:appendix-prompts}
All prompts used in this paper are summarized in Tab.\ref{table:prompt-summary}.

\begin{table*}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{3pt} % 调整列间距
\begin{tabular}{>{\raggedright\arraybackslash}p{4.5cm} p{10cm} c} % 控制列宽
    \toprule
    \textbf{Prompt} & \textbf{Description} & \textbf{\textit{Ref.}} \\
    \midrule
    Wiki Category Classifier & Classifies Wikipedia category tags as cybersecurity-related or not. & Fig.\ref{fig:prompt-wiki-category-classifier} \\
    Style-Based Text Rewriting (Blog, Textbook, Q\&A) & Rewrites text into a specific style, such as blog post, textbook, or Q\&A. & Fig.\ref{fig:prompt-pt-augmentation} \\
    Cybersecurity Classifier & Determines whether a given text is related to cybersecurity. & Fig.\ref{fig:prompt-fineweb-cybersecurity-classifier} \\
    Primus-Instruct Judge & Evaluates response quality when generating \textsc{Primus-Instruct} samples. & Fig.\ref{fig:prompt-primus-instruct-judge} \\
    Step-by-Step Reasoning Generation & Generates reasoning steps for a given query. & Fig.\ref{fig:prompt-o1-reasoning} \\
    Final Answer Generation & Produces the final answer based on the generated reasoning steps. & Fig.\ref{fig:prompt-o1-reasoning} \\
    CoT Evaluation  & Evaluates model performance under CoT. & Fig.\ref{fig:prompt-simple-evals} \\
    \bottomrule
\end{tabular}
\caption{Summary of all prompts used in the study.}
\label{table:prompt-summary}
\end{table*}


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-wiki-category-classifier.pdf}
  \caption{Prompt for classifying Wikipedia category tags into cybersecurity or non-cybersecurity.}
  \label{fig:prompt-wiki-category-classifier}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-pt-aug-blogpost.pdf}
  \vspace{0.5mm} % 可调节间距
  \includegraphics[width=0.85\textwidth]{prompt-pt-aug-textbook.pdf}
  \vspace{1mm}
  \includegraphics[width=0.85\textwidth]{prompt-pt-aug-QA.pdf}
  \caption{Prompts for augmenting text into different styles: blog post, textbook, and Q\&A format.}
  \label{fig:prompt-pt-augmentation}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-fineweb-cybersecurity-classifier.pdf}
  \caption{Prompt for classifying whether a given text is related to cybersecurity.}
  \label{fig:prompt-fineweb-cybersecurity-classifier}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-primus-instruct-judge.pdf}
  \caption{Judge prompt for evaluating response quality during \textsc{Primus-Instruct} generation.}
  \label{fig:prompt-primus-instruct-judge}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-o1-reasoning-steps.pdf}
  \vspace{1mm}
  \includegraphics[width=0.85\textwidth]{prompt-o1-final-answer.pdf}
  \caption{Prompts for step-by-step reasoning and final answer generation. The first prompt generates reasoning steps, while the second produces the final answer based on those steps.}
  \label{fig:prompt-o1-reasoning}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{prompt-simple-evals.pdf}
  \caption{Evaluation prompt for answering with CoT in OpenAI simple-evals and our paper.}
  \label{fig:prompt-simple-evals}
\end{figure*}

\section{Training Hyperparameters}
\label{sec:appendix-hyperparameters}
This section details the hyperparameters used in each training stage of our experiments.

\subsection{Pre-Training}
Framework: \texttt{NeMo} \\
Hardware: \emph{4 nodes, each with 8 $\times$ H200} \\
Training Time: \emph{30 hours (Primus-Seed+Primus-FineWeb)} \\
Epochs: \emph{2} \\
Learning Rate: \emph{1e-6} \\
Pipeline Model Parallel Size: \emph{4} \\
Tensor Model Parallel Size: \emph{8} \\
Context Parallel Size: \emph{1} \\
Global Batch Size: \emph{12} \\
Micro Batch Size: \emph{12} \\
Warmup Ratio: \emph{0.05} \\
Scheduler: \emph{Cosine Annealing} \\
Sequence Length: \emph{16,384} \\

\subsection{Instruction Fine-Tuning}
Framework: \texttt{LLaMA-Factory} \\
Hardware: \emph{4 $\times$ A100} \\
Training Time: \emph{2 hours} \\
Epochs: \emph{2} \\
Learning Rate: \emph{1e-6} \\
Deepspeed: \emph{ZeRO Stage-3 with CPU Offload} \\
Per Device Train Batch Size: \emph{1} \\
Warmup Ratio: \emph{0.1} \\
Scheduler: \emph{Cosine} \\
Cutoff Length: \emph{16,384} \\

\subsection{Reasoning Fine-Tuning}
Framework: \texttt{LLaMA-Factory} \\
Hardware: \emph{4 $\times$ A100} \\
Training Time: \emph{2.5 hour} \\
Epochs: \emph{3} \\
Learning Rate: \emph{1e-6} \\
Deepspeed: \emph{ZeRO Stage-3 with CPU Offload} \\
Per Device Train Batch Size: \emph{3} \\
Warmup Ratio: \emph{0.1} \\
Scheduler: \emph{Cosine} \\
Cutoff Length: \emph{8,192} \\

\section{Safety \& Toxicity}
\label{sec:appendix-safety}
We list Garak's test results in Tab.\ref{table:garak-safety-metrics}.

\begin{table*}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{3pt} % 调整列间距
\begin{tabular}{>{\raggedright\arraybackslash}p{4.5cm} cc} % 控制第一列宽度
    \toprule
    \textbf{Metric} & \textbf{Llama-3.1-8B-Instruct} & \textbf{Llama-Primus-Merged} \\
    \midrule
    \textit{dan (Jailbreak)} & 28.98\% & 41.70\% \\
    \textit{encoding (Jailbreak)} & 100.00\% & 100.00\% \\
    \textit{goodside (Hallucination/Injection)} & 77.08\% & 72.10\% \\
    \textit{latentinjection (Injection)} & 75.55\% & 74.00\% \\
    \textit{leakreplay (Copyright)} & 95.71\% & 96.90\% \\
    \textit{malwaregen (Disallowed)} & 14.34\% & 29.00\% \\
    \textit{realtoxicityprompts (Disallowed)} & 90.03\% & 85.40\% \\
    \textit{snowball (Hallucination)} & 59.67\% & 84.20\% \\
    \textit{xss (Injection)} & 100.00\% & 98.30\% \\
    \bottomrule
\end{tabular}
\caption{Security and toxicity scores evaluated using Garak.}
\label{table:garak-safety-metrics}
\end{table*}



\end{document}
