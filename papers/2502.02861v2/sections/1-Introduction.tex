\section{Introduction}
In recent years, advances in machine learning (ML) models have inspired researchers to revisit the design of classic online algorithms, incorporating insights from ML-based advice to improve decision-making in real-world environments. This research area, termed \emph{algorithms with predictions}, seeks to design algorithms that are both robust to worst-case inputs and achieve performance that improves with prediction accuracy (a desideratum termed \emph{consistency}) \citep{Lykouris18:Competitive}. Many prediction-aided algorithms have been developed for online decision-making tasks ranging from rent-or-buy problems like ski rental \citep{Purohit18:Improving, Anand20:Customizing, Sun24:Online} to sequencing problems like job scheduling \citep{Cho22:Scheduling}.

Algorithms in this framework often rely on \emph{global} uncertainty parameters intended to summarize the trustworthiness of \emph{all} of the model's predictions, with extreme settings indicating that predictions are either all perfect or all uninformative \citep[e.g.,][]{Mahidian07:Allocating, Lykouris18:Competitive, Purohit18:Improving, Rohatgi20:Near, Wei20:Optimal, Antoniadis20:Secretary}. However, ML models often produce \emph{local}, input-specific uncertainty estimates, exposing a disconnect between theory and practice. For instance, many neural networks provide calibrated probabilities or confidence intervals for each data point. In this paper, we demonstrate that \emph{calibration} can serve as a powerful tool to bridge this gap. An ML predictor is said to be calibrated if the probabilities it assigns to events match their observed frequencies; when the model outputs a high probability, the event is indeed likely, and when it assigns a low probability, the event rarely occurs. Calibrated predictors convey their uncertainty on each prediction, allowing decision-makers to safely rely on the model's advice, and eliminating the need for global uncertainty estimates. Moreover, calibrating models after training can be easily achieved through popular methods (e.g. Platt Scaling~\citep{platt1999probabilistic} or Histogram Binning~\citep{zadrozny2001obtaining}) and reduces overconfidence, particularly in neural network models~\citep{vasilev2023calibration}.

Although we are the first to study calibration for algorithms with predictions, \citet{Sun24:Online} proposed using \emph{conformal prediction} in this setting---a common tool in uncertainty quantification~\citep{vovk2005algorithmic, shafer2008tutorial}. Conformal predictions provide instance-specific confidence intervals that cover the true label with high probability. We prove that calibration offers key advantages over conformal prediction, especially when the predicted quantities have high variance. In extreme cases, conformal intervals can become too wide to be informative: for binary predictions, a conformal approach returns $\{0,1\}$ unless the true label is nearly certain to be $0$ or $1$. In contrast, calibration still conveys information that aids decision-making.

\subsection{Our contributions}
We demonstrate the benefit of using calibrated predictors through two case studies: the ski rental and online job scheduling problems. Theoretically, we develop and give performance guarantees for algorithms that incorporate calibrated predictions. We validate our theoretical findings with strong empirical results on real-world data, highlighting the practical benefits of our approach.

\paragraph{Ski rental.}
The \emph{ski rental problem} serves as a prototypical example of a broad family of online rent-or-buy problems, where one must choose between an inexpensive, short-term option (renting) and a more costly, long-term option (buying). In this problem, a skier will ski for an unknown number of days and, each day, must decide to either rent skis or pay a one-time cost to buy them. Generalizations of the ski rental problem have informed a broad array of practical applications in networking~\citep{Karlin01:Dynamic}, caching~\citep{Karlin88:Competitive}, and cloud computing~\citep{Khanafer13:Constrained}. We design an online algorithm for ski rental that incorporates predictions from a calibrated predictor. We prove that our algorithm achieves optimal expected prediction-level performance for general distributions over instances and calibrated predictors. At a distribution level, its performance degrades smoothly as a function of the mean-squared error and calibration error of the predictor. Moreover, we demonstrate that calibrated predictions can be more informative than the conformal predictions of \citet{Sun24:Online} when the distribution over instances has high variance that is not explained by features, leading to better performance.
   
\paragraph{Scheduling.}
We next study online scheduling in a setting where each job has an urgency level, but only a machine-learned estimate of that urgency is available. This framework is motivated by scenarios such as medical diagnostics, where machine-learning tools can flag potentially urgent cases but cannot fully replace human experts. We demonstrate that using a calibrated predictor provides significantly better guarantees than prior work~\citep{Cho22:Scheduling}, which approached this problem by ordering jobs based on the outputs of a binary predictor. We identify that this method implicitly relies on a crude form of calibration that assigns only two distinct values, resulting in many ties that must be broken randomly. In contrast, we prove that a properly calibrated predictor with finer-grained confidence levels provides a more nuanced job ordering, rigorously quantifying the resulting performance gains.

\subsection{Related work}
\paragraph{Algorithms with predictions.} There has been significant recent interest in integrating ML advice into the design of online algorithms (see, e.g., \citet{Mitzenmacher22:Algorithms} for a survey). Much of the research in this area assumes uniform uncertainty over prediction reliability \citep[e.g.,][]{Lykouris18:Competitive, Purohit18:Improving, Wei20:Optimal}. Subsequent work has studied more practical settings, assuming access to ML predictors learned from samples \citep{Anand20:Customizing}, with probabilistic correctness guarantees \citep{Gupta21:Distribution}, or in the case of binary predictions, with a known confusion matrix \citep{Cho22:Scheduling}. Recently, \citet{Sun24:Online} proposed a framework for quantifying prediction-level uncertainty based on conformal prediction. We prove that calibration provides key advantages over conformal prediction in this context, particularly when predicted quantities exhibit high variance.

\paragraph{Calibration for decision-making.}
A recent line of work examines calibration as a tool for downstream decision-making. \citet{Gopalan22:Loss} show that a multi-calibrated predictor can be used to optimize any convex, Lipschitz loss function of an action and binary label. \citet{Zhao21:Calibrating} adapt the required calibration guarantees to specific offline decision-making tasks, while \citet{Noarov23:High} extend this algorithmic framework to the online adversarial setting. Though closely related to our work, these results do not extend to the (often unwieldy) loss functions encountered in competitive analysis.