\newpage
\appendix
\onecolumn

\section{Ski Rental Proofs} \label{appendix: ski-rental-proofs}
\RobustUBs*
\begin{proof}
Recall that $B_v = \{f(X) = v\}$ is the event that $f$ predicts $v \in R(f)$, and $C = \{Z > b\}$ is the event that the true number of days skied is at least $b$. Because $f$ is a predictor of the indicator function $\mathbbm{1}_{C}$ with max calibration error $\alpha$,
\[\Pr[C \mid B_v] = \Pr[Z > b \mid f(X)=v]=v-\alpha_v \leq v + \alpha\]
and
\[\Pr[C^c \mid B_v] = \Pr[Z \leq b \mid f(X)=v]=1- v+\alpha_v \leq 1 - v + \alpha.\]
This establishes (1) and (2).
In the remainder of the proof we will reference the costs from conditions $(i)$-$(iv)$ in \cref{table: cr-landscape}.
    \begin{enumerate}
    \item[(3)] $\E[CR(\cA_k) \mid B_v, C] \leq 1 + \frac{k (v)}{b}$.
    Under the event $C$ ($Z > b$), one of conditions $(iii)$ or $(iv)$ must hold. The bound is tight when condition $(iv)$ holds. Under condition $(iii)$, it must be that $Z \leq k(v)$, so
    \[\frac{\ALG(\cA_k, f(X), Z)}{\OPT(Z)} = \frac{Z}{b} \leq \frac{k(v)}{b} \leq 1+\frac{k(v)}{b}.\]

    \item[(4)] $\E[CR(\cA_k) \mid B_v, C^c] \leq 1 + \mathbbm{1}_{\{k(v) < b\}} \cdot \frac{b}{k(v)}$. 

    Under the event $C^c$ ($Z \leq b)$, one of conditions $(i)$ or $(ii)$ hold. The bound is trivial under condition $(i)$. Under condition $(ii)$, because $k(v) < Z$ and $Z \leq b$,
    \[\frac{\ALG(\cA_k, f(X), Z)}{\OPT(Z)} = \frac{k(v) + b}{Z} \leq \frac{k(v) + b}{k(v)} =1 + \textbf{1}_{\{k(v) < b\}} \cdot \frac{b}{k(v)}.\]


\end{enumerate}
\end{proof}


\ConditionalCRUB*
\begin{proof}
Let $B_v = \{f(X) = v\}$ be the event that $f$ predicts $v \in R(f)$, and let $C = \{Z > b\}$ be the event that the true number of days skied is at least $b$.
By the law of total expectation and \cref{lemma: robust-ubs},
\begin{align*}\label{eq:cr_decompose}
    \E[\CR(\cA_k) \mid B_v] &= \Pr[C \mid B_v] \cdot \E[\CR(\cA_k) \mid C, B_v] + \Pr[C^c \mid B_v] \cdot \E[CR(\cA_k) \mid C^c, B_v] \notag\\
    &\leq (v + \alpha) \cdot \left(1 + \frac{k(v)}{b} \right) + (1 - v + \alpha) \cdot \left(1 + \mathbbm{1}_{\{k(v) < b\}} \cdot\frac{b}{k(v)} \right) \notag \\
    &=1+2\alpha +\frac{(v+\alpha)k(v)}{b} + \mathbbm{1}_{\{k(v)<b\}} \cdot \frac{(1-v+\alpha)b}{k(v)}.
\end{align*}

Finding the number of days to rent skis that minimizes this upper bound on competitive ratio amounts to solving two convex optimization problems --- one for the case $k(v) <b$, and a second for $k(v) \geq b$ --- then taking the minimizing solution.
\begin{align*}
    \text{(a)} \quad \textup{Minimize} &\quad  1 + 2\alpha + \frac{(v+\alpha)\ell}{b} + \frac{(1-v+\alpha)b}{\ell} \hspace{20mm} \text{(b)} \quad \textup{Minimize} &&\hspace{-3mm} 1 + 2\alpha + \frac{(v+\alpha)\ell}{b}\\
    \textup{s.t.} &\quad 0 \leq \ell \leq b \hspace{75mm}\textup{s.t.}&&\hspace{-3mm} \ell \geq b
\end{align*}

 Note first that (b) has optimal solution $\ell_*=b$. The Lagrangian of (a) is
\[\mathcal{L}(\ell, \lambda_1, \lambda_2) = 1 + 2\alpha + \frac{(v+\alpha)\ell}{b} + \frac{(1-v +\alpha)b}{\ell} + \lambda_1(\ell - b ) - \lambda_2\ell \]
with KKT optimality conditions
\begin{align*}
    \frac{v+\alpha}{b} - \frac{(1-v +\alpha)b}{\ell^2} + \lambda_1 - \lambda_2 &= 0 \\
    \ell &\leq b \\
    -\ell &\leq 0 \\
    \lambda_1, \lambda_2 &\geq 0 \\
    \lambda_1(\ell - b) &= 0 \\
    \lambda_2(-\ell) &= 0.
\end{align*}
We'll proceed by finding solutions to this system of equations via case analysis.
\begin{enumerate}
   \item $\lambda_2 \neq 0$. Then, $\ell=0$ and $\lambda_1 = 0$ by complementary slackness. But at least one of the stationarity or dual feasibility constraints are violated, since
    \[0 > \frac{v+\alpha}{b} - \frac{(1-v+\alpha)b}{\ell^2} = \lambda_2.\]
    \item $\lambda_2 = 0$ and $\lambda_1 \neq 0$. Then, $\ell=b$ by complementary slackness. Stationarity and dual feasibility are satisfied only when $0 \leq v \leq 0.5$, since in this case
    \[\frac{v+\alpha}{b} - \frac{1-v+\alpha}{b} = -\lambda_1 \leq 0.\]
    \item  $\lambda_2 = 0$ and $\lambda_1 = 0$. Then, the first constraint gives that
    \begin{align*}
        \ell^2 = \frac{(1-v+\alpha)b^2}{v+\alpha}.
    \end{align*}
    Recall that $0 \leq \ell \leq b$, so this constraint is only satisfied when $0.5 \leq v \leq 1$ and $\ell = b\sqrt{\frac{1-v+\alpha}{v+\alpha}}$.
\end{enumerate}

Because $\ell_* = b$ is the optimal solution to both (a) and (b) when $0 \leq v \leq 0.5$, it must be the case that $k_*(v) = b$ if $0 \leq v \leq 0.5$. When $0.5 < v \leq 1$, the optimal solution to (a) is $\ell_* = b \sqrt{\frac{1-v+\alpha}{v+\alpha}}$ and the optimal solution to (b) is $\ell_* = b$. The value of the former is $1+ 2\alpha+2\sqrt{(v+\alpha)(1-v+\alpha)}$, while the value of the latter is $1+2\alpha+v+\alpha$. Taking the argmin yields
\begin{align*}
    k_*(v) = \begin{cases}
        b &\text{if $0 \leq v \leq \frac{4 + 3\alpha}{5}$} \\
        b \sqrt{\frac{1-v+\alpha}{v+\alpha}} &\text{if $\frac{4 + 3\alpha}{5} < v \leq 1$},
    \end{cases}
\end{align*}
which is exactly \cref{alg: optimal-ski-rental} and achieves a competitive ratio of
\[ \E[\CR(\cA_{k_*}) \mid f(X)=v]\leq 1+2\alpha +\min\bigl\{v+\alpha, 2\sqrt{(v+\alpha)(1-v+\alpha)} \bigr\}.\]

\end{proof}

\ConditionalCRLB*
\begin{table}[tb]
\caption{Objective values for fixed prediction $f(X)=v$, $z$ days skied, and renting for $k(v)$ days.}
\label{table: cr-landscape-appendix}
\vskip 0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Condition  & $\OPT(z)$ & $\ALG(\cA_k, v, z)$ \\
\midrule
$(i) \;\; z \leq \min\{k(v), \; b\}$ & $z$ & $z$ \\
$(ii) \;\; k(v) < z \leq b$ & $z$ & $k(v) + b$ \\
$(iii) \;\; b < z \leq k(v)$ & $b$ & $z$ \\
$(iv) \;\; z > \max\{k(v), \; b\}$ & $b$ & $k(v) + b$  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}
\begin{proof}
    Let $v \in [0,1]$ and $\epsilon > 0$. The calibrated predictor $f$ will deterministically output $v$, while the distribution $\cD_{v}^\epsilon$ will depend on whether algorithm $\cA_k$ buys before or after day $b$.


    \textbf{Case 1:} $k(v) < b$. Define a distribution $\cD_{v}^\epsilon$ where in a $v$ fraction of the data the true number of days skied is $z=b+\epsilon'$, and in a $1 - v$ fraction the number of days skied is $z=k(v) + \epsilon'$, where $\epsilon'$ is sufficiently small that \[k(v) + \epsilon' \leq b \text{\quad \quad and\quad \quad} 2 \sqrt{v(1-v)\left(1-\frac{\epsilon'}{b}\right)} - \frac{2v\epsilon'}{b + \epsilon'} \geq 2\sqrt{v(1-v)} - \epsilon.\]
    
   By construction, condition $(ii)$ from \cref{table: cr-landscape-appendix} is satisfied when $k(v) < b <z= b+\epsilon'$ with \[\ALG(\cA_k, v, z)/\OPT(z) = \frac{k(v)+b}{b+\epsilon'} = 1 + \frac{k(v)-\epsilon'}{b + \epsilon'}.\] Similarly, condition $(ii)$ holds when $k(v) < z=k(v) + \epsilon' \leq b$ with \[\ALG(\cA_k, v, z)/\OPT(z) = \frac{k(v) + b}{k(v) + \epsilon'} = 1 + \frac{b-\epsilon'}{k(v) + \epsilon'}.\] By the law of total expectation,
   \begin{align*}
       \E[CR(\cA_k)]
            &= v \cdot \left(1 + \frac{k(v)-\epsilon'}{b+\epsilon'}\right)+ (1- v) \cdot \left(1 + \frac{b-\epsilon'}{k(v)+\epsilon'}\right) \\
            &\geq \min_{\ell \geq 0} \left\{v \cdot \left(1 + \frac{\ell-\epsilon'}{b+\epsilon'}\right)+ (1- v) \cdot \left(1 + \frac{b-\epsilon'}
            {\ell+\epsilon'}\right)\right\}. \\
    \end{align*}
    Some basic calculus yields $\ell_* = \sqrt{\frac{1-v}{v}(b-\epsilon')(b+\epsilon')}-\epsilon'$, and evaluating the lower bound at $\ell^*$ gives
    \begin{align*}
        \E[CR(\cA_k)]
            &\geq 1 -\frac{2v\epsilon'}{b+\epsilon'}+ 2 \sqrt{v(1-v)\left(1-\frac{\epsilon'}{b}\right)}  \\
            &\geq 1 + 2\sqrt{v(1-v)} - \epsilon.
   \end{align*}
       \textbf{Case 2:} $k(v) \geq b$. 

       Define a distribution $\cD_{v}^\epsilon$ where in a $v$ fraction of the data the true number of days skied is $z=k(v)+\epsilon$, and in a $1 - v$ fraction the number of days skied is $z=b-\epsilon$. Condition $(iv)$ is satisfied when $b \leq k(v) < z=k(v)+\epsilon$ with $\ALG(\cA_k, v, z)/\OPT(z) = 1 + \frac{k(v)}{b}$. Condition $(i)$ is satisfied when $z = b-\epsilon < b \leq k(v)$ with $\ALG(\cA_k, v, z)/\OPT(z) = 1$. By the law of total expectation,
    \begin{align*}
       \E[CR(\cA_k)]        
            &= v \cdot \left(1 + \frac{k(v)}{b} \right) + (1- v) \cdot 1 \\
            &\geq v \cdot 2 + (1- v) \cdot 1 \\
            &= 1 + v.
   \end{align*}
       
   In both cases, $f$ is calibrated with respect to $\cD_v^\epsilon$ since $\Pr[Z>b \mid f(X)=v]=v$. Moreover, because the cases are exhaustive, at least one of the corresponding lower bounds must hold. It follows immediately that
   \[\E[CR(\cA_k) \mid f(X) = v] \geq 1+ \min\left\{v, 2\sqrt{v(1-v)}\right\} -\epsilon.\]
\end{proof}

\MSECalibrationBounds*
\begin{proof}
    We have from the law of total expectation that
    \begin{align*}
    \eta &= \E_{(X, Z) \sim \cD}\left[\left(\mathbbm{1}_{\{Z > b\}} - f(X)\right)^2\right]\\
        &= \sum_{v \in R(f)} \E\left[\left.\left(\mathbbm{1}_{\{Z > b\}} - v\right)^2 \, \right| \, f(X) = v\right] \cdot \Pr\left[f(X) = v\right]  \\
        &= \sum_{v \in R(f)} \left(\E\left[\mathbbm{1}_{\{Z > b\}} \mid f(X) = v\right] - 2v \E\left[\mathbbm{1}_{\{Z > b\}} \mid f(X) = v\right] + v^2\right)\cdot \Pr\left[f(X) = v\right].
    \end{align*}
    Applying the definition of the local calibration error $\alpha_v$,
    \begin{align*}
        \eta &= \sum_{v \in R(f)} \left(\E\left[\mathbbm{1}_{\{Z > b\}} \mid f(X) = v\right] - 2v \E\left[\mathbbm{1}_{\{Z > b\}} \mid f(X) = v\right] + v^2\right) \cdot \Pr\left[f(X) = v\right] \\
            &=  \sum_{v \in R(f)} \biggl((v - \alpha_v) - 2v (v - \alpha_v) + v^2\biggr)\cdot \Pr\left[f(X) = v\right] \\
            &= \sum_{v \in R(f)} \left(v(1-v) + (2v-1) \alpha_v \right)\cdot\Pr\left[f(X) = v\right] \\
            &= \E[f(X)(1-f(X))] + \sum_{v \in R(f)} (2v-1) \alpha_v \cdot \Pr\left[f(X) = v\right].
    \end{align*}
   The observation that $(2v-1)\alpha_v \geq -|\alpha_v|$ gives the result.
\end{proof}

\CRUB*
\begin{proof}
    This result follows from \Cref{thm: conditional-cr-ub}, \Cref{lemma: mse-calibration-bounds}, and an application of Jensen's inequality. To begin,
    \begin{align*}
    \E[\CR(\cA_{k_*})] &=\E\left[\E[\CR(\cA_{k_*}) \mid f(X)]\right] &\text{(Tower property)}\\
            &\leq\E\left[ 1 + 2\alpha + \min\left\{f(X) + \alpha, 2 \sqrt{(f(X)+\alpha)(1-f(X)+\alpha)}\right\}\right] &\text{(\Cref{thm: conditional-cr-ub})}\\
            &\leq 1 + 2\alpha + \min\left\{\E\left[f(X)\right] + \alpha, 2 \E\left[\sqrt{(f(X)+\alpha)(1-f(X)+\alpha)}\right]\right\},\\
    \end{align*}
    with the final line following from the fact that $\E[\min(X, Y)] \leq \min(\E[X], \E[Y])$ for random variables $X, Y$. Next, we argue from basic composition rules that the function $g(y) = \sqrt{(y+\alpha)(1-y+\alpha)}$ is concave for $y \in [0,1]$. The concavity of $g$ over its domain follows from the facts that (1) the $\sqrt{\cdot}$ function is concave and increasing in its argument and (2) $(y+\alpha)(1-y+\alpha)$ is concave. Moreover, $g(y)$ is well-defined for all $y \in [0,1]$. With concavity established, an application of Jensen's inequality yields
    \begin{align*}
            \E[\CR(\cA_{k_*})]&\leq 1 + 2\alpha + \min\left\{\E\left[f(X)\right] + \alpha, 2 \sqrt{\E\left[(f(X)+\alpha)(1-f(X)+\alpha)\right]}\right\}.
    \end{align*}
    To finish the proof, we will bound the term within the square root using \cref{lemma: mse-calibration-bounds}. Notice that
    \begin{align*}
        (f(X)+\alpha)(1-f(X)+\alpha) 
            &= f(X)(1-f(X)) + \alpha+\alpha^2\\ &\leq f(X)(1-f(X))+2\alpha.
    \end{align*}
    Finally,
    \begin{align*}
        \E[\CR(\cA_{k_*})] 
            &\leq 1 + 2\alpha + \min\left\{\E\left[f(X)\right] + \alpha, 2 \sqrt{\E\left[f(X)(1-f(X))\right] + 2\alpha}\right\} \\
            &\leq 1 + 2\alpha + \min\left\{\E\left[f(X)\right] + \alpha, 2 \sqrt{\eta + K_1(f, \cD) + 2\alpha}\right\} &\text{(\Cref{lemma: mse-calibration-bounds})} \\
            &\leq 1 + 2\alpha + \min\left\{\E\left[f(X)\right] + \alpha, 2 \sqrt{\eta + 3\alpha}\right\}.\\
    \end{align*}
\end{proof}

\ConformWorstCase*
\begin{proof}
Let $a \in [0,1/2]$ and consider a distribution that, for each unique feature vector $x \in \mathcal{X}$, has a true number of days skied that is either $z_1 \leq \frac{b}{2}$ with probability $1-a$ or $z_2 \geq 2b$ with probability $a$. By construction, any interval prediction $\textsc{PIP}_\delta(X) = [\ell,u]$ with $\delta < \min\{a, 1-a\} = a$ must satisfy that $\ell \leq z_1$ and $u \geq z_2$. This means $b \in [\ell, u]$, so \cref{alg: conformal-ski-rental} makes a determination of which day to buy based on the relative values of $\zeta(\delta, \ell)$, $\delta + \frac{u}{b}$, and 2. In particular, the algorithm follows the break-even strategy of buying on day $b$ when $\zeta(\delta, \ell) \geq 2$ and $\delta + \frac{u}{b} \geq 2.$

It is clear that $\delta + \frac{u}{b} \geq \frac{u}{b} \geq \frac{z_2}{b} \geq 2$. Next, recall the definition of $\zeta(\delta, \ell)$.
    \begin{align*}
        \zeta(\delta, \ell) = \begin{cases}
            \delta + (1-\delta)\frac{b}{\ell} + 2\sqrt{\delta (1- \delta)b/\ell} &\text{if $\delta \in [0, \frac{\ell}{b + \ell})$} \\
            1 + \frac{b}{\ell} &\text{if $\delta \in [\frac{\ell}{b + \ell}, 1]$}
        \end{cases}
    \end{align*}
    When $\delta \geq \frac{\ell}{b + \ell}$, we see that $\zeta(\delta, \ell) = 1 + \frac{b}{\ell} \geq 3$. To handle the case where $\delta < \frac{\ell}{b + \ell}$, we will show that \[f(\delta, x) = \delta + (1-\delta) x + 2\sqrt{\delta(1-\delta)x} \geq 2\] for all $x \geq 2$ and $\delta \in [0, 1/2]$. Plugging in $x = \frac{b}{\ell} \geq 2$ and noting that $\delta < \frac{\ell}{b + \ell} \leq 0.5$ implies the desired bound. Toward that end, notice that $f(\delta, x)$ is increasing in $x$, and so for all $x \geq 2$ we have that
    \[f(\delta, x) \geq f(\delta, 2) = 2 - \delta + 2\sqrt{2\delta(1-\delta)}.\]
    All that is left is to show that $2\sqrt{2\delta(1-\delta)} \geq \delta$. This is straightforward: for $\delta \in [0, 1/2]$,
    \[2\sqrt{2(1-\delta)} \geq \sqrt{1-\delta} \geq \sqrt{\delta},\]
    and multiplying through by $\sqrt{\delta}$ gives the desired inequality. In summary, we've shown that $b \in [\ell, u]$, $\zeta(\delta, \ell) \geq 2$, and $\delta + \frac{u}{b} \geq 2$ for the family of distributions described above. For this particular case, \cref{alg: conformal-ski-rental} rents for $b$ days.
\end{proof}

\ConformImprov*
\begin{proof}
    Let $a \in [0,1/2]$ and consider any distribution from the infinite family given in \cref{lemma: conform-worst}. In particular, in any of these distributions, the number of days skied is greater than $b$ with probability $a$. Therefore, the expected competitive ratio of the break-even strategy that rents for $b$ days before buying is
    \[\E[\CR(\cA)] = a \cdot 2 + (1-a)\cdot1 = 1 + a.\]
    The result follows from the bound on $\E[\CR(\cA_{k_*})]$ from \cref{thm: ski-rental-cr}.
\end{proof}

\section{Scheduling Proofs} \label{appendix: scheduling-proofs}

\SymUnordering*
\begin{proof}
Beginning with the facts that
\[\sum_{i=1}^n \sum_{j=1}^n g(X_{(i)}, X_{(j)}) = \sum_{i=1}^n \sum_{j=1}^n g(X_{i}, X_{j}) \text{\quad and \quad} \sum_{i=1}^n g(X_{(i)}, X_{(i)})=\sum_{i=1}^n g(X_{i}, X_{i}),\]
it follows from the symmetry of $g$ that
\begin{align*}
        \sum_{i=1}^n \sum_{j = i+1}^n g(X_{(i)}, X_{(j)}) &= \frac{1}{2} \left(\sum_{i=1}^n \sum_{j=1}^n g(X_{(i)}, X_{(j)}) - \sum_{i=1}^n g(X_{(i)}, X_{(i)})\right) \\
            &= \frac{1}{2} \left(\sum_{i=1}^n \sum_{j=1}^n g(X_{i}, X_{j}) - \sum_{i=1}^n g(X_{i}, X_{i})\right) \\
            &= \sum_{i=1}^n \sum_{j = i+1}^n g(X_{i}, X_{j}).
    \end{align*}
\end{proof}

\OrderingImprov*
\begin{proof}
    We'll begin by removing a shared term from both sides of the inequality. Notice that
    \begin{align*}
        \sum_{i=1}^n \sum_{j=i+1}^n X_{(i)} X_{(j)} =  \sum_{i=1}^n \sum_{j=i+1}^n X_{i} X_{j}
    \end{align*}
    by \cref{lemma: sym-unorder} with $g(x,y) = xy$. So, it is sufficient to show that
    \begin{align*}
        \E\left[\sum_{i=1}^n\sum_{j = i+1}^n X_{j} \right] - \E\left[\sum_{i=1}^n \sum_{j = i+1}^n  X_{(j)}\right] \geq  \binom{n}{2} \var(X_1).
    \end{align*}
    By linearity of expectation, the first term on the left-hand side is equal to $\binom{n}{2}\E[X_1]$. The random variables in the second term are not identically distributed, however, so a different approach is required. We will use a trick to express the sum in terms of the symmetric function $g(x,y)=\min(x,y)$, which allows us to remove the dependency on order statistics using \cref{lemma: sym-unorder}.
    \begin{align*}
        \sum_{i=1}^n \sum_{j=i+1}^n X_{(j)} 
            &= \sum_{i=1}^n \sum_{j = i + 1}^n \min\{X_{(i)}, X_{(j)}\} &&\text{($X_{(i)} \geq X_{(j)}$ since $i \leq j$)} \\
            &= \sum_{i=1}^n \sum_{j = i + 1}^n \min\{X_i, X_j\}. &&\text{(\Cref{lemma: sym-unorder} with $g(x,y)=\min\{x,y\}$])}
    \end{align*}
    Thus, the second term on the RHS is equal to $\binom{n}{2} \E[\min\{X_1, X_2\}]$. All that is left is to show that
    \begin{align*}
        \E[X_1] -\E[\min\{X_1, X_2\}] \geq \var(X_1).
    \end{align*}
    Toward that end, we can write
    \begin{align*}
        X_1 - \min\{X_1, X_2\} = \begin{cases}
            0 &\text{if $X_1 \leq X_2$} \\
            |X_1 - X_2| &\text{if $X_1 > X_2$},
        \end{cases}
    \end{align*}
    so $ \E[X_1] -\E[\min\{X_1, X_2\}] = \frac{1}{2} \E |X_1-X_2|$. Finally, using the fact that $|X_1 - X_2| \in [0,1]$ are iid, we have
    \begin{align*}
        \frac{1}{2} \E |X_1-X_2| 
        &\geq \frac{1}{2} \E [(X_1-X_2)^2] \\
        &=\frac{1}{2}\E\left[ X_1^2 - 2X_1X_2 +X_2^2\right] \\
        &=\frac{1}{2} \cdot 2 \left(\E[X_1^2] - \E[X_1]^2 \right) \\
        &= \var(X_1).
    \end{align*}
\end{proof}

\SchedulingImprov*
\begin{proof}
Given $n$ jobs to schedule with features $\vec{X} = (X_1, \dots, X_n)$ and the predictions $f(\vec{X}) = (f(X_1), \dots, f(X_n))$, let $n_1 = |\{i: f(X_i) > \beta\}|$ be a random variable that counts the number of samples from $f$ with prediction larger than $\beta$. We'll begin by computing expectations conditioned on $n_1$ before taking an outer expectation. 
\begin{align*}
    \E[&L(f(\vec{X}), \vec{Y}) \mid n_1] \\
        &= \E\biggl[\E[L(f(\vec{X}), \vec{Y}) \mid f(\vec{X})] \mid n_1\biggr] &&\text{(Tower property)} \\
        &= \E\left[\E\left[\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} \mathbbm{1}_{\{\vec{Y}_{(i)} = 0 \}}\cdot \mathbbm{1}_{\{\vec{Y}_{(j)} = 1 \}} \mid f(\vec{X}) \right] \mid n_1\right] &&\text{(Definition of $X$)}\\
        &= \E\left[\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} \Pr[\vec{Y}_{(i)} = 0 \mid f(\vec{X}_{(i)})] \cdot \Pr[\vec{Y}_{(j)} = 1\mid f(\vec{X}_{(j)})] \mid n_1\right] &&\text{(Independence)}\\
        &= \E\left[\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\right]. &&\text{(Calibration)}\\
\end{align*}
Performing the same computation for counts $M(\cdot)$ and $N(\cdot)$ gives
\[\E[M(f(\vec{X}), \vec{Y}) \mid n_1] = \E\left[\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} (1-f(\vec{X}_{(i)})) \cdot (1-f(\vec{X}_{(j)})) \mid n_1\right]\]
and
\begin{align*}
    \E[N(f(\vec{X}), \vec{Y})\mid n_1] = &\E\left[\sum_{i=1}^{n_1} \sum_{j = n_1+1}^{n} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\right] + \\ &\E\left[\sum_{i=n_1+1}^{n} \sum_{j = i+1}^{n} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\right].
\end{align*}
\[.\]
At this point, we can compute the conditional expectation of $M(\cdot)$ directly. By \cref{lemma: sym-unorder} with $g(x,y) = (1-x)(1-y)$, 
\begin{align*}
    \E\biggl[&\sum_{i = 1}^{n_1} \sum_{j= i + 1}^{n_1} (1-f(\vec{X}_{(i)})) \cdot (1-f(\vec{X}_{(j)})) \mid n_1\biggr] \\
        &= \E\left[\sum_{i = 1}^{n_1} \sum_{j= i + 1}^{n_1} (1-f(\vec{X}_{i})) \cdot (1-f(\vec{X}_{j})) \mid n_1\right] &&\text{(\cref{lemma: sym-unorder})}\\
        &= \binom{n_1}{2} \cdot \E\bigl[(1-f(\vec{X}_i))(1-f(\vec{X}_j)) \mid f(\vec{X}_i) > \beta, f(\vec{X}_j) > \beta\bigr] &&\text{(Independence)} \\
        &= \binom{n_1}{2} \cdot \Pr[Y=0 \mid f(X) > \beta]^2 &&\text{(Calibration)} \\
        &= \binom{n_1}{2} \cdot \frac{\epsilon_0^2(1-\rho)^2}{\Pr[f(X)>\beta]^2}. &&\text{(Bayes' rule)}
\end{align*}
The same technique cannot be used to evaluate the expectations of $L(\cdot)$ and $N(\cdot)$ because the function $g(x,y) = (1-x)y$ is not symmetric. Instead, we will provide upper bounds on the conditional expectations using \cref{lemma: ordering-gap}, then evaluate the unordered results as before. For the conditional expectation of $L(\cdot)$, we have
\begin{align*}
       \E\biggl[&\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\biggr] \\
        &\leq \E\left[\sum_{i=1}^{n_1} \sum_{j = i+1}^{n_1} (1-f(\vec{X}_i)) \cdot f(\vec{X}_j) \mid n_1\right] - \binom{n_1}{2} \var(f(X) \mid f(X) > \beta) &&\hspace{-2mm}\text{(\cref{lemma: ordering-gap})}\\
        &= \binom{n_1}{2} \cdot \biggl(\E[(1-f(\vec{X}_i))f(\vec{X}_j) \mid f(\vec{X}_i), f(\vec{X}_j) > \beta ] - \var(f(X) \mid f(X) > \beta)\biggr) \\
        &= \binom{n_1}{2} \cdot \biggl(\Pr[Y=0 \mid f(X) > \beta] \cdot \Pr[Y=1 \mid f(X) > \beta ] - \var(f(X) \mid f(X) > \beta)\biggr) \\
        &= \binom{n_1}{2} \cdot \biggl(\frac{\rho(1-\rho)(1-\epsilon_1)\epsilon_0}{\Pr[f(X)>\beta]^2} - \var(f(X) \mid f(X) > \beta)\biggr).
\end{align*}
Similarly for the conditional expectation of the second term of $N(\cdot),$
\begin{align*}
       \E\biggl[&\sum_{i=n_1 +1}^{n} \sum_{j = i+1}^{n} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\biggr] \\
        &\leq \E\left[\sum_{i=n_1+1}^{n} \sum_{j = i+1}^{n} (1-f(\vec{X}_i)) \cdot f(\vec{X}_j) \mid n_1\right] - \binom{n-n_1}{2} \var(f(X) \mid f(X) \leq \beta) \\
        &= \binom{n-n_1}{2} \cdot \biggl(\Pr[Y=0 \mid f(X) \leq \beta] \cdot \Pr[Y=1 \mid f(X) \leq \beta ] - \var(f(X) \mid f(X) \leq \beta)\biggr) \\
        &= \binom{n-n_1}{2} \cdot \left(\frac{\rho(1-\rho)(1-\epsilon_0)\epsilon_1}{\Pr[f(X) \leq  \beta]^2} - \var(f(X) \mid f(X) \leq \beta)\right).
\end{align*}
For the first term of $N(\cdot)$, we simply apply the rearrangement inequality in lieu of \cref{lemma: ordering-gap} for unordering, since the sum has the form $\sum_{i} a_i \cdot b_i$ where $a_i = (1-f(\vec{X}_{(i)}))$ is an increasing sequence and $b_i = \sum_{j=i+1}^n f(\vec{X}_{(j)})$ is a decreasing sequence.
\begin{align*}
   \E\left[\sum_{i=1}^{n_1} \sum_{j = n_1+1}^{n} (1-f(\vec{X}_{(i)})) \cdot f(\vec{X}_{(j)}) \mid n_1\right] &\leq \E\left[\sum_{i=1}^{n_1} \sum_{j = n_1+1}^{n} (1-f(\vec{X}_i)) \cdot f(\vec{X}_j) \mid n_1\right] \\  
   &= n_1(n-n_1) \cdot \E[(1-f(\vec{X}_i))f(\vec{X}_j) \mid f(\vec{X}_i) > \beta, f(\vec{X}_j) \leq \beta ] \\
   &= n_1(n-n_1) \cdot \Pr[Y=0 \mid f(X) > \beta] \cdot \Pr[Y=1 \mid f(X) \leq \beta ] \\
    &=n_1(n-n_1) \cdot \frac{\rho(1-\rho)\epsilon_1 \epsilon_0}{\Pr[f(X) > \beta] \cdot \Pr[f(X) \leq \beta]}.
\end{align*}
Next, we take an outer expectation to remove the dependency on $n_1$. Recall that $n_1$ follows a Binomial$(n, \Pr[f(X) > \beta])$ distribution, so one can easily verify that
\begin{enumerate}
    \item $\E[\binom{n_1}{2}] = \binom{n}{2} \cdot \Pr[f(X) > \beta]^2$
    \item $\E[\binom{n-n_1}{2}] = \binom{n}{2} \cdot (1-\Pr[f(X) > \beta])^2 = \binom{n}{2} \cdot \Pr[f(X) \leq \beta]^2$
    \item $\E[n_1(n-n_1)] = 2\binom{n}{2} \cdot \Pr[f(X) > \beta] \cdot \Pr[f(X) \leq \beta].$
\end{enumerate}
It follows immediately that
\begin{align*}
\E[L(f(\vec{X}), \vec{Y})] &= \E[\E[L(f(\vec{X}), \vec{Y}) \mid n_1]] \\
        &\leq \E\left[\binom{n_1}{2} \cdot \biggl(\frac{\rho(1-\rho)(1-\epsilon_1)\epsilon_0}{\Pr[f(X)>\beta]^2} - \var(f(X) \mid f(X) > \beta)\biggr)\right] \\
        &= \binom{n}{2}\cdot\biggl(\rho(1-\rho)(1-\epsilon_1)\epsilon_0 - \kappa_1\biggr) \\\\
\E[M(f(\vec{X}), \vec{Y})] &= \E[\E[M(f(\vec{X}), \vec{Y}) \mid n_1]] \\
        &= \E\left[\binom{n_1}{2} \cdot \frac{\epsilon_0^2(1-\rho)^2}{\Pr[f(X)>\beta]^2}\right] \\
        &= \binom{n}{2} \cdot (1-\rho)^2\epsilon_0^2\\\\
\E[N(f(\vec{X}), \vec{Y})] &= \E[\E[N(f(\vec{X}), \vec{Y}) \mid n_1]] \\
        &\leq \E\left[\binom{n-n_1}{2} \cdot \left(\frac{\rho(1-\rho)(1-\epsilon_0)\epsilon_1}{\Pr[f(X) \leq  \beta]^2} - \var(f(X) \mid f(X) \leq \beta)\right)\right] \\
        &\quad\quad + \E\left[n_1(n-n_1) \cdot \frac{\rho(1-\rho)\epsilon_1 \epsilon_0}{\Pr[f(X) > \beta] \cdot \Pr[f(X) \leq \beta]}\right] \\
        &= \binom{n}{2}\cdot\biggl(\rho(1-\rho)(1-\epsilon_0)\epsilon_1 + 2\rho(1-\rho)\epsilon_1 \epsilon_0 - \kappa_2\biggr) \\
        &= \binom{n}{2}\cdot\biggl(\rho(1-\rho)(1+\epsilon_0)\epsilon_1 - \kappa_2\biggr),
\end{align*}
where
\[\kappa_1 := \Pr[f(X) > \beta]^2 \cdot \var(f(X) \mid f(X) > \beta) \text{\quad and \quad} \kappa_2:= \Pr[f(X) \leq \beta]^2 \cdot\var(f(X) \mid f(X) \leq \beta).\]
\end{proof}

\section{Experimental Details}
\label{appendix: experimental-details}
\subsection{Ski-Rental: CitiBike}
Our experiments with CitiBike use ridership duration data from June 2015. Although summer months have slightly longer rides, the overall shape of the distributions is similar across months (i.e. left-skewed distribution). Figure~\ref{fig:ride_dist} illustrates the distribution of scores. This indicates that using this dataset for ski rental, the breakeven strategy will be better as $b$ increases since most of the rides will be less than $b$. This is an empirical consideration of running these algorithms that prior works do not consider. Thus, we select values of $b$ between $200$ and $1000$ as a reasonable interval for comparison. 
\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/bike_distribution.pdf}
    \caption{Distribution of ride times and quantiles in minutes, most rides are under 900 minutes.}
    \label{fig:ride_dist}
\end{figure}
\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/feature_acc.pdf}
    \caption{Predictor accuracy with different features around final docking station, no information, partial information (approximate latitude), and rich information (approximate latitude and longitude).}
    \label{fig:feature-selection}
\end{figure}

\paragraph{Feature Selection}
The original CitiBike features include per-trip features including user type, start and end times, location, station, gender, and birth year of the rider. We tested predictors with three types of feature: no information about final destination, partial information about final destination (end latitude only), and rich information about final destination (end longitude and latitude). Even with rich information, the best accuracy of the model's we consider are around 80\% accuracy. This is because there are many factors affecting the ride duration. However with no information about the final destination, many of our models were close to random and thus do not serve as good predictor (Figure~\ref{fig:feature-selection}).  

\paragraph{Model Selection}
We tested a variety of models for both classification (e.g. linear regression, gradient boosting, XGBoost, k-Nearest Neighbors, Random Forest and a 2-layered Neural Network) and regression (e.g. Linear Regression, Bayesian Ridge Regression, XGBoost Regression, SGD Regressor, and Elastic Net, and 2-layered Neural Network). We ended up choosing three representative predictors of different model classes: regression, boosting, and neural networks. To fairly compare regression with classification we choose similar model classes: (Linear Regression, Logistic Regression), (XGBoost, XGBoost Regression), and two-layer neural networks.  
\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/no-info-clfXGB-regXGBR_CR.pdf}
    \caption{No info about ride end station}
    \label{fig:no-info-XGB}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/partial-info-clfXGB-regXGBR_CR.pdf}
    \caption{Approx end lat.}
    \label{fig:partial-info-XGB}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/rich-info-clfXGB-regXGBR_CR.pdf}
    \caption{Approx end long. and lat.}
    \label{fig:rich-info-XGB}
  \end{subfigure}
  \caption{XGBoost predictors generally enable the calibrated predictor algorithm to do better than other baselines.}
  \label{fig:all-XGB}
  \vspace{-4mm}
\end{figure}
\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/no-info-clfLR-regLR_CR.pdf}
    \caption{No info about ride end station}
    \label{fig:no-info-LR}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/partial-info-clfLR-regLR_CR.pdf}
    \caption{Approx end lat.}
    \label{fig:partial-info-LR}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/rich-info-clfLR-regLR_CR.pdf}
    \caption{Approx end long. and lat.}
    \label{fig:rich-info-LR}
  \end{subfigure}
  \caption{Linear regression and logistic regression remains similar to break even stretegy regardless of the features used. }
  \label{fig:all-LR}
  \vspace{-5mm}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/no-info-clfNN-regNN_CR.pdf}
    \caption{No info about ride end station}
    \label{fig:no-info-NN}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/partial-info-clfNN-regNN_CR.pdf}
    \caption{Approx end latitude}
    \label{fig:partial-info-NN}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/rich-info-clfNN-regNN_CR.pdf}
    \caption{Approx end long. and lat.}
    \label{fig:rich-info-NN}
  \end{subfigure}
  \caption{Neural network predictors generally enable calibrated predictor algorithm to do better than other baseline when there are informative features}
  \label{fig:all-NN-ski}
  \vspace{-4mm}
\end{figure}

\paragraph{Calibration}
To calibrate an out-of-the box model, we tested histogram calibration~\citep{zadrozny2001obtaining}, binned calibration~\citep{Gupta21:Distribution}, and Platt scaling~\citep{platt1999probabilistic}. While results from histogram and bin calibration were similar, Platt scaling often produced calibrated probabilities within a very small interval. Though it is implemented in our code, we did not use it. 
A key intervention we make for calibration is to calibrate according to balanced classes in the validation set when the label distribution is highly skewed. This approach ensures that probabilities are not artificially skewed due to class imbalance. 
\paragraph{Regression}
For a regression model as a fair comparison, we assume that the regression model also only has access to the 0/1 labels of the binary predictor for each $b$. To use convert the output conformal intervals to be used in the algorithm from ~\citet{Sun24:Online}, we multiply the 0/1 intervals by $b$. 

\subsection{Scheduling: Sepsis Triage}
\paragraph{Dataset} We use a dataset for sepsis prediction: `Sepsis Survival Minimal Clinical Records'. \footnote{\url{https://archive.ics.uci.edu/dataset/827/sepsis+survival+minimal+clinical+records}} This dataset contains three characteristics: age, sex, and number of sepsis episodes. The target variable for prediction is patient mortality. 

\paragraph{Additional Models}
We also include results for additional base models: 2 layer perception (Figure~\ref{fig:schedule-NN}) and XGBoost (Figure~\ref{fig:schedule-XGB})
\begin{figure}[!h]
  \centering
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/hist_sepsis_LR.pdf}
    \caption{Logistic Regression}
    \label{fig:schedule-LR}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/hist_sepsis_NN.pdf}
    \caption{2 Layer Neural Network}
    \label{fig:schedule-NN}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/hist_sepsis_XGB.pdf}
    \caption{XGBoost}
    \label{fig:schedule-XGB}
  \end{subfigure}
  \caption{Comparison of different base models. As $\theta$ increases the performance of the calibrated predictor becomes more similar to the binary predictor.}
  \label{fig:all-NN}
\end{figure}
