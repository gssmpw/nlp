\subsection{Assessment}

\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c p{3.75cm} p{3.5cm} p{2.25cm} p{3.75cm}}
        \toprule
         \textbf{Study} & \textbf{Text Granularity} & \textbf{Best Technique} & \textbf{NLP Task} & \textbf{Assessment Focus} \\
         \midrule
         \multicolumn{5}{c}{\emph{Symptom Detection}}\\
         \midrule

         \citet{18} & Single Post & Emotion Prompting & BC/MCC/EG & Multiple Symptoms \\
         \citet{22} & Multi-turn Dialogue & Fine-Tuning & MLC/IE/SUM & Multiple Symptoms \\
         \citet{24} & Multi-turn Dialogue & Few-Shot Prompting & MLC/IE/SUM & PTSD \\
         \citet{27} & Single Post & Fine-Tuning & MLC/EG & Depression \\
         \citet{66} & Single Post & Few-Shot Prompting & MCC & Multiple Symptoms \\
         \citet{85} & Posts From One User & Chain-of-Thought & IE/SUM & Suicidal Ideation \\
         \citet{91} & Posts From One User & Role Prompting & IE/SUM & Suicidal Ideation \\
         \citet{94} & Single Post & Fine-Tuning & BC/MCC/EG & Multiple Symptoms \\
         \citet{95} & Single Post & Fine-Tuning & BC/EG & Multiple Symptoms \\
         \citet{96} & Single Post & Few-Shot Prompting & MLC & Multiple Symptoms \\
         \citet{108} & Single Post & Fine-Tuning & MLC & Suicidal Ideation \\
         \citet{120} & Single Post & Zero-Shot Prompting & BC & PTSD \\

         \midrule
         \multicolumn{5}{c}{\emph{Symptom Severity}}\\
         \midrule
         \citet{11} & Multi-turn Dialogue & Zero-Shot Prompting & TR & Depression/PTSD \\
         \citet{23} & Multi-turn Dialogue & Zero-Shot Prompting & TR & Depression/Anxiety \\
         \citet{58} & Posts From One User & Zero-Shot Prompting & TR & Depression \\
         \citet{59} & Posts From One User & Zero-Shot Prompting & TR & Depression \\
         \citet{100} & Single Post & Zero-Shot Prompting & TR/MCC & Depression/Suicide \\

         \midrule
         \multicolumn{5}{c}{\emph{Cognition}}\\
         \midrule
         \citet{5} & Single Sentence & Few-Shot Prompting & MLC & Cognitive Distortions \\
         \citet{10} & Single Post & Fine-Tuning & MLC & Cognitive Distortions \\
         \citet{12} & Single Sentence & Few-Shot Prompting & MCC & Cognitive Distortions \\
         \citet{14} & Single-turn Dialogue & Zero-Shot Prompting & BC/MCC/EG & Cognitive Distortions \\
         \citet{16} & Single Post & Zero-Shot Prompting & MLC & Maladaptive Schemas \\
         \citet{38} & Single Post & Zero-Shot Prompting & MCC/SUM & Cognitive Pathways \\
         \citet{45} & Single-turn Dialogue & Multi-Agent Debate & MCC & Cognitive Distortions \\

         \midrule
         \multicolumn{5}{c}{\emph{Behavior}}\\
         \midrule
         \citet{36} & Single Post & Zero-Shot Prompting & MLC/EG & Interpersonal Risk \\
         \citet{60} & Sentence From Dialogue & Few-Shot Prompting & MCC & MI-Adherent Behaviors \\
         \citet{65} & Sentence From Dialogue & Zero-Shot Prompting & MCC & MI-Adherent Behaviors \\
         \citet{67} & Sentence From Dialogue & Zero-Shot Prompting & MCC & MI-Adherent Behaviors \\
         

         \bottomrule
    \end{tabular}
    }
    \vspace{-5pt}
    \caption{Comparison of Psychological Assessment Studies by Input Characteristics and Methodology. \textbf{MLC}: Multi-Label Classification, \textbf{IE}: Information Extraction, \textbf{SUM}: Summarization, \textbf{MCC}: Multi-Class Classification, \textbf{BC}: Binary Classification, \textbf{TR}: Text Regression, \textbf{EG}: Explanation Generation. Studies are categorized through text granularity, optimal technical approach (\textit{Best Technique}), NLP task formulation, and specific assessment focus.}
    \vspace{-4mm}
    \label{tab:assessment_method_comparison}
\end{table*}

\paragraph{Symptom Detection} leverages LLMs to identify mental health conditions including depression, anxiety, PTSD, and suicidal ideation, demonstrating robust performance and multidimensional applicability across diverse scenarios. \citet{18} systematically evaluated GPT-3.5, InstructGPT3, and LLaMA models across 11 datasets, revealing that emotion-enhanced chain-of-thought prompting improves interpretability yet remains inferior to specialized supervised methods. \citet{22} achieved 70.8\% zero-shot symptom retrieval accuracy in Korean psychiatric interviews using GPT-4 Turbo, while their fine-tuned GPT-3.5 attained 0.817 multi-label classification accuracy. Clinical applications show particular promise, as \citet{24} leveraged GPT-4 and Llama-2 to automate PTSD assessments through information extraction from 411 interviews, significantly enhancing diagnostic practicality. 

Social media analysis benefits from approaches like \citet{27}'s interpretable depression detection framework, which demonstrated strong performance across Vicuna-13B and GPT-3.5 environments. Resource development advances include \citet{66}'s \emph{MentalHelp} dataset with 14 million instances, validated through GPT-3.5 zero-shot evaluations. For suicidal ideation monitoring, \citet{85} and \citet{91} achieved state-of-the-art evidence extraction in the CLPsych 2024 shared task through innovative prompting strategies. Open-source initiatives like \emph{MentaLLaMA} by \citet{94} and \emph{Mental-LLM} by \citet{95} enable multi-symptom detection via instruction-tuned LLaMA variants, though \citet{96}'s \emph{WellDunn} framework reveals persistent gaps in GPT-family models' explanation consistency.

Cross-lingual adaptations include \citet{108}'s \emph{PsyGUARD} system based on fine-tuned CHATGLM2-6B for Chinese suicide risk assessment, while \citet{120} demonstrated domain-specific RoBERTa models outperforming GPT-4 in cross-domain PTSD pattern analysis, highlighting the critical balance between model specialization and interpretability.

\paragraph{Symptom Severity} focuses on estimating the level of mental health condition intensity, particularly for depression, anxiety, and PTSD. Clinical evaluations reveal Med-PaLM 2's zero-shot depression scoring attains clinician-level alignment on interview data \citep{11}, though with limited PTSD generalizability. When benchmarked against specialized Transformers on DAIC-WOZ dataset~\cite{gratch-etal-2014-distress}, ChatGPT and Llama-2 exhibit moderate efficacy \citep{23}, suggesting domain-specific architectures retain advantages in structured assessments. Shifting attention to social media data, \citet{58} proposed a pipeline that retrieves depression-relevant text, summarizes it according to the Beck Depression Inventory (BDI)~\cite{jackson2016beck}, and then utilizes LLMs to predict symptom severity, achieving performance similar to expert evaluations on certain measures. In a similar vein, \citet{59} introduced an explainable depression detection system that leverages multiple open-source LLMs to generate BDI-based answers, reporting near state-of-the-art performance without additional training data. Cross-lingual extensions emerge through \citet{100}'s framework enabling severity prediction across 6 languages and 2 mental conditions.

\paragraph{Cognition} centers on identifying and understanding maladaptive thinking patterns, such as cognitive distortions and early maladaptive schemas, using LLMs. \citet{5} introduced a cognitive distortion dataset and employed a few-shot strategy with GPT-3.5 to generate, classify, and reframe them, while \citet{10} constructed two Chinese social media benchmarks for cognitive distortion detection and suicidal risk assessment, demonstrating that fine-tuned LLMs are more closely than zero-/few-shot methods to supervised baselines. In a related effort, \citet{12} released the C2D2 dataset containing 7{,}500 Chinese sentences with distorted thinking patterns.
% thereby facilitating fine-grained exploration of how such distortions manifest in everyday language.
Expanding on detection methods, \citet{14} proposed a Diagnosis of Thought prompting approach for GPT-4 and ChatGPT, which breaks down patient utterances into factual versus subjective content and supports the generation of interpretable diagnostic reasoning. Beyond cognitive distortions, \citet{16} investigated zero-shot approaches with GPT-3.5 to identify early maladaptive schemas in mental health forums, highlighting challenges in label interpretability and prompt sensitivity. Complementarily, \citet{38} presented a hierarchical classification and summarization pipeline to extract cognitive pathways from Chinese social media text, underscoring GPT-4’s strong performance albeit with occasional hallucinations. Finally, \citet{45} introduced a multi-agent debate framework for cognitive distortion classification, reporting substantial gains in both accuracy and specificity by synthesizing multiple LLM opinions before forming a final verdict.

\paragraph{Behavior} highlights how user actions—or in the case of Motivational Interviewing (MI), language itself—can serve as a measurable indicator of one’s readiness for change. For instance, \citet{36} introduced the MAIMS framework, employing mental scales in a zero-shot setting to identify interpersonal risk factors on social media, thereby enhancing both interpretability and accuracy. In clinical dialogues, \citet{60} demonstrated how LLMs can automatically detect a client’s motivational direction (e.g., change versus sustain talk) and commitment level, offering valuable insights for MI-based interventions. Extending such analyses to bilingual settings, \citet{65} proposed the BiMISC dataset and prompt strategies that enable LLMs to code MI behaviors across multiple languages with expert-level performance. Lastly, \citet{67} presented MI-TAGS for automated annotation of global MI scores, illustrating how context-sensitive modeling can approximate human annotations in psychotherapy transcripts.


\paragraph{Advanced} research has evolved beyond foundational assessment tasks to emphasize novel methodological paradigms, bias mitigation, and domain-specific summarization frameworks. For instance, \citet{48} introduced \emph{PsychoGAT}—an interactive, game-based approach that transforms standardized psychometric instruments into engaging narrative experiences, improving psychometric reliability, construct validity, and user satisfaction when measuring constructs such as depression, cognitive distortions, and personality traits. In parallel, \citet{53} systematically investigated potential biases in various LLMs across multiple mental health datasets, revealing that even high-performing models exhibit unfairness related to demographic factors. The authors proposed fairness-aware prompts to substantially reduce such biases without sacrificing predictive accuracy. Furthermore, \citet{99} presented the \emph{PIECE} framework, which adopts a planning-based approach to domain-aligned counseling summarization, structuring and filtering conversation content before integrating domain knowledge.

% \citet{99} presented the \emph{PIECE} framework, which adopts a planning-based approach to domain-aligned counseling summarization. By structuring and filtering conversation content before integrating domain knowledge, PIECE achieved notable improvements over baseline methods on both automated and expert evaluations.

