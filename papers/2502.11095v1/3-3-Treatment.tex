\subsection{Treatment}

\paragraph{LLM as a Virtual Therapist} centers on leveraging LLMs to directly engage in therapeutic conversations, often adopting multi-turn dialogues that incorporate recognized psychotherapeutic frameworks. For instance, \citet{20} proposed \emph{HealMe} to facilitate cognitive reframing and empathetic support in line with established psychotherapy principles. Likewise, \citet{29} introduced \emph{CaiTI}, a system embedded in everyday smart devices that conducts assessments of users’ daily functioning and delivers psychotherapeutic interventions through adaptive dialogue flows. In a similar vein, \citet{40} presented \emph{CoCoA}, specializing in identifying and resolving cognitive distortions via dynamic memory mechanisms and CBT-based strategies, while \citet{54} proposed a step-by-step approach guiding users to execute self-guided cognitive restructuring through multiple interactive sessions. Beyond standard CBT protocols, \citet{55} focused on aiding psychiatric patients in journaling their experiences, thereby offering richer clinical insights, whereas \citet{77} developed a multi-round CBT dataset to refine LLMs for direct counseling-like interactions. Additionally, multi-agent frameworks like \emph{MentalAgora} \citep{78} highlighted personalized mental health support by integrating multiple specialized agents, and \citet{109} further explored “mixed chain-of-psychotherapies” to combine various therapeutic methods, aiming to enhance the emotional support and customization delivered by chatbot interactions.

\paragraph{LLM as an Assistive Tool} refrains from providing a holistic therapy role but instead offers targeted support such as rewriting suboptimal counselor responses, generating controlled reappraisal prompts, or aiding clinicians in specific tasks. For example, \citet{2} proposed to rewrite responses that violate MI principles into MI-adherent forms, ensuring more consistent therapeutic dialogue. Meanwhile, \citet{3} and \citet{5} focused on generating single-turn reframes of negative thoughts—often anchored in cognitive distortions—through controlled language attributes. On the detection side, \citet{35} built a multimodal pipeline to identify depression and provide CBT-style replies, albeit with an emphasis on technological assistance rather than full-fledged therapy. In the Chinese context, \citet{43} combined cognitive distortion detection with “positive reconstruction,” demonstrating a single-round rewrite approach for negative or distorted statements, while \citet{64} showcased a structured Q\&A format that offers professional yet succinct CBT-based responses. From a knowledge-distillation angle, \citet{70} demonstrated how smaller models could replicate GPT-4’s MI-style reflective statements, and \citet{81} introduced a lighter-weight framework \emph{RESORT} to guide smaller LLMs toward effective cognitive reappraisal prompts, thus enabling broader accessibility of self-help tools.

\paragraph{LLM as Simulated Patients for Clinician Education} pivots toward generating synthetic yet realistic patient behaviors or multi-level feedback to train or support mental health practitioners. For instance, \citet{49} leveraged LLMs to deliver multi-tier feedback on novice peer counselors’ conversational skills, significantly reducing the need for continuous expert oversight. Similarly, \citet{51} using LLM-driven patient simulations that help trainees practice CBT core skills in a controlled, repeatable setup. In the realm of assessing therapy quality, \citet{56} showcased a digital patient system to evaluate MI sessions, employing AI-generated transcripts to differentiate novice, intermediate, and expert therapeutic skill levels. Complementarily, \citet{80} offered \emph{Roleplay-doh}, a pipeline wherein domain experts craft specialized principles that guide LLM-based role-playing agents, thereby providing customizable training for new therapists.

\paragraph{LLM for Evaluation and Quality Analysis} targets the appraisal of therapy dialogue, counselor techniques, and treatment processes, typically without delivering direct interventions to clients. For instance, \citet{4} augmented crisis counseling outcome prediction by fusing annotated counseling strategies with LLM-derived features, achieving substantially improved accuracy. In the Chinese context, \citet{19} introduced \emph{CPsyCoun}, employing reports-based dialogue reconstruction and automated evaluation to verify counseling realism and professionalism. Beyond single-session analyses, \citet{32} used simulated clients to assess perceived therapy outcomes, while \citet{37} created the \emph{BOLT} framework for systematically comparing LLM-based therapy behaviors with high- and low-quality human sessions. Further extending to online counseling, \citet{39} proposed an LLM-based approach to measure therapeutic alliance, whereas \citet{62} delineated therapist self-disclosure classification as a new NLP task. In the MI domain, \citet{65} and \citet{67} collected bilingual transcripts to systematically annotate therapist–client exchanges for behavior coding and global scores, respectively. Additionally, multi-session perspectives emerge in \citet{101}, who proposed \emph{IPAEval} to track long-term progress from the client’s viewpoint, and \citet{103} analyzed conversation redirection and its impact on patient–therapist alliance over multiple sessions. Finally, \citet{111} and \citet{122} explored the disparities between LLM- and human-led CBT sessions, highlighting gaps such as empathy and cultural nuance while also introducing \emph{CBT-Bench} to probe LLMs’ deeper psychotherapeutic competencies.

% \subsubsection{Detection and Classification}
% LLMs have been widely used in detecting and classifying cognitive distortions following the CBT psychotherapy framework. Cognitive distortion detection involves identifying and addressing negative thought patterns that can impact mental health. Research highlights LLM's ability to generate, recognize, and reframe unhelpful thoughts. For example, models like T5, R2C2, and GPT-3.5 have been fine-tuned to enhance the identification and reframing of cognitive distortions, evaluated through both automated metrics and human assessments \cite{5}. Similarly, GPT-3.5, GPT-4, and ChatGLM2-6B have achieved high precision and recall in detecting cognitive distortions in Chinese social media \cite{10}. Datasets such as C2D2 aid in annotating cognitive distortions but focus more on dataset quality than model applications \cite{12}. Additionally, specialized prompting techniques with models like ChatGPT, Vicuna, and GPT-4 have improved diagnostic accuracy \cite{14}. Non-English language models like ChatGLM-6B have also shown strong results in detecting cognitive distortions in Mandarin \cite{43}. The ERD framework has advanced classification by using multi-agent debate models to boost accuracy \cite{45}. Interactive systems employing LLMs for cognitive restructuring are promising, though their long-term effectiveness remains untested \cite{54}. Furthermore, GPT-4 has been applied to enhanced CBT interventions by extracting cognitive pathways from social media \cite{38}. Overall, LLMs effectively address cognitive distortions, though further exploration in real-time and cross-cultural contexts is needed.

% Behavioral coding under the motivational interviewing framework is another popular field. It focuses on analyzing and improving the quality of therapeutic conversations, ensuring that dialogue aligns with effective motivational strategies. Studies have explored various approaches in doing so, often fine-tuning models like Blender, GPT-3, GPT-3.5, GPT-4, and Flan-T5 to improve the quality and adherence of therapeutic dialogues. For instance, transforming non-adherent responses into MI-adherent ones and automating MI skill coding have led to improvements in dialogue quality and alignment with expert annotations \cite{2,65}. These outcomes were evaluated using metrics such as BLEU, ROUGE, METEOR, human evaluations, accuracy, precision, recall, and macro F1 scores.  \cite{67} touched up on automatic classification of MI transcripts and global score predictions. Models like GPT-3.5, and GPT-4 have been employed to annotate MI sessions with behavioral codes and predict global scores, with evaluations based on accuracy, Macro F1, ROC AUC, and Pearson correlation Additionally, LLMs have been used to detect client motivational language, which informs therapeutic strategies, evaluated using accuracy and F1 scores \cite{60}.

% Other classification tasks include identifying Early Maladaptive Schemes (EMS) and categorizing Therapist Self-Disclosure (TSD). For EMS, models like GPT-3.5 and Flan-T5 have effectively identified schemas from mental health forum texts with high precision and recall, though their real-time application in therapy remains unexplored \cite{16}. Similarly, GPT-4 has been used to categorize therapist self-disclosure into immediate, non-immediate, or non-TSD categories \cite{62}, demonstrating successful classification. However, the long-term effects of different types of TSD on therapy outcomes have not yet been investigated.

% \subsubsection{Reframing and Positive Reconstruction}
% Reframing and positive reconstruction follow cognitive distortion detection. It helps patients to transform negative thought patterns into more constructive ones. used retrieval-enhanced in-context learning to train GPT-3 to reframe negative thoughts, which reduced negative emotions. fine-tuned T5 and GPT-3.5 with large-scale datasets to improve their ability to recognize and generate effective reframed thoughts. \cite{81} explored the ability of LLMs like GPT-4 turbo, LLaMA-2, and Mistral to offer cognitive reappraisal, a CBT strategy, evaluated through human assessments for empathy, factuality, and harmfulness. Another approach \cite{20} involved creating a specialized system based on LLaMA2-7b-chat to produce empathetic and logically coherent responses. Additionally, \cite{54} explored self-guided cognitive restructuring LLMs, where GPT-3 was trained to develop an interactive framework for providing helpful and empathetic guidance. The effectiveness of these studies is generally validated using methods like BLEU, ROUGE, BERTScore, PANAS, and human assessments.

% \subsubsection{LLMs as therapists, patients, and more}
% \textbf{As Therapists.} The exploration of LLMs as potential therapists has attracted much attention in simulating psychiatrist-patient interactions for training and evaluation purposes. \cite{9} fine-tuned GPT models to simulate both the psychiatrist and patient roles, validated through human evaluations by real psychiatrists and patients. \cite{19} expanded on this concept by focusing on multi-turn dialogues for psychological counseling. Utilizing a range of models, including ChatGPT, LLaMA, and others, the study successfully simulated realistic and emotionally resonant counseling conversations. 
% % These dialogues were evaluated for comprehensiveness, professionalism, and authenticity, demonstrating the effectiveness of LLMs in generating high-quality therapeutic interactions.
% \cite{26} evaluated the efficacy of LLMs in counseling high-functioning autistic adolescents, using them to simulate therapeutic interactions, focusing on empathy and adaptability, which resulted in improved communication skills and psychological well-being among the participants. \cite{29} developed a conversational AI therapist aimed at daily functioning screening and psychotherapeutic intervention, with measures of empathy, communication skills, and therapeutic alliance indicating positive outcomes. A few studies focused on the CBT framework: \cite{77} created a multi-turn dialogue dataset specifically designed for CBT-based psychological counseling. This dataset facilitated the training of LLMs to simulate CBT interactions that adhered closely to CBT principles, as confirmed by evaluations focused on adherence and conversational quality. \cite{97} evaluated whether LLMs could effectively conduct CBT sessions, utilizing various models to simulate CBT interventions, demonstrating that LLMs could deliver therapeutic content effectively with appropriate guidance, as assessed by human evaluations for content accuracy, empathy, and therapeutic alliance. \cite{30} conducted a comparative study using Socratic questioning to generate CBT responses, comparing models like OsakaED and GPT-4. In a related study, \cite{40} introduced a CBT-based conversational counseling agent that utilizes memory systems and dynamic prompting to address cognitive distortions. The effectiveness of this agent was confirmed through human evaluation, which assessed CBT validity, emotional support, and stability in the conversations generated by the model. Similarly, \cite{97} explored the feasibility of LLMs like ChatGPT-3.5-turbo, ERNIE-3.5-8K, iFlytek Spark V3.0, and ChatGLM-3-turbo in conducting CBT, evaluating their performance based on emotion tendency, structured dialogue pattern, and proactive questioning ability, both with and without the integration of a CBT-specific knowledge base.

% Several studies have also focused on developing frameworks for assessing the performance of LLM therapists. \cite{32} uses simulated clients to evaluate the client-centeredness of LLM therapists, particularly focusing on session outcomes and therapeutic alliance. Furthering this line of research, \cite{37} developed a computational framework for behavioral assessment of LLM therapists, comparing LLM-generated behaviors with those of high- and low-quality human therapy. This framework successfully differentiated between therapeutic behaviors of varying quality, using in-context learning to measure psychotherapy techniques.
 
% \textbf{As Patients.} 
% When LLMs are used to simulate therapists, there's a risk that users might receive inaccurate or inappropriate guidance. In contrast, simulating patients with LLMs reduces these risks by allowing trainees to interact with a wide variety of patient scenarios without depending on potentially flawed therapeutic advice from the model. For example, \cite{51} uses GPT-4 Turbo to simulate patient interactions for the training of mental health professionals. This approach successfully trained trainees in formulating cognitive models, which were assessed by experts and trainees for accuracy and realism.

% Other studies have used LLMs to simulate both therapists and patients. \cite{9} explores the use of a prompt-tuned version of ChatGPT to simulate both psychiatrist and patient roles, enhancing training for psychiatric professionals by providing realistic and empathetic interactions. This study reported high ratings in empathy, fluency, and diagnostic accuracy, as evaluated by real psychiatrists and patients. Similarly, \cite{32} assessed the performance of LLM therapists through client simulations, focusing on client-centered approaches. They simulated both clients and therapists, with assessments based on session outcomes and client-centered questionnaires. Additionally, \cite{56} used GPT-3.5-turbo to simulate and evaluate MI sessions by generating digital patients and therapists with varying skill levels. This method allowed for step-by-step interactions, effectively distinguishing therapist expertise and session quality, and the study measured reliability and validity through questionnaires and expert reviews.

% \textbf{As Partners, Coach, etc.} One study \cite{21} explored using LLMs as partners and coaches to enhance interpersonal effectiveness. Utilizing GPT-3.5 to facilitate CBT sessions and guide interactive exercises, the study found that daily interactions with the LLM reduced anxiety and improved mental health. Weekly therapeutic outcomes and pre-/post-session anxiety measures confirmed the LLM's effectiveness as a supportive partner in providing timely feedback and personalized coaching. Another study \cite{55} explored the use of GPT-4 in developing a journaling app designed for psychiatric patients to facilitate daily reflections. The app prompts users with tailored questions and provides summaries of their entries to support consistent journaling. A four-week field study involving patients diagnosed with major depressive disorder evaluated the app's effectiveness by analyzing the frequency and quality of patient interactions, supplemented by interviews with both patients and mental health professionals.

% \subsubsection{Auto-evaluation of Therapy Sessions}
% LLMs have also been used as auto-evaluators of therapy sessions. \cite{39} used GPT-4 to automatically evaluate the therapeutic working alliance in counseling sessions using methodologies such as Chain-of-Thought prompting, detailed guidelines, and metrics like self-consistency and intra-class correlation among annotators, demonstrating a high degree of alignment with human evaluations. \cite{56} used GPT-3.5 to fill two questionnaires (satisfaction and working alliance) after simulated therapy sessions. Its evaluation ability was validated by demonstrating high internal consistency, effective discrimination between different therapist skill levels, and strong alignment with human expert ratings. Another study \cite{70} used GPT-4 as a zero-shot classifier to evaluate the quality of reflections, where Cohen-Kappa score for inter-rater reliability between GPT-4 and human reviewers.














