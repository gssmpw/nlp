% \section{Fragment-level Flow Matching}
\section{FragFM Framework}
\label{sec:fragfm_framework}

% 전체 모델 개요 설명
We propose \methodname{}, a novel molecular generative framework that utilizes discrete flow matching (DFM) at the fragment level graph.
In this approach, fragments and their connections are represented as nodes and edges respectively.
This enables a discrete flow matching procedure on the resulting fragment-level graph. 
Because a single fragments arrangement can correspond to multiple molecular structures depending on how fragments junctions are permuted, we bridge fragment- and atom-level representations via a KL-regularized autoencoder that reconstructs the atom-level graph from its fragment-level graph with a latent variable. 
The learned latent variable contains the missing information during the fragmentation procedure, and it is generated through the flow matching model in conjunction with the fragment-level graph.

\subsection{Fragment Graph Notation} 
\label{subsec:Fragment notation}

% 문제정의
We represent a molecule at the atom level as a graph  $G = (V, E)$, where  $V$ is the set of atoms, and $E$ represents chemical bonds between them. 
Each node $v_k \in V$ corresponds to a distinct atom, while an edge $e_{kl} \in E$ denotes a bond (including non-bond interactions) between atoms $v_k$ and $v_l$.
At the fragment level, we define a coarse-grained representation of the molecule as a graph $\mathcal{G}=(\mathcal{X}, \mathcal{E})$.
Here, $x_i \in \mathcal{F}$ corresponds to a fragment, while each edge $\varepsilon_{ij} \in \mathcal{E}$ corresponds to the connectivity of the fragments.
Each fragment is interpreted as an atom-level graph.
Specifically, $\{x_i\}_i = \{(V_{i}, E_{i})\}_{i}$ are disjoint sub-graphs of $G=(V, E)$, where $V_{i} \subseteq V$ and $E_{i} \subseteq E$, with $V_i\cap V_j =\varnothing$ for different fragment indices $i, j$.
The edges in the fragment-level graph $\mathcal{E}$ are induced from $E$, meaning that two fragments $F_i, F_j \in \mathcal{X}$ are connected if at least one bond exists between their corresponding atoms, i.e.,
\begin{equation} \label{eq:definition of frag edge}
    \varepsilon_{ij}\in \mathcal{E} \quad \text{if} \quad \exists \, e_{kl} \in E \quad \text{such that} \quad v_k \in V_{i}, v_l \in V_{j}.
\end{equation}
% 문제 정의 
% First, we represent a molecule at the atom level as a graph  $G_a = (V_a, E_a)$, $V_a$ is the set of atoms, and each node  $v_i \in V_a$  corresponds to a discrete atom type.
% $E_a$  is the set of chemical bonds, where each edge  $(v_i, v_j) \in E_a$  represents a bond between atoms  $v_i$  and  $v_j$ , with bond type  $b_{ij}$.
% A molecule can also be denoted at the fragment level, where each fragment is treated as a single unit rather than individual atoms. This leads to a smaller fragment-level graph $G_f = (V_f, E_f)$, where $V_f$ is the set of fragments, and each node $f_i \in V_f$ represents a unique molecular fragment type. $E_f$ is the set of fragment connection types, where $(f_i, f_j) \in E_f$ indicates that fragments $f_i$ and $f_j$ are connected.
% While fragment types can be considered as individual types, at the same time,e each fragment $f_i$ in the fragment-level graph corresponds to a subset of atoms in the atom-level graph, $V_f = \{F_i|F_i \subseteq V_a, \forall i \}$. Each fragment consists of a specific group of atoms, and multiple atoms in $G_a$ collectively define a single fragment node in $G_f$.


\subsection{Molecular Graph Compression by Coarse-to-fine Autoencoder}
\label{subsec:molecular_graph_compression_by_coarse_to_fine_autoencoder}

% Fragment graph 압축 Encoder / Decoder
Recent advances in hierarchical generative models \citep{vqvae2, ldm, hierdiff} have demonstrated the effectiveness of learning structured latent representations through autoencoding, enabling efficient perceptual compression and reconstruction of complex data distributions. 
Motivated by this, we extend discrete generative modeling to molecular graphs by incorporating a coarse-to-fine autoencoding framework, where a fragment-level graph serves as a compressed representation of an atom-level graph.
The fragment-level graph provides a higher-level abstraction of molecular structures. 
However, it inherently introduces ambiguity in reconstructing atomic connectivity due to multiple valid atom-level configurations corresponding to the same fragment arrangement. 
To resolve this, we introduce a coarse-to-fine autoencoder, where the encoder maps the atom-level graph to a fragment-level graph $\mathcal{G}$ along with a continuous latent variable $z$, and the decoder reconstructs the atom-level graph using both the fragment representation and the latent space.
In practice, the decoder only predicts the atom-level connectivity of connected fragments, and the results are discretized by the Blossom algorithm \citep{blossom_alg}.
The details about coarse-to-fine conversion is described in \cref{appsubsec:detilas_of_coarse_to_fine_autoencoder,appsubsec:atom_level_graph_reconstruction_from_fragment_graphs}


% Formally, the encoding and decoding process is defined as:
% \begin{align} \label{eq:definition of ae}
%     \mathcal{G} &= \text{Fragmentation}(G), \nonumber \\
%     z &= \text{Encoder}(G;\theta),  \\
%     \hat{E} &= \text{Decoder}(\mathcal{G}, z; \theta), \nonumber
% \end{align}
% where the decoder reconstructs only those atom-level edges $\hat{E}$ corresponding to the fragment connectivity in the coarse representation.
% 
% % AE 학습 loss 설명
% To ensure that the reconstructed graph faithfully preserves the original molecular structure, we optimize the autoencoder using a reconstruction loss:
% \begin{equation} \label{eq:loss of ae}
%     \mathcal{L}_{\text{recon}} =  \mathbb{E}_{G \sim p_{\text{data}}} 
%     \left[\mathcal{L}_{\text{CE}}(E, \hat{E}) \right].
% \end{equation}
% During the fine-graph sampling procedure, we discretize the decoded edges $\hat{E}$. 
% In this process, we employ the blossom algorithm, which yields robust sampling performance. 
% Further details are provided in \cref{SI: details of coarse-to-fine autoencoder, SI: analysis of coarse-to-fine autoencoder}.


% subsec: Coarse graph에서의 DFM 
\subsection{Discrete Flow Matching for Coarse Graph} 
\label{sec:discrete_flow_matching_for_coarse_graph}

% 모델링 개요 설명 - Class 많은 경우 어떻게 할지
% We aim to model the joint distribution over the fragment-level graph and its latent representation $p(\mathcal{G}, z)$ with the flow-matching formulation. 
% While edges $\mathcal{E}$ and latent $z$ are modeled with standard discrete flow matching \citep{dfm_1} and continuous flow matching \citep{cfm} each, directly modeling $\mathcal{F}$ with DFM is computationally unpractical due to the large number of possible fragment types.
% To address this challenge, we introduce a sub-sampling technique for DFM formulation in large category settings, which we detail in \cref{subsubsec:discrete_flow_mathcing_on_fragment_types}.


% subsubsec: Type에 대한 approximation 설명
% \subsubsection{Discrete Flow Matching on Fragment Types}
%\label{subsubsec:discrete_flow_mathcing_on_fragment_types}

We aim to model the joint distribution over the fragment-level graph and its latent representation $p(\mathcal{G}, z)$ with the flow-matching formulation. 
Because the connectivity variable $\varepsilon\in \mathcal{E}$ is binary, and the latent $z$ is a real-valued vector with low dimension, we follow the DFM approach from \citet{dfm_1} for $\varepsilon$ and adopt the \citet{cfm} for $z$. 
Although the fragment $x\in\mathcal{F}$ is also a discrete variable, the potentially large number of fragment types required to span the molecular space makes the transition rate matrix of continuous time Markov chain (CTMC) in the DFM approach prohibitively large. 
To address this, we introduce a stochastic bag selection strategy; for a fragment type variable $x_1$, we set a fragment type where the stochastic path $\{x_t\}_{t\in[0,1]}$ can take the values.
For a given data $x_1^{1:D}$, we sample a type bag $\mathcal{B}$ that includes the types that are in present following a distribution, $\mathcal{B} \sim \mathbb{Q}(\cdot | x_1^{1:D})$, where $D$ is the dimension of the discrete variables.
% We further demonstrate that training under this selection framework leads to unbiased estimation for the target distribution.
With a clean data $x_1$ and $\mathcal{B}$, we define temporal marginal conditioned to $\mathcal{B}$, based on an linear interpolation with a prior distribution: 
\begin{equation}
    p_{t|1}(x_t | x_1, \mathcal{B}) = t \delta ^ {\mathcal{B}}(x_t, x_1) + (1 - t) p_{0}(x_t | \mathcal{B}),
    \label{eq:define_prob_interpolation}
\end{equation}
where $\delta^{\mathcal{B}}(\cdot, \cdot)=\mathbf{1}_{\mathcal{B}}(\cdot)\delta(\cdot, \cdot)$ represents the Kronecker delta multiplied by the indicator function restricted on $\mathcal{B}$ and $p_0(\cdot|\mathcal{B})$ is uniform over $\mathcal{B}$. 

Discrete flow matching is learning a denoising process meeting the marginal distribution using a CTMC formulation.
For a given fragments bag $\mathcal{B}$, sampling $x_t\vert \mathcal{B}$ begins with an initial distribution $p_0(\cdot|\mathcal{B})$ and propagates through a CTMC with transition rate $R_{t}(\cdot, \cdot \vert \mathcal{B})$ which only allows the transition between the states in the $\mathcal{B}$.
The evolution of the process follows the Kolmogorov forward equation: 
\begin{equation} \label{eq:kolmogorov_forward}
p_{t+dt|t}(y|x_t, \mathcal{B}) = \delta ^ \mathcal{B} (x_t,y) + R_{t}(x_{t},y|\mathcal{B})dt.
\end{equation}
In the sampling phase, $x_1$ is sampled based on \cref{eq:kolmogorov_forward}, which requires $\mathcal{B}$.
Thus, the sampling phase begins with sampling $\mathcal{B}$ from $\mathbb{Q}$ and $x_0$ from $p_0(\cdot \vert \mathcal{B})$, where the unconditional bag distribution $\mathbb{Q}$ obtained by marginalizing out $x_1$, $\mathbb{E}_{x_1\sim p_{\text{data}}}\left[\mathbb{Q}(\cdot\vert x_1)\right]$.
The neural network model approximates the distribution of $x_1$ given $x_t$ and $\mathcal{B}$, which is utilized in computing the transition rate $R_t$.
For the generalization of the neural network to diverse molecules, we incorporated a fragment embedding strategy, allowing the model to make predictions on novel fragment compositions.
More details about coarse graph sampling and training algorithms are described in \cref{appsec:method_details}.


%\cite{dfm_1} presented a special case of valid $x_1$ conditional transition rate, that meets the Kolmogorov equation.
%The transition rate conditioned by $\mathcal{B}$ is then defined as: 
%\begin{equation}
%    R_t(x_t, y | x_1, \mathcal{B}) =
%    \frac{
%    \text{ReLU} \Big[ \partial_t p_{t|1}(y | x_1, \mathcal{B}) 
%    - \partial_t p_{t|1}(x_t | x_1, \mathcal{B}) \Big]
%    }{
%    Z_t^{>0} p_{t|1}(x_t | x_1, \mathcal{B})
%    }
%    \text{ for } 
%    x_t \neq y, 
%\end{equation}
%
%The $D$ dimensional joint variable $x_t^{1:D}$ is modeled with the follwing process,
% \begin{equation}
%     \tilde{p}_{t+\Delta t|t} \left( x_{t+\Delta t}^{1:D} | x_t^{1:D}, \mathcal{B} \right) 
%     = \prod_{d=1}^{D} 
%     \left( \delta^{\mathcal{B}} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} \right) 
%     + \mathbb{E}_{p_{1|t}^{(d)} 
%     \left( x_1^{(d)} | x_t^{1:D}, \mathcal{B}\right)} \left [ R_t^{(d)} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} | x_1^{(d)}, \mathcal{B}\right) \right] \Delta t \right)
% \end{equation}



% To get access to the Euler step of the whole possible fragment types, we take the expectation over $\mathcal{B'} \sim \mathcal{P}(\cdot|z_t^{1:D})$ where $\mathcal{P}(\cdot|z_t^{1:D})$ denotes sampling types including the types $z_t^{1:D}$. 
%     \begin{equation}
%         \tilde{p}_{t+\Delta t|t} \left( z_{t+\Delta t}^{1:D} | z_t^{1:D} \right) = \mathbb{E} _ {\mathcal{B'} \sim \mathcal{P}(\cdot|z_t)} \tilde{p}_{t+\Delta t|t} \left( z_{t+\Delta t}^{1:D} | z_t^{1:D}, \mathcal{B'} \right)
%     \end{equation}
% The loss is then
%     \begin{equation}
%         \mathcal{L}_\text{S-DFM} = \,
%         \mathbb{E}_{t \sim \mathcal{U}[0,1],  \,
%         (z_t, z_1) \sim p_{t|1}(\cdot, \cdot), \,
%         \mathcal{B'} \sim \mathcal{P}(\cdot | z_t^{1:D})} \, 
%         \left[- \sum_d \log \left( p_{1|t}^{(d)} (z_1^{(d)} | z_t^{1:D}, \mathcal{B'}; \theta) \right)\right],
%     \end{equation}
% 