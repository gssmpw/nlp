\section{Method Details}
\label{appsec:method_details}


\subsection{Details of Coarse-to-fine Autoencoder}
\label{appsubsec:detilas_of_coarse_to_fine_autoencoder}

\input{figures/method_ae}

We adopted a KL-regularized autoencoder for coarse-to-fine graph conversion.
The coarse-grained graph representation $\mathcal{G}$ can be interpreted as a compressed version of atom-level fine graph $G$.
In the fragmentation procedure, atom-level graph loose the fine-grained connection information.
For reconstructing the original atom-level graph, the fragment-level graph and the missing information is required, which is encoded in the latent variable $z$.
Formally, the encoding and decoding process is defined as:
\begin{align} \label{eq:definition of ae}
    \mathcal{G} &= \text{Fragmentation}(G), \nonumber \\
    z &\sim q_\theta =\mathcal{N}(\text{Encoder}(G;\theta), \sigma),  \\
    \hat{E} &= \text{Decoder}(\mathcal{G}, z; \theta), \nonumber
\end{align}
where the decoder reconstructs only those atom-level edges $\hat{E}$ corresponding to the fragment connectivity in the coarse representation.

% AE 학습 loss 설명
To ensure that the reconstructed graph faithfully preserves the original molecular structure, we optimize the autoencoder using a reconstruction loss. 
Additionally, we introduce a small KL regularization term to the training loss for latent variable to enforce a well-structured and unscaled latent space:
\begin{equation} \label{eq:loss_ae_2}
    \mathcal{L}_{\text{VAE}}(\theta)
    = \mathbb{E}_{G \sim p_{\text{data}}} 
    \left[\mathcal{L}_{\text{CE}}\left(E, \hat{E}(\theta)\right)  
    + \beta D_{\text{KL}}\left( q_{\theta}(z | G) \parallel p(z) \right)\right].
\end{equation}
We set a low regularization coefficient of $\beta = 0.0001$ to maintain high-fidelity reconstruction.

We discretize the decoded edges $\hat{E}$ during the fine-graph sampling procedure. 
In this process, we employ the blossom algorithm detailed in \cref{appsubsec:atom_level_graph_reconstruction_from_fragment_graphs}, which yields robust sampling performance. 


\subsection{Fragment Denoising Flow matching}

% We now want to reverse the noising process defined \cref{eq:define_prob_interpolation}. \citet{dfm_1} proposed a special case of valid $x_1$ conditional transition rate matrix that meets the Kolmogorov forward equation \cref{eq:kolmogorov_forward}, we modified the equation in the $\mathcal{B}$ conditioned form, the conditional rate matrix in $\mathcal{B}$ is as:
For sampling, we now want to design a stochastic process that meets the temporal marginal distributions of \cref{eq:define_prob_interpolation} along with the Kolmogorov forward equation \cref{eq:kolmogorov_forward}.
\citet{dfm_1} proposed such a transition rate conditioned on $x_1$ that aligns with the linear interpolated distributions, and we modified the equation in the $\mathcal{B}$ conditioned form, which is as:
\begin{equation}
    R^{*}_t(x_t, y | x_1, \mathcal{B}) =
    \frac{
    \text{ReLU} \Big[ \partial_t p_{t|1}(y | x_1, \mathcal{B}) 
    - \partial_t p_{t|1}(x_t | x_1, \mathcal{B}) \Big]
    }{
    Z_t^{>0} p_{t|1}(x_t | x_1, \mathcal{B})
    } \quad \text{for} \quad x_t \neq y, 
\end{equation}
while $Z_t^{>0}=|z_t:p_{t|1}(x_t|x_1,\mathcal{B})> 0|$. The entries for $z_t=y$ are calculated by normalization. 
For a $D$ dimensional case, where $D$ is the number of dimensions we model, \citet{dfm_1} proposed an approximation of CTMC under mild conditions, independently conducting the Euler step for each dimension. With a finite time step $\Delta t$: 
\begin{equation}\
    \resizebox{\linewidth}{!}{
    $
    \tilde{p}_{t+\Delta t|t} \left( x_{t+\Delta t}^{1:D} | x_t^{1:D}, \mathcal{B} \right) 
     = \prod_{d=1}^{D} 
     \left( \delta^{\mathcal{B}} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} \right) 
     + \mathbb{E}_{p_{1|t}^{(d)} 
     \left( x_1^{(d)} | x_t^{1:D}, \mathcal{B}\right)} \left [ R_t^{(d)} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} | x_1^{(d)}, \mathcal{B}\right) \right] \Delta t \right)
     $}.
     \label{eq:euler_step_ctmc}
\end{equation}

Based on the transition kernel on \cref{eq:euler_step_ctmc}, we can sample $x_1$ given the fragments bag $\mathcal{B}$.
To define $\mathbb{Q}(\cdot\vert x_1)$ for sampling fragment bags, we use a two-stage procedure: (1) select a fixed number of molecules from the data distribution and include $x_1$ among them, and (2) gather all fragments from these molecules to form the fragment bag $\mathcal{B}$.
During training, we sample $\mathbb{Q}(\cdot \vert x_1)$. 
However, in the sampling (inference) phase, $x_1$ is not available a priori, so we employ the unconditional distribution $\mathbb{Q}$ given by,
\begin{equation}
    \mathbb{Q} = \mathbb{E}_{x_1\sim p_\text{data}}\left[ \mathbb{Q}(\cdot\vert x_1)\right].
\end{equation}
Sampling from $\mathbb{Q}$ follows a similar two-stage procedure, except we do not force the inclusion of any particular $x_1$ in the initial set of molecules.
In practice, we gathered 256 molecules from the training dataset to sample the fragments bag.

\begin{comment}
    ==========
    The $D$ dimensional joint variable $x_t^{1:D}$ is modeled with the follwing process,
    \begin{equation}
        \tilde{p}_{t+\Delta t|t} \left( x_{t+\Delta t}^{1:D} | x_t^{1:D}, \mathcal{B} \right) 
         = \prod_{d=1}^{D} 
         \left( \delta^{\mathcal{B}} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} \right) 
         + \mathbb{E}_{p_{1|t}^{(d)} 
         \left( x_1^{(d)} | x_t^{1:D}, \mathcal{B}\right)} \left [ R_t^{(d)} \left( x_t^{(d)}, x_{t+\Delta t}^{(d)} | x_1^{(d)}, \mathcal{B}\right) \right] \Delta t \right)
    \end{equation}
    To get access to the Euler step of the whole possible fragment types, we take the expectation over $\mathcal{B'} \sim \mathcal{P}(\cdot|z_t^{1:D})$ where $\mathcal{P}(\cdot|z_t^{1:D})$ denotes sampling types including the types $z_t^{1:D}$. 
    \begin{equation}
        \tilde{p}_{t+\Delta t|t} \left( z_{t+\Delta t}^{1:D} | z_t^{1:D} \right) = \mathbb{E} _ {\mathcal{B'} \sim \mathcal{P}(\cdot|z_t)} \tilde{p}_{t+\Delta t|t} \left( z_{t+\Delta t}^{1:D} | z_t^{1:D}, \mathcal{B'} \right)
    \end{equation}
    The loss is then
    \begin{equation}
        \mathcal{L}_\text{S-DFM} = \,
        \mathbb{E}_{t \sim \mathcal{U}[0,1],  \,
        (z_t, z_1) \sim p_{t|1}(\cdot, \cdot), \,
        \mathcal{B'} \sim \mathcal{P}(\cdot | z_t^{1:D})} \, 
        \left[- \sum_d \log \left( p_{1|t}^{(d)} (z_1^{(d)} | z_t^{1:D}, \mathcal{B'}; \theta) \right)\right],
    \end{equation}
\end{comment}


\subsection{Neural network Parameterization}
\label{appsubsec:neural_network_parameterization}

\input{figures/method_frag_pred}

First, we model the coarse-to-fine autoencoder with simple MPNN. We model $p_{1|t}(G_1 | G_t; \phi)$ using a fragment embedding message passing neural network (fragment encoder) and a graph transformer (GT) to facilitate message passing in the fragment-level graph. 
Through fragment MPNN, each fragment type in $\mathcal{F}$ is represented as a 1-dimenstional latent vector: 
\begin{equation}
    h_i = \text{FragmentEncoder}(x_i;\phi), \quad \text{for} \quad x_i \in \mathcal{F}.
\end{equation}
Using these embedded fragment representations, we construct a fragment-level graph where nodes correspond to individual fragment embeddings, edges representing fragment-level connectivity, and global features $z$, a latent variable from the coarse-to-fine autoencoder. 
We incorporate the graph transformer backbone from \citet{digress, defog} to propagate information across the fragment graph. 
After $l$ layers of graph transformer, we obtain node embeddings, edge embeddings, and the global embedding for the fragment graph, denoted as $h_i^{(l)}, e_{ij}^{(l)}, g^{(l)}$. 
For fragment edge types $e_{ij}$ and the continuous latent variable $z$, \methodname{} utilizes simple linear layers.
For fragment type prediction, we perform a softmax operation over the fragment bag, where the logit score is computed as the inner product between the fragment embeddings and the node embeddings:
\begin{equation}
%    \hat{p}_i = \text{Softmax} \left(h_i^{(l)} \cdot x_j \right), \quad \text{for} \quad x_j \in \mathcal{B}.
    \hat{p}_i = \text{Softmax} \left(\{h_i^{(l)} \cdot h_k^{(0)}\}_{x_k\in\mathcal{B}} \right).
\end{equation}
% The training and algorithm of whole \methodname{} framework are in XXX.


\subsection{Atom-level Graph Reconstruction from Fragment Graphs}
\label{appsubsec:atom_level_graph_reconstruction_from_fragment_graphs}

We utilize the Blossom algorithm \citep{blossom_alg} to determine the optimal matching in the atom-level connectivity given coarse-to-fine decoder output.
% The Blossom algorithm is an optimization technique that finds the maximum matching in general graphs by iteratively contracting and expanding odd-length cycles (blossoms) to identify augmenting paths.
The Blossom algorithm is an optimization technique used to find the maximum matching in general graphs by iteratively contracting and expanding odd-length cycles (blossoms) to identify augmenting paths efficiently. 
We leverage this algorithm in our framework to accurately reconstruct atom-level connectivity from fragment-level graphs, ensuring chemically valid molecular structures.
The algorithm takes as input the matching nodes $V_m$, edges $E_m$, and edge weights $w_{ij}$. 
Once the fragment-level graph and the probabilities of atom-level edges from the coarse-to-fine autoencoder are computed, we define $V_m \subseteq \hat{V}$ as the set of junction atoms in fragment graphs, which are marked as * in \cref{fig:moses_fragments}, and $E_m$ as the set of connections between junction atoms belonging to connected fragments. 
Formally, an edge $e_{kl}$ exists in $E_m$ if the corresponding atoms belong to different fragments that are connected in the fragment-level graph, expressed as: 
\begin{equation}
    e_{kl} \in E_m \quad \text{if} \quad v_k \in \hat{V}_i, \quad v_l \in \hat{V}_j, 
    \quad \text{and} \quad \varepsilon_{ij} \in \mathcal{E}.
\end{equation}
The edge weights  $w_{ij}$  correspond to the predicted log probability of each connection obtained from the coarse-to-fine autoencoder. 
The Blossom algorithm is then applied to solve the maximum weighted matching problem, formulated as
\begin{equation}
    M^* = \text{argmax}_{M \subseteq E_m} \sum_{(i,j) \in M} w_{ij}.
\end{equation}
Here, $M^{*}$ represents the optimal set of fragment-level connections that best reconstructs atom-level connectivity; maximizing the joint probability of the autoencoder preidction. Although the algorithm has a $O(N^3)$ complexity for $N$ fragment junctions,  its computational cost remains negligible in our case, as the number of fragment junctions is relatively tiny compared to the total number of atoms in a molecule.


\subsection{Sampling Techniques}
\label{appsubsec:sampling_techniques}


\subsubsection{Target Guidance}
\label{appsubsubsec:target_guidance}

Diffusion and flow models are typically designed to predict the clean data $G_{f,1} = \{V_{f,1}, E_{f,1}\}$ from noisy input. 
Building on this, \citep{defog} proposed a modified sampling method for DFM by adjusting the rate matrix toward the predicted clean data. 
Specifically, the rate matrix is redefined as 
\begin{equation}
    %R_t(x_t, y \mid x_1) = R^*_t(x_t, y \mid x_1) + R^\omega_t(x_t, x_{t+dt} \mid x_1)
    R_t(x_t, y \mid x_1) = R^*_t(x_t, y \mid x_1) + R^\omega_t(x_t, y \mid x_1)
    \label{eq:target_guidance_rate_matrix_adjustment}
\end{equation}
% for $z_t \neq z_{t+dt}$, where
for $x_t \neq y$, where
\begin{equation}
    R^\omega_t(x_t, y \mid x_1) 
    = \omega \frac{\delta(y, x_1)}{Z^{>0}_t p_{t|1}(x_t \mid x_1)}
    \label{eq:define_target_guidance}
\end{equation}
This modification introduces a slight $O(\omega)$ violation of the Kolmogorov equation. 
However, empirical findings suggest that a small $\omega$ improves sample quality without significantly distorting the learned distribution. 
For our results, we use a small value of $\omega=0.002$.
%We report FragFM's results on different levels of target guidance in \cref{appsubsec:effect_of_sampling_techniques}


\subsubsection{Detailed Balance}
\label{appsubsubsec:detailed_balance}

The space of valid rate matrices extends beyond the original formulation of  $R^*_{t}(z_t, z_{t+dt} \mid z_1)$, meaning that alternative formulations can still satisfy the Kolmogorov equation.
Building on this, \citet{dfm_1} explored this space and demonstrated that any rate matrix  $R^{DB}_t$  satisfying the detailed balance condition,
\begin{equation}
    p_{t|1}(x_t \mid x_1) R^{DB}_t(x_t, y \mid x_1) 
    = p_{t|1}(y \mid x_1) R^{DB}_t(y, x_t \mid x_1),
    \label{eq:detailed_balance}
\end{equation}
can be used to construct a modified rate matrix:
\begin{equation}
    R^\eta_t = R^*_t + \eta R^{DB}_t, \quad \eta \in \mathbb{R}^+,
    \label{eq:stochastic_rate_matrix}
\end{equation}
which remains consistent with the Kolmogorov equation. 
Increasing $\eta$ introduces additional stochasticity into CTMC, enabling more transition pathways between states. 
We integrate this stochasticity into our \methodname{} framework, enabling variability in fragment-type transitions while maintaining valid generative pathways. 
We set  $\eta = 0.1$ in our experiments.
%We provide the effect of detailed balance in \cref{appsubsec:effect_of_sampling_techniques}.


\subsection{Classifier-Free Guidance}
\label{appsubsec:classifier-free_guidance}

Classifier-Free Guidance (CFG) allows for controllable molecular generation by interpolating between conditioned and unconditioned models, eliminating the need for explicit property classifiers \citep{classifier_free_guidance}. 
This approach, widely used in continuous diffusion models, was recently extended to discrete flow matching \citep{unlocking_guidance_dfm}. 
We adopt this technique to enhance the controllability of \methodname{} while maintaining sample diversity.

During training, \methodname{} learns both conditional rate matrix $R^\theta_t(x_t, y \mid c)$ and an unconditional rate matrix $R^\theta_t(x_t, y)$ simultaneously by conditioning on property labels $c$ for $90\%$ of samples and conditioning with the masked label $\varphi$ for the remaining $10\%$. 
During sampling, the rate matrix is adjusted using the guidance level $\gamma$:
\begin{equation}
    R^{\theta, \gamma}_t(x_t, y \mid c) = 
    R^\theta_t(x_t, y \mid c)^\gamma 
    R^\theta_t(x_t, y \mid \varphi)^{1-\gamma}.
    \label{eq:cfg_guidance}
\end{equation}
Setting $\gamma=0$ corresponds to purely unconditional generation while increasing $\gamma$ strengthens class adherence, biasing transitions toward the desired property distribution. However, excessively high $\gamma$ values can constrain exploration, generating molecules that diverge from the overall unconditional data distribution. 
\citet{classifier_free_guidance} demonstrated that this trade-off follows a characteristic pattern: as guidance strength increases, sample fidelity (e.g., lower FID) improves at the cost of class adherence (e.g., lower IS).


\section{Experimental Details}
\label{appsec:experimental_details}


\subsection{Baselines}
\label{subsec:baselines}

Our experiments compare \methodname{} with several state-of-the-art graph and molecule generative models. For non-denoising methods, we include JT-VAE \citep{jtvae}, GraphINVENT \citep{graphinvent}, NAGVAE \citep{nagvae}, and MCTS \citep{mcts_molecule}. For diffusion-based models, we evaluate DiGress \citep{digress}, DisCo \citep{disco}, and Cometh \citep{cometh}. In addition, we compare with DeFog \citep{defog} for flow-based approaches.


\subsection{Dataset and Metrics}
\label{appsubsec:dataset_and_metrics}


\subsubsection{MOSES and GuacaMol}
\label{appsubsubsec:moses_and_guacamol}

MOSES and GuacaMol provide standardized molecular-generation benchmarking frameworks, offering predefined training, test datasets, and automated evaluation metrics. Validity refers to the percentage of generated molecules that adhere to fundamental valency constraints, ensuring chemically plausible structures. Uniqueness quantifies the proportion of generated molecules with distinct SMILES representations, indicating non-isomorphism. Novelty measures the number of generated molecules that do not appear in the training dataset, assessing the model’s ability to create new structures.

The filter score evaluates the percentage of molecules that satisfy the same chemical constraints applied during test set construction. Fréchet ChemNet Distance (FCD) quantifies the similarity between training and test molecules based on learned neural network embeddings. SNN (Similarity to Nearest Neighbor) captures how closely generated molecules resemble their closest counterparts in the training set based on Tanimoto similarity. Scaffold similarity assesses how well the distribution of Bemis-Murcko scaffolds in the generated molecules aligns with that of actual molecules. Finally, KL divergence compares the distributions of various physicochemical properties.


\subsubsection{Natural Product Generation Benchmark}
\label{appsubsubsec:natural_product_generation_benchmark}

While understanding natural products is crucial, their intrinsically complex structures and large molecular sizes pose significant challenges for molecular design.
To address this, we designed a natural product generation benchmark to evaluate the ability of generative models to capture and reproduce the biochemical characteristics of natural products.

We  first preprocessed the COCONUT dataset \citep{coconut, coconut2}
, the most extensive open-access collection of natural products by filtering out molecules, including charges, retaining only natural compounds. 
We also excluded metal-containing molecules and selected only those composed of the following atom types: B, N, C, O, F, Si, P, S, Cl, Br, I, Se, and As. 
Additionally, molecules exceeding 99 heavy atoms were removed to avoid arbitrarily large molecules.
This processing resulted in a final dataset of 416,249 molecules, which was randomly split into training ($85\%$), validation ($5\%$), and test ($10\%$) sets.

We sampled $10,000$ molecules for benchmarking and evaluated them based on validity, uniqueness, novelty, and Fréchet ChemNet Distance (FCD), following the MOSES benchmarking protocol.
However, achieving high validity and uniqueness alone does not guarantee that the generated molecules resemble natural products.
To address this, we applied the hierarchical classification scheme from NPClassifier \citep{npclassifier}, which categorizes molecules at three levels: Pathway, representing broad biosynthetic origins with seven categories; Superclass, defining structural groupings within pathways within 70 categories; and Class, providing a finer-grained classification of 672 structural categories.
We compute the Kullback-Leibler (KL) divergence across these categorical distributions to assess the alignment between generated molecules and the training dataset.
Additionally, since the COCONUT dataset includes NP-likeness scores \citep{np_likeness_score}, we also evaluated the KL divergence of the NP-likeness score distribution, quantifying how closely the generated molecules resemble authentic natural products. 


%\clearpage
\section{Additional Results}
\label{appsec:additional_results}

\subsection{Results on GuacaMol Dataset}

\input{tables/guacamol}


\subsection{Analysis of Coarse-to-fine Autoencoder}

\input{tables/ae_accuracy}

To evaluate the performance of the coarse-to-fine autoencoder, we assess the accuracy of bond and graph reconstruction. 
As shown in \cref{tab:ae_accuracy}, the model achieves nearly perfect bond and graph recovery on the MOSES and GuacaMol datasets, with bond accuracy exceeding 99\%, demonstrating its reliability in preserving the information of atom-level graph.  The model also achieves strong recovery performance on the more complex COCONUT dataset, demonstrating its robustness in handling diverse and structurally complicated molecular graphs.


\begin{comment}
\subsection{Other Fragmentization Method}
RBRICS MOSES.

\subsection{Effect of Sampling Techniques}
\label{appsubsec:effect_of_sampling_techniques}

\input{figures/target_guidance_effect}

\input{figures/noise_effect}

We show the effect of target guidance ($\omega$) and stochasticity ($\eta$) in \cref{fig:target_guidance_effect,fig:noise_effect}. 
While increasing amount of target guidance, the tradeoff between     
\end{comment}


\subsection{Sampling  Step}
\label{appsubsec:sampling_step}

\input{tables/sample_step}

A key challenge in generative models based on stochastic processes is the need for multiple iterative refinement steps, which can significantly impact computational efficiency. 
While increasing the number of denoising steps generally improves the sampling of diffusion and flow-based models, it also extends the time required for sampling, making large-scale generation impractical. 
To evaluate this trade-off, we analyzed how different generative models behave under varying step conditions, focusing on their ability to maintain validity, uniqueness, and structural diversity. 

As shown in \cref{tab:sample_step}, reducing the number of denoising steps leads to a general decline in molecular quality across all models.
However, the extent of this degradation varies considerably depending on the model architecture.
DiGress, for example, suffers a catastrophic performance drop, achieving only $6.3\%$ validity and an FCD of $9.30$ at $10$ steps, highlighting its heavy reliance on many iterative refinements.
Continuous-time models, such as DeFoG and Cometh, exhibit better robustness but still experience a significant decline in performance when operating with a low number of steps.

In contrast, \methodname{} maintains high validity even with significantly fewer denoising steps.
In fact, at just 10 steps, \methodname{} achieves $95.8\%$ validity while preserving other key MOSES benchmark metrics.
This performance surpasses other models, typically requiring at least $50$ to $100$ steps to reach comparable results.
This improvement can be attributed to the fragment-based discrete flow matching approach, which reduces the number of nodes and edges in the molecular graph, lowering the computational complexity of edge prediction.
Since molecular fragments inherently capture larger structural motifs, they provide a more structured and stable generative process, allowing for efficient sampling with fewer denoising steps while preserving overall molecular validity.


\clearpage
\subsection{Sampling Time Analysis}
\label{appsubsec:sampling_time_analysis}

\input{tables/sample_time}

While graph representations exhibit a quadratic increase in edge dimensions as the number of nodes grows, fragment-level graphs contain significantly fewer nodes and edges, resulting in lower computational complexity than atom-level graphs. 
A detailed analysis of sampling time is provided in \cref{tab:sampling_time}, where \methodname{} demonstrates the fastest sampling time across all datasets with 500 sampling steps.
Furthermore, as shown in \cref{tab:sample_step}, \methodname{} achieves comparable or superior performance to baseline models even with just 50 sampling steps. This suggests that its sampling time can be further optimized, enabling speeds $\times10$ to $\times30$ times faster than other models while maintaining the generative quality.


\subsection{Conditional Generation}
\label{appsubsec:conditional_generation}

\input{figures/moses_condition_2}


\clearpage
\subsection{Visualization of Generated Molecules}
\label{appsubsec:visualization_of_generated_molecules}

\input{figures/sample_moses}

\input{figures/sample_guacamol}

\input{figures/sample_coconut}

\clearpage

\input{figures/sample_coconut_class}

\input{figures/sample_digress_coconut}

\clearpage

\input{figures/moses_fragments}