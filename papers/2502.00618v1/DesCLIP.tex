%\documentclass[lettersize,journal]{IEEEtran}
\documentclass[10pt,twocolumn,twoside]{IEEEtran}

\usepackage{amsmath,amsfonts}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{array}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{epstopdf}

\usepackage[table]{xcolor}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{braket}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
\makeatother

\begin{document}

%\title{DesCLIP: Continually Tuning Vision-Language Models with General Attribute Descriptions}
\title{DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models}
\author{Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li

 \thanks{Corresponding author: Fanman Meng. The authors are with the School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China (e-mail: cyhe@std.uestc.edu.cn; zihuanqiu@std.uestc.edu.cn; fmmeng@uestc.edu.cn; lfxu@uestc.edu.cn; qbwu@uestc.edu.cn; hlli@uestc.edu.cn.}
}

   
\maketitle
\begin{abstract}
Continual adaptation of vision-language models (VLMs) focuses on leveraging cross-modal pretrained knowledge to incrementally adapt for expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research often focuses on connecting visual features with specific class text in downstream tasks, overlooking the latent relationships between general and specialized knowledge. Our findings reveal that forcing models to optimize inappropriate visual-text matches exacerbates forgetting of VLMs. To tackle this issue, we propose DesCLIP, which leverages general attribute (GA) descriptions to guide the understanding of specific class objects, enabling VLMs to establish robust \textit{vision-GA-class} trilateral associations rather than relying solely on \textit{vision-class} connections. Specifically, we introduce a language assistant to generate concrete GA description candidates via proper request prompts. Then, an anchor-based embedding filter is designed to obtain highly relevant GA description embeddings, which are leveraged as the paired text embeddings for visual-textual instance matching, thereby tuning the visual encoder. Correspondingly, the class text embeddings are gradually calibrated to align with these shared GA description embeddings. Extensive experiments demonstrate the advancements and efficacy of our proposed method, with comprehensive empirical evaluations highlighting its superior performance compared to existing pretrained and VLM-based continual learning methods.
\end{abstract}

\begin{IEEEkeywords}
Continual learning, vision-language model, general attribute description, knowledge forgetting
\end{IEEEkeywords}


\section{Introduction}
In recent years, deep models pretrained on large-scale datasets have achieved remarkable success across visual, linguistic, and multi-modal domains. Pretrained vision-language models (VLMs), exemplified by CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling}, have demonstrated substantial promise in handling open-vocabulary tasks. Despite their strong zero-shot capabilities in common domains, VLMs often underperform on specialized tasks, such as distinguishing low-quality images or identifying fine-grained object categories. Consequently, significant efforts \cite{zhou2022learning,zhou2022conditional,yu2023task,10171397, yao2023visual,yao2024tcp} have focused on adapting VLMs on downstream datasets to adapt to these new tasks. However, as the demand for and volume of data continue to grow, incorporating previous models and data for joint training imposes substantial storage and computational overhead. Considering the significant cost of repeatedly training foundation models, exploring continual learning (CL) becomes particularly valuable in this context.

Recently, Zheng et al. \cite{zheng2023preventing} and Zhang et al. \cite{zhang2024overcoming} have highlighted the risk of losing existing generalization capabilities when adjusting VLMs with generic knowledge to specialized domain models. This adjustment may result in the model losing effectiveness on prior tasks and lacking potential for optimization on subsequent tasks. This phenomenon, known as catastrophic forgetting in the field of continual learning, is particularly pronounced o n VLMs. Unlike conventional scenarios \cite{li2017learning,10347466,10520827,9899753}, catastrophic forgetting o f VLMs impacts not only the task-specific knowledge of previously learned tasks but also the extensive pretrained knowledge, presenting significant challenges in adjusting VLMs for continual tasks.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/intro_fig.pdf}
  \caption{(a) Existing methods: learning to match unfamiliar specific classes leads to a risk of forgetting. (b) Ours: learning to construct connections with highly relevant general attributes and gradually calibrate class-text embeddings.}
  \label{fig:intro}
\end{figure}

%Continual learning (CL) \cite{li2017learning,10347466,10520827,9899753} aims to explore how deep models can retain previously learned knowledge while acquiring new tasks. Numerous efforts \cite{li2017learning, douillard2020podnet, zhu2021prototype, wang2022foster} have guided the continual learning process of deep models. However, traditional CL approaches have primarily focused on small-scale models with random initialization and full fine-tuning methods, which are not well-suited for pretrained VLMs. Recently, with the success of large pretrained vision models \cite{dosovitskiy2020image}, many studies have begun to explore continual learning based on pretrained vision models \cite{zhou2024expandable, zhou2023revisiting, wang2022learning, wang2022dualprompt}. These studies primarily focus on incorporating additional components related to incremental tasks (e.g., learnable prompts \cite{wang2022learning, wang2022dualprompt} and adapters \cite{zhou2024expandable, zhou2023revisiting}) into the original pretrained framework.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/intro_exp2.pdf}
  \vspace{-2.5em}
  \caption{(a) Continual fine-tuning with different task orders: familiar-first order and unfamiliar-first order. (b) Initially matching unfamiliar class texts leads to more severe forgetting, which negatively impacts the learning of subsequent tasks, resulting in overall poorer CL performance. (c) CKA \cite{kornblith2019similarity} similarities of representation compared to a pretrained CLIP. Learning to match unfamiliar classes further disrupts the integrity of pretrained representations.}
  \label{fig:intro_exp}
\end{figure}

Over the past few years, research \cite{wang2023attriclip, zhang2024overcoming, xiang2024language, yu2024boosting, zheng2023preventing} has explored how VLMs can adapt to CL tasks. Conventional adapting approaches \cite{zhou2022learning, zhou2022conditional, yao2023visual, pantazis2022svl, xin2024vmt} have shown limited effectiveness in adapting pretrained VLMs for incremental tasks. This limitation is primarily due to their reliance on shared structures and prompts across all tasks, which often results in forgetting of previously learned knowledge when accommodating new information. To address this issue, Wang et al. \cite{wang2023attriclip} proposed AttriCLIP, a sparse prompt selection mechanism that selects abstract attributes with high visual relevance to enrich textual hint prompts. However, these prompts depend solely on image conditions and lack association with class-relevant information, which restricts the effectiveness. Other works \cite{wang2022sparcl, konishi2023parameter, zhang2024overcoming} have focused on selectively updating parameters of pretrained models to better support incremental continual tasks. More recently, approaches incorporating additional structures \cite{xiang2024language, yu2024boosting} with sparse mechanisms have shown promise in mitigating conflicts with prior knowledge and alleviating forgetting of VLMs. Additionally, Zheng et al. \cite{zheng2023preventing} and Yu et al. \cite{yu2025select} use the additional reference datasets to perform knowledge distillation, effectively mitigating the forgetting of generic knowledge. 

Although these studies have demonstrated some effectiveness in mitigating knowledge forgetting of VLMs, they can largely be regarded as adaptations of traditional and pretrained-vision-model-based CL methods, tailored to fit the VLM framework. Yet, they fail to fully exploit the robust cross-modal knowledge associations established during the VLM pretraining phase. For instance, these approaches overlook the visual-textual associations established via pretraining, instead relying heavily on rudimentary textual prompts (e.g., \texttt{A photo of a [CLS], [XXX] [CLS]}) to correlate with visual information. This introduces a risk of forgetting in unfamiliar downstream tasks. For example, CLIP may struggle with unfamiliar categories like ``European garden spider," resulting in a weak correspondence between its visual and textual representations. Forcing this association can lead to overfitting, which in turn accelerates the forgetting of pretrained and previously learned knowledge. In Fig. \ref{fig:intro_exp}, we demonstrate that beginning with unfamiliar tasks (characterized by low image-text similarity confidence) or from familiar tasks yields markedly different evaluation results on selected subtasks of ImageNet \cite{deng2009imagenet}. Fig. \ref{fig:intro_exp} (b) underscores that forcibly aligning unfamiliar visual-text pairs hinders knowledge retention of VLMs, detracting from subsequent task learning. Additionally, Fig. \ref{fig:intro_exp} (c) uses centered kernel alignment (CKA) \cite{kornblith2019similarity} to analyze representation similarity of continually tuned CLIP to a pretrained CLIP. It can be observed that matching unfamiliar classes further disrupts integrity of pretrained representation, hindering general knowledge preservation.

Another line of research \cite{pratt2023does, yi2024leveraging, song2024fd, saha2024improved} have attempted to enrich textual descriptions of specific classes to assist VLMs in understanding objects in downstream tasks. However, the descriptions generated lack interaction with visual information or fail to provide class-specific representative details, making it difficult to ensure a reliable understanding of the objects.

To overcome these limitations, we emphasize the transfer from generalized knowledge to specialized insights. To our knowledge, existing research has not focused on addressing forgetting by establishing robust associations between general attributes and specialized downstream class. Our approach guides the continual learning process by exploring the context encoding ability of the language branch, forming strong links between general and specialized knowledge. As shown in Fig. \ref{fig:intro}, we advocate for visual representations that align closely with highly-relevant general attribute (GA) embeddings, which are well-known to VLMs, instead of relying on naive class-text embeddings. This prevents the risk of overfitting to unfamiliar classes, as such overfitting can lead to knowledge forgetting. By gradually calibrating text embeddings to align with shared GA embeddings, we form GA-class associations for these incremental downstream tasks. In essence, we redirect the focus from conventional \textbf{vision-class text} connections to establishing robust \textbf{vision-GA-class} trilateral associations, enabling a more effective knowledge transfer that significantly mitigates forgetting of VLMs.
In summary, our main contributions are as follows:

\begin{itemize}
    \item We revisit the continual learning of VLMs, focusing on the incremental transfer from generalized to specialized knowledge. By introducing concrete descriptions of general attributes (GAs), we establish more robust vision-GA-class trilateral associations during downstream incremental phases, effectively mitigating forgetting caused by inappropriate visual-text matching.
    \item We propose an anchor-based embedding filter to identify and retain GA description embeddings highly relevant to visual representations. Building on this, we introduce a GA-Guided progressive visual-textual alignment scheme to guide the learning process.
    \item Our method introduces no additional overhead in terms of model structure, data replay, or feature rehearsal storage. Extensive experiments on CIFAR100 \cite{krizhevsky2009learning}, ImageNet \cite{deng2009imagenet}, and CUB-200 \cite{wah2011caltech} demonstrate the exceptional performance of our approach. Thorough ablation studies and analyses further corroborate its effectiveness.
\end{itemize}


\section{Related Work}
\label{sec:Related Work}

\subsection{Continual Learning}
\label{sec:Continual Learning}

Continual Learning (CL) investigates how deep models can incrementally learn knowledge. Existing CL research can be categorized into several types based on the strategies they employ. Among these, regularization-based methods \cite{li2017learning, kirkpatrick2017overcoming, douillard2020podnet} introduce regularization terms during model training to penalize forgetting old knowledge. These regularization terms can either focus on protecting model parameters \cite{kirkpatrick2017overcoming} or on output distributions \cite{li2017learning, rebuffi2017icarl} (e.g., knowledge distillation). Dynamic network-based methods \cite{yan2021dynamically, boschini2022class, wang2022foster, zhou2022model} aim to learn the model by introducing new structures for new tasks while preserving old knowledge, although this incurs substantial overhead as model parameters increase with the number of tasks. Recently, replay-based methods have become increasingly common. Data replay methods \cite{wang2022anti, zhou2022model} assist models in retaining old knowledge by recalling a small number of real samples. Additionally, some methods \cite{zhu2021class, zhu2021prototype,petit2023fetril} recall old knowledge by storing sample features and the distributions of these features. However, replay-based methods introduce storage costs and require repetitive computation for old data.

In recent years, studies such as \cite{zhou2024expandable, zhou2023revisiting, wang2022learning, wang2022dualprompt} have predominantly focused on integrating additional components for incremental tasks, such as learnable prompts \cite{wang2022learning, wang2022dualprompt, smith2023coda, gao2024consistent} and adapters \cite{zhou2024expandable, zhou2023revisiting}, into pretrained models. This integration necessitates the development of methods for selecting and evaluating the relevance of these components to ensure both their appropriateness and compatibility with the pretrained model. However, a significant limitation arises as the number of tasks increases: the associated computational and storage costs grow substantially, posing challenges to scalability and efficiency.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{images/pipeline_2024_paper.pdf}
   \vspace{-5em}
  \caption{The overview of our proposed \textbf{DesCLIP}. At each task $t$, language assistant are requested to generate sufficient general attribute description candidates for the classes in the current task, which are then encoded into embeddings via the CLIP's textual encoder. Using the anchor-based embedding filter (\textbf{AEF}), we filter the candidate embeddings by selecting those highly relevant to the visual features of the instances. The filtered embeddings are paired with the instance visual features to compute a class-agnostic instance matching loss. Correspondingly, class text embeddings are calibrated through shift weights to align with these shared filtered embeddings.}
  \label{fig:pipeline}
\end{figure*}

\subsection{Vision-Language Models}

With advancements in pre-training techniques, large-scale foundation models \cite{radford2021learning,li2022blip,kirillov2023segment,openai2023gpt4,alayrac2022flamingo} have significantly impacted the industry. For instance, Vision-Language Models such as Contrastive Language-Image Pretraining (CLIP) \cite{radford2021learning} and Adversarially Learned Inference for Image-Text Matching (ALIGN) \cite{jia2021scaling} have demonstrated remarkable zero-shot capabilities for general tasks. However, despite being pre-trained on over 400 million image-text pairs, CLIP still face challenges in specific downstream tasks, such as accurately identifying certain types of vehicles and lizards. 

To better adapt VLMs for downstream tasks, various text prompt-based fine-tuning methods \cite{zhou2022learning, zhou2022conditional, 10171397, yao2023visual, 10814093} have been proposed, which can enhance VLM performance on specific tasks. In more complex scenarios, learnable prompts can be inserted into intermediate layers \cite{yao2024tcp} to incorporate more sophisticated general knowledge. Additionally, the integration of adapter structures \cite{li2024graphadapter,pantazis2022svl,xin2024vmt} has also been shown to be an effective strategy. Other approaches \cite{zhang2024concept, wu2024transferring, luo2024cheap, gao2022pyramidclip} focus on the representation alignment of VLMs and aim to improve the transfer of general knowledge. Although these methods demonstrate excellent performance in CLIP transfer tasks, they are inherently unsuitable for incrementally learning, as the additional learnable structures cannot effectively mitigate catastrophic forgetting.

%\subsection{VLM-based Continual Learning}
\subsection{Continual Adaptation for VLMs}

Investigating the continual learning and adaptation of VLMs for diverse downstream tasks holds significant value, as it reduces data storage requirements and computational redundancy while addressing the challenge of inaccessible previous data. It is crucial to protect the model's gerenic pretrained knowledge and previously learned knowledge. The full fine-tuning strategies discussed in \ref{sec:Continual Learning} will lead to significant forgetting of pre-trained knowledge, which is a notable distinction between pre-trained foundation models (e.g., CLIP) and small-scale deep models. Additionally, frameworks such as CoOp \cite{zhou2022learning} and CoOpOp \cite{zhou2022conditional} have been shown to have limited adjustment capabilities for VLMs in incremental tasks due to their reliance on shared structures and contextual prompts across all tasks, leading to forgetting old knowledge during the process of fitting new knowledge. To solve this, Wang et al. \cite{wang2023attriclip} introduced AttriCLIP, which establishes a shared attribute bank for all tasks and selects suitable contexts based on visual images to bridge the gap between images and text. Yu et al. \cite{yu2024boosting} proposed using a mixture of experts (MoE) framework to adapt knowledge for different tasks, decoupling the model's zero-shot capabilities from its specialized task abilities. From the perspective of parameter sparse updating, efforts from SPG \cite{konishi2023parameter}, SparseCL \cite{wang2022sparcl}, and SPU \cite{zhang2024overcoming} have aimed to update VLM parameters selectively by employing appropriate “important parameter” selection patterns; for example, SPU selects more important parameters for updates based on the gradients accumulated by batches. Additionally, Zheng et al. \cite{zheng2023preventing} and Yu et al. \cite{yu2025select} proposed the use of additional reference datasets to facilitate knowledge distillation in a VLM, effectively mitigating the forgetting of generic knowledge.
\section{Methodology}
\label{sec:Methodology}

\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/conversation.pdf}
  \caption{DRP-guided GA description generation with a language assistant.}
  \label{fig:conversation}
\end{figure*}

\subsection{Preliminaries}
\textit{1) Continual Learning Formulation:} A sequence of task datasets is denoted as \( \{\mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_T\} \). During training on task \( \mathcal{D}_t \) (\( t \in \{1, 2, \ldots, T\} \)), access to data from previous tasks \( \{\mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_{t-1}\} \) is either highly restricted or entirely unavailable. In class-incremental learning (CIL), datasets for different tasks are introduced sequentially. Each task \( t \) is associated with a unique set of classes \( \mathcal{C}_t = \{C_{t,1}, C_{t,2}, \ldots, C_{t,|\mathcal{C}_t|}\} \), where \( |\mathcal{C}_t| \) denotes the number of classes in task \( t \). The classes associated with different tasks are disjoint:
\begin{equation}
    \mathcal{C}_t \cap \mathcal{C}_{t'} = \emptyset, \quad \forall t \neq t', \, t, t' \in \{1, 2, \ldots, T\}.
\end{equation}

\textit{2) CLIP for Incremental Tasks:} CLIP \cite{radford2021learning} comprises an image encoder \( \mathcal{F}_{\Theta}(\cdot) \) and a text encoder \( \mathcal{T}(\cdot) \). Specifically, an image \( x \in \mathbb{R}^{H \times W \times 3} \) and a text prompt, referred to as the rudimentary prompt \( \mathbf{RP}_y\), are input into \( \mathcal{F}_{\Theta}(\cdot) \) and \( \mathcal{T}(\cdot) \), respectively, producing a visual embedding \( \mathbf{z} \in \mathbb{R}^D \) and a text embedding \( \mathbf{w}_y \in \mathbb{R}^D \):
\begin{equation}
    \mathbf{z} = \mathcal{F}_{\Theta}(x), \quad \mathbf{w}_y = \mathcal{T}(\mathbf{RP}_y).
\end{equation}
Here, \( \mathbf{RP}_y \) is derived from hand-crafted prompts, typically following a template such as ``\texttt{A photo of a [CLS]},'' where \texttt{[CLS]} represents the specific class name. The probability of classifying a test image \( x \) as class \( y_i \) is computed using the softmax function:
\begin{equation}
   p\left(y_{i} \mid x\right) = \frac{\exp\left(\left \langle \frac{\mathbf{z}}{\|\mathbf{z}\|},\frac{\mathbf{w}_{y_i}}{\|\mathbf{w}_{y_i}\|}\right \rangle / \tau\right)}{\sum_{k=1}^{K} \exp\left(\left \langle  \frac{\mathbf{z}}{\|\mathbf{z}\|}, \frac{\mathbf{w}_{y_k}}{\|\mathbf{w}_{y_k}\|} \right \rangle / \tau\right)},
\end{equation}
where \( \tau \) is the temperature parameter, \( \mathbf{w}_{y_k} \) is the class text embedding derived from the rudimentary prompt of the \( k \)-th class, and \( K \) denotes the total number of downstream classes. 

Building on this architecture, ContinualCLIP \cite{thengane2022clip} tackles the challenge of continual learning with a training-free approach. For each new task \( t \), the text embedding set \( \mathbf{W}_t \) is expanded to incorporate embeddings for the new task's classes. At task \( t \), the updated text embedding set is defined as:
\begin{equation}
    \mathbf{W}_t = \bigcup_{j=1}^{t} \bigcup_{k=1}^{|C_{j}|} \mathbf{w}_{j,k},
\end{equation}
where \( \mathbf{w}_{j,k} \) denotes the text embedding for the \( k \)-th class of task \( j \) encountered so far. Consequently, the prediction for a test image \( x \) after task \( t \) is computed as:
\begin{equation}
    p\left(y_{i} \mid x\right) = \frac{\exp\left(\left \langle \frac{\mathbf{z}}{\|\mathbf{z}\|},\frac{\mathbf{w}_{y_i}}{\|\mathbf{w}_{y_i}\|}\right \rangle  / \tau\right)}{\sum_{\mathbf{w}_{j,k} \in \mathbf{W}_t } \exp\left(\left \langle \frac{\mathbf{z}}{\|\mathbf{z}\|},\frac{\mathbf{w}_{j,k}}{\|\mathbf{w}_{j,k}\|} \right \rangle/ \tau\right)}.
\end{equation}

\subsection{Overview of proposed DesCLIP}
%In this paper, we present a robust continual tuning framework for pre-trained VLMs. Our approach emphasizes establishing resilient visual-textual multimodal associations based on general attributes pertinent to specific classes, thereby facilitating effective incremental updates to the VLM. 
The overall architecture of our proposed framework is shown in Fig.~\ref{fig:pipeline}. Within our framework, the CLIP's textual encoder \( \mathcal{T}(\cdot) \) remains fixed and comprises two input branches: one processes rudimentary prompts derived from basic class names, while the other generates a diverse pool of general attribute (GA) description candidates via a language assistant. To obtain highly relevant visual-GA text pairs, we introduce the anchor-based embedding filter (\textbf{AEF}). The \textbf{AEF} identifies the most relevant attribute description embeddings from the candidate pool with respect to the current visual features. These filtered embeddings are then paired with visual features to compute a class-agnostic instance-matching loss, which is utilized to fine-tune the visual encoder \( \mathcal{F}_{\Theta}(\cdot) \). Concurrently, the text embeddings are gradually calibrated to align with shared attribute embeddings, further enhancing the consistency among representations of vision, GA, and downstream classes.


%\begin{figure}[t]
%  \centering
%  \includegraphics[width=0.85\linewidth]{images/anchor_selection.pdf}
%  \caption{illustration of proposed anchor-based embedding filter (\textbf{AEF}). $\odot$ represents the cosine similarity computation.}
%  \label{fig:anchor selection}
%\end{figure}


\subsection{General Attribute Description Generation}

CLIP establish robust visual-textual associations during the pre-training phase through instance-level contrastive learning. However, most existing research overlooks this foundational capability, conventionally relying on fixed, hand-crafted templates combined with class names as prompts to derive ``prior" classification weights via the textual encoder. Although Wang et al.~\cite{wang2023attriclip} introduced an ``attribute bank" to enable attribute sharing across different task instances, this approach lacks intrinsic relevance to specific classes. For instance, attributes such as ``white" and ``grass" fail to provide meaningful distinctions between classes like ``cat" and ``dog".

To address this limitation, we propose the use of a language assistant to generate rich, contextually relevant attribute descriptions for specific classes. The language assistant utilizes an advanced large language model (LLM) with a generalized understanding of downstream task entities. Drawing inspiration from~\cite{pratt2023does, yi2024leveraging, saha2024improved}, we design a variety of describe-request prompts (DRPs) to guide the language assistant in generating visually relevant attribute descriptions. Examples of basic DRPs include:
    \begin{itemize}
        \item \texttt{Q: Describe what does a/an [CLS] look like?}  
        \item \texttt{Q: Describe a/an [CLS]'s attribute features.}  
        \item \texttt{Q: Describe a/an [CLS]'s outlook features.}  
    \end{itemize}
Additionally, more complex prompts are designed to produce discriminative attribute descriptions, such as:
\begin{itemize}
    \item \texttt{Q: Describe what are some attribute characteristics of [CLS] compared with other [P-CLS], visually.}  
    \item \texttt{Q: Describe what kind of [P-CLS] a/an [CLS] is, visually.}  
\end{itemize}
Here, \texttt{[P-CLS]} refers to the parent class of \texttt{[CLS]}. Fine-grained DRPs are also employed for tasks with a known general scope, such as identifying objects within the ``birds" supercategory:
\begin{itemize}
    \item \texttt{Describe a/an [CLS]’s attributes from its beak, eyes, body, belly, tail, wings,
breast, etc.}
\end{itemize}
The DRP-guided general attribute description generation are illustrated in Fig.~\ref{fig:conversation}. The language assistant generates \( n_{dsc} \) attribute description candidates (DCs) for the \( k \)-th class of incremental task \( t \), denoted as:  
\begin{equation}
\mathbf{DC}_{\{t,k\}} = \bigg\{\mathbf{DC}_{\{t,k\},1}, \mathbf{DC}_{\{t,k\},2}, \ldots, \mathbf{DC}_{\{t,k\},n_{dsc}}\bigg\},
\end{equation}
where \( k \in \{1, 2, \ldots, |C_t|\} \). These DCs are then embedded using the textual encoder \( \mathcal{T}(\cdot) \) to produce attribute embedding candidates (ECs):  
\begin{equation}
\begin{split}
\mathbf{EC}_{\{t,k\}} &= \mathcal{T}\Big(\mathbf{DC}_{\{t,k\}}\Big) \\
&= \bigg\{\mathbf{EC}_{\{t,k\},1}, \mathbf{EC}_{\{t,k\},2}, \ldots, \mathbf{EC}_{\{t,k\},n_{dsc}}\bigg\}.
\end{split}
\end{equation}
Each element of \( \mathbf{EC}_{\{t,k\}} \) has the same dimension as the rudimentary text embedding \( \mathbf{w}_{t,k} \).  

\subsection{Anchor-based Embedding Filter}
\label{sec:Anchor-based Embedding Filter}
The generated embedding candidates of GAs do not always align with visual representations due to potential domain discrepancies or unrelated information in the text descriptions produced by the language assistant. To establish robust vision-GA associations, we propose an anchor-based embedding filter (\textbf{AEF}) mechanism to refine the embedding candidates. This mechanism identifies candidates that sufficiently match the visual representations, enabling the construction of approximate image-text pairs tailored to the specific requirements of the task.

For a training sample \((x_i, y_i)\), where \(y_i = c = \{t, k\}\) is assumed, the label \(c\) is considered to correspond to the \(k\)-th class of incremental task \(t\), with \(k \in \{1, 2, \dots, |C_t|\}\). As illustrated in Fig.~\ref{fig:pipeline}, the inputs to \textbf{AEF} include the visual features \(\mathbf{z}_i = \mathcal{F}_{\Theta}(x_i)\), the rudimentary text embedding \(\mathbf{w}_{c} = \mathcal{T}(\textbf{RP}_{y_i})\), and the embedding candidates \(\mathbf{EC}_{c}\). The cosine similarity between the visual features \(\mathbf{z}_i\) and the rudimentary text embedding \(\mathbf{w}_{c}\) is calculated as:  
\begin{equation}
CS_i^c = \left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{w}_{c}}{\|\mathbf{w}_{c}\|} \right \rangle.
\end{equation}
Subsequently, the similarity scores between the visual features and each embedding candidate in \(\mathbf{EC}_{c}\) are calculated:  
\begin{equation}
\textbf{EC\_S}_{c}^{i,j} = \left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{EC}_{c,j}}{\|\mathbf{EC}_{c,j}\|} \right \rangle,
\end{equation}
where \(j \in \{1, 2, \dots, n_{dsc}\}\). To mitigate the risk of overfitting in the CLIP's visual encoder, visual features with low relevance to either class text or GA descriptions should be filtered out. Hence, a condition \(\chi(\mathbf{z}_i)\) is defined to filter visual features as:
\begin{equation}
    \chi(\mathbf{z}_i) = 
\begin{cases} 
%1, & \text{if} \min\big(CS_i^c, \max_{j} \textbf{EC\_S}_{c}^{i,j}\big) > \delta_{d}, \\
1, & \text{if} ~~\max\big(CS_i^c, \max_{j} \textbf{EC\_S}_{c}^{i,j}\big) > \delta_{d}, \\
0, & \text{otherwise}.
\end{cases}
\end{equation}
Here, \(\delta_d\) is a predefined threshold. The similarity between retained visual features and the rudimentary text embedding is used to define the anchor threshold \(AT_i^c\):  
\begin{equation}
AT_i^c = \big\{ CS_i^c \; | \; \chi(\mathbf{z}_i) = 1 \big\}.
\end{equation}
We posit that embedding candidates in \(\mathbf{EC}_c\) exhibiting a similarity score surpassing a threshold \(\gamma\) (relative to the anchor threshold \(AT_i^c\)) are more consistent with the visual features of the current sample. These candidates are filtered as follows:  
\begin{equation}
\mathbf{FE}^i = \bigg\{ \textbf{EC}_{c,j} \; | \; \textbf{EC\_S}_{c}^{i,j} > AT_i^c + \gamma \bigg\},
\end{equation}
where $j \in \{1, 2, \dots, n_{dsc}\}$, and sorted with descending order according to $\textbf{EC\_S}_{c}^{i,j}$. To reduce the potential influence of domain discrepancies arising from contextual descriptions, we further restrict the selection process by exclusively retaining attribute description sentences in \(\mathbf{DC}_c\) that explicitly include the class name as a noun (i.e., \texttt{[CLS]+[GA]}).


\subsection{General Attribute-Guided Progressive Visual-Textual Alignment}

This section introduces the methodology for optimizing the visual and textual branches of CLIP in incremental tasks, leveraging the filtered embeddings identified for relevant training samples. Within the CLIP architecture, the optimization focuses on the visual encoder \(\mathcal{F}_{\Theta}(\cdot)\) (specifically, the initial MLP layers within each Transformer block \cite{zhang2024overcoming}) and the rudimentary text embeddings introduced for the current task.

\textit{1) Instance Matching:} To align the visual features with highly relevant embedded GA descriptions, we select the most closest textual representation $\mathbf{h}_i$ as the paired text embedding:
\begin{equation}
\mathbf{h}_i = \mathbf{FE}^i[0].
\end{equation}
The instance matching loss $\mathcal{L}_{\text{IM}}$ is computed across the batch as:
\begin{equation}
    \mathcal{L}_{\text{IM}} = \mathbb{E}_{i \in P} \Big[ - \log \frac{\mathbf{M}_{i,i}}{\mathbf{M}_{i,i} + \sum_{j \in P, j \neq i} \mathbf{M}_{i,j}} \Big],
\end{equation}
where
\begin{equation}
    \mathbf{M}_{i,j} = \exp\Big(\left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{h}_j}{\|\mathbf{h}_j\|} \right \rangle / \tilde{\tau}\Big),
\end{equation}
\begin{equation}
    \mathbf{M}_{i,i} = \exp\Big(\left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{h}_i}{\|\mathbf{h}_i\|} \right \rangle / \tilde{\tau}\Big).
\end{equation}
Here, $\tilde{\tau}$ denotes an elevated temperature scaling factor, defined as $\tilde{\tau} = 10\tau$. The set $P$ represents the indices of valid samples within a batch of size $B$ and is specified as:
\begin{equation}
    P = \Big\{ i \mid i \in \{1, \dots, B\}, \, \chi(\mathbf{z}_i) = 1 \, ,\mathbf{FE}^i \neq \emptyset \Big\},
\end{equation}
where $\chi(\mathbf{z}_i)$ is the condition defined in \ref{sec:Anchor-based Embedding Filter}. For each incremental task \( t \), we tune the model using a contrastive learning framework similar to the pretraining strategy of CLIP \cite{radford2021learning}. To mitigate forgetting, we adhere to a “nearest matching” principle, aligning visual features with GA description embeddings exhibiting higher correlations. This approach minimizes the risk of overfitting visual features to specific class text embeddings, maintaining both generalization and previous knowledge.

\textit{2) Text Embedding Calibration:} General attribute descriptions play a pivotal role in guiding the calibration of text embeddings to achieve better alignment with visual representations. This alignment is particularly crucial because the original rudimentary text embeddings often exhibit weak correlations with visual features in ``unfamiliar'' downstream tasks. Such misalignment can lead to overfitting in the visual branch of the VLM to class labels, thereby exacerbating forgetting. To mitigate this issue, we propose a weight-shifting mechanism that calibrates the rudimentary text embeddings for the classes in task \( t \). This mechanism repositions the text embeddings toward representative attributes shared across the corresponding visual features, fostering stronger alignment between shared general attributes and class-specific text embeddings. Specifically, we define a shifting weight $\mathbf{s}_{t,k}\in \mathbb{R}^D$, and a shift transformation $\Psi(\cdot,\mathbf{s}_{t,k})$ for the calibration of rudimentary text embedding $\mathbf{w}_{t,k}$, where $\{t,k\}$ representing the $k-$th class of incremental task $t$, $k\in \{1,2,...,|C_t|\}$. The calibrated text embedding $\mathbf{w'}_{t,k}$ can be obtained as:
\begin{equation}
\mathbf{w'}_{t,k} = %\Phi(\mathbf{w}_{t,k},\mathbf{s}_{t,k}) = \frac{\mathbf{w}_{t,k} + \alpha \cdot \mathbf{s}_{t,k}}{\|\mathbf{w}_{t,k} + \alpha \cdot \mathbf{s}_{t,k}\|}.
\Psi(\mathbf{w}_{t,k},\mathbf{s}_{t,k}) = \frac{\mathbf{w}_{t,k}}{\|\mathbf{w}_{t,k}\|} + \alpha \cdot \mathbf{s}_{t,k}.
\end{equation}
The key to text embedding calibration is to ensure a strong correlation with the visual representations of the class while preventing an excessive focus on any single attribute text. Therefore, $\mathbf{w}'_{t,k}$ should be aligned with $\mathbf{FE}^i$, which is filtered based on the visual features $\mathbf{z}_i$ of the class $c = \{t, k\}$:
\begin{table*}[h!]
    \centering
    \caption{
Comparison of different methods on the CIFAR100 \cite{krizhevsky2009learning}, ImageNet-Subset \cite{deng2009imagenet}, and CUB-200 \cite{wah2011caltech} datasets under various settings. `\textbf{UB}' denotes the upper bound achieved through joint training.}
    \label{tab:comparison}
    \renewcommand{\arraystretch}{1.2} % Adjust line spacing
    \setlength{\tabcolsep}{2.9pt} % Reduce column spacing
    \resizebox{0.8\textwidth}{!}{ % Resize to fit within the full width
    \fontsize{10}{12}\selectfont % Set font size
    \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc}
        \toprule
        \multicolumn{1}{c|}{\textbf{Method}} & \multicolumn{6}{c|}{{CIFAR100 \cite{krizhevsky2009learning}}} & \multicolumn{6}{c|}{{ImageNet-Subset \cite{deng2009imagenet}}} & \multicolumn{6}{c}{{CUB-200 \cite{wah2011caltech}}} \\
        \cline{2-19}
        & \multicolumn{2}{c|}{$T=5$} & \multicolumn{2}{c|}{$T=10$} & \multicolumn{2}{c|}{$T=20$}
        & \multicolumn{2}{c|}{$T=5$} & \multicolumn{2}{c|}{$T=10$} & \multicolumn{2}{c|}{$T=20$}
        & \multicolumn{2}{c|}{$T=5$} & \multicolumn{2}{c|}{$T=10$} & \multicolumn{2}{c}{$T=20$} \\
        \cline{2-19}
        & Last & Avg  & Last & Avg  & Last & Avg 
        & Last & Avg  & Last & Avg   & Last & Avg
        & Last & Avg   & Last & Avg   & Last & Avg \\
        \midrule
        Seq FT & - & - & 50.6 & 66.9 & - & - & - & - & 58.8 & 73.8 & - & - & - & - & 40.8 & 64.6 & - & - \\ 
        LwF \cite{li2017learning} & - & - & 56.1 & 74.8 & - & - &- & - &56.4 & 75.0 & - & - & - & - & 52.1 & 71.5  & - & - \\ 
        EWC \cite{kirkpatrick2017overcoming} & - & - & 64.4 & 78.4 & - & - & - & -&70.3 & 80.2  & - & - & - & - & 64.4 & 78.6 & - & - \\ 
        \midrule
        CoOp \cite{zhou2022learning} & 78.8 & 85.0 & 75.1 & 83.0 & 78.2 & 85.3 & 78.8 & 81.8 & 78.0 & 81.4 & 74.9 & 80.6 &53.1 & 63.7 & 41.8 & 54.3 & 49.1 & 58.4 \\ 
        CoOpOp \cite{zhou2022conditional} & 76.5 & 80.0 & 76.5 & 76.8 & 70.0 & 73.2 & 68.8 & 76.5 & 62.4 & 71.3 & 61.2 & 70.8 & 51.9 & 66.7 & 53.4 & 66.9 & 50.2 & 65.8 \\ 
        \rowcolor[HTML]{EEEEEE} ContinualCLIP \cite{thengane2022clip}&  {72.8} & - &  {72.8} & - &  {72.8} & - &  {74.6} & - &  {74.6} & - &  {74.6} & - &  {60.9} & - & {60.9} & - &  {60.9} & - \\ 
        L2P \cite{wang2022learning}& - & - & 70.2 & 79.6 & -& -& - & - & 71.1 & 78.4& - & - & - & - & 69.9 & 77.8 & - & - \\ 
        DualPrompt \cite{wang2022learning}& - & - & 72.0 & 81.8 & -& -& - & - & 71.7 & 79.8& - & - & - & - & 64.5 & 75.9 & - & - \\ 
        SLCA \cite{zhang2023slca}&  80.2 & 87.2 & 80.2 & 87.6 & 80.2 & 87.6 & 82.3 & 86.0 & 80.3 & 84.4 & 80.2 & 85.4 & 75.5& 78.9 & 73.9 & 80.0& 71.1 & 79.4 \\  
        AttriCLIP \cite{wang2023attriclip}& 81.9  & 86.8 & 80.9 & 86.3 & 79.6 & 86.0 & 79.2 & 84.3 & 78.5 & 81.8 & 77.4 & 82.1 & 62.8 & 72.2 & 52.5 & 66.4 & 57.1 & 67.4 \\ 
        MoE-Adapter \cite{yu2024boosting}& 83.8 & 88.0 & 82.1 & 87.9 & 81.0 & 86.8 & 81.2 & 85.9 & 82.9 & 85.9 & 82.6 & 86.2 & 78.4 & 82.5 & 75.1& 81.4 & 73.9 & 80.5 \\
        SPU \cite{zhang2024overcoming}& 84.5 & 89.1 & 82.9 & 88.2 & 81.2 & 86.8 & 82.8 & 86.1 & 82.4 & 85.5 & 81.9 & 85.5 & 78.8 & 82.8 & 76.2& 81.8 & 73.9 & 79.8 \\ 
        TaskRes-CL \cite{yu2023task}& 84.2 & 88.3 & 81.2 & 87.3 & 79.9 & 86.4 & 82.9 & 86.3 & 82.5 & 85.7 & 82.1 & 85.8 & 75.4 & 78.9 & 75.0 & 80.4 & 72.7 & 78.3 \\ 
        \rowcolor[HTML]{F5E3EE} \textbf{DesCLIP} (Ours)& \textbf{85.9} & \textbf{90.0} & \textbf{84.5} & \textbf{90.1} & \textbf{82.9} & \textbf{88.8} & \textbf{84.3} & \textbf{87.6} &\textbf{84.2} &\textbf{87.3 }&\textbf{83.2} & \textbf{87.1 }& \textbf{81.3} & \textbf{84.5} & \textbf{78.4} & \textbf{83.8 }& \textbf{75.3} &\textbf{81.7} \\ 
        \midrule
        \textbf{UB} & 90.0 & - & 90.0 & - & 90.0 & - & 87.6 & - & 87.6 & - & 87.6 & - & 84.5 & - & 84.5 & - & 84.5 & - \\ 
        
        \bottomrule
    \end{tabular}
    }
\end{table*}
    \begin{table}[h!]
    \centering
    \caption{Comparison of various methods on CIFAR100 \cite{krizhevsky2009learning} and CUB-200 \cite{wah2011caltech} ($T =10$) with CLIP ViT-B/16 \cite{radford2021learning} backbone. `\#FR' represents using feature replay. `\#AS' represents using additional structure.}
    \label{tab:vit-b-16}
    \renewcommand{\arraystretch}{1.1} % Adjust line spacing
    \setlength{\tabcolsep}{2.4pt} % Reduce column spacing
    \resizebox{0.42\textwidth}{!}{ % Resize to fit within the full width
    \fontsize{10}{12}\selectfont % Set font size
    \begin{tabular}{l|cc|ccc|ccc}
        \toprule
        \multicolumn{1}{c|}{\textbf{Method}} & & &  \multicolumn{3}{c|}{{CIFAR100 \cite{krizhevsky2009learning}}} & \multicolumn{3}{c}{{CUB-200 \cite{wah2011caltech}}} \\
        \cline{2-9}
       
        &\#FR & \#AS & Last & Avg & C. & Last & Avg & C. \\
        \midrule
        Seq FT & \ding{55} & \ding{55} & 46.3 & - & 24.2 &45.7 & - & 39.7 \\ 
         \rowcolor[HTML]{EEEEEE} ContinualCLIP\cite{thengane2022clip} & \ding{55} & \ding{55} & {68.3} & -& {63.6} &{55.1} &- & {63.6}\\ 
        L2P \cite{wang2022learning}& \ding{55} & \checkmark & 64.5 & 72.9 & 41.8 & 66.3 & 75.5 & 43.8\\ 
        SLCA \cite{zhang2023slca}& \checkmark & \ding{55} & 71.5 & 79.8 & 59.1 &50.6 & 58.4 & 62.7\\ 
        AttriCLIP \cite{wang2023attriclip}& \ding{55} & \checkmark & 67.0 & 77.8 & 60.3 & 50.8 & 65.4 & 63.1\\ 
        SPU \cite{zhang2024overcoming}& \ding{55} & \ding{55} & 75.8 & 84.3 & 58.7& 67.8& 75.6 & 59.8 \\
        %SPU \cite{zhang2024overcoming}(ER=200)& \checkmark &  & \textbf{80.9} & \textbf{87.5} & 69.8 & 77.3 \\
        TaskRes-CL \cite{yu2023task}& \ding{55} &  \ding{55}& 75.6 & 83.2 & \textbf{63.6} & 67.1 & 73.8 & \textbf{63.6} \\ 
        MoE-Adapter \cite{yu2024boosting}& \ding{55} & \checkmark & 77.8 & 84.9 & 62.2 &68.3 & 76.2 &62.9\\
        RAPF \cite{huang2025class} & \checkmark & \checkmark & 78.5 & 85.1 & 54.0 &\textbf{73.2} & \textbf{80.4} &52.6\\
        \rowcolor[HTML]{F5E3EE} \textbf{DesCLIP} (Ours)& \ding{55} & \ding{55} & \textbf{79.1} & \textbf{85.7} & 62.0 &72.0 & 78.6 &62.5
        \\
        \bottomrule
    \end{tabular}
    }
\end{table}
\begin{equation}
    %\mathcal{L}_{\text{TA}}^i = \mathbb{E}_{\mathbf{u} \in \mathbf{FE}^i}( \beta \cdot \|\mathbf{w}'_{t,k}-\mathbf{u}\|_2+ (1-\mathbf{w}'_{t,k} \odot \mathbf{u})).
    \mathcal{L}_{\text{TA}}^i = \mathbb{E}_{\mathbf{u} \in \mathbf{FE}^i}\Big[ \beta \cdot \Big\|\frac{\mathbf{w}'_{t,k}}{ \|\mathbf{w}'_{t,k}\|}-\frac{\mathbf{u}}{\|\mathbf{u}\|}\Big\|_2+ \Big(1-\left \langle \frac{\mathbf{w}'_{t,k}}{ \|\mathbf{w}'_{t,k}\|} ,\frac{\mathbf{u}}{\|\mathbf{u}\|} \right \rangle \Big)\Big],
\end{equation}
where $\beta$ is a parameter that determines whether the alignment of text embeddings places greater emphasis on their absolute distance in the text space (e.g., Euclidean distance) or on the directional consistency. Text alignment loss $\mathcal{L}_{\text{TA}}$ of the current batch is: $\mathcal{L}_{\text{TA}} = \sum_{i \in P} \mathcal{L}_{\text{TA}}^i.$

Since, in the context of continual learning, data from previous tasks cannot be revisited during the current task, the weights \(\mathbf{w'}_{t}=\{\mathbf{w'}_{t,1},\mathbf{w'}_{t,2},...,\mathbf{w'}_{t,|C_t|}\}\) are calibrated solely within the scope of the current task \( t \). Consequently, only the shifting weights \(\mathbf{s}_{t}= \{\mathbf{s}_{t,1},\mathbf{s}_{t,2},...,\mathbf{s}_{t,|C_t|}\}\) associated with the current task are learnable, whereas the shifting weights \(\mathbf{s}_{0 : t-1}=\{\mathbf{s}_0,\mathbf{s}_1,...,\mathbf{s}_{t-1}\}\) from previous tasks remain fixed.

\textit{3) Reconstructed Intra-task Classification:} To ensure alignment between text embeddings and the visual branch during optimization, we reconstruct the classification loss for task \( t \). This loss constrains the calibration of text embeddings to remain within the low-loss region of the classification space for the current task, which is a critical prerequisite. Specifically, for each visual feature \(\mathbf{z}_i\) in a batch, its similarity to all calibrated text embeddings for the current task is computed to generate predicted logits. These logits are aligned with the ground truth label \( y_i \), and the prediction classification loss is calculated as:
\begin{equation}
\mathcal{L}_{\text{RIC}}=\mathbb{E}_{i\in\{1,2,...,B\}}\Bigg[ \log\frac{-\exp\Big(\left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{w'}_{y_i}}{\|\mathbf{w'}_{y_i}\|}\right \rangle /\tau \Big)}{\sum_{k=1}^{|C_t|}\exp \Big(\left \langle \frac{\mathbf{z}_i}{\|\mathbf{z}_i\|}, \frac{\mathbf{w'}_{t,k}}{\|\mathbf{w'}_{t,k}\|}\right \rangle / \tau \Big)} \Bigg],
\end{equation}
where \(\mathbf{w'}_{y_i}\) represents the calibrated text embedding for the class corresponding to \(\mathbf{z}_i\). 

\textit{4) Optimization:}
To achieve optimal performance, the total loss for batch optimization is defined as:
\begin{equation}
\min_{\Theta, \mathbf{s}_{t}} \mathcal{L} = \lambda_{\text{IM}} \cdot \mathcal{L}_{\text{IM}} + \lambda_{\text{TA}} \cdot \mathcal{L}_{\text{TA}} + \lambda_{\text{RIC}} \cdot \mathcal{L}_{\text{RIC}},
\end{equation}
where \(\lambda_{\text{IM}}\), \(\lambda_{\text{TA}}\), and \(\lambda_{\text{RIC}}\) are balancing factors that control the contributions of the respective loss terms.

\subsection{Inference Stage}
The GA descriptions and embeddings we introduce do not participate in the inference stage of the CLIP after each training phase, which effectively avoids additional storage overhead and eliminates any increase in inference time. Consequently, the model obtained through our method maintains identical parameter size and inference time during the inference stage as the original zero-shot CLIP. In inference stage after task $t$, we leverage the calibrated text embeddings of all seen classes:
    $\mathbf{W'}_t = \bigcup_{j=1}^{t} \bigcup_{k=1}^{|C_{j}|} \mathbf{w'}_{j,k}.$
Hence, the probability of predicting the testing image \( x \) as the class \( y_i \) can be expressed as:
\begin{equation}
p\left(y_{i} \mid x\right) = \frac{\exp\left(\left \langle \frac{\mathbf{z}}{\|\mathbf{z}\|} , \frac{\mathbf{w'}_{y_{i}}}{\|\mathbf{w'}_{y_{i}}\|}  \right \rangle / \tau\right)}{\sum_{\mathbf{w}'_{j,k} \in \mathbf{W'}_t } \exp\left(\left \langle  \frac{\mathbf{z}}{\|\mathbf{z}\|} , \frac{\mathbf{w}'_{j,k}}{\|\mathbf{w}'_{j,k}\|} \right \rangle / \tau\right)}.
\end{equation}

\section{Experiments}
\subsection{Setup}
\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{images/all_curve_v4.pdf}
  \vspace{-3em}
  \caption{Average accuracy evaluated after each task across different incremental task settings (5, 10, and 20 tasks).}
  \label{fig:all_curve}
\end{figure}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/Deg_or_Imp.pdf}
  \vspace{-3em}
  \caption{Degradation or improvement evaluation on ImageNet-Subset \cite{deng2009imagenet} with 10 incremental tasks. \textbf{Upper}: accuracy differences relative to zero-shot VLM. \textbf{Bottom}: accuracy differences relative to VLM previously learned on the first-half classes.}
  \label{fig:Deg or Imp}
\end{figure*}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/few-shot.pdf}
  \vspace{-0.5em}
  \caption{Performance comparison in few-shot case.}
  \label{fig:few-shot}
\end{figure}

 \begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{images/ablation_des_nums.pdf}
  \vspace{-2em}
  \caption{Ablation study of generated GA description amount.}
  \label{fig:ablation_des_nums}
\end{figure}
\textit{1) Datasets: }The evaluation experiments for continual learning are conducted on CIFAR100 \cite{krizhevsky2009learning}, ImageNet-Subset \cite{deng2009imagenet} and CUB-200 \cite{wah2011caltech}. ImageNet-full \cite{deng2009imagenet} is utilized as a control set to evaluate the retention of the pretrained generalization knowledge in CLIP.



\iffalse
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/CIFAR_curve.pdf}
  \vspace{-2.5em}
  \caption{Average accuracy of different incremental step settings (5, 10, 20 tasks) on CIFAR100 \cite{krizhevsky2009learning}.}
  \label{fig:CIFAR_curve}
\end{figure*}

 \begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/IN100_curve.pdf}
  \vspace{-2.5em}
  \caption{Average accuracy of different incremental step settings (5, 10, 20 tasks) on ImageNet-Subset \cite{deng2009imagenet}.}
  \label{fig:CUB_curve}
\end{figure*}

 \begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/CUB_curve.pdf}
  \vspace{-2.5em}
  \caption{Average accuracy of different incremental step settings (5, 10, 20 tasks) on CUB-200 \cite{wah2011caltech}.}
  \label{fig:CUB_curve}
\end{figure*}
\fi

\begin{itemize}
    
\item \textbf{CIFAR100} \cite{krizhevsky2009learning} introduced a dataset comprising 60,000 images, each with a resolution of \( 32 \times 32 \), distributed across 100 distinct classes. Each class includes 500 training samples and 100 testing samples. CIFAR100 has become a widely recognized benchmark for evaluating continual learning methods. Its low-resolution images pose a significant challenge for VLMs that are pre-trained on high-resolution datasets, highlighting the difficulty of adapting to such datasets in continual learning scenarios.



%\item \textbf{ImageNet100}. ImageNet \cite{} contains 1000 categories of general images with a resolution of 224$\times$224. Each category contains about 1300 training images and 50 test images. Following the category setting of \cite{}, 100 categories were selected as the evaluation benchmark ImageNet100. 

\item \textbf{ImageNet-Subset} \cite{deng2009imagenet}. Following the class configuration proposed in \cite{douillard2020podnet}, we select the challenging ImageNet-Subset benchmark for method evaluation. This subset consists of fine-grained animal images from 100 categories that are unfamiliar to VLMs and prone to misclassification due to their subtle differences.

\item \textbf{Caltech-UCSD Birds-200 (CUB-200)} \cite{wah2011caltech} comprises 11,788 bird images distributed across 200 categories. The subtle variations among bird images pose significant challenges for VLMs to achieve accurate identification. Moreover, the relatively small size of the dataset further intensifies the difficulty, particularly in few-shot scenarios.

\item \textbf{ImageNet-full} \cite{deng2009imagenet} is a large-scale image dataset containing over 1.2 million images across 1,000 classes, widely used for pretraining and evaluating visual representation learning models.


\end{itemize}

\textit{2) Metrics:} To evaluate the continual learning performance of classification models, we employ two primary metrics: `Last' and `Avg'. `Last' denotes the average accuracy across all classes after the model has completed training on the final task. `Avg' represents the mean incremental accuracy calculated over all tasks the model has learned thus far. In addition, The control set accuracy `C.' \cite{zhang2024overcoming} is employed to evaluate the retention of CLIP's zero-shot generalization knowledge after continual learning, assessed on ImageNet-full \cite{deng2009imagenet} (1000 classes in total).

\textit{3) Competitors:} We compare our method against baseline and state-of-the-art methods for continual learning VLMs. These include ContinualCLIP \cite{thengane2022clip} under zero-shot conditions; non-continual adapting methods such as CoOp \cite{zhou2022learning} and CoOpOp \cite{zhou2022conditional}; and conventional continual learning method such as LwF \cite{li2017learning} and EWC \cite{kirkpatrick2017overcoming}. Additionally, we evaluate VLM-specific continual learning methods, including AttriCLIP \cite{wang2023attriclip}, TaskRes-CL \cite{yu2023task}, SPU \cite{zhang2024overcoming}, MoE-Adapter \cite{yu2024boosting}, and RAPF \cite{huang2025class}. For a broader comparison, we also incorporate techniques tailored to address continual learning in visual-only pre-trained models, such as L2P \cite{wang2022learning}, DualPrompt \cite{wang2022dualprompt}, and SLCA \cite{zhang2023slca}.

\textit{4) Implementation Details:} All experiments are conducted on two NVIDIA GeForce RTX 3090 GPUs. Following \cite{wang2023attriclip}, we adopt the pre-trained CLIP ViT-L/14 \cite{radford2021learning} as the backbone. To ensure a fair comparison, no data replay strategies are used during the continual learning process of the VLM.

The model is trained for 10 epochs on the datasets for each incremental task. Stochastic Gradient Descent (SGD) \cite{bottou2010large} is employed as the optimizer, using a cosine learning rate decay schedule with a batch size of 32. The learning rate for the MLPs' weights in the CLIP's visual encoder is set to $1 \times 10^{-5}$, while the learning rate for the text shifting weights is set to 0.1. The coefficient $\alpha$ for shifting weight addition is set to 0.1. Loss balancing factors are set as $\lambda_{\text{IM}}=2.0$, $\lambda_{\text{TA}}=0.5$, and $\lambda_{\text{RIC}}=1.0$ by default. For the fine-grained CUB-200 dataset \cite{wah2011caltech}, $\lambda_{\text{IM}}$ is adjusted to 15.0. To filter out visual features that fail to intuitively reflect attribute or class information, $\delta_d,\gamma$ are estimated based on downstream tasks and set to 0.20, 0.015; 0.25, 0.03; and 0.30, 0.015 for CIFAR100 \cite{krizhevsky2009learning}, ImageNet-Subset \cite{deng2009imagenet}, and CUB-200 \cite{wah2011caltech}, respectively.
 Additionally, we generate $n_{dsc}=30$ attribute descriptions for each class using the language assistant GPT-4 \cite{openai2023gpt4}.

\subsection{Comparison with State-of-the-art Methods}


\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt} % 调整列间距
\renewcommand{\arraystretch}{1.1} % 调整行间距
\caption{Ablation study of each component on Split CUB-200 \cite{wah2011caltech}.}
\label{tab:ablation1}
\resizebox{0.4\textwidth}{!}{ % 表格宽度自适应
\begin{tabular}{c|ccc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Settings}} & \multirow{2}{*}{\textbf{RIC}} & \multirow{2}{*}{\textbf{IM}} & \multirow{2}{*}{\textbf{TA}} & \multicolumn{2}{c|}{$T=10$} & \multicolumn{2}{c}{$T=20$}  \\
\cline{5-8}
 & & & & $\Delta$Last& $\Delta$Avg& $\Delta$Last& $\Delta$Avg\\
\midrule
Zero-shot & -& -& - & 60.9& 66.0& 60.9& 66.0\\
\rowcolor[HTML]{EEEEEE} V.E.(full)& \checkmark &  & & \textcolor{cyan}{-7.8} & \textcolor{cyan}{-7.3} & \textcolor{cyan}{-8.2} & \textcolor{cyan}{-7.9}  \\
\rowcolor[HTML]{EEEEEE} V.E.& \checkmark &  & & \textcolor{cyan}{-2.1} & \textcolor{cyan}{-2.1} & \textcolor{cyan}{-2.7} & \textcolor{cyan}{-2.6}   \\
V.E. & \checkmark & \checkmark & & +3.6 & +3.9 & +2.0 & +2.2   \\
V.E.+\textbf{TEC} &\checkmark & \checkmark & & +12.5 & +13.1 & +11.4 & +12.0  \\
only \textbf{TEC} &\checkmark &  & & +14.1 & +14.4 & +11.8 & +12.3  \\
\rowcolor[HTML]{F5E3EE}V.E.+\textbf{TEC} & \checkmark & \checkmark & \checkmark & \textbf{+17.5}& \textbf{+17.8}& \textbf{+14.4} & \textbf{+15.7} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{2pt} % 调整列间距
\renewcommand{\arraystretch}{1.0} % 调整行间距
\caption{Ablation study of different parameter selections on ImageNet-Subset \cite{deng2009imagenet} and CUB-200 \cite{wah2011caltech}.}
\label{tab:ablation2}
\resizebox{0.4\textwidth}{!}{ % 表格宽度自适应
\begin{tabular}{c|cccc|cc|cc}
\toprule
\multirow{2}{*}{$\lambda_{\textbf{IM}}$} &\multicolumn{4}{c|}{threshold param $\gamma$} &  \multicolumn{2}{c|}{ImageNet-Sub \cite{deng2009imagenet}} & \multicolumn{2}{c}{CUB-200 \cite{wah2011caltech}} \\

\cline{6-9}
 &0 &0.015& 0.03& 0.05 & Last& Avg& Last& Avg\\
\midrule
0 &  &  & \checkmark & &82.8 & 86.0& -& -\\
1.0 &  &  & \checkmark & &83.1 & 86.7& -& -\\
2.0 & \checkmark &  &  & &82.9 &86.4 & -& -\\
2.0 &  & \checkmark &  & &83.5 &86.9 & -& -\\
\rowcolor[HTML]{F5E3EE}2.0 &  & & \checkmark & & \textbf{84.2}& \textbf{87.3}& 76.9& 82.0\\
2.0&  &  & &\checkmark & 83.8 & 87.1 & - &- \\
4.0 &  &  &\checkmark & & 82.5& 84.9 & - & -   \\
\midrule

0 &  & \checkmark &  & & -& - & 75.4 & 81.5   \\
10.0 & & \checkmark& & & - & - & 77.8 & 82.8   \\
15.0 &\checkmark& & & & - & - & 77.6 &  82.0 \\
\rowcolor[HTML]{F5E3EE}15.0 && \checkmark& & & 81.8 & 85.2 & \textbf{78.4} & \textbf{83.8}  \\
15.0 && &\checkmark & & - & - & 78.0 & 83.5 \\
20.0 & &\checkmark & & & - & - & 78.2 & 83.3  \\

\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt} % 调整列间距
\renewcommand{\arraystretch}{1.2} % 调整行间距
\caption{Results of different types of $\Psi(\mathbf{w},\mathbf{s})$ on Split CUB-200 \cite{wah2011caltech}. $\varphi(\mathbf{w})$ represents using class-wise adapters \cite{gao2024clip} on rudimentary text embeddings.}
\label{tab:ablation_tes}
\resizebox{0.4\textwidth}{!}{ % 表格宽度自适应
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{$\Psi(\mathbf{w},\mathbf{s})$} & \multicolumn{2}{c|}{$T=10$} & \multicolumn{2}{c}{$T=20$}  \\
\cline{2-5}
 & Last& Avg& Last& Avg\\
\midrule
$\mathbf{w}$ & 64.5& 69.9& 62.9& 68.2\\
$\mathbf{w} + \alpha \cdot \varphi(\mathbf{w})$ & 68.9& 74.2& 66.7& 72.5\\

$\mathbf{w} + \alpha \cdot \mathbf{s}$ & 77.5& 82.9& 74.8& 81.0\\

$\mathbf{w}/\|\mathbf{w}\| + \alpha \cdot \varphi(\mathbf{w}/\|\mathbf{w}\|) $ & 70.7 &75.8 & 68.1 & 73.2\\

\rowcolor[HTML]{F5E3EE}$\mathbf{w}/\|\mathbf{w}\| + \alpha \cdot \mathbf{s} $& \textbf{78.4}& \textbf{83.8}& \textbf{75.3} & \textbf{81.7} \\
\bottomrule
\end{tabular}
}
\end{table}

The accuracy results for continual learning, including `Last' and `Avg', are summarized in Table \ref{tab:comparison}. These results are derived from comprehensive experiments conducted on multiple datasets under varying incremental task settings. Additionally, Fig. \ref{fig:all_curve} depicts the forgetting curves, comparing recent methods and providing a detailed evaluation of accuracy after each incremental task.

\textit{1) Performance on Coarse Dataset:}  
CIFAR100 \cite{krizhevsky2009learning} is selected as the coarse dataset, where VLMs generally perform well on common classes but may struggle with blurred images. As shown in Table \ref{tab:comparison}, our method consistently achieves superior performance across all incremental task stages, obtaining an average accuracy (`Avg') of 90.1\% under the setting of $T=10$. Notably, our approach surpasses ContinualCLIP \cite{thengane2022clip} by a significant margin of 11.6\% on the `Last' metric. Moreover, it outperforms TaskRes-CL \cite{yu2023task}, MoE-Adapter \cite{yu2024boosting}, and SPU \cite{zhang2024overcoming} by +3.3\%, +2.4\% and +1.6\%, respectively.

\textit{2) Performance on Fine-grained Datasets:}  
Table \ref{tab:comparison} demonstrate that our method consistently achieves the best performance across all incremental task stages on fine-grained datasets ImageNet-Subset \cite{deng2009imagenet} and CUB-200 \cite{wah2011caltech}. On ImageNet-Subset, our method outperforms TaskRes-CL \cite{yu2023task}, SPU \cite{zhang2024overcoming}, and AttriCLIP \cite{wang2023attriclip} by +1.7\%, +1.8\%, and +5.7\%, respectively. For CUB-200, our approach achieves the highest task accuracy at every stage. With $T=10$, our method surpasses SLCA by +4.5\%, SPU by +2.2\%, and TaskRes-CL by +3.4\% on the `Last' metric.

\textit{3) Performance with CLIP ViT-B/16:}  
Table \ref{tab:vit-b-16} presents the continual learning performance across 10 tasks utilizing the efficient backbone CLIP ViT-B/16 \cite{radford2021learning}. Without relying on replay mechanisms or incorporating additional architectural components, our method almost outperforms state-of-the-art approaches on both CIFAR100 \cite{krizhevsky2009learning} and CUB-200 \cite{wah2011caltech}. Notably, while achieving outstanding performance on CIFAR100 and CUB-200, our method maintains accuracies of 62.0$\%$ and 62.5$\%$ on the control set ImageNet-full \cite{deng2009imagenet}, closely matching the original pretrained CLIP accuracy of 63.6$\%$.




 \begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/visualization.pdf}
  \vspace{-2.5em}
  \caption{Visualizations of compliance and noncompliance attribute descriptions decided by \textbf{AEF}.}
  \label{fig:visual1}
\end{figure*}
 \begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/visual2.pdf}
  \vspace{-1em}
  \caption{Visualizations of closest descriptions.}
  \label{fig:visual2}
\end{figure*}

\textit{4) Zero-shot Degradation or Improvement?}  
We propose a metric to evaluate the relative accuracy changes for all classes with respect to the zero-shot CLIP model after incremental tasks. Specifically, we calculate the class-wise accuracy difference between the zero-shot CLIP and the trained CLIP model after half of the incremental tasks (50 classes), denoted as $\Delta_{50|(50,zs)}$, and present the results in Fig. \ref{fig:Deg or Imp} (Upper). Our method achieves the highest averaged $\Delta_{50|(50,zs)}$, reaching +9.3. This demonstrates that our approach experiences the least zero-shot degradation while achieving superior performance, attributed to the more robust visual-textual relationships established during continual learning compared to the original vision-class text relationships.  

Furthermore, we calculate the accuracy difference between the `after all-class incremental learning' state and the `only learning on the first-50 classes' state, denoted as $\Delta_{50|(100,50)}$, as shown in Fig. \ref{fig:Deg or Imp} (Bottom). Our method achieves an averaged $\Delta_{50|(100,50)}$ of +0.02, indicating effective knowledge transfer from previous tasks. This improvement can be attributed to the establishment of a strong connection between general visual-textual knowledge during continual learning.  

\textit{5) Few-shot Performance:}  
Fig. \ref{fig:few-shot} illustrates the continual learning performance in few-shot scenarios on the CUB-200 dataset \cite{wah2011caltech}. When the number of training samples per task is reduced from `full' to `1/5', our method maintains a competitive advantage, with an average performance drop of only -6.4. This decline is significantly smaller compared to TaskRes-CL's -7.3 and SLCA's -18.0, showcasing the robustness of our approach under challenging conditions.


\iffalse
\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CIFAR100 \cite{krizhevsky2009learning}, which is divided into 10 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 10\}$ are reported. \textbf{UB} represents the results tuning from all data jointly, which is considered as the upper-bound.}
    \label{tab:cifar100}
    \renewcommand{\arraystretch}{1.2} % Adjust row height
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \begin{tabular}{lccccccccc>{\columncolor{gray!20}}c|c}
        \toprule[1pt]
        \textbf{Method} & 1 &  2 &  3 &  4 &  5 &  6 &  7 & 8 &  9 &  10(\textit{L-acc}) & \textit{A-acc}\\
        \midrule
        CoOp \cite{} & 96.65 & 93.80 & 87.60 & 82.04 & 81.68 & 79.54 & 79.98 & 77.02 & 76.46 & 75.08 & 82.99\\
        %CoOpOp \cite{} & 96.70 & 91.16 & 86.54 & 81.01 & 76.40 & 72.24 & 69.78 & 66.72 & 63.50 & 61.15 & 76.52\\
        ContinualCLIP \cite{thengane2022clip} & \underline{94.20} & \underline{89.60} & \underline{85.33} & \underline{83.60} & \underline{80.20} & \underline{79.01} & \underline{76.07} & \underline{74.36} & \underline{74.72} &\underline{72.83} & \underline{80.09}\\
        SLCA \cite{zhang2023slca} &96.10 & 94.59 & 93.73 & 90.10 & 87.71 & 85.34 &84.60 & 82.10 &81.07 &80.23 & \\
        AttriCLIP \cite{wang2023attriclip} & 97.14 & 93.57 & 91.20 & 88.78 & 84.50 & 82.18 & 82.04 & 81.25 & 81.40 & 80.85 & 86.31\\
        %SPU \cite{zhang2024overcoming} & 98.03 & 93.64 & 90.07& 86.45 & 82.20 & 78.98 & 75.43 & 72.90 & 68.74 & 66.41 & \\     
        SPU \cite{zhang2024overcoming}  & 98.03 & 94.64 & 91.20 & 89.12 & 87.82 & 85.45 & 83.06 & 82.77 & 82.21 & 82.04 & 87.63\\
        TaskRes-CL\cite{yu2023task} & 96.70 & 94.15 & 90.40 & 89.40 & 87.74 & 84.92 & 84.05 & 82.28 & 82.04 & 81.24 & 87.39\\    
        DesCLIP   & \textbf{98.53} & \textbf{96.07} & \textbf{93.90} & \textbf{91.44}  & \textbf{90.07} & \textbf{88.10} & \textbf{87.73} & \textbf{85.85} & \textbf{85.14} & \textbf{84.45} & 90.13\\
        \midrule
        \textbf{UB}  & - & - & - & - & - & - & - & - & - & 90.05 & -\\
        \bottomrule[1pt]
    \end{tabular}
\end{table*}


\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CIFAR100 \cite{krizhevsky2009learning}, which is divided into 5 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 5\}$ are reported. \textbf{UB} represents the results tuning from all data jointly, which is considered as the upper-bound.}
    \label{tab:cifar100}
    \renewcommand{\arraystretch}{1.2} % Adjust row height
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \begin{tabular}{lccccc}
        \toprule[1pt]
        \textbf{Method} & 1 &  2 &  3 &  4 &  5 \\
        \midrule
        CoOp \cite{} & 94.70 & 88.22 & 83.28 & 80.05 & 78.85 \\
        %CoOpOp \cite{} &  &  &  &  &  \\
        ContinualCLIP \cite{thengane2022clip} & \underline{89.60} & \underline{83.60} & \underline{79.01} & \underline{74.36} & \underline{72.83}\\
        SLCA \cite{zhang2023slca}& 94.90 & 89.03 & 88.47 & 83.48 & 80.20\\
        AttriCLIP \cite{wang2023attriclip} & 94.60 & 90.34 & 85.10 & 82.07 & 81.87\\
        SPU \cite{zhang2024overcoming} & 95.06 & 92.64 & 87.72& 84.54 & 82.98 \\
        TaskRes-CL\cite{yu2023task} & 95.06 & 90.90 & 86.75 & 84.44 & 84.20 \\    
        DesCLIP   & \textbf{95.90} & \textbf{92.23} & \textbf{89.06} & \textbf{86.90}  & \textbf{85.88} \\
        \midrule
        \textbf{UB}  & - & - & - & - & - \\
        \bottomrule[1pt]
    \end{tabular}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CIFAR-100 \cite{krizhevsky2009learning}, which is divided into 20 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 20\}$ are reported.}
    \renewcommand{\arraystretch}{2} % 调整行距
    \setlength{\tabcolsep}{0.5pt} % 调整列间距
    \begin{tabular}{lcccccccccccccccccccccccccccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20}\\
        \midrule
        CoOp \cite{zhou2022learning} & 96.10 & 96.60 & 92.50 & 91.75 & 88.10 & 88.75 & 87.10 & 87.61 & 85.19 & 84.80 & 85.10 & 84.22 & 82.01 & 81.07 & 80.82 & 79.77 & 79.90 & 77.98 & 77.40 & 78.24 \\
        %CoOpOp \cite{zhou2022conditional} \\
        ContinualCLIP \cite{radford2021learning} & \underline{92.70} & \underline{94.20} & \underline{90.81} & \underline{89.60} & \underline{88.72} & \underline{85.33} & \underline{83.46} & \underline{83.60} & \underline{81.07} & \underline{80.20} & \underline{79.88} & \underline{79.01} & \underline{77.01} & \underline{76.07} & \underline{75.50} & \underline{74.36} & \underline{74.40} & \underline{74.72} & \underline{73.60} & \underline{72.83} \\
        SLCA \cite{zhang2023slca} & 96.05 & 96.10 & 96.68 & 92.01 & 92.60 & 91.34 & 90.22 & 91.05 & 89.00 & 88.77 & 87.15 & 85.43 & 83.94 & 83.22 & 82.82 & 82.74 & 81.08 & 81.43 & 80.54 & 80.22\\
        AttriCLIP \cite{wang2023attriclip} & 95.82 & 96.60 & 93.05 & 92.24 & 88.87 & 89.03 & 88.85 & 88.02 & 86.96 & 86.00 & 85.13 & 83.55 & 83.29 & 80.14 & 81.03 & 80.86 & 80.40 & 79.90 & 80.01 &  79.64\\
        SPU \cite{zhang2024overcoming} & 96.88 & 97.70 & 95.14 & 92.95  & 90.47 & 89.08 & 87.40 & 87.15 & 87.25 & 87.00 & 86.78 & 85.35 & 84.02 & 83.10 & 82.40 & 82.00 &  81.33 & 81.10 & 80.40& 80.16 \\ 
        TaskRes-CL \cite{yu2023task} & 96.00 & 97.20 & 94.87 & 92.75 & 92.24 & 89.00 & 87.69 & 87.13 & 86.71 & 86.10 & 85.16 & 84.25 & 82.94 & 82.31 & 80.72 & 80.44 & 80.60 &80.64& 80.40 & 79.94 \\
        DesCLIP   & \textbf{97.20} & \textbf{97.33} & \textbf{96.47}  & \textbf{95.10} & \textbf{93.90} & \textbf{92.00} & \textbf{90.43} & \textbf{89.60} & \textbf{89.44} & \textbf{88.32} & \textbf{87.20} & \textbf{86.00} & \textbf{85.90} & \textbf{84.43} & \textbf{84.06} &\textbf{83.77}&\textbf{83.65}& \textbf{83.83} & \textbf{83.56}& \textbf{82.90}\\  \midrule
        \textbf{UB}  & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 79.08 \\
        \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on ImageNet-Subset \cite{deng2009imagenet}, which is divided into 10 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 10\}$ are reported. \textbf{UB} represents the results tuning from all data jointly, which is considered as the upper-bound.}
    \label{tab:imagenet100}
    \renewcommand{\arraystretch}{1.2} % 调整行距
    \setlength{\tabcolsep}{4pt} % 调整列间距
    \begin{tabular}{lccccccccc>{\columncolor{gray!20}}c|c}
        \toprule[1pt]
         \textbf{Method} & 1 &  2 &  3 &  4 &  5 &  6 &  7 & 8 &  9 &  10(\textit{L-acc}) & \textit{A-acc}\\
        \midrule
        CoOp \cite{} & 88.68 & 90.08 & 86.02 & 82.25 & 80.18 & 78.46 & 76.98 & 76.40 & 77.10 & 78.04 & 81.42\\
        %CoOpOp \cite{} & 89.94 & 91.44 & 89.15 & 84.42 & 78.07 & 75.40 & 74.42 & 69.72 & 67.05 & 67.18 & 79.68\\
        ContinualCLIP \cite{thengane2022clip} & 87.80 & 89.15 & 85.24 & 81.80 & 77.54 & 75.10 & 71.38 & 72.25 & 73.62 & 74.55 & 78.84\\
        SLCA \cite{zhang2023slca} & 90.06 & 92.45 & 90.10 & 85.64 &84.17 & 82.42 & 80.90 & 78.42 & 79.70 & 80.38\\
        AttriCLIP \cite{} & 87.20 & 88.54 & 86.07 & 82.95 & 82.92 & 82.37 & 76.46 & 77.55 & 75.51 & 78.50 & 81.81\\
        
        SPU \cite{zhang2024overcoming} & 89.80 & 92.50 & 91.13 & 85.20  & 83.12 & 82.77 & 80.10 & 78.90 & 79.77 & 80.48 & 84.38\\   
        TaskRes-CL\cite{yu2023task} & 90.08 & 93.60 & 91.53 & 87.15 & 85.20 & 83.43 & 81.03 & 81.30& 81.44 & 82.46 & 85.72 \\
        DesCLIP  & \textbf{91.28} & \textbf{93.90} & \textbf{92.45} & \textbf{89.20}  & \textbf{86.92} & \textbf{85.84} & \textbf{83.75} & \textbf{82.18} & \textbf{82.97} & \textbf{84.20} & \textbf{87.37}\\
        \midrule
        \textbf{UB}  & - & - & - & - & - & - & - & - & - & 88.70 & -\\
        \bottomrule[1pt]
    \end{tabular}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on ImageNet-Subset \cite{deng2009imagenet}, which is divided into 5 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 5\}$ are reported.}
    \renewcommand{\arraystretch}{1.5} % 调整行距
    \setlength{\tabcolsep}{1pt} % 调整列间距
    \begin{tabular}{lcccccccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
        \midrule
        CoOp \cite{zhou2022learning} &92.70 & 82.50 & 80.24 & 74.80 &78.74 \\ 
        %CoOpOp \cite{zhou2022conditional} &\\
        ContinualCLIP \cite{radford2021learning}&89.18 & 81.84 & 76.10 & 72.35 & 74.64  \\
        SLCA \cite{zhang2023slca}& 93.42 & 89.05 & 85.22 & 80.10 & 82.33\\
        AttriCLIP \cite{wang2023attriclip} &94.40 &86.90 &84.57 &76.30 &79.20 \\
        SPU \cite{zhang2024overcoming} & 93.60 & 88.40 & 84.50 & 80.08 &80.80\\ 
        TaskRes-CL \cite{yu2023task} &93.80 & 87.97 & 85.70 & 81.22 & 82.86 \\
        DesCLIP & 94.50 & 89.50 &87.20 &82.37 & 84.28\\  \midrule
        \textbf{UB} \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on ImageNet-Subset \cite{deng2009imagenet}, which is divided into 20 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 20\}$ are reported.}
    \renewcommand{\arraystretch}{2} % 调整行距
    \setlength{\tabcolsep}{0.5pt} % 调整列间距
    \begin{tabular}{lcccccccccccccccccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} &\textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20}\\
        \midrule
        CoOp \cite{zhou2022learning} & 94.00& 87.20 &82.93 &90.07 &88.24 &84.40 &83.77 &80.10 &78.71 &80.64 &79.64 &78.63 &77.35 &74.91 &73.31 &73.90 &76.16 & 75.29 &77.09 &74.86\\
        %CoOpOp \cite{zhou2022conditional} \\
        ContinualCLIP \cite{radford2021learning} & \underline{94.40} & \underline{88.00} & \underline{91.46} & \underline{89.10} & \underline{89.60} & \underline{85.60} & \underline{83.33} & \underline{81.95} & \underline{77.33} & \underline{77.64} & \underline{76.74} & \underline{75.47} & \underline{73.05} & \underline{71.54} & \underline{72.13} & \underline{72.55} & \underline{73.45} & \underline{74.29} & \underline{75.33} & \underline{74.98} \\
        SLCA \cite{zhang2023slca} & 94.40 & 91.40 & 92.80 & 92.67 & 93.10 & 90.58 & 88.77 & 85.90 & 86.08 & 85.31 & 83.00 & 83.40 & 82.94 & 80.05 & 80.32 & 79.04& 78.87 &79.60 &79.92 & 80.18\\
        AttriCLIP \cite{wang2023attriclip} & 93.60 & 89.40 &92.00 &91.60 &92.40 &87.93 &84.20 &82.66 &83.51 & 81.76 & 79.34 & 79.30 &74.55 & 73.34 &74.05 &75.48 &75.62 & 76.42 & 77.87 & 77.40\\
        SPU \cite{zhang2024overcoming}& 93.60 & 91.40 & 92.00 & 91.85 & 92.50 & 90.37 & 88.16 & 87.10 &85.34 & 83.07 &83.68 & 81.70 & 80.08 &77.23 & 79.42 & 79.90& 79.05 & 79.77 &80.00 &79.78\\ 
        TaskRes-CL \cite{yu2023task} &94.40 & 90.40 & 92.80 & 93.60& 93.42 &91.40& 89.21 &87.10 &86.00 &84.18 &84.22 &83.40 &81.10 &79.64 &80.04 &80.10 &80.05 &80.98 &81.70 & 82.06 \\
        DesCLIP & 94.80 & 91.40 & 93.87 & 94.80 &94.32 & 92.53 &90.91 &88.45 &87.56 &85.56 & 85.20 &84.88 &82.37 &80.98 &81.58 &81.25 &82.37 &82.26 &83.14 &83.22 \\  \midrule
        %\textbf{UB}  & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 79.08 \\
        \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CUB-200 \cite{wah2011caltech}, which is divided into 20 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 20\}$ are reported.}
    \renewcommand{\arraystretch}{2} % 调整行距
    \setlength{\tabcolsep}{0.5pt} % 调整列间距
    \begin{tabular}{lcccccccccccccccccccccccccccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20}\\
        \midrule
        CoOp \cite{zhou2022learning} & 90.95 & 83.50 & 67.67 & 63.67 & 65.23 & 57.89 & 58.15 & 62.76 & 60.51 & 60.47 & 59.08 & 52.92 & 49.97 & 48.73 & 50.02 & 49.63 & 47.53 & 46.02 & 43.88 & 49.14 \\
        CoOpOp \cite{zhou2022conditional} & 90.44 & 81.04 & 60.35 & 54.72 & 48.73 & 44.06 & 40.05 & 35.77 & 38.05 & 31.70 & 26.89 & 22.04 & 18.82 & 20.54 & 18.90 & 16.64 & 14.75 & 11.88 & 12.42 &9.70\\
        ContinualCLIP \cite{radford2021learning} & \underline{66.26} & \underline{78.64} & \underline{68.41} & \underline{66.51} & \underline{69.62} & \underline{68.62} & \underline{64.68} & \underline{66.37} & \underline{67.52} & \underline{68.47} & \underline{66.94} & \underline{65.77} & \underline{62.58} & \underline{62.98} & \underline{62.78} & \underline{61.70} & \underline{61.13} & \underline{59.93} & \underline{60.03} & \underline{60.86} \\
        SLCA \cite{zhang2023slca} & 94.05 & 93.10 & 87.68 & 84.01 & 83.60 & 83.34 & 84.22 & 82.05 & 80.10 & 78.77 & 78.05 & 76.43 & 74.34 & 74.12 & 73.82 & 72.94 & 72.08 & 70.43 & 71.54 & 71.12\\
        AttriCLIP \cite{wang2023attriclip} & 92.71 & 81.03 & 76.66 & 72.84 & 73.43 & 71.89 & 66.85 & 68.18 & 68.42 & 67.77 & 66.37 & 63.12 & 62.21 & 61.05 & 60.88 & 59.04 & 61.04 & 58.77 & 58.90 &  57.07\\
        SPU \cite{zhang2024overcoming} & 92.88 & 89.12 & 85.14 & 82.95  & 82.47 & 80.08 & 81.40 & 79.85 & 77.25 & 76.10 & 76.38 & 74.05 & 73.02 & 72.10 & 71.64 & 72.88 &  72.08 & 72.24 & 71.52& 70.88 \\ 
        TaskRes-CL \cite{yu2023task} & 90.71 & 90.10 & 83.15 & 82.75 & 82.15 & 81.20 & 78.39 & 79.09 & 78.04 & 79.06 & 76.70 & 76.46 & 75.01 & 74.55 & 73.50 & 73.27 & 73.46 & 72.59 & 72.18 & 72.70 \\
        DesCLIP   & \textbf{93.83} & \textbf{93.01} & \textbf{86.52}  & \textbf{85.78} & \textbf{85.75} & \textbf{84.34} & \textbf{80.36} & \textbf{81.34} & \textbf{82.69} & \textbf{82.40} & \textbf{81.01} & \textbf{81.08} & \textbf{78.73} & \textbf{78.25} & \textbf{77.23} &\textbf{77.13}&\textbf{76.40}& \textbf{76.04} & \textbf{76.48}& \textbf{75.34}\\  \midrule
        \textbf{UB}  & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 79.08 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CUB-200 \cite{wah2011caltech}, which is divided into 10 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 10\}$ are reported.}
    \renewcommand{\arraystretch}{1.5} % 调整行距
    \setlength{\tabcolsep}{1pt} % 调整列间距
    \begin{tabular}{lcccccccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
        \midrule
        CoOp \cite{zhou2022learning} &84.85 & 63.76 & 58.12 & 54.31 & 52.76 & 49.83 &49.63 &44.21 &43.64& 41.75\\ 
        CoOpOp \cite{zhou2022conditional} &\\
        ContinualCLIP \cite{radford2021learning}& 78.64 & 66.51 &68.62 & 66.37 & 68.47 & 65.77& 62.98 &61.70 & 59.93 & 60.86 \\
        SLCA \cite{zhang2023slca} & 94.48 & 83.44 &83.50 & 82.45 & 79.64 & 77.87 & 74.82 & 74.69 & 74.85 & 73.94 \\
        AttriCLIP \cite{wang2023attriclip} &93.01 &76.92 & 73.90 & 69.32 & 67.18 & 65.86 & 61.37 & 50.14&53.80 & 52.49\\
        SPU \cite{zhang2024overcoming}  &94.08 & 85.36 & 83.10 & 81.44 & 80.08 & 79.01 &77.41 & 76.82 & 73.08 & 72.65\\ 
        TaskRes-CL \cite{yu2023task} &93.40 & 84.22 &83.39 &80.90 &81.22 &78.83 &76.51 &75.79 & 74.36 & 75.03 \\
        DesCLIP & 95.14 &87.43 &86.89 &84.29 &84.95 & 82.92 &80.51 &79.23 &77.98 & 78.41\\  \midrule
        \textbf{UB} \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Average accuracy of different continual learning methods on CUB-200 \cite{wah2011caltech}, which is divided into 5 tasks. The accuracy of task $t$, $t \in \{1, 2, \ldots, 5\}$ are reported.}
    \renewcommand{\arraystretch}{1.5} % 调整行距
    \setlength{\tabcolsep}{1pt} % 调整列间距
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Method} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
        \midrule
        CoOp \cite{zhou2022learning} &76.81 & 68.93 & 61.87 & 57.72 & 53.06\\ 
        CoOpOp \cite{zhou2022conditional} &\\
        ContinualCLIP \cite{radford2021learning}& 65.51 &66.37 &  65.77 &61.70 & 60.86\\
        SLCA \cite{zhang2023slca} & 84.04 & 79.45  & 78.07 & 77.24 & 75.48\\
        AttriCLIP \cite{wang2023attriclip} &85.60 &74.30 &71.46 & 67.08 & 62.79\\
        SPU \cite{zhang2024overcoming} &87.23 & 82.01 & 79.98 & 77.04 & 76.80\\ 
        TaskRes-CL \cite{yu2023task} &83.67&81.34& 78.81 & 75.38 & 75.41 \\
        DesCLIP &88.53 &86.01 &84.34 & 82.37 & 81.34 \\  \midrule
        \textbf{UB} \\
        \bottomrule
    \end{tabular}
\end{table*}
\fi


\subsection{Ablation Study}
\textit{1) Effectiveness of Each Component:}  
We conduct detailed ablation studies on various components of our DesCLIP framework. As shown in Table \ref{tab:ablation1}, the zero-shot model is used as the baseline, and the relative improvements or declines are reported. It is evident that a fully fine-tuned visual encoder (V.E.(full)) suffers from catastrophic forgetting. Without data replay, merely fine-tuning the visual encoder (V.E.) partially (tuning the first MLPs in each Transformer block) proves inadequate for continual learning \cite{zhang2024overcoming}. Based on V.E., by integrating instance matching (\textbf{IM}) with visually filtered attribute description embeddings, we achieve a relative improvement of 2$\sim$3\% over the zero-shot baseline. This improvement highlights the effectiveness of \textbf{IM} in mitigating forgetting, which typically arises from overfitting specific downstream task classes. Furthermore, the impact of text embedding calibration (\textbf{TEC}) is notable. The combination of \text{V.E.}+\textbf{TEC}+\textbf{IM}+\textbf{TA}, leveraging the filtered attribute description embeddings, yields the best performance, showcasing the synergy between these components.


\textit{2) Parameter Selection:}  
Table \ref{tab:ablation2} presents the 10-task performance under different parameter configurations. Our analysis reveals that the effectiveness of the instance matching strength, $\lambda_{\textbf{IM}}$, is highly sensitive to the specific task context. Stronger instance matching proves advantageous in scenarios requiring finer granularity, such as CUB-200 \cite{wah2011caltech}. On the other hand, the filtering of attribute description embeddings plays a crucial role. Excessively conservative filtering (e.g., $\gamma=0$) or overly stringent filtering (e.g., $\gamma=0.05$) leads to noticeable performance degradation.

 \begin{figure}[!t]
  \centering
  \includegraphics[width=0.95\linewidth]{images/Filtered-out.pdf}
  \vspace{-0em}
  \caption{Highly-relevant and filtered-out visual instances.}
  \label{fig:visual3}
\end{figure}

 \begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/t-sne_visual_text.pdf}
  \vspace{-0em}
  \caption{t-SNE visualizations on CIFAR100 \cite{krizhevsky2009learning}. Dots: visual representations; Stars: text embeddings.}
  \label{fig:visual4}
\end{figure}

\textit{3) Types of Text Embedding Calibration:} We demonstrate the comparisons of different types of \textbf{TEC} in Table \ref{tab:ablation_tes}. ``$\Psi(\mathbf{w},\mathbf{s})=\mathbf{w}/\|\mathbf{w}\| + \alpha \cdot \mathbf{s}$" is the optimal solution, which is an calibration of the rudimentary text embedding in cosine space, which can best approximate the alignment of the representative shared attribute description embedding.

\textit{4) Generated Attribute Descriptions:} We analyze the impact of the generated description amount on the performance of continual learning, as shown in Fig. \ref{fig:ablation_des_nums}. For coarse CIFAR100 \cite{krizhevsky2009learning}, we only employed the auxiliary prompt ``\texttt{Please maintain diversity between descriptions}" to generate $n_{dsc}$ attribute descriptions for each class. Our findings indicated that the model performed optimally when $n_{dsc}$ is set to 30. In contrast, for the fine-grained CUB-200 \cite{wah2011caltech}, we have discovered that incorporating additional auxiliary prompts, ``\texttt{Describe a/an [CLS]'s attributes from its beak, eyes, body, belly, tail, wings, breast, etc.}" results in improved performance (red star in Fig. \ref{fig:ablation_des_nums}) even with a smaller $n_{dsc}$. This approach enhances the quality of the descriptions, provided that the language assistant possesses sufficient knowledge of the object's details.


\subsection{Visualizations}

\textit{1) Description of Compliance and Noncompliance:}
In Fig. \ref{fig:visual1}, we present examples visual image along with attribute description sentences that match and do not match the attribute descriptions filtered by \textbf{AEF}. In particular, different instances on the left, also from `Cardinal', select the similar attribute feature description  ``\texttt{a stout, bright orange beak}" Furthermore, the highly matched attribute ``\texttt{crimson red feathers that cover its entire body}" filtered by the first instance is not represented in the second instance.

\textit{2) GA Descriptions Closest to Calibrated Text Embeddings:} In Fig. \ref{fig:visual2}, we present the descriptions that are most closely aligned with the calibrated text embeddings. For each class, we showcase three of the top five closest attribute descriptions across several classes. Notably, some common visual attribute features are reflected in these descriptions, such as ``\texttt{slender, pointed wings; streaked brown and white feathers; a clean white belly; and darker hues on the wings and tail}'' for the ``Brown Creeper'' category.

\textit{3) Filtered-out Visual Instances:} As shown in Fig.~\ref{fig:visual3}, it can be observed that, compared to the retained visual instances, the filtered visual instances lack relevance to both general attributes and class-specific information.

\textit{4) t-SNE Visualization:} Fig. \ref{fig:visual4} presents the t-SNE visualizations of CLIP's visual representations and text embeddings of each class. We employed distribution alignment to transform text embeddings into the visual representation space, rather than using the original text embedding space. Intuitively, compared to Zero-shot, TaskRes-CL \cite{yu2023task} adjusts text embeddings in downstream incremental tasks without optimizing visual representations; SPU \cite{zhang2024overcoming} optimizes visual representations but struggles to align visual-text representations for unfamiliar classes. In contrast, our method achieves superior alignment of visual-text representations in incremental tasks.


\section{Limitations}
Observed in experimental trials, The effectiveness of our DesCLIP framework hinges on the language assistant (or real experts) being knowledgeable about the general attribute features of objects. This can pose challenges in certain applications that require indirect reasoning, such as StanfordCars \cite{krause20133d}, where it is difficult to accurately describe the representative features of a vehicle model associated with a specific license plate. Additionally, there are high standards for the quality of attribute descriptions; inappropriate prompts can introduce domain bias, ultimately hindering the ability of the Anchor-based embedding filter (\textbf{AEF}) to select highly relevant attribute features.


\section{Conclusions}
Current research for VLM-based continual learning predominantly emphasizes to connect visual inputs and specific new-class text in downstream tasks, frequently neglecting the latent relationship between general knowledge and specialized knowledge for VLMs. In this paper, we propose DesCLIP,  a framework that harnesses general attribute (GA) descriptions to enhance VLMs in establishing robust \textit{vision-GA-class text} associations. By going beyond the traditional connections between visual inputs and class texts, DesCLIP employs a language assistant to generate candidates for attribute descriptions through tailored prompting. Additionally, we implemented an anchor-based embedding filter (\textbf{AEF}) to extract highly relevant description embeddings, which serve as paired text embeddings for instance matching (\textbf{IM}). Addtionally, we perform text embedding calibration (\textbf{TEC}) which allows for the progressive calibration of rudimentary text embeddings to align with representative GA representations. Our extensive experiments validate the effectiveness and advancements of DesCLIP, demonstrating its superior performance over existing pretrained and VLM-based continual learning methods.


\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)

%\input{DesCLIP.bbl}  %  正确引入 .bbl

\bibliography{IEEEabrv,DesCLIP}
\begin{comment}
\begin{thebibliography}{1}

\bibitem{ams}
{\it{Mathematics into Type}}, American Mathematical Society. Online available:

\bibitem{oxford}
T.W. Chaundy, P.R. Barrett and C. Batey, {\it{The Printing of Mathematics}}, Oxford University Press. London, 1954.

\bibitem{lacomp}{\it{The \LaTeX Companion}}, by F. Mittelbach and M. Goossens

\bibitem{mmt}{\it{More Math into LaTeX}}, by G. Gr\"atzer

\bibitem{amstyle}{\it{AMS-StyleGuide-online.pdf,}} published by the American Mathematical Society

\bibitem{Sira3}
H. Sira-Ramirez.   On the sliding mode control of nonlinear systems,'' \textit{Systems \& Control Letters}, vol. 19, pp. 303--312, 1992.

\bibitem{Levant}
A. Levant.   Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proceedings of the 45th IEEE Conference on Decision and Control}, San Diego, California, USA, pp. 5585--5590, 2006.

\bibitem{Cedric}
M. Fliess, C. Join, and H. Sira-Ramirez.   Non-linear estimation is easy,'' \textit{International Journal of Modelling, Identification and Control}, vol. 4, no. 1, pp. 12--27, 2008.

\bibitem{Ortega}
R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez.   Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proceedings of the American Control Conference}, Chicago, Illinois, USA, pp. 2245--2249, 2000.

\end{thebibliography}
\end{comment}

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here without a photo.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiography}{IEEE Publications Technology Team}
%In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}


\end{document}


