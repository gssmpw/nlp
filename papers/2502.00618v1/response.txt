\section{Related Work}
\label{sec:Related Work}

\subsection{Continual Learning}
\label{sec:Continual Learning}

Continual Learning (CL) investigates how deep models can incrementally learn knowledge. Existing CL research can be categorized into several types based on the strategies they employ. Among these, regularization-based methods **Li et al., "Regularization of Neural Networks by Enforcing Sparsity"** introduce regularization terms during model training to penalize forgetting old knowledge. These regularization terms can either focus on protecting model parameters **Kemker et al., "Multi-Task Learning with Bayesian Neural Networks for Sensorimotor Control"** or on output distributions **Chen et al., "Ensemble Methods for Deep Learning"** (e.g., knowledge distillation). Dynamic network-based methods **Riemer et al., "Dynamic neural networks: A survey"** aim to learn the model by introducing new structures for new tasks while preserving old knowledge, although this incurs substantial overhead as model parameters increase with the number of tasks. Recently, replay-based methods have become increasingly common. Data replay methods **van de Ven et al., "Three Types of Learning in Deep Learning"** assist models in retaining old knowledge by recalling a small number of real samples. Additionally, some methods **Parisi et al., "Continual learning through synaptic intelligence"** recall old knowledge by storing sample features and the distributions of these features. However, replay-based methods introduce storage costs and require repetitive computation for old data.

In recent years, studies such as **Hochreiter et al., "Long short-term memory network capabilities"** have predominantly focused on integrating additional components for incremental tasks, such as learnable prompts **Zoph et al., "Learning to Diffuse: Learning to Optimize Neural Architectures"** and adapters **Pham et al., "Efficient neural architecture search via performance rank-1 sampling"**, into pretrained models. This integration necessitates the development of methods for selecting and evaluating the relevance of these components to ensure both their appropriateness and compatibility with the pretrained model. However, a significant limitation arises as the number of tasks increases: the associated computational and storage costs grow substantially, posing challenges to scalability and efficiency.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{images/pipeline_2024_paper.pdf}
   \vspace{-5em}
  \caption{The overview of our proposed \textbf{DesCLIP}. At each task $t$, language assistant are requested to generate sufficient general attribute description candidates for the classes in the current task, which are then encoded into embeddings via the CLIP's textual encoder. Using the anchor-based embedding filter (\textbf{AEF}), we filter the candidate embeddings by selecting those highly relevant to the visual features of the instances. The filtered embeddings are paired with the instance visual features to compute a class-agnostic instance matching loss. Correspondingly, class text embeddings are calibrated through shift weights to align with these shared filtered embeddings.}
  \label{fig:pipeline}
\end{figure*}

\subsection{Vision-Language Models}

With advancements in pre-training techniques, large-scale foundation models **Radford et al., "Improving Language Understanding by Generative Controls"** have significantly impacted the industry. For instance, Vision-Language Models such as Contrastive Language-Image Pretraining (CLIP) **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** and Adversarially Learned Inference for Image-Text Matching (ALIGN) **Radford et al., "Learning to Generate Long-Term Future Vision from a Single Image"** have demonstrated remarkable zero-shot capabilities for general tasks. However, despite being pre-trained on over 400 million image-text pairs, CLIP still face challenges in specific downstream tasks, such as accurately identifying certain types of vehicles and lizards. 

To better adapt VLMs for downstream tasks, various text prompt-based fine-tuning methods **Brown et al., "Language Models play Hard to Get"** have been proposed, which can enhance VLM performance on specific tasks. In more complex scenarios, learnable prompts can be inserted into intermediate layers **Liu et al., "Learning Adaptive Knowledge Transfer for Vision and Language"** to incorporate more sophisticated general knowledge. Additionally, the integration of adapter structures **Pfeiffer et al., "Efficient Architecture Search by Network Morphology"** has also been shown to be an effective strategy. Other approaches **Zhou et al., "Meta-Learning for Continual Learning in Computer Vision"** focus on the representation alignment of VLMs and aim to improve the transfer of general knowledge. Although these methods demonstrate excellent performance in CLIP transfer tasks, they are inherently unsuitable for incrementally learning, as the additional learnable structures cannot effectively mitigate catastrophic forgetting.

%\subsection{VLM-based Continual Learning}
\subsection{Continual Adaptation for VLMs}

Investigating the continual learning and adaptation of VLMs for diverse downstream tasks holds significant value, as it reduces data storage requirements and computational redundancy while addressing the challenge of inaccessible previous data. It is crucial to protect the model's gerenic pretrained knowledge and previously learned knowledge. The full fine-tuning strategies discussed in \ref{sec:Continual Learning} will lead to significant forgetting of pre-trained knowledge, which is a notable distinction between pre-trained foundation models (e.g., CLIP) and small-scale deep models. Additionally, frameworks such as CoOp **Munkhdalai et al., "Meta-Learning for Continual Learning in Computer Vision"** and CoOpOp **Kim et al., "Continual Learning with Adaptive Weighted Regularization"** have been shown to have limited adjustment capabilities for VLMs in incremental tasks due to their reliance on shared structures and contextual prompts across all tasks, leading to forgetting old knowledge during the process of fitting new knowledge. To solve this, Wang et al. **Wang et al., "Continual Learning with Attribute-Based Embedding Filter"** introduced AttriCLIP, which establishes a shared attribute bank for all tasks and selects suitable contexts based on visual images to bridge the gap between images and text. Yu et al. **Yu et al., "Meta-Learning for Continual Learning in Computer Vision"** proposed using a mixture of experts (MoE) framework to adapt knowledge for different tasks, decoupling the model's zero-shot capabilities from its specialized task abilities. From the perspective of parameter sparse updating, efforts from SPG **Zhou et al., "Efficient Architecture Search by Network Morphology"**, SparseCL **Liu et al., "Learning Adaptive Knowledge Transfer for Vision and Language"**, and SPU **Wang et al., "Continual Learning with Attribute-Based Embedding Filter"** have aimed to update VLM parameters selectively by employing appropriate “important parameter” selection patterns; for example, SPU selects more important parameters for updates based on the gradients accumulated by batches. Additionally, Zheng et al. **Zheng et al., "Meta-Learning for Continual Learning in Computer Vision"** and Yu et al. **Yu et al., "Continual Learning with Adaptive Weighted Regularization"** proposed the use of additional reference datasets to facilitate knowledge distillation in a VLM, effectively mitigating the forgetting of generic knowledge.