\section{Related Work}
\label{sec:Related Work}

\subsection{Continual Learning}
\label{sec:Continual Learning}

Continual Learning (CL) investigates how deep models can incrementally learn knowledge. Existing CL research can be categorized into several types based on the strategies they employ. Among these, regularization-based methods ____ introduce regularization terms during model training to penalize forgetting old knowledge. These regularization terms can either focus on protecting model parameters ____ or on output distributions ____ (e.g., knowledge distillation). Dynamic network-based methods ____ aim to learn the model by introducing new structures for new tasks while preserving old knowledge, although this incurs substantial overhead as model parameters increase with the number of tasks. Recently, replay-based methods have become increasingly common. Data replay methods ____ assist models in retaining old knowledge by recalling a small number of real samples. Additionally, some methods ____ recall old knowledge by storing sample features and the distributions of these features. However, replay-based methods introduce storage costs and require repetitive computation for old data.

In recent years, studies such as ____ have predominantly focused on integrating additional components for incremental tasks, such as learnable prompts ____ and adapters ____, into pretrained models. This integration necessitates the development of methods for selecting and evaluating the relevance of these components to ensure both their appropriateness and compatibility with the pretrained model. However, a significant limitation arises as the number of tasks increases: the associated computational and storage costs grow substantially, posing challenges to scalability and efficiency.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{images/pipeline_2024_paper.pdf}
   \vspace{-5em}
  \caption{The overview of our proposed \textbf{DesCLIP}. At each task $t$, language assistant are requested to generate sufficient general attribute description candidates for the classes in the current task, which are then encoded into embeddings via the CLIP's textual encoder. Using the anchor-based embedding filter (\textbf{AEF}), we filter the candidate embeddings by selecting those highly relevant to the visual features of the instances. The filtered embeddings are paired with the instance visual features to compute a class-agnostic instance matching loss. Correspondingly, class text embeddings are calibrated through shift weights to align with these shared filtered embeddings.}
  \label{fig:pipeline}
\end{figure*}

\subsection{Vision-Language Models}

With advancements in pre-training techniques, large-scale foundation models ____ have significantly impacted the industry. For instance, Vision-Language Models such as Contrastive Language-Image Pretraining (CLIP) ____ and Adversarially Learned Inference for Image-Text Matching (ALIGN) ____ have demonstrated remarkable zero-shot capabilities for general tasks. However, despite being pre-trained on over 400 million image-text pairs, CLIP still face challenges in specific downstream tasks, such as accurately identifying certain types of vehicles and lizards. 

To better adapt VLMs for downstream tasks, various text prompt-based fine-tuning methods ____ have been proposed, which can enhance VLM performance on specific tasks. In more complex scenarios, learnable prompts can be inserted into intermediate layers ____ to incorporate more sophisticated general knowledge. Additionally, the integration of adapter structures ____ has also been shown to be an effective strategy. Other approaches ____ focus on the representation alignment of VLMs and aim to improve the transfer of general knowledge. Although these methods demonstrate excellent performance in CLIP transfer tasks, they are inherently unsuitable for incrementally learning, as the additional learnable structures cannot effectively mitigate catastrophic forgetting.

%\subsection{VLM-based Continual Learning}
\subsection{Continual Adaptation for VLMs}

Investigating the continual learning and adaptation of VLMs for diverse downstream tasks holds significant value, as it reduces data storage requirements and computational redundancy while addressing the challenge of inaccessible previous data. It is crucial to protect the model's gerenic pretrained knowledge and previously learned knowledge. The full fine-tuning strategies discussed in \ref{sec:Continual Learning} will lead to significant forgetting of pre-trained knowledge, which is a notable distinction between pre-trained foundation models (e.g., CLIP) and small-scale deep models. Additionally, frameworks such as CoOp ____ and CoOpOp ____ have been shown to have limited adjustment capabilities for VLMs in incremental tasks due to their reliance on shared structures and contextual prompts across all tasks, leading to forgetting old knowledge during the process of fitting new knowledge. To solve this, Wang et al. ____ introduced AttriCLIP, which establishes a shared attribute bank for all tasks and selects suitable contexts based on visual images to bridge the gap between images and text. Yu et al. ____ proposed using a mixture of experts (MoE) framework to adapt knowledge for different tasks, decoupling the model's zero-shot capabilities from its specialized task abilities. From the perspective of parameter sparse updating, efforts from SPG ____, SparseCL ____, and SPU ____ have aimed to update VLM parameters selectively by employing appropriate “important parameter” selection patterns; for example, SPU selects more important parameters for updates based on the gradients accumulated by batches. Additionally, Zheng et al. ____ and Yu et al. ____ proposed the use of additional reference datasets to facilitate knowledge distillation in a VLM, effectively mitigating the forgetting of generic knowledge.