\section{Related Work}
\label{sec:Related Work}

\subsection{Continual Learning}
\label{sec:Continual Learning}

Continual Learning (CL) investigates how deep models can incrementally learn knowledge. Existing CL research can be categorized into several types based on the strategies they employ. Among these, regularization-based methods \cite{li2017learning, kirkpatrick2017overcoming, douillard2020podnet} introduce regularization terms during model training to penalize forgetting old knowledge. These regularization terms can either focus on protecting model parameters \cite{kirkpatrick2017overcoming} or on output distributions \cite{li2017learning, rebuffi2017icarl} (e.g., knowledge distillation). Dynamic network-based methods \cite{yan2021dynamically, boschini2022class, wang2022foster, zhou2022model} aim to learn the model by introducing new structures for new tasks while preserving old knowledge, although this incurs substantial overhead as model parameters increase with the number of tasks. Recently, replay-based methods have become increasingly common. Data replay methods \cite{wang2022anti, zhou2022model} assist models in retaining old knowledge by recalling a small number of real samples. Additionally, some methods \cite{zhu2021class, zhu2021prototype,petit2023fetril} recall old knowledge by storing sample features and the distributions of these features. However, replay-based methods introduce storage costs and require repetitive computation for old data.

In recent years, studies such as \cite{zhou2024expandable, zhou2023revisiting, wang2022learning, wang2022dualprompt} have predominantly focused on integrating additional components for incremental tasks, such as learnable prompts \cite{wang2022learning, wang2022dualprompt, smith2023coda, gao2024consistent} and adapters \cite{zhou2024expandable, zhou2023revisiting}, into pretrained models. This integration necessitates the development of methods for selecting and evaluating the relevance of these components to ensure both their appropriateness and compatibility with the pretrained model. However, a significant limitation arises as the number of tasks increases: the associated computational and storage costs grow substantially, posing challenges to scalability and efficiency.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{images/pipeline_2024_paper.pdf}
   \vspace{-5em}
  \caption{The overview of our proposed \textbf{DesCLIP}. At each task $t$, language assistant are requested to generate sufficient general attribute description candidates for the classes in the current task, which are then encoded into embeddings via the CLIP's textual encoder. Using the anchor-based embedding filter (\textbf{AEF}), we filter the candidate embeddings by selecting those highly relevant to the visual features of the instances. The filtered embeddings are paired with the instance visual features to compute a class-agnostic instance matching loss. Correspondingly, class text embeddings are calibrated through shift weights to align with these shared filtered embeddings.}
  \label{fig:pipeline}
\end{figure*}

\subsection{Vision-Language Models}

With advancements in pre-training techniques, large-scale foundation models \cite{radford2021learning,li2022blip,kirillov2023segment,openai2023gpt4,alayrac2022flamingo} have significantly impacted the industry. For instance, Vision-Language Models such as Contrastive Language-Image Pretraining (CLIP) \cite{radford2021learning} and Adversarially Learned Inference for Image-Text Matching (ALIGN) \cite{jia2021scaling} have demonstrated remarkable zero-shot capabilities for general tasks. However, despite being pre-trained on over 400 million image-text pairs, CLIP still face challenges in specific downstream tasks, such as accurately identifying certain types of vehicles and lizards. 

To better adapt VLMs for downstream tasks, various text prompt-based fine-tuning methods \cite{zhou2022learning, zhou2022conditional, 10171397, yao2023visual, 10814093} have been proposed, which can enhance VLM performance on specific tasks. In more complex scenarios, learnable prompts can be inserted into intermediate layers \cite{yao2024tcp} to incorporate more sophisticated general knowledge. Additionally, the integration of adapter structures \cite{li2024graphadapter,pantazis2022svl,xin2024vmt} has also been shown to be an effective strategy. Other approaches \cite{zhang2024concept, wu2024transferring, luo2024cheap, gao2022pyramidclip} focus on the representation alignment of VLMs and aim to improve the transfer of general knowledge. Although these methods demonstrate excellent performance in CLIP transfer tasks, they are inherently unsuitable for incrementally learning, as the additional learnable structures cannot effectively mitigate catastrophic forgetting.

%\subsection{VLM-based Continual Learning}
\subsection{Continual Adaptation for VLMs}

Investigating the continual learning and adaptation of VLMs for diverse downstream tasks holds significant value, as it reduces data storage requirements and computational redundancy while addressing the challenge of inaccessible previous data. It is crucial to protect the model's gerenic pretrained knowledge and previously learned knowledge. The full fine-tuning strategies discussed in \ref{sec:Continual Learning} will lead to significant forgetting of pre-trained knowledge, which is a notable distinction between pre-trained foundation models (e.g., CLIP) and small-scale deep models. Additionally, frameworks such as CoOp \cite{zhou2022learning} and CoOpOp \cite{zhou2022conditional} have been shown to have limited adjustment capabilities for VLMs in incremental tasks due to their reliance on shared structures and contextual prompts across all tasks, leading to forgetting old knowledge during the process of fitting new knowledge. To solve this, Wang et al. \cite{wang2023attriclip} introduced AttriCLIP, which establishes a shared attribute bank for all tasks and selects suitable contexts based on visual images to bridge the gap between images and text. Yu et al. \cite{yu2024boosting} proposed using a mixture of experts (MoE) framework to adapt knowledge for different tasks, decoupling the model's zero-shot capabilities from its specialized task abilities. From the perspective of parameter sparse updating, efforts from SPG \cite{konishi2023parameter}, SparseCL \cite{wang2022sparcl}, and SPU \cite{zhang2024overcoming} have aimed to update VLM parameters selectively by employing appropriate “important parameter” selection patterns; for example, SPU selects more important parameters for updates based on the gradients accumulated by batches. Additionally, Zheng et al. \cite{zheng2023preventing} and Yu et al. \cite{yu2025select} proposed the use of additional reference datasets to facilitate knowledge distillation in a VLM, effectively mitigating the forgetting of generic knowledge.