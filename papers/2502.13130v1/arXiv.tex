% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}
% Useful packages
\input{package}
\usepackage{pifont}

\definecolor{darkpastelred}{rgb}{0.76, 0.23, 0.13}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\magma}{\texttt{Magma}\xspace}
\newcommand{\magmapt}{Multimodal Agentic Pretraining}

% \usepackage[table]{xcolor}

% For author commenting
\newcommand{\jw}[1]{\textcolor{red}{[Jianwei: #1]}} %
\newcommand{\qh}[1]{\textcolor{blue}{[Qianhui: #1]}}

\newcommand{\ensuretext}[1]{#1}
\newcommand{\marker}[2]{\ensuremath{^{\textsc{#1}}_{\textsc{#2}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
\newcommand{\rt}[1]{\arkcomment{\marker{R}{T}}{#1}{olive}}
\newcommand\blfootnote[1]{\begingroup\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{commentcolor}{RGB}{34,139,34}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{1545} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Multimodal Agentic Foundation that Can See, Talk and Act}
\title{\includegraphics[height=0.8cm]{figures/magma_logo.png} Magma: A Foundation Model for Multimodal AI Agents}
% \title{Multimodal Agentic Pretraining with Spatial-Temporal Intelligence}
% \title{Multimodal Agentic Pretraining}

% pre-training with spatial-temporal marks for ...

%%%%%%%%% AUTHORS - PLEASE UPDATE
%\author{
%Jianwei Yang\textsuperscript{1*$\dagger$}~\hspace{3pt}
%Reuben Tan\textsuperscript{1$\dagger$}~\hspace{3pt}
%Qianhui Wu\textsuperscript{1$\dagger$}~\hspace{3pt}
%Ruijie Zheng\textsuperscript{2$\ddagger$}~\hspace{3pt}
%Baolin Peng\textsuperscript{2$\ddagger$}~\hspace{3pt}
%Yongyuan Liang\textsuperscript{2$\ddagger$}~\hspace{3pt} \\
%Yu Gu\textsuperscript{1}~\hspace{5pt}
%Mu Cai\textsuperscript{3}~\hspace{5pt}
%Seonghyeon Ye\textsuperscript{4}~\hspace{5pt}
%Joel Jang\textsuperscript{5}~\hspace{5pt}
%Yuquan Deng\textsuperscript{5}~\hspace{5pt}
%Lars Liden\textsuperscript{1}~\hspace{5pt}
%Jianfeng Gao\textsuperscript{1$\bigtriangledown$}~\hspace{5pt}
%\\
%{\small\textsuperscript{1}Microsoft Research\hspace{5pt} 
%\small\textsuperscript{2}University of Maryland\hspace{5pt} 
%\small\textsuperscript{3}University of Wisconsin-Madison\hspace{5pt}
%\\
%\small\textsuperscript{4}KAIST\hspace{5pt}
%\small\textsuperscript{5}University of Washington}\\ 
%\url{https://microsoft.github.io/Magma}
%}

\author{Jianwei Yang\textsuperscript{1*$\dagger$} \ \ \ Reuben Tan\textsuperscript{1$\dagger$} \ \ \ Qianhui Wu\textsuperscript{1$\dagger$} 
\ \ \ Ruijie Zheng\textsuperscript{2$\ddagger$} \ \ \  Baolin Peng\textsuperscript{2$\ddagger$} \ \ \ Yongyuan Liang\textsuperscript{2$\ddagger$} \\
Yu Gu\textsuperscript{1} \ \ \ Mu Cai\textsuperscript{3} \ \ \ Seonghyeon Ye\textsuperscript{4} \ \ \ Joel Jang\textsuperscript{5} \ \ \ Yuquan Deng\textsuperscript{5} \ \ \ Lars Liden\textsuperscript{1} \ \ \ Jianfeng Gao\textsuperscript{1$\bigtriangledown$} \\
$^{1}$\small{Microsoft Research}, $^{2}$University of Maryland, $^{3}$University of Wisconsin-Madison
\\
$^{4}$\small{KAIST}, $^{5}$University of Washington \\
\url{https://microsoft.github.io/Magma}
\\
}


\begin{document}
%\maketitle
%\renewcommand\twocolumn[1][]{#1}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
   \captionsetup{type=figure}
    \vspace{-0.8cm}    
\includegraphics[width=1.0\linewidth]{figures/magma_teaser.pdf}    
    \vspace{-0.5cm}
    \caption{We introduce \magma, the \textit{first} foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, \magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, \magma bridges verbal and spatial intelligence to navigate complex tasks.}
    \label{fig:teaser} 
\end{center}   
}]

\blfootnote{$^{\dagger}$ First Authors; $^{\ddagger}$ Second Authors; $^*$ Project Lead; $^{\bigtriangledown}$ Leadership}

% \begin{figure*}[t!]% Use '!t' instead of just 't'
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/images/magma_fig_1_2_3_reuben_qianhui.pdf}
%     \vspace{-10pt}
%     \caption{We introduce our Multimodal Agentic Model (\magma), that is capable of interpreting and grounding multimodal inputs within its environment. Given a described goal, \magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, \magma bridges verbal and spatial intelligence to navigate complex tasks and settings. \jw{polish this teaser figure to add a radar plot}}
%     \label{fig:teaser}
% \end{figure*}

\input{arXiv_release/0_abstract}
\input{arXiv_release/1_intro}
\input{arXiv_release/2_related}
\input{arXiv_release/3_method}
\input{arXiv_release/4_experiment}
\input{arXiv_release/5_conclusion}

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}
\input{arXiv_release/X_suppl}

% WARNING: do not forget to delete the supplementary pages from your submission 

\end{document}
