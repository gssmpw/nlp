\section{Related Work}
\label{sec:related}

\textbf{Large Multimodal Models~(LMMs).} Large Language Models (LLMs) like ChatGPT~\citep{chatgpt}, GPT-4~\citep{gpt4}, and Llama~\citep{touvron2023llama} have demonstrated impressive reasoning and generalization capabilities for text. The introduction of models that integrate visual data has brought about a significant shift in the landscape of LLMs, such as GPT-4V(ision)\citep{GPT4V_System_Card}. Building upon open-source LLMs \citep{touvron2023llama,vicuna2023}, a wide range of multimodal models have achieved remarkable progress, led by pioneering models such as LLaVA~\citep{liu2023llava, liu2023improvedllava} and MiniGPT-4~\citep{zhu2023minigpt}, which combine LLMs' capabilities with a CLIP~\citep{radford2021learning} based image encoder. Recently,  a growing number of LMMs have been developed to handle a wider range of tasks and modalities, such as region-level LMMs~\citep{cai2024vipllava, zhang2023gpt4roi, chen2023shikra, peng2023kosmos,zhang2023llavagrounding}, and video LMMs~\citep{lin2023video, zhang2023video, zhang2024llavanextvideo,tan2024koala}. In parallel, more sophisticated benchmarks are proposed to assess these capabilities~\cite{fu2024videommefirstevercomprehensiveevaluation,fu2024blink,cai2024temporalbenchbenchmarkingfinegrainedtemporal}. 

\noindent\textbf{UI Agent in Digital World.} Recently there has been a lot of work on designing autonomous GUI agents to perform tasks in place of human users. One line of work is to train an end-to-end model to directly predict the next action, representative works include Pixel2Act~\cite{pixel2act} and WebGUM\cite{webgum} in web domain, Ferret~\cite{ferretui}, CogAgent~\cite{cogagent}, and Fuyu~\cite{fuyu_8b} in Mobile domain. Another line of work involves leveraging existing multimodal models such as GPT-4V to perform user tasks. Representative works include MindAct~\cite{mind2web}, SeeAct~\cite{zheng2024gpt4vision} in web domain and others ~\cite{gpt4v_wonderland, mobile_agent, aitw} for mobile domain. 
These works often leverage the DOM information in web browsers, or the view hierarchies in mobile apps to get the ground truth position of interactable elements of the screen, and use Set-of-Mark~\cite{setofmark} or more advanced localization model~\cite{lu2024omniparser} to overlay the bounding boxes on top of the screenshot that feed into the vision-language models. 

\noindent\textbf{Vision-Language-Action for Robotics.} Several studies have investigated the application of LMMs in robotics~\citep{brohan2023rt,niu2024llarva,zhu2024vision,li2024llara,kim2024openvla,zheng2024tracevla,ye2024latent}. 
Among these, RT-2~\citep{brohan2023rt} finetuned LMMs on robotic trajectory data, enabling the output of discretized robot action tokens. OpenVLA~\citep{kim2024openvla} is the first open-source VLA foundation that is fine-tuned an open-source Prismatic VLM backbone~\citep{karamcheti2024prismatic}.
LLARVA~\citep{niu2024llarva} generated 2D visual traces for robot arms along with textual representations of actions, using visual trace prediction as an auxiliary task, while TraceVLA~\cite{zheng2024tracevla} used visual trace prompting to improve spatial-temporal awareness of robot policy. Most recently, learning from videos by predicting the latent VQVAE tokens is explored in \cite{cheang2024gr2generativevideolanguageactionmodel,ye2024latentactionpretrainingvideos}. In this work, we follow a similar approach as OpenVLA to represent the action but leverage rich multimodal data far beyond robotics datasets. Also, instead of asking model to predict latent tokens, we propose SoM and ToM techniques to significantly enhance the spatial-temporal intelligence, demonstrating significantly stronger performance and generalization capability for agentic tasks.