\section{Multimodal Agentic Modeling}
\label{sec:formatting}

\subsection{Problem Definition}

A generalist multimodal AI agent should be performant for both multimodal understanding and action-taking. We define a multimodal AI agent ${\pi}$, which takes past visual observations $\mathcal{I} = \{I_1,...,I_k\}$ and a task description $\texttt{task}$ in text as input and outputs a set of $T\geq 1$ tokens $\mathcal{O}$ as:
%$\mathcal{O} = \{o_1,...,o_{k}\}$:
\begin{equation}
    \mathcal{O} = \mathcal{\pi}(\mathcal{I}, \texttt{task}, \texttt{ctx}) = \{o^{l}_{1},\cdot\cdot\cdot,o^{l}_{T}\}
    \label{equ1:original target}
\end{equation}
where $\texttt{ctx}$ denotes the context, $l \in \{\texttt{verbal}, \texttt{spatial}\}$ indicates if the $i$-th token $o_i$ is a verbal or spatial token. This formula generalizes across different tasks:
\begin{itemize}
    \item \textbf{UI navigation in 2D screenshots}. The task could be ``book a hotel'' and output should include both language tokens denoting the semantic type of action (\eg, ``type'', ``click'', \etc) and the location $(x,y)$ or box $(x,y,w,h)$ to which actions are applied.
    \item \textbf{Robotic manipulation in the 3D world}. For a task like ``close the drawer'', the output consists of 6-DoF displacements $(x,y,z,yaw,pitch,roll)$ of the end effector and, in some cases, one additional dimension to indicate whether the gripper is open or not.
    \item \textbf{Multimodal understanding tasks}. When the task is purely about $\mathcal{I}$, \eg, a VQA task, the problem is reduced to a multimodal understanding task that generates a textual description and/or location of objects for input images/videos.
\end{itemize}

For these seemingly different output modalities, we follow a common practice to transform all output into textual tokens to facilitate model learning. Specifically, we convert 2D actions into a textual dictionary as in~\cite{seeclick}, and represent robot actions with the last 256 discrete language tokens that is barely used in LLMs, following~\cite{kim2024openvla}. Despite such unification into language space, we notice considerable conflicts among tasks, as we will show in our experiments. In what follows, we will discuss how to mitigate such challenges to train agentic foundation on a wide range of datasets.

\subsection{Method}

We approach two key challenges while building a highly capable foundation for the multimodal AI agent. 

\noindent \textbf{Pretraining objectives}: How to build a unified pretraining interface to facilitate joint training? A straightforward way would be to predict the \textit{2D} coordinates for the navigation of the UI, \textit{ 3D} positions for the end effectors, and regular textual outputs for VL tasks. However, in our experiments, we observed that these tasks have inherent domain gaps in both input and output. The former results in a huge search space at the pixel level, and the latter directly predicts the output of proprioceptive action, which is not grounded on the observations of the image. \textit{Can we come up with a surrogate task that can bridge the gap among all tasks?}

\noindent \textbf{Data scaling-up}: Existing vision-language-action data have limited amount and diversity, unlike language or image-text corpus for LLMs and LMMs, respectively. 
%as we see in language domain. 
For example, the largest open source robotic dataset OXE~\cite{open_x_embodiment_rt_x_2023} consists of around 1M trajectories taken from 22 environments. On the other hand, large-scale image-text datasets like LAION~\cite{schuhmann2021laion} barely contain useful supervisions for action pretraining as they are all static without the notion of action. Videos, however, depict numerous human actions and human-object interactions.\textit{ Can we largely take advantage of these video data for our agentic pretraining?}

In this work, we propose a simple yet effective method to address the aforementioned challenges. Inspired by the generality of Set-of-Mark~(SoM) rompting~\cite{yang2023set}, we employ it to enable the action grounding onto images for both UI and robotic tasks in that model faces much less difficulties to predict the numeric marks for both clickable buttons or robot arms in the image space. We further extend it along the temporal axis and ask the model to predict Trace-of-Mark~(ToM), which forces the model to learn a longer horizon by predicting distant future ``actions'', and more importantly provides an effective way to leverage unlabeled video data. The combination of SoM and ToM enables a seamless synergy across agentic tasks in digital and physical domains, as well as a scalable way to curate ``action'' supervisions from raw videos. We describe them in detail below in Sec.~\ref{sec:som} and~\ref{sec:tom}, respectively. \looseness=-1

\subsubsection{Set-of-Mark for Action Grounding}
\label{sec:som}
SoM prompting~\cite{yang2023set} was first proposed to enhance the grounding capability of GPT-4V and has then been widely adopted for various agentic tasks~\cite{liu2024moka,nasiriany2024pivot,yan2023gpt,huang2024copa,cheng2024spatialrgpt}. Unlike previous works that exploited it for prompting off-the-shelf LMMs to enhance visual-language grounding, here we propose \textit{to train} an agentic model for action grounding, \ie, locating actionable points / regions for a specific task and further predict atomic actions if needed.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/som_fig.png}
    \caption{Set-of-Mark supervisions for action grounding on UI screenshot~(left), robot manipulation~(middle) and human video~(right). All coordinates are normalized by image size~(height, width) and then quantized into 256 bins. Images better viewed by zooming in.}
    \label{fig:som_illustration}
\end{figure}
Given an image observation $I_t\in \mathcal{R}^{H \times W \times 3}$ at timestep $t$, a task $\texttt{task}$ and context $\texttt{ctx}$, we first extract a set of $K$ candidate regions or points that are actionable $\mathcal{P} = \{p_1,...,p_K\}$, where $p_k$ could be a four-dimensional box coordinate or two-dimensional point coordinates. Subsequently, we overlay the marks and boxes (if any) to the corresponding location of the image with numerical labels, \ie, $\mathcal{M} = \{1:p_1,2:p_2,...,K:p_K\}$ giving us a new marked image $I_t^{M}$.

Given the prompted image $I_t^{M}$ in an atomic action step, the model needs to select the candidate marks along with the original coordinates, significantly easing the action grounding for the agentic model. In this way, Eq.~\eqref{equ1:original target} can be reformulated as:
\begin{equation}
    {o}_t^{mark} = \texttt{action}_t:\texttt{mark}_t = \bm{\pi}(\mathcal{I}_t^{M}, \texttt{task}, \texttt{ctx})
    \label{equ2:reformulate with marker}
\end{equation}
where ${o}_t^{mark}$ is a subset of marks $\mathcal{M}$.

In Fig.~\ref{fig:som_illustration}, we show a few instances to demonstrate the SoM-based action grounding in Fig~\ref{fig:teaser}. 
To obtain candidate regions to mark, we can leverage different proposal networks such as image segmentation models~\cite{zou2023segment,kirillov2023segment}, object detection models~\cite{liu2023grounding,li2021grounded}, or domain-specific models~\cite{lu2024omniparser}. Readers refer to Supp. for more details.

\subsubsection{Trace-of-Mark for Action Planning}
\label{sec:tom}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/tom_fig.png}
    \vspace{-0.7cm}
    \caption{Trace-of-Mark supervisions for robot manipulation~(left) and human action~(right). Same coordinate normalization and quantization is used as SoM. Images show the future traces to predict.}
    \label{fig:tom_illustration}
\end{figure*}
Video data contains a lot of information about human actions and activities, which can essentially be leveraged to boost the capability of agentic models. However, due to the absence of action labels, previous methods rarely explore this direction, apart from a few works focused on world model learning~\cite{mendonca2023structured,liu2024world}.
We extend the strategy of ``overlaying marks'' from static images to dynamic videos by proposing Trace-of-Mark~(ToM) to allow the agentic model to effectively learn to plan and act from videos. 

Given the sequence of visual observations from a video $\mathcal{I} = \{I_1,...,I_t\}$, we extend along the time axis to the future $l$ frames, $\mathcal{I}_{future} = \{I_{t+1}, ..., I_{t+l}\}$. Given the $K$ marks at $t$-th frame $I_t$, we extract the corresponding positions of the overlay marks in the next $l$ frames, denoted traces $\mathcal{T} = \{\mathcal{M}_{t+1}, ..., \mathcal{M}_{t+l}\}$. Following the prediction of action type and valid marks as in Eq.~\eqref{equ2:reformulate with marker}, we further ask the model to predict the future trajectories for the valid marks:
\begin{equation}
\begin{aligned}
  {o}_t^{mark} & = \texttt{action}_t:\texttt{mark}_t:\texttt{trace}_{t+1:t+l} \\
& = \pi(\{\mathcal{I}_1,...,\mathcal{I}_{t-1},\mathcal{I}_t^{M}\}, \texttt{task}, \texttt{ctx})    
\end{aligned}
\label{EQ:ToM}
\end{equation}
where $\texttt{trace}_{t+1:t+l}$ is a subset of the trace sequences for valid marks in $\texttt{mark}_t$ in $\mathcal{T}$. Our proposed ToM predicting is a simple yet effective way of leveraging video data and brings two unique modeling benefits: $(i)$ It forces the model to understand the temporal dynamics in the video observations and to ``look ahead of time'' before taking the next actions; $(ii)$ Unlike predicting next frames as used in~\cite{liu2024worldmodelmillionlengthvideo}, predicting traces uses much fewer tokens to capture much longer temporal horizon and action-related object dynamics, while disregarding ambient contents.

To extract ToM, we employ point tracking models CoTracker~\cite{karaev2023cotracker}, though any performant model can be used. In particular, given a sequence of frames $\{I_t,I_{t+1},...,I_{t+l}\} \in \mathcal{R}^{(l+1) \times H\times W \times 3}$, we apply a dense tracking for $s^2$ grid points to extract $s^2$ traces of length $(l+1)$. Given these $s^2$ traces, we drop those traces whose average motion magnitudes between two adjacent timesteps are smaller than a certain value $\epsilon$ (Please see more details in the supplementary material). The remaining ones are regarded as foreground motions driven by a given task.

\subsection{Modeling}

To retain the multimodal understanding capability required for \magma, we adopt the common practice used in current VLMs~(\eg, LLaVA~\cite{liu2023llava} and Phi-3-Vision~\cite{abdin2024phi3}). Given the visual observations $\mathcal{I}$, we use a vision encoder $\mathcal{V}$ to encode each frame into a number of tokens and then concatenate all tokens into a sequence and feed them to a decoder-only LLM along with the language tokens that encode task descriptions. Due to the task diversity, a vision encoder that can seamlessly encode images and videos of various resolutions is needed. In light of this, we propose to use convolutional networks ConvNeXt~\cite{liu2022convnet} as the vision backbone, considering that it supports arbitrary image resolutions by default. To handle the high-resolution images (\eg, up to 2000 for UI screenshots), we simply perform global encoding without the bells and whistles used in previous work and find that it can encode the global context as well as combining global and local crops~\cite{liu2024llavanext,abdin2024phi3}. 
To that end, we formulate the agentic modeling as an autoregressive decoding procedure:
\begin{equation}
    o^{l,*}_{t+1} \sim p(o_{t+1}^l | \{o_1^l,...,o_t^l\};  \mathcal{V}(\mathcal{I}), \texttt{task}, \texttt{ctx}).
\end{equation}

\section{Multimodal Agentic Pretraining}

\subsection{Datasets}
\label{sec:pretraining_data}
To develop a foundation model with both verbal and spatial intelligence that is capable of handling diverse agentic tasks, we curated a comprehensive pretraining dataset from a wide range of images, videos, and robotics domains.

\begin{table}[t]
    \centering
    \begin{tabular}{l|cc}
        Data Type &  Set-of-Mark & Trace-of-Mark\\
        \toprule
        UI Screenshots & \checkmark & \xmark \\
        Robotics Images & \checkmark & \checkmark \\
        Instructional Videos & \checkmark & \checkmark \\
    \end{tabular}
    \vspace{-0.2cm}
    \caption{SoM and ToM applied to various data types. ToM is not applied to UI data as they are a sequence of discrete screenshots.}
    \label{tab:som_tom_generation}
\end{table}

\begin{itemize}
\item \textbf{Robotics manipulation data}. For robotics task, we follow OpenVLA~\cite{kim2024openvla} and use the robotics dataset of Open-X-Embodiment~\cite{embodimentcollaboration2024openxembodimentroboticlearning}.

\item \textbf{UI navigation Data}. We exploit two pretraining datasets, SeeClick~\cite{seeclick} and Vision2UI~\cite{gui2024vision2uirealworlddatasetlayout}. 
\item \textbf{Instructional videos}. We compile Epic-Kitchen~\cite{Damen2018EPICKITCHENS, Damen2022RESCALING}, Ego4d~\cite{grauman2022ego4dworld3000hours}, Somethingv2~\cite{goyal2017something} and other related considering the coarse but rich goal-driven human actions.
\item \textbf{Multimodal understanding}. Lastly, we include ShareGPT4V~\cite{chen2023sharegpt4vimprovinglargemultimodal}, instruction tuning data in LLaVA-1.5~\cite{liu2024llavanext}, and a few other OCR-related datasets~\cite{masry-etal-2022-chartqa,mathew2021infographicvqa} to attain image understanding capability.
\end{itemize}
We noticed that many more related datasets could be used for our model pretraining, such as large-scale instruction tuning data~\cite{tong2024cambrian,li2024llavaonevision}, more diverse video data~\cite{chen2024panda70mcaptioning70mvideos}. In this study, we focus on the demonstration of our pretraining methodology and leave the further scaling up for future. In the next, we elaborate on how we extract the surrogate action supervisions through Set-of-Mark~(SoM) and Trace-of-Mark~(ToM).

\subsection{SoM and ToM Generation}

As shown in Table~\ref{tab:som_tom_generation}, we apply SoM and ToM for different data types, where SoM is applied to all to learn a uinified action grounding. ToM is not fit for the UI data as it consists of sequences of discrete screenshots.

\subsubsection{SoM for UI Navigation}

For UI screenshots in our pretraining data, we mainly rely on the original annotations extracted based on DoM Tree. In addition to the bounding boxes extracted from HTML code~\cite{seeclick,gui2024vision2uirealworlddatasetlayout}, we further annotate the mobile screenshots in SeeClick data with bounding boxes derived from Android view hierarchies~\cite{rico_semantics}. 
Given the extracted candidate bounding boxes for an image, we apply Alg.~\ref{alg:som_prompting} to assign a textual label (line 3) and draw the boxes around the objects. To minimize overlapping box placements, we determine the optimal position for a label using previously drawn boxes (line 5) before computing the textbox size and assigning its coordinates (line 7). During the evaluation, we follow the common practice by applying OmniParser~\cite{lu2024omniparser} for the zero-shot evaluation on ScreenSpot~\cite{seeclick}, and using the candidate boxes provided by~\cite{mind2web} for downstream training and evaluation on Mind2Web.
\begin{algorithm}[t]
\caption{SoM generation for UI images}
\label{alg:som_generation}
\begin{algorithmic}[1]
    \Require image $I$, bounding boxes $B$, image height and width $(i_h, i_w)$
    % \Ensure Updated image with bounding boxes and indices drawn
    \State $B^* \gets []$ 
    \For{$(idx, b) \in \texttt{enumerate}(B)$}  
        \State $text \gets \texttt{str}(idx+1)$
        \State $I \gets \texttt{DrawRectangle}(I, b)$
        \State $(c_y, c_x) \gets \texttt{FindOptimalCorner}(b, B^*, (i_h, i_w))$
        \textcolor{commentcolor}{\Comment{Find corner that is far away from all boxes in $B^*$}}
        \State $(m_h, m_w) \gets \texttt{GetMarkSize}(text, H, W)$        
        \State $text\_box \gets (c_y, c_x, c_y \pm m_h, c_x \pm m_w)$
        \State $I \gets \texttt{DrawRectangle}(I, text\_box)$
        \State $I \gets \texttt{DrawText}(I, (c_x, c_y), text, color=white)$
        
        \State $B^* \gets B^* + [b]$ \textcolor{commentcolor}{\Comment{Add current drawn box to $B^*$}}
    \EndFor
    
    \State \textbf{Return} $I$
\end{algorithmic}
\label{alg:som_prompting}
\end{algorithm}

\subsubsection{SoM and ToM for Videos and Robotic Data}
\label{sec:main_tom}

We use marks and traces as surrogate action supervisions to pretrain our \magma model for action grounding and planning. To extract reliable traces, we use the state-of-the-art point tracking model CoTracker~\cite{karaev2023cotracker} to track the keypoints in each video segment. Unlike object detection and tracking systems used in previous works~\cite{ravi2024sam2segmentimages,niu2024llarva,li2025hamsterhierarchicalactionmodels}, point tracking provides the finest grained moving trajectories for both end effectors~(robot arms or human hands) and objects, and more importantly can be feasibly applied to any videos as it does not require object recognition.

\begin{algorithm}[t]
\caption{SoM and ToM generation for instructional videos and robotic data}
\label{alg:som_tom_generation}
\begin{algorithmic}[1]
    \Require image sequence $\mathcal{I} = \{I_t,...I_{l}\}$; grid size $s$; global motion threshold $\eta$; foreground threshold $\epsilon$
    % \Ensure Updated image with bounding boxes and indices drawn
    \State $\mathcal{M} = \{M_t,...,M_{l}\} \gets \texttt{CoTracker}(\mathcal{I}, s)$
    % \textcolor{commentcolor}{\Comment{Run CoTracker to extract a grid of traces}}
    \If{$\texttt{HasGlobalMotion}(\mathcal{M}, \eta)$}
      \State $\mathcal{M} \gets \mathcal{H}(\mathcal{M})$ \textcolor{commentcolor}{\Comment{Apply homography transformation}}
    \EndIf
    \State $\mathcal{M}^f, \mathcal{M}^b = \texttt{ClassifyTraces}(\mathcal{M}, \epsilon)$\textcolor{commentcolor}{\Comment{Classify traces into foreground and background ones}}
    \State $k \gets \texttt{Random}(1, \min(5, |\mathcal{M}^f|))$ 
    \State $\mathcal{M}^f,\mathcal{M}^b = \texttt{KMeans}(\mathcal{M}^f, k), \texttt{KMeans}(\mathcal{M}^b, 2k)$    
    \textcolor{commentcolor}{\Comment{Cluster foreground and background traces separately}}
    \State $I_t \gets SoM(I_t, \{M^f_t, M^b_t\})$ \textcolor{commentcolor}{\Comment{Apply SoM on 1st frame}}
    \State \textbf{Return} $\mathcal{I}, \mathcal{M}_f^*$
\end{algorithmic}
\label{alg:tom_prompting}
\end{algorithm}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/tom_fig5.png}
    \vspace{-10pt}
    \caption{An illustration of Alg.~\ref{alg:tom_prompting} to handle videos with camera motions for SoM/ToM generation.}
    \label{fig:homo}
\end{figure}
\begin{figure*}[!t]
    \centering
    % Use a placeholder image, such as a blank box with dimensions
    % {\includegraphics[width=\textwidth]{figures/images/magma_figure_data.pdf}} % Replace 'example-image' with your image or remove it to just show a box
    \begin{minipage}{0.29\textwidth}
        \centering
        %\includegraphics[width=\textwidth]{figures/images/magma_data_sizes_v1.pdf}
        \includegraphics[width=\textwidth]{figures/images/magma_figure_data_sunburst_v5.pdf} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.7\textwidth}
        \centering
        %\includegraphics[width=\textwidth]{figures/images/magma_figure_data_sample.pdf}
        \includegraphics[width=\textwidth]{figures/images/magma_figure_data_sample_v2.pdf}
    \end{minipage}
    \caption{\textbf{Overview of Pretraining Data Sources.} A diverse collection of datasets including instructional videos (\textcolor[RGB]{242,150,75}{orange}), robotics manipulation (\textcolor[RGB]{102,188,156}{green}), UI navigation (\textcolor[RGB]{235,150,170}{pink}), and multimodal understanding (\textcolor[RGB]{70,130,180}{blue}). Note that we count the size of each dataset by the number of image samples. For video and robotics data, we extract the images from the short clips and trajectories, respectively.}
    \label{fig:dataset_vis}
\end{figure*}

\noindent\textbf{Reliability of CoTracker}. To determine the generalizability of such traces, we examine the reliability of CoTracker before running the algorithm on all our pretraining data. We note that CoTracker was already well validated on multiple video datasets such as TAP-Vid~\cite{doersch2022tap} and PointOdyssey~\cite{zheng2023pointodyssey} in the original paper. In this work, we proposed comprehensive strategies to handle scene transition and camera motions in videos~(Alg.~\ref{alg:tom_prompting}), which effectively scale to datasets like Ego4D and other instructional videos~(Fig~\ref{fig:data_dist}). To further validate the reliability of ToM, we quantitatively evaluated the traces on a subset of YouCook2-BB~\cite{ZhLoCoBMVC18} with box annotations by humans. We extract the traces from each annotated box and count the number of future traces still falling into the box 1 second forward. On 1320 clips, we got a precision of \textbf{0.89}, indicating that the traces reliably capture temporal motions. 

\noindent\textbf{Segment and CLIP-score filtering} As the point tracking system works in a short time window, we begin by using the annotations provided, curated or otherwise, to split each video into segments, and then run PySceneDetect~\cite{PySceneDetect} to further break each segment into short video clips with consistent shots. However, the detected video clips may not always be relevant to their associated text annotations. Thus, we use the pretrained CLIP~\cite{radford2021learning} visual and text encoders to compute the cosine similarity score between each clip and text pair, and filter out clips with $<0.25$ scores.



Once we have the fine-grained video clips in hand, we apply Alg.~\ref{alg:tom_prompting} to generate SoM and ToM. Given a video clip with $l$ frames $\{I_1,I_{2},...,I_{l}\} \in \mathcal{R}^{(l) \times H\times W \times 3}$, we start from the time step $t$  and put a grid of equally spaced $s^2$ points on $I_t$. Then, we use CoTracker to extract $s^2$ future traces of length $(l-t)$ each. The output also contains predicted occlusion labels for each trace, which indicate if any points on the trace are obstructed at some time steps. 

\noindent\textbf{Removal of global motions}. Many instructional videos, particularly the ego-centric ones~\cite{grauman2022ego4dworld3000hours}, contain significant camera movements. Consequently, the extracted traces may reflect external movements instead of relevant actions to accomplish a given task. We mitigate this issue by performing the homography transformation~\cite{dubrofsky2009homography}. Specifically, we compute the $3\times 3$ transformation matrix $h_i$ with the future mark positions and current ones:
\begin{equation}
    h_i = \mathcal{H}({M}_t, {M}_{t+i}) \in \mathcal{R}^{3\times 3}
\end{equation}
Given $h_i$, we apply the homography transformation to $M_{t+i}$ to obtain $M^*_{t+i}$ which shares the same coordinate system as $M_{t}$. Valid traces of marks to predict in Eq.~\eqref{EQ:ToM} are then extracted from $\{M_{t}, M^*_{t+1}, M^*_{t+l}\}$. It turns out that the proposed method is effective to remove global camera motions for both ego-centric videos and exo-centric ones, as ilustrated in Fig.~\ref{fig:homo}.

After extracting the traces and applying the homography transformation if needed (lines 2-4), we classify them into two categories, foreground and background traces based on the average motion magnitude between two adjacent time steps, where traces with average motion magnitude of at least $\epsilon$ (line 5) are counted as foreground. Finally, we select the number of clusters (line 6) and perform a K-Means clustering for the foreground and background traces separately~(line 7) before randomly selecting one or more points from each cluster as the final traces. In practice, we set $s$, $\eta$ and $\epsilon$ to be 15, 2 and 2, respectively.

\subsection{Pretraining}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/magma_pt_v3.png}
    \vspace{-0.5cm}
    \caption{Magma pretraining pipeline. For all training data, texts are tokenized into tokens, while images and videos from different domains are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in verbal, spatial and action types. Our proposed method reconcile the multimodal understanding and action prediction tasks.}
    \label{fig:magma_pt}
\end{figure*}

The above data and annotation curation results in a comprehensive pretraining suite which covers $(i)$ different digital and physical environments; $(ii)$ both verbal and spatial annotations and $(iii)$ various multimodal understanding and agentic tasks. As seen in Fig.~\ref{fig:dataset_vis}~(left), we include close to 2.7M UI navigation screenshots from SeeClick~\cite{seeclick} and Vision2UI~\cite{gui2024vision2uirealworlddatasetlayout}. We follow OpenVLA~\cite{kim2024openvla} to incorporate 970K trajectories in Open-X-Embodiment~\cite{open_x_embodiment_rt_x_2023}, which consists of 9.4M image-
language-action triplets. Another majority of the pretraining data are videos which comprise over 25M samples sourced from around 4M shot-consistent video clips. 
Finally, we include 1.2M image and text pairs from ShareGPT4V~\cite{chen2023sharegpt4v}, and LLaVa-1.5~\cite{liu2024improvedbaselinesvisualinstruction} and a few other OCR-related datasets~\cite{masry-etal-2022-chartqa,mathew2021infographicvqa}, which we denote by \textit{ Magma-SFT}~(820K). 

By default, we use LLaMA-3-8B~\cite{llama3} as the language backbone and ConvNext-XXlarge~\cite{liu2022convnet} as the vision backbone. We show the pretraining architecture in Fig.~\ref{fig:magma_pt}. Our proposed SoM and ToM play as the bridge to connect verbal and action supervisions for all four types of data, and significantly enhance model's spatial intelligence as we observe during our experiments.

For comparisons, we run a few variants for the ablation studies in our experiments:
\begin{itemize}
    \item {\magma-8B~(SFT)} is the model trained with \textit{Magma-SFT}~(820K) for the instruction tuning following a conventional recipe used on LMM training. 
    \item {\magma-8B~(UI)} and {\magma-8B~(OXE)} are the models pretrained on UI screenshots and OXE robotics data, respectively.
    \item {\magma-8B~(ACT)} is pretrained jointly on UI screenshots and robotics data.
    \item {\magma-8B~(Full)} is the full model trained with the whole dataset with SoM and ToM annotations.
\end{itemize}

Unless noted otherwise, all pretrainng includes the \textit{Magma-SFT}~(820K). We pretrain our model using our curated data for maximally three epochs with a constant learning rate of 1e-5, and evaluate the pretrained model on different tasks under the zero-shot setting as well as finetune its weights on the downstream tasks. The entire model including the parameters of the language model and the vision encoder is tuned. See Appendix for more detailed settings.