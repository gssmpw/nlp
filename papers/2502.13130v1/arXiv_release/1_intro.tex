\section{Introduction}
\label{sec:intro}
A long-standing research topic of AI is to develop autonomous agents that can perceive visual stimuli, language inputs, and other environmentally-grounded data and produce meaningful embodied actions in physical and digital environments to complete specific tasks.

Recently, there has been a growing interest in developing AI agents based on Vision-Language-Action (VLA) models~\cite{kim2024openvla,driess2023palm, brohan2022rt,brohan2023rt,hong2023cogagent,seeclick}. These models are typically pretrained on large amounts of vision-language datasets and then action trajectories to attain ability to take actions given VL inputs. However, due to the inherent difference between various environments~(\eg, 2D digital world and 3D physical ones), VLA models are typically trained separately for simplicity and then used for different tasks. Exemplary models in the digital world include Pix2ACT~\cite{shaw2023pixels}, WebGUM~\cite{furuta2023multimodal}, and Ferret-UI~\cite{you2023ferret} for UI navitation. VLA models in the 3D physical world include RT-2~\cite{brohan2022rt} and OpenVLA~\cite{kim2024openvla} for robotics manipulation.
Although claimed as generalist, most of these models prioritize learning a task-specific action policy at the cost of a significant decline in generic multimodal understanding capabilities, rendering limited genralizability across tasks and domains. 

In this research, we strive to develop a foundation model for multimodal AI agents and argue that it requires simultaneously possessing the following capabilities:
\begin{itemize}
    \item \textbf{Multimodal Understanding} to understand multimodal input from various domains (both digital and physical) not only semantically, but also spatially and temporally.
    \item \textbf{Multimodal Action Prediction} to break down the long-horizon task into an accurate action sequence, which can be effectively executed by AI agent systems.
\end{itemize}
Such an agent system should be driven by external goals specified by human commands as shown in Fig.~\ref{fig:intro}.

To endow the broad capabilities, we effectively leverage large amounts of heterogeneous vision-language and action datasets, including UI datasets such as SeekClick~\cite{seeclick}, robotic manipulation dataset OXE~\cite{open_x_embodiment_rt_x_2023}, human instructional videos like Ego-4d~\cite{grauman2022ego4dworld3000hours} and image-text pairs used in LMMs~\cite{liu2023llava,chen2023sharegpt4v}. Instead of sequentially training on one domain and adapting to another, we train a \textit{single} foundation model which can be applied in a zero-shot manner to different downstream tasks in various settings.

Simply combining those datasets, however, does not bring benefits to the foundation model, due to the significant gap between multimodal understanding which is mostly verbal (\ie, textual descriptions for images and videos) and the action-taking tasks which are mostly spatial~(\ie, 2D coordinates for UI or 7-DoF for robot arm). To bridge the gap, we propose two surrogate tasks for model training, action grounding and action planning, by asking the model to predict the proximal action outputs given the visual-spatial observations, represented as images or video frames. 
Specifically, in each image, we label the visual objects that are actionable by \textbf{Set-of-Mark (SoM)} (e.g., clickable buttons in Fig.~\ref{fig:teaser} bottom-middle) and labeled in each video the object movements, which are the results of actions, with \textbf{Trace-of-Mark (ToM)} (e.g., the trace of human hand or robotic arm in Fig.~\ref{fig:teaser} top-middle). In this way, the image and video datasets, which are not labeled with actions, are transformed into ``vision-language-action'' data to morph the gap among different types of tasks. 
We show through extensive empirical studies that SoM and ToM achieve are environment-agnostic and easy to generalize to new agentic tasks, offering an effective and efficient approach to scaling up our \magma model pretraining using large amounts of unlabeled videos, such as raw instructional videos.

To the best of our knowledge, \magma is the first foundation model for multimodal AI agents that can understand multimodal inputs~(see Fig.~\ref{fig:teaser}~left), perform action grounding and planning for the future~(see Fig.~\ref{fig:teaser}~middle), and finally adapt to downstream (unseen) agentic tasks in both the digital and physical environments(see Fig.~\ref{fig:teaser}~right). We evaluated Magma on three task categories: UI navigation (e.g., Mind2Web, AITW), where it has to reason and act in evolving digital environments; vision-language understanding (e.g., GQA, VideoMME), where it grounds language in visual objects and events; and finally robotic manipulation (e.g., Bridge, LIBERO), which tests its 3D spatial intelligence for physical interaction. \magma achieves new SOTA results on UI navigation and robotic manipulation tasks, outperforming even domain-specific models while maintaining strong performance on VL tasks which are comparable to SOTA LMMs. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro_fig.png}
    \caption{A multimodal AI agent should be capable of mutimodal understanding and action-prediction towards a given goal.}
    \label{fig:intro}
    \vspace{-0.5cm}
\end{figure}
In summary, the main contributions of this work are:
\begin{itemize}
    \item We propose \magma, the first foundation model that acquires not only multimodal understanding but also spatial-temporal reasoning abilities for agentic tasks in both digial and physical environments. 
    \item We propose the use of Set-of-Mark and Trace-of-Mark techniques to significantly enhance the spatial-temporal intelligence for action grounding and planning, and allow \magma to be pretrained effectively on large amounts of heterogeneous datasets. %significantly mitigate the gap for pretraining on heterogeneous data.
    \item We curate a large-scale pretraining dataset, which consists of not only open-source VL datasets, but also UI, robotics data and human instructional videos, auto-labeled using SoM and ToM. In total, our training corpus contains approximately 39 million diverse samples.
    \item We extensively evaluate the pretrained \magma model to demonstrate the superior model performance across a wide range of tasks. \magma with a single suite of parameters achieves new SOTA on both robotic manipulation and UI navigation over open-sourced counterparts.    
    \item We show that the proposed \magma pretraining method significantly improves model's verbal and spatial-temporal intelligence abilities. For instance, \magma can achieve SOTA performance on the BLINK dataset without instruction fine-tuning, and SOTA performance on video question-answering benchmarks despite being pretrained on much fewer frames. 
    
\end{itemize}