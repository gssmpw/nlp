\clearpage
\appendix
\definecolor{custom_blue}{RGB}{235,244,253}
\tcbset{
  aibox/.style={
    width=\textwidth,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    center,
  }
}

\newtcolorbox{graybox}[1][]{colback=lightgray!20, colframe=lightgray!50!black, boxrule=0pt, enhanced, #1}
\newtcolorbox{redbox}[1][]{colback=red!10, colframe=red!50!black, boxrule=0pt, enhanced, #1}
\newtcolorbox{greenbox}[1][]{colback=green!10, colframe=green!50!black, boxrule=0pt, enhanced, #1}
\newtcolorbox{magentabox}[1][]{colback=magenta!10, colframe=magenta!50!black, boxrule=0pt, enhanced, #1}
\definecolor{lightgreen}{RGB}{144, 238, 144} % RGB values for light green

\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}
\tcbset{
  aiboxsmall/.style={
    width=0.62\textwidth,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    top,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}   
\definecolor{commentcolor}{RGB}{34,139,34} 
\newcommand{\myalgorithm}{
\begingroup
\removelatexerror
\begin{algorithm*}[H]
      \caption{\modelname PyTorch pseudocode.}
      \label{alg:pseudocode}
      \scriptsize
          \Comment{
          $\mathbf{H}_0$: Input embeddings for LLM (Original inputs args for traditional LMM); \\
          $vis\_pos$: the location of visual tokens; \\
          $\mathbf{X}$, $\mathbf{X^{stack}}$: Original visual tokens, Extra high-resolution visual token list; \\
          $l_{start}$, $n$: Index of starting layer, and layer interval for stacking.
          }
  \Function{forward($\mathbf{H}_0$, $\mathbf{X^{stack}}$, $l_{start}$, $n$, $vis\_pos$)}{
     \var{$\mathbf{H}$ = $\mathbf{H}_0$}
     
     \For{($idx$, \var{TransformerLayer)} in enumerate(\var{self.layers})}{
        \Comment{\modelname:}
        \If{$idx$ >= $l_{start}$ \& $(idx - l_{start}) \% n == 0$}
        {
            $\mathbf{H}[vis\_pos]$ += $\mathbf{X^{stack}}[(idx - l_{start})//n]$
        }

        \Comment{Original Transformer:}
        $\mathbf{H}$ = \var{TransformerLayer}($\mathbf{H}$)
     }
  }
\end{algorithm*}
\endgroup}

\maketitlesupplementary

\section{Pretraining and Finetuning}

\begin{table}[h]
    \centering
    \resizebox{0.99\linewidth}{!}{
    \footnotesize
    \begin{tabular}{l | c c c c}
        Setting & Pretraining & \multicolumn{3}{c}{Finetuning}
         \\
        & & UI & Image/Video & Real Robot\\
        \midrule
        batch size          &  1024 & 32   \\
        base learning rate  &  1e-5 & 1e-5  & 1e-5 & 1e-5 \\
        learning rate scheduler & Constant & Cosine & Cosine & Constant \\
        training epochs     &  3  & 3 & 1 & 20 \\
        optimizer          & adamw   & adamw & adamw   & adamw 
        \\
        \midrule
        Image Resolution & 512 & 768 & 768 & 256 \\
        Number of Crops & 4 or 1  & 4 & 4 or 1 & 1\\
        
    \end{tabular}}
    \caption{Experimental settings pretraining and finetuning of \magma models. We maximally use either 32 Nvidia H100s or 64 AMD MI300 GPUs for all training jobs.}
    \label{tab:settings_pt_ft}
\end{table}

For all the model variants, we use the same training recipe as shown in Table~\ref{tab:settings_pt_ft}. To handle different image resolutions from different datasets, we also use a multi-crop strategy to enable batch forward for a given minibatch, though the ConvNext vision backbone can naturally support arbitrary resolutions. Specifically, for our pretraining, we use 512 as the base image size, and resize an input image maximally to 4 crops for UI and image pretraining data, while use 1 crop for video and robotics data.

For downstream finetuning, we following common practice to tune the pretrained magma model as shown in Table~\ref{tab:settings_pt_ft} right. As mentioned above, the vision encoder can be effortlessly adapted to different image resolutions required for different tasks.
\section{Datasets}
\label{sec:supp_pretraining_data}

\begin{table}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{lcc}
     Source & Task & Size \\
     \toprule
     \multirow{4}{*}{SeeClick-Web} & text\_2\_point  & 271K \\
     & text\_2\_bbox & 54K \\
     & point\_2\_text & 54K \\
     & bbox\_2\_text & 54K \\
      \midrule
     \multirow{4}{*}{SeeClick-Mobile} & text\_2\_point  & 274K \\
     & text\_2\_bbox & 56K \\
     & UI summarization & 48K \\
     & widget captioning & 42K \\
      \midrule
     \multirow{4}{*}{Visison2UI} 
     & input\_2\_point & 980K\\
     & input\_2\_bbox & 982K \\
     & text\_2\_point  & \textcolor{Gray}{794K} \\
     & text\_2\_bbox & \textcolor{Gray}{774K} \\
     & point\_2\_text & \textcolor{Gray}{199K} \\
     & bbox\_2\_text & \textcolor{Gray}{193K} \\
     \bottomrule
     \rowcolor{custom_blue}  Magma-PT-UI~(Ours) & Mixed & 2.8M \\
    \end{tabular}}
    \vspace{-5pt}
    \caption{Statistics of UI related pretraining data.}
    \label{tab:ui_pretrain}
\end{table}

\subsection{Pretraining Data}
Due to space constraints, we briefly introduced the datasets for our pretraining in Sec~4.1 of our main submission. To ensure the reproducibility of our pretraining stage, we provide additional details of our pretraining data below.

\begin{figure*}[t]
    \centering
    \includegraphics[width=2.\columnwidth]{figures/images/magma_ui_data_example.pdf}
    \vspace{-5pt}
    \caption{\textbf{Training samples in our Magma-PT-UI.} It covers a wide range of action grounding and UI understanding tasks including: (a) Given the bounding box or point coordinates as the query, assistant should return the natural language description or the content. (b) Given the natural language or the exact content as the query, assistant should return the value of the bounding box coordinates.. (c) Given the natural language as the query, assistant should return the value of the point coordinate. (d) Widget captioning. (e) UI summarization.}
    % 
    \label{fig:magma_pt_ui}
\vspace{-10pt}
\end{figure*}

\subsubsection{UI Navigation} 
Our pretraining data related to UI agent are sourced from two datasets, SeeClick~\cite{seeclick} and Vision2UI~\cite{gui2024vision2uirealworlddatasetlayout}. We further process these source data by adding marks on screenshots to provide grounded supervisions.

\noindent\textbf{SeeClick}. We generally follow the original procedure and make the following modifications to associate with the Set of Mark~\cite{yang2023set} strategy.
For each webpage screenshot, multiple (text, bounding\_box) pairs are available. Therefore, we directly overlay all the bounding boxes with corresponding marks on the screenshot.
For each mobile screenshot, only a single (text, bounding\_box) pair is available in the SeeClick data. To enrich the pairs, we incorporate additional pairs from the RICO dataset~\cite{deka2017rico}, and employ an OCR tool to obtain text boxes. Finally, we display the enriched bounding boxes along with their corresponding marks on the mobile screenshot.


\noindent\textbf{Vision2UI}. We consider all bounding boxes whose ``content'' property is not null. To prevent the marks from overwhelming the main content of the webpage, we sample bounding boxes with varying probabilities based on their "type" property. Specifically, we assign a sampling weight of 0.5 to boxes of type \texttt{h1}, \texttt{h2}, \texttt{a}, \texttt{button}, \texttt{option}, and \texttt{nav} with 0.5, while other types are weighted at 0.1. Given the high importance of input areas for interaction, we include boxes of type \texttt{input} directly without sampling for mark plotting. After obtaining the elements of high interest, we apply similar tasks as SeeClick~\cite{seeclick} to produce the instruction data, including (a) grounding task, which involves two forms: predicting center point coordinates (text\_2\_point) and predicting bounding box (text\_2\_bbox); (b) generating text for elements, categorized into predicting text based on the coordinates of center points (point\_2\_text) or bounding boxes (bbox\_2\_text); and further introduce the task of (C) locating input fields, including predicting center point coordinates (input\_2\_point) and bounding box coordinates (input\_2\_bbox) of the input fields.

Given a webpage, since the first two categories of tasks are grounding or generating texts for the same group of web elements, we further weight the four subtasks, \textit{i.e.}, (text\_2\_point), (text\_2\_bbox), (point\_2\_text), and (bbox\_2\_text) with [0.4, 0.4, 0.1, 0.1], and sample only one of them to construct the pretraining data. Similarly, we sample one subtask from (input\_2\_point) and (input\_2\_bbox) with equal probabilities. We merge the sampled subtasks from the same webpage into one example to improve training efficiency. We denote the full pretraining data related to UI by Magma-PT-UI, and list the sizes of individual subsets in Table~\ref{tab:ui_pretrain}.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/data_dist.pdf}    
    \hfill
    
    \caption{\textbf{Action distributions in three types of action-oriented pretraining datasets}. (a) UI Navigation; (b) Robotic Manipulation; (c) Instructional Videos.}
    \label{fig:data_dist}
\end{figure*}
\subsubsection{Instructional Videos} 
As mentioned in the main submission, we curate the supervisions from human instructional videos to learn the agentic capability for our model. To cover different scenarios, we considered both 3rd point view videos and egocentric videos. In particular, we start with Epic-Kitchen~\cite{Damen2018EPICKITCHENS} video data sets considering that their text annotations are relatively high quality. Afterwards, we expand to Something-Soomething v2~\cite{materzynska2020something} to include more human-object interactions, and Ego4D~\cite{grauman2022ego4dworld3000hours} and other related instructional videos for scaling up.

\noindent \textbf{Epic-Kitchen}~\cite{Damen2018EPICKITCHENS}. Epic-Kitchen contains 495 egocentric videos recorded by 32 participants in kitchen rooms. Each video contains a number of segments labeled with narrations, start and end frame ids. However, the original video narrations (\eg, ``open door'') are too coarse to depict the human actions in a certain time frame.

For the videos in Epic-Kitchen, we apply the video preprocessing method as discussed in Sec~4.2 of our main submission. Concretely, for each of the original video segments in the dataset, we run PySceneDetect to detect the temporal boundaries and split them into sub-segments. During our model pretraining, the textual annotations are used in two ways. Our model is asked to predict the detailed description in the first frame.  In addition, they are used as the task description as input to the model for predicting the traces of marks.

\noindent \textbf{Sth-Sth-v2}~\cite{materzynska2020something}, \textbf{Ego4D}~\cite{grauman2022ego4dworld3000hours}. 
The Sth-Sth v2 dataset is a comprehensive collection of labeled video clips featuring humans performing predefined actions with everyday objects. The list of action classes spans a wide variety of atomic actions, including but not limited to ``pushing something from right to left'', ``throwing something'' and ``covering something with something''. In total, the dataset contains 220,847 seconds-long video clips. To create our pretraining data, we only leverage the videos in the train and validation splits. This amounts to around 160K video clips. We note that we do not use PySceneDetect for Sth-Sth v2 since the original video clips have been highly curated.

The Ego4D dataset is a large-scale egocentric dataset that contains approximately 3,025 hours of videos. It comprises over 3,670 hours of video footage captured from wearable cameras across a diverse environments and activities. The dataset spans a wide range of real-world scenarios, including daily activities and social interactions. Given the duration of these videos can span over 30 minutes, we leverage the original dense caption annotations that are provided to split each videos into seconds-long segments with consistent views.

\subsubsection{Robotic Manipulation} 
\begin{figure*}
    \includegraphics[width=1.0\linewidth]{figures/images/magma_real_description.pdf}
    \vspace{-1.5em}
    \caption{\textbf{Real robot setup.} Magma is deployed on a WidowX 250 robot arm to perform a sequence of kitchen manipulation tasks including object pick-place and soft manipulation.}
    \label{fig:supp_real}
\vspace{-3pt}
\end{figure*}
We follow the training recipe in OpenVLA~\cite{kim2024openvla} to prepare our pretraining data for robotics manipulation. Specifically, we take the data mixture ``siglip-224px+mx-oxe-magic-soup'' as in OpenVLA, which gives us 9.4M image-language-action triplets, extracted from 326K trajectories, from 23 separate datasets. 

\subsubsection{Multimodal Image Understanding} 
We simply include the 1.2M synthetic image-text pairs in ShareGPT4V~\cite{chen2023sharegpt4v} and 665K image instruction tuning data collected by LLaVA-1.5~\cite{liu2024llavanext} as our multimodal image pretraining data. The former helps our pretrained model to have a global understanding of visual contents, while the latter helps to get the model familiar with various types of human instructions. We denote this dataset by Magma-PT-Image.

\subsubsection{Data Statistics}
Given our goal of training a general vision-language-action foundation model, we analyze the distribution of verbs present in the text annotations of the UI and robotic manipulation as well as instructional video datasets in Figure~\ref{fig:data_dist}. We see that the text annotations in the UI navigation component contain many helpful verbs that help guide agents to achieve a specific task such as ``locate'' and ``turn''. This is complemented by the more action-oriented words in the vocabulary of the robot manipulation component, including ``pick'', ``push'' and ``slide''. Such annotations are especially valuable in helping our \magma model to learn to reason about interactions with everyday objects. Finally, we also scale up the amount of training data and diversity of verbs by including data from instructional videos (Figure~\ref{fig:data_dist}c). As evidenced by the relatively high frequency of words such as ``lifting'' and ``throwing'', such annotations can be very beneficial for gaining a stronger understanding the of temporal dynamics involved in common activities. More importantly, the diversity of activities present in these datasets can be effective at helping the model generalize better to a larger variety of tasks.
\subsection{Downstream Data}

\subsubsection{UI Agent Navigation}

We evaluated the UI grounding and navigation capability mainly on three datasets, ScreenSpot~\cite{seeclick}, Mind2Web~\cite{mind2web} and AITW~\cite{aitw}.

\noindent \textbf{ScreenSpot} is a benchmark used to evaluate the UI action grounding proposed in~\cite{seeclick}. It consists of 600 screenshots images associated with 1.2K instructions spanning iOS, Android, macOS, Windows,
 and web pages. The evalaution covers both text
based elements and a variety of widgets and icons. To evaluate the zero-shot action grounding performance for our model, we use OmniParser~\cite{lu2024omniparser} to help parse the screenshot and propose actionable regions/icons/buttons. We used the sample code and default settings provided in the official repo. For these candidate regions, we overlay numeric marks and ask our model to pick one.


\noindent\textbf{Mind2Web} is first proposed in~\cite{mind2web} for text-based web agent. For fair comparison among vision-based web agent, we follow the protocol proposed in SeeClick~\cite{seeclick}. Given a webpage, we convert it into a screenshot associated with ground-truth bounding boxes to which the actions should be applied. As the original screenshot of the full website is usually out of the scope of display. We follow a similar way as in~\cite{seeclick} to crop the region of interests centering around the ground truth boxes, which gives us a local screenshot as wide as original webpage but with maximal height 1344. To propose the candidate marks for our model, we directly exploit the candidate ranks provided in Mind2Web, and use the top 30 candidates for evaluation.

\noindent \textbf{AITW} is a dataset originally collected in~\cite{aitw} for navigation of the android UI. The original dataset contains up to 715K trajectories, resulting in 5.7M screenshots. In our experiments, to examine the efficient finetuning performance, we alteratively follow the same protocol in SeeClick~\cite{seeclick} and include a much smaller number of training samples. Specifically, there are 545, 688, 306, 700, 700 instructions from General/Install/GoogleApps/Single/WebShopping, respectively. $80\%$ of each split is used for training and the remainder is used for evaluation. Instead of finetuning our model for each category, we jointly finetune our pretrained \magma on the combined data and evaluate across all categories using a single model.

\subsubsection{Robot Manipulation}

\noindent\textbf{Simulator}. We employ SimplerEnv~\cite{li24simpler} as the main testbed for our learned robot policy. As we do not need to tune our model on the simulated trajectories, we simply report the numbers following the protocol proposed in the original work.

\noindent\textbf{Real-world Setting}. We design four tabletop manipulation tasks for our physical WidowX-250 robot setup as shown in \ref{fig:supp_real}. As with BridgeData-v2, the RGB image observations from the robot are captured using a stationary third-person camera, maintaining a resolution of $256 \times 256$. For finetuning our pretrained \magma model, we collect approximately 50 robot demonstration trajectories for each task as our finetuning dataset. Our experimental design includes classic soft object manipulation and pick-and-place operations tasks. Detailed language instructions for the designed tasks are presented below. For each trial, we randomize the initial location of the target object and include 2-3 random distracting objects (e.g., corn, eggplant) in the scene. For reproducibility, we release the collected robot trajectories.

\noindent Tasks included in the finetuning dataset:
\begin{itemize}
\item \textbf{Hot dog assembly}: Pick up the hot dog sausage from the desk and place it into the bun. The trial is counted as success only when the robot successfully grasps the sausage and accurately places it within the hot dog bun.

\item \textbf{Mushroom placement}: Pick up the mushroom and place it into the pot. The trial is counted as success only when the robot correctly grasps the mushroom and places it into the cooking pot without dropping or misaligning it.

\item \textbf{Cloth pushing}: Push the cloth from right to left across the surface. The trial is counted as success only when the robot successfully manipulates the cloth in the specified direction without disturbing other objects on the surface.
\end{itemize}

\noindent Unseen task for evaluating generalization:
\begin{itemize}
\item \textbf{Bidirectional cloth manipulation}: Push the cloth in both directions while maintaining its shape. This task examines the model's spatial understanding and reasoning capabilities, as it requires generalization from unidirectional pushing in the training data to bidirectional manipulation in novel scenarios.
\end{itemize}

\subsubsection{Image Instruction Tuning} 

We show a breakdown of our 820k Magma image instruction tuning data in Table~\ref{tab:magma_820k}. As the 760k image instruction tuning data used in LLaVA-1.6~\cite{liu2024llavanext} is not released, we follow their guidance to curate 748k public available data including ShareGPT~\cite{sharegpt}, LLaVA-Instruct~\cite{liu2023llava}, ShareGPT4V~\cite{chen2023sharegpt4v}, LAION-GPT4V~\cite{laion4v}, VQAv2~\cite{goyal2017making}, GQA~\cite{hudson2019gqa}, OKVQA~\cite{marino2019ok}, OCRVQA~\cite{mishra2019ocr}, ChartQA~\cite{masry2022chartqabenchmarkquestionanswering}, DVQA~\cite{kafle2018dvqa}, DocVQA~\cite{mathew2021docvqa}, AI2D~\cite{ai2d}, SynthDog-EN~\cite{kim2022ocr}, A-OKVQA~\cite{schwenk2022okvqa}, RefCOCO~\cite{kazemzadeh2014referitgame} and VG~\cite{krishna2017visual}. To complement the claimed ``improved reasoning, OCR and world knowledge'', we resort to a few other open-sourced datasets including InfoGraphicsVQA~\cite{mathew2021infographicvqa}, augmented ChartQA~\cite{masry2022chartqabenchmarkquestionanswering}, FigureQA~\cite{kahou2018figureqaannotatedfiguredataset}, TQA~\cite{tqa2017} and ScienceQA~\cite{lu2022learnscienceqa}. We denote the full set by Magma-SFT-Image.

\subsubsection{Video Instruction Tuning}
For comparisons with state-of-the-art video LMMs, we adopt the LLava-Video-178K dataset~\cite{zhang2024llavanextvideo} for instruction tuning. It consists of approximately 1.6M video and text instruction samples from 178K videos. The dataset is compiled from multiple video sources ranging from Charades~\cite{sigurdsson2016hollywood}, Sth-SthV2~\cite{materzynska2020something} to Kinetics-700~\cite{carreira2019short}. We refer interested readers to the original papers for more details.

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|cc}
     Dataset & Size & Domain \\
     \toprule
     ShareGPT~\cite{sharegpt}   & 40K  & Text \\
     ShareGPT4V~\cite{chen2023sharegpt4v} & 39K & General \\
     LLaVA-Instruct~\cite{liu2023llava} & 158K & General \\ 
     LAION-GPT4V~\cite{laion4v} & 11K & General \\     
     \hline
     VQAv2~\cite{goyal2017vqav2} & 83K & General VQA \\
     GQA~\cite{hudson2019gqa} & 72K & General VQA \\
     OKVQA~\cite{schwenk2022okvqa} & 9K & Knowledge VQA \\
     OCRVQA~\cite{mishra2019ocr} & 80K & OCR VQA \\
     ChartQA~\cite{masry2022chartqabenchmarkquestionanswering} & 7K & Chart VQA \\
     DVQA~\cite{kafle2018dvqa} & 16K & Chart VQA \\
     DocVQA~\cite{mathew2021docvqa} & 10K & Document VQA \\
     AI2D~\cite{ai2d} & 2K & Infographic VQA \\
     SynthDog-EN~\cite{kim2022ocr} & 20K & Document Understanding\\
     A-OKVQA & 66K & Knowledge VQA \\
     RefCOCO~\cite{yu2016modeling-refcoco} & 48K & Grounding Desc. \\
     VG~\cite{krishna2017visual} & 86K & Referring Exp. \\
      \midrule
     InfographicsVQA~\cite{mathew2021infographicvqa}    &  24k & Infographic VQA \\
     ChartQA~(Aug)~\cite{masry2022chartqabenchmarkquestionanswering} & 20k & Chart VQA \\
     FigureQA~\cite{kahou2018figureqaannotatedfiguredataset} & 20k & Chart/Figure VQA \\
     TQA~\cite{tqa2017} & 1.5k & Textbook VQA \\
     ScienceQA~\cite{lu2022learnscienceqa} & 5k & Textbook VQA \\
     \bottomrule
     \rowcolor{custom_blue} Magma-SFT-Image~(Ours)  & 820k & Mixed \\
    \end{tabular}}
    \vspace{-5pt}
    \caption{A detailed breakdown of our 820k Magma image instruction tuning data used in our multimodal image understanding experiments shown in Table~5 in our main submission.}
    \label{tab:magma_820k}
\end{table}

\subsubsection{Details about SoM for training and evaluation}

we exploit three ways to extract the candidate bounding boxes for the SoM prompt: 
\begin{itemize}
    \item \textbf{DoM Tree}. In addition to the bounding boxes extracted from HTML code~\cite{seeclick,gui2024vision2uirealworlddatasetlayout}, we further annotate the mobile screenshots in SeeClick data with bounding boxes derived from Android view hierarchies~\cite{rico_semantics}. These annotations are used during our model pretraining.
    \item \textbf{Vision model}. For zero-shot evaluation on Screenspot~\cite{seeclick}, we exploit the OmniParser model~\cite{lu2024omniparser} to make a fair comparison with the state-of-the-art methods~\cite{lu2024omniparser,seeclick}. Note that we only use the bounding boxes without local semantics. The original bounding boxes in AITW~\cite{aitw} are identified using an OCR model and IconNet~\cite{rico_semantics}. 

    \item \textbf{Language model}. For evaluation on As discussed earlier, we directly apply the predictions provided by Mind2Web~\cite{mind2web} using a pretrained language model DeBERTa-v3-base. This model gives approximately $85\%$ recall@50.
\end{itemize}

\label{sec:supp_techniques}

\section{Qualitative Analysis}
\label{sec:qualitative}

\subsection{UI Navigation}

Given the performant UI navigation performance across different tasks, we show some Mobile UI navigation samples in Fig.~\ref{fig:supp_aitw_ui}. We prompt the model to complete two daily tasks starting from the home page: ``What's the weather like in Tokyo" and ``Install app `Instagram'". Despite that our model is never trained with the full trajectory, it can handle the tasks in the wild pretty well.

\begin{figure*}
    \includegraphics[width=1.0\linewidth]{figures/images/magma_ui_case_study.pdf}
    \caption{\textbf{Examples for mobile UI navigation sample}. We prompt the model with two tasks: ``What's the weather like in Tokyo" and ``Install app `Instagram'". The model take actions sequentially given the new observation and history action information.}
    \label{fig:supp_aitw_ui}
\end{figure*}

\subsection{Robotics Manipulation}

\renewcommand{\thesubfigure}{\alph{subfigure}} 

\renewcommand{\thesubfigure}{\alph{subfigure}} 
\begin{figure*}[!t]
\begin{subfigure}{1.0\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/openvla_hotdog.png}      
    \caption{Robot policy rollout for task ``Put the sausage to hotdog'' for OpenVLA model. (\textcolor{red}{Failure})}
\end{subfigure}
\begin{subfigure}{1.0\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/openvla_mushroom.png}      
    \caption{Robot policy rollout for task ``Pick up the mushroom to the pot'' for OpenVLA model. (\textcolor{red}{Failure})}
\end{subfigure}

\begin{subfigure}{1.0\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/magma_hotdog.png}   
    \caption{Robot policy rollout for task ``Put the sausage to hotdog'' for Magma model. (\textcolor{lightgreen}{Success})}    
\end{subfigure}
\begin{subfigure}{1.0\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/magma_mushroom.png}   
    \caption{Robot policy rollout for task ``Pick up the mushroom to the pot'' for Magma model. (\textcolor{lightgreen}{Success})}    
\end{subfigure}

    \caption{\textbf{Comparison between OpenVLA (top two rows) and Magma (bottom two rows) for real robot manipulation task.} The two robot policies starts with the same initial stage and asked to perform exactly the same task. The whole task requires precise spatial understanding and planning for the model. For both tasks, OpenVLA failed to accomplish while our model successfully handle.}
    \label{fig:supp_real_robot}
\end{figure*}

We further show the real robot manipulation rollout for OpenVLA and Magma model. As discussed in our main paper, our model exhibits much better generalization ability to different real robot manipulation tasks. In Fig.~\ref{fig:supp_real_robot}, we qualitatively show how two models handle a complicated task of ``Pick up the sausage and put it inside the hotdog''. Thanks to the proposed pretraining techniques, our Magma model can not only precisely pick up the sausage but also move smoothly to the top of the hotdog, demonstrating superior spatial understanding and reasoning capability compared with the counterpart.