\section{Conclusion}
We present the \magma foundation model that can understand and act on multimodal inputs to complete agentic tasks in different environments. 
Our experiments show that the use of SoM and ToM prediction tasks in pretraining helps the model learn to ground and plan actions, respectively. In our experiments, \magma shows strong spatial-temporal reasoning ability and significantly outperforms baselines on downstream UI navigation and robotic manipulation tasks.

\paragraph{Social Impacts and Limitations.}

To develop a foundation model with both verbal and spatial intelligence capable of handling diverse agentic tasks in digital and physical environments, we curated a comprehensive pretraining dataset from a wide range of image, video, and robotics domains: 
\begin{itemize}
    \item \textbf{UI navigation data}. We leverage two pretraining datasets SeeClick and Vision2UI.
    \item \textbf{Instructional videos}. As our goal was to learn an agentic model that can undertake daily tasks like humans, we compile the videos from Epic Kitchen, Ego4d, Something-Something v2 and other instructional videos.

    \item \textbf{Robotics manipulation data}. For robotics task, we follow OpenVLA to leverage the robotics data in Open-X-Embodiment.
    \item \textbf{Multimodal understanding data}. Lastly, we include a small set of multi modal pretraining data ShareGPT4V, and instruction tuning data LlaVA-1.5 plus a number of other domain-specific data to retain the generic multimodal understanding capability of the pre-trained model. 
\end{itemize}
 
The data markup of the robotics and UI navigation data is fairly standardized focusing on generic manipulation tasks (“Place x object on y object”) and generic UI navigation tasks (“Click search button”). We, however, performed a detailed data reflection exercise on the video data of people performing certain tasks. The core inferences we took from these videos were the trajectory of objects over time when the tasks were performed.  

We note that the distribution of identities and activities in the instructional videos are not representative of the global human population and the diversity in society. We are cognizant of the unintended societal, gender, racial and other biases in training with these data, so we will ensure required disclaimers are in place when publishing the models. The training dataset, task list and descriptions focus on the next action to perform only – not describe, act on, or perform any analysis on the subject itself. While there can be unintended outputs from the model based on adverse task descriptions, we will ensure to highlight the use cases the model was trained for and it’s intended use.  

\paragraph{\textbf{Responsible AI}.} It is important to note that the model is specifically designed for UI navigation in a controlled Web UI and Android simulator, and robotic manipulation tasks and should not be broadly applied to other tasks. The recommended usage is within the settings they were trained on, namely, an enclosure equipped with a robotic arm and everyday objects for robotic manipulation and an android simulator running on a computer for UI manipulation. For UI navigation task, researchers should make sure that a human is in the loop and in control for every action the agentic system generates. Since the model cannot act by itself, the sub-module a researcher uses to actually perform the UI navigation action should ensure that no unintended consequences can occur as a result of performing the UI action proposed by the model. 

The model by itself demonstrates good-enough capability in UI navigation and robotic manipulation, but is not usable as is for exploitation scenarios. A threat actor, can however use specific training data for a specific malicious task, to leverage the model as a base to perform automated UI navigation. This is a generic risk associated with the agentic models. 

\paragraph{Acknowledgments.} We would also like to thank Professor Yong Jae Lee for thoughtful discussions, Xiyang Dai for valuable discussions and data support, Mei Yang and Denny Sun for early data engineering effort, and Swadheen Shukla for internal RAI and data reviews. We would also like to thank Doug Burger and Desney Tan for the multifaceted leadership support.