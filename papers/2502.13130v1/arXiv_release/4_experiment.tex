\section{Experiment}
\begin{table*}[ht]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lc|ccccccccccccccc}
    &   & \multicolumn{3}{c}{\rotatebox{0}{\textbf{Multimodal Understanding}}} & \multicolumn{5}{c}{\rotatebox{0}{\textbf{UI Action Grounding and Navigation}}} & \multicolumn{2}{c}{\textbf{Robot Manipulation}} \\
    \toprule
    Model  & Size&  VQAv2 & TextVQA & POPE & \textit{SS}-Mobile & \textit{SS}-Desktop & SS-Web & VWB-Ele-G & VWB-Act-G &  SE-Google Robot & SE-Bridge \\
    \midrule
    {GPT-4V}~\cite{gpt4v} & {n/a} & 77.2 & \textbf{78.0} & {n/a} & {22.6/24.5} & {20.2/11.8} & {9.2/8.8} & {\underline{67.5}} &{\textbf{75.7}} & - & - \\  
    {GPT-4V-OmniParser}~\cite{lu2024omniparser} & {n/a} & n/a & n/a & {n/a} & {\textbf{92.7}/49.4} & {64.9/26.3} & {\textbf{77.3}/39.7} & - & - & - & -\\
          \hline
          \hline
    LLaVA-1.5~\cite{liu2023llava} & 7.4B &78.5 & 58.2 & 85.9 & - & - & - & 12.1 & 13.6 & - & -\\
    % LLaVA-1.5~\cite{liu2023llava} & 13B &&& & - & - & - & 15.0 & 8.7 & - & - \\
    LLaVA-Next~\cite{liu2024llavanext} & 7.4B & \textbf{81.8} & 64.9 & \underline{86.5} & - & - & - & 15.0 & 8.7 & - & - \\    
    Qwen-VL~\cite{Qwen-VL} & 9.6B & 78.8 & 63.8 & n/a & 7.5/4.8 & 5.7/5.0 & 3.5/2.4 & 14.0 & 10.7 & - & -\\
    Qwen-VL-Chat~\cite{Qwen-VL} & 9.6B & 78.2 & 61.5 & n/a & - & - & - & - & - & - & -\\
    
    % \textcolor[rgb]{0.753,0.753,0.753}{CogVLM}~\cite{wang2023cogvlm} & \textcolor[rgb]{0.753,0.753,0.753}{18B} & \textcolor[rgb]{0.753,0.753,0.753}{83.4} & \textcolor[rgb]{0.753,0.753,0.753}{68.1} & \textcolor[rgb]{0.753,0.753,0.753}{87.9} & - & - & - & \textcolor[rgb]{0.753,0.753,0.753}{17.7} & \textcolor[rgb]{0.753,0.753,0.753}{23.3} & - & -\\    
    \midrule
    Fuyu~\cite{fuyu_8b} & 8B  & 74.2 & n/a & n/a & 41.0/1.3 & 33.0/3.6 & 33.9/4.4 & 19.4 & 15.5 & - & -\\    SeeClick~\cite{seeclick} & 9.6B & - & - & - & \underline{78.0}/\underline{52.0} & 72.2/\underline{30.0} & 55.7/32.5 & 9.9 & 1.9 & - & -\\
    % \textcolor[rgb]{0.753,0.753,0.753}{CogAgent}~\cite{cogagent} & \textcolor[rgb]{0.753,0.753,0.753}{18B} & \textcolor[rgb]{0.753,0.753,0.753}{83.7} & \textcolor[rgb]{0.753,0.753,0.753}{76.1} & \textcolor[rgb]{0.753,0.753,0.753}{n/a} & \textcolor[rgb]{0.753,0.753,0.753}{{67.0}/24.0} & \textcolor[rgb]{0.753,0.753,0.753}{\underline{74.2}/20.0} & \textcolor[rgb]{0.753,0.753,0.753}{\underline{70.4}/28.6} & \textcolor[rgb]{0.753,0.753,0.753}{41.6} & \textcolor[rgb]{0.753,0.753,0.753}{23.3} & - & -\\        % \hline
Octo~\cite{team2024octo} & 93M  & -&-&-&- & - & - &   - & -     &   6.0 & \underline{15.9}\\
    RT-1-X~\cite{open_x_embodiment_rt_x_2023} & 35M  & - & - & - & - & - &  -   & -     & -   &   \underline{34.2} & 1.1 \\
    OpenVLA~\cite{kim2024openvla} & 8B  & - & - & - & - & - &  -   & -     & -   &   31.7 & 14.5 \\
    \midrule
    {\magma-8B~(Ours)} & 8.6B & \underline{80.0} & \underline{66.5} & \textbf{87.4} &60.4/\textbf{58.5} & \textbf{75.3/52.9} & {69.1}/\textbf{52.0} & \textbf{96.3} & \underline{71.8}  & \textbf{52.3} & \textbf{35.4} \\    
    \end{tabular}
    }
    \vspace{-3pt}
    \caption{\textbf{Zero-shot evaluation on agentic intelligence}. We report the results for pretrained \magma \textit{without} any domain-specific finetuning. \magma is the only model that can conduct the full task spectrum. ``SS'' denotes the ScreenSpot benchmark proposed in SeeClick~\cite{seeclick}; ``VWB'' denotes VisualWebBench~\cite{liu2024visualwebbench}; ``SE'' denotes the SimplerEnv simulator~\cite{li24simpler}. `n/a' means not available and `-' means not supported. For all related evaluations, we use OmniParser to provide the detection results only, without local semantics.}
\label{tab:agentic_evaluation}
\end{table*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/images/magma_simpler.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{SimplerEnv performance comparison on Google Robots and Bridge.} \texttt{Magma}(OXE) represents our model trained solely on Open-X-Embodiment (OXE)~\cite{embodimentcollaboration2024openxembodimentroboticlearning}, while \texttt{Magma} is our pretrained model. Results for each task are averaged across visual matching and variant aggregation scenarios.}
    \label{fig:simpler_env}
\vspace{-10pt}
\end{figure*}

\subsection{Evaluating Agentic Capability}

We examine the effectiveness of \magma as the foundation model for multmodal agents on UI Navigation tasks in the digital world, the robotic manipulation in the physical world, as well as the generic multimodal understanding.

\subsubsection{Zero-Shot Evaluation}

% We start with the evaluation of 
To evaluate the zero-shot transferability of \magma,  we employ ScreenSpot~\cite{seeclick} and VisualWebBench~\cite{liu2024visualwebbench} for evaluating UI action grounding and navigation, and SimplerEnv~\cite{li24simpler} for robotic manipulation. In addition to these evaluations, we also validate our model on generic~\cite{goyal2017vqav2} and text-rich~\cite{singh2019textvqa} VQA tasks as well as hallucination benchmark POPE~\cite{li2023pope}. As shown in Table~\ref{tab:agentic_evaluation}, \magma~consistently outperforms all other general-domain LMMs~(\eg, LLaVA, Qwen-VL) and domain-specific agentic models such as SeeClick~\cite{seeclick} for UI navigation and OpenVLA~\cite{kim2024openvla} for robotic manipulation. Notably, the zero-shot performance of \magma on UI is much better than the state-of-the-art vision-based method that uses GPT-4V and Omniparser~\cite{lu2024omniparserpurevisionbased}. 
% Furthermore, \magma outperforms SeeAct which uses GPT-4V and a combination DoM tree and screenshot.
We report the results on two commonly used simulator embodiments in SimplerEnv~\cite{li24simpler}, Bridge and Google Robot including 8 tasks with 172 visual matching and variant aggregation scenarios. 
%As all pretraining data are real-robot trajectories, 
Since OpenVLA uses real robot trajectories for pre-training, the model is susceptible to the domain gap for real-to-sim adaptation. In contrast, our \magma model, trained for multimodal understanding and action prediction on a wide range of heterogeneous datasets, is significantly more resilient to the gap and achieves significantly better success rates. 
% At this point, we would like to highlight that our \magma model is the first multimodal agentic model that can understand images and produce actions in both digital and physical environments. 

% To further verify the effectiveness of \magma on robotic manipulation tasks and the benefit of using heterogeneous training data, we have developed a robot-only \magma, which is pretrained only on the Open-X-Embodiments dataset~\citep{open_x_embodiment_rt_x_2023}, namely \magma~(OXE), and compare both \magma models with other VLA models that are tailored to robotic manipulation, including Octo, RT-1-X, and OpenVLA, in the finetuning setting.    

Fig.~\ref{fig:simpler_env} shows detailed comparisons between our pretrained \magma model and other representative models. Remarkably, \magma surpasses the second-place OpenVLA by \textbf{19.6}\%, nearly doubling the average success rate. On those challenging tasks such as ``Put Object in Drawer'' and ``Put Carrot on Plate'', \magma achieves a remarkable success rate while most baselines fail entirely. Notably, \magma tuned on our pretrained model showcases substantially better results than the version trained solely on robotic datasets, highlighting the value of spatial intelligence learned from diverse datasets for physical robotic manipulation tasks.

\begin{table}[t]
\vspace{-4pt}
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lc|ccccc}
    \toprule
    Model & SoM+ToM & SS-Overal & VWB-Ele-G & VWB-Act-G& SE-Bridge & SE-Google\\
    \midrule
    % \magma-8B~(UI) & 53.8 & 60.0 & 59.2 & 0.0 & 0.0 \\
    % \magma-8B~(OXE) & 0.0 & 0.0 & 0.0 & 22.2 & 35.7 \\
    % \magma-8B~(I) & 49.3 & 57.8  & 59.2 & 17.5 & 31.5 \\
    \magma-8B~(UI) & \xmark & 57.7 & 68.5 & 58.3 & - & -\\
    \magma-8B~(OXE) & \xmark & - & - & - & 22.2 & 35.7 \\
    \magma-8B~(ACT) & \xmark & {56.2} & 89.1 & 21.4 & 17.5 & 31.5\\
    \magma-8B~(Full) & \xmark & 57.4 & {90.1} & {25.2} & {17.7} & {37.5} \\
    \magma-8B~(Full) & \cmark & \textbf{61.4} & \textbf{96.3} & \textbf{71.8} & \textbf{35.4} & \textbf{52.3} \\
    \bottomrule
    \end{tabular}}
    \vspace{-10pt}
    \caption{Ablation study on the effect of data mixtures and pretraining techniques. w/o SoM+Tom means using original action supervisions~(2D coordinates for UI and 7DoF for robots.)}
\label{tab:ablations}
    \vspace{-8pt}
\end{table}

\noindent\textbf{Ablation Studies.} We ablate our model pretraining techniques and data mixtures. The results are shown in Table~\ref{tab:ablations}. First, we observe from the top three rows that simply combining UI and robotics data does not bring gains, but instead hurts the performance for both tasks. This is expected because the two agentic tasks have significantly different image domains as well as action spaces~(2D coordinates \textit{v.s.} 7-DoF). Adding video data to the pretraining slightly improves the performance across board but still can not fill the gap in between, as the additional video narrations can only enhance the verbal intelligence. However, once we apply SoM and ToM to all the pretraining data to put them into the unified interface, our model can learn effectively from the heterogeneous data for both verbal and spatial intelligence. This study highlights the effectiveness of our proposed method and indicates equally importance of verbal and spatial understanding for agentic tasks.

% In the end, the strong zero-shot results of \magma 
% validate our model design choice of having one VLA model for both robotic manipulation and UI navigation tasks by unifying and reformulating these tasks as the problem of spatially grounding actions in vision-language observations. 

\subsubsection{Efficient Finetuning}

\begin{table*}[]
    \centering
    \footnotesize
    \resizebox{\textwidth}{!}{%    
    \begin{tabular}{l|lcccccccccccccccc}
        \multirow{2}{*}{Method} &  \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Input Source} & \multicolumn{3}{c}{Cross-Website}  & \multicolumn{3}{c}{Cross-Task} & \multicolumn{3}{c}{Cross-Domain} \\
         & & DoM Tree & Image &  Ele. Acc & Op. F1 & Step SR & Ele. Acc & Op. F1 & Step SR & Ele. Acc & Op. F1 & Step SR  \\        
        \toprule
         \multirow{1}{*}{GPT-4-MindAct~\cite{mind2web}} & GPT-4~\cite{gpt4} & \cmark & & 35.8 & 51.1 & 30.1 & 41.6 & 60.6 & 36.2 & 37.1 & 46.5 & 26.4 \\
         \multirow{1}{*}{GPT-4V-OmniParser~\cite{lu2024omniparser}} & GPT-4V~\cite{gpt4v} & \cmark & \cmark & 41.0 & \textbf{84.8} & 36.5 & 42.4 & \textbf{87.6} & 39.4 & 45.5 & \textbf{85.7} & 42.0 \\
          % & GPT-4V & & \cmark &  \\        
        \midrule 
        \multirow{3}{*}{SeeAct~\cite{zheng2024gpt4vision}} & GPT-4V~\cite{gpt4v} &  & \cmark & & & 13.9 & - & - & 20.3 & - & - & 23.7 \\
        & Gemini-Pro~\cite{geminiteam2024gemini} & \cmark & \cmark & 21.5 & 67.7 & 19.6 & 21.5 & 67.7 & 19.6 & 20.7 & 64.3 & 18.0\\
        & GPT-4V~\cite{gpt4v} & \cmark & \cmark & 38.0 & 67.8 & 32.4 & 46.4 & 73.4 & 40.2 & 42.4 &69.3 &36.8\\
          \hline
          \hline
          % \textcolor{red}{CogAgent}~\cite{cogagent} & CogVLM~\cite{wang2023cogvlm} & \cmark & \cmark & 18.4 & 42.2 & 13.4 & 22.4 & 53.0 & 17.6 & 20.6 &42.0 &15.5\\
          Fuyu-8B$^{\ddag}$ & Fuyu-8B~\cite{fuyu_8b} & & \cmark  & 4.8 & 81.3 & 4.0 & 8.3 & 83.9 & 6.6 & 3.6 & 83.0 & 3.0\\
          Fuyu-8B-GUI~\cite{chen2024guicourse} & Fuyu-8B~\cite{fuyu_8b} & & \cmark  & 13.9 & 80.7 & 12.2 & 19.1 & 86.1 & 15.6 & 14.2 & 83.1 & 11.7\\
          MiniCPM-V$^{\ddag}$ & MiniCPM-V~\cite{yao2024minicpm} & & \cmark  & 8.2 & 78.2 & 6.0 & 11.0 & 85.6 & 8.5 & 6.5 & 81.4 & 5.2\\
          MiniCPM-V-GUI~\cite{chen2024guicourse} & MiniCPM-V~\cite{yao2024minicpm} & & \cmark  & 20.3 & 81.7 & 17.3 & 23.8 & 86.8 & 20.8 & 17.9 & 74.5 & 17.6\\
          Qwen-VL$^{\natural}$ & Qwen-VL~\cite{Qwen-VL} & & \cmark  &13.2 & 83.5 &9.2 &15.9 &86.7 &13.3 & 14.1 & 84.3 & 12.0\\
          SeeClick~\cite{seeclick} & Qwen-VL~\cite{Qwen-VL} & & \cmark & 21.4 & 80.6 & 16.4 & 28.3 & 87.0 & 25.5 & 23.2 & 84.8 & 20.8 \\
          CogAgent$^{\dag}$~\cite{cogagent} & CogVLM~\cite{wang2023cogvlm} &  & \cmark & 27.3 & - & 23.4 & 30.2 & - & 26.9 & 33.1 & - & 28.5 \\
          Qwen2-UIX~\cite{multiUI} & Qwen2~\cite{Qwen2} & & \cmark & 39.2 & - & 31.0 & 43.4 & - & 38.2 & 40.4 & - & 34.9 \\     
        \midrule
          \magma-8B~(Ours) & LLaMA3~\cite{llama-3} & & \cmark & \textbf{57.2} & 76.9 & \textbf{45.4} & \textbf{54.8} & 79.7 & \textbf{43.4} & \textbf{55.7} & 80.6 & \textbf{47.3} \\
          % Magma~(Ours) & LLaMA3~\cite{llama-3} & & \cmark & \textbf{42.3} & 77.1 & \textbf{34.0} & \textbf{42.7} & 79.7 & \textbf{34.4} & \textbf{55.7} & 80.6 & \textbf{47.3} \\
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Efficient finetuning on Mind2Web for web UI navigation}. ``Ele. Acc'' denotes element selection accuracy. ``Op. F1'' denotes the token-wise F1 score between predicted ground-truth operation. ``Step SR'' denotes the step-wise success rate. $^{\ddag}$ Numbers reported in \citet{chen2024guicourse}. $^{\natural}$ Numbers reported in \citet{seeclick}. $^{\dag}$ Numbers reported in \citet{multiUI}.}
    \label{tab:mind2web}
\end{table*}


\begin{table*}[]
    \centering
    \footnotesize
    \resizebox{\textwidth}{!}{%    
    \begin{tabular}{l|lcccccccccccccc}
         Method & Backbone & DoM Tree & Image &  General & Install & GoogleApps & Single & WebShopping & Overall \\        
        \hline
         \multirow{1}{*}{GPT-4V-SeeAct$^{\dag}$~\cite{zheng2024gpt4vision}} & GPT-4V~\cite{gpt4v} &  & \cmark & 34.1 & 39.4 & 40.0 & 46.2 & 38.2 & 39.6 \\
         \multirow{1}{*}{GPT-4V-ReAct$^{\dag}$~\cite{yao2022react}} & GPT-4V~\cite{gpt4v} &  & \cmark & 36.2 & 42.5 & 46.6 & 49.1 & 39.2 & 42.7 \\
         \multirow{1}{*}{GPT-4V-OmniParser~\cite{lu2024omniparser}} & GPT-4V~\cite{gpt4v} & \cmark & \cmark & 48.3 & 57.8 & 51.6 & 77.4 & 52.9 & 57.7 \\
         \hline
         \hline
          Fuyu-8B$^{\ddag}$ & Fuyu-8B~\cite{fuyu_8b} & & \cmark & - & 45.9 & 40.0 & 47.2 & 40.8 & - \\
          Fuyu-8B-GUI~\cite{chen2024guicourse} & Fuyu-8B~\cite{fuyu_8b}  & & \cmark & - & 50.9 & 41.6 & 45.7 & 43.8 & - \\
          MiniCPM-V$^{\ddag}$ & MiniCPM-V~\cite{yao2024minicpm} & & \cmark & - & 50.2 & 45.1 & 56.2 & 44.0 & - \\
          MiniCPM-V-GUI~\cite{chen2024guicourse} & MiniCPM-V~\cite{yao2024minicpm} & & \cmark & - & 62.3 & 46.5 & 67.3 & 57.5 & - \\
          Qwen-VL$^{\natural}$ & Qwen-VL~\cite{Qwen-VL} & & \cmark  & 49.5 & 59.9 & 46.9 & 64.7 & 50.7 & 54.3 \\
          SeeClick~\cite{seeclick} & Qwen-VL~\cite{Qwen-VL} & & \cmark & {54.0} & {66.4} & {54.9} & 63.5 & {57.6} & {59.3}\\
          
        \hline
          \magma-8B~(Ours) & LLaMA3~\cite{llama-3} & & \cmark & \textbf{61.5} & \textbf{73.2} & \textbf{62.7} & \textbf{77.5} & \textbf{61.7} & \textbf{67.3} \\
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Efficient finetuning on AITW for mobile UI navigation}. We compared models either using DoM tree or image screenshot. We finetune our \magma jointly and then report the results on individual tasks. $^{\dag}$ Numbers reported in \citet{zhang2024dynamic}. $^{\ddag}$ Numbers reported in \citet{chen2024guicourse}. $^{\natural}$ Numbers reported in \citet{seeclick}.} 
    \label{tab:aitw}
\end{table*}

With moderate finetuning, the pretrained \magma model can be easily transferred to various downstream agentic tasks.

\noindent \textbf{{UI Navigation}}. Following the prior works~\cite{seeclick,cogagent}, we finetune \magma on Mind2Web and AITW, to examine the web and mobile UI navigation capabilities, respectively. For Mind2Web, we first apply the SoM prompting to the training samples according to the top candidates selected by~\cite{zheng2023seeact}, and then finetune \magma on the same samples as in SeeClick~\cite{seeclick}. Table~\ref{tab:mind2web} shows the results in three subtasks, and clearly indicates \magma's superiority to both general-domain and specific-domain LMMs. Similarly, on AITW \magma outperforms the state-of-the-art methods based on open-source or prosperity models. Considering that we use a similar size of LLM and a moderate amount of UI-related pretraining data, this decent performance is largely due to the proposed SoM and ToM modeling techniques, which significantly facilitate action grounding for UI navigation.

\noindent \textbf{{Robotics Manipulation}}. Table~\ref{tab:agentic_evaluation} shows that the \magma model without domain-specific finetuning already outperforms the recently proposed OpenVLA model pretrained for 27 epochs on the same amount of OXE data. Below, we testify the effectiveness of the finetuned \magma model by comparing it with OpenVLA in three settings:
\begin{itemize}
    \item Finetune on real robot data to evaluate on out-of-distribution manipulation tasks;
    \item Finetune in simulated robot settings with a limited number of trajectories using the LIBERO benchmark to evaluate \magma's capability of task adaptation; and
    \item Evaluate on the physical WidoxW 250 Arm.
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/images/magma_real.pdf}
    \vspace{-5pt}
    \caption{\textbf{Few-shot finetuning and generalization performance on real robot.} On a WidowX robot, we evaluate \magma on 4 tasks including diverse everyday object manipulation.}
    \label{fig:real}
\vspace{-10pt}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/images/magma_libero.png}
    \vspace{-5pt}
    \caption{\textbf{Few-shot finetuning results on the LIBERO} simulation benchmark, using 10 trajectories per task for fine-tuning.}
    \label{fig:libero}
\vspace{-10pt}
\end{figure}



%The results on SimplerEnv is shown in Fig.~\ref{fig:simpler_env}. Compared with VLA models like Octo, RT-1-X and OpenVLA, the finetuned \magma achieves uniform improvements on various tasks. Remarkably, \magma outperforms the second place OpenVLA by \textbf{19.6}\%, almost doubling the success rate on average. On Google Robot tasks, \magma uniformly outperforms RT-1-X, including the challenging task of ``Put Object in Drawer'', while the other methods completely fail.

%In the evaluation on the real WidowX 250 Arm, we validate the effectiveness of \magma. 
We collect four manipulation tasks each of which has roughly 50 trajectories (See details in our supplementary material), and finetune both OpenVLA and \magma jointly on these tasks. For evaluation, we perform 10 trials per task, ensuring the same initial states (positions and orientations of end-effector and objects) across models. As shown in Fig.~\ref{fig:real}, the results clearly demonstrate \magma's superior performance. For those challenging tasks that involve everyday objects like ``Pick Place Hotdog Sausage'', ``Put Mushroom in Pot'', and ``Push Cloth Right to Left'', OpenVLA can hardly accomplish the tasks, mainly because of the imprecise arm movement and object localization per our observation. In contrast, \magma performs well on these sophisticated tasks, largely owing to its strong spatial understanding and grounding capability obtained from pertaining. Additionally, we evaluate models' performance on an unseen task ``Push Cloth Left to Right'' which are not included in our finetuning dataset. \magma substantially outperforms the baseline, indicating a stronger ability to preserve pretrained knowledge and generalize to new tasks.

The efficient adaptation (via finetuning) capability of \magma is further validated through few-shot finetuning evaluations on the LIBERO benchmark. For each task suite in the benchmark, we sample only 10 trajectories for finetuning. During the evaluation, we perform 100 trials per task suite. The results, shown in Fig.~\ref{fig:libero}, indicate that \magma achieves a significantly higher average success rate in all task suites. 
%highlighting its strong adaptability. 
Additionally, removing SoM and ToM during pretraining has a negative impact on model performance, underscoring the effectiveness of our pretraining method.

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|cccccccc}
    & \multirow{2}{*}{VSR} & \multirow{2}{*}{BLINK-val} & \multicolumn{3}{c}{SpatialEval\footnotemark } \\
    Model & & & \makecell{Spatial Map } & \makecell{Maze Nav.} & \makecell{Spatial Grid } \\
    \midrule
    % \rowcolor{gray!20}
    %GPT-4o & 74.8 & 69.2 & 59.8 & 60.0 & 67.7 & 38.3 & 82.2\\
    GPT-4o & 74.8 & 60.0 & - & - & -\\
    % \rowcolor{gray!20}
    Gemini  & - & 61.4 & - & - & -\\
          \hline
          \hline
    LLaVA-1.5-7B  &  57.1* & 37.1 & 28.4 & 28.8 &  41.6\\
    LLaVA-1.6-7B~\cite{liu2024llavanext}  & 52.2* & - & 28.0 & 34.8 & 32.2\\
    Qwen-VL-9.6B~\cite{Qwen-VL}  & - & 40.3 & 28.7 & 31.8 & 25.7\\
    \midrule
    \magma-8B~(Act\textsuperscript{w/o}) & 62.8 & 30.1 & 36.9 & \textbf{44.8} & 37.5    \\
    \magma-8B~(Full\textsuperscript{w/o}) & 58.1  & 38.3 & 27.5 & 33.5 & 47.3 \\
    \magma-8B~(Full) & \textbf{65.1} & \textbf{41.0} & \textbf{43.4} & 36.5 & \textbf{64.5} \\    
    \end{tabular}}
    \vspace{-3pt}
    \caption{\textbf{Spatial reasoning evaluations.} We use * to denote results that are obtained by us evaluating the provided model weights. Superscript `w/o' means models pretrained without SoM/ToM. 
    }
    \label{tab:spatial_eval_results}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/magma_spatial_visualizations_v2.png}
    \vspace{-0.6cm}
    \caption{\textbf{Spatial evaluation predictions.} Spatial reasoning questions are challenging even for GPT-4o but \magma can answer relatively well despite relying on much fewer pretraining data.}
    \label{fig:spatial_eval_examples}
\end{figure}

\subsection{Evaluating Spatial Reasoning}
We attribute the much improved performance of our \magma model on the tasks of UI navigation and robotic manipulation, as shown above, to its improved ability to perform spatial reasoning. To verify this hypothesis, we evaluate the effectiveness of the spatial intelligence that is learned in our pretrained model on the challenging Visual Spatial Reasoning (VSR)~\cite{liu2023visual}, BLINK~\cite{fu2024blink} and SpatialEval~\cite{wang2024picture} benchmarks under the zero-shot setting. The results are summarized in Table~\ref{tab:spatial_eval_results}. We see that \magma outperforms existing approaches by significant margins on VSR and SpatialEval, and that \magma performs on par with CogVLM, despite only using $\sim$29M images for pretraining as compared to $\sim$1.5B images in the latter. In addition, our ablation study demonstrates the effectiveness of the SoM and ToM pretraining tasks in helping \magma improve its spatial reasoning capabilities. Last but not least, we also note the benefits of using video data during pretraining by showing that removing vidoes from training data leads to $\sim$8\% performance drop on BLINK. Finally, we also provide some example predictions of our \magma model in Figure~\ref{fig:spatial_eval_examples}. We observe that spatial reasoning questions are also challenging for SOTA proprietary models such as GPT-4o. Despite the lack of pretraining on data with mazes, we see that \magma is still able to answer spatial reasoning questions about them. 

\begin{table}[]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|cccccccc}
    Model & VQAv2 & GQA & MME & POPE & TextVQA & ChartQA & DocVQA \\
    \midrule
    LLaVA-1.5-7B~\cite{li2023llava} & 76.6 & 62.6 & 1510.8 & 85.9 & 46.1 & 18.2 & 28.1 \\
    % \textcolor{blue}{CLIP (Yujia)}    & VIT-L/336 & & 63.2 &  & 68.9 &  & 86.0 &  & \\
    LLaVA-Next-7B~\cite{liu2024llavanext} & 80.1 & \textbf{64.2} & 1519.3 & \textbf{86.4} & 64.9 & 54.8 & 74.4 \\
    % VILA-7B~\cite{}  & & &&& & \\
    \midrule
    \magma-8B~(SFT) &79.5 & 61.5 & 1510.1 & 86.2 & 67.7 & 73.0 & 80.4 \\    
    \magma-8B~(Act\textsuperscript{w/o}) &81.3 & 63.5 & 1559.5 & 86.1 & 69.8 & 71.0  & 84.1 \\ 
    \magma-8B~(Full\textsuperscript{w/o}) & 81.3& 62.9 & 1576.0 & 86.3 & 69.6& 71.7 & 83.8 \\ 
    \magma-8B~(Full) & \textbf{81.4} & 64.0 & \textbf{1588.7} & 86.3 & \textbf{70.2} & \textbf{76.2} & \textbf{84.8} \\
    \end{tabular}}
    \vspace{-5pt}
    \caption{\textbf{Finetuned performance on multimodal image understanding tasks}. Pretraining on full set with SoM and ToM~(last row) attains the overall best performance compared with our own baselines and counterparts of the same model class.
    }
    \vspace{-1.5em}
    \label{tab:image_qa_results}
\end{table}


\footnotetext{We evaluate our model using the standard option matching before the official evaluation pipeline was released and will update in the next version.}

\begin{table*}[t]
    \centering
    \small
    \resizebox{\textwidth}{!}{%    
    \begin{tabular}{l|lcccccccccccccc}
        \multirow{2}{*}{Method} &  \multirow{2}{*}{Backbone} & \multicolumn{1}{c}{IntentQA}  & \multicolumn{1}{c}{Next-QA}  & \multicolumn{3}{c}{VideoMME (w/o subs)} & \multicolumn{4}{c}{MVBench} \\
         &   & Overall & Overall & Short & Medium & Long & Action Prediction & Action Sequence & Action localization  & Overall\\        
        \toprule
        % \rowcolor{gray!20}
        \multirow{1}{*}{Gemini-1.5~\cite{geminiteam2024gemini}} & - & - & - & 81.7 & 74.3 & 67.4 & - & - & - & 37.7\\
        % \rowcolor{gray!20}
         \multirow{1}{*}{GPT-4V~\cite{achiam2023gpt}} & GPT-4  & - & - & 70.5 & 55.8 & 53.5  & - & - & - & 43.7\\      
        % \midrule 
        \hline
        \hline
         LLaVA-OV~\cite{li2024llavaonevision} & Qwen2-7B   & - & \underline{79.4} & \underline{68.1} & \underline{54.9} & \textbf{47.8} & 46.0 & 74.5 & 48.0 & 56.7\\
          Long-Llava 9B~\cite{wang2024longllava}  & Long-Llava 9B & - & - & 52.4 & 42.2 & 36.4 & - &- &- & 49.1\\
          % LongVA & Qwen2-7B & - & - & - & - & - & 69.3 & 61.1 & 50.4 & 46.2 & - &- &- & -\\
           LongVA~\cite{zhang2024long} & Qwen2-7B  & - & 69.3 & 61.1 & 50.4 & \underline{46.2} & 49.0 & 53.0 &42.5 & 51.3 \\

          ShareGPT4Video~\cite{chen2024sharegpt4video} & LLaMA3-8B & - & - & 48.3 & 36.3 & 35.0 & 40.0 & 49.5 & 41.5 & 51.2\\

          Video-Llama2~\cite{cheng2024videollama} & Llama2-7B  & - & - & 55.9 & 45.4 & 42.1 & - &- &- & 34.1\\
          
          Video-Chat2~\cite{li2024mvbenchcomprehensivemultimodalvideo} & Mistral 7B  & - & 43.3 & 48.3 & 37.0 & 33.2 & 47.5 & \underline{75.0} & \underline{50.5} & \textbf{60.4}\\
          Video-Llava~\cite{lin2023video} & Vicuna-7B  & - & 51.4 & 45.3 & 38.0 & 36.2 & \underline{50.0} & 38.5 & 30.5 & 43.0\\ 
          IG-VLM~\cite{kim2024image} & Vicuna-7B & \underline{60.3} & - & - & - & - & - & - & - & -\\
          SF-LLaVA~\cite{xu2024slowfast} & Vicuna-7B & 60.1 & - & - & - & - & - & - & - & -\\ 
          \midrule
           Magma-8B~(Ours) & LLaMA3-8B  & \textbf{88.6} & \textbf{80.9} & \textbf{72.9} & \textbf{55.8} & 44.3 & \textbf{65.0} & \textbf{79.0} & \textbf{55.5} & \underline{59.4}\\
    \end{tabular}}
    \caption{\textbf{Zero-shot Video QA benchmarks.} We compare our Magma model to other state-of-the-art approaches with comparable numbers of parameters. Our \magma model performs competitively and even outperforms some state-of-the-art approaches such as Video-Llama2 and ShareGPT4Video on most benchmarks, despite using much fewer video instruction tuning data.}
    \label{tab:video_qa_results}
\end{table*}

\subsection{Evaluating Multimodal Understanding}

\paragraph{Image instruction tuning.} To further assess \magma's multimodal understanding capability, we conduct continuous finetuning on our Magma-SFT-820K data. Then, we compare the finetuned \magma model with existing VLMs on a suite of commonly used image reasoning benchmarks, \eg MME and GQA. As shown in Table~\ref{tab:image_qa_results}, \magma outperforms recently-proposed VLMs on most of the tasks, with notable gains of $\sim$5\% and $\sim$22\% on TextVQA and ChartQA, respectively. Similarly to our observations in Table~\ref{tab:spatial_eval_results}, our ablation study highlights the effectiveness of using SoM and ToM for pre-training, which leads to $\sim$ 5\% improvement in ChartQA.

\vspace{-0.3em}
\paragraph{Video Instruction Tuning} 
In Table~\ref{tab:video_qa_results}, we report the performance of our \magma model on multiple challenging video question answering (QA) benchmarks including IntentQA~\cite{Li2023IntentQACV}, NextQA~\cite{xiao2021next}, VideoMME~\cite{fu2024videommefirstevercomprehensiveevaluation} and MVBench~\cite{li2024mvbenchcomprehensivemultimodalvideo}. We use the LMMs-Eval framework~\cite{lmms_eval2024} for the latter three benchmarks to ensure reproducibility of our evaluation results. 

The results demonstrate the effectiveness of our pretraining approach, where we outperform most state-of-the-art models with comparable number of parameters consistently across the different benchmarks. For instance, our \magma model achieves a performance gain over the IG-VLM and SF-LLaVA models by approximately 28\%. The IntentQA benchmark evaluates a model's capability to discern the intentions behind observed actions in videos. Thus, the significant improvement on this dataset achieved by \magma can possibly be attributed to the effectiveness of our ToM pretraining task, where it encourages the model to reason about temporal dynamics in future video frames. This is also corroborated by the notable improvement on the subtask of action prediction in MVBench that \magma obtains over state-of-the-art models such as VideoChat2 and LLaVA-OV.

State-of-the-art video LMMs often rely on much large video and text datasets such as Webvid and ShareGPT4Video for pretraining and these datasets span over 4M samples with curated text. Moreover, the aforementioned models also use a higher number of frames during pretraining. In contrast, even when multi-frame pretraining is performed in our case, we only use a maximum of 4 frames due to computational constraints. Thus, it is especially significant that \magma outperforms approaches such as LLaVA-OV and ShareGPT4Video on VideoMME and MVBench, since these approaches often use larger instruction tuning datasets that include both image and video data. Additionally, as evidenced by the performance gain obtained by \magma over the proprietary GPT-4V model, we note that such improvements in results are not solely due to using a more recent and powerful language model like LLama-3. It is also notable that Magma achieves substantially better performance than LongVA, despite using only 32 frames instead of the 64 frames used by the latter. 