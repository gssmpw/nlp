
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{comment}
\usepackage{wrapfig}
\usepackage{graphicx} 

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}



\usepackage{subcaption}


\title{Enhancing Hallucination Detection through Noise Injection}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\lliu}[1]{{\color{brown}{#1}}}
\newcommand{\rebuttal}[1]{{\color{blue}{#1}}}
\newcommand{\INPUT}{\item[\textbf{Input:}]}  % Define custom \INPUT command
\newcommand{\OUTPUT}{\item[\textbf{Output:}]} % Define custom \OUTPUT command


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Large Language Models (LLMs) are observed to generate plausible yet incorrect responses, known as hallucinations.
Effectively detecting such hallucination instances is crucial for the safe deployment of LLMs.
Recent research has linked hallucination to model uncertainty, 
suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from the model. 
While using the model's next token probabilities used during 
training is a natural way to obtain samples, in this work, we argue that for the purpose of hallucination detection, it is overly restrictive and hence sub-optimal. 
Motivated by this viewpoint, we perform an extensive 
empirical analysis showing that an alternative way to measure 
uncertainty - by perturbing hidden unit activations in intermediate layers of the model - is complementary to sampling, 
and can significantly improve detection accuracy over mere sampling. 
%While existing methods rely on next-token sampling as the sole source of randomness for measuring uncertainty, they largely overlook how randomness in earlier stages of computation can affect model behavior.
%In this work, we investigate the effects of injecting noise to perturb the hidden states of intermediate layers, thereby introducing randomness in earlier stage of computation.
%We show that intermediate layer randomness has a complementary effect on model uncertainty compared to prediction layer sampling. We therefore propose combining both sources of randomness to enhance hallucination detection. 
%Through extensive experiments, we demonstrate the effectiveness of this simple scheme. 
%and demonstrate its effectiveness through extensive experiments.
%combining intermediate layer noise injection with prediction layer sampling enhances hallucination detection effectiveness.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have made significant advancements in recent years \citep{achiam2023gpt, zhao2023survey}. 
However, despite the strides, LLMs are observed to sometimes generate plausible yet incorrect responses -- a phenomenon known as hallucination \citep{ji2023survey, kuhn2023semantic}.
To ensure the safe deployment of LLMs, effective detection of hallucination is essential, and it 
has gained significant research attention \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}.
Many research efforts focus on detecting hallucinations by assessing model uncertainty across samples drawn from the model. %, measured by the divergence in outputs produced by the model for the same input. 
For example, \cite{malinin2020uncertainty} proposes leveraging predictive uncertainty for hallucination detection. Similarly, \cite{lin2022towards} and \cite{lin2023generating} propose semantic consistency and quantify lexical similarity across samples.
%Specifically, \lliu{[cite]} groups multiple generations into semantic categories and measure uncertainty within the semantic space; whereas \lliu{[cite]} quantifies uncertainty from the embedding of multiple generations. 
The core principle underlying this line of work is simple: the greater the observed uncertainty, the higher the likelihood of hallucination. %across multiple generations

\begin{wrapfigure}{r}{0.52\textwidth}
\vspace{-8mm}
\centering
\includegraphics[width=0.40\textwidth]{overview.pdf}
\vspace{-5mm}
\caption{\textbf{Source of Randomness in Hallucination Detection}.
Prior work uses prediction layer sampling and measures model uncertainty across samples for hallucination detection. 
Additionally, we explore noise injection that randomly perturbs intermediate representations, introducing a second source of randomness at earlier stages. 
%\lliu{need to re-draw for copy-right issue. add prior/us in plot}
}\label{fig:overview}
\vspace{-8mm}
\end{wrapfigure}

Since a language model defines the probability distribution over the next tokens, the most obvious way to generate such samples is therefore to repeatedly sample from the conditional distribution over tokens given the context so far. A benefit of this way of sampling is that it stays faithful to the probability distribution defined by the model (up to any deviations from the training temperature). Generating faithful samples from the model also makes sense when the goal is to
generate individual answers, say, to a given prompt. 

We note, however, that in the case of hallucination detection, the purpose of sampling is \emph{not} to generate standalone answers. Instead, it is to measure the coherence of a model’s responses to a given prompt. The above-mentioned approaches can in this context also be viewed as performing a type of sensitivity analysis that makes it possible to assess the likelihood of a given prompt to elicit a hallucination in a model. A distribution of responses that stays coherent under perturbations is considered as evidence for the model to ‘‘know’’ the correct response for a given prompt, and for an answer generated by the model accordingly to be truthful. 

It is commonly assumed in language modeling that hidden unit activations tend to capture the more abstract and high-level representations of a given phrase or thought, while logits and low-level token embeddings capture representations that reduce it to a specific syntactic form. This suggests that, even though it is tempting to rely on sampling from the model to assess coherence 
for a given prompt, a better way to assess coherence should
involve perturbations of these hidden representations. 
\rebuttal{
Unlike sampling, which preserves the order of tokens with respect to likelihood regardless of the temperature, the perturbation of hidden representations does not.
Perturbing hidden representations can therefore be complementary for the purpose of measuring coherence. 
%These distinct impacts suggest that perturbing hidden representations could provide a complementary view of coherence, particularly for hallucination detection.
}

%dropout... 

%In this work, we focus on a critical yet previously overlooked aspect of model generation—the source of randomness. The source of randomness drives diversity across generations and enables the effective assessment of model uncertainty. A key limitation of current methods is their reliance on sampling at the prediction layer as the sole source of randomness, as 

%Gives rise to simple method 
%shown in Figure~\ref{fig:overview}. 
%The sampling process introduces variability across generation by selecting the next token from a probability distribution.  However, by limiting randomness to the final stage of LLM computation, prior methods overlook how the model behaves when randomness is introduced earlier. Since different layers of LLMs serve distinct functions, ignoring randomness in the intermediate layers restricts our understanding of model uncertainty. Therefore, incorporating randomness throughout earlier stages of LLM computation can provide a more comprehensive view of the relation between model uncertainty and hallucination.
%In this work, we delve into a critical yet previously overlooked aspect of this line of work—the source of randomness. 
%The source of randomness drives diversity across generations and enables the effective assessment of model uncertainty.
%One key limitation of current methods is that they solely rely on the sampling at prediction layer for randomness, as illustrated in Figure~\ref{fig:overview}. 
%Such sampling process introduces randomness to the final stage of LLM computation by selecting the next token from a probability distribution, which creates diversity across generations.
%However, by focusing solely on the final step of the LLM's computation, these methods overlook the model’s behavior when randomness is introduced in earlier stages. 
%Since different layers in the model perform distinct functions, neglecting randomness in intermediate layers can limit our understanding of model uncertainty. 
%Therefore, incorporating randomness at various stages can provide a more comprehensive and nuanced view of uncertainty, which is crucial for enhancing hallucination detection.  %\lliu{[Buffer up this part]}. 


To this end, we study model behavior under randomness introduced in earlier stages of LLM computation. 
Particularly, we inject noise to perturb intermediate layer representations, as illustrated in Figure~\ref{fig:overview}. 
Under noise perturbation, we hypothesize that a model would exhibit higher uncertainty when hallucinating, consistent with the relationship between model uncertainty and hallucination found in prior research.
We empirically validate the hypothesis in Figure~\ref{fig:scheme-demo}~(a),
\rebuttal{where hallucination cases (grey) show higher variance under noise injection, reflected by higher entropy.}
%We first hypothesize and validate that under this standalone source of randomness, model response exhibits greater variability when hallucinating, as shown 
%This aligns with the relation between model uncertainty and hallucination revealed by prior research. 
Additionally, we examine the interplay between intermediate layer noise injection and the prediction layer sampling. 
Since two sources of randomness operate at different layers, we hypothesize and validate that they have complementary effects on the model uncertainty, as shown in Figure~\ref{fig:complementary}.
Based on our observation, we propose combining intermediate layer noise injection with prediction layer sampling to enhance hallucination detection. 
We empirically validate that this combination improves the separation between hallucination and non-hallucination instances in terms of model uncertainty in Figure~\ref{fig:scheme-demo}~(b). 
Extensive experiments demonstrate the effectiveness of noise injection in enhancing hallucination detection across various datasets, uncertainty metrics, and model architectures \rebuttal{such as \texttt{Llama2-7B-chat}, \texttt{Llama2-13B-chat}, and  \texttt{Mistral}}.

%We extensively demonstrate through experiments that noise injection enhances hallucination 
%\lliu{blablabla...} 

%Further, we observe in Figure~\ref{fig:scheme-demo}~\textit{Right} that combining intermediate layer noise injection with prediction layer sampling improves hallucination detection over using prediction layer sampling alone.
%Based on our observation, we propose a meta-algorithm that systematically improves current techniques. 
%Through our extensive experiments, we demonstrate blablablas... 
% \lliu{give intuition on duplication effect here. }

%Inspired by model uncertainty, 
%We first hypothesize and validate that model responses are more robust to noise when the model is not hallucinating. 
%As model uncertainty is 
%We inject noise into intermediate layer outputs. 
%We first observe that blabla is effective...
%We then observe that the effect is orthogonal....

\begin{figure}[t]
\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=\textwidth]{scheme-demo.pdf}
\caption{
\textbf{Effect of Intermediate Layer Randomness on Hallucination Detection.}
\textit{(a) Standalone Effect.} With noise injected to randomly perturb intermediate representations, LLM exhibits greater uncertainty when hallucination (grey) compared to non-hallucination (blue); 
\textit{(b) Combined Effect.} Injecting noise improves hallucination/non-hallucination separation, enhancing hallucination detection effectiveness. 
\textit{(b) Left:}  prediction layer sampling alone; 
\textit{(b) Right:}  noise injection and prediction layer sampling. 
Model uncertainty measured by Equation~\ref{eq:answer_entropy}. 
A higher value indicates a higher uncertainty level. 
Evaluation performed on GSM8K dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

\begin{comment}
We summarize our contribution as follows:
\begin{itemize}
    %\item \lliu{(extension of scope)} We extend hallucination detection beyond QA tasks to include reasoning tasks, and we propose answer entropy as an effective metric for evaluating uncertainty in these scenarios. 
     %pinpoint an underexplored area in hallucination detection and 
    % \item \textbf{Observation:} We introduce a new source of randomness by injecting noise to perturb intermediate representations.
    % Our observations show that model uncertainty from this new source signals hallucinations and complements the model uncertainty under the classical randomness source—prediction layer sampling.
    % \item \textbf{Algorithm:} Based on our observation, we propose a simple yet effective scheme, combining both sources of randomness for hallucination detection. 
    % \item \textbf{Experiments: } We extend hallucination detection beyond classical question-answer tasks to include reasoning tasks. Our extensive experiments \lliu{blablabla.... }
    \item \textbf{Hypothesis:} We propose a hypothesis to improve hallucination detection in LLMs through adding a new source of randomness by injecting noise to perturb intermediate representations.
    \item \textbf{Experiments: } We validate our hypothesis through extensive experiments. We extend hallucination detection beyond classical question-answer tasks to include reasoning tasks. Our observations show that model uncertainty from this new source signals hallucinations and complements the model uncertainty under the classical randomness source—prediction layer sampling.
    \item \textbf{Algorithm:} Based on the hypothesis and the observations, we propose a simple yet effective scheme, combining both sources of randomness for hallucination detection. 
    
\end{itemize} 
\end{comment}



\section{Problem Statement}
\label{sec:problem_statement}
%Hallucination detection is connected to model uncertainty. 
Prior work \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside} connects hallucination detection to model uncertainty estimation. 
Given an uncertainty metric $E(\cdot)$, detecting whether the model is hallucinating for a given input context $\bm{x}$ can be framed as a binary classification problem: 
\[ D(\bm{x}) = \begin{cases} \text{Non-Hallucination} & \text{if } E(\mathcal{Y}) < \tau \\ \text{Hallucination} & \text{if } E(\mathcal{Y}) \geq \tau \end{cases}, \]
where $\tau$ is the threshold and \( \mathcal{Y} = \{\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}\) denotes $K$ generations for the given input context.  
A higher level of uncertainty indicates model hallucination. 

\paragraph{Uncertainty Metric}
One critical aspect of hallucination detection is the design of uncertainty metrics $E(\cdot)$ over generations \( \mathcal{Y} \). 
A commonly used metric is \textit{Entropy}, computed from the sequence joint distribution:
\begin{equation}\label{eq:entropy}
    E_{raw}(\mathcal{Y}) = - \mathbb{E}_{\bm{y} \in \mathcal{Y}} \sum_{t=1}^{T} \log p(y_t \mid y_{<t}, \bm{x})
\end{equation}
However, entropy can be biased against longer sequences due to smaller joint probabilities. 
To address this, \cite{malinin2020uncertainty} proposes \textit{Length Normalized Entropy}: 
\begin{equation}\label{eq:ln_entropy}
    E_{normalized}(\mathcal{Y}) = - \mathbb{E}_{\bm{y} \in \mathcal{Y}} \frac{1}{T_{\bm{y}}} \sum_{t=1}^{T} \log p(y_t \mid y_{<t}, \bm{x})
\end{equation}

% Lexical Similarity is an uncertainty metric used to gauge how similar text samples are \citep{lin2022towards, lin2023generating}. It specifically calculates the average Rouge-L score across a set of sampled answers for a particular question as follows:
% \begin{equation}\label{eq:lexical_sim}
%     E_{lexical}(\mathcal{Y}) = \frac{1}{C} \sum_{i=1}^{K} \sum_{j=i+1}^{K} RougeL(\bm{y}_i, \bm{y}_j), C=K*(K-1)/2
% \end{equation}

For reasoning tasks, we also consider an uncertainty metric focused on the answer space, as detailed in Section~\ref{sec:case_setup}. 
The metric targets the final answer rather than intermediate tokens, making it particularly well-suited for reasoning tasks with lengthy intermediate steps.


%Therefore, effective hallucination detection through model uncertainty estimation relies on two key elements: the uncertainty metric and the source of randomness, which we discuss in the following. 
%Additionally, to be compatible with reasoning tasks, we propose to measure uncertainty directly in the answer space, emphasizing the uncertainty of the final answer rather than the intermediate reasoning tokens. 
%Details of the metric in Section~\lliu{[ref]}. 
%This emphasizes the uncertainty of the final answer rather than, which is particularly meaningful for reasoning tasks.
\begin{comment}
In addition to the two metrics, we also consider an intuitive alternative: measuring uncertainty directly in the answer space.
This approach is particularly meaningful for reasoning tasks where the answers can be lengthy, rendering the previous metrics less effective.
Specifically, we separate the response sequence into two parts: the reasoning and the answer, i.e. \(\bm{y} = [\bm{r}, \bm{a}]\). 
The reasoning part can sometimes be empty, as in the case of TriviaQA, while the answer is treated as a complete string. 
We then calculate \textbf{Answer Entropy} the uncertainty by counting the number of occurrences of each answer string, with examples provided in Table~\lliu{[add]}. \lliu{Should this be a contribution? }
\begin{equation}\label{eq:answer_entropy}
E_{answer}(\mathcal{Y}) = -\sum_j p(\bm{a}_j)\log p(\bm{a}_j) 
\end{equation}
\end{comment}

\paragraph{Source of Randomness}

To effectively quantify model uncertainty requires not only an uncertainty metric $E(\cdot)$ but also a sufficiently diverse set of generations $\mathcal{Y}$, necessitating the introduction of randomness during generation.
Prior work typically introduces randomness only at the final prediction stage by sampling from the next token distribution \( p(y_t \mid y_{<t}, \bm{x})\). 
In addition, we introduce randomness at earlier stages. 


Consider a typical LLM consisting of an embedding layer, a stack of $L$ transformer layers, and a prediction layer $W$. 
At each decoding step \( t \), intermediate representations \( \bm{h}_t^l \) are computed layer by layer for a given input \( \bm{x} \). 
\rebuttal{ 
The next token probability \( p(y_t \mid y_{<t}, \bm{x}) \) explicitly conditioned on \( \bm{h}_t^L \) (and \( \bm{h}_t^{L-1} \) via skip connections) but is implicitly affected by earlier layers, as they shape these final representations.
This relationship can be expressed as:
}
%And the next token probability is computed as: 
\begin{equation}
p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^L).     
\end{equation}
%Prior work typically introduces randomness only at the final prediction stage by sampling from the next token distribution \( p(y_t \mid y_{<t}, \bm{x})\). 
We inject noise to perturb the intermediate representation at layers $l_1$ through $l_2$. 
As a result, \rebuttal{given noise $\epsilon$,} the next token distribution is stochastically modified as 
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}, \rebuttal{\epsilon}) = f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L ), \]
where each $\bm{\Tilde{h}}_t^l$ is a noise-perturbed version of $\bm{h}_t^l$. 
\rebuttal{Notably, for \( l' > l^1 \), $\bm{h}_t^l$ is computed from the perturbed representations of prior layers.
%For all subsequent layers $l' > l_2$, the representations implicitly depend on the injected noise due to the perturbation of prior layers propagating forward during computation. 
With noise sampled from $g(\epsilon)$ and randomized across generations, sampling from \(\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \epsilon)\) at each generation combines randomness at the prediction and intermediate layer. %:
%With noise sampled from $g(\epsilon)$ and randomized across generations, the resulting marginal next-token distribution is given by
%\begin{equation}
%\Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_\epsilon \Tilde{p}(y_t \mid y_{<t}, \bm{x}, \epsilon)g(\epsilon).    
%\end{equation}
} 
%Sampling from \(\Tilde{p}(y_t \mid y_{<t}, \bm{x})\) combines randomness at the prediction and intermediate layer; whereas sampling from \(p(y_t \mid y_{<t}, \bm{x})\) only introduces randomness at the prediction layer. 
%While the randomness of \(p(y_t \mid y_{<t}, \bm{x})\) can be further adjusted by temperature, 


\begin{comment}
Consider a typical LLM consisting of an embedding layer, a stack of $L$ transformer layers, and a prediction layer $W$. 
At each decoding step \( t \), intermediate representations \( \bm{h}_t^l \) are computed layer by layer for a given input \( \bm{x} \). 
The next token probability \( p(y_t \mid y_{<t}, \bm{x}) \) directly depends on \( \bm{h}_t^L \) (and \( \bm{h}_t^{L-1} \) via skip connections) but is indirectly influenced by earlier layers, as they shape these final representations.
In other word, 
\[ p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^L). \]  

We inject noise to perturb the intermediate representation at layers $l_1$ through $l_2$. 
As a result, given a sampled noise $n$, the next token distribution is stochastically modified as 
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}, n) = f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}(n), \dots, \bm{\Tilde{h}}_t^{l_2}(n), \dots,\bm{h}_t^L(n)), \]
where each $\bm{\Tilde{h}}_t^l(n)$ is a noise-perturbed version of $\bm{h}_t^l$ given $n$.
For all subsequent layers $l' > l^2$, the representations implicitly depend on injected noise, due to the perturbation of prior layers propagating forward during computation. 
With noise sampled from $g(n)$, and randomized across generations, the resulting marginal next-token distribution is given by
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_n \Tilde{p}(y_t \mid y_{<t}, \bm{x}, n)g(n), \]
Sampling from \(\Tilde{p}(y_t \mid y_{<t}, x)\) combines randomness at the prediction and intermediate layer.
In contrast, sampling from \(p(y_t \mid y_{<t}), x\) only introduces randomness at the prediction layer. 
%\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_n f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L | n )g(n), \]
\end{comment}



%We remark that this approach overlooks model uncertainty under potential sources of randomness earlier in the model's computations.
%Specifically, we consider a typical LLM consisting of an embedding layer, a stack of $L$ transformer layers, and a prediction layer $W$. 
%Hidden states \( \bm{h}_t^l \) are computed at each decoding step \( t \) within each transformer layer \( l \) for given input context $\bm{x}$.
%And the next token probability is predicted as: 
%\[ p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^L). \]
%Instead of sampling from $p(y_t \mid y_{<t}, \bm{x})$, we inject noise in intermediate layers, stochastically modifying the next token distribution:
%\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = f(\bm{\Tilde{h}}_t^1, \dots, \bm{\Tilde{h}}_t^L ), \]
%where each $\bm{\Tilde{h}}_t^l$ is a noise-perturbed version of $\bm{h}_t^l$. 
%\lliu{In the following, we demonstrate that this not only works as a standalone method but also can enhance the sampling-based randomness blabals}

%Before the final prediction stage, the model computes hidden states \( \bm{h}_t^l \) at each decoding step \( t \) within each transformer layer \( l \) for given input context $\bm{x}$. 
%At the prediction stage, the hidden states are used for autoregressively predicting the probability of the next token:
%\[ p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^L). \]


%The multiple responses are typically sampled with non-zero temperature \lliu{[cite]}, which introduces randomness to the output of the final prediction layer $W$. 




%For example, \lliu{add typical choice of $E(\cdot)$ includes Entropy, Length Normalized Entropy, Lexical Similarity. }


\section{Intermediate Layer Randomness and Hallucination Detection}\label{sec:case_study}

In this section, we conduct a case study to investigate LLM behavior under intermediate layer randomness. 
We first hypothesize and validate that, with noise injected to modify intermediate layer representations, model responses exhibit greater variability when the model hallucinates. 
We then observe that intermediate layer noise injection has a complementary effect on model uncertainty compared to prediction layer sampling. 
Based on our observations, we propose to combine noise injection with prediction layer sampling to enhance hallucination detection. 
%amplify the uncertainty difference between hallucination and non-hallucination cases, thereby improving the effectiveness of hallucination detection.
%\lliu{TODO: replace plots.}


%noise + temperature. 

%model behavior under the randomness of non-zero sampling temperature 

\subsection{Case Study Setup}\label{sec:case_setup}

We focus this case study on mathematical reasoning tasks using the GSM8K \citep{cobbe2021training} dataset. 
\rebuttal{We experiment with the GSM8K test set, containing 1319 questions, using in-context learning examples from \cite{wei2022chain}.}
\rebuttal{As shown in Table~\ref{tab:demo}, following in-context learning examples, LLM can produce coherent yet incorrect answers—i.e., hallucinations—highlighting the need for effective hallucination detection in such reasoning tasks.}
This extends beyond prior work on hallucination detection \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}, which primarily focuses on question-and-answer tasks such as TriviaQA \citep{joshi2017triviaqa}. % and CoQA \citep{reddy2019coqa} \lliu{[to change]}. 
Section~\ref{sec:experiments} demonstrates that our algorithm also generalizes to knowledge-based question-and-answer tasks.

GSM8K consists of mathematical question-response pairs \(\{\bm{x}, \bm{y}\}\), where each response includes both the reasoning and the answer: \(\bm{y} = [\bm{r}, \bm{a}]\).
As shown in Table~\ref{tab:demo}, the reasoning chains for GSM8K can be lengthy, yet the final answer is more critical. 
Therefore, treating all tokens equally in uncertainty estimation, as in Equations~\ref{eq:entropy} and \ref{eq:ln_entropy}, can be less effective. 
To address this, we estimate uncertainty by counting the occurrences of each answer string and introduce the metric of \textit{Answer Entropy}: 
\begin{equation}\label{eq:answer_entropy}
E_{answer}(\mathcal{Y}) = -\sum_j p(\bm{a}_j)\log p(\bm{a}_j) 
\end{equation}
where \( p(\bm{a}_j) \) is the empirical probability of each unique answer \(\bm{a}_j\) over the \(K\) final answers $\{ \bm{a}^1, \bm{a}^2, \dots, \bm{a}^K \}$ extracted from \(K\) responses $\mathcal{Y} = \{ \bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}$.
An example of answer entropy computation is provided in Table~\ref{tab:demo}.

\begin{table}[t]
%\vspace{-2mm}
\caption{
\textbf{Example of Answer Entropy Computation on GSM8K dataset.}
For each response, the answer string is marked in \textbf{bold}, with the remaining text representing the reasoning part. 
We estimate uncertainty by counting the occurrence of each answer string.
In this example, with $K = 3$ responses, $E_{answer}(\mathcal{Y}) = - 0.67\times\log 0.67 - 0.33 \times \log 0.33$. 
%\lliu{[to change to table]}
}\label{tab:demo}
%\vspace{-1mm}
\begin{center}
\includegraphics[width=\textwidth]{demo_answer_entropy.pdf}
\end{center}
\vspace{-2mm}
\end{table}

\begin{comment}
\begin{table}[]
\begin{tabular}{l|l|c}
\hline
\begin{tabular}[c]{@{}l@{}}Question: A robe takes 2 bolts of blue fiber and half\\ that much white fiber. How many bolts in total does it take?\end{tabular}    & Answer & \multicolumn{1}{l}{Answer Frequency} \\ \hline
\begin{tabular}[c]{@{}l@{}}Response 1: Half of 2 bolts of white fiber is 2/2 = 1 bolt. So, it takes 2 + 1 = 3 bolts\\ in total. The answer is 3.\end{tabular}   & 3      & \multirow{2}{*}{67\%}                \\ \cline{1-2}
\begin{tabular}[c]{@{}l@{}}Response 2: 2 bolts of blue fiber and half that much white fiber is 2 + half of 2 = 2 +\\ 1 = 3 bolts. The answer is 3.\end{tabular} & 3      &                                      \\ \hline
\begin{tabular}[c]{@{}l@{}}Response 3: 2 bolts of blue fiber and half that much white fiber is 2 x 2 = 4 bolts of\\ blue fiber. The answer is 4.\end{tabular}   & 4      & 22\%                                 \\ \hline
\end{tabular}
\end{table}  
\end{comment}


Our case study focuses on the \texttt{Llama2-13B-chat} model, where uniform noise sampled from $U(0, 0.05)$ to additively perturb the MLP layer outputs of \(25 - 40\) transformer layers. 
We follow the default generation configuration with $\text{top-k} = 50$ and $\text{top-p} = 1$.
%For each question, we assess model uncertainty across $K = 30$ generations. 
When prediction layer sampling is enabled, we set temperature as $T = 0.8$, which optimizes GSM8K accuracy within the set $T = \{0.2, 0.5, 0.8, 1.0\}$. % as shown in Appendix~\lliu{[add]}. 
%Detailed experimental setup is provided in Appendix~\lliu{[add]}. 
Experiments involving alternative datasets, uncertainty functions, models, injection layers, and noise types are discussed in Section~\ref{sec:experiments}. 
% temperature: when non-zero, is set to be 0.8. 

\begin{comment}
In addition to the two metrics, we also consider an intuitive alternative: measuring uncertainty directly in the answer space.
This approach is particularly meaningful for reasoning tasks where the answers can be lengthy, rendering the previous metrics less effective.
Specifically, we separate the response sequence into two parts: the reasoning and the answer, i.e. \(\bm{y} = [\bm{r}, \bm{a}]\). 
The reasoning part can sometimes be empty, as in the case of TriviaQA, while the answer is treated as a complete string. 
We then calculate \textbf{Answer Entropy} the uncertainty by counting the number of occurrences of each answer string, with examples provided in Table~\lliu{[add]}. \lliu{Should this be a contribution? }
\begin{equation}\label{eq:answer_entropy}
E_{answer}(\mathcal{Y}) = -\sum_j p(\bm{a}_j)\log p(\bm{a}_j) 
\end{equation}
where \( p(\bm{a}_j) \) represents the empirical probability of each unique answer \(\bm{a}_j\) over the \(K\) final answers $\{ \bm{a}^1, \bm{a}^2, \dots, \bm{a}^K \}$ extracted from \(K\) responses $\mathcal{Y} = \{ \bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}$.


We measure model uncertainty by computing the entropy of the final answers.
Specifically, we define the uncertainty function as: 
\begin{equation}\label{eq:answer_entropy}
E(\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K ) = -\sum_j p(\bm{a}_j)\log p(\bm{a}_j) 
\end{equation}
where \( p(\bm{a}_j) \) represents the empirical probability of each unique answer \(\bm{a}_j\) over the \(K\) final answers $\{ \bm{a}^1, \bm{a}^2, \dots, \bm{a}^K \}$ extracted from \(K\) responses $\{ \bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}$.
\end{comment}
%The reasoning part can sometimes be empty, as in the case of TriviaQA, while the answer is treated as a complete string.


%We compute the entropy of the final answer \(a\) across \(K\) runs to evaluate the impact of noise injection on model responses.
%To study the effect of noise injection on hallucination detection, we use the GSM8K \lliu{[cite]} dataset as a case study, which consists of mathematical question-answer pairs. 
%To study the effect of noise injection in hallucination detection, we experiment with the GSM8K \lliu{[cite]} dataset as a case study, which consists of mathematical question-answer pairs.
%inject additive uniform noise to mlp_output of layer \lliu{[cite]}
%We inject additive uniform noise to mlp_output of layer \lliu{[cite]}, 
%GSM8K dataset is a 
%For the rest of the section, all experiments are on \texttt{Llama2-13B-chat} model. 

\subsection{Hallucination Increases Response Variability under Noise Injection}\label{sec:standalone}
% Effect of Noise Injection on Response Variability: Hallucination vs. Non-Hallucination

In this study, we investigate how LLMs behave under noise injection in intermediate layers as the sole source of randomness. 
%We compare model behavior during hallucination versus non-hallucination. 
Given that prior research indicates model uncertainty increases during hallucination, we hypothesize that the model's response will exhibit greater variability when hallucinating.
%how under noise injection in intermediate layers as the sole source of randomness, how the model behaves when hallucinating and not hallucinating. 
%model hallucination affects response variability under noise injection in intermediate layers.
%Since prior research indicates that hallucination increases model uncertainty under non-zero inference temperature, we hypothesize that hallucination will similarly lead to greater response variability when noise is injected.
To validate our hypothesis, at each decoding step, we perturbed the MLP output of \( 25-40 \) transformer layers as $\bm{\Tilde{h}}_t^l = \bm{h}_t^l + \epsilon$, with $\epsilon$ is uniformly sampled from \( U(0, 0.05) \). 
The next token prediction is thus stochastically modified at each generation as \(\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \epsilon) = f(\bm{h}_t^1, \dots, \bm{h}_t^{24}, \bm{\Tilde{h}}_t^{25}, \dots, \bm{\Tilde{h}}_t^{40} )\). 
To isolate the effect of noise injection, we set the sampling temperature to zero and greedily select the next token with the largest likelihood, removing randomness from the prediction layer sampling process. 

To assess model uncertainty under the noise injection, we generate $K = 5$ responses for each question and compute answer entropy following~\ref{eq:answer_entropy}. 
We classify model hallucination on a question level and model responses to a question are considered as hallucinating if the majority of the \( K = 5 \) generated answers are incorrect, and as non-hallucinating otherwise. 
In Figure~\ref{fig:scheme-demo}~\textit{Left}, we compare answer entropy between hallucinating and non-hallucinating cases by overlaying the histograms of the two groups.
We observe that the model exhibits greater variability under noise when hallucinating (grey), as evidenced by higher entropy values. 
This observation matches our intuition: less variability implies the robustness of the model response to noise, suggesting greater certainty and a lower likelihood of hallucination.
%We perform \( K = 50 \) runs and classify questions into hallucinating (incorrect answers) and non-hallucinating (correct answers) groups based on the majority vote. 
%We then compute model uncertainty per question using answer entropy as defined in Equation~\ref{eq:answer_entropy}. 
%In Figure~\ref{fig:scheme-demo}~\textit{Left}, we compare model uncertainty when hallucinating and non-hallucinating by overlaying the histogram of the two groups under the metric of answer entropy. 

%Figure X: \textbf{Hallucination increases response variability under noise injection} (Llama2-13B-chat model). 
%The table reports the histogram of answer entropy for both hallucination and non-hallucination groups. 
%\lliu{The noise is sampled from a uniform distribution $[0, a]$, with the noise magnitude $a$ noted in the table.}

%We determine for each question the model correctness based on the majority vote of final answers. 



%We consider the aggregated model performance across $K$ runs. 



%We did 50 runs for each question and determined model correctness based on the majority vote. 


%a higher percentage of questions have answers that remain unchanged when the model is not hallucinating.






\begin{comment}
Based on the correctness of answers under standard greedy decoding with zero noise, we identify two groups: questions where the model is hallucinating (incorrect answers) and questions where it is not (correct answers).
For each group, we report the percentage of questions where the model answer remains unchanged under noise injection, i.e. the ratio of unchanged answers to the total number of questions per group.
As shown in Table~\lliu{[add]}, a higher percentage of questions have answers remain unchanged when the model is not hallucinating.

Table 1: \textbf{Hallucination increases response variability under noise injection} (Llama2-13B-chat model). 
The table reports the percentage of questions where answers remain unchanged under noise injection for both hallucination and non-hallucination groups. 
The noise is sampled from a uniform distribution $[0, a]$, with the noise magnitude $a$ noted in the table.
\end{comment}

%exhibits less variability under noise when the model is not hallucinating, as indicated by a larger percentage. 


%Based on answer correctness under standard greedy decoding (zero noise), we consider two groups: model hallucinating v.s. model not hallucinating. 
%As we increase the magnitude of noise injection, 

%For each question, we consider the model to be hallucinating if the answer under standard greedy decoding (zero noise) is correct; otherwise, it is classified as incorrect. 
%And we increase the noise magnitude and record weather model answer to the question changes. 
%we experiment with GSM8K \lliu{[cite]} as a case study, which consists of mathematical question-answer pairs. 
%For each question, we consider the model as hallucinating if the model answer to the question under zero noise, i.e., standard greedy decoding, is correct; and vice versa. 
%Questions are grouped into two classes: those with correct answers and those with incorrect answers. 
%And we report the percentage of questions whose answers remain the same under noise injection.
%In Table \lliu{add}, we observe that as we increase the noise magnitude, questions within the correct answer set tend to stay more same in comparison to the incorrect answer set. 

%\lliu{Reminder to explain noise magnitude somewhere.}



%We experiment with \texttt{Llama2-13B-chat} and set the sampling temperature to zero to solely focus on the effect of sampling noise. 
%In Table~\lliu{[add]}, we inject noise at 

%compare the response variability under noise injection when the model is hallucinating or not. 
%We consider the model as hallucinating when the answer of greedy 



%We consider the model response to a question as a hallucination if the answer from the majority vote across multiple runs is incorrect; and vice versa. 


%For the rest of the section, all experiments are on \texttt{Llama2-13B-chat} model. 
%We also set the generation temperature to zero to study the standalone effect of noise injection. 


%GSM8K consists of mathematical question-answer pairs, where the final final answer is reasoning. 


%We experiment with GSM8K \lliu{[cite]} as a case study to understand how hallucination model response 


%Percentage of questions where the model answer remains unchanged under noise injection reported. 
%Questions are grouped into two classes: those with correct answers and those with incorrect answers. The table reports the percentage of questions whose answers remain the same under noise injection.

%Questions are grouped into two classes: those with correct answers and those with incorrect answers. The table reports the percentage of questions whose answers remain the same under noise injection. As expected, answers to correctly answered questions are harder to change, indicating greater certainty about the question and reducing the likelihood that the model is making random guesses or hallucinating.


\subsection{Complementary Effect of Noise Injection and Prediction Layer Sampling}\label{sec:combine}

We now extend our investigation beyond a single source of randomness. 
Particularly, we study the interplay between noise injection and the standard source of randomness -- prediction layer sampling. 
Since the two sources of randomness operate at different layers with distinctive roles in model prediction, we hypothesize that they would have complementary effects on model uncertainty. 

\rebuttal{
This hypothesis is theoretically grounded in the distinct impacts of each randomness source:
prediction layer sampling preserves token likelihood ordering for any temperature.
In contrast, noise injection perturbs intermediate representations, potentially reversing token orderings.
These distinct mechanisms operate at different stages, suggesting complementary effects on model uncertainty. 
}

To test our hypothesis, we compare model uncertainty under two sources of randomness.

\paragraph{Intermediate Layer Noise Injection:} We follow the setup outlined in Section~\ref{sec:standalone}, injecting noise sampled from \(U(0, 0.05)\) and setting  the temperature to zero. 

\begin{wrapfigure}{r}{0.49\textwidth}
\vspace{-1mm}
\centering
\includegraphics[width=0.46\textwidth]{complementary_effect.png}
\vspace{-3mm}
\caption{\textbf{Complementary Effect of Different Randomness Sources}.
The x-axis presents model uncertainty with prediction layer sampling whereas the y-axis presents model uncertainty under intermediate layer noise injection. 
A Pearson correlation of 0.67 indicates a complementary relationship between the two sources.
%\lliu{change axis, $\epsilon$}
}\label{fig:complementary}
\vspace{-4mm}
\end{wrapfigure}


\paragraph{Prediction Layer Sampling:} We do not perturb model computation; instead we sample with temperature $T = 0.8$ from the unmodified next token probability \( p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^{40})\). 
The non-zero temperature introduces sampling randomness at the prediction layer, with \( T = 0.8 \) selected to maximize model accuracy.

For each setup, we assess model uncertainty across $K =50$ generations for each question following Equation~\ref{eq:answer_entropy}. 
We then compare the model uncertainty under two sources of randomness, as illustrated in Figure~\ref{fig:complementary}. 
The scatter plot displays each question of the GSM8K test set as a point, with the x-value representing model uncertainty under prediction layer sampling alone, whereas the y-value represents model uncertainty under intermediate layer noise injection. 
The plot reveals that model uncertainty under the two sources of randomness is related but not identical, with a Pearson correlation \citep{sedgwick2012pearson} of 0.67.
This indicates a positive correlation but also highlights the complementary effects between the two randomness sources.
We further validate the complementary effect in Section~\ref{sec:ablation_tempt}
% X-axis: Answer Entropy (Temperature = 0.5, No Noise)
% Y-axis: Answer Entropy (Noise Injection: $U(0, 0.05)$, Temperature = 0) 



\subsection{Algorithm: Noise Injection as a Hallucination Detection Amplifier}

\begin{table}[t]
\vspace{-2mm}
\caption{
\textbf{Case Study: Effectiveness of Noise Injection for Enhancing Hallucination Detection.}
Noise injection (first row) improves detection effectiveness compared to no noise (second row), as indicated by a higher AUROC, without degrading model accuracy.
Evaluation on GSM8K dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:case_study}
\vspace{-2mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{case_study.pdf}
\end{center}
\vspace{-2mm}
\end{table}

To leverage the complementary effect of different sources of randomness revealed in Section~\ref{sec:combine}, we incorporate noise injection alongside prediction layer sampling and propose our Noise Enhanced Hallucination Detector. 
The design is illustrated with additive uniform noise in Algorithm~\ref{alg:NED}. 
%We illustrate our design with additive uniform noise in Algorithm~\ref{alg:NED}. 

Specifically, for a given noise magnitude $\alpha$ and a set of layers $l^1$ through $l^2$, we inject additive uniform noise \( \bm{\epsilon} \sim U(0, \alpha)^d\) to the MLP output of the selected layers, where $d$ is the model dimension.
At each decoding step, the selected layers are perturbed as $\bm{\Tilde{h}}_t^l = \bm{h}_t^l + \epsilon$, \rebuttal{where $\bm{h}_t^l$ with \( l' > l^1 \) is computed from the perturbed representations of prior layers.}
This perturbation stochastically modifies the next token probability as 
\(\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \rebuttal{\epsilon}) = f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L )\).
Across generations, we sample noise $\epsilon$ independently and draw samples from the temperature-adjusted distribution \(\Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \epsilon)\) with temperature \(T\).  
\rebuttal{
Effectively, our sampling process integrates over noise and follows the marginal distribution
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_\epsilon \Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \epsilon)g(\epsilon), \]
where $g(\epsilon)$ is the probability density function of $U(0, \alpha)^d$.
}
By perturbing the intermediate layer outputs and sampling with a non-zero temperature at the final layer, our approach effectively combines two complementary sources of randomness.
To identify hallucinations, we compute the hallucination detection score over $K$ generations and apply a threshold to classify outputs.
%While the unselected layers \( l' > l^2 \) remain noise-free, their representations implicitly incorporate the noise due to the perturbations propagating from earlier layers. 
%We then sample from this modified distribution with temperature $T$ to collect $K$ generations and apply thresholding on the hallucination detection score to identify hallucinations.
%We hypothesize that injecting noise into intermediate layers will amplify the difference in model uncertainty between hallucinated and non-hallucinated cases. 
%This approach serves as a meta-algorithm, using noise injection as the hallucination detection amplifier to enhance detection accuracy.
%For additive uniform noise in our case study, 

\begin{algorithm}
\caption{Noise Enhanced Hallucination Detection}\label{alg:NED}
\begin{algorithmic}[1]
\INPUT Input context: \( \bm{x} \), noise magnitude \( \alpha \), number of generations \( K \), sampling temperature \( T\), perturbed layers \( l_1 \) to \( l_2 \), uncertainty metric \(E(\cdot)\). 
  \OUTPUT Hallucination detection score: \( s(\bm{x}) \)
\FOR{each generation \( k = 1 \) to \( K \)}
    \STATE Sample noise \( \bm{\epsilon} \sim U(0, \alpha)^d \) 
    \FOR{each decoding step \( t \)}
        \FOR{each layer {l}}
            \STATE \rebuttal{Compute $\bm{h}^l$ using the potentially perturbed prior layer representations.}
            \STATE Perturb the MLP outputs: \( \bm{\Tilde{h}}^l = \bm{h}^l + \bm{\epsilon} \) if \( l \in [l_1, l_2] \). 
        \ENDFOR    
        \STATE Modify next token probability:
            \[\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \epsilon) = f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L )\]
        \STATE Sample token \( y_t \) from \( \Tilde{p}(y_t \mid y_{<t}, \bm{x}, \epsilon)\) with temperature T, append it to generation \( \bm{y}^k\). 
    \ENDFOR
\ENDFOR
\RETURN Hallucination detection score \( s(\bm{x}) = E(\mathcal{Y} ) \), where \( \mathcal{Y} = \{\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}\)
\end{algorithmic}
\end{algorithm}



In Table~\ref{tab:case_study}, we validate the effectiveness of our scheme under the case study setup.
 We perturb the MLP outputs of layers 25 to 40 with additive uniform noise of magnitude \(\alpha = 0.05\), sampled from \(U(0, 0.05)\), and evaluate over \(K = 5\) generations. 
In practice, the noise magnitude can be selected based on the validation set, and we present an ablation study on different noise magnitudes in Section~\ref{sec:ablation_tempt}.
Following established literature \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}., we assess the effectiveness of hallucination detection using the threshold-free metric, the area under the receiver operating characteristic curve (AUROC), where a higher value indicates better detection performance.
As shown in Table~\ref{tab:case_study}, our scheme effectively detects hallucination instances with AUROC value $> 50$. 

We further compare our scheme with prior schemes which solely rely on prediction layer sampling without noise injection during model computation. 
The setup of the noiseless scheme follows Section~\ref{sec:combine}.
As shown in Table~\ref{tab:case_study}, our scheme with noise injection significantly improves detection effectiveness and achieves a higher AUROC value. 
Additionally, this performance enhancement is visualized in Figure~\ref{fig:scheme-demo}~\textit{(b)}, where noise injection increases the separation and reduces the overlap in the histograms from left to right.
%We observe that noise injection significantly improves detection effectiveness. 
%Specifically, we compare the model behavior across $K = 50$ runs with temperature 0.8 and noise 0 against the behavior with temperature 0.8 and noise sampled from \(U(0, 0.05)\). 
%We assess the effectiveness of hallucination detection using the area under the receiver operating characteristic curve (AUROC), where a higher AUROC indicates better detection performance. 


%We also visualize the amplification effect of noise injection in hallucination detection in Figure~\ref{fig:scheme-demo}~\textit{(b)}, where noise injection enhances the histogram separation from left to right. 
Further, we evaluate model accuracy on the GSM8K dataset based on majority vote, both with and without noise injection.
As shown in Table~\ref{tab:case_study}, noise injection can boost model accuracy. 
%\rebuttal{This supports our intuition that incorrect answers -- hallucination cases -- are less robust to noise injection, as indicated by their higher entropy, which reflects greater variability.
%This increased variability makes incorrect answers less likely to persist across generations, reducing their chances of being selected in a majority vote process."
%}
This supports our intuition that incorrect answers \rebuttal{produced during hallucination} are less robust to noise injection \rebuttal{, as indicated by higher entropy.}
\rebuttal{Consequently, the consistency of incorrect answers across generations reduces with noise injected}, making them less likely to be selected by majority vote. 
\rebuttal{This shift improves the likelihood of correct answers being chosen, thereby enhancing accuracy under the majority vote scheme. }
%\rebuttal{across generations; explicitly say reducing consistency.}
%This makes incorrect answers less likely to duplicate in generations and be selected by majority vote.


%when the model provides correct, non-hallucinated responses, its answers remain robust to noise, leaving accuracy unaffected.




%noise injection does not negatively affect model accuracy.
%In other words, the model does not exhibit an increased tendency to hallucinate under moderate noise injection 
%This aligns with our intuition that when the model provides correct, non-hallucinated responses, its answers remain robust to noise, leaving accuracy unaffected.

%In addition to hallucination detection effectiveness, we also test if the model tends to hallucinate more under noise injection by evaluating the model accuracy from majority vote across runs on the GSM8K dataset. 
%As shown in Table~\ref{tab:case_study}, noise injection has little effect on model accuracy.




%To validate our hypothesis, we 
%In Figure~\ref{fig:scheme-demo}~\textit{(b)}, we compare the model behavior across $K = 50$ runs with temperature 0.5 and noise 0 (left) against the behavior with temperature 0.5 and noise sampled from \(U(0, 0.05)\) (right). 
%We quantify the separation with the Area Under the ROC Curve (AUROC), where higher AUROC indicates better hallucination detection. 
%With no noise, the AUROC is 78.51, while with noise, AUROC improves to 81.31. 
%Notable, noise injection has little effect on model accuracy. 
%With and without noise, the majority vote yields accuracies of \lliu{[add]} versus \lliu{[add]}. 
%This aligns with our intuition that when the model answers correctly and does not hallucinate, its response remains robust to noise, leaving accuracy unaffected.

%Based on the observation, we propose a meta-algorithm by using noise injection as a hallucination detection amplifier. \lliu{[how to select hyperparameters?]}


%\lliu{[Add theorem]}


%\lliu{remark on the meaning of AUROC and its implication for hallucination detection.}




\section{Experiments}\label{sec:experiments}

In this section, we move beyond the case study and extensively validate the effectiveness of our algorithm across different datasets, uncertainty metrics, and model architectures. 
Further, we conduct a comprehensive ablation study to understand the effect of the number of generations, injection layers, sampling temperature, and noise magnitude.  

\subsection{Generalizability across Diverse Datasets and Uncertainty Metrics}\label{sec:ablation_datasets}
In addition to mathematical reasoning tasks, we validate our hypothesis on question-and-answer datasets including TriviaQA \citep{joshi2017triviaqa}, CSQA \citep{talmor-etal-2019-commonsenseqa}, and ProntoQA \citep{SaparovHe2023}. 
For TriviaQA, we utilize the validation portion of the \texttt{rc.nocontext} subset, which contains $9,960$ unique questions. The \texttt{rc.nocontext} subset of TriviaQA is designed for question-answering tasks without providing additional context from the source documents. For CSQA, we use the validation set containing $1,221$ questions related to commonsense world knowledge in a multiple-choice format. Following the methodology of \cite{wei2022chain}, we include their hand-written $7$-shot chain-of-thought exemplars for evaluation. PrOntoQA is a synthetic question-answering dataset comprised of procedurally-generated symbolic world models and reasoning chains to resolve the truthfulness of a claim. We extract the generated questions and ground truth reasoning chains for the $1$-Hop fictional subset from their provided model outputs, totaling $400$ question-answer pairs. 
%Answer formats are free-form, multiple-choice, and True or False for TriviaQA, CSQA, and PrOntoQA, respectively.

\begin{table}[t]
\vspace{-2mm}
\caption{
\textbf{Intermediate Layers Noise Injection Enhances Hallucination Detection across Diverse Datasets and Uncertainty Metrics.}
Hallucination detection AUROC reported, the higher the better. 
Noise magnitude fixed as $\alpha = 0.05$ based on GSM8K performance. 
Evaluation with \texttt{Llama2-13B-chat} model across 5 generations. 
}\label{tab:main}
\vspace{-2mm}
\begin{center}
\includegraphics[width=\textwidth]{main_dataset.pdf}
\end{center}
\vspace{-2mm}
\end{table}


\begin{figure}[t]
\centering
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{gsm8k_13B_evolution_auroc.png}
    \caption{AUROC}
    \label{fig:auroc}
\end{subfigure}
%\hspace{7mm}
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{gsm8k_13B_evolution_acc.png}
    \caption{ACC}
    \label{fig:acc}
\end{subfigure}
\vspace{-0.25cm}
\caption{\textbf{Noise Injection Enhances Hallucination Detection without Degrading Model Accuracy Across Different Number of Generations. }
Evaluation with GSM8K datasets on \texttt{Llama2-13B-chat} model across 1 
 - 20 generations.
Hallucination detection AUROC (a) and model accuracy (b) reported; higher values are better. 
The mean and standard deviation across random seeds are shown in the plot. }
\label{fig:gsm8k-13b-evolution}
%\vspace{-4mm}
\end{figure}


For each dataset, we select the temperature within $T = \{ 0.2, 0.5, 0.8, 1.0\}$ which optimizes the model accuracy on this dataset. 
For GSM8K, TriviaQA, CSQA, and ProntoQA, the temperature is set to be $0.8$, $0.2$, $0.8$, and $0.8$, respectively. 
We follow the setup of Section~\ref{sec:case_setup} and select the noise magnitude as $\alpha = 0.05$ based on GSM8K performance. 
We remark that $\alpha = 0.05$ is not the optimal noise magnitude for each dataset and performance can be further boosted through hyperparameter search, as demonstrated in Appendix~\ref{appendix:noise-ablation}. 
%Our goal here is to demonstrate feasibility rather than performance optimization.
For each dataset, we evaluate with uncertainty metrics: Predictive Entropy (see Equation~\ref{eq:entropy}), Normalized Predictive Entropy (see Equation~\ref{eq:ln_entropy}), and Answer Entropy (see Equation~\ref{eq:answer_entropy}). 
%We also report the accuracy with and without noise injection. 
%We report the average behavior across \lliu{[to Sunny]: Explain how average and std are taken.} here and report the standard deviation in Appendix~\lliu{[add]}. 
Looking into Table~\ref{tab:main}, noise injection is most effective on GSM8K with answer entropy, as expected since it is the optimized metric. 
However, our method remains effective across most datasets and metrics, validating that noise injection generally enhances model performance across various uncertainty metrics.


%. Explain how average and std are taken

%To display: bar plot of accuracy and auroc, showing gain per dataset per uncertainty measure from noise injection.; ideally with error bars. 

%Take-away: generalizability. fixed noise magnitude. 

%\lliu{[To Sunny]: briefly introduce CSQA and PrOntoQA here. Explain how average and std are taken}

%Additional datasets: CommonSense QA; Truthful QA; MMLU;  PrOntoQA. 


\begin{table}[t]
\vspace{-2mm}
\caption{
\textbf{Ablation on Temperature and Noise Magnitude.}
Noise injection (right two columns) improves detection effectiveness compared to no noise (left column), as indicated by a higher AUROC.
Evaluation on GSM8K dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_noise}
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.99\textwidth]{ablation_temp_noise.pdf}
\end{center}
\vspace{-4mm}
\end{table}

\begin{table}[t]
\vspace{-2mm}
\caption{
\textbf{Noise injection across all layers enhances performance}, with the upper layer demonstrating the greatest effectiveness.
AUROC and ACC reported. 
The higher the values, the better. 
Evaluation on GSM8K dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_layers}
\vspace{-3mm}
\begin{center}
\includegraphics[width=\textwidth]{ablation_layers.pdf}
\end{center}
\vspace{-8mm}
\end{table}


\subsection{Ablation on Number of Generations}\label{sec:abalation_runs}

%\lliu{TODO: unify with the bar plot in 4.1; worst case scenario, only report the error bar here. }

So far, we have presented results based on $K = 5$ generations in Section~\ref{sec:case_study} and Section~\ref{sec:ablation_datasets}.
We now extend this study to explore the effect of noise injection across different numbers of generations. 
In Figure~\ref{fig:gsm8k-13b-evolution}, we present the hallucination detection AUROC (left) and model accuracy on GSM8K (right) for $K = 1$ to $K = 20$ generations.
The rest of the setup follows Section~\ref{sec:case_setup}. 
For each $K$, we report the mean and standard deviation across 20 groups of $K$ runs. 
As shown in Figure~\ref{fig:gsm8k-13b-evolution}, both hallucination detection AUROC and model accuracy on GSM8K improve with an increasing number of generations.  
Notably, noise injection consistently enhances the effectiveness of hallucination detection across different numbers of generations without degrading model accuracy. 
In practice, the number of generations can be adjusted based on the computational budget and accuracy requirements. 
Nevertheless, our experiments demonstrate that noise injection improves hallucination detection effectiveness, regardless of the specific number of generations used.




  
%In Section~\ref{sec:case_study} and Section~\ref{sec:ablation_datasets}, 
%AUROC and ACC across runs from 1 to 50;
%Take-away: both ACC and AUROC improve as the number of runs improves; the exact runs can be chosen in practice w.r.t. computation r

\subsection{Ablation on Sampling Temperature and Noise Magnitude}\label{sec:ablation_tempt}

In Section~\ref{sec:ablation_datasets}, we select the temperature temperature per dataset based on model accuracy and set the noise magnitude to 0.05. 
Table~\ref{tab:ablation_noise}, further explores the effect of varying sampling temperature and noise magnitude. 
The rest of the experiment setup follows Section~\ref{sec:case_setup}. 
As shown in Table~\ref{tab:ablation_noise}, while the optimal noise magnitude varies with temperature, moderate noise injection generally enhances hallucination detection.
%\rebuttal{Additional experiments with CSQA in Appendix~\ref{appendix:noise-temp-ablation} further validate this finding.}
Additionally, the table highlights the complementary effects of noise and temperature. 
As randomness increases from $T = 0.8$ to $T = 1.0$ without noise, hallucination detection AUROC drops.
Yet injecting noise at $T = 0.8$, adds a different source of randomness and improves performance.



%To display: \lliu{[To Sunny] plots with a line for each temperature, x-axis is alpha/y-axis is auroc. if the plot looks good, otherwise Table. } 

%Takeaway: For all sampling temperatures, there exists a noise magnitude that improves the performance. Some magnitude is reasonably good as shown in 4.1.  The best noise magnitude needs to be selected through validation. 

\vspace{-2mm}
\subsection{Ablation on Noise Injection Layers}

We now investigate the effect of noise injection on different layers across the LLAMA-13B architecture, which has 40 layers in total. 
In addition to the upper layers noise (25 - 40 layers) injection, we studied so far, we experiment with middle layers (15 - 25 layers) and lower layers (0 - 15 layers) noise injection.
In Table~\ref{tab:ablation_layers}, we report the hallucination detection AUROC with noise injected on different layers. 
The noise magnitude is set to 0.05, 0.02, 0.01 for upper layers, middle layers, and lower layers, respectively, each achieving the optimal performance across noise injection level $\{0.01, 0.02, 0.03, 0.04, 0.05 \}$ for the corresponding layers. 
As we observe from Table~\ref{tab:ablation_layers}, while noise injection enhances hallucination across layers, upper-layer injection is the most effective. 
This may be because upper layers tolerate more noise without disrupting generation, reflected by the higher optimal noise magnitude. 
In contrast, lower layers have less tolerance due to error propagation.
%One reason is that upper layers can tolerate more noise without completely disrupting the generation.
%This aligns with prior work showing that the upper layer of neural networks captures more abstract concept. 
%Moreover, the optimal noise for the upper layer is larger,  likely because errors in lower layers can propagate through the network.


\subsection{Ablation on Alternative Architectures}


We extend our case study beyond the \texttt{Llama2-13B-chat} model, experimenting with the \texttt{Llama2-7B-chat} from the same Llama family and the \texttt{Mistral-7B} model \citep{jiang2023mistral} from a different family.
Both models have 32 layers in total, and we inject noise into layers 22 to 32 to perturb the upper layer representations.
We evaluate GSM8K, following the setup from our case study in Section~\ref{sec:case_setup}. 
As shown in Table~\ref{tab:ablation_arch}, on both architectures, noise injection improves the AUROC of hallucination detection. 
Notably, the effective noise magnitude differs: while \texttt{Llama2-7B-chat} performs well with $\alpha = 0.05$, \texttt{Mistral-7B} requires a smaller noise level of $\alpha = 0.02$, indicating the need for model-specific hyperparameter tuning.


\begin{table}[t] 
    \centering
    \caption{
    Noise injection improves hallucination detection on \texttt{Llama2-7B-chat} and \texttt{Mistral}.  
    Evaluation of GSM8K across 5 generations.
    AUROC value reported; the higher the better. 
    }
    %\vspace{2mm}
    \label{tab:ablation_arch}
    \begin{tabular}{ccc}
     \hline\
       & \texttt{Llama2-7B-chat} & \texttt{Mistral} \\ [0.5ex] 
     \hline
     No Noise           & 75.09 &  77.03 \\ 
     \hline
    Noise Injection  & 76.80 & 82.95 \\
     \hline
    \end{tabular}
\end{table}

%Different noise magnitude in different layers. 
%Take-away: Enhancement observed across layers while the upper layer is the most significant. The optimal noise magnitude is different. 
%No error bar for now. GSM8K first 5 runs. 

\subsection{Alternative Uncertainty Metric}
In addition to the uncertainty metrics defined in Section~\ref{sec:problem_statement}, we investigate other metrics including Lexical Similarity \citep{lin2022towards, lin2023generating} and Semantic Entropy \cite{semanticentropy}.
Lexical Similarity is an uncertainty metric used to gauge how similar text samples are. It specifically calculates the average Rouge-L score across a set of sampled answers \( \mathcal{Y} = \{\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}\) for a given context $\bm{x}$ as $\frac{1}{C} \sum_{i=1}^{K} \sum_{j=i+1}^{K} RougeL(\bm{y}^i, \bm{y}^j)$ where $C=K*(K-1)/2$. 
Semantic entropy combines the uncertainties of individual tokens within groups of similar meanings. To calculate it, first, the generated outputs are grouped into clusters that share the same semantic meaning. Then, the semantic entropy is determined by summing up the uncertainties within each cluster.

Among the datasets analyzed, only TriviaQA is appropriately suited for evaluating Lexical Similarity and Semantic Entropy. The True/False format of ProntoQA and the multiple-choice format of CSQA are not conducive to Rouge-L measurement. Similarly, the numerical answers in GSM8K are incompatible with the clustering required for Semantic Entropy analysis. Conversely, the short, free-form answers in TriviaQA make it an ideal candidate for both metrics.

In Table~\ref{tab:ablation_metric}, we present the AUROC numbers for Lexical Similarity and Semantic Entropy on TriviaQA, evaluated at a temperature of 0.2 and noise magnitudes of $\alpha=0$ and $\alpha=0.05$. The data clearly indicate that both uncertainty metrics show improvement following the introduction of noise.

\begin{table}[h]
    \centering
    \caption{Noise Injection Enhances Hallucination Detection under Lexical Similarity and Semantic Entropy. Evaluation on TriviaQA dataset with \texttt{Llama2-13B-chat} model across 5 generations.}
    \vspace{-1mm}
    \label{tab:ablation_metric}
    \begin{tabular}{ccc}
     \hline\
       & Lexical Similarity & Semantic Entropy \\ [0.5ex] 
     \hline
     Noise = 0           & 64.74 & 63.62 \\ 
     \hline
     Noise \textasciitilde{} U (0,0.05)  & \textbf{66.59} & \textbf{65.51} \\
     \hline
    \end{tabular}
    \vspace{-2mm}
\end{table}









%\subsection{Ablation on Temperature}


%\subsection{Ablation on Type of Noise / Noise Magnitude. } 

%\lliu{TO-RUN}: 

%a. Gaussain/Uniform.

%b. Multiplicative -- if there's time. 



%\subsection{Sanity Check: Higher Precision Model}

%a. unquantized model. 

%b. pyvene mention and request. 




\section{Related Work}


Several recent works have demonstrated a strong correlation between model uncertainty and the likelihood of hallucination. Measures of model uncertainty include  the entropy of answer \citep{malinin2021uncertainty}, semantic \citep{kuhn2023semantic, chen2024inside, farquhar2024detecting}, predictive \citep{xiao2021hallucination}, and lexical \citep{lin2022towards, lin2023generating} distributions. These methods rely on a diverse set of model generations which primarily used temperature-based sampling techniques. Our work is complementary to these approaches and introduces an additional source of randomness.

% Activations / hidden space
In addition to entropy-based estimates, intermediate model activations have been shown to provide insights into model confidence. \cite{chuang2023dola} demonstrates that the divergence in activations between correct and incorrect tokens tends to increase across layers, with contrasted activations growing sharper for correct tokens. 
%\cite{chen2024incontext}
Additionally, \cite{li2024inference} shows that hidden embeddings encode an LLM's sense of ``truthfulness'', which may be steered along a vector of truth through test-time intervention. 
% Self-reported
Self-reported confidence as explored by \cite{manakul-etal-2023-selfcheckgpt} and \cite{kadavath2022language} is a promising direction but requires the model to be well-calibrated and can suffer out-of-distribution. 




\section{Conclusion}
Our study highlights the critical issue of hallucinations in Large Language Models (LLMs) and the importance of detecting these instances for safe deployment. We have established a link between hallucinations and model uncertainty, noting that existing methods primarily focus on next-token sampling as the sole source of randomness. Our investigation into the effects of injecting noise into the hidden states of intermediate layers reveals that introducing randomness at earlier stages of computation has a complementary impact on model uncertainty. By combining both intermediate layer randomness and prediction layer sampling, we propose an enhanced approach for hallucination detection. Extensive experiments validate the effectiveness of this combined scheme, demonstrating its potential to improve the reliability of LLMs.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage 

\appendix

%\section{Generalizability across Diverse Datasets and Uncertainty Metrics }

\section{Ablation of Noise Magnitude on Alternative Datasets}
\label{appendix:noise-ablation}

In Section~\ref{sec:ablation_datasets}, we select $\alpha = 0.05$ for all datasets based on GSM8K performance and demonstrate that this choice remains effective across other datasets. 
However, the optimal noise magnitude varies by dataset. 
In Table~\ref{tab:ablation_prontoqa}, we ablate the noise magnitude for PrOntoQA and find that $\alpha = 0.08$ outperforms $\alpha = 0.05$. 
While our approach in Section~\ref{sec:ablation_datasets} simplifies hyperparameter selection by choosing a single noise magnitude per model, these results suggest that tuning noise magnitude for each dataset individually could improve performance, albeit at the cost of additional computation.

\begin{table}[h]
\vspace{-2mm}
\caption{
\textbf{Ablation of Noise Magnitude on ProntoQA.}
Noise level $\alpha = 0.08$ further improves detection effectiveness compared to $\alpha = 0.05$, as indicated by a higher AUROC.
Evaluation with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_prontoqa}
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.55\textwidth]{prontoqa.pdf}
\end{center}
\vspace{-2mm}
\end{table}

\begin{comment}
\rebuttal{
\section{Ablation of Sampling Temperature on Alternative Datasets}
\label{appendix:noise-temp-ablation}

In Table~\ref{tab:ablation_noise_csqa}, we extend our study in Table~\ref{tab:ablation_noise} to the CSQA dataset. 
We observe that noise injection improves hallucination detection across the temperatures. 
}

\begin{table}[h]
\vspace{-2mm}
\caption{
\textbf{Ablation on Temperature and Noise Magnitude.}
Noise injection (right two columns) improves detection effectiveness compared to no noise (left column), as indicated by a higher AUROC.
Evaluation on CSQA dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_noise_csqa}
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.99\textwidth]{ablation_temp_noise_csqa.pdf}
\end{center}
\vspace{-4mm}
\end{table}
\end{comment}




\section{Implementation Details}

\subsection{Datasets}\label{app:datasets}

We use in-context examples to demonstrate correct answer formatting and simplify answer extraction following free-form rationales, where applicable.
For \textbf{GSM8K} and \textbf{CSQA}, we use the same prompts as in \cite{wei2022chain}. For \textbf{PrOntoQA}, \cite{SaparovHe2023} generate a unique set of examples for each question. We extract the pre-generated prompts from the distributed model outputs. For \textbf{TriviaQA}, we ensemble a 10-shot prompt from the first 10 training examples of the format:

\texttt{Q: Which Oscar-nominated film had You Sexy Thing as its theme
song? A: The Full Monty Q: Which Joan’s career revived in
Whatever Happened to Baby Jane? A: Crawford Q: Which much-loved
actor won the Best Actor Oscar for The Philadelphia Story? A:
James Stewart (...) Q: In which river is the Boulder Dam? A:}

To address instances where the model maintains the Q:...A:... format after delivering an answer, we trim all generations using pattern matching with a set of stopwords identified in the outputs. 
In evaluation, when the model fails to produce the result in format, we consider the answer as invalid. 

\subsection{Models}


All models evaluated in this work are off-the-shelf with no additional fine-tuning. 
Perturbation on the model is implemented using \texttt{pyvene} \citep{wu-etal-2024-pyvene}. 
We run all of our experiments on 80GB NVIDIA A100s.
And there is no noticeable latency overhead with or without noise injection, confirming that our method introduces no practical delay.

\section{Analysis on Noise Sampling Mechanism}

Each layer sampling v.s. Sample once and apply on all layers.

Deliberate design to ensure noise does not cancel in the computation. 

\section{Analysis on Noise Injection Positions}

So far, we have injected noise into the MLP layer output of selected transformer layers, demonstrating that this perturbation improves hallucination detection. 
We now extend our study to noise injection at an alternative transformer component—the attention block output (see Figure~\ref{fig:overview}).
In Table~\ref{tab:ablation_position}, we show that noise injection at the attention layer output has a similar effect to MLP layer output, both enhancing detection effectiveness. The rest of the setup follows Section~\ref{sec:case_setup}.
This aligns with our expectations: due to skipping connections (see Figure~\ref{fig:overview}), adding noise to either the MLP or attention output similarly affects the overall computational flow. 
These results highlight the robustness of our algorithm against different injection positions.


\begin{table}[h]
\vspace{-2mm}
\caption{
\textbf{Analysis on Noise Injection Positions.}
Noise injection at the attention output (third row) performs comparably to injection at the MLP output (second row), both improving detection effectiveness compared to no noise (first row), as indicated by higher AUROC scores. 
Meanwhile, the model accuracy slightly improves.
Evaluation on GSM8K dataset with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_position}
\vspace{-2mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{ablation_position.pdf}
\end{center}
\vspace{-2mm}
\end{table}


\section{Visualization of Hallucination/Non-Hallucination Separation}

In Figure~\ref{fig:scheme-demo}, we visualize the enhancement of hallucination/non-hallucination separation with the number of generations $K = 5$. 
In the following, we visualize the same for $K = 10,  15,  20$. 
Across all visualizations, we observe that injecting noise enhances the separation between hallucination and non-hallucination instances, improving the effectiveness of detection. 

\begin{figure}[h]
%\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{10_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-13B-chat} model across \textbf{10} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

\begin{figure}[h]
%\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{15_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-13B-chat} model across \textbf{15} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

\begin{figure}[h]
\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{20_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-13B-chat} model across \textbf{20} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

%\section{Alternative of Majority Vote}



%For 
%\lliu{[To Sunny]} Briefly describe the icl example construction for CSQA and ProToQA.




 

%\section{Alternative Accuracy Measure}

%\lliu{[To Sunny]} If we use TriviaQA, explain your clustered voting scheme. Display results of exact match = {true; false}; clustering vote = {true; false}

%\section{Discussion on Accuracy}

%\lliu{Pending on more generation of GSM8K}


\end{document}
