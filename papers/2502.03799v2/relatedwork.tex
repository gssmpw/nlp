\section{Related Work}
\myparagraph{Bayesian Neural Networks.} Standard neural networks typically learn only a single point estimate, neglecting epistemic and aleatoric uncertainty. Bayesian methods \citep{mackay1992practical,neal2012bayesian} learn a posterior distribution over models to capture epistemic and aleatoric uncertainty, but at a high computational cost. \citet{gal2016dropout} addressed this issue using variational inference with a Bernoulli approximation of the weight distribution, subsequently extended to CNNs by \citet{Gal2016Bayesian}. In case of LLMs, \citet{HouLQAC024} contend that Bayesian methods are not compatible with LLMs due to computational costs and thus aim to quantify epistemic and aleatoric uncertainty using clarification questions. Here, we deal with the challenge posed by computational costs with a novel Bayesian training-free approach using noise injection.


\myparagraph{Hallucination Detection.}
Several recent works have demonstrated a strong correlation between model uncertainty and the likelihood of hallucination. Measures of model uncertainty include  the entropy of answer \citep{malinin2021uncertainty}, semantic \citep{kuhn2023semantic, chen2024inside, farquhar2024detecting}, predictive \citep{xiao2021hallucination}, and lexical \citep{lin2022towards, lin2023generating} distributions. These methods rely on a diverse set of model samples generated by temperature-based sampling, which primarily captures aleatoric uncertainty. Our work is complementary to these approaches and introduces epistemic uncertainty.

% Activations / hidden space
In addition to entropy-based estimates, intermediate model activations have been shown to provide insights into model confidence. \citet{chuang2023dola} demonstrates that the divergence in activations between correct and incorrect tokens increases across layers, with contrasted activations growing sharper for correct tokens. 
%\cite{chen2024incontext}
Additionally, \citet{li2024inference} shows that hidden embeddings encode an LLM's sense of ``truthfulness'', which may be steered along a vector of truth through test-time intervention. 
% Self-reported
Self-reported confidence as explored by \citet{manakul-etal-2023-selfcheckgpt} and \citet{kadavath2022language} is a promising direction but requires the model to be well-calibrated and can be sensitive to distribution shifts. %can suffer out-of-distribution.