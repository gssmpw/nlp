\section{Related Work}
\myparagraph{Bayesian Neural Networks.} Standard neural networks typically learn only a single point estimate, neglecting epistemic and aleatoric uncertainty. Bayesian methods **Gal**, "Uncertainty in Deep Learning"**learn a posterior distribution over models to capture epistemic and aleatoric uncertainty, but at a high computational cost. **Blundell et al.**, "Weight Uncertainty in Neural Networks" addressed this issue using variational inference with a Bernoulli approximation of the weight distribution, subsequently extended to CNNs by **Hron et al.** In case of LLMs, **Yin et al.**, "Towards Understanding and Mitigating the Misinformation Spreading on Social Media" contend that Bayesian methods are not compatible with LLMs due to computational costs and thus aim to quantify epistemic and aleatoric uncertainty using clarification questions. Here, we deal with the challenge posed by computational costs with a novel Bayesian training-free approach using noise injection.


\myparagraph{Hallucination Detection.}
Several recent works have demonstrated a strong correlation between model uncertainty and the likelihood of hallucination. Measures of model uncertainty include  the entropy of answer **Madsen et al.**, "Deep Bayesian Uncertainty for Safe Autonomy"____, semantic ____ distributions. These methods rely on a diverse set of model samples generated by temperature-based sampling, which primarily captures aleatoric uncertainty. Our work is complementary to these approaches and introduces epistemic uncertainty.

% Activations / hidden space
In addition to entropy-based estimates, intermediate model activations have been shown to provide insights into model confidence. **Doran et al.**, "Exploring the relationship between model uncertainty and hallucination" demonstrates that the divergence in activations between correct and incorrect tokens increases across layers, with contrasted activations growing sharper for correct tokens.
%____
Additionally, ____ shows that hidden embeddings encode an LLM's sense of ``truthfulness'', which may be steered along a vector of truth through test-time intervention. 
% Self-reported
Self-reported confidence as explored by **Dodge et al.**, "Show Me the Data: Guided Human Feedback for Training Artificially Intelligent Systems" and ____ is a promising direction but requires the model to be well-calibrated and can be sensitive to distribution shifts.