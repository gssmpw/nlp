%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{amsmath,bm}
\usepackage{tabularx}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithmic}
\newcommand{\RETURN}{\item[\textbf{return}]}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{soul}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\lliu}[1]{{\color{brown}{#1}}}
\newcommand{\sunny}[1]{\textbf{\color{teal}[SP: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\apratim}[1]{\textbf{\color{red}[AB: #1]}}
\newcommand{\myparagraph}[1]{\vspace{0.0em}\noindent\textbf{#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Enhancing Hallucination Detection through Noise Injection}

\begin{document}

\twocolumn[
\icmltitle{Enhancing Hallucination Detection through Noise Injection}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
%\icmlauthor{Firstname1 Lastname1}{equal,yyy}
%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Litian Liu}{comp}
\icmlauthor{Reza Pourreza}{comp}
\icmlauthor{Sunny Panchal}{comp}
\icmlauthor{Apratim Bhattacharyya}{comp}
\icmlauthor{Yao Qin}{sch}
\icmlauthor{Roland Memisevic}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
\icmlaffiliation{comp}{Qualcomm AI Research}
\icmlaffiliation{sch}{UC Santa Barbara}

%\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Litian Liu}{litiliu@qti.qualcomm.com}
\icmlcorrespondingauthor{Roland Memisevic}{rmemisev@qti.qualcomm.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations.
Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs.
Recent research has linked hallucinations to model uncertainty, 
suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from a model. 
While drawing from the distribution over tokens defined by the model is a 
natural way to obtain samples, in this work, we argue that it is 
sub-optimal for the purpose of detecting hallucinations. 
We show that detection can be improved significantly by taking 
into account model uncertainty in the Bayesian sense. 
To this end, we propose a very simple and efficient approach that perturbs an appropriate subset of model parameters, or equivalently 
hidden unit activations, during sampling. 
We demonstrate its effectiveness across a wide range of datasets
and model architectures.
%Motivated by this viewpoint, we perform an extensive empirical analysis showing that an alternative way to measure uncertainty - by perturbing hidden unit activations in intermediate layers of the model - is complementary to sampling, and can significantly improve detection accuracy over mere sampling. 
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have made significant advancements in recent years \citep{achiam2023gpt, zhao2023survey}. 
However, despite the strides, LLMs sometimes generate plausible yet incorrect responses -- a phenomenon known as hallucination \citep{ji2023survey, kuhn2023semantic}.
To ensure the safe deployment of LLMs, effective detection of hallucination is essential, and 
has gained significant attention in recent years \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}.
Many research efforts focus on detecting hallucinations by assessing uncertainty across samples drawn from the model. %, measured by the divergence in outputs produced by the model for the same input. 
For example, \citet{malinin2020uncertainty} propose leveraging predictive uncertainty for hallucination detection. Similarly, \citet{lin2022towards} and \citet{lin2023generating} propose semantic consistency and quantify lexical similarity across samples.
%Specifically, \lliu{[cite]} groups multiple generations into semantic categories and measure uncertainty within the semantic space; whereas \lliu{[cite]} quantifies uncertainty from the embedding of multiple generations. 
The core principle underlying this line of work is simple: the greater the observed uncertainty associated with a prediction, the higher the likelihood of hallucination. %across multiple generations

Since a language model defines the probability distribution over the next tokens, an obvious way to generate samples is to repeatedly draw from the conditional distribution over tokens given the context so far. This way of sampling stays faithful to the probability distribution defined by the model (up to any deviations from the training temperature), and it makes sense when the goal is to
generate multiple answers, say, to a given prompt.

\begin{figure}
%\vspace{-8mm}
\centering
\includegraphics[width=0.40\textwidth]{overview_v3.pdf}
\vspace{-0.5cm}
\caption{Prior work relies solely on prediction layer sampling for hallucination detection, capturing mainly aleatoric uncertainty. 
We introduce noise injection to perturb intermediate representations. 
Combining noise injection with prediction layer sampling, our approach captures both epistemic and aleatoric uncertainty.
%\textbf{Uncertainty for Hallucination detection}.
%\lliu{need to re-draw for copy-right issue. add prior/us in plot}
}\label{fig:overview}
%\vspace{-8mm}
\end{figure}



\begin{figure*}[t]
\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=\textwidth]{scheme_demo_v2.pdf}
\vspace{-0.9cm}
\caption{
\textbf{Effect of Intermediate Layer Noise on Hallucination Detection.}
\textit{(a) Standalone Effect.} Noise injection induces epistemic uncertainty, where the LLM shows greater uncertainty for hallucinations (grey) than non-hallucinations (blue), as reflected by larger answer entropy (Equation~\ref{eq:answer_entropy}).
\textit{(b) Combined Effect.} Combining noise injection with prediction layer sampling \textit{(b Right)} improves hallucination/non-hallucination separation compared to using sampling alone \textit{(b Left)}, enhancing detection effectiveness.
%Compared to prediction layer sampling alone \textit{(b Left)}, combining noise injection with sampling \textit{(b Right)} improves hallucination/non-hallucination separation, enhancing detection effectiveness.
This highlights the importance of combining epistemic uncertainty with aleatoric uncertainty captured by sampling for hallucination detection. 
%\textit{(b) Left:}  prediction layer sampling alone; 
%\textit{(b) Right:} noise injection and prediction layer sampling. 
%Model uncertainty measured by Equation~\ref{eq:answer_entropy}. 
%A higher value indicates a higher uncertainty level. 
Evaluation on GSM8K dataset with \texttt{Llama-2-7B-chat} model across 10 samples. 
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure*}

However, in the case of hallucination detection, the purpose of sampling is \emph{not} to generate multiple different alternatives as answers to a given prompt. 
Instead, it is to estimate the coherence of a model’s responses to the prompt, via a 
kind of ``sensitivity analysis'' that makes it possible to assess the likelihood of a given prompt 
to elicit a hallucination in a model. 
A distribution of responses that is coherent under perturbations is considered as evidence for the model knowing the correct response for a given prompt and, accordingly, for an answer generated by the model to be considered truthful.

%To measure the uncertainty associated with a prediction, a popular approach is to sample from the conditional distribution over tokens given the context predicted by the model \textbf{[CITE]}. 
More formally, sampling from the model using next-token prediction can be considered as a way to 
capture uncertainty in the data distribution, 
%, also referred to as \emph{aleatoric uncertainty} \cite{osband2016risk}. 
%Risk in this context refers to aleatoric or observation uncertainty, the volatility inherent in the data, i.e., when there are multiple plausible completions of a certain prompt or multiple possible answers to a question. 
whereas to detect hallucinations, we are also interested in the model uncertainty, 
which is the result of training on a finite training set. 
The distinction between these two types of uncertainty has been studied formally by 
\citet{osband2016risk}, who refers to the first (data uncertainty) as \emph{aleatoric} and the latter 
(model uncertainty) as \emph{epistemic}. 

This distinction is also reflected in a Bayesian perspective, where uncertainty over 
the model parameters reflects the epistemic and the model's output distribution the 
aleatoric uncertainty. 
%when the model does not know what should be the correct completion of a certain prompt or the correct answer to a question.
%\apratim{To more accurately capture uncertainty associated with model predictions, we adopt the established Bayesian framework which systematically captures both model (epistemic) and observation (aleatoric) uncertainty \cite{kendall2017uncertainties}.
%Such approaches capture model uncertainty by modeling the distribution of plausible models given the training data.
However, a full Bayesian treatment is challenging for LLMs, which contain billions of parameters and are trained on datasets containing billions, and sometimes 
trillions, of tokens \citep{HouLQAC024}. 
In this work, we devise a novel, simple but effective \emph{training-free} approach to obtain a surrogate distribution over models that are plausible given the training data, using pre-trained model weights 
as a starting point. Our approach is illustrated in Figure~\ref{fig:overview}.

%\st{It is commonly assumed in language modeling that hidden unit activations tend to capture the more abstract and high-level representations of a given phrase or thought, while logits and low-level token embeddings capture representations that reduce it to a specific syntactic form. This suggests that, even though it is tempting to rely on sampling from the model to assess coherence for a given prompt, a better way to assess coherence should involve perturbations of these hidden representations. Unlike sampling, which preserves the order of tokens with respect to likelihood regardless of the temperature, the perturbation of hidden representations does not. Perturbing hidden representations can therefore be complementary for the purpose of measuring coherence.}
%%These distinct impacts suggest that perturbing hidden representations could provide a complementary view of coherence, particularly for hallucination detection.

To approximate the distribution over plausible models, we consider random perturbations of the parameters of a pre-trained model, which, as we show, is equivalent to perturbing hidden unit activations in some layers of the LLM for an appropriately chosen subset of parameters. 
Conveniently, the hidden activations also tend to capture the more abstract and high-level representations of a given phrase or ``thought''. 
This differentiates them from the output logits, 
which represent meaning at a much lower, syntactic level, potentially making 
stability of hidden activations a better candidate to assess a model's faithfulness 
to the prompt in the context of detecting hallucinations.  

Concretely, our surrogate distribution is uniformly distributed and centered at the pre-trained parameter weights of the hidden units and whose variance is defined by a single hyper-parameter. 
This allows models in the surrogate distribution to explain the training data well, while possessing sufficiently high coverage of plausible models in order to capture key aspects of the 
additional model uncertainty. 
This is illustrated in \cref{fig:scheme-demo}, where we show the uncertainty associated with a prediction in the case of a hallucination, highlighting the effectiveness of jointly capturing epistemic and aleatoric uncertainty in a Bayesian framework.

In this work, we show how our insight gives rise to a novel, simple and efficient, yet effective approach to incorporate 
model uncertainty into hallucination detection, and we demonstrate its effectiveness empirically 
across a wide range of datasets and model 
architectures (\texttt{Gemma-2B-it}, \texttt{Phi-3-mini-4k-instruct}, \texttt{Mistral-7B-Instruct}, \texttt{Llama-2-7B-chat}, and \texttt{Llama-2-13B-chat}.). 

%Compared to using only temperature based sampling, epistemic uncertainty better reflects to higher uncertainty demonstrated by the model.

%To this end, we study model behavior under randomness introduced in earlier stages of LLM computation. Particularly, we inject noise to perturb intermediate layer representations, as illustrated in Figure~\ref{fig:overview}. Under noise perturbation, we hypothesize that a model would exhibit higher uncertainty when hallucinating, consistent with the relationship between model uncertainty and hallucination found in prior research.We empirically validate the hypothesis in Figure~\ref{fig:scheme-demo}~(a), where hallucination cases (grey) show higher variance under noise injection, reflected by higher entropy.
%%We first hypothesize and validate that under this standalone source of randomness, model response exhibits greater variability when hallucinating, as shown 
%%This aligns with the relation between model uncertainty and hallucination revealed by prior research. 
%Additionally, we examine the interplay between intermediate layer noise injection and the prediction layer sampling. 
%Since two sources of randomness operate at different layers, we hypothesize and validate that they have complementary effects on the model uncertainty, as shown in Figure~\ref{fig:complementary}. Based on our observation, we propose combining intermediate layer noise injection with prediction layer sampling to enhance hallucination detection. We empirically validate that this combination improves the separation between hallucination and non-hallucination instances in terms of model uncertainty in Figure~\ref{fig:scheme-demo}~(b). 

\begin{table*}[t]
%\vspace{-2mm}
\caption{
\textbf{Example of Answer Entropy Computation on GSM8K dataset.}
For each response, the answer string is marked in \textbf{bold}, with the remaining text representing the reasoning part. 
We estimate uncertainty by counting the occurrence of each answer string.
In this example, with $K = 3$ samples, $E_{answer}(\mathcal{Y}) = - 0.67\times\log 0.67 - 0.33 \times \log 0.33$. 
%\lliu{[to change to table]}
}\label{tab:demo}
%\vspace{-1mm}
\begin{center}
\includegraphics[width=0.96\textwidth]{demo_answer_entropy_v2.pdf}
\end{center}
\vspace{-5mm}
\end{table*}

\section{Problem Statement}
\label{sec:problem_statement}
%Hallucination detection is connected to model uncertainty. 
Prior work \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside} connects hallucination detection to model uncertainty estimation. 
Given an uncertainty metric $E(\cdot)$, detecting whether the model is hallucinating for a given input context $\text{x}$ can be formulated 
as a binary classification problem: 
\begin{align}\label{eq1}
    D(\bm{x}) = \begin{cases} \text{Non-Hallucination} & \text{if } E(\mathcal{Y}) < \tau \\ \text{Hallucination} & \text{if } E(\mathcal{Y}) \geq \tau \end{cases},
\end{align}
where $\tau$ is threshold and \( \mathcal{Y} = \{\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}\) denotes $K$ samples given the input.  
A higher level of uncertainty indicates hallucination. 


For reasoning tasks, we introduce an uncertainty metric focused on the answer space, as detailed in Section~\ref{sec:case_setup}. 
This metric targets the final answer rather than intermediate tokens, making it particularly well-suited for reasoning tasks with lengthy 
intermediate rationales.



%\subsection{Modeling Uncertainty} 
To compute the uncertainty associated with a certain generation, we propose to 
use both aleatoric (observation) and epistemic (model) uncertainty. To capture model uncertainty, instead of considering a single LLM, we consider the distribution of plausible models given the training data $\text{D}$. 
\iffalse Under this formulation, the predictive distribution of the token $\text{y}_t$ at position $t$ in the output sequence, given the previous tokens $\text{y}_{<t}$, the prompt $\text{x}$ and the training data $\text{D}$ is,
\begin{align}\label{eq2}
\begin{split}
p(\text{y}_t |\text{y}_{<t},  \text{x},  \text{D}) &= \int p(\text{y}_t, f |\text{y}_{<t},  \text{x},  \text{D}) \, df\\
%&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  f, \text{D}) p(f | \text{D}) \, df\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  f)  p(f | \text{D} )\, df
\end{split}
\end{align}
%&\approx \int p(\text{y}_t |\text{y}_{<t},  \text{x},  \omega)  q(\omega)\, d\omega\\
%Here, in the first step in \cref{eq1} 
We marginalize over plausible models $f$ and use the observation that the likelihood of token $\text{y}_t$ is independent of the training data $\text{D}$ given $f$. 
This is the classic Bayesian formulation, where the predictive distribution of token $\text{y}_t$ at position $t$ can be expressed as an integral over the posterior distribution of models $p(f | \text{D} )$ given the data $\text{D}$ and the likelihood of the token $\text{y}_t$ given the model. The posterior distribution $p(f | \text{D} )$ encodes the model uncertainty and the likelihood term $p(\text{y}_t |\text{y}_{<t},  \text{x}, f)$ given the model $f$ encodes the observation uncertainty.
\fi 
%However, the integral in \cref{eq2} is intractable. Therefore, first we constrain the set of possible model $f$ to those that can be described by a the finite set of (random) parameters $\omega$. In detail, this is equivalent to constraining the set of models $f$ to LLMs with random weights $\omega$. These weights $\omega$ can be a subset of all the weight matrices in the LLM. 
Using the classic Bayesian formulation, we can express the distribution over 
token $\textbf{y}_t$ given prompt $\textbf{x}$, previous tokens $\textbf{y}_{<t}$, and the training data $\text{D}$, as the expectation 
%Thus, \cref{eq2} can be expressed as,
\begin{align}\label{eq3}
p(\text{y}_t |\text{y}_{<t},  \text{x}, \text{D}) &= \int p(\text{y}_t |\text{y}_{<t}, \text{x}, \omega) p(\omega | \text{D})\, d\omega \,,
\end{align}
where $\omega$ denotes model parameters and $p(\omega | \textbf{D})$ the 
posterior over parameters given the training data. 
%Secondly, as we do not have access to $p(\omega | \text{D})$, we replace $p(\omega | \text{D})$ with an surrogate distribution $q(\omega)$. 
%Secondly, 

Since \( p(\omega | \text{D}) \) is not directly accessible in practice, 
it is a common practice to replace it in the integral with a surrogate 
distribution \( q(\omega) \).
One way to estimate $q(\omega)$ is to minimize the KL divergence between $q(\omega)$ and the true posterior $p(\omega | \text{D})$ using a variational lower bound on the Bayesian evidence \citep{MacKay2003,gal2016dropout}. 
However, since LLMs are very expensive to train, Bayesian treatment of LLMs for hallucination detection has remained illusive \cite{HouLQAC024}.  In this work, instead of training from scratch to estimate $q(\omega)$, we suggest using pre-trained models 
as an initial estimate for the surrogate distribution $q(\omega)$. Concretely, we define $q(\omega)$ as,
\begin{align}\label{eq4}
q(\mathbf{w}) = \prod_{i \notin \mathcal{S}} \delta(w_{i} - \bar{w}_{i}) \cdot \prod_{i \in \mathcal{S}} \mathcal{U} (w_{i} \mid \bar{w}_i, \bar{w}_i + \alpha) \,,
\end{align}
where 
$w_{i}$ represents the $i^{\text{th}}$ parameter of the model, $\bar{w_i}$ is the value of $w_{i}$ as stored in the pre-trained checkpoint. 
For parameters in $\mathcal{S}$, we perturb according to a uniform distribution $\mathcal{U(\cdot)}$ centered near the respective pre-trained parameters, with $\alpha$ the magnitude of the perturbation. 
For parameters not in $\mathcal{S}$, we fix them at their checkpoint values, effectively applying a Dirac delta distribution $\delta(\cdot)$ centered at the pre-trained. 
Our choice of $q(\omega)$ is inspired by \citet{kendall2017uncertainties}, illustrating that as the volume of training data increases, the estimated distribution parameters become narrowly concentrated.
In LLMs, we expect this effect to be amplified due to their massive data scale.

 %$i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer, 
%$\bar{\text{w}}^l_{i,j}$ is the pre-trained weight of the parameter $\text{w}^l_{i,j}$ 
%and $\mathcal{U(\cdot)}$ denotes the uniform distribution.
%$\xi^l_{i,j}$ is uniformly distributed in the range $[-\alpha,\alpha]$.  As $\bar{\text{w}}^l_{i,j}$ comes from the pre-trained weights, small perturbations around $\bar{\text{w}}^l_{i,j}$ through $\xi^l_{i,j}$ would likely yield models which explain the training dataset $\text{D}$ well. % However, the set of weights $\text{w}^l_{i,j} \in \omega$ and the noise magnitude $\alpha$ needs careful section. 

In \cref{sec:case_study}, we exemplify the effectiveness of this Bayesian view with a simple approach. 
Restricting $\mathcal{S}$ to %set of weights $\text{w}^l_{i,j} \in \omega$ to 
the bias terms in the MLP blocks allows us to equivalently implement 
the approximation by injecting noise into the MLP layer activation, i.e., to the activation $\text{h}^{l}_{i}$ of the $i^{\text{th}}$ neuron. 
Through extensive experiments in \cref{sec:experiments}, we show that even 
this lightweight ``noise injection'' approach can already greatly enhance hallucination detection accuracy.
We further show in Appendix~\ref{app:bias_weight} that perturbing the bias has a similar effect as perturbing the weights.
%The approach is also reminiscent of the well-known Laplace-approximation to the posterior. 
And we demonstrate the general effectiveness of this Bayesian perspective in Appendix~\ref{app:ablation_position} with an alternative example that perturbs a different set of model parameters.

%Appendix~\lliu{ablation}, this method clearly demonstrates the underlying principles' effectiveness.
%In Section~\ref{sec:case_study}, we exemplify the effectiveness of this Bayesian view with a simple approach 
%Specifically, we inject noise into the MLP layer output, which akin to modifying the bias term of the MLP layers. 
%Through extensive experiment, we show that this lightweight approach can already utilize both aleatoric and epistemic uncertainty, enhancing hallucination detection.
%We also ablate on the perturbation position in Appedix~\lliu[add], showing the general applicability of this Baysian view. 

%Note that, as $q(\omega)$ is defined by a single hyper-parameter$\alpha$, which can be set efficiently using limited validation data.

\begin{table*}[!t]
\vspace{-2mm}
\caption{
\textbf{Case Study: Effectiveness of Noise Injection for Enhancing Hallucination Detection.}
With the same aleatoric uncertainty fixed by sampling temperature, noise injection (first row) introduces epistemic uncertainty, improving detection effectiveness over no noise (second row), as shown by a higher AUROC.
Such improvement is achieved without degrading model accuracy.
Evaluation on GSM8K dataset with \texttt{Llama-2-7B-chat} model across 10 samples. 
%\lliu{add ours}
}\label{tab:case_study}
%\vspace{-2mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{case_study_v2.pdf}
\end{center}
\vspace{-2mm}
\end{table*}

\section{Uncertainty and Hallucination Detection}\label{sec:case_study}
In this section, we conduct a case study to investigate the effectiveness of the surrogate model distribution $q(\omega)$ described above, in capturing epistemic uncertainty.
We first hypothesize and validate that, when sampling under the model distribution $q(\omega)$ (\cref{eq3}), responses exhibit greater variability when the model hallucinates. 
We then observe that such epistemic uncertainty has a complementary effect when compared to aleatoric uncertainty for hallucination detection. 
Overall, combining epistemic and aleatoric uncertainty yields 
the best performance.
%Based on our observations, we propose to combine noise injection with prediction layer sampling to enhance hallucination detection. 
%amplify the uncertainty difference between hallucination and non-hallucination cases, thereby improving the effectiveness of hallucination detection.
%\lliu{TODO: replace plots.}


%noise + temperature. 

%model behavior under the randomness of non-zero sampling temperature 

\subsection{Case Study Setup}\label{sec:case_setup}
We perform an initial case study using the GSM8K dataset \citep{cobbe2021training}. 
%We focus this case study on mathematical reasoning tasks using the GSM8K \citep{cobbe2021training} dataset. 
%This setup extends beyond prior work on hallucination detection \citep{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}, which primarily focuses on question-and-answer tasks such as TriviaQA \citep{joshi2017triviaqa}. % and CoQA \citep{reddy2019coqa} \lliu{[to change]}. 
Section~\ref{sec:experiments} demonstrates that our algorithm also generalizes to knowledge-based question-and-answer tasks. 

In this study, we use the GSM8K test set, containing 1319 questions, along with in-context learning examples from \citet{wei2022chain}.
The dataset consists of mathematical question-response pairs \(\{\text{x}, \text{y}\}\), where each response includes both the reasoning and the answer: \(\text{y} = [\text{r}, \text{a}]\).
As shown in \cref{tab:demo}, following in-context learning examples, 
an LLM can produce coherent yet incorrect answers—i.e., hallucinations—highlighting the need for effective hallucination detection in such reasoning tasks.

For effective hallucination detection through uncertainty estimation, we need to design an effective uncertainty metric as described in \cref{eq1}. 
As illustrated in \cref{tab:demo}, reasoning chains can be extensive, although the final answer holds greater importance. Consequently, assigning equal weight to all tokens during uncertainty estimation may be suboptimal.
%, as in Equations~\ref{eq:entropy} and \ref{eq:ln_entropy}, can be less effective. 
To address this, we estimate uncertainty by counting the number of  occurrences of each answer string and introduce the metric of \textit{Answer Entropy}: 
\begin{equation}\label{eq:answer_entropy}
\mathcal{H}_{ans}(\mathcal{Y}) = -\sum_j p(\text{a}_j)\log p(\text{a}_j) 
\end{equation}
where \( p(\text{a}_j) \) is the empirical probability of each unique answer \(\text{a}_j\) over the \(K\) final answers $\{ \text{a}^1, \text{a}^2, \dots, \text{a}^K \}$ extracted from \(K\) responses $\mathcal{Y} = \{ \hat{\text{y}}^1, \hat{\text{y}}^2, \dots, \hat{\text{y}}^K \}$.
An example of the answer entropy computation is provided in Table~\ref{tab:demo}.

In the following, we focus on the \texttt{Llama-2-7B-chat} model \cite{touvron2023llama}. Experiments with additional datasets, uncertainty metrics, and models are discussed in \cref{sec:experiments}. %, and injection layers 

%Our case study , where uniform noise sampled from $U(0, 0.07)$ is injected to perturb the MLP blocks of \(20 - 32\) transformer layers.  We follow the default generation configuration with $\text{top-k} = 50$ and $\text{top-p} = 1$.
%For each question, we assess model uncertainty across $K = 30$ generations. When prediction layer sampling is enabled to capture observation uncertainty, we set the temperature as $T = 0.5$. 
%, which optimizes GSM8K accuracy within the set $T = \{0.2, 0.5, 0.8, 1.0\}$. % as shown in Appendix~\lliu{[add]}. 
%Detailed experimental setup is provided in Appendix~\lliu{[add]}. 

\subsection{Hallucination Detection Under Epistemic Uncertainty}\label{sec:standalone}
% Effect of Noise Injection on Response Variability: Hallucination vs. Non-Hallucination
We capture epistemic uncertainty through noise injection and examine model hallucination behavior under it.
Specifically, we inject uniform noise sampled from $\mathcal{U}(0, 0.07)$ to perturb the MLP activations of layers \(20 - 32\) of the transformer. 
This approximately modifies the MLP bias of the model and thus effectively samples a model $\hat{\omega}$ from our surrogate distribution $q(\omega)$.
To isolate this effect, we set the prediction layer sampling temperature to zero and decode greedily to eliminate aleatoric uncertainty from sampling. 
%We first study the effectiveness of our surrogate distribution $q(\omega)$ obtained through noise injection (\cref{eq4}), which captures epistemic uncertainty, for hallucination detection.
%To be effective in detecting hallucinations, the responses obtained from models sampled from $q(\omega)$ should exhibit greater variability when hallucinating. 
%We use uniform noise sampled from $U(0, 0.07)$ to perturb the MLP blocks of \(20 - 32\) transformer layers and thus sample from our surrogate distribution $q(\omega)$. 
%To isolate the effect of capturing epistemic uncertainty through noise injection, we set the prediction layer sampling temperature to zero and greedily select the next token with the largest likelihood, removing randomness from the prediction layer sampling process. 


%To compute answer entropy for hallucination detection, 
We generate \( K\!=\!10 \) samples for each question and compute answer entropy following \cref{eq:answer_entropy}. 
We classify model hallucination on a question level; model responses to a question are considered as hallucinating if the majority of the \( K\!=\!10 \) generated answers are incorrect, and as non-hallucinating otherwise. 
In \cref{fig:scheme-demo} (left), we compare answer entropy between hallucinating and non-hallucinating cases by overlaying the histograms of the two groups. 
We observe that with the model stochastically sampled from $q(\omega)$, responses exhibit greater variability when hallucinating (grey), as evidenced by higher entropy values.
%We observe that the responses of models sampled from $q(\omega)$ exhibits greater variability when hallucinating (grey), as evidenced by higher entropy values. 
This shows the effectiveness of using noise injection for capturing epistemic uncertainty and thus detecting hallucinations.
%This observation matches our intuition: less variability implies the robustness of the model response to noise, suggesting greater certainty and a lower likelihood of hallucination.

\begin{figure}
%\vspace{-2mm}
\centering
\includegraphics[width=0.86\columnwidth]{complementary_effect_v1.pdf}
%\vspace{-5mm}
\caption{\textbf{Complementary Effect of Epistemic Uncertainty and Aleatoric Uncertainty}.
The x-axis presents answer entropy (Equation~\ref{eq:answer_entropy}) with prediction layer sampling only, which mainly captures aleatoric uncertainty.  
The y-axis presents answer entropy under intermediate layer noise injection only, which mainly captures epistemic uncertainty. 
A Pearson correlation of 0.58 indicates a complementary relationship between the two types of uncertainty.
%\lliu{change axis, $\alpha$}
}\label{fig:complementary}
\vspace{-5mm}
\end{figure}

\subsection{Complementary Effect of Aleatoric and Epistemic Uncertainty}\label{sec:combine}
We now examine the interplay between aleatoric and epistemic uncertainty and their impact on model performance.

\textbf{Epistemic Uncertainty: } We evaluate model performance with noise sampled from $\mathcal{U}(0,0.07)$ and sampling temperature set to zero as in Section~\ref{sec:standalone}. 
%With sampling disabled, we isolate the aleatoric uncertainty captured by a single model and focus on the epistemic uncertainty from stochastic model parameters modified by the injected noise.  

\textbf{Aleatoric Uncertainty: } We evaluate model performance with temperature $T = 0.5$ and without noise injection. 
This inference scheme leverages the aleatoric uncertainty as captured by the original model. 


For each setup, we assess answer entropy across $K =10$ samples for each question following \cref{eq:answer_entropy}. 
In the scatter plot in \cref{fig:complementary}, we display each question of the GSM8K test set as a point, with the x-value representing answer entropy under aleatoric uncertainty, and the y-value representing the same 
under epistemic uncertainty. 
The plot shows that model performance under the two types of uncertainty is only weakly correlated, with a Pearson correlation of 0.58. 
This suggests that there is a positive but complementary relationship.
We further validate the complementarity in Section~\ref{sec:ablation_tempt}.

\subsection{Algorithm: Noise Injection for Hallucination Detection}

To capture both epistemic uncertainty and aleatoric uncertainty, as suggested by \cref{sec:combine}, we incorporate noise injection alongside prediction layer sampling and propose our Noise Enhanced Hallucination Detection algorithm. The algorithm is described in detail in \cref{alg:NED}. 
%We illustrate our design with additive uniform noise in Algorithm~\ref{alg:NED}. 

%Overall, our hallucination detection algorithm computes the hallucination detection score defined in \cref{eq1} using Monte Carlo sampling, by drawing samples from our predictive distribution defined in \cref{eq3}.

%First, we sample a model from our model distribution $\hat{\omega} \sim q(\omega)$, to capture epistemic uncertainty. 
%When we restrict the set of parameters in the sampling set $\mathcal{S}$ to be the MLP layers' bias terms, 
%this step simply amounts to injecting additive uniform noise \( \xi^{l}_{i} \sim \mathcal{U}(0, \alpha)\) \apratim{check} to the MLP output $\text{h}^l_{i}$ of selected layers $L_1 \leq l \leq L_2$ in the model. 
%Here, $\alpha$ is the given noise magnitude.
First, to capture epistemic uncertainty, we inject noise to MLP activations, which effectively samples a model from our model distribution $\hat{\omega} \sim q(\omega)$ (see \cref{eq4}) as in \cref{sec:standalone}.
Second, to capture aleatoric uncertainty, we sample from the temperature-adjusted categorical distribution $p(\text{y}_t |\text{y}_{<t},  \text{x}, \hat{\omega})$.
Repeating the sampling process effectively draws samples from,
\begin{align}
\int p(\text{y}_t |\text{y}_{<t}, \text{x}, \omega) q(\omega)\, d\omega \,.
\end{align}
To detect hallucinations, we compute the answer entropy over the $K$ generated samples and apply a threshold.
%These steps are repeated $K$ times. Finally, to detect hallucinations, we compute the hallucination detection score over the $K$ generated samples from the model and apply a threshold to classify outputs.

\begin{algorithm}
\caption{Noise Enhanced Hallucination Detection}\label{alg:NED}
\begin{algorithmic}[1]
\INPUT Input context: \( \text{x} \), number of samples: \( K \), surrogate model distribution: $q(\omega)$ (built from noise magnitude \( \alpha \), perturbed layers \( L_1 \) to \( L_2 \)), model dimension $d$, temperature: \( T\),  uncertainty metric: \(\mathcal{H}(\cdot)\). 
  \OUTPUT Hallucination detection score: \( s(\text{x}) \)
\FOR{\( k = 1 \) to \( K \)} 
    %\COMMENT{Sample model $\hat{\omega} \sim q(\omega)$}
    \STATE \texttt{// Sample model from $\hat{\omega} \sim q(\omega)$ //}
    \STATE Sample noise: \( {\epsilon} \sim \mathcal{U}(0, \alpha)^d \) 
    \FOR{each token $\hat{\text{y}}^k_t \in \hat{\text{y}}^k$}
        \FOR{each layer $l$}
                \STATE Compute $\text{h}^l$ using the potentially perturbed prior layer representations.
                \IF{\( l \in [L_1, L_2] \)}
                    \STATE Perturb the MLP activations: $\hat{\text{h}}^l = \text{h}^l + \epsilon$. 
                \ENDIF
        \ENDFOR
    \STATE \texttt{// Sample tokens from model $\hat{\omega}$ //}
    %\FOR{each token $\hat{\text{y}}^k_t \in \hat{\text{y}}^k$}
        %\STATE Modify next token probability:
        %    \[\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \alpha) = f(\bm{h}_t^1, \dots, %\bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L )\]
        \STATE Sample token $\hat{\text{y}}^k_t \sim p(\text{y}_t \mid \text{y}_{<t}, \text{x}, \hat{\omega})$ with temperature T. 
    \ENDFOR
\ENDFOR
\RETURN Hallucination detection score \( s(\text{x}) = \mathcal{H}(\mathcal{Y} ) \), where \( \mathcal{Y} = \{\text{y}^1, \text{y}^2, \dots, \text{y}^K \}\)
\end{algorithmic}
\end{algorithm}

\begin{comment}
\begin{algorithm}
\caption{Noise Enhanced Hallucination Detection \lliu{alogorithm is wrong; noise applied twice.}}\label{alg:NED}
\begin{algorithmic}[1]
\INPUT Input context: \( \text{x} \), number of samples: \( K \), surrogate model distribution: $q(\omega)$ (with noise magnitude \( \alpha \), perturbed layers \( L_1 \) to \( L_2 \)),  temperature: \( T\),  uncertainty metric: \(\mathcal{H}(\cdot)\). 
  \OUTPUT Hallucination detection score: \( s(\text{x}) \)
\FOR{\( k = 1 \) to \( K \) samples $\hat{\omega} \sim q(\omega)$} 
    %\COMMENT{Sample model $\hat{\omega} \sim q(\omega)$}
    \STATE Sample noise: \( {\sigma} \sim \mathcal{U}(0, \alpha) \) 
    \FOR{each layer $L_1 \leq l \leq L_2$}
            \STATE Compute activation $\text{h}^l_i$ of every neuron $i$ in $l$
            \STATE Inject sampled noise $\alpha$ to the activations $\text{h}^l_i$
    \ENDFOR
    \FOR{each token $\hat{\text{y}}^k_t \in \hat{\text{y}}^k$}
        %\STATE Modify next token probability:
        %    \[\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \alpha) = f(\bm{h}_t^1, \dots, %\bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L )\]
        \STATE Sample token $\hat{\text{y}}^k_t \sim p(\text{y}_t \mid \text{y}_{<t}, \text{x}, \hat{\omega})$ with temperature T. 
    \ENDFOR
\ENDFOR
\RETURN Hallucination detection score \( s(\text{x}) = \mathcal{H}(\mathcal{Y} ) \), where \( \mathcal{Y} = \{\text{y}^1, \text{y}^2, \dots, \text{y}^K \}\)
\end{algorithmic}
\end{algorithm}
\end{comment}

\myparagraph{Empirical Validation.} In \cref{tab:case_study}, we validate the effectiveness of our scheme under the case study setup.
 We perturb the MLP activation of layers 20 to 32 with additive uniform noise of magnitude \(\alpha = 0.07\), sampled from \(\mathcal{U}(0, 0.07)\), and evaluate over \(K = 10\) samples. 
In practice, the noise magnitude can be selected based on the validation set, and we present an ablation study on different noise magnitudes in \cref{sec:ablation_tempt}.
Following \citet{malinin2020uncertainty, lin2022towards, lin2023generating, kuhn2023semantic, chen2024inside}, we assess the effectiveness of hallucination detection using the threshold-free metric, the area under the receiver operating characteristic curve (AUROC), where a higher value indicates better detection performance.
As shown in Table~\ref{tab:case_study}, our scheme effectively detects hallucination instances with AUROC value $> 50$. 

We further compare our approach with previously proposed schemes that solely rely on prediction layer sampling without noise injection and thus do not capture epistemic uncertainty. 
%The setup of the noiseless \apratim{noiseless sounds strange} scheme follows \cref{sec:combine}.
The setup without noise injection follows \cref{sec:combine}.
As shown in \cref{tab:case_study}, our approach significantly improves detection effectiveness and achieves a higher AUROC value. 
The improvement is also visualized in \cref{fig:scheme-demo}~\textit{(b)}, where noise injection increases the separation and reduces the overlap in the histograms from left to right.

Finally, we evaluate model accuracy on the GSM8K dataset. 
To this end, we use majority voting based on the generated samples. 
Like before, we compare the aleatoric and epistemic settings. 
%and compare between both with and without noise injection.
As shown in \cref{tab:case_study}, taking into account epistemic uncertainty improves hallucination detection performance without degrading model accuracy. 
%not only improves hallucination detection performance 
%but also sightly improves model accuracy. 

Overall, our case study strongly supports our hypothesis regarding  
the relative importance of taking into account epistemic uncertainty 
when generating samples. 
%\apratim{This part needs update}
%This supports our intuition that incorrect answers produced during hallucination are less robust to noise injection, as indicated by higher entropy.
%Consequently, the consistency of incorrect answers across generations reduces with noise injected, making them less likely to be selected by majority vote. 
%This shift improves the likelihood of correct answers being chosen, thereby enhancing accuracy under the majority vote scheme. 

\begin{table*}[t]
\vspace{-2mm}
\caption{
\textbf{Intermediate Layers Noise Injection Enhances Hallucination Detection across Models and Datasets}.
The gain shows the benefits of epistemic uncertainty alongside aleatoric uncertainty.
Answer entropy over $K = 10$ samples is used for hallucination detection.
Detection AUROC is reported with mean and 95\% confidence intervals.
Higher mean values indicate better performance.
%Noise magnitude fixed as $\alpha = 0.05$ based on GSM8K performance. 
%Evaluation with \texttt{Llama2-13B-chat} model across 5 generations. 
}\label{tab:main}
\vspace{-2mm}
\begin{center}
\includegraphics[width=\textwidth]{main_dataset_v2.pdf}
\end{center}
\vspace{-2mm}
\end{table*}

\section{Experiments}\label{sec:experiments}

In this section, we move beyond the case study and extensively validate the effectiveness of our algorithm across different datasets and model architectures. 
Further, we conduct a comprehensive ablation study to understand the effects of varying the number of samples, noise injection layers, noise magnitude, sampling temperature, and uncertainty metrics. % \apratim{should be hallucination detection metrics?}. 

\subsection{Main Result}\label{sec:ablation_datasets}
In Table~\ref{tab:main}, we validate the effectiveness of noise injection for enhancing hallucination detection.

\textbf{Dataset:} 
Beyond the mathematical reasoning task GSM8K, we also test on CSQA \citep{talmor-etal-2019-commonsenseqa}, which assesses commonsense world knowledge in a multiple-choice format, and TriviaQA \citep{joshi2017triviaqa}, which evaluates factual question answering.
For CSQA, we test on the validation set with 1,221 questions and prompt the model using 7-shot chain-of-thought exemplars for in-context learning, following \citet{wei2022chain}.
For TriviaQA, we test the \texttt{rc.nocontext} validation subset with 9,960 unique questions, designed for free-form, context-free question-answering.
To evaluate TriviaQA, we prompt the model with 10 in-context learning examples from the corresponding training set following \citet{kuhn2023semantic}. 
Overall, our datasets cover different topics, formats (free-form vs. multiple choice), and prompting styles (with or without chain-of-thought).
For more details, see Section~\ref{app:datasets}.

\myparagraph{Model:} We evaluate a diverse range of LLMs across various sizes, including \texttt{Gemma-2B-it} \cite{team2024gemma}, \texttt{Phi-3-mini-4k-instruct} (2.8B) \cite{abdin2024phi}, \texttt{Mistral-7B-Instruct} \citep{jiang2023mistral}, \texttt{Llama-2-7B-chat}, and \texttt{Llama-2-13B-chat}.

\myparagraph{Setup:} Following \cref{sec:case_setup}, we inject random uniform noise sampled from $\mathcal{U}(0,\alpha)$ into the MLP layer activation of upper layers.
Since our performance is not sensitive to specific layers (Section~\ref{sec:ablation-layers}), we inject noise into roughly the top third of the layers (see Appendix~\ref{app:model_spec} for exact layer ranges).
%As our performance is not sensitive to specific layers, as shown in Section~\ref{sec:ablation-layers}, we choose roughly the last one-third of all layers, including the last layer, to inject noise without additional tuning. 
For the noise magnitude $\alpha$,  we select from $[0.01,0.03,0.05,0.07,0.09,0.11]$ based on validation sets, as detailed in \ref{app:alpha_selection}. 
The temperature is fixed at $T = 0.5$, and answer entropy over $K = 10$ samples is used for hallucination detection.

For each setup, we bootstrap \( K = 10 \) samples 25 times from a total of 40 samples and report the mean AUROC with its 95\% confidence interval.


\begin{figure*}[t]
\centering
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{gsm8k-7b-t5-a7-auroc-v2.pdf}
    %\caption{AUROC}
    %\label{fig:auroc}
\end{subfigure}
%\hspace{7mm}
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{gsm8k-7b-t5-a7-acc-v2.pdf}
    %\caption{ACC}
    %\label{fig:acc}
\end{subfigure}
\vspace{-0.5cm}
\caption{\textbf{Noise Injection Enhances Hallucination Detection Across Different Number of Samples. }
Evaluation under $T = 0.5$ for GSM8K datasets on \texttt{Llama-2-7B-chat} model across 1 
 - 20 samples. 
With noise magnitude $\alpha = 0$, only aleatoric uncertainty [A] is captured; with $\alpha = 0.07$, both aleatoric [A] and epistemic [E] uncertainty are captured.
Hallucination detection AUROC (Left) and model accuracy (Right) reported; higher values are better. 
Mean and 95\% confidence intervals are shown. }
\label{fig:gsm8k-13b-evolution}
%\vspace{-4mm}
\end{figure*}


\myparagraph{Performance:} Looking at \cref{tab:main}, we observe that taking into account 
epistemic uncertainty consistently improves hallucination detection, 
as indicated by higher AUROC scores. 
The improvement is more pronounced on GSM8K and CSQA than on TriviaQA. 
This may be because GSM8K and CSQA involve chain-of-thought reasoning, 
where the model is called across multiple steps so that uncertainty accumulates. 
This is unlike TriviaQA, which relies on directly short answers. 
Nonetheless, noise injection remains effective on TriviaQA. 
Notably, on TriviaQA with \texttt{Phi-3-mini-4k-instruct}, the baseline AUROC is already the highest across the board, suggesting performance saturation, which limits the impact of noise.


%PrOntoQA is a synthetic question-answering dataset comprised of procedurally-generated symbolic world models and reasoning chains to resolve the truthfulness of a claim. We extract the generated questions and ground truth reasoning chains for the $1$-Hop fictional subset from their provided model activation, totaling $400$ question-answer pairs. 
%Answer formats are free-form, multiple-choice, and True or False for TriviaQA, CSQA, and PrOntoQA, respectively.


%For each dataset, we select the temperature within $T = \{ 0.2, 0.5, 0.8, 1.0\}$ which optimizes the model accuracy on this dataset. 
%For GSM8K, TriviaQA, CSQA, and ProntoQA, the temperature is set to be $0.8$, $0.2$, $0.8$, and $0.8$, respectively. 
%We follow the setup of Section~\ref{sec:case_setup} and select the noise magnitude as $\alpha = 0.05$ based on GSM8K performance. 
%We remark that $\alpha = 0.05$ is not the optimal noise magnitude for each dataset and performance can be further boosted through hyperparameter search, as demonstrated in Appendix~\ref{appendix:noise-ablation}. 
%Our goal here is to demonstrate feasibility rather than performance optimization.
%For each dataset, we evaluate with uncertainty metrics: Predictive Entropy (see Equation~\ref{eq:entropy}), Normalized Predictive Entropy (see Equation~\ref{eq:ln_entropy}), and Answer Entropy (see Equation~\ref{eq:answer_entropy}). 
%Looking into Table~\ref{tab:main}, noise injection is most effective on GSM8K with answer entropy, as expected since it is the optimized metric. 
%However, our method remains effective across most datasets and metrics, validating that noise injection generally enhances model performance across various uncertainty metrics.

\subsection{Ablation of Number of Samples}\label{sec:abalation_runs}

%\lliu{TODO: unify with the bar plot in 4.1; worst case scenario, only report the error bar here. }

So far, we have presented results based on $K = 10$ samples in Section~\ref{sec:case_study} and Section~\ref{sec:ablation_datasets}.
We now extend this study to explore the effect of noise injection across different numbers of samples. 
In Figure~\ref{fig:gsm8k-13b-evolution}, we present the hallucination detection AUROC (left) and model accuracy (right) on GSM8K for $K = 1$ to $K = 20$.
The rest of the setup follows Section~\ref{sec:case_setup}. 
For each $K$, we report the mean and 95\% interval across 25 bootstraps of $K$ samples from a total of 40 samples.
As shown in Figure~\ref{fig:gsm8k-13b-evolution}, both hallucination detection AUROC and model accuracy on GSM8K improve with an increasing number of samples.  
Notably, noise injection to capture epistemic uncertainty consistently enhances the effectiveness of hallucination detection across different numbers of samples without degrading model accuracy. 
In practice, the number of samples can be adjusted based on the computational budget and accuracy requirements. 
Nevertheless, our experiments demonstrate that noise injection improves hallucination detection effectiveness, regardless of the specific number of samples.

\begin{table*}[t]
\vspace{-2mm}
\caption{
\textbf{Noise injection at varying magnitudes improves hallucination detection across temperatures} 
AUROC values for different temperature-noise magnitude combinations are color-coded, with darker shades indicating better performance.
Temperature adjustment only reaches a plateau, indicating the limit of aleatoric uncertainty, while noise injection further improves performance, showing the complementary effect of epistemic uncertainty. 
Evaluation on GSM8K dataset with \texttt{Llama-2-7B-chat} model across 10 samples. 
%\lliu{add ours}
%Hallucination detection performance under varying sampling temperature and noise magnitude.
}\label{tab:ablation_noise}
%\vspace{-3mm}
\begin{center}
\includegraphics[width=0.95\textwidth]{noise-vs-temp-heatmap.pdf}
\end{center}
\vspace{-4mm}
\end{table*}

  
%In Section~\ref{sec:case_study} and Section~\ref{sec:ablation_datasets}, 
%AUROC and ACC across runs from 1 to 50;
%Take-away: both ACC and AUROC improve as the number of runs improves; the exact runs can be chosen in practice w.r.t. computation r

\subsection{Ablation of Temperature and Noise Magnitude}\label{sec:ablation_tempt}

%Recall in Section~\ref{sec:ablation_datasets}, we fix the sampling temperature $T = 0.5$ and set noise magnitude based on validation datasets. 
In \cref{tab:ablation_noise}, we explore the effectiveness of our algorithm under varying temperatures and noise magnitude. 
The experimental setup follows Section~\ref{sec:case_setup}. 
As shown in \cref{tab:ablation_noise}, while the optimal noise magnitude varies with temperature, moderate noise injection generally enhances hallucination detection.
%\rebuttal{Additional experiments with CSQA in Appendix~\ref{appendix:noise-temp-ablation} further validate this finding.}
Further, the table highlights the complementary effects of epistemic and aleatoric uncertainty. 
As temperature increases from $T \! = \! 0.8$ to $1.0$ without noise injection, hallucination detection AUROC plateaus, revealing the limits of modeling aleatoric uncertainty alone.
However, injecting noise at $T = 0.8$ improves performance by capturing both epistemic and aleatoric uncertainty.




\begin{table}[t]
\vspace{-2mm}
\caption{
\textbf{Noise injection across different layers consistently enhances hallucination detection}. 
AUROC is reported, with higher values indicating better performance. 
Evaluation on CSQA dataset with \texttt{Llama-2-7B-chat} model across 10 samples. 
%\lliu{add ours}
}\label{tab:ablation_layers}
%\vspace{-3mm}
\begin{center}
\includegraphics[width=\columnwidth]{ablation_layer_v2.pdf}
\end{center}
\vspace{-5mm}
\end{table}


\vspace{-2mm}
\subsection{Ablation on Noise Injection Layers}\label{sec:ablation-layers}

We now investigate the effect of noise injection across different layers. 
We perform ablation experiments on \texttt{LLaMA-2-7B-chat}, which consists of 32 layers in total. 
In addition to injecting noise in the upper layers (20-32), we also experiment with noise injection in the middle (10-20) and lower (0-10) layers.
In \cref{tab:ablation_layers}, we report the hallucination detection AUROC with noise injected on different layers. 
The noise magnitude is set to $\{0.05, 0.01, 0.01\}$ for upper layers, middle layers, and lower layers, respectively, each achieving the optimal performance across noise injection level $\{0.01, 0.03, 0.05, 0.07, 0.09 \}$ for the corresponding layers.
As expected, lower layers require smaller magnitudes due to their lower tolerance for error propagation. 
From \cref{tab:ablation_layers}, we observe that noise injection across different layers yields similar detection effectiveness, with all setups improving performance compared to the baseline where no noise is injected.
This suggests our algorithm is robust to the layer of choice and highlights the overall effectiveness of introducing epistemic uncertainty.
%while noise injection enhances hallucination across layers, upper-layer injection is the most effective. 
%This may be because upper layers tolerate more noise without disrupting generation, reflected by the higher optimal noise magnitude. 
%In contrast, lower layers have less tolerance due to error propagation.

\begin{comment}
\subsection{Ablation on Alternative Architectures}
We extend our case study beyond the \texttt{Llama2-13B-chat} model, experimenting with the \texttt{Llama2-7B-chat} from the same Llama family and the \texttt{Mistral-7B} model \citep{jiang2023mistral} from a different family.
Both models have 32 layers in total, and we inject noise into layers 22 to 32 to perturb the upper layer representations.
We evaluate GSM8K, following the setup from our case study in Section~\ref{sec:case_setup}. 
As shown in Table~\ref{tab:ablation_arch}, on both architectures, noise injection improves the AUROC of hallucination detection. 
Notably, the effective noise magnitude differs: while \texttt{Llama2-7B-chat} performs well with $\alpha = 0.05$, \texttt{Mistral-7B} requires a smaller noise level of $\alpha = 0.02$, indicating the need for model-specific hyperparameter tuning.


\begin{table}[t] 
    \centering
    \caption{
    Noise injection improves hallucination detection on \texttt{Llama2-7B-chat} and \texttt{Mistral}.  
    Evaluation of GSM8K across 5 generations.
    AUROC value reported; the higher the better. 
    }
    %\vspace{2mm}
    \label{tab:ablation_arch}
    \begin{tabular}{ccc}
     \hline\
       & \texttt{Llama2-7B-chat} & \texttt{Mistral} \\ [0.5ex] 
     \hline
     No Noise           & 75.09 &  77.03 \\ 
     \hline
    Noise Injection  & 76.80 & 82.95 \\
     \hline
    \end{tabular}
\end{table}
\end{comment}


\subsection{Ablation on Uncertainty Metric}


% \paragraph{Uncertainty Metric}
%One critical aspect of hallucination detection is the design of uncertainty metrics $E(\cdot)$ over generations \( \mathcal{Y} \).% we extend our analysis beyond answer entropy and
% Here, investigate the impact of noise injection on hallucination detection using alternative uncertainty metrics $\mathcal{H}(\cdot)$. 
% First, we consider Predictive Entropy \citep{malinin2020uncertainty}, that employs normalization to eliminate the bias caused by sequence length,
% \begin{equation}\label{eq:ln_entropy}
%     \mathcal{H}_{pred}(\mathcal{Y}) = - \frac{1}{T_{\text{y}}} \Big( \mathbb{E}_{\text{y} \in \mathcal{Y}}  \sum_{t=1}^{T} \log p(\text{y}_t \mid \text{y}_{<t}, \text{x}) \Big)
% \end{equation}

% % Lexical Similarity is an uncertainty metric used to gauge how similar text samples are \citep{lin2022towards, lin2023generating}. It specifically calculates the average Rouge-L score across a set of sampled answers for a particular question as follows:
% % \begin{equation}\label{eq:lexical_sim}
% %     E_{lexical}(\mathcal{Y}) = \frac{1}{C} \sum_{i=1}^{K} \sum_{j=i+1}^{K} RougeL(\bm{y}_i, \bm{y}_j), C=K*(K-1)/2
% % \end{equation}

% We also consider Lexical Similarity \citep{lin2022towards, lin2023generating} and Semantic Entropy \cite{semanticentropy}.
% Lexical Similarity gauges how similar text samples are by calculating the average Rouge-L score across a set of sampled answers \( \mathcal{Y} = \{\hat{\text{y}}^1, \hat{\text{y}}^2, \dots, \hat{\text{y}}^K \}\) for a given context $\text{x}$ as $\frac{1}{C} \sum_{i=1}^{K} \sum_{j=i+1}^{K} \text{RougeL}(\text{y}^i, \text{y}^j)$ where $C=K*(K-1)/2$. 
% Semantic entropy combines tokens semantically to measure uncertainty. First, the generations are grouped into clusters that share semantic meaning. Semantic entropy is computed by summing the entropy within each cluster.

Here, we investigate the impact of noise injection on hallucination detection using alternative uncertainty metrics $\mathcal{H}(\cdot)$. 
We consider predictive entropy (entropy normalized for sequence length) \citep{malinin2020uncertainty}, lexical similarity (based on Rouge-L scores) \citep{lin2022towards, lin2023generating}, and semantic entropy (which clusters similar generated texts before calculating entropy) \cite{semanticentropy}.

Among the datasets analyzed, TriviaQA stands out as the most suitable for evaluating the specified metrics. The multiple-choice format of CSQA does not lend itself well to Rouge-L measurement. Similarly, the numerical answers in GSM8K are incompatible with the clustering required for Semantic Entropy analysis. In contrast, the short, free-form answers in TriviaQA make it an ideal candidate for all the metrics under consideration.

In \cref{tab:ablation_metric}, we present the AUROC metric for Predictive Entropy, Lexical Similarity, and Semantic Entropy on TriviaQA, evaluated at a temperature of $T \, = \, 0.5$ and noise magnitudes of $\{0,0.09\}$. The data show that all uncertainty metrics improve with noise injection, demonstrating our algorithm's robustness to metric selection.

\begin{table}[htbp]
    \centering
    \caption{Noise Injection Enhances Hallucination Detection under Predictive Entropy, Lexical Similarity, and Semantic Entropy. Evaluation on TriviaQA dataset with \texttt{Llama-2-7B-chat} model across 10 samples.}
    %\vspace{-1mm}
    \label{tab:ablation_metric}
    %\resizebox{\columnwidth}{!}{
    % \begin{tabular}{ccc}
    %  \hline
    %    & Lexical Similarity & Semantic Entropy \\ [0.5ex] 
    %  \hline
    %  Noise = 0           & 64.74 & 63.62 \\ 
    %  \hline
    %  Noise \textasciitilde{} U (0,0.05)  & \textbf{66.59} & \textbf{65.51} \\
    %  \hline
    % \end{tabular}
    \begin{tabularx}{\linewidth}{Xcc}
    \hline
      & $\epsilon = 0$ &  $\epsilon \sim \mathcal{U}(0,0.09)$ \\ %[0.5ex] 
    \hline
    Predictive Entropy   & 79.28 & \textbf{79.92} \\ 
    \hline
    Lexical Similarity   & 77.40 & \textbf{78.90} \\ 
    \hline
    Semantic Entropy     & 75.70 & \textbf{77.21} \\
    \hline
    \end{tabularx}
    %}
\end{table}

\section{Related Work}

\myparagraph{Bayesian Neural Networks.} Standard neural networks typically learn only a single point estimate, neglecting epistemic and aleatoric uncertainty. Bayesian methods \citep{mackay1992practical,neal2012bayesian} learn a posterior distribution over models to capture epistemic and aleatoric uncertainty, but at a high computational cost. \citet{gal2016dropout} addressed this issue using variational inference with a Bernoulli approximation of the weight distribution, subsequently extended to CNNs by \citet{Gal2016Bayesian}. In case of LLMs, \citet{HouLQAC024} contend that Bayesian methods are not compatible with LLMs due to computational costs and thus aim to quantify epistemic and aleatoric uncertainty using clarification questions. Here, we deal with the challenge posed by computational costs with a novel Bayesian training-free approach using noise injection.


\myparagraph{Hallucination Detection.}
Several recent works have demonstrated a strong correlation between model uncertainty and the likelihood of hallucination. Measures of model uncertainty include  the entropy of answer \citep{malinin2021uncertainty}, semantic \citep{kuhn2023semantic, chen2024inside, farquhar2024detecting}, predictive \citep{xiao2021hallucination}, and lexical \citep{lin2022towards, lin2023generating} distributions. These methods rely on a diverse set of model samples generated by temperature-based sampling, which primarily captures aleatoric uncertainty. Our work is complementary to these approaches and introduces epistemic uncertainty.

% Activations / hidden space
In addition to entropy-based estimates, intermediate model activations have been shown to provide insights into model confidence. \citet{chuang2023dola} demonstrates that the divergence in activations between correct and incorrect tokens increases across layers, with contrasted activations growing sharper for correct tokens. 
%\cite{chen2024incontext}
Additionally, \citet{li2024inference} shows that hidden embeddings encode an LLM's sense of ``truthfulness'', which may be steered along a vector of truth through test-time intervention. 
% Self-reported
Self-reported confidence as explored by \citet{manakul-etal-2023-selfcheckgpt} and \citet{kadavath2022language} is a promising direction but requires the model to be well-calibrated and can be sensitive to distribution shifts. %can suffer out-of-distribution. 




\section{Conclusion}
This work addresses the important issue of hallucination detection for the safe deployment of LLMs. We have established a link between hallucinations and epistemic uncertainty, noting that existing methods primarily focus on aleatoric uncertainty through next-token sampling. Our investigation shows that the often overlooked epistemic uncertainty can be complementary, and that taking epistemic uncertainty into account when generating samples can therefore greatly benefit hallucination detection and also model accuracy. We show that we can efficiently capture epistemic uncertainty efficiently in a \emph{training-free} paradigm is possible in approximation 
by simply perturbing hidden unit activations during the generation of samples. 
Extensive experiments validate the effectiveness of our approach, 
demonstrating its potential to improve the reliability of LLMs.



\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here. 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\begin{comment}
\section{Ablation of Noise Magnitude on Alternative Datasets}
\label{appendix:noise-ablation}

In Section~\ref{sec:ablation_datasets}, we select $\alpha = 0.05$ for all datasets based on GSM8K performance and demonstrate that this choice remains effective across other datasets. 
However, the optimal noise magnitude varies by dataset. 
In Table~\ref{tab:ablation_prontoqa}, we ablate the noise magnitude for PrOntoQA and find that $\alpha = 0.08$ outperforms $\alpha = 0.05$. 
While our approach in Section~\ref{sec:ablation_datasets} simplifies hyperparameter selection by choosing a single noise magnitude per model, these results suggest that tuning noise magnitude for each dataset individually could improve performance, albeit at the cost of additional computation.

\begin{table}[h]
%\vspace{-2mm}
\caption{
\textbf{Ablation of Noise Magnitude on ProntoQA.}
Noise level $\alpha = 0.08$ further improves detection effectiveness compared to $\alpha = 0.05$, as indicated by a higher AUROC.
Evaluation with \texttt{Llama2-13B-chat} model across 5 generations. 
%\lliu{add ours}
}\label{tab:ablation_prontoqa}
%\vspace{-3mm}
\begin{center}
\includegraphics[width=0.55\textwidth]{prontoqa.pdf}
\end{center}
\vspace{-2mm}
\end{table}
\end{comment}

\section{Implementation Details}

\subsection{Datasets}\label{app:datasets}

%\lliu{SUNNY: to confirm} \sunny{confirmed}

We use in-context examples to demonstrate correct answer formatting and simplify answer extraction following free-form rationales, where applicable.
For \textbf{GSM8K} and \textbf{CSQA}, we adopt the examplers in \cite{wei2022chain} as our in-context-learning examples.
For \textbf{TriviaQA}, we ensemble a 10-shot prompt from the first 10 training examples following \cite{kuhn2023semantic}.

On \texttt{Mistral-7B}, \texttt{Llama-2-7B-chat} and \texttt{Llama-2-13B-chat}, we concatenate in-context learning examples to form prompt using format Q:...A:...Q:...A:....
An example prompt for \textbf{TriviaQA} is: 

\texttt{Q: Which Oscar-nominated film had You Sexy Thing as its theme
song? A: The Full Monty Q: Which Joan’s career revived in
Whatever Happened to Baby Jane? A: Crawford Q: Which much-loved
actor won the Best Actor Oscar for The Philadelphia Story? A:
James Stewart (...) Q: In which river is the Boulder Dam? A:}

If the model continues the Q:...A:... format after completing the answer, we trim generations using pattern matching with stopwords. 
For \texttt{Gemma-2B-it} and \texttt{Phi-3-mini-instruct}, we apply the chat template available on the respective model tokenizers as available on Huggingface. 

In evaluation, when the model fails to produce the answer with the correct format, we treat it as invalid. 

\subsection{Models}\label{app:model_spec}

All models evaluated in this work are off-the-shelf with no additional fine-tuning. 
We inject noise into roughly the top third of layers. Specifically, \texttt{Gemma-2B-it} has 18 layers in total, and we inject noise into layers 12-18. Similarly, for \texttt{Phi-3-mini-4k-instruct}, which has 30 layers, noise is injected into layers 20-30; for \texttt{Mistral-7B-Instruct} and \texttt{Llama-2-7B-chat}, both with 32 layers, noise is injected into layers 20-32; and for \texttt{Llama-2-13B-chat} with 40 layers, noise is injected into layers 25-40.

All models evaluated in this work are off-the-shelf with no additional fine-tuning. 
Perturbations on the model are implemented using \texttt{pyvene} \citep{wu-etal-2024-pyvene}. 
We run all of our experiments on 80GB NVIDIA A100s.
And there is no noticeable latency overhead with or without noise injection, confirming that our method introduces no practical delay.



%Specifically, we inject noise on layers 12-18 for \texttt{Gemma-2B-it}, layers 20-30 for \texttt{Phi-3-mini-4k-instruct}, layers 20-32 for \texttt{Mistral-7B-Instruct} and \texttt{Llama-2-7B-chat}, and layers 25-40 for \texttt{Llama-2-13B-chat}.

\subsection{Noise Magnitude Selection}\label{app:alpha_selection}

We select the noise magnitude $\alpha$ based on the results of the validation datasets.
On \texttt{Gemma-2-it}, we set $\alpha$ as $0.05, 0.09, 0.11$ for GSM8K, CSQA, TriviaQA, respectively.  
On \texttt{Phi-3-mini-instruct}, we set $\alpha$ as $0.05, 0.07, 0.09$ for GSM8K, CSQA, TriviaQA, respectively.  
On \texttt{Mistral-7B-Instruct}, we set $\alpha$ as $0.03, 0.07, 0.03$ for GSM8K, CSQA, TriviaQA, respectively. 
On \texttt{Llama-2-7B-chat}, we set $\alpha$ as $0.07, 0.03, 0.09$ for GSM8K, CSQA, TriviaQA, respectively. 
On \texttt{Llama-2-13B-chat}, we set $\alpha$ as $0.05, 0.05, 0.09$ for GSM8K, CSQA, TriviaQA, respectively. 

Alternatively, if per-dataset tuning raises computational concerns, hyperparameters can be selected per model instead. For example, using $\alpha = 0.05$ across all datasets on \texttt{Llama-2-13B-chat} (jointly tuned on the validation set), noise injection improves detection AUROC for all datasets: GSM8K improves from 77.20 to 79.23, CSQA from 67.44 to 69.10, and TriviaQA from 73.39 to 74.37. While per-model selection may not achieve the same performance as per-dataset tuning, it remains an effective strategy for enhancing hallucination detection.

\begin{comment}
12 -18 for Gemma-2b
20 - 30 for Phi 
20 -32 for Mistral ; 7B model
25 - 40 for 13B model
\end{comment}

%\section{Analysis on Noise Sampling Mechanism}
%Each layer sampling v.s. Sample once and apply on all layers.
%Deliberate design to ensure noise does not cancel in the computation. 


\section{Connection Between Weights and Bias Perturbation}\label{app:bias_weight}

%Key argument: adding noise to bias is similar to adding noise to the weight. 
We now show that injecting noise into the bias and the weight has a similar effect.

Consider an intermediate MLP layer with input \(\text{h}^{\text{in}}\) and activation \(\text{h}^{\text{out}}\). 
Assume the model is well-regularized, such that the input \(\text{h}^{\text{in}}\) has a similar average magnitude $\sum_j{h^{\text{in}}_j}$  across samples.
For computing the $i^{\text{th}}$ output element \(\text{h}^{\text{out}}_i\), applying uniform noise into the corresponding weights \(\theta_{i,j}\) is equivalent to injecting uniform noise from a rescaled magnitude into the layer's bias \(\gamma_i\).
Let $\epsilon$ be noise sampled from uniform distribution $\mathcal{U}(0, \beta)$. 
The output after injecting noise into the weights is computed as follows:

\begin{align}\label{eq5}
\text{h}_{\text{out}}^{i} &= \sigma\Big( \sum_{j} \big({\theta}_{i,j} + \epsilon)\text{h}^{\text{in}}_j + \bm{\gamma}_i \Big)  \\
&= \sigma\Big( \sum_{j}{\theta}_{i,j}\text{h}^{\text{in}}_j + \sum_j \epsilon \text{h}^{\text{in}}_j + \bm{\gamma}_i\Big) \\
&= \sigma\Big( \sum_{j}{\theta}_{i,j}\text{h}^{\text{in}}_j + (\epsilon\sum_j \text{h}^{\text{in}}_j + \bm{\gamma}_i) \Big),
\end{align}

where $\sigma(\cdot)$ is the activation function. 
By our well-regularization assumption, this is equivalent to perturbing the bias $\gamma^i$ with noise sampled from $\mathcal{U}(0, \beta\sum_j \text{h}_{\text{in}}^j)$. 


\begin{comment}
Next, we analyze the distribution of activations induced by the uniform distribution on the weights defined in \cref{eq4}. For the $i^{\text{th}}$ neuron in the $l^{\text{th}}$ layer, it's activation $\text{h}^{l}_{i}$ for input $\text{h}^{l-1}_{i}$ (of the $l^{\text{th}}$ layer) is,
\begin{align}\label{eq5}
\text{h}^{l}_{i} &= \sigma\Big( \sum_{j} \big({w}^l_{i,j} + \alpha^l_{i,j} \big) \, \text{h}^{l-1}_{i,j} + {b}^l_{i,n+1} + \alpha^l_{i,n+1} \Big)
\end{align}

If $\text{h}^{l-1}_{i,j}$ is uniformly distributed as $\text{h}^{l-1}_{i,j} = \bar{\text{h}}^{l-1}_{i,j} + \alpha^{l-1}_{i,j}$,
\begin{align*}\label{eq5}
\text{h}^{l}_{i} &= \sigma\Big( \sum_{j} \big( {w}^l_{i,j} + \alpha^l_{i,j} \big) \big(\bar{\text{h}}^{l-1}_{i,j} + \alpha^{l-1}_{i,j} \big) + {b}^l_{i,n+1} + \alpha^l_{i,n+1} \Big) \\
&= \sigma\Big( \sum_{j} \big({w}^l_{i,j} \bar{\text{h}}^{l-1}_{i,j} + {w}^l_{i,j}\alpha^{l-1}_{i,j} + \alpha^l_{i,j} \bar{\text{h}}^{l-1}_{i,j} + \alpha^l_{i,j} \alpha^{l-1}_{i,j} \big) \\
& + {b}^l_{i,n+1} + \alpha^l_{i,n+1} \Big)
\end{align*}
\end{comment}

\section{Perturbing the Attention Block: An Alternative Bayesian Approach}\label{app:ablation_position}


In this section, we explore an alternative instantiation of the Bayesian perspective by injecting noise into the attention block, as opposed to the MLP layer (see Figure~\ref{fig:overview}). Specifically, we inject noise into the attention block activation, akin to modifying the unperturbed (zero) bias of the attention mechanism. In Table~\ref{tab:ablation_position}, we experiment with \texttt{Llama-2-7B-chat} on CSQA, perturbing the 20-32 layer activations with uniform noise. We sweep the noise magnitude $\alpha$ from ${0.01, 0.03, 0.05, 0.07, 0.09}$ and report the best performance at $\alpha = 0.01$. Our experiments show that this alternative perturbation achieves performance similar to MLP-activation perturbation, with both approaches enhancing hallucination detection. This further demonstrates the general effectiveness of Bayesian-inspired noise injection in capturing both aleatoric and epistemic uncertainty, ultimately enhancing hallucination detection.






\begin{table}[h]
%\vspace{-2mm}
\caption{
\textbf{Analysis on Perturbation Position.}
Noise injection at the attention activation (third row) performs comparably to injection at the MLP activation (second row), both improving detection effectiveness compared to no noise (first row), as indicated by higher AUROC scores. This further demonstrates the general effectiveness of Bayesian-inspired noise injection in capturing both aleatoric and epistemic uncertainty. Evaluation performed on the CSQA dataset with the \texttt{Llama-2-7B-chat} model across 10 samples. 
%\lliu{add ours}
}\label{tab:ablation_position}
\vspace{-2mm}
\begin{center}
\includegraphics[width=0.72\textwidth]{ablation_position_v2.pdf}
\end{center}
\vspace{-2mm}
\end{table}


\begin{comment}
\section{Visualization of Hallucination/Non-Hallucination Separation}

In Figure~\ref{fig:scheme-demo}, we visualize the enhancement of hallucination/non-hallucination separation with the number of generations $K = 10$. 
In the following, we visualize the same for $K = 5,  15,  20$. 
Across all visualizations, we observe that injecting noise enhances the separation between hallucination and non-hallucination instances, improving the effectiveness of detection. 

\begin{figure}[h]
%\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{10_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-7B-chat} model across \textbf{5} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

\begin{figure}[h]
%\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{15_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-7B-chat} model across \textbf{15} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}

\begin{figure}[h]
\vspace{-6mm}
\vspace{-1mm}
\begin{center}
\includegraphics[width=0.85\textwidth]{20_runs.png}
\caption{
\textbf{Intermediate Layer Randomness Enhances Hallucination Detection.}
Evaluation performed on GSM8K dataset with \texttt{Llama2-7B-chat} model across \textbf{20} generations. 
Rest of setup up follows Figure~\ref{fig:scheme-demo}~(b)
%\lliu{[change to better results / add AUROC / add legend / add (a)(b).]}
}%\label{fig:scheme-demo}
\end{center}
\vspace{-5mm}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{comment}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

\iffalse
Furthermore, as LLMs are usually 

use the pre-trained LLM weights $\bar{\text{w}}^l_{i,j}$ in \cref{eq4}.



Secondly, we approximate the distribution of likely models $p(\omega | \text{X},\text{Y})$ using a surrogate distribution $q(\omega)$. So, \cref{eq2} can be re-written as,
\begin{align}\label{eq4}
p(\text{y}_t |\text{y}_{<t},  \text{x}, \text{X},\text{Y}) \approx \int p(\text{y}_t |\text{y}_{<t}, \text{x}, \omega) q(\omega) d\omega \,.
\end{align}

Prior work focuses on the aleatoric uncertainty derived from a single LLM with weight $\bm{w}$. 
To measure such uncertainty, they sample multiple generations from the next token distribution \( \Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \bm{w})\) with non-zero temperature $T$ from a given prompt.
In this work, we propose to introduce epistemic uncertainty into hallucination detection. 
Specifically, we consider an ensemble of LLM with weight $\bm{w}$ sampled from distribution $g(\bm{w})$. 
Incooperating the epistemic uncertainty with the aleotoric uncertainty (sampling) leads to sample from distribution:  
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_{\bm{w}} \Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \bm{w})g(\bm{w}), \]
And as shown in prior work, this improves the overall uncertainty estimation effectiveness.  


%Specifically, they 
%at the final prediction stage by sampling from the next token distribution \( p(y_t \mid y_{<t}, \bm{x})\). 
%To effectively quantify model uncertainty requires not only an uncertainty metric $E(\cdot)$ but also a sufficiently diverse set of generations $\mathcal{Y}$, necessitating the introduction of randomness during generation.
%Prior work typically introduces randomness only at the final prediction stage by sampling from the next token distribution \( p(y_t \mid y_{<t}, \bm{x})\). 
%In addition, we introduce randomness at earlier stages. 


While training multiple LLMs from scratch to form an ensemble is prohibitively expensive, we introduce a simple, low-cost yet effective approach of creating multiple LLMs through noise injection.
The paragraph below is how we do it. 
This is equivalent to multiple LLMs because we effectively changes add noise to the LLM weights in a controlled manner.  



Consider a typical LLM consisting of an embedding layer, a stack of $L$ transformer layers, and a prediction layer $W$. 
At each decoding step \( t \), intermediate representations \( \bm{h}_t^l \) are computed layer by layer for a given input \( \bm{x} \). 
The next token probability \( p(y_t \mid y_{<t}, \bm{x}) \) explicitly conditioned on \( \bm{h}_t^L \) (and \( \bm{h}_t^{L-1} \) via skip connections) but is implicitly affected by earlier layers, as they shape these final representations.
This relationship can be expressed as:
%And the next token probability is computed as: 
\begin{equation}
p(y_t \mid y_{<t}, \bm{x}) = f(\bm{h}_t^1, \dots, \bm{h}_t^L).     
\end{equation}
%Prior work typically introduces randomness only at the final prediction stage by sampling from the next token distribution \( p(y_t \mid y_{<t}, \bm{x})\). 
We inject noise to perturb the intermediate representation at layers $l_1$ through $l_2$. 
As a result, given noise $\alpha$, the next token distribution is stochastically modified as 
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}, \alpha) = f(\bm{h}_t^1, \dots, \bm{\Tilde{h}}_t^{l_1}, \dots, \bm{\Tilde{h}}_t^{l_2},  \dots,\bm{h}_t^L ), \]
where each $\bm{\Tilde{h}}_t^l$ is a noise-perturbed version of $\bm{h}_t^l$. 
Notably, for \( l' > l^1 \), $\bm{h}_t^l$ is computed from the perturbed representations of prior layers.
%For all subsequent layers $l' > l_2$, the representations implicitly depend on the injected noise due to the perturbation of prior layers propagating forward during computation. 
With noise sampled from $g(\alpha)$ and randomized across generations, sampling from \(\Tilde{p}(y_t \mid y_{<t}, \bm{x}, \alpha)\) at each generation combines randomness at the prediction and intermediate layer. %:
%With noise sampled from $g(\alpha)$ and randomized across generations, the resulting marginal next-token distribution is given by
%\begin{equation}
%\Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_\alpha \Tilde{p}(y_t \mid y_{<t}, \bm{x}, \alpha)g(\alpha).    
%\end{equation}
 
%Sampling from \(\Tilde{p}(y_t \mid y_{<t}, \bm{x})\) combines randomness at the prediction and intermediate layer; whereas sampling from \(p(y_t \mid y_{<t}, \bm{x})\) only introduces randomness at the prediction layer. 
%While the randomness of \(p(y_t \mid y_{<t}, \bm{x})\) can be further adjusted by temperature,

\newpage
\section{Bayesian Interpretation}
To capture model uncertainty, we need to integrate over the distribution of likely models,  
\begin{align*}
p(\text{y}_t |\text{y}_{<t},  \text{x}) &= \int p(\text{y}_t, f |\text{y}_{<t},  \text{x}) \, df.\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x}, f) p(f) \, df.
\end{align*}
here, $f$ and $\text{x}$, $\text{y}_{<t}$ are independent.

This integral is intractable. But not all models are likely. Given the training set, we can update our belief of likely models,
\begin{align*}
p(\text{y}_t |\text{y}_{<t},  \text{x},  \text{X},  \text{Y}) &= \int p(\text{y}_t, f |\text{y}_{<t},  \text{x},  \text{X},  \text{Y}) \, df.\\
&= \int p(\text{y}_t, f |\text{y}_{<t},  \text{x},  \text{X},  \text{Y}) \, df.\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  f)  p(f | \text{X},  \text{Y} )\, df.\\
\end{align*}
here, $\text{y}_{t}$ is conditionally independent of $\text{X}$, $\text{Y}$ given $f$.


\begin{align*}
p(\text{y}_t |\text{y}_{<t},  \text{x},  \text{D}) &= \int p(\text{y}_t, \omega |\text{y}_{<t},  \text{x},  \text{D}) \, d\omega\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  \omega, \text{D}) p(\omega | \text{D}) \, d\omega\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  \omega)  p(\omega | \text{D} )\, d\omega\\
&\approx \int p(\text{y}_t |\text{y}_{<t},  \text{x},  \omega)  q(\omega)\, d\omega\\
\end{align*}
here, $\text{y}_{t}$ is conditionally independent of $\text{X}$, $\text{Y}$ given $f$.


If we want to have the prior term above,
\begin{align*}
p(\text{y}_t |\text{y}_{<t},  \text{x},  \text{X},  \text{Y})
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  f)  p(f | \text{X},  \text{Y} )\, df.\\
&= \int p(\text{y}_t |\text{y}_{<t},  \text{x},  f)  p(\text{X},  \text{Y} | f ) p(f)\, df + \text{const.}\\
\end{align*}

We framework is designed capture both model (epistemic) and observation (aleatoric) uncertainty \cite{kendall2017uncertainties}. To this end, we model the distribution of likely models $p(f | \text{X},\text{Y})$ given the training data $\{ \text{X},\text{Y} \}$, to capture epistemic uncertainty. 

Aleatoric uncertainty is modeled using a categorical output distribution over each token $\text{y}_t$,
\begin{align}\label{eq1}
\hat{\text{y}}_t \sim \text{Categorical}_{\hat{f}}(p_{v_1},p_{v_2}, \dots, p_{v_K})
\end{align}
where, $p_{v_1},p_{v_2}, \dots, p_{v_K}$ represents the probability of the $i^{th}$ token in the vocabulary assigned by the model $ \hat{f} \sim p(f | \text{X},\text{Y})$.

Here, we constrain the set of models to transformer based LLMs. The set of variables $\omega$ is further constrained to weights the MLP layers (after every self-attention layer). In detail, for the $l^{\text{th}}$ layer ($1 \leq l \leq \text{L}$), the variational distribution of the $j^{\text{th}}$ weight of the $i^{\text{th}}$ neuron, $\text{w}^l_{i,j}$ is defined by the uniform distribution,
\begin{align}\label{eq4}
q(\text{w}^l_{i}) = \bar{\text{w}}^l_{i,j} + \alpha^l_{i,j} \,.
\end{align}

Thus, we can estimate the integral in $\cref{eq2}$ using Monte Carlo sampling in two steps, i) sample weights ${\text{w}}^l_{i,j}$ using \cref{eq4}, ii) sample the next token $\text{y}_t$ using \cref{eq1}. 



where, $\sigma$ is the non-linear activation function. In case of ReLU or GELU type activation functions usually used in LLMs, 
\begin{align}\label{eq6}
p(\text{h}^{l}_{i}) \approx \mathcal{U}( \max(0,\text{low}^{l}_{i}), \max(0,\text{high}^{l}_{i}) ) 
\end{align}
Thus, for our choice of uniform variational distribution $q(\omega)$, the activations of the MLP layers in the LLM follow a uniform distribution.

\begin{align}\label{eq5}
\text{h}^{l}_{i} &= \sigma\Big( \sum_{j} \big(\bar{\text{w}}^l_{i,j} + \mathcal{U}(0,\alpha^l_{i,j}) \big) \, \text{h}^{l-1}_{i,j} + \mathcal{U}(0,\alpha^l_{i,j}) \Big) \\
 &= \sigma\Big( \sum_{j} \text{h}^{l-1}_{i,j} \, \bar{\text{w}}^l_{i,j}  + \text{h}^{l-1}_{i,j} \, \mathcal{U}(0,\alpha^l_{i,j}) + \mathcal{U}(0,\alpha^l_{i,j}) \Big) \\
 &== \sigma\Big( \sum_{j} \text{h}^{l-1}_{i,j} \, \bar{\text{w}}^l_{i,j} + \mathcal{U}(0,\alpha^l_{i,j}) \Big) \\
\end{align}

\myparagraph{Training.}

$\text{KL}(q(\omega) \, || \, p(\omega | \text{X}, \text{Y}))$

In order to estimate $q(\omega)$, the standard approach is to minimize the KL divergence between the variational distribution $q(\omega)$ and the true posterior $p(\omega | \text{X},\text{Y})$. This is usually done by minimizing the ELBO. However, LLMs are very expensive to train. Instead of training LLMs from scratch to estimate $q(\omega)$, we use pre-trained LLMs as an estimate for the variational distribution $q(\omega)$. Concretely, we use the pre-trained LLM weights an estimate for $\bar{\text{w}}^l_{i,j}$ in \cref{eq4}.
\fi

\iffalse
At each decoding step, the selected layers are perturbed and this perturbation stochastically modifies the next token probability
$p(\text{y}_t |\text{y}_{<t},  \text{x}, \text{D})$.
Across generations, we sample noise $\alpha$ independently and draw samples from the temperature-adjusted distribution \(\Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \alpha)\) with temperature \(T\).  
Effectively, our sampling process integrates over noise and follows the marginal distribution
\[ \Tilde{p}(y_t \mid y_{<t}, \bm{x}) = \int_\alpha \Tilde{p}_T(y_t \mid y_{<t}, \bm{x}, \alpha)g(\alpha), \]
where $g(\alpha)$ is the probability density function of $U(0, \alpha)^d$.
By perturbing the intermediate layer activation and sampling with a non-zero temperature at the final layer, our approach effectively combines two complementary sources of randomness.
To identify hallucinations, we compute the hallucination detection score over $K$ generations and apply a threshold to classify outputs.
\fi

\begin{algorithm}
\caption{Noise Enhanced Hallucination Detection}\label{alg:NED}
\begin{algorithmic}[1]
\INPUT Input context: \( \bm{x} \), noise magnitude \( \alpha \), number of samples \( K \), sampling temperature \( T\), perturbed layers \( l_1 \) to \( l_2 \), uncertainty metric \(E(\cdot)\). 
  \OUTPUT Hallucination detection score: \( s(\bm{x}) \)
\FOR{each generation \( k = 1 \) to \( K \)}
    %\STATE \texttt{// Initialize an empty set to store generated sequences}\\
    \STATE Sample noise \( \bm{\epsilon} \sim U(0, \alpha)^d \) 
    \FOR{each decoding step \( t \)}
        \FOR{each layer {l}}
            \STATE Compute $\bm{h}^l$ using the potentially perturbed prior layer representations.
            \STATE Perturb the MLP outputs: \( \bm{\Tilde{h}}^l = \bm{h}^l + \bm{\epsilon} \) if \( l \in [l_1, l_2] \). 
        \ENDFOR    
        \STATE %Noise injection effectively samples model weights $\hat{\omega}$ from $q(\omega)$ in \cref{eq4}, where perturbed set $\mathcal{S}$ akins to the bias in layer \( l_1 \) to \( l_2 \); 
        Sample token $\hat{\text{y}}^k_t \sim p(\text{y}_t \mid \text{y}_{<t}, \text{x}, \hat{\omega})$ with temperature T, since noise $\bm{\epsilon}$ modifies model as $\hat{\omega} \sim q(\omega)$ in \cref{eq4}. 
    \ENDFOR
\ENDFOR
\RETURN Hallucination detection score \( s(\bm{x}) = E(\mathcal{Y} ) \), where \( \mathcal{Y} = \{\bm{y}^1, \bm{y}^2, \dots, \bm{y}^K \}\)
\end{algorithmic}
\end{algorithm}