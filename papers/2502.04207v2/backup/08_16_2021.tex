\documentclass[]{spie}  %>>> use for US letter paper
%\documentclass[a4paper]{spie}  %>>> use this instead for A4 paper
%\documentclass[nocompress]{spie}  %>>> to avoid compression of citations

\renewcommand{\baselinestretch}{1.0} % Change to 1.65 for double spacing
 
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{cite} 
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{mwe}
\usepackage{acro}
\usepackage{amssymb}
\usepackage{xcolor,colortbl}
\usepackage{tabularx}
\usepackage{relsize}
\usepackage{pifont}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{multicol}
\usepackage{adjustbox}
\usepackage{float}

\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Self-supervised Learning with Large-scale Web Image Mining for Global Glomerulosclerosis Classification}

\author[a]{Tianyuan Yao}
\author[a]{Yuzhe Lu}
\author[a]{Zheyu Zhu}
\author[a]{Zuhayr Asad}
\author[b]{Haichun Yang}
\author[c]{Lee E. Wheless}
\author[b]{Agnes B. Fogo}
\author[a]{Yuankai Huo}

\affil[a]{Department of Computer Science, Vanderbilt University, Nashville, TN, 37235, USA}
\affil[b]{Department of Pathology, Microbiology and Immunology, Vanderbilt University Medical Center, Nashville, TN, 37232, USA}
\affil[c]{Department of Dermatology, Vanderbilt University Medical Center, Nashville, TN, 37232, USA}


\authorinfo{Further author information: Tianyuan Yao: E-mail: tianyuan.yao@vanderbilt.edu\\  Corresponding author: Yuankai Huo: E-mail: yuankai.huo@vanderbilt.edu}

% Option to view page numbers
\pagestyle{empty} % change to \pagestyle{plain} for page numbers   
\setcounter{page}{301} % Set start page numbering at e.g. 301
 
\begin{document} 
\maketitle

\begin{abstract}

% Global glomerulosclerosis (GGS) is a prevelent pathological phenotype for patients with both normal aging and kidney disease, 
% recent studies have demonstrated the diagnostic and prognostic values of GGS in IgA nephropathy, aging, and end-stage renal disease. The pathologic approach to the classification of GGS is problematic because morphological features are nonspecific and can occur in a variety of other conditions. Large-scale annotated medical images are typically more difficult to achieve compared with natural images, limited by domain knowledge and potential privacy constraints. Previous deep learning based methods in classifying GGS required extensive manual efforts to achieve tens of thousands annotated glomeruli from whole slide images(WSI). Fortunately, national-level efforts have been made to provide efficient access to obtain biomedical image data from previous scientific publications. Our previous work proposed a compound figure separation network to utilize the unannotated individual medical images extracted from compound figures in those abundant resources. In this study, we continue to evaluate the efficacy of leveraging self-supervised learning on GGS classification task with weak classification annotations from individual images. The pretrained self-supervised learning model on large-scale mined figures improved the accuracy of downstream image classification tasks with a contrastive learning algorithm.


\end{abstract}

% % Include a list of keywords after the abstract 
% \keywords{}

%   \begin{figure}[b]
%   \begin{center}
  
%   \includegraphics[height=8cm]{fig/problem1.pdf}

% 	\end{center}
%   \caption[example] 
%   { \label{fig:stroy} 
% This figure shows the overall motivation of this study, which is to measure partial liver volumetric variations across paired inspiratory-expiratory chest (PIEC) CT scans.}
%   \end{figure} 
 
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{fig/2.pdf}
\end{center}
   \caption{This figure shows the hurdle (red arrow) of training self-supervised machine learning algorithms directly using large-scale biomedical image data from biomedical image databases (e.g., NIH OpenI) and academic journals (e.g., AJKD). When searching desired tissues  (e.g., search ``glomeruli"), a large amount of data are compound figures. Such data would advance medical image research via recent self-supervised learning algorithms, such as self-supervised learning, contrasting learning, and auto encoder networks~\cite{huo2021ai}}
\label{fig:idea}
\end{figure*}   
\section{INTRODUCTION}
\label{sec:intro}  % \label{} allows reference to this section

Glomeruli are clusters of capillaries which are responsible for filtering the blood to form urine, thus excreting waster and maintaining fluid and acid-base balance. The detection and characterization of glomeruli are key elements in diagnostic and experimental nephropathology. Although the field of machine vision has already advanced the detection, classification, and prognostication of diseases in the specialties of radiology and oncology, renal pathology is just entering the digital imaging era. A quantitative approach that uses minimal human efforts (e.g., self-supervised learning) to achieve fine-grained classification of extent and type of global glomerulosclerosis(GGS) from whole slide image(WSI) would be very useful. The fine-grained glomerulosclerosis phenotype could provide more precise evidence to support both scientific research and clinical decision making. However, previous deep learning based methods in classification of GGS require extensive manual process. In renal pathology, dealing with large-scale images often requires domian experts'(pathologists) fully annotation to the whole dataset, sometimes even a manually correction by pathologists for missing, incorrectly and partiallly segmented glomerular units is needed, which are highly time cosuming and labor intensive. The important role of having large-scale images(even without annotations) for training a more generalizable AI model has been widely recognized. 
 

Recently, a new family of self-supervised representation learning, called contrastive learning, shows its superior performance in various vision tasks. Learning from large-scale unlabeled data, contrastive learning can learn discriminative features for downstream tasks. SimCLR maximizes the similarity between images in the same category and repels the representations of different category images. Wu et al. uses an offline dictionary to store all data representation and randomly selects training data to maximize negative pairs. MoCo introduces a momentum design to maintain a negative sample pool instead of an offline dictionary. Such works demand a large batch size in order to include sufficient negative samples. To eliminate the needs of negative samples, BYOL was proposed to train a model with an asynchronous momentum encoder. Recently, SimSiam was proposed to further eliminate the momentum encoder in BYOL, allowing for less GPU memory consumption. 

In our previous work, we demonstrate the application of our SimCFS framework and how it helps to provide massive biomedical image data and benefits further data analysis with self-supervised representation learning. We apply our SimCFS technique to conduct compound figure separation for renal pathology. Online resources (e.g., NIH  Open-i$^\circledR$~\cite{demner2012design} 
search engine, academic images released by journals) have provided a cost-effective and scalable way of obtaining large-scale images. Tens of thousands of free biomedical data are achieved by searching the desired tissue types. In renal phenotyping classification task, the self-supervised learning strategy achieved better accuracy than the fully supervised approach with ImageNet~\cite{deng2009imagenet} initialization.

In this study, we assess the feasibility of using self-supervised learning methods for fine-grained GGS classification. Global glomerulosclerosis was defined as hyaline deposition or scarring lesion occurring in more than 50$\%$ of any one glomerulus. Three types of GGS were assessed-solidified (S-GGS, associated with hypertension-related injury), disappearing (D-GGS, a further end result of the SGGS becoming contiguous with fibrotic interstitium) and obsolescent (O-GGS, nonspecifc GGS increasing with aging). We employed the SimSiam network as the baseline method of contrastive learning. The model was trained and tested with standard five-fold cross-validation. Beyond such a comparison, we also comprehensively evaluated the performance of our self-supervised pretrained model.


% \begin{figure} 
% \begin{center}
% \includegraphics[height=10cm]{fig/method.pdf}
% \end{center}
% \caption{This figure presents the registration framework of the proposed Hierarchical Intra-Patient Organ-specific  (HIPO)  registration  pipeline. The registration pipeline is divided into 3 stages: (1) scan-wise global registration, (2) liver mask based organ alignment, and (3) liver-wise local registration.}
% \label{fig:method} 
% \end{figure}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1\linewidth]{fig/4.pdf}
\end{center}
   \caption{}
\label{fig:method}
\end{figure*}

\begin{figure}

\centering

\subfigure[]{
\begin{minipage}[b]{0.5\textwidth}

\includegraphics[width=0.8\textwidth]{fig/5.pdf}

\end{minipage}

}

\subfigure[]{
\begin{minipage}[b]{0.5\textwidth}

\includegraphics[width=0.8\textwidth]{fig/6.pdf}

\end{minipage}

}

\subfigure[]{
\begin{minipage}[b]{0.5\textwidth}

\includegraphics[width=0.8\textwidth]{fig/7.pdf}

\end{minipage}

}

\caption{} 
\label{fig:task}

\end{figure}



\section{Method}

Our quantitative approach that uses minimal human efforts (e.g., self-supervised learning) to achieve fine-grained GGS classification consists of two parts: (1) data mining from biomedical databases and compound figure separation , and (2) self-supervised learning and downstream classification task.

% In image synthesis, our goal is to synthesize realistic looking images from the simulated masks. Then, the paired synthetic images and masks are used to train another segmentation network. \textbf{Note that, no manual annotations are used in our training either for CycleGAN or U-Net, as an unsupervised framework.}

\subsection{Object detection based compound figure separation}
In order to obtain massive scale of data of glomerulus with multi morphological features, we collected data from biomedical image databases (e.g., NIH OpenI) and academic journals (e.g., AJKD). Facing with the compound figure issues, Tsutsui and Crandall (2017)~\cite{tsutsui2017data} adapted a completely data-driven approach that treats compound figure segmentation as an object localization problem, and use Convolutional Neural Networks (CNNs), I.e. YoLo~\cite{redmon2016yolo9000} to estimate bounding boxes around each of a compound figure’s component parts. YOLOv5, the latest version in the YOLO family~\cite{bochkovskiy2020yolov4}, is employed as the backbone network for sub-figure detection. The rationale for choosing YOLOv5 is that the sub-figures in compound figures are typically located in horizontal or vertical orders. Herein, the grid-based design with anchor boxes is well adaptable to our application.

In our previous study, we extend Tsutsui's work by utilizing the latest YOLO and proposing a new side loss which is optimized for compound figure separation. Our proposed Side loss is designed based on the knowledge that there is no overlapping case in compound figures. By adding a penalty for the overestimated bounding box, the predictions are less overlapped as compared to the true box regions. 


\subsection{Self-supervised learning method}
Supervised learning refers the usage of a set of input variables to predict the value of a labeled output variable. It requires labeled data (like an answer key that the model can use to evaluate its performance). Conversely, self-supervised learning\cite{celebi2016unsupervised} refers to inferring underlying patterns from an unlabeled dataset without any reference to labeled outcomes or predictions.

We used the SimSiam network~\cite{chen2020simple} as the baseline method of self-supervised learning. Siamese networks are natural and effective tools for modeling invariance, which is
a focus of representation learning.

The SimSiam network can be interpreted as an iterative process of two steps: (1) unsupervised clustering and (2) feature updates based on clustering (similar to K-means or EM algorithms)~\cite{chen2020exploring}. Two random augmentations from the same image were used as training data. We take a ResNet without any classification head (backbone) and we add a shallow fully-connected network (projection head) on top of it, which is known as the encoder. Then the output of the encoder is passed through a predictor which is again a shallow fully-connected network having an autoencoder like structure. The network trains the encoder to maximize the cosine similarity between the two different versions of our dataset. The Simsiam network is pretrained on the massive single images extracted from the compound figures from the open biomedical database.


\section{Experiments}
We assess if the self-supervised learning method is optimal for fine-grained global glomerulosclerosis classification task by using the classical ResNet-50 architecture as the baseline and comparing our self-supervised pretrained model with the ImageNet pretrained models. The experiments are as follows: (1) evaluation of the convergence rate and performances by retraining all the layers among unpretrained ResNet-50 model, ResNet-50 model pretrained on Imagenet and our SimSiam pretrained model, meanwhile, the training time was recorded; (2) linear evaluation on different size of datasets, and (3) comparison of computational resources and time consumption. To demonstrate statistical significance, each experiment is evaluated for 5 runs.

To evaluate the result, as a multi-class setting, the macro-level average F1 score was used~\cite{attia2018multilingual}. Balanced accuracy was also broadly used to show the model performance on unbalanced data~\cite{5597285}.


% comparing our self-supervised pretrained model with the supervised learning method using ResNet-50 models pretrained on ImageNet. We used the SimSiam network as the baseline method of contrastive learning.  Our pretrained model is training on over 30,000 unannotated glomeruli pathologies which are obtained from online scientific publications. The model was then trained and tested with standard five-fold cross-validation on 11,462 glomeruli and 10,619 non-glomeruli pathologies covering 7,841 globally sclerotic glomeruli pathologies. Beyond such a comparison, we also comprehensively evaluate the performance of our self-supervised pretrained model through(1) time consumption and (2) different size of data .


\subsection{Data}

We collected two in-house datasets and one public dataset for evaluating our quantitative approach of fine-grained GGS classification. 


Our pretrained model is training on over 30,000 unannotated glomeruli pathologies which are obtained from online scientific publications. We first collected 10,000 compound figures with the keywords “glomerular OR glomeruli OR glomerulus” through the NIH Open-i$^\circledR$ search engine. Then we used our SimCFS network to process all compound images to get more than 30,000 glomeruli pathologies with a confidence threshold of 0.7. These images are obtained by different microscopy or in different stains which are in multi morphological features.

The model was then trained and tested on 11,458 glomeruli and 10,619 non-glomeruli pathologies covering 7,841 globally sclerotic glomeruli pathologies. These glomeruli patches are from human nephrectomy tissues that were acquired from 157 patients, whose tissues were routinely processed and paraffin-embedded, with 3 $\mu$m thickness sections cut and stained with PAS. 22,077 glomeruli were extracted from WSI using the EasierPath semi-manual annotation software~\cite{2020arXiv200713952Z}. Then, all glomeruli were manually annotated to 5 classes, including 3,617 normal glomeruli, 6,647 global obsolescent glomeruli, 735 global solidified glomeruli, and 459 global disappearing glomeruli, and 10,619 non-glomeruli. The join of class of non-glomeruli is for hard negative mining to deal with the data imbalance problem. The images were resized to 256$\times$256-pixel for training, validation, and testing. The data were de-identified, and studies were approved by the Institutional Review Board (IRB). All the model was trained and tested with standard five-fold cross-validation on this dataset. Furthermore, The data was split into five folds at patient level, where each fold was withheld as testing data once. The remaining data for each fold was split as 75\% training data and 25\% validation data. Therefore, for each fold, the final split was 60\% training, 20\% validation, and 20\% test. To avoid data contamination, all glomeruli from a patient were used either for training, validation or testing.

An external glomeruli dataset \cite{bueno2020data} is also used to evaluate the performance of our model. The dataset consists of 2,340 images containing a single glomerulus. Among them, 1,170 glomeruli are normal and 1,170 glomeruli are sclerosed. As this dataset is collected from a completely different population, it can be used to demonstrate the robustness of our approach to distribution shift. As the models were trained in five-fold cross-validation. When applying the trained model on this test dataset, the predicted probabilities from five models were averaged. Then, the class with the largest ensemble probability was used as the predicted label. 



\subsection{Experiment design}

\subsubsection{Supervised learning}

We used ResNet-50 as the backbone in supervised training. The model was then trained using the Adam optimizer with a base learning rate of 1e-4. The optimizer learning rate followed (linear scaling~\cite{goyal2017accurate}) $lr\times$BatchSize$/256$. To weaken the contribution of easy cases to the loss function, we use the focal loss \cite{lin2017focal} with $\gamma=2.5$. Each fold was trained for 70 epoch, and the epoch with the best balanced accuracy score on the validation set is used for testing. Our pretrained model and the ImageNet pretrained model are also applied with the same implementation details above. The best epoch, training time consumption and gpu occupancy rate is recorded.


\subsubsection{Self-supervised learning}

We used the SimSiam network~\cite{chen2020simple} as the baseline method of contrastive learning. Two random augmentations from the same image were used as training data. In all of our self-supervised pre-training, images for model training were resized to $224\times224$ pixels. We used momentum SGD as the optimizer. Weight decay was set to 0.0001. Base learning rate was $lr=0.05$ and batch size equals 64. Learning rate was $lr\times$BatchSize$/256$, which followed a cosine decay schedule~\cite{loshchilov2017sgdr}.

\subsubsection{Linear Evaluation}

To apply the self-supervised pre-training networks, we froze our pretrained ResNet-50 model by adding one extra linear layer which followed the global average pooling layer. When finetuning with manually annotated glomeruli data, only the extra linear layer was trained. We used focal loss and the Adam optimizer to train linear classifier with a based (initial) learning rate $lr$=30, weight decay=0, momentum=0.9, and batch size=64 (follows~\cite{chen2020exploring}). we use the focal loss \cite{lin2017focal} with $\gamma=2.5$. We trained linear classifiers for 70 epochs and selected the best model based on the validation set.

To evaluate the impact of training data size, we fine-tuned the classifier of our self-supervised learning model and the ImageNet pretrained model on 1, 10 and 100 percentages of annotated training data.

\section{Results}

The results presented in this section are divided into two main parts to explore each aspect of our experimentation: (1) fine-grained classification of GGS and (2) external validation on public dataset.

\subsection{Compared with supervised method}

\noindent\textbf{Convergence speed} 

As is evident here, model pre-training speeds up convergence on the target task. Models that are pre-trained are good at detecting high-level features like edges, patterns, etc. These models understand certain feature representations, which can be reused. This helps in quicker convergence and is used in state-of-the-art approaches to tasks like object detection, segmentation, and activity recognition.
Our pretrained contrastive learning model can boost accuracy without taking much time to converge, as compared to the ImageNet pretrained ResNet-50. Retraining all the layers with the same optimizer and learning rate for 70 epochs, the SimSiam pretrained model gets an balance accuracy of 75\% at an average number of 42 epochs, as compared to the average epoch of 53 of ImageNet pretrained model.

\noindent\textbf{Performance} 

As shown in Table \ref{table:performance}, fine-tuning our pretrained SimSiam (Backbone:ResNet-50) on the same size of dataset is significantly better then training from scratch. Interestingly, our model also outperformed ResNet-50 models pretrained on ImageNet in different labeled data level. Even using only 10$\%$ of all available labeled data, our model still achieved close performance compared with supervised learning. Besides, with our quantitative approach, the required amount of training time and human efforts is far smaller than the previous supervised method to reach a desired result.



\begin{table}[]
\centering{}
\caption{Classification performance.}
\begin{tabular}{lllllll}
\toprule
Model                & Unlabeled data & Labeled data & Balance acc & F1    & Time \\
\midrule
Random int           & 0              & 100\%        & 65.0        & 61.7  &   159.8   \\
\midrule
\multirow{3}{*}{Supervised}  & \multirow{3}{*}{0}      & 100\%        & 62.0     & 51.9  &   43.7   \\
                              &                           & 10\%         & 59.2        & 48.4  &   29.05   \\
                              &                          & 1\%          & 52.3        & 42.1  &   28.2   \\
\midrule
\multirow{3}{*}{SimSiam}     & \multirow{3}{*}{30k}    & 100\%    & \textbf{68.0}    & \textbf{62.5} &   42.1  \\
                              &                          & 10\%         & 65.1        & 59.7  &  33.2    \\
                              &                          & 1\%          & 59.2        & 52.8  &  27.8   \\
\bottomrule
\end{tabular}
\label{table:performance}
\newline
\text{*For supervised method, we use ResNet-50 models pretrained on ImageNet.}
\newline
\text{*The time is measured for each epoch and calculated the average.}
\newline

\end{table}





\subsection{External Validation on Public Dataset}

In external validation, as shown in  Table \ref{table:performance2}, our models show robust performance in distribution shift. The contrastive learing model can achieve an AUC score of 94.5 when testing directly on the external dataset without any finetuning. Even the SimSiam model previous finetuned on 10$\%$ of all available labeled data can reach a better accuracy than the supervised method. 

\begin{table}
\caption{Performance on external data set}
\begin{center}
\begin{tabular}{lll}
\toprule
Methods  & Labeled data   &AUC         \\
\midrule
Random Int  & 100\%      &91.3          \\
Supervised  & 100\%        &92.4           \\ 
\midrule
\multirow{2}{*}{SimSiam}    & 100\%         &\textbf{94.5}       \\ 
                            &  10\%         & 92.6                 \\ 
\bottomrule
\label{table:performance2}
\end{tabular}
\end{center}

\end{table}

\section{Conclusions and Discussion}
\label{sec:conclusion}
In this study, we evaluated an self-supervised learning method on GGS classification task with weak classification annotations from individual images. The proposed quantitative approach, the required amount of training resources and human efforts is far smaller than the previous supervised method to reach a desired result.

\noindent\textbf{Future Validations:} Future validations will be made a comparison between current contrastive learning sratigies, e.g. BYOL, SWAV, etc.

\noindent\textbf{Future Improvements:} Future improvements could be build a more balanced data set and trained a more
generalizable AI model. 



% \section{Conclusions and Discussion}
% \label{sec:conclusion}
% In this study, we evaluated the volumetric variations across PIEC CT scans from 23 patients, by proposing the HIPO registration pipeline. After performing the cycle registration error based QA and the visual QA, 13 patients passed the QA process. For the 13 patients, the average volumetric variations of liver were $6.0\%$, from inspiratory phase to expiratory phase.

% \noindent\textbf{Future Validations:} As a pilot study, the average liver volumetric variations were measured to be $6.0\%$  using our HIPO method. Although an automatic QA (circle consistent error) and another manual QA (visual inspection) were performed, more rigorous future clinical validations will still be necessary to validate this finding. One potential method to validate the algorithm will be to mine the large-scale ImageVU cohort at Vanderbilt University to find a subset of patients with the complete liver included in PIEC CT scans. Next, the manual liver annotations will be performed on both paired scans to quantify the liver volumetric variations. Another method, even better, will be to recruit a small cohort of patients (half cirrhosis half normal) to perform contrast-enhanced abdominal CT imaging under inspiratory-expiratory protocol, containing complete livers. Then, the more precise manual liver annotations will be enabled on contrast-enhanced CT scans to validate the liver volumetric variations across the respiratory cycle.


% \noindent\textbf{Future Improvements:} The future technical improvements would be: (1) to replace the manual liver annotation and cropping with automatic methods~\cite{raju2020co} or even one-shot learning based methods~\cite{lu2020learning}, (2) to investigate and develop more precise registration algorithms to further reduce the registration error, (3) to include more subjects with clinical phenotypes to expand the study cohort, and (4) to associate the volumetric variation with clinical phenotypes, such as hepatic fibrosis and cirrhosis. 




\section{ACKNOWLEDGMENTS}       
This work has not been submitted for publication or presentation elsewhere.This work was supported by NIH NIDDK DK56942(ABF). This work was support by a grant from the Skin Cancer Foun-dation and the Dermatology Foundation.



% References
\bibliography{main} % bibliography data in report.bib
\bibliographystyle{spiebib} % makes bibtex use spiebib.bst

\end{document} 
