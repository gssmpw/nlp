\section{Experiments}
\label{sec:evaluation}
In this section, we first describe the datasets used. Secondly, we detail the framework techniques and baseline \revision{conformance checking}-based techniques with which we experiment. Finally, the goals of the experiments, the performance metrics used, and the results of the experiments are reported and discussed.

\subsection{Datasets}
The datasets include well-known benchmarks for anomaly detection in event logs (PDC 2020/2021), simulation related to a railways case study (ERTMS), and real-world data from a healthcare case study (COVAS).

\paragraph{PDC 2020/2021}
The PDC datasets\footnote{\url{https://www.tf-pm.org/competitions-awards/discovery-contest}} are generated by simulation of a variety of Petri nets. For each PDC dataset, the different Petri nets are obtained by tuning several characteristics of a base Petri net, introducing more complex control-flow patterns such as non-local dependencies between activities. PDC 2020 contains two event log sets named \texttt{ground\_truth} and \texttt{training}. Each event log of the \texttt{training} set is generated by randomly walking one of the Petri net variants 1000 times, thus producing 1000 traces. In addition to simulating the Petri net variant, the training set contains an event log corresponding to a Petri net variant with noise introduced in the traces. For example, given $N$ the Petri net variant and $L_N$ the simulated event log of the \texttt{training} set, the authors produce the noisy event log $\tilde{L}_N$.
Regarding the \texttt{ground\_truth} set, there are as many event logs of 1000 traces as Petri net variants. However, the authors did not mention whether these event logs were obtained by randomly walking the corresponding Petri net variant. Instead, they label each trace as either \texttt{positive} or \texttt{negative} to allow practitioners to test their algorithms. In our experimentation, we organize the dataset as follows. We take the \texttt{training} event logs of two Petri net variants and the corresponding \texttt{ground\_truth} event logs. Then, we consider normal one of the two variants, and anomalous the other. Specifically, all the traces of the \texttt{training} event logs of the normal variant are considered normal, and \texttt{positive} traces of the \texttt{ground\_truth} event log are also considered normal. Conversely, all traces of the \texttt{training} event logs of the anomalous variant are considered anomalous, and \texttt{positive} traces of the \texttt{ground\_truth} event log are also considered anomalous because they fit the anomalous variant. The same process is applied to the PDC 2021 dataset. Finally, we consider the base Petri net in both datasets as the input Petri net for \revision{conformance checking} \revision{process mining}-based feature extraction.

\paragraph{ERTMS}
The European Rail Traffic Management System (ERTMS)\footnote{\url{https://www.ertms.net/}} is a standard that regulates European railways for the smooth and safe operation of trains across Europe. ERTMS prescribes textual requirements for several procedures and provides informal activity and state diagrams to facilitate the implementation of these procedures. In this work, we consider the RBC/RBC Handover procedure, which regulates the process that trains must follow when their journey involves changing their supervision from one RBC to another, where the RBC is an entity whose supervision scope involves a limited area of on-track equipment and trains. This procedure was considered in \cite{debenedictis2023dtadiiot} and used as proof of concept for the experimentation of anomaly detection in simulated traces generated by a BPMN model of the RBC/RBC Handover. We replicate the anomaly injection process proposed by the authors as follows. We consider the resources associated with the activities of the BPMN model of the RBC/RBC Handover and randomly walk the model to generate 2000 traces. Then, we split these 2000 traces into two normal and anomalous event logs of 1000 traces each. Each trace of both event logs is injected with anomalies. This injection: randomly selects a resource among those involved in RBC/RBC Handover, considers the activities associated with this resource, and proceeds to introduce missed, duplicated, or wrongly-ordered activities in the trace uniformly according to a given probability. This probability depends on the nature of the trace. In our simulations, we set the probability to 5\% for each anomaly type in normal traces and 25\% for each anomaly type in anomalous traces. This means that, given a normal trace and assuming the introduction of each anomaly is independent, there is $1-0.95^3\approx15\%$ probability of having at least one of the anomalies, whereas, given an anomalous trace and assuming the same, there is $1-0.75^3\approx57\%$ probability of having at least one of the anomalies.

\paragraph{COVAS}
The COVID-19 Aachen Study (COVAS) involved monitoring the treatment process of COVID-19 patients from March 2020 to June 2021 \cite{pegoraro2021covas,benevento2022covas}. The data are preprocessed by cleaning incomplete traces, using abstractions for some activities of negligible significance, and filtering infrequent variants. Furthermore, the data are split into three event logs, according to three different time frames matching the first, second and third infection waves. This split is backed up by literature, as there have been significant differences in COVID-19 patients' clinical course and treatment among these three infection waves. The three COVAS event logs have 106, 59, and 22 traces. The traces are labeled following the viewpoint of the first infection wave. Hence, the first-wave event log contains normal traces, whereas the other second- and third-wave event logs are anomalous. To achieve a dataset whose dimension is comparable to the others, we augment the data by random re-sampling of traces of the same kind. Finally, we consider a Petri net related to the first infection wave and discovered by authors in \cite{benevento2022covas} as the input Petri net for \revision{conformance checking} \revision{process mining}-based feature extraction.

For all datasets, the normal and anomalous traces are grouped into event logs of 5 traces each, resulting in two sets of normal and anomalous event logs. All anomalous event logs are retained in the test set, whereas the normal event logs are split as follows. 25\% of normal event logs are held out and placed into the test set and the remainder is used as the training set. Next, 20\% of the training set is held out and placed into the validation set. Table \ref{DATASETS_INFO} collects the properties of the datasets. These properties outline whether the target application is real or abstract, the nature is synthetic or real-world, and the labeling is synthetic or done by domain experts. For example, let us consider the ERTMS dataset. It is a real application because it involves a railway standard, its nature is synthetic because the data is obtained by simulating the model, and the labels are synthetic since they depend on the probabilistic injection of anomalies. Furthermore, we report the number of normal and anomalous traces, and the resulting training, validation and test event logs obtained according to the percentages above. 
\begin{table}[!t]
\centering
\caption{The properties of the datasets for the experiments.}
\label{DATASETS_INFO}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllll}
\toprule
\textbf{Property}              & \textbf{COVAS}    & \textbf{ERTMS} & \textbf{PDC 2020} & \textbf{PDC 2021} \\ \midrule
\textbf{Application}           & Real              & Real           & Abstract          & Abstract          \\
\textbf{Nature}                & Real-world        & Synthetic      & Synthetic         & Synthetic         \\
\textbf{Labeling}              & By domain experts & Synthetic      & Synthetic         & Synthetic         \\
\textbf{Normal traces}         & 1000 (augmented)             & 1000           & 3020              & 4250              \\
\textbf{Anomalous traces}      & 1000 (augmented)             & 1000           & 2496              & 2125              \\
\textbf{Training event logs}   & 120               & 120            & 363               & 511               \\
\textbf{Validation event logs} & 30                & 30             & 90                & 127               \\
\textbf{Test event logs}       & 250               & 250            & 650               & 637               \\ \bottomrule
\end{tabular}%
}
\end{table}
\subsection{Techniques}
\subsubsection{Baselines}
In this work, we consider the \revision{conformance checking}-based techniques that rely on fitness thresholding as the baselines \cite{bezerra2009pmad, bezerra2013adlogspais, myers2018icsadpm, pecchia2020applicationfailuresanalysispm}. As mentioned, Figure \ref{CC_UNBALANCING} highlights that setting a threshold for the fitness measure is challenging since the fitness values are spread inconsistently, even though the normal event log and the reference Petri net are, respectively, the set of first-wave traces and Petri net from the COVAS case study. This is a critical issue, especially because the reference Petri net was obtained after laborious pre-processing, hence it should be of reasonable quality. Regardless, \revision{conformance checking}-based techniques that rely on fitness thresholding are worth comparing due to their explainable nature. In fact, the fitness measure comes from a clear-cut model-based definition (e.g., Definition \ref{def:ab_fitness}), which provides a straightforward explanation of why an event log is classified as anomalous.

We implement two baseline \revision{conformance checking}-based techniques. Firstly, token-based/alignment-based \revision{conformance checking} is applied to all event logs in the validation set against the reference Petri net to obtain the fitness measures for all the validation event logs. Secondly, a threshold is automatically assigned using the minimum of all the fitness measures. This is because, given that every event log in the validation set is normal, the best generalization should be achieved by considering the least fitting event log. We name AB\_CC\_B and TB\_CC\_B the baselines employing token-based and alignment-based \revision{conformance checking}, respectively.

\subsubsection{Framework}
The framework allows implementing a wide range of \revision{conformance checking}-based and \revision{conformance checking}-independent control-flow anomaly detection techniques by combining a \revision{process mining}-based feature extraction approach with a dimensionality reduction technique. This combination allows assessing the impact of diverse \revision{process mining}-based feature extraction approaches on the results \cite{ko2023adsystematicreview, tavares2023pmtraceencoding} and replicating the reconstruction-based approach for control-flow anomaly detection \cite{nolle2018processadautoencoders, vijayakamal2020bpaead, elaziz2023drlbpad, chinnaiah2024deepaead, kan2024aebasedelad}.

We implement 16 framework techniques for control-flow anomaly detection. These techniques combine the proposed (Section \ref{sec:abccfe}) and reviewed (Section \ref{sec:framework}) \revision{process mining}-based feature extraction approaches, namely the alignment-based \revision{conformance checking} (AB\_CC), token-based \revision{conformance checking} (TB\_CC), N-grams (NG) and directly-follows (DF) approaches, and the reviewed (Section \ref{sec:framework}) dimensionality reduction techniques, namely the PCA, SPCA, KPCA and autoencoder (AE). Table \ref{DR_TECHNIQUES_CONFIGURATIONS} provides an overview of the parameters used to configure the dimensionality reduction techniques during the validation step of the framework, which performs an exhaustive search of all the parameter values to minimize the reconstruction error. Each framework technique is labeled by a pair of values (PF, DR), where PF $\in$ \{AB\_CC, TB\_CC, NG, DF\} indicates the \revision{process mining}-based feature extraction approach and DR $\in$ \{PCA, KPCA, SPCA, AE\} the dimensionality reduction technique.

\begin{table*}[!t]
\centering
\caption{Parameters used to configure the dimensionality reduction techniques during the validation step of the framework. N/A = Not Applicable.}
\label{DR_TECHNIQUES_CONFIGURATIONS}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccc}
\toprule
\textbf{$\boldsymbol{\mathcal{DR}}$} & \textbf{Parameter \#1}                                          & \textbf{Parameter \#2}                                                                                  & \textbf{Parameter \#3}                                                     & \textbf{Parameter \#4}                                                                           & \textbf{Parameter \#5}                                                     & \textbf{Parameter \#6}                                                 \\ \midrule
\textbf{PCA}            & \begin{tabular}[c]{@{}c@{}}$f_R$\\ \{2, 4, 8, 16\}\end{tabular} & N/A                                                                                                     & N/A                                                                        & N/A                                                                                              & N/A                                                                        & N/A                                                                    \\
\textbf{SPCA}           & \begin{tabular}[c]{@{}c@{}}$f_R$\\ \{2, 4, 8, 16\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Regression regularization\\ \{0.01, 0.1, 0.25, 0.5, 0.75, 1.0\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sparsity coefficient\\ \{0.1, 0.5, 1, 2, 3\}\end{tabular}   & N/A                                                                                              & N/A                                                                        & N/A                                                                    \\
\textbf{KPCA}           & \begin{tabular}[c]{@{}c@{}}$f_R$\\ \{2, 4, 8, 16\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Regression regularization\\ \{0.01, 0.1, 0.25, 0.5, 0.75, 1.0\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Kernel\\ \{poly, rbf, sigmoid\}\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Kernel coefficient\\ \{0.01, 0.05, 0.1\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Polynomial degree\\ \{3, 4, 5, 6\}\end{tabular} & N/A                                                                    \\
\textbf{Autoencoder}             & \begin{tabular}[c]{@{}c@{}}$f_R$\\ \{2, 4, 8, 16\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Hidden neurons\\ \{32, 64, 128, 256\}\end{tabular}                           & \begin{tabular}[c]{@{}c@{}}Optimizer\\ \{adam, rmsprop, SGD\}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Batch size\\ \{8, 16, 32, 64\}\end{tabular}                           & \begin{tabular}[c]{@{}c@{}}Epochs\\ \{100, 250, 500\}\end{tabular}         & \begin{tabular}[c]{@{}c@{}}Activation function\\ \{ReLu\}\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Evaluation}
The experiments are designed to evaluate two aspects of our research, which are the following: 1) compare to the baseline \revision{conformance checking}-based techniques the detection effectiveness and explainability of the framework techniques using our proposed \revision{process mining}-based feature extraction approach; and 2) compare the detection effectiveness of \revision{conformance checking}-based and \revision{conformance checking}-independent framework techniques and evaluate the factors that impact their performance. The performance of the techniques is evaluated by their accuracy ($A$), recall ($R$), precision ($P$) and F1-Score ($F1$).
\begin{definition}[Accuracy, recall, precision and F1-score]
Let $\mathcal{L}\in(\mathcal{B}(\mathcal{A}^*))^n$ be an $n$-tuple of labeled test event logs. Let positive samples be normal event logs and negative samples be anomalous event logs. Finally, let $TP$, $TN$, $FP$ and $FN$ be, respectively, the true positives, true negatives, false positives, and false negatives. Accuracy $A$, recall $R$, precision $P$, and F1-Score $F1$ are defined as follows:
\begin{equation*}
A=\frac{TP+TN}{TP+TN+FP+FN};\,R=\frac{TP}{TP+FN};
\end{equation*}
\begin{equation}
    P=\frac{TP}{TP+FP};\,F1=\frac{2RP}{R+P}.
\end{equation}
\end{definition}

The software for the experiments is implemented using Python and was run on a Windows 10 machine with an Intel® Core™ i9-11900K CPU @ 3.50GHz and 32GB of RAM memory. The software uses libraries for machine learning and \revision{process mining}, such as \texttt{scikit-learn}, \texttt{tensorflow} and \texttt{pm4py}. In addition, it includes a set of DOS batch scripts to automatically replicate the experiments carried out in this work, available online on GitHub\footnote{\url{https://github.com/francescovitale/pm_dr_framework}}.

\subsubsection{Baselines comparison}
In this part, we compare the detection effectiveness of the framework techniques using our proposed \revision{process mining}-based feature extraction approach, namely (AB\_CC, PCA), (AB\_CC, SPCA), (AB\_CC, KPCA) and (AB\_CC, AE), with the baselines. Each technique is applied to each dataset 5 times to account for statistical fluctuations due to the random split of training, validation and test sets.


\paragraph{Results} Table \ref{BASELINES_COMPARISON} reports the mean $A$, $R$, $P$ and $F1$ measures and their standard deviation in subscripts for each technique and dataset. Except for AB\_CC\_B's $R$ measure for the COVAS dataset (89.1\%), the \revision{conformance checking}-based framework techniques using our proposed \revision{process mining}-based feature extraction approach outperform both baselines, in some cases by a very large degree. For the PDC 2020 and 2021 datasets, the (AB\_CC, AE) technique achieves 97.3\% and 87.2\% $F1$, whereas the AB\_CC\_B and TB\_CC\_B baselines achieve 36.1\% and 19.4\% $F1$ for the PDC 2020 dataset, and 33.4\% and 56.6\% $F1$ for the PDC 2021 dataset. The performance gap is smaller for the ERTMS and COVAS datasets, for which (AB\_CC, AE) and (AB\_CC, KPCA) achieve 85.2\% and 88.5\% $F1$, whereas the AB\_CC\_B and TB\_CC\_B baselines both achieve 78.8\% $F1$ for the ERTMS dataset, and 79.1\% and 18.4\% $F1$ for the COVAS dataset.

\begin{table*}[!t]
\centering
\caption{The mean $A$, $R$, $P$ and $F1$ measures and their standard deviation in subscripts for each framework/baseline technique and dataset. Measures in bold indicate the highest figure per dataset.}
\label{BASELINES_COMPARISON}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllllllllllllllllll}
\toprule
\multicolumn{2}{l}{\textbf{}}          &  & \multicolumn{4}{l}{\textbf{PDC 2020}}                                                         &  & \multicolumn{4}{l}{\textbf{PDC 2021}}                                                         &  & \multicolumn{4}{l}{\textbf{ERTMS}}                                                            &  & \multicolumn{4}{l}{\textbf{COVAS}}                                                            \\ \cmidrule{4-7} \cmidrule{9-12} \cmidrule{14-17} \cmidrule{19-22} 
\multicolumn{2}{l}{\textbf{Technique}} &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       \\ \midrule
\multicolumn{2}{l}{(AB\_CC, PCA)}      &  & 96.2$_{1.0}$          & 92.7$_{1.2}$          & 96.5$_{2.2}$          & 94.4$_{1.4}$          &  & 86.0$_{2.4}$          & 83.5$_{2.4}$          & 84.8$_{3.0}$          & 84.0$_{2.6}$          &  & 87.6$_{1.3}$          & 76.8$_{4.6}$          & 82.3$_{1.6}$          & 78.8$_{3.4}$          &  & 85.1$_{6.4}$          & 77.5$_{5.8}$          & 78.5$_{9.9}$          & 77.5$_{7.7}$          \\
\multicolumn{2}{l}{(AB\_CC, SPCA)}     &  & 96.8$_{1.0}$          & 93.8$_{1.4}$          & 97.1$_{2.0}$          & 95.3$_{1.4}$          &  & 86.3$_{2.9}$          & 83.1$_{2.6}$          & 85.8$_{3.8}$          & 84.1$_{3.1}$          &  & 88.2$_{2.3}$          & 78.8$_{5.5}$          & 82.5$_{3.4}$          & 80.3$_{4.7}$          &  & 91.0$_{4.8}$          & 87.6$_{7.2}$          & 85.8$_{7.5}$          & 86.5$_{7.0}$          \\
\multicolumn{2}{l}{(AB\_CC, KPCA)}     &  & 96.4$_{0.7}$          & 92.1$_{1.5}$          & 97.8$_{0.4}$          & 94.6$_{1.1}$          &  & 86.6$_{1.4}$          & 83.5$_{2.3}$          & 85.9$_{1.6}$          & 84.4$_{1.9}$          &  & 89.3$_{1.7}$          & \textbf{87.8$_{2.3}$} & 83.1$_{3.4}$          & 84.7$_{1.7}$          &  & \textbf{92.8$_{1.0}$} & 87.9$_{3.2}$          & \textbf{89.5$_{1.1}$} & \textbf{88.5$_{2.0}$} \\
\multicolumn{2}{l}{(AB\_CC, AE)}       &  & \textbf{98.1$_{0.3}$} & \textbf{96.6$_{1.3}$} & \textbf{98.1$_{0.5}$} & \textbf{97.3$_{0.5}$} &  & \textbf{88.9$_{1.1}$} & \textbf{86.2$_{1.3}$} & \textbf{88.4$_{1.4}$} & \textbf{87.2$_{1.3}$} &  & \textbf{91.2$_{1.9}$} & 83.3$_{5.8}$          & \textbf{88.3$_{1.6}$} & \textbf{85.2$_{4.1}$} &  & 91.4$_{3.1}$          & 85.6$_{6.5}$          & 87.0$_{5.0}$          & 86.2$_{5.7}$          \\ \midrule
\multicolumn{2}{l}{AB\_CC\_B}          &  & 36.6$_{4.3}$          & 58.6$_{2.2}$          & 62.3$_{0.5}$          & 36.1$_{4.0}$          &  & 38.7$_{4.6}$          & 53.8$_{3.2}$          & 65.9$_{1.9}$          & 33.4$_{6.7}$          &  & 82.3$_{10.0}$         & 87.6$_{5.6}$          & 78.2$_{8.7}$          & 78.8$_{10.3}$         &  & 82.8$_{4.9}$          & \textbf{89.1$_{2.7}$} & 77.3$_{4.4}$          & 79.1$_{5.2}$          \\
\multicolumn{2}{l}{TB\_CC\_B}          &  & 23.1$_{0.2}$          & 49.9$_{0.6}$          & 54.7$_{7.6}$          & 19.4$_{0.5}$          &  & 57.0$_{2.4}$          & 67.6$_{1.8}$          & 71.4$_{0.7}$          & 56.6$_{2.7}$          &  & 81.9$_{11.9}$         & 87.7$_{6.5}$          & 78.7$_{9.5}$          & 78.8$_{12.0}$         &  & 21.2$_{0.0}$          & 50.4$_{0.0}$          & 56.9$_{4.0}$          & 18.4$_{0.0}$          \\ \bottomrule
\end{tabular}%
}
\end{table*}
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{images/SHAP_EXPLANATION.png}
\caption{SHAP (absoulte) values related to the additive feature-based explanation of the four \revision{conformance checking}-based framework techniques (AB\_CC,PCA), (AB\_CC,SPCA), (AB\_CC,KPCA), (AB\_CC,AE) using the PDC 2020 dataset.}
\label{SHAP_EXPLANATION}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{images/FITNESS_VALUES_DISTRIBUTION.png}
\caption{The normal and anomalous histograms that show the frequency of occurrence of the fitness measures of normal and anomalous event logs per dataset for the AB\_CC\_B and TB\_CC\_B baseline approaches. The intersection highlights the overlap between anomalous and normal histograms.}
\label{FITNESS_VALUES_DISTRIBUTION}
\end{figure*}


\paragraph{Framework techniques explanation} Let us consider the anomaly explanation of techniques (AB\_CC, PCA), (AB\_CC, SPCA), (AB\_CC, KPCA), (AB\_CC, AE) for the PDC 2020 dataset, whose Petri net is depicted on the left side of Figure \ref{PETRI_NET_TRACES}. Figure \ref{SHAP_EXPLANATION} shows the SHAP (absolute) values for each technique. These values show the influence of the per-activity cost of each event log activity and labeled transition of the reference Petri net on the reconstruction error. For each activity, the mean SHAP value of anomalous (red) and normal (green) event logs is shown. The bar plot highlights that the high detection effectiveness of each framework technique is due to: 1) the small reconstruction error of per-activity costs in normal event logs; and 2) the high reconstruction error of per-activity costs in anomalous event logs. For example, the SHAP values of (AB\_CC, AE) highlight that the influence on reconstruction error of per-activity costs of normal event logs is strictly less than the reconstruction error of per-activity costs of anomalous event logs. In addition, each technique outlines that \texttt{t21} and \texttt{t75} influence the reconstruction error significantly, which suggests that the traces of anomalous event logs often lead to mismatching moves such as (\texttt{t21},$\nomove$), ($\nomove$,\texttt{t21}), which, in turn, could be due to \texttt{t21} being duplicated, skipped, or wrongly-ordered according to the Petri net's control-flow relations. In the case of \texttt{t75}, the activity is simply unknown, hence leading to the mismatching move (\texttt{t75}, $\nomove$) in the alignments.

\paragraph{Baselines explanation} Let us consider the distribution of fitness measures of normal and anomalous event logs for the AB\_CC\_B and TB\_CC\_B baselines. Figure \ref{FITNESS_VALUES_DISTRIBUTION} shows these distributions for each dataset, highlighting the intersection of the anomalous and normal distributions. These histograms provide insight into why the baseline approaches may fail at correctly classifying normal and anomalous event logs. In fact, the worst performance is obtained when there is excessive overlap of fitness values from normal and anomalous event logs, such as the PDC 2020/2021 datasets, where the four histograms of AB\_CC\_B and TB\_CC\_B show a significant overlap of normal and anomalous fitness values. Clearly, a higher-quality Petri net could provide much better results with the baselines. This is the case for the ERTMS dataset, where the anomalous and normal histograms of both the baselines are better separated, and, therefore, fitness thresholding is much more effective. Finally, the COVAS dataset leads to little overlap for AB\_CC\_B, whereas, for TB\_CC\_B, there is significant overlap. 
\begin{table*}[!t]
\centering
\caption{The mean $A$, $R$, $P$ and $F1$ and their standard deviation in subscripts for each framework and baseline technique and dataset. Measures in bold indicate the highest figure per dataset.}
\label{COMPARISON_RESULTS}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllllllllllllllllll}
\toprule
\multicolumn{2}{l}{\textbf{}}          &  & \multicolumn{4}{l}{\textbf{PDC 2020}}                                                         &  & \multicolumn{4}{l}{\textbf{PDC 2021}}                                                         &  & \multicolumn{4}{l}{\textbf{ERTMS}}                                                            &  & \multicolumn{4}{l}{\textbf{COVAS}}                                                            \\ \cmidrule{4-7} \cmidrule{9-12} \cmidrule{14-17} \cmidrule{19-22} 
\multicolumn{2}{l}{\textbf{Technique}} &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       \\ \midrule
\multicolumn{2}{l}{(AB\_CC, PCA)}      &  & 96.2$_{1.0}$          & 92.7$_{1.2}$          & 96.5$_{2.2}$          & 94.4$_{1.4}$          &  & 86.0$_{2.4}$          & 83.5$_{2.4}$          & 84.8$_{3.0}$          & 84.0$_{2.6}$          &  & 87.6$_{1.3}$          & 76.8$_{4.6}$          & 82.3$_{1.6}$          & 78.8$_{3.4}$          &  & 85.1$_{6.4}$          & 77.5$_{5.8}$          & 78.5$_{9.9}$          & 77.5$_{7.7}$          \\
\multicolumn{2}{l}{(AB\_CC, SPCA)}     &  & 96.8$_{1.0}$          & 93.8$_{1.4}$          & 97.1$_{2.0}$          & 95.3$_{1.4}$          &  & 86.3$_{2.9}$          & 83.1$_{2.6}$          & 85.8$_{3.8}$          & 84.1$_{3.1}$          &  & 88.2$_{2.3}$          & 78.8$_{5.5}$          & 82.5$_{3.4}$          & 80.3$_{4.7}$          &  & 91.0$_{4.8}$          & 87.6$_{7.2}$          & 85.8$_{7.5}$          & 86.5$_{7.0}$          \\
\multicolumn{2}{l}{(AB\_CC, KPCA)}     &  & 96.4$_{0.7}$          & 92.1$_{1.5}$          & 97.8$_{0.4}$          & 94.6$_{1.1}$          &  & 86.6$_{1.4}$          & 83.5$_{2.3}$          & 85.9$_{1.6}$          & 84.4$_{1.9}$          &  & 89.3$_{1.7}$          & 87.8$_{2.3}$          & 83.1$_{3.4}$          & 84.7$_{1.7}$          &  & \textbf{92.8$_{1.0}$} & \textbf{87.9$_{3.2}$} & \textbf{89.5$_{1.1}$} & \textbf{88.5$_{2.0}$} \\
\multicolumn{2}{l}{(AB\_CC, AE)}       &  & \textbf{98.1$_{0.3}$} & \textbf{96.6$_{1.3}$} & \textbf{98.1$_{0.5}$} & \textbf{97.3$_{0.5}$} &  & 88.9$_{1.1}$          & 86.2$_{1.3}$          & 88.4$_{1.4}$          & 87.2$_{1.3}$          &  & 91.2$_{1.9}$          & 83.3$_{5.8}$          & 88.3$_{1.6}$          & 85.2$_{4.1}$          &  & 91.4$_{3.1}$          & 85.6$_{6.5}$          & 87.0$_{5.0}$          & 86.2$_{5.7}$          \\ \midrule
\multicolumn{2}{l}{(TB\_CC, PCA)}      &  & 96.1$_{0.3}$          & 91.7$_{3.1}$          & 97.5$_{0.2}$          & 94.1$_{0.5}$          &  & 90.0$_{0.6}$          & 86.6$_{0.6}$          & 90.7$_{1.0}$          & 88.2$_{0.7}$          &  & 89.6$_{2.1}$          & 75.6$_{5.3}$          & 90.6$_{1.7}$          & 80.0$_{5.1}$          &  & 52.1$_{3.0}$          & 57.9$_{3.0}$          & 55.2$_{2.1}$          & 48.6$_{0.9}$          \\
\multicolumn{2}{l}{(TB\_CC, SPCA)}     &  & 96.5$_{0.7}$          & 92.9$_{1.4}$          & 97.2$_{1.5}$          & 94.8$_{1.1}$          &  & \textbf{90.6$_{0.9}$} & \textbf{87.1$_{1.3}$} & \textbf{91.7$_{0.8}$} & \textbf{88.9$_{1.2}$} &  & 92.6$_{1.0}$          & 85.5$_{3.6}$          & 90.9$_{1.7}$          & 87.6$_{2.3}$          &  & 55.6$_{1.7}$          & 61.2$_{1.3}$          & 57.2$_{0.8}$          & 52.0$_{1.2}$          \\
\multicolumn{2}{l}{(TB\_CC, KPCA)}     &  & 96.5$_{1.4}$          & 92.6$_{3.1}$          & 97.5$_{1.0}$          & 94.7$_{2.3}$          &  & 67.0$_{4.8}$          & 70.2$_{3.1}$          & 68.2$_{2.4}$          & 66.3$_{4.4}$          &  & 92.4$_{1.4}$          & 88.6$_{1.6}$          & 88.1$_{2.6}$          & 88.2$_{1.7}$          &  & 40.8$_{5.5}$          & 53.1$_{3.2}$          & 52.4$_{2.6}$          & 39.6$_{3.9}$          \\
\multicolumn{2}{l}{(TB\_CC, AE)}       &  & 96.6$_{0.4}$          & 92.6$_{1.0}$          & 97.9$_{0.2}$          & 95.0$_{0.7}$          &  & 90.0$_{0.6}$          & 87.3$_{1.8}$          & 91.3$_{0.8}$          & 88.8$_{1.5}$          &  & \textbf{94.0$_{0.5}$} & \textbf{89.8$_{2.7}$} & \textbf{91.5$_{1.3}$} & \textbf{90.5$_{1.0}$} &  & 73.8$_{3.1}$          & 69.7$_{2.8}$          & 64.8$_{2.3}$          & 65.7$_{2.7}$          \\ \midrule
\multicolumn{2}{l}{(DF, PCA)}          &  & 58.6$_{5.6}$          & 60.4$_{3.4}$          & 57.4$_{2.4}$          & 54.5$_{4.2}$          &  & 79.4$_{1.6}$          & 77.1$_{1.4}$          & 77.0$_{1.8}$          & 76.9$_{1.5}$          &  & 89.2$_{1.1}$          & 76.9$_{3.8}$          & 87.1$_{1.6}$          & 80.4$_{3.4}$          &  & 89.5$_{2.0}$          & 78.6$_{5.4}$          & 86.6$_{2.4}$          & 81.4$_{4.5}$          \\
\multicolumn{2}{l}{(DF, SPCA)}         &  & 62.2$_{2.8}$          & 62.4$_{0.4}$          & 58.9$_{0.4}$          & 57.2$_{1.5}$          &  & 80.4$_{1.7}$          & 78.6$_{1.2}$          & 78.3$_{1.9}$          & 78.2$_{1.3}$          &  & 88.4$_{2.0}$          & 74.8$_{5.7}$          & 86.7$_{1.8}$          & 78.4$_{5.0}$          &  & 89.2$_{1.1}$          & 78.4$_{4.3}$          & 85.8$_{0.3}$          & 81.1$_{3.1}$          \\
\multicolumn{2}{l}{(DF, KPCA)}         &  & 70.3$_{2.2}$          & 67.9$_{1.7}$          & 63.7$_{1.5}$          & 64.1$_{1.9}$          &  & 78.4$_{0.9}$          & 76.6$_{0.8}$          & 75.9$_{1.0}$          & 76.1$_{0.6}$          &  & 89.2$_{1.1}$          & 77.2$_{3.2}$          & 86.8$_{1.3}$          & 80.6$_{2.8}$          &  & 91.9$_{0.0}$          & 83.5$_{1.2}$          & 90.1$_{1.8}$          & 86.2$_{1.3}$          \\
\multicolumn{2}{l}{(DF, AE)}           &  & 71.6$_{2.2}$          & 69.3$_{0.0}$          & 64.8$_{0.0}$          & 65.4$_{0.0}$          &  & 70.1$_{0.0}$          & 72.3$_{0.0}$          & 69.8$_{0.0}$          & 69.2$_{0.0}$          &  & 89.7$_{1.3}$          & 80.2$_{2.4}$          & 86.0$_{2.6}$          & 82.6$_{2.4}$          &  & 90.8$_{1.4}$          & 83.9$_{3.2}$          & 86.4$_{2.1}$          & 85.0$_{2.7}$          \\ \midrule
\multicolumn{2}{l}{(NG, PCA)}          &  & 84.0$_{1.0}$          & 75.5$_{2.1}$          & 77.3$_{1.6}$          & 76.3$_{1.8}$          &  & 75.0$_{2.5}$          & 73.5$_{2.2}$          & 72.3$_{2.4}$          & 72.7$_{2.4}$          &  & 87.6$_{1.1}$          & 77.8$_{3.7}$          & 81.6$_{1.5}$          & 79.3$_{2.6}$          &  & 91.3$_{0.8}$          & 83.5$_{1.4}$          & 88.3$_{1.7}$          & 85.6$_{4.0}$          \\
\multicolumn{2}{l}{(NG, SPCA)}         &  & 83.8$_{0.0}$          & 76.0$_{1.7}$          & 77.1$_{1.0}$          & 76.5$_{1.1}$          &  & 75.3$_{0.9}$          & 73.5$_{1.8}$          & 72.6$_{1.0}$          & 72.8$_{1.3}$          &  & 87.6$_{2.0}$          & 76.8$_{6.8}$          & 82.1$_{2.2}$          & 78.5$_{5.4}$          &  & 87.6$_{0.8}$          & 75.3$_{1.5}$          & 83.0$_{1.7}$          & 78.1$_{1.5}$          \\
\multicolumn{2}{l}{(NG, KPCA)}         &  & 85.6$_{0.7}$          & 75.7$_{3.1}$          & 80.9$_{0.9}$          & 77.6$_{2.0}$          &  & 72.9$_{2.5}$          & 73.1$_{1.6}$          & 71.1$_{1.4}$          & 71.2$_{1.6}$          &  & 87.6$_{1.4}$          & 75.3$_{1.1}$          & 83.2$_{3.9}$          & 78.2$_{2.0}$          &  & 92.2$_{2.1}$          & 86.0$_{5.6}$          & 89.1$_{2.5}$          & 87.2$_{4.0}$          \\
\multicolumn{2}{l}{(NG, AE)}           &  & 87.2$_{0.0}$          & 78.2$_{0.0}$          & 83.3$_{0.0}$          & 80.3$_{0.0}$          &  & 77.0$_{0.0}$          & 75.3$_{0.0}$          & 74.3$_{0.0}$          & 74.7$_{0.0}$          &  & 86.0$_{0.8}$          & 73.1$_{2.3}$          & 79.5$_{1.4}$          & 75.5$_{2.0}$          &  & 92.0$_{0.0}$          & 83.7$_{0.0}$          & 90.1$_{0.0}$          & 86.4$_{0.0}$          \\ \bottomrule
\end{tabular}%
}
\end{table*}
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{images/BAR_PLOT_COMPARISON.png}
\caption{The mean $A$, $R$, $P$ and $F1$ per dataset of the best-performing dimensionality reduction technique for each \revision{process mining}-based feature extraction approach.}
\label{BAR_PLOT_COMPARISON}
\end{figure*}

\subsubsection{Framework techniques comparisons}
In this part, we compare the detection effectiveness of framework techniques using all four \revision{process mining}-based feature extraction approaches. Each technique is applied to each dataset 5 times to account for statistical fluctuations due to the random split of training, validation and test sets.

\paragraph{Results}
Table \ref{COMPARISON_RESULTS} reports the mean $A$, $R$, $P$ and $F1$ measures and their standard deviation in subscripts for each technique and dataset. For the PDC 2020 and COVAS datasets, (AB\_CC, AE) and (AB\_CC, KPCA) show the best performance, achieving 97.3\% and 88.5\% $F1$. For the PDC 2021 and ERTMS datasets, (TB\_CC, SPCA) and (TB\_CC, AE) show the best performance, achieving 88.9\% and 90.5\% $F1$. Arguably, TB\_CC \revision{process mining}-based feature extraction is better than AB\_CC for the PDC 2021 and ERTMS dataset because of the explanation in Figure \ref{FITNESS_VALUES_DISTRIBUTION}. From this figure, the fitness measures of the TB\_CC\_B baseline with respect to the ERTMS dataset are better separated than those fitness measures of the AB\_CC\_B baseline. Hence, the TB\_CC approach could have led to better features than AB\_CC. Figure \ref{BAR_PLOT_COMPARISON} depicts the bar plots that capture the impact of the different \revision{process mining}-based feature extraction approaches on the performance measures for each dataset. In particular, given a PF value, the dimensionality reduction technique providing the best performance is considered to compare the best-performing PF-DR value pairs. For example, given the PDC 2020 dataset, the best-performing PF-DR value pairs are (AB\_CC, AE), (TB\_CC, AE), (DF, AE) and (NG, AE), with $F1$ equal to 97.3\%, 95.0\%, 65.4\% and 80.3\%. 

\paragraph{Analysis of variance}
We conduct an analysis of variance to evaluate whether the dataset, the \revision{process mining}-based feature extraction approach, and the two combined result in statistically significant differences. To this aim, we design a two-factor full factorial experiment with factors PF and DS, of which the latter is the dataset factor. In like manner as Figure \ref{BAR_PLOT_COMPARISON}, for each PF value, we take the best-performing dimensionality reduction technique for the given dataset. Table \ref{ANOVA_COMPARISON} reports the mean values of the performance measures for each DS-PF value pair, specifying whether the DS factor, PF factor, and DS and PF factors combined are significant for the target metric using the non-parametric Friedman test (a p-value less than 0.05 implies that the factor is significant with 95\% confidence). In addition, the table shows how important the aforementioned factors are in terms of explained variance. Notably, the proposed AB\_CC \revision{process mining}-based feature extraction approach shows satisfying performance in all cases, e.g., $F1$ drops only as low as 85.2\% for the ERTMS dataset and peaks at 97.3\% for the PDC 2020 dataset. However, it is worth noting that AB\_CC is outperformed by by TB\_CC for the ERTMS and PDC 2021 datasets. Therefore, it is safe to state that there is no one-size-fits-all \revision{process mining}-based feature extraction approach valid for all datasets; besides, this is also confirmed statistically by the analysis of variance, which outlines that the interaction between the DS and PF factors is significant and important for all the measures considered.
\begin{table*}[!t]
\centering
\caption{The mean $A$, $R$, $P$ and $F1$ measures and their standard deviation in subscripts per dataset (DS) for each \revision{process mining}-based feature extraction (PF) approach. Measures in bold indicate the highest figure per dataset. At the bottom is the significance of the DS and PF factors per metric $X$, where $p_X$ indicates the p-value of the Friedman test ($p_X<0.05$ means the factor is significant with 95\% confidence) and $I_X$ the percentage of variance in $X$ explained by the factor.}
\label{ANOVA_COMPARISON}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllllllllllllllll}
\toprule
\multirow{2}{*}{\textbf{\diagbox[]{DS}{PF}}} & \multicolumn{4}{l}{\textbf{AB\_CC}}                                                           &  & \multicolumn{4}{l}{\textbf{TB\_CC}}                                                           &  & \multicolumn{4}{l}{\textbf{DF}}                                    &  & \multicolumn{4}{l}{\textbf{NG}}                                    \\ \cmidrule{2-5} \cmidrule{7-10} \cmidrule{12-15} \cmidrule{17-20} 
                                & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)}        & \textbf{R(\%)}        & \textbf{P(\%)}        & \textbf{F1(\%)}       &  & \textbf{A(\%)} & \textbf{R(\%)} & \textbf{P(\%)} & \textbf{F1(\%)} &  & \textbf{A(\%)} & \textbf{R(\%)} & \textbf{P(\%)} & \textbf{F1(\%)} \\ \midrule
\textbf{PDC 2020}               & \textbf{98.1$_{0.3}$} & \textbf{96.6$_{1.3}$} & \textbf{98.1$_{0.5}$} & \textbf{97.3$_{0.5}$} &  & 96.6$_{0.4}$          & 92.9$_{1.4}$          & 97.9$_{0.2}$          & 95.0$_{0.7}$          &  & 71.6$_{2.2}$   & 69.3$_{0.0}$   & 64.8$_{0.0}$   & 65.4$_{0.0}$    &  & 87.2$_{0.0}$   & 78.2$_{0.0}$   & 83.3$_{0.0}$   & 80.3$_{0.0}$    \\
\textbf{PDC 2021}               & 88.9$_{1.1}$          & 86.2$_{1.3}$          & 88.4$_{1.4}$          & 87.2$_{1.3}$          &  & \textbf{90.6$_{0.9}$} & \textbf{87.1$_{1.3}$} & \textbf{91.7$_{0.8}$} & \textbf{88.9$_{1.2}$} &  & 80.4$_{1.7}$   & 78.6$_{1.2}$   & 78.3$_{1.9}$   & 78.2$_{1.3}$    &  & 77.0$_{0.0}$   & 75.3$_{0.0}$   & 74.3$_{0.0}$   & 74.7$_{0.0}$    \\
\textbf{ERTMS}                  & 91.2$_{1.9}$          & 83.3$_{5.8}$          & 88.3$_{1.6}$          & 85.2$_{4.1}$          &  & \textbf{94.0$_{0.5}$} & \textbf{89.8$_{2.7}$} & \textbf{91.5$_{1.3}$} & \textbf{90.5$_{1.0}$} &  & 89.7$_{1.3}$   & 80.2$_{2.4}$   & 86.0$_{2.6}$   & 82.6$_{2.4}$    &  & 87.6$_{1.1}$   & 77.8$_{3.7}$   & 81.6$_{1.5}$   & 79.3$_{2.6}$    \\
\textbf{COVAS}                  & \textbf{92.8$_{1.0}$} & \textbf{87.9$_{3.2}$} & \textbf{89.5$_{1.1}$} & \textbf{88.5$_{2.0}$} &  & 73.8$_{3.1}$          & 69.7$_{2.8}$          & 64.8$_{2.3}$          & 65.7$_{2.7}$          &  & 91.9$_{0.0}$   & 83.5$_{1.2}$   & 90.1$_{1.8}$   & 86.2$_{1.3}$    &  & 92.2$_{2.1}$   & 86.0$_{5.6}$   & 89.1$_{2.5}$   & 87.2$_{4.0}$    \\ \midrule
\multicolumn{20}{l}{\textbf{DS significance and importance: $p_A=0.00, I_A=8.7\%; p_R=0.13, I_R=2.0\%; p_P=0.05, I_P=3.2\%; p_{F1}=0.02, I_{F1}=2.6\%$}}                                                                                                                                                                                                                           \\
\multicolumn{20}{l}{\textbf{PF significance and importance: $p_A=0.00, I_A=20.0\%; p_R=0.00, I_R=34.3\%; p_P=0.00, I_P=17.8\%; p_{F1}=0.00, I_{F1}=25.7\%$}}                                                                                                                                                                                                                       \\
\multicolumn{20}{l}{\textbf{DS and PF significance and importance: $p_A=0.00, I_A=68.7\%; p_R=0.00, I_R=55.2\%; p_P=0.00, I_P=76.8\%; p_{F1}=0.00, I_{F1}=66.0\%$}}                                                                                                                                                                                                                \\ \bottomrule
\end{tabular}%
}
\end{table*}



