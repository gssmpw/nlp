\section{Framework}
\label{sec:framework}
\subsection{Problem statement}
The framework combines \revision{process mining}-based feature extraction and dimensionality reduction to allow the flexible development of several \revision{conformance checking}-based and \revision{conformance checking}-independent techniques for control-flow anomaly detection. This enables a fair comparison of multiple techniques, which may include our proposed \revision{process mining}-based feature extraction approach using alignment-based \revision{conformance checking}. 

\paragraph{\revision{Framework guidelines}}
Following Chandola et al.'s guidelines \cite{chandola2009ad} for developing anomaly detection techniques, which include defining the nature of data, type of anomalies, and the research areas employed, the framework:
\begin{itemize}
    \item Handles the event logs of business processes (see Definition \ref{def:el}) and address control-flow anomalies such as unknown, wrongly-ordered, and skipped activities;
    \item Uses \revision{conformance checking} and other types of trace encodings for \revision{process mining}-based feature extraction and generate tabular data from event logs;
    \item Employs dimensionality reduction with the tabular data resulting from training and validation event logs to build a classifier, and subsequently use the classifier to discriminate normal and anomalous event logs.
\end{itemize}

\paragraph{\revision{Scenario type}}
\revision{The type of data used during the training phase of anomaly detection techniques is highly dependent on the scenario type. For example, whereas supervised scenarios have plenty of data with accurate labels for normal and anomalous data, unsupervised scenarios involve unlabeled data only and are effective when anomalous data are infrequent \cite{chandola2009ad}. There is an intermediate scenario, namely the weakly-supervised scenario. This scenario brings the benefit of having loads of labeled normal data and a limited amount of anomalous data, and is common in business process monitoring \cite{zhang2021noisyfd, zhao2023admoe, guan2024wake}. By using only normal data during the training phase, the technique is tuned to accurately discriminate those patterns that are consistent with normal behavior, and to detect \emph{any} deviations from it. Due to weak supervision being a common scenario in business process monitoring, and the potential benefits of tailoring the anomaly detection technique to normal data only, the proposed framework deals with the weakly-supervised scenario. Hence, the framework includes only normal event logs during training and retains anomalous event logs in the test set. However, it is worth noting that weak supervision means the data may contain inaccuracies and incomplete information \cite{zhou2022weaklysupervisedaes, jiang2023weaklysupervisedad}. Consequently, noisy event logs and low-quality Petri nets can lead to issues such as those shown in Figure \ref{CC_UNBALANCING}.}

\paragraph{\revision{Explainability}}
In addition to \revision{Chandola et al's guidelines}, the framework aims to provide an explanation of the anomalies according to Li et al's \cite{li2023explainablead} definitions, which are the \textit{oracle-definition}, \textit{detection-definition}, and \textit{explanation-definition}. The \textbf{oracle-definition} is domain-specific and reflects the above-mentioned unknown, wrongly-ordered and skipped activities. For example, wrong medical treatment could lead to a doctor skipping key analyses for their patient, thus skipping activities. The remaining two types are framework-dependent and strongly connected to \revision{process mining}-based feature extraction and dimensionality reduction, which are the framework's main steps depicted in Figure \ref{FW} and described in the following.

\subsection{\revision{Process mining}-based feature extraction}
Firstly, event logs and a reference Petri net are obtained from the business process to check for deviations. On the one hand, the event logs are monitored while the business process is being executed and are labeled as normal or anomalous. On the other hand, the reference Petri net could be obtained either manually by domain experts or through the automatic generation from normal event logs, for example by using process discovery algorithms. The event logs are split into training, validation and test sets so that the normal control flow is characterized using the training set, the ability to generalize is evaluated with the validation set, and the performance is assessed with the test set, which is the only set including anomalous event logs. \revision{Process mining}-based feature extraction applies to all three sets to obtain tabular data from event data.
\begin{definition}[\revision{Process mining}-based feature extraction]
Let $\mathcal{L}\in(\mathcal{B}(\mathcal{A}^*))^n$ be an $n$-tuple of event logs and $N\in\mathcal{N}$ a Petri net. Statistical \revision{process mining}-based feature extraction $\mathcal{F}_S$ is defined as
\begin{equation}
\mathcal{F}_S:(\mathcal{B}(\mathcal{A}^*))^{n}\rightarrow\mathbb{R}^{n\times f},
\end{equation}
such that $\mathcal{F}_S(\mathcal{L})$ is an $n\times f$ matrix, where the columns are obtained directly from statistics in the event logs of $\mathcal{L}$. \revision{Conformance checking} \revision{process mining}-based feature extraction $\mathcal{F}_{CC}$ is defined as
\begin{equation}
\mathcal{F}_{CC}:(\mathcal{B}(\mathcal{A}^*))^{n}\times\mathcal{N}\rightarrow\mathbb{R}^{n\times f},
\end{equation}
such that $\mathcal{F}_{CC}(\mathcal{L},N)$ is an $n\times f$ matrix, where the columns are obtained by checking the event logs of $\mathcal{L}$ against $N$ by $CC$.
\end{definition}
Statistical \revision{process mining}-based feature extraction has the advantage of being \revision{conformance checking}-independent, as features are extracted from the statistical occurrence of certain patterns within the traces of event logs. However, the disadvantage is the absence of a reference Petri net, which allows an improved explanation of anomaly detection results. For example, Figure \ref{PETRI_NET_TRACES} shows that \revision{conformance checking} \revision{process mining}-based feature extraction implemented with our proposed approach associates the per-activity cost with model elements, flagging those transitions that cause mismatches. In the following, we review the other \revision{process mining}-based feature extraction approaches we use in Section \ref{sec:evaluation}. We denote $\mathcal{C}_{\mathcal{L}}\subseteq\mathcal{A}$ the set of activities found in the traces of the event logs of $\mathcal{L}$. Besides, recall we denote $\mathcal{C}_{\mathcal{L},N}\subseteq\mathcal{A}$ the set of activities found either in the traces of the event logs of $\mathcal{L}$ or in labeled transitions of a reference Petri net (Definition \ref{def:ab_cc}).

\paragraph{N-grams} The N-grams statistical \revision{process mining}-based feature extraction involves counting the number of times an N-tuple of activities in the traces of the event logs of $\mathcal{L}$ occurs \cite{tavares2023pmtraceencoding}. Let $\mathcal{C}_{\mathcal{L}}\times\cdots\times C_{\mathcal{L}}=(\mathcal{C}_{\mathcal{L}})^N$ be the set of N-grams and $|\mathcal{C}_{\mathcal{L}}^N|$ the total number of N-grams. $\mathcal{F}_S:(\mathcal{B}(\mathcal{\mathcal{A}^*}))^n\rightarrow\mathbb{N}^{n\times |\mathcal{C}_{\mathcal{L}}^N|}$ associates each $L\in\mathcal{L}$ with a $|\mathcal{C}_{\mathcal{L}}^N|$-tuple $(f_1,\dots, f_{|\mathcal{C}_{\mathcal{L}}^N|})$ such that $f_i\in\mathbb{N}$ counts the number of occurrences of the corresponding N-gram in $L$.

\paragraph{Directly-follows} The directly-follows statistical \revision{process mining}-based feature extraction involves counting the number of times a directly-follows relation occurs in traces of $\mathcal{L}$, where a directly-follows relation is such that, given a trace $\sigma\in L$ and activities $a_1,a_2\in\sigma$, either $a_2$ follows $a_1$ or vice versa \cite{aalst2022pmhandbook}. Let $\mathcal{DF}=(\mathcal{C}_{\mathcal{L}}\times \mathcal{C}_{\mathcal{L}})\setminus\{(c,c):c\in \mathcal{C}_{\mathcal{L}}\}$ be the set of directly-follows relations and $|\mathcal{DF}|$ the total number of directly-follows relations. $\mathcal{F}_S:(\mathcal{B}(\mathcal{\mathcal{A}^*}))^n\rightarrow\mathbb{N}^{n\times|\mathcal{DF}|}$ associates each $L\in\mathcal{L}$ with a $|\mathcal{DF}|$-tuple $(f_1,\dots,f_{|\mathcal{DF}|})$ such that $f_i\in\mathbb{N}$ counts the number of occurrences of a given directly-follows relation in $L$.

\paragraph{Token-based \revision{conformance checking}} The token-based \revision{conformance checking} \revision{process mining}-based feature extraction \cite{singh2022lapmsh, debenedictis2023dtadiiot} involves tracking the token dynamics while checking whether the traces of an event log meet the control-flow constraints of a reference Petri net. Specifically, when an activity of a trace is not allowed to execute according to the current marking of the reference Petri net, token-based \revision{conformance checking} tracks the missing ($m$) tokens. In like manner, when the last activity of a trace executes, token-based \revision{conformance checking} tracks the remaining ($r$) tokens in the places of the reference Petri net. Finally, token-based \revision{conformance checking} tracks the consumed ($c$) and produced ($r$) tokens, and the total number of activities executed. Formally, $\mathcal{F}_{CC}:(\mathcal{B}(\mathcal{A}^*))^n\times\mathcal{N}\rightarrow \mathbb{R}^{n\times 3+|\mathcal{C}_{\mathcal{L},N}|}$ associates each $L\in\mathcal{L}$ with a $(3+|\mathcal{C}_{\mathcal{L},N}|)$-tuple $(F_{L,N}, f_m, f_r, f_{\mathcal{C}_{\mathcal{L},N},1},\dots,f_{\mathcal{C}_{\mathcal{L},N},|\mathcal{C}_{\mathcal{L},N}|})$ such that $F_{L,N}=\frac{1}{|L|}\sum_{\sigma\in L}\frac{1}{2}(1-\frac{m}{c})+\frac{1}{2}(1-\frac{r}{p})$ is the fitness obtained by token-based \revision{conformance checking} \cite{aalst2022pmhandbook}, $f_m, f_r\in\mathbb{N}$ count the number of missing and remaining tokens, and $f_{\mathcal{C}_{\mathcal{L},N},i}$ counts the number of times a given activity $c_i\in\mathcal{C}_{\mathcal{L},N}$ executed.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{images/FW_EXTENDED.png}
\caption{The refined view of the proposed framework for combining \revision{process mining}-based feature extraction with dimensionality reduction for control-flow anomaly detection.}
\label{FW}
\end{figure*}
\subsection{Dimensionality reduction}
Regardless of the type of \revision{process mining}-based feature extraction used, the resulting tabular data are managed by a dimensionality reduction technique that reconstructs the data by encoding and decoding functions. This allows control-flow anomaly detection by the reconstruction-based approach, which has shown satisfying performance on account of the ability of dimensionality reduction to handle high-dimensional and possibly non-linear feature spaces.
\begin{definition}[Dimensionality reduction technique]
Let $T\in\mathbb{R}^{n\times f}$ be a matrix with $n$ rows and $f$ columns. A dimensionality reduction technique $\mathcal{DR}$ is a pair of functions defined as follows:
\begin{equation}
\begin{cases}
\mathcal{DR}_E:\mathbb{R}^{f}\rightarrow\mathbb{R}^{f_R}\\
\mathcal{DR}_D:\mathbb{R}^{f}\rightarrow\mathbb{R}^{f}
\end{cases}\quad f_R<f,
\end{equation}
where $\mathcal{DR}_E$ is the encoding function and $\mathcal{DR}_D$ the decoding function. $\hat{t}_i=\mathcal{DR}_D(t_i)$ is the reconstruction of the $i$-th row of $T$ by the decoding function of the technique after obtaining the encoding model upon the application of the encoding function. Let us define $\mathcal{E}:\mathbb{R}^f\rightarrow \mathbb{R}$ the function that calculates the reconstruction error. We denote $\mathcal{E}_{t_i}=\mathcal{E}(t_i)= ||t_i-\hat{t}_i||$ the reconstruction error of $t_i$.
\end{definition}

The dimensionality reduction phase is two-stepped. These are the training and validation steps. Firstly, the encoding part of a dimensionality reduction technique $\mathcal{DR}_E$ is used to project the training tabular data $TR\in\mathbb{R}^{n_{TR}\times f}$ onto a new lower-dimensional coordinate system with domain $\mathbb{R}^{f_R}$. This coordinate system is used to project validation tabular data $V\in\mathbb{R}^{n_V\times f}$ onto it. After this projection, each validation sample is reconstructed in the original space with the decoding part of the dimensionality reduction technique $\mathcal{DR}_D$. The process repeats with different configurations of the dimensionality reduction technique until the reconstruction error on the validation set is minimized. In the following, we review the dimensionality reduction techniques we use in Section \ref{sec:evaluation}. We denote $T\in\mathbb{R}^{n\times f}$ a matrix obtained by \revision{process mining}-based feature extraction with centered columns, i.e., the mean of each column is 0 and their standard deviation is 1, $t\in \mathbb{R}^f$ a row vector of $T$, and $X'$ ($x'$) the transpose of a generic matrix (row vector) $X$ ($x$).

\paragraph{Principal Component Analysis} The Principal Component Analysis (PCA) is a widespread linear approach for dimensionality reduction \cite{abdi2010pca}. The PCA projects $T$ onto a new coordinate system by eigendecomposition of the $T$'s covariance matrix: $\frac{1}{n}T'T=U\Delta U'$, where $U\in\mathbb{R}^{f\times f}$ is the matrix whose columns are eigenvectors and $\Delta\in\mathbb{R}^{f\times f}$ is a diagonal matrix containing the eigenvalues corresponding to the eigenvectors. The columns of $U$ represent the so-called principal components, whose linear combination with the rows of $T$ results in the projection of $T$ onto the new coordinate system. The PCA is such that the first few principal components concentrate most of the explained variance. Hence, on the one hand, the encoding part of the PCA can be formulated as $\mathcal{DR}_E(t)=tU_P$, where $U_P\in\mathbb{R}^{f\times f_R}$ is $U$ with the first $f_R$ columns retained. On the other hand, the decoding part of the PCA can be formulated as $\mathcal{DR}_D(t)=\mathcal{DR}_E(t)U_P'$.

\paragraph{Sparse Principal Component Analysis} The Sparse PCA (SPCA) extends the PCA by leaving out some features of the input space when building the new coordinate system to include only the significant ones \cite{zou2006sparsepca}. This is enforced by converting the PCA formulation as a sparse regression problem with a regression regularization parameter and a sparsity coefficient, leading to the sparse matrix $U_S\in\mathbb{R}^{f\times f}$ and the reduced version $U_{S,P}\in\mathbb{R}^{f\times f_R}$. The encoding and decoding parts of the SPCA are the same as the PCA.

\paragraph{Kernel Principal Component Analysis} The Kernel PCA (KPCA) extends the PCA by finding a high-dimensional hyperplane where $T$ can be linearly separated using the so-called kernel trick \cite{nguyen2010kpcafd}. Firstly, each row of $T$ is projected onto a higher-dimensional hyperplane with domain $\mathbb{R}^{f_e}, f_e>f$ by a mapping $\phi: \mathbb{R}^f \rightarrow \mathbb{R}^{f_e}$. The covariance matrix of the transformed (centered) data is as follows: $C=\frac{1}{n}\sum_{i=1}^n\phi(t_i)'\phi(t_i)\in\mathbb{R}^{f_e\times f_e}$, where $t_i\in\mathbb{R}^f$ is a row of $T$. Let us consider the kernel function $k(t_i,t_j)=\phi(t_i)\phi(t_j)'\in\mathbb{R}$ and the kernel matrix $K\in\mathbb{R}^{n\times n}$ such that $K_{ij}=k(t_i,t_j)$. The eigenvalue equation $\lambda V=CV$, where $\lambda\in\mathbb{R}$ and $V\in \mathbb{R}^{f_e}$, transforms to $K\nu = n\lambda\nu$, where $\nu\in \mathbb{R}^n$ is an eigenvector of $K$. The eigenvectors represent the kernel principal components, leading to the matrix $U_e\in\mathbb{R}^{n\times n}$ and the reduced version $U_{e,P}\in\mathbb{R}^{n\times f_R}$. The encoding part of the KPCA can be formulated as $\mathcal{DR}_E(t)=\kappa(t)U_{e,P},\kappa(t)=(\phi(t,t_1),\dots,\phi(t,t_n))\in\mathbb{R}^{n}$. The decoding part involves ridge regression, which requires solving the following minimization problem: $\min_{\beta}\{||U_{e,P}\beta-T||^2+\gamma||\beta||^2\}$, where $||\cdot||^2$ indicates the squared norm, $\gamma\in\mathbb{R}$ is a regularization term, and $\beta\in\mathbb{R}^{f_e\times f}$ the matrix of regression components. Finally, the decoding part of the KPCA can be formulated as $\mathcal{DR}_{D}(t)=\kappa(t)\beta$.

\paragraph{Autoencoder} The autoencoder is a widespread feed-forward neural network approach for dimensionality reduction which accounts for non-linear dependencies in $T$'s features through the non-linear transformations of the network's neurons \cite{sakurada2014adeutoencoders}. The simplest autoencoder structure is a symmetric feed-forward network with three fully-connected hidden layers, namely the encoder, code and decoder layers. The code layer implements $\mathcal{DR}_E(t)=W_C\sigma_E(W_Et+b_E)+b_C$, where $b_E\in\mathbb{R}^{f_{E,D}}$ and $b_C\in\mathbb{R}^{f_R}$ are the encoder and code bias vectors, $W_E\in\mathbb{R}^{f_{E,D}\times f}$ and $W_C\in\mathbb{R}^{f_R \times f_{E,D}}$ are the encoder and code weight matrices, and $\sigma_E$ the activation function applied by the encoder layer's neurons. The output provides the reconstructed data, implementing the decoding part $\mathcal{DR}_D(t)=W_O\sigma_D(W_D\mathcal{DR}_E(t)+b_D)+b_O$, where $b_D\in\mathbb{R}^{f_{E,D}}$ and $b_O\in\mathbb{R}^{f}$ are the decoder and output bias vectors, $W_D\in\mathbb{R}^{f_R\times f_{E,D}}$ and $W_O\in\mathbb{R}^{f \times f_{E,D}}$ are the decoder and output weight matrices, and $\sigma_D$ the activation function applied by the decoder layer's neurons.

Table \ref{DR_TECHNIQUES} briefly describes the techniques above, mentioning the approach followed, the complexity and type of the encoding and decoding parts, and the framework formulation. The choice of the specific technique depends on the computational resources available and the data at hand. For example, on the one hand, the PCA can be preferred to other methods if the feature space of data does not exhibit non-linear relationships and/or there are limited computational resources available. On the other hand, if computational resources are not constrained and there are loads of data with non-linear relationships in the feature space, the autoencoder could provide better results. Finally, it is worth mentioning that there are other dimensionality reduction techniques, such as the linear discriminant analysis \cite{tharwat2017lda} and t-distributed stochastic neighbor embedding \cite{belkina2019tsne}, as well as further extensions of the ones we presented, such as the robust PCA \cite{bouwmans2014rpca} and variational autoencoder \cite{kingma2014vae}.
\begin{table}[!t]
\centering
\caption{A set of dimensionality reduction techniques with their framework formulation.}
\label{DR_TECHNIQUES}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\toprule
\textbf{Technique} & \textbf{Approach}                                                                                        & \textbf{Complexity} & \textbf{Type} & \textbf{Formulation}                                                                                                                                  \\ \midrule
\textbf{PCA}       & Eigendecomposition                                                                                       & Low                 & Linear        & \begin{tabular}[c]{@{}c@{}}$\mathcal{DR}_E(t)=tU_P$\\ $\mathcal{DR}_D(t)=\mathcal{DR}_E(t)U_P'$\end{tabular}                                          \\
\textbf{SPCA}      & Sparse regression                                                                                        & Medium              & Linear        & \begin{tabular}[c]{@{}c@{}}$\mathcal{DR}_E(t)=tU_{S,P}$\\ $\mathcal{DR}_D(t)=\mathcal{DR}_E(t)U_{S,P}'$\end{tabular}                                  \\
\textbf{KPCA}      & \begin{tabular}[c]{@{}c@{}}Kernel trick,\\ eigendecomposition,\\ kernel ridge regression\end{tabular} & Medium                & Non-linear    & \begin{tabular}[c]{@{}c@{}}$\mathcal{DR}_E(t)=\kappa(t)U_{e,P}$\\ $\mathcal{DR}_D(t)=\kappa(t)\beta$\end{tabular}                                     \\
\textbf{Autoencoder}        & Neural network                                                                              & High                & Non-linear    & \begin{tabular}[c]{@{}c@{}}$\mathcal{DR}_E(t)=W_C\sigma_E(W_Et+b_E)+b_C$\\ $\mathcal{DR}_D(t)=W_O\sigma_D(W_D\mathcal{DR}_E(t)+b_D)+b_O$\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Anomaly detection and explanation}
A threshold on the reconstruction error $\mathcal{E}_{th}$ can be calculated for classifying test data during anomaly detection. Let $\mathcal{E}_{v_i}=||v_i-\hat{v}_i||,i\in\{1,\dots,n_V\}$ be the reconstruction errors of rows $v_i\in\mathbb{R}^{f}$ of validation tabular data $V\in\mathbb{R}^{n_V\times f}$. Let $\sum_i||v_i-\hat{v}_i||$ be the sum of such errors. $\mathcal{E}_{th}$ is calculated by a function $r$ over the sum of reconstruction errors, i.e., $\mathcal{E}_{th}=r(\sum_i||v_i-\hat{v}_i||)$. For example, $r$ can be the mean squared error: $\mathcal{E}_{th}=\frac{\sum_i ||v_i-\hat{v}_i||^2}{n_V}$.
\begin{definition}[Anomaly detection]
\label{def:ad}
Let $t\in\mathbb{R}^{f}$ be a test sample, $\mathcal{DR}_D$ the decoding part of a dimensionality reduction technique tailored to a training set, $\hat{t}=\mathcal{DR}_D(t)$ the reconstruction of $t$ using $\mathcal{DR}_D$, $\mathcal{E}_t=||t-\hat{t}||$ the reconstruction error of $t$, and $\mathcal{E}_{th}$ a threshold to the reconstruction error obtained with a validation set. The classification of $t$ is \emph{normal} if $\mathcal{E}_{t}<\mathcal{E}_{th}$, \emph{anomalous} otherwise.
\end{definition}

The combination of a \revision{process mining}-based feature extraction approach and a dimensionality reduction technique forms a \textit{framework technique}. The \textbf{detection-definition} of framework techniques is linked to Definition \ref{def:ad}: the framework techniques identify as anomalous those event logs whose \revision{process mining}-based feature extraction leads to a reconstruction error higher than the threshold. However, while this definition outlines \textit{what is an anomaly}, it does not provide insight into \textit{why} the event log led to an anomalous reconstruction error. There are several anomaly explanation techniques available to provide an \textbf{explanation-definition}. In this work, we focus on additive feature-based explanation, which explains the output of an anomaly detection technique by superposition of the impact of the features of input data \cite{lundberg2017unifiedapproach}.

\begin{definition}[Anomaly explanation]
\label{def:ad_exp}
Let $t\in\mathbb{R}^{f}$ be a test sample, $\mathcal{DR}_D$ the decoding part of a dimensionality reduction technique tailored to a training set, $\mathcal{DR}_D(t)=\hat{t}$ the reconstruction of $t$ using $\mathcal{DR}_D$, and $\mathcal{E}$ the function that calculates the reconstruction error. The additive explanation of $\mathcal{E}_t$ is $\tilde{\mathcal{E}}_t=\theta+\sum_{i=1}^f\eta_i\mathcal{E}_t,\theta,\eta_{i}\in\mathbb{R}$. $\eta=(\eta_1,\dots,\eta_f)\in\mathbb{R}^f$ is the \emph{anomaly explanation}, where $\eta_i$ denotes the effect of feature $f_i$ on $\mathcal{E}_t$.
\end{definition}

Several methods such as Local Interpretable Model-agnostic Explanations, DeepLift, and Classic Shapley Value Estimation can be used to calculate $\eta$. These methods are unified under a framework that calculates the so-called SHapley Additive exPlanations (SHAP) values \cite{lundberg2017unifiedapproach}. These values have been used to explain the results of several dimensionality reduction approaches to anomaly detection \cite{takeishi2019pcashapexplanation,antwarg2021aeshapexplanation}. In conclusion, we would like to remark that not only does this explanation-definition depend on the dimensionality reduction technique but also on \revision{process mining}-based feature extraction. For example, SHAP values linked to the features of our proposed \revision{process mining}-based feature extraction approach directly connect to the elements of the reference Petri net. This connection gives additional model-based insight into the reason why test data are classified as anomalous.

