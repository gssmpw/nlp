@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{dhruva2020aggregating,
  title={Aggregating multiple real-world data sources using a patient-centered health-data-sharing platform},
  author={Dhruva, Sanket S and Ross, Joseph S and Akar, Joseph G and Caldwell, Brittany and Childers, Karla and Chow, Wing and Ciaccio, Laura and Coplan, Paul and Dong, Jun and Dykhoff, Hayley J and others},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={60},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{nguyen2021federated,
  title={Federated learning for internet of things: A comprehensive survey},
  author={Nguyen, Dinh C and Ding, Ming and Pathirana, Pubudu N and Seneviratne, Aruna and Li, Jun and Poor, H Vincent},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={23},
  number={3},
  pages={1622--1658},
  year={2021},
  publisher={IEEE}
}

@article{guan2024federated,
  title={Federated learning for medical image analysis: A survey},
  author={Guan, Hao and Yap, Pew-Thian and Bozoki, Andrea and Liu, Mingxia},
  journal={Pattern Recognition},
  pages={110424},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhang2023fedala,
  title={Fedala: Adaptive local aggregation for personalized federated learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={11237--11244},
  year={2023}
}

@inproceedings{huang2021personalized,
  title={Personalized cross-silo federated learning on non-iid data},
  author={Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={7865--7873},
  year={2021}
}

@article{t2020personalized,
  title={Personalized federated learning with moreau envelopes},
  author={T Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21394--21405},
  year={2020}
}

@article{smith2017federated,
  title={Federated multi-task learning},
  author={Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{yi2023fedgh,
  title={Fedgh: Heterogeneous federated learning with generalized global header},
  author={Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={8686--8696},
  year={2023}
}

@article{zhang2023pfllib,
  title={PFLlib: Personalized Federated Learning Algorithm Library},
  author={Zhang, Jianqing and Liu, Yang and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Cao, Jian},
  journal={arXiv preprint arXiv:2312.04992},
  year={2023}
}

@inproceedings{NEURIPS2020_24389bfe,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v139-li21h,
  title = 	 {Ditto: Fair and Robust Federated Learning Through Personalization},
  author =       {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6357--6368},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21h.html},
  abstract = 	 {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.}
}

@inproceedings{luo2022adapt,
  title={Adapt to adaptation: Learning personalization for cross-silo federated learning},
  author={Luo, Jun and Wu, Shandong},
  booktitle={IJCAI: proceedings of the conference},
  volume={2022},
  pages={2166},
  year={2022},
  organization={NIH Public Access}
}


@InProceedings{pmlr-v139-collins21a,
  title = 	 {Exploiting Shared Representations for Personalized Federated Learning},
  author =       {Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2089--2099},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/collins21a/collins21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/collins21a.html},
  abstract = 	 {Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.}
}

@inproceedings{niu2022federated,
  title={Federated learning for face recognition with gradient correction},
  author={Niu, Yifan and Deng, Weihong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={2},
  pages={1999--2007},
  year={2022}
}

@inproceedings{zhang2023fedcp,
  title={Fedcp: Separating feature information for personalized federated learning via conditional policy},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3249--3261},
  year={2023}
}

@article{wu2022communication,
  title={Communication-efficient federated learning via knowledge distillation},
  author={Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={2032},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{tan2022fedproto,
  title={Fedproto: Federated prototype learning across heterogeneous clients},
  author={Tan, Yue and Long, Guodong and Liu, Lu and Zhou, Tianyi and Lu, Qinghua and Jiang, Jing and Zhang, Chengqi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={8432--8440},
  year={2022}
}

@article{hanzely2020lower,
  title={Lower bounds and optimal algorithms for personalized federated learning},
  author={Hanzely, Filip and Hanzely, Slavom{\'\i}r and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2304--2315},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@article{chen2021bridging,
  title={On bridging generic and personalized federated learning for image classification},
  author={Chen, Hong-You and Chao, Wei-Lun},
  journal={arXiv preprint arXiv:2107.00778},
  year={2021}
}

@article{arivazhagan2019federated,
  title={Federated learning with personalization layers},
  author={Arivazhagan, Manoj Ghuhan and Aggarwal, Vinay and Singh, Aaditya Kumar and Choudhary, Sunav},
  journal={arXiv preprint arXiv:1912.00818},
  year={2019}
}

@inproceedings{cheng2020club,
  title={Club: A contrastive log-ratio upper bound of mutual information},
  author={Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  booktitle={International Conference on Machine Learning},
  pages={1779--1788},
  year={2020},
  organization={PMLR}
}

@article{alemi2016deep,
  title={Deep variational information bottleneck},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
  journal={arXiv preprint arXiv:1612.00410},
  year={2016}
}

@article{barber2004algorithm,
  title={The im algorithm: a variational approach to information maximization},
  author={Barber, David and Agakov, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  number={320},
  pages={201},
  year={2004}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={12},
  year={2010}
}

@inproceedings{belghazi2018mutual,
  title={Mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle={International Conference on Machine Learning},
  pages={531--540},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2021model,
  title={Model-contrastive federated learning},
  author={Li, Qinbin and He, Bingsheng and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10713--10722},
  year={2021}
}

@article{luo2021no,
  title={No fear of heterogeneity: Classifier calibration for federated learning with non-iid data},
  author={Luo, Mi and Chen, Fei and Hu, Dapeng and Zhang, Yifan and Liang, Jian and Feng, Jiashi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5972--5984},
  year={2021}
}

@inproceedings{zhang2023gpfl,
  title={Gpfl: Simultaneously learning global and personalized feature information for personalized federated learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Cao, Jian and Guan, Haibing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5041--5051},
  year={2023}
}

@inproceedings{
oh2022fedbabu,
title={Fed{BABU}: Toward Enhanced Representation for Federated Image Classification},
author={Jaehoon Oh and SangMook Kim and Se-Young Yun},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=HuaYQfggn5u}
}

@article{DING2024112317,
title = {To be global or personalized: Generalized federated learning with cooperative adaptation for data heterogeneity},
journal = {Knowledge-Based Systems},
volume = {301},
pages = {112317},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112317},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124009511},
author = {Kaijian Ding and Xiang Feng and Huiqun Yu},
keywords = {Federated learning, Knowledge distillation, Data heterogeneity, Healthcare},
abstract = {As federated learning (FL) continues to advance, research has branched into two major directions: enhancing the individual global model and developing multiple personalized models. However, few studies took both orientations into consideration in parallel. The underlying reason lies in the objective heterogeneity between global and personalized models, where the former emphasizes generalization, while the latter focuses on specialization. The dilemma of sacrificing the personalized deviation in exchange for stable global enhancement, or forsaking the global model for personalization, is particularly acute in highly heterogeneous data scenarios (as a curse). To establish the coexistence of global and personalized models in highly heterogeneous data scenarios, we propose FedAKD, a generalized federated learning framework with adaptive knowledge distillation to optimizes both global and personalized models. Specifically, based on the similarity of predictive distributions over a reference dataset, two distinct adaptive weighting strategies are employed to match the divergent focuses of global and personalized optimization, leveraging data heterogeneity to foster cooperative adaptation and establish their coexistence (as a blessing). The client-side strategy through cluster-oriented optimization to facilitate the domain adaptation in local tasks. Meanwhile, the server-side strategy adjusts the global model to accommodate varying optimization directions from diverse implicit clusters. Experiments in label and feature shift settings demonstrate that our method outperforms state-of-the-art methods in both global and personalized performance with faster convergence [e.g. the accuracy improvements on the physical activity monitoring dataset (PAMAP2) for global and personalized models exceed 26.51% and 11.00%, respectively]. Furthermore, our method is equally applicable to real-world healthcare scenarios.}
}

@inproceedings{niizumi2022masked,
  title={Masked spectrogram modeling using masked autoencoders for learning general-purpose audio representation},
  author={Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio},
  booktitle={HEAR: Holistic Evaluation of Audio Representations},
  pages={1--24},
  year={2022},
  organization={PMLR}
}

@inproceedings{li2023s,
  title={What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders},
  author={Li, Jintang and Wu, Ruofan and Sun, Wangbin and Chen, Liang and Tian, Sheng and Zhu, Liang and Meng, Changhua and Zheng, Zibin and Wang, Weiqiang},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1268--1279},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{chang2020synthetic,
  title={Synthetic learning: Learn from distributed asynchronized discriminator gan without sharing medical image data},
  author={Chang, Qi and Qu, Hui and Zhang, Yikai and Sabuncu, Mert and Chen, Chao and Zhang, Tong and Metaxas, Dimitris N},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13856--13866},
  year={2020}
}

@inproceedings{wang2023flexifed,
  title={Flexifed: Personalized federated learning for edge clients with heterogeneous model architectures},
  author={Wang, Kaibin and He, Qiang and Chen, Feifei and Chen, Chunyang and Huang, Faliang and Jin, Hai and Yang, Yun},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={2979--2990},
  year={2023}
}

@inproceedings{deng2023hsfl,
  title={HSFL: Efficient and privacy-preserving offloading for split and federated learning in IoT services},
  author={Deng, Ruijun and Du, Xin and Lu, Zhihui and Duan, Qiang and Huang, Shih-Chia and Wu, Jie},
  booktitle={2023 IEEE International Conference on Web Services (ICWS)},
  pages={658--668},
  year={2023},
  organization={IEEE}
}

@inproceedings{wang2024feddse,
  title={FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices},
  author={Wang, Haozhao and Jia, Yabo and Zhang, Meng and Hu, Qinghao and Ren, Hao and Sun, Peng and Wen, Yonggang and Zhang, Tianwei},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={2902--2913},
  year={2024}
}

@inproceedings{zhang2024few,
  title={How Few Davids Improve One Goliath: Federated Learning in Resource-Skewed Edge Computing Environments},
  author={Zhang, Jiayun and Li, Shuheng and Huang, Haiyu and Wang, Zihan and Fu, Xiaohan and Hong, Dezhi and Gupta, Rajesh K and Shang, Jingbo},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={2976--2985},
  year={2024}
}

@inproceedings{fang2023eva,
  title={Eva: Exploring the limits of masked visual representation learning at scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19358--19369},
  year={2023}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  journal={Technical Report},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{ghifary2016scatter,
  title={Scatter component analysis: A unified framework for domain adaptation and domain generalization},
  author={Ghifary, Muhammad and Balduzzi, David and Kleijn, W Bastiaan and Zhang, Mengjie},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={7},
  pages={1414--1430},
  year={2016},
  publisher={IEEE}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1406--1415},
  year={2019}
}