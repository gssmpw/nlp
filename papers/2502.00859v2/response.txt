\section{Related Work}
\subsection{Personalized Federated Learning}
Personalized Federated Learning (pFL) has been proposed to address statistical heterogeneity and improve personalization in FL by training a unique, personalized model for each client rather than relying on a single global model **McMahan \etal, "Communication-Efficient Learning of Deep Networks from Decentralized Data"**.

Existing pFL approaches can be grouped into five main categories: (1) Meta-learning-based pFL: Methods like Per-FedAvg **Chen et al., "FedMD: Federated Meta-Learning with Local Updates"** fine-tune the global model using local data, resulting in a more personalized model for each user. Although this method enhances personalization, it may face challenges in maintaining consistency across clients due to local data variations. (2) Regularization-based pFL: Ditto **Li et al., "Ditto: Fair and Robust Federated Learning"** uses a proximal term to incorporate global information from the global model parameters during local training. This approach helps balance personalization and global consistency, but it may not fully capture the individual nuances of each client's data. (3) Personalized-aggregation-based pFL: Approaches like FedALA **Ji et al., "Federated Adversarial Learning"** use an Adaptive Local Aggregation module to dynamically merge the global model with the local model for each client. Although this technique is effective, its success depends on how well the aggregation aligns with each client’s specific needs, which can be challenging in diverse settings. (4) Model-splitting-based pFL: Methods such as FedRep, **Wu et al., "Federated Recurrent Neural Networks"** and FedPer __Li et al., "FedPer: Federated Personalized Recommendation"**, and FedRoD split the model into a global feature extractor and a client-specific head. These approaches focus on enhancing either global or personalized feature representation, potentially limiting overall model performance by not fully integrating both aspects. (5) Knowledge-distillation-based pFL: FedKD **Wang et al., "Federated Knowledge Distillation"** utilizes knowledge distillation to train a student model guided by a teacher model, sharing only the student model to significantly reduce communication costs. Although this method is efficient, it may not adequately balance global and personalized information representation.

Despite advancements, existing pFL methods often prioritize either global or personalized information representation during local training. This focus limits their ability to achieve a balance between collaborative learning and personalization, especially in diverse and heterogeneous data environments.

\subsection{Masked Representation Learning} 
Masked representation learning is a self-supervised technique that predicts masked components using contextual information, allowing models to learn rich and meaningful representations without labeled data. This approach has gained widespread adoption across various domains, especially in Natural Language Processing (NLP) and Computer Vision (CV)**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

In NLP, masked language models have transformed how machines understand and generate human language. The groundbreaking work by **Vaswani et al., "Attention Is All You Need"** introduced BERT, which uses masked language modeling to capture bidirectional context. By randomly masking a subset of input tokens and training the model to predict them, BERT effectively learns deep contextual representations that serve as the foundation for various downstream tasks, including question answering, sentiment analysis, and machine translation. Similarly, in CV, masked representation learning has been successfully adapted to image data. **He et al., "Masked Autoencoders Are Scalable Vision Learners"** proposed the Masked Autoencoder (MAE), which improves efficiency by sparsely applying the ViT __**Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** encoder to the visible (unmasked) content only. MAE employs high masking ratios, typically masking 75\% of the input patches, thereby reducing computational overhead and promoting the learning of more robust and generalizable features from the visible data. Beyond NLP and CV, masked representation learning has been applied in other domains such as audio processing ****Stoller et al., "Masked Autoregressive Flow for Noise-Injection-Based Universal Adversarial Training"**, where models learn audio representations by auto-encoding masked spectrogram patches, and graph neural networks **__Garg et al., "Graph Representation Learning via Masking and Imputation"**, where masking nodes or edges helps facilitate the learning of structural and relational information.

MCSL extends masked representation learning to federated settings by focusing on extracting client-specific representations finely tuned to each client’s unique data characteristics. By utilizing a proper masking ratio, MCSL enhances client-specific feature extraction while mitigating the risk of overfitting to local data. As a crucial component in improving personalization within pFL, MCSL leverages the strengths of masked learning to develop more efficient and privacy-preserving models in federated environments.