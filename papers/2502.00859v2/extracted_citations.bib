@inproceedings{NEURIPS2020_24389bfe,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{arivazhagan2019federated,
  title={Federated learning with personalization layers},
  author={Arivazhagan, Manoj Ghuhan and Aggarwal, Vinay and Singh, Aaditya Kumar and Choudhary, Sunav},
  journal={arXiv preprint arXiv:1912.00818},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{fang2023eva,
  title={Eva: Exploring the limits of masked visual representation learning at scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19358--19369},
  year={2023}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{li2023s,
  title={What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders},
  author={Li, Jintang and Wu, Ruofan and Sun, Wangbin and Chen, Liang and Tian, Sheng and Zhu, Liang and Meng, Changhua and Zheng, Zibin and Wang, Weiqiang},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1268--1279},
  year={2023}
}

@inproceedings{niizumi2022masked,
  title={Masked spectrogram modeling using masked autoencoders for learning general-purpose audio representation},
  author={Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio},
  booktitle={HEAR: Holistic Evaluation of Audio Representations},
  pages={1--24},
  year={2022},
  organization={PMLR}
}

@InProceedings{pmlr-v139-li21h,
  title = 	 {Ditto: Fair and Robust Federated Learning Through Personalization},
  author =       {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6357--6368},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21h.html},
  abstract = 	 {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.}
}

@article{t2020personalized,
  title={Personalized federated learning with moreau envelopes},
  author={T Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21394--21405},
  year={2020}
}

@article{wu2022communication,
  title={Communication-efficient federated learning via knowledge distillation},
  author={Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={2032},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{yi2023fedgh,
  title={Fedgh: Heterogeneous federated learning with generalized global header},
  author={Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={8686--8696},
  year={2023}
}

@inproceedings{zhang2023fedala,
  title={Fedala: Adaptive local aggregation for personalized federated learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={11237--11244},
  year={2023}
}

@article{zhang2023pfllib,
  title={PFLlib: Personalized Federated Learning Algorithm Library},
  author={Zhang, Jianqing and Liu, Yang and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Cao, Jian},
  journal={arXiv preprint arXiv:2312.04992},
  year={2023}
}

