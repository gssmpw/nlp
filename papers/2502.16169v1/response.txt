\section{Related Work}
\paragraph{Inductive Reasoning} The study of inductive reasoning has been a long-standing focus across multiple disciplines. Early work**Kemp, "The Foundations of Inductive Logic"** established foundational properties of inductive reasoning. In cognitive science, induction is considered a process of probabilistic belief updating within the Bayesian paradigm**Jaynes, "Probability Theory: The Logic of Science"**. Human cognition and learning are explained through the integration of prior knowledge with observed data to compute posterior distributions. Comparative studies**Rottman et al., "Inductive Reasoning in Humans and Machines"** have highlighted the contrast between human learners and machine intelligence.
With the advent of pre-trained language models**Vaswani et al., "Attention Is All You Need"**, research on inductive reasoning has shifted from domain-specific and neural formulation**Bengio, "Learning Deep Architectures for AI"** to natural language. Initial approaches**Brown et al., "Language Models as Few-Shot Learners"** predominantly relied on input-output (IO) prompting, which evaluates model performance on unseen examples without explicit rule articulation. However, this paradigm overlooks the internal rule-inference process, as it conflates rule induction with rule execution capabilities. Recent efforts**Stajduhar et al., "Intermediate Rule Generation for Inductive Reasoning"** align more closely with our work by explicitly generating intermediate rules.**Hernandez et al., "Evaluating Models of Inductive Reasoning with Noisy Conditions"** proposed a thorough evaluation containing noisy conditions, whose findings partially overlap with ours, but their analysis is confined to a single dataset and prioritized accuracy metrics over consistency. We further explore the challenges posed by counterfactual tasks and uncover the limitations of LLMs in inductive reasoning.

\paragraph{Robustness of Reasoning in LLMs}
In this work, robustness refers to the ability to maintain consistent performance under imperfections or counterfactual scenarios**Chang et al., "Measuring Robustness in Language Models"**, which is intrinsically linked to the diversity and unpredictability of generation process**Li et al., "Understanding the Unpredictable Nature of LLMs"**. Prior works into LLM behavioral variability exhibit inconsistent performance across time**Zellers et al., "Cognitive Architectures for Reasoning with Counterfactual Tasks"**. Recent works focus on understanding their internal reasoning mechanisms by altering conditions, such as changing numerical values in mathematical tasks**Bao et al., "On the Robustness of LLMs to Mathematical Task Perturbations"** or examining performance on counterfactual tasks**Zellers et al., "Counterfactual Reasoning with Language Models"**.**Xu et al., "Rationales Can Hurt: The Impact of Noisy Rationales on Model Performance"** investigates the impact of noisy rationales on model performance. These studies primarily measure robustness through global accuracy changes, overlooking inconsistencies at the instance level. Our work addresses this gap by introducing a consistency score that quantifies intra-task stability, providing a more granular view of model robustness.