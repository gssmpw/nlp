\section{Related Work}
\paragraph{Inductive Reasoning} The study of inductive reasoning has been a long-standing focus across multiple disciplines. Early work~\cite{Heit2000} established foundational properties of inductive reasoning. In cognitive science, induction is considered a process of probabilistic belief updating within the Bayesian paradigm~\cite{doi:10.1126/science.1192788}. Human cognition and learning are explained through the integration of prior knowledge with observed data to compute posterior distributions. Comparative studies~\cite{doi:10.1126/science.aab3050,Lake_Ullman_Tenenbaum_Gershman_2017} have highlighted the contrast between human learners and machine intelligence.
With the advent of pre-trained language models~\cite{brown2020language}, research on inductive reasoning has shifted from domain-specific and neural formulation~\cite{tian2020learning,odena2021bustlebottomupprogramsynthesis,SABLEMEYER2022101527} to natural language. Initial approaches~\cite{pmlr-v139-alet21a,ijcai2024p693,mirchandani2023large,yang-etal-2024-language} predominantly relied on input-output (IO) prompting, which evaluates model performance on unseen examples without explicit rule articulation. However, this paradigm overlooks the internal rule-inference process, as it conflates rule induction with rule execution capabilities. Recent efforts~\cite{wang2024hypothesis} align more closely with our work by explicitly generating intermediate rules.~\citet{qiu2024phenomenal} proposed a thorough evaluation containing noisy conditions, whose findings partially overlap with ours, but their analysis is confined to a single dataset and prioritized accuracy metrics over consistency. We further explore the challenges posed by counterfactual tasks and uncover the limitations of LLMs in inductive reasoning.

\paragraph{Robustness of Reasoning in LLMs}
In this work, robustness refers to the ability to maintain consistent performance under imperfections or counterfactual scenarios~\cite{elazar-etal-2021-measuring}, which is intrinsically linked to the diversity and unpredictability of generation process~\cite{zhang2023sirenssongaiocean,huang2025hallu}. Prior works into LLM behavioral variability exhibit inconsistent performance across time~\cite{tu2024chatlogcarefullyevaluatingevolution,chen2023chatgptsbehaviorchangingtime}. Recent works focus on understanding their internal reasoning mechanisms by altering conditions, such as changing numerical values in mathematical tasks~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,huang2025mathperturbbenchmarkingllmsmath} or examining performance on counterfactual tasks~\cite{wu-etal-2024-reasoning}.~\citet{zhou2024can} investigates the impact of noisy rationales on model performance. These studies primarily measure robustness through global accuracy changes, overlooking inconsistencies at the instance level. Our work addresses this gap by introducing a consistency score that quantifies intra-task stability, providing a more granular view of model robustness.